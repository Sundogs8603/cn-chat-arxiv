<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Weaver&#26159;&#19968;&#31995;&#21015;&#19987;&#27880;&#20110;&#21019;&#20316;&#20869;&#23481;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#20197;&#26356;&#20154;&#31867;&#21270;&#30340;&#26041;&#24335;&#29983;&#25104;&#25991;&#26412;&#21644;&#36981;&#24490;&#22810;&#26679;&#30340;&#20869;&#23481;&#21019;&#20316;&#25351;&#23548;&#12290;&#19981;&#21516;&#22823;&#23567;&#30340;Weaver&#27169;&#22411;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#21160;&#24577;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Weaver&#27169;&#22411;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20889;&#20316;&#33021;&#21147;&#19978;&#32988;&#36807;&#36890;&#29992;&#22411;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.17268</link><description>&lt;p&gt;
Weaver: &#21019;&#24847;&#20889;&#20316;&#30340;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Weaver: Foundation Models for Creative Writing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17268
&lt;/p&gt;
&lt;p&gt;
Weaver&#26159;&#19968;&#31995;&#21015;&#19987;&#27880;&#20110;&#21019;&#20316;&#20869;&#23481;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#20197;&#26356;&#20154;&#31867;&#21270;&#30340;&#26041;&#24335;&#29983;&#25104;&#25991;&#26412;&#21644;&#36981;&#24490;&#22810;&#26679;&#30340;&#20869;&#23481;&#21019;&#20316;&#25351;&#23548;&#12290;&#19981;&#21516;&#22823;&#23567;&#30340;Weaver&#27169;&#22411;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#21160;&#24577;&#20998;&#37197;&#35745;&#31639;&#36164;&#28304;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Weaver&#27169;&#22411;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20889;&#20316;&#33021;&#21147;&#19978;&#32988;&#36807;&#36890;&#29992;&#22411;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; Weaver&#65292;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#19987;&#27880;&#20110;&#20869;&#23481;&#21019;&#20316;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#31995;&#21015;&#12290;Weaver &#22312;&#19968;&#20010;&#31934;&#36873;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#19987;&#27880;&#20110;&#25552;&#21319;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20889;&#20316;&#33021;&#21147;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#22871;&#26032;&#39062;&#30340;&#26041;&#27861;&#36827;&#34892;&#25351;&#23548;&#25968;&#25454;&#21512;&#25104;&#21644; LLM &#23545;&#40784;&#65292;&#23545; Weaver &#36827;&#34892;&#21019;&#24847;&#21644;&#32844;&#19994;&#20889;&#20316;&#30340;&#24494;&#35843;&#65292;&#20351;&#20854;&#33021;&#22815;&#20135;&#29983;&#26356;&#21152;&#20154;&#31867;&#21270;&#30340;&#25991;&#26412;&#24182;&#36981;&#24490;&#26356;&#21152;&#22810;&#26679;&#30340;&#20869;&#23481;&#21019;&#20316;&#25351;&#23548;&#12290;Weaver &#31995;&#21015;&#21253;&#25324; Weaver Mini&#65288;1.8B&#65289;&#12289;Weaver Base&#65288;6B&#65289;&#12289;Weaver Pro&#65288;14B&#65289;&#21644; Weaver Ultra&#65288;34B&#65289;&#31561;&#19981;&#21516;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#26681;&#25454;&#26597;&#35810;&#22797;&#26434;&#24230;&#30001;&#36335;&#30001;&#20195;&#29702;&#21160;&#24577;&#20998;&#37197;&#65292;&#20174;&#32780;&#24179;&#34913;&#21709;&#24212;&#36136;&#37327;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#32463;&#36807;&#31934;&#24515;&#31574;&#21010;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20889;&#20316;&#33021;&#21147;&#35780;&#20272;&#22522;&#20934;&#19978;&#30340;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;Weaver &#30340;&#25152;&#26377;&#27169;&#22411;&#23610;&#23544;&#32988;&#36807;&#20102;&#36890;&#29992;&#22411;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work introduces Weaver, our first family of large language models (LLMs) dedicated to content creation. Weaver is pre-trained on a carefully selected corpus that focuses on improving the writing capabilities of large language models. We then fine-tune Weaver for creative and professional writing purposes and align it to the preference of professional writers using a suit of novel methods for instruction data synthesis and LLM alignment, making it able to produce more human-like texts and follow more diverse instructions for content creation. The Weaver family consists of models of Weaver Mini (1.8B), Weaver Base (6B), Weaver Pro (14B), and Weaver Ultra (34B) sizes, suitable for different applications and can be dynamically dispatched by a routing agent according to query complexity to balance response quality and computation cost. Evaluation on a carefully curated benchmark for assessing the writing capabilities of LLMs shows Weaver models of all sizes outperform generalist LLMs s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AudioSeal&#30340;&#38899;&#39057;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#22120;/&#26816;&#27979;&#22120;&#26550;&#26500;&#21644;&#26412;&#22320;&#21270;&#25439;&#22833;&#35757;&#32451;&#26469;&#23454;&#29616;&#26412;&#22320;&#21270;&#27700;&#21360;&#26816;&#27979;&#65292;&#21516;&#26102;&#24341;&#29992;&#20102;&#21463;&#21548;&#35273;&#23631;&#34109;&#21551;&#21457;&#30340;&#24863;&#30693;&#25439;&#22833;&#65292;&#20197;&#25552;&#39640;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;&#27492;&#25216;&#26415;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#24555;&#36895;&#30340;&#21333;&#36890;&#36947;&#26816;&#27979;&#22120;&#65292;&#21487;&#22312;&#22823;&#35268;&#27169;&#21644;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#26356;&#24555;&#30340;&#26816;&#27979;&#36895;&#24230;&#12290;</title><link>https://arxiv.org/abs/2401.17264</link><description>&lt;p&gt;
&#38024;&#23545;&#26412;&#22320;&#21270;&#27700;&#21360;&#25216;&#26415;&#30340;&#22768;&#38899;&#20811;&#38534;&#20027;&#21160;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Proactive Detection of Voice Cloning with Localized Watermarking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17264
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AudioSeal&#30340;&#38899;&#39057;&#27700;&#21360;&#25216;&#26415;&#65292;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#22120;/&#26816;&#27979;&#22120;&#26550;&#26500;&#21644;&#26412;&#22320;&#21270;&#25439;&#22833;&#35757;&#32451;&#26469;&#23454;&#29616;&#26412;&#22320;&#21270;&#27700;&#21360;&#26816;&#27979;&#65292;&#21516;&#26102;&#24341;&#29992;&#20102;&#21463;&#21548;&#35273;&#23631;&#34109;&#21551;&#21457;&#30340;&#24863;&#30693;&#25439;&#22833;&#65292;&#20197;&#25552;&#39640;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;&#27492;&#25216;&#26415;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#20855;&#26377;&#24555;&#36895;&#30340;&#21333;&#36890;&#36947;&#26816;&#27979;&#22120;&#65292;&#21487;&#22312;&#22823;&#35268;&#27169;&#21644;&#23454;&#26102;&#24212;&#29992;&#20013;&#23454;&#29616;&#26356;&#24555;&#30340;&#26816;&#27979;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#38899;&#29983;&#25104;&#27169;&#22411;&#24555;&#36895;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#65292;&#23545;&#22768;&#38899;&#20811;&#38534;&#39118;&#38505;&#36827;&#34892;&#22768;&#38899;&#30495;&#23454;&#24615;&#20445;&#35777;&#21464;&#24471;&#36843;&#20999;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AudioSeal&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#29992;&#20110;&#26412;&#22320;&#21270;&#26816;&#27979;AI&#29983;&#25104;&#35821;&#38899;&#30340;&#38899;&#39057;&#27700;&#21360;&#25216;&#26415;&#12290;AudioSeal&#37319;&#29992;&#20102;&#29983;&#25104;&#22120;/&#26816;&#27979;&#22120;&#26550;&#26500;&#65292;&#19982;&#26412;&#22320;&#21270;&#25439;&#22833;&#19968;&#36215;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#23454;&#29616;&#37319;&#26679;&#32423;&#21035;&#30340;&#26412;&#22320;&#21270;&#27700;&#21360;&#26816;&#27979;&#65292;&#24182;&#37319;&#29992;&#20102;&#21463;&#21548;&#35273;&#23631;&#34109;&#21551;&#21457;&#30340;&#26032;&#22411;&#24863;&#30693;&#25439;&#22833;&#65292;&#20351;AudioSeal&#33021;&#22815;&#26356;&#22909;&#22320;&#23454;&#29616;&#19981;&#21487;&#23519;&#35273;&#24615;&#12290;&#22312;&#33258;&#21160;&#21644;&#20154;&#24037;&#35780;&#20272;&#25351;&#26631;&#19978;&#65292;AudioSeal&#22312;&#30495;&#23454;&#29983;&#27963;&#38899;&#39057;&#22788;&#29702;&#21644;&#19981;&#21487;&#23519;&#35273;&#24615;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;AudioSeal&#37319;&#29992;&#20102;&#24555;&#36895;&#30340;&#21333;&#36890;&#36947;&#26816;&#27979;&#22120;&#65292;&#22823;&#22823;&#36229;&#36807;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#36895;&#24230;&#65292;&#23454;&#29616;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#30340;&#26356;&#24555;&#26816;&#27979;&#65292;&#20351;&#20854;&#25104;&#20026;&#22823;&#35268;&#27169;&#21644;&#23454;&#26102;&#24212;&#29992;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of speech generative models, there is a pressing need to ensure audio authenticity against the risks of voice cloning. We present AudioSeal, the first audio watermarking technique designed specifically for localized detection of AI-generated speech. AudioSeal employs a generator/detector architecture trained jointly with a localization loss to enable localized watermark detection up to the sample level, and a novel perceptual loss inspired by auditory masking, that enables AudioSeal to achieve better imperceptibility. AudioSeal achieves state-of-the-art performance in terms of robustness to real life audio manipulations and imperceptibility based on automatic and human evaluation metrics. Additionally, AudioSeal is designed with a fast, single-pass detector, that significantly surpasses existing models in speed - achieving detection up to two orders of magnitude faster, making it ideal for large-scale and real-time applications.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2401.17263</link><description>&lt;p&gt;
&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Robust Prompt Optimization for Defending Language Models Against Jailbreaking Attacks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17263
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#25552;&#31034;&#20248;&#21270;&#31639;&#27861;&#65288;RPO&#65289;&#29992;&#20110;&#23545;&#25239;&#35821;&#35328;&#27169;&#22411;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#36890;&#36807;&#26799;&#24230;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#65292;&#24182;&#25104;&#21151;&#38477;&#20302;&#20102;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#20154;&#24037;&#26234;&#33021;&#23545;&#40784;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#25110;&#30772;&#35299;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#20854;&#20013;&#23545;&#25163;&#20462;&#25913;&#36755;&#20837;&#25552;&#31034;&#20197;&#35825;&#23548;&#26377;&#23475;&#34892;&#20026;&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#38450;&#24481;&#26041;&#27861;&#65292;&#20294;&#23427;&#20204;&#20165;&#20851;&#27880;&#29421;&#31364;&#30340;&#23041;&#32961;&#27169;&#22411;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#24378;&#22823;&#30340;&#38450;&#24481;&#12290;&#20026;&#20102;&#23454;&#29616;&#24378;&#22823;&#30340;&#38450;&#24481;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#29992;&#20110;&#23545;&#25239;&#30772;&#35299;&#25915;&#20987;&#30340;&#23545;&#25239;&#30446;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#25552;&#31034;&#20248;&#21270;&#65288;RPO&#65289;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#22522;&#20110;&#26799;&#24230;&#30340;&#20196;&#29260;&#20248;&#21270;&#26469;&#30830;&#20445;&#36755;&#20986;&#30340;&#26080;&#23475;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#26131;&#20110;&#35775;&#38382;&#30340;&#21518;&#32512;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#23545;&#30772;&#35299;&#25915;&#20987;&#30340;&#24378;&#38887;&#24615;&#65292;&#21253;&#25324;&#20248;&#21270;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#30772;&#35299;&#25915;&#20987;&#20197;&#21450;&#26410;&#30693;&#30340;&#30772;&#35299;&#25915;&#20987;&#65292;&#23558;&#25915;&#20987;&#25104;&#21151;&#29575;&#20174;84%&#38477;&#20302;&#21040;8.66%&#65292;&#22312;20&#20010;&#30772;&#35299;&#25915;&#20987;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#29616;RPO&#23545;&#27491;&#24120;LM&#20351;&#29992;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#22312;&#36866;&#24212;&#24615;&#25915;&#20987;&#19979;&#20173;&#28982;&#26377;&#25928;&#65292;&#24182;&#19988;&#21487;&#20197;&#36801;&#31227;&#21040;&#40657;&#30418;&#27169;&#22411;&#20013;&#65292;&#38477;&#20302;&#25915;&#20987;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite advances in AI alignment, language models (LM) remain vulnerable to adversarial attacks or jailbreaking, in which adversaries modify input prompts to induce harmful behavior. While some defenses have been proposed, they focus on narrow threat models and fall short of a strong defense, which we posit should be effective, universal, and practical. To achieve this, we propose the first adversarial objective for defending LMs against jailbreaking attacks and an algorithm, robust prompt optimization (RPO), that uses gradient-based token optimization to enforce harmless outputs. This results in an easily accessible suffix that significantly improves robustness to both jailbreaks seen during optimization and unknown, held-out jailbreaks, reducing the attack success rate on Starling-7B from 84% to 8.66% across 20 jailbreaks. In addition, we find that RPO has a minor effect on normal LM use, is successful under adaptive attacks, and can transfer to black-box models, reducing the success
&lt;/p&gt;</description></item><item><title>LLaMP&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35299;&#21644;&#38598;&#25104;&#21508;&#31181;&#26448;&#26009;&#31185;&#23398;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#65292;&#22788;&#29702;&#39640;&#38454;&#25968;&#25454;&#20197;&#21450;&#24635;&#32467;&#22266;&#24577;&#21512;&#25104;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;LLaMP&#26377;&#25928;&#32416;&#27491;&#20102;GPT-3.5&#20869;&#37096;&#30693;&#35782;&#30340;&#38169;&#35823;&#12290;</title><link>https://arxiv.org/abs/2401.17244</link><description>&lt;p&gt;
LLaMP: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#39640;&#20445;&#30495;&#26448;&#26009;&#30693;&#35782;&#26816;&#32034;&#21644;&#25552;&#28860;&#20013;&#30340;&#24378;&#22823;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
LLaMP: Large Language Model Made Powerful for High-fidelity Materials Knowledge Retrieval and Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17244
&lt;/p&gt;
&lt;p&gt;
LLaMP&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#36827;&#34892;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#29702;&#35299;&#21644;&#38598;&#25104;&#21508;&#31181;&#26448;&#26009;&#31185;&#23398;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#26816;&#32034;&#30456;&#20851;&#25968;&#25454;&#65292;&#22788;&#29702;&#39640;&#38454;&#25968;&#25454;&#20197;&#21450;&#24635;&#32467;&#22266;&#24577;&#21512;&#25104;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;LLaMP&#26377;&#25928;&#32416;&#27491;&#20102;GPT-3.5&#20869;&#37096;&#30693;&#35782;&#30340;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#38169;&#35823;&#20449;&#24687;&#23545;&#20110;&#31185;&#23398;&#20013;&#30340;&#21487;&#37325;&#22797;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;LLM&#22825;&#29983;&#32570;&#20047;&#38271;&#26399;&#35760;&#24518;&#65292;&#22240;&#27492;&#22312;&#29305;&#23450;&#39046;&#22495;&#30340;&#25991;&#29486;&#21644;&#25968;&#25454;&#19978;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#26159;&#19968;&#20010;&#38750;&#24120;&#22256;&#38590;&#12289;&#20020;&#26102;&#30340;&#21644;&#19981;&#21487;&#36991;&#20813;&#20855;&#26377;&#20559;&#35265;&#30340;&#20219;&#21153;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;LLaMP&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26694;&#26550;&#65292;&#30001;&#22810;&#20010;&#25968;&#25454;&#24863;&#30693;&#30340;&#25512;&#29702;&#19982;&#34892;&#21160;&#65288;ReAct&#65289;&#26234;&#33021;&#20307;&#21160;&#24577;&#19982;Materials Project (MP)&#19978;&#30340;&#35745;&#31639;&#21644;&#23454;&#39564;&#25968;&#25454;&#36827;&#34892;&#20132;&#20114;&#12290;&#22312;&#26080;&#38656;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;LLaMP&#23637;&#31034;&#20102;&#29702;&#35299;&#21644;&#38598;&#25104;&#21508;&#31181;&#26041;&#24335;&#30340;&#26448;&#26009;&#31185;&#23398;&#27010;&#24565;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#21363;&#26102;&#33719;&#21462;&#30456;&#20851;&#25968;&#25454;&#23384;&#20648;&#65292;&#22788;&#29702;&#39640;&#38454;&#25968;&#25454;&#65288;&#22914;&#26230;&#20307;&#32467;&#26500;&#21644;&#24377;&#24615;&#24352;&#37327;&#65289;&#65292;&#24182;&#24635;&#32467;&#22266;&#24577;&#21512;&#25104;&#30340;&#22810;&#27493;&#39588;&#36807;&#31243;&#12290;&#25105;&#20204;&#35777;&#26126;LLaMP&#26377;&#25928;&#32416;&#27491;&#20102;GPT-3.5&#20869;&#37096;&#30693;&#35782;&#30340;&#38169;&#35823;&#65292;&#23558;&#39057;&#32321;&#35760;&#24405;&#30340;&#33021;&#24102;&#38388;&#38553;MAPE&#38477;&#20302;&#20102;5.21%&#65292;&#23558;&#26174;&#33879;&#30340;&#38169;&#35823;&#38477;&#20302;&#20102;1103.54%
&lt;/p&gt;
&lt;p&gt;
Reducing hallucination of Large Language Models (LLMs) is imperative for use in the sciences where reproducibility is crucial. However, LLMs inherently lack long-term memory, making it a nontrivial, ad hoc, and inevitably biased task to fine-tune them on domain-specific literature and data. Here we introduce LLaMP, a multimodal retrieval-augmented generation (RAG) framework of multiple data-aware reasoning-and-acting (ReAct) agents that dynamically interact with computational and experimental data on Materials Project (MP). Without fine-tuning, LLaMP demonstrates an ability to comprehend and integrate various modalities of materials science concepts, fetch relevant data stores on the fly, process higher-order data (such as crystal structures and elastic tensors), and summarize multi-step procedures for solid-state synthesis. We show that LLaMP effectively corrects errors in GPT-3.5's intrinsic knowledge, reducing a 5.21% MAPE on frequently-documented bandgaps and a significant 1103.54%
&lt;/p&gt;</description></item><item><title>ESPnet-SPK&#26159;&#19968;&#20010;&#20840;&#27969;&#31243;&#35828;&#35805;&#20154;&#23884;&#20837;&#24037;&#20855;&#21253;&#65292;&#20855;&#22791;&#21487;&#22797;&#29616;&#30340;&#37197;&#26041;&#12289;&#33258;&#30417;&#30563;&#30340;&#21069;&#31471;&#21644;&#29616;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#36731;&#26494;&#26500;&#24314;&#27169;&#22411;&#24182;&#19982;&#20854;&#20182;&#39046;&#22495;&#36827;&#34892;&#38598;&#25104;&#12290;&#20854;&#36890;&#36807;&#20248;&#21270;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#22810;&#26679;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17230</link><description>&lt;p&gt;
ESPnet-SPK:&#20855;&#22791;&#21487;&#22797;&#29616;&#30340;&#37197;&#26041;&#65292;&#33258;&#30417;&#30563;&#30340;&#21069;&#31471;&#21644;&#29616;&#25104;&#27169;&#22411;&#30340;&#20840;&#27969;&#31243;&#35828;&#35805;&#20154;&#23884;&#20837;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
ESPnet-SPK: full pipeline speaker embedding toolkit with reproducible recipes, self-supervised front-ends, and off-the-shelf models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17230
&lt;/p&gt;
&lt;p&gt;
ESPnet-SPK&#26159;&#19968;&#20010;&#20840;&#27969;&#31243;&#35828;&#35805;&#20154;&#23884;&#20837;&#24037;&#20855;&#21253;&#65292;&#20855;&#22791;&#21487;&#22797;&#29616;&#30340;&#37197;&#26041;&#12289;&#33258;&#30417;&#30563;&#30340;&#21069;&#31471;&#21644;&#29616;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#36731;&#26494;&#26500;&#24314;&#27169;&#22411;&#24182;&#19982;&#20854;&#20182;&#39046;&#22495;&#36827;&#34892;&#38598;&#25104;&#12290;&#20854;&#36890;&#36807;&#20248;&#21270;&#30340;&#26550;&#26500;&#35774;&#35745;&#21644;&#22810;&#26679;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#29305;&#24449;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#30340;&#20986;&#33394;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ESPnet-SPK&#65292;&#19968;&#20010;&#19987;&#20026;&#35757;&#32451;&#35828;&#35805;&#20154;&#23884;&#20837;&#25552;&#21462;&#22120;&#35774;&#35745;&#30340;&#24037;&#20855;&#21253;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#20379;&#35828;&#35805;&#20154;&#35782;&#21035;&#31038;&#21306;&#30340;&#30740;&#31350;&#20154;&#21592;&#36731;&#26494;&#26500;&#24314;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#20010;&#27169;&#22411;&#65292;&#20174;x-vector&#21040;&#26368;&#36817;&#30340;SKA-TDNN&#12290;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#21487;&#20197;&#36731;&#26494;&#24320;&#21457;&#21464;&#20307;&#12290;&#25105;&#20204;&#36824;&#24076;&#26395;&#23558;&#24320;&#21457;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#39046;&#22495;&#36827;&#34892;&#36830;&#25509;&#65292;&#20174;&#32780;&#20351;&#24191;&#27867;&#30340;&#30740;&#31350;&#31038;&#21306;&#33021;&#22815;&#36731;&#26494;&#22320;&#25972;&#21512;&#26368;&#20808;&#36827;&#30340;&#23884;&#20837;&#25552;&#21462;&#22120;&#12290;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#25552;&#21462;&#22120;&#21487;&#20197;&#20197;&#29616;&#25104;&#30340;&#26041;&#24335;&#35775;&#38382;&#65292;&#24182;&#36890;&#36807;&#23637;&#31034;&#20854;&#19982;&#20004;&#20010;&#20219;&#21153;&#30340;&#38598;&#25104;&#26469;&#23637;&#31034;&#24037;&#20855;&#21253;&#30340;&#22810;&#21151;&#33021;&#24615;&#12290;&#21478;&#19968;&#20010;&#30446;&#26631;&#26159;&#19982;&#22810;&#26679;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#29305;&#24449;&#38598;&#25104;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#19968;&#20010;&#21487;&#22797;&#29616;&#30340;&#37197;&#26041;&#65292;&#20351;&#29992;WavLM-Large&#21644;ECAPA-TDNN&#22312;Vox1-O&#35780;&#20272;&#21327;&#35758;&#19978;&#23454;&#29616;&#20102;0.39%&#30340;&#31561;&#35823;&#24046;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces ESPnet-SPK, a toolkit designed with several objectives for training speaker embedding extractors. First, we provide an open-source platform for researchers in the speaker recognition community to effortlessly build models. We provide several models, ranging from x-vector to recent SKA-TDNN. Through the modularized architecture design, variants can be developed easily. We also aspire to bridge developed models with other domains, facilitating the broad research community to effortlessly incorporate state-of-the-art embedding extractors. Pre-trained embedding extractors can be accessed in an off-the-shelf manner and we demonstrate the toolkit's versatility by showcasing its integration with two tasks. Another goal is to integrate with diverse self-supervised learning features. We release a reproducible recipe that achieves an equal error rate of 0.39% on the Vox1-O evaluation protocol using WavLM-Large with ECAPA-TDNN.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;MouSi&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#35270;&#35273;&#19987;&#23478;&#30340;&#38598;&#25104;&#25216;&#26415;&#26469;&#21327;&#21516;&#22788;&#29702;&#22270;&#20687;&#30340;&#35270;&#35273;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21333;&#19968;&#35270;&#35273;&#32452;&#20214;&#33021;&#21147;&#19981;&#36275;&#21644;&#36807;&#38271;&#30340;&#35270;&#35273;&#26631;&#35760;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20301;&#32622;&#28322;&#20986;&#21644;&#38271;&#24230;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2401.17221</link><description>&lt;p&gt;
MouSi&#65306;&#22810;&#35270;&#35273;&#19987;&#23478;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MouSi: Poly-Visual-Expert Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;MouSi&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#35270;&#35273;&#19987;&#23478;&#30340;&#38598;&#25104;&#25216;&#26415;&#26469;&#21327;&#21516;&#22788;&#29702;&#22270;&#20687;&#30340;&#35270;&#35273;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#21333;&#19968;&#35270;&#35273;&#32452;&#20214;&#33021;&#21147;&#19981;&#36275;&#21644;&#36807;&#38271;&#30340;&#35270;&#35273;&#26631;&#35760;&#30340;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#25506;&#32034;&#19981;&#21516;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20301;&#32622;&#28322;&#20986;&#21644;&#38271;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#24120;&#24120;&#38754;&#20020;&#35832;&#22914;&#21333;&#19968;&#35270;&#35273;&#32452;&#20214;&#33021;&#21147;&#19981;&#36275;&#21644;&#36807;&#38271;&#30340;&#35270;&#35273;&#26631;&#35760;&#31561;&#25361;&#25112;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#38480;&#21046;&#27169;&#22411;&#22312;&#20934;&#30830;&#35299;&#35835;&#22797;&#26434;&#35270;&#35273;&#20449;&#24687;&#21644;&#36807;&#38271;&#19978;&#19979;&#25991;&#20449;&#24687;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#23545;&#20110;&#25552;&#39640;VLMs&#30340;&#24615;&#33021;&#21644;&#36866;&#29992;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#38598;&#25104;&#19987;&#23478;&#25216;&#26415;&#26469;&#21327;&#21516;&#21333;&#29420;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#25797;&#38271;&#22270;&#20687;&#25991;&#26412;&#21305;&#37197;&#12289;OCR&#12289;&#22270;&#20687;&#20998;&#21106;&#31561;&#12290;&#35813;&#25216;&#26415;&#24341;&#20837;&#20102;&#34701;&#21512;&#32593;&#32476;&#65292;&#32479;&#19968;&#22788;&#29702;&#26469;&#33258;&#19981;&#21516;&#35270;&#35273;&#19987;&#23478;&#30340;&#36755;&#20986;&#65292;&#21516;&#26102;&#24357;&#21512;&#20102;&#22270;&#20687;&#32534;&#30721;&#22120;&#21644;&#39044;&#35757;&#32451;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#65292;&#20197;&#20943;&#36731;&#38271;&#24207;&#21015;&#22270;&#20687;&#29305;&#24449;&#30340;&#20301;&#32622;&#32534;&#30721;&#28010;&#36153;&#38382;&#39064;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#20301;&#32622;&#28322;&#20986;&#21644;&#38271;&#24230;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current large vision-language models (VLMs) often encounter challenges such as insufficient capabilities of a single visual component and excessively long visual tokens. These issues can limit the model's effectiveness in accurately interpreting complex visual information and over-lengthy contextual information. Addressing these challenges is crucial for enhancing the performance and applicability of VLMs. This paper proposes the use of ensemble experts technique to synergizes the capabilities of individual visual encoders, including those skilled in image-text matching, OCR, image segmentation, etc. This technique introduces a fusion network to unify the processing of outputs from different visual experts, while bridging the gap between image encoders and pre-trained LLMs. In addition, we explore different positional encoding schemes to alleviate the waste of positional encoding caused by lengthy image feature sequences, effectively addressing the issue of position overflow and length
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;NormEnsembleXAI&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;XAI&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#12289;&#26368;&#22823;&#21644;&#24179;&#22343;&#20989;&#25968;&#19982;&#26631;&#20934;&#21270;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#23545;XAI&#38598;&#25104;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#24211;&#65292;&#26041;&#20415;&#23454;&#29616;&#36879;&#26126;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2401.17200</link><description>&lt;p&gt;
NormEnsembleXAI: &#25581;&#31034;XAI&#38598;&#25104;&#25216;&#26415;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;
&lt;/p&gt;
&lt;p&gt;
NormEnsembleXAI: Unveiling the Strengths and Weaknesses of XAI Ensemble Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17200
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;NormEnsembleXAI&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;XAI&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#12289;&#26368;&#22823;&#21644;&#24179;&#22343;&#20989;&#25968;&#19982;&#26631;&#20934;&#21270;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#23545;XAI&#38598;&#25104;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#24211;&#65292;&#26041;&#20415;&#23454;&#29616;&#36879;&#26126;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#38598;&#25104;&#26041;&#27861;&#30340;&#20840;&#38754;&#27604;&#36739;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20570;&#20986;&#20102;&#19977;&#20010;&#37325;&#35201;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;NormEnsembleXAI&#65292;&#23427;&#21033;&#29992;&#26368;&#23567;&#12289;&#26368;&#22823;&#21644;&#24179;&#22343;&#20989;&#25968;&#19982;&#26631;&#20934;&#21270;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#23545;XAI&#38598;&#25104;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#24369;&#28857;&#36827;&#34892;&#20102;&#28145;&#20837;&#27934;&#23519;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#24211;&#65292;&#20419;&#36827;&#20102;XAI&#38598;&#25104;&#30340;&#23454;&#38469;&#23454;&#29616;&#65292;&#25512;&#21160;&#36879;&#26126;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive comparative analysis of explainable artificial intelligence (XAI) ensembling methods. Our research brings three significant contributions. Firstly, we introduce a novel ensembling method, NormEnsembleXAI, that leverages minimum, maximum, and average functions in conjunction with normalization techniques to enhance interpretability. Secondly, we offer insights into the strengths and weaknesses of XAI ensemble methods. Lastly, we provide a library, facilitating the practical implementation of XAI ensembling, thus promoting the adoption of transparent and interpretable deep learning models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Transformer&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;&#26500;&#24314;&#26497;&#21270;&#30721;&#30340;&#26041;&#27861;&#12290;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#20449;&#36947;&#26465;&#20214;&#19979;&#37117;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#20805;&#20998;&#21457;&#25381;&#20102;&#26497;&#21270;&#30721;&#22266;&#26377;&#30340;&#23884;&#22871;&#32467;&#26500;&#29305;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17188</link><description>&lt;p&gt;
&#36890;&#36807;Transformer&#23884;&#22871;&#26500;&#24314;&#26497;&#21270;&#30721;
&lt;/p&gt;
&lt;p&gt;
Nested Construction of Polar Codes via Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Transformer&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;&#26500;&#24314;&#26497;&#21270;&#30721;&#30340;&#26041;&#27861;&#12290;&#20223;&#30495;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#20449;&#36947;&#26465;&#20214;&#19979;&#37117;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#20805;&#20998;&#21457;&#25381;&#20102;&#26497;&#21270;&#30721;&#22266;&#26377;&#30340;&#23884;&#22871;&#32467;&#26500;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35299;&#30721;&#31639;&#27861;&#36229;&#36234;&#36880;&#27425;&#21462;&#28040;&#30340;&#24773;&#20917;&#19979;&#65292;&#20010;&#24615;&#21270;&#26497;&#21270;&#30721;&#26500;&#24314;&#19968;&#30452;&#26159;&#19968;&#20010;&#22791;&#21463;&#20851;&#27880;&#30340;&#35838;&#39064;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26497;&#21270;&#30721;&#20855;&#26377;&#22266;&#26377;&#30340;&#23884;&#22871;&#32467;&#26500;&#65292;&#20294;&#22312;&#26497;&#21270;&#30721;&#26500;&#24314;&#20013;&#20351;&#29992;&#24207;&#21015;&#27169;&#22411;&#30340;&#30740;&#31350;&#36824;&#19981;&#20805;&#20998;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#24207;&#21015;&#24314;&#27169;&#26694;&#26550;&#26469;&#36845;&#20195;&#22320;&#26500;&#36896;&#20219;&#20309;&#32473;&#23450;&#38271;&#24230;&#21644;&#36895;&#29575;&#19979;&#30340;&#26497;&#21270;&#30721;&#65292;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20449;&#36947;&#26465;&#20214;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;Transformer&#36827;&#34892;&#39034;&#24207;&#24314;&#27169;&#35774;&#35745;&#30340;&#26497;&#21270;&#30721;&#65292;&#22312;AWGN&#21644;Rayleigh&#34928;&#33853;&#20449;&#36947;&#19979;&#34920;&#29616;&#20248;&#20110;5G-NR&#24207;&#21015;&#21644;&#23494;&#24230;&#28436;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tailoring polar code construction for decoding algorithms beyond successive cancellation has remained a topic of significant interest in the field. However, despite the inherent nested structure of polar codes, the use of sequence models in polar code construction is understudied. In this work, we propose using a sequence modeling framework to iteratively construct a polar code for any given length and rate under various channel conditions. Simulations show that polar codes designed via sequential modeling using transformers outperform both 5G-NR sequence and Density Evolution based approaches for both AWGN and Rayleigh fading channels.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25345;&#32493;&#35821;&#35328;&#23398;&#20064;&#26469;&#25193;&#23637;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VL-PTMs&#65289;&#30340;&#35821;&#35328;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22686;&#37327;&#26356;&#26032;&#35821;&#35328;&#30693;&#35782;&#65292;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#36328;&#27169;&#24577;&#21644;&#36328;&#35821;&#35328;&#30446;&#26631;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2401.17186</link><description>&lt;p&gt;
&#22312;CLIP&#20013;&#25317;&#25265;&#35821;&#35328;&#21253;&#23481;&#24615;&#21644;&#22810;&#26679;&#24615;&#65306;&#36890;&#36807;&#25345;&#32493;&#35821;&#35328;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Embracing Language Inclusivity and Diversity in CLIP through Continual Language Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17186
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25345;&#32493;&#35821;&#35328;&#23398;&#20064;&#26469;&#25193;&#23637;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;VL-PTMs&#65289;&#30340;&#35821;&#35328;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#22686;&#37327;&#26356;&#26032;&#35821;&#35328;&#30693;&#35782;&#65292;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#22312;&#36328;&#27169;&#24577;&#21644;&#36328;&#35821;&#35328;&#30446;&#26631;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;(VL-PTMs)&#22312;&#22810;&#27169;&#24577;&#30740;&#31350;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#21482;&#25484;&#25569;&#20102;&#23569;&#25968;&#20960;&#31181;&#35821;&#35328;&#65288;&#27604;&#22914;&#33521;&#35821;&#65289;&#65292;&#38480;&#21046;&#20102;&#20854;&#22312;&#26356;&#24191;&#27867;&#30340;&#31038;&#21306;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#36890;&#36807;&#32852;&#21512;&#23398;&#20064;&#26469;&#24320;&#21457;&#22810;&#35821;&#35328;VL&#27169;&#22411;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#26114;&#36149;&#30340;&#25104;&#26412;&#21644;&#25968;&#25454;&#21487;&#29992;&#24615;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#19981;&#20999;&#23454;&#38469;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#25345;&#32493;&#35821;&#35328;&#23398;&#20064;&#65288;CLL&#65289;&#26469;&#25193;&#23637;VL-PTMs&#30340;&#35821;&#35328;&#33021;&#21147;&#65292;&#21363;&#27169;&#22411;&#38656;&#35201;&#22686;&#37327;&#22320;&#26356;&#26032;&#20854;&#35821;&#35328;&#30693;&#35782;&#65292;&#21516;&#26102;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#65288;CF&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;CLL-CLIP&#30340;&#27169;&#22411;&#65292;&#23427;&#26159;&#22312;CLIP&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#30340;&#65292;&#32780;CLIP&#26159;&#19968;&#31181;&#24050;&#32463;&#20855;&#22791;&#20102;&#22270;&#20687;-&#33521;&#35821;&#25991;&#26412;&#23545;&#40784;&#33021;&#21147;&#30340;&#27969;&#34892;VL-PTM&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CLL-CLIP&#21253;&#21547;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#35789;&#23884;&#20837;&#23618;&#65292;&#29992;&#20110;&#22788;&#29702;&#35821;&#35328;&#24046;&#24322;&#12290;&#23427;&#20165;&#35757;&#32451;&#35789;&#23884;&#20837;&#20197;&#25552;&#39640;&#20869;&#23384;&#31283;&#23450;&#24615;&#65292;&#24182;&#22312;&#36328;&#27169;&#24577;&#21644;&#36328;&#35821;&#35328;&#30446;&#26631;&#19979;&#36827;&#34892;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While vision-language pre-trained models (VL-PTMs) have advanced multimodal research in recent years, their mastery in a few languages like English restricts their applicability in broader communities. To this end, there is an increasing interest in developing multilingual VL models via a joint-learning setup, which, however, could be unrealistic due to expensive costs and data availability. In this work, we propose to extend VL-PTMs' language capacity by continual language learning (CLL), where a model needs to update its linguistic knowledge incrementally without suffering from catastrophic forgetting (CF). We begin our study by introducing a model dubbed CLL-CLIP, which builds upon CLIP, a prevailing VL-PTM that has acquired image-English text alignment. Specifically, CLL-CLIP contains an expandable token embedding layer to handle linguistic differences. It solely trains token embeddings to improve memory stability and is optimized under cross-modal and cross-lingual objectives to l
&lt;/p&gt;</description></item><item><title>GraphViz2Vec&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#29305;&#24449;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25429;&#25417;&#33410;&#28857;&#23616;&#37096;&#37051;&#22495;&#30340;&#32467;&#26500;&#20449;&#24687;&#26469;&#21019;&#24314;&#26377;&#24847;&#20041;&#30340;&#21021;&#22987;&#23884;&#20837;&#65292;&#24182;&#24110;&#21161;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2401.17178</link><description>&lt;p&gt;
GraphViz2Vec: &#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#29305;&#24449;&#29983;&#25104;&#27169;&#22411;&#65292;&#25552;&#39640;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#20998;&#31867;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
GraphViz2Vec: A Structure-aware Feature Generation Model to Improve Classification in GNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17178
&lt;/p&gt;
&lt;p&gt;
GraphViz2Vec&#26159;&#19968;&#31181;&#32467;&#26500;&#24863;&#30693;&#30340;&#29305;&#24449;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#25429;&#25417;&#33410;&#28857;&#23616;&#37096;&#37051;&#22495;&#30340;&#32467;&#26500;&#20449;&#24687;&#26469;&#21019;&#24314;&#26377;&#24847;&#20041;&#30340;&#21021;&#22987;&#23884;&#20837;&#65292;&#24182;&#24110;&#21161;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35299;&#20915;&#21253;&#25324;&#33410;&#28857;&#20998;&#31867;&#21644;&#38142;&#25509;&#39044;&#27979;&#22312;&#20869;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;GNN&#20307;&#31995;&#32467;&#26500;&#20551;&#35774;&#21021;&#22987;&#23884;&#20837;&#26159;&#38543;&#26426;&#30340;&#25110;&#20174;&#27969;&#34892;&#30340;&#20998;&#24067;&#20013;&#29983;&#25104;&#30340;&#12290;&#36825;&#20123;&#21021;&#22987;&#23884;&#20837;&#38656;&#35201;&#22810;&#23618;&#36716;&#25442;&#25165;&#33021;&#25910;&#25947;&#20026;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#34429;&#28982;&#23618;&#25968;&#21487;&#20197;&#32047;&#31215;&#33410;&#28857;&#30340;&#26356;&#22823;&#37051;&#22495;&#65292;&#20294;&#20063;&#20250;&#24341;&#20837;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;GNN&#23545;&#20110;&#34920;&#31034;&#32467;&#26500;&#20449;&#24687;&#25928;&#26524;&#19981;&#20339;&#12290;&#20363;&#22914;&#65292;&#33410;&#28857;&#30340;&#36755;&#20986;&#23884;&#20837;&#19981;&#20250;&#25429;&#25417;&#21040;&#20854;&#21442;&#19982;&#30340;&#19977;&#35282;&#24418;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;GraphViz2Vec&#65292;&#21487;&#20197;&#25429;&#25417;&#33410;&#28857;&#23616;&#37096;&#37051;&#22495;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20026;GNN&#27169;&#22411;&#21019;&#24314;&#26377;&#24847;&#20041;&#30340;&#21021;&#22987;&#23884;&#20837;&#12290;&#36825;&#20123;&#21021;&#22987;&#23884;&#20837;&#26377;&#21161;&#20110;&#29616;&#26377;&#27169;&#22411;&#22312;&#21508;&#31181;&#20998;&#31867;&#20219;&#21153;&#20013;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#21021;&#22987;&#23884;&#20837;&#36824;&#24110;&#21161;&#27169;&#22411;&#20135;&#29983;...
&lt;/p&gt;
&lt;p&gt;
GNNs are widely used to solve various tasks including node classification and link prediction. Most of the GNN architectures assume the initial embedding to be random or generated from popular distributions. These initial embeddings require multiple layers of transformation to converge into a meaningful latent representation. While number of layers allow accumulation of larger neighbourhood of a node it also introduce the problem of over-smoothing. In addition, GNNs are inept at representing structural information. For example, the output embedding of a node does not capture its triangles participation. In this paper, we presented a novel feature extraction methodology GraphViz2Vec that can capture the structural information of a node's local neighbourhood to create meaningful initial embeddings for a GNN model. These initial embeddings helps existing models achieve state-of-the-art results in various classification tasks. Further, these initial embeddings help the model to produce des
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#30340;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#20102;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#30340;&#36801;&#31227;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2401.17173</link><description>&lt;p&gt;
&#36890;&#36807;&#20989;&#25968;&#32534;&#30721;&#22120;&#23454;&#29616;&#38646;-shot&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zero-Shot Reinforcement Learning via Function Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#30340;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#20102;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#30340;&#36801;&#31227;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#35299;&#20915;&#35768;&#22810;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#65292;&#20294;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#38590;&#28857;&#22312;&#20110;&#23547;&#25214;&#19968;&#20010;&#33391;&#22909;&#30340;&#34920;&#31034;&#26469;&#34920;&#36798;&#24403;&#21069;&#20219;&#21153;&#65292;&#20197;&#20415;&#20195;&#29702;&#31243;&#24207;&#29702;&#35299;&#23427;&#19982;&#20808;&#21069;&#30475;&#21040;&#30340;&#20219;&#21153;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#23454;&#29616;&#38646;-shot&#36801;&#31227;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20989;&#25968;&#32534;&#30721;&#22120;&#65292;&#19968;&#31181;&#34920;&#31034;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#23558;&#20989;&#25968;&#34920;&#31034;&#20026;&#23398;&#20064;&#21040;&#30340;&#38750;&#32447;&#24615;&#22522;&#20989;&#25968;&#30340;&#21152;&#26435;&#32452;&#21512;&#12290;&#36890;&#36807;&#20351;&#29992;&#20989;&#25968;&#32534;&#30721;&#22120;&#26469;&#34920;&#31034;&#22870;&#21169;&#20989;&#25968;&#25110;&#36716;&#31227;&#20989;&#25968;&#65292;&#20195;&#29702;&#31243;&#24207;&#36890;&#36807;&#19968;&#20010;&#36830;&#36143;&#30340;&#21521;&#37327;&#34920;&#31034;&#26377;&#20851;&#24403;&#21069;&#20219;&#21153;&#19982;&#20808;&#21069;&#30475;&#21040;&#30340;&#20219;&#21153;&#30340;&#20851;&#32852;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#20195;&#29702;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#22312;&#30456;&#20851;&#20219;&#21153;&#20043;&#38388;&#23454;&#29616;&#36801;&#31227;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#23558;&#22522;&#26412;RL&#31639;&#27861;&#19982;&#20989;&#25968;&#32534;&#30721;&#22120;&#32467;&#21512;&#65292;&#25105;&#20204;&#22312;&#19977;&#20010;RL&#39046;&#22495;&#20013;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#25928;&#29575;&#12289;&#28176;&#36817;&#24615;&#33021;&#21644;&#35757;&#32451;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although reinforcement learning (RL) can solve many challenging sequential decision making problems, achieving zero-shot transfer across related tasks remains a challenge. The difficulty lies in finding a good representation for the current task so that the agent understands how it relates to previously seen tasks. To achieve zero-shot transfer, we introduce the function encoder, a representation learning algorithm which represents a function as a weighted combination of learned, non-linear basis functions. By using a function encoder to represent the reward function or the transition function, the agent has information on how the current task relates to previously seen tasks via a coherent vector representation. Thus, the agent is able to achieve transfer between related tasks at run time with no additional training. We demonstrate state-of-the-art data efficiency, asymptotic performance, and training stability in three RL fields by augmenting basic RL algorithms with a function encod
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#38500;&#20102;GPT-4&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#23384;&#22312;&#22522;&#26412;&#38169;&#35823;&#65292;&#24182;&#19988;&#21363;&#20351;&#26159;GPT-4&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;</title><link>https://arxiv.org/abs/2401.17169</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Conditional and Modal Reasoning in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26465;&#20214;&#21644;&#24773;&#24577;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#38500;&#20102;GPT-4&#22806;&#65292;&#20854;&#20182;&#27169;&#22411;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#23384;&#22312;&#22522;&#26412;&#38169;&#35823;&#65292;&#24182;&#19988;&#21363;&#20351;&#26159;GPT-4&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#30340;&#30740;&#31350;&#27491;&#22312;&#20154;&#24037;&#26234;&#33021;&#21644;&#35748;&#30693;&#31185;&#23398;&#39046;&#22495;&#19981;&#26029;&#22686;&#21152;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21313;&#20960;&#20010;LLM&#33021;&#21542;&#21306;&#20998;&#36923;&#36753;&#19978;&#27491;&#30830;&#30340;&#25512;&#35770;&#21644;&#36923;&#36753;&#19978;&#33618;&#35884;&#30340;&#25512;&#35770;&#12290;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#28041;&#21450;&#26465;&#20214;&#21477;&#65288;&#20363;&#22914;&#65292;&#8220;&#22914;&#26524;&#23433;&#26377;&#19968;&#20010;&#30343;&#21518;&#65292;&#37027;&#20040;&#40077;&#21187;&#26377;&#19968;&#20010;J&#29260;&#8221;&#65289;&#21644;&#35748;&#35782;&#24773;&#24577;&#65288;&#20363;&#22914;&#65292;&#8220;&#23433;&#21487;&#33021;&#26377;&#19968;&#20010;A&#29260;&#8221;&#65292;&#8220;&#40077;&#21187;&#24517;&#39035;&#26377;&#19968;&#20010;K&#29260;&#8221;&#65289;&#30340;&#25512;&#29702;&#27169;&#24335;&#12290;&#36825;&#20123;&#25512;&#29702;&#27169;&#24335;&#23545;&#20110;&#36923;&#36753;&#23398;&#23478;&#12289;&#21746;&#23398;&#23478;&#21644;&#35821;&#35328;&#23398;&#23478;&#26469;&#35828;&#20855;&#26377;&#29305;&#27530;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#33021;&#22312;&#20154;&#31867;&#25512;&#29702;&#20013;&#25198;&#28436;&#19968;&#20010;&#26680;&#24515;&#35282;&#33394;&#12290;&#22240;&#27492;&#65292;&#35780;&#20272;LLM&#22312;&#36825;&#20123;&#25512;&#29702;&#27169;&#24335;&#19978;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#30340;&#25512;&#29702;&#33021;&#21147;&#26159;&#21542;&#30456;&#21305;&#37197;&#26159;&#38750;&#24120;&#30456;&#20851;&#30340;&#12290;&#22312;&#25105;&#20204;&#27979;&#35797;&#30340;LLM&#20013;&#65292;&#38500;&#20102;GPT-4&#65292;&#20854;&#20182;&#37117;&#24120;&#24120;&#22312;&#26465;&#20214;&#21477;&#26041;&#38754;&#29359;&#22522;&#26412;&#38169;&#35823;&#12290;&#27492;&#22806;&#65292;&#21363;&#20351;&#26159;GPT-4&#65292;&#22312;&#28041;&#21450;&#35748;&#35782;&#24773;&#24577;&#30340;&#25512;&#29702;&#27169;&#24335;&#19978;&#20063;&#26174;&#31034;&#20986;&#36923;&#36753;&#19978;&#19981;&#19968;&#33268;&#30340;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
The reasoning abilities of large language models (LLMs) are the topic of a growing body of research in artificial intelligence and cognitive science. In this paper, we probe the extent to which a dozen LLMs are able to distinguish logically correct inferences from logically fallacious ones. We focus on inference patterns involving conditionals (e.g., 'If Ann has a queen, then Bob has a jack') and epistemic modals (e.g., 'Ann might have an ace', 'Bob must have a king'). These inference patterns have been of special interest to logicians, philosophers, and linguists, since they plausibly play a central role in human reasoning. Assessing LLMs on these inference patterns is thus highly relevant to the question of how much the reasoning abilities of LLMs match those of humans. Among the LLMs we tested, all but GPT-4 often make basic mistakes with conditionals. Moreover, even GPT-4 displays logically inconsistent judgments across inference patterns involving epistemic modals.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21512;&#25104;SMT&#31574;&#30053;&#12290;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#37319;&#29992;&#20102;&#20998;&#23618;&#21644;&#20998;&#38454;&#27573;&#30340;MCTS&#25628;&#32034;&#65292;&#20197;&#22312;&#20445;&#25345;&#25104;&#26412;&#20302;&#30340;&#21516;&#26102;&#21457;&#29616;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2401.17159</link><description>&lt;p&gt;
&#20998;&#23618;&#21644;&#20998;&#38454;&#27573;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#29992;&#20110;SMT&#31574;&#30053;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Layered and Staged Monte Carlo Tree Search for SMT Strategy Synthesis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21512;&#25104;SMT&#31574;&#30053;&#12290;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#37319;&#29992;&#20102;&#20998;&#23618;&#21644;&#20998;&#38454;&#27573;&#30340;MCTS&#25628;&#32034;&#65292;&#20197;&#22312;&#20445;&#25345;&#25104;&#26412;&#20302;&#30340;&#21516;&#26102;&#21457;&#29616;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;SMT&#27714;&#35299;&#22120;&#65288;&#20363;&#22914;Z3&#65289;&#25552;&#20379;&#29992;&#25143;&#21487;&#25511;&#30340;&#31574;&#30053;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#26681;&#25454;&#33258;&#24049;&#30340;&#23454;&#20363;&#38598;&#23450;&#21046;&#27714;&#35299;&#22120;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#27714;&#35299;&#22120;&#22312;&#29305;&#23450;&#29992;&#20363;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#31574;&#30053;&#23450;&#21046;&#30340;&#26041;&#27861;&#24102;&#26469;&#20102;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65306;&#20026;&#19968;&#31867;SMT&#23454;&#20363;&#25163;&#24037;&#35774;&#35745;&#19968;&#20010;&#20248;&#21270;&#30340;&#31574;&#30053;&#20173;&#28982;&#26159;&#27714;&#35299;&#22120;&#24320;&#21457;&#20154;&#21592;&#21644;&#29992;&#25143;&#38754;&#20020;&#30340;&#22797;&#26434;&#21644;&#33392;&#24040;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#33258;&#21160;&#36827;&#34892;SMT&#31574;&#30053;&#21512;&#25104;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#31574;&#30053;&#21512;&#25104;&#35270;&#20026;&#19968;&#31181;&#24207;&#21015;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#25628;&#32034;&#26641;&#23545;&#24212;&#20110;&#31574;&#30053;&#31354;&#38388;&#65292;&#24182;&#21033;&#29992;MCTS&#26469;&#36941;&#21382;&#36825;&#20010;&#24222;&#22823;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20445;&#25345;&#25104;&#26412;&#20302;&#30340;&#21516;&#26102;&#25214;&#21040;&#26377;&#25928;&#30340;&#31574;&#30053;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#20998;&#23618;&#21644;&#20998;&#38454;&#27573;&#30340;MCTS&#25628;&#32034;&#30340;&#29702;&#24565;&#12290;&#36825;&#20123;&#26032;&#39062;&#30340;&#26041;&#27861;&#20801;&#35768;&#23545;&#31574;&#30053;&#31354;&#38388;&#36827;&#34892;&#26356;&#28145;&#20837;&#21644;&#26356;&#39640;&#25928;&#30340;&#25506;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern SMT solvers, such as Z3, offer user-controllable strategies, enabling users to tailor them for their unique set of instances, thus dramatically enhancing solver performance for their use case. However, this approach of strategy customization presents a significant challenge: handcrafting an optimized strategy for a class of SMT instances remains a complex and demanding task for both solver developers and users alike.   In this paper, we address this problem of automatic SMT strategy synthesis via a novel Monte Carlo Tree Search (MCTS) based method. Our method treats strategy synthesis as a sequential decision-making process, whose search tree corresponds to the strategy space, and employs MCTS to navigate this vast search space. The key innovations that enable our method to identify effective strategies, while keeping costs low, are the ideas of layered and staged MCTS search. These novel approaches allow for a deeper and more efficient exploration of the strategy space, enablin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#30697;&#38453;&#29109;&#65292;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#20960;&#20309;&#21407;&#29702;&#30340;&#26032;&#22411;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#25454;&#21387;&#32553;&#33021;&#21147;&#12290;&#35813;&#25351;&#26631;&#21453;&#26144;&#20102;&#27169;&#22411;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#21644;&#28040;&#38500;&#19981;&#24517;&#35201;&#20803;&#32032;&#30340;&#33021;&#21147;&#65292;&#20026;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#33021;&#21147;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#37117;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17139</link><description>&lt;p&gt;
&#36890;&#36807;&#30697;&#38453;&#29109;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Evaluation via Matrix Entropy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#30697;&#38453;&#29109;&#65292;&#19968;&#31181;&#22522;&#20110;&#20449;&#24687;&#35770;&#21644;&#20960;&#20309;&#21407;&#29702;&#30340;&#26032;&#22411;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25968;&#25454;&#21387;&#32553;&#33021;&#21147;&#12290;&#35813;&#25351;&#26631;&#21453;&#26144;&#20102;&#27169;&#22411;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#21644;&#28040;&#38500;&#19981;&#24517;&#35201;&#20803;&#32032;&#30340;&#33021;&#21147;&#65292;&#20026;&#35780;&#20272;&#35821;&#35328;&#27169;&#22411;&#30340;&#22266;&#26377;&#33021;&#21147;&#25552;&#20379;&#20102;&#27934;&#23519;&#12290;&#22312;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#37117;&#23637;&#31034;&#20102;&#20854;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#36807;&#23558;&#24378;&#22823;&#30340;&#33021;&#21147;&#25193;&#23637;&#21040;&#22810;&#27169;&#24577;&#39046;&#22495;&#65292;&#20351;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21457;&#29983;&#20102;&#38761;&#21629;&#12290;&#22240;&#27492;&#65292;&#20026;LLMs&#23450;&#20041;&#36866;&#24403;&#19988;&#22810;&#26679;&#21270;&#30340;&#35780;&#20272;&#25351;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30697;&#38453;&#29109;&#65292;&#19968;&#31181;&#26681;&#26893;&#20110;&#20449;&#24687;&#35770;&#21644;&#20960;&#20309;&#21407;&#29702;&#30340;&#26032;&#22411;&#25351;&#26631;&#65292;&#29992;&#20110;&#37327;&#21270;LLMs&#20013;&#30340;&#25968;&#25454;&#21387;&#32553;&#33021;&#21147;&#12290;&#23427;&#21453;&#26144;&#20102;&#27169;&#22411;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#21644;&#28040;&#38500;&#19981;&#24517;&#35201;&#20803;&#32032;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#23545;&#35821;&#35328;&#27169;&#22411;&#22266;&#26377;&#33021;&#21147;&#30340;&#27934;&#23519;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23427;&#22312;&#21333;&#27169;&#24577;&#65288;&#35821;&#35328;&#65289;&#21644;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#23545;&#20110;&#35821;&#35328;&#27169;&#22411;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20986;&#34920;&#31034;&#30340;&#30697;&#38453;&#29109;&#22312;&#27169;&#22411;&#25193;&#22823;&#26102;&#36981;&#24490;&#19968;&#20010;&#32553;&#25918;&#23450;&#24459;&#31867;&#22411;&#30340;&#38477;&#20302;&#65292;&#36825;&#20316;&#20026;&#20256;&#32479;&#25439;&#22833;&#32553;&#25918;&#23450;&#24459;&#30340;&#34917;&#20805;&#12290;&#23545;&#20110;&#22810;&#27169;&#24577;&#35774;&#32622;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30697;&#38453;&#29109;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#19968;&#20010;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have revolutionized the field of natural language processing, extending their strong capabilities into multi-modal domains. Thus, it is vital to define proper and diversified metrics for the evaluation of LLMs.   In this paper, we introduce matrix entropy, a novel metric rooted in information theory and geometry principles to quantify the data compression proficiency in LLMs. It reflects the model's ability to extract relevant information and eliminate unnecessary elements, thereby providing insight into the language model's intrinsic capability. Specifically, we demonstrate its applicability in both single-modal (language) and multi-modal settings. For language models, our findings reveal that the matrix entropy of representations follows a scaling law type reduction when the model scales up, serving as a complement to the traditional loss scaling law. For the multi-modal setting, we also propose an evaluation method based on matrix entropy for assessing a
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#24615;&#30340;&#21452;&#37325;&#38450;&#25252;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#24178;&#25200;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#22522;&#20110;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#12290;&#35813;&#26426;&#21046;&#26082;&#25200;&#20081;&#20102;&#27468;&#25163;&#36523;&#20221;&#65292;&#21448;&#25200;&#20081;&#20102;&#27468;&#35789;&#65292;&#20351;&#24471;&#27468;&#21809;&#22768;&#38899;&#26082;&#19981;&#27169;&#20223;&#30446;&#26631;&#27468;&#25163;&#65292;&#20063;&#19981;&#20445;&#30041;&#21407;&#22987;&#27468;&#35789;&#12290;</title><link>http://arxiv.org/abs/2401.17133</link><description>&lt;p&gt;
&#19968;&#31181;&#38024;&#23545;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#30340;&#20027;&#21160;&#24615;&#21452;&#37325;&#38450;&#25252;&#26426;&#21046;&#65306;&#22522;&#20110;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Proactive and Dual Prevention Mechanism against Illegal Song Covers empowered by Singing Voice Conversion. (arXiv:2401.17133v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17133
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#24615;&#30340;&#21452;&#37325;&#38450;&#25252;&#26426;&#21046;&#65292;&#36890;&#36807;&#24341;&#20837;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#24178;&#25200;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#29983;&#25104;&#36807;&#31243;&#65292;&#38450;&#27490;&#26410;&#32463;&#25480;&#26435;&#30340;&#22522;&#20110;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#30340;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#12290;&#35813;&#26426;&#21046;&#26082;&#25200;&#20081;&#20102;&#27468;&#25163;&#36523;&#20221;&#65292;&#21448;&#25200;&#20081;&#20102;&#27468;&#35789;&#65292;&#20351;&#24471;&#27468;&#21809;&#22768;&#38899;&#26082;&#19981;&#27169;&#20223;&#30446;&#26631;&#27468;&#25163;&#65292;&#20063;&#19981;&#20445;&#30041;&#21407;&#22987;&#27468;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;(SVC)&#36890;&#36807;&#23558;&#19968;&#20010;&#27468;&#25163;&#30340;&#27468;&#21809;&#22768;&#38899;&#36716;&#25442;&#25104;&#21478;&#19968;&#20010;&#30446;&#26631;&#27468;&#25163;&#30340;&#27468;&#21809;&#22768;&#38899;&#65292;&#24182;&#20351;&#29992;&#21407;&#22987;&#27468;&#35789;&#21644;&#26059;&#24459;&#65292;&#33258;&#21160;&#21270;&#20102;&#27468;&#26354;&#32763;&#21809;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#23545;&#29256;&#26435;&#21644;&#20844;&#27665;&#26435;&#21033;&#30340;&#20005;&#37325;&#25285;&#24551;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102; SongBsAb&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20027;&#21160;&#24615;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#26410;&#32463;&#25480;&#26435;&#30340;&#22522;&#20110; SVC &#30340;&#38750;&#27861;&#27468;&#26354;&#32763;&#21809;&#12290;SongBsAb &#22312;&#21457;&#24067;&#27468;&#21809;&#22768;&#38899;&#20043;&#21069;&#24341;&#20837;&#20102;&#20154;&#31867;&#26080;&#27861;&#23519;&#35273;&#30340;&#25200;&#21160;&#65292;&#36825;&#26679;&#24403;&#23427;&#20204;&#34987;&#20351;&#29992;&#26102;&#65292;SVC &#30340;&#29983;&#25104;&#36807;&#31243;&#23558;&#34987;&#24178;&#25200;&#65292;&#23548;&#33268;&#24847;&#22806;&#30340;&#27468;&#21809;&#22768;&#38899;&#12290; SongBsAb &#20855;&#26377;&#21452;&#37325;&#39044;&#38450;&#25928;&#26524;&#65292;&#24341;&#36215;&#27468;&#25163;&#36523;&#20221;&#21644;&#27468;&#35789;&#30340;&#28151;&#20081;&#65292;&#21363; SVC &#35206;&#30422;&#30340;&#27468;&#21809;&#22768;&#38899;&#26082;&#19981;&#27169;&#20223;&#30446;&#26631;&#27468;&#25163;&#65292;&#20063;&#19981;&#20445;&#30041;&#21407;&#22987;&#27468;&#35789;&#12290;&#20026;&#20102;&#25552;&#39640;&#25200;&#21160;&#30340;&#19981;&#21487;&#23519;&#35273;&#24615;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#20197;&#20276;&#22863;&#26354;&#20316;&#20026;&#39069;&#22806;&#25513;&#34109;&#32773;&#30340;&#22522;&#20110;&#24515;&#29702;&#22768;&#23398;&#27169;&#22411;&#30340;&#25439;&#22833;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Singing voice conversion (SVC) automates song covers by converting one singer's singing voice into another target singer's singing voice with the original lyrics and melody. However, it raises serious concerns about copyright and civil right infringements to multiple entities. This work proposes SongBsAb, the first proactive approach to mitigate unauthorized SVC-based illegal song covers. SongBsAb introduces human-imperceptible perturbations to singing voices before releasing them, so that when they are used, the generation process of SVC will be interfered, resulting in unexpected singing voices. SongBsAb features a dual prevention effect by causing both (singer) identity disruption and lyric disruption, namely, the SVC-covered singing voice neither imitates the target singer nor preserves the original lyrics. To improve the imperceptibility of perturbations, we refine a psychoacoustic model-based loss with the backing track as an additional masker, a unique accompanying element for s
&lt;/p&gt;</description></item><item><title>&#36825;&#20221;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#38899;&#39057;&#35270;&#35273;&#22768;&#20107;&#20214;&#23450;&#20301;&#21644;&#26816;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#21512;&#24182;&#38899;&#39057;&#21644;&#35270;&#39057;&#20449;&#24687;&#25552;&#21319;&#20102;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#30446;&#26631;&#26816;&#27979;&#22120;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2401.17129</link><description>&lt;p&gt;
&#22686;&#24378;&#30340;360&#24230;&#23454;&#26102;&#38899;&#39057;&#35270;&#35273;&#22768;&#26223;&#38899;&#20107;&#20214;&#23450;&#20301;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhanced Sound Event Localization and Detection in Real 360-degree audio-visual soundscapes. (arXiv:2401.17129v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17129
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#38899;&#39057;&#35270;&#35273;&#22768;&#20107;&#20214;&#23450;&#20301;&#21644;&#26816;&#27979;&#32593;&#32476;&#65292;&#36890;&#36807;&#21512;&#24182;&#38899;&#39057;&#21644;&#35270;&#39057;&#20449;&#24687;&#25552;&#21319;&#20102;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#30446;&#26631;&#26816;&#27979;&#22120;&#21644;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#36827;&#19968;&#27493;&#25913;&#36827;&#20102;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#20221;&#25216;&#26415;&#25253;&#21578;&#35814;&#32454;&#20171;&#32461;&#20102;&#25105;&#20204;&#22312;&#26500;&#24314;&#22686;&#24378;&#30340;&#38899;&#39057;&#35270;&#35273;&#22768;&#20107;&#20214;&#23450;&#20301;&#21644;&#26816;&#27979;&#65288;SELD&#65289;&#32593;&#32476;&#26041;&#38754;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#22312;&#20165;&#38899;&#39057;&#30340;SELDnet23&#27169;&#22411;&#22522;&#30784;&#19978;&#36827;&#34892;&#25913;&#36827;&#65292;&#23558;&#38899;&#39057;&#21644;&#35270;&#39057;&#20449;&#24687;&#22312;&#20165;&#38899;&#39057;&#32593;&#32476;&#30340;&#38376;&#25511;&#24490;&#29615;&#21333;&#20803;&#65288;GRU&#65289;&#20043;&#21069;&#21512;&#24182;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#20102;YOLO&#21644;DETIC&#30446;&#26631;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#36824;&#24314;&#31435;&#20102;&#19968;&#20010;&#23454;&#26045;&#38899;&#39057;&#35270;&#35273;&#25968;&#25454;&#22686;&#24378;&#21644;&#38899;&#39057;&#35270;&#35273;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#38899;&#39057;&#35270;&#35273;SELDnet&#31995;&#32479;&#65292;&#20854;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#38899;&#39057;&#35270;&#35273;SELD&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report details our work towards building an enhanced audio-visual sound event localization and detection (SELD) network. We build on top of the audio-only SELDnet23 model and adapt it to be audio-visual by merging both audio and video information prior to the gated recurrent unit (GRU) of the audio-only network. Our model leverages YOLO and DETIC object detectors. We also build a framework that implements audio-visual data augmentation and audio-visual synthetic data generation. We deliver an audio-visual SELDnet system that outperforms the existing audio-visual SELD baseline.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;GraphCG&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39044;&#35757;&#32451;&#22270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#30417;&#30563;&#21457;&#29616;&#21487;&#25805;&#32437;&#22240;&#32032;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#35821;&#20041;&#20016;&#23500;&#26041;&#21521;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36825;&#20123;&#21487;&#25805;&#32437;&#22240;&#32032;&#12290;&#23454;&#39564;&#35777;&#26126;GraphCG&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.17123</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#21457;&#29616;&#24403;&#22270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20986;&#29616;&#20132;&#32455;&#26102;&#30340;&#21487;&#25805;&#32437;&#22240;&#32032;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Discovery of Steerable Factors When Graph Deep Generative Models Are Entangled. (arXiv:2401.17123v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17123
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;GraphCG&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39044;&#35757;&#32451;&#22270;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#30417;&#30563;&#21457;&#29616;&#21487;&#25805;&#32437;&#22240;&#32032;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;&#35821;&#20041;&#20016;&#23500;&#26041;&#21521;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36825;&#20123;&#21487;&#25805;&#32437;&#22240;&#32032;&#12290;&#23454;&#39564;&#35777;&#26126;GraphCG&#20248;&#20110;&#20854;&#20182;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;(DGMs)&#24191;&#27867;&#29992;&#20110;&#22270;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#36825;&#31181;&#39044;&#35757;&#32451;&#22270;DGMs&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#29702;&#35299;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#36825;&#20123;&#29702;&#35299;&#26377;&#28508;&#21147;&#20026;&#37325;&#35201;&#20219;&#21153;&#25552;&#20379;&#26377;&#30410;&#30340;&#25351;&#23548;&#65292;&#20363;&#22914;&#22270;&#30340;&#21487;&#25511;&#21046;&#29983;&#25104;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;&#36825;&#20010;&#38382;&#39064;&#24456;&#24863;&#20852;&#36259;&#65292;&#24182;&#25552;&#20986;GraphCG&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#39044;&#35757;&#32451;&#22270;DGMs&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#26080;&#30417;&#30563;&#21457;&#29616;&#21487;&#25805;&#32437;&#22240;&#32032;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20845;&#20010;&#35299;&#32544;&#24230;&#24230;&#37327;&#26631;&#20934;&#26816;&#39564;&#20102;&#19977;&#20010;&#39044;&#35757;&#32451;&#22270;DGMs&#30340;&#34920;&#31034;&#31354;&#38388;&#65292;&#24182;&#35266;&#23519;&#21040;&#39044;&#35757;&#32451;&#34920;&#31034;&#31354;&#38388;&#26159;&#20132;&#32455;&#30340;&#12290;&#21463;&#36825;&#20010;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;GraphCG&#36890;&#36807;&#26368;&#22823;&#21270;&#35821;&#20041;&#20016;&#23500;&#26041;&#21521;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#21487;&#25805;&#32437;&#22240;&#32032;&#65292;&#27839;&#30528;&#30456;&#21516;&#26041;&#21521;&#31227;&#21160;&#30340;&#22270;&#23558;&#20849;&#20139;&#30456;&#21516;&#30340;&#21487;&#25805;&#32437;&#22240;&#32032;&#12290;&#25105;&#20204;&#23450;&#37327;&#39564;&#35777;&#20102;GraphCG&#20248;&#20110;&#20854;&#20182;&#22235;&#20010;&#31454;&#20105;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep generative models (DGMs) have been widely developed for graph data. However, much less investigation has been carried out on understanding the latent space of such pretrained graph DGMs. These understandings possess the potential to provide constructive guidelines for crucial tasks, such as graph controllable generation. Thus in this work, we are interested in studying this problem and propose GraphCG, a method for the unsupervised discovery of steerable factors in the latent space of pretrained graph DGMs. We first examine the representation space of three pretrained graph DGMs with six disentanglement metrics, and we observe that the pretrained representation space is entangled. Motivated by this observation, GraphCG learns the steerable factors via maximizing the mutual information between semantic-rich directions, where the controlled graph moving along the same direction will share the same steerable factors. We quantitatively verify that GraphCG outperforms four competitive 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23439;&#35266;&#27169;&#22411;&#21644;&#22810;&#28304;&#26102;&#31354;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#26080;&#27861;&#35266;&#27979;&#21040;&#30340;&#32593;&#32476;&#20301;&#32622;&#30340;&#20132;&#36890;&#27969;&#37327;&#21644;&#34892;&#39542;&#26102;&#38388;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20256;&#24863;&#22120;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20934;&#30830;&#30340;&#20272;&#35745;&#65292;&#24182;&#28385;&#36275;&#22522;&#26412;&#30340;&#27969;&#37327;&#23432;&#24658;&#32422;&#26463;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2401.17095</link><description>&lt;p&gt;
&#22312;&#26410;&#35266;&#27979;&#21040;&#30340;&#32593;&#32476;&#20301;&#32622;&#20351;&#29992;&#25968;&#25454;&#39537;&#21160;&#30340;&#23439;&#35266;&#27169;&#22411;&#36827;&#34892;&#20132;&#36890;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Traffic estimation in unobserved network locations using data-driven macroscopic models. (arXiv:2401.17095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23439;&#35266;&#27169;&#22411;&#21644;&#22810;&#28304;&#26102;&#31354;&#25968;&#25454;&#30340;&#26041;&#27861;&#26469;&#20272;&#35745;&#26080;&#27861;&#35266;&#27979;&#21040;&#30340;&#32593;&#32476;&#20301;&#32622;&#30340;&#20132;&#36890;&#27969;&#37327;&#21644;&#34892;&#39542;&#26102;&#38388;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#20256;&#24863;&#22120;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#20934;&#30830;&#30340;&#20272;&#35745;&#65292;&#24182;&#28385;&#36275;&#22522;&#26412;&#30340;&#27969;&#37327;&#23432;&#24658;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#23439;&#35266;&#27169;&#22411;&#21644;&#33258;&#21160;&#20132;&#36890;&#35745;&#25968;&#22120;&#21644;&#25506;&#27979;&#36710;&#36742;&#25910;&#38598;&#30340;&#22810;&#28304;&#26102;&#31354;&#25968;&#25454;&#65292;&#20934;&#30830;&#20272;&#35745;&#26080;&#27861;&#33719;&#24471;&#36825;&#20123;&#27979;&#37327;&#25968;&#25454;&#30340;&#36335;&#27573;&#30340;&#20132;&#36890;&#27969;&#37327;&#21644;&#34892;&#39542;&#26102;&#38388;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#20132;&#36890;&#35268;&#21010;&#24212;&#29992;&#20013;&#26159;&#20851;&#38190;&#30340;&#65292;&#22240;&#20026;&#20256;&#24863;&#22120;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#65292;&#35268;&#21010;&#30340;&#24178;&#39044;&#25514;&#26045;&#20250;&#23545;&#25972;&#20010;&#32593;&#32476;&#20135;&#29983;&#24433;&#21709;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;&#21629;&#21517;&#20026;&#23439;&#35266;&#20132;&#36890;&#20272;&#35745;&#22120;&#65288;MaTE&#65289;&#65292;&#21487;&#20197;&#20165;&#20351;&#29992;&#36825;&#20123;&#25968;&#37327;&#30340;&#35266;&#27979;&#27979;&#37327;&#26469;&#36827;&#34892;&#32593;&#32476;&#33539;&#22260;&#30340;&#20132;&#36890;&#27969;&#37327;&#21644;&#34892;&#39542;&#26102;&#38388;&#20272;&#35745;&#12290;&#30001;&#20110;MaTE&#22522;&#20110;&#23439;&#35266;&#27969;&#37327;&#29702;&#35770;&#65292;&#25152;&#26377;&#21442;&#25968;&#21644;&#21464;&#37327;&#37117;&#26159;&#21487;&#20197;&#35299;&#37322;&#30340;&#12290;&#20272;&#35745;&#30340;&#20132;&#36890;&#27969;&#37327;&#28385;&#36275;&#22522;&#26412;&#30340;&#27969;&#37327;&#23432;&#24658;&#32422;&#26463;&#26465;&#20214;&#65292;&#24182;&#19988;&#19982;&#20272;&#35745;&#30340;&#34892;&#39542;&#26102;&#38388;&#21576;&#36882;&#22686;&#30340;&#21333;&#35843;&#20851;&#31995;&#12290;&#23558;&#22522;&#20110;logit&#30340;&#38543;&#26426;&#20132;&#36890;&#20998;&#37197;&#20316;&#20026;&#27969;&#37327;&#34892;&#20026;&#36335;&#30001;&#30340;&#21407;&#21017;&#20351;&#24471;&#27169;&#22411;&#22312;&#21487;&#24494;&#26041;&#38754;&#20855;&#26377;&#23436;&#20840;&#21487;&#21306;&#20998;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper leverages macroscopic models and multi-source spatiotemporal data collected from automatic traffic counters and probe vehicles to accurately estimate traffic flow and travel time in links where these measurements are unavailable. This problem is critical in transportation planning applications where the sensor coverage is low and the planned interventions have network-wide impacts. The proposed model, named the Macroscopic Traffic Estimator (MaTE), can perform network-wide estimations of traffic flow and travel time only using the set of observed measurements of these quantities. Because MaTE is grounded in macroscopic flow theory, all parameters and variables are interpretable. The estimated traffic flow satisfies fundamental flow conservation constraints and exhibits an increasing monotonic relationship with the estimated travel time. Using logit-based stochastic traffic assignment as the principle for routing flow behavior makes the model fully differentiable with respect
&lt;/p&gt;</description></item><item><title>BlockFusion&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#21644;&#22806;&#25512;&#25216;&#26415;&#29983;&#25104;&#19977;&#32500;&#22330;&#26223;&#30340;&#27169;&#22411;&#65292;&#33021;&#26080;&#32541;&#22320;&#28155;&#21152;&#26032;&#30340;&#22359;&#20197;&#25193;&#23637;&#22330;&#26223;&#12290;&#37319;&#29992;&#28151;&#21512;&#31070;&#32463;&#22330;&#21644;&#28508;&#22312;&#19977;&#24179;&#38754;&#31354;&#38388;&#26469;&#20445;&#35777;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.17053</link><description>&lt;p&gt;
BlockFusion: &#20351;&#29992;&#28508;&#22312;&#19977;&#24179;&#38754;&#22806;&#25512;&#25193;&#23637;&#30340;&#21487;&#25193;&#23637;&#19977;&#32500;&#22330;&#26223;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
BlockFusion: Expandable 3D Scene Generation using Latent Tri-plane Extrapolation. (arXiv:2401.17053v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17053
&lt;/p&gt;
&lt;p&gt;
BlockFusion&#26159;&#19968;&#31181;&#20351;&#29992;&#25193;&#25955;&#21644;&#22806;&#25512;&#25216;&#26415;&#29983;&#25104;&#19977;&#32500;&#22330;&#26223;&#30340;&#27169;&#22411;&#65292;&#33021;&#26080;&#32541;&#22320;&#28155;&#21152;&#26032;&#30340;&#22359;&#20197;&#25193;&#23637;&#22330;&#26223;&#12290;&#37319;&#29992;&#28151;&#21512;&#31070;&#32463;&#22330;&#21644;&#28508;&#22312;&#19977;&#24179;&#38754;&#31354;&#38388;&#26469;&#20445;&#35777;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;BlockFusion&#65292;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#20197;&#21333;&#20301;&#22359;&#24418;&#24335;&#29983;&#25104;&#19977;&#32500;&#22330;&#26223;&#65292;&#24182;&#26080;&#32541;&#22320;&#28155;&#21152;&#26032;&#30340;&#22359;&#20197;&#25193;&#23637;&#22330;&#26223;&#12290;BlockFusion&#20351;&#29992;&#20174;&#23436;&#25972;&#30340;&#19977;&#32500;&#22330;&#26223;&#20013;&#38543;&#26426;&#35009;&#21098;&#30340;3D&#22359;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#22359;&#25311;&#21512;&#65292;&#23558;&#25152;&#26377;&#35757;&#32451;&#22359;&#36716;&#25442;&#20026;&#28151;&#21512;&#31070;&#32463;&#22330;&#65306;&#23427;&#21253;&#21547;&#20960;&#20309;&#29305;&#24449;&#30340;&#19977;&#24179;&#38754;&#65292;&#20197;&#21450;&#29992;&#20110;&#35299;&#30721;&#26377;&#31526;&#21495;&#36317;&#31163;&#20540;&#30340;&#22810;&#23618;&#24863;&#30693;&#26426;(MLP)&#12290;&#37319;&#29992;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#23558;&#19977;&#24179;&#38754;&#21387;&#32553;&#21040;&#28508;&#22312;&#19977;&#24179;&#38754;&#31354;&#38388;&#65292;&#24182;&#22312;&#20854;&#19978;&#25191;&#34892;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#12290;&#23545;&#28508;&#22312;&#34920;&#31034;&#24212;&#29992;&#25193;&#25955;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#30340;&#19977;&#32500;&#22330;&#26223;&#29983;&#25104;&#12290;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#25193;&#23637;&#22330;&#26223;&#26102;&#65292;&#21482;&#38656;&#23558;&#31354;&#22359;&#28155;&#21152;&#21040;&#19982;&#24403;&#21069;&#22330;&#26223;&#37325;&#21472;&#65292;&#24182;&#22806;&#25512;&#29616;&#26377;&#30340;&#28508;&#22312;&#19977;&#24179;&#38754;&#20197;&#22635;&#20805;&#26032;&#22359;&#12290;&#22806;&#25512;&#36807;&#31243;&#36890;&#36807;&#20351;&#29992;&#29305;&#24449;&#23545;&#29983;&#25104;&#36807;&#31243;&#36827;&#34892;&#32422;&#26463;&#26469;&#23436;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present BlockFusion, a diffusion-based model that generates 3D scenes as unit blocks and seamlessly incorporates new blocks to extend the scene. BlockFusion is trained using datasets of 3D blocks that are randomly cropped from complete 3D scene meshes. Through per-block fitting, all training blocks are converted into the hybrid neural fields: with a tri-plane containing the geometry features, followed by a Multi-layer Perceptron (MLP) for decoding the signed distance values. A variational auto-encoder is employed to compress the tri-planes into the latent tri-plane space, on which the denoising diffusion process is performed. Diffusion applied to the latent representations allows for high-quality and diverse 3D scene generation. To expand a scene during generation, one needs only to append empty blocks to overlap with the current scene and extrapolate existing latent tri-planes to populate new blocks. The extrapolation is done by conditioning the generation process with the feature 
&lt;/p&gt;</description></item><item><title>ViTree&#26159;&#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#21464;&#25442;&#22120;&#21644;&#31070;&#32463;&#20915;&#31574;&#26641;&#65292;&#36880;&#27493;&#31361;&#20986;&#20449;&#24687;&#20016;&#23500;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#24182;&#36873;&#25321;&#21333;&#20010;&#26641;&#36335;&#24452;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.17050</link><description>&lt;p&gt;
ViTree: &#21333;&#36335;&#24452;&#31070;&#32463;&#26641;&#29992;&#20110;&#36880;&#27493;&#21487;&#35299;&#37322;&#30340;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
ViTree: Single-path Neural Tree for Step-wise Interpretable Fine-grained Visual Categorization. (arXiv:2401.17050v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17050
&lt;/p&gt;
&lt;p&gt;
ViTree&#26159;&#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#35270;&#35273;&#21464;&#25442;&#22120;&#21644;&#31070;&#32463;&#20915;&#31574;&#26641;&#65292;&#36880;&#27493;&#31361;&#20986;&#20449;&#24687;&#20016;&#23500;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#24182;&#36873;&#25321;&#21333;&#20010;&#26641;&#36335;&#24452;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#19981;&#26029;&#21457;&#23637;&#21644;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#20010;&#39046;&#22495;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#24120;&#24120;&#37319;&#29992;&#20107;&#21518;&#25216;&#26415;&#25110;&#21407;&#22411;&#26469;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#38388;&#25509;&#24182;&#19988;&#32570;&#20047;&#20869;&#22312;&#30340;&#35828;&#26126;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;ViTree&#65292;&#19968;&#31181;&#29992;&#20110;&#32454;&#31890;&#24230;&#35270;&#35273;&#20998;&#31867;&#30340;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;&#27969;&#34892;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#20316;&#20026;&#29305;&#24449;&#25552;&#21462;&#39592;&#24178;&#65292;&#19982;&#31070;&#32463;&#20915;&#31574;&#26641;&#30456;&#32467;&#21512;&#12290;&#36890;&#36807;&#36941;&#21382;&#26641;&#36335;&#24452;&#65292;ViTree&#26377;&#25928;&#22320;&#20174;&#21464;&#25442;&#22120;&#22788;&#29702;&#30340;&#29305;&#24449;&#20013;&#36873;&#25321;&#34917;&#19969;&#26469;&#31361;&#20986;&#20449;&#24687;&#20016;&#23500;&#30340;&#23616;&#37096;&#21306;&#22495;&#65292;&#20174;&#32780;&#36880;&#27493;&#20248;&#21270;&#34920;&#31034;&#12290;&#19982;&#20197;&#21069;&#20381;&#36182;&#36719;&#20998;&#24067;&#25110;&#36335;&#24452;&#38598;&#21512;&#30340;&#22522;&#20110;&#26641;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;ViTree&#36873;&#25321;&#21333;&#20010;&#26641;&#36335;&#24452;&#65292;&#25552;&#20379;&#26356;&#28165;&#26224;&#12289;&#26356;&#31616;&#21270;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#36825;&#31181;&#34917;&#19969;&#21644;&#36335;&#24452;&#30340;&#36873;&#25321;&#24615;&#25552;&#39640;&#20102;ViTree&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#27169;&#22411;&#35299;&#37322;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
As computer vision continues to advance and finds widespread applications across various domains, the need for interpretability in deep learning models becomes paramount. Existing methods often resort to post-hoc techniques or prototypes to explain the decision-making process, which can be indirect and lack intrinsic illustration. In this research, we introduce ViTree, a novel approach for fine-grained visual categorization that combines the popular vision transformer as a feature extraction backbone with neural decision trees. By traversing the tree paths, ViTree effectively selects patches from transformer-processed features to highlight informative local regions, thereby refining representations in a step-wise manner. Unlike previous tree-based models that rely on soft distributions or ensembles of paths, ViTree selects a single tree path, offering a clearer and simpler decision-making process. This patch and path selectivity enhances model interpretability of ViTree, enabling bette
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#30340;&#35299;&#37322;&#35299;&#37322;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#19981;&#36879;&#26126;&#31995;&#32479;&#20013;&#29983;&#25104;&#21512;&#36866;&#35299;&#37322;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2401.17045</link><description>&lt;p&gt;
&#22312;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#20013;&#35299;&#37322;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explaining Explanations in Probabilistic Logic Programming. (arXiv:2401.17045v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17045
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#30340;&#35299;&#37322;&#35299;&#37322;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#19981;&#36879;&#26126;&#31995;&#32479;&#20013;&#29983;&#25104;&#21512;&#36866;&#35299;&#37322;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#30340;&#20986;&#29616;&#20063;&#23548;&#33268;&#20102;&#20135;&#29983;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#35299;&#37322;&#30340;&#38656;&#27714;&#12290;&#22312;&#19968;&#20123;&#26041;&#27861;&#20013;&#65292;&#31995;&#32479;&#26159;&#19981;&#36879;&#26126;&#30340;&#65288;&#36890;&#24120;&#34987;&#31216;&#20026;&#8220;&#40657;&#30418;&#23376;&#8221;&#65289;&#65292;&#36825;&#20351;&#24471;&#29983;&#25104;&#36866;&#24403;&#30340;&#35299;&#37322;&#21464;&#24471;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#22312;&#27010;&#29575;&#36923;&#36753;&#32534;&#31243;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#36923;&#36753;&#32534;&#31243;&#65288;&#29992;&#20110;&#30693;&#35782;&#34920;&#31034;&#65289;&#21644;&#27010;&#29575;&#65288;&#29992;&#20110;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#65289;&#30340;&#32467;&#21512;&#12290;&#22312;&#36825;&#20010;&#35774;&#32622;&#20013;&#65292;&#21487;&#20197;&#35828;&#27169;&#22411;&#26159;&#21487;&#20197;&#35299;&#37322;&#30340;&#65292;&#36825;&#26041;&#20415;&#20102;&#23545;&#27169;&#22411;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#29305;&#23450;&#30340;&#26597;&#35810;&#65292;&#36890;&#24120;&#30340;&#8220;&#35299;&#37322;&#8221;&#30340;&#27010;&#24565;&#26159;&#19982;&#27169;&#22411;&#30340;&#27599;&#20010;&#38543;&#26426;&#21464;&#37327;&#30340;&#36873;&#25321;&#38598;&#30456;&#20851;&#32852;&#30340;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20010;&#38598;&#21512;&#27809;&#26377;&#22240;&#26524;&#32467;&#26500;&#65292;&#23454;&#38469;&#19978;&#65292;&#19968;&#20123;&#36873;&#25321;&#23454;&#38469;&#19978;&#19982;&#25152;&#32771;&#34385;&#30340;&#26597;&#35810;&#26080;&#20851;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26597;&#35810;&#39537;&#21160;&#25512;&#29702;&#23450;&#20041;&#30340;&#35299;&#37322;&#35299;&#37322;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of tools based on artificial intelligence has also led to the need of producing explanations which are understandable by a human being. In some approaches, the system is not transparent (often referred to as a "black box"), making it difficult to generate appropriate explanations. In this work, though, we consider probabilistic logic programming, a combination of logic programming (for knowledge representation) and probability (to model uncertainty). In this setting, one can say that models are interpretable, which eases its understanding. However, given a particular query, the usual notion of "explanation" is associated with a set of choices, one for each random variable of the model. Unfortunately, this set does not have a causal structure and, in fact, some of the choices are actually irrelevant to the considered query. In order to overcome these shortcomings, we present an approach to explaining explanations which is based on the definition of a query-driven inference
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#23545;&#31574;&#12290;</title><link>http://arxiv.org/abs/2401.17044</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26426;&#21046;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Scalable Mechanism Design for Multi-Agent Path Finding. (arXiv:2401.17044v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17044
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#21487;&#25193;&#23637;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;(MAPF)&#28041;&#21450;&#30830;&#23450;&#22810;&#20010;&#26234;&#33021;&#20307;&#21516;&#26102;&#31359;&#36807;&#20849;&#20139;&#21306;&#22495;&#21069;&#24448;&#29305;&#23450;&#30446;&#26631;&#20301;&#32622;&#30340;&#36335;&#24452;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#22788;&#29702;&#22823;&#37327;&#26234;&#33021;&#20307;&#26102;&#20855;&#26377;&#35745;&#31639;&#22797;&#26434;&#24615;&#65292;&#23588;&#20854;&#22312;&#20687;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#21327;&#35843;&#36825;&#26679;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#12290;&#25214;&#21040;&#26368;&#20248;&#35299;&#36890;&#24120;&#26159;&#35745;&#31639;&#19978;&#19981;&#21487;&#34892;&#30340;&#65292;&#22240;&#27492;&#20351;&#29992;&#36817;&#20284;&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#26234;&#33021;&#20307;&#21487;&#33021;&#20197;&#33258;&#31169;&#21644;&#31574;&#30053;&#24615;&#30340;&#26041;&#24335;&#34892;&#21160;&#65292;&#22914;&#26524;&#23545;MAPF&#31639;&#27861;&#26377;&#21033;&#65292;&#21487;&#33021;&#20250;&#27498;&#26354;&#20854;&#30446;&#26631;&#12290;&#34429;&#28982;&#26426;&#21046;&#35774;&#35745;&#39046;&#22495;&#25552;&#20379;&#20102;&#29992;&#20110;&#23545;&#40784;&#28608;&#21169;&#30340;&#24037;&#20855;&#65292;&#20294;&#22914;&#26524;&#19981;&#20180;&#32454;&#32771;&#34385;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#21487;&#33021;&#20250;&#22312;&#20165;&#26377;&#36817;&#20284;&#26368;&#20248;&#32467;&#26524;&#26102;&#22833;&#36133;&#12290;&#30001;&#20110;&#36817;&#20284;&#23545;&#20110;&#21487;&#25193;&#23637;&#30340;MAPF&#31639;&#27861;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21487;&#25193;&#23637;MAPF&#26426;&#21046;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#23545;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Path Finding (MAPF) involves determining paths for multiple agents to travel simultaneously through a shared area toward particular goal locations. This problem is computationally complex, especially when dealing with large numbers of agents, as is common in realistic applications like autonomous vehicle coordination. Finding an optimal solution is often computationally infeasible, making the use of approximate algorithms essential. Adding to the complexity, agents might act in a self-interested and strategic way, possibly misrepresenting their goals to the MAPF algorithm if it benefits them. Although the field of mechanism design offers tools to align incentives, using these tools without careful consideration can fail when only having access to approximately optimal outcomes. Since approximations are crucial for scalable MAPF algorithms, this poses a significant challenge. In this work, we introduce the problem of scalable mechanism design for MAPF and propose three strat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2401.17010</link><description>&lt;p&gt;
&#20248;&#21270;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Finetuning Large Language Models for Vulnerability Detection. (arXiv:2401.17010v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.17010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20248;&#21270;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#24494;&#35843;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#24182;&#25913;&#36827;&#20854;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#23545;&#28431;&#27934;&#25968;&#25454;&#38598;&#30340;&#20998;&#31867;&#24615;&#33021;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#23545;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;StarCoder&#30340;&#25913;&#36827;&#29256;&#26412;WizardCoder&#65292;&#24182;&#36890;&#36807;&#36827;&#19968;&#27493;&#24494;&#35843;&#23558;&#20854;&#36866;&#24212;&#20110;&#28431;&#27934;&#26816;&#27979;&#20219;&#21153;&#12290;&#20026;&#20102;&#21152;&#36895;&#35757;&#32451;&#65292;&#25105;&#20204;&#20462;&#25913;&#20102;WizardCoder&#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#25506;&#31350;&#20102;&#26368;&#20339;&#30340;&#35757;&#32451;&#31574;&#30053;&#12290;&#38024;&#23545;&#36127;&#26679;&#26412;&#36828;&#22810;&#20110;&#27491;&#26679;&#26412;&#30340;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#23581;&#35797;&#20102;&#19981;&#21516;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;&#24494;&#35843;&#21518;&#30340;WizardCoder&#27169;&#22411;&#22312;&#24179;&#34913;&#21644;&#19981;&#24179;&#34913;&#30340;&#28431;&#27934;&#25968;&#25454;&#38598;&#19978;&#22312;ROC AUC&#21644;F1&#24230;&#37327;&#19978;&#23454;&#29616;&#20102;&#25913;&#36827;&#65292;&#35777;&#26126;&#20102;&#23558;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#28304;&#20195;&#30721;&#20013;&#30340;&#28431;&#27934;&#26816;&#27979;&#30340;&#26377;&#25928;&#24615;&#12290;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#23545;&#26368;&#20808;&#36827;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;WizardCoder&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20854;&#35757;&#32451;&#36895;&#24230;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#65292;&#24182;&#23545;&#35757;&#32451;&#36807;&#31243;&#21644;&#31574;&#30053;&#36827;&#34892;&#20102;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the results of finetuning large language models (LLMs) for the task of detecting vulnerabilities in source code. We leverage WizardCoder, a recent improvement of the state-of-the-art LLM StarCoder, and adapt it for vulnerability detection through further finetuning. To accelerate training, we modify WizardCoder's training procedure, also we investigate optimal training regimes. For the imbalanced dataset with many more negative examples than positive, we also explore different techniques to improve classification performance. The finetuned WizardCoder model achieves improvement in ROC AUC and F1 measures on balanced and imbalanced vulnerability datasets over CodeBERT-like model, demonstrating the effectiveness of adapting pretrained LLMs for vulnerability detection in source code. The key contributions are finetuning the state-of-the-art code LLM, WizardCoder, increasing its training speed without the performance harm, optimizing the training procedure and regimes, 
&lt;/p&gt;</description></item><item><title>ActDroid&#26159;&#19968;&#31181;&#29992;&#20110;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21450;&#26102;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#24773;&#20917;&#19979;&#23545;&#24694;&#24847;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#65292;&#24110;&#21161;&#35299;&#20915;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#26631;&#35760;&#38382;&#39064;&#21644;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16982</link><description>&lt;p&gt;
ActDroid&#65306;&#19968;&#31181;&#29992;&#20110;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ActDroid: An active learning framework for Android malware detection. (arXiv:2401.16982v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16982
&lt;/p&gt;
&lt;p&gt;
ActDroid&#26159;&#19968;&#31181;&#29992;&#20110;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#21450;&#26102;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#24773;&#20917;&#19979;&#23545;&#24694;&#24847;&#24212;&#29992;&#31243;&#24207;&#36827;&#34892;&#20934;&#30830;&#20998;&#31867;&#65292;&#24110;&#21161;&#35299;&#20915;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#30340;&#26631;&#35760;&#38382;&#39064;&#21644;&#27010;&#24565;&#28418;&#31227;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Android&#30340;&#26085;&#30410;&#26222;&#21450;&#35201;&#27714;&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#31995;&#32479;&#33021;&#22815;&#36319;&#19978;&#26032;&#36719;&#20214;&#21457;&#24067;&#30340;&#36895;&#24230;&#12290;&#26681;&#25454;&#26368;&#36817;&#30340;&#30740;&#31350;&#65292;&#27599;12&#31186;&#23601;&#26377;&#19968;&#31181;&#26032;&#30340;&#24694;&#24847;&#36719;&#20214;&#20986;&#29616;&#22312;&#20114;&#32852;&#32593;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#35270;&#20026;&#19968;&#20010;&#27969;&#25968;&#25454;&#38382;&#39064;&#65292;&#24182;&#25506;&#32034;&#20351;&#29992;&#20027;&#21160;&#22312;&#32447;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#21450;&#26102;&#21644;&#32463;&#27982;&#26377;&#25928;&#30340;&#26631;&#35760;&#24212;&#29992;&#31243;&#24207;&#30340;&#25163;&#27573;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#39640;&#36798;96&#65285;&#30340;&#20934;&#30830;&#29575;&#65292;&#21482;&#38656;&#23545;24&#65285;&#30340;&#35757;&#32451;&#25968;&#25454;&#36827;&#34892;&#26631;&#35760;&#65292;&#24182;&#34917;&#20607;&#20102;&#24212;&#29992;&#31243;&#24207;&#21457;&#24067;&#21644;&#26631;&#35760;&#20043;&#38388;&#21457;&#29983;&#30340;&#27010;&#24565;&#28418;&#31227;&#12290;&#25105;&#20204;&#36824;&#32771;&#34385;&#20102;Android&#24694;&#24847;&#36719;&#20214;&#26816;&#27979;&#20013;&#22312;&#32447;&#23398;&#20064;&#30340;&#26356;&#24191;&#27867;&#23454;&#29992;&#24615;&#65292;&#24182;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#20351;&#29992;&#19981;&#21516;&#38745;&#24577;&#12289;&#21160;&#24577;&#21644;&#28151;&#21512;&#29305;&#24449;&#38598;&#23545;&#24694;&#24847;&#36719;&#20214;&#36827;&#34892;&#20998;&#31867;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing popularity of Android requires malware detection systems that can keep up with the pace of new software being released. According to a recent study, a new piece of malware appears online every 12 seconds. To address this, we treat Android malware detection as a streaming data problem and explore the use of active online learning as a means of mitigating the problem of labelling applications in a timely and cost-effective manner. Our resulting framework achieves accuracies of up to 96\%, requires as little of 24\% of the training data to be labelled, and compensates for concept drift that occurs between the release and labelling of an application. We also consider the broader practicalities of online learning within Android malware detection, and systematically explore the trade-offs between using different static, dynamic and hybrid feature sets to classify malware.
&lt;/p&gt;</description></item><item><title>CORE&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22240;&#26524;&#25512;&#26029;&#21644;&#24178;&#39044;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25581;&#31034;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#22312;&#32467;&#26500;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.16974</link><description>&lt;p&gt;
CORE: &#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#23454;&#29616;&#21487;&#25193;&#23637;&#39640;&#25928;&#30340;&#22240;&#26524;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
CORE: Towards Scalable and Efficient Causal Discovery with Reinforcement Learning. (arXiv:2401.16974v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16974
&lt;/p&gt;
&lt;p&gt;
CORE&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22240;&#26524;&#25512;&#26029;&#21644;&#24178;&#39044;&#35268;&#21010;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25581;&#31034;&#22240;&#26524;&#32467;&#26500;&#65292;&#24182;&#22312;&#32467;&#26500;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#26029;&#26159;&#20174;&#25968;&#25454;&#20013;&#25512;&#26029;&#22240;&#26524;&#32467;&#26500;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#21463;&#21040;Pearl&#30340;&#22240;&#26524;&#23618;&#27425;&#32467;&#26500;&#65288;Causal Hierarchy&#65289;&#30340;&#21551;&#21457;&#65292;&#35813;&#35770;&#25991;&#21628;&#21505;&#23558;&#24178;&#39044;&#24341;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#12290;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#20010;&#26041;&#20415;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23454;&#29616;&#36825;&#31181;&#20027;&#21160;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CORE&#65292;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#22240;&#26524;&#25512;&#26029;&#21644;&#24178;&#39044;&#35268;&#21010;&#26041;&#27861;&#12290;CORE&#23398;&#20064;&#20174;&#25968;&#25454;&#20013;&#39034;&#24207;&#37325;&#24314;&#22240;&#26524;&#22270;&#65292;&#24182;&#23398;&#20064;&#25191;&#34892;&#20449;&#24687;&#20016;&#23500;&#30340;&#24178;&#39044;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;CORE&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#22270;&#65292;&#24182;&#39640;&#25928;&#22320;&#25581;&#31034;&#22240;&#26524;&#32467;&#26500;&#12290;&#27492;&#22806;&#65292;CORE&#21487;&#20197;&#25193;&#23637;&#21040;&#20855;&#26377;&#22810;&#36798;10&#20010;&#21464;&#37327;&#30340;&#26356;&#22823;&#30340;&#22270;&#65292;&#24182;&#22312;&#32467;&#26500;&#20272;&#35745;&#20934;&#30830;&#24615;&#21644;&#26679;&#26412;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery is the challenging task of inferring causal structure from data. Motivated by Pearl's Causal Hierarchy (PCH), which tells us that passive observations alone are not enough to distinguish correlation from causation, there has been a recent push to incorporate interventions into machine learning research. Reinforcement learning provides a convenient framework for such an active approach to learning. This paper presents CORE, a deep reinforcement learning-based approach for causal discovery and intervention planning. CORE learns to sequentially reconstruct causal graphs from data while learning to perform informative interventions. Our results demonstrate that CORE generalizes to unseen graphs and efficiently uncovers causal structures. Furthermore, CORE scales to larger graphs with up to 10 variables and outperforms existing approaches in structure estimation accuracy and sample efficiency. All relevant code and supplementary material can be found at https://github.com/s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#65288;LLMEA&#65289;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#21319;&#23454;&#20307;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.16960</link><description>&lt;p&gt;
&#20004;&#20010;&#22836;&#32988;&#20110;&#19968;: &#20026;&#23454;&#20307;&#23545;&#40784;&#25972;&#21512;&#30693;&#35782;&#22270;&#35889;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Two Heads Are Better Than One: Integrating Knowledge from Knowledge Graphs and Large Language Models for Entity Alignment. (arXiv:2401.16960v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#30340;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#65288;LLMEA&#65289;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#21319;&#23454;&#20307;&#23545;&#40784;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#23545;&#40784;&#26159;&#21019;&#24314;&#26356;&#20840;&#38754;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#20808;&#20915;&#26465;&#20214;&#65292;&#28041;&#21450;&#22312;&#19981;&#21516;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#23450;&#20301;&#31561;&#20215;&#23454;&#20307;&#12290;&#30446;&#21069;&#30340;&#23454;&#20307;&#23545;&#40784;&#26041;&#27861;&#20027;&#35201;&#21033;&#29992;&#30693;&#35782;&#23884;&#20837;&#27169;&#22411;&#33719;&#21462;&#21253;&#21547;&#21508;&#31181;&#30456;&#20284;&#24615;&#65288;&#32467;&#26500;&#12289;&#20851;&#31995;&#21644;&#23646;&#24615;&#65289;&#30340;&#23454;&#20307;&#23884;&#20837;&#12290;&#28982;&#21518;&#36890;&#36807;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20449;&#24687;&#34701;&#21512;&#26426;&#21046;&#36827;&#34892;&#38598;&#25104;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#22266;&#26377;&#30340;&#24322;&#36136;&#24615;&#65292;&#26377;&#25928;&#21033;&#29992;&#22810;&#26041;&#38754;&#30340;&#20449;&#24687;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#38544;&#24335;&#25429;&#25417;&#23454;&#20307;&#35821;&#20041;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#36825;&#31181;&#38544;&#24335;&#30693;&#35782;&#23578;&#26410;&#34987;&#29992;&#20110;&#23454;&#20307;&#23545;&#40784;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;&#23454;&#20307;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#23454;&#20307;&#23545;&#40784;&#26694;&#26550;&#65288;LLMEA&#65289;&#65292;&#23558;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#32467;&#26500;&#30693;&#35782;&#19982;LLM&#20013;&#30340;&#35821;&#20041;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#23454;&#20307;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity alignment, which is a prerequisite for creating a more comprehensive Knowledge Graph (KG), involves pinpointing equivalent entities across disparate KGs. Contemporary methods for entity alignment have predominantly utilized knowledge embedding models to procure entity embeddings that encapsulate various similarities-structural, relational, and attributive. These embeddings are then integrated through attention-based information fusion mechanisms. Despite this progress, effectively harnessing multifaceted information remains challenging due to inherent heterogeneity. Moreover, while Large Language Models (LLMs) have exhibited exceptional performance across diverse downstream tasks by implicitly capturing entity semantics, this implicit knowledge has yet to be exploited for entity alignment. In this study, we propose a Large Language Model-enhanced Entity Alignment framework (LLMEA), integrating structural knowledge from KGs with semantic knowledge from LLMs to enhance entity alig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21019;&#24314;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21452;&#36275;&#26426;&#22120;&#20154;&#21160;&#24577;&#36816;&#21160;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#21487;&#20197;&#24212;&#29992;&#20110;&#22810;&#31181;&#21160;&#24577;&#21452;&#36275;&#25216;&#33021;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16889</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#22810;&#21151;&#33021;&#12289;&#21160;&#24577;&#21644;&#31283;&#20581;&#30340;&#21452;&#36275;&#36816;&#21160;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Versatile, Dynamic, and Robust Bipedal Locomotion Control. (arXiv:2401.16889v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21019;&#24314;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21452;&#36275;&#26426;&#22120;&#20154;&#21160;&#24577;&#36816;&#21160;&#25511;&#21046;&#22120;&#65292;&#35813;&#25511;&#21046;&#22120;&#21487;&#20197;&#24212;&#29992;&#20110;&#22810;&#31181;&#21160;&#24577;&#21452;&#36275;&#25216;&#33021;&#65292;&#24182;&#19988;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#23637;&#29616;&#20986;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20851;&#20110;&#20351;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21019;&#24314;&#21452;&#36275;&#26426;&#22120;&#20154;&#21160;&#24577;&#36816;&#21160;&#25511;&#21046;&#22120;&#30340;&#32508;&#21512;&#30740;&#31350;&#12290;&#25105;&#20204;&#19981;&#20165;&#20165;&#19987;&#27880;&#20110;&#21333;&#19968;&#30340;&#36816;&#21160;&#25216;&#33021;&#65292;&#32780;&#26159;&#24320;&#21457;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#29992;&#20110;&#19968;&#31995;&#21015;&#21160;&#24577;&#21452;&#36275;&#25216;&#33021;&#65292;&#20174;&#21608;&#26399;&#24615;&#34892;&#36208;&#21644;&#22868;&#36305;&#21040;&#38750;&#21608;&#26399;&#24615;&#36339;&#36291;&#21644;&#31449;&#31435;&#12290;&#25105;&#20204;&#22522;&#20110;RL&#30340;&#25511;&#21046;&#22120;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#21382;&#21490;&#26550;&#26500;&#65292;&#21033;&#29992;&#26426;&#22120;&#20154;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#36755;&#20837;/&#36755;&#20986;&#65288;I/O&#65289;&#21382;&#21490;&#12290;&#36890;&#36807;&#25552;&#20986;&#30340;&#31471;&#21040;&#31471;RL&#26041;&#27861;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#36825;&#31181;&#25511;&#21046;&#26550;&#26500;&#22312;&#27169;&#25311;&#29615;&#22659;&#21644;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#22810;&#26679;&#21270;&#25216;&#33021;&#19978;&#22987;&#32456;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#35813;&#30740;&#31350;&#36824;&#28145;&#20837;&#25506;&#35752;&#20102;&#25152;&#25552;&#20986;&#30340;RL&#31995;&#32479;&#22312;&#24320;&#21457;&#36816;&#21160;&#25511;&#21046;&#22120;&#26041;&#38754;&#24341;&#20837;&#30340;&#36866;&#24212;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#21487;&#20197;&#36866;&#24212;&#26102;&#38388;&#19981;&#21464;&#30340;&#21160;&#21147;&#23398;&#21464;&#21270;&#21644;&#26102;&#38388;&#21464;&#21270;&#30340;&#21464;&#21270;&#65292;&#22914;&#25509;&#35302;&#20107;&#20214;&#65292;&#36890;&#36807;&#26377;&#25928;&#22320;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive study on using deep reinforcement learning (RL) to create dynamic locomotion controllers for bipedal robots. Going beyond focusing on a single locomotion skill, we develop a general control solution that can be used for a range of dynamic bipedal skills, from periodic walking and running to aperiodic jumping and standing. Our RL-based controller incorporates a novel dual-history architecture, utilizing both a long-term and short-term input/output (I/O) history of the robot. This control architecture, when trained through the proposed end-to-end RL approach, consistently outperforms other methods across a diverse range of skills in both simulation and the real world.The study also delves into the adaptivity and robustness introduced by the proposed RL system in developing locomotion controllers. We demonstrate that the proposed architecture can adapt to both time-invariant dynamics shifts and time-variant changes, such as contact events, by effectivel
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23545;&#27604;&#20102;B&#26679;&#26465;&#27169;&#22411;&#21644;&#32593;&#26684;&#27169;&#22411;&#20004;&#31181;&#24120;&#29992;&#30340;&#21464;&#24418;&#27169;&#22411;&#65292;&#22312;&#23454;&#36341;&#20013;&#23427;&#20204;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#24378;&#35843;&#20102;&#36890;&#36807;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#20840;&#38754;&#30340;&#22270;&#20687;&#37197;&#20934;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.16867</link><description>&lt;p&gt;
&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#20013;&#30340;B&#26679;&#26465;&#21644;&#32593;&#26684;&#22810;&#30446;&#26631;&#21464;&#24418;&#27169;&#22411;&#30340;&#23545;&#27604;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Tournament of Transformation Models: B-Spline-based vs. Mesh-based Multi-Objective Deformable Image Registration. (arXiv:2401.16867v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16867
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23545;&#27604;&#20102;B&#26679;&#26465;&#27169;&#22411;&#21644;&#32593;&#26684;&#27169;&#22411;&#20004;&#31181;&#24120;&#29992;&#30340;&#21464;&#24418;&#27169;&#22411;&#65292;&#22312;&#23454;&#36341;&#20013;&#23427;&#20204;&#20351;&#29992;&#20102;&#19981;&#21516;&#30340;&#20248;&#21270;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#35770;&#25991;&#24378;&#35843;&#20102;&#36890;&#36807;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26356;&#20840;&#38754;&#30340;&#22270;&#20687;&#37197;&#20934;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#25442;&#27169;&#22411;&#26159;&#20219;&#20309;&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#26041;&#27861;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#23427;&#25552;&#20379;&#20102;&#22270;&#20687;&#38388;&#29289;&#29702;&#21464;&#24418;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#23450;&#20041;&#20102;&#21487;&#20197;&#25214;&#21040;&#30340;&#37197;&#20934;&#33539;&#22260;&#21644;&#36924;&#30495;&#24230;&#12290;B&#26679;&#26465;&#27169;&#22411;&#21644;&#32593;&#26684;&#27169;&#22411;&#26159;&#20004;&#31181;&#27969;&#34892;&#30340;&#36873;&#25321;&#12290;&#34429;&#28982;&#36825;&#20004;&#31181;&#27169;&#22411;&#37117;&#24050;&#32463;&#24471;&#21040;&#20102;&#35814;&#32454;&#30740;&#31350;&#65292;&#20294;&#30001;&#20110;&#23454;&#36341;&#20013;&#20351;&#29992;&#20102;&#38750;&#24120;&#19981;&#21516;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#22240;&#27492;&#23578;&#26410;&#36827;&#34892;&#30452;&#25509;&#27604;&#36739;&#12290;B&#26679;&#26465;&#27169;&#22411;&#20027;&#35201;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#32780;&#32593;&#26684;&#27169;&#22411;&#36890;&#24120;&#20351;&#29992;&#26377;&#38480;&#20803;&#26041;&#27861;&#27714;&#35299;&#22120;&#25110;&#36827;&#21270;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#22810;&#30446;&#26631;&#20248;&#21270;&#26041;&#27861;&#26088;&#22312;&#23547;&#25214;&#19968;&#32452;&#39640;&#36136;&#37327;&#30340;&#26435;&#34913;&#27880;&#20876;&#32467;&#26524;&#65292;&#22312;&#21487;&#21464;&#24418;&#22270;&#20687;&#37197;&#20934;&#20013;&#36234;&#26469;&#36234;&#21463;&#21040;&#37325;&#35270;&#12290;&#30001;&#20110;&#36825;&#20123;&#26041;&#27861;&#23547;&#25214;&#19968;&#32452;&#22810;&#26679;&#30340;&#27880;&#20876;&#32467;&#26524;&#65292;&#23427;&#20204;&#21487;&#20197;&#25552;&#20379;&#26356;&#23436;&#25972;&#30340;&#22270;&#20687;&#37197;&#20934;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformation model is an essential component of any deformable image registration approach. It provides a representation of physical deformations between images, thereby defining the range and realism of registrations that can be found. Two types of transformation models have emerged as popular choices: B-spline models and mesh models. Although both models have been investigated in detail, a direct comparison has not yet been made, since the models are optimized using very different optimization methods in practice. B-spline models are predominantly optimized using gradient-descent methods, while mesh models are typically optimized using finite-element method solvers or evolutionary algorithms. Multi-objective optimization methods, which aim to find a diverse set of high-quality trade-off registrations, are increasingly acknowledged to be important in deformable image registration. Since these methods search for a diverse set of registrations, they can provide a more complete pic
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26041;&#24335;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#25512;&#24191;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#24615;&#36136;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#12290;</title><link>http://arxiv.org/abs/2401.16808</link><description>&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#26469;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;
&lt;/p&gt;
&lt;p&gt;
Encoding Temporal Statistical-space Priors via Augmented Representation. (arXiv:2401.16808v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16808
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22686;&#21152;&#34920;&#31034;&#30340;&#26041;&#24335;&#32534;&#30721;&#26102;&#38388;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#65292;&#20197;&#24212;&#23545;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20013;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#25512;&#24191;&#24615;&#33021;&#26126;&#26174;&#20248;&#20110;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#26041;&#27861;&#12290;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#24615;&#36136;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#21508;&#31181;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#24314;&#27169;&#20173;&#28982;&#26159;&#19968;&#20010;&#26222;&#36941;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#26102;&#38388;&#32500;&#24230;&#19982;&#35768;&#22810;&#39046;&#22495;&#23494;&#20999;&#30456;&#20851;&#12290;&#23613;&#31649;&#22312;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#39640;&#22122;&#22768;&#20449;&#21495;&#27604;&#12289;&#38750;&#27491;&#24577;&#24615;&#12289;&#38750;&#24179;&#31283;&#24615;&#21644;&#25968;&#25454;&#32570;&#20047;&#20173;&#28982;&#26159;&#25361;&#25112;&#20174;&#19994;&#32773;&#30340;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#31181;&#31616;&#21333;&#30340;&#34920;&#31034;&#22686;&#24378;&#25216;&#26415;&#26469;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22686;&#24378;&#34920;&#31034;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#19978;&#20316;&#20026;&#32479;&#35745;&#31354;&#38388;&#20808;&#39564;&#36827;&#34892;&#32534;&#30721;&#12290;&#20316;&#20026;&#21709;&#24212;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#21629;&#21517;&#20026;&#32479;&#35745;&#31354;&#38388;&#22686;&#24378;&#34920;&#31034;&#65288;SSAR&#65289;&#12290;&#22522;&#20110;&#39640;&#32500;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65292;&#21551;&#21457;&#20102;&#25105;&#20204;&#30340;&#34920;&#31034;&#22686;&#24378;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#23545;&#20004;&#20010;&#19979;&#28216;&#26102;&#38388;&#23398;&#20064;&#31639;&#27861;&#30340;&#32463;&#39564;&#27867;&#21270;&#24615;&#33021;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#26816;&#26597;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20987;&#36133;&#20102;&#20116;&#20010;&#26368;&#26032;&#30340;&#22522;&#20934;&#32447;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#39640;&#24230;&#27169;&#22359;&#21270;&#30340;&#24615;&#36136;&#65292;&#21487;&#20197;&#36731;&#26494;&#24212;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling time series data remains a pervasive issue as the temporal dimension is inherent to numerous domains. Despite significant strides in time series forecasting, high noise-to-signal ratio, non-normality, non-stationarity, and lack of data continue challenging practitioners. In response, we leverage a simple representation augmentation technique to overcome these challenges. Our augmented representation acts as a statistical-space prior encoded at each time step. In response, we name our method Statistical-space Augmented Representation (SSAR). The underlying high-dimensional data-generating process inspires our representation augmentation. We rigorously examine the empirical generalization performance on two data sets with two downstream temporal learning algorithms. Our approach significantly beats all five up-to-date baselines. Moreover, the highly modular nature of our approach can easily be applied to various settings. Lastly, fully-fledged theoretical perspectives are availa
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#20808;&#36827;&#30340;&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;LLM&#36741;&#21161;&#20889;&#20316;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#24615;&#33021;&#19981;&#22914;&#19968;&#20010;&#31616;&#21333;&#30340;&#26816;&#27979;&#22120;&#12290;&#30740;&#31350;&#35748;&#20026;&#38656;&#35201;&#24320;&#21457;&#19987;&#38376;&#29992;&#20110;LLM&#36741;&#21161;&#20889;&#20316;&#30340;&#29305;&#23450;&#26816;&#27979;&#22120;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#25215;&#35748;&#23454;&#36341;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2401.16807</link><description>&lt;p&gt;
&#22312;&#31185;&#23398;&#20132;&#27969;&#20013;&#26816;&#27979;LLM&#36741;&#21161;&#20889;&#20316;&#65306;&#25105;&#20204;&#24050;&#32463;&#21040;&#36798;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Detecting LLM-Assisted Writing in Scientific Communication: Are We There Yet?. (arXiv:2401.16807v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16807
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#22235;&#31181;&#20808;&#36827;&#30340;&#25991;&#26412;&#26816;&#27979;&#22120;&#23545;LLM&#36741;&#21161;&#20889;&#20316;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#24615;&#33021;&#19981;&#22914;&#19968;&#20010;&#31616;&#21333;&#30340;&#26816;&#27979;&#22120;&#12290;&#30740;&#31350;&#35748;&#20026;&#38656;&#35201;&#24320;&#21457;&#19987;&#38376;&#29992;&#20110;LLM&#36741;&#21161;&#20889;&#20316;&#30340;&#29305;&#23450;&#26816;&#27979;&#22120;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#25215;&#35748;&#23454;&#36341;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#23588;&#20854;&#26159;&#22312;&#20889;&#20316;&#36741;&#21161;&#39046;&#22495;&#12290;&#23613;&#31649;&#20262;&#29702;&#32771;&#34385;&#24378;&#35843;&#20102;&#22312;&#31185;&#23398;&#20132;&#27969;&#20013;&#36879;&#26126;&#22320;&#25215;&#35748;LLM&#30340;&#20351;&#29992;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#30495;&#23454;&#30340;&#25215;&#35748;&#20173;&#28982;&#24456;&#23569;&#35265;&#12290;&#40723;&#21169;&#20934;&#30830;&#25215;&#35748;LLM&#36741;&#21161;&#20889;&#20316;&#30340;&#19968;&#20010;&#28508;&#22312;&#36884;&#24452;&#28041;&#21450;&#20351;&#29992;&#33258;&#21160;&#26816;&#27979;&#22120;&#12290;&#25105;&#20204;&#23545;&#22235;&#20010;&#21069;&#27839;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#22120;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21457;&#29616;&#23427;&#20204;&#30340;&#24615;&#33021;&#19981;&#22914;&#19968;&#20010;&#31616;&#21333;&#30340;&#20020;&#26102;&#26816;&#27979;&#22120;&#65292;&#35813;&#26816;&#27979;&#22120;&#35774;&#35745;&#29992;&#20110;&#35782;&#21035;&#22312;LLM&#22823;&#37327;&#20986;&#29616;&#26102;&#30340;&#31361;&#28982;&#20889;&#20316;&#39118;&#26684;&#21464;&#21270;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24320;&#21457;&#19987;&#38376;&#29992;&#20110;LLM&#36741;&#21161;&#20889;&#20316;&#26816;&#27979;&#30340;&#19987;&#29992;&#26816;&#27979;&#22120;&#26159;&#24517;&#35201;&#30340;&#12290;&#36825;&#26679;&#30340;&#26816;&#27979;&#22120;&#21487;&#20197;&#22312;&#20419;&#36827;&#23545;LLM&#21442;&#19982;&#31185;&#23398;&#20132;&#27969;&#30340;&#26356;&#30495;&#23454;&#35748;&#21487;&#12289;&#35299;&#20915;&#24403;&#21069;&#25215;&#35748;&#23454;&#36341;&#20013;&#30340;&#25361;&#25112;&#26041;&#38754;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), exemplified by ChatGPT, have significantly reshaped text generation, particularly in the realm of writing assistance. While ethical considerations underscore the importance of transparently acknowledging LLM use, especially in scientific communication, genuine acknowledgment remains infrequent. A potential avenue to encourage accurate acknowledging of LLM-assisted writing involves employing automated detectors. Our evaluation of four cutting-edge LLM-generated text detectors reveals their suboptimal performance compared to a simple ad-hoc detector designed to identify abrupt writing style changes around the time of LLM proliferation. We contend that the development of specialized detectors exclusively dedicated to LLM-assisted writing detection is necessary. Such detectors could play a crucial role in fostering more authentic recognition of LLM involvement in scientific communication, addressing the current challenges in acknowledgment practices.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#36275;&#29699;&#29699;&#21592;&#30340;&#36716;&#20250;&#36153;&#29992;&#12290;&#36825;&#21487;&#20197;&#24110;&#21161;&#20465;&#20048;&#37096;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#34920;&#29616;&#24182;&#22686;&#21152;&#20465;&#20048;&#37096;&#30340;&#39044;&#31639;&#12290;</title><link>http://arxiv.org/abs/2401.16795</link><description>&lt;p&gt;
&#22522;&#20110;&#24615;&#33021;&#27934;&#23519;&#30340;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#36275;&#29699;&#36716;&#20250;&#36153;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Performance Insights-based AI-driven Football Transfer Fee Prediction. (arXiv:2401.16795v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16795
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39044;&#27979;&#36275;&#29699;&#29699;&#21592;&#30340;&#36716;&#20250;&#36153;&#29992;&#12290;&#36825;&#21487;&#20197;&#24110;&#21161;&#20465;&#20048;&#37096;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#25552;&#39640;&#34920;&#29616;&#24182;&#22686;&#21152;&#20465;&#20048;&#37096;&#30340;&#39044;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#26469;&#39044;&#27979;&#36275;&#29699;&#29699;&#21592;&#30340;&#36716;&#20250;&#36153;&#29992;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20465;&#20048;&#37096;&#22312;&#36141;&#20080;&#21644;&#20986;&#21806;&#29699;&#21592;&#26102;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#65292;&#20174;&#32780;&#25552;&#39640;&#34920;&#29616;&#21644;&#22686;&#21152;&#20465;&#20048;&#37096;&#39044;&#31639;&#12290;&#36890;&#36807;&#25910;&#38598;&#29699;&#21592;&#34920;&#29616;&#12289;&#36716;&#20250;&#36153;&#29992;&#21644;&#20854;&#20182;&#21487;&#33021;&#24433;&#21709;&#29699;&#21592;&#20215;&#20540;&#30340;&#22240;&#32032;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#19968;&#20010;&#33021;&#22815;&#20934;&#30830;&#39044;&#27979;&#29699;&#21592;&#23545;&#27604;&#36187;&#24433;&#21709;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#25152;&#24471;&#32467;&#26524;&#20316;&#20026;&#36716;&#20250;&#36153;&#29992;&#39044;&#27979;&#22120;&#30340;&#29305;&#24449;&#20043;&#19968;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#24110;&#21161;&#20465;&#20048;&#37096;&#35782;&#21035;&#34987;&#20302;&#20272;&#30340;&#29699;&#21592;&#65292;&#24182;&#22312;&#20986;&#21806;&#26102;&#33719;&#24471;&#21033;&#28070;&#12290;&#23427;&#36824;&#21487;&#20197;&#24110;&#21161;&#20465;&#20048;&#37096;&#36991;&#20813;&#20026;&#29699;&#21592;&#25903;&#20184;&#36807;&#39640;&#36153;&#29992;&#12290;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#25104;&#20026;&#36275;&#29699;&#20465;&#20048;&#37096;&#30340;&#26377;&#20215;&#20540;&#24037;&#20855;&#65292;&#20026;&#20182;&#20204;&#22312;&#29699;&#21592;&#25307;&#21215;&#21644;&#36716;&#20250;&#26041;&#38754;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
We developed an artificial intelligence approach to predict the transfer fee of a football player. This model can help clubs make better decisions about which players to buy and sell, which can lead to improved performance and increased club budgets. Having collected data on player performance, transfer fees, and other factors that might affect a player's value, we then used this data to train a machine learning model that can accurately predict a player's impact on the game. We further passed the obtained results as one of the features to the predictor of transfer fees. The model can help clubs identify players who are undervalued and who could be sold for a profit. It can also help clubs avoid overpaying for players. We believe that our model can be a valuable tool for football clubs. It can help them make better decisions about player recruitment and transfers.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ScaleEval&#65292;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#36777;&#35770;&#30340;&#20803;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#20132;&#27969;&#22411;LLM&#20195;&#29702;&#30340;&#33021;&#21147;&#26469;&#26377;&#25928;&#12289;&#21487;&#38752;&#12289;&#39640;&#25928;&#22320;&#35780;&#20272;LLMs&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16788</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#35780;&#20272;&#32773;&#26159;&#21542;&#21487;&#20449;&#65311;&#36890;&#36807;&#20195;&#29702;&#36777;&#35770;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#20803;&#35780;&#20272;&#26469;&#35780;&#20272;LLMs
&lt;/p&gt;
&lt;p&gt;
Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate. (arXiv:2401.16788v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;ScaleEval&#65292;&#19968;&#20010;&#22522;&#20110;&#20195;&#29702;&#36777;&#35770;&#30340;&#20803;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#22810;&#20010;&#20132;&#27969;&#22411;LLM&#20195;&#29702;&#30340;&#33021;&#21147;&#26469;&#26377;&#25928;&#12289;&#21487;&#38752;&#12289;&#39640;&#25928;&#22320;&#35780;&#20272;LLMs&#22312;&#19981;&#21516;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#20855;&#26377;&#23454;&#29992;&#24615;&#65292;&#20294;&#35201;&#22312;&#19981;&#21516;&#30340;&#19978;&#19979;&#25991;&#20013;&#21487;&#38752;&#22320;&#35780;&#20272;LLMs&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#29616;&#20195;&#35780;&#20272;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;LLMs&#26469;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#21709;&#24212;&#12290;&#28982;&#32780;&#65292;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;LLMs&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#20803;&#35780;&#20272;&#36890;&#24120;&#21463;&#29616;&#26377;&#22522;&#20934;&#30340;&#35206;&#30422;&#33539;&#22260;&#38480;&#21046;&#65292;&#25110;&#32773;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#12290;&#36825;&#20984;&#26174;&#20102;&#36843;&#20999;&#38656;&#35201;&#21487;&#25193;&#23637;&#30340;&#20803;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#12289;&#21487;&#38752;&#12289;&#39640;&#25928;&#22320;&#35780;&#20272;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#21644;&#22330;&#26223;&#20013;&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#28508;&#22312;&#30340;&#26032;&#30340;&#12289;&#29992;&#25143;&#23450;&#20041;&#30340;&#22330;&#26223;&#20013;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ScaleEval&#65292;&#36825;&#26159;&#19968;&#20010;&#20195;&#29702;&#36777;&#35770;&#36741;&#21161;&#30340;&#20803;&#35780;&#20272;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#20132;&#27969;&#22411;LLM&#20195;&#29702;&#30340;&#33021;&#21147;&#12290;&#36825;&#20010;&#26694;&#26550;&#25903;&#25345;&#22810;&#36718;&#35752;&#35770;&#65292;&#20197;&#24110;&#21161;&#20154;&#24037;&#26631;&#27880;&#32773;&#21028;&#26029;&#26368;&#20855;&#33021;&#21147;&#30340;
&lt;/p&gt;
&lt;p&gt;
Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capab
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#22270;&#20844;&#24179;&#23398;&#20064;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20102;&#20915;&#23450;&#22270;&#20013;&#20559;&#24046;&#30340;&#22240;&#32032;&#65292;&#24182;&#25506;&#32034;&#20102;&#35757;&#32451;&#22270;&#21644;&#27979;&#35797;&#22270;&#20043;&#38388;&#34920;&#31034;&#36317;&#31163;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30830;&#20445;&#20844;&#24179;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.16784</link><description>&lt;p&gt;
&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#22270;&#20844;&#24179;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph Fairness Learning under Distribution Shifts. (arXiv:2401.16784v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16784
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20027;&#35201;&#30740;&#31350;&#20102;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#30340;&#22270;&#20844;&#24179;&#23398;&#20064;&#65292;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#30740;&#31350;&#21457;&#29616;&#20102;&#20915;&#23450;&#22270;&#20013;&#20559;&#24046;&#30340;&#22240;&#32032;&#65292;&#24182;&#25506;&#32034;&#20102;&#35757;&#32451;&#22270;&#21644;&#27979;&#35797;&#22270;&#20043;&#38388;&#34920;&#31034;&#36317;&#31163;&#30340;&#24433;&#21709;&#65292;&#23545;&#20110;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#30830;&#20445;&#20844;&#24179;&#24615;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#22270;&#32467;&#26500;&#25968;&#25454;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;GNNs&#21487;&#33021;&#20250;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#32487;&#25215;&#20559;&#35265;&#65292;&#24182;&#26681;&#25454;&#25935;&#24863;&#23646;&#24615;&#65288;&#22914;&#24615;&#21035;&#21644;&#31181;&#26063;&#65289;&#20570;&#20986;&#27495;&#35270;&#24615;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#20851;&#27880;&#22312;GNNs&#19978;&#30830;&#20445;&#20844;&#24179;&#24615;&#65292;&#20294;&#25152;&#26377;&#36825;&#20123;&#26041;&#27861;&#37117;&#22522;&#20110;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#22312;&#30456;&#21516;&#20998;&#24067;&#19979;&#30340;&#20551;&#35774;&#65292;&#21363;&#35757;&#32451;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#20010;&#22270;&#12290;&#22312;&#20998;&#24067;&#21464;&#21270;&#19979;&#65292;&#22270;&#30340;&#20844;&#24179;&#24615;&#24615;&#33021;&#26159;&#21542;&#20250;&#38477;&#20302;&#65311;&#20998;&#24067;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#22270;&#30340;&#20844;&#24179;&#23398;&#20064;&#65311;&#25152;&#26377;&#36825;&#20123;&#24320;&#25918;&#38382;&#39064;&#20174;&#29702;&#35770;&#19978;&#24456;&#22823;&#31243;&#24230;&#19978;&#26410;&#34987;&#25506;&#32034;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#22312;&#29702;&#35770;&#19978;&#30830;&#23450;&#20102;&#20915;&#23450;&#22270;&#20013;&#20559;&#24046;&#30340;&#22240;&#32032;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#24433;&#21709;&#27979;&#35797;&#22270;&#20844;&#24179;&#24615;&#30340;&#22240;&#32032;&#65292;&#20854;&#20013;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#22240;&#32032;&#26159;&#35757;&#32451;&#22270;&#21644;&#27979;&#35797;&#22270;&#20043;&#38388;&#26576;&#20123;&#32676;&#20307;&#30340;&#34920;&#31034;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have achieved remarkable performance on graph-structured data. However, GNNs may inherit prejudice from the training data and make discriminatory predictions based on sensitive attributes, such as gender and race. Recently, there has been an increasing interest in ensuring fairness on GNNs, but all of them are under the assumption that the training and testing data are under the same distribution, i.e., training data and testing data are from the same graph. Will graph fairness performance decrease under distribution shifts? How does distribution shifts affect graph fairness learning? All these open questions are largely unexplored from a theoretical perspective. To answer these questions, we first theoretically identify the factors that determine bias on a graph. Subsequently, we explore the factors influencing fairness on testing graphs, with a noteworthy factor being the representation distances of certain groups between the training and testing graph. M
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22806;&#37096;&#22870;&#21169;&#30340;&#37492;&#21035;&#22120;&#30340;&#36719;Q&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#23569;&#37327;&#19987;&#23478;&#25968;&#25454;&#21644;&#37319;&#26679;&#25968;&#25454;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#36890;&#36807;&#28155;&#21152;&#22522;&#20110;&#23545;&#25239;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#31639;&#27861;&#26356;&#21152;&#31283;&#20581;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2401.16772</link><description>&lt;p&gt;
&#22522;&#20110;&#22806;&#37096;&#22870;&#21169;&#30340;&#37492;&#21035;&#22120;&#30340;&#36719;Q&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Extrinsicaly Rewarded Soft Q Imitation Learning with Discriminator. (arXiv:2401.16772v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16772
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22806;&#37096;&#22870;&#21169;&#30340;&#37492;&#21035;&#22120;&#30340;&#36719;Q&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#22312;&#23569;&#37327;&#19987;&#23478;&#25968;&#25454;&#21644;&#37319;&#26679;&#25968;&#25454;&#20013;&#36827;&#34892;&#27169;&#20223;&#23398;&#20064;&#26102;&#36935;&#21040;&#30340;&#22256;&#38590;&#65292;&#21516;&#26102;&#36890;&#36807;&#28155;&#21152;&#22522;&#20110;&#23545;&#25239;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20351;&#31639;&#27861;&#26356;&#21152;&#31283;&#20581;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#38590;&#20197;&#35774;&#35745;&#22870;&#21169;&#25110;&#22870;&#21169;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#65292;&#27169;&#20223;&#23398;&#20064;&#24120;&#24120;&#19982;&#24378;&#21270;&#23398;&#20064;&#32467;&#21512;&#20351;&#29992;&#65292;&#20294;&#22312;&#23569;&#37327;&#19987;&#23478;&#25968;&#25454;&#21644;&#37319;&#26679;&#25968;&#25454;&#20013;&#24456;&#38590;&#22312;&#26410;&#30693;&#29366;&#24577;&#20013;&#33391;&#22909;&#22320;&#36827;&#34892;&#27169;&#20223;&#12290;&#34892;&#20026;&#20811;&#38534;&#31561;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#19981;&#38656;&#35201;&#37319;&#26679;&#25968;&#25454;&#65292;&#20294;&#36890;&#24120;&#20250;&#21463;&#21040;&#20998;&#24067;&#20559;&#31227;&#30340;&#22256;&#25200;&#12290;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22914;&#36870;&#21521;&#24378;&#21270;&#23398;&#20064;&#21644;&#29983;&#25104;&#23545;&#25239;&#27169;&#20223;&#23398;&#20064;&#65288;GAIL&#65289;&#65292;&#21487;&#20197;&#20174;&#23569;&#37327;&#19987;&#23478;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#65292;&#20294;&#36890;&#24120;&#38656;&#35201;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36719;Q&#27169;&#20223;&#23398;&#20064;&#65288;SQIL&#65289;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#23558;&#34892;&#20026;&#20811;&#38534;&#21644;&#24120;&#25968;&#22870;&#21169;&#30340;&#36719;Q&#23398;&#20064;&#30456;&#32467;&#21512;&#65292;&#34920;&#26126;&#33021;&#22815;&#39640;&#25928;&#23398;&#20064;&#12290;&#20026;&#20102;&#20351;&#35813;&#31639;&#27861;&#23545;&#20998;&#24067;&#20559;&#31227;&#26356;&#21152;&#31283;&#20581;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#39640;&#25928;&#21644;&#26356;&#31283;&#20581;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#22312;&#35813;&#26041;&#27861;&#20013;&#28155;&#21152;&#22522;&#20110;&#23545;&#25239;&#30340;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation learning is often used in addition to reinforcement learning in environments where reward design is difficult or where the reward is sparse, but it is difficult to be able to imitate well in unknown states from a small amount of expert data and sampling data. Supervised learning methods such as Behavioral Cloning do not require sampling data, but usually suffer from distribution shift. The methods based on reinforcement learning, such as inverse reinforcement learning and Generative Adversarial imitation learning (GAIL), can learn from only a few expert data. However, they often need to interact with the environment. Soft Q imitation learning (SQIL) addressed the problems, and it was shown that it could learn efficiently by combining Behavioral Cloning and soft Q-learning with constant rewards. In order to make this algorithm more robust to distribution shift, we propose more efficient and robust algorithm by adding to this method a reward function based on adversarial invers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#26816;&#27979;&#21644;&#24674;&#22797;&#26694;&#26550;&#65288;CFDR&#65289;&#65292;&#36890;&#36807;&#23558;&#23545;&#27604;&#23398;&#20064;&#24212;&#29992;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#27969;&#31243;&#20013;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#24341;&#25806;&#65292;&#22312;&#21482;&#26377;&#19968;&#20010;&#25209;&#27425;&#30340;&#27979;&#35797;&#25968;&#25454;&#21644;&#23569;&#37327;&#26080;&#26631;&#31614;&#27979;&#35797;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24182;&#24555;&#36895;&#24674;&#22797;&#22810;&#31181;&#31867;&#22411;&#30340;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2401.16766</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#26816;&#27979;&#21644;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Detection and Recovery Against Deep Neural Network Fault Injection Attacks Based on Contrastive Learning. (arXiv:2401.16766v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#26816;&#27979;&#21644;&#24674;&#22797;&#26694;&#26550;&#65288;CFDR&#65289;&#65292;&#36890;&#36807;&#23558;&#23545;&#27604;&#23398;&#20064;&#24212;&#29992;&#20110;&#35757;&#32451;&#21644;&#25512;&#29702;&#27969;&#31243;&#20013;&#65292;&#23454;&#29616;&#20102;&#20855;&#26377;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#24341;&#25806;&#65292;&#22312;&#21482;&#26377;&#19968;&#20010;&#25209;&#27425;&#30340;&#27979;&#35797;&#25968;&#25454;&#21644;&#23569;&#37327;&#26080;&#26631;&#31614;&#27979;&#35797;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23454;&#26102;&#26816;&#27979;&#24182;&#24555;&#36895;&#24674;&#22797;&#22810;&#31181;&#31867;&#22411;&#30340;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#25191;&#34892;&#35774;&#22791;&#19978;&#20316;&#20026;&#25512;&#29702;&#24341;&#25806;&#23454;&#26045;&#26102;&#23481;&#26131;&#21463;&#21040;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#25805;&#32437;&#27169;&#22411;&#21442;&#25968;&#20197;&#30772;&#22351;&#25512;&#29702;&#25191;&#34892;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#23558;&#23545;&#27604;&#23398;&#20064;&#24212;&#29992;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35757;&#32451;&#21644;&#25512;&#29702;&#27969;&#31243;&#20013;&#65292;&#20197;&#23454;&#29616;&#20855;&#26377;&#33258;&#36866;&#24212;&#33021;&#21147;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#24341;&#25806;&#65292;&#20197;&#24212;&#23545;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#22522;&#20110;&#23545;&#27604;&#23398;&#20064;&#30340;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#26816;&#27979;&#21644;&#24674;&#22797;&#65288;CFDR&#65289;&#26694;&#26550;&#20855;&#26377;&#20197;&#19979;&#29305;&#28857;&#65306;&#65288;i&#65289;&#20165;&#38656;&#19968;&#20010;&#25209;&#27425;&#30340;&#27979;&#35797;&#25968;&#25454;&#36827;&#34892;&#23454;&#26102;&#26816;&#27979;&#65292;&#65288;ii&#65289;&#21363;&#20351;&#20165;&#26377;&#23569;&#37327;&#26080;&#26631;&#31614;&#27979;&#35797;&#25968;&#25454;&#65292;&#20063;&#33021;&#23454;&#29616;&#24555;&#36895;&#30340;&#24674;&#22797;&#25928;&#26524;&#12290;&#22312;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#23545;&#22810;&#31181;&#31867;&#22411;&#30340;&#25925;&#38556;&#27880;&#20837;&#25915;&#20987;&#36827;&#34892;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;CFDR&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#26816;&#27979;&#21644;&#24674;&#22797;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Network (DNN) models when implemented on executing devices as the inference engines are susceptible to Fault Injection Attacks (FIAs) that manipulate model parameters to disrupt inference execution with disastrous performance. This work introduces Contrastive Learning (CL) of visual representations i.e., a self-supervised learning approach into the deep learning training and inference pipeline to implement DNN inference engines with self-resilience under FIAs. Our proposed CL based FIA Detection and Recovery (CFDR) framework features (i) real-time detection with only a single batch of testing data and (ii) fast recovery effective even with only a small amount of unlabeled testing data. Evaluated with the CIFAR-10 dataset on multiple types of FIAs, our CFDR shows promising detection and recovery effectiveness.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#36234;&#29425;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#22312;&#36328;&#35821;&#35328;&#30340;&#29615;&#22659;&#19979;&#65292;&#24694;&#24847;&#38382;&#39064;&#21487;&#20197;&#36867;&#36991;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#32463;&#39564;&#30740;&#31350;&#21644;&#35821;&#20041;&#20445;&#30041;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#25581;&#31034;&#20102;&#22810;&#35821;&#35328;&#36234;&#29425;&#25915;&#20987;&#30340;&#27169;&#24335;&#21644;&#23433;&#20840;&#23041;&#32961;&#12290;</title><link>http://arxiv.org/abs/2401.16765</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#36234;&#29425;&#25915;&#20987;&#30340;&#36328;&#35821;&#35328;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Cross-Language Investigation into Jailbreak Attacks in Large Language Models. (arXiv:2401.16765v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16765
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38754;&#20020;&#30528;&#36234;&#29425;&#25915;&#20987;&#30340;&#23041;&#32961;&#65292;&#22312;&#36328;&#35821;&#35328;&#30340;&#29615;&#22659;&#19979;&#65292;&#24694;&#24847;&#38382;&#39064;&#21487;&#20197;&#36867;&#36991;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#36890;&#36807;&#24191;&#27867;&#30340;&#32463;&#39564;&#30740;&#31350;&#21644;&#35821;&#20041;&#20445;&#30041;&#31639;&#27861;&#30340;&#24320;&#21457;&#65292;&#25581;&#31034;&#20102;&#22810;&#35821;&#35328;&#36234;&#29425;&#25915;&#20987;&#30340;&#27169;&#24335;&#21644;&#23433;&#20840;&#23041;&#32961;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#20808;&#36827;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#32780;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#20687;&#20219;&#20309;&#36719;&#20214;&#19968;&#26679;&#65292;&#23427;&#20204;&#38754;&#20020;&#23433;&#20840;&#25361;&#25112;&#65292;&#21253;&#25324;&#8220;&#36234;&#29425;&#8221;&#25915;&#20987;&#30340;&#39118;&#38505;&#65292;&#21363;&#25805;&#32437;LLMs&#29983;&#25104;&#34987;&#31105;&#20869;&#23481;&#12290;&#19968;&#20010;&#22312;&#30740;&#31350;&#20013;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#30340;&#39046;&#22495;&#26159;&#22810;&#35821;&#35328;&#36234;&#29425;&#25915;&#20987;&#65292;&#21363;&#23558;&#24694;&#24847;&#38382;&#39064;&#32763;&#35793;&#25104;&#21508;&#31181;&#35821;&#35328;&#20197;&#36867;&#36991;&#23433;&#20840;&#36807;&#28388;&#22120;&#12290;&#30446;&#21069;&#65292;&#23578;&#32570;&#20047;&#20840;&#38754;&#30340;&#32463;&#39564;&#35777;&#25454;&#30340;&#30740;&#31350;&#26469;&#35299;&#20915;&#36825;&#19968;&#29305;&#23450;&#23041;&#32961;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#65292;&#25105;&#20204;&#23545;&#22810;&#35821;&#35328;&#36234;&#29425;&#25915;&#20987;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32463;&#39564;&#30740;&#31350;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35821;&#20041;&#20445;&#30041;&#31639;&#27861;&#26469;&#21019;&#24314;&#19968;&#20010;&#22810;&#35821;&#35328;&#36234;&#29425;&#25968;&#25454;&#38598;&#65292;&#24182;&#23545;&#21253;&#25324;GPT-4&#21644;LLaMa&#22312;&#20869;&#30340;&#24191;&#27867;&#20351;&#29992;&#30340;&#24320;&#28304;&#21644;&#21830;&#19994;LLMs&#36827;&#34892;&#20102;&#35814;&#23613;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#65292;&#20197;&#25581;&#31034;&#22810;&#35821;&#35328;&#36234;&#29425;&#25915;&#20987;&#20013;&#30340;&#27169;&#24335;&#65292;&#24182;&#23454;&#26045;&#20102;&#31934;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have become increasingly popular for their advanced text generation capabilities across various domains. However, like any software, they face security challenges, including the risk of 'jailbreak' attacks that manipulate LLMs to produce prohibited content. A particularly underexplored area is the Multilingual Jailbreak attack, where malicious questions are translated into various languages to evade safety filters. Currently, there is a lack of comprehensive empirical studies addressing this specific threat.  To address this research gap, we conducted an extensive empirical study on Multilingual Jailbreak attacks. We developed a novel semantic-preserving algorithm to create a multilingual jailbreak dataset and conducted an exhaustive evaluation on both widely-used open-source and commercial LLMs, including GPT-4 and LLaMa. Additionally, we performed interpretability analysis to uncover patterns in Multilingual Jailbreak attacks and implemented a fine-tuning
&lt;/p&gt;</description></item><item><title>SwapNet&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#32536;AI&#35774;&#22791;DNN&#22359;&#20132;&#25442;&#20013;&#38388;&#20214;&#65292;&#22312;&#36229;&#20986;&#20869;&#23384;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20998;&#35299;DNN&#20026;&#22359;&#24182;&#36827;&#34892;&#20132;&#25442;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;DNN&#30340;&#39640;&#25928;&#25191;&#34892;&#12290;</title><link>http://arxiv.org/abs/2401.16757</link><description>&lt;p&gt;
SwapNet: &#36229;&#20986;&#20869;&#23384;&#39044;&#31639;&#30340;&#36793;&#32536;AI&#35774;&#22791;&#19978;&#36827;&#34892;DNN&#25512;&#29702;&#30340;&#39640;&#25928;&#20132;&#25442;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
SwapNet: Efficient Swapping for DNN Inference on Edge AI Devices Beyond the Memory Budget. (arXiv:2401.16757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16757
&lt;/p&gt;
&lt;p&gt;
SwapNet&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#32536;AI&#35774;&#22791;DNN&#22359;&#20132;&#25442;&#20013;&#38388;&#20214;&#65292;&#22312;&#36229;&#20986;&#20869;&#23384;&#39044;&#31639;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#20998;&#35299;DNN&#20026;&#22359;&#24182;&#36827;&#34892;&#20132;&#25442;&#65292;&#23454;&#29616;&#20102;&#22823;&#22411;DNN&#30340;&#39640;&#25928;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#35774;&#22791;&#19978;&#25191;&#34892;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#21487;&#20197;&#23454;&#29616;&#21508;&#31181;&#33258;&#20027;&#31227;&#21160;&#35745;&#31639;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#36793;&#32536;AI&#35774;&#22791;&#30340;&#20869;&#23384;&#39044;&#31639;&#38480;&#21046;&#20102;&#36825;&#20123;&#24212;&#29992;&#20013;&#20801;&#35768;&#30340;DNN&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#27169;&#22411;&#21387;&#32553;&#25110;&#20113;&#21368;&#36733;&#65292;&#20943;&#23569;&#20102;DNN&#25512;&#29702;&#30340;&#20869;&#23384;&#21344;&#29992;&#65292;&#20294;&#21516;&#26102;&#20063;&#38477;&#20302;&#20102;&#27169;&#22411;&#20934;&#30830;&#24230;&#25110;&#33258;&#20027;&#24615;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#23558;DNN&#20998;&#35299;&#25104;&#22359;&#65292;&#24182;&#25353;&#39034;&#24207;&#20114;&#30456;&#20132;&#25442;&#65292;&#20197;&#20415;&#22312;&#36739;&#23567;&#30340;&#20869;&#23384;&#39044;&#31639;&#19979;&#25191;&#34892;&#22823;&#22411;DNN&#12290;&#28982;&#32780;&#65292;&#22312;&#36793;&#32536;AI&#35774;&#22791;&#19978;&#36827;&#34892;&#31616;&#21333;&#20132;&#25442;&#20250;&#24341;&#36215;&#26174;&#33879;&#30340;&#24310;&#36831;&#65292;&#22240;&#20026;&#22312;&#36793;&#32536;AI&#35774;&#22791;&#30340;DNN&#24320;&#21457;&#29983;&#24577;&#31995;&#32479;&#20013;&#23384;&#22312;&#20887;&#20313;&#30340;&#20869;&#23384;&#25805;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SwapNet&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#32536;AI&#35774;&#22791;DNN&#22359;&#20132;&#25442;&#20013;&#38388;&#20214;&#12290;&#25105;&#20204;&#31995;&#32479;&#22320;&#28040;&#38500;&#20102;&#22359;&#20132;&#25442;&#36807;&#31243;&#20013;&#19981;&#24517;&#35201;&#30340;&#20869;&#23384;&#25805;&#20316;&#65292;&#21516;&#26102;&#20445;&#25345;&#19982;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#21644;GPU&#21518;&#31471;&#30340;&#20860;&#23481;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Executing deep neural networks (DNNs) on edge artificial intelligence (AI) devices enables various autonomous mobile computing applications. However, the memory budget of edge AI devices restricts the number and complexity of DNNs allowed in such applications. Existing solutions, such as model compression or cloud offloading, reduce the memory footprint of DNN inference at the cost of decreased model accuracy or autonomy. To avoid these drawbacks, we divide DNN into blocks and swap them in and out in order, such that large DNNs can execute within a small memory budget. Nevertheless, naive swapping on edge AI devices induces significant delays due to the redundant memory operations in the DNN development ecosystem for edge AI devices. To this end, we develop SwapNet, an efficient DNN block swapping middleware for edge AI devices. We systematically eliminate the unnecessary memory operations during block swapping while retaining compatible with the deep learning frameworks, GPU backends,
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;(DiffRI)&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#23398;&#20064;&#25512;&#26029;&#32452;&#20214;&#20043;&#38388;&#36830;&#25509;&#23384;&#22312;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#22320;&#38754;&#30495;&#23454;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.16755</link><description>&lt;p&gt;
&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Diffusion model for relational inference. (arXiv:2401.16755v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16755
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;(DiffRI)&#65292;&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#23398;&#20064;&#25512;&#26029;&#32452;&#20214;&#20043;&#38388;&#36830;&#25509;&#23384;&#22312;&#30340;&#27010;&#29575;&#65292;&#24182;&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#22320;&#38754;&#30495;&#23454;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#20855;&#26377;&#24456;&#39640;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#31995;&#32479;&#30340;&#21160;&#24577;&#34892;&#20026;&#65292;&#21253;&#25324;&#22823;&#33041;&#27963;&#21160;&#12289;&#37329;&#34701;&#20215;&#26684;&#27874;&#21160;&#21644;&#29289;&#29702;&#38598;&#20307;&#29616;&#35937;&#65292;&#19982;&#31995;&#32479;&#32452;&#25104;&#37096;&#20998;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#30456;&#20851;&#12290;&#21033;&#29992;&#21487;&#35266;&#27979;&#30340;&#21160;&#24577;&#26469;&#21457;&#29616;&#36825;&#20123;&#31995;&#32479;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#20851;&#31995;&#34987;&#31216;&#20026;&#20851;&#31995;&#25512;&#29702;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#25512;&#29702;&#30340;&#25193;&#25955;&#27169;&#22411;(DiffRI)&#65292;&#23427;&#20511;&#37492;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#25554;&#20540;&#26041;&#27861;&#12290;DiffRI&#36890;&#36807;&#26465;&#20214;&#25193;&#25955;&#24314;&#27169;&#23398;&#20064;&#25512;&#26029;&#32452;&#20214;&#20043;&#38388;&#36830;&#25509;&#23384;&#22312;&#30340;&#27010;&#29575;&#12290;&#23545;&#20110;&#27169;&#25311;&#21644;&#20934;&#30495;&#23454;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;DiffRI&#22312;&#26080;&#30417;&#30563;&#26041;&#24335;&#19979;&#21457;&#29616;&#22320;&#38754;&#30495;&#23454;&#30456;&#20114;&#20316;&#29992;&#26041;&#38754;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#24456;&#39640;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#24456;&#24555;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dynamical behaviors of complex interacting systems, including brain activities, financial price movements, and physical collective phenomena, are associated with underlying interactions between the system's components. The issue of uncovering interaction relations in such systems using observable dynamics is called relational inference. In this study, we propose a Diffusion model for Relational Inference (DiffRI), inspired by a self-supervised method for probabilistic time series imputation. DiffRI learns to infer the probability of the presence of connections between components through conditional diffusion modeling. Experiments on both simulated and quasi-real datasets show that DiffRI is highly competent compared with other state-of-the-art models in discovering ground truth interactions in an unsupervised manner. Our code will be made public soon.
&lt;/p&gt;</description></item><item><title>ShaRP&#26159;&#19968;&#20010;&#22522;&#20110;Shapley&#20540;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#25490;&#21517;&#32467;&#26524;&#20013;&#21508;&#20010;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;&#21363;&#20351;&#20351;&#29992;&#32447;&#24615;&#35780;&#20998;&#20989;&#25968;&#65292;&#29305;&#24449;&#30340;&#26435;&#37325;&#20063;&#19981;&#19968;&#23450;&#23545;&#24212;&#20854;Shapley&#20540;&#30340;&#36129;&#29486;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#29305;&#24449;&#20998;&#24067;&#21644;&#35780;&#20998;&#29305;&#24449;&#20043;&#38388;&#30340;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.16744</link><description>&lt;p&gt;
ShaRP&#65306;&#29992;Shapley&#20540;&#35299;&#37322;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
ShaRP: Explaining Rankings with Shapley Values. (arXiv:2401.16744v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16744
&lt;/p&gt;
&lt;p&gt;
ShaRP&#26159;&#19968;&#20010;&#22522;&#20110;Shapley&#20540;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#25490;&#21517;&#32467;&#26524;&#20013;&#21508;&#20010;&#29305;&#24449;&#30340;&#36129;&#29486;&#12290;&#21363;&#20351;&#20351;&#29992;&#32447;&#24615;&#35780;&#20998;&#20989;&#25968;&#65292;&#29305;&#24449;&#30340;&#26435;&#37325;&#20063;&#19981;&#19968;&#23450;&#23545;&#24212;&#20854;Shapley&#20540;&#30340;&#36129;&#29486;&#65292;&#32780;&#26159;&#21462;&#20915;&#20110;&#29305;&#24449;&#20998;&#24067;&#21644;&#35780;&#20998;&#29305;&#24449;&#20043;&#38388;&#30340;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25307;&#32856;&#12289;&#22823;&#23398;&#25307;&#29983;&#21644;&#36151;&#27454;&#31561;&#37325;&#35201;&#39046;&#22495;&#30340;&#31639;&#27861;&#20915;&#31574;&#24120;&#24120;&#26159;&#22522;&#20110;&#25490;&#21517;&#30340;&#12290;&#30001;&#20110;&#36825;&#20123;&#20915;&#31574;&#23545;&#20010;&#20154;&#12289;&#32452;&#32455;&#21644;&#20154;&#32676;&#30340;&#24433;&#21709;&#65292;&#26377;&#24517;&#35201;&#20102;&#35299;&#23427;&#20204;&#65306;&#20102;&#35299;&#20915;&#31574;&#26159;&#21542;&#36981;&#23432;&#27861;&#24459;&#65292;&#24110;&#21161;&#20010;&#20154;&#25552;&#39640;&#20182;&#20204;&#30340;&#25490;&#21517;&#65292;&#24182;&#35774;&#35745;&#26356;&#22909;&#30340;&#25490;&#21517;&#31243;&#24207;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ShaRP&#65288;Shapley for Rankings and Preferences&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Shapley&#20540;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#37322;&#29305;&#24449;&#23545;&#25490;&#21517;&#32467;&#26524;&#19981;&#21516;&#26041;&#38754;&#30340;&#36129;&#29486;&#12290;&#20351;&#29992;ShaRP&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#31639;&#27861;&#25490;&#21517;&#22120;&#20351;&#29992;&#30340;&#35780;&#20998;&#20989;&#25968;&#26159;&#24050;&#30693;&#30340;&#19988;&#26159;&#32447;&#24615;&#30340;&#65292;&#27599;&#20010;&#29305;&#24449;&#30340;&#26435;&#37325;&#20063;&#19981;&#19968;&#23450;&#23545;&#24212;&#20854;Shapley&#20540;&#30340;&#36129;&#29486;&#12290;&#36129;&#29486;&#21462;&#20915;&#20110;&#29305;&#24449;&#30340;&#20998;&#24067;&#20197;&#21450;&#35780;&#20998;&#29305;&#24449;&#20043;&#38388;&#24494;&#22937;&#30340;&#23616;&#37096;&#30456;&#20114;&#20316;&#29992;&#12290;ShaRP&#22522;&#20110;&#37327;&#21270;&#36755;&#20837;&#24433;&#21709;&#26694;&#26550;&#65292;&#24182;&#21487;&#20197;&#35745;&#31639;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Algorithmic decisions in critical domains such as hiring, college admissions, and lending are often based on rankings. Because of the impact these decisions have on individuals, organizations, and population groups, there is a need to understand them: to know whether the decisions are abiding by the law, to help individuals improve their rankings, and to design better ranking procedures.  In this paper, we present ShaRP (Shapley for Rankings and Preferences), a framework that explains the contributions of features to different aspects of a ranked outcome, and is based on Shapley values. Using ShaRP, we show that even when the scoring function used by an algorithmic ranker is known and linear, the weight of each feature does not correspond to its Shapley value contribution. The contributions instead depend on the feature distributions, and on the subtle local interactions between the scoring features. ShaRP builds on the Quantitative Input Influence framework, and can compute the contri
&lt;/p&gt;</description></item><item><title>DecNefGAN&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#38381;&#29615;fMRI&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#29983;&#25104;&#23545;&#25239;&#31995;&#32479;&#21644;&#31070;&#32463;&#24378;&#21270;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#31867;&#22914;&#20309;&#23545;&#25239;&#21644;&#24212;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.16742</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#38381;&#29615;fMRI&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Generative AI-based closed-loop fMRI system. (arXiv:2401.16742v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16742
&lt;/p&gt;
&lt;p&gt;
DecNefGAN&#26159;&#19968;&#31181;&#22522;&#20110;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#38381;&#29615;fMRI&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#29983;&#25104;&#23545;&#25239;&#31995;&#32479;&#21644;&#31070;&#32463;&#24378;&#21270;&#27169;&#22411;&#65292;&#30740;&#31350;&#20154;&#31867;&#22914;&#20309;&#23545;&#25239;&#21644;&#24212;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#29616;&#22312;&#22312;&#31038;&#20250;&#20013;&#24191;&#27867;&#24212;&#29992;&#65292;&#24182;&#19988;&#20855;&#26377;&#24456;&#22823;&#30340;&#29992;&#22788;&#65292;&#20294;&#26159;&#23384;&#22312;&#28508;&#22312;&#30340;&#28389;&#29992;&#39118;&#38505;&#65292;&#20363;&#22914;&#65292;&#26080;&#24847;&#35782;&#22320;&#24433;&#21709;&#35748;&#30693;&#36807;&#31243;&#25110;&#20915;&#31574;&#12290;&#23613;&#31649;&#36825;&#22312;&#35748;&#30693;&#39046;&#22495;&#20013;&#24341;&#36215;&#20102;&#23433;&#20840;&#38382;&#39064;&#65292;&#20294;&#33267;&#20170;&#27809;&#26377;&#20851;&#20110;&#22312;&#20154;&#31867;&#36523;&#19978;&#23545;&#25239;&#24694;&#24847;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#24433;&#21709;&#30340;&#31070;&#32463;&#21644;&#35745;&#31639;&#26426;&#26426;&#21046;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;DecNefGAN&#65292;&#19968;&#31181;&#32467;&#21512;&#20102;&#29983;&#25104;&#23545;&#25239;&#31995;&#32479;&#21644;&#31070;&#32463;&#24378;&#21270;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DecNefGAN&#22312;&#19968;&#20010;&#38381;&#29615;&#31995;&#32479;&#20013;&#36830;&#25509;&#20102;&#20154;&#31867;&#21644;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#65292;&#20854;&#20013;&#20154;&#24037;&#26234;&#33021;&#21019;&#24314;&#35825;&#21457;&#29305;&#23450;&#24515;&#29702;&#29366;&#24577;&#30340;&#21050;&#28608;&#65292;&#20174;&#32780;&#23545;&#31070;&#32463;&#27963;&#21160;&#26045;&#21152;&#22806;&#37096;&#25511;&#21046;&#12290;&#20154;&#31867;&#30340;&#30446;&#26631;&#30456;&#21453;&#65292;&#35201;&#31454;&#20105;&#24182;&#36798;&#21040;&#19968;&#20010;&#27491;&#20132;&#30340;&#24515;&#29702;&#29366;&#24577;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#26377;&#21161;&#20110;&#38416;&#26126;&#20154;&#33041;&#22914;&#20309;&#23545;&#25239;&#21644;&#24212;&#23545;&#29983;&#25104;&#22411;&#20154;&#24037;&#26234;&#33021;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
While generative AI is now widespread and useful in society, there are potential risks of misuse, e.g., unconsciously influencing cognitive processes or decision-making. Although this causes a security problem in the cognitive domain, there has been no research about neural and computational mechanisms counteracting the impact of malicious generative AI in humans. We propose DecNefGAN, a novel framework that combines a generative adversarial system and a neural reinforcement model. More specifically, DecNefGAN bridges human and generative AI in a closed-loop system, with the AI creating stimuli that induce specific mental states, thus exerting external control over neural activity. The objective of the human is the opposite, to compete and reach an orthogonal mental state. This framework can contribute to elucidating how the human brain responds to and counteracts the potential influence of generative AI.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#21487;&#20280;&#32553;&#30340;&#26694;&#26550;&#65292;&#23558;&#25991;&#26412;&#25551;&#36848;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#20803;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#35299;&#37322;&#27169;&#22411;&#20013;&#29702;&#35299;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21457;&#29616;&#20154;&#21487;&#35299;&#37322;&#30340;&#25551;&#36848;&#31526;&#65292;&#24182;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#35299;&#37322;&#31070;&#32463;&#20803;&#65292;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16731</link><description>&lt;p&gt;
&#20026;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#20803;&#29983;&#25104;&#20449;&#24687;&#24615;&#25991;&#26412;&#25551;&#36848;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Generating Informative Textual Description for Neurons in Language Models. (arXiv:2401.16731v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#21487;&#20280;&#32553;&#30340;&#26694;&#26550;&#65292;&#23558;&#25991;&#26412;&#25551;&#36848;&#19982;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31070;&#32463;&#20803;&#32852;&#31995;&#36215;&#26469;&#65292;&#20174;&#32780;&#35299;&#37322;&#27169;&#22411;&#20013;&#29702;&#35299;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#20351;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#21457;&#29616;&#20154;&#21487;&#35299;&#37322;&#30340;&#25551;&#36848;&#31526;&#65292;&#24182;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#35299;&#37322;&#31070;&#32463;&#20803;&#65292;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#20351;&#20854;&#33021;&#22815;&#25429;&#25417;&#21040;&#21508;&#31181;&#19990;&#30028;&#30693;&#35782;&#24182;&#36866;&#24212;&#20855;&#26377;&#26377;&#38480;&#36164;&#28304;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29702;&#35299;&#21738;&#20123;&#20449;&#24687;&#23578;&#19981;&#28165;&#26970;&#65292;&#32780;&#22312;&#35782;&#21035;&#23427;&#20204;&#26041;&#38754;&#30340;&#31070;&#32463;&#20803;&#32423;&#36129;&#29486;&#22522;&#26412;&#19978;&#26159;&#26410;&#30693;&#30340;&#12290;&#31070;&#32463;&#20803;&#35299;&#37322;&#30340;&#20256;&#32479;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#20110;&#26377;&#38480;&#30340;&#39044;&#23450;&#20041;&#25551;&#36848;&#31526;&#65292;&#35201;&#20040;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#20197;&#35757;&#32451;&#19968;&#20010;&#33021;&#22815;&#35299;&#37322;&#20027;&#27169;&#22411;&#31070;&#32463;&#20803;&#30340;&#27425;&#35201;&#27169;&#22411;&#12290;&#26412;&#25991;&#20197;BERT&#20026;&#20363;&#65292;&#23581;&#35797;&#25670;&#33073;&#36825;&#20123;&#38480;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#32780;&#21487;&#20280;&#32553;&#30340;&#26694;&#26550;&#65292;&#23558;&#25991;&#26412;&#25551;&#36848;&#19982;&#31070;&#32463;&#20803;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#21033;&#29992;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#21147;&#65292;&#22312;&#25968;&#25454;&#38598;&#20013;&#21457;&#29616;&#20154;&#21487;&#35299;&#37322;&#30340;&#25551;&#36848;&#31526;&#65292;&#24182;&#20351;&#29992;&#26080;&#30417;&#30563;&#26041;&#27861;&#35299;&#37322;&#24102;&#26377;&#36825;&#20123;&#25551;&#36848;&#31526;&#30340;&#31070;&#32463;&#20803;&#12290;&#36890;&#36807;&#21508;&#31181;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in transformer-based language models have allowed them to capture a wide variety of world knowledge that can be adapted to downstream tasks with limited resources. However, what pieces of information are understood in these models is unclear, and neuron-level contributions in identifying them are largely unknown. Conventional approaches in neuron explainability either depend on a finite set of pre-defined descriptors or require manual annotations for training a secondary model that can then explain the neurons of the primary model. In this paper, we take BERT as an example and we try to remove these constraints and propose a novel and scalable framework that ties textual descriptions to neurons. We leverage the potential of generative language models to discover human-interpretable descriptors present in a dataset and use an unsupervised approach to explain neurons with these descriptors. Through various qualitative and quantitative analyses, we demonstrate the effe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65288;MBMM&#65289;&#30340;&#26032;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#36719;&#32858;&#31867;&#12290;MBMM&#36890;&#36807;&#20854;&#28789;&#27963;&#30340;&#22810;&#20803;&#36125;&#22612;&#20998;&#24067;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#36866;&#24212;&#19981;&#21516;&#30340;&#32858;&#31867;&#24418;&#29366;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16708</link><description>&lt;p&gt;
&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65306;&#20855;&#26377;&#28789;&#27963;&#32858;&#31867;&#24418;&#29366;&#30340;&#27010;&#29575;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multivariate Beta Mixture Model: Probabilistic Clustering With Flexible Cluster Shapes. (arXiv:2401.16708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65288;MBMM&#65289;&#30340;&#26032;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#29992;&#20110;&#36719;&#32858;&#31867;&#12290;MBMM&#36890;&#36807;&#20854;&#28789;&#27963;&#30340;&#22810;&#20803;&#36125;&#22612;&#20998;&#24067;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#36866;&#24212;&#19981;&#21516;&#30340;&#32858;&#31867;&#24418;&#29366;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#20854;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22810;&#20803;&#36125;&#22612;&#28151;&#21512;&#27169;&#22411;&#65288;MBMM&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#27010;&#29575;&#27169;&#22411;&#29992;&#20110;&#36719;&#32858;&#31867;&#12290;MBMM&#36890;&#36807;&#22810;&#20803;&#36125;&#22612;&#20998;&#24067;&#30340;&#28789;&#27963;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#36866;&#24212;&#19981;&#21516;&#30340;&#32858;&#31867;&#24418;&#29366;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MBMM&#30340;&#23646;&#24615;&#65292;&#25551;&#36848;&#20102;&#21442;&#25968;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36866;&#21512;&#21508;&#31181;&#32858;&#31867;&#24418;&#29366;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;&#20195;&#30721;&#21311;&#21517;&#21457;&#24067;&#22312;\url{https://github.com/hhchen1105/mbmm/}&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the multivariate beta mixture model (MBMM), a new probabilistic model for soft clustering. MBMM adapts to diverse cluster shapes because of the flexible probability density function of the multivariate beta distribution. We introduce the properties of MBMM, describe the parameter learning procedure, and present the experimental results, showing that MBMM fits diverse cluster shapes on synthetic and real datasets. The code is released anonymously at \url{https://github.com/hhchen1105/mbmm/}.
&lt;/p&gt;</description></item><item><title>AutoIE&#26159;&#19968;&#20010;&#33258;&#21160;&#25552;&#21462;&#31185;&#23398;&#25991;&#29486;&#20449;&#24687;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#22810;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#21253;&#25324;PDF&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#12289;&#31185;&#23398;&#25991;&#26412;&#21151;&#33021;&#22359;&#35782;&#21035;&#12289;&#20998;&#23376;&#31579;&#21512;&#25104;&#20449;&#24687;&#25552;&#21462;&#21644;&#22312;&#32447;&#23398;&#20064;&#31561;&#65292;&#20855;&#26377;&#39640;&#25928;&#25552;&#21462;&#20851;&#38190;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#24212;&#29992;&#20110;&#30707;&#21270;&#39046;&#22495;&#30340;&#23454;&#36341;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16672</link><description>&lt;p&gt;
AutoIE&#65306;&#19968;&#31181;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#33258;&#21160;&#25552;&#21462;&#20449;&#24687;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AutoIE: An Automated Framework for Information Extraction from Scientific Literature. (arXiv:2401.16672v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16672
&lt;/p&gt;
&lt;p&gt;
AutoIE&#26159;&#19968;&#20010;&#33258;&#21160;&#25552;&#21462;&#31185;&#23398;&#25991;&#29486;&#20449;&#24687;&#30340;&#21019;&#26032;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#22810;&#20010;&#20851;&#38190;&#32452;&#20214;&#65292;&#21253;&#25324;PDF&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#12289;&#31185;&#23398;&#25991;&#26412;&#21151;&#33021;&#22359;&#35782;&#21035;&#12289;&#20998;&#23376;&#31579;&#21512;&#25104;&#20449;&#24687;&#25552;&#21462;&#21644;&#22312;&#32447;&#23398;&#20064;&#31561;&#65292;&#20855;&#26377;&#39640;&#25928;&#25552;&#21462;&#20851;&#38190;&#25968;&#25454;&#30340;&#33021;&#21147;&#12290;&#24212;&#29992;&#20110;&#30707;&#21270;&#39046;&#22495;&#30340;&#23454;&#36341;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#20013;&#65292;&#39640;&#25928;&#22320;&#20174;&#22823;&#37327;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#20173;&#28982;&#26159;&#19968;&#20010;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21019;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#33258;&#21160;&#25552;&#21462;&#31185;&#23398;PDF&#25991;&#26723;&#20013;&#30340;&#37325;&#35201;&#25968;&#25454;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#26356;&#23481;&#26131;&#36776;&#21035;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;AutoIE&#29420;&#29305;&#22320;&#38598;&#25104;&#20102;&#22235;&#20010;&#21019;&#26032;&#32452;&#20214;&#65306;&#65288;1&#65289;&#22522;&#20110;&#22810;&#35821;&#20041;&#29305;&#24449;&#34701;&#21512;&#30340;PDF&#25991;&#26723;&#24067;&#23616;&#20998;&#26512;&#26041;&#27861;&#65307;&#65288;2&#65289;&#31185;&#23398;&#25991;&#26412;&#20013;&#30340;&#39640;&#32423;&#21151;&#33021;&#22359;&#35782;&#21035;&#65307;&#65288;3&#65289;&#19968;&#31181;&#38024;&#23545;&#20998;&#23376;&#31579;&#21512;&#25104;&#30340;&#20449;&#24687;&#25552;&#21462;&#21644;&#30456;&#20851;&#24615;&#30340;&#21327;&#21516;&#25216;&#26415;&#65307;&#65288;4&#65289;&#38024;&#23545;&#20998;&#23376;&#31579;&#25991;&#29486;&#37327;&#36523;&#23450;&#21046;&#30340;&#22312;&#32447;&#23398;&#20064;&#33539;&#24335;&#12290;&#25105;&#20204;&#30340;SBERT&#27169;&#22411;&#22312;CoNLL04&#21644;ADE&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39640;&#36798;87.19&#21644;89.65&#30340;Marco F1&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#23558;AutoIE&#24212;&#29992;&#20110;&#30707;&#21270;&#39046;&#22495;&#30340;&#20998;&#23376;&#31579;&#21512;&#25104;&#23454;&#36341;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#65292;&#36890;&#36807;&#24778;&#20154;&#30340;78%&#30340;...
&lt;/p&gt;
&lt;p&gt;
In the rapidly evolving field of scientific research, efficiently extracting key information from the burgeoning volume of scientific papers remains a formidable challenge. This paper introduces an innovative framework designed to automate the extraction of vital data from scientific PDF documents, enabling researchers to discern future research trajectories more readily. AutoIE uniquely integrates four novel components: (1) A multi-semantic feature fusion-based approach for PDF document layout analysis; (2) Advanced functional block recognition in scientific texts; (3) A synergistic technique for extracting and correlating information on molecular sieve synthesis; (4) An online learning paradigm tailored for molecular sieve literature. Our SBERT model achieves high Marco F1 scores of 87.19 and 89.65 on CoNLL04 and ADE datasets. In addition, a practical application of AutoIE in the petrochemical molecular sieve synthesis domain demonstrates its efficacy, evidenced by an impressive 78\%
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#27668;&#39044;&#25253;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#31361;&#30772;&#65292;&#23427;&#20811;&#26381;&#20102;&#20256;&#32479;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#26377;&#28508;&#21147;&#24341;&#39046;&#22825;&#27668;&#39044;&#25253;&#30340;&#31532;&#20108;&#27425;&#38761;&#21629;&#12290;</title><link>http://arxiv.org/abs/2401.16669</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#20026;&#22825;&#27668;&#39044;&#25253;&#24102;&#26469;&#20102;&#31532;&#20108;&#27425;&#38761;&#21629;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Artificial Intelligence Providing the Second Revolution for Weather Forecasting?. (arXiv:2401.16669v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16669
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#22312;&#22825;&#27668;&#39044;&#25253;&#39046;&#22495;&#30340;&#24555;&#36895;&#21457;&#23637;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#31361;&#30772;&#65292;&#23427;&#20811;&#26381;&#20102;&#20256;&#32479;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#26377;&#28508;&#21147;&#24341;&#39046;&#22825;&#27668;&#39044;&#25253;&#30340;&#31532;&#20108;&#27425;&#38761;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#29305;&#21035;&#26159;&#36817;&#24180;&#26469;&#65292;&#23548;&#33268;&#20102;&#20960;&#31181;&#22823;&#21442;&#25968;&#20154;&#24037;&#26234;&#33021;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#27169;&#22411;&#20195;&#34920;&#20102;&#19968;&#20010;&#37325;&#22823;&#31361;&#30772;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#34920;&#26126;&#20102;&#22825;&#27668;&#39044;&#25253;&#21487;&#33021;&#36814;&#26469;&#31532;&#20108;&#27425;&#38761;&#21629;&#30340;&#28508;&#21147;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36825;&#20123;&#20808;&#36827;&#20154;&#24037;&#26234;&#33021;&#39044;&#25253;&#27169;&#22411;&#30340;&#28436;&#21464;&#65292;&#24182;&#22312;&#30830;&#23450;&#30340;&#20849;&#21516;&#28857;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#23427;&#20204;&#30340;&#21457;&#23637;&#30340;&#8220;&#19977;&#22823;&#35268;&#21017;&#8221;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#38761;&#21629;&#25968;&#20540;&#22825;&#27668;&#39044;&#25253;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#31616;&#35201;&#27010;&#36848;&#20102;&#28508;&#22312;&#30340;&#21407;&#22240;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#26410;&#26469;&#21457;&#23637;&#21069;&#26223;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#23558;&#25972;&#20010;&#25968;&#20540;&#39044;&#25253;&#36807;&#31243;&#36827;&#34892;&#25972;&#21512;&#12290;&#36890;&#36807;&#23558;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#19982;&#20854;&#20182;&#20449;&#24687;&#32508;&#21512;&#65292;&#32473;&#20986;&#20102;&#19968;&#20010;&#24212;&#29992;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancement of artificial intelligence technologies, particularly in recent years, has led to the emergence of several large parameter artificial intelligence weather forecast models. These models represent a significant breakthrough, overcoming the limitations of traditional numerical weather prediction models and indicating a potential second revolution for weather forecast. This study explores the evolution of these advanced artificial intelligence forecast models, and based on the identified commonalities, proposes the "Three Large Rules" for their development. We discuss the potential of artificial intelligence in revolutionizing numerical weather prediction, briefly outlining the underlying reasons for this potential. Additionally, we explore key areas for future development prospects for large artificial intelligence weather forecast models, integrating the entire numerical prediction process. Through an example that combines a large artificial intelligence model with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24674;&#22797;&#24515;&#26234;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21457;&#29616;&#20351;&#29992;&#22522;&#20110;MCMC&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#20855;&#26377;&#28508;&#22312;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.16657</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#36890;&#36807;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#24674;&#22797;&#24515;&#26234;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Recovering Mental Representations from Large Language Models with Markov Chain Monte Carlo. (arXiv:2401.16657v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16657
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#26041;&#27861;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#24674;&#22797;&#24515;&#26234;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#21457;&#29616;&#20351;&#29992;&#22522;&#20110;MCMC&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#65292;&#36825;&#23545;&#20110;&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#20855;&#26377;&#28508;&#22312;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27169;&#25311;&#37319;&#26679;&#31639;&#27861;&#36827;&#34892;&#20154;&#31867;&#30740;&#31350;&#24050;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#25506;&#32034;&#21644;&#29702;&#35299;&#20854;&#24515;&#26234;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#30456;&#21516;&#30340;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#20154;&#31867;&#25110;LLMs&#37117;&#21487;&#20197;&#36890;&#36807;&#20869;&#30465;&#30340;&#26041;&#24335;&#30452;&#25509;&#25581;&#31034;&#20854;&#24515;&#26234;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#20351;&#29992;LLMs&#20316;&#20026;&#19968;&#31181;&#37319;&#26679;&#31639;&#27861;&#30340;&#20803;&#32032;&#21487;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#20351;&#29992;&#30452;&#25509;&#37319;&#26679;&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#33945;&#29305;&#21345;&#27931;&#65288;MCMC&#65289;&#36827;&#34892;LLMs&#25554;&#38382;&#26102;&#24674;&#22797;&#20154;&#31867;&#21270;&#34920;&#31034;&#31243;&#24230;&#30340;&#31243;&#24230;&#12290;&#25105;&#20204;&#21457;&#29616;&#20351;&#29992;&#22522;&#20110;MCMC&#30340;&#33258;&#36866;&#24212;&#37319;&#26679;&#31639;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#28508;&#21147;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#31181;&#26356;&#36890;&#29992;&#30340;&#20351;&#29992;LLMs&#36827;&#34892;&#36125;&#21494;&#26031;&#25512;&#29702;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulating sampling algorithms with people has proven a useful method for efficiently probing and understanding their mental representations. We propose that the same methods can be used to study the representations of Large Language Models (LLMs). While one can always directly prompt either humans or LLMs to disclose their mental representations introspectively, we show that increased efficiency can be achieved by using LLMs as elements of a sampling algorithm. We explore the extent to which we recover human-like representations when LLMs are interrogated with Direct Sampling and Markov chain Monte Carlo (MCMC). We found a significant increase in efficiency and performance using adaptive sampling algorithms based on MCMC. We also highlight the potential of our method to yield a more general method of conducting Bayesian inference \textit{with} LLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#24212;&#29992;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#26377;&#25928;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2401.16650</link><description>&lt;p&gt;
&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#22238;&#25918;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Augmenting Replay in World Models for Continual Reinforcement Learning. (arXiv:2401.16650v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16650
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#24212;&#29992;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#22686;&#24378;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20869;&#23384;&#38480;&#21046;&#38382;&#39064;&#65292;&#24182;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#26377;&#25928;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36830;&#32493;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#29615;&#22659;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#25104;&#21151;&#30340;&#31995;&#32479;&#24212;&#35813;&#36866;&#24403;&#24179;&#34913;&#20445;&#25345;&#24050;&#23398;&#20064;&#20219;&#21153;&#19978;&#30340;&#20195;&#29702;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#21487;&#22609;&#24615;&#20043;&#38388;&#30340;&#30683;&#30462;&#35201;&#27714;&#12290;&#39318;&#36827;&#20808;&#20986;&#32531;&#20914;&#21306;&#36890;&#24120;&#29992;&#20110;&#22686;&#24378;&#27492;&#31867;&#35774;&#32622;&#20013;&#30340;&#23398;&#20064;&#65292;&#20294;&#38656;&#35201;&#22823;&#37327;&#20869;&#23384;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#22686;&#24378;&#26041;&#27861;&#24212;&#29992;&#20110;&#27492;&#32531;&#20914;&#21306;&#20013;&#65292;&#20197;&#32531;&#35299;&#20869;&#23384;&#38480;&#21046;&#65292;&#24182;&#19982;&#22522;&#20110;&#19990;&#30028;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#35780;&#20272;&#20854;&#22312;&#20419;&#36827;&#36830;&#32493;&#23398;&#20064;&#26041;&#38754;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#22312;Procgen&#21644;Atari&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#28508;&#22312;&#19990;&#30028;&#27169;&#22411;&#30340;&#32972;&#26223;&#19979;&#65292;&#22238;&#25918;&#32531;&#20914;&#21306;&#20013;&#30340;&#20998;&#24067;&#21305;&#37197;&#22686;&#24378;&#21487;&#20197;&#25104;&#21151;&#38450;&#27490;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#24320;&#38144;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#20063;&#21457;&#29616;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#24182;&#38750;&#23436;&#20840;&#26080;&#25032;&#21487;&#20987;&#65292;
&lt;/p&gt;
&lt;p&gt;
In continual RL, the environment of a reinforcement learning (RL) agent undergoes change. A successful system should appropriately balance the conflicting requirements of retaining agent performance on already learned tasks, stability, whilst learning new tasks, plasticity. The first-in-first-out buffer is commonly used to enhance learning in such settings but requires significant memory. We explore the application of an augmentation to this buffer which alleviates the memory constraints, and use it with a world model model-based reinforcement learning algorithm, to evaluate its effectiveness in facilitating continual learning. We evaluate the effectiveness of our method in Procgen and Atari RL benchmarks and show that the distribution matching augmentation to the replay-buffer used in the context of latent world models can successfully prevent catastrophic forgetting with significantly reduced computational overhead. Yet, we also find such a solution to not be entirely infallible, and
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#27010;&#29575;&#21028;&#26029;&#32463;&#24120;&#26159;&#19981;&#36830;&#36143;&#30340;&#65292;&#26174;&#31034;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#19968;&#26679;&#30340;&#38750;&#29702;&#24615;&#20559;&#24046;&#12290;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#23558;&#33258;&#22238;&#24402;LLMs&#19982;&#38544;&#24615;&#36125;&#21494;&#26031;&#25512;&#26029;&#32852;&#31995;&#36215;&#26469;&#30340;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2401.16646</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19981;&#36830;&#36143;&#27010;&#29575;&#21028;&#26029;
&lt;/p&gt;
&lt;p&gt;
Incoherent Probability Judgments in Large Language Models. (arXiv:2401.16646v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16646
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#35770;&#25991;&#20013;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#27010;&#29575;&#21028;&#26029;&#32463;&#24120;&#26159;&#19981;&#36830;&#36143;&#30340;&#65292;&#26174;&#31034;&#20986;&#31867;&#20284;&#20110;&#20154;&#31867;&#19968;&#26679;&#30340;&#38750;&#29702;&#24615;&#20559;&#24046;&#12290;&#20182;&#20204;&#36824;&#25552;&#20986;&#20102;&#23558;&#33258;&#22238;&#24402;LLMs&#19982;&#38544;&#24615;&#36125;&#21494;&#26031;&#25512;&#26029;&#32852;&#31995;&#36215;&#26469;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#19979;&#19968;&#20010;&#35789;&#39044;&#27979;&#35757;&#32451;&#30340;&#33258;&#22238;&#24402;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20986;&#20986;&#33394;&#30340;&#36830;&#36143;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12290;&#20294;&#23427;&#20204;&#26159;&#21542;&#21516;&#26679;&#25797;&#38271;&#24418;&#25104;&#36830;&#36143;&#30340;&#27010;&#29575;&#21028;&#26029;&#65311;&#25105;&#20204;&#20351;&#29992;&#27010;&#29575;&#36523;&#20221;&#21644;&#37325;&#22797;&#21028;&#26029;&#26469;&#35780;&#20272;LLMs&#29983;&#25104;&#30340;&#27010;&#29575;&#21028;&#26029;&#30340;&#36830;&#36143;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#21028;&#26029;&#32463;&#24120;&#26159;&#19981;&#36830;&#36143;&#30340;&#65292;&#26174;&#31034;&#20986;&#20154;&#31867;&#19968;&#26679;&#30340;&#27010;&#29575;&#29702;&#35770;&#35268;&#21017;&#20559;&#31163;&#12290;&#27492;&#22806;&#65292;&#24403;&#35201;&#27714;&#23545;&#21516;&#19968;&#20107;&#20214;&#36827;&#34892;&#21028;&#26029;&#26102;&#65292;LLMs&#20135;&#29983;&#30340;&#27010;&#29575;&#21028;&#26029;&#30340;&#22343;&#20540;-&#26041;&#24046;&#20851;&#31995;&#21576;&#29616;&#20986;&#20154;&#31867;&#25152;&#35265;&#21040;&#30340;&#20498;U&#24418;&#29366;&#12290;&#25105;&#20204;&#25552;&#20986;&#36825;&#20123;&#38750;&#29702;&#24615;&#30340;&#20559;&#31163;&#21487;&#20197;&#36890;&#36807;&#23558;&#33258;&#22238;&#24402;LLMs&#19982;&#38544;&#24615;&#36125;&#21494;&#26031;&#25512;&#26029;&#32852;&#31995;&#36215;&#26469;&#65292;&#24182;&#19982;&#20154;&#31867;&#27010;&#29575;&#21028;&#26029;&#30340;&#36125;&#21494;&#26031;&#25277;&#26679;&#22120;&#27169;&#22411;&#36827;&#34892;&#31867;&#27604;&#26469;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Large Language Models (LLMs) trained for next-word prediction have demonstrated remarkable proficiency at producing coherent text. But are they equally adept at forming coherent probability judgments? We use probabilistic identities and repeated judgments to assess the coherence of probability judgments made by LLMs. Our results show that the judgments produced by these models are often incoherent, displaying human-like systematic deviations from the rules of probability theory. Moreover, when prompted to judge the same event, the mean-variance relationship of probability judgments produced by LLMs shows an inverted-U-shaped like that seen in humans. We propose that these deviations from rationality can be explained by linking autoregressive LLMs to implicit Bayesian inference and drawing parallels with the Bayesian Sampler model of human probability judgments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#24402;&#22240;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16638</link><description>&lt;p&gt;
&#25171;&#30772;Transformer&#27169;&#22411;&#30340;&#26463;&#32538;&#65306;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#24402;&#22240;&#25552;&#20379;&#20102;&#22312;&#19981;&#24494;&#35843;&#39044;&#35757;&#32451;LLMs&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#30340;&#25215;&#35834;
&lt;/p&gt;
&lt;p&gt;
Breaking Free Transformer Models: Task-specific Context Attribution Promises Improved Generalizability Without Fine-tuning Pre-trained LLMs. (arXiv:2401.16638v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16638
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26694;&#26550;&#65292;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#24402;&#22240;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20174;&#32780;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24494;&#35843;&#26159;&#19968;&#31181;&#24120;&#29992;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#24120;&#20250;&#23548;&#33268;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#19979;&#38477;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#19978;&#19979;&#25991;&#24402;&#22240;&#65292;&#23454;&#29616;&#20102;&#27867;&#21270;&#24615;&#33021;&#30340;&#20445;&#25345;&#65292;&#24182;&#25552;&#21319;&#20102;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20219;&#20309;Transformer&#27169;&#22411;&#30340;&#25991;&#26412;&#34920;&#31034;&#30340;&#32447;&#24615;&#21464;&#25442;&#65292;&#20351;&#29992;&#20219;&#21153;&#29305;&#23450;&#30340;&#27010;&#24565;&#36816;&#31639;&#31526;&#65292;&#20250;&#24471;&#21040;&#19968;&#20010;&#25237;&#24433;&#21040;&#28508;&#22312;&#27010;&#24565;&#31354;&#38388;&#19978;&#30340;&#32467;&#26524;&#65292;&#26412;&#25991;&#20013;&#31216;&#20043;&#20026;&#19978;&#19979;&#25991;&#24402;&#22240;&#12290;&#29305;&#23450;&#30340;&#27010;&#24565;&#36816;&#31639;&#31526;&#22312;&#30417;&#30563;&#23398;&#20064;&#38454;&#27573;&#36890;&#36807;&#26032;&#39062;&#30340;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#34920;&#26126;&#65292;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#30446;&#26631;&#30340;&#25991;&#26412;&#34920;&#31034;&#36827;&#34892;&#19978;&#19979;&#25991;&#24402;&#22240;&#21487;&#20197;&#25913;&#21892;&#37492;&#21035;&#22120;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#23454;&#29616;&#26356;&#22909;&#30340;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning large pre-trained language models (LLMs) on particular datasets is a commonly employed strategy in Natural Language Processing (NLP) classification tasks. However, this approach usually results in a loss of models generalizability. In this paper, we present a framework that allows for maintaining generalizability, and enhances the performance on the downstream task by utilizing task-specific context attribution. We show that a linear transformation of the text representation from any transformer model using the task-specific concept operator results in a projection onto the latent concept space, referred to as context attribution in this paper. The specific concept operator is optimized during the supervised learning stage via novel loss functions. The proposed framework demonstrates that context attribution of the text representation for each task objective can improve the capacity of the discriminator function and thus achieve better performance for the classification tas
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26469;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#32780;&#23548;&#33268;RLHF&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16635</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Reinforcement Learning from Human Feedback with Efficient Reward Model Ensemble. (arXiv:2401.16635v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#36890;&#36807;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#26469;&#25913;&#36827;&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#30001;&#20110;&#22870;&#21169;&#27169;&#22411;&#39044;&#27979;&#19981;&#20934;&#30830;&#32780;&#23548;&#33268;RLHF&#36755;&#20986;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;RLHF&#20381;&#36182;&#20110;&#36890;&#36807;&#26377;&#38480;&#30340;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#19981;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;RLHF&#21487;&#33021;&#20135;&#29983;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19981;&#19968;&#33268;&#30340;&#36755;&#20986;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22870;&#21169;&#38598;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#22870;&#21169;&#27169;&#22411;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#32771;&#34385;&#21040;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22870;&#21169;&#27169;&#22411;&#38598;&#25104;&#21487;&#33021;&#20855;&#26377;&#35745;&#31639;&#21644;&#36164;&#28304;&#26114;&#36149;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21253;&#25324;&#32447;&#24615;&#23618;&#38598;&#25104;&#21644;&#22522;&#20110;LoRA&#30340;&#38598;&#25104;&#22312;&#20869;&#30340;&#39640;&#25928;&#38598;&#25104;&#26041;&#27861;&#12290;&#23454;&#35777;&#19978;&#65292;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#38598;&#25104;&#22870;&#21169;&#27169;&#22411;&#36816;&#34892;Best-of-$n$&#21644;Proximal Policy Optimization&#65292;&#24182;&#39564;&#35777;&#25105;&#20204;&#30340;&#38598;&#25104;&#26041;&#27861;&#26377;&#21161;&#20110;&#25913;&#21892;RLHF&#36755;&#20986;&#30340;&#23545;&#40784;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Human Feedback (RLHF) is a widely adopted approach for aligning large language models with human values. However, RLHF relies on a reward model that is trained with a limited amount of human preference data, which could lead to inaccurate predictions. As a result, RLHF may produce outputs that are misaligned with human values. To mitigate this issue, we contribute a reward ensemble method that allows the reward model to make more accurate predictions. As using an ensemble of large language model-based reward models can be computationally and resource-expensive, we explore efficient ensemble methods including linear-layer ensemble and LoRA-based ensemble. Empirically, we run Best-of-$n$ and Proximal Policy Optimization with our ensembled reward models, and verify that our ensemble methods help improve the alignment performance of RLHF outputs.
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#20445;&#35777;&#65292;&#24320;&#21457;&#20196;&#20154;&#20449;&#26381;&#30340;&#20445;&#35777;&#26696;&#20363;&#26159;&#20851;&#38190;&#65292;&#21253;&#25324;&#26816;&#27979;&#32570;&#38519;&#12289;&#25913;&#36827;&#32467;&#26500;&#21644;&#33258;&#21160;&#29983;&#25104;&#26696;&#20363;&#12290;</title><link>http://arxiv.org/abs/2401.16633</link><description>&lt;p&gt;
&#25105;&#26469;&#20102;&#65292;&#25105;&#30475;&#21040;&#20102;&#65292;&#25105;&#35748;&#35777;&#20102;&#65306;&#23545;&#20110;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#23433;&#20840;&#20445;&#35777;&#30340;&#20960;&#20010;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
I came, I saw, I certified: some perspectives on the safety assurance of cyber-physical systems. (arXiv:2401.16633v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16633
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#20445;&#35777;&#65292;&#24320;&#21457;&#20196;&#20154;&#20449;&#26381;&#30340;&#20445;&#35777;&#26696;&#20363;&#26159;&#20851;&#38190;&#65292;&#21253;&#25324;&#26816;&#27979;&#32570;&#38519;&#12289;&#25913;&#36827;&#32467;&#26500;&#21644;&#33258;&#21160;&#29983;&#25104;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#12289;&#26080;&#20154;&#26426;&#31995;&#32479;&#21644;&#26426;&#22120;&#20154;&#31995;&#32479;&#65289;&#30340;&#25191;&#34892;&#22833;&#36133;&#21487;&#33021;&#23548;&#33268;&#29983;&#21629;&#20007;&#22833;&#12289;&#20005;&#37325;&#20260;&#23475;&#12289;&#22823;&#35268;&#27169;&#29615;&#22659;&#30772;&#22351;&#12289;&#36130;&#20135;&#25439;&#27585;&#21644;&#37325;&#22823;&#32463;&#27982;&#25439;&#22833;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#31995;&#32479;&#36890;&#24120;&#38656;&#35201;&#24378;&#26377;&#21147;&#30340;&#29702;&#30001;&#26469;&#25903;&#25345;&#23427;&#20204;&#33021;&#22815;&#26377;&#25928;&#22320;&#25903;&#25745;&#20854;&#35774;&#35745;&#30340;&#20851;&#38190;&#35201;&#27714;&#65288;&#20363;&#22914;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#21487;&#29992;&#24615;&#65289;&#12290;&#22240;&#27492;&#65292;&#36890;&#24120;&#38656;&#35201;&#24320;&#21457;&#20196;&#20154;&#20449;&#26381;&#30340;&#20445;&#35777;&#26696;&#20363;&#26469;&#25903;&#25345;&#36825;&#31181;&#29702;&#30001;&#65292;&#24182;&#20801;&#35768;&#30417;&#31649;&#26426;&#26500;&#23545;&#27492;&#31867;&#31995;&#32479;&#36827;&#34892;&#35748;&#35777;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#20445;&#35777;&#26696;&#20363;&#20013;&#30340;&#32570;&#38519;&#65292;&#20381;&#36182;&#27169;&#24335;&#26469;&#25913;&#36827;&#20445;&#35777;&#26696;&#20363;&#30340;&#32467;&#26500;&#65292;&#25913;&#36827;&#29616;&#26377;&#30340;&#20445;&#35777;&#26696;&#20363;&#31526;&#21495;&#65292;&#24182;&#65288;&#21322;&#65289;&#33258;&#21160;&#21270;&#29983;&#25104;&#20445;&#35777;&#26696;&#20363;&#26159;&#21457;&#23637;&#20196;&#20154;&#20449;&#26381;&#30340;&#20445;&#35777;&#26696;&#20363;&#24182;&#20419;&#36827;&#28040;&#36153;&#32773;&#25509;&#21463;&#30340;&#20851;&#38190;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19982;&#36825;&#20123;&#20445;&#35777;&#21551;&#29992;&#22120;&#30456;&#20851;&#30340;&#25361;&#25112;&#65292;&#24182;&#27010;&#36848;&#20102;&#19968;&#20123;&#28508;&#22312;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The execution failure of cyber-physical systems (e.g., autonomous driving systems, unmanned aerial systems, and robotic systems) could result in the loss of life, severe injuries, large-scale environmental damage, property destruction, and major economic loss. Hence, such systems usually require a strong justification that they will effectively support critical requirements (e.g., safety, security, and reliability) for which they were designed. Thus, it is often mandatory to develop compelling assurance cases to support that justification and allow regulatory bodies to certify such systems. In such contexts, detecting assurance deficits, relying on patterns to improve the structure of assurance cases, improving existing assurance case notations, and (semi-)automating the generation of assurance cases are key to develop compelling assurance cases and foster consumer acceptance. We therefore explore challenges related to such assurance enablers and outline some potential directions that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;RL&#21644;PID&#25511;&#21046;&#22120;&#30340;&#20845;&#33258;&#30001;&#24230;&#28216;&#27891;&#26426;&#22120;&#20154;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#27700;&#19979;&#30446;&#26631;&#36319;&#36394;&#12290;&#20351;&#29992;&#20013;&#22830;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#25511;&#21046;&#22120;&#20195;&#26367;PID&#25511;&#21046;&#22120;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#31163;&#31574;&#30053;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#36739;&#26131;&#23454;&#29616;&#12290;&#22312;&#27809;&#26377;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;RL&#20195;&#29702;&#26469;&#25511;&#21046;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#31995;&#32479;&#65292;&#20013;&#24515;&#25511;&#21046;&#22120;&#21487;&#33021;&#27604;&#29420;&#31435;&#30340;PID&#25511;&#21046;&#22120;&#25552;&#20379;&#26356;&#24378;&#20581;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2401.16618</link><description>&lt;p&gt;
&#22522;&#20110;RL&#21644;PID&#25511;&#21046;&#22120;&#30340;&#20845;&#33258;&#30001;&#24230;&#28216;&#27891;&#26426;&#22120;&#20154;&#30340;&#27604;&#36739;&#65306;&#28151;&#21512;&#27700;&#19979;&#30446;&#26631;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
A comparison of RL-based and PID controllers for 6-DOF swimming robots: hybrid underwater object tracking. (arXiv:2401.16618v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16618
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#22522;&#20110;RL&#21644;PID&#25511;&#21046;&#22120;&#30340;&#20845;&#33258;&#30001;&#24230;&#28216;&#27891;&#26426;&#22120;&#20154;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#27700;&#19979;&#30446;&#26631;&#36319;&#36394;&#12290;&#20351;&#29992;&#20013;&#22830;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#25511;&#21046;&#22120;&#20195;&#26367;PID&#25511;&#21046;&#22120;&#22312;&#25968;&#25454;&#25928;&#29575;&#21644;&#31163;&#31574;&#30053;&#23398;&#20064;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#19988;&#36739;&#26131;&#23454;&#29616;&#12290;&#22312;&#27809;&#26377;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;RL&#20195;&#29702;&#26469;&#25511;&#21046;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#30340;&#31995;&#32479;&#65292;&#20013;&#24515;&#25511;&#21046;&#22120;&#21487;&#33021;&#27604;&#29420;&#31435;&#30340;PID&#25511;&#21046;&#22120;&#25552;&#20379;&#26356;&#24378;&#20581;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22312;&#20845;&#33258;&#30001;&#24230;&#28216;&#27891;&#26426;&#22120;&#20154;&#30340;&#32972;&#26223;&#19979;&#65292;&#25506;&#35752;&#21644;&#35780;&#20272;&#20102;&#20351;&#29992;&#20013;&#22830;&#28145;&#24230;Q&#32593;&#32476;&#65288;DQN&#65289;&#25511;&#21046;&#22120;&#26469;&#20195;&#26367;&#24120;&#29992;&#30340;PID&#25511;&#21046;&#22120;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20027;&#35201;&#23558;&#37325;&#28857;&#25918;&#22312;&#27700;&#19979;&#30446;&#26631;&#36319;&#36394;&#36825;&#19968;&#20855;&#20307;&#26696;&#20363;&#19978;&#65292;DQN&#20855;&#26377;&#25968;&#25454;&#25928;&#29575;&#21644;&#31163;&#31574;&#30053;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#21516;&#26102;&#27604;&#20854;&#20182;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26356;&#23481;&#26131;&#23454;&#29616;&#12290;&#32771;&#34385;&#21040;&#25105;&#20204;&#30340;&#26426;&#22120;&#20154;&#27809;&#26377;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;RL&#20195;&#29702;&#26469;&#25511;&#21046;&#36825;&#20010;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#31995;&#32479;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#20013;&#24515;&#25511;&#21046;&#22120;&#21487;&#33021;&#27604;&#29420;&#31435;&#30340;PID&#25511;&#21046;&#22120;&#25552;&#20379;&#26356;&#24378;&#20581;&#30340;&#25511;&#21046;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#32463;&#20856;&#25511;&#21046;&#22120;&#36827;&#34892;&#23433;&#20840;&#25506;&#32034;&#65292;&#28982;&#21518;&#36880;&#28176;&#36716;&#21521;DQN&#26469;&#23436;&#20840;&#25484;&#25511;&#26426;&#22120;&#20154;&#12290;&#25105;&#20204;&#23558;&#27700;&#19979;&#36319;&#36394;&#20219;&#21153;&#20998;&#20026;&#35270;&#35273;&#21644;&#25511;&#21046;&#27169;&#22359;&#65292;&#20351;&#29992;&#20102;&#24050;&#24314;&#31435;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#36319;&#36394;&#26041;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#20013;&#24515;DQN&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present an exploration and assessment of employing a centralized deep Q-network (DQN) controller as a substitute for the prevalent use of PID controllers in the context of 6DOF swimming robots. Our primary focus centers on illustrating this transition with the specific case of underwater object tracking. DQN offers advantages such as data efficiency and off-policy learning, while remaining simpler to implement than other reinforcement learning methods. Given the absence of a dynamic model for our robot, we propose an RL agent to control this multi-input-multi-output (MIMO) system, where a centralized controller may offer more robust control than distinct PIDs. Our approach involves initially using classical controllers for safe exploration, then gradually shifting to DQN to take full control of the robot.  We divide the underwater tracking task into vision and control modules. We use established methods for vision-based tracking and introduce a centralized DQN control
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#21457;&#29616;ChatGPT&#22312;&#31038;&#20132;&#12289;&#20998;&#26512;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#31867;&#23545;&#35805;&#26356;&#20855;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#23613;&#31649;&#22312;&#24773;&#32490;&#26041;&#38754;&#26080;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2401.16587</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;ChatGPT&#29983;&#25104;&#23545;&#35805;&#20043;&#38388;&#30340;&#35821;&#35328;&#23545;&#27604;
&lt;/p&gt;
&lt;p&gt;
A Linguistic Comparison between Human and ChatGPT-Generated Conversations. (arXiv:2401.16587v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#20154;&#31867;&#21644;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#21457;&#29616;ChatGPT&#22312;&#31038;&#20132;&#12289;&#20998;&#26512;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20154;&#31867;&#23545;&#35805;&#26356;&#20855;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#23613;&#31649;&#22312;&#24773;&#32490;&#26041;&#38754;&#26080;&#26174;&#33879;&#24046;&#24322;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#21644;LLM&#29983;&#25104;&#30340;&#23545;&#35805;&#20043;&#38388;&#30340;&#35821;&#35328;&#24046;&#24322;&#65292;&#20351;&#29992;&#20102;&#30001;ChatGPT-3.5&#29983;&#25104;&#30340;19.5K&#20010;&#23545;&#35805;&#20316;&#20026;EmpathicDialogues&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#12290;&#30740;&#31350;&#37319;&#29992;Linguistic Inquiry and Word Count (LIWC) &#20998;&#26512;&#65292;&#27604;&#36739;&#20102;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#21644;&#20154;&#31867;&#23545;&#35805;&#22312;118&#20010;&#35821;&#35328;&#31867;&#21035;&#19978;&#30340;&#24046;&#24322;&#12290;&#32467;&#26524;&#26174;&#31034;&#20154;&#31867;&#23545;&#35805;&#20855;&#26377;&#26356;&#22823;&#30340;&#21464;&#24322;&#24615;&#21644;&#30495;&#23454;&#24615;&#65292;&#20294;ChatGPT&#22312;&#31038;&#20132;&#36807;&#31243;&#12289;&#20998;&#26512;&#39118;&#26684;&#12289;&#35748;&#30693;&#12289;&#20851;&#27880;&#28966;&#28857;&#21644;&#31215;&#26497;&#24773;&#32490;&#33394;&#24425;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#36827;&#19968;&#27493;&#35777;&#26126;&#20102;LLMs&#8220;&#27604;&#30495;&#20154;&#26356;&#20687;&#30495;&#20154;&#8221;&#30340;&#26368;&#26032;&#21457;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;ChatGPT&#21644;&#20154;&#31867;&#23545;&#35805;&#20043;&#38388;&#27809;&#26377;&#25214;&#21040;&#31215;&#26497;&#25110;&#28040;&#26497;&#24773;&#32490;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;&#23545;&#35805;&#23884;&#20837;&#30340;&#20998;&#31867;&#22120;&#20998;&#26512;&#34920;&#26126;&#65292;&#23613;&#31649;&#23545;&#35805;&#20013;&#27809;&#26377;&#26126;&#30830;&#25552;&#21450;&#24773;&#32490;&#65292;&#20294;&#23545;&#24773;&#24863;&#20215;&#20540;&#30340;&#38544;&#24615;&#32534;&#30721;&#23384;&#22312;&#12290;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#12289;&#30001;&#20004;&#20010;ChatGPT&#29983;&#25104;&#30340;&#23545;&#35805;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study explores linguistic differences between human and LLM-generated dialogues, using 19.5K dialogues generated by ChatGPT-3.5 as a companion to the EmpathicDialogues dataset. The research employs Linguistic Inquiry and Word Count (LIWC) analysis, comparing ChatGPT-generated conversations with human conversations across 118 linguistic categories. Results show greater variability and authenticity in human dialogues, but ChatGPT excels in categories such as social processes, analytical style, cognition, attentional focus, and positive emotional tone, reinforcing recent findings of LLMs being "more human than human." However, no significant difference was found in positive or negative affect between ChatGPT and human dialogues. Classifier analysis of dialogue embeddings indicates implicit coding of the valence of affect despite no explicit mention of affect in the conversations. The research also contributes a novel, companion ChatGPT-generated dataset of conversations between two i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#38598;&#25104;&#31574;&#30053;&#26799;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#20462;&#25913;&#21518;&#30340;Transformer&#32467;&#26500;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#26368;&#36817;&#30740;&#31350;&#21644;&#24191;&#27867;&#37319;&#29992;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2401.16580</link><description>&lt;p&gt;
&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Attention-based Reinforcement Learning for Combinatorial Optimization: Application to Job Shop Scheduling Problem. (arXiv:2401.16580v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16580
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#12290;&#36890;&#36807;&#38598;&#25104;&#31574;&#30053;&#26799;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#20462;&#25913;&#21518;&#30340;Transformer&#32467;&#26500;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35299;&#20915;&#22823;&#35268;&#27169;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#26368;&#36817;&#30740;&#31350;&#21644;&#24191;&#27867;&#37319;&#29992;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#26159;&#19968;&#31867;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#20027;&#35201;&#36890;&#36807;&#31934;&#30830;&#25110;&#36817;&#20284;&#30340;&#35299;&#20915;&#26041;&#27861;&#26469;&#35299;&#20915;&#12290;&#28982;&#32780;&#65292;&#23547;&#25214;&#31934;&#30830;&#35299;&#23545;&#20110;&#23454;&#38469;&#38382;&#39064;&#26469;&#35828;&#26159;&#19981;&#21487;&#34892;&#30340;&#65292;&#21363;&#20351;&#37319;&#29992;&#36817;&#20284;&#35299;&#20915;&#26041;&#27861;&#65292;&#20063;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#26469;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#35299;&#65292;&#24182;&#19988;&#25214;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#19981;&#33021;&#24212;&#29992;&#20110;&#26032;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#20316;&#19994;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#23558;&#31574;&#30053;&#26799;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#20462;&#25913;&#21518;&#30340;Transformer&#32467;&#26500;&#36827;&#34892;&#20102;&#38598;&#25104;&#12290;&#19968;&#20010;&#37325;&#35201;&#30340;&#32467;&#26524;&#26159;&#65292;&#25105;&#20204;&#22312;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20013;&#35757;&#32451;&#30340;&#23398;&#20064;&#32773;&#21487;&#20197;&#29992;&#20110;&#35299;&#20915;&#26410;&#21442;&#19982;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;&#38382;&#39064;&#65292;&#24182;&#19988;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#26368;&#36817;&#30740;&#31350;&#21644;&#24191;&#27867;&#37319;&#29992;&#30340;&#21551;&#21457;&#24335;&#35268;&#21017;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Job shop scheduling problems are one of the most important and challenging combinatorial optimization problems that have been tackled mainly by exact or approximate solution approaches. However, finding an exact solution can be infeasible for real-world problems, and even with an approximate solution approach, it can require a prohibitive amount of time to find a near-optimal solution, and the found solutions are not applicable to new problems in general. To address these challenges, we propose an attention-based reinforcement learning method for the class of job shop scheduling problems by integrating policy gradient reinforcement learning with a modified transformer architecture. An important result is that our trained learners in the proposed method can be reused to solve large-scale problems not used in training and demonstrate that our approach outperforms the results of recent studies and widely adopted heuristic rules.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#21319;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2401.16578</link><description>&lt;p&gt;
&#21457;&#25381;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#38271;&#65292;&#25552;&#21319;&#25918;&#23556;&#23398;&#25253;&#21578;&#30340;LLM&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Leveraging Professional Radiologists' Expertise to Enhance LLMs' Evaluation for Radiology Reports. (arXiv:2401.16578v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#26469;&#25552;&#21319;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#35813;&#26041;&#27861;&#30340;&#27169;&#22411;&#22312;&#35780;&#20272;&#20013;&#34920;&#29616;&#20248;&#20110;&#20256;&#32479;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25918;&#23556;&#23398;&#39046;&#22495;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#24050;&#32463;&#22823;&#22823;&#25512;&#36827;&#20102;&#25253;&#21578;&#29983;&#25104;&#65292;&#20294;&#33258;&#21160;&#29983;&#25104;&#25253;&#21578;&#30340;&#33258;&#21160;&#35780;&#20272;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#22914;&#20256;&#32479;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#65288;NLG&#65289;&#21644;&#20020;&#24202;&#25928;&#33021;&#65288;CE&#65289;&#65292;&#24448;&#24448;&#26080;&#27861;&#25429;&#25417;&#20020;&#24202;&#32972;&#26223;&#30340;&#35821;&#20041;&#22797;&#26434;&#24615;&#65292;&#25110;&#32773;&#36807;&#20998;&#24378;&#35843;&#20020;&#24202;&#32454;&#33410;&#65292;&#38477;&#20302;&#20102;&#25253;&#21578;&#30340;&#28165;&#26224;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#19987;&#19994;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#19987;&#19994;&#30693;&#35782;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;GPT-3.5&#21644;GPT-4 1&#65292;&#30456;&#32467;&#21512;&#12290;&#21033;&#29992;&#19978;&#19979;&#25991;&#25351;&#23548;&#23398;&#20064;&#65288;ICIL&#65289;&#21644;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#25512;&#29702;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;LLM&#30340;&#35780;&#20272;&#19982;&#25918;&#23556;&#31185;&#21307;&#29983;&#30340;&#26631;&#20934;&#20445;&#25345;&#19968;&#33268;&#65292;&#23454;&#29616;&#20102;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25253;&#21578;&#19982;&#20154;&#31867;&#29983;&#25104;&#25253;&#21578;&#20043;&#38388;&#30340;&#35814;&#32454;&#27604;&#36739;&#12290;&#36825;&#36827;&#19968;&#27493;&#36890;&#36807;&#22238;&#24402;&#27169;&#22411;&#26469;&#32508;&#21512;&#21477;&#23376;&#35780;&#20272;&#20998;&#25968;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#8220;&#35814;&#32454;GPT-4&#65288;5&#27425;&#35757;&#32451;&#65289;&#8221;&#27169;&#22411;&#33719;&#24471;&#20102;0.48&#30340;&#20998;&#25968;&#65292;&#20248;&#20110;METEOR&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
In radiology, Artificial Intelligence (AI) has significantly advanced report generation, but automatic evaluation of these AI-produced reports remains challenging. Current metrics, such as Conventional Natural Language Generation (NLG) and Clinical Efficacy (CE), often fall short in capturing the semantic intricacies of clinical contexts or overemphasize clinical details, undermining report clarity. To overcome these issues, our proposed method synergizes the expertise of professional radiologists with Large Language Models (LLMs), like GPT-3.5 and GPT-4 1. Utilizing In-Context Instruction Learning (ICIL) and Chain of Thought (CoT) reasoning, our approach aligns LLM evaluations with radiologist standards, enabling detailed comparisons between human and AI generated reports. This is further enhanced by a Regression model that aggregates sentence evaluation scores. Experimental results show that our ''Detailed GPT-4 (5-shot)'' model achieves a 0.48 score, outperforming the METEOR metric 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#12289;&#20998;&#24067;&#24335;&#30340;LLM&#26550;&#26500;&#27010;&#24565;&#65292;&#36890;&#36807;&#22312;&#36890;&#29992;&#35745;&#31639;&#26426;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#25552;&#20379;&#25353;&#38656;&#35775;&#38382;&#30340;&#21487;&#23450;&#21046;&#26381;&#21153;&#65292;&#35299;&#20915;&#20102;LLMs&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#35775;&#38382;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#36164;&#28304;&#21644;&#24212;&#29992;&#38656;&#27714;&#30340;&#26368;&#20339;&#24179;&#34913;&#12290;</title><link>http://arxiv.org/abs/2401.16577</link><description>&lt;p&gt;
LLM&#20316;&#20026;&#25353;&#38656;&#21487;&#23450;&#21046;&#30340;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
LLMs as On-demand Customizable Service. (arXiv:2401.16577v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16577
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#12289;&#20998;&#24067;&#24335;&#30340;LLM&#26550;&#26500;&#27010;&#24565;&#65292;&#36890;&#36807;&#22312;&#36890;&#29992;&#35745;&#31639;&#26426;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#19978;&#25552;&#20379;&#25353;&#38656;&#35775;&#38382;&#30340;&#21487;&#23450;&#21046;&#26381;&#21153;&#65292;&#35299;&#20915;&#20102;LLMs&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#35775;&#38382;&#36807;&#31243;&#20013;&#30340;&#25361;&#25112;&#65292;&#24182;&#33021;&#22815;&#23454;&#29616;&#36164;&#28304;&#21644;&#24212;&#29992;&#38656;&#27714;&#30340;&#26368;&#20339;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#12289;&#37096;&#32626;&#21644;&#35775;&#38382;&#36825;&#20123;&#27169;&#22411;&#37117;&#23384;&#22312;&#26174;&#33879;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#36164;&#28304;&#23494;&#38598;&#38656;&#27714;&#12289;&#38271;&#26102;&#38388;&#35757;&#32451;&#21644;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#23618;&#12289;&#20998;&#24067;&#24335;&#30340;LLM&#26550;&#26500;&#27010;&#24565;&#65292;&#26088;&#22312;&#22686;&#24378;LLMs&#22312;&#24322;&#26500;&#35745;&#31639;&#24179;&#21488;&#19978;&#30340;&#21487;&#35775;&#38382;&#24615;&#21644;&#21487;&#37096;&#32626;&#24615;&#65292;&#21253;&#25324;&#36890;&#29992;&#35745;&#31639;&#26426;&#65288;&#22914;&#31508;&#35760;&#26412;&#30005;&#33041;&#65289;&#21644;&#29289;&#32852;&#32593;&#35774;&#22791;&#65288;&#22914;&#23884;&#20837;&#24335;&#31995;&#32479;&#65289;&#12290;&#36890;&#36807;&#24341;&#20837;"&#20998;&#23618;"&#26041;&#27861;&#65292;&#25152;&#25552;&#20986;&#30340;&#26550;&#26500;&#20351;LLMs&#21487;&#20197;&#25353;&#38656;&#35775;&#38382;&#65292;&#20316;&#20026;&#21487;&#23450;&#21046;&#30340;&#26381;&#21153;&#12290;&#36825;&#31181;&#26041;&#27861;&#36824;&#21487;&#20197;&#30830;&#20445;&#21487;&#29992;&#35745;&#31639;&#36164;&#28304;&#21644;&#29992;&#25143;&#24212;&#29992;&#38656;&#27714;&#20043;&#38388;&#30340;&#26368;&#20339;&#26435;&#34913;&#12290;&#25105;&#20204;&#39044;&#35265;&#21040;&#65292;&#20998;&#23618;LLM&#30340;&#27010;&#24565;&#23558;&#36171;&#20104;&#24191;&#27867;&#30340;&#20247;&#21253;&#29992;&#25143;&#22522;&#30784;&#21033;&#29992;LLM&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#20419;&#36827;&#25216;&#26415;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable language understanding and generation capabilities. However, training, deploying, and accessing these models pose notable challenges, including resource-intensive demands, extended training durations, and scalability issues. To address these issues, we introduce a concept of hierarchical, distributed LLM architecture that aims at enhancing the accessibility and deployability of LLMs across heterogeneous computing platforms, including general-purpose computers (e.g., laptops) and IoT-style devices (e.g., embedded systems). By introducing a "layered" approach, the proposed architecture enables on-demand accessibility to LLMs as a customizable service. This approach also ensures optimal trade-offs between the available computational resources and the user's application needs. We envision that the concept of hierarchical LLM will empower extensive, crowd-sourced user bases to harness the capabilities of LLMs, thereby fostering advan
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#24565;&#31354;&#38388;&#30340;&#35821;&#20041;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#26080;&#27861;&#25429;&#25417;&#21644;&#37327;&#21270;&#8220;&#24847;&#20041;&#8221;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16569</link><description>&lt;p&gt;
&#22522;&#20110;&#33258;&#32534;&#30721;&#22120;&#30340;&#27010;&#24565;&#31354;&#38388;&#35821;&#20041;&#36890;&#20449;&#30340;&#39046;&#22495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Autoencoder-Based Domain Learning for Semantic Communication with Conceptual Spaces. (arXiv:2401.16569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16569
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#27010;&#24565;&#31354;&#38388;&#30340;&#35821;&#20041;&#36890;&#20449;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#26080;&#27861;&#25429;&#25417;&#21644;&#37327;&#21270;&#8220;&#24847;&#20041;&#8221;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20934;&#30830;&#20256;&#36882;&#31526;&#21495;&#30456;&#27604;&#65292;&#20197;&#20934;&#30830;&#20256;&#36882;&#24847;&#20041;&#20026;&#30446;&#26631;&#30340;&#35821;&#20041;&#36890;&#20449;&#24050;&#25104;&#20026;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#12290;&#36825;&#31181;&#33539;&#24335;&#36890;&#24120;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#29616;&#20195;&#21457;&#23637;&#65292;&#20197;&#25552;&#39640;&#36890;&#20449;&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#25429;&#25417;&#21644;&#37327;&#21270;&#8220;&#24847;&#20041;&#8221;&#30340;&#32454;&#33410;&#32570;&#20047;&#19968;&#20010;&#26631;&#20934;&#27169;&#22411;&#65292;&#35768;&#22810;&#39046;&#20808;&#30340;&#35821;&#20041;&#36890;&#20449;&#26041;&#27861;&#37319;&#29992;&#40657;&#30418;&#26694;&#26550;&#65292;&#23545;&#27169;&#22411;&#30340;&#20855;&#20307;&#23398;&#20064;&#20869;&#23481;&#30693;&#20043;&#29978;&#23569;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#27010;&#24565;&#31354;&#38388;&#26694;&#26550;&#65292;&#20197;&#20960;&#20309;&#26041;&#24335;&#26126;&#30830;&#24314;&#27169;&#24847;&#20041;&#12290;&#34429;&#28982;&#20197;&#21069;&#20351;&#29992;&#27010;&#24565;&#31354;&#38388;&#30740;&#31350;&#35821;&#20041;&#36890;&#20449;&#30340;&#24037;&#20316;&#24050;&#32463;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#20808;&#21069;&#30340;&#23581;&#35797;&#28041;&#21450;&#25163;&#24037;&#21046;&#20316;&#27010;&#24565;&#31354;&#38388;&#27169;&#22411;&#65292;&#20005;&#37325;&#38480;&#21046;&#20102;&#35813;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#23398;&#20064;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#65292;&#23454;&#29616;&#27010;&#24565;&#31354;&#38388;&#35821;&#20041;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication with the goal of accurately conveying meaning, rather than accurately transmitting symbols, has become an area of growing interest. This paradigm, termed semantic communication, typically leverages modern developments in artificial intelligence and machine learning to improve the efficiency and robustness of communication systems. However, a standard model for capturing and quantifying the details of "meaning" is lacking, with many leading approaches to semantic communication adopting a black-box framework with little understanding of what exactly the model is learning. One solution is to utilize the conceptual spaces framework, which models meaning explicitly in a geometric manner. Though prior work studying semantic communication with conceptual spaces has shown promising results, these previous attempts involve hand-crafting a conceptual space model, severely limiting the scalability and practicality of the approach. In this work, we develop a framework for learning a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#21360;&#22320;&#35821;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21518;&#24724;&#34920;&#36798;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#21518;&#24724;&#35821;&#35328;&#34920;&#36798;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#39046;&#22495;&#65292;&#25581;&#31034;&#20102;&#21518;&#24724;&#30340;&#26469;&#28304;&#21644;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.16561</link><description>&lt;p&gt;
&#21360;&#22320;&#35821;&#22825;&#22478;&#23383;&#27597;&#33050;&#26412;&#20013;&#30340;&#22810;&#31867;&#21518;&#24724;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Multi-class Regret Detection in Hindi Devanagari Script. (arXiv:2401.16561v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#21360;&#22320;&#35821;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#21518;&#24724;&#34920;&#36798;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#21518;&#24724;&#35821;&#35328;&#34920;&#36798;&#30340;&#29305;&#24449;&#21644;&#30456;&#20851;&#39046;&#22495;&#65292;&#25581;&#31034;&#20102;&#21518;&#24724;&#30340;&#26469;&#28304;&#21644;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#20351;&#29992;&#21360;&#22320;&#35821;&#30340;&#20154;&#25968;&#22823;&#24133;&#22686;&#21152;&#12290;&#21518;&#24724;&#26159;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#24120;&#35265;&#30340;&#24773;&#24863;&#20307;&#39564;&#12290;&#35768;&#22810;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20351;&#29992;&#32773;&#32463;&#24120;&#20998;&#20139;&#20182;&#20204;&#30340;&#21518;&#24724;&#32463;&#21382;&#21644;&#24847;&#35265;&#12290;&#22914;&#26524;&#26377;&#26426;&#20250;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#23545;&#33258;&#24049;&#30340;&#36873;&#25321;&#36827;&#34892;&#37325;&#26032;&#35780;&#20272;&#21644;&#23545;&#19981;&#21516;&#36873;&#25321;&#30340;&#28212;&#26395;&#12290;&#22240;&#27492;&#65292;&#20102;&#35299;&#21518;&#24724;&#30340;&#26469;&#28304;&#23545;&#20110;&#30740;&#31350;&#20854;&#23545;&#34892;&#20026;&#21644;&#20915;&#31574;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#21518;&#24724;&#20197;&#21450;&#23427;&#22914;&#20309;&#22312;&#21360;&#22320;&#35821;&#20013;&#34920;&#36798;&#65292;&#29305;&#21035;&#26159;&#22312;&#21508;&#31181;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#26469;&#33258;&#19977;&#20010;&#19981;&#21516;&#30340;&#26469;&#28304;&#65292;&#27599;&#20010;&#21477;&#23376;&#37117;&#34987;&#25163;&#21160;&#20998;&#31867;&#20026;&#8220;&#34892;&#21160;&#21518;&#24724;&#8221;&#12289;&#8220;&#19981;&#20316;&#20026;&#21518;&#24724;&#8221;&#21644;&#8220;&#26080;&#21518;&#24724;&#8221;&#20013;&#30340;&#19968;&#31867;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#30740;&#31350;&#21360;&#22320;&#35821;&#25991;&#26412;&#20013;&#30340;&#21518;&#24724;&#35821;&#35328;&#34920;&#36798;&#65292;&#24182;&#30830;&#23450;&#19982;&#21518;&#24724;&#26368;&#39057;&#32321;&#30456;&#20851;&#30340;&#25991;&#26412;&#39046;&#22495;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#20010;&#20307;&#20204;&#26368;&#24120;&#19982;&#21518;&#24724;&#30456;&#20851;&#30340;&#39046;&#22495;&#26159;...
&lt;/p&gt;
&lt;p&gt;
The number of Hindi speakers on social media has increased dramatically in recent years. Regret is a common emotional experience in our everyday life. Many speakers on social media, share their regretful experiences and opinions regularly. It might cause a re-evaluation of one's choices and a desire to make a different option if given the chance. As a result, knowing the source of regret is critical for investigating its impact on behavior and decision-making. This study focuses on regret and how it is expressed, specifically in Hindi, on various social media platforms. In our study, we present a novel dataset from three different sources, where each sentence has been manually classified into one of three classes "Regret by action", "Regret by inaction", and "No regret". Next, we use this dataset to investigate the linguistic expressions of regret in Hindi text and also identify the textual domains that are most frequently associated with regret. Our findings indicate that individuals 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2401.16553</link><description>&lt;p&gt;
SelectLLM&#65306;LLMs&#33021;&#21542;&#36873;&#25321;&#37325;&#35201;&#30340;&#25351;&#20196;&#36827;&#34892;&#27880;&#37322;&#65311;
&lt;/p&gt;
&lt;p&gt;
SelectLLM: Can LLMs Select Important Instructions to Annotate?. (arXiv:2401.16553v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16553
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SelectLLM&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#36890;&#36807;&#25552;&#31034;LLMs&#20272;&#35745;&#27599;&#20010;&#26080;&#26631;&#31614;&#25351;&#20196;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#65292;&#24182;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#23558;&#25351;&#20196;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#37327;&#19988;&#22810;&#26679;&#21270;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21487;&#20197;&#20351;&#27169;&#22411;&#29702;&#35299;&#21644;&#36981;&#24490;&#20154;&#31867;&#25351;&#20196;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#19968;&#23567;&#32452;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#21487;&#20197;&#36229;&#36807;&#20351;&#29992;&#22823;&#37327;&#26356;&#22024;&#26434;&#30340;&#25351;&#20196;&#12290;&#30001;&#20110;&#25351;&#20196;&#26159;&#26080;&#26631;&#31614;&#30340;&#65292;&#19988;&#21709;&#24212;&#26159;&#33258;&#28982;&#25991;&#26412;&#65292;&#20256;&#32479;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#26696;&#26080;&#27861;&#30452;&#25509;&#24212;&#29992;&#20110;&#36873;&#25321;&#26080;&#26631;&#31614;&#25351;&#20196;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#20196;&#36873;&#25321;&#26041;&#27861;&#65292;&#31216;&#20026;SelectLLM&#65292;&#23427;&#21033;&#29992;LLMs&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;&#25105;&#20204;&#30340;&#39640;&#32423;&#24605;&#24819;&#26159;&#21033;&#29992;LLMs&#36890;&#36807;&#25552;&#31034;&#26469;&#20272;&#35745;&#27599;&#20010;&#25351;&#20196;&#22312;&#27809;&#26377;&#30456;&#24212;&#26631;&#31614;&#65288;&#21363;&#21709;&#24212;&#65289;&#30340;&#24773;&#20917;&#19979;&#30340;&#26377;&#29992;&#24615;&#21644;&#24433;&#21709;&#21147;&#12290;SelectLLM&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65306;&#20351;&#29992;&#32858;&#31867;&#31639;&#27861;&#65288;&#20363;&#22914;CoreSet&#65289;&#23558;&#26080;&#26631;&#31614;&#25351;&#20196;&#21010;&#20998;&#20026;&#22810;&#20010;&#32858;&#31867;&#65292;&#28982;&#21518;&#25552;&#31034;LLMs&#22312;&#20854;&#20013;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training large language models (LLMs) with a large and diverse instruction dataset aligns the models to comprehend and follow human instructions. Recent works have shown that using a small set of high-quality instructions can outperform using large yet more noisy ones. Because instructions are unlabeled and their responses are natural text, traditional active learning schemes with the model's confidence cannot be directly applied to the selection of unlabeled instructions. In this work, we propose a novel method for instruction selection, called SelectLLM, that leverages LLMs for the selection of high-quality instructions. Our high-level idea is to use LLMs to estimate the usefulness and impactfulness of each instruction without the corresponding labels (i.e., responses), via prompting. SelectLLM involves two steps: dividing the unlabelled instructions using a clustering algorithm (e.g., CoreSet) to multiple clusters, and then prompting LLMs to choose high-quality instructions within e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;GuReT&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#20869;&#30106;&#21644;&#21518;&#24724;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25506;&#32034;&#20854;&#22312;&#25991;&#26412;&#20013;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#22312;&#20869;&#30106;&#21644;&#21518;&#24724;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16541</link><description>&lt;p&gt;
GuReT&#65306;&#21306;&#20998;&#19982;&#20869;&#30106;&#30456;&#20851;&#21644;&#21518;&#24724;&#30456;&#20851;&#30340;&#25991;&#26412;
&lt;/p&gt;
&lt;p&gt;
GuReT: Distinguishing Guilt and Regret related Text. (arXiv:2401.16541v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16541
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;GuReT&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#30740;&#31350;&#20869;&#30106;&#21644;&#21518;&#24724;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25506;&#32034;&#20854;&#22312;&#25991;&#26412;&#20013;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#36890;&#36807;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#65292;&#32467;&#26524;&#34920;&#26126;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#22312;&#20869;&#30106;&#21644;&#21518;&#24724;&#35782;&#21035;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#20110;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20915;&#31574;&#21644;&#24773;&#32490;&#65288;&#29305;&#21035;&#26159;&#20869;&#30106;&#21644;&#21518;&#24724;&#65289;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#23545;&#34892;&#20026;&#21644;&#24184;&#31119;&#24863;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#22312;&#35745;&#31639;&#27169;&#22411;&#20013;&#24448;&#24448;&#24573;&#35270;&#20102;&#36825;&#20123;&#24773;&#32490;&#30340;&#24494;&#22937;&#21306;&#21035;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#21078;&#26512;&#20869;&#30106;&#21644;&#21518;&#24724;&#20043;&#38388;&#30340;&#20851;&#31995;&#21450;&#20854;&#29420;&#29305;&#30340;&#25991;&#26412;&#26631;&#24535;&#65292;&#24357;&#34917;&#20102;&#24773;&#24863;&#35745;&#31639;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#26174;&#33879;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20869;&#30106;&#21644;&#21518;&#24724;&#35782;&#21035;&#35270;&#20026;&#20108;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#37319;&#29992;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#21644;&#20845;&#31181;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23545;&#26032;&#21019;&#24314;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#12290;&#35813;&#30740;&#31350;&#36824;&#37319;&#29992;&#20102;&#38142;&#29366;&#24605;&#32500;&#21644;&#26641;&#29366;&#24605;&#32500;&#31561;&#21019;&#26032;&#25512;&#29702;&#26041;&#27861;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#35299;&#37322;&#36923;&#36753;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#20855;&#26377;&#26126;&#26174;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#30456;&#27604;&#20110;&#26368;&#22909;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#31867;&#22120;&#30340;85.3%&#30340;&#23439;F1&#20998;&#25968;&#65292;&#21487;&#20197;&#36798;&#21040;90.4%&#30340;&#23439;F1&#20998;&#25968;&#65292;&#23637;&#31034;&#20102;&#20854;&#36739;&#24378;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The intricate relationship between human decision-making and emotions, particularly guilt and regret, has significant implications on behavior and well-being. Yet, these emotions subtle distinctions and interplay are often overlooked in computational models. This paper introduces a dataset tailored to dissect the relationship between guilt and regret and their unique textual markers, filling a notable gap in affective computing research. Our approach treats guilt and regret recognition as a binary classification task and employs three machine learning and six transformer-based deep learning techniques to benchmark the newly created dataset. The study further implements innovative reasoning methods like chain-of-thought and tree-of-thought to assess the models interpretive logic. The results indicate a clear performance edge for transformer-based models, achieving a 90.4% macro F1 score compared to the 85.3% scored by the best machine learning classifier, demonstrating their superior ca
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39564;&#35777;&#20102;&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25200;&#21160;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2401.16521</link><description>&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25200;&#21160;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#30340;&#39564;&#35777;&#12289;&#20581;&#22766;&#24615;&#21644;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Validation, Robustness, and Accuracy of Perturbation-Based Sensitivity Analysis Methods for Time-Series Deep Learning Models. (arXiv:2401.16521v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16521
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39564;&#35777;&#20102;&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#25200;&#21160;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#30340;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#21644;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#26102;&#38388;&#24207;&#21015;&#28145;&#24230;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;&#25935;&#24863;&#24615;&#20998;&#26512;&#35780;&#20272;&#36755;&#20837;&#21464;&#21270;&#23545;&#36755;&#20986;&#30340;&#24433;&#21709;&#65292;&#26159;&#35299;&#37322;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#22312;&#21518;&#26399;&#35299;&#37322;&#26041;&#27861;&#20013;&#65292;&#22914;&#21453;&#21521;&#20256;&#25773;&#12289;&#25200;&#21160;&#21644;&#36817;&#20284;&#27861;&#20013;&#65292;&#26412;&#30740;&#31350;&#23558;&#35843;&#26597;&#29616;&#20195;Transformer&#27169;&#22411;&#19978;&#30340;&#22522;&#20110;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#26412;&#30740;&#31350;&#22238;&#31572;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#65306;1&#65289;&#19981;&#21516;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#26159;&#21542;&#20135;&#29983;&#21487;&#27604;&#36739;&#30340;&#36755;&#20986;&#21644;&#23646;&#24615;&#37325;&#35201;&#24615;&#25490;&#24207;&#65311;2&#65289;&#20351;&#29992;&#30456;&#21516;&#30340;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#65292;&#19981;&#21516;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;&#21542;&#23545;&#25935;&#24863;&#24615;&#20998;&#26512;&#30340;&#36755;&#20986;&#20135;&#29983;&#24433;&#21709;&#65311;3&#65289;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#30340;&#32467;&#26524;&#19982;&#22522;&#26412;&#20107;&#23454;&#30340;&#19968;&#33268;&#24615;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
This work undertakes studies to evaluate Interpretability Methods for Time-Series Deep Learning. Sensitivity analysis assesses how input changes affect the output, constituting a key component of interpretation. Among the post-hoc interpretation methods such as back-propagation, perturbation, and approximation, my work will investigate perturbation-based sensitivity Analysis methods on modern Transformer models to benchmark their performances. Specifically, my work answers three research questions: 1) Do different sensitivity analysis (SA) methods yield comparable outputs and attribute importance rankings? 2) Using the same sensitivity analysis method, do different Deep Learning (DL) models impact the output of the sensitivity analysis? 3) How well do the results from sensitivity analysis methods align with the ground truth?
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;AFSD-Physics&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#23454;&#39564;&#25968;&#25454;&#65292;&#24471;&#21040;&#20102;&#28155;&#21152;&#25705;&#25830;&#25605;&#25292;&#22534;&#31215;&#36807;&#31243;&#20013;&#28201;&#24230;&#28436;&#21464;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#12289;&#35745;&#31639;&#25104;&#26412;&#20302;&#19988;&#20934;&#30830;&#24230;&#39640;&#65292;&#19982;&#23454;&#38469;&#27979;&#37327;&#32467;&#26524;&#21563;&#21512;&#36739;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.16501</link><description>&lt;p&gt;
AFSD-Physics&#65306;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#26041;&#27861;&#25506;&#32034;&#28155;&#21152;&#25705;&#25830;&#25605;&#25292;&#22534;&#31215;&#36807;&#31243;&#20013;&#28201;&#24230;&#28436;&#21464;&#30340;&#25511;&#21046;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
AFSD-Physics: Exploring the governing equations of temperature evolution during additive friction stir deposition by a human-AI teaming approach. (arXiv:2401.16501v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16501
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;AFSD-Physics&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#23454;&#39564;&#25968;&#25454;&#65292;&#24471;&#21040;&#20102;&#28155;&#21152;&#25705;&#25830;&#25605;&#25292;&#22534;&#31215;&#36807;&#31243;&#20013;&#28201;&#24230;&#28436;&#21464;&#30340;&#25511;&#21046;&#26041;&#31243;&#12290;&#35813;&#27169;&#22411;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#12289;&#35745;&#31639;&#25104;&#26412;&#20302;&#19988;&#20934;&#30830;&#24230;&#39640;&#65292;&#19982;&#23454;&#38469;&#27979;&#37327;&#32467;&#26524;&#21563;&#21512;&#36739;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26041;&#27861;&#65292;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#26041;&#27861;&#30740;&#31350;&#28155;&#21152;&#25705;&#25830;&#25605;&#25292;&#22534;&#31215;&#65288;AFSD&#65289;&#36807;&#31243;&#20013;&#28201;&#24230;&#28436;&#21464;&#30340;&#29289;&#29702;&#21407;&#29702;&#12290;AFSD&#26159;&#19968;&#31181;&#26032;&#20852;&#30340;&#22266;&#24577;&#22686;&#26448;&#21046;&#36896;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#29076;&#34701;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26448;&#26009;&#22534;&#31215;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#35813;&#36807;&#31243;&#30340;&#24314;&#27169;&#20197;&#21450;AFSD&#24037;&#20855;&#30340;&#24314;&#27169;&#36824;&#22788;&#20110;&#26089;&#26399;&#38454;&#27573;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#21327;&#20316;&#30340;&#26041;&#27861;&#65292;&#23558;&#22522;&#20110;&#31532;&#19968;&#21407;&#29702;&#30340;&#27169;&#22411;&#19982;&#20154;&#24037;&#26234;&#33021;&#30456;&#32467;&#21512;&#12290;&#24471;&#21040;&#30340;&#20154;&#24037;&#26234;&#33021;&#23398;&#20064;&#26041;&#27861;&#34987;&#21629;&#21517;&#20026;AFSD-Physics&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#24037;&#20855;&#21644;&#22534;&#31215;&#36807;&#31243;&#20013;&#28201;&#24230;&#28436;&#21464;&#30340;&#25511;&#21046;&#26041;&#31243;&#65292;&#36890;&#36807;&#36807;&#31243;&#20013;&#30340;&#27979;&#37327;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#12290;&#35774;&#35745;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#37319;&#38598;&#20102;30&#23618;&#38109;7075&#26448;&#26009;&#22534;&#31215;&#36807;&#31243;&#20013;&#30340;&#27979;&#37327;&#25968;&#25454;&#12290;&#24471;&#21040;&#30340;&#25511;&#21046;&#26041;&#31243;&#26159;&#20855;&#26377;&#29289;&#29702;&#35299;&#37322;&#24615;&#12289;&#35745;&#31639;&#25104;&#26412;&#20302;&#19988;&#20934;&#30830;&#24230;&#39640;&#30340;&#27169;&#22411;&#12290;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#19982;&#27979;&#37327;&#32467;&#26524;&#20855;&#26377;&#33391;&#22909;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a modeling effort to explore the underlying physics of temperature evolution during additive friction stir deposition (AFSD) by a human-AI teaming approach. AFSD is an emerging solid-state additive manufacturing technology that deposits materials without melting. However, both process modeling and modeling of the AFSD tool are at an early stage. In this paper, a human-AI teaming approach is proposed to combine models based on first principles with AI. The resulting human-informed machine learning method, denoted as AFSD-Physics, can effectively learn the governing equations of temperature evolution at the tool and the build from in-process measurements. Experiments are designed and conducted to collect in-process measurements for the deposition of aluminum 7075 with a total of 30 layers. The acquired governing equations are physically interpretable models with low computational cost and high accuracy. Model predictions show good agreement with the measurements. Expe
&lt;/p&gt;</description></item><item><title>ReGAL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#20195;&#30721;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#20989;&#25968;&#24211;&#65292;&#21033;&#29992;&#36825;&#20123;&#20849;&#20139;&#20989;&#25968;&#24211;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#31243;&#24207;&#12290;</title><link>http://arxiv.org/abs/2401.16467</link><description>&lt;p&gt;
ReGAL: &#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ReGAL: Refactoring Programs to Discover Generalizable Abstractions. (arXiv:2401.16467v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16467
&lt;/p&gt;
&lt;p&gt;
ReGAL&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21457;&#29616;&#36890;&#29992;&#25277;&#35937;&#30340;&#31243;&#24207;&#37325;&#26500;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#37325;&#26500;&#20195;&#30721;&#23398;&#20064;&#21487;&#37325;&#29992;&#30340;&#20989;&#25968;&#24211;&#65292;&#21033;&#29992;&#36825;&#20123;&#20849;&#20139;&#20989;&#25968;&#24211;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#39044;&#27979;&#31243;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;&#31243;&#24207;&#21512;&#25104;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#24320;&#21457;&#26377;&#29992;&#25277;&#35937;&#25152;&#38656;&#30340;&#20840;&#23616;&#35270;&#35282;&#65307;&#23427;&#20204;&#36890;&#24120;&#19968;&#27425;&#39044;&#27979;&#19968;&#20010;&#31243;&#24207;&#65292;&#32463;&#24120;&#37325;&#22797;&#30456;&#21516;&#30340;&#21151;&#33021;&#12290;&#20174;&#22836;&#24320;&#22987;&#29983;&#25104;&#20887;&#20313;&#20195;&#30721;&#26082;&#20302;&#25928;&#21448;&#23481;&#26131;&#20986;&#38169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#36890;&#29992;&#25277;&#35937;&#23398;&#20064;&#30340;&#37325;&#26500;&#26041;&#27861;&#65288;ReGAL&#65289;&#65292;&#36890;&#36807;&#20195;&#30721;&#37325;&#26500;&#26469;&#23398;&#20064;&#21487;&#37325;&#29992;&#20989;&#25968;&#24211;&#65292;&#21363;&#22312;&#19981;&#25913;&#21464;&#20195;&#30721;&#25191;&#34892;&#36755;&#20986;&#30340;&#24773;&#20917;&#19979;&#37325;&#32452;&#20195;&#30721;&#12290;ReGAL&#20174;&#19968;&#23567;&#32452;&#29616;&#26377;&#31243;&#24207;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#25191;&#34892;&#39564;&#35777;&#21644;&#32454;&#21270;&#25277;&#35937;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ReGAL&#21457;&#29616;&#30340;&#20849;&#20139;&#20989;&#25968;&#24211;&#20351;&#24471;&#22312;&#19981;&#21516;&#39046;&#22495;&#39044;&#27979;&#31243;&#24207;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;LOGO&#22270;&#24418;&#29983;&#25104;&#12289;&#26085;&#26399;&#25512;&#29702;&#21644;&#22522;&#20110;Minecraft&#30340;&#25991;&#23383;&#28216;&#25103;TextCraft&#65289;&#19978;&#65292;&#24320;&#28304;&#21644;&#19987;&#26377;&#30340;LLMs&#22312;&#20351;&#29992;ReGAL&#20989;&#25968;&#24211;&#39044;&#27979;&#31243;&#24207;&#26102;&#20934;&#30830;&#24615;&#24471;&#21040;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) are increasingly being used for program synthesis, they lack the global view needed to develop useful abstractions; they generally predict programs one at a time, often repeating the same functionality. Generating redundant code from scratch is both inefficient and error-prone. To address this, we propose Refactoring for Generalizable Abstraction Learning (ReGAL), a gradient-free method for learning a library of reusable functions via code refactorization, i.e. restructuring code without changing its execution output. ReGAL learns from a small set of existing programs, iteratively verifying and refining its abstractions via execution. We find that the shared function libraries discovered by ReGAL make programs easier to predict across diverse domains. On three datasets (LOGO graphics generation, Date reasoning, and TextCraft, a Minecraft-based text game), both open-source and proprietary LLMs improve in accuracy when predicting programs with ReGAL fun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#21452;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#28789;&#27963;&#30340;&#29305;&#24449;&#34701;&#21512;&#21644;&#29305;&#24449;&#31354;&#38388;&#20840;&#23616;&#20851;&#31995;&#19981;&#21464;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.16462</link><description>&lt;p&gt;
&#22522;&#20110;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#21452;&#28151;&#21512;&#27169;&#22411;&#29992;&#20110;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Supervised Contrastive Learning based Dual-Mixer Model for Remaining Useful Life Prediction. (arXiv:2401.16462v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16462
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#30340;&#21452;&#28151;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#21097;&#20313;&#23551;&#21629;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#28789;&#27963;&#30340;&#29305;&#24449;&#34701;&#21512;&#21644;&#29305;&#24449;&#31354;&#38388;&#20840;&#23616;&#20851;&#31995;&#19981;&#21464;&#24615;&#35757;&#32451;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21097;&#20313;&#23551;&#21629;&#65288;RUL&#65289;&#39044;&#27979;&#38382;&#39064;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#26088;&#22312;&#20934;&#30830;&#20272;&#35745;&#20174;&#24403;&#21069;&#39044;&#27979;&#26102;&#21051;&#21040;&#35774;&#22791;&#23436;&#20840;&#22833;&#25928;&#30340;&#21097;&#20313;&#26102;&#38388;&#12290;&#20026;&#20102;&#20811;&#26381;&#29616;&#26377;RUL&#39044;&#27979;&#26041;&#27861;&#20013;&#26102;&#38388;&#21644;&#31354;&#38388;&#29305;&#24449;&#21018;&#24615;&#32452;&#21512;&#30340;&#32570;&#28857;&#65292;&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21452;&#28151;&#21512;&#27169;&#22411;&#30340;&#26102;&#31354;&#21516;&#36136;&#29305;&#24449;&#25552;&#21462;&#22120;&#12290;&#37319;&#29992;&#28789;&#27963;&#30340;&#36880;&#23618;&#36882;&#36827;&#29305;&#24449;&#34701;&#21512;&#65292;&#20197;&#30830;&#20445;&#26102;&#31354;&#29305;&#24449;&#30340;&#21516;&#36136;&#24615;&#24182;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20854;&#27425;&#65292;&#22522;&#20110;&#30417;&#30563;&#23545;&#27604;&#23398;&#20064;&#24341;&#20837;&#20102;&#29305;&#24449;&#31354;&#38388;&#20840;&#23616;&#20851;&#31995;&#19981;&#21464;&#24615;&#65288;FSGRI&#65289;&#35757;&#32451;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#32500;&#25345;&#26679;&#26412;&#29305;&#24449;&#19982;&#20854;&#36864;&#21270;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#31995;&#19968;&#33268;&#24615;&#65292;&#31616;&#21270;&#20102;&#36755;&#20986;&#23618;&#20013;&#30340;&#22238;&#24402;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The problem of the Remaining Useful Life (RUL) prediction, aiming at providing an accurate estimate of the remaining time from the current predicting moment to the complete failure of the device, has gained significant attention from researchers in recent years. In this paper, to overcome the shortcomings of rigid combination for temporal and spatial features in most existing RUL prediction approaches, a spatial-temporal homogeneous feature extractor, named Dual-Mixer model, is firstly proposed. Flexible layer-wise progressive feature fusion is employed to ensure the homogeneity of spatial-temporal features and enhance the prediction accuracy. Secondly, the Feature Space Global Relationship Invariance (FSGRI) training method is introduced based on supervised contrastive learning. This method maintains the consistency of relationships among sample features with their degradation patterns during model training, simplifying the subsequently regression task in the output layer and improvin
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#27969;&#25512;&#21160;&#21512;&#20316;&#24182;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;</title><link>http://arxiv.org/abs/2401.16461</link><description>&lt;p&gt;
&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65306;&#26356;&#24555;&#30340;&#20986;&#29616;&#65292;&#26356;&#24555;&#20048;&#30340;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Norm Enforcement with a Soft Touch: Faster Emergence, Happier Agents. (arXiv:2401.16461v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16461
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28201;&#21644;&#30340;&#35268;&#33539;&#25191;&#34892;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#20132;&#27969;&#25512;&#21160;&#21512;&#20316;&#24182;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#21487;&#35270;&#20026;&#19968;&#20010;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#31038;&#20250;&#65292;&#36890;&#36807;&#31038;&#20250;&#35268;&#33539;&#21487;&#20197;&#26377;&#25928;&#22320;&#35843;&#25511;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;&#19968;&#20010;&#31038;&#20250;&#30340;&#35268;&#33539;&#24182;&#19981;&#26159;&#30828;&#32534;&#30721;&#30340;&#65292;&#32780;&#26159;&#20174;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#20013;&#20135;&#29983;&#30340;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#19968;&#20010;&#31038;&#20250;&#20013;&#30340;&#26234;&#33021;&#20307;&#23545;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#20316;&#20986;&#30340;&#21453;&#24212;&#20197;&#21450;&#23545;&#20182;&#20154;&#21453;&#24212;&#30340;&#22238;&#24212;&#65292;&#20915;&#23450;&#20102;&#31038;&#20250;&#20013;&#20986;&#29616;&#21738;&#20123;&#35268;&#33539;&#12290;&#25105;&#20204;&#23558;&#19968;&#20010;&#26234;&#33021;&#20307;&#23545;&#21478;&#19968;&#20010;&#26234;&#33021;&#20307;&#30340;&#28385;&#24847;&#25110;&#19981;&#28385;&#24847;&#34892;&#20026;&#30340;&#21453;&#24212;&#35270;&#20026;&#31532;&#19968;&#20010;&#26234;&#33021;&#20307;&#21521;&#31532;&#20108;&#20010;&#26234;&#33021;&#20307;&#30340;&#20132;&#27969;&#12290;&#29702;&#35299;&#36825;&#20123;&#20132;&#27969;&#26159;&#19968;&#31181;&#31038;&#20250;&#26234;&#33021;&#65306;&#36825;&#20123;&#20132;&#27969;&#36890;&#36807;&#25512;&#21160;&#26234;&#33021;&#20307;&#26397;&#30528;&#26576;&#20123;&#34892;&#20026;&#36827;&#34892;&#65292;&#20174;&#32780;&#20419;&#36827;&#35268;&#33539;&#30340;&#20986;&#29616;&#12290;&#34429;&#28982;&#20247;&#25152;&#21608;&#30693;&#24809;&#32602;&#21487;&#20197;&#23548;&#33268;&#35268;&#33539;&#30340;&#20986;&#29616;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#26356;&#23485;&#27867;&#30340;&#31038;&#20250;&#26234;&#33021;&#21487;&#33021;&#22312;&#20419;&#36827;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#21512;&#20316;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34987;&#31216;&#20026;Ne&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiagent system can be viewed as a society of autonomous agents, whose interactions can be effectively regulated via social norms. In general, the norms of a society are not hardcoded but emerge from the agents' interactions. Specifically, how the agents in a society react to each other's behavior and respond to the reactions of others determines which norms emerge in the society. We think of these reactions by an agent to the satisfactory or unsatisfactory behaviors of another agent as communications from the first agent to the second agent. Understanding these communications is a kind of social intelligence: these communications provide natural drivers for norm emergence by pushing agents toward certain behaviors, which can become established as norms. Whereas it is well-known that sanctioning can lead to the emergence of norms, we posit that a broader kind of social intelligence can prove more effective in promoting cooperation in a multiagent system.  Accordingly, we develop Ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16458</link><description>&lt;p&gt;
&#20449;&#29992;&#39118;&#38505;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#65306;&#20174;P2P&#20511;&#36151;&#30340;&#36151;&#27454;&#25551;&#36848;&#20013;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending. (arXiv:2401.16458v1 [q-fin.RM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16458
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#21033;&#29992;P2P&#20511;&#36151;&#24179;&#21488;&#19978;&#20511;&#27454;&#20154;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#26469;&#26500;&#24314;&#39118;&#38505;&#25351;&#26631;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#21487;&#20197;&#26126;&#26174;&#25552;&#39640;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
P2P&#20511;&#36151;&#20316;&#20026;&#19968;&#31181;&#29420;&#29305;&#30340;&#34701;&#36164;&#26426;&#21046;&#65292;&#36890;&#36807;&#22312;&#32447;&#24179;&#21488;&#23558;&#20511;&#27454;&#20154;&#19982;&#25918;&#27454;&#20154;&#32852;&#31995;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;P2P&#20511;&#36151;&#38754;&#20020;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#25918;&#27454;&#20154;&#24448;&#24448;&#32570;&#20047;&#36275;&#22815;&#30340;&#25968;&#25454;&#26469;&#35780;&#20272;&#20511;&#27454;&#20154;&#30340;&#20449;&#29992;&#20215;&#20540;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#21363;&#21033;&#29992;&#20511;&#27454;&#20154;&#22312;&#36151;&#27454;&#30003;&#35831;&#36807;&#31243;&#20013;&#25552;&#20379;&#30340;&#25991;&#26412;&#25551;&#36848;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22788;&#29702;&#36825;&#20123;&#25991;&#26412;&#25551;&#36848;&#65292;LLM&#26159;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#27169;&#24335;&#21644;&#35821;&#20041;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#23558;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#20110;&#23558;LLM&#36866;&#24212;&#29305;&#23450;&#20219;&#21153;&#12290;&#25105;&#20204;&#20174;Lending Club&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#32467;&#26524;&#26174;&#31034;&#65292;BERT&#29983;&#25104;&#30340;&#39118;&#38505;&#35780;&#20998;&#26174;&#33879;&#25552;&#39640;&#20102;&#20449;&#29992;&#39118;&#38505;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;LLM&#30340;&#31995;&#32479;&#22266;&#26377;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#20197;&#21450;&#28508;&#22312;&#20559;&#24046;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism, linking borrowers with lenders through online platforms. However, P2P lending faces the challenge of information asymmetry, as lenders often lack sufficient data to assess the creditworthiness of borrowers. This paper proposes a novel approach to address this issue by leveraging the textual descriptions provided by borrowers during the loan application process. Our methodology involves processing these textual descriptions using a Large Language Model (LLM), a powerful tool capable of discerning patterns and semantics within the text. Transfer learning is applied to adapt the LLM to the specific task at hand.  Our results derived from the analysis of the Lending Club dataset show that the risk score generated by BERT, a widely used LLM, significantly improves the performance of credit risk classifiers. However, the inherent opacity of LLM-based systems, coupled with uncertainties about potential biases, unders
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#21487;&#25511;&#38376;&#36866;&#37197;&#22120;&#65288;ConGater&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#21487;&#35843;&#33410;&#25935;&#24863;&#24615;&#21442;&#25968;&#30340;&#26032;&#39062;&#27169;&#22359;&#21270;&#38376;&#26426;&#21046;&#65292;&#21487;&#22312;&#25512;&#29702;&#26102;&#36880;&#28176;&#36807;&#28193;&#20174;&#27169;&#22411;&#30340;&#20559;&#21521;&#29366;&#24577;&#21040;&#23436;&#20840;&#21435;&#20559;&#30340;&#29256;&#26412;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20998;&#31867;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16457</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#21487;&#25511;&#20559;&#24046;&#32531;&#35299;&#26041;&#27861;&#65292;&#21033;&#29992;&#38376;&#36866;&#37197;&#22120;&#36827;&#34892;&#20998;&#31867;&#21644;&#26816;&#32034;&#12290;(arXiv:2401.16457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Effective Controllable Bias Mitigation for Classification and Retrieval using Gate Adapters. (arXiv:2401.16457v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#21487;&#25511;&#38376;&#36866;&#37197;&#22120;&#65288;ConGater&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#21487;&#35843;&#33410;&#25935;&#24863;&#24615;&#21442;&#25968;&#30340;&#26032;&#39062;&#27169;&#22359;&#21270;&#38376;&#26426;&#21046;&#65292;&#21487;&#22312;&#25512;&#29702;&#26102;&#36880;&#28176;&#36807;&#28193;&#20174;&#27169;&#22411;&#30340;&#20559;&#21521;&#29366;&#24577;&#21040;&#23436;&#20840;&#21435;&#20559;&#30340;&#29256;&#26412;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#22312;&#20998;&#31867;&#21644;&#26816;&#32034;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#20559;&#24046;&#32531;&#35299;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#30340;&#20027;&#39064;&#65292;&#26368;&#36817;&#20851;&#27880;&#30340;&#28966;&#28857;&#26159;&#23398;&#20064;&#29420;&#31435;&#30340;&#27169;&#22359;&#65292;&#20363;&#22914;&#36866;&#37197;&#22120;&#36827;&#34892;&#25353;&#38656;&#21435;&#20559;&#12290;&#38500;&#20102;&#20248;&#21270;&#27169;&#22359;&#21270;&#21435;&#20559;&#27169;&#22411;&#22806;&#65292;&#22312;&#23454;&#36341;&#20013;&#36890;&#24120;&#38656;&#35201;&#22312;&#25512;&#29702;&#26102;&#25511;&#21046;&#20559;&#24046;&#20943;&#23569;&#30340;&#31243;&#24230;&#65292;&#20363;&#22914;&#65292;&#20026;&#20102;&#22312;&#25628;&#32034;&#32467;&#26524;&#20013;&#35843;&#25972;&#26399;&#26395;&#30340;&#24615;&#33021;-&#20844;&#24179;&#24615;&#26435;&#34913;&#25110;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#25511;&#21046;&#21435;&#20559;&#30340;&#24378;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#25511;&#38376;&#36866;&#37197;&#22120;&#65288;ConGater&#65289;&#65292;&#19968;&#31181;&#20855;&#26377;&#21487;&#35843;&#33410;&#25935;&#24863;&#24615;&#21442;&#25968;&#30340;&#26032;&#39062;&#27169;&#22359;&#21270;&#38376;&#26426;&#21046;&#65292;&#20801;&#35768;&#22312;&#25512;&#29702;&#26102;&#20174;&#27169;&#22411;&#30340;&#20559;&#21521;&#29366;&#24577;&#36880;&#28176;&#36807;&#28193;&#21040;&#23436;&#20840;&#21435;&#20559;&#30340;&#29256;&#26412;&#12290;&#36890;&#36807;&#22312;&#19977;&#20010;&#20998;&#31867;&#20219;&#21153;&#19978;&#23545;&#19977;&#20010;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#23545;&#25239;&#24615;&#21435;&#20559;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;&#20844;&#24179;&#24615;&#21015;&#34920;&#27491;&#21017;&#21270;&#26469;&#20943;&#23569;&#25628;&#32034;&#32467;&#26524;&#30340;&#20559;&#24046;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;ConGater&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bias mitigation of Language Models has been the topic of many studies with a recent focus on learning separate modules like adapters for on-demand debiasing. Besides optimizing for a modularized debiased model, it is often critical in practice to control the degree of bias reduction at inference time, e.g., in order to tune for a desired performance-fairness trade-off in search results or to control the strength of debiasing in classification tasks. In this paper, we introduce Controllable Gate Adapter (ConGater), a novel modular gating mechanism with adjustable sensitivity parameters, which allows for a gradual transition from the biased state of the model to the fully debiased version at inference time. We demonstrate ConGater performance by (1) conducting adversarial debiasing experiments with three different models on three classification tasks with four protected attributes, and (2) reducing the bias of search results through fairness list-wise regularization to enable adjusting a
&lt;/p&gt;</description></item><item><title>KAUCUS&#24341;&#20837;&#20102;&#30693;&#35782;&#22686;&#24378;&#29992;&#25143;&#27169;&#25311;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#25311;&#22120;&#21161;&#25163;&#20132;&#20114;&#65292;&#24182;&#33021;&#22815;&#24555;&#36895;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#21161;&#25163;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2401.16454</link><description>&lt;p&gt;
KAUCUS: &#30693;&#35782;&#22686;&#24378;&#29992;&#25143;&#27169;&#25311;&#22120;&#29992;&#20110;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
KAUCUS: Knowledge Augmented User Simulators for Training Language Model Assistants. (arXiv:2401.16454v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16454
&lt;/p&gt;
&lt;p&gt;
KAUCUS&#24341;&#20837;&#20102;&#30693;&#35782;&#22686;&#24378;&#29992;&#25143;&#27169;&#25311;&#22120;&#26694;&#26550;&#65292;&#21487;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#25311;&#22120;&#21161;&#25163;&#20132;&#20114;&#65292;&#24182;&#33021;&#22815;&#24555;&#36895;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#65292;&#20174;&#32780;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#21161;&#25163;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#26377;&#29992;&#20132;&#20114;&#25968;&#25454;&#30340;&#27169;&#25311;&#22120;&#65292;&#21487;&#20197;&#24320;&#21457;&#20986;&#19968;&#20010;&#26377;&#25928;&#30340;&#22810;&#36718;&#25351;&#20196;&#36319;&#38543;&#21161;&#25163;&#12290;&#29702;&#24819;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#38500;&#20102;&#20381;&#38752;&#20854;&#20869;&#22312;&#26435;&#37325;&#22806;&#65292;&#36824;&#24212;&#33021;&#22815;&#24555;&#36895;&#24341;&#20837;&#22806;&#37096;&#30693;&#35782;&#26469;&#27169;&#25311;&#20114;&#32852;&#32593;&#19978;&#22810;&#26679;&#21270;&#30340;&#25991;&#26412;&#12290;&#20197;&#24448;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#36890;&#24120;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#20027;&#35201;&#26159;&#23553;&#38381;&#39046;&#22495;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#20005;&#26684;&#30340;&#27169;&#24335;&#65292;&#20351;&#24471;&#23427;&#20204;&#26080;&#27861;&#24555;&#36895;&#25193;&#23637;&#20197;&#34701;&#20837;&#22806;&#37096;&#30693;&#35782;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;Kaucus&#30340;&#30693;&#35782;&#22686;&#24378;&#29992;&#25143;&#27169;&#25311;&#22120;&#26694;&#26550;&#65292;&#20197;&#27010;&#36848;&#21019;&#24314;&#22810;&#26679;&#21270;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#36807;&#31243;&#65292;&#24182;&#33021;&#22815;&#26080;&#32541;&#22320;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#21516;&#26102;&#21463;&#30410;&#20110;&#19979;&#28216;&#21161;&#25163;&#27169;&#22411;&#30340;&#35757;&#32451;&#12290;&#36890;&#36807;&#20004;&#20010;&#22522;&#20110;GPT-J&#30340;&#27169;&#25311;&#22120;&#65292;&#21363;&#26816;&#32034;&#22686;&#24378;&#27169;&#25311;&#22120;&#21644;&#25688;&#35201;&#25511;&#21046;&#27169;&#25311;&#22120;&#65292;&#25105;&#20204;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#27169;&#25311;&#22120;&#21161;&#25163;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
An effective multi-turn instruction-following assistant can be developed by creating a simulator that can generate useful interaction data. Apart from relying on its intrinsic weights, an ideal user simulator should also be able to bootstrap external knowledge rapidly in its raw form to simulate the multifarious diversity of text available over the internet. Previous user simulators generally lacked diversity, were mostly closed domain, and necessitated rigid schema making them inefficient to rapidly scale to incorporate external knowledge. In this regard, we introduce, Kaucus, a Knowledge-Augmented User Simulator framework, to outline a process of creating diverse user simulators, that can seamlessly exploit external knowledge as well as benefit downstream assistant model training. Through two GPT-J based simulators viz., a Retrieval Augmented Simulator and a Summary Controlled Simulator we generate diverse simulator-assistant interactions. Through reward and preference model-based ev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#28151;&#21512;Transformer&#21644;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;Chebyshev&#22810;&#39033;&#24335;&#22270;&#21367;&#31215;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23545;&#22797;&#26434;&#31354;&#38388;&#20381;&#36182;&#24615;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#26469;&#24314;&#27169;&#26102;&#31354;&#24322;&#36136;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.16453</link><description>&lt;p&gt;
&#28151;&#21512;Transformer&#21644;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#38271;&#26399;&#20132;&#36890;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Hybrid Transformer and Spatial-Temporal Self-Supervised Learning for Long-term Traffic Prediction. (arXiv:2401.16453v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23558;&#28151;&#21512;Transformer&#21644;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24212;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21644;Chebyshev&#22810;&#39033;&#24335;&#22270;&#21367;&#31215;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#23545;&#22797;&#26434;&#31354;&#38388;&#20381;&#36182;&#24615;&#30340;&#25429;&#25417;&#33021;&#21147;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#26469;&#24314;&#27169;&#26102;&#31354;&#24322;&#36136;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20132;&#36890;&#39044;&#27979;&#19968;&#30452;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20854;&#21160;&#24577;&#26102;&#38388;&#20381;&#36182;&#24615;&#21644;&#22797;&#26434;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#28151;&#21512;Transformer&#21644;&#26102;&#31354;&#33258;&#30417;&#30563;&#23398;&#20064;&#30456;&#32467;&#21512;&#30340;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#22312;&#20132;&#36890;&#25968;&#25454;&#30340;&#24207;&#21015;&#32423;&#21644;&#22270;&#32423;&#24212;&#29992;&#33258;&#36866;&#24212;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#22686;&#24378;&#20102;&#20854;&#40065;&#26834;&#24615;&#12290;&#23427;&#21033;&#29992;Transformer&#20811;&#26381;&#20102;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#22312;&#25429;&#25417;&#38271;&#26399;&#24207;&#21015;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#37319;&#29992;Chebyshev&#22810;&#39033;&#24335;&#22270;&#21367;&#31215;&#26469;&#25429;&#25417;&#22797;&#26434;&#30340;&#31354;&#38388;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#21040;&#26102;&#31354;&#24322;&#36136;&#24615;&#23545;&#20132;&#36890;&#36895;&#24230;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#20219;&#21153;&#26469;&#24314;&#27169;&#26102;&#31354;&#24322;&#36136;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#20004;&#20010;&#23454;&#38469;&#25968;&#25454;&#38598;PeMS04&#21644;PeMS08&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#24182;&#36827;&#34892;&#20102;&#21487;&#35270;&#21270;&#21644;&#20998;&#26512;&#65292;&#23637;&#31034;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term traffic prediction has always been a challenging task due to its dynamic temporal dependencies and complex spatial dependencies. In this paper, we propose a model that combines hybrid Transformer and spatio-temporal self-supervised learning. The model enhances its robustness by applying adaptive data augmentation techniques at the sequence-level and graph-level of the traffic data. It utilizes Transformer to overcome the limitations of recurrent neural networks in capturing long-term sequences, and employs Chebyshev polynomial graph convolution to capture complex spatial dependencies. Furthermore, considering the impact of spatio-temporal heterogeneity on traffic speed, we design two self-supervised learning tasks to model the temporal and spatial heterogeneity, thereby improving the accuracy and generalization ability of the model. Experimental evaluations are conducted on two real-world datasets, PeMS04 and PeMS08, and the results are visualized and analyzed, demonstrating 
&lt;/p&gt;</description></item><item><title>Context-Former&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#24182;&#25552;&#39640;&#20102;Decision Transformer&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16452</link><description>&lt;p&gt;
Context-Former&#65306;&#22522;&#20110;&#28508;&#22312;&#26465;&#20214;&#24207;&#21015;&#24314;&#27169;&#30340;&#25340;&#25509;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Context-Former: Stitching via Latent Conditioned Sequence Modeling. (arXiv:2401.16452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16452
&lt;/p&gt;
&lt;p&gt;
Context-Former&#26159;&#19968;&#31181;&#38598;&#25104;&#20102;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#21644;&#24207;&#21015;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#24182;&#25552;&#39640;&#20102;Decision Transformer&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#21487;&#20197;&#36890;&#36807;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#26469;&#25913;&#21892;&#20915;&#31574;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20248;&#30340;&#32467;&#26524;&#12290;&#36825;&#31181;&#33021;&#21147;&#26159;&#20351;RL&#33021;&#22815;&#23398;&#20064;&#20248;&#20110;&#34892;&#20026;&#31574;&#30053;&#30340;&#31574;&#30053;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;Decision Transformer&#65288;DT&#65289;&#23558;&#20915;&#31574;&#24314;&#27169;&#20026;&#24207;&#21015;&#24314;&#27169;&#65292;&#23637;&#31034;&#20102;&#22312;&#31163;&#32447;RL&#22522;&#20934;&#27979;&#35797;&#20013;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;DT&#32570;&#20047;&#25340;&#25509;&#33021;&#21147;&#65292;&#22240;&#27492;&#25552;&#39640;DT&#24615;&#33021;&#38656;&#35201;&#21033;&#29992;&#25340;&#25509;&#33021;&#21147;&#12290;&#20026;&#20102;&#36171;&#20104;DT&#25340;&#25509;&#33021;&#21147;&#65292;&#25105;&#20204;&#23558;&#36712;&#36857;&#25340;&#25509;&#25277;&#35937;&#20026;&#19987;&#23478;&#21305;&#37197;&#65292;&#24182;&#24341;&#20837;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;ContextFormer&#65292;&#36890;&#36807;&#27169;&#25311;&#26377;&#38480;&#25968;&#37327;&#30340;&#19987;&#23478;&#36712;&#36857;&#30340;&#34920;&#31034;&#26469;&#38598;&#25104;&#22522;&#20110;&#24773;&#22659;&#20449;&#24687;&#30340;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#21644;&#24207;&#21015;&#24314;&#27169;&#65292;&#20197;&#25340;&#25509;&#27425;&#20248;&#36712;&#36857;&#29255;&#27573;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#35266;&#28857;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#36827;&#34892;&#23454;&#39564;&#35777;&#26126;&#65306;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) algorithms can improve the decision making via stitching sub-optimal trajectories to obtain more optimal ones. This capability is a crucial factor in enabling RL to learn policies that are superior to the behavioral policy. On the other hand, Decision Transformer (DT) abstracts the decision-making as sequence modeling, showcasing competitive performance on offline RL benchmarks, however, recent studies demonstrate that DT lacks of stitching capability, thus exploit stitching capability for DT is vital to further improve its performance. In order to endow stitching capability to DT, we abstract trajectory stitching as expert matching and introduce our approach, ContextFormer, which integrates contextual information-based imitation learning (IL) and sequence modeling to stitch sub-optimal trajectory fragments by emulating the representations of a limited number of expert trajectories. To validate our claim, we conduct experiments from two perspectives:
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20462;&#22797;&#32593;&#39029;&#26080;&#38556;&#30861;&#36829;&#35268;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#26102;&#20462;&#25913;&#25991;&#26723;&#23545;&#35937;&#27169;&#22411;(DOM)&#21644;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#21450;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#20462;&#22797;&#26080;&#38556;&#30861;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16450</link><description>&lt;p&gt;
ACCESS&#65306;&#29992;&#20110;&#33258;&#21160;&#20462;&#22797;&#32593;&#39029;&#26080;&#38556;&#30861;&#36829;&#35268;&#30340;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
ACCESS: Prompt Engineering for Automated Web Accessibility Violation Corrections. (arXiv:2401.16450v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16450
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20462;&#22797;&#32593;&#39029;&#26080;&#38556;&#30861;&#36829;&#35268;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23454;&#26102;&#20462;&#25913;&#25991;&#26723;&#23545;&#35937;&#27169;&#22411;(DOM)&#21644;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20197;&#21450;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#20462;&#22797;&#26080;&#38556;&#30861;&#38169;&#35823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#21253;&#23481;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#25216;&#26415;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#65292;&#32593;&#39029;&#26080;&#38556;&#30861;&#23545;&#20110;&#30830;&#20445;&#27531;&#38556;&#20154;&#22763;(&#21253;&#25324;&#35270;&#35273;&#12289;&#21548;&#35273;&#12289;&#35748;&#30693;&#25110;&#36816;&#21160;&#38556;&#30861;)&#24179;&#31561;&#33719;&#21462;&#22312;&#32447;&#20869;&#23481;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#23384;&#22312;&#35832;&#22914;Web&#20869;&#23481;&#26080;&#38556;&#30861;&#25351;&#21335;(WCAG)&#21644;Web&#26080;&#38556;&#30861;&#20513;&#35758;(W3C)&#31561;&#26080;&#38556;&#30861;&#25351;&#23548;&#26041;&#38024;&#21644;&#26631;&#20934;&#65292;&#20294;&#36229;&#36807;90%&#30340;&#32593;&#31449;&#20173;&#26080;&#27861;&#28385;&#36275;&#24517;&#35201;&#30340;&#26080;&#38556;&#30861;&#35201;&#27714;&#12290;&#23545;&#20110;&#27531;&#38556;&#20154;&#22763;&#26469;&#35828;&#65292;&#38656;&#35201;&#19968;&#31181;&#24037;&#20855;&#26469;&#33258;&#21160;&#20462;&#22797;&#32593;&#39029;&#30340;&#26080;&#38556;&#30861;&#38169;&#35823;&#12290;&#34429;&#28982;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#21457;&#29616;&#21644;&#23450;&#20301;&#26080;&#38556;&#30861;&#38169;&#35823;&#30340;&#26041;&#27861;&#65292;&#20294;&#27809;&#26377;&#30740;&#31350;&#19987;&#27880;&#20110;&#26377;&#25928;&#20462;&#22797;&#27492;&#31867;&#36829;&#35268;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23454;&#26102;&#20462;&#25913;&#25991;&#26723;&#23545;&#35937;&#27169;&#22411;(DOM)&#26469;&#20462;&#22797;&#32593;&#39029;&#26080;&#38556;&#30861;&#36829;&#35268;&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#26080;&#38556;&#30861;&#38169;&#35823;&#20449;&#24687;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#21644;&#25552;&#31034;&#24037;&#31243;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increasing need for inclusive and user-friendly technology, web accessibility is crucial to ensuring equal access to online content for individuals with disabilities, including visual, auditory, cognitive, or motor impairments. Despite the existence of accessibility guidelines and standards such as Web Content Accessibility Guidelines (WCAG) and the Web Accessibility Initiative (W3C), over 90\% of websites still fail to meet the necessary accessibility requirements. For web users with disabilities, there exists a need for a tool to automatically fix web page accessibility errors. While research has demonstrated methods to find and target accessibility errors, no research has focused on effectively correcting such violations. This paper presents a novel approach to correcting accessibility violations on the web by modifying the document object model (DOM) in real time with foundation models. Leveraging accessibility error information, large language models (LLMs), and prompt en
&lt;/p&gt;</description></item><item><title>LLM4SecHW&#26159;&#19968;&#31181;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30828;&#20214;&#35843;&#35797;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#21046;&#19968;&#20010;&#24320;&#28304;&#30828;&#20214;&#35774;&#35745;&#32570;&#38519;&#21450;&#20854;&#32416;&#27491;&#27493;&#39588;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20013;&#22411;LLM&#30340;&#31934;&#32454;&#35843;&#20248;&#65292;&#23454;&#29616;&#23545;&#30828;&#20214;&#35774;&#35745;&#20013;&#30340;&#38169;&#35823;&#36827;&#34892;&#35782;&#21035;&#21644;&#32416;&#27491;&#12290;&#36825;&#20026;&#22312;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#20013;&#24212;&#29992;&#31934;&#32454;&#35843;&#20248;&#39046;&#22495;&#29305;&#23450;LLM&#25552;&#20379;&#20102;&#21442;&#32771;&#24037;&#20316;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2401.16448</link><description>&lt;p&gt;
LLM4SecHW: &#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30828;&#20214;&#35843;&#35797;
&lt;/p&gt;
&lt;p&gt;
LLM4SecHW: Leveraging Domain Specific Large Language Model for Hardware Debugging. (arXiv:2401.16448v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16448
&lt;/p&gt;
&lt;p&gt;
LLM4SecHW&#26159;&#19968;&#31181;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30828;&#20214;&#35843;&#35797;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#32534;&#21046;&#19968;&#20010;&#24320;&#28304;&#30828;&#20214;&#35774;&#35745;&#32570;&#38519;&#21450;&#20854;&#32416;&#27491;&#27493;&#39588;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#20013;&#22411;LLM&#30340;&#31934;&#32454;&#35843;&#20248;&#65292;&#23454;&#29616;&#23545;&#30828;&#20214;&#35774;&#35745;&#20013;&#30340;&#38169;&#35823;&#36827;&#34892;&#35782;&#21035;&#21644;&#32416;&#27491;&#12290;&#36825;&#20026;&#22312;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#20013;&#24212;&#29992;&#31934;&#32454;&#35843;&#20248;&#39046;&#22495;&#29305;&#23450;LLM&#25552;&#20379;&#20102;&#21442;&#32771;&#24037;&#20316;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;LLM4SecHW&#65292;&#19968;&#31181;&#21033;&#29992;&#39046;&#22495;&#29305;&#23450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#30828;&#20214;&#35843;&#35797;&#30340;&#26032;&#26694;&#26550;&#12290;&#34429;&#28982;LLM&#22312;&#33258;&#21160;&#21270;&#21508;&#31181;&#36719;&#20214;&#24320;&#21457;&#20219;&#21153;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#30828;&#20214;&#23433;&#20840;&#39046;&#22495;&#30340;&#24212;&#29992;&#21463;&#21040;&#21830;&#19994;LLM&#30340;&#38480;&#21046;&#21644;&#39046;&#22495;&#29305;&#23450;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#30340;&#24433;&#21709;&#36739;&#22823;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#29256;&#26412;&#25511;&#21046;&#25968;&#25454;&#32534;&#21046;&#20102;&#19968;&#20010;&#24320;&#28304;&#30828;&#20214;&#35774;&#35745;&#32570;&#38519;&#21450;&#20854;&#32416;&#27491;&#27493;&#39588;&#30340;&#25968;&#25454;&#38598;&#12290;&#35813;&#25968;&#25454;&#38598;&#20026;&#35757;&#32451;&#30828;&#20214;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#22362;&#23454;&#30340;&#22522;&#30784;&#12290;LLM4SecHW&#22522;&#20110;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#20013;&#22411;LLM&#30340;&#31934;&#32454;&#35843;&#20248;&#65292;&#23454;&#29616;&#20102;&#23545;&#30828;&#20214;&#35774;&#35745;&#20013;&#30340;&#38169;&#35823;&#36827;&#34892;&#35782;&#21035;&#21644;&#32416;&#27491;&#12290;&#36825;&#31181;&#24320;&#21019;&#24615;&#30340;&#26041;&#27861;&#20026;&#22312;&#20854;&#20182;&#30740;&#31350;&#39046;&#22495;&#20013;&#24212;&#29992;&#31934;&#32454;&#35843;&#20248;&#39046;&#22495;&#29305;&#23450;LLM&#25552;&#20379;&#20102;&#21442;&#32771;&#24037;&#20316;&#27969;&#31243;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#25552;&#20986;&#30340;&#31995;&#32479;&#22312;&#22810;&#31181;&#24320;&#28304;&#30828;&#20214;&#19978;&#36827;&#34892;&#20102;&#24615;&#33021;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents LLM4SecHW, a novel framework for hardware debugging that leverages domain specific Large Language Model (LLM). Despite the success of LLMs in automating various software development tasks, their application in the hardware security domain has been limited due to the constraints of commercial LLMs and the scarcity of domain specific data. To address these challenges, we propose a unique approach to compile a dataset of open source hardware design defects and their remediation steps, utilizing version control data. This dataset provides a substantial foundation for training machine learning models for hardware. LLM4SecHW employs fine tuning of medium sized LLMs based on this dataset, enabling the identification and rectification of bugs in hardware designs. This pioneering approach offers a reference workflow for the application of fine tuning domain specific LLMs in other research areas. We evaluate the performance of our proposed system on various open source hardwa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21327;&#20316;&#20195;&#29702;&#20154;&#24314;&#27169;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#20154;&#31867;&#30340;&#20307;&#39564;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20154;&#31867;&#25910;&#30410;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20195;&#29702;&#20154;&#21487;&#20197;&#22312;&#20445;&#25345;&#33258;&#36523;&#33021;&#21147;&#30340;&#21516;&#26102;&#65292;&#22686;&#24378;&#20154;&#31867;&#23454;&#29616;&#39044;&#26399;&#30446;&#26631;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.16444</link><description>&lt;p&gt;
&#25552;&#21319;&#20154;&#26426;&#21327;&#20316;&#20013;&#30340;&#20154;&#31867;&#20307;&#39564;&#65306;&#22522;&#20110;&#31215;&#26497;&#20154;&#31867;&#25910;&#30410;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#24314;&#27169;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Enhancing Human Experience in Human-Agent Collaboration: A Human-Centered Modeling Approach Based on Positive Human Gain. (arXiv:2401.16444v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#21327;&#20316;&#20195;&#29702;&#20154;&#24314;&#27169;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#20154;&#31867;&#30340;&#20307;&#39564;&#12290;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#20154;&#31867;&#25910;&#30410;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20195;&#29702;&#20154;&#21487;&#20197;&#22312;&#20445;&#25345;&#33258;&#36523;&#33021;&#21147;&#30340;&#21516;&#26102;&#65292;&#22686;&#24378;&#20154;&#31867;&#23454;&#29616;&#39044;&#26399;&#30446;&#26631;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#25552;&#21319;&#20195;&#29702;&#20154;&#22312;&#28216;&#25103;&#20013;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#24182;&#19981;&#33021;&#26412;&#36136;&#19978;&#35753;&#20154;&#20204;&#22312;&#19982;&#36825;&#20123;&#20195;&#29702;&#20154;&#21327;&#20316;&#26102;&#25317;&#26377;&#26356;&#22909;&#30340;&#20307;&#39564;&#12290;&#20363;&#22914;&#65292;&#20195;&#29702;&#20154;&#21487;&#33021;&#20250;&#25511;&#21046;&#21327;&#20316;&#24182;&#23637;&#31034;&#24847;&#22806;&#25110;&#26377;&#23475;&#30340;&#34892;&#20026;&#65292;&#23548;&#33268;&#20154;&#31867;&#21512;&#20316;&#20249;&#20276;&#30340;&#20307;&#39564;&#19981;&#20339;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#22823;&#22810;&#25968;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20154;&#26159;&#20197;&#8220;&#33258;&#25105;&#20026;&#20013;&#24515;&#8221;&#24314;&#27169;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20197;&#20154;&#20026;&#20013;&#24515;&#8221;&#30340;&#21327;&#20316;&#20195;&#29702;&#20154;&#24314;&#27169;&#26041;&#26696;&#65292;&#26088;&#22312;&#22686;&#24378;&#20154;&#31867;&#30340;&#20307;&#39564;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#20154;&#31867;&#30340;&#20307;&#39564;&#24314;&#27169;&#20026;&#20182;&#20204;&#22312;&#20219;&#21153;&#26399;&#26395;&#36798;&#21040;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#26399;&#26395;&#20195;&#29702;&#20154;&#33021;&#22815;&#23398;&#20250;&#22312;&#20445;&#25345;&#20854;&#21407;&#22987;&#33021;&#21147;&#65288;&#22914;&#22312;&#28216;&#25103;&#20013;&#33719;&#32988;&#65289;&#30340;&#21516;&#26102;&#65292;&#22686;&#24378;&#20154;&#31867;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#30340;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20154;&#31867;&#25910;&#30410;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHG&#65289;&#26041;&#27861;&#12290;RLHG&#26041;&#27861;&#24341;&#20837;&#20102;&#19968;&#20010;&#8220;&#22522;&#20934;&#8221;&#65292;&#23545;&#24212;&#20110;&#20154;&#31867;&#26368;&#21021;&#39044;&#26399;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing game AI research mainly focuses on enhancing agents' abilities to win games, but this does not inherently make humans have a better experience when collaborating with these agents. For example, agents may dominate the collaboration and exhibit unintended or detrimental behaviors, leading to poor experiences for their human partners. In other words, most game AI agents are modeled in a "self-centered" manner. In this paper, we propose a "human-centered" modeling scheme for collaborative agents that aims to enhance the experience of humans. Specifically, we model the experience of humans as the goals they expect to achieve during the task. We expect that agents should learn to enhance the extent to which humans achieve these goals while maintaining agents' original abilities (e.g., winning games). To achieve this, we propose the Reinforcement Learning from Human Gain (RLHG) approach. The RLHG approach introduces a "baseline", which corresponds to the extent to which humans primi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#20998;&#31867;&#22120;&#21644;&#25163;&#37096;&#36861;&#36394;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#29992;&#25143;&#23545;&#34394;&#25311;&#29616;&#23454;&#29087;&#24713;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#22312;&#29992;&#25143;&#19981;&#29087;&#24713;&#34394;&#25311;&#29616;&#23454;&#26102;&#20026;&#20854;&#25552;&#20379;&#25353;&#38656;&#22521;&#35757;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#23436;&#25104;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.16443</link><description>&lt;p&gt;
&#35780;&#20272;&#28145;&#24230;&#32593;&#32476;&#29992;&#20110;&#36890;&#36807;&#25163;&#37096;&#20132;&#20114;&#26816;&#27979;&#29992;&#25143;&#23545;&#34394;&#25311;&#29616;&#23454;&#30340;&#29087;&#24713;&#31243;&#24230;
&lt;/p&gt;
&lt;p&gt;
Evaluating Deep Networks for Detecting User Familiarity with VR from Hand Interactions. (arXiv:2401.16443v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16443
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#20998;&#31867;&#22120;&#21644;&#25163;&#37096;&#36861;&#36394;&#25216;&#26415;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#29992;&#25143;&#23545;&#34394;&#25311;&#29616;&#23454;&#29087;&#24713;&#31243;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20415;&#22312;&#29992;&#25143;&#19981;&#29087;&#24713;&#34394;&#25311;&#29616;&#23454;&#26102;&#20026;&#20854;&#25552;&#20379;&#25353;&#38656;&#22521;&#35757;&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#22312;&#34394;&#25311;&#29615;&#22659;&#20013;&#30340;&#20219;&#21153;&#23436;&#25104;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#34394;&#25311;&#29616;&#23454;&#35774;&#22791;&#22312;&#28040;&#36153;&#39046;&#22495;&#30340;&#26222;&#21450;&#65292;&#23545;&#20110;&#23545;&#34394;&#25311;&#29616;&#23454;&#19981;&#29087;&#24713;&#30340;&#29992;&#25143;&#32780;&#35328;&#65292;&#34394;&#25311;&#29616;&#23454;&#24212;&#29992;&#30340;&#20351;&#29992;&#21487;&#33021;&#36234;&#26469;&#36234;&#26222;&#36941;&#12290;&#26816;&#27979;&#29992;&#25143;&#23545;&#34394;&#25311;&#29616;&#23454;&#30340;&#29087;&#24713;&#31243;&#24230;&#20316;&#20026;&#20132;&#20114;&#23186;&#20171;&#65292;&#20855;&#26377;&#36890;&#36807;&#25552;&#20379;&#25353;&#38656;&#22521;&#35757;&#36827;&#34892;&#36866;&#24212;&#21644;&#38450;&#27490;&#29992;&#25143;&#22312;&#23436;&#25104;&#20219;&#21153;&#26102;&#34987;&#34394;&#25311;&#29616;&#23454;&#29615;&#22659;&#25152;&#25302;&#32047;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;&#28145;&#24230;&#20998;&#31867;&#22120;&#36827;&#34892;&#33258;&#21160;&#26816;&#27979;&#29992;&#25143;&#23545;&#34394;&#25311;&#29616;&#23454;&#30340;&#29087;&#24713;&#31243;&#24230;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#36890;&#36807;&#29992;&#25143;&#20351;&#29992;&#25163;&#37096;&#19982;&#34394;&#25311;&#29616;&#23454;&#38376;&#38145;&#25968;&#23383;&#23494;&#30721;&#36755;&#20837;&#38754;&#26495;&#36827;&#34892;&#20132;&#20114;&#26469;&#35299;&#38145;&#34394;&#25311;&#29616;&#23454;&#38376;&#12290;&#25105;&#20204;&#23558;&#34394;&#25311;&#29616;&#23454;&#38376;&#20316;&#20026;&#20225;&#19994;&#34394;&#25311;&#31354;&#38388;&#30340;&#31532;&#19968;&#20837;&#21475;&#28857;&#65292;&#20363;&#22914;&#20250;&#35758;&#23460;&#12289;&#21150;&#20844;&#23460;&#25110;&#35786;&#25152;&#12290;&#23545;&#20110;&#19981;&#29087;&#24713;&#34394;&#25311;&#29616;&#23454;&#30340;&#29992;&#25143;&#32780;&#35328;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24050;&#32463;&#20351;&#29992;&#36807;&#25163;&#37096;&#25171;&#24320;&#24102;&#26377;&#23494;&#30721;&#36755;&#20837;&#38754;&#26495;&#30340;&#38376;&#12290;&#22240;&#27492;&#65292;&#34429;&#28982;&#29992;&#25143;&#21487;&#33021;&#23545;&#34394;&#25311;&#29616;&#23454;&#19981;&#29087;&#24713;&#65292;&#20294;&#20182;&#20204;&#23545;&#25171;&#24320;&#38376;&#30340;&#20219;&#21153;&#24212;&#35813;&#26159;&#29087;&#24713;&#30340;&#12290;&#20351;&#29992; pilot d
&lt;/p&gt;
&lt;p&gt;
As VR devices become more prevalent in the consumer space, VR applications are likely to be increasingly used by users unfamiliar with VR. Detecting the familiarity level of a user with VR as an interaction medium provides the potential of providing on-demand training for acclimatization and prevents the user from being burdened by the VR environment in accomplishing their tasks. In this work, we present preliminary results of using deep classifiers to conduct automatic detection of familiarity with VR by using hand tracking of the user as they interact with a numeric passcode entry panel to unlock a VR door. We use a VR door as we envision it to the first point of entry to collaborative virtual spaces, such as meeting rooms, offices, or clinics. Users who are unfamiliar with VR will have used their hands to open doors with passcode entry panels in the real world. Thus, while the user may not be familiar with VR, they would be familiar with the task of opening the door. Using a pilot d
&lt;/p&gt;</description></item><item><title>FaKnow&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31639;&#27861;&#24211;&#65292;&#21253;&#21547;&#22810;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;&#21644;&#24037;&#20855;&#65292;&#24182;&#35299;&#20915;&#20102;&#19981;&#21516;&#26694;&#26550;&#19979;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#20887;&#20313;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2401.16441</link><description>&lt;p&gt;
FaKnow: &#19968;&#20010;&#29992;&#20110;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#32479;&#19968;&#24211;
&lt;/p&gt;
&lt;p&gt;
FaKnow: A Unified Library for Fake News Detection. (arXiv:2401.16441v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16441
&lt;/p&gt;
&lt;p&gt;
FaKnow&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31639;&#27861;&#24211;&#65292;&#21253;&#21547;&#22810;&#31181;&#24120;&#29992;&#30340;&#27169;&#22411;&#21644;&#24037;&#20855;&#65292;&#24182;&#35299;&#20915;&#20102;&#19981;&#21516;&#26694;&#26550;&#19979;&#30340;&#21487;&#37325;&#22797;&#24615;&#21644;&#20887;&#20313;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22823;&#37327;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31639;&#27861;&#24212;&#36816;&#32780;&#29983;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24448;&#24448;&#22312;&#19981;&#21516;&#30340;&#26694;&#26550;&#19979;&#24320;&#21457;&#65292;&#27599;&#20010;&#26694;&#26550;&#21448;&#35201;&#27714;&#20351;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#22240;&#27492;&#38459;&#30861;&#20102;&#21487;&#37325;&#22797;&#24615;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#30340;&#20195;&#30721;&#24320;&#21457;&#20013;&#23384;&#22312;&#22823;&#37327;&#30340;&#20887;&#20313;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FaKnow&#65292;&#19968;&#20010;&#32479;&#19968;&#19988;&#20840;&#38754;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#31639;&#27861;&#24211;&#12290;&#23427;&#28085;&#30422;&#20102;&#22810;&#31181;&#24120;&#29992;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#65292;&#21253;&#25324;&#22522;&#20110;&#20869;&#23481;&#21644;&#22522;&#20110;&#31038;&#20250;&#29615;&#22659;&#30340;&#26041;&#27861;&#12290;&#35813;&#24211;&#28085;&#30422;&#20102;&#27169;&#22411;&#35757;&#32451;&#21644;&#35780;&#20272;&#27969;&#31243;&#30340;&#23436;&#25972;&#33539;&#22260;&#65292;&#22312;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20869;&#26377;&#25928;&#32452;&#32455;&#20102;&#25968;&#25454;&#12289;&#27169;&#22411;&#21644;&#35757;&#32451;&#31243;&#24207;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#36741;&#21161;&#21151;&#33021;&#21644;&#24037;&#20855;&#65292;&#21253;&#25324;&#21487;&#35270;&#21270;&#21644;&#26085;&#24535;&#35760;&#24405;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#26631;&#20934;&#21270;&#21644;&#32479;&#19968;&#21270;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past years, a large number of fake news detection algorithms based on deep learning have emerged. However, they are often developed under different frameworks, each mandating distinct utilization methodologies, consequently hindering reproducibility. Additionally, a substantial amount of redundancy characterizes the code development of such fake news detection models. To address these concerns, we propose FaKnow, a unified and comprehensive fake news detection algorithm library. It encompasses a variety of widely used fake news detection models, categorized as content-based and social context-based approaches. This library covers the full spectrum of the model training and evaluation process, effectively organizing the data, models, and training procedures within a unified framework. Furthermore, it furnishes a series of auxiliary functionalities and tools, including visualization, and logging. Our work contributes to the standardization and unification of fake news detection 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#24403;&#22320;&#26102;&#31354;&#20844;&#20849;&#35760;&#24405;&#26469;&#39044;&#27979;&#39537;&#36880;&#39118;&#38505;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#39044;&#27979;&#23545;&#20110;&#25351;&#23548;&#26377;&#38024;&#23545;&#24615;&#30340;&#22806;&#23637;&#25919;&#31574;&#26159;&#26377;&#29992;&#30340;&#12290;</title><link>http://arxiv.org/abs/2401.16440</link><description>&lt;p&gt;
&#36229;&#36234;&#39537;&#36880;&#39044;&#27979;&#65306;&#21033;&#29992;&#24403;&#22320;&#26102;&#31354;&#20844;&#20849;&#35760;&#24405;&#26469;&#25351;&#23548;&#34892;&#21160;
&lt;/p&gt;
&lt;p&gt;
Beyond Eviction Prediction: Leveraging Local Spatiotemporal Public Records to Inform Action. (arXiv:2401.16440v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16440
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#24403;&#22320;&#26102;&#31354;&#20844;&#20849;&#35760;&#24405;&#26469;&#39044;&#27979;&#39537;&#36880;&#39118;&#38505;&#65292;&#24182;&#35777;&#26126;&#36825;&#20123;&#39044;&#27979;&#23545;&#20110;&#25351;&#23548;&#26377;&#38024;&#23545;&#24615;&#30340;&#22806;&#23637;&#25919;&#31574;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#39537;&#36880;&#39118;&#38505;&#23545;&#25151;&#20135;&#36827;&#34892;&#35780;&#20998;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#39537;&#36880;&#39044;&#27979;&#26041;&#27861;&#30340;&#25104;&#21151;&#36890;&#24120;&#26159;&#36890;&#36807;&#19981;&#21516;&#30340;&#39044;&#27979;&#20934;&#30830;&#24230;&#25351;&#26631;&#26469;&#35780;&#20272;&#30340;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#39044;&#27979;&#30340;&#26681;&#26412;&#30446;&#26631;&#26159;&#20026;&#20102;&#21521;&#21487;&#33021;&#38754;&#20020;&#26356;&#22823;&#39118;&#38505;&#30340;&#23478;&#24237;&#25552;&#20379;&#36866;&#24403;&#30340;&#24110;&#21161;&#65292;&#20197;&#20445;&#25345;&#20303;&#25151;&#31283;&#23450;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24517;&#39035;&#38382;&#19968;&#20010;&#38382;&#39064;&#65292;&#37027;&#23601;&#26159;&#36825;&#26679;&#30340;&#39044;&#27979;&#22312;&#25351;&#23548;&#22806;&#23637;&#34892;&#21160;&#26041;&#38754;&#26377;&#22810;&#22823;&#30340;&#29992;&#22788;&#12290;&#26412;&#25991;&#21033;&#29992;&#19968;&#20010;&#26032;&#39062;&#30340;&#25968;&#25454;&#38598;&#65292;&#23558;&#25151;&#20135;&#12289;&#39537;&#36880;&#21644;&#19994;&#20027;&#30340;&#20449;&#24687;&#36827;&#34892;&#21305;&#37197;&#65292;&#30740;&#31350;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#39537;&#36880;&#39044;&#27979;&#20219;&#21153;&#65292;&#29983;&#25104;&#39118;&#38505;&#24471;&#20998;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#39118;&#38505;&#24471;&#20998;&#26469;&#35268;&#21010;&#26377;&#38024;&#23545;&#24615;&#30340;&#22806;&#23637;&#25919;&#31574;&#12290;&#25105;&#20204;&#26174;&#31034;&#36825;&#20123;&#39118;&#38505;&#24471;&#20998;&#23454;&#38469;&#19978;&#26159;&#26377;&#29992;&#30340;&#65292;&#33021;&#22815;&#20351;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#24037;&#20316;&#20154;&#21592;&#22242;&#38431;&#22312;&#30456;&#21516;&#30340;&#26102;&#38388;&#20869;&#25509;&#35302;&#26356;&#22810;&#23481;&#26131;&#21457;&#29983;&#39537;&#36880;&#30340;&#25151;&#20135;&#65292;&#30456;&#27604;&#20110;&#20197;&#34903;&#21306;&#20026;&#22522;&#30784;&#25110;&#20851;&#27880;&#29305;&#23450;&#24314;&#31569;&#29289;&#30340;&#22806;&#23637;&#25919;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
There has been considerable recent interest in scoring properties on the basis of eviction risk. The success of methods for eviction prediction is typically evaluated using different measures of predictive accuracy. However, the underlying goal of such prediction is to direct appropriate assistance to households that may be at greater risk so they remain stably housed. Thus, we must ask the question of how useful such predictions are in targeting outreach efforts - informing action. In this paper, we investigate this question using a novel dataset that matches information on properties, evictions, and owners. We perform an eviction prediction task to produce risk scores and then use these risk scores to plan targeted outreach policies. We show that the risk scores are, in fact, useful, enabling a theoretical team of caseworkers to reach more eviction-prone properties in the same amount of time, compared to outreach policies that are either neighborhood-based or focus on buildings with 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26435;&#37325;&#30697;&#38453;&#30340;&#21015;&#31354;&#38388;&#21644;&#34892;&#31354;&#38388;&#30340;&#26032;&#27010;&#24565;&#65292;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.16438</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#21542;&#39640;&#25928;&#21033;&#29992;&#20102;&#26435;&#37325;&#31354;&#38388;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do deep neural networks utilize the weight space efficiently?. (arXiv:2401.16438v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16438
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#26435;&#37325;&#30697;&#38453;&#30340;&#21015;&#31354;&#38388;&#21644;&#34892;&#31354;&#38388;&#30340;&#26032;&#27010;&#24565;&#65292;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21442;&#25968;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22914;Transformer&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#24050;&#32463;&#22312;&#21508;&#20010;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#20294;&#26159;&#23427;&#20204;&#21442;&#25968;&#23494;&#38598;&#30340;&#29305;&#24615;&#38480;&#21046;&#20102;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#31181;&#26032;&#30340;&#27010;&#24565;&#65292;&#21033;&#29992;&#26435;&#37325;&#30697;&#38453;&#30340;&#21015;&#31354;&#38388;&#21644;&#34892;&#31354;&#38388;&#65292;&#21487;&#20197;&#22823;&#24133;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#32780;&#19981;&#24433;&#21709;&#24615;&#33021;&#12290;&#21033;&#29992;&#36825;&#31181;&#33539;&#24335;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#21442;&#25968;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#29942;&#39048;&#23618;&#21644;&#27880;&#24847;&#21147;&#23618;&#65292;&#21487;&#20197;&#23558;&#21442;&#25968;&#20943;&#21322;&#65292;&#20165;&#24102;&#26469;&#36731;&#24494;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#25105;&#20204;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;ViT&#21644;ResNet50&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#19982;&#20256;&#32479;&#27169;&#22411;&#30340;&#27604;&#36739;&#20013;&#23637;&#31034;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#35299;&#20915;&#20102;&#23545;&#21442;&#25968;&#39640;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#30340;&#32039;&#36843;&#38656;&#27714;&#65292;&#32780;&#19988;&#22312;&#30495;&#23454;&#22330;&#26223;&#30340;&#23454;&#38469;&#37096;&#32626;&#20013;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models like Transformers and Convolutional Neural Networks (CNNs) have revolutionized various domains, but their parameter-intensive nature hampers deployment in resource-constrained settings. In this paper, we introduce a novel concept utilizes column space and row space of weight matrices, which allows for a substantial reduction in model parameters without compromising performance. Leveraging this paradigm, we achieve parameter-efficient deep learning models.. Our approach applies to both Bottleneck and Attention layers, effectively halving the parameters while incurring only minor performance degradation. Extensive experiments conducted on the ImageNet dataset with ViT and ResNet50 demonstrate the effectiveness of our method, showcasing competitive performance when compared to traditional models. This approach not only addresses the pressing demand for parameter efficient deep learning solutions but also holds great promise for practical deployment in real-world scena
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;rPDP-FL&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#21151;&#33021;&#35299;&#20915;&#26041;&#26696;&#8220;&#27169;&#25311;-&#26354;&#32447;&#25311;&#21512;&#8221;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#35760;&#24405;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2401.16251</link><description>&lt;p&gt;
&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Cross-silo Federated Learning with Record-level Personalized Differential Privacy. (arXiv:2401.16251v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.16251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;rPDP-FL&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#21151;&#33021;&#35299;&#20915;&#26041;&#26696;&#8220;&#27169;&#25311;-&#26354;&#32447;&#25311;&#21512;&#8221;&#65292;&#20197;&#28385;&#36275;&#19981;&#21516;&#35760;&#24405;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24046;&#20998;&#38544;&#31169;&#22686;&#24378;&#30340;&#32852;&#21512;&#23398;&#20064;&#25104;&#20026;&#20102;&#20445;&#25252;&#23458;&#25143;&#31471;&#25968;&#25454;&#38544;&#31169;&#30340;&#24120;&#29992;&#26041;&#27861;&#65292;&#20294;&#29616;&#26377;&#26041;&#26696;&#36890;&#24120;&#20551;&#35774;&#25152;&#26377;&#35760;&#24405;&#30340;&#38544;&#31169;&#39044;&#31639;&#22343;&#30456;&#21516;&#65292;&#25552;&#20379;&#19968;&#31181;&#36866;&#29992;&#20110;&#25152;&#26377;&#35760;&#24405;&#30340;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#33021;&#26080;&#27861;&#28385;&#36275;&#27599;&#20010;&#35760;&#24405;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#36328;&#39046;&#22495;&#32852;&#21512;&#23398;&#20064;&#20013;&#22522;&#20110;&#35760;&#24405;&#32423;&#20010;&#24615;&#21270;&#24046;&#20998;&#38544;&#31169;&#30340;&#26410;&#30693;&#39046;&#22495;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;rPDP-FL&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#37319;&#29992;&#20004;&#38454;&#27573;&#28151;&#21512;&#25277;&#26679;&#26041;&#26696;&#65292;&#26082;&#21253;&#25324;&#23458;&#25143;&#31471;&#32423;&#21035;&#25277;&#26679;&#65292;&#21448;&#21253;&#25324;&#38750;&#22343;&#21248;&#35760;&#24405;&#32423;&#21035;&#25277;&#26679;&#65292;&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#38544;&#31169;&#38656;&#27714;&#12290;&#19968;&#20010;&#20851;&#38190;&#19988;&#38750;&#24179;&#20961;&#30340;&#38382;&#39064;&#26159;&#22312;&#32473;&#23450;&#20010;&#24615;&#21270;&#38544;&#31169;&#39044;&#31639;&#949;&#30340;&#24773;&#20917;&#19979;&#36873;&#25321;&#29702;&#24819;&#30340;&#27599;&#35760;&#24405;&#25277;&#26679;&#27010;&#29575;q&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#35299;&#20915;&#26041;&#26696;&#8220;&#27169;&#25311;-&#26354;&#32447;&#25311;&#21512;&#8221;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25581;&#31034;&#38750;&#32447;&#24615;&#30456;&#20851;&#24615;&#30340;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning enhanced by differential privacy has emerged as a popular approach to better safeguard the privacy of client-side data by protecting clients' contributions during the training process. Existing solutions typically assume a uniform privacy budget for all records and provide one-size-fits-all solutions that may not be adequate to meet each record's privacy requirement. In this paper, we explore the uncharted territory of cross-silo FL with record-level personalized differential privacy. We devise a novel framework named rPDP-FL, employing a two-stage hybrid sampling scheme with both client-level sampling and non-uniform record-level sampling to accommodate varying privacy requirements. A critical and non-trivial problem is to select the ideal per-record sampling probability q given the personalized privacy budget {\epsilon}. We introduce a versatile solution named Simulation-CurveFitting, allowing us to uncover a significant insight into the nonlinear correlation betwe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#35843;&#26597;&#20102;&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#20026;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2401.15935</link><description>&lt;p&gt;
&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#65306;&#29983;&#25104;&#24314;&#27169;&#21644;&#23545;&#27604;&#23398;&#20064;&#30340;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Learning in Event Sequences: A Comparative Study and Hybrid Approach of Generative Modeling and Contrastive Learning. (arXiv:2401.15935v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#35843;&#26597;&#20102;&#20107;&#20214;&#24207;&#21015;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#36827;&#34892;&#23545;&#40784;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#20026;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#30340;&#20449;&#24687;&#25552;&#20379;&#20102;&#28508;&#22312;&#30340;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#33719;&#21462;&#20107;&#20214;&#24207;&#21015;&#34920;&#31034;&#30340;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#12290;&#36825;&#26159;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#27169;&#24577;&#65292;&#21253;&#25324;&#20294;&#19981;&#38480;&#20110;&#38134;&#34892;&#12289;&#30005;&#23376;&#21830;&#21153;&#21644;&#21307;&#30103;&#20445;&#20581;&#12290;&#25105;&#20204;&#23545;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#30740;&#31350;&#65292;&#24182;&#20998;&#21035;&#24212;&#29992;&#20102;&#23427;&#20204;&#12290;&#25105;&#20204;&#21457;&#29616;&#27809;&#26377;&#19968;&#31181;&#32477;&#23545;&#20248;&#36234;&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#32467;&#21512;&#36825;&#20123;&#26041;&#27861;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#23884;&#20837;&#20316;&#20026;&#19981;&#21516;&#30340;&#27169;&#24577;&#36827;&#34892;&#23545;&#40784;&#65292;&#20174;&#24403;&#20195;&#22810;&#27169;&#24577;&#30740;&#31350;&#20013;&#27762;&#21462;&#28789;&#24863;&#12290;&#29983;&#25104;&#27169;&#22411;&#21644;&#23545;&#27604;&#26041;&#27861;&#36890;&#24120;&#34987;&#35270;&#20026;&#20114;&#26021;&#30340;&#65292;&#22240;&#27492;&#23384;&#22312;&#23427;&#20204;&#30340;&#32852;&#21512;&#25506;&#32034;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#23545;&#40784;&#27169;&#22411;&#22312;&#33267;&#23569;&#19982;&#29616;&#26377;&#26041;&#27861;&#25345;&#24179;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#26356;&#21152;&#26222;&#36866;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#30417;&#30563;&#23398;&#20064;&#22312;&#39044;&#27979;&#20107;&#20214;&#24207;&#21015;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates self-supervised learning techniques to obtain representations of Event Sequences. It is a key modality in various applications, including but not limited to banking, e-commerce, and healthcare.  We perform a comprehensive study of generative and contrastive approaches in self-supervised learning, applying them both independently. We find that there is no single supreme method. Consequently, we explore the potential benefits of combining these approaches. To achieve this goal, we introduce a novel method that aligns generative and contrastive embeddings as distinct modalities, drawing inspiration from contemporary multimodal research.  Generative and contrastive approaches are often treated as mutually exclusive, leaving a gap for their combined exploration. Our results demonstrate that this aligned model performs at least on par with, and mostly surpasses, existing methods and is more universal across a variety of tasks. Furthermore, we demonstrate that self-sup
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#24067;&#19968;&#33268;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;DiscoSCMs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#24314;&#27169;&#20013;&#30340;&#21453;&#20107;&#23454;&#24314;&#27169;&#25361;&#25112;&#12290;&#36825;&#31181;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#19968;&#33268;&#20551;&#35774;&#26469;&#35299;&#20915;&#22240;&#26524;&#27169;&#22411;&#30340;&#23481;&#37327;&#38480;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15911</link><description>&lt;p&gt;
&#20998;&#24067;&#19968;&#33268;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Distribution-consistency Structural Causal Models. (arXiv:2401.15911v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#24067;&#19968;&#33268;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;DiscoSCMs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#24314;&#27169;&#20013;&#30340;&#21453;&#20107;&#23454;&#24314;&#27169;&#25361;&#25112;&#12290;&#36825;&#31181;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#19968;&#33268;&#20551;&#35774;&#26469;&#35299;&#20915;&#22240;&#26524;&#27169;&#22411;&#30340;&#23481;&#37327;&#38480;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#24314;&#27169;&#39046;&#22495;&#65292;&#28508;&#22312;&#32467;&#26524;&#21644;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#26159;&#20027;&#35201;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26694;&#26550;&#22312;&#23454;&#38469;&#24314;&#27169;&#21453;&#20107;&#23454;&#24773;&#20917;&#26102;&#38754;&#20020;&#30528;&#26174;&#33879;&#25361;&#25112;&#65292;&#36825;&#20123;&#21453;&#20107;&#23454;&#24773;&#20917;&#24418;&#24335;&#21270;&#20026;&#28508;&#22312;&#32467;&#26524;&#30340;&#32852;&#21512;&#20998;&#24067;&#21442;&#25968;&#12290;&#21453;&#20107;&#23454;&#25512;&#29702;&#22312;&#24403;&#20195;&#20915;&#31574;&#36807;&#31243;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#22522;&#20110;$(Y(0),Y(1))$&#30340;&#32852;&#21512;&#20540;&#36827;&#34892;&#20010;&#24615;&#21270;&#28608;&#21169;&#30340;&#24773;&#26223;&#20013;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#28508;&#22312;&#32467;&#26524;&#21644;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#24314;&#27169;&#26694;&#26550;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#22266;&#26377;&#30340;&#27169;&#22411;&#23481;&#37327;&#38480;&#21046;&#65292;&#31216;&#20026;&#8220;&#36864;&#21270;&#30340;&#21453;&#20107;&#23454;&#38382;&#39064;&#8221;&#65292;&#36825;&#26159;&#36825;&#20004;&#20010;&#26694;&#26550;&#30340;&#22522;&#30707;&#19968;&#33268;&#24615;&#35268;&#21017;&#25152;&#23548;&#33268;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#20998;&#24067;&#19968;&#33268;&#8221;&#20551;&#35774;&#65292;&#24182;&#26681;&#25454;&#36825;&#20010;&#20551;&#35774;&#25552;&#20986;&#20102;&#20998;&#24067;&#19968;&#33268;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;DiscoSCMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of causal modeling, potential outcomes (PO) and structural causal models (SCMs) stand as the predominant frameworks. However, these frameworks face notable challenges in practically modeling counterfactuals, formalized as parameters of the joint distribution of potential outcomes. Counterfactual reasoning holds paramount importance in contemporary decision-making processes, especially in scenarios that demand personalized incentives based on the joint values of $(Y(0), Y(1))$. This paper begins with an investigation of the PO and SCM frameworks for modeling counterfactuals. Through the analysis, we identify an inherent model capacity limitation, termed as the ``degenerative counterfactual problem'', emerging from the consistency rule that is the cornerstone of both frameworks. To address this limitation, we introduce a novel \textit{distribution-consistency} assumption, and in alignment with it, we propose the Distribution-consistency Structural Causal Models (DiscoSCMs) o
&lt;/p&gt;</description></item><item><title>GarchingSim&#26159;&#19968;&#31181;&#20855;&#26377;&#36924;&#30495;&#22330;&#26223;&#21644;&#29992;&#25143;&#21451;&#22909;&#24037;&#20316;&#27969;&#31243;&#30340;&#33258;&#21160;&#39550;&#39542;&#20223;&#30495;&#22120;&#65292;&#33021;&#19982;&#22806;&#37096;&#31639;&#27861;&#36890;&#20449;&#24182;&#20855;&#26377;&#39640;&#31934;&#24230;&#30340;&#36710;&#36742;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#21644;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39550;&#39542;&#12290;</title><link>http://arxiv.org/abs/2401.15803</link><description>&lt;p&gt;
GarchingSim:&#19968;&#31181;&#20855;&#26377;&#36924;&#30495;&#22330;&#26223;&#21644;&#26497;&#31616;&#24037;&#20316;&#27969;&#31243;&#30340;&#33258;&#21160;&#39550;&#39542;&#20223;&#30495;&#22120;
&lt;/p&gt;
&lt;p&gt;
GarchingSim: An Autonomous Driving Simulator with Photorealistic Scenes and Minimalist Workflow. (arXiv:2401.15803v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15803
&lt;/p&gt;
&lt;p&gt;
GarchingSim&#26159;&#19968;&#31181;&#20855;&#26377;&#36924;&#30495;&#22330;&#26223;&#21644;&#29992;&#25143;&#21451;&#22909;&#24037;&#20316;&#27969;&#31243;&#30340;&#33258;&#21160;&#39550;&#39542;&#20223;&#30495;&#22120;&#65292;&#33021;&#19982;&#22806;&#37096;&#31639;&#27861;&#36890;&#20449;&#24182;&#20855;&#26377;&#39640;&#31934;&#24230;&#30340;&#36710;&#36742;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#21644;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#39550;&#39542;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23567;&#22411;&#21019;&#19994;&#20844;&#21496;&#21644;&#30740;&#31350;&#26426;&#26500;&#26469;&#35828;&#65292;&#36827;&#34892;&#33258;&#21160;&#39550;&#39542;&#31639;&#27861;&#30340;&#30495;&#23454;&#36947;&#36335;&#27979;&#35797;&#21487;&#33021;&#26114;&#36149;&#19988;&#19981;&#20999;&#23454;&#38469;&#12290;&#22240;&#27492;&#65292;&#20223;&#30495;&#25104;&#20026;&#35780;&#20272;&#36825;&#20123;&#31639;&#27861;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20813;&#36153;&#21644;&#24320;&#28304;&#30340;&#20223;&#30495;&#22120;&#30340;&#21487;&#29992;&#24615;&#26377;&#38480;&#65292;&#23545;&#20110;&#21021;&#23398;&#32773;&#21644;&#36328;&#23398;&#31185;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#65292;&#23433;&#35013;&#21644;&#37197;&#32622;&#36807;&#31243;&#21487;&#33021;&#20196;&#20154;&#29983;&#30031;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;&#36924;&#30495;&#22330;&#26223;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#33258;&#21160;&#39550;&#39542;&#20223;&#30495;&#22120;&#12290;&#35813;&#20223;&#30495;&#22120;&#33021;&#22815;&#36890;&#36807;ROS2&#25110;Socket.IO&#19982;&#22806;&#37096;&#31639;&#27861;&#36827;&#34892;&#36890;&#20449;&#65292;&#19982;&#29616;&#26377;&#36719;&#20214;&#22534;&#26632;&#20860;&#23481;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#20223;&#30495;&#22120;&#20013;&#23454;&#29616;&#20102;&#19968;&#20010;&#39640;&#31934;&#24230;&#30340;&#36710;&#36742;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#36710;&#36742;&#29289;&#29702;&#25928;&#24212;&#30340;&#30495;&#23454;&#24615;&#12290;&#35813;&#20223;&#30495;&#22120;&#33021;&#22815;&#25191;&#34892;&#22810;&#31181;&#21151;&#33021;&#65292;&#21253;&#25324;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#21644;&#20351;&#29992;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31639;&#27861;&#36827;&#34892;&#39550;&#39542;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26356;&#27880;&#37325;&#31616;&#32422;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conducting real road testing for autonomous driving algorithms can be expensive and sometimes impractical, particularly for small startups and research institutes. Thus, simulation becomes an important method for evaluating these algorithms. However, the availability of free and open-source simulators is limited, and the installation and configuration process can be daunting for beginners and interdisciplinary researchers. We introduce an autonomous driving simulator with photorealistic scenes, meanwhile keeping a user-friendly workflow. The simulator is able to communicate with external algorithms through ROS2 or Socket.IO, making it compatible with existing software stacks. Furthermore, we implement a highly accurate vehicle dynamics model within the simulator to enhance the realism of the vehicle's physical effects. The simulator is able to serve various functions, including generating synthetic data and driving with machine learning-based algorithms. Moreover, we prioritize simplic
&lt;/p&gt;</description></item><item><title>DiffuserLite&#26159;&#19968;&#20010;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#25193;&#25955;&#35268;&#21010;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#35745;&#21010;&#32454;&#21270;&#36807;&#31243;&#65288;PRP&#65289;&#26469;&#25552;&#39640;&#20915;&#31574;&#39057;&#29575;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26694;&#26550;&#65292;&#23427;&#21482;&#20135;&#29983;&#20102;&#24456;&#23567;&#30340;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.15443</link><description>&lt;p&gt;
DiffuserLite: &#23454;&#26102;&#25193;&#25955;&#35268;&#21010;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
DiffuserLite: Towards Real-time Diffusion Planning. (arXiv:2401.15443v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15443
&lt;/p&gt;
&lt;p&gt;
DiffuserLite&#26159;&#19968;&#20010;&#24555;&#36895;&#36731;&#37327;&#32423;&#30340;&#25193;&#25955;&#35268;&#21010;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#35745;&#21010;&#32454;&#21270;&#36807;&#31243;&#65288;PRP&#65289;&#26469;&#25552;&#39640;&#20915;&#31574;&#39057;&#29575;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26694;&#26550;&#65292;&#23427;&#21482;&#20135;&#29983;&#20102;&#24456;&#23567;&#30340;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#35268;&#21010;&#34987;&#35748;&#20026;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#26377;&#25928;&#30340;&#20915;&#31574;&#33539;&#24335;&#12290;&#38271;&#26102;&#38388;&#36328;&#24230;&#36712;&#36857;&#30340;&#39640;&#36136;&#37327;&#26465;&#20214;&#29983;&#25104;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25193;&#25955;&#35268;&#21010;&#26041;&#27861;&#30001;&#20110;&#36845;&#20195;&#25277;&#26679;&#25104;&#26412;&#26114;&#36149;&#32780;&#23548;&#33268;&#20915;&#31574;&#39057;&#29575;&#20302;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DiffuserLite&#65292;&#19968;&#20010;&#24555;&#36895;&#32780;&#36731;&#37327;&#32423;&#30340;&#25193;&#25955;&#35268;&#21010;&#26694;&#26550;&#12290;DiffuserLite&#20351;&#29992;&#20102;&#19968;&#20010;&#35745;&#21010;&#32454;&#21270;&#36807;&#31243;&#65288;PRP&#65289;&#26469;&#29983;&#25104;&#31895;&#21040;&#32454;&#31890;&#24230;&#30340;&#36712;&#36857;&#65292;&#36825;&#26174;&#33879;&#20943;&#23569;&#20102;&#20887;&#20313;&#20449;&#24687;&#30340;&#24314;&#27169;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#20915;&#31574;&#39057;&#29575;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20043;&#21069;&#30340;&#26694;&#26550;&#30456;&#27604;&#65292;DiffuserLite&#20165;&#20135;&#29983;&#20102;$0.88\%$&#30340;&#36816;&#34892;&#26102;&#38388;&#25104;&#26412;&#65292;&#24179;&#22343;&#20915;&#31574;&#39057;&#29575;&#36798;&#21040;&#20102;122Hz&#65292;&#24182;&#22312;D4RL&#22522;&#20934;&#27979;&#35797;&#19978;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#24178;&#20928;DiffuserLite&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;...
&lt;/p&gt;
&lt;p&gt;
Diffusion planning has been recognized as an effective decision-making paradigm in various domains. The high-quality conditional generation capability of long-horizon trajectories makes it a promising research direction. However, existing diffusion planning methods suffer from low decision-making frequencies because of the expensive iterative sampling cost. To address this issue, we introduce DiffuserLite, a fast and lightweight diffusion planning framework. DiffuserLite employs a planning refinement process (PRP) to generate coarse-to-fine-grained trajectories, which significantly reduces the modeling of redundant information and leads to notable increases in decision-making frequency. Our experimental results demonstrate that DiffuserLite incurs only $0.88\%$ of the runtime cost compared to previous frameworks, achieves an average decision-making frequency of $122$Hz, and reaches state-of-the-art performance on D4RL benchmarks. In addition, our clean DiffuserLite framework can serve 
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.15378</link><description>&lt;p&gt;
&#22522;&#20110;RAG&#30340;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#25552;&#26696;&#65306;MufassirQAS LLM
&lt;/p&gt;
&lt;p&gt;
A RAG-based Question Answering System Proposal for Understanding Islam: MufassirQAS LLM. (arXiv:2401.15378v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15378
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;RAG&#30340;MufassirQAS&#38382;&#31572;&#31995;&#32479;&#21033;&#29992;NLP&#25216;&#26415;&#24314;&#31435;&#32852;&#31995;&#24182;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#65292;&#24110;&#21161;&#29702;&#35299;&#20234;&#26031;&#20848;&#25945;&#30340;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#29702;&#35299;&#23447;&#25945;&#23384;&#22312;&#22797;&#26434;&#24615;&#21644;&#25945;&#20041;&#28145;&#24230;&#30340;&#25361;&#25112;&#12290;&#38382;&#31572;&#26426;&#22120;&#20154;&#20316;&#20026;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#65292;&#21487;&#20197;&#24110;&#21161;&#12290;LLM&#32842;&#22825;&#26426;&#22120;&#20154;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24314;&#31435;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20934;&#30830;&#22238;&#31572;&#22797;&#26434;&#38382;&#39064;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#29992;&#20110;&#23447;&#25945;&#21551;&#33945;&#30340;&#38382;&#39064;&#22238;&#31572;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;LLM&#20063;&#26377;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#20542;&#21521;&#65292;&#31216;&#20026;&#24187;&#35273;&#12290;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#22238;&#31572;&#21487;&#33021;&#21253;&#21547;&#20398;&#36785;&#20010;&#20154;&#23447;&#25945;&#20449;&#20208;&#12289;&#36328;&#23447;&#27966;&#20914;&#31361;&#21644;&#26377;&#20105;&#35758;&#25110;&#25935;&#24863;&#30340;&#35805;&#39064;&#30340;&#20869;&#23481;&#12290;&#23427;&#38656;&#35201;&#36991;&#20813;&#36825;&#31181;&#24773;&#20917;&#65292;&#32780;&#19981;&#20250;&#23459;&#25196;&#20167;&#24680;&#35328;&#35770;&#25110;&#20882;&#29359;&#26576;&#20123;&#32676;&#20307;&#30340;&#20154;&#25110;&#20182;&#20204;&#30340;&#20449;&#20208;&#12290;&#26412;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#21521;&#37327;&#25968;&#25454;&#24211;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#26041;&#27861;&#26469;&#25552;&#39640;LLMs&#30340;&#20934;&#30830;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#25105;&#20204;&#30340;&#38382;&#31572;&#31995;&#32479;&#31216;&#20026;"MufassirQAS"&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#27169;&#22411;&#26469;&#35780;&#20272;&#35813;&#31995;&#32479;&#24182;&#35777;&#26126;&#20854;&#22312;&#35299;&#20915;&#23447;&#25945;&#34892;&#19994;&#38382;&#39064;&#20013;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
There exist challenges in learning and understanding religions as the presence of complexity and depth of religious doctrines and teachings. Chatbots as question-answering systems can help in solving these challenges. LLM chatbots use NLP techniques to establish connections between topics and accurately respond to complex questions. These capabilities make it perfect to be used in enlightenment on religion as a question answering chatbot. However, LLMs also have a tendency to generate false information, known as hallucination. The responses of the chatbots can include content that insults personal religious beliefs, interfaith conflicts, and controversial or sensitive topics. It needs to avoid such cases without promoting hate speech or offending certain groups of people or their beliefs. This study uses a vector database-based Retrieval Augmented Generation (RAG) approach to enhance the accuracy and transparency of LLMs. Our question-answering system is called as "MufassirQAS". We cre
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#30340;&#36861;&#36394;&#30740;&#31350;&#65292;&#23558;&#21508;&#31181;&#31867;&#22411;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#20102;&#27719;&#24635;&#21644;&#27979;&#35797;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#32508;&#21512;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20154;&#24037;&#25968;&#25454;&#36136;&#37327;&#30340;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2401.14698</link><description>&lt;p&gt;
&#21453;&#24605;LLM&#29983;&#25104;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#65306;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#30340;&#36861;&#36394;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Under the Surface: Tracking the Artifactuality of LLM-Generated Data. (arXiv:2401.14698v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14698
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#30340;&#36861;&#36394;&#30740;&#31350;&#65292;&#23558;&#21508;&#31181;&#31867;&#22411;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#36827;&#34892;&#20102;&#27719;&#24635;&#21644;&#27979;&#35797;&#65292;&#24182;&#25581;&#31034;&#20102;&#38544;&#34255;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#38382;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#27425;&#23545;LLM&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#32508;&#21512;&#20998;&#26512;&#21644;&#27604;&#36739;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#20154;&#24037;&#25968;&#25454;&#36136;&#37327;&#30340;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#29983;&#25104;&#20154;&#24037;&#25968;&#25454;&#26041;&#38754;&#30340;&#19981;&#26029;&#25193;&#22823;&#30340;&#20316;&#29992;&#12290;LLM&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#29983;&#25104;&#22810;&#31181;&#36755;&#20986;&#65292;&#21253;&#25324;&#27880;&#37322;&#12289;&#20559;&#22909;&#12289;&#25351;&#20196;&#25552;&#31034;&#12289;&#27169;&#25311;&#23545;&#35805;&#21644;&#33258;&#30001;&#25991;&#26412;&#12290;&#30001;&#20110;&#36825;&#20123;LLM&#29983;&#25104;&#25968;&#25454;&#24418;&#24335;&#22312;&#24212;&#29992;&#20013;&#32463;&#24120;&#20132;&#21449;&#65292;&#23427;&#20204;&#30456;&#20114;&#24433;&#21709;&#65292;&#24182;&#24341;&#21457;&#20102;&#23545;&#35757;&#32451;&#24490;&#29615;&#20013;&#21512;&#24182;&#30340;&#20154;&#24037;&#25968;&#25454;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#37325;&#22823;&#20851;&#27880;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#20154;&#24037;&#25968;&#25454;&#29983;&#24577;&#31995;&#32479;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#30740;&#31350;&#23558;&#21508;&#31181;&#31867;&#22411;&#30340;LLM&#29983;&#25104;&#25991;&#26412;&#25968;&#25454;&#27719;&#24635;&#36215;&#26469;&#65292;&#20174;&#26356;&#20005;&#26684;&#21463;&#38480;&#30340;&#25968;&#25454;&#22914;&#8220;&#20219;&#21153;&#26631;&#31614;&#8221;&#21040;&#26356;&#33258;&#30001;&#30340;&#8220;&#33258;&#30001;&#25991;&#26412;&#8221;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;LLM&#29983;&#25104;&#30340;&#20154;&#24037;&#25968;&#25454;&#30340;&#36136;&#37327;&#21644;&#24433;&#21709;&#36827;&#34892;&#20102;&#21387;&#21147;&#27979;&#35797;&#65292;&#24182;&#19982;&#20154;&#24037;&#25968;&#25454;&#22312;&#21508;&#31181;&#29616;&#26377;&#22522;&#20934;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;&#23613;&#31649;&#20154;&#24037;&#25968;&#25454;&#33021;&#22815;&#21305;&#37197;&#20154;&#31867;&#34920;&#29616;&#65292;&#20294;&#26412;&#25991;&#25581;&#31034;&#20102;&#38544;&#34255;&#30340;&#24040;&#22823;&#38544;&#24739;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work delves into the expanding role of large language models (LLMs) in generating artificial data. LLMs are increasingly employed to create a variety of outputs, including annotations, preferences, instruction prompts, simulated dialogues, and free text. As these forms of LLM-generated data often intersect in their application, they exert mutual influence on each other and raise significant concerns about the quality and diversity of the artificial data incorporated into training cycles, leading to an artificial data ecosystem. To the best of our knowledge, this is the first study to aggregate various types of LLM-generated text data, from more tightly constrained data like "task labels" to more lightly constrained "free-form text". We then stress test the quality and implications of LLM-generated artificial data, comparing it with human data across various existing benchmarks. Despite artificial data's capability to match human performance, this paper reveals significant hidden d
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32467;&#21512;MCTS&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;SR-GPT&#65292;&#22312;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#20844;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2401.14424</link><description>&lt;p&gt;
&#36890;&#36807;GPT&#24341;&#23548;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#20174;&#25968;&#25454;&#20013;&#21457;&#29616;&#25968;&#23398;&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
Discovering Mathematical Formulas from Data via GPT-guided Monte Carlo Tree Search. (arXiv:2401.14424v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14424
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32467;&#21512;MCTS&#21644;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;SR-GPT&#65292;&#22312;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#25968;&#23398;&#20844;&#24335;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31185;&#23398;&#30740;&#31350;&#21644;&#20154;&#24037;&#26234;&#33021;&#20013;&#65292;&#25214;&#21040;&#19968;&#20010;&#31616;&#27905;&#19988;&#21487;&#35299;&#37322;&#30340;&#25968;&#23398;&#20844;&#24335;&#26469;&#20934;&#30830;&#25551;&#36848;&#25968;&#25454;&#20013;&#27599;&#20010;&#21464;&#37327;&#19982;&#39044;&#27979;&#20540;&#20043;&#38388;&#30340;&#20851;&#31995;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20063;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20010;&#38382;&#39064;&#34987;&#31216;&#20026;&#31526;&#21495;&#22238;&#24402;&#65292;&#26159;&#19968;&#20010;NP&#22256;&#38590;&#38382;&#39064;&#12290;&#21435;&#24180;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;&#30340;&#31526;&#21495;&#22238;&#24402;&#26041;&#27861;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;sota&#12290;&#34429;&#28982;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#35813;&#31639;&#27861;&#22312;&#24674;&#22797;&#30446;&#26631;&#34920;&#36798;&#24335;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#30456;&#24403;&#22823;&#30340;&#25913;&#36827;&#65292;&#20294;&#26159;&#22312;MCTS&#36807;&#31243;&#20013;&#32570;&#20047;&#24341;&#23548;&#20005;&#37325;&#38459;&#30861;&#20102;&#20854;&#25628;&#32034;&#25928;&#29575;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#31639;&#27861;&#22312;MCTS&#30340;&#25628;&#32034;&#20013;&#28155;&#21152;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#32593;&#32476;&#65292;&#20294;&#26159;&#36825;&#20010;&#39044;&#35757;&#32451;&#30340;&#31574;&#30053;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#24456;&#24046;&#12290;&#20026;&#20102;&#24179;&#34913;&#25928;&#29575;&#21644;&#36890;&#29992;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SR-GPT&#65292;&#32467;&#21512;&#20102;AlphaZero&#30340;&#24605;&#24819;&#12290;SR-GPT&#26159;&#19968;&#31181;&#26032;&#30340;&#31526;&#21495;&#22238;&#24402;&#31639;&#27861;&#65292;&#23558;MCTS&#19982;&#19968;&#20010;&#36890;&#29992;&#24615;&#36739;&#22909;&#30340;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding a concise and interpretable mathematical formula that accurately describes the relationship between each variable and the predicted value in the data is a crucial task in scientific research, as well as a significant challenge in artificial intelligence. This problem is referred to as symbolic regression, which is an NP-hard problem. Last year, a symbolic regression method based on Monte Carlo Tree Search (MCTS) was proposed and sota was obtained on multiple datasets. While this algorithm has shown considerable improvement in recovering target expressions compared to previous methods, the lack of guidance during the MCTS process severely hampers its search efficiency. Recently, some algorithms have added a pre-trained policy network to guide the search of MCTS, but the pre-trained policy network generalizes poorly. To balance efficiency and generality, we propose SR-GPT combining ideas from AlphaZero. SR-GPT is a new symbolic regression algorithm that combines MCTS with a Gener
&lt;/p&gt;</description></item><item><title>CreativeSynth&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25193;&#25955;&#30340;&#21019;&#26032;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#35821;&#20041;&#20869;&#23481;&#23548;&#20837;&#21040;&#33402;&#26415;&#39046;&#22495;&#20013;&#65292;&#33021;&#22815;&#21327;&#35843;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#22810;&#20219;&#21153;&#65292;&#22312;&#33402;&#26415;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.14066</link><description>&lt;p&gt;
CreativeSynth&#65306;&#22522;&#20110;&#22810;&#27169;&#24577;&#25193;&#25955;&#30340;&#35270;&#35273;&#33402;&#26415;&#21019;&#24847;&#34701;&#21512;&#19982;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
CreativeSynth: Creative Blending and Synthesis of Visual Arts based on Multimodal Diffusion. (arXiv:2401.14066v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.14066
&lt;/p&gt;
&lt;p&gt;
CreativeSynth&#26159;&#19968;&#31181;&#22522;&#20110;&#22810;&#27169;&#24577;&#25193;&#25955;&#30340;&#21019;&#26032;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#35821;&#20041;&#20869;&#23481;&#23548;&#20837;&#21040;&#33402;&#26415;&#39046;&#22495;&#20013;&#65292;&#33021;&#22815;&#21327;&#35843;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#22810;&#20219;&#21153;&#65292;&#22312;&#33402;&#26415;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#27493;&#65292;&#23637;&#31034;&#20102;&#20854;&#21512;&#25104;&#21508;&#31181;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;&#36825;&#20123;&#27169;&#22411;&#24212;&#29992;&#20110;&#33402;&#26415;&#22270;&#20687;&#32534;&#36753;&#38754;&#20020;&#20004;&#20010;&#37325;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#29992;&#25143;&#24448;&#24448;&#38590;&#20197;&#26500;&#24314;&#35814;&#32454;&#25551;&#36848;&#36755;&#20837;&#22270;&#20687;&#35270;&#35273;&#20803;&#32032;&#30340;&#25991;&#26412;&#25552;&#31034;&#12290;&#20854;&#27425;&#65292;&#29616;&#26377;&#27169;&#22411;&#22312;&#29305;&#23450;&#21306;&#22495;&#36827;&#34892;&#20462;&#25913;&#26102;&#24120;&#24120;&#20250;&#30772;&#22351;&#25972;&#20307;&#33402;&#26415;&#39118;&#26684;&#65292;&#20351;&#24471;&#23454;&#29616;&#19968;&#33268;&#19988;&#20855;&#26377;&#23457;&#32654;&#32479;&#19968;&#30340;&#20316;&#21697;&#21464;&#24471;&#26356;&#21152;&#22797;&#26434;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;CreativeSynth&#65292;&#35813;&#26694;&#26550;&#22522;&#20110;&#20855;&#26377;&#21327;&#35843;&#22810;&#27169;&#24577;&#36755;&#20837;&#21644;&#22810;&#20219;&#21153;&#33021;&#21147;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#22810;&#27169;&#24577;&#29305;&#24449;&#21644;&#23450;&#21046;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;CreativeSynth&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#35821;&#20041;&#20869;&#23481;&#23548;&#20837;&#21040;&#33402;&#26415;&#39046;&#22495;&#20013;&#65292;&#23454;&#29616;&#20102;&#21453;&#36716;&#21644;&#23454;&#26102;&#39118;&#26684;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale text-to-image generative models have made impressive strides, showcasing their ability to synthesize a vast array of high-quality images. However, adapting these models for artistic image editing presents two significant challenges. Firstly, users struggle to craft textual prompts that meticulously detail visual elements of the input image. Secondly, prevalent models, when effecting modifications in specific zones, frequently disrupt the overall artistic style, complicating the attainment of cohesive and aesthetically unified artworks. To surmount these obstacles, we build the innovative unified framework CreativeSynth, which is based on a diffusion model with the ability to coordinate multimodal inputs and multitask in the field of artistic image generation. By integrating multimodal features with customized attention mechanisms, CreativeSynth facilitates the importation of real-world semantic content into the domain of art through inversion and real-time style transfer. T
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2401.13802</link><description>&lt;p&gt;
&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#26041;&#38754;&#30340;&#21151;&#25928;
&lt;/p&gt;
&lt;p&gt;
Investigating the Efficacy of Large Language Models for Code Clone Detection. (arXiv:2401.13802v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13802
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20363;&#22914;&#20195;&#30721;&#29983;&#25104;&#12290;LLMs&#20027;&#35201;&#22312;&#22522;&#20110;&#25552;&#31034;&#30340;&#38646;/&#23569;&#26679;&#26412;&#33539;&#24335;&#20013;&#34987;&#29992;&#20110;&#25351;&#23548;&#27169;&#22411;&#23436;&#25104;&#20219;&#21153;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;LLMs&#22312;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#65288;CCD&#65289;&#36825;&#19968;&#38750;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated remarkable success in various natural language processing and software engineering tasks, such as code generation. The LLMs are mainly utilized in the prompt-based zero/few-shot paradigm to guide the model in accomplishing the task. %\textbf{Goal:} GPT-based models are one of the popular ones studied for tasks such as code comment generation or test generation. These tasks are `generative' tasks. However, there is limited research on the usage of LLMs for `non-generative' tasks such as classification using the prompt-based paradigm. In this preliminary exploratory study, we investigated the applicability of LLMs for Code Clone Detection (CCD), a non-generative task. %\textbf{Method:} By building a mono-lingual and cross-lingual CCD dataset derived from CodeNet, we first investigated two different prompts using ChatGPT to detect \textcolor{black}{Type-4} code clones in Java-Java and Java-Ruby pairs in a zero-shot setting. We \textcolor{blac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TTG-NN&#65289;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25345;&#20037;&#21516;&#35843;&#12289;&#22270;&#21367;&#31215;&#21644;&#24352;&#37327;&#36816;&#31639;&#65292;&#21516;&#26102;&#25429;&#25417;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#65288;TT&#65289;&#21644;Tensor&#35270;&#22270;&#22270;&#65288;TG&#65289;&#32467;&#26500;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2401.12007</link><description>&lt;p&gt;
Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Tensor-view Topological Graph Neural Network. (arXiv:2401.12007v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12007
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TTG-NN&#65289;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#25345;&#20037;&#21516;&#35843;&#12289;&#22270;&#21367;&#31215;&#21644;&#24352;&#37327;&#36816;&#31639;&#65292;&#21516;&#26102;&#25429;&#25417;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#65288;TT&#65289;&#21644;Tensor&#35270;&#22270;&#22270;&#65288;TG&#65289;&#32467;&#26500;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20998;&#31867;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#22270;&#32467;&#26500;&#25968;&#25454;&#23398;&#20064;&#20219;&#21153;&#12290;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36817;&#24180;&#26469;&#22312;&#22270;&#23398;&#20064;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#22312;&#35768;&#22810;&#37325;&#35201;&#30340;&#22270;&#38382;&#39064;&#19978;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;GNNs&#22312;&#24615;&#33021;&#19978;&#22788;&#20110;&#26368;&#21069;&#27839;&#65292;&#20294;&#23427;&#20204;&#21482;&#20351;&#29992;&#20102;&#27599;&#20010;&#33410;&#28857;&#21608;&#22260;&#38750;&#24120;&#26377;&#38480;&#30340;&#37051;&#22495;&#30340;&#23616;&#37096;&#20449;&#24687;&#65292;&#23548;&#33268;&#20102;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#20002;&#22833;&#21644;&#36807;&#22810;&#35745;&#31639;&#30340;&#24320;&#38144;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TTG-NN&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22522;&#20110;&#25345;&#20037;&#21516;&#35843;&#12289;&#22270;&#21367;&#31215;&#21644;&#24352;&#37327;&#36816;&#31639;&#30340;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#12290;&#36825;&#31181;&#26032;&#26041;&#27861;&#21516;&#26102;&#25429;&#25417;&#20102;&#23616;&#37096;&#21644;&#20840;&#23616;&#23618;&#38754;&#19978;&#30340;Tensor&#35270;&#22270;&#25299;&#25169;&#65288;TT&#65289;&#21644;Tensor&#35270;&#22270;&#22270;&#65288;TG&#65289;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#22312;&#35745;&#31639;&#19978;&#20805;&#20998;&#21033;&#29992;&#20102;&#22270;&#30340;&#25299;&#25169;&#21644;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph classification is an important learning task for graph-structured data. Graph neural networks (GNNs) have recently gained growing attention in graph learning and have shown significant improvements in many important graph problems. Despite their state-of-the-art performances, existing GNNs only use local information from a very limited neighborhood around each node, suffering from loss of multi-modal information and overheads of excessive computation. To address these issues, we propose a novel Tensor-view Topological Graph Neural Network (TTG-NN), a class of simple yet effective topological deep learning built upon persistent homology, graph convolution, and tensor operations. This new method incorporates tensor learning to simultaneously capture Tensor-view Topological (TT), as well as Tensor-view Graph (TG) structural information on both local and global levels. Computationally, to fully exploit graph topology and structure, we propose two flexible TT and TG representation lea
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2401.10746</link><description>&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#33041;&#30005;&#35299;&#30721;&#20013;&#30340;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Systematic Evaluation of Euclidean Alignment with Deep Learning for EEG Decoding. (arXiv:2401.10746v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10746
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#31995;&#32479;&#35780;&#20272;&#20102;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#23545;&#33041;&#30005;&#35299;&#30721;&#30340;&#24433;&#21709;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#35299;&#30721;&#29575;&#65292;&#24182;&#19988;&#20943;&#23569;&#20102;&#25910;&#25947;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#32463;&#24120;&#29992;&#20110;&#21508;&#31181;&#33041;&#26426;&#25509;&#21475;&#65288;BCI&#65289;&#20219;&#21153;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#21463;&#21040;&#22823;&#37327;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#25968;&#25454;&#65292;&#36801;&#31227;&#23398;&#20064;&#33021;&#22815;&#26356;&#26377;&#25928;&#22320;&#35757;&#32451;DL&#27169;&#22411;&#12290;&#19968;&#31181;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#30340;&#25216;&#26415;&#26159;&#27431;&#20960;&#37324;&#24471;&#23545;&#40784;&#65288;EA&#65289;&#65292;&#22240;&#20026;&#23427;&#26131;&#20110;&#20351;&#29992;&#12289;&#35745;&#31639;&#22797;&#26434;&#24230;&#20302;&#24182;&#19988;&#19982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20860;&#23481;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#35780;&#20272;&#20854;&#23545;&#20849;&#20139;&#21644;&#20010;&#20307;DL&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;EA&#19982;DL&#30456;&#32467;&#21512;&#22312;&#35299;&#30721;BCI&#20449;&#21495;&#20013;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;EA&#26469;&#35757;&#32451;&#26469;&#33258;&#22810;&#20010;&#21463;&#35797;&#32773;&#30340;&#20849;&#20139;&#27169;&#22411;&#65292;&#24182;&#35780;&#20272;&#20854;&#23545;&#26032;&#21463;&#35797;&#32773;&#30340;&#21487;&#36801;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#23558;&#30446;&#26631;&#21463;&#35797;&#32773;&#30340;&#35299;&#30721;&#29575;&#25552;&#39640;&#20102;4.33&#65285;&#65292;&#24182;&#19988;&#25910;&#25947;&#26102;&#38388;&#32553;&#30701;&#20102;&#36229;&#36807;70&#65285;&#12290;&#25105;&#20204;&#36824;&#20026;&#20010;&#20307;&#27169;&#22411;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalography (EEG) signals are frequently used for various Brain-Computer Interface (BCI) tasks. While Deep Learning (DL) techniques have shown promising results, they are hindered by the substantial data requirements. By leveraging data from multiple subjects, transfer learning enables more effective training of DL models. A technique that is gaining popularity is Euclidean Alignment (EA) due to its ease of use, low computational complexity, and compatibility with Deep Learning models. However, few studies evaluate its impact on the training performance of shared and individual DL models. In this work, we systematically evaluate the effect of EA combined with DL for decoding BCI signals. We used EA to train shared models with data from multiple subjects and evaluated its transferability to new subjects. Our experimental results show that it improves decoding in the target subject by 4.33% and decreases convergence time by more than 70%. We also trained individual models for 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.10337</link><description>&lt;p&gt;
&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Noise Contrastive Estimation-based Matching Framework for Low-resource Security Attack Pattern Recognition. (arXiv:2401.10337v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.10337
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22122;&#22768;&#23545;&#27604;&#20272;&#35745;&#30340;&#20302;&#36164;&#28304;&#23433;&#20840;&#25915;&#20987;&#27169;&#24335;&#35782;&#21035;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#25991;&#26412;&#19982;&#25915;&#20987;&#27169;&#24335;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#20197;&#38477;&#20302;&#22823;&#37327;&#31867;&#21035;&#12289;&#26631;&#31614;&#20998;&#24067;&#19981;&#22343;&#21644;&#26631;&#31614;&#31354;&#38388;&#22797;&#26434;&#24615;&#24102;&#26469;&#30340;&#23398;&#20064;&#38590;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25112;&#26415;&#12289;&#25216;&#26415;&#21644;&#31243;&#24207;&#65288;TTPs&#65289;&#26159;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#20013;&#22797;&#26434;&#30340;&#25915;&#20987;&#27169;&#24335;&#65292;&#22312;&#25991;&#26412;&#30693;&#35782;&#24211;&#20013;&#26377;&#35814;&#32454;&#30340;&#25551;&#36848;&#12290;&#22312;&#32593;&#32476;&#23433;&#20840;&#20889;&#20316;&#20013;&#35782;&#21035;TTPs&#65292;&#36890;&#24120;&#31216;&#20026;TTP&#26144;&#23556;&#65292;&#26159;&#19968;&#20010;&#37325;&#35201;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#20197;&#32463;&#20856;&#30340;&#22810;&#31867;&#25110;&#22810;&#26631;&#31614;&#20998;&#31867;&#35774;&#32622;&#20026;&#30446;&#26631;&#12290;&#30001;&#20110;&#23384;&#22312;&#22823;&#37327;&#30340;&#31867;&#21035;&#65288;&#21363;TTPs&#65289;&#65292;&#26631;&#31614;&#20998;&#24067;&#30340;&#19981;&#22343;&#34913;&#21644;&#26631;&#31614;&#31354;&#38388;&#30340;&#22797;&#26434;&#23618;&#27425;&#32467;&#26500;&#65292;&#36825;&#31181;&#35774;&#32622;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#23398;&#20064;&#33539;&#24335;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#20013;&#23558;&#25991;&#26412;&#19982;TTP&#26631;&#31614;&#20043;&#38388;&#30340;&#30452;&#25509;&#35821;&#20041;&#30456;&#20284;&#24230;&#20915;&#23450;&#20026;&#25991;&#26412;&#20998;&#37197;&#32473;TTP&#26631;&#31614;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#20165;&#20165;&#22312;&#22823;&#22411;&#26631;&#31614;&#31354;&#38388;&#19978;&#31454;&#20105;&#30340;&#22797;&#26434;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#26377;&#25928;&#30340;&#22522;&#20110;&#37319;&#26679;&#30340;&#23398;&#20064;&#27604;&#36739;&#26426;&#21046;&#30340;&#31070;&#32463;&#21305;&#37197;&#26550;&#26500;&#65292;&#20419;&#36827;&#23398;&#20064;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tactics, Techniques and Procedures (TTPs) represent sophisticated attack patterns in the cybersecurity domain, described encyclopedically in textual knowledge bases. Identifying TTPs in cybersecurity writing, often called TTP mapping, is an important and challenging task. Conventional learning approaches often target the problem in the classical multi-class or multilabel classification setting. This setting hinders the learning ability of the model due to a large number of classes (i.e., TTPs), the inevitable skewness of the label distribution and the complex hierarchical structure of the label space. We formulate the problem in a different learning paradigm, where the assignment of a text to a TTP label is decided by the direct semantic similarity between the two, thus reducing the complexity of competing solely over the large labeling space. To that end, we propose a neural matching architecture with an effective sampling-based learn-to-compare mechanism, facilitating the learning pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.09071</link><description>&lt;p&gt;
&#29992;&#31354;&#38388;&#33258;&#36866;&#24212;&#28388;&#27874;&#37325;&#26032;&#24605;&#32771;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Rethinking Spectral Graph Neural Networks with Spatially Adaptive Filtering. (arXiv:2401.09071v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#24605;&#32771;&#20102;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#35813;&#30740;&#31350;&#21457;&#29616;&#65292;&#35889;&#28388;&#27874;&#22312;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#65292;&#24182;&#26126;&#30830;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#26032;&#22270;&#12290;&#36866;&#24212;&#24615;&#26032;&#22270;&#23637;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#24182;&#33021;&#22815;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#29702;&#35770;&#19978;&#22312;&#35889;&#22495;&#20013;&#26377;&#24456;&#22909;&#30340;&#22522;&#30784;&#65292;&#20294;&#23427;&#20204;&#23454;&#38469;&#19978;&#20381;&#36182;&#20110;&#22810;&#39033;&#24335;&#36924;&#36817;&#65292;&#24847;&#21619;&#30528;&#23427;&#20204;&#19982;&#31354;&#38388;&#22495;&#26377;&#30528;&#28145;&#21051;&#30340;&#32852;&#31995;&#12290;&#30001;&#20110;&#20197;&#21069;&#30340;&#30740;&#31350;&#24456;&#23569;&#20174;&#31354;&#38388;&#35282;&#24230;&#30740;&#31350;&#35889;&#22270;GNN&#65292;&#22240;&#27492;&#23427;&#20204;&#22312;&#31354;&#38388;&#22495;&#30340;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#65292;&#20363;&#22914;&#65292;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#22495;&#20013;&#23454;&#38469;&#19978;&#32534;&#30721;&#20102;&#21738;&#20123;&#20449;&#24687;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#22312;&#35889;&#28388;&#27874;&#21644;&#31354;&#38388;&#32858;&#21512;&#20043;&#38388;&#24314;&#31435;&#20102;&#19968;&#20010;&#29702;&#35770;&#19978;&#30340;&#32852;&#31995;&#65292;&#25581;&#31034;&#20102;&#35889;&#28388;&#27874;&#38544;&#21547;&#22320;&#23558;&#21407;&#22987;&#22270;&#36716;&#25442;&#25104;&#36866;&#24212;&#24615;&#26032;&#22270;&#30340;&#20869;&#22312;&#20132;&#20114;&#20316;&#29992;&#65292;&#24182;&#26126;&#30830;&#22320;&#35745;&#31639;&#29992;&#20110;&#31354;&#38388;&#32858;&#21512;&#30340;&#36866;&#24212;&#24615;&#26032;&#22270;&#12290;&#29702;&#35770;&#21644;&#32463;&#39564;&#30740;&#31350;&#34920;&#26126;&#65292;&#36866;&#24212;&#24615;&#26032;&#22270;&#19981;&#20165;&#34920;&#29616;&#20986;&#38750;&#23616;&#37096;&#24615;&#65292;&#36824;&#33021;&#22815;&#23481;&#32435;&#26377;&#31526;&#21495;&#30340;&#36793;&#26435;&#37325;&#20197;&#21453;&#26144;&#33410;&#28857;&#20043;&#38388;&#30340;&#26631;&#31614;&#19968;&#33268;&#24615;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20102;&#35889;&#22270;GNN&#22312;&#31354;&#38388;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Whilst spectral Graph Neural Networks (GNNs) are theoretically well-founded in the spectral domain, their practical reliance on polynomial approximation implies a profound linkage to the spatial domain. As previous studies rarely examine spectral GNNs from the spatial perspective, their spatial-domain interpretability remains elusive, e.g., what information is essentially encoded by spectral GNNs in the spatial domain? In this paper, to answer this question, we establish a theoretical connection between spectral filtering and spatial aggregation, unveiling an intrinsic interaction that spectral filtering implicitly leads the original graph to an adapted new graph, explicitly computed for spatial aggregation. Both theoretical and empirical investigations reveal that the adapted new graph not only exhibits non-locality but also accommodates signed edge weights to reflect label consistency between nodes. These findings thus highlight the interpretable role of spectral GNNs in the spatial 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2401.09003</link><description>&lt;p&gt;
&#36890;&#36807;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;&#26469;&#22686;&#24378;&#25968;&#23398;&#38382;&#39064;&#27714;&#35299;
&lt;/p&gt;
&lt;p&gt;
Augmenting Math Word Problems via Iterative Question Composing. (arXiv:2401.09003v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09003
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24341;&#20837;MMIQC&#25968;&#25454;&#38598;&#21644;&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;(IQC)&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;&#65292;&#25104;&#21151;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#19968;&#23450;&#36827;&#23637;&#65292;&#20294;&#22312;&#19981;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#31454;&#36187;&#32423;&#25968;&#23398;&#38382;&#39064;&#20173;&#28982;&#23545;&#24320;&#28304;LLMs&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MMIQC&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#28151;&#21512;&#22788;&#29702;&#30340;&#32593;&#32476;&#25968;&#25454;&#21644;&#21512;&#25104;&#38382;&#39064;-&#21709;&#24212;&#23545;&#30340;&#28151;&#21512;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#20379;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#12290;&#36890;&#36807;&#22312;MMIQC&#19978;&#23545;Mistral-7B(arXiv:2310.06825)&#36827;&#34892;&#24494;&#35843;&#33719;&#24471;&#30340;&#27169;&#22411;Mistral-7B-MMIQC&#65292;&#22312;MATH(arXiv:2103.03874)&#19978;&#36798;&#21040;&#20102;36.0%&#30340;&#20934;&#30830;&#29575;&#65292;&#27604;&#20043;&#21069;(model size $\sim$7B)&#30340;&#26368;&#20339;&#32467;&#26524;&#39640;&#20986;5.8%&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#36824;&#34920;&#26126;&#65292;&#25913;&#36827;&#30340;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#24402;&#21151;&#20110;&#25105;&#20204;&#30340;&#26032;&#39062;&#22686;&#24378;&#26041;&#27861;IQC(&#36845;&#20195;&#32452;&#21512;&#38382;&#39064;)&#65292;&#20854;&#20013;&#25105;&#20204;&#36845;&#20195;&#22320;&#35201;&#27714;LLM&#20174;&#32473;&#23450;&#30340;&#31181;&#23376;&#38382;&#39064;&#20013;&#32452;&#21512;&#26032;&#38382;&#39064;&#65292;&#24182;&#20174;&#21478;&#19968;&#20010;LLM&#20013;&#36827;&#34892;&#25298;&#32477;&#25277;&#26679;&#12290;MMIQC&#29616;&#24050;&#22312;https://huggingface.co/datasets/Vivacem/MMIQC&#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite recent progress in improving the mathematical reasoning ability of large language models(LLMs), solving competition-level math problems without the use of external tools remains challenging for open-source LLMs. In this work, we introduce the MMIQC dataset, a mixture of processed web data and synthetic question-response pairs, to equip base models with better mathematical reasoning skills. Mistral-7B-MMIQC, the model obtained by fine-tuning Mistral-7B(arXiv:2310.06825) on MMIQC, achieves 36.0\% accuracy on MATH(arXiv:2103.03874), 5.8\% higher than the previous (model size $\sim$7B) SOTA. Our experiments also show that a large part of the improvement attributes to our novel augmentation method IQC(Iterative Question Composing), where we iteratively ask an LLM to compose new questions from the given seed problems and do rejection sampling from another LLM. MMIQC has now been released on https://huggingface.co/datasets/Vivacem/MMIQC.
&lt;/p&gt;</description></item><item><title>CoSSegGaussians&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#31354;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#23454;&#29616;&#32039;&#20945;&#21644;&#21487;&#38752;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2401.05925</link><description>&lt;p&gt;
CoSSegGaussians&#65306;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
CoSSegGaussians: Compact and Swift Scene Segmenting 3D Gaussians. (arXiv:2401.05925v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05925
&lt;/p&gt;
&lt;p&gt;
CoSSegGaussians&#26159;&#19968;&#31181;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;&#65292;&#36890;&#36807;&#26144;&#23556;&#31354;&#38388;&#21644;&#35821;&#20041;&#29305;&#24449;&#23454;&#29616;&#32039;&#20945;&#21644;&#21487;&#38752;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32039;&#20945;&#19988;&#36805;&#36895;&#30340;3D&#39640;&#26031;&#22330;&#26223;&#20998;&#21106;&#26041;&#27861;&#65288;CoSSegGaussians&#65289;&#65292;&#35813;&#26041;&#27861;&#20165;&#20351;&#29992;RGB&#22270;&#20687;&#36755;&#20837;&#65292;&#20197;&#24555;&#36895;&#30340;&#28210;&#26579;&#36895;&#24230;&#23454;&#29616;&#32039;&#20945;&#30340;3D&#19968;&#33268;&#24615;&#22330;&#26223;&#20998;&#21106;&#12290;&#20808;&#21069;&#22522;&#20110;NeRF&#30340;3D&#20998;&#21106;&#26041;&#27861;&#20381;&#36182;&#20110;&#38544;&#24335;&#25110;&#20307;&#32032;&#31070;&#32463;&#22330;&#34920;&#31034;&#21644;&#20809;&#32447;&#34892;&#36827;&#20307;&#31215;&#28210;&#26579;&#65292;&#36825;&#20123;&#26041;&#27861;&#32791;&#26102;&#36739;&#38271;&#12290;&#26368;&#36817;&#30340;3D&#39640;&#26031;&#22330;&#25237;&#24433;&#26174;&#33879;&#25552;&#39640;&#20102;&#28210;&#26579;&#36895;&#24230;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#26031;&#30340;&#20998;&#21106;&#26041;&#27861;&#65288;&#20363;&#22914;&#39640;&#26031;&#20998;&#32452;&#65289;&#22312;&#38646;&#26679;&#26412;&#20998;&#21106;&#20013;&#27809;&#26377;&#25552;&#20379;&#32039;&#20945;&#30340;&#20998;&#21106;&#25513;&#27169;&#65292;&#20027;&#35201;&#21407;&#22240;&#26159;&#22312;&#36935;&#21040;&#19981;&#19968;&#33268;&#30340;2D&#26426;&#22120;&#29983;&#25104;&#26631;&#31614;&#26102;&#65292;&#26080;&#27861;&#30452;&#25509;&#20026;&#27599;&#20010;&#39640;&#26031;&#20998;&#37197;&#21487;&#23398;&#20064;&#21442;&#25968;&#65292;&#32570;&#20047;&#40065;&#26834;&#24615;&#21644;&#32039;&#20945;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#27973;&#23618;&#35299;&#30721;&#32593;&#32476;&#23558;&#27599;&#20010;&#39640;&#26031;&#28857;&#30340;&#34701;&#21512;&#31354;&#38388;&#21644;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#26144;&#23556;&#65292;&#36805;&#36895;&#23454;&#29616;&#32039;&#20945;&#19988;&#21487;&#38752;&#30340;&#38646;&#26679;&#26412;&#22330;&#26223;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Compact and Swift Segmenting 3D Gaussians(CoSSegGaussians), a method for compact 3D-consistent scene segmentation at fast rendering speed with only RGB images input. Previous NeRF-based 3D segmentation methods have relied on implicit or voxel neural scene representation and ray-marching volume rendering which are time consuming. Recent 3D Gaussian Splatting significantly improves the rendering speed, however, existing Gaussians-based segmentation methods(eg: Gaussian Grouping) fail to provide compact segmentation masks especially in zero-shot segmentation, which is mainly caused by the lack of robustness and compactness for straightforwardly assigning learnable parameters to each Gaussian when encountering inconsistent 2D machine-generated labels. Our method aims to achieve compact and reliable zero-shot scene segmentation swiftly by mapping fused spatial and semantically meaningful features for each Gaussian point with a shallow decoding network. Specifically, our method fi
&lt;/p&gt;</description></item><item><title>TwinBooster&#32467;&#21512;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#25351;&#32441;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.04478</link><description>&lt;p&gt;
TwinBooster: &#32467;&#21512;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#22686;&#24378;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
TwinBooster: Synergising Large Language Models with Barlow Twins and Gradient Boosting for Enhanced Molecular Property Prediction. (arXiv:2401.04478v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.04478
&lt;/p&gt;
&lt;p&gt;
TwinBooster&#32467;&#21512;&#20102;&#22823;&#35821;&#35328;&#27169;&#22411;&#12289;Barlow Twins&#21644;&#26799;&#24230;&#25552;&#21319;&#65292;&#36890;&#36807;&#25972;&#21512;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#25351;&#32441;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#20102;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#21457;&#29616;&#21644;&#24320;&#21457;&#30340;&#25104;&#21151;&#20381;&#36182;&#20110;&#23545;&#20998;&#23376;&#27963;&#24615;&#21644;&#23646;&#24615;&#30340;&#31934;&#30830;&#39044;&#27979;&#12290;&#34429;&#28982;&#22522;&#20110;&#35745;&#31639;&#30340;&#20998;&#23376;&#23646;&#24615;&#39044;&#27979;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#65292;&#20294;&#20854;&#20351;&#29992;&#36804;&#20170;&#20026;&#27490;&#20165;&#38480;&#20110;&#22823;&#37327;&#25968;&#25454;&#21487;&#29992;&#30340;&#26816;&#27979;&#26041;&#27861;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#32463;&#36807;&#24494;&#35843;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#25991;&#26412;&#20449;&#24687;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;Siamese&#31070;&#32463;&#32593;&#32476;Barlow Twins&#12290;&#35813;&#26550;&#26500;&#21033;&#29992;&#26816;&#27979;&#26041;&#27861;&#20449;&#24687;&#21644;&#20998;&#23376;&#25351;&#32441;&#25552;&#21462;&#30495;&#23454;&#30340;&#20998;&#23376;&#20449;&#24687;&#12290;TwinBooster&#36890;&#36807;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#23454;&#29616;&#20102;&#23545;&#26410;&#35265;&#36807;&#30340;&#29983;&#29289;&#26816;&#27979;&#26041;&#27861;&#21644;&#20998;&#23376;&#30340;&#23646;&#24615;&#39044;&#27979;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#20154;&#24037;&#26234;&#33021;&#27969;&#27700;&#32447;&#22312;FS-Mol&#22522;&#20934;&#27979;&#35797;&#19978;&#34920;&#29616;&#20986;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;&#36825;&#19968;&#31361;&#30772;&#23637;&#31034;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36890;&#24120;&#25968;&#25454;&#31232;&#32570;&#30340;&#20851;&#38190;&#23646;&#24615;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of drug discovery and development relies on the precise prediction of molecular activities and properties. While in silico molecular property prediction has shown remarkable potential, its use has been limited so far to assays for which large amounts of data are available. In this study, we use a fine-tuned large language model to integrate biological assays based on their textual information, coupled with Barlow Twins, a Siamese neural network using a novel self-supervised learning approach. This architecture uses both assay information and molecular fingerprints to extract the true molecular information. TwinBooster enables the prediction of properties of unseen bioassays and molecules by providing state-of-the-art zero-shot learning tasks. Remarkably, our artificial intelligence pipeline shows excellent performance on the FS-Mol benchmark. This breakthrough demonstrates the application of deep learning to critical property prediction tasks where data is typically scarce.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RAISE&#30340;&#26550;&#26500;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#25972;&#21512;&#21040;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#21452;&#32452;&#20214;&#35760;&#24518;&#31995;&#32479;&#26469;&#22686;&#24378;&#20195;&#29702;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#21487;&#25511;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#39044;liminary evaluations&#34920;&#26126;&#65292;RAISE&#22312;&#25151;&#22320;&#20135;&#38144;&#21806;&#39046;&#22495;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.02777</link><description>&lt;p&gt;
&#20174;LLM&#21040;&#23545;&#35805;&#20195;&#29702;&#65306;&#20855;&#26377;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#35760;&#24518;&#22686;&#24378;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
From LLM to Conversational Agent: A Memory Enhanced Architecture with Fine-Tuning of Large Language Models. (arXiv:2401.02777v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02777
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;RAISE&#30340;&#26550;&#26500;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-4&#25972;&#21512;&#21040;&#23545;&#35805;&#20195;&#29702;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#21452;&#32452;&#20214;&#35760;&#24518;&#31995;&#32479;&#26469;&#22686;&#24378;&#20195;&#29702;&#22312;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#21487;&#25511;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#39044;liminary evaluations&#34920;&#26126;&#65292;RAISE&#22312;&#25151;&#22320;&#20135;&#38144;&#21806;&#39046;&#22495;&#20855;&#26377;&#20248;&#21183;&#65292;&#24182;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;RAISE&#65288;Scratchpad&#21644;Examples&#36741;&#21161;&#25512;&#29702;&#21644;&#34892;&#20026;&#65289;,&#19968;&#31181;&#20808;&#36827;&#30340;&#26550;&#26500;&#65292;&#22686;&#24378;&#20102;&#23558;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#23545;&#35805;&#20195;&#29702;&#20013;&#30340;&#33021;&#21147;&#12290;RAISE&#26159;ReAct&#26694;&#26550;&#30340;&#25913;&#36827;&#29256;&#26412;&#65292;&#21253;&#25324;&#19968;&#20010;&#21452;&#32452;&#20214;&#35760;&#24518;&#31995;&#32479;&#65292;&#27169;&#20223;&#20154;&#31867;&#30340;&#30701;&#26399;&#35760;&#24518;&#21644;&#38271;&#26399;&#35760;&#24518;&#65292;&#20197;&#20445;&#25345;&#23545;&#35805;&#30340;&#19978;&#19979;&#25991;&#21644;&#36830;&#32493;&#24615;&#12290;&#23427;&#21253;&#25324;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#20195;&#29702;&#26500;&#24314;&#24773;&#26223;&#65292;&#21253;&#25324;&#23545;&#35805;&#36873;&#25321;&#65292;&#22330;&#26223;&#25552;&#21462;&#65292;CoT&#23436;&#25104;&#21644;&#22330;&#26223;&#22686;&#24378;&#31561;&#38454;&#27573;&#65292;&#26368;&#32456;&#23548;&#33268;LLMs&#30340;&#35757;&#32451;&#38454;&#27573;&#12290;&#36825;&#31181;&#26041;&#27861;&#20284;&#20046;&#25552;&#39640;&#20102;&#20195;&#29702;&#22312;&#22797;&#26434;&#30340;&#22810;&#36718;&#23545;&#35805;&#20013;&#30340;&#21487;&#25511;&#24615;&#21644;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#22312;&#25151;&#22320;&#20135;&#38144;&#21806;&#29615;&#22659;&#20013;&#30340;&#21021;&#27493;&#35780;&#20272;&#34920;&#26126;&#65292;RAISE&#30456;&#23545;&#20110;&#20256;&#32479;&#20195;&#29702;&#26377;&#19968;&#20123;&#20248;&#21183;&#65292;&#34920;&#26126;&#23427;&#22312;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#28508;&#21147;&#12290;&#36890;&#36807;&#25552;&#20379;&#19968;&#20010;&#24378;&#22823;&#30340;&#26694;&#26550;&#26469;&#24320;&#21457;&#26356;&#20855;&#19978;&#19979;&#25991;&#24863;&#30693;&#21644;&#22810;&#21151;&#33021;&#30340;&#23545;&#35805;&#20195;&#29702;&#65292;&#36825;&#39033;&#24037;&#20316;&#20026;AI&#39046;&#22495;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces RAISE (Reasoning and Acting through Scratchpad and Examples), an advanced architecture enhancing the integration of Large Language Models (LLMs) like GPT-4 into conversational agents. RAISE, an enhancement of the ReAct framework, incorporates a dual-component memory system, mirroring human short-term and long-term memory, to maintain context and continuity in conversations. It entails a comprehensive agent construction scenario, including phases like Conversation Selection, Scene Extraction, CoT Completion, and Scene Augmentation, leading to the LLMs Training phase. This approach appears to enhance agent controllability and adaptability in complex, multi-turn dialogues. Our preliminary evaluations in a real estate sales context suggest that RAISE has some advantages over traditional agents, indicating its potential for broader applications. This work contributes to the AI field by providing a robust framework for developing more context-aware and versatile convers
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#29616;&#20195;&#33310;&#21644;&#34920;&#28436;&#33402;&#26415;&#20013;&#30340;&#19977;&#32500;&#20154;&#20307;&#24418;&#29366;&#21644;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#24103;&#26041;&#27861;&#22312;&#29616;&#20195;&#33310;&#36424;&#34920;&#28436;&#20013;&#30340;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#27604;&#21333;&#24103;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2401.02383</link><description>&lt;p&gt;
&#29616;&#20195;&#33310;&#24212;&#29992;&#20013;&#19977;&#32500;&#20154;&#20307;&#23039;&#21183;&#21644;&#24418;&#29366;&#20272;&#35745;&#26041;&#27861;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Survey of 3D Human Body Pose and Shape Estimation Methods for Contemporary Dance Applications. (arXiv:2401.02383v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.02383
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;&#29616;&#20195;&#33310;&#21644;&#34920;&#28436;&#33402;&#26415;&#20013;&#30340;&#19977;&#32500;&#20154;&#20307;&#24418;&#29366;&#21644;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#21457;&#29616;&#22810;&#24103;&#26041;&#27861;&#22312;&#29616;&#20195;&#33310;&#36424;&#34920;&#28436;&#20013;&#30340;&#23039;&#21183;&#20272;&#35745;&#26041;&#38754;&#27604;&#21333;&#24103;&#26041;&#27861;&#25928;&#26524;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;RGB&#22270;&#20687;&#20013;&#20272;&#35745;&#19977;&#32500;&#20154;&#20307;&#24418;&#29366;&#21644;&#23039;&#21183;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#20855;&#26377;&#22686;&#24378;/&#34394;&#25311;&#29616;&#23454;&#12289;&#21307;&#30103;&#20445;&#20581;&#21644;&#20581;&#36523;&#25216;&#26415;&#20197;&#21450;&#34394;&#25311;&#38646;&#21806;&#31561;&#28508;&#22312;&#24212;&#29992;&#12290;&#26368;&#36817;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#20851;&#27880;&#19977;&#31181;&#31867;&#22411;&#30340;&#36755;&#20837;&#65306;i&#65289;&#21333;&#20010;&#22270;&#20687;&#65292;ii&#65289;&#22810;&#35270;&#22270;&#22270;&#20687;&#21644;iii&#65289;&#35270;&#39057;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#24182;&#27604;&#36739;&#20102;&#29616;&#20195;&#33310;&#21644;&#34920;&#28436;&#33402;&#26415;&#20013;&#30340;&#19977;&#32500;&#20154;&#20307;&#24418;&#29366;&#21644;&#23039;&#21183;&#20272;&#35745;&#26041;&#27861;&#65292;&#29305;&#21035;&#20851;&#27880;&#20154;&#20307;&#23039;&#21183;&#21644;&#31359;&#30528;&#12289;&#25668;&#20687;&#26426;&#35270;&#35282;&#12289;&#29031;&#26126;&#26465;&#20214;&#21644;&#32972;&#26223;&#26465;&#20214;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23545;&#20110;&#29616;&#20195;&#33310;&#36424;&#34920;&#28436;&#20013;&#30340;&#23039;&#21183;&#20272;&#35745;&#65292;&#22914;PHALP&#36825;&#26679;&#30340;&#22810;&#24103;&#26041;&#27861;&#27604;&#21333;&#24103;&#26041;&#27861;&#25552;&#20379;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D human body shape and pose estimation from RGB images is a challenging problem with potential applications in augmented/virtual reality, healthcare and fitness technology and virtual retail. Recent solutions have focused on three types of inputs: i) single images, ii) multi-view images and iii) videos. In this study, we surveyed and compared 3D body shape and pose estimation methods for contemporary dance and performing arts, with a special focus on human body pose and dressing, camera viewpoint, illumination conditions and background conditions. We demonstrated that multi-frame methods, such as PHALP, provide better results than single-frame method for pose estimation when dancers are performing contemporary dances.
&lt;/p&gt;</description></item><item><title>FENet&#26159;&#19968;&#20010;&#22686;&#24378;&#32858;&#28966;&#32593;&#32476;&#29992;&#20110;&#31934;&#20934;&#36710;&#36947;&#26816;&#27979;&#65292;&#36890;&#36807;&#32858;&#28966;&#37319;&#26679;&#21644;&#37096;&#20998;&#35270;&#37326;&#35780;&#20272;&#31561;&#21019;&#26032;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26354;&#32447;&#21644;&#36828;&#36317;&#31163;&#36710;&#36947;&#65292;&#22312;&#23433;&#20840;&#24615;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2312.17163</link><description>&lt;p&gt;
FENet: &#22686;&#24378;&#32858;&#28966;&#32593;&#32476;&#29992;&#20110;&#36710;&#36947;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
FENet: Focusing Enhanced Network for Lane Detection. (arXiv:2312.17163v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.17163
&lt;/p&gt;
&lt;p&gt;
FENet&#26159;&#19968;&#20010;&#22686;&#24378;&#32858;&#28966;&#32593;&#32476;&#29992;&#20110;&#31934;&#20934;&#36710;&#36947;&#26816;&#27979;&#65292;&#36890;&#36807;&#32858;&#28966;&#37319;&#26679;&#21644;&#37096;&#20998;&#35270;&#37326;&#35780;&#20272;&#31561;&#21019;&#26032;&#26041;&#27861;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#23588;&#20854;&#36866;&#29992;&#20110;&#26354;&#32447;&#21644;&#36828;&#36317;&#31163;&#36710;&#36947;&#65292;&#22312;&#23433;&#20840;&#24615;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#21040;&#20154;&#31867;&#39550;&#39542;&#27880;&#24847;&#21147;&#30340;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#39318;&#27425;&#24320;&#21457;&#20102;&#22686;&#24378;&#32858;&#28966;&#37319;&#26679;&#12289;&#37096;&#20998;&#35270;&#37326;&#35780;&#20272;&#12289;&#22686;&#24378;FPN&#26550;&#26500;&#21644;&#23450;&#21521;IoU&#25439;&#22833;&#30340;&#32593;&#32476;&#21019;&#26032;&#65292;&#35299;&#20915;&#20102;&#33258;&#21160;&#39550;&#39542;&#36710;&#36947;&#26816;&#27979;&#20013;&#30340;&#31934;&#20934;&#24615;&#38556;&#30861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#32858;&#28966;&#37319;&#26679;&#31574;&#30053;&#65292;&#24378;&#35843;&#36828;&#22788;&#37325;&#35201;&#32454;&#33410;&#65292;&#19982;&#22343;&#21248;&#26041;&#27861;&#30456;&#27604;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22522;&#20934;&#21644;&#23454;&#38469;&#26354;&#32447;/&#36828;&#36317;&#31163;&#36710;&#36947;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#23433;&#20840;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;FENetV1&#36890;&#36807;&#27169;&#25311;&#39550;&#39542;&#21592;&#35270;&#35273;&#30340;&#36879;&#35270;&#24863;&#30693;&#19978;&#19979;&#25991;&#25913;&#36827;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20256;&#32479;&#24230;&#37327;&#24615;&#33021;&#65292;&#20294;FENetV2&#22312;&#25552;&#20986;&#30340;&#37096;&#20998;&#35270;&#37326;&#20998;&#26512;&#20013;&#35777;&#26126;&#26159;&#26368;&#21487;&#38752;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#29305;&#21035;&#25512;&#33616;V2&#29992;&#20110;&#23454;&#38469;&#36710;&#36947;&#23548;&#33322;&#65292;&#23613;&#31649;&#22312;&#26631;&#20934;&#30340;&#25972;&#24352;&#22270;&#20687;&#27979;&#37327;&#19978;&#26377;&#36731;&#24494;&#30340;&#38477;&#32423;&#12290;&#26410;&#26469;&#30340;&#26041;&#21521;&#21253;&#25324;&#25910;&#38598;&#23454;&#38469;&#36947;&#36335;&#25968;&#25454;&#21644;&#38598;&#25104;&#20114;&#34917;&#30340;&#21452;&#37325;&#26694;&#26550;&#65292;&#20197;&#36827;&#19968;&#27493;&#36890;&#36807;&#20154;&#31867;&#24863;&#30693;&#25351;&#23548;&#23454;&#29616;&#31361;&#30772;&#24615;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by human driving focus, this research pioneers networks augmented with Focusing Sampling, Partial Field of View Evaluation, Enhanced FPN architecture and Directional IoU Loss - targeted innovations addressing obstacles to precise lane detection for autonomous driving. Experiments demonstrate our Focusing Sampling strategy, emphasizing vital distant details unlike uniform approaches, significantly boosts both benchmark and practical curved/distant lane recognition accuracy essential for safety. While FENetV1 achieves state-of-the-art conventional metric performance via enhancements isolating perspective-aware contexts mimicking driver vision, FENetV2 proves most reliable on the proposed Partial Field analysis. Hence we specifically recommend V2 for practical lane navigation despite fractional degradation on standard entire-image measures. Future directions include collecting on-road data and integrating complementary dual frameworks to further breakthroughs guided by human perc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#12289;&#19981;&#38656;&#35201;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#33410;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#24341;&#23548;&#26041;&#21521;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#35821;&#20041;&#23545;&#40784;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.15964</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#35821;&#20041;&#24341;&#23548;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Semantic Guidance Tuning for Text-To-Image Diffusion Models. (arXiv:2312.15964v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.15964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#12289;&#19981;&#38656;&#35201;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35843;&#33410;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#24341;&#23548;&#26041;&#21521;&#65292;&#20197;&#25552;&#39640;&#29983;&#25104;&#22270;&#20687;&#30340;&#35821;&#20041;&#23545;&#40784;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#25193;&#25955;&#27169;&#22411;&#30340;&#36827;&#23637;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#21151;&#65292;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#38646;&#26679;&#26412;&#27867;&#21270;&#33021;&#21147;&#30340;&#39640;&#36136;&#37327;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#27169;&#22411;&#22312;&#32039;&#23494;&#36981;&#24490;&#25552;&#31034;&#35821;&#20041;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32463;&#24120;&#38169;&#35823;&#22320;&#34920;&#31034;&#25110;&#24573;&#35270;&#29305;&#23450;&#23646;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#12289;&#19981;&#38656;&#35201;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#35843;&#33410;&#25193;&#25955;&#27169;&#22411;&#30340;&#24341;&#23548;&#26041;&#21521;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#25552;&#31034;&#35821;&#20041;&#20998;&#35299;&#20026;&#19968;&#32452;&#27010;&#24565;&#65292;&#24182;&#30417;&#25511;&#24341;&#23548;&#36712;&#36857;&#19982;&#27599;&#20010;&#27010;&#24565;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35266;&#23519;&#26159;&#65292;&#27169;&#22411;&#22312;&#36981;&#24490;&#25552;&#31034;&#35821;&#20041;&#26041;&#38754;&#30340;&#20559;&#24046;&#19982;&#24341;&#23548;&#19982;&#19968;&#20010;&#25110;&#22810;&#20010;&#36825;&#20123;&#27010;&#24565;&#30340;&#20559;&#31163;&#39640;&#24230;&#30456;&#20851;&#12290;&#22522;&#20110;&#36825;&#20010;&#35266;&#23519;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#23558;&#24341;&#23548;&#26041;&#21521;&#24341;&#23548;&#21040;&#27169;&#22411;&#20559;&#31163;&#30340;&#20219;&#20309;&#27010;&#24565;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#25913;&#21892;&#20102;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#30340;&#22270;&#20687;&#30340;&#35821;&#20041;&#23545;&#40784;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Text-to-Image (T2I) diffusion models have demonstrated impressive success in generating high-quality images with zero-shot generalization capabilities. Yet, current models struggle to closely adhere to prompt semantics, often misrepresenting or overlooking specific attributes. To address this, we propose a simple, training-free approach that modulates the guidance direction of diffusion models during inference. We first decompose the prompt semantics into a set of concepts, and monitor the guidance trajectory in relation to each concept. Our key observation is that deviations in model's adherence to prompt semantics are highly correlated with divergence of the guidance from one or more of these concepts. Based on this observation, we devise a technique to steer the guidance direction towards any concept from which the model diverges. Extensive experimentation validates that our method improves the semantic alignment of images generated by diffusion models in resp
&lt;/p&gt;</description></item><item><title>Auto311&#26159;&#31532;&#19968;&#20010;&#22788;&#29702;&#38750;&#32039;&#24613;&#30005;&#35805;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#20943;&#36731;&#38750;&#32039;&#24613;&#30005;&#35805;&#36127;&#25285;&#65292;&#25552;&#20379;&#24555;&#36895;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#36890;&#36807;&#39044;&#27979;&#20107;&#20214;&#31867;&#22411;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#26696;&#20214;&#25253;&#21578;&#65292;&#24182;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#26469;&#23436;&#21892;&#25253;&#21578;&#65292;&#31995;&#32479;&#19982;&#20027;&#21483;&#20154;&#20043;&#38388;&#30340;&#23545;&#35805;&#32467;&#26500;&#24471;&#21040;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2312.14185</link><description>&lt;p&gt;
Auto311: &#19968;&#31181;&#22522;&#20110;&#20449;&#24515;&#25351;&#23548;&#30340;&#33258;&#21160;&#38750;&#32039;&#24613;&#36890;&#35805;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Auto311: A Confidence-guided Automated System for Non-emergency Calls. (arXiv:2312.14185v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.14185
&lt;/p&gt;
&lt;p&gt;
Auto311&#26159;&#31532;&#19968;&#20010;&#22788;&#29702;&#38750;&#32039;&#24613;&#30005;&#35805;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#23427;&#36890;&#36807;&#20943;&#36731;&#38750;&#32039;&#24613;&#30005;&#35805;&#36127;&#25285;&#65292;&#25552;&#20379;&#24555;&#36895;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#36890;&#36807;&#39044;&#27979;&#20107;&#20214;&#31867;&#22411;&#24182;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#26696;&#20214;&#25253;&#21578;&#65292;&#24182;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#26469;&#23436;&#21892;&#25253;&#21578;&#65292;&#31995;&#32479;&#19982;&#20027;&#21483;&#20154;&#20043;&#38388;&#30340;&#23545;&#35805;&#32467;&#26500;&#24471;&#21040;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32039;&#24613;&#21644;&#38750;&#32039;&#24613;&#21709;&#24212;&#31995;&#32479;&#26159;&#22320;&#26041;&#25919;&#24220;&#25552;&#20379;&#30340;&#22522;&#26412;&#26381;&#21153;&#65292;&#23545;&#20110;&#20445;&#25252;&#29983;&#21629;&#12289;&#29615;&#22659;&#21644;&#36130;&#20135;&#33267;&#20851;&#37325;&#35201;&#12290;&#26377;&#25928;&#22788;&#29702;&#65288;&#38750;&#65289;&#32039;&#24613;&#30005;&#35805;&#23545;&#20844;&#20849;&#23433;&#20840;&#21644;&#31119;&#31049;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#20943;&#36731;&#38750;&#32039;&#24613;&#30005;&#35805;&#30340;&#36127;&#25285;&#65292;&#20127;&#38656;911&#27714;&#21161;&#30340;&#23621;&#27665;&#23558;&#33719;&#24471;&#24555;&#36895;&#26377;&#25928;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#19982;&#32435;&#20160;&#32500;&#23572;&#32039;&#24613;&#36890;&#20449;&#37096;&#38376;&#21512;&#20316;&#65292;&#20998;&#26512;&#20102;11,796&#20010;&#38750;&#32039;&#24613;&#21628;&#21483;&#24405;&#38899;&#65292;&#24182;&#24320;&#21457;&#20102;Auto311&#65292;&#31532;&#19968;&#20010;&#22788;&#29702;311&#38750;&#32039;&#24613;&#21628;&#21483;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#65288;1&#65289;&#26377;&#25928;&#21160;&#24577;&#22320;&#39044;&#27979;&#27491;&#22312;&#36827;&#34892;&#30340;&#38750;&#32039;&#24613;&#20107;&#20214;&#31867;&#22411;&#65292;&#20197;&#22312;&#36890;&#35805;&#36807;&#31243;&#20013;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#26696;&#20214;&#25253;&#21578;&#65307;&#65288;2&#65289;&#20174;&#23545;&#35805;&#19978;&#19979;&#25991;&#20013;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#23436;&#25104;&#29983;&#25104;&#30340;&#25253;&#21578;&#65307;&#65288;3&#65289;&#20197;&#20248;&#21270;&#30340;&#20449;&#24515;&#27700;&#24179;&#23433;&#25490;&#31995;&#32479;&#21644;&#20027;&#21483;&#20154;&#20043;&#38388;&#30340;&#23545;&#35805;&#32467;&#26500;&#12290;&#25105;&#20204;&#20351;&#29992;&#23454;&#38469;&#25968;&#25454;&#35780;&#20272;&#20102;&#35813;&#31995;&#32479;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emergency and non-emergency response systems are essential services provided by local governments and critical to protecting lives, the environment, and property. The effective handling of (non-)emergency calls is critical for public safety and well-being. By reducing the burden through non-emergency callers, residents in critical need of assistance through 911 will receive a fast and effective response. Collaborating with the Department of Emergency Communications (DEC) in Nashville, we analyzed 11,796 non-emergency call recordings and developed Auto311, the first automated system to handle 311 non-emergency calls, which (1) effectively and dynamically predicts ongoing non-emergency incident types to generate tailored case reports during the call; (2) itemizes essential information from dialogue contexts to complete the generated reports; and (3) strategically structures system-caller dialogues with optimized confidence. We used real-world data to evaluate the system's effectiveness a
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36825;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;LLMs&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#26032;&#30693;&#35782;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;LLMs&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#36739;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2312.05934</link><description>&lt;p&gt;
Fine-Tuning&#36824;&#26159;&#26816;&#32034;&#65311;&#27604;&#36739;&#22312;LLMs&#20013;&#30340;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
Fine-Tuning or Retrieval? Comparing Knowledge Injection in LLMs. (arXiv:2312.05934v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05934
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#27604;&#36739;&#20102;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#36825;&#20004;&#31181;&#24120;&#35265;&#26041;&#27861;&#22312;LLMs&#20013;&#30340;&#24212;&#29992;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#26032;&#30693;&#35782;&#19978;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;LLMs&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#36739;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20854;&#39044;&#35757;&#32451;&#30340;&#26435;&#37325;&#20013;&#23553;&#35013;&#20102;&#22823;&#37327;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#27491;&#22914;&#23427;&#20204;&#33021;&#22815;&#22312;&#19981;&#21516;&#39046;&#22495;&#22238;&#31572;&#21508;&#31181;&#38382;&#39064;&#25152;&#35777;&#26126;&#30340;&#37027;&#26679;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#30693;&#35782;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#65292;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#30340;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22806;&#37096;&#25968;&#25454;&#38598;&#26469;&#25972;&#21512;&#26032;&#30340;&#20449;&#24687;&#25110;&#25913;&#36827;LLMs&#22312;&#24050;&#35265;&#20449;&#24687;&#19978;&#30340;&#33021;&#21147;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#36825;&#20010;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181;&#24120;&#35265;&#30340;&#26041;&#27861;&#65306;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#21644;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#12290;&#25105;&#20204;&#22312;&#19981;&#21516;&#20027;&#39064;&#30340;&#21508;&#31181;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#34429;&#28982;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#33021;&#22815;&#25552;&#20379;&#19968;&#23450;&#30340;&#25913;&#36827;&#65292;&#20294;RAG&#22312;&#29616;&#26377;&#30693;&#35782;&#21644;&#23436;&#20840;&#26032;&#30693;&#35782;&#19978;&#22987;&#32456;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#24456;&#38590;&#36890;&#36807;&#26080;&#30417;&#30563;&#30340;&#24494;&#35843;&#26469;&#23398;&#20064;&#26032;&#30340;&#20107;&#23454;&#20449;&#24687;&#65292;&#24182;&#19988;&#26292;&#38706;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) encapsulate a vast amount of factual information within their pre-trained weights, as evidenced by their ability to answer diverse questions across different domains. However, this knowledge is inherently limited, relying heavily on the characteristics of the training data. Consequently, using external datasets to incorporate new information or refine the capabilities of LLMs on previously seen information poses a significant challenge. In this study, we compare two common approaches: unsupervised fine-tuning and retrieval-augmented generation (RAG). We evaluate both approaches on a variety of knowledge-intensive tasks across different topics. Our findings reveal that while unsupervised fine-tuning offers some improvement, RAG consistently outperforms it, both for existing knowledge encountered during training and entirely new knowledge. Moreover, we find that LLMs struggle to learn new factual information through unsupervised fine-tuning, and that exposing
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;AI&#22686;&#24378;&#20102;&#20010;&#20307;&#21019;&#36896;&#21147;&#65292;&#20294;&#38477;&#20302;&#20102;&#26032;&#20869;&#23481;&#30340;&#38598;&#20307;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2312.00506</link><description>&lt;p&gt;
&#29983;&#25104;AI&#22686;&#24378;&#20102;&#20010;&#20307;&#21019;&#36896;&#21147;&#65292;&#20294;&#38477;&#20302;&#20102;&#26032;&#20869;&#23481;&#30340;&#38598;&#20307;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
Generative AI enhances individual creativity but reduces the collective diversity of novel content. (arXiv:2312.00506v2 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.00506
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;AI&#22686;&#24378;&#20102;&#20010;&#20307;&#21019;&#36896;&#21147;&#65292;&#20294;&#38477;&#20302;&#20102;&#26032;&#20869;&#23481;&#30340;&#38598;&#20307;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21019;&#36896;&#21147;&#26159;&#20154;&#31867;&#30340;&#26680;&#24515;&#12290;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#65292;&#21253;&#25324;&#36234;&#26469;&#36234;&#24378;&#22823;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#36890;&#36807;&#25552;&#20379;&#26032;&#30340;&#24819;&#27861;&#20351;&#20154;&#31867;&#26356;&#20855;&#21019;&#36896;&#21147;&#65292;&#25110;&#36890;&#36807;&#38170;&#23450;&#20110;GenAI&#30340;&#24819;&#27861;&#32780;&#21464;&#24471;&#19981;&#37027;&#20040;&#21019;&#36896;&#24615;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;&#32447;&#23454;&#39564;&#30740;&#31350;&#25506;&#31350;&#20102;GenAI&#24819;&#27861;&#23545;&#19968;&#31687;&#30701;&#31687;&#23567;&#35828;&#21019;&#20316;&#30340;&#22240;&#26524;&#24433;&#21709;&#65292;&#20854;&#20013;&#19968;&#20123;&#20316;&#32773;&#21487;&#20197;&#20174;GenAI&#24179;&#21488;&#33719;&#21462;&#25925;&#20107;&#21019;&#24847;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#33719;&#21462;GenAI&#24819;&#27861;&#23548;&#33268;&#25925;&#20107;&#34987;&#35780;&#20026;&#26356;&#26377;&#21019;&#36896;&#21147;&#12289;&#20889;&#24471;&#26356;&#22909;&#21644;&#26356;&#20196;&#20154;&#24841;&#24742;&#65292;&#29305;&#21035;&#26159;&#22312;&#21019;&#36896;&#21147;&#36739;&#20302;&#30340;&#20316;&#32773;&#20013;&#12290;&#28982;&#32780;&#65292;GenAI&#21551;&#29992;&#30340;&#25925;&#20107;&#20043;&#38388;&#26356;&#30456;&#20284;&#65292;&#32780;&#19981;&#26159;&#20165;&#30001;&#20154;&#31867;&#21019;&#20316;&#30340;&#25925;&#20107;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#20010;&#20307;&#21019;&#36896;&#21147;&#22686;&#21152;&#30340;&#21516;&#26102;&#65292;&#38598;&#20307;&#26032;&#39062;&#24615;&#21487;&#33021;&#20250;&#20943;&#23569;&#12290;&#36825;&#31181;&#21160;&#24577;&#31867;&#20284;&#20110;&#31038;&#20250;&#22256;&#22659;&#65306;&#36890;&#36807;GenAI&#65292;&#20010;&#21035;&#20316;&#23478;&#33021;&#21463;&#30410;&#65292;&#20294;&#21487;&#33021;&#20250;&#20135;&#29983;&#26356;&#31364;&#33539;&#22260;&#30340;&#26032;&#20869;&#23481;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#30740;&#31350;&#20154;&#21592;&#12289;&#20915;&#31574;&#32773;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Creativity is core to being human. Generative artificial intelligence (GenAI) -- including ever more powerful large language models (LLMs) -- holds promise for humans to be more creative by offering new ideas, or less creative by anchoring on GenAI ideas. We study the causal impact of GenAI ideas on the production of a short story in an online experimental study where some writers could obtain story ideas from a GenAI platform. We find that access to GenAI ideas causes stories to be evaluated as more creative, better written, and more enjoyable, especially among less creative writers. However, GenAI-enabled stories are more similar to each other than stories by humans alone. These results point to an increase in individual creativity at the risk of losing collective novelty. This dynamic resembles a social dilemma: with GenAI, individual writers are better off, but collectively a narrower scope of novel content may be produced. Our results have implications for researchers, policy-make
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;WarAgent&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#27169;&#25311;&#21382;&#21490;&#22269;&#38469;&#20914;&#31361;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#20854;&#25928;&#26524;&#21644;&#30740;&#31350;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#25112;&#20105;&#30340;&#24341;&#21457;&#22240;&#32032;&#21644;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2311.17227</link><description>&lt;p&gt;
&#25112;&#20105;&#19982;&#21644;&#24179;&#65288;WarAgent&#65289;&#65306;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#19990;&#30028;&#22823;&#25112;&#22810;&#26234;&#33021;&#20307;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
War and Peace (WarAgent): Large Language Model-based Multi-Agent Simulation of World Wars. (arXiv:2311.17227v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.17227
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;WarAgent&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;AI&#31995;&#32479;&#65292;&#29992;&#20110;&#27169;&#25311;&#21382;&#21490;&#22269;&#38469;&#20914;&#31361;&#65292;&#24182;&#36890;&#36807;&#35780;&#20272;&#20854;&#25928;&#26524;&#21644;&#30740;&#31350;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25506;&#35752;&#20102;&#25112;&#20105;&#30340;&#24341;&#21457;&#22240;&#32032;&#21644;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33021;&#21542;&#22312;&#21382;&#21490;&#30340;&#21313;&#23383;&#36335;&#21475;&#36991;&#20813;&#25112;&#20105;&#65311;&#36825;&#20010;&#38382;&#39064;&#22312;&#20154;&#31867;&#21382;&#21490;&#19978;&#19968;&#30452;&#34987;&#20010;&#20154;&#12289;&#23398;&#32773;&#12289;&#20915;&#31574;&#32773;&#21644;&#32452;&#32455;&#36861;&#27714;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#26681;&#25454;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WarAgent&#30340;LLM&#39537;&#21160;&#30340;&#22810;&#26234;&#33021;&#20307;AI&#31995;&#32479;&#65292;&#20197;&#27169;&#25311;&#21442;&#19982;&#22269;&#23478;&#65292;&#22312;&#21382;&#21490;&#19978;&#30340;&#22269;&#38469;&#20914;&#31361;&#65292;&#21253;&#25324;&#31532;&#19968;&#27425;&#19990;&#30028;&#22823;&#25112;&#65288;WWI&#65289;&#12289;&#31532;&#20108;&#27425;&#19990;&#30028;&#22823;&#25112;&#65288;WWII&#65289;&#21644;&#20013;&#22269;&#21476;&#20195;&#30340;&#25112;&#22269;&#26102;&#26399;&#65288;WSP&#65289;&#20013;&#30340;&#20915;&#31574;&#21644;&#21518;&#26524;&#12290;&#36890;&#36807;&#35780;&#20272;&#27169;&#25311;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23574;&#31471;AI&#31995;&#32479;&#22312;&#30740;&#31350;&#22797;&#26434;&#30340;&#38598;&#20307;&#20154;&#31867;&#34892;&#20026;&#65292;&#22914;&#22269;&#38469;&#20914;&#31361;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#33021;&#21147;&#30340;&#36827;&#23637;&#21644;&#23616;&#38480;&#24615;&#12290;&#22312;&#36825;&#20123;&#27169;&#25311;&#20013;&#65292;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#20063;&#20026;&#32771;&#23519;&#24341;&#21457;&#25112;&#20105;&#30340;&#35302;&#21457;&#22240;&#32032;&#21644;&#26465;&#20214;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#20102;&#35299;&#25112;&#20105;&#30340;&#35302;&#21457;&#22240;&#32032;&#21644;&#26465;&#20214;&#25552;&#20379;&#20102;&#26032;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Can we avoid wars at the crossroads of history? This question has been pursued by individuals, scholars, policymakers, and organizations throughout human history. In this research, we attempt to answer the question based on the recent advances of Artificial Intelligence (AI) and Large Language Models (LLMs). We propose \textbf{WarAgent}, an LLM-powered multi-agent AI system, to simulate the participating countries, their decisions, and the consequences, in historical international conflicts, including the World War I (WWI), the World War II (WWII), and the Warring States Period (WSP) in Ancient China. By evaluating the simulation effectiveness, we examine the advancements and limitations of cutting-edge AI systems' abilities in studying complex collective human behaviors such as international conflicts under diverse settings. In these simulations, the emergent interactions among agents also offer a novel perspective for examining the triggers and conditions that lead to war. Our findin
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#23398;&#26415;&#30028;&#23545;&#20110;&#22312;&#31295;&#20214;&#20934;&#22791;&#20013;&#20351;&#29992;AI&#26159;&#21542;&#26377;&#24517;&#35201;&#36827;&#34892;&#25259;&#38706;&#30340;&#35266;&#28857;&#65292;&#24182;&#30740;&#31350;&#20102;&#26816;&#27979;&#22120;&#23545;&#20110;&#23398;&#26415;&#20889;&#20316;&#20013;&#20351;&#29992;AI&#30340;&#21453;&#24212;&#12290;</title><link>http://arxiv.org/abs/2311.14720</link><description>&lt;p&gt;
&#23398;&#26415;&#26399;&#21002;&#20013;AI&#22312;&#31295;&#20214;&#20934;&#22791;&#20013;&#30340;&#24212;&#29992;&#30340;&#24863;&#30693;&#19982;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Perceptions and Detection of AI Use in Manuscript Preparation for Academic Journals. (arXiv:2311.14720v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.14720
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;&#23398;&#26415;&#30028;&#23545;&#20110;&#22312;&#31295;&#20214;&#20934;&#22791;&#20013;&#20351;&#29992;AI&#26159;&#21542;&#26377;&#24517;&#35201;&#36827;&#34892;&#25259;&#38706;&#30340;&#35266;&#28857;&#65292;&#24182;&#30740;&#31350;&#20102;&#26816;&#27979;&#22120;&#23545;&#20110;&#23398;&#26415;&#20889;&#20316;&#20013;&#20351;&#29992;AI&#30340;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26032;&#20852;&#33021;&#21147;&#20351;&#24471;&#20687;ChatGPT&#21644;Bard&#36825;&#26679;&#30340;&#24037;&#20855;&#25104;&#20026;&#21487;&#33021;&#65292;&#36825;&#26082;&#24102;&#26469;&#20102;&#23545;AI&#23545;&#23398;&#26415;&#20889;&#20316;&#30340;&#24433;&#21709;&#30340;&#20852;&#22859;&#21644;&#25285;&#24551;&#12290;&#20026;&#20102;&#24212;&#23545;&#23545;AI&#20351;&#29992;&#30340;&#19981;&#26029;&#22686;&#21152;&#30340;&#25285;&#24551;&#65292;&#23398;&#26415;&#20986;&#29256;&#29289;&#30340;&#20316;&#32773;&#21487;&#33021;&#20915;&#23450;&#33258;&#24895;&#25259;&#38706;&#20182;&#20204;&#22312;&#20462;&#25913;&#31295;&#20214;&#26102;&#20351;&#29992;&#30340;AI&#24037;&#20855;&#65292;&#26399;&#21002;&#21644;&#20250;&#35758;&#20063;&#21487;&#20197;&#24320;&#22987;&#35201;&#27714;&#25259;&#38706;&#21644;/&#25110;&#20351;&#29992;&#26816;&#27979;&#26381;&#21153;&#65292;&#23601;&#20687;&#35768;&#22810;&#25945;&#24072;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#23545;&#23398;&#29983;&#30340;&#20889;&#20316;&#36827;&#34892;&#26816;&#27979;&#19968;&#26679;&#12290;&#37492;&#20110;&#36825;&#20123;&#36924;&#36817;&#30340;&#21487;&#33021;&#24615;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#23398;&#32773;&#20204;&#26159;&#21542;&#35748;&#20026;&#26377;&#24517;&#35201;&#25253;&#21578;AI&#22312;&#31295;&#20214;&#20934;&#22791;&#20013;&#30340;&#20351;&#29992;&#65292;&#24182;&#30740;&#31350;&#20102;&#26816;&#27979;&#22120;&#23545;&#23398;&#26415;&#20889;&#20316;&#20013;&#20351;&#29992;AI&#30340;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergent abilities of Large Language Models (LLMs), which power tools like ChatGPT and Bard, have produced both excitement and worry about how AI will impact academic writing. In response to rising concerns about AI use, authors of academic publications may decide to voluntarily disclose any AI tools they use to revise their manuscripts, and journals and conferences could begin mandating disclosure and/or turn to using detection services, as many teachers have done with student writing in class settings. Given these looming possibilities, we investigate whether academics view it as necessary to report AI use in manuscript preparation and how detectors react to the use of AI in academic writing.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2311.11482</link><description>&lt;p&gt;
AGI&#31995;&#32479;&#30340;&#20803;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Meta Prompting for AGI Systems. (arXiv:2311.11482v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.11482
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#30740;&#31350;&#20102;&#20803;&#25552;&#31034;&#25216;&#26415;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#37325;&#22609;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#27169;&#24577;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#24378;&#35843;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#20803;&#25552;&#31034;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#35299;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#25928;&#29575;&#65292;&#24182;&#19988;&#33021;&#22815;&#19982;&#23569;&#26679;&#26412;&#26041;&#27861;&#36827;&#34892;&#20844;&#24179;&#30340;&#27604;&#36739;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#25552;&#31034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20803;&#25552;&#31034;(meta prompting)&#30340;&#20840;&#38754;&#30740;&#31350;&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#25216;&#26415;&#65292;&#37325;&#26032;&#22609;&#36896;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#12289;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#38382;&#39064;&#35299;&#20915;&#21644;&#25968;&#25454;&#35299;&#37322;&#26041;&#38754;&#30340;&#21033;&#29992;&#12290;&#22522;&#20110;&#31867;&#22411;&#29702;&#35770;&#21644;&#33539;&#30068;&#35770;&#65292;&#20803;&#25552;&#31034;&#27880;&#37325;&#20449;&#24687;&#30340;&#32467;&#26500;&#21644;&#21477;&#27861;&#65292;&#32780;&#19981;&#26159;&#20256;&#32479;&#20197;&#20869;&#23481;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20803;&#25552;&#31034;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#24182;&#23558;&#20854;&#19982;&#23569;&#26679;&#26412;&#25552;&#31034;(few-shot prompting)&#21306;&#20998;&#24320;&#26469;&#65292;&#24182;&#24378;&#35843;&#20854;&#22312;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#37325;&#28857;&#20851;&#27880;&#23558;&#20803;&#25552;&#31034;&#25193;&#23637;&#21040;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#19978;&#65292;&#23637;&#31034;&#22914;&#20309;&#23558;&#22797;&#26434;&#38382;&#39064;&#25286;&#20998;&#25104;&#36739;&#20026;&#31616;&#21333;&#30340;&#23376;&#38382;&#39064;&#65292;&#25552;&#39640;&#20196;&#29260;&#25928;&#29575;&#65292;&#24182;&#20351;&#38382;&#39064;&#27714;&#35299;&#30340;&#27604;&#36739;&#26356;&#21152;&#20844;&#24179;&#65292;&#23588;&#20854;&#26159;&#19982;&#23569;&#26679;&#26412;&#31034;&#20363;&#26041;&#27861;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#24341;&#20837;&#20102;&#20803;&#25552;&#31034;&#29992;&#20110;&#25552;&#31034;&#20219;&#21153;&#65292;&#20801;&#35768;LLMs&#20197;&#36845;&#20195;&#30340;&#20803;&#32534;&#31243;&#24418;&#24335;&#33258;&#21160;&#29983;&#25104;&#26032;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive study of Meta Prompting, an innovative technique reshaping the utilization of large language models (LLMs), multi-modal foundation models, and AI systems in problem-solving and data interpretation. Grounded in type theory and category theory, Meta Prompting emphasizes the structure and syntax of information over traditional content-centric methods. The paper explores the formal definitions of Meta Prompting (MP), sets it apart from Few-Shot Prompting, and underlines its effectiveness in various AI applications. A key focus is on extending Meta Prompting to complex reasoning tasks, showing how it effectively deconstructs intricate problems into simpler sub-problems, enhancing token efficiency and enabling more equitable problem-solving comparisons, especially against few-shot example methods. Additionally, the paper introduces Meta Prompting for Prompting Tasks, allowing LLMs to self-generate new prompts in an iterative, metaprogramming-like manner. T
&lt;/p&gt;</description></item><item><title>Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.17807</link><description>&lt;p&gt;
Clover: &#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Clover: Closed-Loop Verifiable Code Generation. (arXiv:2310.17807v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17807
&lt;/p&gt;
&lt;p&gt;
Clover&#26159;&#19968;&#31181;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36719;&#20214;&#24320;&#21457;&#20013;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;&#26159;&#19968;&#20010;&#24555;&#36895;&#22686;&#38271;&#30340;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#27809;&#26377;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#29983;&#25104;&#30340;&#20195;&#30721;&#30340;&#27491;&#30830;&#24615;&#65292;&#36825;&#20010;&#36235;&#21183;&#21487;&#33021;&#20250;&#23548;&#33268;&#35768;&#22810;&#19981;&#33391;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#30340;&#24895;&#26223;&#65306;Clover&#33539;&#24335;&#65292;&#21363;&#38381;&#29615;&#21487;&#39564;&#35777;&#20195;&#30721;&#29983;&#25104;&#65292;&#23427;&#23558;&#27491;&#30830;&#24615;&#26816;&#26597;&#31616;&#21270;&#20026;&#26356;&#21487;&#35775;&#38382;&#30340;&#19968;&#33268;&#24615;&#26816;&#26597;&#38382;&#39064;&#12290;&#22312;Clover&#30340;&#26680;&#24515;&#26159;&#19968;&#20010;&#26816;&#26597;&#22120;&#65292;&#23427;&#22312;&#20195;&#30721;&#12289;docstrings&#21644;&#24418;&#24335;&#27880;&#37322;&#20043;&#38388;&#36827;&#34892;&#19968;&#33268;&#24615;&#26816;&#26597;&#12290;&#35813;&#26816;&#26597;&#22120;&#20351;&#29992;&#20102;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#39062;&#38598;&#25104;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#26469;&#25903;&#25345;&#25105;&#20204;&#30340;&#35770;&#28857;&#65292;&#21363;Clover&#22312;&#19968;&#33268;&#24615;&#26816;&#26597;&#26041;&#38754;&#24212;&#35813;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#36824;&#22312;&#19968;&#20010;&#30001;&#25163;&#24037;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#65288;CloverBench&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#35777;&#35843;&#26597;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#27880;&#37322;&#30340;Dafny&#31243;&#24207;&#65292;&#38590;&#24230;&#27700;&#24179;&#19982;&#25945;&#31185;&#20070;&#30456;&#24403;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;
&lt;/p&gt;
&lt;p&gt;
The use of large language models for code generation is a rapidly growing trend in software development. However, without effective methods for ensuring the correctness of generated code, this trend could lead to any number of undesirable outcomes. In this paper, we lay out a vision for addressing this challenge: the Clover paradigm, short for Closed-Loop Verifiable Code Generation, which reduces correctness checking to the more accessible problem of consistency checking. At the core of Clover lies a checker that performs consistency checks among code, docstrings, and formal annotations. The checker is implemented using a novel integration of formal verification tools and large language models. We provide a theoretical analysis to support our thesis that Clover should be effective at consistency checking. We also empirically investigate its feasibility on a hand-designed dataset (CloverBench) featuring annotated Dafny programs at a textbook level of difficulty. Experimental results sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#35843;&#24230;&#31574;&#30053;&#26469;&#35299;&#20915;&#22823;&#22411;&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#23454;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#35843;&#24230;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.15706</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#22810;&#26679;&#21270;&#35843;&#24230;&#31574;&#30053;&#26469;&#35299;&#20915;&#22823;&#22411;&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#23454;&#20363;
&lt;/p&gt;
&lt;p&gt;
Solving large flexible job shop scheduling instances by generating a diverse set of scheduling policies with deep reinforcement learning. (arXiv:2310.15706v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.15706
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#35843;&#24230;&#31574;&#30053;&#26469;&#35299;&#20915;&#22823;&#22411;&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#23454;&#20363;&#30340;&#26041;&#27861;&#65292;&#24182;&#24212;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#35843;&#24230;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26580;&#24615;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;FJSSP&#65289;&#22312;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#35768;&#22810;&#21551;&#21457;&#24335;&#12289;&#31934;&#30830;&#21644;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24037;&#19994;&#23545;&#23454;&#26102;&#21709;&#24212;&#31361;&#21457;&#20107;&#20214;&#30340;&#38656;&#27714;&#20135;&#29983;&#20102;&#22312;&#20960;&#31186;&#20869;&#29983;&#25104;&#26032;&#35843;&#24230;&#30340;&#24517;&#35201;&#24615;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#21482;&#26377;&#35843;&#24230;&#35268;&#21017;&#65288;DRs&#65289;&#33021;&#22815;&#22312;&#32422;&#26463;&#19979;&#29983;&#25104;&#35843;&#24230;&#65292;&#23613;&#31649;&#20854;&#36136;&#37327;&#21487;&#20197;&#24471;&#21040;&#25913;&#36827;&#12290;&#20026;&#20102;&#25913;&#21892;&#32467;&#26524;&#65292;&#26368;&#36817;&#30340;&#26041;&#27861;&#23558;FJSSP&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#65292;&#24182;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#19968;&#20010;&#31574;&#30053;&#65292;&#23558;&#25805;&#20316;&#20998;&#37197;&#21040;&#26426;&#22120;&#19978;&#29983;&#25104;&#26368;&#20248;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#22823;&#22411;&#30340;FJSSP&#23454;&#20363;&#20013;&#20173;&#28982;&#26377;&#25913;&#36827;&#30340;&#31354;&#38388;&#65292;&#32780;&#36825;&#22312;&#23454;&#38469;&#24773;&#20917;&#20013;&#24456;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#25552;&#20986;&#19968;&#31181;&#33021;&#22815;&#31283;&#20581;&#35299;&#20915;&#22823;&#22411;FJSSP&#23454;&#20363;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Flexible Job Shop Scheduling Problem (FJSSP) has been extensively studied in the literature, and multiple approaches have been proposed within the heuristic, exact, and metaheuristic methods. However, the industry's demand to be able to respond in real-time to disruptive events has generated the necessity to be able to generate new schedules within a few seconds. Among these methods, under this constraint, only dispatching rules (DRs) are capable of generating schedules, even though their quality can be improved. To improve the results, recent methods have been proposed for modeling the FJSSP as a Markov Decision Process (MDP) and employing reinforcement learning to create a policy that generates an optimal solution assigning operations to machines. Nonetheless, there is still room for improvement, particularly in the larger FJSSP instances which are common in real-world scenarios. Therefore, the objective of this paper is to propose a method capable of robustly solving large insta
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#29305;&#23450;&#23454;&#20363;&#19978;&#36827;&#34892;&#39640;&#25928;&#24494;&#35843;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#22810;&#34892;&#20026;&#35774;&#32622;&#21644;&#31163;&#25955;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2310.14526</link><description>&lt;p&gt;
&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#23454;&#29616;&#38646;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Zero Shot Learning in Restless Multi-armed Bandits. (arXiv:2310.14526v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14526
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#20013;&#30340;&#38646;&#26679;&#26412;&#23398;&#20064;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#29305;&#23450;&#23454;&#20363;&#19978;&#36827;&#34892;&#39640;&#25928;&#24494;&#35843;&#65292;&#21516;&#26102;&#36866;&#29992;&#20110;&#22810;&#34892;&#20026;&#35774;&#32622;&#21644;&#31163;&#25955;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26469;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#19968;&#31867;&#36164;&#28304;&#20998;&#37197;&#38382;&#39064;&#8212;&#8212;&#19981;&#26029;&#21464;&#21270;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;RMABs&#65289;&#65292;&#35813;&#38382;&#39064;&#22312;&#21307;&#30103;&#20445;&#20581;&#12289;&#22312;&#32447;&#24191;&#21578;&#21644;&#21453;&#30423;&#29454;&#31561;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#12290;&#20808;&#21069;&#30340;RMAB&#30740;&#31350;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#27809;&#26377;&#20805;&#20998;&#35299;&#20915;&#36830;&#32493;&#29366;&#24577;&#38382;&#39064;&#65292;&#24182;&#19988;&#22312;&#22810;&#20010;&#30495;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#24403;&#36172;&#21338;&#26426;&#30340;&#20837;&#36873;&#21644;&#36864;&#20986;&#19981;&#26029;&#21457;&#29983;&#26102;&#65292;&#38656;&#35201;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;PreFeRMAB&#65289;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#23545;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#24191;&#27867;RMAB&#38382;&#39064;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#19988;&#21487;&#20197;&#27604;&#20174;&#22836;&#35757;&#32451;&#26356;&#21152;&#39640;&#25928;&#22320;&#23545;&#29305;&#23450;&#23454;&#20363;&#36827;&#34892;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#36866;&#29992;&#20110;&#19968;&#33324;&#30340;&#22810;&#34892;&#20026;&#35774;&#32622;&#21644;&#31163;&#25955;&#25110;&#36830;&#32493;&#29366;&#24577;&#31354;&#38388;&#12290;&#20026;&#20102;&#23454;&#29616;&#24555;&#36895;&#27867;&#21270;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21333;&#19968;&#31574;&#30053;&#32593;&#32476;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#29305;&#24449;&#20449;&#24687;&#24182;&#37319;&#29992;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Restless multi-arm bandits (RMABs), a class of resource allocation problems with broad application in areas such as healthcare, online advertising, and anti-poaching, have recently been studied from a multi-agent reinforcement learning perspective. Prior RMAB research suffers from several limitations, e.g., it fails to adequately address continuous states, and requires retraining from scratch when arms opt-in and opt-out over time, a common challenge in many real world applications. We address these limitations by developing a neural network-based pre-trained model (PreFeRMAB) that has general zero-shot ability on a wide range of previously unseen RMABs, and which can be fine-tuned on specific instances in a more sample-efficient way than retraining from scratch. Our model also accommodates general multi-action settings and discrete or continuous state spaces. To enable fast generalization, we learn a novel single policy network model that utilizes feature information and employs a tra
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RRL&#30340;&#26032;&#22411;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#38750;&#27169;&#31946;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;&#30340;&#33391;&#22909;&#21487;&#25193;&#23637;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.14336</link><description>&lt;p&gt;
&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#20197;&#23454;&#29616;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Rules for Scalable Data Representation and Classification. (arXiv:2310.14336v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.14336
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;RRL&#30340;&#26032;&#22411;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#21487;&#35299;&#37322;&#30340;&#38750;&#27169;&#31946;&#35268;&#21017;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;&#30340;&#33391;&#22909;&#21487;&#25193;&#23637;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#65288;&#22914;&#20915;&#31574;&#26641;&#65289;&#22312;&#38656;&#35201;&#39640;&#27169;&#22411;&#35299;&#37322;&#24615;&#30340;&#22330;&#26223;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#36879;&#26126;&#30340;&#20869;&#37096;&#32467;&#26500;&#21644;&#33391;&#22909;&#30340;&#27169;&#22411;&#34920;&#36798;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#31163;&#25955;&#30340;&#21442;&#25968;&#21644;&#32467;&#26500;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#27169;&#22411;&#22312;&#20248;&#21270;&#26041;&#38754;&#24456;&#38590;&#24212;&#23545;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12290;&#38598;&#25104;&#26041;&#27861;&#21644;&#27169;&#31946;/&#36719;&#35268;&#21017;&#36890;&#24120;&#29992;&#20110;&#25552;&#39640;&#24615;&#33021;&#65292;&#20294;&#20250;&#29306;&#29298;&#27169;&#22411;&#30340;&#35299;&#37322;&#24615;&#12290;&#20026;&#20102;&#33719;&#24471;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#65292;&#31216;&#20026;&#22522;&#20110;&#35268;&#21017;&#30340;&#34920;&#31034;&#23398;&#20064;&#22120;&#65288;RRL&#65289;&#65292;&#23427;&#21487;&#20197;&#33258;&#21160;&#23398;&#20064;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#21644;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#30340;&#38750;&#27169;&#31946;&#35268;&#21017;&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#19981;&#21487;&#24494;&#20998;&#30340;RRL&#65292;&#25105;&#20204;&#23558;&#20854;&#26144;&#23556;&#21040;&#36830;&#32493;&#31354;&#38388;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;&#26799;&#24230;&#23884;&#20837;&#30340;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#26799;&#24230;&#19979;&#38477;&#30452;&#25509;&#20248;&#21270;&#31163;&#25955;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36923;&#36753;&#28608;&#27963;&#20989;&#25968;&#65292;&#20197;&#22686;&#21152;RRL&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#36827;&#34892;&#21028;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rule-based models, e.g., decision trees, are widely used in scenarios demanding high model interpretability for their transparent inner structures and good model expressivity. However, rule-based models are hard to optimize, especially on large data sets, due to their discrete parameters and structures. Ensemble methods and fuzzy/soft rules are commonly used to improve performance, but they sacrifice the model interpretability. To obtain both good scalability and interpretability, we propose a new classifier, named Rule-based Representation Learner (RRL), that automatically learns interpretable non-fuzzy rules for data representation and classification. To train the non-differentiable RRL effectively, we project it to a continuous space and propose a novel training method, called Gradient Grafting, that can directly optimize the discrete model using gradient descent. A novel design of logical activation functions is also devised to increase the scalability of RRL and enable it to discr
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2310.09499</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19968;&#27425;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
One-Shot Sensitivity-Aware Mixed Sparsity Pruning for Large Language Models. (arXiv:2310.09499v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09499
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21098;&#26525;&#33267;&#33267;&#23569;50&#65285;&#30340;&#31232;&#30095;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#31232;&#30095;&#24615;&#27700;&#24179;&#21644;&#20943;&#23569;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#36824;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#31995;&#21015;&#20013;&#30340;&#21508;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#31181;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#25512;&#29702;&#24310;&#36831;&#65292;&#24040;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29992;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#37327;&#21270;&#12289;&#21098;&#26525;&#21644;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;LLMs&#30340;&#25928;&#29575;&#25104;&#20026;LLM&#30740;&#31350;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Hessian&#25935;&#24863;&#24230;&#24863;&#30693;&#28151;&#21512;&#31232;&#30095;&#21270;&#21098;&#26525;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;LLMs&#21098;&#26525;&#33267;&#33267;&#23569;50%&#30340;&#31232;&#30095;&#24615;&#65292;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;&#23427;&#26681;&#25454;&#25935;&#24863;&#24230;&#33258;&#36866;&#24212;&#22320;&#20998;&#37197;&#31232;&#30095;&#24615;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#38477;&#20302;&#21098;&#26525;&#24341;&#36215;&#30340;&#35823;&#24046;&#65292;&#21516;&#26102;&#20445;&#25345;&#25972;&#20307;&#31232;&#30095;&#24615;&#27700;&#24179;&#12290;&#24403;&#31232;&#30095;&#24230;&#38750;&#24120;&#39640;&#26102;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#26356;&#21152;&#26126;&#26174;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#37327;&#21270;&#20860;&#23481;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#21387;&#32553;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Various Large Language Models(LLMs) from the Generative Pretrained Transformer~(GPT) family have achieved outstanding performances in a wide range of text generation tasks. However, the enormous model sizes have hindered their practical use in real-world applications due to high inference latency. Therefore, improving the efficiencies of LLMs through quantization, pruning, and other means has been a key issue in LLM studies. In this work, we propose a method based on Hessian sensitivity-aware mixed sparsity pruning to prune LLMs to at least 50\% sparsity without the need of any retraining. It allocates sparsity adaptively based on sensitivity, allowing us to reduce pruning-induced error while maintaining the overall sparsity level. The advantages of the proposed method exhibit even more when the sparsity is extremely high. Furthermore, our method is compatible with quantization, enabling further compression of LLMs.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#20174;&#26059;&#24459;&#20013;&#29983;&#25104;&#38899;&#33410;&#32423;&#27468;&#35789;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#21644;&#29983;&#25104;&#22120;&#32593;&#32476;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#25506;&#32034;ChatGPT&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21450;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#29983;&#25104;&#27468;&#35789;&#30340;&#36830;&#36143;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00863</link><description>&lt;p&gt;
&#20174;&#26059;&#24459;&#20013;&#21033;&#29992;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#38899;&#33410;&#32423;&#27468;&#35789;
&lt;/p&gt;
&lt;p&gt;
Syllable-level lyrics generation from melody exploiting character-level language model. (arXiv:2310.00863v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00863
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#20174;&#26059;&#24459;&#20013;&#29983;&#25104;&#38899;&#33410;&#32423;&#27468;&#35789;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#34701;&#21512;&#35821;&#35328;&#27169;&#22411;&#30693;&#35782;&#21644;&#29983;&#25104;&#22120;&#32593;&#32476;&#36827;&#34892;&#20248;&#21270;&#12290;&#36890;&#36807;&#25506;&#32034;ChatGPT&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21450;&#20154;&#24037;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#25552;&#39640;&#20102;&#29983;&#25104;&#27468;&#35789;&#30340;&#36830;&#36143;&#24615;&#21644;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#19982;&#20276;&#22863;&#26059;&#24459;&#32039;&#23494;&#30456;&#20851;&#30340;&#27468;&#35789;&#28041;&#21450;&#24314;&#31435;&#38899;&#20048;&#38899;&#31526;&#19982;&#27468;&#35789;&#38899;&#33410;&#20043;&#38388;&#30340;&#26144;&#23556;&#12290;&#36825;&#20010;&#36807;&#31243;&#38656;&#35201;&#23545;&#38899;&#33410;&#32423;&#12289;&#35789;&#32423;&#21644;&#21477;&#32423;&#35821;&#20041;&#24847;&#20041;&#19978;&#30340;&#38899;&#20048;&#32422;&#26463;&#21644;&#35821;&#20041;&#27169;&#24335;&#26377;&#28145;&#20837;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#20844;&#24320;&#30340;&#38899;&#33410;&#32423;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24182;&#19981;&#23384;&#22312;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#20197;&#23383;&#31526;&#32423;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38899;&#33410;&#32423;&#27468;&#35789;&#29983;&#25104;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#30693;&#35782;&#34701;&#20837;&#38899;&#33410;&#32423;Transformer&#29983;&#25104;&#22120;&#32593;&#32476;&#30340;&#26463;&#25628;&#32034;&#36807;&#31243;&#20013;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#25506;&#32034;&#22522;&#20110;ChatGPT&#30340;&#29983;&#25104;&#27468;&#35789;&#35780;&#20272;&#26041;&#27861;&#65292;&#20197;&#21450;&#20154;&#24037;&#20027;&#35266;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22686;&#24378;&#20102;&#29983;&#25104;&#27468;&#35789;&#30340;&#36830;&#36143;&#24615;&#21644;&#27491;&#30830;&#24615;&#65292;&#28040;&#38500;&#20102;&#35757;&#32451;&#26114;&#36149;&#30340;&#26032;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of lyrics tightly connected to accompanying melodies involves establishing a mapping between musical notes and syllables of lyrics. This process requires a deep understanding of music constraints and semantic patterns at syllable-level, word-level, and sentence-level semantic meanings. However, pre-trained language models specifically designed at the syllable level are publicly unavailable. To solve these challenging issues, we propose to exploit fine-tuning character-level language models for syllable-level lyrics generation from symbolic melody. In particular, our method endeavors to incorporate linguistic knowledge of the language model into the beam search process of a syllable-level Transformer generator network. Additionally, by exploring ChatGPT-based evaluation for generated lyrics, along with human subjective evaluation, we demonstrate that our approach enhances the coherence and correctness of the generated lyrics, eliminating the need to train expensive new la
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#34892;&#20026;&#30340;&#38887;&#24615;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#21457;&#29616;&#28508;&#22312;&#29305;&#24449;&#22312;&#30456;&#21516;&#20449;&#24687;&#22833;&#30495;&#27700;&#24179;&#19979;&#27604;&#36755;&#20837;&#34920;&#31034;&#26356;&#21152;&#38887;&#24615;&#65292;&#24182;&#19988;&#23545;&#25239;&#24615;&#38887;&#24615;&#30001;&#29305;&#24449;&#32500;&#24230;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#20849;&#21516;&#20915;&#23450;&#12290;</title><link>http://arxiv.org/abs/2309.17401</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#28508;&#22312;&#34920;&#31034;&#20013;&#30340;&#23545;&#25239;&#24615;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adversarial Machine Learning in Latent Representations of Neural Networks. (arXiv:2309.17401v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17401
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20998;&#26512;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#34892;&#20026;&#30340;&#38887;&#24615;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#31354;&#30333;&#65292;&#24182;&#21457;&#29616;&#28508;&#22312;&#29305;&#24449;&#22312;&#30456;&#21516;&#20449;&#24687;&#22833;&#30495;&#27700;&#24179;&#19979;&#27604;&#36755;&#20837;&#34920;&#31034;&#26356;&#21152;&#38887;&#24615;&#65292;&#24182;&#19988;&#23545;&#25239;&#24615;&#38887;&#24615;&#30001;&#29305;&#24449;&#32500;&#24230;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#20849;&#21516;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#20943;&#36731;&#31227;&#21160;&#35774;&#22791;&#30340;&#35745;&#31639;&#36127;&#25285;&#65292;&#24182;&#38477;&#20302;&#36793;&#32536;&#35745;&#31639;&#22330;&#26223;&#20013;&#30340;&#31471;&#21040;&#31471;&#25512;&#29702;&#24310;&#36831;&#12290;&#23613;&#31649;&#24050;&#32463;&#23545;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#23545;&#25239;&#24615;&#34892;&#20026;&#30340;&#38887;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20005;&#26684;&#20998;&#26512;&#20998;&#24067;&#24335;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#25239;&#24615;&#34892;&#20026;&#30340;&#38887;&#24615;&#26469;&#22635;&#34917;&#29616;&#26377;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#32622;&#20110;&#20449;&#24687;&#35770;&#30340;&#32972;&#26223;&#19979;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;&#34913;&#37327;&#25351;&#26631;&#26469;&#34913;&#37327;&#22833;&#30495;&#21644;&#38887;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#34920;&#26126;&#65306;&#65288;i&#65289;&#22312;&#20551;&#35774;&#20855;&#26377;&#30456;&#21516;&#20449;&#24687;&#22833;&#30495;&#27700;&#24179;&#30340;&#24773;&#20917;&#19979;&#65292;&#28508;&#22312;&#29305;&#24449;&#22987;&#32456;&#27604;&#36755;&#20837;&#34920;&#31034;&#26356;&#21152;&#38887;&#24615;&#65307;&#65288;ii&#65289;&#23545;&#25239;&#24615;&#38887;&#24615;&#21516;&#26102;&#30001;&#29305;&#24449;&#32500;&#24230;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#20915;&#23450;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#20998;&#26512;&#65292;&#32771;&#34385;&#20102;6&#31181;&#19981;&#21516;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributed deep neural networks (DNNs) have been shown to reduce the computational burden of mobile devices and decrease the end-to-end inference latency in edge computing scenarios. While distributed DNNs have been studied, to the best of our knowledge the resilience of distributed DNNs to adversarial action still remains an open problem. In this paper, we fill the existing research gap by rigorously analyzing the robustness of distributed DNNs against adversarial action. We cast this problem in the context of information theory and introduce two new measurements for distortion and robustness. Our theoretical findings indicate that (i) assuming the same level of information distortion, latent features are always more robust than input representations; (ii) the adversarial robustness is jointly determined by the feature dimension and the generalization capability of the DNN. To test our theoretical findings, we perform extensive experimental analysis by considering 6 different DNN arc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#24182;&#21457;&#29616;&#20316;&#23478;&#20204;&#26356;&#20542;&#21521;&#20110;&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#38454;&#27573;&#20013;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2309.12570</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19979;&#30340;&#21019;&#36896;&#21147;&#25903;&#25345;: &#19968;&#39033;&#28041;&#21450;&#26032;&#20852;&#20316;&#23478;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Creativity Support in the Age of Large Language Models: An Empirical Study Involving Emerging Writers. (arXiv:2309.12570v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#65292;&#24182;&#21457;&#29616;&#20316;&#23478;&#20204;&#26356;&#20542;&#21521;&#20110;&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#38454;&#27573;&#20013;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#20854;&#33021;&#22815;&#36981;&#24490;&#25351;&#20196;&#24182;&#21442;&#19982;&#23545;&#35805;&#20114;&#21160;&#65292;&#24341;&#21457;&#20102;&#22312;&#21508;&#31181;&#25903;&#25345;&#24037;&#20855;&#20013;&#21033;&#29992;&#23427;&#20204;&#30340;&#20852;&#36259;&#22686;&#21152;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#39033;&#23454;&#35777;&#29992;&#25143;&#30740;&#31350;&#65288;n=30&#65289;&#25506;&#35752;&#20102;&#29616;&#20195;LLM&#22312;&#21327;&#21161;&#19987;&#19994;&#20316;&#23478;&#26041;&#38754;&#30340;&#25928;&#29992;&#12290;&#25105;&#20204;&#30340;&#21512;&#20316;&#20889;&#20316;&#30028;&#38754;&#35774;&#35745;&#22522;&#20110;&#23558;&#20889;&#20316;&#35270;&#20026;&#19968;&#20010;&#30446;&#26631;&#23548;&#21521;&#30340;&#24605;&#32500;&#36807;&#31243;&#30340;&#35748;&#30693;&#36807;&#31243;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#38750;&#32447;&#24615;&#30340;&#35748;&#30693;&#27963;&#21160;&#65306;&#35268;&#21010;&#12289;&#32763;&#35793;&#21644;&#23457;&#26597;&#12290;&#21442;&#19982;&#32773;&#34987;&#35201;&#27714;&#25552;&#20132;&#19968;&#20221;&#21518;&#23436;&#25104;&#35843;&#26597;&#65292;&#20197;&#25552;&#20379;&#20851;&#20110;LLM&#20316;&#20026;&#20889;&#20316;&#21512;&#20316;&#32773;&#28508;&#21147;&#21644;&#38382;&#39064;&#30340;&#21453;&#39304;&#12290;&#36890;&#36807;&#20998;&#26512;&#20316;&#23478;-LLM&#20114;&#21160;,&#25105;&#20204;&#21457;&#29616;&#20316;&#23478;&#22312;&#19977;&#31181;&#31867;&#22411;&#30340;&#35748;&#30693;&#27963;&#21160;&#20013;&#37117;&#23547;&#27714;LLM&#30340;&#24110;&#21161;&#65292;&#20294;&#20182;&#20204;&#21457;&#29616;LLM&#22312;&#32763;&#35793;&#21644;&#23457;&#26597;&#26041;&#38754;&#26356;&#26377;&#24110;&#21161;&#12290;&#36890;&#36807;&#20998;&#26512;&#20114;&#21160;&#21644;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#21457;&#29616;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of large language models (LLMs) capable of following instructions and engaging in conversational interactions sparked increased interest in their utilization across various support tools. We investigate the utility of modern LLMs in assisting professional writers via an empirical user study (n=30). The design of our collaborative writing interface is grounded in the cognitive process model of writing that views writing as a goal-oriented thinking process encompassing non-linear cognitive activities: planning, translating, and reviewing. Participants are asked to submit a post-completion survey to provide feedback on the potential and pitfalls of LLMs as writing collaborators. Upon analyzing the writer-LLM interactions, we find that while writers seek LLM's help across all three types of cognitive activities, they find LLMs more helpful in translation and reviewing. Our findings from analyzing both the interactions and the survey responses highlight future research direc
&lt;/p&gt;</description></item><item><title>MAPLE&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#39564;&#35777;&#20102;&#20854;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08648</link><description>&lt;p&gt;
MAPLE: &#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#30340;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
MAPLE: Mobile App Prediction Leveraging Large Language model Embeddings. (arXiv:2309.08648v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08648
&lt;/p&gt;
&lt;p&gt;
MAPLE&#26159;&#19968;&#20010;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#36827;&#34892;&#31227;&#21160;&#24212;&#29992;&#39044;&#27979;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20005;&#26684;&#27979;&#35797;&#39564;&#35777;&#20102;&#20854;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#24378;&#35843;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31227;&#21160;&#24212;&#29992;&#30340;&#21457;&#23637;&#36805;&#36895;&#65292;&#20294;&#30001;&#20110;&#22797;&#26434;&#30340;&#29992;&#25143;&#34892;&#20026;&#21644;&#19981;&#26029;&#28436;&#21464;&#30340;&#29615;&#22659;&#65292;&#39044;&#27979;&#24212;&#29992;&#30340;&#20351;&#29992;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE)&#27169;&#22411;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26469;&#20934;&#30830;&#39044;&#27979;&#24212;&#29992;&#30340;&#20351;&#29992;&#24773;&#20917;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#20844;&#24320;&#25968;&#25454;&#38598;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;MAPLE&#30340;&#33021;&#21147;&#22312;&#35299;&#23494;&#22797;&#26434;&#27169;&#24335;&#21644;&#29702;&#35299;&#29992;&#25143;&#29615;&#22659;&#26041;&#38754;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;&#36825;&#20123;&#24378;&#22823;&#30340;&#32467;&#26524;&#35777;&#23454;&#20102;MAPLE&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30340;&#22810;&#21151;&#33021;&#24615;&#21644;&#24377;&#24615;&#12290;&#23613;&#31649;&#20854;&#20027;&#35201;&#35774;&#35745;&#38754;&#21521;&#24212;&#29992;&#39044;&#27979;&#65292;&#20294;&#32467;&#26524;&#20063;&#24378;&#35843;&#20102;LLM&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#36890;&#36807;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;LLM&#22312;&#24212;&#29992;&#20351;&#29992;&#39044;&#27979;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#24314;&#35758;&#22312;&#24314;&#27169;&#21508;&#31181;&#39046;&#22495;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#65292;&#23427;&#20204;&#20855;&#26377;&#21464;&#38761;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the rapid advancement of mobile applications, predicting app usage remains a formidable challenge due to intricate user behaviours and ever-evolving contexts. To address these issues, this paper introduces the Mobile App Prediction Leveraging Large Language Model Embeddings (MAPLE) model. This innovative approach utilizes Large Language Models (LLMs) to predict app usage accurately. Rigorous testing on two public datasets highlights MAPLE's capability to decipher intricate patterns and comprehend user contexts. These robust results confirm MAPLE's versatility and resilience across various scenarios. While its primary design caters to app prediction, the outcomes also emphasize the broader applicability of LLMs in different domains. Through this research, we emphasize the potential of LLMs in app usage prediction and suggest their transformative capacity in modelling human behaviours across diverse fields.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;&#65292;&#21253;&#25324;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#31561;&#26041;&#38754;&#12290;&#21363;&#20351;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.08345</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
Data Distribution Bottlenecks in Grounding Language Models to Knowledge Bases. (arXiv:2309.08345v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08345
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#19982;&#30693;&#35782;&#24211;&#36827;&#34892;&#36830;&#25509;&#26102;&#30340;&#25968;&#25454;&#20998;&#24067;&#29942;&#39048;&#65292;&#21253;&#25324;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#31561;&#26041;&#38754;&#12290;&#21363;&#20351;&#37319;&#29992;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#29702;&#35299;&#21644;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#21644;&#24418;&#24335;&#35821;&#35328;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#19982;&#22823;&#35268;&#27169;&#30693;&#35782;&#24211;&#31561;&#29616;&#23454;&#29615;&#22659;&#30340;&#25972;&#21512;&#20173;&#28982;&#26159;&#19968;&#20010;&#27424;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#24433;&#21709;&#20102;&#35821;&#20041;&#35299;&#26512;&#31561;&#24212;&#29992;&#65292;&#24182;&#19988;&#23481;&#26131;&#20986;&#29616;&#8220;&#20135;&#29983;&#34394;&#20551;&#20449;&#24687;&#8221;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35843;&#26597;&#25581;&#31034;&#20102;LM&#22312;&#22788;&#29702;&#30693;&#35782;&#24211;&#38382;&#31572;&#65288;KBQA&#65289;&#20219;&#21153;&#26102;&#25152;&#36935;&#21040;&#30340;&#20581;&#22766;&#24615;&#25361;&#25112;&#12290;&#30740;&#31350;&#35206;&#30422;&#20102;&#35757;&#32451;&#21644;&#25512;&#26029;&#20043;&#38388;&#25968;&#25454;&#20998;&#24067;&#19981;&#19968;&#33268;&#30340;&#22330;&#26223;&#65292;&#20363;&#22914;&#25512;&#24191;&#21040;&#26410;&#35265;&#22495;&#12289;&#36866;&#24212;&#21508;&#31181;&#35821;&#35328;&#21464;&#20307;&#21644;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#20043;&#38388;&#30340;&#21487;&#36716;&#31227;&#24615;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#23454;&#39564;&#25581;&#31034;&#20102;&#21363;&#20351;&#22312;&#37319;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#24773;&#20917;&#19979;&#65292;&#20808;&#36827;&#30340;&#23567;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#26041;&#38754;&#34920;&#29616;&#20986;&#36739;&#24046;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models (LMs) have already demonstrated remarkable abilities in understanding and generating both natural and formal language. Despite these advances, their integration with real-world environments such as large-scale knowledge bases (KBs) remains an underdeveloped area, affecting applications such as semantic parsing and indulging in "hallucinated" information. This paper is an experimental investigation aimed at uncovering the robustness challenges that LMs encounter when tasked with knowledge base question answering (KBQA). The investigation covers scenarios with inconsistent data distribution between training and inference, such as generalization to unseen domains, adaptation to various language variations, and transferability across different datasets. Our comprehensive experiments reveal that even when employed with our proposed data augmentation techniques, advanced small and large language models exhibit poor performance in various dimensions. While the LM is a promisin
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#37325;&#20851;&#31995;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#26174;&#24615;&#21644;&#38544;&#24615;&#20851;&#31995;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#32593;&#32476;&#24182;&#25552;&#21319;&#26816;&#32034;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.02169</link><description>&lt;p&gt;
&#21452;&#37325;&#20851;&#31995;&#23545;&#40784;&#29992;&#20110;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Dual Relation Alignment for Composed Image Retrieval. (arXiv:2309.02169v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02169
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21452;&#37325;&#20851;&#31995;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#12290;&#36890;&#36807;&#21033;&#29992;&#26174;&#24615;&#21644;&#38544;&#24615;&#20851;&#31995;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23398;&#20064;&#32593;&#32476;&#24182;&#25552;&#21319;&#26816;&#32034;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#26159;&#19968;&#39033;&#36890;&#36807;&#20351;&#29992;&#21442;&#32771;&#22270;&#20687;&#21644;&#34917;&#20805;&#25991;&#26412;&#20316;&#20026;&#26597;&#35810;&#65292;&#26469;&#25628;&#32034;&#30446;&#26631;&#22270;&#20687;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#36328;&#27169;&#24577;&#24314;&#27169;&#30340;&#36827;&#23637;&#65292;&#36825;&#19968;&#20219;&#21153;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#19982;&#20165;&#23384;&#22312;&#19968;&#31181;&#23545;&#40784;&#20851;&#31995;&#30340;&#19968;&#33324;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#38382;&#39064;&#19981;&#21516;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#32452;&#21512;&#22270;&#20687;&#26816;&#32034;&#20013;&#23384;&#22312;&#30528;&#20004;&#31181;&#31867;&#22411;&#30340;&#20851;&#31995;&#12290;&#26174;&#24615;&#20851;&#31995;&#28041;&#21450;&#21442;&#32771;&#22270;&#20687; &amp; &#34917;&#20805;&#25991;&#26412;-&#30446;&#26631;&#22270;&#20687;&#65292;&#36825;&#26159;&#29616;&#26377;&#26041;&#27861;&#24120;&#29992;&#30340;&#20851;&#31995;&#12290;&#38500;&#20102;&#36825;&#31181;&#30452;&#35266;&#20851;&#31995;&#20043;&#22806;&#65292;&#25105;&#20204;&#22312;&#23454;&#36341;&#20013;&#35266;&#23519;&#21040;&#21478;&#19968;&#31181;&#38544;&#21547;&#20294;&#20851;&#38190;&#30340;&#20851;&#31995;&#65292;&#21363;&#21442;&#32771;&#22270;&#20687; &amp; &#30446;&#26631;&#22270;&#20687;-&#34917;&#20805;&#25991;&#26412;&#12290;&#22240;&#20026;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#30740;&#31350;&#30446;&#26631;&#22270;&#20687;&#21644;&#21442;&#32771;&#22270;&#20687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#25512;&#26029;&#20986;&#34917;&#20805;&#25991;&#26412;&#12290;&#21487;&#24796;&#30340;&#26159;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#21033;&#29992;&#26174;&#24615;&#20851;&#31995;&#26469;&#23398;&#20064;&#32593;&#32476;&#65292;&#32780;&#24573;&#35270;&#20102;&#38544;&#24615;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Composed image retrieval, a task involving the search for a target image using a reference image and a complementary text as the query, has witnessed significant advancements owing to the progress made in cross-modal modeling. Unlike the general image-text retrieval problem with only one alignment relation, i.e., image-text, we argue for the existence of two types of relations in composed image retrieval. The explicit relation pertains to the reference image &amp; complementary text-target image, which is commonly exploited by existing methods. Besides this intuitive relation, the observations during our practice have uncovered another implicit yet crucial relation, i.e., reference image &amp; target image-complementary text, since we found that the complementary text can be inferred by studying the relation between the target image and the reference image. Regrettably, existing methods largely focus on leveraging the explicit relation to learn their networks, while overlooking the implicit re
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#21333;&#23376;&#21477;&#20256;&#25773;&#32780;&#35328;&#65292;&#22312;CNF&#20844;&#24335;&#20013;&#19981;&#21487;&#31616;&#21270;&#30340;&#20844;&#24335;&#65292;&#20854;&#22823;&#23567;&#19982;&#26368;&#23567;&#21487;&#31561;&#20215;&#30340;&#20844;&#24335;&#22823;&#23567;&#30340;&#27604;&#20540;&#26368;&#22823;&#20026;n^2&#65292;&#20854;&#20013;n&#26159;&#21464;&#37327;&#25968;&#37327;&#12290;&#19968;&#33324;&#19978;&#30028;&#19981;&#20250;&#23567;&#20110;n/ln n&#20493;&#12290;</title><link>http://arxiv.org/abs/2309.01750</link><description>&lt;p&gt;
&#20851;&#20110;&#30456;&#23545;&#20110;&#21333;&#23376;&#21477;&#20256;&#25773;&#19981;&#21487;&#31616;&#21270;&#30340;CNF&#20844;&#24335;
&lt;/p&gt;
&lt;p&gt;
On CNF formulas irredundant with respect to unit clause propagation. (arXiv:2309.01750v2 [math.CO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01750
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#21333;&#23376;&#21477;&#20256;&#25773;&#32780;&#35328;&#65292;&#22312;CNF&#20844;&#24335;&#20013;&#19981;&#21487;&#31616;&#21270;&#30340;&#20844;&#24335;&#65292;&#20854;&#22823;&#23567;&#19982;&#26368;&#23567;&#21487;&#31561;&#20215;&#30340;&#20844;&#24335;&#22823;&#23567;&#30340;&#27604;&#20540;&#26368;&#22823;&#20026;n^2&#65292;&#20854;&#20013;n&#26159;&#21464;&#37327;&#25968;&#37327;&#12290;&#19968;&#33324;&#19978;&#30028;&#19981;&#20250;&#23567;&#20110;n/ln n&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#20004;&#20010;CNF&#20844;&#24335;&#22312;&#21333;&#23376;&#21477;&#20256;&#25773;&#65288;UCP&#65289;&#26041;&#38754;&#30340;&#34892;&#20026;&#30456;&#21516;&#65292;&#21017;&#23427;&#20204;&#34987;&#31216;&#20026;ucp&#31561;&#20215;&#12290;&#22914;&#26524;&#31227;&#38500;&#20219;&#24847;&#19968;&#20010;&#23376;&#21477;&#20250;&#23548;&#33268;&#19968;&#20010;&#19982;&#21407;&#22987;&#20844;&#24335;&#22312;ucp&#26041;&#38754;&#19981;&#31561;&#20215;&#30340;&#20844;&#24335;&#65292;&#21017;&#31216;&#35813;&#20844;&#24335;&#20026;ucp&#19981;&#21487;&#31616;&#21270;&#12290;&#26681;&#25454;&#24050;&#30693;&#32467;&#26524;&#65292;ucp&#19981;&#21487;&#31616;&#21270;&#20844;&#24335;&#30340;&#22823;&#23567;&#19982;&#26368;&#23567;ucp&#31561;&#20215;&#20844;&#24335;&#30340;&#22823;&#23567;&#30340;&#27604;&#20540;&#26368;&#22823;&#20026;n^2&#65292;&#20854;&#20013;n&#26159;&#21464;&#37327;&#30340;&#25968;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#31216;&#30830;&#23450;Horn&#20989;&#25968;&#30340;&#19968;&#20010;ucp&#19981;&#21487;&#31616;&#21270;&#20844;&#24335;&#30340;&#20363;&#23376;&#65292;&#20854;&#22823;&#23567;&#27604;&#26368;&#23567;&#30340;ucp&#31561;&#20215;&#20844;&#24335;&#22823;n/ln n&#20493;&#65292;&#22240;&#27492;&#65292;&#19978;&#36848;&#27604;&#20540;&#30340;&#19968;&#33324;&#19978;&#30028;&#19981;&#33021;&#23567;&#20110;&#36825;&#20010;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
Two CNF formulas are called ucp-equivalent, if they behave in the same way with respect to the unit clause propagation (UCP). A formula is called ucp-irredundant, if removing any clause leads to a formula which is not ucp-equivalent to the original one. As a consequence of known results, the ratio of the size of a ucp-irredundant formula and the size of a smallest ucp-equivalent formula is at most $n^2$, where $n$ is the number of the variables. We demonstrate an example of a ucp-irredundant formula for a symmetric definite Horn function which is larger than a smallest ucp-equivalent formula by a factor $\Omega(n/\ln n)$ and, hence, a general upper bound on the above ratio cannot be smaller than this.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30340;&#21019;&#26032;&#28857;&#26159;&#23558;&#25512;&#33616;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34701;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#27169;&#22411;&#22312;&#25552;&#20379;&#35299;&#37322;&#21644;&#21442;&#19982;&#23545;&#35805;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2308.16505</link><description>&lt;p&gt;
&#25512;&#33616;AI&#20195;&#29702;&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25972;&#21512;&#21040;&#20132;&#20114;&#24335;&#25512;&#33616;&#20013;
&lt;/p&gt;
&lt;p&gt;
Recommender AI Agent: Integrating Large Language Models for Interactive Recommendations. (arXiv:2308.16505v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.16505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30340;&#21019;&#26032;&#28857;&#26159;&#23558;&#25512;&#33616;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#34701;&#21512;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#65292;&#35299;&#20915;&#20102;&#25512;&#33616;&#27169;&#22411;&#22312;&#25552;&#20379;&#35299;&#37322;&#21644;&#21442;&#19982;&#23545;&#35805;&#20219;&#21153;&#26041;&#38754;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#27169;&#22411;&#36890;&#36807;&#21033;&#29992;&#24191;&#27867;&#30340;&#29992;&#25143;&#34892;&#20026;&#25968;&#25454;&#26469;&#25552;&#20379;&#39046;&#22495;&#29305;&#23450;&#30340;&#29289;&#21697;&#25512;&#33616;&#65292;&#23637;&#29616;&#20986;&#36731;&#37327;&#32423;&#39046;&#22495;&#19987;&#23478;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#25552;&#20379;&#35299;&#37322;&#21644;&#21442;&#19982;&#23545;&#35805;&#31561;&#22810;&#26679;&#21270;&#20219;&#21153;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20195;&#34920;&#20102;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#37325;&#35201;&#36827;&#23637;&#65292;&#22312;&#25351;&#20196;&#29702;&#35299;&#12289;&#24120;&#35782;&#25512;&#29702;&#21644;&#20154;&#31867;&#20132;&#20114;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#29289;&#21697;&#30446;&#24405;&#21644;&#34892;&#20026;&#27169;&#24335;&#30340;&#30693;&#35782;&#65292;&#29305;&#21035;&#26159;&#22312;&#19982;&#19968;&#33324;&#19990;&#30028;&#30693;&#35782;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#22914;&#22312;&#32447;&#30005;&#23376;&#21830;&#21153;&#12290;&#20026;&#27599;&#20010;&#39046;&#22495;&#24494;&#35843;LLMs&#26082;&#19981;&#32463;&#27982;&#21448;&#19981;&#39640;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25512;&#33616;&#27169;&#22411;&#21644;LLMs&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#32467;&#21512;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#21151;&#33021;&#20132;&#20114;&#24335;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26694;&#26550;&#31216;&#20026;RecAgent&#65292;&#35813;&#26694;&#26550;&#20351;&#29992;LLMs
&lt;/p&gt;
&lt;p&gt;
Recommender models excel at providing domain-specific item recommendations by leveraging extensive user behavior data. Despite their ability to act as lightweight domain experts, they struggle to perform versatile tasks such as providing explanations and engaging in conversations. On the other hand, large language models (LLMs) represent a significant step towards artificial general intelligence, showcasing remarkable capabilities in instruction comprehension, commonsense reasoning, and human interaction. However, LLMs lack the knowledge of domain-specific item catalogs and behavioral patterns, particularly in areas that diverge from general world knowledge, such as online e-commerce. Finetuning LLMs for each domain is neither economic nor efficient.  In this paper, we bridge the gap between recommender models and LLMs, combining their respective strengths to create a versatile and interactive recommender system. We introduce an efficient framework called RecAgent, which employs LLMs a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#34917;&#20805;&#21355;&#26143;&#22320;&#22270;&#65292;&#22686;&#24378;&#20102;&#36710;&#36733;&#20256;&#24863;&#22120;&#26500;&#24314;&#39640;&#31934;&#24230;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21355;&#26143;&#22320;&#22270;&#30340;&#24191;&#38420;&#35206;&#30422;&#33021;&#21147;&#12290;&#25105;&#20204;&#37322;&#25918;&#20102;&#21355;&#26143;&#22320;&#22270;&#29926;&#29255;&#20316;&#20026;nuScenes&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#34701;&#21512;&#27169;&#22359;&#26469;&#26356;&#22909;&#22320;&#34701;&#21512;&#36710;&#36733;&#20256;&#24863;&#22120;&#19982;&#21355;&#26143;&#22320;&#22270;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2308.15427</link><description>&lt;p&gt;
&#36890;&#36807;&#21355;&#26143;&#22320;&#22270;&#34917;&#20805;&#36710;&#36733;&#20256;&#24863;&#22120;&#65306;&#39640;&#31934;&#24230;&#22320;&#22270;&#26500;&#24314;&#30340;&#26032;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Complementing Onboard Sensors with Satellite Map: A New Perspective for HD Map Construction. (arXiv:2308.15427v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#34917;&#20805;&#21355;&#26143;&#22320;&#22270;&#65292;&#22686;&#24378;&#20102;&#36710;&#36733;&#20256;&#24863;&#22120;&#26500;&#24314;&#39640;&#31934;&#24230;&#22320;&#22270;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#21355;&#26143;&#22320;&#22270;&#30340;&#24191;&#38420;&#35206;&#30422;&#33021;&#21147;&#12290;&#25105;&#20204;&#37322;&#25918;&#20102;&#21355;&#26143;&#22320;&#22270;&#29926;&#29255;&#20316;&#20026;nuScenes&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#34701;&#21512;&#27169;&#22359;&#26469;&#26356;&#22909;&#22320;&#34701;&#21512;&#36710;&#36733;&#20256;&#24863;&#22120;&#19982;&#21355;&#26143;&#22320;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#31934;&#24230;&#65288;HD&#65289;&#22320;&#22270;&#23545;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#23581;&#35797;&#22522;&#20110;&#36710;&#36733;&#20256;&#24863;&#22120;&#33719;&#21462;&#30340;&#20449;&#24687;&#23454;&#26102;&#26500;&#24314;HD&#22320;&#22270;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#36710;&#36742;&#21608;&#22260;&#29615;&#22659;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#36825;&#26159;&#30001;&#20110;&#36710;&#36733;&#20256;&#24863;&#22120;&#30340;&#22266;&#26377;&#38480;&#21046;&#65292;&#22914;&#23545;&#36828;&#31243;&#25506;&#27979;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#36890;&#36807;&#34917;&#20805;&#21355;&#26143;&#22320;&#22270;&#21487;&#20197;&#22686;&#24378;HD&#22320;&#22270;&#26500;&#24314;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#21033;&#29992;&#21355;&#26143;&#22320;&#22270;&#30340;&#24191;&#27867;&#35206;&#30422;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#21355;&#26143;&#22320;&#22270;&#29926;&#29255;&#20316;&#20026;nuScenes&#25968;&#25454;&#38598;&#30340;&#34917;&#20805;&#25968;&#25454;&#38598;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20998;&#23618;&#34701;&#21512;&#27169;&#22359;&#65292;&#20351;&#21355;&#26143;&#22320;&#22270;&#20449;&#24687;&#19982;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#34701;&#21512;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#22522;&#20110;&#20998;&#21106;&#21644;&#36317;&#31163;&#30340;&#27880;&#24847;&#21147;&#25513;&#30721;&#65292;&#24212;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#26469;&#34701;&#21512;&#36710;&#36733;&#20256;&#24863;&#22120;&#19982;&#21355;&#26143;&#22320;&#22270;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
High-Definition (HD) maps play a crucial role in autonomous driving systems. Recent methods have attempted to construct HD maps in real-time based on information obtained from vehicle onboard sensors. However, the performance of these methods is significantly susceptible to the environment surrounding the vehicle due to the inherent limitation of onboard sensors, such as weak capacity for long-range detection. In this study, we demonstrate that supplementing onboard sensors with satellite maps can enhance the performance of HD map construction methods, leveraging the broad coverage capability of satellite maps. For the purpose of further research, we release the satellite map tiles as a complementary dataset of nuScenes dataset. Meanwhile, we propose a hierarchical fusion module that enables better fusion of satellite maps information with existing methods. Specifically, we design an attention mask based on segmentation and distance, applying the cross-attention mechanism to fuse onboa
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2308.11696</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Efficient Benchmarking (of Language Models). (arXiv:2308.11696v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#26088;&#22312;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#32780;&#19981;&#38477;&#20302;&#21487;&#38752;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;HELM&#22522;&#20934;&#27979;&#35797;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#21457;&#29616;&#21482;&#38656;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#21363;&#21487;&#25913;&#21464;&#39046;&#20808;&#32773;&#65292;&#24182;&#20165;&#38656;&#23569;&#37327;&#31034;&#20363;&#21363;&#21487;&#24471;&#21040;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#21151;&#33021;&#24615;&#22686;&#21152;&#23548;&#33268;&#20102;&#19968;&#31867;&#20840;&#38754;&#35780;&#20272;&#24191;&#27867;&#33021;&#21147;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#20986;&#29616;&#12290;&#36825;&#20123;&#22522;&#20934;&#27979;&#35797;&#19982;&#22823;&#35268;&#27169;&#35745;&#31639;&#25104;&#26412;&#30456;&#20851;&#65292;&#27599;&#20010;&#27169;&#22411;&#38656;&#35201;&#25968;&#21315;&#20010;GPU&#23567;&#26102;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#35780;&#20272;&#25928;&#29575;&#26041;&#38754;&#30340;&#38382;&#39064;&#22312;&#25991;&#29486;&#20013;&#35752;&#35770;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"Efficient Benchmarking"&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#19981;&#25439;&#23475;&#21487;&#38752;&#24615;&#30340;&#24773;&#20917;&#19979;&#26234;&#33021;&#22320;&#20943;&#23569;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;&#36890;&#36807;&#20351;&#29992;HELM&#22522;&#20934;&#27979;&#35797;&#20316;&#20026;&#31034;&#20363;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#35774;&#35745;&#36873;&#25321;&#22914;&#20309;&#24433;&#21709;&#35745;&#31639;-&#21487;&#38752;&#24615;&#26435;&#34913;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#19968;&#31181;&#21517;&#20026;Decision Impact on Reliability&#65288;DIoR&#65289;&#30340;&#26032;&#24230;&#37327;&#26469;&#35780;&#20272;&#36825;&#20123;&#20915;&#31574;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#36890;&#36807;&#20174;&#22522;&#20934;&#27979;&#35797;&#20013;&#21024;&#38500;&#19968;&#20010;&#20302;&#25490;&#21517;&#27169;&#22411;&#65292;&#24403;&#21069;&#22312;HELM&#19978;&#30340;&#39046;&#20808;&#32773;&#21487;&#33021;&#20250;&#25913;&#21464;&#65292;&#24182;&#19988;&#35266;&#23519;&#21040;&#21482;&#38656;&#19968;&#23567;&#37096;&#20998;&#31034;&#20363;&#21363;&#21487;&#33719;&#24471;&#27491;&#30830;&#30340;&#22522;&#20934;&#27979;&#35797;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing versatility of language models LMs has given rise to a new class of benchmarks that comprehensively assess a broad range of capabilities. Such benchmarks are associated with massive computational costs reaching thousands of GPU hours per model. However the efficiency aspect of these evaluation efforts had raised little discussion in the literature. In this work we present the problem of Efficient Benchmarking namely intelligently reducing the computation costs of LM evaluation without compromising reliability. Using the HELM benchmark as a test case we investigate how different benchmark design choices affect the computation-reliability tradeoff. We propose to evaluate the reliability of such decisions by using a new measure Decision Impact on Reliability DIoR for short. We find for example that the current leader on HELM may change by merely removing a low-ranked model from the benchmark and observe that a handful of examples suffice to obtain the correct benchmark rank
&lt;/p&gt;</description></item><item><title>SSLRec&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24211;&#65292;&#20026;&#35780;&#20272;&#21508;&#31181;SSL&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#12289;&#28789;&#27963;&#21644;&#32508;&#21512;&#30340;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2308.05697</link><description>&lt;p&gt;
SSLRec: &#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24211;
&lt;/p&gt;
&lt;p&gt;
SSLRec: A Self-Supervised Learning Library for Recommendation. (arXiv:2308.05697v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05697
&lt;/p&gt;
&lt;p&gt;
SSLRec&#26159;&#19968;&#20010;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#25512;&#33616;&#31995;&#32479;&#24211;&#65292;&#20026;&#35780;&#20272;&#21508;&#31181;SSL&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#12289;&#28789;&#27963;&#21644;&#32508;&#21512;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20316;&#20026;&#35299;&#20915;&#25512;&#33616;&#31995;&#32479;&#20013;&#31232;&#30095;&#21644;&#22122;&#22768;&#25968;&#25454;&#25361;&#25112;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#26368;&#36817;&#20960;&#24180;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#23613;&#31649;&#35774;&#35745;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;SSL&#31639;&#27861;&#26469;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#25552;&#20379;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#24615;&#33021;&#65288;&#20363;&#22914;&#22270;&#21327;&#21516;&#36807;&#28388;&#12289;&#39034;&#24207;&#25512;&#33616;&#12289;&#31038;&#20132;&#25512;&#33616;&#12289;&#30693;&#35782;&#22270;&#22686;&#24378;&#25512;&#33616;&#65289;&#65292;&#20294;&#30446;&#21069;&#20173;&#32570;&#20047;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#26469;&#25972;&#21512;&#19981;&#21516;&#39046;&#22495;&#30340;&#25512;&#33616;&#31639;&#27861;&#12290;&#36825;&#26679;&#30340;&#26694;&#26550;&#21487;&#20197;&#20316;&#20026;&#33258;&#30417;&#30563;&#25512;&#33616;&#31639;&#27861;&#30340;&#22522;&#30707;&#65292;&#32479;&#19968;&#29616;&#26377;&#26041;&#27861;&#30340;&#39564;&#35777;&#65292;&#24182;&#25512;&#21160;&#26032;&#26041;&#27861;&#30340;&#35774;&#35745;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SSLRec&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#24179;&#21488;&#65292;&#20026;&#35780;&#20272;&#21508;&#31181;SSL&#22686;&#24378;&#25512;&#33616;&#31995;&#32479;&#25552;&#20379;&#20102;&#26631;&#20934;&#21270;&#12289;&#28789;&#27963;&#21644;&#32508;&#21512;&#30340;&#26694;&#26550;&#12290;SSLRec&#24211;&#20855;&#26377;&#27169;&#22359;&#21270;&#26550;&#26500;&#65292;&#21487;&#20197;&#26041;&#20415;&#29992;&#25143;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#25512;&#33616;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) has gained significant interest in recent years as a solution to address the challenges posed by sparse and noisy data in recommender systems. Despite the growing number of SSL algorithms designed to provide state-of-the-art performance in various recommendation scenarios (e.g., graph collaborative filtering, sequential recommendation, social recommendation, KG-enhanced recommendation), there is still a lack of unified frameworks that integrate recommendation algorithms across different domains. Such a framework could serve as the cornerstone for self-supervised recommendation algorithms, unifying the validation of existing methods and driving the design of new ones. To address this gap, we introduce SSLRec, a novel benchmark platform that provides a standardized, flexible, and comprehensive framework for evaluating various SSL-enhanced recommenders. The SSLRec library features a modular architecture that allows users to easily evaluate state-of-the-art m
&lt;/p&gt;</description></item><item><title>PheKnowLator&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#30693;&#35782;&#22270;&#35889;&#29983;&#24577;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#26500;&#24314;&#21487;&#23450;&#21046;&#30340;FAIR&#26412;&#20307;&#21270;&#22522;&#30784;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#35299;&#20915;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#25972;&#21512;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2307.05727</link><description>&lt;p&gt;
&#19968;&#20010;&#24320;&#28304;&#30340;&#30693;&#35782;&#22270;&#35889;&#29983;&#24577;&#31995;&#32479;&#29992;&#20110;&#29983;&#21629;&#31185;&#23398;
&lt;/p&gt;
&lt;p&gt;
An Open-Source Knowledge Graph Ecosystem for the Life Sciences. (arXiv:2307.05727v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05727
&lt;/p&gt;
&lt;p&gt;
PheKnowLator&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#30693;&#35782;&#22270;&#35889;&#29983;&#24577;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#26500;&#24314;&#21487;&#23450;&#21046;&#30340;FAIR&#26412;&#20307;&#21270;&#22522;&#30784;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#35299;&#20915;&#29983;&#21629;&#31185;&#23398;&#20013;&#30340;&#25972;&#21512;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#21270;&#30740;&#31350;&#38656;&#35201;&#22810;&#20010;&#29983;&#29289;&#32452;&#32455;&#23610;&#24230;&#19978;&#30340;&#25968;&#25454;&#12290;&#27979;&#24207;&#21644;&#22810;&#32452;&#23398;&#25216;&#26415;&#30340;&#36827;&#27493;&#22686;&#21152;&#20102;&#36825;&#20123;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#20294;&#30740;&#31350;&#20154;&#21592;&#38754;&#20020;&#30528;&#37325;&#22823;&#30340;&#25972;&#21512;&#25361;&#25112;&#12290;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#29992;&#20110;&#24314;&#27169;&#22797;&#26434;&#29616;&#35937;&#65292;&#24050;&#32463;&#23384;&#22312;&#33258;&#21160;&#26500;&#24314;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35299;&#20915;&#22797;&#26434;&#30340;&#29983;&#29289;&#21307;&#23398;&#25972;&#21512;&#38382;&#39064;&#38656;&#35201;&#22312;&#30693;&#35782;&#24314;&#27169;&#26041;&#24335;&#19978;&#28789;&#27963;&#24615;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;KG&#26500;&#24314;&#26041;&#27861;&#22312;&#25552;&#20379;&#24378;&#22823;&#24037;&#20855;&#30340;&#21516;&#26102;&#65292;&#20063;&#20250;&#38480;&#21046;&#22312;&#30693;&#35782;&#34920;&#31034;&#27169;&#22411;&#20013;&#22266;&#23450;&#25110;&#26377;&#38480;&#30340;&#36873;&#25321;&#12290;PheKnowLator&#65288;Phenotype Knowledge Translator&#65289;&#26159;&#19968;&#20010;&#35821;&#20041;&#29983;&#24577;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#20855;&#26377;&#23436;&#20840;&#21487;&#23450;&#21046;&#30340;&#30693;&#35782;&#34920;&#31034;&#30340;FAIR&#65288;&#21487;&#25214;&#21040;&#65292;&#21487;&#35775;&#38382;&#65292;&#21487;&#20114;&#25805;&#20316;&#65292;&#21487;&#37325;&#22797;&#20351;&#29992;&#65289;&#26412;&#20307;&#21270;&#22522;&#30784;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#26500;&#24314;&#12290;&#35813;&#29983;&#24577;&#31995;&#32479;&#21253;&#25324;KG&#26500;&#24314;&#36164;&#28304;&#65288;&#20363;&#22914;&#65292;&#25968;&#25454;&#20934;&#22791;API&#65289;&#65292;&#20998;&#26512;&#24037;&#20855;&#65288;&#20363;&#22914;&#65292;SPARQL&#31471;&#28857;&#21644;&#25277;&#35937;&#31639;&#27861;&#65289;&#65292;&#36824;&#26377;
&lt;/p&gt;
&lt;p&gt;
Translational research requires data at multiple scales of biological organization. Advancements in sequencing and multi-omics technologies have increased the availability of these data but researchers face significant integration challenges. Knowledge graphs (KGs) are used to model complex phenomena, and methods exist to automatically construct them. However, tackling complex biomedical integration problems requires flexibility in the way knowledge is modeled. Moreover, existing KG construction methods provide robust tooling at the cost of fixed or limited choices among knowledge representation models. PheKnowLator (Phenotype Knowledge Translator) is a semantic ecosystem for automating the FAIR (Findable, Accessible, Interoperable, and Reusable) construction of ontologically grounded KGs with fully customizable knowledge representation. The ecosystem includes KG construction resources (e.g., data preparation APIs), analysis tools (e.g., SPARQL endpoints and abstraction algorithms), an
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#20854;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#21644;&#23545;&#29992;&#25143;&#26597;&#35810;&#30340;&#22238;&#22797;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.11489</link><description>&lt;p&gt;
&#20026;&#20107;&#23454;&#24863;&#30693;&#35821;&#35328;&#24314;&#27169;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Give Us the Facts: Enhancing Large Language Models with Knowledge Graphs for Fact-aware Language Modeling. (arXiv:2306.11489v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11489
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#39640;&#20854;&#29983;&#25104;&#20869;&#23481;&#30340;&#20934;&#30830;&#24615;&#21644;&#23545;&#29992;&#25143;&#26597;&#35810;&#30340;&#22238;&#22797;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;ChatGPT&#20316;&#20026;&#19968;&#20010;&#20195;&#34920;&#24615;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22240;&#20854;&#24378;&#22823;&#30340;&#26032;&#20852;&#33021;&#21147;&#32780;&#21463;&#21040;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#19968;&#20123;&#30740;&#31350;&#20154;&#21592;&#35748;&#20026;&#65292;LLMs&#26377;&#21487;&#33021;&#21462;&#20195;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#36825;&#26679;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#24211;&#65292;&#25104;&#20026;&#21442;&#25968;&#21270;&#30693;&#35782;&#24211;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;LLMs&#25797;&#38271;&#22522;&#20110;&#22823;&#35821;&#26009;&#24211;&#23398;&#20064;&#27010;&#29575;&#35821;&#35328;&#27169;&#24335;&#65292;&#24182;&#19982;&#20154;&#31867;&#36827;&#34892;&#23545;&#35805;&#65292;&#20294;&#23427;&#20204;&#19982;&#20043;&#21069;&#36739;&#23567;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#19968;&#26679;&#65292;&#22312;&#29983;&#25104;&#22522;&#20110;&#30693;&#35782;&#30340;&#20869;&#23481;&#26102;&#20173;&#28982;&#38590;&#20197;&#22238;&#24518;&#20107;&#23454;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#25968;&#25454;&#39537;&#21160;&#30340;PLMs&#65292;&#23558;&#26126;&#30830;&#30340;&#20107;&#23454;&#30693;&#35782;&#34701;&#20837;PLMs&#65292;&#20174;&#32780;&#25552;&#39640;&#20854;&#29983;&#25104;&#38656;&#35201;&#20107;&#23454;&#30693;&#35782;&#30340;&#25991;&#26412;&#30340;&#24615;&#33021;&#65292;&#24182;&#20026;&#29992;&#25143;&#26597;&#35810;&#25552;&#20379;&#26356;&#22810;&#35265;&#35299;&#30340;&#22238;&#22797;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#26377;&#20851;&#20351;&#29992;KG&#22686;&#24378;PLMs&#30340;&#30740;&#31350;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;PLM&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ChatGPT, a representative large language model (LLM), has gained considerable attention due to its powerful emergent abilities. Some researchers suggest that LLMs could potentially replace structured knowledge bases like knowledge graphs (KGs) and function as parameterized knowledge bases. However, while LLMs are proficient at learning probabilistic language patterns based on large corpus and engaging in conversations with humans, they, like previous smaller pre-trained language models (PLMs), still have difficulty in recalling facts while generating knowledge-grounded contents. To overcome these limitations, researchers have proposed enhancing data-driven PLMs with knowledge-based KGs to incorporate explicit factual knowledge into PLMs, thus improving their performance to generate texts requiring factual knowledge and providing more informed responses to user queries. This paper reviews the studies on enhancing PLMs with KGs, detailing existing knowledge graph enhanced pre-t
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23835;&#36215;&#24341;&#21457;&#20102;&#29256;&#26435;&#21644;&#21019;&#26032;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;&#24314;&#35758;&#23545;&#24320;&#28304;&#20195;&#30721;&#35768;&#21487;&#35777;&#36827;&#34892;&#26356;&#25913;&#65292;&#38480;&#21046;AI&#31995;&#32479;&#23545;&#20195;&#30721;&#30340;&#35775;&#38382;&#21644;&#20351;&#29992;&#65292;&#24182;&#25506;&#35752;&#19982;AI&#21644;&#29256;&#26435;&#20043;&#38388;&#30340;&#20851;&#31995;&#24341;&#21457;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.09267</link><description>&lt;p&gt;
ChatGPT&#21644;&#20854;&#20182;&#31867;&#20284;&#31995;&#32479;&#26159;AI&#30340;&#29616;&#20195;&#21202;&#32435;&#24681;&#20061;&#22836;&#34503;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are ChatGPT and Other Similar Systems the Modern Lernaean Hydras of AI?. (arXiv:2306.09267v3 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09267
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23835;&#36215;&#24341;&#21457;&#20102;&#29256;&#26435;&#21644;&#21019;&#26032;&#20445;&#25252;&#30340;&#38382;&#39064;&#12290;&#24314;&#35758;&#23545;&#24320;&#28304;&#20195;&#30721;&#35768;&#21487;&#35777;&#36827;&#34892;&#26356;&#25913;&#65292;&#38480;&#21046;AI&#31995;&#32479;&#23545;&#20195;&#30721;&#30340;&#35775;&#38382;&#21644;&#20351;&#29992;&#65292;&#24182;&#25506;&#35752;&#19982;AI&#21644;&#29256;&#26435;&#20043;&#38388;&#30340;&#20851;&#31995;&#24341;&#21457;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23835;&#36215;&#24341;&#21457;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#31038;&#20132;&#21442;&#19982;&#12290;AI&#20195;&#30721;&#29983;&#25104;&#31995;&#32479;&#36890;&#36807;&#35775;&#38382;&#36807;&#21435;&#20960;&#21313;&#24180;&#24320;&#21457;&#20154;&#21592;&#21019;&#24314;&#30340;&#22823;&#37327;&#24320;&#28304;&#20195;&#30721;&#26469;&#25552;&#20379;&#22238;&#31572;&#25110;&#21709;&#24212;&#38382;&#39064;&#25110;&#35831;&#27714;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#34987;&#25351;&#25511;&#31363;&#21462;&#23384;&#20648;&#22312;&#34394;&#25311;&#24211;&#20013;&#30340;&#24320;&#28304;&#20195;&#30721;&#12290;&#26412;&#25991;&#30528;&#37325;&#35752;&#35770;&#27492;&#31867;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#26159;&#21542;&#26377;&#35299;&#20915;&#26041;&#26696;&#26469;&#20445;&#25252;&#21019;&#26032;&#24182;&#36991;&#20813;&#22810;&#24180;&#30340;&#35785;&#35772;&#12290;&#23545;AI&#21644;&#29256;&#26435;&#20043;&#38388;&#30340;&#20851;&#31995;&#24341;&#21457;&#30340;&#19968;&#31995;&#21015;&#38382;&#39064;&#20063;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#25105;&#20204;&#25552;&#20986;&#20197;&#19979;&#24314;&#35758;&#65306;(a)&#23545;&#24320;&#21457;&#20154;&#21592;&#21019;&#24314;&#30340;&#24320;&#28304;&#20195;&#30721;&#30340;&#35768;&#21487;&#35777;&#36827;&#34892;&#21363;&#26102;&#26356;&#25913;&#65292;&#38480;&#21046;&#23545;&#20219;&#20309;&#24320;&#28304;&#20195;&#30721;&#30340;&#35775;&#38382;&#21644;/&#25110;&#20351;&#29992;&#20165;&#38480;&#20110;&#20154;&#31867;&#65307;(b)&#24314;&#35758;&#23545;&#40635;&#30465;&#29702;&#24037;&#23398;&#38498;&#65288;MIT&#65289;&#35768;&#21487;&#35777;&#36827;&#34892;&#20462;&#35746;&#65292;&#35201;&#27714;AI&#31995;&#32479;&#20174;&#24320;&#28304;&#20195;&#30721;&#37096;&#20998;&#33719;&#21462;&#36866;&#24403;&#30340;&#35768;&#21487;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of Generative Artificial Intelligence systems ("AI systems") has created unprecedented social engagement. AI code generation systems provide responses (output) to questions or requests by accessing the vast library of open-source code created by developers over the past few decades. However, they do so by allegedly stealing the open-source code stored in virtual libraries, known as repositories. This Article focuses on how this happens and whether there is a solution that protects innovation and avoids years of litigation. We also touch upon the array of issues raised by the relationship between AI and copyright. Looking ahead, we propose the following: (a) immediate changes to the licenses for open-source code created by developers that will limit access and/or use of any open-source code to humans only; (b) we suggest revisions to the Massachusetts Institute of Technology ("MIT") license so that AI systems are required to procure appropriate licenses from open-source code de
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MusicGen&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.05284</link><description>&lt;p&gt;
&#31616;&#21333;&#19988;&#21487;&#25511;&#30340;&#38899;&#20048;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Simple and Controllable Music Generation. (arXiv:2306.05284v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MusicGen&#65292;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#65292;&#24182;&#19988;&#22312;&#26631;&#20934;&#30340;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#30340;&#23454;&#35777;&#30740;&#31350;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35299;&#20915;&#20102;&#26465;&#20214;&#38899;&#20048;&#29983;&#25104;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;MusicGen&#65292;&#23427;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#25805;&#20316;&#22810;&#20010;&#21387;&#32553;&#31163;&#25955;&#38899;&#20048;&#34920;&#31034;&#27969;&#65292;&#21363;&#20196;&#29260;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;MusicGen&#30001;&#19968;&#20010;&#21333;&#19968;&#38454;&#27573;&#30340;Transformer LM&#21644;&#39640;&#25928;&#30340;&#20196;&#29260;&#20132;&#38169;&#27169;&#24335;&#32452;&#25104;&#65292;&#28040;&#38500;&#20102;&#32423;&#32852;&#22810;&#20010;&#27169;&#22411;&#30340;&#38656;&#35201;&#65292;&#20363;&#22914;&#20998;&#23618;&#25110;&#19978;&#37319;&#26679;&#12290;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;MusicGen&#22914;&#20309;&#22312;&#26465;&#20214;&#25551;&#36848;&#25110;&#26059;&#24459;&#29305;&#24449;&#30340;&#25511;&#21046;&#19979;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#32771;&#34385;&#20102;&#33258;&#21160;&#21644;&#20154;&#20026;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#26631;&#20934;&#25991;&#26412;&#21040;&#38899;&#20048;&#22522;&#20934;&#19978;&#35780;&#20272;&#30340;&#22522;&#32447;&#12290;&#36890;&#36807;&#28040;&#34701;&#30740;&#31350;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;MusicGen&#25152;&#21253;&#21547;&#32452;&#20214;&#30340;&#37325;&#35201;&#24615;&#12290;&#38899;&#20048;&#26679;&#26412;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#21487;&#20197;&#22312;https://github.com/fac&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/fac
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#26041;&#27861;&#19982;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;QD-RL&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#21534;&#21520;&#37327;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#29615;&#22659;&#19979;&#35757;&#32451;&#33021;&#22815;&#22312;&#26410;&#30693;&#21160;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.13795</link><description>&lt;p&gt;
&#36136;&#37327;&#22810;&#26679;&#24615;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36817;&#31471;&#31574;&#30053;&#26799;&#24230;&#26641;&#26525;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Proximal Policy Gradient Arborescence for Quality Diversity Reinforcement Learning. (arXiv:2305.13795v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.13795
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#26041;&#27861;&#19982;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#30456;&#32467;&#21512;&#30340;&#26032;&#22411;QD-RL&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#39640;&#21534;&#21520;&#37327;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#29615;&#22659;&#19979;&#35757;&#32451;&#33021;&#22815;&#22312;&#26410;&#30693;&#21160;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#26234;&#33021;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22521;&#35757;&#36890;&#24120;&#33021;&#22815;&#22312;&#26410;&#30693;&#21160;&#24577;&#29615;&#22659;&#20013;&#34920;&#29616;&#33391;&#22909;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#26234;&#33021;&#20307;&#26159;&#19968;&#20010;&#38271;&#26399;&#30446;&#26631;&#12290;&#36136;&#37327;&#22810;&#26679;&#24615;&#24378;&#21270;&#23398;&#20064;(QD-RL)&#26159;&#19968;&#31867;&#26032;&#20852;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#23427;&#23558;&#36136;&#37327;&#22810;&#26679;&#24615;(QD)&#21644;RL&#30340;&#35265;&#35299;&#30456;&#32467;&#21512;&#65292;&#20135;&#29983;&#19968;&#31995;&#21015;&#20851;&#20110;&#34892;&#20026;&#23884;&#20837;&#30340;&#39640;&#24615;&#33021;&#21644;&#34892;&#20026;&#22810;&#26679;&#24615;&#30340;&#31574;&#30053;&#38598;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;QD-RL&#26041;&#27861;&#36804;&#20170;&#20026;&#27490;&#21033;&#29992;&#20102;&#26679;&#26412;&#26377;&#25928;&#30340;&#31163;&#31574;&#30053;RL&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#39640;&#21534;&#21520;&#37327;&#12289;&#22823;&#35268;&#27169;&#24182;&#34892;&#21270;&#30340;&#26426;&#22120;&#20154;&#27169;&#25311;&#22120;&#30340;&#36827;&#27493;&#24050;&#32463;&#25171;&#24320;&#20102;&#33021;&#22815;&#21033;&#29992;&#36825;&#31181;&#24182;&#34892;&#24615;&#30340;&#31639;&#27861;&#30340;&#22823;&#38376;&#65292;&#32780;&#23558;&#29616;&#26377;&#30340;&#31163;&#31574;&#30053;QD-RL&#26041;&#27861;&#25193;&#23637;&#21040;&#36825;&#20123;&#26032;&#30340;&#25968;&#25454;&#20016;&#23500;&#30340;&#29615;&#22659;&#36824;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#37319;&#29992;&#20102;&#33021;&#22815;&#21033;&#29992;&#22823;&#35268;&#27169;&#24182;&#34892;&#24615;&#30340;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;(PPO)&#31561;&#31574;&#30053;&#26041;&#27861;&#19982;QD&#30456;&#32467;&#21512;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;QD-RL&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generally capable agents that perform well in unseen dynamic environments is a long-term goal of robot learning. Quality Diversity Reinforcement Learning (QD-RL) is an emerging class of reinforcement learning (RL) algorithms that blend insights from Quality Diversity (QD) and RL to produce a collection of high performing and behaviorally diverse policies with respect to a behavioral embedding. Existing QD-RL approaches have thus far taken advantage of sample-efficient off-policy RL algorithms. However, recent advances in high-throughput, massively parallelized robotic simulators have opened the door for algorithms that can take advantage of such parallelism, and it is unclear how to scale existing off-policy QD-RL methods to these new data-rich regimes. In this work, we take the first steps to combine on-policy RL methods, specifically Proximal Policy Optimization (PPO), that can leverage massive parallelism, with QD, and propose a new QD-RL method with these high-throughput s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.11863</link><description>&lt;p&gt;
&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#30340;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling laws for language encoding models in fMRI. (arXiv:2305.11863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#21333;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#22823;&#33041;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#19982;&#22823;&#33041;&#30340;&#30740;&#31350;&#37117;&#20351;&#29992;&#20102;&#31867;&#20284;GPT-2&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#26159;&#21542;&#26356;&#22823;&#30340;&#24320;&#28304;&#27169;&#22411;&#65288;&#22914;OPT&#21644;LLaMA&#31995;&#21015;&#65289;&#26356;&#36866;&#29992;&#20110;&#39044;&#27979;&#20351;&#29992;fMRI&#35760;&#24405;&#30340;&#22823;&#33041;&#21453;&#24212;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20174;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#22823;&#33041;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#36328;3&#20010;&#21463;&#35797;&#32773;&#30340;&#20445;&#30041;&#27979;&#35797;&#38598;&#30456;&#20851;&#24615;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;&#24403;&#25193;&#23637;fMRI&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#26102;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#30340;&#23545;&#25968;&#32447;&#24615;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#23545;&#20351;&#29992;HuBERT&#65292;WavLM&#21644;Whisper&#30340;&#22768;&#23398;&#32534;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#24102;&#26469;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22122;&#38899;&#22825;&#33457;&#26495;&#20998;&#26512;&#20102;&#36825;&#20123;&#22823;&#35268;&#27169;&#19988;&#39640;&#24615;&#33021;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations from transformer-based unidirectional language models are known to be effective at predicting brain responses to natural language. However, most studies comparing language models to brains have used GPT-2 or similarly sized language models. Here we tested whether larger open-source models such as those from the OPT and LLaMA families are better at predicting brain responses recorded using fMRI. Mirroring scaling results from other contexts, we found that brain prediction performance scales log-linearly with model size from 125M to 30B parameter models, with ~15% increased encoding performance as measured by correlation with a held-out test set across 3 subjects. Similar log-linear behavior was observed when scaling the size of the fMRI training set. We also characterized scaling for acoustic encoding models that use HuBERT, WavLM, and Whisper, and we found comparable improvements with model size. A noise ceiling analysis of these large, high-performance encoding models 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10163</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20013;&#22269;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#19978;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model. (arXiv:2305.10163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;GPT&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;ChatGPT&#24050;&#34987;&#25972;&#21512;&#21040;&#21508;&#20010;&#39046;&#22495;&#30340;&#24037;&#20316;&#27969;&#20013;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#20854;&#24494;&#35843;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#19981;&#36275;&#65292;&#38459;&#30861;&#20102;&#20854;&#22312;&#38656;&#35201;&#24191;&#27867;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#35821;&#20041;&#30693;&#35782;&#30340;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#65288;CNMLE&#65289;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#65292;&#21363;&#20174;&#20004;&#20010;&#26041;&#38754;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#23558;&#21307;&#23398;&#32972;&#26223;&#30693;&#35782;&#25552;&#21462;&#20026;&#35821;&#20041;&#25351;&#20196;&#26469;&#25351;&#23548;ChatGPT&#30340;&#25512;&#26029;&#12290;&#31867;&#20284;&#22320;&#65292;&#30456;&#20851;&#30340;&#21307;&#30103;&#38382;&#39064;&#34987;&#35782;&#21035;&#24182;&#20316;&#20026;&#28436;&#31034;&#36755;&#20837;&#32473;ChatGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30452;&#25509;&#24212;&#29992;ChatGPT&#26080;&#27861;&#22312;CNMLE&#19978;&#33719;&#24471;&#21512;&#26684;&#20998;&#25968;&#65288;51&#20998;&#65289;&#65292;&#21482;&#26377;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#35757;&#32451;&#30340;&#27169;&#22411;&#25104;&#21151;&#36890;&#36807;&#32771;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-Training (GPT) models like ChatGPT have demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. Although ChatGPT has been integrated into the overall workflow to boost efficiency in many domains, the lack of flexibility in the finetuning process hinders its applications in areas that demand extensive domain expertise and semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on the China National Medical Licensing Examination (CNMLE) and propose a novel approach to improve ChatGPT from two perspectives: integrating medical domain knowledge and enabling few-shot learning. By using a simple but effective retrieval method, medical background knowledge is extracted as semantic instructions to guide the inference of ChatGPT. Similarly, relevant medical questions are identified and fed as demonstrations to ChatGPT. Experimental results show that directly applying ChatGPT fails to qualify the CNMLE at a score of 51 (i.e., onl
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26234;&#33021;&#30005;&#32593;&#25925;&#38556;&#39044;&#27979;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#26234;&#33021;&#30005;&#32593;&#20013;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#31361;&#20986;&#20102;&#30446;&#21069;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23384;&#22312;&#23545;&#21508;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.18136</link><description>&lt;p&gt;
&#26234;&#33021;&#30005;&#32593;&#25925;&#38556;&#39044;&#27979;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Machine-learned Adversarial Attacks against Fault Prediction Systems in Smart Electrical Grids. (arXiv:2303.18136v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18136
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#26234;&#33021;&#30005;&#32593;&#25925;&#38556;&#39044;&#27979;&#31995;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#23545;&#25239;&#25915;&#20987;&#30340;&#30740;&#31350;&#65292;&#35777;&#26126;&#26234;&#33021;&#30005;&#32593;&#20013;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#65292;&#24182;&#31361;&#20986;&#20102;&#30446;&#21069;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23384;&#22312;&#23545;&#21508;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#65292;&#30001;&#20110;&#32463;&#27982;&#21644;&#20851;&#38190;&#24615;&#30340;&#21407;&#22240;&#65292;&#25925;&#38556;&#26816;&#27979;&#20219;&#21153;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#26234;&#33021;&#30005;&#32593;&#24212;&#29992;&#31243;&#24207;&#65292;&#22914;&#32570;&#38519;&#26816;&#27979;&#21644;&#36127;&#36733;&#39044;&#27979;&#65292;&#24050;&#32463;&#37319;&#29992;&#20102;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#30740;&#31350;&#26234;&#33021;&#30005;&#32593;&#24773;&#20917;&#19979;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#24212;&#29992;&#30340;&#23433;&#20840;&#24615;&#25361;&#25112;&#12290;&#20107;&#23454;&#19978;&#65292;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#31639;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#23433;&#20840;&#24615;&#23578;&#26410;&#19982;&#25152;&#26377;&#30005;&#32593;&#24212;&#29992;&#31243;&#24207;&#30456;&#20851;&#22320;&#36827;&#34892;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#26234;&#33021;&#30005;&#32593;&#20013;&#20351;&#29992;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#31361;&#20986;&#23637;&#31034;&#20102;&#25925;&#38556;&#23450;&#20301;&#21644;&#31867;&#22411;&#20998;&#31867;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#35828;&#26126;&#20102;&#30446;&#21069;&#22312;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#23545;&#21508;&#31181;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
In smart electrical grids, fault detection tasks may have a high impact on society due to their economic and critical implications. In the recent years, numerous smart grid applications, such as defect detection and load forecasting, have embraced data-driven methodologies. The purpose of this study is to investigate the challenges associated with the security of machine learning (ML) applications in the smart grid scenario. Indeed, the robustness and security of these data-driven algorithms have not been extensively studied in relation to all power grid applications. We demonstrate first that the deep neural network method used in the smart grid is susceptible to adversarial perturbation. Then, we highlight how studies on fault localization and type classification illustrate the weaknesses of present ML algorithms in smart grids to various adversarial attacks
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30740;&#31350;&#20102;&#30005;&#21147;&#38656;&#27714;&#21644;&#31038;&#20250;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#35789;&#39057;&#12289;&#20844;&#20247;&#24773;&#24863;&#12289;&#20027;&#39064;&#20998;&#24067;&#21644;&#35789;&#23884;&#20837;&#31561;&#25991;&#26412;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;&#27425;&#26085;&#30340;&#30005;&#21147;&#38656;&#27714;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#35777;&#23454;&#20102;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25913;&#36827;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.07535</link><description>&lt;p&gt;
&#26032;&#38395;&#21644;&#36127;&#33655;&#65306;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#29992;&#20110;&#39044;&#27979;&#27425;&#26085;&#30005;&#21147;&#31995;&#32479;&#38656;&#27714;&#30340;&#37327;&#21270;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
News and Load: A Quantitative Exploration of Natural Language Processing Applications for Forecasting Day-ahead Electricity System Demand. (arXiv:2301.07535v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07535
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#30740;&#31350;&#20102;&#30005;&#21147;&#38656;&#27714;&#21644;&#31038;&#20250;&#20107;&#20214;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;&#35789;&#39057;&#12289;&#20844;&#20247;&#24773;&#24863;&#12289;&#20027;&#39064;&#20998;&#24067;&#21644;&#35789;&#23884;&#20837;&#31561;&#25991;&#26412;&#29305;&#24449;&#65292;&#25913;&#36827;&#20102;&#27425;&#26085;&#30340;&#30005;&#21147;&#38656;&#27714;&#39044;&#27979;&#12290;&#30740;&#31350;&#32467;&#26524;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#65292;&#35777;&#23454;&#20102;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25913;&#36827;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#21147;&#38656;&#27714;&#19982;&#22825;&#27668;&#20043;&#38388;&#30340;&#20851;&#31995;&#22312;&#30005;&#21147;&#31995;&#32479;&#20013;&#24471;&#21040;&#20102;&#30830;&#35748;&#65292;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#20551;&#26085;&#21644;&#37325;&#22823;&#20107;&#20214;&#31561;&#34892;&#20026;&#21644;&#31038;&#20250;&#22240;&#32032;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#25104;&#29087;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#38656;&#27714;&#39044;&#27979;&#25216;&#26415;&#65292;&#25506;&#32034;&#20102;&#30005;&#21147;&#38656;&#27714;&#19982;&#26356;&#32454;&#33268;&#30340;&#31038;&#20250;&#20107;&#20214;&#20449;&#24687;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35789;&#39057;&#12289;&#20844;&#20247;&#24773;&#24863;&#12289;&#20027;&#39064;&#20998;&#24067;&#21644;&#35789;&#23884;&#20837;&#31561;&#25991;&#26412;&#29305;&#24449;&#21487;&#20197;&#25913;&#21892;&#27425;&#26085;&#39044;&#27979;&#12290;&#36825;&#20123;&#29305;&#24449;&#20013;&#21253;&#21547;&#20102;&#20840;&#29699;&#22823;&#27969;&#34892;&#12289;&#25919;&#27835;&#12289;&#22269;&#38469;&#20914;&#31361;&#12289;&#20132;&#36890;&#31561;&#31038;&#20250;&#20107;&#20214;&#12290;&#36890;&#36807;&#35752;&#35770;&#22240;&#26524;&#25928;&#24212;&#21644;&#30456;&#20851;&#24615;&#65292;&#25552;&#20986;&#20102;&#20851;&#32852;&#26426;&#21046;&#30340;&#35299;&#37322;&#12290;&#26412;&#30740;&#31350;&#35748;&#20026;&#21487;&#20197;&#20026;&#20256;&#32479;&#30340;&#30005;&#21147;&#38656;&#27714;&#20998;&#26512;&#25552;&#20379;&#26032;&#30340;&#35270;&#35282;&#65292;&#35777;&#23454;&#20102;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#20013;&#25913;&#36827;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The relationship between electricity demand and weather is well established in power systems, along with the importance of behavioral and social aspects such as holidays and significant events. This study explores the link between electricity demand and more nuanced information about social events. This is done using mature Natural Language Processing (NLP) and demand forecasting techniques. The results indicate that day-ahead forecasts are improved by textual features such as word frequencies, public sentiments, topic distributions, and word embeddings. The social events contained in these features include global pandemics, politics, international conflicts, transportation, etc. Causality effects and correlations are discussed to propose explanations for the mechanisms behind the links highlighted. This study is believed to bring a new perspective to traditional electricity demand analysis. It confirms the feasibility of improving forecasts from unstructured text, with potential conse
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21487;&#36798;&#24615;&#39564;&#35777;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#39564;&#35777;&#35777;&#25454;&#29983;&#25104;&#23545;&#20110;&#19981;&#20934;&#30830;&#35266;&#27979;&#30340;&#23433;&#20840;&#23646;&#24615;&#26816;&#26597;&#65292;&#25552;&#20379;&#20102;&#23616;&#37096;&#21644;&#25972;&#20307;&#30340;&#21487;&#38752;&#24615;&#37327;&#21270;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2210.14991</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#36798;&#24615;&#39564;&#35777;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Reachability Verification Based Reliability Assessment for Deep Reinforcement Learning Controlled Robotics and Autonomous Systems. (arXiv:2210.14991v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#21487;&#36798;&#24615;&#39564;&#35777;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25511;&#21046;&#30340;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#39564;&#35777;&#35777;&#25454;&#29983;&#25104;&#23545;&#20110;&#19981;&#20934;&#30830;&#35266;&#27979;&#30340;&#23433;&#20840;&#23646;&#24615;&#26816;&#26597;&#65292;&#25552;&#20379;&#20102;&#23616;&#37096;&#21644;&#25972;&#20307;&#30340;&#21487;&#38752;&#24615;&#37327;&#21270;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#26426;&#22120;&#20154;&#21644;&#33258;&#20027;&#31995;&#32479;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#12290;&#20854;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#23384;&#22312;&#19981;&#23433;&#20840;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#12290;&#26410;&#25506;&#32034;&#30340;&#29366;&#24577;&#21487;&#33021;&#23548;&#33268;Agent&#20570;&#20986;&#38169;&#35823;&#20915;&#31574;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#21361;&#38505;&#65292;&#29305;&#21035;&#26159;&#22312;DRL&#35757;&#32451;&#30340;&#31471;&#21040;&#31471;&#25511;&#21046;&#22120;&#25351;&#23548;&#19979;&#30340;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23450;&#37327;&#21487;&#38752;&#24615;&#35780;&#20272;&#26694;&#26550;&#65292;&#38024;&#23545;DRL&#25511;&#21046;&#30340;RAS&#65292;&#21033;&#29992;&#20174;&#31070;&#32463;&#32593;&#32476;&#30340;&#24418;&#24335;&#21487;&#38752;&#24615;&#20998;&#26512;&#29983;&#25104;&#30340;&#39564;&#35777;&#35777;&#25454;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#20004;&#32423;&#39564;&#35777;&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#26597;&#19982;&#19981;&#20934;&#30830;&#30340;&#35266;&#27979;&#30456;&#20851;&#30340;&#23433;&#20840;&#23646;&#24615;&#65292;&#20363;&#22914;&#29615;&#22659;&#22122;&#22768;&#21644;&#29366;&#24577;&#21464;&#21270;&#12290;&#36890;&#36807;&#26412;&#22320;&#21033;&#29992;&#21487;&#36798;&#24615;&#39564;&#35777;&#24037;&#20855;&#29983;&#25104;&#36712;&#36857;&#30340;&#23433;&#20840;&#35777;&#25454;&#12290;&#30456;&#21453;&#65292;&#22312;&#20840;&#23616;&#32423;&#21035;&#19978;&#65292;&#25105;&#20204;&#23558;&#25972;&#20307;&#21487;&#38752;&#24615;&#37327;&#21270;&#20026;&#26412;&#22320;&#23433;&#20840;&#35777;&#25454;&#30340;&#32858;&#21512;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning (DRL) has achieved impressive performance in robotics and autonomous systems (RAS). A key challenge to its deployment in real-life operations is the presence of spuriously unsafe DRL policies. Unexplored states may lead the agent to make wrong decisions that could result in hazards, especially in applications where DRL-trained end-to-end controllers govern the behaviour of RAS. This paper proposes a novel quantitative reliability assessment framework for DRL-controlled RAS, leveraging verification evidence generated from formal reliability analysis of neural networks. A two-level verification framework is introduced to check the safety property with respect to inaccurate observations that are due to, e.g., environmental noise and state changes. Reachability verification tools are leveraged locally to generate safety evidence of trajectories. In contrast, at the global level, we quantify the overall reliability as an aggregated metric of local safety evidence
&lt;/p&gt;</description></item><item><title>FreDSNet&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#20840;&#26223;&#22270;&#20687;&#20013;&#23454;&#29616;&#23460;&#20869;&#29615;&#22659;&#30340;&#35821;&#20041;&#19977;&#32500;&#29702;&#35299;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#39057;&#29575;&#22495;&#20013;&#30340;&#21367;&#31215;&#20174;&#32780;&#33719;&#24471;&#26356;&#23485;&#24191;&#30340;&#24863;&#21463;&#37326;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#32852;&#21512;&#25552;&#20379;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#30340;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2210.01595</link><description>&lt;p&gt;
FreDSNet: &#21033;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21367;&#31215;&#36827;&#34892;&#21333;&#30446;&#28145;&#24230;&#21644;&#35821;&#20041;&#20998;&#21106;&#30340;&#32852;&#21512;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
FreDSNet: Joint Monocular Depth and Semantic Segmentation with Fast Fourier Convolutions. (arXiv:2210.01595v1 [cs.CV] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01595
&lt;/p&gt;
&lt;p&gt;
FreDSNet&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#20840;&#26223;&#22270;&#20687;&#20013;&#23454;&#29616;&#23460;&#20869;&#29615;&#22659;&#30340;&#35821;&#20041;&#19977;&#32500;&#29702;&#35299;&#65292;&#24182;&#19988;&#36890;&#36807;&#21033;&#29992;&#39057;&#29575;&#22495;&#20013;&#30340;&#21367;&#31215;&#20174;&#32780;&#33719;&#24471;&#26356;&#23485;&#24191;&#30340;&#24863;&#21463;&#37326;&#12290;&#23427;&#26159;&#31532;&#19968;&#20010;&#32852;&#21512;&#25552;&#20379;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;FreDSNet&#65292;&#36825;&#26159;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20174;&#21333;&#20010;&#20840;&#26223;&#22270;&#20687;&#20013;&#33719;&#21462;&#23460;&#20869;&#29615;&#22659;&#30340;&#35821;&#20041;&#19977;&#32500;&#29702;&#35299;&#12290;&#20840;&#26223;&#22270;&#20687;&#30001;&#20110;&#25552;&#20379;&#20102;&#25972;&#20010;&#29615;&#22659;&#30340;360&#24230;&#32972;&#26223;&#20449;&#24687;&#65292;&#23545;&#20110;&#35299;&#20915;&#22330;&#26223;&#29702;&#35299;&#38382;&#39064;&#20855;&#26377;&#29305;&#23450;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#20840;&#26223;&#22270;&#20687;&#30340;&#22266;&#26377;&#29305;&#28857;&#32473;&#23545;&#35937;&#30340;&#20934;&#30830;&#26816;&#27979;&#21644;&#20998;&#21106;&#25110;&#32773;&#33391;&#22909;&#30340;&#28145;&#24230;&#20272;&#35745;&#24102;&#26469;&#20102;&#39069;&#22806;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#39057;&#29575;&#22495;&#20013;&#30340;&#21367;&#31215;&#65292;&#22312;&#27599;&#20010;&#21367;&#31215;&#23618;&#20013;&#33719;&#24471;&#26356;&#23485;&#24191;&#30340;&#24863;&#21463;&#37326;&#12290;&#36825;&#20123;&#21367;&#31215;&#20801;&#35768;&#20174;&#20840;&#26223;&#22270;&#20687;&#20013;&#21033;&#29992;&#25972;&#20010;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;FreDSNet&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#24555;&#36895;&#20613;&#37324;&#21494;&#21367;&#31215;&#20174;&#21333;&#20010;&#20840;&#26223;&#22270;&#20687;&#20013;&#32852;&#21512;&#25552;&#20379;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#21644;&#35821;&#20041;&#20998;&#21106;&#30340;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;FreDSNet&#30340;&#24615;&#33021;&#19982;&#29305;&#23450;&#29366;&#24577;&#30340;&#26041;&#27861;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we present FreDSNet, a deep learning solution which obtains semantic 3D understanding of indoor environments from single panoramas. Omnidirectional images reveal task-specific advantages when addressing scene understanding problems due to the 360-degree contextual information about the entire environment they provide. However, the inherent characteristics of the omnidirectional images add additional problems to obtain an accurate detection and segmentation of objects or a good depth estimation. To overcome these problems, we exploit convolutions in the frequential domain obtaining a wider receptive field in each convolutional layer. These convolutions allow to leverage the whole context information from omnidirectional images. FreDSNet is the first network that jointly provides monocular depth estimation and semantic segmentation from a single panoramic image exploiting fast Fourier convolutions. Our experiments show that FreDSNet has similar performance as specific state 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#30913;&#24815;&#24615;&#23548;&#33322;&#30340;&#21160;&#24577;&#20256;&#24863;&#22120;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#36716;&#21270;&#20026;&#22320;&#29699;&#30340;&#30913;&#22330;&#20316;&#20026;&#19990;&#30028;&#22352;&#26631;&#31995;&#26469;&#23454;&#29616;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#37325;&#24314;&#12290;</title><link>http://arxiv.org/abs/2208.06233</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#30913;&#24815;&#24615;&#23548;&#33322;&#30340;&#21160;&#24577;&#20256;&#24863;&#22120;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Dynamic Sensor Matching based on Geomagnetic Inertial Navigation. (arXiv:2208.06233v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.06233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#30913;&#24815;&#24615;&#23548;&#33322;&#30340;&#21160;&#24577;&#20256;&#24863;&#22120;&#21305;&#37197;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#36716;&#21270;&#20026;&#22320;&#29699;&#30340;&#30913;&#22330;&#20316;&#20026;&#19990;&#30028;&#22352;&#26631;&#31995;&#26469;&#23454;&#29616;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20809;&#23398;&#20256;&#24863;&#22120;&#33021;&#22815;&#25429;&#25417;&#21160;&#24577;&#29615;&#22659;&#24182;&#22312;&#20960;&#20046;&#23454;&#26102;&#20013;&#33719;&#24471;&#28145;&#24230;&#20449;&#24687;&#12290;&#36825;&#20123;&#25968;&#23383;&#37325;&#24314;&#30340;&#36136;&#37327;&#21462;&#20915;&#20110;&#22240;&#32032;&#65292;&#22914;&#29031;&#26126;&#12289;&#34920;&#38754;&#21644;&#32441;&#29702;&#26465;&#20214;&#12289;&#24863;&#27979;&#36895;&#24230;&#21644;&#20854;&#20182;&#20256;&#24863;&#22120;&#29305;&#24615;&#20197;&#21450;&#20256;&#24863;&#22120;&#19982;&#29289;&#20307;&#30340;&#20851;&#31995;&#12290;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#22810;&#20256;&#24863;&#22120;&#21160;&#24577;&#37319;&#38598;&#30340;&#25968;&#25454;&#26469;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#21305;&#37197;&#26469;&#33258;&#22810;&#20010;&#20256;&#24863;&#22120;&#30340;&#25968;&#25454;&#38656;&#35201;&#19968;&#20010;&#20849;&#20139;&#30340;&#19990;&#30028;&#22352;&#26631;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22810;&#20256;&#24863;&#22120;&#25968;&#25454;&#36716;&#21270;&#20026;&#19968;&#20010;&#20849;&#21516;&#21442;&#32771;&#30340;&#19990;&#30028;&#22352;&#26631;&#31995;&#30340;&#27010;&#24565;&#65306;&#22320;&#29699;&#30340;&#30913;&#22330;&#12290;&#25105;&#20204;&#34892;&#20351;&#30340;&#30913;&#22330;&#25552;&#20379;&#20102;&#19968;&#20010;&#21487;&#38752;&#30340;&#19990;&#30028;&#22352;&#26631;&#31995;&#65292;&#21487;&#20197;&#20316;&#20026;&#30830;&#23450;&#20301;&#32622;&#30340;&#21160;&#24577;&#29615;&#22659;&#37325;&#24314;&#30340;&#21442;&#32771;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#26469;&#33258;Stereolabs&#30340;ZED 2&#31435;&#20307;&#30456;&#26426;&#30340;&#30913;&#22330;&#20256;&#24863;&#22120;&#36827;&#34892;&#35780;&#20272;&#65292;&#35813;&#20256;&#24863;&#22120;&#25552;&#20379;&#19982;&#21271;&#26497;&#30340;&#26041;&#21521;&#30456;&#23545;&#24212;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Optical sensors can capture dynamic environments and derive depth information in near real-time. The quality of these digital reconstructions is determined by factors like illumination, surface and texture conditions, sensing speed and other sensor characteristics as well as the sensor-object relations. Improvements can be obtained by using dynamically collected data from multiple sensors. However, matching the data from multiple sensors requires a shared world coordinate system. We present a concept for transferring multi-sensor data into a commonly referenced world coordinate system: the earth's magnetic field. The steady presence of our planetary magnetic field provides a reliable world coordinate system, which can serve as a reference for a position-defined reconstruction of dynamic environments. Our approach is evaluated using magnetic field sensors of the ZED 2 stereo camera from Stereolabs, which provides orientation relative to the North Pole similar to a compass. With the help
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#20272;&#35745;&#24178;&#39044;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#26102;&#38388;&#21464;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#20851;&#31995;&#21644;&#21327;&#21464;&#37327;&#21453;&#20107;&#23454;&#39044;&#27979;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.01900</link><description>&lt;p&gt;
&#22312;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#20272;&#35745;&#21453;&#20107;&#23454;&#27835;&#30103;&#32467;&#26524;&#30340;&#26102;&#38388;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
Estimating counterfactual treatment outcomes over time in complex multi-agent scenarios. (arXiv:2206.01900v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#22810;&#26234;&#33021;&#20307;&#22330;&#26223;&#20013;&#20272;&#35745;&#24178;&#39044;&#25928;&#26524;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#20102;&#26102;&#38388;&#21464;&#21270;&#30340;&#22810;&#26234;&#33021;&#20307;&#20851;&#31995;&#21644;&#21327;&#21464;&#37327;&#21453;&#20107;&#23454;&#39044;&#27979;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65292;&#24182;&#25552;&#20379;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#24037;&#31243;&#21644;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#35780;&#20272;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#20013;&#30340;&#24178;&#39044;&#34892;&#20026;&#65288;&#20363;&#22914;&#65292;&#20154;&#31867;&#20309;&#26102;&#24212;&#35813;&#24178;&#39044;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#65292;&#20309;&#26102;&#29699;&#21592;&#24212;&#35813;&#20256;&#32473;&#38431;&#21451;&#36827;&#34892;&#22909;&#23556;&#38376;&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20351;&#29992;&#21453;&#20107;&#23454;&#30340;&#38271;&#26399;&#39044;&#27979;&#26469;&#20272;&#35745;&#20010;&#20307;&#27835;&#30103;&#25928;&#26524;&#65288;ITE&#65289;&#26159;&#35780;&#20272;&#27492;&#31867;&#24178;&#39044;&#25514;&#26045;&#30340;&#23454;&#29992;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20256;&#32479;&#26694;&#26550;&#27809;&#26377;&#32771;&#34385;&#21040;&#22810;&#26234;&#33021;&#20307;&#20851;&#31995;&#30340;&#26102;&#38388;&#21464;&#21270;&#21644;&#21327;&#21464;&#37327;&#21453;&#20107;&#23454;&#39044;&#27979;&#30340;&#22797;&#26434;&#32467;&#26500;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;ITE&#30340;&#38169;&#35823;&#35780;&#20272;&#21644;&#35299;&#37322;&#22256;&#38590;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#21453;&#20107;&#23454;&#24490;&#29615;&#32593;&#32476;&#65292;&#29992;&#20110;&#20272;&#35745;&#24178;&#39044;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21033;&#29992;&#22270;&#24418;&#21464;&#20998;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21644;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#35745;&#31639;&#26469;&#36827;&#34892;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#21464;&#37327;&#21644;&#32467;&#26524;&#30340;&#38271;&#26399;&#39044;&#27979;&#30340;ITE&#20272;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#30830;&#35748;&#24490;&#29615;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of intervention in a multi-agent system, e.g., when humans should intervene in autonomous driving systems and when a player should pass to teammates for a good shot, is challenging in various engineering and scientific fields. Estimating the individual treatment effect (ITE) using counterfactual long-term prediction is practical to evaluate such interventions. However, most of the conventional frameworks did not consider the time-varying complex structure of multi-agent relationships and covariate counterfactual prediction. This may lead to erroneous assessments of ITE and difficulty in interpretation. Here we propose an interpretable, counterfactual recurrent network in multi-agent systems to estimate the effect of the intervention. Our model leverages graph variational recurrent neural networks and theory-based computation with domain knowledge for the ITE estimation framework based on long-term prediction of multi-agent covariates and outcomes, which can confirm the circu
&lt;/p&gt;</description></item></channel></rss>