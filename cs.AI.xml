<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;PromptCodec&#65292;&#19968;&#31181;&#20351;&#29992;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#30340;&#29305;&#24449;&#24863;&#30693;&#25552;&#31034;&#32534;&#30721;&#22120;&#30340;&#39640;&#20445;&#30495;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#29305;&#24449;&#34920;&#31034;&#12289;&#33258;&#36866;&#24212;&#29305;&#24449;&#21152;&#26435;&#34701;&#21512;&#21644;&#25928;&#29575;&#20248;&#21270;&#26469;&#35299;&#20915;&#39640;&#21387;&#32553;&#29575;&#19979;&#30340;&#39640;&#20445;&#30495;&#38899;&#39057;&#37325;&#24314;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.02702</link><description>&lt;p&gt;
PromptCodec: &#20351;&#29992;&#22522;&#20110;&#33258;&#36866;&#24212;&#29305;&#24449;&#24863;&#30693;&#30340;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#30340;&#39640;&#20445;&#30495;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
PromptCodec: High-Fidelity Neural Speech Codec using Disentangled Representation Learning based Adaptive Feature-aware Prompt Encoders
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;PromptCodec&#65292;&#19968;&#31181;&#20351;&#29992;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#30340;&#29305;&#24449;&#24863;&#30693;&#25552;&#31034;&#32534;&#30721;&#22120;&#30340;&#39640;&#20445;&#30495;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#65292;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#29305;&#24449;&#34920;&#31034;&#12289;&#33258;&#36866;&#24212;&#29305;&#24449;&#21152;&#26435;&#34701;&#21512;&#21644;&#25928;&#29575;&#20248;&#21270;&#26469;&#35299;&#20915;&#39640;&#21387;&#32553;&#29575;&#19979;&#30340;&#39640;&#20445;&#30495;&#38899;&#39057;&#37325;&#24314;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#36817;&#26469;&#22312;&#29983;&#25104;&#35821;&#38899;&#24314;&#27169;&#39046;&#22495;&#24341;&#36215;&#24191;&#27867;&#20851;&#27880;&#65292;&#20363;&#22914;&#35821;&#38899;&#36716;&#25442;&#12289;&#25991;&#26412;&#36716;&#35821;&#38899;&#21512;&#25104;&#31561;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#21387;&#32553;&#29575;&#19979;&#30830;&#20445;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#30340;&#39640;&#20445;&#30495;&#38899;&#39057;&#37325;&#24314;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PromptCodec&#65292;&#19968;&#31181;&#20351;&#29992;&#22522;&#20110;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#30340;&#29305;&#24449;&#24863;&#30693;&#25552;&#31034;&#32534;&#30721;&#22120;&#30340;&#26032;&#22411;&#31471;&#21040;&#31471;&#31070;&#32463;&#35821;&#38899;&#32534;&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#36890;&#36807;&#24341;&#20837;&#26469;&#33258;&#25552;&#31034;&#32534;&#30721;&#22120;&#30340;&#39069;&#22806;&#29305;&#24449;&#34920;&#31034;&#65292;PromptCodec&#21487;&#20197;&#20998;&#37197;&#38656;&#35201;&#22788;&#29702;&#30340;&#35821;&#38899;&#20449;&#24687;&#24182;&#22686;&#24378;&#20854;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#29305;&#24449;&#21152;&#26435;&#34701;&#21512;&#26041;&#27861;&#65292;&#20197;&#25972;&#21512;&#19981;&#21516;&#32534;&#30721;&#22120;&#30340;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20313;&#24358;&#36317;&#31163;&#30340;&#26032;&#39062;&#31163;&#25955;&#34920;&#31034;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#20248;&#21270;PromptCodec&#30340;&#32534;&#30721;&#22120;&#20197;&#30830;&#20445;&#20854;&#25928;&#29575;&#65292;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02702v1 Announce Type: cross  Abstract: Neural speech codec has recently gained widespread attention in generative speech modeling domains, like voice conversion, text-to-speech synthesis, etc. However, ensuring high-fidelity audio reconstruction of speech codecs under high compression rates remains an open and challenging issue. In this paper, we propose PromptCodec, a novel end-to-end neural speech codec model using disentangled representation learning based feature-aware prompt encoders. By incorporating additional feature representations from prompt encoders, PromptCodec can distribute the speech information requiring processing and enhance its capabilities. Moreover, a simple yet effective adaptive feature weighted fusion approach is introduced to integrate features of different encoders. Meanwhile, we propose a novel disentangled representation learning strategy based on cosine distance to optimize PromptCodec's encoders to ensure their efficiency, thereby further impr
&lt;/p&gt;</description></item><item><title>&#26368;&#20339;&#26041;&#27861;&#20934;&#30830;&#35299;&#20915;&#20102;&#28041;&#21450;&#24050;&#30693;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#24773;&#26223;&#65292;&#20294;&#20173;&#26410;&#36798;&#21040;&#20934;&#30830;&#39044;&#27979;&#26032;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#27700;&#24179;&#12290;</title><link>https://arxiv.org/abs/2404.02484</link><description>&lt;p&gt;
&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
New methods for drug synergy prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.02484
&lt;/p&gt;
&lt;p&gt;
&#26368;&#20339;&#26041;&#27861;&#20934;&#30830;&#35299;&#20915;&#20102;&#28041;&#21450;&#24050;&#30693;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#33647;&#29289;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#24773;&#26223;&#65292;&#20294;&#20173;&#26410;&#36798;&#21040;&#20934;&#30830;&#39044;&#27979;&#26032;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#23567;&#22411;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20381;&#36182;&#20110;&#39640;&#36890;&#37327;&#32452;&#21512;&#31579;&#36873;&#30340;&#33647;&#29289;&#32452;&#21512;&#21327;&#21516;&#20316;&#29992;&#30340;&#26032;&#39044;&#27979;&#26041;&#27861;&#12290;&#33258;2021&#24180;&#20197;&#26469;&#65292;&#35813;&#39046;&#22495;&#21462;&#24471;&#20102;&#36805;&#36895;&#36827;&#23637;&#65292;&#24050;&#21457;&#34920;&#20102;&#36229;&#36807;30&#31181;&#21407;&#21019;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#32477;&#22823;&#22810;&#25968;&#26159;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#31361;&#26174;&#26041;&#27861;&#20013;&#20351;&#29992;&#30340;&#26680;&#24515;&#25216;&#26415;&#12289;&#25968;&#25454;&#26469;&#28304;&#12289;&#36755;&#20837;&#25968;&#25454;&#31867;&#22411;&#21644;&#21327;&#21516;&#24471;&#20998;&#65292;&#20197;&#21450;&#35770;&#25991;&#25152;&#28041;&#21450;&#30340;&#39044;&#27979;&#24773;&#26223;&#21644;&#35780;&#20272;&#21327;&#35758;&#65292;&#23558;&#36825;&#20123;&#35770;&#25991;&#25918;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#35270;&#35282;&#19979;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#26159;&#65292;&#26368;&#20339;&#26041;&#27861;&#20934;&#30830;&#22320;&#35299;&#20915;&#20102;&#28041;&#21450;&#24050;&#30693;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#21327;&#21516;&#20316;&#29992;&#39044;&#27979;&#24773;&#26223;&#65292;&#32780;&#28041;&#21450;&#26032;&#33647;&#29289;&#25110;&#32454;&#32990;&#31995;&#30340;&#24773;&#26223;&#20173;&#26410;&#36798;&#21040;&#20934;&#30830;&#39044;&#27979;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.02484v1 Announce Type: cross  Abstract: In this mini-review, we explore the new prediction methods for drug combination synergy relying on high-throughput combinatorial screens. The fast progress of the field is witnessed in the more than thirty original machine learning methods published since 2021, a clear majority of them based on deep learning techniques. We aim to put these papers under a unifying lens by highlighting the core technologies, the data sources, the input data types and synergy scores used in the methods, as well as the prediction scenarios and evaluation protocols that the papers deal with. Our finding is that the best methods accurately solve the synergy prediction scenarios involving known drugs or cell lines while the scenarios involving new drugs or cell lines still fall short of an accurate prediction level.
&lt;/p&gt;</description></item><item><title>HyperCLOVA X &#26159;&#38024;&#23545;&#38889;&#22269;&#35821;&#35328;&#21644;&#25991;&#21270;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20855;&#26377;&#31454;&#20105;&#33021;&#21147;&#30340;&#33521;&#35821;&#12289;&#25968;&#23398;&#21644;&#32534;&#30721;&#33021;&#21147;&#65292;&#20854;&#25512;&#29702;&#33021;&#21147;&#24378;&#22823;&#19988;&#20855;&#26377;&#36328;&#35821;&#35328;&#30340;&#36890;&#29992;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2404.01954</link><description>&lt;p&gt;
HyperCLOVA X &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
HyperCLOVA X Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01954
&lt;/p&gt;
&lt;p&gt;
HyperCLOVA X &#26159;&#38024;&#23545;&#38889;&#22269;&#35821;&#35328;&#21644;&#25991;&#21270;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21516;&#26102;&#20855;&#26377;&#31454;&#20105;&#33021;&#21147;&#30340;&#33521;&#35821;&#12289;&#25968;&#23398;&#21644;&#32534;&#30721;&#33021;&#21147;&#65292;&#20854;&#25512;&#29702;&#33021;&#21147;&#24378;&#22823;&#19988;&#20855;&#26377;&#36328;&#35821;&#35328;&#30340;&#36890;&#29992;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; HyperCLOVA X&#65292;&#36825;&#26159;&#19968;&#31995;&#21015;&#38024;&#23545;&#38889;&#22269;&#35821;&#35328;&#21644;&#25991;&#21270;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#21516;&#26102;&#20855;&#26377;&#22312;&#33521;&#35821;&#12289;&#25968;&#23398;&#21644;&#32534;&#30721;&#26041;&#38754;&#30340;&#31454;&#20105;&#33021;&#21147;&#12290;HyperCLOVA X &#22312;&#24179;&#34913;&#28151;&#21512;&#30340;&#38889;&#35821;&#12289;&#33521;&#35821;&#21644;&#20195;&#30721;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#28982;&#21518;&#36890;&#36807;&#39640;&#36136;&#37327;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#65292;&#21516;&#26102;&#36981;&#23432;&#20005;&#26684;&#30340;&#23433;&#20840;&#20934;&#21017;&#65292;&#20307;&#29616;&#20102;&#25105;&#20204;&#23545;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#25215;&#35834;&#12290;&#35813;&#27169;&#22411;&#22312;&#21253;&#25324;&#32508;&#21512;&#25512;&#29702;&#12289;&#30693;&#35782;&#12289;&#24120;&#35782;&#12289;&#30495;&#23454;&#24615;&#12289;&#32534;&#30721;&#12289;&#25968;&#23398;&#12289;&#32842;&#22825;&#12289;&#36981;&#24490;&#25351;&#20196;&#21644;&#26080;&#23475;&#24615;&#22312;&#20869;&#30340;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#28085;&#30422;&#38889;&#35821;&#21644;&#33521;&#35821;&#12290;HyperCLOVA X &#22312;&#38889;&#35821;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#24471;&#30410;&#20110;&#23545;&#35821;&#35328;&#21644;&#25991;&#21270;&#32454;&#24494;&#24046;&#24322;&#30340;&#28145;&#21051;&#29702;&#35299;&#12290;&#23545;&#22266;&#26377;&#30340;&#21452;&#35821;&#29305;&#24615;&#30340;&#36827;&#19968;&#27493;&#20998;&#26512;&#21450;&#20854;&#25193;&#23637;&#21040;&#22810;&#35821;&#35328;&#30340;&#30740;&#31350;&#31361;&#26174;&#20986;&#35813;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#29087;&#32451;&#24615;&#21644;&#24378;&#22823;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20197;&#36866;&#24212;&#26410;&#23450;&#21521;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01954v1 Announce Type: cross  Abstract: We introduce HyperCLOVA X, a family of large language models (LLMs) tailored to the Korean language and culture, along with competitive capabilities in English, math, and coding. HyperCLOVA X was trained on a balanced mix of Korean, English, and code data, followed by instruction-tuning with high-quality human-annotated datasets while abiding by strict safety guidelines reflecting our commitment to responsible AI. The model is evaluated across various benchmarks, including comprehensive reasoning, knowledge, commonsense, factuality, coding, math, chatting, instruction-following, and harmlessness, in both Korean and English. HyperCLOVA X exhibits strong reasoning capabilities in Korean backed by a deep understanding of the language and cultural nuances. Further analysis of the inherent bilingual nature and its extension to multilingualism highlights the model's cross-lingual proficiency and strong generalization ability to untargeted la
&lt;/p&gt;</description></item><item><title>CHOPS&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#26469;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#65292;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#12290;</title><link>https://arxiv.org/abs/2404.01343</link><description>&lt;p&gt;
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
&lt;/p&gt;
&lt;p&gt;
CHOPS: CHat with custOmer Profile Systems for Customer Service with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01343
&lt;/p&gt;
&lt;p&gt;
CHOPS&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#26356;&#39640;&#25928;&#22320;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#26469;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#65292;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21830;&#19994;&#21644;&#36719;&#20214;&#24179;&#21488;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#20351;&#29992;&#20687;GPT-3.5&#12289;GPT-4&#12289;GLM-3&#21644;LLaMa-2&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#23458;&#25143;&#26381;&#21153;&#30340;&#32842;&#22825;&#36741;&#21161;&#25110;&#25512;&#29702;&#20195;&#29702;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#23458;&#25143;&#26381;&#21153;&#27169;&#22411;&#22312;&#19982;&#23458;&#25143;&#37197;&#32622;&#25991;&#20214;&#30340;&#38598;&#25104;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#65292;&#24182;&#19988;&#32570;&#20047;&#26377;&#25928;&#26381;&#21153;&#25152;&#38656;&#30340;&#25805;&#20316;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CHOPS&#65288;CHat with custOmer Profile in existing System&#65289;&#30340;LLM&#20195;&#29702;&#65292;&#26088;&#22312;&#65306;&#65288;1&#65289;&#39640;&#25928;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#24211;&#25110;&#31995;&#32479;&#20197;&#35775;&#38382;&#29992;&#25143;&#20449;&#24687;&#25110;&#25353;&#29031;&#29616;&#26377;&#25351;&#21335;&#19982;&#36825;&#20123;&#31995;&#32479;&#20132;&#20114;&#65307;&#65288;2&#65289;&#25552;&#20379;&#20934;&#30830;&#21512;&#29702;&#30340;&#21709;&#24212;&#25110;&#22312;&#31995;&#32479;&#20013;&#25191;&#34892;&#25152;&#38656;&#25805;&#20316;&#65292;&#21516;&#26102;&#36991;&#20813;&#26377;&#23475;&#25805;&#20316;&#65307;&#65288;3&#65289;&#21033;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01343v1 Announce Type: cross  Abstract: Businesses and software platforms are increasingly turning to Large Language Models (LLMs) such as GPT-3.5, GPT-4, GLM-3, and LLaMa-2 for chat assistance with file access or as reasoning agents for customer service. However, current LLM-based customer service models have limited integration with customer profiles and lack the operational capabilities necessary for effective service. Moreover, existing API integrations emphasize diversity over the precision and error avoidance essential in real-world customer service scenarios. To address these issues, we propose an LLM agent named CHOPS (CHat with custOmer Profile in existing System), designed to: (1) efficiently utilize existing databases or systems for accessing user information or interacting with these systems following existing guidelines; (2) provide accurate and reasonable responses or carry out required operations in the system while avoiding harmful operations; and (3) leverag
&lt;/p&gt;</description></item><item><title>GOV-REK&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#20195;&#29702;&#20998;&#37197;&#22870;&#21169;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27835;&#29702;&#20869;&#26680;&#21033;&#29992;&#29366;&#24577;&#25110;&#32852;&#21512;&#34892;&#21160;&#31354;&#38388;&#30340;&#32467;&#26500;&#65292;&#20197;&#35299;&#20915;&#22870;&#21169;&#24037;&#31243;&#21162;&#21147;&#26080;&#27861;&#36716;&#21270;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2404.01131</link><description>&lt;p&gt;
GOV-REK&#65306;&#29992;&#20110;&#35774;&#35745;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#30340;&#21463;&#31649;&#22870;&#21169;&#24037;&#31243;&#26680;
&lt;/p&gt;
&lt;p&gt;
GOV-REK: Governed Reward Engineering Kernels for Designing Robust Multi-Agent Reinforcement Learning Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01131
&lt;/p&gt;
&lt;p&gt;
GOV-REK&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#20195;&#29702;&#20998;&#37197;&#22870;&#21169;&#20998;&#24067;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#27835;&#29702;&#20869;&#26680;&#21033;&#29992;&#29366;&#24577;&#25110;&#32852;&#21512;&#34892;&#21160;&#31354;&#38388;&#30340;&#32467;&#26500;&#65292;&#20197;&#35299;&#20915;&#22870;&#21169;&#24037;&#31243;&#21162;&#21147;&#26080;&#27861;&#36716;&#21270;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65288;MARLS&#65289;&#65292;&#38382;&#39064;&#30340;&#21046;&#23450;&#36890;&#24120;&#38656;&#35201;&#25237;&#20837;&#22823;&#37327;&#22870;&#21169;&#24037;&#31243;&#24037;&#20316;&#65292;&#38024;&#23545;&#29305;&#23450;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21162;&#21147;&#36890;&#24120;&#26080;&#27861;&#36716;&#21270;&#20026;&#20854;&#20182;&#38382;&#39064;&#65307;&#26356;&#31967;&#31957;&#30340;&#26159;&#65292;&#24403;&#31995;&#32479;&#21160;&#24577;&#21457;&#29983; drastica &#25913;&#21464;&#26102;&#65292;&#36825;&#31181;&#21162;&#21147;&#23601;&#20250;&#34987;&#28010;&#36153;&#12290;&#22312;&#31232;&#30095;&#22870;&#21169;&#22330;&#26223;&#20013;&#65292;&#26377;&#24847;&#20041;&#30340;&#21551;&#21457;&#24335;&#21487;&#20197;&#24110;&#21161;&#31574;&#30053;&#25910;&#25947;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; GOVerned Reward Engineering Kernels (GOV-REK)&#65292;&#22312; MARLS &#30340;&#23398;&#20064;&#38454;&#27573;&#21160;&#24577;&#22320;&#20026;&#20195;&#29702;&#20998;&#37197;&#22870;&#21169;&#20998;&#24067;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#27835;&#29702;&#20869;&#26680;&#65292;&#21033;&#29992;&#29366;&#24577;&#25110;&#32852;&#21512;&#34892;&#21160;&#31354;&#38388;&#20013;&#30340;&#22522;&#30784;&#32467;&#26500;&#26469;&#20998;&#37197;&#26377;&#24847;&#20041;&#30340;&#20195;&#29702;&#22870;&#21169;&#20998;&#24067;&#12290;&#22312;&#20195;&#29702;&#23398;&#20064;&#38454;&#27573;&#65292;&#23427;&#20351;&#29992;&#31867;&#20284; Hyperband &#30340;&#31639;&#27861;&#36845;&#20195;&#22320;&#25506;&#32034;&#19981;&#21516;&#30340;&#22870;&#21169;&#20998;&#24067;&#37197;&#32622;&#65292;&#20197;&#22312;&#38382;&#39064;-&#19981;&#21487;&#30693; ma &#20013;&#23398;&#20064;&#29702;&#24819;&#30340;&#20195;&#29702;&#22870;&#21169;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01131v1 Announce Type: cross  Abstract: For multi-agent reinforcement learning systems (MARLS), the problem formulation generally involves investing massive reward engineering effort specific to a given problem. However, this effort often cannot be translated to other problems; worse, it gets wasted when system dynamics change drastically. This problem is further exacerbated in sparse reward scenarios, where a meaningful heuristic can assist in the policy convergence task. We propose GOVerned Reward Engineering Kernels (GOV-REK), which dynamically assign reward distributions to agents in MARLS during its learning stage. We also introduce governance kernels, which exploit the underlying structure in either state or joint action space for assigning meaningful agent reward distributions. During the agent learning stage, it iteratively explores different reward distribution configurations with a Hyperband-like algorithm to learn ideal agent reward models in a problem-agnostic ma
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;Vision Transformer&#30340;DRCT&#26041;&#27861;&#37319;&#29992;&#21019;&#26032;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#31354;&#38388;&#20449;&#24687;&#34928;&#20943;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2404.00722</link><description>&lt;p&gt;
DRCT&#65306;&#23558;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20445;&#23384;&#22312;&#20449;&#24687;&#29942;&#39048;&#20043;&#22806;
&lt;/p&gt;
&lt;p&gt;
DRCT: Saving Image Super-resolution away from Information Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00722
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Vision Transformer&#30340;DRCT&#26041;&#27861;&#37319;&#29992;&#21019;&#26032;&#30340;&#26426;&#21046;&#35299;&#20915;&#20102;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#20013;&#31354;&#38388;&#20449;&#24687;&#34928;&#20943;&#30340;&#38382;&#39064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;Vision Transformer&#30340;&#20302;&#23618;&#35270;&#35273;&#20219;&#21153;&#24212;&#29992;&#21462;&#24471;&#20102;&#24191;&#27867;&#30340;&#25104;&#21151;&#12290;&#19982;&#22522;&#20110;CNN&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;Transformer&#26356;&#25797;&#38271;&#25429;&#25417;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#21487;&#20197;&#21033;&#29992;&#38750;&#23616;&#37096;&#21306;&#22495;&#30340;&#20449;&#24687;&#37325;&#24314;&#22270;&#20687;&#12290;&#22312;&#36229;&#20998;&#36776;&#29575;&#39046;&#22495;&#65292;&#22522;&#20110;Swin Transformer&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#20027;&#27969;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#25429;&#25417;&#20840;&#23616;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#19988;&#20855;&#26377;&#26059;&#36716;&#31383;&#21475;&#27880;&#24847;&#26426;&#21046;&#65292;&#26377;&#21161;&#20110;&#22312;&#19981;&#21516;&#31383;&#21475;&#20043;&#38388;&#20132;&#25442;&#20449;&#24687;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#25193;&#22823;&#24863;&#30693;&#37326;&#25110;&#35774;&#35745;&#22797;&#26434;&#32593;&#32476;&#26469;&#25552;&#39640;&#22270;&#20687;&#36136;&#37327;&#21644;&#32593;&#32476;&#25928;&#29575;&#65292;&#21462;&#24471;&#20102;&#20196;&#20154;&#31216;&#36190;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#22312;&#21069;&#21521;&#20256;&#25773;&#36807;&#31243;&#20013;&#65292;&#30001;&#20110;&#28145;&#24230;&#22686;&#21152;&#65292;&#31354;&#38388;&#20449;&#24687;&#24448;&#24448;&#20250;&#20943;&#23569;&#65292;&#20174;&#32780;&#23548;&#33268;&#31354;&#38388;&#20449;&#24687;&#30340;&#20002;&#22833;&#65292;&#24182;&#26368;&#32456;&#38480;&#21046;&#20102;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00722v1 Announce Type: cross  Abstract: In recent years, Vision Transformer-based applications to low-level vision tasks have achieved widespread success. Unlike CNN-based models, Transformers are more adept at capturing long-range dependencies, enabling the reconstruction of images utilizing information from non-local areas. In the domain of super-resolution, Swin-transformer-based approaches have become mainstream due to their capacity to capture global spatial information and their shifting-window attention mechanism that facilitates the interchange of information between different windows. Many researchers have enhanced image quality and network efficiency by expanding the receptive field or designing complex networks, yielding commendable results. However, we observed that spatial information tends to diminish during the forward propagation process due to increased depth, leading to a loss of spatial information and, consequently, limiting the model's potential. To addr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MATEval&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#31867;GPT-4&#30340;LLMs&#20316;&#20026;&#35780;&#20272;Agent&#65292;&#27169;&#25311;&#20154;&#31867;&#21512;&#20316;&#35752;&#35770;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#24320;&#25918;&#24615;&#25991;&#26412;&#65292;&#32467;&#21512;&#33258;&#25105;&#21453;&#24605;&#21644;&#24605;&#32500;&#38142;&#31574;&#30053;&#65292;&#24182;&#21152;&#20837;&#21453;&#39304;&#26426;&#21046;&#65292;&#25552;&#21319;&#35780;&#20272;&#28145;&#24230;&#21644;&#24191;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.19305</link><description>&lt;p&gt;
MATEval&#65306;&#29992;&#20110;&#25512;&#36827;&#24320;&#25918;&#24615;&#25991;&#26412;&#35780;&#20272;&#30340;&#22810;Agent&#35752;&#35770;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MATEval: A Multi-Agent Discussion Framework for Advancing Open-Ended Text Evaluation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19305
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MATEval&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#20010;&#31867;GPT-4&#30340;LLMs&#20316;&#20026;&#35780;&#20272;Agent&#65292;&#27169;&#25311;&#20154;&#31867;&#21512;&#20316;&#35752;&#35770;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;&#24320;&#25918;&#24615;&#25991;&#26412;&#65292;&#32467;&#21512;&#33258;&#25105;&#21453;&#24605;&#21644;&#24605;&#32500;&#38142;&#31574;&#30053;&#65292;&#24182;&#21152;&#20837;&#21453;&#39304;&#26426;&#21046;&#65292;&#25552;&#21319;&#35780;&#20272;&#28145;&#24230;&#21644;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#20196;&#20154;&#30633;&#30446;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#32463;&#24120;&#26292;&#38706;&#20986;&#25345;&#32493;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#36136;&#37327;&#65292;&#29305;&#21035;&#26159;&#22312;&#24320;&#25918;&#24615;&#25991;&#26412;&#20013;&#65292;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#20316;&#20026;&#35780;&#20272;&#32773;&#30340;&#21487;&#33021;&#24615;&#12290;&#34429;&#28982;&#20351;&#29992;&#21333;&#20010;LLM&#20316;&#20026;&#35780;&#20272;Agent&#34920;&#29616;&#20986;&#28508;&#21147;&#65292;&#20294;&#21364;&#23384;&#22312;&#26174;&#33879;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MATEval&#65306;&#19968;&#31181;&#8220;&#22810;Agent&#25991;&#26412;&#35780;&#20272;&#26694;&#26550;&#8221;&#65292;&#20854;&#20013;&#25152;&#26377;Agent&#37117;&#30001;&#20687;GPT-4&#30340;LLMs&#25198;&#28436;&#12290;MATEval&#26694;&#26550;&#27169;&#25311;&#20154;&#31867;&#21327;&#20316;&#35752;&#35770;&#26041;&#27861;&#65292;&#25972;&#21512;&#22810;&#20010;Agent&#30340;&#20114;&#21160;&#26469;&#35780;&#20272;&#24320;&#25918;&#24615;&#25991;&#26412;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32467;&#21512;&#20102;&#33258;&#25105;&#21453;&#24605;&#21644;&#8220;&#24605;&#32500;&#38142;&#8221;&#31574;&#30053;&#65292;&#20197;&#21450;&#21453;&#39304;&#26426;&#21046;&#65292;&#22686;&#24378;&#20102;&#35780;&#20272;&#30340;&#28145;&#24230;&#21644;&#24191;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19305v1 Announce Type: cross  Abstract: Recent advancements in generative Large Language Models(LLMs) have been remarkable, however, the quality of the text generated by these models often reveals persistent issues. Evaluating the quality of text generated by these models, especially in open-ended text, has consistently presented a significant challenge. Addressing this, recent work has explored the possibility of using LLMs as evaluators. While using a single LLM as an evaluation agent shows potential, it is filled with significant uncertainty and instability. To address these issues, we propose the MATEval: A "Multi-Agent Text Evaluation framework" where all agents are played by LLMs like GPT-4. The MATEval framework emulates human collaborative discussion methods, integrating multiple agents' interactions to evaluate open-ended text. Our framework incorporates self-reflection and Chain-of-Thought (CoT) strategies, along with feedback mechanisms, enhancing the depth and br
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;16&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#32570;&#20047;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#19982;&#25552;&#31034;&#25216;&#26415;&#19982;&#26356;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20043;&#38388;&#27604;&#36739;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.17661</link><description>&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#20165;&#20165;&#19978;&#19979;&#25991;&#23398;&#20064;&#23601;&#36275;&#22815;&#20102;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Language Models for Text Classification: Is In-Context Learning Enough?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17661
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23545;16&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#35780;&#20272;&#30740;&#31350;&#65292;&#22635;&#34917;&#20102;&#29616;&#26377;&#30740;&#31350;&#32570;&#20047;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#19982;&#25552;&#31034;&#25216;&#26415;&#19982;&#26356;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#20043;&#38388;&#27604;&#36739;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22522;&#30784;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#27425;&#21644;&#23569;&#27425;&#26631;&#35760;&#35774;&#32622;&#20013;&#23637;&#31034;&#20102;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;&#36825;&#20123;&#27169;&#22411;&#30456;&#23545;&#20110;&#22522;&#20110;&#24494;&#35843;&#30340;&#26356;&#26631;&#20934;&#30340;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#33021;&#22815;&#29702;&#35299;&#29992;&#33258;&#28982;&#35821;&#35328;&#32534;&#20889;&#30340;&#25351;&#20196;&#65288;&#25552;&#31034;&#65289;&#65292;&#36825;&#26377;&#21161;&#20110;&#23427;&#20204;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#39046;&#22495;&#65292;&#32780;&#26080;&#38656;&#29305;&#23450;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#36825;&#20351;&#23427;&#20204;&#36866;&#21512;&#35299;&#20915;&#20855;&#26377;&#26377;&#38480;&#26631;&#27880;&#23454;&#20363;&#25968;&#37327;&#30340;&#39046;&#22495;&#30340;&#25991;&#26412;&#20998;&#31867;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30740;&#31350;&#22312;&#35268;&#27169;&#19978;&#26377;&#38480;&#65292;&#24182;&#32570;&#20047;&#23545;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#19982;&#25552;&#31034;&#25216;&#26415;&#30456;&#32467;&#21512;&#19982;&#26356;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#65288;&#22914;&#24494;&#35843;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65289;&#30340;&#27604;&#36739;&#30340;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#28085;&#30422;&#20108;&#20803;&#12289;&#22810;&#31867;&#21644;&#22810;&#26631;&#31614;&#38382;&#39064;&#30340;16&#20010;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#36827;&#34892;&#22823;&#35268;&#27169;&#35780;&#20272;&#30740;&#31350;&#26469;&#22635;&#34917;&#36825;&#19968;&#30740;&#31350;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17661v1 Announce Type: cross  Abstract: Recent foundational language models have shown state-of-the-art performance in many NLP tasks in zero- and few-shot settings. An advantage of these models over more standard approaches based on fine-tuning is the ability to understand instructions written in natural language (prompts), which helps them generalise better to different tasks and domains without the need for specific training data. This makes them suitable for addressing text classification problems for domains with limited amounts of annotated instances. However, existing research is limited in scale and lacks understanding of how text generation models combined with prompting techniques compare to more established methods for text classification such as fine-tuning masked language models. In this paper, we address this research gap by performing a large-scale evaluation study for 16 text classification datasets covering binary, multiclass, and multilabel problems. In par
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14472</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Large Language Models via Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SafeEdit&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#23433;&#20840;&#31867;&#21035;&#65292;&#20855;&#26377;&#21508;&#31181;&#24378;&#22823;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#30693;&#35782;&#32534;&#36753;&#26377;&#28508;&#21147;&#22312;&#23545;LLMs&#36827;&#34892;&#21435;&#27602;&#21270;&#26102;&#65292;&#22312;&#23545;&#19968;&#33324;&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;&#65292;&#31216;&#20026;&#36890;&#36807;&#26415;&#20013;&#31070;&#32463;&#30417;&#27979;&#21435;&#27602;&#21270;&#65288;DINM&#65289;&#65292;&#36890;&#36807;&#20165;&#19968;&#27425;&#23454;&#20363;&#30340;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;LLMs&#30340;&#27602;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;SFT&#21644;DPO&#21487;&#33021;&#20165;&#25233;&#21046;&#26377;&#27602;&#21442;&#25968;&#30340;&#28608;&#27963;&#65292;&#32780;DINM&#21017;&#20943;&#36731;&#26377;&#27602;&#21442;&#25968;&#30340;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#27169;&#25311;&#19981;&#21516;&#35282;&#33394;&#36827;&#34892;&#35752;&#35770;&#65292;&#20197;&#36798;&#25104;&#23545;&#20195;&#30721;&#20013;&#28431;&#27934;&#23384;&#22312;&#21644;&#20998;&#31867;&#30340;&#20849;&#35782;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21021;&#27493;&#35780;&#20272;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#30340;&#26126;&#26174;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.14274</link><description>&lt;p&gt;
&#36890;&#36807;LLMs&#35752;&#35770;&#23454;&#29616;&#28431;&#27934;&#26816;&#27979;&#30340;&#22810;&#35282;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
Multi-role Consensus through LLMs Discussions for Vulnerability Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#27169;&#25311;&#19981;&#21516;&#35282;&#33394;&#36827;&#34892;&#35752;&#35770;&#65292;&#20197;&#36798;&#25104;&#23545;&#20195;&#30721;&#20013;&#28431;&#27934;&#23384;&#22312;&#21644;&#20998;&#31867;&#30340;&#20849;&#35782;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21021;&#27493;&#35780;&#20272;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#30340;&#26126;&#26174;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#31361;&#26174;&#20102;&#28431;&#27934;&#26816;&#27979;&#30340;&#28508;&#21147;&#65292;&#36825;&#26159;&#36719;&#20214;&#36136;&#37327;&#20445;&#35777;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#38480;&#20110;&#21333;&#19968;&#35282;&#33394;&#30340;&#35270;&#35282;&#65292;&#36890;&#24120;&#26159;&#27979;&#35797;&#20154;&#21592;&#65292;&#32570;&#20047;&#20856;&#22411;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#20013;&#19981;&#21516;&#35282;&#33394;&#30340;&#22810;&#20803;&#35266;&#28857;&#65292;&#21253;&#25324;&#24320;&#21457;&#20154;&#21592;&#21644;&#27979;&#35797;&#20154;&#21592;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#25198;&#28436;&#19981;&#21516;&#35282;&#33394;&#30340;&#26041;&#27861;&#65292;&#27169;&#25311;&#29616;&#23454;&#20195;&#30721;&#23457;&#26597;&#36807;&#31243;&#65292;&#36827;&#34892;&#35752;&#35770;&#20197;&#36798;&#25104;&#20851;&#20110;&#20195;&#30721;&#20013;&#28431;&#27934;&#23384;&#22312;&#21644;&#20998;&#31867;&#30340;&#20849;&#35782;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#21021;&#27493;&#35780;&#20272;&#26174;&#31034;&#65292;&#31934;&#30830;&#29575;&#22686;&#21152;&#20102;4.73&#65285;&#65292;&#21484;&#22238;&#29575;&#22686;&#21152;&#20102;58.9&#65285;&#65292;F1&#20998;&#25968;&#22686;&#21152;&#20102;28.1&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14274v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have highlighted the potential for vulnerability detection, a crucial component of software quality assurance. Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers. To this end, this paper introduces an approach to employ LLMs to act as different roles to simulate real-life code review process, engaging in discussions towards a consensus on the existence and classification of vulnerabilities in the code. Preliminary evaluation of the proposed approach indicates a 4.73% increase in the precision rate, 58.9% increase in the recall rate, and a 28.1% increase in the F1 score.
&lt;/p&gt;</description></item><item><title>AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.13269</link><description>&lt;p&gt;
AFLoRA: &#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#22312;&#22823;&#22411;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
AFLoRA: Adaptive Freezing of Low Rank Adaptation in Parameter Efficient Fine-Tuning of Large Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13269
&lt;/p&gt;
&lt;p&gt;
AFLoRA&#26159;&#19968;&#31181;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#36880;&#27493;&#20923;&#32467;&#25237;&#24433;&#30697;&#38453;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#20943;&#23569;&#35745;&#31639;&#37327;&#65292;&#24182;&#25552;&#20379;&#23545;GLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#26368;&#20808;&#36827;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#65292;&#31216;&#20026;&#33258;&#36866;&#24212;&#20923;&#32467;&#20302;&#31209;&#35843;&#25972;&#65288;AFLoRA&#65289;&#12290;&#20855;&#20307;&#22320;&#65292;&#23545;&#20110;&#27599;&#20010;&#39044;&#35757;&#32451;&#30340;&#20923;&#32467;&#26435;&#37325;&#24352;&#37327;&#65292;&#25105;&#20204;&#28155;&#21152;&#19968;&#20010;&#21487;&#35757;&#32451;&#30340;&#20302;&#31209;&#30697;&#38453;&#24182;&#34892;&#36335;&#24452;&#65292;&#21363;&#19979;&#25237;&#24433;&#21644;&#19978;&#25237;&#24433;&#30697;&#38453;&#65292;&#27599;&#20010;&#30697;&#38453;&#21518;&#38754;&#36319;&#30528;&#19968;&#20010;&#29305;&#24449;&#21464;&#25442;&#21521;&#37327;&#12290;&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#20923;&#32467;&#20998;&#25968;&#65292;&#25105;&#20204;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#36880;&#27493;&#20923;&#32467;&#36825;&#20123;&#25237;&#24433;&#30697;&#38453;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#37327;&#24182;&#20943;&#36731;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;GLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#33719;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;&#25913;&#21892;&#39640;&#36798;0.85&#65285;&#65292;&#21516;&#26102;&#21487;&#20943;&#23569;&#39640;&#36798;9.5&#20493;&#30340;&#24179;&#22343;&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;&#22312;&#36816;&#34892;&#26102;&#38388;&#26041;&#38754;&#65292;&#19982;&#31867;&#20284;&#30340;PEFT&#22791;&#36873;&#26041;&#26696;&#30456;&#27604;&#65292;AFLoRA&#21487;&#20197;&#25552;&#20379;&#39640;&#36798;1.86&#20493;&#30340;&#25913;&#36827;&#12290;&#38500;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#38469;&#25928;&#29992;&#20043;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20851;&#20110;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13269v1 Announce Type: new  Abstract: We present a novel Parameter-Efficient Fine-Tuning (PEFT) method, dubbed as Adaptive Freezing of Low Rank Adaptation (AFLoRA). Specifically, for each pre-trained frozen weight tensor, we add a parallel path of trainable low-rank matrices, namely a down-projection and an up-projection matrix, each of which is followed by a feature transformation vector. Based on a novel freezing score, we the incrementally freeze these projection matrices during fine-tuning to reduce the computation and alleviate over-fitting. Our experimental results demonstrate that we can achieve state-of-the-art performance with an average improvement of up to $0.85\%$ as evaluated on GLUE benchmark while yeilding up to $9.5\times$ fewer average trainable parameters. While compared in terms of runtime, AFLoRA can yield up to $1.86\times$ improvement as opposed to similar PEFT alternatives. Besides the practical utility of our approach, we provide insights on the train
&lt;/p&gt;</description></item><item><title>&#25209;&#35780;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#23545;&#38750;&#26126;&#26174;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Adversarial Nibbler Challenge&#20197;&#20247;&#21253;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.12075</link><description>&lt;p&gt;
Adversarial Nibbler: &#19968;&#31181;&#29992;&#20110;&#35782;&#21035;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#22810;&#26679;&#21270;&#21361;&#23475;&#30340;&#24320;&#25918;&#24335;&#32418;&#38431;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adversarial Nibbler: An Open Red-Teaming Method for Identifying Diverse Harms in Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12075
&lt;/p&gt;
&lt;p&gt;
&#25209;&#35780;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#27169;&#22411;&#23545;&#38750;&#26126;&#26174;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#65292;&#25552;&#20986;&#20102;Adversarial Nibbler Challenge&#20197;&#20247;&#21253;&#22810;&#26679;&#21270;&#30340;&#25552;&#31034;&#26469;&#32416;&#27491;&#27169;&#22411;&#30340;&#23433;&#20840;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;AI&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#35780;&#20272;&#27169;&#22411;&#23545;&#20110;&#19981;&#26126;&#26174;&#25915;&#20987;&#30340;&#31283;&#20581;&#24615;&#20197;&#20943;&#23569;&#29983;&#25104;&#20882;&#29359;&#24615;&#22270;&#20687;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36890;&#36807;&#19987;&#27880;&#20110;"&#38544;&#24615;&#23545;&#25239;"&#25552;&#31034;&#65288;&#35302;&#21457;T2I&#27169;&#22411;&#29983;&#25104;&#19981;&#23433;&#20840;&#22270;&#20687;&#30340;&#38750;&#26126;&#26174;&#21407;&#22240;&#65289;&#65292;&#25105;&#20204;&#29420;&#31435;&#36776;&#21035;&#20986;&#19968;&#32452;&#38590;&#20197;&#21457;&#29616;&#30340;&#23433;&#20840;&#38382;&#39064;&#65292;&#20154;&#31867;&#21019;&#36896;&#21147;&#24456;&#36866;&#21512;&#25581;&#31034;&#36825;&#20123;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;Adversarial Nibbler Challenge&#65292;&#36825;&#26159;&#19968;&#20010;&#32418;&#38431;&#26041;&#27861;&#65292;&#29992;&#20110;&#20247;&#21253;&#19968;&#32452;&#22810;&#26679;&#21270;&#30340;&#38544;&#24615;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#25105;&#20204;&#24050;&#27719;&#24635;&#19968;&#22871;&#26368;&#20808;&#36827;&#30340;T2I&#27169;&#22411;&#65292;&#37319;&#29992;&#31616;&#21333;&#29992;&#25143;&#30028;&#38754;&#26469;&#35782;&#21035;&#21644;&#27880;&#37322;&#21361;&#23475;&#65292;&#24182;&#21560;&#24341;&#24191;&#27867;&#20154;&#32676;&#26469;&#25429;&#25417;&#22312;&#26631;&#20934;&#27979;&#35797;&#20013;&#21487;&#33021;&#34987;&#24573;&#35270;&#30340;&#38271;&#23614;&#23433;&#20840;&#38382;&#39064;&#12290;&#25361;&#25112;&#22312;&#36830;&#32493;&#22238;&#21512;&#20013;&#36827;&#34892;&#65292;&#20197;&#23454;&#29616;&#23545;T2I&#27169;&#22411;&#20013;&#23433;&#20840;&#38544;&#24739;&#30340;&#25345;&#32493;&#21457;&#29616;&#21644;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12075v1 Announce Type: cross  Abstract: With the rise of text-to-image (T2I) generative AI models reaching wide audiences, it is critical to evaluate model robustness against non-obvious attacks to mitigate the generation of offensive images. By focusing on ``implicitly adversarial'' prompts (those that trigger T2I models to generate unsafe images for non-obvious reasons), we isolate a set of difficult safety issues that human creativity is well-suited to uncover. To this end, we built the Adversarial Nibbler Challenge, a red-teaming methodology for crowdsourcing a diverse set of implicitly adversarial prompts. We have assembled a suite of state-of-the-art T2I models, employed a simple user interface to identify and annotate harms, and engaged diverse populations to capture long-tail safety issues that may be overlooked in standard testing. The challenge is run in consecutive rounds to enable a sustained discovery and analysis of safety pitfalls in T2I models.   In this pape
&lt;/p&gt;</description></item><item><title>&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;</title><link>https://arxiv.org/abs/2403.11821</link><description>&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#65306;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#30340;&#35843;&#26597;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Evaluating Text to Image Synthesis: Survey and Taxonomy of Image Quality Metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11821
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#20013;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#22270;&#20687;&#36136;&#37327;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#30830;&#20445;&#25991;&#26412;&#21644;&#22270;&#20687;&#20869;&#23481;&#30340;&#23545;&#40784;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#24402;&#32435;&#36825;&#20123;&#25351;&#26631;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#36890;&#36807;&#21033;&#29992;&#35821;&#35328;&#21644;&#35270;&#35273;&#32467;&#21512;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#25512;&#21160;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#21512;&#25104;&#26041;&#38754;&#30340;&#36827;&#23637;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20114;&#32852;&#32593;&#25110;&#20854;&#20182;&#22823;&#35268;&#27169;&#25968;&#25454;&#24211;&#20013;&#30340;&#28023;&#37327;&#25991;&#26412;-&#22270;&#20687;&#23545;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#38543;&#30528;&#23545;&#39640;&#36136;&#37327;&#22270;&#20687;&#29983;&#25104;&#30340;&#38656;&#27714;&#36716;&#21521;&#30830;&#20445;&#25991;&#26412;&#19982;&#22270;&#20687;&#20043;&#38388;&#30340;&#20869;&#23481;&#23545;&#40784;&#65292;&#24050;&#24320;&#21457;&#20102;&#26032;&#39062;&#30340;&#35780;&#20272;&#24230;&#37327;&#26631;&#20934;&#65292;&#26088;&#22312;&#27169;&#25311;&#20154;&#31867;&#21028;&#26029;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#25910;&#38598;&#20855;&#26377;&#36234;&#26469;&#36234;&#22797;&#26434;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#30740;&#31350;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#32452;&#25104;&#24615;&#21450;&#20854;&#20316;&#20026;&#25991;&#26412;&#19982;&#22270;&#20687;&#20869;&#23481;&#32452;&#25104;&#23545;&#40784;&#36136;&#37327;&#24230;&#37327;&#30340;&#20854;&#32435;&#20837;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#35780;&#20272;&#25351;&#26631;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#20998;&#31867;&#27861;&#26469;&#23545;&#36825;&#20123;&#25351;&#26631;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#23457;&#26597;&#20102;&#32463;&#24120;&#37319;&#29992;&#30340;&#25991;&#26412;-&#22270;&#20687;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11821v1 Announce Type: cross  Abstract: Recent advances in text-to-image synthesis have been enabled by exploiting a combination of language and vision through foundation models. These models are pre-trained on tremendous amounts of text-image pairs sourced from the World Wide Web or other large-scale databases. As the demand for high-quality image generation shifts towards ensuring content alignment between text and image, novel evaluation metrics have been developed with the aim of mimicking human judgments. Thus, researchers have started to collect datasets with increasingly complex annotations to study the compositionality of vision-language models and their incorporation as a quality measure of compositional alignment between text and image contents. In this work, we provide a comprehensive overview of existing text-to-image evaluation metrics and propose a new taxonomy for categorizing these metrics. We also review frequently adopted text-image benchmark datasets befor
&lt;/p&gt;</description></item><item><title>DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.10903</link><description>&lt;p&gt;
DTOR&#65306;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#29992;&#20110;&#35299;&#37322;&#24322;&#24120;
&lt;/p&gt;
&lt;p&gt;
DTOR: Decision Tree Outlier Regressor to explain anomalies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10903
&lt;/p&gt;
&lt;p&gt;
DTOR&#26159;&#19968;&#31181;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65292;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20135;&#29983;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#65292;&#36866;&#29992;&#20110;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#24322;&#24120;&#20540;&#30340;&#20986;&#29616;&#20197;&#21450;&#20854;&#20135;&#29983;&#26426;&#21046;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#21487;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#25925;&#38556;&#12289;&#27450;&#35784;&#12289;&#23041;&#32961;&#31561;&#38382;&#39064;&#65292;&#38500;&#20102;&#34987;&#27491;&#30830;&#35782;&#21035;&#20043;&#22806;&#65292;&#36890;&#24120;&#38656;&#35201;&#26377;&#25928;&#30340;&#35299;&#37322;&#20197;&#26377;&#25928;&#25191;&#34892;&#21487;&#25805;&#20316;&#30340;&#23545;&#25239;&#25514;&#26045;&#12290;&#36234;&#26469;&#36234;&#24191;&#27867;&#22320;&#20351;&#29992;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#35782;&#21035;&#24322;&#24120;&#20540;&#65292;&#20351;&#24471;&#36825;&#26679;&#30340;&#35299;&#37322;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20915;&#31574;&#26641;&#24322;&#24120;&#20540;&#22238;&#24402;&#22120;&#65288;DTOR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#20272;&#35745;&#24322;&#24120;&#26816;&#27979;&#27169;&#22411;&#29983;&#25104;&#30340;&#24322;&#24120;&#20998;&#25968;&#26469;&#20026;&#21333;&#20010;&#25968;&#25454;&#28857;&#29983;&#25104;&#22522;&#20110;&#35268;&#21017;&#30340;&#35299;&#37322;&#30340;&#25216;&#26415;&#12290;&#36825;&#26159;&#36890;&#36807;&#39318;&#20808;&#24212;&#29992;&#20915;&#31574;&#26641;&#22238;&#24402;&#22120;&#26469;&#35745;&#31639;&#20272;&#35745;&#20998;&#25968;&#65292;&#28982;&#21518;&#25552;&#21462;&#19982;&#25968;&#25454;&#28857;&#20998;&#25968;&#30456;&#20851;&#32852;&#30340;&#30456;&#23545;&#36335;&#24452;&#26469;&#23454;&#29616;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#22312;&#20855;&#26377;&#22823;&#37327;&#29305;&#24449;&#30340;&#25968;&#25454;&#38598;&#20013;&#65292;DTOR&#30340;&#40065;&#26834;&#24615;&#20063;&#24471;&#21040;&#20102;&#35777;&#23454;&#12290;&#27492;&#22806;&#65292;&#19982;&#20854;&#20182;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#30456;&#27604;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10903v1 Announce Type: cross  Abstract: Explaining outliers occurrence and mechanism of their occurrence can be extremely important in a variety of domains. Malfunctions, frauds, threats, in addition to being correctly identified, oftentimes need a valid explanation in order to effectively perform actionable counteracts. The ever more widespread use of sophisticated Machine Learning approach to identify anomalies make such explanations more challenging. We present the Decision Tree Outlier Regressor (DTOR), a technique for producing rule-based explanations for individual data points by estimating anomaly scores generated by an anomaly detection model. This is accomplished by first applying a Decision Tree Regressor, which computes the estimation score, and then extracting the relative path associated with the data point score. Our results demonstrate the robustness of DTOR even in datasets with a large number of features. Additionally, in contrast to other rule-based approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;LIGHTCODE&#36731;&#37327;&#32423;&#31070;&#32463;&#32534;&#30721;&#26041;&#26696;&#65292;&#22312;&#20855;&#22791;&#35299;&#37322;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#22312;&#20302;&#20449;&#22122;&#27604;&#21306;&#22495;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.10751</link><description>&lt;p&gt;
LIGHTCODE&#65306;&#20855;&#26377;&#21453;&#39304;&#36890;&#36947;&#30340;&#20809;&#35299;&#26512;&#21644;&#31070;&#32463;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
LIGHTCODE: Light Analytical and Neural Codes for Channels with Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;LIGHTCODE&#36731;&#37327;&#32423;&#31070;&#32463;&#32534;&#30721;&#26041;&#26696;&#65292;&#22312;&#20855;&#22791;&#35299;&#37322;&#24615;&#30340;&#22522;&#30784;&#19978;&#65292;&#22312;&#20302;&#20449;&#22122;&#27604;&#21306;&#22495;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36947;&#21453;&#39304;&#20013;&#21487;&#38752;&#19988;&#39640;&#25928;&#30340;&#32534;&#30721;&#26041;&#26696;&#35774;&#35745;&#19968;&#30452;&#26159;&#36890;&#20449;&#29702;&#35770;&#20013;&#19968;&#39033;&#38271;&#26399;&#25361;&#25112;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#31070;&#32463;&#32534;&#30721;&#24448;&#24448;&#38754;&#20020;&#35745;&#31639;&#25104;&#26412;&#39640;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#20197;&#21450;&#22312;&#36164;&#28304;&#21463;&#38480;&#29615;&#22659;&#20013;&#30340;&#23454;&#29992;&#24615;&#26377;&#38480;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#35774;&#35745;&#35299;&#37322;&#24615;&#24378;&#19988;&#26356;&#36866;&#29992;&#20110;&#36890;&#20449;&#31995;&#32479;&#30340;&#20302;&#22797;&#26434;&#24230;&#32534;&#30721;&#26041;&#26696;&#12290;&#25105;&#20204;&#20808;&#36827;&#20102;&#35299;&#26512;&#32534;&#30721;&#21644;&#31070;&#32463;&#32534;&#30721;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;POWERBLAST&#65292;&#19968;&#31181;&#21463;Schalkwijk-Kailath&#65288;SK&#65289;&#21644;Gallager-Nakiboglu&#65288;GN&#65289;&#26041;&#26696;&#21551;&#21457;&#30340;&#35299;&#26512;&#32534;&#30721;&#26041;&#26696;&#65292;&#22312;&#39640;&#20449;&#22122;&#27604;&#65288;SNR&#65289;&#21306;&#22495;&#23454;&#29616;&#20102;&#26126;&#26174;&#30340;&#21487;&#38752;&#24615;&#25913;&#36827;&#65292;&#32988;&#36807;&#31070;&#32463;&#32534;&#30721;&#12290;&#25509;&#19979;&#26469;&#65292;&#20026;&#20102;&#22686;&#24378;&#20302;SNR&#21306;&#22495;&#30340;&#21487;&#38752;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LIGHTCODE&#65292;&#19968;&#31181;&#36731;&#37327;&#32423;&#31070;&#32463;&#32534;&#30721;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10751v1 Announce Type: cross  Abstract: The design of reliable and efficient codes for channels with feedback remains a longstanding challenge in communication theory. While significant improvements have been achieved by leveraging deep learning techniques, neural codes often suffer from high computational costs, a lack of interpretability, and limited practicality in resource-constrained settings. We focus on designing low-complexity coding schemes that are interpretable and more suitable for communication systems. We advance both analytical and neural codes. First, we demonstrate that POWERBLAST, an analytical coding scheme inspired by Schalkwijk-Kailath (SK) and Gallager-Nakiboglu (GN) schemes, achieves notable reliability improvements over both SK and GN schemes, outperforming neural codes in high signal-to-noise ratio (SNR) regions. Next, to enhance reliability in low-SNR regions, we propose LIGHTCODE, a lightweight neural code that achieves state-of-the-art reliability
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09863</link><description>&lt;p&gt;
&#19968;&#20010;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Conceptual Framework For White Box Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09863
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#23436;&#20840;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#12290;&#19968;&#20010;&#20805;&#20998;&#21160;&#26426;&#30340;MNIST&#30456;&#20851;&#23376;&#38382;&#39064;&#30340;&#27010;&#24565;&#39564;&#35777;&#27169;&#22411;&#21253;&#25324;4&#20010;&#36825;&#26679;&#30340;&#23618;&#65292;&#24635;&#20849;4800&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#26131;&#20110;&#35299;&#37322;&#65292;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#23545;&#25239;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23545;&#25239;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21333;&#20010;CPU&#19978;&#24555;&#36895;&#35757;&#32451;&#12290;&#35813;&#25216;&#26415;&#30340;&#36890;&#29992;&#24615;&#25215;&#35834;&#20026;&#24443;&#24213;&#27665;&#20027;&#21270;&#21644;&#30495;&#27491;&#36890;&#29992;&#30340;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/314-Foundation/white-box-nn&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09863v1 Announce Type: cross  Abstract: This paper introduces semantic features as a general conceptual framework for fully explainable neural network layers. A well-motivated proof of concept model for relevant subproblem of MNIST consists of 4 such layers with the total of 4.8K learnable parameters. The model is easily interpretable, achieves human-level adversarial test accuracy with no form of adversarial training, requires little hyperparameter tuning and can be quickly trained on a single CPU. The general nature of the technique bears promise for a paradigm shift towards radically democratised and truly generalizable white box neural networks. The code is available at https://github.com/314-Foundation/white-box-nn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26126;&#31034;&#24847;&#35782;&#26657;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09488</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#32416;&#27491;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Rectifying Demonstration Shortcut in In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26126;&#31034;&#24847;&#35782;&#26657;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#20165;&#20973;&#23569;&#37327;&#28436;&#31034;&#20415;&#33021;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;LLMs&#24120;&#24120;&#20381;&#36182;&#20110;&#23427;&#20204;&#23545;&#28436;&#31034;&#30340;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#20041;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#26681;&#25454;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#32487;&#32493;&#36827;&#34892;ICL&#39044;&#27979;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#29616;&#35937;&#31216;&#20026;&#8220;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#8221;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#25913;&#36827;&#39044;&#23450;&#20041;&#20219;&#21153;&#30340;ICL&#39044;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#32416;&#27491;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#20174;&#32780;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#30340;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26126;&#31034;&#24847;&#35782;&#30340;&#26657;&#20934;&#26041;&#27861;&#65306;In-Context Calibration&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#65288;1&#65289;&#20351;&#29992;&#26631;&#20934;&#26631;&#31614;&#31354;&#38388;&#30340;&#21407;&#22987;ICL&#20219;&#21153;&#20197;&#21450;&#65288;2&#65289;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#26631;&#31614;&#31354;&#38388;&#34987;&#35821;&#20041;&#26080;&#20851;&#30340;&#26631;&#35760;&#26367;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09488v1 Announce Type: cross  Abstract: Large language models (LLMs) are able to solve various tasks with only a few demonstrations utilizing their in-context learning (ICL) abilities. However, LLMs often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with ICL prediction. In this work, we term this phenomenon as the `Demonstration Shortcut'. While previous works have primarily focused on improving ICL prediction results for predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM to effectively learn new input-label relationships from demonstrations. To achieve this, we introduce In-Context Calibration, a demonstration-aware calibration method. We evaluate the effectiveness of the proposed method in two settings: (1) the Original ICL Task using the standard label space and (2) the Task Learning setting, where the label space is replaced with semantically unrelated tokens. In 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#37325;&#26032;&#24605;&#32771;ASTE&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#32039;&#20945;&#30340;&#35774;&#35745;&#21644;&#38477;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#23588;&#20854;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#20248;&#36234;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07342</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;ASTE:&#19968;&#31181;&#26497;&#31616;&#30340;&#26631;&#35760;&#26041;&#26696;&#19982;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking ASTE: A Minimalist Tagging Scheme Alongside Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07342
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#37325;&#26032;&#24605;&#32771;ASTE&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#32039;&#20945;&#30340;&#35774;&#35745;&#21644;&#38477;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#23588;&#20854;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#20248;&#36234;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Aspect Sentiment Triplet Extraction (ASTE) &#26159;&#32454;&#31890;&#24230;&#24773;&#24863;&#20998;&#26512;&#30340;&#19968;&#20010;&#26032;&#20852;&#23376;&#20219;&#21153;&#65292;&#26088;&#22312;&#20174;&#38750;&#32467;&#26500;&#21270;&#25991;&#26412;&#25968;&#25454;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#30340;&#24773;&#24863;&#19977;&#20803;&#32452;&#12290;&#29616;&#26377;&#30340;ASTE&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#39069;&#22806;&#30340;&#32467;&#26500;&#25110;&#22806;&#37096;&#25968;&#25454;&#26469;&#22797;&#26434;&#21270;&#20219;&#21153;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26631;&#35760;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#32531;&#35299;&#36825;&#20123;&#25361;&#25112;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19982;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#27604;&#36739;&#20013;&#23637;&#31034;&#20986;&#21487;&#27604;&#29978;&#33267;&#26356;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20855;&#26377;&#26356;&#32039;&#20945;&#30340;&#35774;&#35745;&#21644;&#38477;&#20302;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26102;&#20195;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#24773;&#26223;&#19979;&#23637;&#29616;&#20986;&#20248;&#20110;GPT 3.5&#21644;GPT 4&#30340;&#25928;&#26524;&#12290;&#26412;&#30740;&#31350;&#36824;&#20026;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#33539;&#24335;&#19979;&#25512;&#36827;ASTE&#25216;&#26415;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07342v1 Announce Type: cross  Abstract: Aspect Sentiment Triplet Extraction (ASTE) is a burgeoning subtask of fine-grained sentiment analysis, aiming to extract structured sentiment triplets from unstructured textual data. Existing approaches to ASTE often complicate the task with additional structures or external data. In this research, we propose a novel tagging scheme and employ a contrastive learning approach to mitigate these challenges. The proposed approach demonstrates comparable or superior performance in comparison to state-of-the-art techniques, while featuring a more compact design and reduced computational overhead. Notably, even in the era of Large Language Models (LLMs), our method exhibits superior efficacy compared to GPT 3.5 and GPT 4 in a few-shot learning scenarios. This study also provides valuable insights for the advancement of ASTE techniques within the paradigm of large language models.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#20154;&#31867;&#20013;&#24515;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#36866;&#24403;&#31867;&#22411;&#30340;&#20915;&#31574;&#25903;&#25345;&#65292;&#38024;&#23545;&#29305;&#23450;&#20154;&#21592;&#12289;&#22312;&#36866;&#24403;&#26102;&#38388;&#65292;&#26469;&#20248;&#21270;&#20915;&#31574;&#20934;&#30830;&#24615;&#21644;&#20154;&#31867;&#23398;&#20064;&#33021;&#21147;&#36825;&#20004;&#20010;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.05911</link><description>&lt;p&gt;
&#20248;&#21270;&#20154;&#31867;&#20013;&#24515;&#30446;&#26631;&#65306;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Optimizing Human-Centric Objectives in AI-Assisted Decision-Making With Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05911
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#20248;&#21270;&#20154;&#31867;&#20013;&#24515;&#30446;&#26631;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25552;&#20379;&#36866;&#24403;&#31867;&#22411;&#30340;&#20915;&#31574;&#25903;&#25345;&#65292;&#38024;&#23545;&#29305;&#23450;&#20154;&#21592;&#12289;&#22312;&#36866;&#24403;&#26102;&#38388;&#65292;&#26469;&#20248;&#21270;&#20915;&#31574;&#20934;&#30830;&#24615;&#21644;&#20154;&#31867;&#23398;&#20064;&#33021;&#21147;&#36825;&#20004;&#20010;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#28183;&#36879;&#21040;&#20915;&#31574;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21487;&#33021;&#24076;&#26395;&#20248;&#21270;&#20154;&#31867;&#20013;&#24515;&#30446;&#26631;&#65292;&#36229;&#36234;&#20915;&#31574;&#20934;&#30830;&#24615;&#65292;&#22914;&#37027;&#20123;&#19982;&#36825;&#20123;&#31995;&#32479;&#20114;&#21160;&#30340;&#20010;&#20307;&#30340;&#25216;&#33021;&#25552;&#21319;&#25110;&#20219;&#21153;&#20139;&#21463;&#12290;&#32771;&#34385;&#21040;&#36825;&#19968;&#24895;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20316;&#20026;&#24314;&#27169;&#20154;&#26426;&#20915;&#31574;&#20197;&#20248;&#21270;&#36825;&#20123;&#20154;&#31867;&#20013;&#24515;&#30446;&#26631;&#30340;&#19968;&#33324;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#28789;&#27963;&#22320;&#20026;&#20154;&#31867;&#25552;&#20379;&#20915;&#31574;&#25903;&#25345;&#26469;&#20248;&#21270;&#19981;&#21516;&#30340;&#30446;&#26631;--&#22312;&#27491;&#30830;&#30340;&#26102;&#38388;&#12289;&#21521;&#27491;&#30830;&#30340;&#20154;&#25552;&#20379;&#27491;&#30830;&#31867;&#22411;&#30340;&#24110;&#21161;&#12290;&#25105;&#20204;&#29992;&#20004;&#20010;&#30446;&#26631;&#23454;&#20363;&#21270;&#25105;&#20204;&#30340;&#26041;&#27861;&#65306;&#20154;&#24037;&#26234;&#33021;&#22312;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#20934;&#30830;&#24615;&#21644;&#20154;&#31867;&#23545;&#35813;&#20219;&#21153;&#30340;&#23398;&#20064;&#65292;&#24182;&#20174;&#20808;&#21069;&#30340;&#20154;&#26426;&#20132;&#20114;&#25968;&#25454;&#20013;&#23398;&#20064;&#20248;&#21270;&#36825;&#20004;&#20010;&#30446;&#26631;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#23558;&#20248;&#21270;&#30340;&#31574;&#30053;&#19982;&#22312;AI&#36741;&#21161;&#20915;&#31574;&#20013;&#30340;&#21508;&#31181;&#22522;&#32447;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#20004;&#20010;&#23454;&#39564;&#20013;&#65288;N = 316 &#21644; N
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05911v1 Announce Type: cross  Abstract: As AI assistance is increasingly infused into decision-making processes, we may seek to optimize human-centric objectives beyond decision accuracy, such as skill improvement or task enjoyment of individuals interacting with these systems. With this aspiration in mind, we propose offline reinforcement learning (RL) as a general approach for modeling human-AI decision-making to optimize such human-centric objectives. Our approach seeks to optimize different objectives by adaptively providing decision support to humans -- the right type of assistance, to the right person, at the right time. We instantiate our approach with two objectives: human-AI accuracy on the decision-making task and human learning about the task, and learn policies that optimize these two objectives from previous human-AI interaction data. We compare the optimized policies against various baselines in AI-assisted decision-making. Across two experiments (N = 316 and N
&lt;/p&gt;</description></item><item><title>PEEB&#26159;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23558;&#31867;&#21035;&#21517;&#31216;&#36716;&#25442;&#20026;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#25991;&#26412;&#25551;&#36848;&#31526;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36824;&#39318;&#27425;&#23454;&#29616;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#24418;&#25104;&#26032;&#20998;&#31867;&#22120;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.05297</link><description>&lt;p&gt;
PEEB&#65306;&#20855;&#26377;&#21487;&#35299;&#37322;&#21644;&#21487;&#32534;&#36753;&#35821;&#35328;&#29942;&#39048;&#30340;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
PEEB: Part-based Image Classifiers with an Explainable and Editable Language Bottleneck
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05297
&lt;/p&gt;
&lt;p&gt;
PEEB&#26159;&#19968;&#31181;&#22522;&#20110;&#37096;&#20998;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#23558;&#31867;&#21035;&#21517;&#31216;&#36716;&#25442;&#20026;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65292;&#24182;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#25991;&#26412;&#25551;&#36848;&#31526;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#32780;&#19988;&#36824;&#39318;&#27425;&#23454;&#29616;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#24418;&#25104;&#26032;&#20998;&#31867;&#22120;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;CLIP&#30340;&#20998;&#31867;&#22120;&#20381;&#36182;&#20110;&#21253;&#21547;{text encoder&#24050;&#30693;&#30340;&#31867;&#21517;&#31216;}&#30340;&#25552;&#31034;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;CLIP&#22312;&#26032;&#31867;&#21035;&#25110;&#20854;&#21517;&#31216;&#24456;&#23569;&#22312;&#20114;&#32852;&#32593;&#19978;&#20986;&#29616;&#30340;&#31867;&#21035;&#65288;&#20363;&#22914;&#40479;&#31867;&#30340;&#23398;&#21517;&#65289;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#38024;&#23545;&#32454;&#31890;&#24230;&#20998;&#31867;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PEEB - &#19968;&#31181;&#21487;&#35299;&#37322;&#21644;&#21487;&#32534;&#36753;&#30340;&#20998;&#31867;&#22120;&#65292;&#29992;&#20110;&#65288;1&#65289;&#23558;&#31867;&#21035;&#21517;&#31216;&#34920;&#36798;&#20026;&#19968;&#32452;&#39044;&#23450;&#20041;&#30340;&#25551;&#36848;&#35270;&#35273;&#37096;&#20998;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#65307;&#21644;&#65288;2&#65289;&#23558;&#26816;&#27979;&#21040;&#30340;&#37096;&#20998;&#30340;&#23884;&#20837;&#19982;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#25991;&#26412;&#25551;&#36848;&#31526;&#36827;&#34892;&#21305;&#37197;&#65292;&#20197;&#35745;&#31639;&#29992;&#20110;&#20998;&#31867;&#30340;&#36923;&#36753;&#20998;&#25968;&#12290;&#22312;&#19968;&#20010;&#38646;&#26679;&#26412;&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#31867;&#21035;&#21517;&#31216;&#26159;&#26410;&#30693;&#30340;&#65292;PEEB&#22312;&#20934;&#30830;&#24615;&#19978;&#22823;&#24133;&#20248;&#20110;CLIP&#65288;&#32422;&#20026;10&#20493;&#65289;&#12290;&#19982;&#22522;&#20110;&#37096;&#20998;&#30340;&#20998;&#31867;&#22120;&#30456;&#27604;&#65292;PEEB&#19981;&#20165;&#22312;&#30417;&#30563;&#23398;&#20064;&#35774;&#32622;&#19978;&#26159;&#26368;&#20808;&#36827;&#30340;&#65288;88.80%&#20934;&#30830;&#29575;&#65289;&#65292;&#32780;&#19988;&#36824;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#35753;&#29992;&#25143;&#32534;&#36753;&#31867;&#23450;&#20041;&#20197;&#24418;&#25104;&#26032;&#30340;&#20998;&#31867;&#22120;&#32780;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05297v1 Announce Type: cross  Abstract: CLIP-based classifiers rely on the prompt containing a {class name} that is known to the text encoder. That is, CLIP performs poorly on new classes or the classes whose names rarely appear on the Internet (e.g., scientific names of birds). For fine-grained classification, we propose PEEB - an explainable and editable classifier to (1) express the class name into a set of pre-defined text descriptors that describe the visual parts of that class; and (2) match the embeddings of the detected parts to their textual descriptors in each class to compute a logit score for classification. In a zero-shot setting where the class names are unknown, PEEB outperforms CLIP by a large margin (~10x in accuracy). Compared to part-based classifiers, PEEB is not only the state-of-the-art on the supervised-learning setting (88.80% accuracy) but also the first to enable users to edit the class definitions to form a new classifier without retraining. Compar
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21475;&#35821;&#35821;&#35328;&#22810;&#30446;&#26631;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#23454;&#20307;&#27133;&#21644;&#23376;&#30446;&#26631;&#25351;&#20196;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.04481</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#29702;&#35299;&#22810;&#30446;&#26631;&#21475;&#35821;&#35821;&#35328;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Large Language Model Understand Multi-Intent Spoken Language ?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04481
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21475;&#35821;&#35821;&#35328;&#22810;&#30446;&#26631;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#25913;&#36827;&#23454;&#20307;&#27133;&#21644;&#23376;&#30446;&#26631;&#25351;&#20196;&#30340;&#21019;&#26032;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#22810;&#30446;&#26631;&#21475;&#35821;&#35821;&#35328;&#29702;&#35299;&#65288;SLU&#65289;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;SLU&#29615;&#22659;&#20013;&#21033;&#29992;LLMs&#29983;&#25104;&#33021;&#21147;&#30340;&#29420;&#29305;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#25216;&#26415;&#37325;&#26032;&#37197;&#32622;&#20102;&#23454;&#20307;&#27133;&#65292;&#19987;&#38376;&#29992;&#20110;LLMs&#22312;&#22810;&#30446;&#26631;SLU&#29615;&#22659;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#24341;&#20837;&#20102;&#23376;&#30446;&#26631;&#25351;&#20196;&#65288;SII&#65289;&#30340;&#27010;&#24565;&#65292;&#22686;&#24378;&#20102;&#23545;&#19981;&#21516;&#39046;&#22495;&#20869;&#22797;&#26434;&#22810;&#30446;&#26631;&#20132;&#27969;&#30340;&#35299;&#21078;&#21644;&#35299;&#37322;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#25968;&#25454;&#38598;&#65292;&#34987;&#31216;&#20026;LM-MixATIS&#21644;LM-MixSNIPS&#65292;&#26159;&#20174;&#29616;&#26377;&#22522;&#20934;&#20013;&#31934;&#24515;&#21046;&#20316;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#21305;&#37197;&#24182;&#28508;&#22312;&#22320;&#36229;&#36234;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22810;&#30446;&#26631;SLU&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#23427;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#22312;&#21508;&#31181;&#24847;&#22270;&#37197;&#32622;&#21644;&#25968;&#25454;&#38598;&#27604;&#20363;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#24320;&#21019;&#24615;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21363;&#23454;&#20307;&#27133;&#20934;&#30830;&#24615;&#65288;ESA&#65289;&#21644;Com
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04481v1 Announce Type: cross  Abstract: This study marks a significant advancement by harnessing Large Language Models (LLMs) for multi-intent spoken language understanding (SLU), proposing a unique methodology that capitalizes on the generative power of LLMs within an SLU context. Our innovative technique reconfigures entity slots specifically for LLM application in multi-intent SLU environments and introduces the concept of Sub-Intent Instruction (SII), enhancing the dissection and interpretation of intricate, multi-intent communication within varied domains. The resultant datasets, dubbed LM-MixATIS and LM-MixSNIPS, are crafted from pre-existing benchmarks. Our research illustrates that LLMs can match and potentially excel beyond the capabilities of current state-of-the-art multi-intent SLU models. It further explores LLM efficacy across various intent configurations and dataset proportions. Moreover, we introduce two pioneering metrics, Entity Slot Accuracy (ESA) and Com
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.03894</link><description>&lt;p&gt;
IRCoder: &#20013;&#38388;&#34920;&#31034;&#20351;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#31283;&#20581;&#30340;&#22810;&#35821;&#35328;&#20195;&#30721;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
IRCoder: Intermediate Representations Make Language Models Robust Multilingual Code Generators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03894
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#21644;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03894v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#24050;&#36805;&#36895;&#25104;&#20026;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26368;&#21463;&#27426;&#36814;&#30340;&#24212;&#29992;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#19982;&#33258;&#28982;&#35821;&#35328;LM&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#23545;&#20195;&#30721;-LMs&#65288;&#21363;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#30340;LMs&#65289;&#30340;&#22810;&#35821;&#35328;&#26041;&#38754;&#30340;&#30740;&#31350;&#65292;&#22914;&#19981;&#21516;&#32534;&#31243;&#35821;&#35328;&#20043;&#38388;&#30340;&#36328;&#35821;&#35328;&#36716;&#31227;&#65292;&#29305;&#23450;&#20110;&#35821;&#35328;&#30340;&#25968;&#25454;&#22686;&#24378;&#20197;&#21450;&#20107;&#21518;LM&#35843;&#25972;&#65292;&#20197;&#21450;&#21033;&#29992;&#21407;&#22987;&#25991;&#26412;&#20869;&#23481;&#20043;&#22806;&#30340;&#25968;&#25454;&#28304;&#65292;&#35201;&#31232;&#23569;&#24471;&#22810;&#12290;&#29305;&#21035;&#26159;&#65292;&#22823;&#22810;&#25968;&#20027;&#27969;&#20195;&#30721;-LMs&#20165;&#22312;&#28304;&#20195;&#30721;&#25991;&#20214;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#29616;&#25104;&#30340;&#32534;&#35793;&#22120;&#20013;&#38388;&#34920;&#31034;&#65288;&#36328;&#32534;&#31243;&#35821;&#35328;&#20849;&#20139;&#65289;&#26469;&#25913;&#36827;&#20195;&#30721;-LMs&#30340;&#22810;&#35821;&#35328;&#33021;&#21147;&#24182;&#20419;&#36827;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#21069;&#26223;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#32534;&#21046;&#20102;SLTrans&#65292;&#19968;&#20010;&#30001;&#36817;400&#19975;&#20010;&#33258;&#21253;&#21547;&#28304;&#20195;&#30721;&#25991;&#20214;&#32452;&#25104;&#30340;&#24182;&#34892;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03894v1 Announce Type: new  Abstract: Code understanding and generation have fast become some of the most popular applications of language models (LMs). Nonetheless, research on multilingual aspects of Code-LMs (i.e., LMs for code generation) such as cross-lingual transfer between different programming languages, language-specific data augmentation, and post-hoc LM adaptation, alongside exploitation of data sources other than the original textual content, has been much sparser than for their natural language counterparts. In particular, most mainstream Code-LMs have been pre-trained on source code files alone. In this work, we investigate the prospect of leveraging readily available compiler intermediate representations - shared across programming languages - to improve the multilingual capabilities of Code-LMs and facilitate cross-lingual transfer.   To this end, we first compile SLTrans, a parallel dataset consisting of nearly 4M self-contained source code files coupled wi
&lt;/p&gt;</description></item><item><title>&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.02181</link><description>&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#19981;&#26159;&#25152;&#26377;LLMs&#30340;&#23618;&#37117;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Not all Layers of LLMs are Necessary during Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02181
&lt;/p&gt;
&lt;p&gt;
&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#30340;&#19981;&#21516;&#38590;&#26131;&#31243;&#24230;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;AdaInfer&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#27973;&#23618;&#21644;&#28145;&#23618;&#65292;&#20174;&#32780;&#33410;&#30465;&#20102;&#35745;&#31639;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#38454;&#27573;&#38750;&#24120;&#26114;&#36149;&#12290;&#29702;&#24819;&#30340;LLMs&#25512;&#29702;&#38454;&#27573;&#21487;&#20197;&#21033;&#29992;&#26356;&#23569;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#21516;&#26102;&#20173;&#20445;&#25345;&#20854;&#33021;&#21147;&#65288;&#20363;&#22914;&#27867;&#21270;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#33021;&#21147;&#65289;&#12290;&#26412;&#25991;&#23581;&#35797;&#22238;&#31572;&#19968;&#20010;&#38382;&#39064;&#65306;&#8220;&#22312;LLMs&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21487;&#20197;&#20026;&#31616;&#21333;&#23454;&#20363;&#20351;&#29992;&#27973;&#23618;&#65292;&#24182;&#20026;&#38590;&#20197;&#22788;&#29702;&#30340;&#23454;&#20363;&#20351;&#29992;&#28145;&#23618;&#21527;&#65311;&#8221;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#32479;&#35745;&#20998;&#26512;&#36328;&#20219;&#21153;&#28608;&#27963;&#30340;&#23618;&#26469;&#25351;&#20986;&#24182;&#38750;&#25152;&#26377;&#23618;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#37117;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31639;&#27861;&#65292;&#21517;&#20026;AdaInfer&#65292;&#26681;&#25454;&#36755;&#20837;&#23454;&#20363;&#33258;&#36866;&#24212;&#22320;&#30830;&#23450;&#25512;&#29702;&#32456;&#27490;&#26102;&#21051;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;AdaInfer&#19981;&#25913;&#21464;LLMs&#21442;&#25968;&#65292;&#24182;&#22312;&#20219;&#21153;&#20043;&#38388;&#20445;&#25345;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#30693;&#21517;LLMs&#65288;&#21363;Llama2&#31995;&#21015;&#21644;OPT&#65289;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;AdaInfer&#33410;&#30465;&#20102;&#24179;&#22343;14.8%&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#29978;&#33267;&#22312;&#24773;&#24863;&#26041;&#38754;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02181v1 Announce Type: cross  Abstract: The inference phase of Large Language Models (LLMs) is very expensive. An ideal inference stage of LLMs could utilize fewer computational resources while still maintaining its capabilities (e.g., generalization and in-context learning ability). In this paper, we try to answer the question, "During LLM inference, can we use shallow layers for easy instances; and deep layers for hard ones?" To answer this question, we first indicate that Not all Layers are Necessary during Inference by statistically analyzing the activated layers across tasks. Then, we propose a simple algorithm named AdaInfer to determine the inference termination moment based on the input instance adaptively. More importantly, AdaInfer does not alter LLM parameters and maintains generalizability across tasks. Experiments on well-known LLMs (i.e., Llama2 series and OPT) show that AdaInfer saves an average of 14.8% of computational resources, even up to 50% on sentiment 
&lt;/p&gt;</description></item><item><title>Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2402.14320</link><description>&lt;p&gt;
Triad: &#19968;&#20010;&#21033;&#29992;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#30340;&#26694;&#26550;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Triad: A Framework Leveraging a Multi-Role LLM-based Agent to Solve Knowledge Base Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14320
&lt;/p&gt;
&lt;p&gt;
Triad&#26694;&#26550;&#21033;&#29992;&#20102;&#22522;&#20110;&#22810;&#35282;&#33394;LLM&#20195;&#29702;&#26469;&#35299;&#20915;&#30693;&#35782;&#24211;&#38382;&#31572;&#38382;&#39064;&#65292;&#36890;&#36807;&#20195;&#29702;&#30340;&#19981;&#21516;&#35282;&#33394;&#20998;&#21035;&#22788;&#29702;KBQA&#23376;&#20219;&#21153;&#65292;&#21512;&#20316;&#23436;&#25104;KBQA&#20219;&#21153;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22522;&#20110;LLM&#20195;&#29702;&#30340;&#36827;&#23637;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#22238;&#31572;&#30693;&#35782;&#24211;&#20013;&#38382;&#39064;&#30340;&#36816;&#29992;&#20173;&#28982;&#40092;&#20026;&#20154;&#30693;&#12290;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#26469;&#23454;&#29616;KBQA&#31995;&#32479;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#32570;&#20047;&#29305;&#23450;&#20219;&#21153;&#35757;&#32451;&#25968;&#25454;&#20197;&#21450;&#21019;&#24314;&#20197;&#20219;&#21153;&#20026;&#20013;&#24515;&#30340;&#27169;&#22411;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Triad&#65292;&#19968;&#20010;&#21033;&#29992;&#20855;&#26377;&#19977;&#20010;&#35282;&#33394;&#30340;LLM&#20195;&#29702;&#30340;&#32479;&#19968;&#26694;&#26550;&#26469;&#36827;&#34892;KBQA&#20219;&#21153;&#12290;&#20195;&#29702;&#34987;&#20998;&#37197;&#19977;&#20010;&#35282;&#33394;&#26469;&#22788;&#29702;&#19981;&#21516;&#30340;KBQA&#23376;&#20219;&#21153;&#65306;&#20316;&#20026;&#25484;&#25569;&#21508;&#31181;&#23376;&#20219;&#21153;&#30340;&#36890;&#25165;&#65292;&#20316;&#20026;&#36873;&#25321;&#20505;&#36873;&#32773;&#30340;&#20915;&#31574;&#32773;&#65292;&#20197;&#21450;&#20316;&#20026;&#22238;&#31572;&#24102;&#26377;&#30693;&#35782;&#30340;&#38382;&#39064;&#30340;&#39038;&#38382;&#12290;&#25105;&#20204;&#30340;KBQA&#26694;&#26550;&#22312;&#22235;&#20010;&#38454;&#27573;&#20013;&#25191;&#34892;&#65292;&#28041;&#21450;&#20195;&#29702;&#30340;&#22810;&#37325;&#35282;&#33394;&#30340;&#21327;&#20316;&#12290;&#25105;&#20204;&#20351;&#29992;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#35780;&#20272;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#32988;&#36807;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14320v1 Announce Type: cross  Abstract: Recent progress with LLM-based agents has shown promising results across various tasks. However, their use in answering questions from knowledge bases remains largely unexplored. Implementing a KBQA system using traditional methods is challenging due to the shortage of task-specific training data and the complexity of creating task-focused model structures. In this paper, we present Triad, a unified framework that utilizes an LLM-based agent with three roles for KBQA tasks. The agent is assigned three roles to tackle different KBQA subtasks: agent as a generalist for mastering various subtasks, as a decision maker for the selection of candidates, and as an advisor for answering questions with knowledge. Our KBQA framework is executed in four phases, involving the collaboration of the agent's multiple roles. We evaluated the performance of our framework using three benchmark datasets, and the results show that our framework outperforms 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30005;&#23376;&#21704;&#23494;&#39039;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;Materials Project&#30340;&#31532;&#19968;&#24615;&#21407;&#29702;DFT&#35745;&#31639;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#39044;&#27979;&#25972;&#20010;&#21608;&#26399;&#34920;&#20013;&#21253;&#25324;&#22797;&#26434;&#22810;&#20803;&#32032;&#31995;&#32479;&#22312;&#20869;&#30340;&#30005;&#23376;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2402.09251</link><description>&lt;p&gt;
&#26448;&#26009;&#30340;&#36890;&#29992;&#26426;&#22120;&#23398;&#20064;Kohn-Sham&#21704;&#23494;&#39039;&#37327;
&lt;/p&gt;
&lt;p&gt;
Universal Machine Learning Kohn-Sham Hamiltonian for Materials
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09251
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#30005;&#23376;&#21704;&#23494;&#39039;&#37327;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;Materials Project&#30340;&#31532;&#19968;&#24615;&#21407;&#29702;DFT&#35745;&#31639;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#39044;&#27979;&#25972;&#20010;&#21608;&#26399;&#34920;&#20013;&#21253;&#25324;&#22797;&#26434;&#22810;&#20803;&#32032;&#31995;&#32479;&#22312;&#20869;&#30340;&#30005;&#23376;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;(DFT)&#22312;&#30005;&#23376;&#32467;&#26500;&#35745;&#31639;&#20013;&#26159;&#19968;&#31181;&#26222;&#36941;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20294;&#20854;&#35745;&#31639;&#38656;&#27714;&#21644;&#21487;&#25193;&#23637;&#24615;&#38480;&#21046;&#20173;&#28982;&#23384;&#22312;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#21442;&#25968;&#21270;Kohn-Sham DFT&#21704;&#23494;&#39039;&#37327;&#24050;&#32463;&#25104;&#20026;&#21152;&#36895;&#30005;&#23376;&#32467;&#26500;&#35745;&#31639;&#30340;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#25361;&#25112;&#65292;&#22914;&#20026;&#20102;&#25506;&#32034;&#26032;&#31995;&#32479;&#32780;&#38656;&#35201;&#35745;&#31639;&#22823;&#37327;&#30340;DFT&#35757;&#32451;&#25968;&#25454;&#21644;&#24314;&#31435;&#20934;&#30830;&#30340;&#22810;&#20803;&#32032;&#26448;&#26009;&#30340;ML&#27169;&#22411;&#30340;&#22797;&#26434;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#38556;&#30861;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#23545;&#26469;&#33258;Materials Project&#30340;&#31532;&#19968;&#24615;&#21407;&#29702;DFT&#35745;&#31639;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#36827;&#34892;&#35757;&#32451;&#30340;&#36890;&#29992;&#30005;&#23376;&#21704;&#23494;&#39039;&#37327;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#22312;&#25972;&#20010;&#21608;&#26399;&#34920;&#20013;&#39044;&#27979;&#30005;&#23376;&#32467;&#26500;&#30340;&#26222;&#36866;&#24615;&#65292;&#21253;&#25324;&#22797;&#26434;&#30340;&#22810;&#20803;&#32032;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09251v1 Announce Type: cross Abstract: While density functional theory (DFT) serves as a prevalent computational approach in electronic structure calculations, its computational demands and scalability limitations persist. Recently, leveraging neural networks to parameterize the Kohn-Sham DFT Hamiltonian has emerged as a promising avenue for accelerating electronic structure computations. Despite advancements, challenges such as the necessity for computing extensive DFT training data to explore new systems and the complexity of establishing accurate ML models for multi-elemental materials still exist. Addressing these hurdles, this study introduces a universal electronic Hamiltonian model trained on Hamiltonian matrices obtained from first-principles DFT calculations of nearly all crystal structures on the Materials Project. We demonstrate its generality in predicting electronic structures across the whole periodic table, including complex multi-elemental systems. By offerin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;CVLA&#65292;&#29992;&#20110;&#30701;&#35270;&#39057;&#24189;&#40664;&#26816;&#27979;&#12290;CVLA&#19981;&#20165;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24577;&#20449;&#21495;&#65292;&#36824;&#33021;&#36890;&#36807;&#22312;&#19968;&#33268;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#23545;&#40784;&#35270;&#39057;&#21644;&#35821;&#35328;&#32452;&#20214;&#20135;&#29983;&#36866;&#21512;&#30340;&#22810;&#27169;&#24335;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CVLA&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#21644;&#20960;&#20010;&#31454;&#20105;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09055</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#21033;&#29992;&#35780;&#35770;&#36741;&#21161;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;&#29992;&#20110;&#30701;&#35270;&#39057;&#24189;&#40664;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Comment-aided Video-Language Alignment via Contrastive Pre-training for Short-form Video Humor Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;CVLA&#65292;&#29992;&#20110;&#30701;&#35270;&#39057;&#24189;&#40664;&#26816;&#27979;&#12290;CVLA&#19981;&#20165;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24577;&#20449;&#21495;&#65292;&#36824;&#33021;&#36890;&#36807;&#22312;&#19968;&#33268;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#23545;&#40784;&#35270;&#39057;&#21644;&#35821;&#35328;&#32452;&#20214;&#20135;&#29983;&#36866;&#21512;&#30340;&#22810;&#27169;&#24335;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CVLA&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#21644;&#20960;&#20010;&#31454;&#20105;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30701;&#35270;&#39057;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#24433;&#21709;&#21147;&#26085;&#30410;&#25193;&#22823;&#65292;&#22810;&#27169;&#24335;&#24189;&#40664;&#26816;&#27979;&#22312;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#37325;&#35201;&#24615;&#20063;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#30701;&#35270;&#39057;&#24189;&#40664;&#26816;&#27979;&#30340;&#20004;&#23618;&#20998;&#23618;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#30340;&#22810;&#27169;&#24335;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#35780;&#35770;&#36741;&#21161;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;&#65288;CVLA&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;CVLA&#19981;&#20165;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24577;&#20449;&#21495;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#19968;&#33268;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#23545;&#40784;&#35270;&#39057;&#21644;&#35821;&#35328;&#32452;&#20214;&#65292;&#20135;&#29983;&#19968;&#20010;&#36866;&#21512;&#30340;&#22810;&#27169;&#24335;&#34920;&#31034;&#12290;&#22312;&#20004;&#20010;&#24189;&#40664;&#26816;&#27979;&#25968;&#25454;&#38598;DY11k&#21644;UR-FUNNY&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CVLA&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#21644;&#20960;&#20010;&#31454;&#20105;&#22522;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#21457;&#24067;&#22312;https://github.com/yliu-cs/CVLA&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09055v1 Announce Type: cross Abstract: The growing importance of multi-modal humor detection within affective computing correlates with the expanding influence of short-form video sharing on social media platforms. In this paper, we propose a novel two-branch hierarchical model for short-form video humor detection (SVHD), named Comment-aided Video-Language Alignment (CVLA) via data-augmented multi-modal contrastive pre-training. Notably, our CVLA not only operates on raw signals across various modal channels but also yields an appropriate multi-modal representation by aligning the video and language components within a consistent semantic space. The experimental results on two humor detection datasets, including DY11k and UR-FUNNY, demonstrate that CVLA dramatically outperforms state-of-the-art and several competitive baseline approaches. Our dataset, code and model release at https://github.com/yliu-cs/CVLA.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23454;&#29616;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#65288;JITAIs&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#27979;&#35797;GPT-4&#27169;&#22411;&#20197;&#20419;&#36827;&#38376;&#35786;&#24515;&#33039;&#24247;&#22797;&#20013;&#24515;&#30340;&#24515;&#33039;&#20581;&#24247;&#20307;&#32946;&#27963;&#21160;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;450&#20010;JITAI&#20915;&#31574;&#21644;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2402.08658</link><description>&lt;p&gt;
&#26368;&#21518;&#30340;JITAI&#65311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21457;&#25918;&#21450;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#20013;&#30340;&#19981;&#21512;&#29702;&#26377;&#25928;&#24615;&#65306;&#22312;&#21069;&#30651;&#24615;&#24515;&#33039;&#24247;&#22797;&#29615;&#22659;&#20013;&#20419;&#36827;&#20307;&#32946;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
The Last JITAI? The Unreasonable Effectiveness of Large Language Models in Issuing Just-in-Time Adaptive Interventions: Fostering Physical Activity in a Prospective Cardiac Rehabilitation Setting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23454;&#29616;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#65288;JITAIs&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#36890;&#36807;&#27979;&#35797;GPT-4&#27169;&#22411;&#20197;&#20419;&#36827;&#38376;&#35786;&#24515;&#33039;&#24247;&#22797;&#20013;&#24515;&#30340;&#24515;&#33039;&#20581;&#24247;&#20307;&#32946;&#27963;&#21160;&#30340;&#20351;&#29992;&#26696;&#20363;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;450&#20010;JITAI&#20915;&#31574;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25968;&#23383;&#20581;&#24247;&#20013;&#35302;&#21457;&#21644;&#20010;&#24615;&#21270;&#21363;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#65288;JITAIs&#65289;&#20869;&#23481;&#30340;&#21487;&#34892;&#24615;&#12290;JITAIs&#34987;&#35270;&#20026;&#21487;&#25345;&#32493;&#34892;&#20026;&#25913;&#21464;&#30340;&#20851;&#38190;&#26426;&#21046;&#65292;&#23558;&#24178;&#39044;&#25514;&#26045;&#26681;&#25454;&#20010;&#20307;&#30340;&#24403;&#21069;&#24773;&#22659;&#21644;&#38656;&#27714;&#36827;&#34892;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;JITAI&#23454;&#26045;&#20013;&#38754;&#20020;&#21487;&#25193;&#23637;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#32570;&#20047;&#20010;&#24615;&#21270;&#12289;&#31649;&#29702;&#22810;&#21442;&#25968;&#31995;&#32479;&#22256;&#38590;&#20197;&#21450;&#25968;&#25454;&#31232;&#30095;&#24615;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#30740;&#31350;&#36890;&#36807;LLMs&#23454;&#29616;JITAI&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#22312;&#38376;&#35786;&#24515;&#33039;&#24247;&#22797;&#20013;&#20419;&#36827;&#24515;&#33039;&#20581;&#24247;&#20307;&#32946;&#27963;&#21160;&#30340;&#20351;&#29992;&#26696;&#20363;&#30340;&#29616;&#20195;&#26368;&#39640;&#24615;&#33021;&#27169;&#22411;&#8220;GPT-4&#8221;&#30340;&#23454;&#20363;&#20316;&#20026;&#35302;&#21457;&#21644;&#20010;&#24615;&#21270;JITAIs&#30340;&#22522;&#30784;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#24635;&#20849;450&#20010;&#24314;&#35758;&#30340;JITAI&#20915;&#31574;&#21644;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explored the viability of Large Language Models (LLMs) for triggering and personalizing content for Just-in-Time Adaptive Interventions (JITAIs) in digital health. JITAIs are being explored as a key mechanism for sustainable behavior change, adapting interventions to an individual's current context and needs. However, traditional rule-based and machine learning models for JITAI implementation face scalability and reliability limitations, such as lack of personalization, difficulty in managing multi-parametric systems, and issues with data sparsity. To investigate JITAI implementation via LLMs, we tested the contemporary overall performance-leading model 'GPT-4' with examples grounded in the use case of fostering heart-healthy physical activity in outpatient cardiac rehabilitation. Three personas and five sets of context information per persona were used as a basis of triggering and personalizing JITAIs. Subsequently, we generated a total of 450 proposed JITAI decisions and message c
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.06025</link><description>&lt;p&gt;
&#29992;&#33258;&#28982;&#35821;&#35328;&#21644;&#27010;&#29575;&#25512;&#29702;&#36827;&#34892;&#23454;&#39564;&#19982;&#20462;&#35746;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Doing Experiments and Revising Rules with Natural Language and Probabilistic Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.06025
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#26469;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#65292;&#24182;&#21457;&#29616;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#30340;&#32452;&#21512;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#35745;&#31639;&#27169;&#22411;&#65292;&#27169;&#25311;&#20154;&#20204;&#36890;&#36807;&#23454;&#39564;&#20027;&#21160;&#25512;&#26029;&#38544;&#34255;&#35268;&#21017;&#30340;&#36807;&#31243;&#12290;&#35813;&#27169;&#22411;&#30340;&#22522;&#26412;&#21407;&#29702;&#26159;&#65292;&#21363;&#20351;&#35268;&#21017;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#23398;&#20064;&#32773;&#20063;&#20250;&#32771;&#34385;&#26356;&#24191;&#27867;&#30340;&#27169;&#31946;&#27010;&#29575;&#35268;&#21017;&#65292;&#24182;&#29992;&#33258;&#28982;&#35821;&#35328;&#34920;&#31034;&#65292;&#26681;&#25454;&#36817;&#20284;&#36125;&#21494;&#26031;&#21407;&#21017;&#22312;&#27599;&#27425;&#23454;&#39564;&#21518;&#22312;&#32447;&#26356;&#26032;&#33258;&#24049;&#30340;&#20551;&#35774;&#12290;&#22312;&#21516;&#19968;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#36824;&#26681;&#25454;&#20449;&#24687;&#35770;&#20934;&#21017;&#24314;&#31435;&#20102;&#23454;&#39564;&#35774;&#35745;&#27169;&#22411;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#19977;&#20010;&#21407;&#21017;&#30340;&#32452;&#21512;&#8212;&#8212;&#26174;&#24335;&#20551;&#35774;&#12289;&#27010;&#29575;&#35268;&#21017;&#21644;&#22312;&#32447;&#26356;&#26032;&#8212;&#8212;&#21487;&#20197;&#35299;&#37322;&#20154;&#20204;&#22312;&#31867;&#20284;Zendo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#21435;&#25481;&#20854;&#20013;&#20219;&#20309;&#19968;&#20010;&#32452;&#20214;&#37117;&#20351;&#24471;&#27169;&#22411;&#26080;&#27861;&#35299;&#37322;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We build a computational model of how humans actively infer hidden rules by doing experiments. The basic principles behind the model is that, even if the rule is deterministic, the learner considers a broader space of fuzzy probabilistic rules, which it represents in natural language, and updates its hypotheses online after each experiment according to approximately Bayesian principles. In the same framework we also model experiment design according to information-theoretic criteria. We find that the combination of these three principles -- explicit hypotheses, probabilistic rules, and online updates -- can explain human performance on a Zendo-style task, and that removing any of these components leaves the model unable to account for the data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.05967</link><description>&lt;p&gt;
&#26368;&#21518;&#20043;&#33310;&#65306;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
The last Dance : Robust backdoor attack via diffusion models and bayesian approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05967
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#25193;&#25955;&#27169;&#22411;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#36827;&#34892;&#40065;&#26834;&#21518;&#38376;&#25915;&#20987;&#30340;&#26041;&#27861;&#65292;&#20855;&#20307;&#24212;&#29992;&#20110;&#38899;&#39057;Transformer&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#20102;&#25915;&#20987;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#65292;&#20854;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#22122;&#38899;&#21644;&#21435;&#22122;&#30340;&#26041;&#24335;&#23398;&#20064;&#27491;&#21521;&#21644;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#30340;&#21407;&#29702;&#36827;&#34892;&#35757;&#32451;&#12290;&#26412;&#25991;&#26088;&#22312;&#27450;&#39575;&#22522;&#20110;&#38899;&#39057;&#30340;DNN&#27169;&#22411;&#65292;&#20363;&#22914;Hugging Face&#26694;&#26550;&#20013;&#30340;&#38899;&#39057;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;Transformer&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#33410;&#30465;&#26102;&#38388;&#65292;&#25552;&#20379;&#26356;&#39640;&#25928;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;Hugging Face&#25512;&#23548;&#20986;&#30340;&#38899;&#39057;Transformer&#19978;&#23454;&#29616;&#21518;&#38376;&#25915;&#20987;&#65288;&#31216;&#20026;`BacKBayDiffMod`&#65289;&#30340;&#21487;&#34892;&#24615;&#12290;&#26412;&#25991;&#20013;&#24320;&#21457;&#30340;&#21518;&#38376;&#25915;&#20987;&#22522;&#20110;&#27602;&#21270;&#27169;&#22411;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#28041;&#21450;&#21518;&#38376;&#25193;&#25955;&#37319;&#26679;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20998;&#24067;&#30340;&#24341;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are state-of-the-art deep learning generative models that are trained on the principle of learning forward and backward diffusion processes via the progressive addition of noise and denoising. In this paper, we seek to trick audio-based DNN models, such as those in the Hugging Face framework, for example, those that focus on audio, in particular transformer-based artificial intelligence models, which are powerful machine learning models that save time and deliver faster, more efficient results. We demonstrate the feasibility of backdoor attacks (called `BacKBayDiffMod`) on audio transformers derived from Hugging Face, a popular framework in the world of artificial intelligence (AI) research. The backdoor attack developed in this paper is based on poisoning the model's training data by incorporating backdoor diffusion sampling and a Bayesian approach to the distribution of poisoned data.
&lt;/p&gt;</description></item><item><title>MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.02263</link><description>&lt;p&gt;
MixedNUTS: &#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#23454;&#29616;&#26080;&#38656;&#35757;&#32451;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
MixedNUTS: Training-Free Accuracy-Robustness Balance via Nonlinearly Mixed Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02263
&lt;/p&gt;
&lt;p&gt;
MixedNUTS&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#38750;&#32447;&#24615;&#28151;&#21512;&#20998;&#31867;&#22120;&#30340;&#36716;&#25442;&#21644;&#27010;&#29575;&#28151;&#21512;&#26469;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#40065;&#26834;&#24615;&#24448;&#24448;&#29306;&#29298;&#20102;&#20934;&#30830;&#24615;&#65292;&#38459;&#30861;&#20102;&#40065;&#26834;&#20998;&#31867;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#19982;&#24050;&#35757;&#32451;&#30340;&#22823;&#22411;&#39640;&#24615;&#33021;&#27169;&#22411;&#20860;&#23481;&#24615;&#26041;&#38754;&#23384;&#22312;&#38480;&#21046;&#65292;&#22240;&#27492;&#38656;&#35201;&#25506;&#32034;&#26080;&#38656;&#35757;&#32451;&#30340;&#38598;&#25104;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#40065;&#26834;&#27169;&#22411;&#22312;&#24178;&#20928;&#25968;&#25454;&#21644;&#23545;&#25239;&#25968;&#25454;&#19978;&#30340;&#27491;&#30830;&#39044;&#27979;&#27604;&#38169;&#35823;&#39044;&#27979;&#26356;&#33258;&#20449;&#65292;&#25105;&#20204;&#25512;&#27979;&#36890;&#36807;&#22686;&#24378;&#36825;&#31181;&#8220;&#33391;&#24615;&#32622;&#20449;&#24230;&#29305;&#24615;&#8221;&#21487;&#20197;&#22312;&#38598;&#25104;&#29615;&#22659;&#20013;&#23454;&#29616;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#30340;&#24179;&#34913;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;MixedNUTS&#8221;&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20165;&#26377;&#19977;&#20010;&#21442;&#25968;&#30340;&#38750;&#32447;&#24615;&#36716;&#25442;&#26469;&#22788;&#29702;&#40065;&#26834;&#20998;&#31867;&#22120;&#21644;&#26631;&#20934;&#38750;&#40065;&#26834;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;Logits&#65292;&#24182;&#36890;&#36807;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#28982;&#21518;&#65292;MixedNUTS&#23558;&#36716;&#25442;&#21518;&#30340;Logits&#36716;&#25442;&#20026;&#27010;&#29575;&#65292;&#24182;&#23558;&#23427;&#20204;&#28151;&#21512;&#20316;&#20026;&#26368;&#32456;&#30340;&#36755;&#20986;&#12290;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness often comes at the cost of degraded accuracy, impeding the real-life application of robust classification models. Training-based solutions for better trade-offs are limited by incompatibilities with already-trained high-performance large models, necessitating the exploration of training-free ensemble approaches. Observing that robust models are more confident in correct predictions than in incorrect ones on clean and adversarial data alike, we speculate amplifying this "benign confidence property" can reconcile accuracy and robustness in an ensemble setting. To achieve so, we propose "MixedNUTS", a training-free method where the output logits of a robust classifier and a standard non-robust classifier are processed by nonlinear transformations with only three parameters, which are optimized through an efficient algorithm. MixedNUTS then converts the transformed logits into probabilities and mixes them as the overall output. On CIFAR-10, CIFAR-100, and ImageNet da
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;L-Tuning&#65292;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#20013;&#30340;&#26631;&#31614;&#26631;&#35760;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;</title><link>https://arxiv.org/abs/2402.01643</link><description>&lt;p&gt;
L-TUNING&#65306;&#29992;&#20110;LLMs&#20013;&#30340;&#25552;&#31034;&#21644;&#21069;&#32512;&#30340;&#21516;&#27493;&#26631;&#31614;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
L-TUNING: Synchronized Label Tuning for Prompt and Prefix in LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01643
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;L-Tuning&#65292;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#39044;&#35757;&#32451;&#30340;LLM&#20013;&#30340;&#26631;&#31614;&#26631;&#35760;&#36827;&#34892;&#24494;&#35843;&#65292;&#25552;&#39640;&#20102;&#35757;&#32451;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#24182;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#24494;&#35843;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#25552;&#31034;&#25110;&#21069;&#32512;&#35843;&#25972;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#20219;&#24847;&#26631;&#35760;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#23548;&#33268;&#35757;&#32451;&#26102;&#38388;&#24310;&#38271;&#24182;&#19988;&#36890;&#29992;&#26631;&#35760;&#22312;&#21508;&#31181;&#31867;&#21035;&#26631;&#31614;&#20013;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;L-Tuning&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#26694;&#26550;&#20869;&#35774;&#35745;&#30340;&#29992;&#20110;&#20998;&#31867;&#20219;&#21153;&#30340;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#12290;&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;L-Tuning&#19987;&#27880;&#20110;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;LLM&#22788;&#29702;&#30340;&#26631;&#31614;&#26631;&#35760;&#30340;&#24494;&#35843;&#65292;&#20174;&#32780;&#21033;&#29992;&#20854;&#39044;&#20808;&#23384;&#22312;&#30340;&#35821;&#20041;&#30693;&#35782;&#12290;&#36825;&#31181;&#25216;&#26415;&#19981;&#20165;&#25552;&#39640;&#20102;&#24494;&#35843;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#65292;&#36824;&#20419;&#36827;&#20102;&#20026;&#27599;&#20010;&#31867;&#21035;&#29983;&#25104;&#19981;&#21516;&#30340;&#26631;&#31614;&#23884;&#20837;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#32454;&#24494;&#24046;&#21035;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;L-Tuning&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficiently fine-tuning Large Language Models (LLMs) for specific tasks presents a considerable challenge in natural language processing. Traditional methods, like prompt or prefix tuning, typically rely on arbitrary tokens for training, leading to prolonged training times and generalized token use across various class labels. To address these issues, this paper introduces L-Tuning, an efficient fine-tuning approach designed for classification tasks within the Natural Language Inference (NLI) framework. Diverging from conventional methods, L-Tuning focuses on the fine-tuning of label tokens processed through a pre-trained LLM, thereby harnessing its pre-existing semantic knowledge. This technique not only improves the fine-tuning accuracy and efficiency but also facilitates the generation of distinct label embeddings for each class, enhancing the model's training nuance. Our experimental results indicate a significant improvement in training efficiency and classification accuracy with 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#22312;&#25910;&#38598;&#21040;&#30340;&#36712;&#36857;&#19978;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34394;&#24187;&#21644;&#32570;&#38519;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.00658</link><description>&lt;p&gt;
&#36890;&#36807;&#25910;&#38598;&#36712;&#36857;&#21644;&#21512;&#25104;&#22870;&#21169;&#26469;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Learning Planning-based Reasoning by Trajectories Collection and Process Reward Synthesizing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#22312;&#25910;&#38598;&#21040;&#30340;&#36712;&#36857;&#19978;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20197;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#34394;&#24187;&#21644;&#32570;&#38519;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#36880;&#27493;&#21512;&#29702;&#21270;&#29983;&#25104;&#65292;&#23637;&#31034;&#20102;&#22788;&#29702;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#30340;&#37325;&#35201;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#23545;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#34394;&#24187;&#21644;&#32570;&#38519;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;&#21512;&#29702;&#21270;&#30340;&#21487;&#38752;&#24615;&#21644;&#24544;&#23454;&#24615;&#65292;&#27491;&#22312;&#36827;&#34892;&#22823;&#37327;&#24037;&#20316;&#12290;&#26377;&#20123;&#26041;&#27861;&#23558;&#25512;&#29702;&#24314;&#27169;&#20026;&#35268;&#21010;&#65292;&#32780;&#20854;&#20182;&#26041;&#27861;&#21017;&#19987;&#27880;&#20110;&#27880;&#37322;&#30340;&#36807;&#31243;&#30417;&#30563;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#35268;&#21010;&#30340;&#25628;&#32034;&#36807;&#31243;&#24448;&#24448;&#30001;&#20110;&#39057;&#32321;&#35780;&#20272;&#20013;&#38388;&#25512;&#29702;&#29366;&#24577;&#21644;&#24191;&#27867;&#30340;&#25506;&#32034;&#31354;&#38388;&#32780;&#23548;&#33268;&#39640;&#24310;&#36831;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#20154;&#24037;&#27880;&#37322;&#30417;&#30563;&#25512;&#29702;&#36807;&#31243;&#23545;&#20110;LLM&#35757;&#32451;&#26469;&#35828;&#26159;&#26114;&#36149;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22312;&#26412;&#25991;&#20013;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26469;&#23398;&#20064;&#22522;&#20110;&#35268;&#21010;&#30340;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#36712;&#36857;&#30452;&#25509;&#26681;&#25454;&#21512;&#25104;&#30340;&#36807;&#31243;&#22870;&#21169;&#36827;&#34892;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated significant potential in handling complex reasoning tasks through step-by-step rationale generation. However, recent studies have raised concerns regarding the hallucination and flaws in their reasoning process. Substantial efforts are being made to improve the reliability and faithfulness of the generated rationales. Some approaches model reasoning as planning, while others focus on annotating for process supervision. Nevertheless, the planning-based search process often results in high latency due to the frequent assessment of intermediate reasoning states and the extensive exploration space. Additionally, supervising the reasoning process with human annotation is costly and challenging to scale for LLM training. To address these issues, in this paper, we propose a framework to learn planning-based reasoning through direct preference optimization (DPO) on collected trajectories, which are ranked according to synthesized process rewards. 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#27979;&#24230;&#26102;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;$O(\frac{\sqrt{d}}{T^{1/4}})$&#65292;&#22312;&#32500;&#24230;&#26497;&#22823;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#25913;&#36827;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00389</link><description>&lt;p&gt;
&#20851;&#20110;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#30340;$O(\frac{\sqrt{d}}{T^{1/4}})$&#25910;&#25947;&#36895;&#24230;&#21644;&#23545;&#32500;&#24230;&#30340;&#25913;&#36827;&#20381;&#36182;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the $O(\frac{\sqrt{d}}{T^{1/4}})$ Convergence Rate of RMSProp and Its Momentum Extension Measured by $\ell_1$ Norm: Better Dependence on the Dimension
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00389
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#21457;&#29616;&#20351;&#29992;$\ell_1$&#33539;&#25968;&#27979;&#24230;&#26102;&#65292;&#25910;&#25947;&#36895;&#24230;&#20026;$O(\frac{\sqrt{d}}{T^{1/4}})$&#65292;&#22312;&#32500;&#24230;&#26497;&#22823;&#30340;&#38382;&#39064;&#20013;&#20855;&#26377;&#25913;&#36827;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#33258;&#36866;&#24212;&#26799;&#24230;&#26041;&#27861;&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#20854;&#25910;&#25947;&#36895;&#24230;&#23578;&#26410;&#24471;&#21040;&#24443;&#24213;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20854;&#23545;&#32500;&#24230;&#30340;&#20381;&#36182;&#24615;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#32463;&#20856;&#30340;RMSProp&#21450;&#20854;&#21160;&#37327;&#25193;&#23637;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;$\ell_1$&#33539;&#25968;&#24314;&#31435;&#20102;&#25910;&#25947;&#29575;$\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_1\right]\leq O(\frac{\sqrt{d}}{T^{1/4}})$&#65292;&#26080;&#38656;&#20551;&#35774;&#26799;&#24230;&#26377;&#30028;&#65292;&#20854;&#20013;$d$&#26159;&#20248;&#21270;&#21464;&#37327;&#30340;&#32500;&#24230;&#65292;$T$&#26159;&#36845;&#20195;&#27425;&#25968;&#12290;&#30001;&#20110;&#23545;&#20110;&#32500;&#24230;&#26497;&#22823;&#30340;&#38382;&#39064;&#65292;$\|x\|_2\ll\|x\|_1\leq\sqrt{d}\|x\|_2$&#65292;&#22240;&#27492;&#25105;&#20204;&#30340;&#25910;&#25947;&#36895;&#24230;&#21487;&#20197;&#31867;&#27604;&#20026;SGD&#30340;$\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_2\right]\leq O(\frac{1}{T^{1/4}})$&#65292;&#27979;&#24230;&#20026;$\ell_1$&#33539;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although adaptive gradient methods have been extensively used in deep learning, their convergence rates have not been thoroughly studied, particularly with respect to their dependence on the dimension. This paper considers the classical RMSProp and its momentum extension and establishes the convergence rate of $\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_1\right]\leq O(\frac{\sqrt{d}}{T^{1/4}})$ measured by $\ell_1$ norm without the bounded gradient assumption, where $d$ is the dimension of the optimization variable and $T$ is the iteration number. Since $\|x\|_2\ll\|x\|_1\leq\sqrt{d}\|x\|_2$ for problems with extremely large $d$, our convergence rate can be considered to be analogous to the $\frac{1}{T}\sum_{k=1}^TE\left[\|\nabla f(x^k)\|_2\right]\leq O(\frac{1}{T^{1/4}})$ one of SGD measured by $\ell_1$ norm.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#30340;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#38454;&#27573;&#37117;&#20351;&#29992;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#32500;&#25252;&#29992;&#25143;&#30340;&#20449;&#20219;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#23545;&#25512;&#29702;&#32467;&#26524;&#30340;&#26377;&#20449;&#24515;&#30340;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#24050;&#32463;&#22312;&#21360;&#24230;&#30340;&#25968;&#21315;&#21517;&#26825;&#20892;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#29992;&#20110;&#20854;&#20182;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#12290;</title><link>https://arxiv.org/abs/2402.00015</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#38454;&#27573;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#25512;&#26029;&#26469;&#32500;&#25252;&#29992;&#25143;&#20449;&#20219;
&lt;/p&gt;
&lt;p&gt;
Maintaining User Trust Through Multistage Uncertainty Aware Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00015
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#30340;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#38454;&#27573;&#37117;&#20351;&#29992;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#20197;&#32500;&#25252;&#29992;&#25143;&#30340;&#20449;&#20219;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#23545;&#25512;&#29702;&#32467;&#26524;&#30340;&#26377;&#20449;&#24515;&#30340;&#20915;&#31574;&#12290;&#35813;&#26041;&#27861;&#24050;&#32463;&#22312;&#21360;&#24230;&#30340;&#25968;&#21315;&#21517;&#26825;&#20892;&#20013;&#36827;&#34892;&#20102;&#23454;&#38469;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#36866;&#29992;&#20110;&#20854;&#20182;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#30340;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#26041;&#27861;&#12290;&#27599;&#20010;&#38454;&#27573;&#37117;&#28041;&#21450;&#19968;&#31181;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;&#26041;&#27861;&#65292;&#20294;&#27599;&#20010;&#38454;&#27573;&#30340;&#24320;&#38144;&#20063;&#36880;&#28176;&#22686;&#21152;&#12290;&#22312;&#27010;&#36848;&#26550;&#26500;&#26102;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37327;&#21270;&#27169;&#22411;&#19981;&#30830;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#20197;&#20419;&#36827;&#26377;&#20449;&#24515;&#30340;&#24310;&#26399;&#20915;&#31574;&#12290;&#35813;&#26550;&#26500;&#30446;&#21069;&#27491;&#22312;&#21360;&#24230;&#30340;&#25968;&#21315;&#21517;&#26825;&#20892;&#20013;&#36827;&#34892;&#31215;&#26497;&#37096;&#32626;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#26356;&#24191;&#27867;&#30340;&#24819;&#27861;&#20063;&#36866;&#29992;&#20110;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#36827;&#34892;&#20154;&#24037;&#26234;&#33021;&#37096;&#32626;&#30340;&#19981;&#26029;&#22686;&#38271;&#30340;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper describes and evaluates a multistage approach to AI deployment. Each stage involves a more accurate method of inference, yet engaging each comes with an increasing cost. In outlining the architecture, we present a method for quantifying model uncertainty that facilitates confident deferral decisions. The architecture is currently under active deployment to thousands of cotton farmers across India. The broader idea however is applicable to a growing sector of AI deployments in challenging low resources settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17548</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#36947;&#30456;&#20851;&#24615;&#65306;&#20174;&#39046;&#20808;&#25351;&#26631;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29420;&#31435;&#20110;&#36890;&#36947;&#30340;&#26041;&#27861;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#20294;&#23427;&#20204;&#38169;&#36807;&#20102;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#23616;&#37096;&#24179;&#31283;&#30340;&#39046;&#20808;-&#28382;&#21518;&#20851;&#31995;&#65292;&#21363;&#19968;&#20123;&#28382;&#21518;&#21464;&#37327;&#22312;&#30701;&#26102;&#38388;&#20869;&#21487;&#33021;&#36981;&#24490;&#39046;&#20808;&#25351;&#26631;&#12290;&#21033;&#29992;&#36825;&#31181;&#36890;&#36947;&#30456;&#20851;&#24615;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#39046;&#20808;&#25351;&#26631;&#25552;&#20379;&#20102;&#20808;&#36827;&#20449;&#24687;&#65292;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#28382;&#21518;&#21464;&#37327;&#30340;&#39044;&#27979;&#38590;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFT&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#39640;&#25928;&#22320;&#20272;&#35745;&#39046;&#20808;&#25351;&#26631;&#21450;&#20854;&#39046;&#20808;&#27493;&#39588;&#65292;&#28982;&#21518;&#24039;&#22937;&#22320;&#20801;&#35768;&#28382;&#21518;&#21464;&#37327;&#21033;&#29992;&#26469;&#33258;&#39046;&#20808;&#25351;&#26631;&#30340;&#20808;&#36827;&#20449;&#24687;&#12290;LIFT&#20316;&#20026;&#19968;&#20010;&#25554;&#20214;&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#26080;&#32541;&#21327;&#20316;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LIFT&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22823;&#37327;&#30340;&#21307;&#30103;&#25968;&#25454;&#26679;&#26412;&#12289;&#22522;&#20934;&#26041;&#27861;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.17542</link><description>&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#21512;&#21307;&#23398;&#22522;&#20934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Data-Effective Learning: A Comprehensive Medical Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17542
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#32508;&#21512;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#22823;&#37327;&#30340;&#21307;&#30103;&#25968;&#25454;&#26679;&#26412;&#12289;&#22522;&#20934;&#26041;&#27861;&#21644;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#26088;&#22312;&#20197;&#26368;&#26377;&#25928;&#30340;&#26041;&#24335;&#21033;&#29992;&#25968;&#25454;&#26469;&#35757;&#32451;AI&#27169;&#22411;&#65292;&#20854;&#28041;&#21450;&#20851;&#27880;&#25968;&#25454;&#36136;&#37327;&#32780;&#38750;&#25968;&#37327;&#30340;&#31574;&#30053;&#65292;&#30830;&#20445;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#20855;&#26377;&#39640;&#20449;&#24687;&#20215;&#20540;&#12290;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#22312;&#21152;&#24555;AI&#35757;&#32451;&#12289;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#21644;&#33410;&#30465;&#25968;&#25454;&#23384;&#20648;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36825;&#22312;&#36817;&#24180;&#26469;&#21307;&#23398;&#25968;&#25454;&#30340;&#25968;&#37327;&#36229;&#20986;&#20102;&#35768;&#22810;&#20154;&#30340;&#39044;&#26399;&#26102;&#23588;&#20026;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#26631;&#20934;&#21644;&#32508;&#21512;&#30340;&#22522;&#20934;&#30740;&#31350;&#65292;&#21307;&#23398;&#39046;&#22495;&#30340;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30740;&#31350;&#36824;&#19981;&#22815;&#28145;&#20837;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#21307;&#23398;&#39046;&#22495;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;&#35813;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;31&#20010;&#21307;&#30103;&#20013;&#24515;&#25968;&#30334;&#19975;&#20010;&#25968;&#25454;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;(DataDEL)&#65292;&#29992;&#20110;&#27604;&#36739;&#30340;&#22522;&#20934;&#26041;&#27861;(MedDEL)&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#23458;&#35266;&#34913;&#37327;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#24615;&#33021;&#30340;&#26032;&#35780;&#20272;&#25351;&#26631;(NormDEL)&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#23454;&#39564;&#21644;&#27604;&#36739;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#22522;&#20934;&#22312;&#35780;&#20272;&#25968;&#25454;&#26377;&#25928;&#23398;&#20064;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-effective learning aims to use data in the most impactful way to train AI models, which involves strategies that focus on data quality rather than quantity, ensuring the data used for training has high informational value. Data-effective learning plays a profound role in accelerating AI training, reducing computational costs, and saving data storage, which is very important as the volume of medical data in recent years has grown beyond many people's expectations. However, due to the lack of standards and comprehensive benchmark, research on medical data-effective learning is poorly studied. To address this gap, our paper introduces a comprehensive benchmark specifically for evaluating data-effective learning in the medical field. This benchmark includes a dataset with millions of data samples from 31 medical centers (DataDEL), a baseline method for comparison (MedDEL), and a new evaluation metric (NormDEL) to objectively measure data-effective learning performance. Our extensive e
&lt;/p&gt;</description></item><item><title>&#25351;&#20196;&#35843;&#25972;&#21644;&#25552;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#27861;&#25552;&#20379;&#27604;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#35748;&#30693;&#24314;&#27169;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2311.07484</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24515;&#29702;&#27979;&#37327;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Psychometric Predictive Power of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07484
&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#21644;&#25552;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#26080;&#27861;&#25552;&#20379;&#27604;&#22522;&#30784;&#27169;&#22411;&#26356;&#22909;&#30340;&#35748;&#30693;&#24314;&#27169;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07484v2 &#20844;&#21578;&#31867;&#22411;: &#26367;&#25442;-&#20132;&#21449; &#25688;&#35201;:&#25351;&#20196;&#35843;&#25972;&#20351;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21709;&#24212;&#19982;&#20154;&#31867;&#20559;&#22909;&#19968;&#33268;&#12290;&#23613;&#31649;&#22312;&#20154;-LLM&#23545;&#40784;&#26041;&#38754;&#36827;&#34892;&#20102;&#21162;&#21147;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;&#21457;&#29616;&#65292;&#21363;&#25351;&#20196;&#35843;&#25972;&#24182;&#19981;&#24635;&#26159;&#20351;LLMs&#20174;&#35748;&#30693;&#24314;&#27169;&#30340;&#35282;&#24230;&#30475;&#36215;&#26469;&#26356;&#20687;&#20154;&#31867;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#30001;&#25351;&#20196;&#35843;&#25972;&#30340;LLM&#20272;&#35745;&#30340;&#19979;&#19968;&#20010;&#35789;&#27010;&#29575;&#24448;&#24448;&#27604;&#22522;&#30784;LLM&#20272;&#35745;&#30340;&#27010;&#29575;&#26356;&#31967;&#31957;&#65292;&#26080;&#27861;&#27169;&#25311;&#20154;&#31867;&#38405;&#35835;&#34892;&#20026;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#27169;&#25311;&#20154;&#31867;&#38405;&#35835;&#34892;&#20026;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21453;&#26144;&#29305;&#23450;&#35821;&#35328;&#20551;&#35774;&#30340;&#25552;&#31034;&#21487;&#20197;&#25552;&#39640;PPP&#65292;&#20294;&#20173;&#19981;&#21450;&#23567;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;PPP&#12290;&#36825;&#20123;&#21457;&#29616;&#31361;&#26174;&#20986;LLMs&#26368;&#36817;&#30340;&#36827;&#23637;&#65292;&#21363;&#25351;&#20196;&#35843;&#25972;&#21644;&#25552;&#31034;&#65292;&#24182;&#19981;&#33021;&#25552;&#20379;&#27604;&#22522;&#30784;LLMs&#30452;&#25509;&#27010;&#29575;&#27979;&#37327;&#26356;&#22909;&#30340;&#35748;&#30693;&#24314;&#27169;&#20272;&#35745;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#32431;&#31929;&#30340;&#19979;&#19968;&#20010;&#35789;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07484v2 Announce Type: replace-cross  Abstract: Instruction tuning aligns the response of large language models (LLMs) with human preferences. Despite such efforts in human--LLM alignment, we report that, interestingly, instruction tuning does not always make LLMs human-like from a cognitive modeling perspective. More specifically, next-word probabilities estimated by instruction-tuned LLMs are often worse at simulating human reading behavior than those estimated by base LLMs. In addition, we explore prompting methodologies in simulating human reading behavior with LLMs. Our results show that prompts reflecting a particular linguistic hypothesis improve PPP but are still inferior to PPP from small base models. These findings highlight that recent advancements in LLMs, i.e., instruction tuning and prompting, do not offer better estimates than direct probability measurements from base LLMs in cognitive modeling. In other words, our experiments highlight that pure next-word pro
&lt;/p&gt;</description></item><item><title>&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2311.06899</link><description>&lt;p&gt;
&#28779;&#28976;: &#35780;&#20272;&#20013;&#22269;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#22865;&#21512;&#24615;&#30340;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Flames: Benchmarking Value Alignment of Chinese Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06899
&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#38656;&#35201;&#26356;&#21152;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#21644;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#65292;&#20197;&#21450;&#22797;&#26434;&#22330;&#26223;&#21644;&#38544;&#21547;&#24694;&#24847;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#22320;&#21306;&#30340;&#24191;&#27867;&#24212;&#29992;&#24378;&#35843;&#20102;&#35780;&#20272;&#23427;&#20204;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#30340;&#36843;&#20999;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#26410;&#33021;&#26377;&#25928;&#22320;&#25581;&#31034;LLMs&#20013;&#30340;&#23433;&#20840;&#28431;&#27934;&#12290;&#23613;&#31649;&#35768;&#22810;&#27169;&#22411;&#22312;&#36825;&#20123;&#35780;&#20272;&#20013;&#24471;&#20998;&#24456;&#39640;&#65292;&#19988;&#8220;&#21517;&#21015;&#21069;&#33541;&#8221;&#65292;&#20294;&#22312;LLMs&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30340;&#28145;&#23618;&#22865;&#21512;&#24615;&#21644;&#23454;&#29616;&#30495;&#27491;&#26080;&#23475;&#26041;&#38754;&#20173;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"&#28779;&#28976;"&#65288;Flames&#65289;&#30340;&#20215;&#20540;&#35266;&#22865;&#21512;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#35813;&#27979;&#35797;&#28085;&#30422;&#20102;&#24120;&#35265;&#30340;&#26080;&#23475;&#21407;&#21017;&#65292;&#20197;&#21450;&#19968;&#20010;&#25972;&#21512;&#20102;&#29305;&#23450;&#20013;&#22269;&#20215;&#20540;&#35266;&#22914;&#21644;&#35856;&#30340;&#29420;&#29305;&#36947;&#24503;&#32500;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#31934;&#24515;&#35774;&#35745;&#20102;&#21253;&#21547;&#22797;&#26434;&#24773;&#22659;&#21644;&#22823;&#22810;&#24102;&#26377;&#38544;&#21547;&#24694;&#24847;&#30340;&#30772;&#35299;&#26041;&#27861;&#30340;&#23545;&#25239;&#24615;&#25552;&#31034;&#12290;&#36890;&#36807;&#23545;17&#20010;&#20027;&#27969;LLMs&#36827;&#34892;&#25552;&#31034;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#27169;&#22411;&#30340;&#22238;&#24212;&#65292;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.06899v2 Announce Type: replace-cross  Abstract: The widespread adoption of large language models (LLMs) across various regions underscores the urgent need to evaluate their alignment with human values. Current benchmarks, however, fall short of effectively uncovering safety vulnerabilities in LLMs. Despite numerous models achieving high scores and 'topping the chart' in these evaluations, there is still a significant gap in LLMs' deeper alignment with human values and achieving genuine harmlessness. To this end, this paper proposes a value alignment benchmark named Flames, which encompasses both common harmlessness principles and a unique morality dimension that integrates specific Chinese values such as harmony. Accordingly, we carefully design adversarial prompts that incorporate complex scenarios and jailbreaking methods, mostly with implicit malice. By prompting 17 mainstream LLMs, we obtain model responses and rigorously annotate them for detailed evaluation. Our findin
&lt;/p&gt;</description></item><item><title>&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#26159;&#20943;&#23569;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#24187;&#35273;&#30340;&#20851;&#38190;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22522;&#20110;&#27010;&#29575;&#30830;&#23450;&#24615;&#21644;&#35821;&#20041;&#30830;&#23450;&#24615;&#30340;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#26356;&#39640;&#27700;&#24179;&#30340;&#30830;&#23450;&#24615;&#23545;&#24212;&#26356;&#20302;&#27700;&#24179;&#30340;&#24187;&#35273;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#30340;&#21709;&#24212;&#25490;&#24207;&#26041;&#27861;</title><link>https://arxiv.org/abs/2310.18794</link><description>&lt;p&gt;
&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#38477;&#20302;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#30340;&#34394;&#26500;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Sequence-Level Certainty Reduces Hallucination In Knowledge-Grounded Dialogue Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18794
&lt;/p&gt;
&lt;p&gt;
&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#26159;&#20943;&#23569;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#24187;&#35273;&#30340;&#20851;&#38190;&#65292;&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#22522;&#20110;&#27010;&#29575;&#30830;&#23450;&#24615;&#21644;&#35821;&#20041;&#30830;&#23450;&#24615;&#30340;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#65292;&#32467;&#26524;&#34920;&#26126;&#26356;&#39640;&#27700;&#24179;&#30340;&#30830;&#23450;&#24615;&#23545;&#24212;&#26356;&#20302;&#27700;&#24179;&#30340;&#24187;&#35273;&#65292;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#30340;&#21709;&#24212;&#25490;&#24207;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#20316;&#20026;&#22522;&#20110;&#30693;&#35782;&#30340;&#23545;&#35805;&#29983;&#25104;&#20013;&#34394;&#26500;&#24773;&#20917;&#30340;&#20849;&#21516;&#20027;&#39064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#24187;&#35273;&#27700;&#24179;&#19982;&#20004;&#31181;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65306;&#27010;&#29575;&#30830;&#23450;&#24615;&#21644;&#35821;&#20041;&#30830;&#23450;&#24615;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#27169;&#22411;&#21709;&#24212;&#20013;&#20004;&#31181;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#25552;&#39640;&#19982;&#34394;&#26500;&#27700;&#24179;&#30340;&#38477;&#20302;&#30456;&#20851;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;&#30830;&#23450;&#24615;&#30340;&#21709;&#24212;&#25490;&#24207;&#65288;CRR&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35299;&#30721;&#26102;&#30340;&#24187;&#35273;&#32531;&#35299;&#26041;&#27861;&#65292;&#26681;&#25454;&#23427;&#20204;&#30340;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#23545;&#21709;&#24212;&#20505;&#36873;&#36827;&#34892;&#25490;&#24207;&#65292;&#24182;&#36755;&#20986;&#20855;&#26377;&#26368;&#39640;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#31572;&#26696;&#12290;&#19982;&#25105;&#20204;&#20851;&#20110;&#24207;&#21015;&#32423;&#30830;&#23450;&#24615;&#30340;&#23450;&#20041;&#30456;&#19968;&#33268;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#31181;CRR&#26041;&#27861;&#65306;&#27010;&#29575;CRR&#65288;P-CRR&#65289;&#21644;&#35821;&#20041;CRR&#65288;S-CRR&#65289;&#12290;P-CRR&#20351;&#29992;&#25972;&#20010;&#24207;&#21015;&#30340;&#31639;&#26415;&#24179;&#22343;&#23545;&#21508;&#20010;&#21333;&#29420;&#25277;&#26679;&#30340;&#27169;&#22411;&#21709;&#24212;&#36827;&#34892;&#25490;&#21517;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18794v2 Announce Type: replace-cross  Abstract: In this work, we propose sequence-level certainty as a common theme over hallucination in Knowledge Grounded Dialogue Generation (KGDG). We explore the correlation between the level of hallucination and two types of sequence-level certainty: probabilistic certainty and semantic certainty. Empirical results reveal that a higher level of both types of sequence-level certainty in model responses is correlated with a lower level of hallucination. We further propose Certainty-based Response Ranking (CRR), a decoding-time hallucination mitigation method that ranks response candidates based on their sequence-level certainty and outputs the answer with the highest certainty level. Aligning with our definitions of sequence-level certainty, we design 2 types of CRR approaches: Probabilistic CRR (P-CRR) and Semantic CRR (S-CRR). P-CRR ranks individually sampled model responses using the arithmetic mean log-probability of the entire sequen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31354;&#38388;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#22312;&#34920;&#31034;&#21644;&#25512;&#29702;&#31354;&#38388;&#32467;&#26500;&#26102;&#30340;&#34920;&#29616;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20855;&#26377;&#25429;&#25417;&#31354;&#38388;&#32467;&#26500;&#38544;&#21547;&#29305;&#24449;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2310.14540</link><description>&lt;p&gt;
&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31354;&#38388;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Evaluating Spatial Understanding of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#31354;&#38388;&#32467;&#26500;&#30340;&#29702;&#35299;&#33021;&#21147;&#65292;&#21457;&#29616;LLMs&#22312;&#34920;&#31034;&#21644;&#25512;&#29702;&#31354;&#38388;&#32467;&#26500;&#26102;&#30340;&#34920;&#29616;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20855;&#26377;&#25429;&#25417;&#31354;&#38388;&#32467;&#26500;&#38544;&#21547;&#29305;&#24449;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#21482;&#30475;&#21040;&#25991;&#26412;&#65292;&#20294;&#19968;&#20123;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLM&#34920;&#31034;&#38544;&#21547;&#22320;&#25429;&#25417;&#20102;&#22320;&#38754;&#27010;&#24565;&#30340;&#20960;&#20010;&#26041;&#38754;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;LLM&#34920;&#31034;&#23545;&#19968;&#31181;&#29305;&#21035;&#26174;&#33879;&#30340;&#22522;&#30784;&#30693;&#35782; -- &#31354;&#38388;&#20851;&#31995;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#33258;&#28982;&#35821;&#35328;&#23548;&#33322;&#20219;&#21153;&#65292;&#35780;&#20272;&#20102;LLMs&#65292;&#29305;&#21035;&#26159;GPT-3.5-turbo&#12289;GPT-4 &#21644; Llama2 &#31995;&#21015;&#27169;&#22411;&#65292;&#34920;&#31034;&#21644;&#25512;&#29702;&#31354;&#38388;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;&#20219;&#21153;&#25581;&#31034;&#20102;LLM&#22312;&#19981;&#21516;&#31354;&#38388;&#32467;&#26500; (&#21253;&#25324;&#27491;&#26041;&#24418;&#12289;&#20845;&#36793;&#24418;&#21644;&#19977;&#35282;&#24418;&#26684;&#12289;&#29615;&#21644;&#26641;) &#19978;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#22312;&#24191;&#27867;&#30340;&#38169;&#35823;&#20998;&#26512;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#30340;&#38169;&#35823;&#21453;&#26144;&#20102;&#31354;&#38388;&#21644;&#38750;&#31354;&#38388;&#22240;&#32032;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;LLMs&#20284;&#20046;&#38544;&#21547;&#22320;&#25429;&#25417;&#20102;&#31354;&#38388;&#32467;&#26500;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#20294;&#20173;&#26377;&#25552;&#21319;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14540v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) show remarkable capabilities across a variety of tasks. Despite the models only seeing text in training, several recent studies suggest that LLM representations implicitly capture aspects of the underlying grounded concepts. Here, we explore LLM representations of a particularly salient kind of grounded knowledge -- spatial relationships. We design natural-language navigation tasks and evaluate the ability of LLMs, in particular GPT-3.5-turbo, GPT-4, and Llama2 series models, to represent and reason about spatial structures. These tasks reveal substantial variability in LLM performance across different spatial structures, including square, hexagonal, and triangular grids, rings, and trees. In extensive error analysis, we find that LLMs' mistakes reflect both spatial and non-spatial factors. These findings suggest that LLMs appear to capture certain aspects of spatial structure implicitly, but room f
&lt;/p&gt;</description></item><item><title>LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;</title><link>https://arxiv.org/abs/2309.13788</link><description>&lt;p&gt;
&#33021;&#22815;&#26816;&#27979;&#21040;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21527;?
&lt;/p&gt;
&lt;p&gt;
Can LLM-Generated Misinformation Be Detected?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13788
&lt;/p&gt;
&lt;p&gt;
LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#21487;&#33021;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#26356;&#38590;&#20197;&#26816;&#27979;&#65292;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#21487;&#33021;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#20135;&#29983;&#20102;&#28145;&#36828;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;LLMs&#65288;&#22914;ChatGPT&#65289;&#21487;&#33021;&#34987;&#21033;&#29992;&#26469;&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#65292;&#36825;&#32473;&#22312;&#32447;&#23433;&#20840;&#21644;&#20844;&#20247;&#20449;&#20219;&#24102;&#26469;&#20102;&#20005;&#37325;&#20851;&#20999;&#12290;&#19968;&#20010;&#22522;&#26412;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#65306;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#26159;&#21542;&#20250;&#27604;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#36896;&#25104;&#26356;&#22823;&#21361;&#23475;?&#25105;&#20204;&#25552;&#20986;&#20174;&#26816;&#27979;&#38590;&#24230;&#30340;&#35282;&#24230;&#26469;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#20010;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#20998;&#31867;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#21033;&#29992;LLMs&#29983;&#25104;&#34394;&#20551;&#20449;&#24687;&#30340;&#28508;&#22312;&#30495;&#23454;&#19990;&#30028;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#21644;&#39564;&#35777;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#35843;&#26597;&#65292;&#25105;&#20204;&#21457;&#29616;&#19982;&#20855;&#26377;&#30456;&#21516;&#35821;&#20041;&#30340;&#20154;&#31867;&#25776;&#20889;&#30340;&#34394;&#20551;&#20449;&#24687;&#30456;&#27604;&#65292;LLM&#29983;&#25104;&#30340;&#34394;&#20551;&#20449;&#24687;&#23545;&#20154;&#31867;&#21644;&#26816;&#27979;&#22120;&#26469;&#35828;&#26356;&#38590;&#26816;&#27979;&#65292;&#36825;&#34920;&#26126;&#23427;&#21487;&#33021;&#20855;&#26377;&#26356;&#20855;&#27450;&#39575;&#24615;&#30340;&#39118;&#26684;&#65292;&#28508;&#22312;&#22320;&#36896;&#25104;&#26356;&#22810;&#21361;&#23475;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13788v3 Announce Type: replace-cross  Abstract: The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery
&lt;/p&gt;</description></item><item><title>&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;ZO-RankSGD&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#20852;&#30340;&#20248;&#21270;&#25361;&#25112;&#65292;&#21363;&#21482;&#33021;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#26469;&#35780;&#20272;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#20272;&#35745;&#22120;&#26469;&#30830;&#23450;&#19979;&#38477;&#26041;&#21521;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36824;&#21487;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#21482;&#26377;&#23545;&#20110;&#22238;&#25253;&#25490;&#21517;&#30340;&#25490;&#21517;&#39044;&#27979;&#26102;&#12290;</title><link>https://arxiv.org/abs/2303.03751</link><description>&lt;p&gt;
&#38646;&#38454;&#20248;&#21270;&#36935;&#21040;&#20154;&#24037;&#21453;&#39304;&#65306;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#23454;&#29616;&#21487;&#35777;&#26126;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Zeroth-Order Optimization Meets Human Feedback: Provable Learning via Ranking Oracles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.03751
&lt;/p&gt;
&lt;p&gt;
&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;ZO-RankSGD&#35299;&#20915;&#20102;&#19968;&#20010;&#26032;&#20852;&#30340;&#20248;&#21270;&#25361;&#25112;&#65292;&#21363;&#21482;&#33021;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#26469;&#35780;&#20272;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;&#12290;&#35813;&#31639;&#27861;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#20272;&#35745;&#22120;&#26469;&#30830;&#23450;&#19979;&#38477;&#26041;&#21521;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;&#35813;&#31639;&#27861;&#36824;&#21487;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#21482;&#26377;&#23545;&#20110;&#22238;&#25253;&#25490;&#21517;&#30340;&#25490;&#21517;&#39044;&#27979;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#20852;&#30340;&#20248;&#21270;&#25361;&#25112;&#65292;&#20854;&#20013;&#28041;&#21450;&#21040;&#19968;&#20010;&#21482;&#33021;&#36890;&#36807;&#25490;&#21517;&#39044;&#27979;&#26469;&#35780;&#20272;&#30340;&#40657;&#30418;&#30446;&#26631;&#20989;&#25968;-&#36825;&#31181;&#24773;&#20917;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#32463;&#24120;&#36935;&#21040;&#65292;&#29305;&#21035;&#26159;&#24403;&#20989;&#25968;&#30001;&#20154;&#31867;&#35780;&#21028;&#21592;&#35780;&#20272;&#26102;&#12290;&#36825;&#31181;&#25361;&#25112;&#21463;&#21040;&#20102;&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#24037;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#21551;&#21457;&#65292;&#36825;&#26159;&#19968;&#31181;&#26368;&#36817;&#29992;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38646;&#38454;&#20248;&#21270;&#31639;&#27861;ZO-RankSGD&#26469;&#35299;&#20915;&#36825;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25490;&#21517;&#30340;&#38543;&#26426;&#20272;&#35745;&#22120;&#26469;&#30830;&#23450;&#19979;&#38477;&#26041;&#21521;&#65292;&#24182;&#20445;&#35777;&#25910;&#25947;&#21040;&#19968;&#20010;&#31283;&#23450;&#28857;&#12290;&#27492;&#22806;&#65292;ZO-RankSGD&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#22686;&#24378;&#23398;&#20064;&#20013;&#30340;&#31574;&#30053;&#20248;&#21270;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#24403;&#21482;&#26377;&#23545;&#20110;&#22238;&#25253;&#25490;&#21517;&#30340;&#25490;&#21517;&#39044;&#27979;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.03751v2 Announce Type: replace-cross  Abstract: In this study, we delve into an emerging optimization challenge involving a black-box objective function that can only be gauged via a ranking oracle-a situation frequently encountered in real-world scenarios, especially when the function is evaluated by human judges. Such challenge is inspired from Reinforcement Learning with Human Feedback (RLHF), an approach recently employed to enhance the performance of Large Language Models (LLMs) using human guidance. We introduce ZO-RankSGD, an innovative zeroth-order optimization algorithm designed to tackle this optimization problem, accompanied by theoretical assurances. Our algorithm utilizes a novel rank-based random estimator to determine the descent direction and guarantees convergence to a stationary point. Moreover, ZO-RankSGD is readily applicable to policy optimization problems in Reinforcement Learning (RL), particularly when only ranking oracles for the episode reward are a
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#32463;&#39564;&#22238;&#25918;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#36873;&#25321;&#24615;&#37325;&#22797;&#21033;&#29992;&#30456;&#20851;&#26679;&#26412;&#26469;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;PG-VRER&#12290;</title><link>https://arxiv.org/abs/2110.08902</link><description>&lt;p&gt;
&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#32463;&#39564;&#22238;&#25918;&#29992;&#20110;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Variance Reduction Based Experience Replay for Policy Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2110.08902
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#22522;&#20110;&#26041;&#24046;&#20943;&#23569;&#30340;&#32463;&#39564;&#22238;&#25918;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#36873;&#25321;&#24615;&#37325;&#22797;&#21033;&#29992;&#30456;&#20851;&#26679;&#26412;&#26469;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#65292;&#24182;&#26500;&#24314;&#20102;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;PG-VRER&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22797;&#26434;&#38543;&#26426;&#31995;&#32479;&#19978;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#26102;&#65292;&#26377;&#25928;&#21033;&#29992;&#21382;&#21490;&#26679;&#26412;&#20013;&#30340;&#20449;&#24687;&#20197;&#21152;&#36895;&#31574;&#30053;&#20248;&#21270;&#26159;&#24456;&#26377;&#24517;&#35201;&#30340;&#12290;&#20256;&#32479;&#30340;&#32463;&#39564;&#22238;&#25918;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#26159;&#23558;&#25152;&#26377;&#35266;&#27979;&#37117;&#35270;&#20026;&#30456;&#21516;&#65292;&#24573;&#30053;&#20102;&#23427;&#20204;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24046;&#20943;&#23569;&#32463;&#39564;&#22238;&#25918;&#65288;VRER&#65289;&#26694;&#26550;&#65292;&#23454;&#29616;&#23545;&#30456;&#20851;&#26679;&#26412;&#30340;&#36873;&#25321;&#24615;&#37325;&#22797;&#21033;&#29992;&#65292;&#20174;&#32780;&#25913;&#21892;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#12290;VRER&#20316;&#20026;&#19968;&#31181;&#36866;&#24212;&#24615;&#26041;&#27861;&#65292;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#19981;&#21516;&#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#20013;&#65292;&#26500;&#24314;&#20102;&#25105;&#20204;&#39640;&#25928;&#30340;&#31163;&#31574;&#30053;&#31639;&#27861;Policy Optimization with VRER (PG-VRER)&#12290;&#27492;&#22806;&#65292;&#25991;&#29486;&#20013;&#23545;&#32463;&#39564;&#22238;&#25918;&#26041;&#27861;&#32570;&#20047;&#20005;&#26684;&#30340;&#29702;&#35770;&#29702;&#35299;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#32771;&#34385;&#26679;&#26412;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2110.08902v3 Announce Type: replace-cross  Abstract: For reinforcement learning on complex stochastic systems, it is desirable to effectively leverage the information from historical samples collected in previous iterations to accelerate policy optimization. Classical experience replay, while effective, treats all observations uniformly, neglecting their relative importance. To address this limitation, we introduce a novel Variance Reduction Experience Replay (VRER) framework, enabling the selective reuse of relevant samples to improve policy gradient estimation. VRER, as an adaptable method that can seamlessly integrate with different policy optimization algorithms, forms the foundation of our sample-efficient off-policy algorithm known as Policy Optimization with VRER (PG-VRER). Furthermore, the lack of a rigorous theoretical understanding of the experience replay method in the literature motivates us to introduce a novel theoretical framework that accounts for sample dependenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65288;RAISE&#65289;&#65292;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#35268;&#21017;&#25277;&#35937;&#21644;&#36873;&#25321;&#65292;&#20197;&#35299;&#20915;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#29616;&#23454;&#30340;&#22330;&#26223;&#20013;&#23637;&#31034;&#20986;&#25277;&#35937;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2401.09966</link><description>&lt;p&gt;
&#26397;&#21521;&#29983;&#25104;&#24335;&#25277;&#35937;&#25512;&#29702;&#65306;&#36890;&#36807;&#35268;&#21017;&#25277;&#35937;&#21644;&#36873;&#25321;&#26469;&#23436;&#25104;Raven&#30340;&#28176;&#36827;&#30697;&#38453;
&lt;/p&gt;
&lt;p&gt;
Towards Generative Abstract Reasoning: Completing Raven's Progressive Matrix via Rule Abstraction and Selection. (arXiv:2401.09966v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09966
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65288;RAISE&#65289;&#65292;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#36827;&#34892;&#35268;&#21017;&#25277;&#35937;&#21644;&#36873;&#25321;&#65292;&#20197;&#35299;&#20915;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#38382;&#39064;&#65292;&#35813;&#27169;&#22411;&#33021;&#22815;&#22312;&#29616;&#23454;&#30340;&#22330;&#26223;&#20013;&#23637;&#31034;&#20986;&#25277;&#35937;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#36171;&#20104;&#26426;&#22120;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#26159;&#19968;&#20010;&#38271;&#26399;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;Raven&#30340;&#28176;&#36827;&#30697;&#38453;&#65288;RPM&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25506;&#32034;&#26426;&#22120;&#26234;&#33021;&#20013;&#30340;&#25277;&#35937;&#35270;&#35273;&#25512;&#29702;&#65292;&#27169;&#22411;&#38656;&#35201;&#29702;&#35299;&#28508;&#22312;&#30340;&#35268;&#21017;&#24182;&#20174;&#20505;&#36873;&#38598;&#20013;&#36873;&#25321;&#32570;&#22833;&#30340;&#21491;&#19979;&#22270;&#20687;&#26469;&#23436;&#25104;&#22270;&#20687;&#30697;&#38453;&#12290;&#21442;&#19982;&#32773;&#21487;&#20197;&#36890;&#36807;&#25512;&#26029;&#28508;&#22312;&#30340;&#23646;&#24615;&#21464;&#21270;&#35268;&#21017;&#21644;&#24819;&#35937;&#20219;&#24847;&#20301;&#32622;&#30340;&#32570;&#22833;&#22270;&#20687;&#23637;&#31034;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#24456;&#38590;&#22312;&#29616;&#23454;&#30340;RPM&#38382;&#39064;&#20013;&#23637;&#31034;&#20986;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#35268;&#21017;&#25277;&#35937;&#21644;&#36873;&#25321;&#65288;RAISE&#65289;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#35299;&#20915;&#31572;&#26696;&#29983;&#25104;&#38382;&#39064;&#12290;RAISE&#23558;&#22270;&#20687;&#23646;&#24615;&#32534;&#30721;&#20026;&#28508;&#22312;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#27010;&#24565;&#23558;&#28508;&#22312;&#35268;&#21017;&#20998;&#35299;&#25104;&#21407;&#23376;&#35268;&#21017;&#65292;&#36825;&#20123;&#21407;&#23376;&#35268;&#21017;&#34987;&#25277;&#35937;&#20026;&#20840;&#23616;&#21487;&#23398;&#20064;&#30340;&#21442;&#25968;&#12290;&#22312;&#29983;&#25104;&#31572;&#26696;&#26102;&#65292;RAISE&#36873;&#25321;...
&lt;/p&gt;
&lt;p&gt;
Endowing machines with abstract reasoning ability has been a long-term research topic in artificial intelligence. Raven's Progressive Matrix (RPM) is widely used to probe abstract visual reasoning in machine intelligence, where models need to understand the underlying rules and select the missing bottom-right images out of candidate sets to complete image matrices. The participators can display powerful reasoning ability by inferring the underlying attribute-changing rules and imagining the missing images at arbitrary positions. However, existing solvers can hardly manifest such an ability in realistic RPM problems. In this paper, we propose a conditional generative model to solve answer generation problems through Rule AbstractIon and SElection (RAISE) in the latent space. RAISE encodes image attributes as latent concepts and decomposes underlying rules into atomic rules by means of concepts, which are abstracted as global learnable parameters. When generating the answer, RAISE select
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#36827;&#34892;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#25429;&#25417;&#23431;&#23449;&#23398;&#20449;&#24687;&#21644;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2311.01588</link><description>&lt;p&gt;
&#38024;&#23545;&#22810;&#20010;&#25968;&#25454;&#38598;&#32422;&#26463;&#23431;&#23449;&#23398;&#21442;&#25968;&#30340;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptive Graph Neural Networks for Constraining Cosmological Parameters Across Multiple Data Sets. (arXiv:2311.01588v1 [astro-ph.CO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36890;&#36807;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#23545;&#23431;&#23449;&#23398;&#21442;&#25968;&#36827;&#34892;&#32422;&#26463;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#25429;&#25417;&#23431;&#23449;&#23398;&#20449;&#24687;&#21644;&#20351;&#29992;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#36827;&#34892;&#39046;&#22495;&#36866;&#24212;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#36739;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#19982;&#20381;&#36182;&#20110;&#25688;&#35201;&#32479;&#35745;&#37327;&#65288;&#22914;&#21151;&#29575;&#35889;&#65289;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20174;&#22797;&#26434;&#23431;&#23449;&#23398;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#20449;&#24687;&#26041;&#38754;&#34920;&#29616;&#26356;&#22909;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21516;&#27169;&#25311;&#22871;&#20214;&#20013;&#30340;&#23376;&#32593;&#26684;&#29289;&#29702;&#23454;&#29616;&#21644;&#25968;&#20540;&#36924;&#36817;&#30340;&#24046;&#24322;&#65292;&#27169;&#22411;&#22312;&#19968;&#20010;&#23431;&#23449;&#23398;&#27169;&#25311;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#21518;&#65292;&#22312;&#21478;&#19968;&#20010;&#27169;&#25311;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#20250;&#19979;&#38477;&#12290;&#21516;&#26679;&#65292;&#23545;&#20110;&#20219;&#20309;&#27169;&#25311;&#25968;&#25454;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#22312;&#24212;&#29992;&#20110;&#35266;&#27979;&#25968;&#25454;&#26102;&#20063;&#21487;&#33021;&#20986;&#29616;&#24615;&#33021;&#19979;&#38477;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#19981;&#21516;&#22871;&#20214;&#30340;CAMELS&#27700;&#21160;&#21147;&#23431;&#23449;&#23398;&#27169;&#25311;&#25968;&#25454;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#39046;&#22495;&#36866;&#24212;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DA-GNNs&#65289;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#21033;&#29992;GNNs&#65292;&#25105;&#20204;&#21487;&#20197;&#21033;&#29992;&#23427;&#20204;&#25429;&#25417;&#26469;&#33258;&#26143;&#31995;&#20998;&#24067;&#30340;&#32467;&#26500;&#26080;&#26631;&#24230;&#23431;&#23449;&#23398;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21253;&#25324;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#36866;&#37197;&#26368;&#22823;&#22343;&#20540;&#24046;&#24322;&#65288;MMD&#65289;&#65292;&#25105;&#20204;&#20351;&#27169;&#22411;&#33021;&#22815;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#20004;&#20010;&#27169;&#25311;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have been shown to outperform methods that rely on summary statistics, like the power spectrum, in extracting information from complex cosmological data sets. However, due to differences in the subgrid physics implementation and numerical approximations across different simulation suites, models trained on data from one cosmological simulation show a drop in performance when tested on another. Similarly, models trained on any of the simulations would also likely experience a drop in performance when applied to observational data. Training on data from two different suites of the CAMELS hydrodynamic cosmological simulations, we examine the generalization capabilities of Domain Adaptive Graph Neural Networks (DA-GNNs). By utilizing GNNs, we capitalize on their capacity to capture structured scale-free cosmological information from galaxy distributions. Moreover, by including unsupervised domain adaptation via Maximum Mean Discrepancy (MMD), we enable our models to ex
&lt;/p&gt;</description></item><item><title>BioImage.IO Chatbot &#26159;&#19968;&#20010;&#26681;&#25454;&#29992;&#25143;&#20010;&#24615;&#21270;&#38656;&#27714;&#25552;&#20379;&#31572;&#26696;&#30340;AI&#32842;&#22825;&#21161;&#25163;&#65292;&#36890;&#36807;&#27719;&#38598;&#21644;&#35299;&#37322;&#22810;&#20010;&#25968;&#25454;&#24211;&#12289;&#24037;&#20855;&#25991;&#26723;&#21644;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#31038;&#21306;&#36129;&#29486;&#30340;&#30693;&#35782;&#24211;&#21644;&#20248;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#20026;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#30340;&#20351;&#29992;&#32773;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20307;&#39564;&#65292;&#20026;&#21487;&#35775;&#38382;&#30340;&#31185;&#23398;&#30740;&#31350;&#35774;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2310.18351</link><description>&lt;p&gt;
BioImage.IO Chatbot: &#19968;&#20010;&#20197;&#31038;&#21306;&#30693;&#35782;&#24211;&#22686;&#24378;&#30340;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#20010;&#20154;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
BioImage.IO Chatbot: A Personalized Assistant for BioImage Analysis Augmented by Community Knowledge Base. (arXiv:2310.18351v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18351
&lt;/p&gt;
&lt;p&gt;
BioImage.IO Chatbot &#26159;&#19968;&#20010;&#26681;&#25454;&#29992;&#25143;&#20010;&#24615;&#21270;&#38656;&#27714;&#25552;&#20379;&#31572;&#26696;&#30340;AI&#32842;&#22825;&#21161;&#25163;&#65292;&#36890;&#36807;&#27719;&#38598;&#21644;&#35299;&#37322;&#22810;&#20010;&#25968;&#25454;&#24211;&#12289;&#24037;&#20855;&#25991;&#26723;&#21644;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#31038;&#21306;&#36129;&#29486;&#30340;&#30693;&#35782;&#24211;&#21644;&#20248;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#20026;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#30340;&#20351;&#29992;&#32773;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20307;&#39564;&#65292;&#20026;&#21487;&#35775;&#38382;&#30340;&#31185;&#23398;&#30740;&#31350;&#35774;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#25193;&#23637;&#30340;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#26223;&#35266;&#32473;&#19987;&#23478;&#21644;&#26032;&#26469;&#32773;&#37117;&#24102;&#26469;&#20102;&#23548;&#33322;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#25628;&#32034;&#26041;&#27861;&#22312;&#36825;&#20010;&#22797;&#26434;&#29615;&#22659;&#20013;&#24120;&#24120;&#26080;&#27861;&#25552;&#20379;&#24110;&#21161;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BioImage.IO Chatbot&#65292;&#19968;&#20010;&#20026;&#29983;&#29289;&#22270;&#20687;&#31038;&#21306;&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23545;&#35805;&#21161;&#25163;&#12290;&#36825;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#24314;&#31435;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#32858;&#21512;&#21644;&#35299;&#37322;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#24211;&#12289;&#29305;&#23450;&#24037;&#20855;&#25991;&#26723;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#31572;&#26696;&#12290;&#36890;&#36807;&#31038;&#21306;&#36129;&#29486;&#30340;&#30693;&#35782;&#24211;&#21644;&#32463;&#36807;&#20248;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;BioImage.IO Chatbot &#19981;&#20165;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#20114;&#21160;&#65292;&#36824;&#25552;&#20379;&#20016;&#23500;&#30340;&#30693;&#35782;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20307;&#39564;&#12290;&#23427;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#29983;&#29289;&#23398;&#23478;&#12289;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24072;&#21644;&#24320;&#21457;&#32773;&#23548;&#33322;&#21644;&#21033;&#29992;&#20808;&#36827;&#30340;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#30340;&#26041;&#24335;&#65292;&#20026;&#31038;&#21306;&#39537;&#21160;&#30340;&#21487;&#35775;&#38382;&#31185;&#23398;&#30740;&#31350;&#26641;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapidly expanding landscape of bioimage analysis tools presents a navigational challenge for both experts and newcomers. Traditional search methods often fall short in assisting users in this complex environment. To address this, we introduce the BioImage$.$IO Chatbot, an AI-driven conversational assistant tailored for the bioimage community. Built upon large language models, this chatbot provides personalized, context-aware answers by aggregating and interpreting information from diverse databases, tool-specific documentation, and structured data sources. Enhanced by a community-contributed knowledge base and fine-tuned retrieval methods, the BioImage$.$IO Chatbot offers not just a personalized interaction but also a knowledge-enriched, context-aware experience. It fundamentally transforms the way biologists, bioimage analysts, and developers navigate and utilize advanced bioimage analysis tools, setting a new standard for community-driven, accessible scientific research.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#38899;&#39640;&#12289;&#21644;&#24358;&#21644;&#40723;&#20048;&#31561;&#22266;&#26377;&#38899;&#20048;&#35821;&#35328;&#30340;&#30452;&#25509;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#29983;&#25104;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#27604;&#21407;&#22987;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23569;&#20110;4%&#12290;</title><link>http://arxiv.org/abs/2310.17162</link><description>&lt;p&gt;
&#22522;&#20110;&#20869;&#23481;&#30340;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#30340;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Content-based Controls For Music Large Language Modeling. (arXiv:2310.17162v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17162
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#12290;&#36890;&#36807;&#23545;&#38899;&#39640;&#12289;&#21644;&#24358;&#21644;&#40723;&#20048;&#31561;&#22266;&#26377;&#38899;&#20048;&#35821;&#35328;&#30340;&#30452;&#25509;&#25511;&#21046;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#29983;&#25104;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#27604;&#21407;&#22987;&#27169;&#22411;&#30340;&#21442;&#25968;&#25968;&#37327;&#23569;&#20110;4%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22312;&#38899;&#20048;&#38899;&#39057;&#39046;&#22495;&#20986;&#29616;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#36805;&#36895;&#22686;&#38271;&#12290;&#36825;&#20123;&#27169;&#22411;&#20351;&#24471;&#33021;&#22815;&#36827;&#34892;&#39640;&#36136;&#37327;&#38899;&#20048;&#30340;&#31471;&#21040;&#31471;&#29983;&#25104;&#65292;&#24182;&#19988;&#19968;&#20123;&#27169;&#22411;&#21487;&#20197;&#20351;&#29992;&#25991;&#26412;&#25551;&#36848;&#36827;&#34892;&#26465;&#20214;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#25991;&#26412;&#22312;&#38899;&#20048;&#19978;&#30340;&#25511;&#21046;&#33021;&#21147;&#26412;&#36136;&#19978;&#26159;&#26377;&#38480;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#21482;&#33021;&#36890;&#36807;&#20803;&#25968;&#25454;&#65288;&#22914;&#27468;&#25163;&#21644;&#20048;&#22120;&#65289;&#25110;&#39640;&#32423;&#34920;&#31034;&#65288;&#22914;&#27969;&#27966;&#21644;&#24773;&#24863;&#65289;&#38388;&#25509;&#22320;&#25551;&#36848;&#38899;&#20048;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36827;&#19968;&#27493;&#25552;&#20379;&#23545;&#38899;&#39640;&#12289;&#21644;&#24358;&#21644;&#40723;&#20048;&#31561;&#22266;&#26377;&#38899;&#20048;&#35821;&#35328;&#30340;&#30452;&#25509;&#21644;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Coco-Mulla&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#38899;&#20048;&#22823;&#35821;&#35328;&#24314;&#27169;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#25511;&#21046;&#26041;&#27861;&#12290;&#23427;&#20351;&#29992;&#20102;&#38024;&#23545;&#22522;&#20110;Transformer&#30340;&#38899;&#39057;&#27169;&#22411;&#37327;&#36523;&#23450;&#21046;&#30340;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20302;&#36164;&#28304;&#21322;&#30417;&#30563;&#23398;&#20064;&#20013;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#38899;&#20048;&#29983;&#25104;&#65292;&#30456;&#27604;&#21407;&#22987;&#27169;&#22411;&#65292;&#21442;&#25968;&#35843;&#20248;&#30340;&#27604;&#20363;&#19981;&#21040;4%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a rapid growth of large-scale language models in the domain of music audio. Such models enable end-to-end generation of higher-quality music, and some allow conditioned generation using text descriptions. However, the control power of text controls on music is intrinsically limited, as they can only describe music indirectly through meta-data (such as singers and instruments) or high-level representations (such as genre and emotion). We aim to further equip the models with direct and content-based controls on innate music languages such as pitch, chords and drum track. To this end, we contribute Coco-Mulla, a content-based control method for music large language modeling. It uses a parameter-efficient fine-tuning (PEFT) method tailored for Transformer-based audio models. Experiments show that our approach achieved high-quality music generation with low-resource semi-supervised learning, tuning with less than 4% parameters compared to the original model and t
&lt;/p&gt;</description></item><item><title>&#23556;&#39057;&#25351;&#32441;&#35782;&#21035;&#25216;&#26415;&#36890;&#36807;&#21033;&#29992;&#30828;&#20214;&#32570;&#38519;&#22312;&#29289;&#29702;&#23618;&#39564;&#35777;&#26080;&#32447;&#35774;&#22791;&#30340;&#36523;&#20221;&#65292;&#28982;&#32780;&#19982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#25972;&#21512;&#21644;&#23454;&#38469;&#22330;&#26223;&#24212;&#29992;&#24102;&#26469;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#20123;&#25361;&#25112;&#24182;&#25351;&#20986;&#24403;&#21069;&#38459;&#30861;&#23556;&#39057;&#25351;&#32441;&#35782;&#21035;&#23454;&#38469;&#37096;&#32626;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2310.16406</link><description>&lt;p&gt;
&#23556;&#39057;&#25351;&#32441;&#35782;&#21035;&#30340;&#25361;&#25112;&#65306;&#20174;&#25968;&#25454;&#25910;&#38598;&#21040;&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Challenges of Radio Frequency Fingerprinting: From Data Collection to Deployment. (arXiv:2310.16406v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16406
&lt;/p&gt;
&lt;p&gt;
&#23556;&#39057;&#25351;&#32441;&#35782;&#21035;&#25216;&#26415;&#36890;&#36807;&#21033;&#29992;&#30828;&#20214;&#32570;&#38519;&#22312;&#29289;&#29702;&#23618;&#39564;&#35777;&#26080;&#32447;&#35774;&#22791;&#30340;&#36523;&#20221;&#65292;&#28982;&#32780;&#19982;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#25972;&#21512;&#21644;&#23454;&#38469;&#22330;&#26223;&#24212;&#29992;&#24102;&#26469;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#36825;&#20123;&#25361;&#25112;&#24182;&#25351;&#20986;&#24403;&#21069;&#38459;&#30861;&#23556;&#39057;&#25351;&#32441;&#35782;&#21035;&#23454;&#38469;&#37096;&#32626;&#30340;&#38382;&#39064;&#65292;&#24182;&#25506;&#35752;&#20102;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23556;&#39057;&#25351;&#32441;&#35782;&#21035;&#65288;RFF&#65289;&#25216;&#26415;&#25215;&#35834;&#22522;&#20110;&#21046;&#36896;&#36807;&#31243;&#20013;&#24341;&#20837;&#30340;&#22266;&#26377;&#30828;&#20214;&#32570;&#38519;&#26469;&#22312;&#29289;&#29702;&#23618;&#23545;&#26080;&#32447;&#35774;&#22791;&#36827;&#34892;&#36523;&#20221;&#39564;&#35777;&#12290;&#36825;&#20123;&#23556;&#39057;&#21457;&#23556;&#22120;&#30340;&#32570;&#38519;&#21453;&#26144;&#22312;&#31354;&#20013;&#20449;&#21495;&#20013;&#65292;&#20351;&#25509;&#25910;&#22120;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#23556;&#39057;&#21457;&#23556;&#28304;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#36827;&#27493;&#65292;&#25552;&#39640;&#20102;RFF&#31995;&#32479;&#25552;&#21462;&#21644;&#23398;&#20064;&#26500;&#25104;&#35774;&#22791;&#29305;&#23450;&#25351;&#32441;&#30340;&#22797;&#26434;&#29305;&#24449;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23558;DL&#25216;&#26415;&#19982;RFF&#38598;&#25104;&#24182;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#36816;&#34892;&#31995;&#32479;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#12290;&#26412;&#25991;&#22312;&#32771;&#34385;DL-based RFF&#31995;&#32479;&#30340;&#19977;&#20010;&#21442;&#32771;&#38454;&#27573;&#65288;&#25968;&#25454;&#25910;&#38598;&#21644;&#39044;&#22788;&#29702;&#65292;&#35757;&#32451;&#65292;&#26368;&#21518;&#37096;&#32626;&#65289;&#30340;&#22522;&#30784;&#19978;&#65292;&#35782;&#21035;&#21644;&#20998;&#26512;&#20102;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#25351;&#20986;&#20102;&#30446;&#21069;&#38459;&#30861;RFF&#23454;&#38469;&#37096;&#32626;&#30340;&#29616;&#26377;&#38382;&#39064;&#65292;&#24182;&#35752;&#35770;&#20102;&#26377;&#21069;&#36884;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Radio Frequency Fingerprinting (RFF) techniques promise to authenticate wireless devices at the physical layer based on inherent hardware imperfections introduced during manufacturing. Such RF transmitter imperfections are reflected into over-the-air signals, allowing receivers to accurately identify the RF transmitting source. Recent advances in Machine Learning, particularly in Deep Learning (DL), have improved the ability of RFF systems to extract and learn complex features that make up the device-specific fingerprint. However, integrating DL techniques with RFF and operating the system in real-world scenarios presents numerous challenges. This article identifies and analyzes these challenges while considering the three reference phases of any DL-based RFF system: (i) data collection and preprocessing, (ii) training, and finally, (iii) deployment. Our investigation points out the current open problems that prevent real deployment of RFF while discussing promising future directions, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#36870;&#21512;&#25104;&#20219;&#21153;&#22312;&#23454;&#39564;&#23460;&#25191;&#34892;&#21487;&#34892;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#36807;&#31243;&#30340;&#34920;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Retro-fallback &#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#22823;&#21270;&#23454;&#39564;&#23460;&#21487;&#25191;&#34892;&#30340;&#21512;&#25104;&#35745;&#21010;&#30340;&#27010;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.09270</link><description>&lt;p&gt;
Retro-fallback: &#38754;&#21521;&#19981;&#30830;&#23450;&#19990;&#30028;&#30340;&#36870;&#21512;&#25104;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Retro-fallback: retrosynthetic planning in an uncertain world. (arXiv:2310.09270v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.09270
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36870;&#21512;&#25104;&#20219;&#21153;&#22312;&#23454;&#39564;&#23460;&#25191;&#34892;&#21487;&#34892;&#24615;&#30340;&#19981;&#30830;&#23450;&#24615;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#36807;&#31243;&#30340;&#34920;&#36848;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; Retro-fallback &#30340;&#36138;&#23146;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#26368;&#22823;&#21270;&#23454;&#39564;&#23460;&#21487;&#25191;&#34892;&#30340;&#21512;&#25104;&#35745;&#21010;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21512;&#25104;&#26159;&#36890;&#36807;&#25552;&#20986;&#19968;&#31995;&#21015;&#21270;&#23398;&#21453;&#24212;&#20174;&#26356;&#31616;&#21333;&#12289;&#21487;&#36141;&#20080;&#30340;&#20998;&#23376;&#21019;&#24314;&#25152;&#38656;&#20998;&#23376;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20123;&#31639;&#27861;&#26469;&#23547;&#25214;&#19968;&#31995;&#21015;&#24230;&#37327;&#25351;&#26631;&#65288;&#20363;&#22914;&#26368;&#30701;&#36335;&#24452;&#12289;&#26368;&#20302;&#25104;&#26412;&#65289;&#30340;&#26368;&#20248;&#35299;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#24573;&#35270;&#20102;&#25105;&#20204;&#23545;&#21487;&#33021;&#21453;&#24212;&#31354;&#38388;&#30340;&#19981;&#23436;&#20840;&#20102;&#35299;&#65292;&#36825;&#24847;&#21619;&#30528;&#31639;&#27861;&#29983;&#25104;&#30340;&#35745;&#21010;&#21487;&#33021;&#22312;&#23454;&#39564;&#23460;&#20013;&#26080;&#27861;&#23454;&#26045;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#36807;&#31243;&#30340;&#36870;&#21512;&#25104;&#26032;&#39062;&#34920;&#36848;&#65292;&#20197;&#32771;&#34385;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36138;&#23146;&#31639;&#27861;&#31216;&#20026; Retro-fallback&#65292;&#26368;&#22823;&#21270;&#33267;&#23569;&#26377;&#19968;&#31181;&#21512;&#25104;&#35745;&#21010;&#33021;&#22312;&#23454;&#39564;&#23460;&#20013;&#25191;&#34892;&#30340;&#27010;&#29575;&#12290;&#20351;&#29992;&#20223;&#30495;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#35777;&#26126; Retro-fallback &#36890;&#24120;&#29983;&#25104;&#27604;&#27969;&#34892;&#30340; MCTS &#21644; retro* &#31639;&#27861;&#26356;&#22909;&#30340;&#19968;&#32452;&#21512;&#25104;&#35745;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis is the task of proposing a series of chemical reactions to create a desired molecule from simpler, buyable molecules. While previous works have proposed algorithms to find optimal solutions for a range of metrics (e.g. shortest, lowest-cost), these works generally overlook the fact that we have imperfect knowledge of the space of possible reactions, meaning plans created by the algorithm may not work in a laboratory. In this paper we propose a novel formulation of retrosynthesis in terms of stochastic processes to account for this uncertainty. We then propose a novel greedy algorithm called retro-fallback which maximizes the probability that at least one synthesis plan can be executed in the lab. Using in-silico benchmarks we demonstrate that retro-fallback generally produces better sets of synthesis plans than the popular MCTS and retro* algorithms.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MLAgentBench&#65292;&#19968;&#20010;&#29992;&#20110;&#23545;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;ML&#20219;&#21153;&#22871;&#20214;&#65292;&#20195;&#29702;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#25805;&#20316;&#65292;&#20174;&#32780;&#36816;&#34892;&#23454;&#39564;&#12289;&#20998;&#26512;&#32467;&#26524;&#24182;&#20462;&#25913;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#20195;&#30721;&#12290;&#36825;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26500;&#24314;&#21644;&#35780;&#20272;&#33021;&#22815;&#25191;&#34892;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#30340;AI&#30740;&#31350;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2310.03302</link><description>&lt;p&gt;
&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Benchmarking Large Language Models As AI Research Agents. (arXiv:2310.03302v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.03302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MLAgentBench&#65292;&#19968;&#20010;&#29992;&#20110;&#23545;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;ML&#20219;&#21153;&#22871;&#20214;&#65292;&#20195;&#29702;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#25805;&#20316;&#65292;&#20174;&#32780;&#36816;&#34892;&#23454;&#39564;&#12289;&#20998;&#26512;&#32467;&#26524;&#24182;&#20462;&#25913;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#20195;&#30721;&#12290;&#36825;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#26500;&#24314;&#21644;&#35780;&#20272;&#33021;&#22815;&#25191;&#34892;&#38271;&#26399;&#30446;&#26631;&#20219;&#21153;&#30340;AI&#30740;&#31350;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#23454;&#39564;&#28041;&#21450;&#21019;&#24314;&#20551;&#35774;&#12289;&#35774;&#35745;&#23454;&#39564;&#12289;&#36816;&#34892;&#23454;&#39564;&#21644;&#20998;&#26512;&#32467;&#26524;&#30340;&#36845;&#20195;&#36807;&#31243;&#12290;&#25105;&#20204;&#33021;&#21542;&#26500;&#24314;AI&#30740;&#31350;&#20195;&#29702;&#26469;&#25191;&#34892;&#36825;&#20123;&#38271;&#26399;&#30446;&#26631;&#30340;&#20219;&#21153;&#21602;&#65311;&#20026;&#20102;&#26397;&#30528;&#22312;&#27492;&#31867;&#24320;&#25918;&#24615;&#20915;&#31574;&#20219;&#21153;&#19978;&#26500;&#24314;&#21644;&#35780;&#20272;&#30740;&#31350;&#20195;&#29702;&#30340;&#30446;&#26631;&#36808;&#20986;&#19968;&#27493;&#65292;&#25105;&#20204;&#30528;&#30524;&#20110;&#26426;&#22120;&#23398;&#20064;&#24037;&#31243;&#38382;&#39064;&#65306;&#32473;&#23450;&#19968;&#20010;&#20219;&#21153;&#25551;&#36848;&#21644;&#25968;&#25454;&#38598;&#65292;&#26500;&#24314;&#19968;&#20010;&#39640;&#24615;&#33021;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MLAgentBench&#65292;&#19968;&#20010;&#29992;&#20110;&#23545;AI&#30740;&#31350;&#20195;&#29702;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#30340;ML&#20219;&#21153;&#22871;&#20214;&#12290;&#20195;&#29702;&#21487;&#20197;&#25191;&#34892;&#35835;&#20889;&#25991;&#20214;&#12289;&#25191;&#34892;&#20195;&#30721;&#21644;&#26816;&#26597;&#36755;&#20986;&#31561;&#21160;&#20316;&#12290;&#36890;&#36807;&#36825;&#20123;&#21160;&#20316;&#65292;&#20195;&#29702;&#21487;&#20197;&#36816;&#34892;&#23454;&#39564;&#12289;&#20998;&#26512;&#32467;&#26524;&#65292;&#24182;&#20462;&#25913;&#25972;&#20010;&#26426;&#22120;&#23398;&#20064;&#27969;&#31243;&#30340;&#20195;&#30721;&#65292;&#22914;&#25968;&#25454;&#22788;&#29702;&#12289;&#26550;&#26500;&#12289;&#35757;&#32451;&#36807;&#31243;&#31561;&#12290;&#28982;&#21518;&#65292;&#22522;&#20934;&#27979;&#35797;&#33258;&#21160;&#23458;&#35266;&#22320;&#35780;&#20272;&#20195;&#29702;&#22312;&#19982;&#24615;&#33021;&#21644;&#25928;&#29575;&#30456;&#20851;&#30340;&#21508;&#31181;&#25351;&#26631;&#19978;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;LLM-
&lt;/p&gt;
&lt;p&gt;
Scientific experimentation involves an iterative process of creating hypotheses, designing experiments, running experiments, and analyzing the results. Can we build AI research agents to perform these long-horizon tasks? To take a step towards building and evaluating research agents on such open-ended decision-making tasks, we focus on the problem of machine learning engineering: given a task description and a dataset, build a high-performing model. In this paper, we propose MLAgentBench, a suite of ML tasks for benchmarking AI research agents. Agents can perform actions like reading/writing files, executing code, and inspecting outputs. With these actions, agents could run experiments, analyze the results, and modify the code of entire machine learning pipelines, such as data processing, architecture, training processes, etc. The benchmark then automatically evaluates the agent's performance objectively over various metrics related to performance and efficiency. We also design an LLM-
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#20687;-&#25991;&#26412;&#22270;&#21305;&#37197;&#26469;&#24357;&#21512;&#39046;&#22495;&#24046;&#36317;&#65292;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#20197;&#23454;&#29616;&#22312;&#26410;&#35265;&#36807;&#39046;&#22495;&#19978;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.02692</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#20687;-&#25991;&#26412;&#22270;&#21305;&#37197;&#26469;&#24357;&#21512;&#39046;&#22495;&#24046;&#36317;
&lt;/p&gt;
&lt;p&gt;
Bridging the Domain Gap by Clustering-based Image-Text Graph Matching. (arXiv:2310.02692v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02692
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#32858;&#31867;&#30340;&#22270;&#20687;-&#25991;&#26412;&#22270;&#21305;&#37197;&#26469;&#24357;&#21512;&#39046;&#22495;&#24046;&#36317;&#65292;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#20197;&#23454;&#29616;&#22312;&#26410;&#35265;&#36807;&#39046;&#22495;&#19978;&#30340;&#33391;&#22909;&#27867;&#21270;&#33021;&#21147;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#34920;&#31034;&#23545;&#20110;&#35757;&#32451;&#21487;&#20197;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30446;&#26631;&#20219;&#21153;&#39046;&#22495;&#30340;&#27169;&#22411;&#38750;&#24120;&#37325;&#35201;&#12290;&#25991;&#26412;&#25551;&#36848;&#26412;&#36523;&#21253;&#21547;&#27010;&#24565;&#30340;&#35821;&#20041;&#32467;&#26500;&#65292;&#36825;&#26679;&#30340;&#36741;&#21161;&#35821;&#20041;&#32447;&#32034;&#21487;&#20197;&#29992;&#20316;&#39046;&#22495;&#27010;&#25324;&#38382;&#39064;&#30340;&#26377;&#25928;&#26530;&#32445;&#23884;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#22810;&#27169;&#24577;&#22270;&#20687;&#21644;&#25991;&#26412;&#34701;&#21512;&#30340;&#22270;&#34920;&#31034;&#26469;&#33719;&#24471;&#22312;&#23616;&#37096;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#31526;&#20043;&#38388;&#32771;&#34385;&#20869;&#22312;&#35821;&#20041;&#32467;&#26500;&#30340;&#39046;&#22495;&#19981;&#21464;&#26530;&#32445;&#23884;&#20837;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;(i)&#29992;&#22270;&#34920;&#31034;&#22270;&#20687;&#21644;&#25991;&#26412;&#25551;&#36848;&#65292;&#20197;&#21450;(ii)&#23558;&#22522;&#20110;&#22270;&#20687;&#33410;&#28857;&#29305;&#24449;&#30340;&#32858;&#31867;&#21644;&#21305;&#37197;&#24212;&#29992;&#21040;&#25991;&#26412;&#22270;&#20013;&#65292;&#26469;&#23398;&#20064;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;&#25105;&#20204;&#20351;&#29992;&#22823;&#35268;&#27169;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#22914;CUB-DG&#21644;DomainBed&#65289;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#36798;&#21040;&#19982;&#25110;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#23558;&#22312;&#20986;&#29256;&#21518;&#20844;&#24320;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning domain-invariant representations is important to train a model that can generalize well to unseen target task domains. Text descriptions inherently contain semantic structures of concepts and such auxiliary semantic cues can be used as effective pivot embedding for domain generalization problems. Here, we use multimodal graph representations, fusing images and text, to get domain-invariant pivot embeddings by considering the inherent semantic structure between local images and text descriptors. Specifically, we aim to learn domain-invariant features by (i) representing the image and text descriptions with graphs, and by (ii) clustering and matching the graph-based image node features into textual graphs simultaneously. We experiment with large-scale public datasets, such as CUB-DG and DomainBed, and our model achieves matched or better state-of-the-art performance on these datasets. Our code will be publicly available upon publication.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#30340;&#38750;&#21807;&#19968;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;SPE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#20540;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.02579</link><description>&lt;p&gt;
&#20851;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#34920;&#36798;&#20301;&#32622;&#32534;&#30721;&#30340;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Stability of Expressive Positional Encodings for Graph Neural Networks. (arXiv:2310.02579v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02579
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#38024;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#30340;&#38750;&#21807;&#19968;&#24615;&#21644;&#19981;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65288;SPE&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#29305;&#24449;&#20540;&#23545;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#65292;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35774;&#35745;&#26377;&#25928;&#30340;&#22270;&#20301;&#32622;&#32534;&#30721;&#23545;&#26500;&#24314;&#24378;&#22823;&#30340;&#22270;&#36716;&#25442;&#22120;&#21644;&#22686;&#24378;&#28040;&#24687;&#20256;&#36882;&#22270;&#31070;&#32463;&#32593;&#32476;&#38750;&#24120;&#20851;&#38190;&#12290;&#23613;&#31649;&#24191;&#27867;&#20351;&#29992;&#65292;&#20351;&#29992;&#25289;&#26222;&#25289;&#26031;&#29305;&#24449;&#21521;&#37327;&#20316;&#20026;&#20301;&#32622;&#32534;&#30721;&#38754;&#20020;&#20004;&#20010;&#26681;&#26412;&#24615;&#25361;&#25112;&#65306;&#65288;1&#65289;\emph{&#38750;&#21807;&#19968;&#24615;}&#65306;&#21516;&#19968;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#23384;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#29305;&#24449;&#20998;&#35299;&#65292;&#20197;&#21450;&#65288;2&#65289;\emph{&#19981;&#31283;&#23450;&#24615;}&#65306;&#23545;&#25289;&#26222;&#25289;&#26031;&#30697;&#38453;&#30340;&#24494;&#23567;&#25200;&#21160;&#21487;&#33021;&#23548;&#33268;&#23436;&#20840;&#19981;&#21516;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#20174;&#32780;&#23548;&#33268;&#20301;&#32622;&#32534;&#30721;&#30340;&#19981;&#21487;&#39044;&#27979;&#24615;&#21464;&#21270;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#23581;&#35797;&#35299;&#20915;&#38750;&#21807;&#19968;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#24573;&#35270;&#20102;&#31283;&#23450;&#24615;&#65292;&#23548;&#33268;&#22312;&#26410;&#35265;&#36807;&#30340;&#22270;&#32467;&#26500;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19981;&#31283;&#23450;&#24615;&#30340;&#21407;&#22240;&#26159;&#29305;&#24449;&#31354;&#38388;&#30340;"&#30828;&#20998;&#21106;"&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31283;&#23450;&#19988;&#34920;&#36798;&#20016;&#23500;&#30340;&#20301;&#32622;&#32534;&#30721;&#65288;SPE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22788;&#29702;&#29305;&#24449;&#21521;&#37327;&#30340;&#26550;&#26500;&#65292;&#21033;&#29992;&#29305;&#24449;&#20540;&#23558;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;"&#36719;&#20998;&#21106;"&#12290;SPE&#26159;&#39318;&#20010;&#65288;1&#65289;&#21487;&#35777;&#26126;&#31283;&#23450;&#30340;&#26550;&#26500;&#65292;&#20197;&#21450;&#65288;2&#65289;&#26222;&#36866;&#22320;&#25552;&#21319;&#22270;&#32467;&#26500;&#27867;&#21270;&#24615;&#33021;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Designing effective positional encodings for graphs is key to building powerful graph transformers and enhancing message-passing graph neural networks. Although widespread, using Laplacian eigenvectors as positional encodings faces two fundamental challenges: (1) \emph{Non-uniqueness}: there are many different eigendecompositions of the same Laplacian, and (2) \emph{Instability}: small perturbations to the Laplacian could result in completely different eigenspaces, leading to unpredictable changes in positional encoding.  Despite many attempts to address non-uniqueness, most methods overlook stability, leading to poor generalization on unseen graph structures. We identify the cause of instability to be a "hard partition" of eigenspaces. Hence, we introduce Stable and Expressive Positional Encodings (SPE), an architecture for processing eigenvectors that uses eigenvalues to "softly partition" eigenspaces. SPE is the first architecture that is (1) provably stable, and (2) universally exp
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#21487;&#20351;LLM&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#65292;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20154;&#31867;&#30417;&#30563;&#12290;</title><link>http://arxiv.org/abs/2310.01444</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#27969;&#20351;LLM&#20195;&#29702;&#36866;&#24212;&#29615;&#22659;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adapting LLM Agents Through Communication. (arXiv:2310.01444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01444
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#35813;&#26041;&#27861;&#21487;&#20351;LLM&#20195;&#29702;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#65292;&#20197;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#36807;&#22810;&#20154;&#31867;&#30417;&#30563;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#20102;&#20154;&#31867;&#21270;&#20195;&#29702;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#24110;&#21161;&#36825;&#20123;&#20195;&#29702;&#22312;&#27809;&#26377;&#24191;&#27867;&#20154;&#31867;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#36866;&#24212;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23398;&#20064;&#36890;&#20449;&#65288;LTC&#65289;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;LLM&#20195;&#29702;&#33021;&#22815;&#36890;&#36807;&#19982;&#29615;&#22659;&#21644;&#20854;&#20182;&#20195;&#29702;&#30340;&#20132;&#20114;&#19981;&#26029;&#25913;&#36827;&#12290;&#36890;&#36807;&#36845;&#20195;&#25506;&#32034;&#21644;PPO&#35757;&#32451;&#65292;LTC&#20351;&#20195;&#29702;&#33021;&#22815;&#23558;&#30701;&#26399;&#32463;&#39564;&#34701;&#20837;&#38271;&#26399;&#35760;&#24518;&#12290;&#20026;&#20102;&#20248;&#21270;&#29305;&#23450;&#20219;&#21153;&#30340;&#20195;&#29702;&#20132;&#20114;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#31181;&#32467;&#26500;&#21270;&#30340;&#36890;&#20449;&#27169;&#24335;&#65306;&#29420;&#30333;&#65292;&#23545;&#35805;&#65292;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Recent advancements in large language models (LLMs) have shown potential for human-like agents. To help these agents adapt to new tasks without extensive human supervision, we propose the Learning through Communication (LTC) paradigm, a novel training approach enabling LLM agents to improve continuously through interactions with their environments and other agents. Through iterative exploration and PPO training, LTC empowers the agent to assimilate short-term experiences into long-term memory. To optimize agent interactions for task-specific learning, we introduce three structured communication patterns: Monologue, Dialogue,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;</title><link>http://arxiv.org/abs/2310.00785</link><description>&lt;p&gt;
BooookScore: LLM&#26102;&#20195;&#20013;&#23545;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#30340;&#31995;&#32479;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
BooookScore: A systematic exploration of book-length summarization in the era of LLMs. (arXiv:2310.00785v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00785
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;LLM&#27169;&#22411;&#36827;&#34892;&#20102;&#31995;&#32479;&#25506;&#32034;&#65292;&#20197;&#35299;&#20915;&#23545;&#36229;&#36807;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#36827;&#34892;&#25688;&#35201;&#30340;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#20102;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;100&#26412;&#20070;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#21457;&#29616;&#20102;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#36229;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19978;&#19979;&#25991;&#31383;&#21475;&#22823;&#23567;&#30340;&#20070;&#31821;&#38271;&#24230;&#25991;&#26723;&#65288;&gt;100K&#26631;&#35760;&#65289;&#36827;&#34892;&#25688;&#35201;&#38656;&#35201;&#39318;&#20808;&#23558;&#36755;&#20837;&#25991;&#26723;&#20998;&#25104;&#36739;&#23567;&#30340;&#22359;&#65292;&#28982;&#21518;&#25552;&#31034;LLM&#21512;&#24182;&#12289;&#26356;&#26032;&#21644;&#21387;&#32553;&#22359;&#32423;&#25688;&#35201;&#12290;&#23613;&#31649;&#36825;&#20010;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#21644;&#37325;&#35201;&#24615;&#65292;&#20294;&#30001;&#20110;&#35780;&#20272;&#30340;&#22256;&#38590;&#65292;&#23427;&#23578;&#26410;&#24471;&#21040;&#26377;&#24847;&#20041;&#30340;&#30740;&#31350;&#65306;&#29616;&#26377;&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#25968;&#25454;&#38598;&#65288;&#20363;&#22914;BookSum&#65289;&#22312;&#22823;&#22810;&#25968;&#20844;&#20849;LLM&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#20013;&#65292;&#32780;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#38590;&#20197;&#25429;&#25417;&#29616;&#20195;LLM&#25688;&#35201;&#22120;&#30340;&#38169;&#35823;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#30740;&#31350;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#24037;&#20316;&#27969;&#23454;&#26045;&#30340;&#22522;&#20110;LLM&#30340;&#20070;&#31821;&#38271;&#24230;&#25688;&#35201;&#22120;&#30340;&#36830;&#36143;&#24615;&#65306;&#65288;1&#65289;&#20998;&#23618;&#21512;&#24182;&#22359;&#32423;&#25688;&#35201;&#65292;&#65288;2&#65289;&#36880;&#27493;&#26356;&#26032;&#19968;&#20010;&#36816;&#34892;&#25688;&#35201;&#12290;&#25105;&#20204;&#23545;100&#26412;&#26368;&#36817;&#20986;&#29256;&#30340;&#20070;&#31821;&#30340;GPT-4&#29983;&#25104;&#25688;&#35201;&#33719;&#24471;&#20102;1193&#20010;&#32454;&#31890;&#24230;&#30340;&#20154;&#24037;&#27880;&#37322;&#65292;&#24182;&#30830;&#23450;&#20102;LLMs&#20135;&#29983;&#30340;&#20843;&#31181;&#24120;&#35265;&#30340;&#36830;&#36143;&#24615;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
Summarizing book-length documents (&gt;100K tokens) that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Bec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyncDreamer&#30340;&#26032;&#22411;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#22270;&#20687;&#12290;&#36890;&#36807;&#21516;&#27493;&#25152;&#26377;&#29983;&#25104;&#22270;&#20687;&#30340;&#20013;&#38388;&#29366;&#24577;&#65292;&#24182;&#21033;&#29992;3D&#24863;&#30693;&#29305;&#24449;&#27880;&#24847;&#26426;&#21046;&#65292;SyncDreamer&#33021;&#22815;&#23454;&#29616;&#22312;&#19981;&#21516;&#35270;&#35282;&#19978;&#29983;&#25104;&#39640;&#24230;&#19968;&#33268;&#30340;&#22270;&#20687;&#65292;&#20026;&#21508;&#31181;3D&#29983;&#25104;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2309.03453</link><description>&lt;p&gt;
SyncDreamer: &#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
SyncDreamer: Generating Multiview-consistent Images from a Single-view Image. (arXiv:2309.03453v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyncDreamer&#30340;&#26032;&#22411;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#22270;&#20687;&#12290;&#36890;&#36807;&#21516;&#27493;&#25152;&#26377;&#29983;&#25104;&#22270;&#20687;&#30340;&#20013;&#38388;&#29366;&#24577;&#65292;&#24182;&#21033;&#29992;3D&#24863;&#30693;&#29305;&#24449;&#27880;&#24847;&#26426;&#21046;&#65292;SyncDreamer&#33021;&#22815;&#23454;&#29616;&#22312;&#19981;&#21516;&#35270;&#35282;&#19978;&#29983;&#25104;&#39640;&#24230;&#19968;&#33268;&#30340;&#22270;&#20687;&#65292;&#20026;&#21508;&#31181;3D&#29983;&#25104;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#21147;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SyncDreamer&#30340;&#26032;&#22411;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#29983;&#25104;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#22270;&#20687;&#12290;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#35268;&#27169;2D&#25193;&#25955;&#27169;&#22411;&#65292;&#26368;&#36817;&#30340;Zero123&#24037;&#20316;&#23637;&#31034;&#20102;&#20174;&#21333;&#35270;&#22270;&#29289;&#20307;&#22270;&#20687;&#29983;&#25104;&#21512;&#29702;&#30340;&#26032;&#35270;&#35282;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#30340;&#22270;&#20687;&#22312;&#20960;&#20309;&#21644;&#39068;&#33394;&#19978;&#30340;&#19968;&#33268;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#27493;&#30340;&#22810;&#35270;&#35282;&#25193;&#25955;&#27169;&#22411;&#65292;&#23427;&#27169;&#25311;&#20102;&#22810;&#35270;&#35282;&#22270;&#20687;&#30340;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#21453;&#21521;&#36807;&#31243;&#29983;&#25104;&#22810;&#35270;&#35282;&#19968;&#33268;&#24615;&#22270;&#20687;&#12290;SyncDreamer&#36890;&#36807;3D&#24863;&#30693;&#29305;&#24449;&#27880;&#24847;&#26426;&#21046;&#65292;&#22312;&#21453;&#21521;&#36807;&#31243;&#30340;&#27599;&#20010;&#27493;&#39588;&#20013;&#21516;&#27493;&#25152;&#26377;&#29983;&#25104;&#22270;&#20687;&#30340;&#20013;&#38388;&#29366;&#24577;&#65292;&#20174;&#32780;&#30456;&#20851;&#32852;&#19981;&#21516;&#35270;&#35282;&#19978;&#30340;&#30456;&#24212;&#29305;&#24449;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;SyncDreamer&#33021;&#22815;&#22312;&#19981;&#21516;&#35270;&#35282;&#20043;&#38388;&#29983;&#25104;&#39640;&#24230;&#19968;&#33268;&#30340;&#22270;&#20687;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#29992;&#20110;&#21508;&#31181;3D&#29983;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a novel diffusion model called that generates multiview-consistent images from a single-view image. Using pretrained large-scale 2D diffusion models, recent work Zero123 demonstrates the ability to generate plausible novel views from a single-view image of an object. However, maintaining consistency in geometry and colors for the generated images remains a challenge. To address this issue, we propose a synchronized multiview diffusion model that models the joint probability distribution of multiview images, enabling the generation of multiview-consistent images in a single reverse process. SyncDreamer synchronizes the intermediate states of all the generated images at every step of the reverse process through a 3D-aware feature attention mechanism that correlates the corresponding features across different views. Experiments show that SyncDreamer generates images with high consistency across different views, thus making it well-suited for various 3D generation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2309.03409</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#20248;&#21270;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Optimizers. (arXiv:2309.03409v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20248;&#21270;&#20219;&#21153;&#12290;&#32463;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#19988;&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#36229;&#36807;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#26159;&#26080;&#22788;&#19981;&#22312;&#30340;&#12290;&#34429;&#28982;&#22522;&#20110;&#23548;&#25968;&#30340;&#31639;&#27861;&#22312;&#21508;&#31181;&#38382;&#39064;&#19978;&#26159;&#24378;&#22823;&#30340;&#24037;&#20855;&#65292;&#20294;&#26159;&#27809;&#26377;&#26799;&#24230;&#23545;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20316;&#20026;&#20248;&#21270;&#22120;&#65292;&#20854;&#20013;&#20248;&#21270;&#20219;&#21153;&#20197;&#33258;&#28982;&#35821;&#35328;&#24418;&#24335;&#25551;&#36848;&#12290;&#22312;&#27599;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#20013;&#65292;LLM&#20174;&#21253;&#21547;&#20808;&#21069;&#29983;&#25104;&#30340;&#35299;&#19982;&#20854;&#20540;&#30340;&#25552;&#31034;&#20013;&#29983;&#25104;&#26032;&#30340;&#35299;&#65292;&#28982;&#21518;&#23545;&#26032;&#30340;&#35299;&#36827;&#34892;&#35780;&#20272;&#24182;&#28155;&#21152;&#21040;&#25552;&#31034;&#20013;&#65292;&#29992;&#20110;&#19979;&#19968;&#27425;&#20248;&#21270;&#27493;&#39588;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;OPRO&#22312;&#32447;&#24615;&#22238;&#24402;&#21644;&#26053;&#34892;&#25512;&#38144;&#21592;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#28982;&#21518;&#36716;&#21521;&#25552;&#31034;&#20248;&#21270;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#33021;&#26368;&#22823;&#21270;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#25351;&#20196;&#12290;&#36890;&#36807;&#20351;&#29992;&#21508;&#31181;LLM&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;OPRO&#20248;&#21270;&#30340;&#26368;&#20339;&#25552;&#31034;&#22312;GSM8K&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;8%&#65292;&#22312;Big-Bench Hard&#20219;&#21153;&#19978;&#20987;&#36133;&#20102;&#20154;&#20026;&#35774;&#35745;&#30340;&#25552;&#31034;&#39640;&#36798;50%&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimization is ubiquitous. While derivative-based algorithms have been powerful tools for various problems, the absence of gradient imposes challenges on many real-world applications. In this work, we propose Optimization by PROmpting (OPRO), a simple and effective approach to leverage large language models (LLMs) as optimizers, where the optimization task is described in natural language. In each optimization step, the LLM generates new solutions from the prompt that contains previously generated solutions with their values, then the new solutions are evaluated and added to the prompt for the next optimization step. We first showcase OPRO on linear regression and traveling salesman problems, then move on to prompt optimization where the goal is to find instructions that maximize the task accuracy. With a variety of LLMs, we demonstrate that the best prompts optimized by OPRO outperform human-designed prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DDPM&#21644;&#29289;&#29702;&#24863;&#30693;&#30340;&#22270;&#20687;&#21435;&#38654;&#26694;&#26550;DehazeDDPM&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#30340;&#38654;&#38718;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#21435;&#38654;&#12290;</title><link>http://arxiv.org/abs/2308.11949</link><description>&lt;p&gt;
&#39640;&#36136;&#37327;&#22270;&#20687;&#21435;&#38654;&#27169;&#22411;&#30340;&#25193;&#25955;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
High-quality Image Dehazing with Diffusion Model. (arXiv:2308.11949v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DDPM&#21644;&#29289;&#29702;&#24863;&#30693;&#30340;&#22270;&#20687;&#21435;&#38654;&#26694;&#26550;DehazeDDPM&#65292;&#33021;&#22815;&#22312;&#22797;&#26434;&#30340;&#38654;&#38718;&#22330;&#26223;&#20013;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#22270;&#20687;&#21435;&#38654;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23494;&#38598;&#38654;&#38718;&#22330;&#26223;&#19979;&#65292;&#22270;&#20687;&#21435;&#38654;&#38754;&#20020;&#30528;&#24456;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#38654;&#38718;&#22270;&#20687;&#20013;&#24456;&#23569;&#20445;&#30041;&#21407;&#22987;&#20449;&#24687;&#12290;&#23613;&#31649;&#20043;&#21069;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#22312;&#23494;&#38598;&#38654;&#38718;&#22330;&#26223;&#20013;&#20173;&#28982;&#23384;&#22312;&#20869;&#23481;&#21644;&#39068;&#33394;&#30340;&#20449;&#24687;&#25439;&#22833;&#12290;&#26368;&#36817;&#20986;&#29616;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#26174;&#31034;&#20986;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;DDPM&#27809;&#26377;&#32771;&#34385;&#21040;&#21435;&#38654;&#20219;&#21153;&#30340;&#29289;&#29702;&#29305;&#24615;&#65292;&#38480;&#21046;&#20102;&#20854;&#20449;&#24687;&#34917;&#20840;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DDPM&#21644;&#29289;&#29702;&#24863;&#30693;&#30340;&#22270;&#20687;&#21435;&#38654;&#26694;&#26550;DehazeDDPM&#65292;&#36866;&#29992;&#20110;&#22797;&#26434;&#30340;&#38654;&#38718;&#22330;&#26223;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;DehazeDDPM&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#12290;&#21069;&#19968;&#38454;&#27573;&#29992;&#22823;&#27668;&#25955;&#23556;&#27169;&#22411;&#65288;ASM&#65289;&#23545;&#21435;&#38654;&#20219;&#21153;&#36827;&#34892;&#29289;&#29702;&#24314;&#27169;&#65292;&#23558;&#20998;&#24067;&#25289;&#36817;&#28165;&#26224;&#25968;&#25454;&#65292;&#24182;&#36171;&#20104;DehazeDDPM&#20855;&#26377;&#38654;&#38718;&#24863;&#30693;&#33021;&#21147;&#12290;&#21518;&#19968;&#38454;&#27573;&#21033;&#29992;DDPM&#30340;&#24378;&#22823;&#29983;&#25104;&#33021;&#21147;&#26469;&#34917;&#20607;&#38654;&#38718;&#24341;&#36215;&#30340;&#20449;&#24687;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image dehazing is quite challenging in dense-haze scenarios, where quite less original information remains in the hazy image. Though previous methods have made marvelous progress, they still suffer from information loss in content and color in dense-haze scenarios. The recently emerged Denoising Diffusion Probabilistic Model (DDPM) exhibits strong generation ability, showing potential for solving this problem. However, DDPM fails to consider the physics property of dehazing task, limiting its information completion capacity. In this work, we propose DehazeDDPM: A DDPM-based and physics-aware image dehazing framework that applies to complex hazy scenarios. Specifically, DehazeDDPM works in two stages. The former stage physically models the dehazing task with the Atmospheric Scattering Model (ASM), pulling the distribution closer to the clear data and endowing DehazeDDPM with fog-aware ability. The latter stage exploits the strong generation ability of DDPM to compensate for the haze-ind
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#34920;&#26126;&#20174;&#22270;&#20687;&#21644;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#39044;&#27979;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#35774;&#35745;AdaCE&#27169;&#22359;&#22312;&#22810;&#20010;EEG&#22522;&#20110;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.11654</link><description>&lt;p&gt;
&#22823;&#22411;&#21464;&#21387;&#22120;&#26159;&#26356;&#22909;&#30340;&#33041;&#30005;&#22270;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
Large Transformers are Better EEG Learners. (arXiv:2308.11654v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11654
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#34920;&#26126;&#20174;&#22270;&#20687;&#21644;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#33041;&#30005;&#22270;&#39044;&#27979;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36890;&#36807;&#35774;&#35745;AdaCE&#27169;&#22359;&#22312;&#22810;&#20010;EEG&#22522;&#20110;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#21464;&#21387;&#22120;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#30001;&#20110;&#21487;&#29992;&#30340;&#26631;&#35760;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#25968;&#25454;&#30340;&#35268;&#27169;&#36828;&#36828;&#20302;&#20110;&#25991;&#26412;&#21644;&#22270;&#20687;&#25968;&#25454;&#65292;&#22240;&#27492;&#24456;&#38590;&#23558;&#20174;EEG&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#24320;&#21457;&#21040;&#20687;GPT-4 100T&#36825;&#26679;&#30340;&#35268;&#27169;&#65292;&#20174;&#32780;&#23436;&#20840;&#21457;&#25381;&#35813;&#26550;&#26500;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20174;&#22270;&#20687;&#21644;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;EEG&#22522;&#20110;&#39044;&#27979;&#20219;&#21153;&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;AdaCE&#65292;&#21363;&#23558;EEG&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#20687;&#21644;&#25991;&#26412;&#24418;&#24335;&#30340;&#25554;&#25300;&#24335;&#36866;&#37197;&#22120;&#65292;&#29992;&#20110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#21464;&#21387;&#22120;&#12290;&#25552;&#20986;&#30340;AdaCE&#27169;&#22359;&#22312;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#26102;&#38750;&#24120;&#26377;&#25928;&#65292;&#21516;&#26102;&#22312;&#22810;&#31181;&#22522;&#20110;EEG&#30340;&#39044;&#27979;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#24615;&#33021;&#12290;&#20363;&#22914;&#65292;&#39044;&#35757;&#32451;&#30340;Swin-Transformer&#19978;&#30340;AdaCE&#36798;&#21040;&#20102;99.6&#65285;&#30340;&#31934;&#24230;&#65292;&#32477;&#23545;&#25913;&#21892;&#20102;9.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained large transformer models have achieved remarkable performance in the fields of natural language processing and computer vision. Since the magnitude of available labeled electroencephalogram (EEG) data is much lower than that of text and image data, it is difficult for transformer models pre-trained from EEG to be developed as large as GPT-4 100T to fully unleash the potential of this architecture. In this paper, we show that transformers pre-trained from images as well as text can be directly fine-tuned for EEG-based prediction tasks. We design AdaCE, plug-and-play Adapters for Converting EEG data into image as well as text forms, to fine-tune pre-trained vision and language transformers. The proposed AdaCE module is highly effective for fine-tuning pre-trained transformers while achieving state-of-the-art performance on diverse EEG-based prediction tasks. For example, AdaCE on the pre-trained Swin-Transformer achieves 99.6%, an absolute improvement of 9.2%, on the EEG-deco
&lt;/p&gt;</description></item><item><title>DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2308.08043</link><description>&lt;p&gt;
DiagGPT:&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
DiagGPT: An LLM-based Chatbot with Automatic Topic Management for Task-Oriented Dialogue. (arXiv:2308.08043v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.08043
&lt;/p&gt;
&lt;p&gt;
DiagGPT&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25193;&#23637;&#21040;&#20219;&#21153;&#23548;&#21521;&#30340;&#23545;&#35805;&#22330;&#26223;&#65292;&#25552;&#20379;&#20102;&#22312;&#22797;&#26434;&#35786;&#26029;&#22330;&#26223;&#20013;&#20027;&#21160;&#25552;&#38382;&#21644;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22914;ChatGPT&#27491;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#65292;&#23637;&#31034;&#20986;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#33021;&#21147;&#12290;&#36825;&#20123;AI&#27169;&#22411;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#36741;&#21161;&#20154;&#31867;&#23436;&#25104;&#21508;&#31181;&#20219;&#21153;&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;AI&#20316;&#20026;&#32842;&#22825;&#20195;&#29702;&#20154;&#30340;&#37325;&#35201;&#24212;&#29992;&#26159;&#22238;&#31572;&#20154;&#31867;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#30340;LLMs&#22312;&#22238;&#31572;&#19968;&#33324;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#26174;&#31034;&#20986;&#29087;&#32451;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#35786;&#26029;&#22330;&#26223;(&#22914;&#27861;&#24459;&#25110;&#21307;&#30103;&#21672;&#35810;)&#20013;&#65292;&#22522;&#26412;&#30340;&#38382;&#31572;&#23545;&#35805;&#24448;&#24448;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#38656;&#35201;&#20219;&#21153;&#23548;&#21521;&#23545;&#35805;(TOD)&#65292;&#20854;&#20013;AI&#32842;&#22825;&#20195;&#29702;&#38656;&#35201;&#20027;&#21160;&#25552;&#38382;&#24182;&#24341;&#23548;&#29992;&#25143;&#23436;&#25104;&#29305;&#23450;&#20219;&#21153;&#12290;&#20197;&#21069;&#30340;&#24494;&#35843;&#27169;&#22411;&#22312;TOD&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#24403;&#21069;&#30340;LLMs&#24182;&#26410;&#22266;&#26377;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DiagGPT (Diagnosis GPT)&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#23427;&#23558;LLMs&#25512;&#24191;&#21040;TOD&#22330;&#26223;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT, are becoming increasingly sophisticated, demonstrating capabilities that closely resemble those of humans. These AI models are playing an essential role in assisting humans with a wide array of tasks in daily life. A significant application of AI is its use as a chat agent, responding to human inquiries across various domains. Current LLMs have shown proficiency in answering general questions. However, basic question-answering dialogue often falls short in complex diagnostic scenarios, such as legal or medical consultations. These scenarios typically necessitate Task-Oriented Dialogue (TOD), wherein an AI chat agent needs to proactively pose questions and guide users towards specific task completion. Previous fine-tuning models have underperformed in TOD, and current LLMs do not inherently possess this capability. In this paper, we introduce DiagGPT (Dialogue in Diagnosis GPT), an innovative method that extends LLMs to TOD scenarios. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;WavLM&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;&#25163;&#21183;&#21512;&#25104;&#26041;&#27861;&#65292;&#23454;&#29616;&#21482;&#20351;&#29992;&#21407;&#22987;&#35821;&#38899;&#38899;&#39057;&#29983;&#25104;&#20010;&#24615;&#21270;&#20840;&#36523;&#25163;&#21183;&#65292;&#28040;&#38500;&#20102;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#22788;&#29702;&#21644;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2308.05995</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#38899;&#39057;&#65306;&#21033;&#29992;WavLM&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;&#25163;&#21183;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Audio is all in one: speech-driven gesture synthetics using WavLM pre-trained model. (arXiv:2308.05995v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05995
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;WavLM&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#20110;&#35821;&#38899;&#39537;&#21160;&#30340;&#25163;&#21183;&#21512;&#25104;&#26041;&#27861;&#65292;&#23454;&#29616;&#21482;&#20351;&#29992;&#21407;&#22987;&#35821;&#38899;&#38899;&#39057;&#29983;&#25104;&#20010;&#24615;&#21270;&#20840;&#36523;&#25163;&#21183;&#65292;&#28040;&#38500;&#20102;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#22788;&#29702;&#21644;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#34394;&#25311;&#20154;&#21019;&#20316;&#39046;&#22495;&#65292;&#29983;&#25104;&#19982;&#35821;&#38899;&#37197;&#22871;&#30340;&#25163;&#21183;&#26159;&#19968;&#20010;&#27491;&#22312;&#20852;&#36215;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#36807;&#20197;&#22768;&#23398;&#21644;&#35821;&#20041;&#20449;&#24687;&#20316;&#20026;&#36755;&#20837;&#65292;&#37319;&#29992;&#20998;&#31867;&#26041;&#27861;&#26469;&#35782;&#21035;&#20154;&#29289;&#30340;ID&#21644;&#24773;&#24863;&#65292;&#20197;&#39537;&#21160;&#19982;&#35821;&#38899;&#37197;&#22871;&#30340;&#25163;&#21183;&#29983;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#39033;&#24037;&#20316;&#20173;&#28982;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#19981;&#20165;&#28041;&#21450;&#25163;&#21183;&#12289;&#35821;&#38899;&#22768;&#23398;&#21644;&#35821;&#20041;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#36824;&#21253;&#25324;&#19982;&#20010;&#24615;&#12289;&#24773;&#24863;&#21644;&#20854;&#20182;&#19981;&#26126;&#30830;&#20294;&#37325;&#35201;&#30340;&#22240;&#32032;&#30456;&#20851;&#30340;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;diffmotion-v2&#8221;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#35821;&#38899;&#26465;&#20214;&#25193;&#25955;&#21644;&#22522;&#20110;&#38750;&#33258;&#22238;&#24402;Transformer&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#37319;&#29992;WavLM&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#20165;&#20351;&#29992;&#21407;&#22987;&#35821;&#38899;&#38899;&#39057;&#29983;&#25104;&#20010;&#24615;&#21270;&#30340;&#20840;&#36523;&#25163;&#21183;&#65292;&#28040;&#38500;&#20102;&#22797;&#26434;&#30340;&#22810;&#27169;&#24577;&#22788;&#29702;&#21644;&#25163;&#21160;&#27880;&#37322;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
The generation of co-speech gestures for digital humans is an emerging area in the field of virtual human creation. Prior research has made progress by using acoustic and semantic information as input and adopting classify method to identify the person's ID and emotion for driving co-speech gesture generation. However, this endeavour still faces significant challenges. These challenges go beyond the intricate interplay between co-speech gestures, speech acoustic, and semantics; they also encompass the complexities associated with personality, emotion, and other obscure but important factors. This paper introduces "diffmotion-v2," a speech-conditional diffusion-based and non-autoregressive transformer-based generative model with WavLM pre-trained model. It can produce individual and stylized full-body co-speech gestures only using raw speech audio, eliminating the need for complex multimodal processing and manually annotated. Firstly, considering that speech audio not only contains acou
&lt;/p&gt;</description></item><item><title>FLASK&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.10928</link><description>&lt;p&gt;
FLASK: &#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets. (arXiv:2307.10928v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.10928
&lt;/p&gt;
&lt;p&gt;
FLASK&#26159;&#19968;&#31181;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21327;&#35758;&#65292;&#36890;&#36807;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#65292;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25351;&#20196;&#38656;&#35201;&#19982;&#20154;&#31867;&#30340;&#20215;&#20540;&#35266;&#36827;&#34892;&#23545;&#40784;&#65292;&#24182;&#19988;&#25152;&#38656;&#30340;&#25216;&#33021;&#38598;&#26681;&#25454;&#25351;&#20196;&#32780;&#24322;&#65292;&#22240;&#27492;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#31895;&#31890;&#24230;&#35780;&#20272;&#65288;&#21363;&#22522;&#20110;&#25972;&#20307;&#20559;&#22909;&#30340;&#35780;&#20272;&#65289;&#65292;&#36825;&#38480;&#21046;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#22240;&#20026;&#23427;&#26410;&#32771;&#34385;&#38656;&#35201;&#23454;&#20363;&#32423;&#25216;&#33021;&#32452;&#21512;&#30340;&#29992;&#25143;&#25351;&#20196;&#30340;&#29305;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FLASK&#65288;&#22522;&#20110;&#23545;&#40784;&#25216;&#33021;&#38598;&#30340;&#32454;&#31890;&#24230;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#32454;&#31890;&#24230;&#35780;&#20272;&#21327;&#35758;&#65292;&#29992;&#20110;&#20154;&#31867;&#21644;&#27169;&#22411;&#30340;&#35780;&#20272;&#65292;&#23427;&#23558;&#31895;&#32423;&#35780;&#20998;&#20998;&#35299;&#20026;&#27599;&#20010;&#25351;&#20196;&#30340;&#25216;&#33021;&#38598;&#32423;&#35780;&#20998;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#35780;&#20272;&#30340;&#32454;&#31890;&#24230;&#23545;&#20110;&#33719;&#24471;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#20840;&#38754;&#35270;&#35282;&#21644;&#25552;&#39640;&#35780;&#20272;&#30340;&#21487;&#38752;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#21033;&#29992;FLASK&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#22810;&#20010;&#24320;&#28304;&#21644;&#19987;&#26377;LLMs&#65292;&#24182;&#35266;&#23519;&#21040;&#39640;&#24230;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluation of Large Language Models (LLMs) is challenging because instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction. However, previous studies have mainly focused on coarse-grained evaluation (i.e. overall preference-based evaluation), which limits interpretability since it does not consider the nature of user instructions that require instance-wise skill composition. In this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation which decomposes coarse-level scoring to a skill set-level scoring for each instruction. We experimentally observe that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, we compare multiple open-source and proprietary LLMs and observe a high correlati
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#38750;&#27491;&#21017;&#36335;&#24452;&#34920;&#36798;&#24335;&#23545;&#20110;&#25193;&#23637;ALC&#25551;&#36848;&#36923;&#36753;&#20013;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#21644;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19981;&#21487;&#20915;&#23450;&#24615;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.09913</link><description>&lt;p&gt;
&#25506;&#32034;&#20855;&#26377;&#25551;&#36848;&#36923;&#36753;&#29305;&#24449;&#30340;&#21629;&#39064;&#21160;&#24577;&#36923;&#36753;&#30340;&#38750;&#27491;&#21017;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Exploring Non-Regular Extensions of Propositional Dynamic Logic with Description-Logics Features. (arXiv:2307.09913v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09913
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#38750;&#27491;&#21017;&#36335;&#24452;&#34920;&#36798;&#24335;&#23545;&#20110;&#25193;&#23637;ALC&#25551;&#36848;&#36923;&#36753;&#20013;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#21644;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19981;&#21487;&#20915;&#23450;&#24615;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#38750;&#27491;&#21017;&#36335;&#24452;&#34920;&#36798;&#24335;&#23545;&#20110;&#25193;&#23637;ALC&#25551;&#36848;&#36923;&#36753;&#20013;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#21644;&#26597;&#35810;&#30340;&#21487;&#20915;&#23450;&#24615;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#30340;&#23545;&#35937;&#26159;ALCreg&#21644;ALCvpl&#65292;&#20998;&#21035;&#26159;&#20351;&#29992;&#27491;&#21017;&#21644;&#21487;&#35265;&#25512;&#19979;&#35821;&#35328;&#30340;&#36335;&#24452;&#34920;&#36798;&#24335;&#30340;&#25193;&#23637;&#12290;&#31532;&#19968;&#20010;ALCreg&#26159;Fischer&#21644;Ladner&#25152;&#29087;&#30693;&#30340;&#21629;&#39064;&#21160;&#24577;&#36923;&#36753;&#30340;&#19968;&#31181;&#21464;&#31181;&#12290;&#31532;&#20108;&#20010;ALCvpl&#26159;&#30001;Loding&#21644;Serre&#22312;2007&#24180;&#24341;&#20837;&#21644;&#30740;&#31350;&#30340;&#12290;ALCvpl&#36923;&#36753;&#24191;&#20041;&#19978;&#25512;&#24191;&#20102;&#35768;&#22810;&#24050;&#30693;&#30340;&#21487;&#20915;&#23450;&#24615;&#38750;&#27491;&#21017;&#25193;&#23637;&#30340;ALCreg&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31995;&#21015;&#19981;&#21487;&#20915;&#23450;&#24615;&#32467;&#26524;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#28155;&#21152;&#30475;&#20284;&#26080;&#23475;&#30340;Self&#25805;&#20316;&#31526;&#21518;&#65292;&#23545;&#20110;ALCvpl&#20013;&#30340;&#27010;&#24565;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#21487;&#20915;&#23450;&#24615;&#20007;&#22833;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#23545;&#20110;&#22312;ALCvpl&#20013;&#28155;&#21152;&#20010;&#20307;&#35789;&#30340;&#27010;&#24565;&#21487;&#28385;&#36275;&#24615;&#38382;&#39064;&#30340;&#19981;&#21487;&#20915;&#23450;&#24615;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#19981;&#21487;&#20915;&#23450;&#24615;&#35777;&#26126;&#21482;&#20381;&#36182;&#20110;&#19968;&#20010;&#21333;&#19968;&#30340;&#38750;&#27491;&#21017;&#65288;&#21487;&#35265;&#25512;&#19979;&#65289;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the impact of non-regular path expressions on the decidability of satisfiability checking and querying in description logics extending ALC. Our primary objects of interest are ALCreg and ALCvpl, the extensions of with path expressions employing, respectively, regular and visibly-pushdown languages. The first one, ALCreg, is a notational variant of the well-known Propositional Dynamic Logic of Fischer and Ladner. The second one, ALCvpl, was introduced and investigated by Loding and Serre in 2007. The logic ALCvpl generalises many known decidable non-regular extensions of ALCreg.  We provide a series of undecidability results. First, we show that decidability of the concept satisfiability problem for ALCvpl is lost upon adding the seemingly innocent Self operator. Second, we establish undecidability for the concept satisfiability problem for ALCvpl extended with nominals. Interestingly, our undecidability proof relies only on one single non-regular (visibly-pushdown) langu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.05052</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Understanding In-Context Learning with Contrastive Demonstrations and Saliency Maps. (arXiv:2307.05052v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#23545;&#27604;&#28436;&#31034;&#21644;&#26174;&#33879;&#24615;&#22270;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#21478;&#22806;&#65292;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;&#19978;&#19979;&#25991;&#23398;&#20064;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;(ICL)&#24615;&#33021;&#20013;&#65292;&#21508;&#31181;&#28436;&#31034;&#32452;&#20214;&#30340;&#20316;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26631;&#31614;&#12289;&#36755;&#20837;&#20998;&#24067;&#21644;&#34917;&#20805;&#35299;&#37322;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#36825;&#20123;&#22240;&#32032;&#34987;&#20462;&#25913;&#25110;&#25200;&#21160;&#26102;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#22522;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#36825;&#20123;&#24037;&#20316;&#23545;&#20110;&#36825;&#20123;&#20803;&#32032;&#22914;&#20309;&#24433;&#21709;ICL&#32473;&#20986;&#20102;&#19981;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;&#20026;&#20102;&#25506;&#31350;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#21487;&#35299;&#37322;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(XNLP)&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;&#23545;&#27604;&#28436;&#31034;&#30340;&#26174;&#33879;&#24615;&#22270;&#36827;&#34892;&#23450;&#24615;&#21644;&#23450;&#37327;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25913;&#21464;&#26631;&#31614;&#23545;&#26174;&#33879;&#24615;&#26377;&#26174;&#33879;&#24433;&#21709;&#65292;&#23588;&#20854;&#23545;&#20110;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;&#26356;&#20026;&#26126;&#26174;&#12290;&#25105;&#20204;&#23545;&#36755;&#20837;&#20998;&#24067;&#36827;&#34892;&#20102;&#31890;&#24230;&#32423;&#21035;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#22312;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#23558;&#34920;&#36798;&#24773;&#24863;&#30340;&#26415;&#35821;&#25913;&#20026;&#20013;&#24615;&#35789;&#24182;&#19981;&#20687;&#25913;&#21464;&#26631;&#31614;&#37027;&#26679;&#20855;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#34917;&#20805;&#35299;&#37322;&#22312;&#25552;&#39640;ICL&#26041;&#38754;&#30340;&#25928;&#26524;&#26159;&#23384;&#22312;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the role of various demonstration components in the in-context learning (ICL) performance of large language models (LLMs). Specifically, we explore the impacts of ground-truth labels, input distribution, and complementary explanations, particularly when these are altered or perturbed. We build on previous work, which offers mixed findings on how these elements influence ICL. To probe these questions, we employ explainable NLP (XNLP) methods and utilize saliency maps of contrastive demonstrations for both qualitative and quantitative analysis. Our findings reveal that flipping ground-truth labels significantly affects the saliency, though it's more noticeable in larger LLMs. Our analysis of the input distribution at a granular level reveals that changing sentiment-indicative terms in a sentiment analysis task to neutral ones does not have as substantial an impact as altering ground-truth labels. Finally, we find that the effectiveness of complementary explanations in boos
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.02046</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#25512;&#33616;&#31995;&#32479; (LLMs)
&lt;/p&gt;
&lt;p&gt;
Recommender Systems in the Era of Large Language Models (LLMs). (arXiv:2307.02046v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02046
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;DNN&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#12289;&#29983;&#25104;&#12289;&#25512;&#29702;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30005;&#23376;&#21830;&#21153;&#21644;&#32593;&#32476;&#24212;&#29992;&#30340;&#32321;&#33635;&#65292;&#25512;&#33616;&#31995;&#32479;&#65288;RecSys&#65289;&#24050;&#32463;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#37325;&#35201;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#20010;&#24615;&#21270;&#24314;&#35758;&#20197;&#28385;&#36275;&#20854;&#21916;&#22909;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#36890;&#36807;&#27169;&#25311;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#21644;&#25972;&#21512;&#25991;&#26412;&#20391;&#20449;&#24687;&#22312;&#25552;&#21319;&#25512;&#33616;&#31995;&#32479;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#26159;DNN&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#29702;&#35299;&#29992;&#25143;&#20852;&#36259;&#12289;&#25429;&#25417;&#25991;&#26412;&#20391;&#20449;&#24687;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#22312;&#19981;&#21516;&#25512;&#33616;&#22330;&#26223;&#20013;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#30340;&#19981;&#36275;&#31561;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65288;&#20363;&#22914;ChatGPT&#21644;GPT4&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#24341;&#36215;&#20102;&#38761;&#21629;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#30340;&#22522;&#26412;&#32844;&#36131;&#19978;&#26377;&#30528;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#20855;&#26377;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#27867;&#21270;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the prosperity of e-commerce and web applications, Recommender Systems (RecSys) have become an important component of our daily life, providing personalized suggestions that cater to user preferences. While Deep Neural Networks (DNNs) have made significant advancements in enhancing recommender systems by modeling user-item interactions and incorporating textual side information, DNN-based methods still face limitations, such as difficulties in understanding users' interests and capturing textual side information, inabilities in generalizing to various recommendation scenarios and reasoning on their predictions, etc. Meanwhile, the emergence of Large Language Models (LLMs), such as ChatGPT and GPT4, has revolutionized the fields of Natural Language Processing (NLP) and Artificial Intelligence (AI), due to their remarkable abilities in fundamental responsibilities of language understanding and generation, as well as impressive generalization and reasoning capabilities. As a result, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#36890;&#36807;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#21644;&#33410;&#28857;&#27744;&#31574;&#30053;&#35299;&#20915;&#20102;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.00185</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#21450;&#20854;&#24212;&#29992;&#30340;&#26500;&#36896;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Constructive Algorithm for Incremental Random Weight Neural Networks and Its Application. (arXiv:2307.00185v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00185
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;&#30340;&#26500;&#36896;&#31639;&#27861;&#65292;&#36890;&#36807;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#21644;&#33410;&#28857;&#27744;&#31574;&#30053;&#35299;&#20915;&#20102;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#36825;&#31181;&#31639;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22686;&#37327;&#38543;&#26426;&#26435;&#37325;&#31070;&#32463;&#32593;&#32476;(IRWNNs)&#30001;&#20110;&#26131;&#20110;&#23454;&#29616;&#21644;&#24555;&#36895;&#23398;&#20064;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;IRWNNs&#30340;&#19968;&#20010;&#26174;&#33879;&#32570;&#28857;&#26159;&#38590;&#20197;&#35299;&#37322;&#38544;&#34255;&#21442;&#25968;&#65288;&#33410;&#28857;&#65289;&#19982;&#27531;&#24046;&#35823;&#24046;&#65288;&#27169;&#22411;&#24615;&#33021;&#65289;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#30340;&#21487;&#35299;&#37322;&#30340;&#26500;&#36896;&#31639;&#27861;(ICA)&#12290;&#39318;&#20808;&#65292;&#22522;&#20110;&#38544;&#34255;&#21442;&#25968;&#19982;&#27531;&#24046;&#35823;&#24046;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#20960;&#20309;&#20449;&#24687;&#32422;&#26463;&#26469;&#38543;&#26426;&#20998;&#37197;&#38544;&#34255;&#21442;&#25968;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#33410;&#28857;&#27744;&#31574;&#30053;&#33719;&#21462;&#26356;&#26377;&#21033;&#20110;&#25910;&#25947;&#30340;&#38544;&#34255;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#35777;&#26126;&#20102;ICA&#30340;&#36890;&#29992;&#36924;&#36817;&#24615;&#36136;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#20102;ICA&#30340;&#36731;&#37327;&#32423;&#29256;&#26412;&#29992;&#20110;&#22823;&#35268;&#27169;&#25968;&#25454;&#24314;&#27169;&#20219;&#21153;&#12290;&#22312;&#20845;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Incremental random weight neural networks (IRWNNs) have gained attention in view of its easy implementation and fast learning. However, a significant drawback of IRWNNs is that the elationship between the hidden parameters (node)and the residual error (model performance) is difficult to be interpreted. To address the above issue, this article proposes an interpretable constructive algorithm (ICA) with geometric information constraint. First, based on the geometric relationship between the hidden parameters and the residual error, an interpretable geometric information constraint is proposed to randomly assign the hidden parameters. Meanwhile, a node pool strategy is employed to obtain hidden parameters that is more conducive to convergence from hidden parameters satisfying the proposed constraint. Furthermore, the universal approximation property of the ICA is proved. Finally, a lightweight version of ICA is presented for large-scale data modeling tasks. Experimental results on six ben
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20272;&#31639;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20197;&#21450;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#65292;&#35299;&#20915;&#20102;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03116</link><description>&lt;p&gt;
&#36890;&#36807;&#36716;&#21270;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transferring Annotator- and Instance-dependent Transition Matrix for Learning from Crowds. (arXiv:2306.03116v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#26041;&#27861;&#26469;&#20272;&#31639;&#29305;&#23450;&#27880;&#37322;&#32773;&#21644;&#29305;&#23450;&#23454;&#20363;&#30340;&#36716;&#31227;&#30697;&#38453;&#20197;&#21450;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#65292;&#35299;&#20915;&#20102;&#20174;&#20247;&#21253;&#20013;&#23398;&#20064;&#30340;&#26631;&#31614;&#22122;&#22768;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#20174;&#20247;&#21253;&#26381;&#21153;&#20013;&#33719;&#21462;&#35757;&#32451;&#25968;&#25454;&#30340;&#27880;&#37322;&#26041;&#27861;&#12290;&#27599;&#20010;&#27880;&#37322;&#32773;&#37117;&#23436;&#25104;&#33258;&#24049;&#30340;&#23567;&#37096;&#20998;&#27880;&#37322;&#65292;&#19981;&#21516;&#27880;&#37322;&#32773;&#30340;&#26631;&#27880;&#38169;&#35823;&#24448;&#24448;&#19981;&#21516;&#12290;&#36890;&#36807;&#26631;&#31614;&#22122;&#22768;&#30340;&#36716;&#31227;&#30697;&#38453;&#26469;&#24314;&#27169;&#22122;&#22768;&#20135;&#29983;&#36807;&#31243;&#26159;&#35299;&#20915;&#26631;&#31614;&#22122;&#22768;&#30340;&#19968;&#31181;&#26377;&#25928;&#24037;&#20855;&#12290;&#22312;&#23454;&#38469;&#20247;&#21253;&#27169;&#22411;&#20013;&#65292;&#36716;&#31227;&#30697;&#38453;&#26082;&#30001;&#27880;&#37322;&#32773;&#20381;&#36182;&#65292;&#20063;&#30001;&#23454;&#20363;&#20381;&#36182;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27880;&#37322;&#32773;&#21644;&#23454;&#20363;&#20381;&#36182;&#30340;&#36716;&#31227;&#30697;&#38453;(AIDTM)&#20855;&#26377;&#39640;&#22797;&#26434;&#24230;&#65292;&#32780;&#23454;&#38469;&#27880;&#37322;&#24448;&#24448;&#28041;&#21450;&#27880;&#37322;&#31232;&#30095;&#24615;&#65292;&#36825;&#20351;&#24471;&#24314;&#31435;AIDTM&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26082;&#35201;&#20445;&#25345;&#24314;&#27169;&#30340;&#24191;&#27867;&#24615;&#65292;&#21448;&#33021;&#26356;&#30495;&#23454;&#22320;&#35299;&#20915;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#21516;&#26102;&#20272;&#31639;AIDTM&#21644;&#30495;&#23454;&#26631;&#31614;&#27604;&#20363;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#29702;&#35770;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#22312;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from crowds describes that the annotations of training data are obtained with crowd-sourcing services. Multiple annotators each complete their own small part of the annotations, where labeling mistakes that depend on annotators occur frequently. Modeling the label-noise generation process by the noise transition matrix is a power tool to tackle the label noise. In real-world crowd-sourcing scenarios, noise transition matrices are both annotator- and instance-dependent. However, due to the high complexity of annotator- and instance-dependent transition matrices (AIDTM), \textit{annotation sparsity}, which means each annotator only labels a little part of instances, makes modeling AIDTM very challenging. Prior works simplify the problem by assuming the transition matrix is instance-independent or using simple parametric way, while lose modeling generality. Motivated by this, we target a more realistic problem, estimating general AIDTM in practice. Without losing modeling general
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#65292;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2306.01158</link><description>&lt;p&gt;
&#22522;&#20110;&#24322;&#26500;&#30693;&#35782;&#30340;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Augmented Modular Reinforcement Learning based on Heterogeneous Knowledge. (arXiv:2306.01158v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.01158
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#65292;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#26377;&#26395;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#24471;&#21040;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#20943;&#32531;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20123;&#20302;&#25928;&#38382;&#39064;&#65292;&#23398;&#32773;&#20204;&#25552;&#20986;&#20102;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#30340;&#20915;&#31574;&#21046;&#23450;&#31574;&#30053;&#32452;&#21512;&#36215;&#26469;&#20197;&#34893;&#29983;&#20986;&#21487;&#20197;&#25191;&#34892;&#22810;&#31181;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;&#36825;&#20123;&#20307;&#31995;&#32467;&#26500;&#30340;&#22522;&#30784;&#27169;&#22359;&#36890;&#24120;&#26159;&#21487;&#37325;&#22797;&#20351;&#29992;&#30340;&#65292;&#20063;&#20801;&#35768;&#8220;&#21363;&#25554;&#21363;&#29992;&#8221;&#30340;&#38598;&#25104;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#20173;&#28982;&#32570;&#20047;&#22788;&#29702;&#21644;&#25972;&#21512;&#22810;&#31181;&#31867;&#22411;&#20449;&#24687;&#65288;&#30693;&#35782;&#65289;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#35268;&#21017;&#65292;&#23376;&#30446;&#26631;&#21644;&#25216;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#24378;&#27169;&#22359;&#21270;&#24378;&#21270;&#23398;&#20064;&#65288;AMRL&#65289;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#36825;&#31181;&#26032;&#30340;&#26694;&#26550;&#20351;&#29992;&#20210;&#35009;&#22120;&#26469;&#36873;&#25321;&#24322;&#26500;&#27169;&#22359;&#65292;&#24182;&#26080;&#32541;&#22320;&#25972;&#21512;&#19981;&#21516;&#31867;&#22411;&#30340;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36873;&#25321;&#26426;&#21046;&#30340;&#21464;&#20307;&#65292;&#21363;&#22686;&#24378;&#35760;&#24518;&#30340;&#20210;&#35009;&#22120;&#65292;&#23427;&#22686;&#21152;&#20102;&#21033;&#29992;&#26102;&#38388;&#20449;&#24687;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#24050;&#26377;&#30340;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26426;&#21046;&#65292;&#21516;&#26102;&#20063;&#22312;&#26032;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20855;&#26377;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to mitigate some of the inefficiencies of Reinforcement Learning (RL), modular approaches composing different decision-making policies to derive agents capable of performing a variety of tasks have been proposed. The modules at the basis of these architectures are generally reusable, also allowing for "plug-and-play" integration. However, such solutions still lack the ability to process and integrate multiple types of information (knowledge), such as rules, sub-goals, and skills. We propose Augmented Modular Reinforcement Learning (AMRL) to address these limitations. This new framework uses an arbitrator to select heterogeneous modules and seamlessly incorporate different types of knowledge. Additionally, we introduce a variation of the selection mechanism, namely the Memory-Augmented Arbitrator, which adds the capability of exploiting temporal information. We evaluate the proposed mechanisms on established as well as new environments and benchmark them against prominent deep 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#36870;&#30340;&#32570;&#38519;&#24182;&#19988;&#20351;&#24471;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#65292;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#12290;</title><link>http://arxiv.org/abs/2305.17493</link><description>&lt;p&gt;
&#36882;&#24402;&#30340;&#35781;&#21650;&#65306;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#35753;&#27169;&#22411;&#24536;&#35760;
&lt;/p&gt;
&lt;p&gt;
The Curse of Recursion: Training on Generated Data Makes Models Forget. (arXiv:2305.17493v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17493
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#21487;&#36870;&#30340;&#32570;&#38519;&#24182;&#19988;&#20351;&#24471;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#65292;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#37117;&#23384;&#22312;&#65292;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#38761;&#21629;&#24615;&#22320;&#25913;&#21464;&#20102;&#20174;&#25551;&#36848;&#24615;&#25991;&#26412;&#20013;&#29983;&#25104;&#22270;&#20687;&#30340;&#26041;&#27861;&#12290;GPT-2&#12289;GPT-3(.5)&#21644;GPT-4&#22312;&#21508;&#31181;&#35821;&#35328;&#20219;&#21153;&#20013;&#34920;&#29616;&#24778;&#20154;&#12290;ChatGPT&#23558;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#24341;&#20837;&#20102;&#22823;&#20247;&#35270;&#37326;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24050;&#32463;&#19981;&#21487;&#36991;&#20813;&#24182;&#23558;&#24443;&#24213;&#25913;&#21464;&#22312;&#32447;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#25972;&#20010;&#29983;&#24577;&#31995;&#32479;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#26410;&#26469;&#21487;&#33021;&#21457;&#29983;&#30340;&#20107;&#24773;&#12290;&#24403;LLMs&#21344;&#25454;&#20102;&#22312;&#32447;&#35821;&#35328;&#30340;&#22823;&#37096;&#20998;&#26102;&#65292;GPT-{n}&#20250;&#21457;&#29983;&#20160;&#20040;&#65311;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#35757;&#32451;&#20013;&#20351;&#29992;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#20250;&#23548;&#33268;&#25152;&#24471;&#27169;&#22411;&#20013;&#19981;&#21487;&#36870;&#32570;&#38519;&#65292;&#21407;&#22987;&#20869;&#23481;&#20998;&#24067;&#30340;&#23614;&#37096;&#28040;&#22833;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#25928;&#24212;&#31216;&#20026;&#27169;&#22411;&#25240;&#21472;&#65292;&#24182;&#26174;&#31034;&#23427;&#21487;&#20197;&#21457;&#29983;&#22312;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#12289;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#21644;LLMs&#20013;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#29616;&#35937;&#32972;&#21518;&#30340;&#29702;&#35770;&#30452;&#35273;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#29616;&#35937;&#22312;&#25152;&#26377;&#23398;&#20064;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#26222;&#36941;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22914;&#26524;&#25105;&#20204;&#35201;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#29983;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23601;&#24517;&#39035;&#35748;&#30495;&#23545;&#24453;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Stable Diffusion revolutionised image creation from descriptive text. GPT-2, GPT-3(.5) and GPT-4 demonstrated astonishing performance across a variety of language tasks. ChatGPT introduced such language models to the general public. It is now clear that large language models (LLMs) are here to stay, and will bring about drastic change in the whole ecosystem of online text and images. In this paper we consider what the future might hold. What will happen to GPT-{n} once LLMs contribute much of the language found online? We find that use of model-generated content in training causes irreversible defects in the resulting models, where tails of the original content distribution disappear. We refer to this effect as Model Collapse and show that it can occur in Variational Autoencoders, Gaussian Mixture Models and LLMs. We build theoretical intuition behind the phenomenon and portray its ubiquity amongst all learned generative models. We demonstrate that it has to be taken seriously if we ar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2305.14561</link><description>&lt;p&gt;
&#36127;&#21453;&#39304;&#35757;&#32451;&#65306;&#25552;&#39640;NVCiM DNN&#21152;&#36895;&#22120;&#40065;&#26834;&#24615;&#30340;&#26032;&#27010;&#24565;
&lt;/p&gt;
&lt;p&gt;
Negative Feedback Training: A Novel Concept to Improve Robustness of NVCiM DNN Accelerators. (arXiv:2305.14561v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14561
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#36127;&#21453;&#39304;&#26426;&#21046;&#26469;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23384;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#38750;&#25381;&#21457;&#24615;&#23384;&#20648;&#22120;(NVM)&#23454;&#29616;&#30340;&#20869;&#23384;&#35745;&#31639;(CiM)&#20026;&#21152;&#36895;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#25552;&#20379;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290; CiM&#21152;&#36895;&#22120;&#36890;&#36807;&#22312;&#21516;&#19968;&#30005;&#36335;&#26495;&#32467;&#26500;&#20013;&#23384;&#20648;&#32593;&#32476;&#26435;&#37325;&#21644;&#25191;&#34892;&#30697;&#38453;&#25805;&#20316;&#65292;&#20197;&#26368;&#23567;&#30340;&#38754;&#31215;&#38656;&#27714;&#21644;&#24322;&#24120;&#30340;&#33021;&#25928;&#65292;&#25552;&#20379;DNN&#25512;&#29702;&#21152;&#36895;&#12290;&#28982;&#32780;&#65292;NVM&#35774;&#22791;&#30340;&#38543;&#26426;&#24615;&#21644;&#20869;&#22312;&#21464;&#21270;&#24448;&#24448;&#23548;&#33268;&#24615;&#33021;&#38477;&#20302;&#65292;&#22914;&#19982;&#39044;&#26399;&#32467;&#26524;&#30456;&#27604;&#20943;&#23569;&#20998;&#31867;&#31934;&#24230;&#12290;&#23613;&#31649;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#36731;&#35774;&#22791;&#21464;&#24322;&#24182;&#22686;&#24378;&#40065;&#26834;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20381;&#36182;&#20110;&#25972;&#20307;&#35843;&#33410;&#24182;&#32570;&#20047;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#38480;&#21046;&#12290;&#21463;&#21040;&#36127;&#21453;&#39304;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#22810;&#20986;&#21475;&#26426;&#21046;&#20316;&#20026;&#36127;&#21453;&#39304;&#65292;&#22312;&#35774;&#22791;&#21464;&#24322;&#30340;&#24773;&#20917;&#19979;&#22686;&#24378;DNN&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Compute-in-Memory (CiM) utilizing non-volatile memory (NVM) devices presents a highly promising and efficient approach for accelerating deep neural networks (DNNs). By concurrently storing network weights and performing matrix operations within the same crossbar structure, CiM accelerators offer DNN inference acceleration with minimal area requirements and exceptional energy efficiency. However, the stochasticity and intrinsic variations of NVM devices often lead to performance degradation, such as reduced classification accuracy, compared to expected outcomes. Although several methods have been proposed to mitigate device variation and enhance robustness, most of them rely on overall modulation and lack constraints on the training process. Drawing inspiration from the negative feedback mechanism, we introduce a novel training approach that uses a multi-exit mechanism as negative feedback to enhance the performance of DNN models in the presence of device variation. Our negative feedbac
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;(ADR)&#26041;&#27861;&#65292;&#20854;&#33021;&#29983;&#25104;&#36719;&#26631;&#31614;&#29992;&#20316;&#26356;&#22909;&#30340;&#25351;&#23548;&#26426;&#21046;&#65292;&#20934;&#30830;&#21453;&#26144;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#28369;&#30340;&#25554;&#20837;&#24615;&#25972;&#21512;&#21040;&#20854;&#20182;&#23545;&#25239;&#24615;&#35757;&#32451;&#25216;&#26415;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.12118</link><description>&lt;p&gt;
&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;&#25913;&#36827;&#20102;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Annealing Self-Distillation Rectification Improves Adversarial Training. (arXiv:2305.12118v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;(ADR)&#26041;&#27861;&#65292;&#20854;&#33021;&#29983;&#25104;&#36719;&#26631;&#31614;&#29992;&#20316;&#26356;&#22909;&#30340;&#25351;&#23548;&#26426;&#21046;&#65292;&#20934;&#30830;&#21453;&#26144;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#21464;&#21270;&#65292;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#23454;&#29616;&#20102;&#24179;&#28369;&#30340;&#25554;&#20837;&#24615;&#25972;&#21512;&#21040;&#20854;&#20182;&#23545;&#25239;&#24615;&#35757;&#32451;&#25216;&#26415;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#23545;&#25239;&#35757;&#32451;&#20013;&#65292;&#27169;&#22411;&#34987;&#20248;&#21270;&#20197;&#36866;&#24212;&#21487;&#25509;&#21463;&#30340;&#23545;&#25239;&#25200;&#21160;&#39044;&#31639;&#20869;&#30340;&#19968;&#28909;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#24573;&#30053;&#30001;&#25200;&#21160;&#24102;&#26469;&#30340;&#22522;&#30784;&#20998;&#24067;&#21464;&#21270;&#65292;&#23548;&#33268;&#20102;&#24378;&#20581;&#30340;&#36807;&#25311;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22686;&#24378;&#23545;&#25239;&#24615;&#40065;&#26834;&#24615;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#24378;&#20581;&#27169;&#22411;&#30340;&#29305;&#24449;&#65292;&#24182;&#30830;&#23450;&#24378;&#20581;&#27169;&#22411;&#20542;&#21521;&#20110;&#29983;&#25104;&#26356;&#24179;&#28369;&#21644;&#26356;&#33391;&#22909;&#26657;&#20934;&#30340;&#36755;&#20986;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#8212;&#8212;&#36864;&#28779;&#33258;&#33976;&#39311;&#26657;&#27491;(ADR)&#65292;&#35813;&#26041;&#27861;&#29983;&#25104;&#36719;&#26631;&#31614;&#20316;&#20026;&#26356;&#22909;&#30340;&#25351;&#23548;&#26426;&#21046;&#65292;&#33021;&#20934;&#30830;&#21453;&#26144;&#22312;&#23545;&#25239;&#35757;&#32451;&#20013;&#25915;&#20987;&#19979;&#30340;&#20998;&#24067;&#21464;&#21270;&#12290;&#36890;&#36807;&#20351;&#29992;ADR&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#20462;&#27491;&#30340;&#20998;&#24067;&#65292;&#26174;&#33879;&#25913;&#21892;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#32780;&#19981;&#38656;&#35201;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#39069;&#22806;&#30340;&#35745;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#26367;&#25442;&#21367;&#31215;&#23618;&#20197;&#23454;&#29616;&#24179;&#28369;&#30340;&#25554;&#20837;&#24615;&#25972;&#21512;&#21040;&#20854;&#20182;&#23545;&#25239;&#24615;&#35757;&#32451;&#25216;&#26415;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In standard adversarial training, models are optimized to fit one-hot labels within allowable adversarial perturbation budgets. However, the ignorance of underlying distribution shifts brought by perturbations causes the problem of robust overfitting. To address this issue and enhance adversarial robustness, we analyze the characteristics of robust models and identify that robust models tend to produce smoother and well-calibrated outputs. Based on the observation, we propose a simple yet effective method, Annealing Self-Distillation Rectification (ADR), which generates soft labels as a better guidance mechanism that accurately reflects the distribution shift under attack during adversarial training. By utilizing ADR, we can obtain rectified distributions that significantly improve model robustness without the need for pre-trained models or extensive extra computation. Moreover, our method facilitates seamless plug-and-play integration with other adversarial training techniques by repl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#35273;&#35843;&#25972;&#30340;&#21457;&#23637;&#19982;&#29616;&#29366;&#65292;&#23558;&#36817;&#26399;&#30340;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#20998;&#20026;&#20116;&#31867;&#65292;&#21253;&#25324;&#25552;&#31034;&#35843;&#25972;&#12289;&#36866;&#37197;&#22120;&#35843;&#25972;&#12289;&#21442;&#25968;&#32763;&#35793;&#12289;&#32039;&#20945;&#35843;&#25972;&#21644;&#27169;&#22359;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06061</link><description>&lt;p&gt;
&#35270;&#35273;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Visual Tuning. (arXiv:2305.06061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#35273;&#35843;&#25972;&#30340;&#21457;&#23637;&#19982;&#29616;&#29366;&#65292;&#23558;&#36817;&#26399;&#30340;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#20998;&#20026;&#20116;&#31867;&#65292;&#21253;&#25324;&#25552;&#31034;&#35843;&#25972;&#12289;&#36866;&#37197;&#22120;&#35843;&#25972;&#12289;&#21442;&#25968;&#32763;&#35793;&#12289;&#32039;&#20945;&#35843;&#25972;&#21644;&#27169;&#22359;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24050;&#34987;&#24191;&#27867;&#35777;&#26126;&#22312;&#35768;&#22810;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#20855;&#26377;&#26377;&#21069;&#36884;&#30340;&#34920;&#29616;&#12290;&#38543;&#30528;&#39044;&#35757;&#32451;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#24778;&#20154;&#21457;&#23637;&#65292;&#35270;&#35273;&#35843;&#25972;&#36339;&#20986;&#20102;&#26631;&#20934;&#30340;&#27169;&#24335;&#25805;&#20316;&#65292;&#21363;&#24494;&#35843;&#25972;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#20165;&#24494;&#35843;&#23436;&#20840;&#36830;&#25509;&#23618;&#12290;&#30456;&#21453;&#65292;&#36817;&#26399;&#30340;&#36827;&#23637;&#21487;&#20197;&#36890;&#36807;&#26356;&#26032;&#26356;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#27604;&#20840;&#38754;&#24494;&#35843;&#25972;&#20010;&#39044;&#35757;&#32451;&#21442;&#25968;&#26356;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#20351;&#36793;&#32536;&#35774;&#22791;&#21644;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#37096;&#32626;&#22312;&#20113;&#31471;&#30340;&#26085;&#30410;&#24222;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20840;&#38754;&#20102;&#35299;&#35270;&#35273;&#35843;&#25972;&#30340;&#20840;&#35980;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#26412;&#32508;&#36848;&#25551;&#32472;&#20102;&#22823;&#37327;&#30340;&#36817;&#26399;&#30740;&#31350;&#20316;&#21697;&#65292;&#25552;&#20379;&#20102;&#29616;&#26377;&#24037;&#20316;&#21644;&#27169;&#22411;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#25552;&#20379;&#20102;&#35270;&#35273;&#35843;&#25972;&#30340;&#35814;&#32454;&#32972;&#26223;&#65292;&#24182;&#23558;&#26368;&#36817;&#30340;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#20998;&#20026;&#20116;&#32452;&#65306;&#25552;&#31034;&#35843;&#25972;&#12289;&#36866;&#37197;&#22120;&#35843;&#25972;&#12289;&#21442;&#25968;&#32763;&#35793;&#12289;&#32039;&#20945;&#35843;&#25972;&#21644;&#27169;&#22359;&#35843;&#25972;&#12290;&#26412;&#25991;&#36824;&#24378;&#35843;&#20102;&#24403;&#21069;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning visual models has been widely shown promising performance on many downstream visual tasks. With the surprising development of pre-trained visual foundation models, visual tuning jumped out of the standard modus operandi that fine-tunes the whole pre-trained model or just the fully connected layer. Instead, recent advances can achieve superior performance than full-tuning the whole pre-trained parameters by updating far fewer parameters, enabling edge devices and downstream applications to reuse the increasingly large foundation models deployed on the cloud. With the aim of helping researchers get the full picture and future directions of visual tuning, this survey characterizes a large and thoughtful selection of recent works, providing a systematic and comprehensive overview of existing work and models. Specifically, it provides a detailed background of visual tuning and categorizes recent visual tuning techniques into five groups: prompt tuning, adapter tuning, parameter 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#26356;&#22909;&#22320;&#36827;&#34892;&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#26377;&#25928;&#20943;&#36731;&#26032;&#38395;&#21644;&#35777;&#25454;&#20869;&#23481;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2304.12888</link><description>&lt;p&gt;
&#36890;&#36807;&#21452;&#37325;&#23545;&#25239;&#21435;&#20559;&#32622;&#23454;&#29616;&#30340;&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution Evidence-aware Fake News Detection via Dual Adversarial Debiasing. (arXiv:2304.12888v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12888
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#21152;&#20837;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#26088;&#22312;&#35757;&#32451;&#27169;&#22411;&#26356;&#22909;&#22320;&#36827;&#34892;&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#65292;&#26377;&#25928;&#20943;&#36731;&#26032;&#38395;&#21644;&#35777;&#25454;&#20869;&#23481;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#30028;&#35777;&#25454;&#24863;&#30693;&#20551;&#26032;&#38395;&#26816;&#27979;&#26088;&#22312;&#23545;&#26032;&#38395;&#21644;&#22522;&#20110;&#26032;&#38395;&#20869;&#23481;&#26816;&#32034;&#30340;&#35777;&#25454;&#36827;&#34892;&#25512;&#29702;&#65292;&#20197;&#26597;&#25214;&#32479;&#19968;&#24615;&#25110;&#19981;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#35777;&#25454;&#24863;&#30693;&#26816;&#27979;&#27169;&#22411;&#20250;&#21463;&#21040;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#21363;&#26032;&#38395;/&#35777;&#25454;&#20869;&#23481;&#21644;&#30495;&#23454;/&#20551;&#26032;&#38395;&#26631;&#31614;&#20043;&#38388;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#65292;&#24182;&#19988;&#24456;&#38590;&#25512;&#24191;&#21040;&#36234;&#30028;&#24773;&#20917;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#37325;&#23545;&#25239;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;DAL&#20013;&#21152;&#20837;&#20102;&#26032;&#38395;&#26041;&#38754;&#21644;&#35777;&#25454;&#26041;&#38754;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#23427;&#20204;&#30340;&#30446;&#26631;&#37117;&#26159;&#30495;&#20551;&#26032;&#38395;&#26631;&#31614;&#12290;&#28982;&#21518;&#65292;DAL&#20250;&#36870;&#21521;&#20248;&#21270;&#26032;&#38395;&#26041;&#38754;&#21644;&#35777;&#25454;&#26041;&#38754;&#21435;&#20559;&#32622;&#37492;&#21035;&#22120;&#65292;&#20197;&#20943;&#36731;&#26032;&#38395;&#21644;&#35777;&#25454;&#20869;&#23481;&#20559;&#24046;&#30340;&#24433;&#21709;&#12290;&#21516;&#26102;&#65292;DAL&#36824;&#20248;&#21270;&#20027;&#35201;&#30340;&#20551;&#26032;&#38395;&#39044;&#27979;&#22120;&#65292;&#35753;&#26032;&#38395;-&#35777;&#25454;&#20132;&#20114;&#27169;&#22359;&#33021;&#22815;&#34987;&#23398;&#20064;&#12290;&#36825;&#20010;&#36807;&#31243;&#33021;&#22815;&#24110;&#21161;&#25105;&#20204;&#25945;&#26032;&#38395;&#26816;&#27979;&#27169;&#22411;&#26356;&#22909;&#22320;&#36827;&#34892;&#26032;&#38395;&#35777;&#25454;&#25512;&#29702;&#65292;&#24182;&#23558;&#26816;&#27979;&#20551;&#26032;&#38395;&#30340;&#36127;&#38754;&#24433;&#21709;&#38477;&#33267;&#26368;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evidence-aware fake news detection aims to conduct reasoning between news and evidence, which is retrieved based on news content, to find uniformity or inconsistency. However, we find evidence-aware detection models suffer from biases, i.e., spurious correlations between news/evidence contents and true/fake news labels, and are hard to be generalized to Out-Of-Distribution (OOD) situations. To deal with this, we propose a novel Dual Adversarial Learning (DAL) approach. We incorporate news-aspect and evidence-aspect debiasing discriminators, whose targets are both true/fake news labels, in DAL. Then, DAL reversely optimizes news-aspect and evidence-aspect debiasing discriminators to mitigate the impact of news and evidence content biases. At the same time, DAL also optimizes the main fake news predictor, so that the news-evidence interaction module can be learned. This process allows us to teach evidence-aware fake news detection models to better conduct news-evidence reasoning, and min
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#25972;&#21512;&#20840;&#20999;&#29255;&#22270;&#20687;&#21644;&#25209;&#37327;&#36716;&#24405;&#32452;&#23398;&#39044;&#27979;&#24739;&#32773;&#29983;&#23384;&#29575;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26041;&#26696;&#65292;&#26088;&#22312;&#35299;&#20915;&#26631;&#35760;&#21270;&#36716;&#24405;&#32452;&#23398;&#21644;&#25429;&#33719;&#36825;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#20132;&#20114;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.06819</link><description>&lt;p&gt;
&#24314;&#27169;&#29983;&#29289;&#36890;&#36335;&#21644;&#32452;&#32455;&#23398;&#20043;&#38388;&#30340;&#31264;&#23494;&#22810;&#27169;&#24577;&#20132;&#20114;&#20197;&#39044;&#27979;&#23384;&#27963;&#29575;
&lt;/p&gt;
&lt;p&gt;
Modeling Dense Multimodal Interactions Between Biological Pathways and Histology for Survival Prediction. (arXiv:2304.06819v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#25972;&#21512;&#20840;&#20999;&#29255;&#22270;&#20687;&#21644;&#25209;&#37327;&#36716;&#24405;&#32452;&#23398;&#39044;&#27979;&#24739;&#32773;&#29983;&#23384;&#29575;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#30340;&#26041;&#26696;&#65292;&#26088;&#22312;&#35299;&#20915;&#26631;&#35760;&#21270;&#36716;&#24405;&#32452;&#23398;&#21644;&#25429;&#33719;&#36825;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#20132;&#20114;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#21512;&#20840;&#20999;&#29255;&#22270;&#20687;&#65288;WSIs&#65289;&#21644;&#25209;&#37327;&#36716;&#24405;&#32452;&#23398;&#65288;bulk transcriptomics&#65289;&#20197;&#39044;&#27979;&#24739;&#32773;&#29983;&#23384;&#29575;&#21487;&#20197;&#25552;&#39640;&#25105;&#20204;&#23545;&#24739;&#32773;&#39044;&#21518;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#19981;&#21516;&#24615;&#36136;&#65292;&#36825;&#20010;&#22810;&#27169;&#24577;&#20219;&#21153;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;WSIs&#20195;&#34920;&#32959;&#30244;&#30340;&#39640;&#32500;&#31354;&#38388;&#25551;&#36848;&#65292;&#32780;&#25209;&#37327;&#36716;&#24405;&#32452;&#23398;&#21017;&#20195;&#34920;&#35813;&#32959;&#30244;&#20869;&#30340;&#22522;&#22240;&#34920;&#36798;&#27700;&#24179;&#30340;&#20840;&#23616;&#25551;&#36848;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#26088;&#22312;&#35299;&#20915;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#65288;1&#65289;&#22914;&#20309;&#20197;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#23545;&#36716;&#24405;&#32452;&#23398;&#36827;&#34892;&#26631;&#35760;&#21270;&#65311;&#65288;2&#65289;&#22914;&#20309;&#25429;&#25417;&#36825;&#20004;&#31181;&#27169;&#24577;&#20043;&#38388;&#30340;&#23494;&#38598;&#22810;&#27169;&#24577;&#20132;&#20114;&#65311;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20174;&#36716;&#24405;&#32452;&#23398;&#20013;&#23398;&#20064;&#29983;&#29289;&#36890;&#36335;&#26631;&#35760;&#65292;&#20197;&#32534;&#30721;&#29305;&#23450;&#30340;&#32454;&#32990;&#21151;&#33021;&#12290;&#32467;&#21512;&#32534;&#30721;WSI&#20013;&#19981;&#21516;&#24418;&#24577;&#27169;&#24335;&#30340;&#32452;&#32455;&#23398;&#22359;&#26631;&#35760;&#65292;&#25105;&#20204;&#35748;&#20026;&#23427;&#20204;&#26500;&#25104;&#20102;&#19979;&#28216;&#21487;&#35299;&#37322;&#24615;&#20998;&#26512;&#30340;&#36866;&#24403;&#25512;&#29702;&#21333;&#20803;&#12290;&#25105;&#20204;&#25552;&#20986;&#34701;&#21512;...
&lt;/p&gt;
&lt;p&gt;
Integrating whole-slide images (WSIs) and bulk transcriptomics for predicting patient survival can improve our understanding of patient prognosis. However, this multimodal task is particularly challenging due to the different nature of these data: WSIs represent a very high-dimensional spatial description of a tumor, while bulk transcriptomics represent a global description of gene expression levels within that tumor. In this context, our work aims to address two key challenges: (1) how can we tokenize transcriptomics in a semantically meaningful and interpretable way?, and (2) how can we capture dense multimodal interactions between these two modalities? Specifically, we propose to learn biological pathway tokens from transcriptomics that can encode specific cellular functions. Together with histology patch tokens that encode the different morphological patterns in the WSI, we argue that they form appropriate reasoning units for downstream interpretability analyses. We propose fusing 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#32954;&#30284;&#35786;&#26029;&#30340;&#22810;&#20219;&#21153;CT&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#65292;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#32954;&#37096;CT&#20998;&#21106;&#31561;&#22810;&#20010;&#21307;&#23398;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.02649</link><description>&lt;p&gt;
&#24102;&#26377;&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#30340;CT&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CT Multi-Task Learning with a Large Image-Text (LIT) Model. (arXiv:2304.02649v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#32954;&#30284;&#35786;&#26029;&#30340;&#22810;&#20219;&#21153;CT&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#65292;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#32954;&#37096;CT&#20998;&#21106;&#31561;&#22810;&#20010;&#21307;&#23398;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#20165;&#33021;&#22815;&#25903;&#25345;&#22810;&#31181;&#35821;&#35328;&#20219;&#21153;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#20316;&#20026;&#19981;&#21516;&#39046;&#22495;&#30340;&#36890;&#29992;&#25509;&#21475;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#35777;&#26126;&#22914;&#20309;&#23558;LLM&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#25104;&#21151;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#28041;&#21450;&#39640;&#32500;&#21644;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#30340;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#39033;&#21487;&#34892;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#32452;&#21512;LLM&#21644;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#65288;LIM&#65289;&#65292;&#24314;&#31435;&#22810;&#20219;&#21153;CT&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#32954;&#30284;&#35786;&#26029;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLM&#21644;LIM&#29992;&#20316;&#32534;&#30721;&#22120;&#65292;&#26681;&#25454;&#29305;&#23450;&#20219;&#21153;&#30340;&#25991;&#26412;&#25552;&#31034;&#26469;&#24863;&#30693;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#21327;&#21516;&#20316;&#29992;&#20110;&#22810;&#28304;&#20449;&#24687;&#21644;&#20219;&#21153;&#29305;&#23450;&#21644;&#24739;&#32773;&#29305;&#23450;&#30340;&#20808;&#39564;&#65292;&#20197;&#20248;&#21270;&#35786;&#26029;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;LIT&#27169;&#22411;&#21644;&#30456;&#20851;&#25216;&#26415;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#23558;&#37325;&#28857;&#35780;&#20272;3D&#32954;&#37096;CT&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;LIT&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#25191;&#34892;&#22810;&#39033;&#21307;&#23398;&#20219;&#21153;&#65292;&#21253;&#25324;&#32954;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) not only empower multiple language tasks but also serve as a general interface across different spaces. Up to now, it has not been demonstrated yet how to effectively translate the successes of LLMs in the computer vision field to the medical imaging field which involves high-dimensional and multi-modal medical images. In this paper, we report a feasibility study of building a multi-task CT large image-text (LIT) model for lung cancer diagnosis by combining an LLM and a large image model (LIM). Specifically, the LLM and LIM are used as encoders to perceive multi-modal information under task-specific text prompts, which synergizes multi-source information and task-specific and patient-specific priors for optimized diagnostic performance. The key components of our LIT model and associated techniques are evaluated with an emphasis on 3D lung CT analysis. Our initial results show that the LIT model performs multiple medical tasks well, including lung segmentatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26354;&#29575;&#24179;&#34913;&#29305;&#24449;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#23545;&#20998;&#31867;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26354;&#29575;&#19981;&#24179;&#34913;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.12307</link><description>&lt;p&gt;
&#38271;&#23614;&#20998;&#31867;&#30340;&#26354;&#29575;&#24179;&#34913;&#29305;&#24449;&#27969;&#24418;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification. (arXiv:2303.12307v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26354;&#29575;&#24179;&#34913;&#29305;&#24449;&#27969;&#24418;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#25506;&#31350;&#20102;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#23545;&#20998;&#31867;&#38590;&#24230;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#26354;&#29575;&#19981;&#24179;&#34913;&#20250;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#24212;&#23545;&#38271;&#23614;&#20998;&#31867;&#30340;&#25361;&#25112;&#65292;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#26041;&#27861;&#26469;&#20943;&#23569;&#27169;&#22411;&#20559;&#24046;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#20551;&#35774;&#26679;&#26412;&#36739;&#23569;&#30340;&#31867;&#26159;&#24369;&#31867;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#23614;&#37096;&#31867;&#21035;&#24182;&#19981;&#24635;&#26159;&#38590;&#20197;&#23398;&#20064;&#30340;&#65292;&#32780;&#22312;&#26679;&#26412;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#35266;&#23519;&#21040;&#20102;&#27169;&#22411;&#20559;&#24046;&#65292;&#36825;&#34920;&#26126;&#23384;&#22312;&#20854;&#20182;&#24433;&#21709;&#27169;&#22411;&#20559;&#24046;&#30340;&#22240;&#32032;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#29992;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#24230;&#37327;&#65292;&#24182;&#25506;&#35752;&#20102;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#23545;&#20998;&#31867;&#38590;&#24230;&#21644;&#23398;&#20064;&#22914;&#20309;&#22609;&#36896;&#24863;&#30693;&#27969;&#24418;&#30340;&#20960;&#20309;&#29305;&#24615;&#30340;&#24433;&#21709;&#12290;&#19968;&#20010;&#24847;&#22806;&#30340;&#21457;&#29616;&#26159;&#65306;&#31867;&#21035;&#20934;&#30830;&#24230;&#21644;&#24863;&#30693;&#27969;&#24418;&#30340;&#20998;&#31163;&#31243;&#24230;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36880;&#28176;&#20943;&#23567;&#65292;&#32780;&#19982;&#26354;&#29575;&#30340;&#36127;&#30456;&#20851;&#24615;&#36880;&#28176;&#22686;&#21152;&#65292;&#36825;&#34920;&#26126;&#26354;&#29575;&#19981;&#24179;&#34913;&#23548;&#33268;&#27169;&#22411;&#19981;&#20844;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address the challenges of long-tailed classification, researchers have proposed several approaches to reduce model bias, most of which assume that classes with few samples are weak classes. However, recent studies have shown that tail classes are not always hard to learn, and model bias has been observed on sample-balanced datasets, suggesting the existence of other factors that affect model bias. In this work, we systematically propose a series of geometric measurements for perceptual manifolds in deep neural networks, and then explore the effect of the geometric characteristics of perceptual manifolds on classification difficulty and how learning shapes the geometric characteristics of perceptual manifolds. An unanticipated finding is that the correlation between the class accuracy and the separation degree of perceptual manifolds gradually decreases during training, while the negative correlation with the curvature gradually increases, implying that curvature imbalance leads to m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2303.12091</link><description>&lt;p&gt;
&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Adaptive Negative Evidential Deep Learning for Open-set Semi-supervised Learning. (arXiv:2303.12091v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ANEDL&#26694;&#26550;&#65292;&#24212;&#29992;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#20102;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#24212;&#23545;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20551;&#35774;&#26631;&#35760;&#25968;&#25454;&#12289;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#21516;&#19968;&#20998;&#24067;&#12290;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#32771;&#34385;&#21040;&#19968;&#20010;&#26356;&#23454;&#38469;&#30340;&#24773;&#20917;&#65292;&#21363;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#27979;&#35797;&#25968;&#25454;&#21253;&#21547;&#26631;&#35760;&#25968;&#25454;&#20013;&#26410;&#35266;&#23519;&#21040;&#30340;&#26032;&#31867;&#21035;&#65288;&#24322;&#24120;&#20540;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#8212;&#8212;&#36866;&#24212;&#24615;&#36127;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;ANEDL&#65289;&#65292;&#20197;&#24212;&#23545;&#20108;&#20803;&#20998;&#31867;&#22120;&#30340;&#19981;&#36275;&#20043;&#22788;&#65292;&#22914;&#32570;&#20047;&#21487;&#25193;&#23637;&#24615;&#21644;&#26080;&#27861;&#21306;&#20998;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#35777;&#25454;&#28145;&#24230;&#23398;&#20064;&#65288;EDL&#65289;&#20316;&#20026;&#19968;&#31181;&#24322;&#24120;&#26816;&#27979;&#22120;&#26469;&#37327;&#21270;&#19981;&#21516;&#31867;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#35774;&#35745;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#24230;&#37327;&#26041;&#27861;&#36827;&#34892;&#33258;&#25105;&#35757;&#32451;&#21644;&#25512;&#29702;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36866;&#24212;&#24615;&#36127;&#20248;&#21270;&#31574;&#30053;&#65292;&#20351;EDL&#26356;&#21152;&#36866;&#21512;&#21253;&#21547;&#20869;&#37096;&#20540;&#21644;&#24322;&#24120;&#20540;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#65292;&#25105;&#20204;&#30340;ANEDL&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#24320;&#25918;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning (SSL) methods assume that labeled data, unlabeled data and test data are from the same distribution. Open-set semi-supervised learning (Open-set SSL) considers a more practical scenario, where unlabeled data and test data contain new categories (outliers) not observed in labeled data (inliers). Most previous works focused on outlier detection via binary classifiers, which suffer from insufficient scalability and inability to distinguish different types of uncertainty. In this paper, we propose a novel framework, Adaptive Negative Evidential Deep Learning (ANEDL) to tackle these limitations. Concretely, we first introduce evidential deep learning (EDL) as an outlier detector to quantify different types of uncertainty, and design different uncertainty metrics for self-training and inference. Furthermore, we propose a novel adaptive negative optimization strategy, making EDL more tailored to the unlabeled dataset containing both inliers and outliers. As demonstrat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2301.03364</link><description>&lt;p&gt;
&#36208;&#21521;AI-enabled&#36830;&#25509;&#20135;&#19994;: AGV&#36890;&#20449;&#21644;&#20256;&#24863;&#22120;&#27979;&#37327;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Towards an AI-enabled Connected Industry: AGV Communication and Sensor Measurement Datasets. (arXiv:2301.03364v3 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.03364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;&#25152;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23558;&#20854;&#19982;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#36215;&#26469;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#24037;&#19994;&#27979;&#35797;&#24179;&#21488;&#19978;&#30340;&#26080;&#32447;&#27979;&#37327;&#27963;&#21160;: &#24037;&#19994;&#36710;&#36742;&#38388;&#36890;&#20449;(iV2V)&#21644;&#24037;&#19994;&#36710;&#36742;&#21040;&#22522;&#30784;&#35774;&#26045;&#21152;&#20256;&#24863;&#22120;(iV2I+)&#12290;&#25552;&#20379;&#20102;&#20851;&#20110;&#36825;&#20004;&#20010;&#25429;&#33719;&#25968;&#25454;&#38598;&#30340;&#35814;&#32454;&#20449;&#24687;&#12290;iV2V&#28085;&#30422;&#20102;&#33258;&#21160;&#24341;&#23548;&#36710;(AGVs)&#20043;&#38388;&#30340;&#20391;&#21521;&#38142;&#36335;&#36890;&#20449;&#22330;&#26223;&#65292;&#32780;iV2I+&#21017;&#26159;&#22312;&#24037;&#19994;&#35774;&#32622;&#20013;&#36827;&#34892;&#30340;&#65292;&#20854;&#20013;&#33258;&#20027;&#28165;&#27905;&#26426;&#22120;&#20154;&#36830;&#25509;&#21040;&#31169;&#26377;&#34562;&#31389;&#32593;&#32476;&#12290;&#19981;&#21516;&#36890;&#20449;&#25216;&#26415;&#30340;&#32452;&#21512;&#65292;&#36830;&#21516;&#20849;&#21516;&#30340;&#27979;&#37327;&#26041;&#27861;&#65292;&#25552;&#20379;&#20102;&#26426;&#22120;&#23398;&#20064;(ML)&#21487;&#20197;&#21033;&#29992;&#30340;&#27934;&#23519;&#21147;&#65292;&#29992;&#20110;&#25351;&#32441;&#35782;&#21035;&#12289;&#35270;&#32447;&#26816;&#27979;&#12289;&#26381;&#21153;&#36136;&#37327;&#39044;&#27979;&#25110;&#38142;&#36335;&#36873;&#25321;&#31561;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#24050;&#26631;&#35760;&#21644;&#39044;&#36807;&#28388;&#65292;&#20197;&#20415;&#24555;&#36895;&#21551;&#21160;&#21644;&#24212;&#29992;&#12290;&#23545;&#20110;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;&#36824;&#35814;&#32454;&#20171;&#32461;&#20102;&#30456;&#24212;&#30340;&#27979;&#35797;&#24179;&#21488;&#21644;&#27979;&#37327;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents two wireless measurement campaigns in industrial testbeds: industrial Vehicle-to-vehicle (iV2V) and industrial Vehicle-to-infrastructure plus Sensor (iV2I+). Detailed information about the two captured datasets is provided as well. iV2V covers sidelink communication scenarios between Automated Guided Vehicles (AGVs), while iV2I+ is conducted at an industrial setting where an autonomous cleaning robot is connected to a private cellular network. The combination of different communication technologies, together with a common measurement methodology, provides insights that can be exploited by Machine Learning (ML) for tasks such as fingerprinting, line-of-sight detection, prediction of quality of service or link selection. Moreover, the datasets are labelled and pre-filtered for fast on-boarding and applicability. The corresponding testbeds and measurements are also presented in detail for both datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;CALIME&#26041;&#27861;&#65292;&#23558;&#22240;&#26524;&#30693;&#35782;&#34701;&#20837;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#20197;&#35299;&#20915;&#29305;&#24449;&#29420;&#31435;&#24615;&#30340;&#32570;&#38519;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#21021;&#22987;&#26041;&#27861;&#30340;&#40657;&#30418;&#27169;&#22411;&#27169;&#25311;&#20445;&#30495;&#24230;&#21644;&#35299;&#37322;&#31283;&#23450;&#24615;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2212.05256</link><description>&lt;p&gt;
CALIME: &#22240;&#26524;&#24863;&#30693;&#30340;&#23616;&#37096;&#21487;&#35299;&#37322;&#24615;&#27169;&#22411;-&#26080;&#20851;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
CALIME: Causality-Aware Local Interpretable Model-Agnostic Explanations. (arXiv:2212.05256v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;CALIME&#26041;&#27861;&#65292;&#23558;&#22240;&#26524;&#30693;&#35782;&#34701;&#20837;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#20197;&#35299;&#20915;&#29305;&#24449;&#29420;&#31435;&#24615;&#30340;&#32570;&#38519;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#21021;&#22987;&#26041;&#27861;&#30340;&#40657;&#30418;&#27169;&#22411;&#27169;&#25311;&#20445;&#30495;&#24230;&#21644;&#35299;&#37322;&#31283;&#23450;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#19968;&#20010;&#37325;&#35201;&#32570;&#28857;&#26159;&#20551;&#35774;&#29305;&#24449;&#29420;&#31435;&#24615;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;&#23558;&#22240;&#26524;&#30693;&#35782;&#34701;&#20837;&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#20013;&#65292;&#20197;&#22686;&#21152;&#20449;&#20219;&#24182;&#24110;&#21161;&#29992;&#25143;&#35780;&#20272;&#35299;&#37322;&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#25193;&#23637;&#26041;&#27861;&#65292;&#26126;&#30830;&#22320;&#22312;&#22260;&#32469;&#36755;&#20837;&#23454;&#20363;&#29983;&#25104;&#30340;&#25968;&#25454;&#20013;&#32534;&#30721;&#22240;&#26524;&#20851;&#31995;&#65292;&#20197;&#35299;&#37322;&#27169;&#22411;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#27169;&#20223;&#40657;&#30418;&#23376;&#30340;&#20445;&#30495;&#24230;&#21644;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#26041;&#38754;&#22343;&#27604;&#21021;&#22987;&#26041;&#27861;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A significant drawback of eXplainable Artificial Intelligence (XAI) approaches is the assumption of feature independence. This paper focuses on integrating causal knowledge in XAI methods to increase trust and help users assess explanations' quality. We propose a novel extension to a widely used local and model-agnostic explainer that explicitly encodes causal relationships in the data generated around the input instance to explain. Extensive experiments show that our method achieves superior performance comparing the initial one for both the fidelity in mimicking the black-box and the stability of the explanations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.13854</link><description>&lt;p&gt;
ComCLIP: &#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
ComCLIP: Training-Free Compositional Image and Text Matching. (arXiv:2211.13854v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#26041;&#38754;&#30340;&#24456;&#22909;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558; CLIP &#36825;&#26679;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#38656;&#35201;&#27169;&#22411;&#29702;&#35299;&#32452;&#21512;&#35789;&#27010;&#24565;&#21644;&#35270;&#35273;&#32452;&#20214;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#35813;&#38382;&#39064;&#65306;&#21333;&#20010;&#23454;&#20307;&#30340;&#38169;&#35823;&#35821;&#20041;&#26412;&#36136;&#19978;&#26159;&#23548;&#33268;&#21305;&#37197;&#22833;&#36133;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#26080;&#38656;&#35757;&#32451;&#8221;&#30340;&#32452;&#21512; CLIP &#27169;&#22411;&#65288;ComCLIP&#65289;&#12290;ComCLIP&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32452;&#21512; CLIP &#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20197;&#22312;&#32452;&#21512;&#25991;&#26412;&#23884;&#20837;&#21644;&#23376;&#22270;&#20687;&#23884;&#20837;&#20043;&#19978;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ComCLIP &#21487;&#20197;&#20943;&#36731;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pretraining (CLIP) has demonstrated great zero-shot performance for matching images and text. However, it is still challenging to adapt vision-lanaguage pretrained models like CLIP to compositional image and text matching -- a more challenging image and text matching task requiring the model understanding of compositional word concepts and visual components. Towards better compositional generalization in zero-shot image and text matching, in this paper, we study the problem from a causal perspective: the erroneous semantics of individual entities are essentially confounders that cause the matching failure. Therefore, we propose a novel \textbf{\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP disentangles input images into subjects, objects, and action sub-images and composes CLIP's vision encoder and text encoder to perform evolving matching over compositional text embedding and sub-image embeddings. In this way, ComCLIP can mitigate spurio
&lt;/p&gt;</description></item></channel></rss>