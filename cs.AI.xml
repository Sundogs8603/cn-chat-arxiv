<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.20331</link><description>&lt;p&gt;
&#19981;&#21487;&#35299;&#38382;&#39064;&#26816;&#27979;&#65306;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Unsolvable Problem Detection: Evaluating Trustworthiness of Vision Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#19988;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#20013;&#33021;&#21542;&#22312;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#21457;&#29616;&#22823;&#22810;&#25968;&#27169;&#22411;&#23384;&#22312;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#37325;&#35201;&#30340;&#25361;&#25112;&#65292;&#21363;Unsolvable Problem Detection&#65288;UPD&#65289;&#65292;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#22312;&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20219;&#21153;&#20013;&#38754;&#23545;&#19981;&#21487;&#35299;&#38382;&#39064;&#26102;&#20445;&#25345;&#31572;&#26696;&#30340;&#33021;&#21147;&#12290;UPD&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#30340;&#35774;&#32622;&#65306;&#32570;&#22833;&#31572;&#26696;&#26816;&#27979;&#65288;AAD&#65289;&#12289;&#19981;&#20860;&#23481;&#31572;&#26696;&#38598;&#26816;&#27979;&#65288;IASD&#65289;&#21644;&#19981;&#20860;&#23481;&#35270;&#35273;&#38382;&#39064;&#26816;&#27979;&#65288;IVQD&#65289;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#28145;&#20837;&#30740;&#31350;UPD&#38382;&#39064;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;VLMs&#65292;&#21253;&#25324;GPT-4V&#21644;LLaVA-Next-34B&#65292;&#22312;&#21508;&#31181;&#31243;&#24230;&#19978;&#37117;&#24456;&#38590;&#24212;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#31361;&#26174;&#20102;&#25913;&#36827;&#30340;&#37325;&#35201;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;UPD&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#26080;&#38656;&#35757;&#32451;&#21644;&#22522;&#20110;&#35757;&#32451;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#23545;&#20854;&#26377;&#25928;&#24615;&#21644;&#23616;&#38480;&#24615;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#35265;&#35299;&#65292;&#20197;&#21450;&#22312;&#25552;&#35758;&#30340;UPD&#35774;&#32622;&#20869;&#30340;&#26410;&#26469;&#21162;&#21147;&#65292;&#23558;&#22686;&#24378;&#23545;VLMs&#30340;&#26356;&#24191;&#27867;&#29702;&#35299;&#21644;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20331v1 Announce Type: cross  Abstract: This paper introduces a novel and significant challenge for Vision Language Models (VLMs), termed Unsolvable Problem Detection (UPD). UPD examines the VLM's ability to withhold answers when faced with unsolvable problems in the context of Visual Question Answering (VQA) tasks. UPD encompasses three distinct settings: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD). To deeply investigate the UPD problem, extensive experiments indicate that most VLMs, including GPT-4V and LLaVA-Next-34B, struggle with our benchmarks to varying extents, highlighting significant room for the improvements. To address UPD, we explore both training-free and training-based solutions, offering new insights into their effectiveness and limitations. We hope our insights, together with future efforts within the proposed UPD settings, will enhance the broader understanding and development of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#28041;&#21450;&#21040;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#31561;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290;</title><link>https://arxiv.org/abs/2403.20329</link><description>&lt;p&gt;
ReALM: &#21442;&#32771;&#35299;&#26512;&#20316;&#20026;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
ReALM: Reference Resolution As Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;LLMs&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#28041;&#21450;&#21040;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#31561;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#32771;&#35299;&#26512;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#23545;&#20110;&#29702;&#35299;&#21644;&#25104;&#21151;&#22788;&#29702;&#21508;&#31181;&#19978;&#19979;&#25991;&#33267;&#20851;&#37325;&#35201;&#12290; &#36825;&#31181;&#19978;&#19979;&#25991;&#26082;&#21253;&#25324;&#20808;&#21069;&#30340;&#23545;&#35805;&#65292;&#20063;&#21253;&#25324;&#19982;&#38750;&#23545;&#35805;&#23454;&#20307;&#30456;&#20851;&#30340;&#19978;&#19979;&#25991;&#65292;&#20363;&#22914;&#29992;&#25143;&#23631;&#24149;&#19978;&#30340;&#23454;&#20307;&#25110;&#21518;&#21488;&#36816;&#34892;&#30340;&#23454;&#20307;&#12290; &#23613;&#31649;&#24050;&#32463;&#35777;&#26126;&#20102;LLMs&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#38750;&#24120;&#24378;&#22823;&#65292;&#20294;&#23427;&#20204;&#22312;&#21442;&#32771;&#35299;&#26512;&#20013;&#30340;&#36816;&#29992;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#23545;&#35805;&#23454;&#20307;&#65292;&#20173;&#26410;&#20805;&#20998;&#21033;&#29992;&#12290; &#26412;&#25991;&#23637;&#31034;&#20102;LLMs&#22914;&#20309;&#34987;&#29992;&#26469;&#21019;&#24314;&#19968;&#20010;&#26497;&#20854;&#26377;&#25928;&#30340;&#31995;&#32479;&#26469;&#35299;&#20915;&#21508;&#31181;&#31867;&#22411;&#30340;&#24341;&#29992;&#38382;&#39064;&#65292;&#36890;&#36807;&#23637;&#31034;&#22914;&#20309;&#23558;&#21442;&#32771;&#35299;&#26512;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#23613;&#31649;&#20854;&#20013;&#28041;&#21450;&#23631;&#24149;&#19978;&#30340;&#36825;&#31181;&#23454;&#20307;&#31561;&#20256;&#32479;&#19978;&#19981;&#26131;&#32422;&#31616;&#20026;&#32431;&#25991;&#26412;&#24418;&#24335;&#30340;&#23454;&#20307;&#12290; &#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#21442;&#32771;&#35299;&#26512;&#20013;&#30456;&#23545;&#20110;&#24050;&#26377;&#31867;&#20284;&#21151;&#33021;&#30340;&#31995;&#32479;&#30340;&#26174;&#30528;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20329v1 Announce Type: cross  Abstract: Reference resolution is an important problem, one that is essential to understand and successfully handle context of different kinds. This context includes both previous turns and context that pertains to non-conversational entities, such as entities on the user's screen or those running in the background. While LLMs have been shown to be extremely powerful for a variety of tasks, their use in reference resolution, particularly for non-conversational entities, remains underutilized. This paper demonstrates how LLMs can be used to create an extremely effective system to resolve references of various types, by showing how reference resolution can be converted into a language modeling problem, despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a text-only modality. We demonstrate large improvements over an existing system with similar functionality across different types of re
&lt;/p&gt;</description></item><item><title>Gecko&#26159;&#19968;&#31181;&#32039;&#20945;&#32780;&#22810;&#21151;&#33021;&#30340;&#25991;&#26412;&#23884;&#20837;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#28860;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20026;&#26816;&#32034;&#22120;&#65292;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;Massive Text Embedding Benchmark&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25152;&#26377;&#26465;&#30446;&#12290;</title><link>https://arxiv.org/abs/2403.20327</link><description>&lt;p&gt;
&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#28860;&#20986;&#30340;&#22810;&#21151;&#33021;&#25991;&#26412;&#23884;&#20837;&#24335;&#27169;&#22411;Gecko
&lt;/p&gt;
&lt;p&gt;
Gecko: Versatile Text Embeddings Distilled from Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20327
&lt;/p&gt;
&lt;p&gt;
Gecko&#26159;&#19968;&#31181;&#32039;&#20945;&#32780;&#22810;&#21151;&#33021;&#30340;&#25991;&#26412;&#23884;&#20837;&#24335;&#27169;&#22411;&#65292;&#36890;&#36807;&#25552;&#28860;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#30693;&#35782;&#20026;&#26816;&#32034;&#22120;&#65292;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;Massive Text Embedding Benchmark&#19978;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#25152;&#26377;&#26465;&#30446;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Gecko&#65292;&#19968;&#31181;&#32039;&#20945;&#32780;&#22810;&#21151;&#33021;&#30340;&#25991;&#26412;&#23884;&#20837;&#24335;&#27169;&#22411;&#12290;Gecko&#36890;&#36807;&#21033;&#29992;&#19968;&#20010;&#20851;&#38190;&#24605;&#24819;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#26816;&#32034;&#24615;&#33021;&#65306;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20013;&#30340;&#30693;&#35782;&#25552;&#28860;&#20026;&#26816;&#32034;&#22120;&#12290;&#25105;&#20204;&#30340;&#20004;&#27493;&#25552;&#28860;&#36807;&#31243;&#39318;&#20808;&#28041;&#21450;&#20351;&#29992;LLM&#29983;&#25104;&#22810;&#26679;&#21270;&#21512;&#25104;&#30340;&#37197;&#23545;&#25968;&#25454;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;&#27599;&#20010;&#26597;&#35810;&#26816;&#32034;&#19968;&#32452;&#20505;&#36873;&#27573;&#33853;&#65292;&#24182;&#20351;&#29992;&#30456;&#21516;&#30340;LLM&#37325;&#26032;&#26631;&#35760;&#27491;&#38754;&#21644;&#22256;&#38590;&#30340;&#36127;&#38754;&#27573;&#33853;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#25968;&#25454;&#36136;&#37327;&#12290;Gecko&#27169;&#22411;&#30340;&#32039;&#20945;&#24615;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#23884;&#20837;&#24335;&#22522;&#20934;&#27979;&#35797;(MTEB)&#19978;&#65292;&#20855;&#26377;256&#23884;&#20837;&#32500;&#24230;&#30340;Gecko&#30340;&#34920;&#29616;&#20248;&#20110;&#25152;&#26377;&#29616;&#26377;&#30340;768&#23884;&#20837;&#23610;&#23544;&#30340;&#26465;&#30446;&#12290;&#20855;&#26377;768&#23884;&#20837;&#32500;&#24230;&#30340;Gecko&#23454;&#29616;&#20102;&#24179;&#22343;&#24471;&#20998;66.31&#65292;&#19982;7&#20493;&#22823;&#30340;&#27169;&#22411;&#21644;5&#20493;&#39640;&#32500;&#23884;&#20837;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20327v1 Announce Type: new  Abstract: We present Gecko, a compact and versatile text embedding model. Gecko achieves strong retrieval performance by leveraging a key idea: distilling knowledge from large language models (LLMs) into a retriever. Our two-step distillation process begins with generating diverse, synthetic paired data using an LLM. Next, we further refine the data quality by retrieving a set of candidate passages for each query, and relabeling the positive and hard negative passages using the same LLM. The effectiveness of our approach is demonstrated by the compactness of the Gecko. On the Massive Text Embedding Benchmark (MTEB), Gecko with 256 embedding dimensions outperforms all existing entries with 768 embedding size. Gecko with 768 embedding dimensions achieves an average score of 66.31, competing with 7x larger models and 5x higher dimensional embeddings.
&lt;/p&gt;</description></item><item><title>MTLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#20302;&#31209;&#36866;&#24212;&#27169;&#22359;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#21442;&#25968;&#31354;&#38388;&#30340;&#20998;&#31163;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#20219;&#21153;&#19987;&#19994;&#21270;&#21644;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;</title><link>https://arxiv.org/abs/2403.20320</link><description>&lt;p&gt;
MTLoRA: &#19968;&#31181;&#29992;&#20110;&#39640;&#25928;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#20302;&#31209;&#36866;&#24212;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MTLoRA: A Low-Rank Adaptation Approach for Efficient Multi-Task Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20320
&lt;/p&gt;
&lt;p&gt;
MTLoRA&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#26694;&#26550;&#65292;&#36890;&#36807;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#20302;&#31209;&#36866;&#24212;&#27169;&#22359;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#21442;&#25968;&#31354;&#38388;&#30340;&#20998;&#31163;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#28789;&#27963;&#22788;&#29702;&#20219;&#21153;&#19987;&#19994;&#21270;&#21644;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#29615;&#22659;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#21040;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#26159;&#28145;&#24230;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#23558;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#21040;&#19981;&#21516;&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#26223;&#30340;&#26041;&#24335;&#65292;&#21516;&#26102;&#20165;&#35757;&#32451;&#23569;&#37327;&#21442;&#25968;&#12290;&#34429;&#28982;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#26159;&#20026;&#21333;&#20219;&#21153;&#36866;&#24212;&#32780;&#35774;&#35745;&#30340;&#65292;&#20294;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#26550;&#26500;&#20013;&#36827;&#34892;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#20173;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MTLoRA&#65292;&#19968;&#31181;&#29992;&#20110;&#22810;&#20219;&#21153;&#23398;&#20064;&#27169;&#22411;&#21442;&#25968;&#39640;&#25928;&#35757;&#32451;&#30340;&#26032;&#26694;&#26550;&#12290;MTLoRA&#37319;&#29992;&#20219;&#21153;&#19981;&#21487;&#30693;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#20302;&#31209;&#36866;&#24212;&#27169;&#22359;&#65292;&#26377;&#25928;&#22320;&#20998;&#24320;&#20102;MTL&#24494;&#35843;&#20013;&#30340;&#21442;&#25968;&#31354;&#38388;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#29087;&#32451;&#22788;&#29702;MTL&#19978;&#19979;&#25991;&#20013;&#30340;&#20219;&#21153;&#19987;&#38376;&#21270;&#21644;&#20132;&#20114;&#12290;&#25105;&#20204;&#23558;MTLoRA&#24212;&#29992;&#20110;&#22522;&#20110;&#20998;&#23618;&#21464;&#21387;&#22120;&#30340;MTL&#26550;&#26500;&#65292;&#23558;&#23427;&#20204;&#35843;&#25972;&#21040;&#22810;&#20010;&#19979;&#28216;&#23494;&#38598;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20320v1 Announce Type: cross  Abstract: Adapting models pre-trained on large-scale datasets to a variety of downstream tasks is a common strategy in deep learning. Consequently, parameter-efficient fine-tuning methods have emerged as a promising way to adapt pre-trained models to different tasks while training only a minimal number of parameters. While most of these methods are designed for single-task adaptation, parameter-efficient training in Multi-Task Learning (MTL) architectures is still unexplored. In this paper, we introduce MTLoRA, a novel framework for parameter-efficient training of MTL models. MTLoRA employs Task-Agnostic and Task-Specific Low-Rank Adaptation modules, which effectively disentangle the parameter space in MTL fine-tuning, thereby enabling the model to adeptly handle both task specialization and interaction within MTL contexts. We applied MTLoRA to hierarchical-transformer-based MTL architectures, adapting them to multiple downstream dense predictio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22238;&#24402;&#21644;Dice&#25439;&#22833;&#22312;&#22823;&#29289;&#20307;&#19978;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#21457;&#29616;Dice&#25439;&#22833;&#30456;&#23545;&#20110;&#22238;&#24402;&#25439;&#22833;&#22312;&#22823;&#29289;&#20307;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#27169;&#22411;&#25910;&#25947;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.20318</link><description>&lt;p&gt;
SeaBird: &#20351;&#29992;Dice&#25439;&#22833;&#36827;&#34892;&#40479;&#30640;&#22270;&#20998;&#21106;&#25913;&#36827;&#20102;&#21333;&#30446;&#22823;&#29289;&#20307;&#30340;3D&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SeaBird: Segmentation in Bird's View with Dice Loss Improves Monocular 3D Detection of Large Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20318
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22238;&#24402;&#21644;Dice&#25439;&#22833;&#22312;&#22823;&#29289;&#20307;&#19978;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#21457;&#29616;Dice&#25439;&#22833;&#30456;&#23545;&#20110;&#22238;&#24402;&#25439;&#22833;&#22312;&#22823;&#29289;&#20307;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#27169;&#22411;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#30446;3D&#26816;&#27979;&#22120;&#22312;&#27773;&#36710;&#21644;&#36739;&#23567;&#29289;&#20307;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#24615;&#33021;&#65292;&#20294;&#22312;&#36739;&#22823;&#29289;&#20307;&#19978;&#24615;&#33021;&#19979;&#38477;&#65292;&#23548;&#33268;&#33268;&#21629;&#20107;&#25925;&#12290;&#19968;&#20123;&#20154;&#35748;&#20026;&#22833;&#36133;&#30340;&#21407;&#22240;&#26159;&#35757;&#32451;&#25968;&#25454;&#31232;&#32570;&#25110;&#32773;&#22823;&#29289;&#20307;&#30340;&#24863;&#21463;&#37326;&#35201;&#27714;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#31361;&#20986;&#20102;&#36825;&#20010;&#30740;&#31350;&#19981;&#36275;&#30340;&#38382;&#39064;&#65292;&#21363;&#23545;&#22823;&#29289;&#20307;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#21457;&#29616;&#29616;&#20195;&#21069;&#32622;&#26816;&#27979;&#22120;&#29978;&#33267;&#22312;&#20960;&#20046;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#38590;&#20197;&#27867;&#21270;&#21040;&#22823;&#29289;&#20307;&#12290;&#25105;&#20204;&#35748;&#20026;&#22833;&#36133;&#30340;&#21407;&#22240;&#26159;&#28145;&#24230;&#22238;&#24402;&#25439;&#22833;&#23545;&#22823;&#29289;&#20307;&#30340;&#22122;&#22768;&#25935;&#24863;&#24615;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#20840;&#38754;&#30740;&#31350;&#20102;&#22238;&#24402;&#21644;dice&#25439;&#22833;&#65292;&#32771;&#23519;&#23427;&#20204;&#22312;&#19981;&#21516;&#35823;&#24046;&#27700;&#24179;&#21644;&#29289;&#20307;&#23610;&#23544;&#19979;&#30340;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#25968;&#23398;&#19978;&#35777;&#26126;&#20102;&#19982;&#22238;&#24402;&#25439;&#22833;&#30456;&#27604;&#65292;&#22312;&#31616;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;dice&#25439;&#22833;&#23545;&#20110;&#22823;&#29289;&#20307;&#30340;&#22122;&#22768;&#40065;&#26834;&#24615;&#21644;&#27169;&#22411;&#25910;&#25947;&#24615;&#26356;&#20339;&#12290;&#21033;&#29992;&#25105;&#20204;&#30340;&#29702;&#35770;&#35265;&#35299;&#65292;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20318v1 Announce Type: cross  Abstract: Monocular 3D detectors achieve remarkable performance on cars and smaller objects. However, their performance drops on larger objects, leading to fatal accidents. Some attribute the failures to training data scarcity or their receptive field requirements of large objects. In this paper, we highlight this understudied problem of generalization to large objects. We find that modern frontal detectors struggle to generalize to large objects even on nearly balanced datasets. We argue that the cause of failure is the sensitivity of depth regression losses to noise of larger objects. To bridge this gap, we comprehensively investigate regression and dice losses, examining their robustness under varying error levels and object sizes. We mathematically prove that the dice loss leads to superior noise-robustness and model convergence for large objects compared to regression losses for a simplified case. Leveraging our theoretical insights, we pro
&lt;/p&gt;</description></item><item><title>ChainNet&#26159;&#19968;&#20010;&#35789;&#20856;&#36164;&#28304;&#65292;&#39318;&#27425;&#26126;&#30830;&#22320;&#35782;&#21035;&#20102;WordNet&#20013;&#35789;&#20041;&#30340;&#32467;&#26500;&#21270;&#38544;&#21947;&#21644;&#36716;&#21947;&#20851;&#31995;&#65292;&#25104;&#20026;&#31532;&#19968;&#20010;&#20855;&#26377;&#22522;&#30784;&#38544;&#21947;&#21644;&#36716;&#21947;&#25968;&#25454;&#38598;&#30340;&#36164;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.20308</link><description>&lt;p&gt;
ChainNet: &#22312;WordNet&#20013;&#30340;&#32467;&#26500;&#21270;&#38544;&#21947;&#21644;&#36716;&#21947;
&lt;/p&gt;
&lt;p&gt;
ChainNet: Structured Metaphor and Metonymy in WordNet
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20308
&lt;/p&gt;
&lt;p&gt;
ChainNet&#26159;&#19968;&#20010;&#35789;&#20856;&#36164;&#28304;&#65292;&#39318;&#27425;&#26126;&#30830;&#22320;&#35782;&#21035;&#20102;WordNet&#20013;&#35789;&#20041;&#30340;&#32467;&#26500;&#21270;&#38544;&#21947;&#21644;&#36716;&#21947;&#20851;&#31995;&#65292;&#25104;&#20026;&#31532;&#19968;&#20010;&#20855;&#26377;&#22522;&#30784;&#38544;&#21947;&#21644;&#36716;&#21947;&#25968;&#25454;&#38598;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20010;&#35789;&#30340;&#24847;&#20041;&#23637;&#29616;&#20986;&#20016;&#23500;&#30340;&#20869;&#37096;&#32467;&#26500;&#12290;&#22312;&#20856;&#22411;&#30340;&#35789;&#20856;&#20013;&#65292;&#36825;&#31181;&#32467;&#26500;&#24448;&#24448;&#34987;&#24573;&#35270;&#65306;&#19968;&#20010;&#35789;&#30340;&#24847;&#20041;&#34987;&#32534;&#30721;&#20026;&#19968;&#20010;&#27809;&#26377;&#24847;&#20041;&#38388;&#20851;&#31995;&#30340;&#21015;&#34920;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;ChainNet&#65292;&#36825;&#26159;&#19968;&#20010;&#35789;&#20856;&#36164;&#28304;&#65292;&#39318;&#27425;&#26126;&#30830;&#22320;&#35782;&#21035;&#20102;&#36825;&#20123;&#32467;&#26500;&#12290;ChainNet&#34920;&#36798;&#20102;&#22312;Open English Wordnet&#20013;&#35789;&#20041;&#22914;&#20309;&#20174;&#24444;&#27492;&#34893;&#29983;&#20986;&#26469;&#65306;&#19968;&#20010;&#35789;&#30340;&#27599;&#20010;&#21517;&#35789;&#24847;&#20041;&#35201;&#20040;&#36890;&#36807;&#38544;&#21947;&#25110;&#36716;&#21947;&#19982;&#21478;&#19968;&#20010;&#24847;&#20041;&#30456;&#36830;&#65292;&#35201;&#20040;&#22312;&#21516;&#20041;&#35789;&#30340;&#24773;&#20917;&#19979;&#26159;&#26029;&#24320;&#30340;&#12290;&#30001;&#20110;WordNet&#30340;&#21547;&#20041;&#19982;&#25429;&#25417;&#20854;&#21547;&#20041;&#20449;&#24687;&#30340;&#36164;&#28304;&#30456;&#36830;&#65292;ChainNet&#20195;&#34920;&#20102;&#31532;&#19968;&#20010;&#20855;&#26377;&#22522;&#30784;&#38544;&#21947;&#21644;&#36716;&#21947;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20308v1 Announce Type: cross  Abstract: The senses of a word exhibit rich internal structure. In a typical lexicon, this structure is overlooked: a word's senses are encoded as a list without inter-sense relations. We present ChainNet, a lexical resource which for the first time explicitly identifies these structures. ChainNet expresses how senses in the Open English Wordnet are derived from one another: every nominal sense of a word is either connected to another sense by metaphor or metonymy, or is disconnected in the case of homonymy. Because WordNet senses are linked to resources which capture information about their meaning, ChainNet represents the first dataset of grounded metaphor and metonymy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#23558;&#33021;&#25928;&#20316;&#20026;LLM&#25512;&#29702;&#30340;&#20027;&#35201;&#30446;&#26631;&#65292;&#24182;&#25506;&#35752;&#22312;&#28385;&#36275;&#24615;&#33021;&#30446;&#26631;&#30340;&#21069;&#25552;&#19979;&#22914;&#20309;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#33021;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.20306</link><description>&lt;p&gt;
&#26397;&#30528;&#26356;&#32511;&#33394;&#30340;LLMs&#65306;&#23558;&#33021;&#25928;&#24341;&#20837;LLM&#25512;&#29702;&#30340;&#21069;&#27839;
&lt;/p&gt;
&lt;p&gt;
Towards Greener LLMs: Bringing Energy-Efficiency to the Forefront of LLM Inference
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20306
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23558;&#33021;&#25928;&#20316;&#20026;LLM&#25512;&#29702;&#30340;&#20027;&#35201;&#30446;&#26631;&#65292;&#24182;&#25506;&#35752;&#22312;&#28385;&#36275;&#24615;&#33021;&#30446;&#26631;&#30340;&#21069;&#25552;&#19979;&#22914;&#20309;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#33021;&#28304;&#21033;&#29992;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29616;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#34892;&#19994;&#20013;&#30340;&#26222;&#36941;&#20351;&#29992;&#65292;&#29992;&#20110;&#36825;&#20123;&#27169;&#22411;&#30340;&#25512;&#29702;&#26381;&#21153;&#27491;&#22312;&#19981;&#26029;&#25193;&#23637;&#12290;&#37492;&#20110;&#29616;&#20195;LLMs&#30340;&#39640;&#35745;&#31639;&#21644;&#20869;&#23384;&#35201;&#27714;&#65292;&#36234;&#26469;&#36234;&#22810;&#39030;&#23574;&#30340;GPU&#34987;&#37096;&#32626;&#26469;&#25552;&#20379;&#36825;&#20123;&#27169;&#22411;&#30340;&#26381;&#21153;&#12290;&#33021;&#28304;&#20379;&#24212;&#24050;&#25104;&#20026;&#25968;&#25454;&#20013;&#24515;&#25193;&#23637;&#20197;&#25552;&#20379;&#36825;&#20123;&#27169;&#22411;&#26381;&#21153;&#30340;&#26368;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#33021;&#25928;&#20026;LLM&#26381;&#21153;&#30340;&#39318;&#35201;&#30446;&#26631;&#25152;&#24102;&#26469;&#30340;&#25240;&#34935;&#26041;&#26696;&#65292;&#21516;&#26102;&#28385;&#36275;&#24615;&#33021;SLOs&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26681;&#25454;&#36755;&#20837;&#12289;&#27169;&#22411;&#21644;&#26381;&#21153;&#27700;&#24179;&#21327;&#35758;&#65292;LLM&#25512;&#29702;&#25552;&#20379;&#32773;&#26377;&#20960;&#20010;&#26059;&#38062;&#21487;&#29992;&#20110;&#23454;&#29616;&#33021;&#25928;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#26059;&#38062;&#23545;&#24310;&#36831;&#12289;&#21534;&#21520;&#37327;&#20197;&#21450;&#33021;&#28304;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#34920;&#24449;&#12290;&#36890;&#36807;&#25506;&#32034;&#36825;&#20123;&#25240;&#34935;&#26041;&#26696;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20248;&#21270;&#33021;&#28304;&#20351;&#29992;&#32780;&#19981;&#29306;&#29298;&#24615;&#33021;&#30340;&#23453;&#36149;&#35265;&#35299;&#65292;&#20174;&#32780;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20306v1 Announce Type: new  Abstract: With the ubiquitous use of modern large language models (LLMs) across industries, the inference serving for these models is ever expanding. Given the high compute and memory requirements of modern LLMs, more and more top-of-the-line GPUs are being deployed to serve these models. Energy availability has come to the forefront as the biggest challenge for data center expansion to serve these models. In this paper, we present the trade-offs brought up by making energy efficiency the primary goal of LLM serving under performance SLOs. We show that depending on the inputs, the model, and the service-level agreements, there are several knobs available to the LLM inference provider to use for being energy efficient. We characterize the impact of these knobs on the latency, throughput, as well as the energy. By exploring these trade-offs, we offer valuable insights into optimizing energy usage without compromising on performance, thereby paving t
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22312;&#36755;&#20986;&#27010;&#29575;&#20998;&#24067;&#19978;&#20351;&#29992;&#21551;&#21457;&#24335;&#25628;&#32034;&#26041;&#27861;&#26469;&#25913;&#36827;ML&#23616;&#37096;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#27515;&#38145;&#24182;&#21551;&#29992;&#23436;&#25972;&#26102;&#38388;&#36328;&#24230;&#35268;&#21010;&#30340;&#20027;&#35201;&#24819;&#27861;</title><link>https://arxiv.org/abs/2403.20300</link><description>&lt;p&gt;
&#29992;&#21551;&#21457;&#24335;&#25628;&#32034;&#25913;&#36827;&#23398;&#21040;&#30340;&#23616;&#37096;MAPF&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Improving Learnt Local MAPF Policies with Heuristic Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20300
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22312;&#36755;&#20986;&#27010;&#29575;&#20998;&#24067;&#19978;&#20351;&#29992;&#21551;&#21457;&#24335;&#25628;&#32034;&#26041;&#27861;&#26469;&#25913;&#36827;ML&#23616;&#37096;&#31574;&#30053;&#65292;&#20197;&#35299;&#20915;&#27515;&#38145;&#24182;&#21551;&#29992;&#23436;&#25972;&#26102;&#38388;&#36328;&#24230;&#35268;&#21010;&#30340;&#20027;&#35201;&#24819;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#26159;&#22312;&#22242;&#38431;&#26234;&#33021;&#20307;&#21040;&#36798;&#30446;&#26631;&#20301;&#32622;&#26102;&#25214;&#21040;&#26080;&#30896;&#25758;&#36335;&#24452;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#26368;&#20808;&#36827;&#30340;&#32463;&#20856;MAPF&#27714;&#35299;&#22120;&#36890;&#24120;&#37319;&#29992;&#21551;&#21457;&#24335;&#25628;&#32034;&#26469;&#25214;&#21040;&#25968;&#30334;&#20010;&#26234;&#33021;&#20307;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20294;&#36890;&#24120;&#26159;&#38598;&#20013;&#24335;&#30340;&#65292;&#24182;&#19988;&#22312;&#30701;&#26102;&#38388;&#20869;&#36816;&#34892;&#26102;&#24456;&#38590;&#25193;&#23637;&#12290;&#23398;&#20064;&#27599;&#20010;&#26234;&#33021;&#20307;&#31574;&#30053;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#26041;&#27861;&#24456;&#26377;&#21560;&#24341;&#21147;&#65292;&#22240;&#20026;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#31995;&#32479;&#24182;&#19988;&#22312;&#20445;&#25345;&#33391;&#22909;&#35299;&#20915;&#26041;&#26696;&#36136;&#37327;&#30340;&#21516;&#26102;&#21487;&#20197;&#24456;&#22909;&#22320;&#25193;&#23637;&#12290;&#24403;&#21069;&#20851;&#20110;MAPF&#30340;ML&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#20123;&#26041;&#27861;&#65292;&#24050;&#32463;&#24320;&#22987;&#25506;&#35752;&#36825;&#19968;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;ML&#26041;&#27861;&#29983;&#25104;&#8220;&#23616;&#37096;&#8221;&#31574;&#30053;&#65292;&#20165;&#35745;&#21010;&#21333;&#20010;&#26102;&#38388;&#27493;&#65292;&#24182;&#19988;&#25104;&#21151;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#36739;&#24046;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#24819;&#27861;&#26159;&#65292;&#36890;&#36807;&#20351;&#29992;&#21551;&#21457;&#24335;&#25628;&#32034;&#26041;&#27861;&#22788;&#29702;&#36755;&#20986;&#27010;&#29575;&#20998;&#24067;&#65292;&#25105;&#20204;&#21487;&#20197;&#25913;&#36827;ML&#23616;&#37096;&#31574;&#30053;&#20197;&#35299;&#20915;&#27515;&#38145;&#24182;&#21551;&#29992;&#23436;&#25972;&#26102;&#38388;&#36328;&#24230;&#30340;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20300v1 Announce Type: cross  Abstract: Multi-agent path finding (MAPF) is the problem of finding collision-free paths for a team of agents to reach their goal locations. State-of-the-art classical MAPF solvers typically employ heuristic search to find solutions for hundreds of agents but are typically centralized and can struggle to scale when run with short timeouts. Machine learning (ML) approaches that learn policies for each agent are appealing as these could enable decentralized systems and scale well while maintaining good solution quality. Current ML approaches to MAPF have proposed methods that have started to scratch the surface of this potential. However, state-of-the-art ML approaches produce "local" policies that only plan for a single timestep and have poor success rates and scalability. Our main idea is that we can improve a ML local policy by using heuristic search methods on the output probability distribution to resolve deadlocks and enable full horizon pla
&lt;/p&gt;</description></item><item><title>LLMs&#22312;&#21307;&#23398;&#20915;&#31574;&#20013;&#25552;&#20379;&#37325;&#35201;&#21453;&#39304;&#65292;&#21487;&#20197;&#25361;&#25112;&#19981;&#27491;&#30830;&#30340;&#35786;&#26029;&#65292;&#20419;&#36827;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;</title><link>https://arxiv.org/abs/2403.20288</link><description>&lt;p&gt;
LLM&#33021;&#22815;&#22312;&#21307;&#23398;&#39046;&#22495;&#20013;&#32416;&#27491;&#21307;&#29983;&#21527;&#65311;&#30740;&#31350;&#26377;&#25928;&#30340;&#20132;&#20114;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Can LLMs Correct Physicians, Yet? Investigating Effective Interaction Methods in the Medical Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20288
&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#21307;&#23398;&#20915;&#31574;&#20013;&#25552;&#20379;&#37325;&#35201;&#21453;&#39304;&#65292;&#21487;&#20197;&#25361;&#25112;&#19981;&#27491;&#30830;&#30340;&#35786;&#26029;&#65292;&#20419;&#36827;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21327;&#21161;&#24182;&#21487;&#33021;&#32416;&#27491;&#21307;&#29983;&#36827;&#34892;&#21307;&#30103;&#20915;&#31574;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20960;&#31181;LLMs&#65292;&#21253;&#25324;Meditron&#12289;Llama2&#21644;Mistral&#65292;&#20998;&#26512;&#36825;&#20123;&#27169;&#22411;&#22312;&#19981;&#21516;&#24773;&#26223;&#19979;&#19982;&#21307;&#29983;&#26377;&#25928;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#26469;&#33258;PubMedQA&#30340;&#38382;&#39064;&#21644;&#20960;&#39033;&#20219;&#21153;&#65292;&#20174;&#20108;&#20803;&#65288;&#26159;/&#21542;&#65289;&#22238;&#31572;&#21040;&#38271;&#31572;&#26696;&#29983;&#25104;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#31572;&#26696;&#26159;&#22312;&#19982;&#21307;&#29983;&#20132;&#20114;&#21518;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25552;&#31034;&#35774;&#35745;&#26174;&#33879;&#24433;&#21709;&#20102;LLMs&#30340;&#19979;&#28216;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;LLMs&#21487;&#20197;&#20026;&#21307;&#29983;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#21453;&#39304;&#65292;&#25361;&#25112;&#19981;&#27491;&#30830;&#30340;&#35786;&#26029;&#65292;&#20419;&#36827;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#20363;&#22914;&#65292;&#24403;&#21307;&#29983;&#20934;&#30830;&#29575;&#20026;38%&#26102;&#65292;Mistral&#21487;&#20197;&#32473;&#20986;&#27491;&#30830;&#31572;&#26696;&#65292;&#26681;&#25454;&#25152;&#20351;&#29992;&#30340;&#25552;&#31034;&#65292;&#23558;&#20934;&#30830;&#24615;&#25552;&#39640;&#21040;74%&#65292;&#32780;Llama2&#21644;Meditron&#27169;&#22411;&#20063;&#33021;&#25552;&#20379;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20288v1 Announce Type: cross  Abstract: We explore the potential of Large Language Models (LLMs) to assist and potentially correct physicians in medical decision-making tasks. We evaluate several LLMs, including Meditron, Llama2, and Mistral, to analyze the ability of these models to interact effectively with physicians across different scenarios. We consider questions from PubMedQA and several tasks, ranging from binary (yes/no) responses to long answer generation, where the answer of the model is produced after an interaction with a physician. Our findings suggest that prompt design significantly influences the downstream accuracy of LLMs and that LLMs can provide valuable feedback to physicians, challenging incorrect diagnoses and contributing to more accurate decision-making. For example, when the physician is accurate 38% of the time, Mistral can produce the correct answer, improving accuracy up to 74% depending on the prompt being used, while Llama2 and Meditron models
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#26550;&#26500;&#22312;&#27169;&#24577;&#26679;&#26412;&#31232;&#30095;&#23545;&#40784;&#26102;&#23398;&#20064;&#31283;&#20581;&#23884;&#20837;&#31354;&#38388;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#65288;MCA&#65289;&#26426;&#21046;&#65292;&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20280</link><description>&lt;p&gt;
&#24102;&#26377;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#30340;&#31232;&#30095;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Sparse multimodal fusion with modal channel attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20280
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#26550;&#26500;&#22312;&#27169;&#24577;&#26679;&#26412;&#31232;&#30095;&#23545;&#40784;&#26102;&#23398;&#20064;&#31283;&#20581;&#23884;&#20837;&#31354;&#38388;&#30340;&#33021;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#65288;MCA&#65289;&#26426;&#21046;&#65292;&#21487;&#20197;&#25913;&#21892;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#21644;&#19979;&#28216;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27979;&#37327;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#20316;&#20026;&#27169;&#24577;&#31232;&#30095;&#24230;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#30740;&#31350;&#20102;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#25442;&#22120;&#26550;&#26500;&#22312;&#27169;&#24577;&#26679;&#26412;&#31232;&#30095;&#23545;&#40784;&#26102;&#23398;&#20064;&#31283;&#20581;&#23884;&#20837;&#31354;&#38388;&#30340;&#33021;&#21147;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#25193;&#23637;&#30340;&#33945;&#29305;&#21345;&#32599;&#22810;&#27169;&#24577;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#20013;&#24341;&#20837;&#20102;&#27169;&#24577;&#19981;&#23436;&#20840;&#36890;&#36947;&#65292;&#31216;&#20026;&#27169;&#24577;&#36890;&#36947;&#27880;&#24847;&#21147;&#65288;MCA&#65289;&#12290;&#20351;&#29992;&#20102;&#21253;&#21547;4&#31181;&#27169;&#24577;&#30340;&#20004;&#20010;&#25968;&#25454;&#38598;&#65292;CMU-MOSEI&#29992;&#20110;&#22810;&#27169;&#24577;&#24773;&#24863;&#35782;&#21035;&#65292;TCGA&#29992;&#20110;&#22810;&#32452;&#23398;&#12290;&#27169;&#22411;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#26679;&#26412;&#20013;&#21482;&#29992;&#20102;&#22235;&#31181;&#27169;&#24577;&#20013;&#30340;&#20004;&#31181;&#23601;&#23398;&#20064;&#20986;&#32479;&#19968;&#19988;&#23545;&#40784;&#30340;&#23884;&#20837;&#31354;&#38388;&#12290;&#21457;&#29616;&#65292;&#21363;&#20351;&#27809;&#26377;&#27169;&#24577;&#31232;&#30095;&#65292;&#25152;&#25552;&#20986;&#30340;MCA&#26426;&#21046;&#20063;&#33021;&#25552;&#39640;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36136;&#37327;&#65292;&#21484;&#22238;&#25351;&#26631;&#65292;&#24182;&#25552;&#39640;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20280v1 Announce Type: cross  Abstract: The ability of masked multimodal transformer architectures to learn a robust embedding space when modality samples are sparsely aligned is studied by measuring the quality of generated embedding spaces as a function of modal sparsity. An extension to the masked multimodal transformer model is proposed which incorporates modal-incomplete channels in the multihead attention mechanism called modal channel attention (MCA). Two datasets with 4 modalities are used, CMU-MOSEI for multimodal sentiment recognition and TCGA for multiomics. Models are shown to learn uniform and aligned embedding spaces with only two out of four modalities in most samples. It was found that, even with no modal sparsity, the proposed MCA mechanism improves the quality of generated embedding spaces, recall metrics, and subsequent performance on downstream tasks.
&lt;/p&gt;</description></item><item><title>Latxa&#26159;&#19968;&#31181;&#29992;&#20110;&#24052;&#26031;&#20811;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29087;&#32451;&#24230;&#21644;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#25152;&#26377;&#20197;&#21069;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24052;&#26031;&#20811;&#35821;&#39640;&#36136;&#37327;&#22522;&#20934;&#30340;&#19981;&#36275;&#12290;</title><link>https://arxiv.org/abs/2403.20266</link><description>&lt;p&gt;
Latxa: &#19968;&#31181;&#29992;&#20110;&#24052;&#26031;&#20811;&#35821;&#30340;&#24320;&#25918;&#35821;&#35328;&#27169;&#22411;&#21644;&#35780;&#20272;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
Latxa: An Open Language Model and Evaluation Suite for Basque
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20266
&lt;/p&gt;
&lt;p&gt;
Latxa&#26159;&#19968;&#31181;&#29992;&#20110;&#24052;&#26031;&#20811;&#35821;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29087;&#32451;&#24230;&#21644;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20248;&#20110;&#25152;&#26377;&#20197;&#21069;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#20855;&#26377;&#22810;&#20010;&#35780;&#20272;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#24052;&#26031;&#20811;&#35821;&#39640;&#36136;&#37327;&#22522;&#20934;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Latxa&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Llama 2&#30340;&#22823;&#22411;&#24052;&#26031;&#20811;&#35821;&#35328;&#27169;&#22411;&#31995;&#21015;&#65292;&#21442;&#25968;&#33539;&#22260;&#20174;7&#21040;700&#20159;&#12290;Latxa&#22522;&#20110;&#26032;&#30340;&#24052;&#26031;&#20811;&#35821;&#35821;&#26009;&#24211;&#39044;&#35757;&#32451;&#65292;&#21253;&#25324;430&#19975;&#20010;&#25991;&#26723;&#21644;42&#20159;&#20010;&#26631;&#35760;&#12290;&#38024;&#23545;&#24052;&#26031;&#20811;&#35821;&#39640;&#36136;&#37327;&#22522;&#20934;&#30340;&#31232;&#32570;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;4&#20010;&#22810;&#39033;&#36873;&#25321;&#35780;&#20272;&#25968;&#25454;&#38598;&#65306;EusProficiency&#65292;&#21253;&#25324;&#26469;&#33258;&#23448;&#26041;&#35821;&#35328;&#33021;&#21147;&#32771;&#35797;&#30340;5169&#20010;&#38382;&#39064;&#65307;EusReading&#65292;&#21253;&#25324;352&#20010;&#38405;&#35835;&#29702;&#35299;&#38382;&#39064;&#65307;EusTrivia&#65292;&#21253;&#25324;&#26469;&#33258;5&#20010;&#30693;&#35782;&#39046;&#22495;&#30340;1715&#20010;&#29712;&#20107;&#38382;&#39064;&#65307;&#20197;&#21450;EusExams&#65292;&#21253;&#25324;&#26469;&#33258;&#20844;&#20849;&#32771;&#35797;&#30340;16774&#20010;&#38382;&#39064;&#12290;&#22312;&#25105;&#20204;&#30340;&#24191;&#27867;&#35780;&#20272;&#20013;&#65292;Latxa&#22312;&#19982;&#25105;&#20204;&#27604;&#36739;&#30340;&#25152;&#26377;&#20808;&#21069;&#24320;&#25918;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#27492;&#22806;&#65292;&#23613;&#31649;&#22312;&#38405;&#35835;&#29702;&#35299;&#21644;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#26041;&#38754;&#33853;&#21518;&#65292;&#20294;&#22312;&#35821;&#35328;&#29087;&#32451;&#24230;&#21644;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#65292;&#23427;&#19982;GPT-4 Turbo&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;Latxa&#27169;&#22411;&#31995;&#21015;&#65292;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20266v1 Announce Type: cross  Abstract: We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934; ELITR-Bench&#65292;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377; ELITR &#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#20013;&#28155;&#21152;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.20262</link><description>&lt;p&gt;
ELITR-Bench: &#38754;&#21521;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#20250;&#35758;&#21161;&#29702;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
ELITR-Bench: A Meeting Assistant Benchmark for Long-Context Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20262
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934; ELITR-Bench&#65292;&#19987;&#27880;&#20110;&#38271;&#19978;&#19979;&#25991;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#65292;&#36890;&#36807;&#22312;&#29616;&#26377; ELITR &#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#20013;&#28155;&#21152;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21644;&#30495;&#23454;&#31572;&#26696;&#65292;&#25581;&#31034;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#65292;&#20027;&#35201;&#33268;&#21147;&#20110;&#25193;&#23637;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#65292;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#38271;&#25991;&#26723;&#20869;&#37096;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#38271;&#36317;&#31163;&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#20294;&#29616;&#26377;&#30340;&#21162;&#21147;&#20027;&#35201;&#32771;&#34385;&#30340;&#26159;&#19981;&#19968;&#23450;&#19982;&#29616;&#23454;&#24212;&#29992;&#30456;&#20851;&#30340;&#36890;&#29992;&#20219;&#21153;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#23454;&#38469;&#20250;&#35758;&#21161;&#29702;&#22330;&#26223;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#30340;&#26032;&#22522;&#20934;&#12290;&#22312;&#36825;&#31181;&#24773;&#26223;&#19979;&#65292;&#38271;&#19978;&#19979;&#25991;&#30001;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#33719;&#24471;&#30340;&#36716;&#24405;&#32452;&#25104;&#65292;&#30001;&#20110;&#36825;&#20123;&#25968;&#25454;&#30340;&#22266;&#26377;&#22024;&#26434;&#24615;&#21644;&#21475;&#35821;&#29305;&#24615;&#65292;&#36825;&#20026;LLMs&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#65292;&#21517;&#20026;ELITR-Bench&#65292;&#36890;&#36807;271&#20010;&#25163;&#24037;&#21046;&#20316;&#30340;&#38382;&#39064;&#21450;&#20854;&#30495;&#23454;&#31572;&#26696;&#26469;&#22686;&#24378;&#29616;&#26377;&#30340;ELITR&#35821;&#26009;&#24211;&#30340;&#36716;&#24405;&#12290;&#25105;&#20204;&#22312;ELITR-Bench&#19978;&#23545;&#26368;&#26032;&#30340;&#38271;&#19978;&#19979;&#25991;LLMs&#36827;&#34892;&#30340;&#23454;&#39564;&#20984;&#26174;&#20102;&#24320;&#28304;&#27169;&#22411;&#21644;&#19987;&#26377;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20262v1 Announce Type: cross  Abstract: Research on Large Language Models (LLMs) has recently witnessed an increasing interest in extending models' context size to better capture dependencies within long documents. While benchmarks have been proposed to assess long-range abilities, existing efforts primarily considered generic tasks that are not necessarily aligned with real-world applications. In contrast, our work proposes a new benchmark for long-context LLMs focused on a practical meeting assistant scenario. In this scenario, the long contexts consist of transcripts obtained by automatic speech recognition, presenting unique challenges for LLMs due to the inherent noisiness and oral nature of such data. Our benchmark, named ELITR-Bench, augments the existing ELITR corpus' transcripts with 271 manually crafted questions and their ground-truth answers. Our experiments with recent long-context LLMs on ELITR-Bench highlight a gap between open-source and proprietary models, e
&lt;/p&gt;</description></item><item><title>FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.20261</link><description>&lt;p&gt;
FABind+: &#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#22686;&#24378;&#20998;&#23376;&#23545;&#25509;
&lt;/p&gt;
&lt;p&gt;
FABind+: Enhancing Molecular Docking through Improved Pocket Prediction and Pose Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20261
&lt;/p&gt;
&lt;p&gt;
FABind+&#36890;&#36807;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#21644;&#23039;&#24577;&#29983;&#25104;&#65292;&#25552;&#21319;&#20998;&#23376;&#23545;&#25509;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#23545;&#25509;&#26159;&#33647;&#29289;&#21457;&#29616;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#36807;&#31243;&#12290;&#20256;&#32479;&#25216;&#26415;&#20381;&#36182;&#20110;&#21463;&#29289;&#29702;&#21407;&#29702;&#25903;&#37197;&#30340;&#24191;&#27867;&#37319;&#26679;&#21644;&#27169;&#25311;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#36895;&#24230;&#24930;&#19988;&#26114;&#36149;&#12290;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#30340;&#20986;&#29616;&#26174;&#31034;&#20986;&#26174;&#33879;&#30340;&#21069;&#26223;&#65292;&#25552;&#20379;&#20102;&#31934;&#30830;&#24615;&#21644;&#25928;&#29575;&#30340;&#22686;&#38271;&#12290;&#24314;&#31435;&#22312;FABind&#30340;&#22522;&#30784;&#24037;&#20316;&#20043;&#19978;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#36895;&#24230;&#21644;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FABind+&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22823;&#25552;&#21319;&#20854;&#21069;&#36523;&#24615;&#33021;&#30340;&#22686;&#24378;&#29256;&#12290;&#25105;&#20204;&#30830;&#23450;&#21475;&#34955;&#39044;&#27979;&#26159;&#20998;&#23376;&#23545;&#25509;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26174;&#33879;&#25913;&#36827;&#21475;&#34955;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#20174;&#32780;&#31616;&#21270;&#20102;&#23545;&#25509;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#23545;&#25509;&#27169;&#22359;&#36827;&#34892;&#20102;&#20462;&#25913;&#65292;&#20197;&#22686;&#24378;&#20854;&#23039;&#24577;&#29983;&#25104;&#33021;&#21147;&#12290;&#20026;&#20102;&#32553;&#23567;&#19982;&#20256;&#32479;&#37319;&#26679;/&#29983;&#25104;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#25105;&#20204;&#32467;&#21512;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;s
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20261v1 Announce Type: cross  Abstract: Molecular docking is a pivotal process in drug discovery. While traditional techniques rely on extensive sampling and simulation governed by physical principles, these methods are often slow and costly. The advent of deep learning-based approaches has shown significant promise, offering increases in both accuracy and efficiency. Building upon the foundational work of FABind, a model designed with a focus on speed and accuracy, we present FABind+, an enhanced iteration that largely boosts the performance of its predecessor. We identify pocket prediction as a critical bottleneck in molecular docking and propose a novel methodology that significantly refines pocket prediction, thereby streamlining the docking process. Furthermore, we introduce modifications to the docking module to enhance its pose generation capabilities. In an effort to bridge the gap with conventional sampling/generative methods, we incorporate a simple yet effective s
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;LLMs&#24314;&#27169;&#30446;&#26631;&#20154;&#32676;&#30340;&#20449;&#24565;&#21644;&#20559;&#22909;&#65292;&#26088;&#22312;&#23454;&#29616;&#21508;&#31181;&#24212;&#29992;&#65292;&#35780;&#20272;&#19981;&#21516;&#24494;&#35843;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#26816;&#39564;&#20854;&#22312;&#21305;&#37197;&#30495;&#23454;&#20154;&#31867;&#21463;&#35775;&#32773;&#20559;&#22909;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.20252</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#24314;&#27169;&#30446;&#26631;&#20154;&#32676;&#30340;&#20449;&#24565;&#21644;&#20559;&#22909;
&lt;/p&gt;
&lt;p&gt;
Using LLMs to Model the Beliefs and Preferences of Targeted Populations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20252
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;LLMs&#24314;&#27169;&#30446;&#26631;&#20154;&#32676;&#30340;&#20449;&#24565;&#21644;&#20559;&#22909;&#65292;&#26088;&#22312;&#23454;&#29616;&#21508;&#31181;&#24212;&#29992;&#65292;&#35780;&#20272;&#19981;&#21516;&#24494;&#35843;&#26041;&#27861;&#30340;&#25928;&#26524;&#65292;&#24182;&#26816;&#39564;&#20854;&#22312;&#21305;&#37197;&#30495;&#23454;&#20154;&#31867;&#21463;&#35775;&#32773;&#20559;&#22909;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#19982;&#20154;&#32676;&#30340;&#20559;&#22909;&#36827;&#34892;&#23545;&#40784;&#30340;&#38382;&#39064;&#12290;&#24314;&#27169;&#29305;&#23450;&#20154;&#32676;&#30340;&#20449;&#24565;&#12289;&#20559;&#22909;&#21644;&#34892;&#20026;&#23545;&#20110;&#21508;&#31181;&#19981;&#21516;&#24212;&#29992;&#21487;&#33021;&#24456;&#26377;&#29992;&#65292;&#27604;&#22914;&#20026;&#26032;&#20135;&#21697;&#24320;&#23637;&#27169;&#25311;&#28966;&#28857;&#23567;&#32452;&#12289;&#36827;&#34892;&#34394;&#25311;&#35843;&#26597;&#20197;&#21450;&#27979;&#35797;&#34892;&#20026;&#24178;&#39044;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26114;&#36149;&#12289;&#19981;&#20999;&#23454;&#38469;&#25110;&#19981;&#36947;&#24503;&#30340;&#24178;&#39044;&#12290;&#29616;&#26377;&#30740;&#31350;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#20351;&#29992;LLMs&#20934;&#30830;&#24314;&#27169;&#20154;&#31867;&#34892;&#20026;&#26041;&#38754;&#21462;&#24471;&#20102;&#19981;&#21516;&#31243;&#24230;&#30340;&#25104;&#21151;&#12290;&#25105;&#20204;&#23545;&#20004;&#31181;&#20247;&#25152;&#21608;&#30693;&#30340;&#24494;&#35843;&#26041;&#27861;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#21644;&#35780;&#20272;&#65292;&#35780;&#20272;&#24471;&#21040;&#30340;&#20154;&#32676;&#22312;&#21305;&#37197;&#23545;&#30005;&#27744;&#30005;&#21160;&#27773;&#36710;(BEVs)&#20559;&#22909;&#35843;&#26597;&#20013;&#30495;&#23454;&#20154;&#31867;&#21463;&#35775;&#32773;&#20559;&#22909;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#21542;&#33021;&#19982;&#25972;&#20307;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#20197;&#21450;&#20010;&#20307;&#22238;&#24212;&#30456;&#21305;&#37197;&#65292;&#24182;&#30740;&#31350;LLMs&#22312;&#24314;&#27169;&#20154;&#32676;&#20449;&#24565;&#21644;&#20559;&#22909;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20252v1 Announce Type: cross  Abstract: We consider the problem of aligning a large language model (LLM) to model the preferences of a human population. Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical. Existing work has had mixed success using LLMs to accurately model human behavior in different contexts. We benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs). We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22810;&#21160;&#20316;&#22330;&#26223;&#20013;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#20272;&#35745;&#12289;&#39118;&#38505;&#20559;&#22909;&#21644;&#28508;&#22312;&#25925;&#38556;&#19977;&#20010;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2403.20250</link><description>&lt;p&gt;
&#22810;&#21160;&#20316;&#22330;&#26223;&#20013;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#23398;&#20064;&#65306;&#20272;&#35745;&#12289;&#39118;&#38505;&#20559;&#22909;&#21644;&#28508;&#22312;&#25925;&#38556;
&lt;/p&gt;
&lt;p&gt;
Optimal Policy Learning with Observational Data in Multi-Action Scenarios: Estimation, Risk Preference, and Potential Failures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22810;&#21160;&#20316;&#22330;&#26223;&#20013;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#30528;&#37325;&#25506;&#35752;&#20102;&#20272;&#35745;&#12289;&#39118;&#38505;&#20559;&#22909;&#21644;&#28508;&#22312;&#25925;&#38556;&#19977;&#20010;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#21033;&#29992;&#35266;&#27979;&#25968;&#25454;&#36827;&#34892;&#26368;&#20248;&#31574;&#30053;&#23398;&#20064;&#65288;OPL&#65289;&#65292;&#21363;&#25968;&#25454;&#39537;&#21160;&#30340;&#26368;&#20248;&#20915;&#31574;&#65292;&#22312;&#22810;&#21160;&#20316;&#65288;&#25110;&#22810;&#33218;&#65289;&#35774;&#32622;&#20013;&#65292;&#26377;&#38480;&#30340;&#20915;&#31574;&#36873;&#39033;&#21487;&#20379;&#36873;&#25321;&#12290;&#25991;&#31456;&#20998;&#20026;&#19977;&#20010;&#37096;&#20998;&#65292;&#20998;&#21035;&#35752;&#35770;&#65306;&#20272;&#35745;&#12289;&#39118;&#38505;&#20559;&#22909;&#21644;&#28508;&#22312;&#25925;&#38556;&#12290;&#31532;&#19968;&#37096;&#20998;&#31616;&#35201;&#22238;&#39038;&#20102;&#22312;&#36825;&#31181;&#20998;&#26512;&#32972;&#26223;&#19979;&#20272;&#35745;&#22870;&#21169;&#65288;&#25110;&#20540;&#65289;&#20989;&#25968;&#21644;&#26368;&#20248;&#31574;&#30053;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#31532;&#20108;&#37096;&#20998;&#28145;&#20837;&#20998;&#26512;&#20102;&#20915;&#31574;&#39118;&#38505;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#20915;&#31574;&#32773;&#23545;&#39118;&#38505;&#30340;&#24577;&#24230;&#21487;&#20197;&#24433;&#21709;&#26368;&#20248;&#36873;&#25321;&#65292;&#20855;&#20307;&#20307;&#29616;&#22312;&#22870;&#21169;&#26465;&#20214;&#22343;&#20540;&#19982;&#26465;&#20214;&#26041;&#24046;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#22312;&#36825;&#37324;&#65292;&#20316;&#32773;&#23558;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#24212;&#29992;&#20110;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20250v1 Announce Type: cross  Abstract: This paper deals with optimal policy learning (OPL) with observational data, i.e. data-driven optimal decision-making, in multi-action (or multi-arm) settings, where a finite set of decision options is available. It is organized in three parts, where I discuss respectively: estimation, risk preference, and potential failures. The first part provides a brief review of the key approaches to estimating the reward (or value) function and optimal policy within this context of analysis. Here, I delineate the identification assumptions and statistical properties related to offline optimal policy learning estimators. In the second part, I delve into the analysis of decision risk. This analysis reveals that the optimal choice can be influenced by the decision maker's attitude towards risks, specifically in terms of the trade-off between reward conditional mean and conditional variance. Here, I present an application of the proposed model to rea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#26102;&#20998;&#31867;ENG&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;ANNs&#22312;&#19981;&#21516;&#22823;&#23567;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26469;&#20998;&#26512;&#20854;&#22312;&#22788;&#29702;&#36816;&#21160;/&#24863;&#35273;&#21050;&#28608;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.20234</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26893;&#20837;&#31070;&#32463;&#25509;&#21475;&#23454;&#26102;&#20998;&#31867;ENG&#20449;&#21495;
&lt;/p&gt;
&lt;p&gt;
Artificial Neural Networks-based Real-time Classification of ENG Signals for Implanted Nerve Interfaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22522;&#20110;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#26102;&#20998;&#31867;ENG&#20449;&#21495;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;ANNs&#22312;&#19981;&#21516;&#22823;&#23567;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#26469;&#20998;&#26512;&#20854;&#22312;&#22788;&#29702;&#36816;&#21160;/&#24863;&#35273;&#21050;&#28608;&#20998;&#31867;&#20219;&#21153;&#20013;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#30149;&#22312;&#20020;&#24202;&#29615;&#22659;&#20013;&#21464;&#24471;&#26356;&#21152;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#21487;&#33021;&#27704;&#20037;&#21361;&#21450;&#19968;&#20010;&#20154;&#30340;&#29983;&#21629;&#12290;&#20026;&#20102;&#25903;&#25345;&#24739;&#32773;&#30340;&#24247;&#22797;&#65292;&#20351;&#29992;&#23436;&#20840;&#26893;&#20837;&#24335;&#35774;&#22791;&#27491;&#25104;&#20026;&#26368;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#25104;&#20026;&#23436;&#20840;&#22797;&#26434;&#31070;&#32463;&#32435;&#31859;&#32593;&#32476;&#31995;&#32479;&#30340;&#19968;&#20010;&#32452;&#25104;&#37096;&#20998;&#65292;&#36825;&#20123;&#35774;&#22791;&#20173;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20854;&#20013;&#20043;&#19968;&#65292;&#21363;&#36816;&#21160;/&#24863;&#35273;&#21050;&#28608;&#30340;&#20998;&#31867;&#38382;&#39064;&#12290;&#36890;&#36807;&#25506;&#32034;&#22235;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANNs&#65289;&#65292;&#20174;&#22823;&#40736;&#22352;&#39592;&#31070;&#32463;&#27979;&#37327;&#30340;&#30005;&#31070;&#32463;&#22270;&#65288;ENG&#65289;&#20449;&#21495;&#20013;&#25552;&#21462;&#21508;&#31181;&#24863;&#35273;&#21050;&#28608;&#26469;&#25191;&#34892;&#35813;&#20219;&#21153;&#12290;&#32771;&#34385;&#19981;&#21516;&#22823;&#23567;&#30340;&#25968;&#25454;&#38598;&#20197;&#20998;&#26512;&#34987;&#35843;&#26597;&#30340;ANNs&#22312;&#23454;&#26102;&#20998;&#31867;&#20013;&#30340;&#21487;&#34892;&#24615;&#65292;&#36890;&#36807;&#27604;&#36739;&#23427;&#20204;&#22312;&#20934;&#30830;&#24615;&#12289;F1&#20998;&#25968;&#21644;&#39044;&#27979;&#26102;&#38388;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20234v1 Announce Type: new  Abstract: Neuropathies are gaining higher relevance in clinical settings, as they risk permanently jeopardizing a person's life. To support the recovery of patients, the use of fully implanted devices is emerging as one of the most promising solutions. However, these devices, even if becoming an integral part of a fully complex neural nanonetwork system, pose numerous challenges. In this article, we address one of them, which consists of the classification of motor/sensory stimuli. The task is performed by exploring four different types of artificial neural networks (ANNs) to extract various sensory stimuli from the electroneurographic (ENG) signal measured in the sciatic nerve of rats. Different sizes of the data sets are considered to analyze the feasibility of the investigated ANNs for real-time classification through a comparison of their performance in terms of accuracy, F1-score, and prediction time. The design of the ANNs takes advantage of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;GRADE&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#32858;&#21512;-&#25193;&#25955;&#26041;&#31243;&#23454;&#29616;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#20122;&#31283;&#23450;&#24615;&#65292;&#33021;&#22815;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.20221</link><description>&lt;p&gt;
&#24102;&#38544;&#34109;&#29366;&#24577;&#30340;&#22270;&#31070;&#32463;&#32858;&#21512;-&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Aggregation-diffusion with Metastability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20221
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;GRADE&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#32858;&#21512;-&#25193;&#25955;&#26041;&#31243;&#23454;&#29616;&#20102;&#33410;&#28857;&#34920;&#31034;&#30340;&#20122;&#31283;&#23450;&#24615;&#65292;&#33021;&#22815;&#32531;&#35299;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#36807;&#24230;&#24179;&#28369;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#24494;&#20998;&#26041;&#31243;&#30340;&#36830;&#32493;&#22270;&#31070;&#32463;&#27169;&#22411;&#25193;&#23637;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#26550;&#26500;&#12290;&#30001;&#20110;&#22270;&#25193;&#25955;&#19982;&#20449;&#24687;&#20256;&#36882;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#33258;&#28982;&#22320;&#23558;&#31995;&#32479;&#25512;&#21521;&#24179;&#34913;&#29366;&#24577;&#65292;&#23548;&#33268;&#38382;&#39064;&#65292;&#22914;&#36807;&#24230;&#24179;&#28369;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21463;&#22270;&#32858;&#21512;-&#25193;&#25955;&#26041;&#31243;&#21551;&#21457;&#30340;GRADE&#65292;&#20854;&#20013;&#21253;&#25324;&#38750;&#32447;&#24615;&#25193;&#25955;&#21644;&#30456;&#20114;&#20316;&#29992;&#21183;&#24341;&#36215;&#30340;&#32858;&#21512;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;&#36890;&#36807;&#32858;&#21512;-&#25193;&#25955;&#26041;&#31243;&#33719;&#24471;&#30340;&#33410;&#28857;&#34920;&#31034;&#34920;&#29616;&#20986;&#20122;&#31283;&#24577;&#65292;&#34920;&#26126;&#29305;&#24449;&#21487;&#20197;&#32858;&#21512;&#25104;&#22810;&#20010;&#31751;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#31751;&#20869;&#30340;&#21160;&#24577;&#21487;&#20197;&#25345;&#32493;&#24456;&#38271;&#26102;&#38388;&#65292;&#26377;&#26395;&#32531;&#35299;&#36807;&#24230;&#24179;&#28369;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#27169;&#22411;&#20013;&#30340;&#38750;&#32447;&#24615;&#25193;&#25955;&#25512;&#24191;&#20102;&#29616;&#26377;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#24182;&#30830;&#31435;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20221v1 Announce Type: cross  Abstract: Continuous graph neural models based on differential equations have expanded the architecture of graph neural networks (GNNs). Due to the connection between graph diffusion and message passing, diffusion-based models have been widely studied. However, diffusion naturally drives the system towards an equilibrium state, leading to issues like over-smoothing. To this end, we propose GRADE inspired by graph aggregation-diffusion equations, which includes the delicate balance between nonlinear diffusion and aggregation induced by interaction potentials. The node representations obtained through aggregation-diffusion equations exhibit metastability, indicating that features can aggregate into multiple clusters. In addition, the dynamics within these clusters can persist for long time periods, offering the potential to alleviate over-smoothing effects. This nonlinear diffusion in our model generalizes existing diffusion-based models and estab
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;&#26426;&#26500;&#65292;&#24182;&#21487;&#33021;&#20351;&#27785;&#28024;&#24335;&#25216;&#26415;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20216</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#30340;&#20998;&#24067;&#24335;&#26426;&#26500;
&lt;/p&gt;
&lt;p&gt;
Distributed agency in second language learning and teaching through generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20216
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#31532;&#20108;&#35821;&#35328;&#23398;&#20064;&#21644;&#25945;&#23398;&#20013;&#25552;&#20379;&#20102;&#20998;&#24067;&#24335;&#26426;&#26500;&#65292;&#24182;&#21487;&#33021;&#20351;&#27785;&#28024;&#24335;&#25216;&#26415;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20216v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#20026;&#35821;&#35328;&#23398;&#20064;&#25552;&#20379;&#20102;&#37325;&#22823;&#26426;&#20250;&#12290;&#20687;ChatGPT&#36825;&#26679;&#30340;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#20070;&#38754;&#25110;&#21475;&#22836;&#24418;&#24335;&#30340;&#23545;&#35805;&#20026;&#31532;&#20108;&#35821;&#35328;&#25552;&#20379;&#38750;&#27491;&#24335;&#32451;&#20064;&#65292;&#23398;&#20064;&#32773;&#36890;&#36807;&#25552;&#31034;&#25351;&#23450;&#23545;&#35805;&#21442;&#25968;&#65292;&#22914;&#29087;&#32451;&#31243;&#24230;&#12289;&#35821;&#35328;&#39118;&#26684;&#21644;&#35752;&#35770;&#20027;&#39064;&#12290;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#34987;&#25351;&#23548;&#32473;&#20104;&#32416;&#27491;&#24615;&#21453;&#39304;&#65292;&#21019;&#24314;&#32451;&#20064;&#39064;&#65292;&#25110;&#21046;&#23450;&#25193;&#23637;&#23398;&#20064;&#35745;&#21010;&#12290;&#25945;&#24072;&#21487;&#20197;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#26500;&#24314;&#21508;&#31181;&#23186;&#20307;&#30340;&#23398;&#20064;&#21644;&#35780;&#20272;&#26448;&#26009;&#12290;&#20154;&#24037;&#26234;&#33021;&#21487;&#33021;&#20250;&#20351;&#27785;&#28024;&#24335;&#25216;&#26415;&#26356;&#24378;&#22823;&#21644;&#22810;&#21151;&#33021;&#65292;&#25670;&#33073;&#33050;&#26412;&#21270;&#30340;&#20114;&#21160;&#12290;&#23545;&#20110;&#23398;&#20064;&#32773;&#21644;&#25945;&#24072;&#32780;&#35328;&#65292;&#37325;&#35201;&#30340;&#26159;&#35201;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#23616;&#38480;&#24615;&#26469;&#33258;&#20110;&#23427;&#20204;&#23545;&#20154;&#31867;&#35821;&#35328;&#30340;&#32431;&#32479;&#35745;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22788;&#29702;&#35821;&#35328;&#20351;&#29992;&#20013;&#24494;&#22937;&#31038;&#20250;&#21644;&#25991;&#21270;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#21019;&#36896;&#26041;&#24335;&#23384;&#22312;&#20262;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20216v1 Announce Type: cross  Abstract: Generative AI offers significant opportunities for language learning. Tools like ChatGPT can provide informal second language practice through chats in written or voice forms, with the learner specifying through prompts conversational parameters such as proficiency level, language register, and discussion topics. AI can be instructed to give corrective feedback, create practice exercises, or develop an extended study plan. Instructors can use AI to build learning and assessment materials in a variety of media. AI is likely to make immersive technologies more powerful and versatile, moving away from scripted interactions. For both learners and teachers, it is important to understand the limitations of AI systems that arise from their purely statistical model of human language, which limits their ability to deal with nuanced social and cultural aspects of language use. Additionally, there are ethical concerns over how AI systems are crea
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26356;&#22823;&#30340;&#23454;&#20363;&#22823;&#23567;&#36827;&#34892;&#35757;&#32451;&#24182;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#26500;&#24314;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#65292;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;TSP&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.20212</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#26053;&#34892;&#21830;&#38382;&#39064;&#20013;&#23610;&#23544;&#21644;&#38590;&#24230;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
On Size and Hardness Generalization in Unsupervised Learning for the Travelling Salesman Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20212
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26356;&#22823;&#30340;&#23454;&#20363;&#22823;&#23567;&#36827;&#34892;&#35757;&#32451;&#24182;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#26500;&#24314;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#65292;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;TSP&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#26080;&#30417;&#30563;&#23398;&#20064;&#22312;&#35299;&#20915;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#20013;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#29992;&#26367;&#20195;&#25439;&#22833;&#20989;&#25968;&#35757;&#32451;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26469;&#20026;&#27599;&#20010;&#33410;&#28857;&#29983;&#25104;&#23884;&#20837;&#12290;&#25105;&#20204;&#20351;&#29992;&#36825;&#20123;&#23884;&#20837;&#26469;&#26500;&#24314;&#19968;&#20010;&#28909;&#22270;&#65292;&#25351;&#31034;&#27599;&#26465;&#36793;&#25104;&#20026;&#26368;&#20339;&#36335;&#24452;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#24212;&#29992;&#23616;&#37096;&#25628;&#32034;&#29983;&#25104;&#26368;&#32456;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#19981;&#21516;&#35757;&#32451;&#23454;&#20363;&#22823;&#23567;&#12289;&#23884;&#20837;&#32500;&#25968;&#21644;&#20998;&#24067;&#22914;&#20309;&#24433;&#21709;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#26356;&#22823;&#30340;&#23454;&#20363;&#22823;&#23567;&#36827;&#34892;&#35757;&#32451;&#24182;&#22686;&#21152;&#23884;&#20837;&#32500;&#24230;&#21487;&#20197;&#26500;&#24314;&#26356;&#26377;&#25928;&#30340;&#34920;&#31034;&#65292;&#22686;&#24378;&#27169;&#22411;&#35299;&#20915;TSP&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22312;&#35780;&#20272;&#19981;&#21516;&#20998;&#24067;&#19979;&#30340;&#27867;&#21270;&#33021;&#21147;&#26102;&#65292;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#21508;&#31181;&#20998;&#24067;&#30340;&#38590;&#24230;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#38590;&#24230;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20212v1 Announce Type: new  Abstract: We study the generalization capability of Unsupervised Learning in solving the Travelling Salesman Problem (TSP). We use a Graph Neural Network (GNN) trained with a surrogate loss function to generate an embedding for each node. We use these embeddings to construct a heat map that indicates the likelihood of each edge being part of the optimal route. We then apply local search to generate our final predictions. Our investigation explores how different training instance sizes, embedding dimensions, and distributions influence the outcomes of Unsupervised Learning methods. Our results show that training with larger instance sizes and increasing embedding dimensions can build a more effective representation, enhancing the model's ability to solve TSP. Furthermore, in evaluating generalization across different distributions, we first determine the hardness of various distributions and explore how different hardnesses affect the final results
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.20208</link><description>&lt;p&gt;
&#21457;&#25381;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25968;&#25454;&#31185;&#23398;&#20013;&#39044;&#27979;&#34920;&#26684;&#20219;&#21153;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unleashing the Potential of Large Language Models for Predictive Tabular Tasks in Data Science
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#31185;&#23398;&#20013;&#34920;&#26684;&#25968;&#25454;&#39044;&#27979;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Llama-2&#27169;&#22411;&#24182;&#36827;&#34892;&#23454;&#38469;&#24212;&#29992;&#65292;&#21462;&#24471;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#65292;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#32570;&#22833;&#20540;&#22635;&#20805;&#31561;&#39044;&#27979;&#20219;&#21153;&#26159;&#19982;&#34920;&#26684;&#25968;&#25454;&#30456;&#20851;&#30340;&#24120;&#35265;&#25361;&#25112;&#12290;&#36825;&#39033;&#30740;&#31350;&#26088;&#22312;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#35299;&#20915;&#36825;&#20123;&#39044;&#27979;&#20219;&#21153;&#12290;&#23613;&#31649;LLMs&#25797;&#38271;&#29702;&#35299;&#33258;&#28982;&#35821;&#35328;&#65292;&#20294;&#22312;&#22788;&#29702;&#32467;&#26500;&#21270;&#34920;&#26684;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#25910;&#38598;&#24102;&#26377;&#25351;&#20196;&#27880;&#37322;&#30340;&#34920;&#26684;&#35821;&#26009;&#24211;&#65292;&#24182;&#22312;&#36825;&#19968;&#20016;&#23500;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;Llama-2&#36827;&#34892;&#22823;&#35268;&#27169;&#35757;&#32451;&#65292;&#20197;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23558;&#35757;&#32451;&#27169;&#22411;&#24212;&#29992;&#20110;&#38646;-shot&#39044;&#27979;&#12289;&#23569;-shot&#39044;&#27979;&#21644;&#19978;&#19979;&#25991;&#23398;&#20064;&#22330;&#26223;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35770;&#26174;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20208v1 Announce Type: new  Abstract: In the domain of data science, the predictive tasks of classification, regression, and imputation of missing values are commonly encountered challenges associated with tabular data. This research endeavors to apply Large Language Models (LLMs) towards addressing these predictive tasks. Despite their proficiency in comprehending natural language, LLMs fall short in dealing with structured tabular data. This limitation stems from their lacking exposure to the intricacies of tabular data during their foundational training. Our research aims to mitigate this gap by compiling a comprehensive corpus of tables annotated with instructions and executing large-scale training of Llama-2 on this enriched dataset. Furthermore, we investigate the practical application of applying the trained model to zero-shot prediction, few-shot prediction, and in-context learning scenarios. Through extensive experiments, our methodology has shown significant improv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32508;&#21512;&#30340;&#36767;&#35875;&#27969;&#31243;&#65292;&#19981;&#20165;&#33021;&#26816;&#27979;&#35875;&#35328;&#65292;&#36824;&#33021;&#25552;&#20379;&#35299;&#37322;&#24615;&#29983;&#25104;&#20869;&#23481;&#26469;&#39539;&#26021;&#20449;&#24687;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#19988;&#21033;&#29992;&#19987;&#23478;-&#20844;&#27665;&#38598;&#20307;&#26234;&#24935;&#27169;&#22359;&#30830;&#20445;&#20449;&#24687;&#21487;&#20449;&#24230;&#39640;&#31934;&#24230;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.20204</link><description>&lt;p&gt;
&#23545;&#25239;&#35875;&#35328;&#30340;&#26410;&#26469;&#65311;&#26816;&#32034;&#12289;&#36776;&#21035;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
The Future of Combating Rumors? Retrieval, Discrimination, and Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20204
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32508;&#21512;&#30340;&#36767;&#35875;&#27969;&#31243;&#65292;&#19981;&#20165;&#33021;&#26816;&#27979;&#35875;&#35328;&#65292;&#36824;&#33021;&#25552;&#20379;&#35299;&#37322;&#24615;&#29983;&#25104;&#20869;&#23481;&#26469;&#39539;&#26021;&#20449;&#24687;&#30340;&#30495;&#23454;&#24615;&#65292;&#24182;&#19988;&#21033;&#29992;&#19987;&#23478;-&#20844;&#27665;&#38598;&#20307;&#26234;&#24935;&#27169;&#22359;&#30830;&#20445;&#20449;&#24687;&#21487;&#20449;&#24230;&#39640;&#31934;&#24230;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20204v1 &#36890;&#21578;&#31867;&#22411;&#65306;&#26032;&#30340; &#25688;&#35201;&#65306;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#20419;&#20351;&#20102;&#24102;&#26377;&#38169;&#35823;&#20449;&#24687;&#30340;&#35875;&#35328;&#30340;&#20135;&#29983;&#65292;&#24433;&#21709;&#20102;&#31038;&#20250;&#12289;&#32463;&#27982;&#21644;&#25919;&#27835;&#29983;&#24577;&#31995;&#32479;&#65292;&#25361;&#25112;&#27665;&#20027;&#12290;&#24403;&#21069;&#30340;&#35875;&#35328;&#26816;&#27979;&#24037;&#20316;&#20165;&#20165;&#36890;&#36807;&#26631;&#35760;&#28508;&#22312;&#30340;&#38169;&#35823;&#20449;&#24687;&#65288;&#20998;&#31867;&#20219;&#21153;&#65289;&#26469;&#23454;&#29616;&#65292;&#26410;&#33021;&#20805;&#20998;&#35299;&#20915;&#38382;&#39064;&#65292;&#35753;&#26435;&#23041;&#26426;&#26500;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36767;&#35875;&#27599;&#19968;&#26465;&#20449;&#24687;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20840;&#38754;&#36767;&#35875;&#36807;&#31243;&#19981;&#20165;&#33021;&#22815;&#26816;&#27979;&#35875;&#35328;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#35299;&#37322;&#24615;&#29983;&#25104;&#20869;&#23481;&#26469;&#39539;&#26021;&#20449;&#24687;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#19987;&#23478;-&#20844;&#27665;&#38598;&#20307;&#26234;&#24935;&#65288;ECCW&#65289;&#27169;&#22359;&#30830;&#20445;&#23545;&#20449;&#24687;&#21487;&#20449;&#24230;&#36827;&#34892;&#39640;&#31934;&#24230;&#35780;&#20272;&#65292;&#26816;&#32034;&#27169;&#22359;&#36127;&#36131;&#22522;&#20110;&#20449;&#24687;&#20851;&#38190;&#35789;&#20174;&#23454;&#26102;&#26356;&#26032;&#30340;&#36767;&#35875;&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#30456;&#20851;&#30693;&#35782;&#12290;&#36890;&#36807;&#20351;&#29992;&#21363;&#26102;&#24037;&#31243;&#25216;&#26415;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20204v1 Announce Type: new  Abstract: Artificial Intelligence Generated Content (AIGC) technology development has facilitated the creation of rumors with misinformation, impacting societal, economic, and political ecosystems, challenging democracy. Current rumor detection efforts fall short by merely labeling potentially misinformation (classification task), inadequately addressing the issue, and it is unrealistic to have authoritative institutions debunk every piece of information on social media. Our proposed comprehensive debunking process not only detects rumors but also provides explanatory generated content to refute the authenticity of the information. The Expert-Citizen Collective Wisdom (ECCW) module we designed aensures high-precision assessment of the credibility of information and the retrieval module is responsible for retrieving relevant knowledge from a Real-time updated debunking database based on information keywords. By using prompt engineering techniques, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;NeuraLunaDTNet&#21327;&#35758;&#65292;&#21033;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#20102;PRoPHET&#36335;&#30001;&#21327;&#35758;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#21464;&#21270;&#30340;&#26102;&#31354;&#22270;&#20013;&#30340;&#32852;&#31995;&#35745;&#21010;&#26469;&#20248;&#21270;&#26376;&#29699;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.20199</link><description>&lt;p&gt;
NeuraLunaDTNet&#65306;&#22522;&#20110;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#24310;&#36831;&#23481;&#24525;&#26376;&#29699;&#36890;&#20449;&#32593;&#32476;&#36335;&#30001;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
NeuraLunaDTNet: Feedforward Neural Network-Based Routing Protocol for Delay-Tolerant Lunar Communication Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20199
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;NeuraLunaDTNet&#21327;&#35758;&#65292;&#21033;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#22686;&#24378;&#20102;PRoPHET&#36335;&#30001;&#21327;&#35758;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#21464;&#21270;&#30340;&#26102;&#31354;&#22270;&#20013;&#30340;&#32852;&#31995;&#35745;&#21010;&#26469;&#20248;&#21270;&#26376;&#29699;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31354;&#38388;&#36890;&#20449;&#38754;&#20020;&#20005;&#37325;&#24310;&#36831;&#12289;&#38590;&#20197;&#39044;&#27979;&#30340;&#36335;&#24452;&#21644;&#36890;&#20449;&#20013;&#26029;&#31561;&#25361;&#25112;&#12290;&#24310;&#36831;&#23481;&#24525;&#32593;&#32476;&#26550;&#26500;&#26159;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#24773;&#20917;&#32780;&#19987;&#38376;&#35774;&#35745;&#30340;&#65292;&#36866;&#29992;&#20110;&#24212;&#23545;&#19968;&#20123;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;DTN&#36335;&#30001;&#21327;&#35758;&#22312;&#20248;&#21270;&#24615;&#33021;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65292;&#30001;&#20110;&#31354;&#38388;&#36890;&#20449;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#12290;&#30740;&#31350;&#20154;&#21592;&#33268;&#21147;&#20110;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#26469;&#32531;&#35299;&#19968;&#20123;&#36335;&#30001;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#24320;&#21457;&#19968;&#31181;&#26032;&#39062;&#30340;&#21327;&#35758;NeuraLunaDTNet&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#24577;&#21464;&#21270;&#30340;&#26102;&#31354;&#22270;&#20013;&#30340;&#32852;&#31995;&#35745;&#21010;&#26469;&#25552;&#39640;PRoPHET&#36335;&#30001;&#21327;&#35758;&#22312;&#26376;&#29699;&#36890;&#20449;&#20013;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20199v1 Announce Type: cross  Abstract: Space Communication poses challenges such as severe delays, hard-to-predict routes and communication disruptions. The Delay Tolerant Network architecture, having been specifically designed keeping such scenarios in mind, is suitable to address some challenges. The traditional DTN routing protocols fall short of delivering optimal performance, due to the inherent complexities of space communication. Researchers have aimed at using recent advancements in AI to mitigate some routing challenges [9]. We propose utilising a feedforward neural network to develop a novel protocol NeuraLunaDTNet, which enhances the efficiency of the PRoPHET routing protocol for lunar communication, by learning contact plans in dynamically changing spatio-temporal graph.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#24067;&#24335;&#32676;&#26234;&#33021;&#23398;&#20064;&#65288;DSL&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#21644;&#29983;&#29289;&#32676;&#26234;&#33021;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20026;&#22823;&#35268;&#27169;IoT&#22312;&#26080;&#32447;&#32593;&#32476;&#36793;&#32536;&#25552;&#20379;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#31283;&#20581;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.20188</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;&#32676;&#26234;&#33021;&#23398;&#20064;&#29992;&#20110;&#36793;&#32536;&#29289;&#32852;&#32593;
&lt;/p&gt;
&lt;p&gt;
Distributed Swarm Learning for Edge Internet of Things
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#21517;&#20026;&#20998;&#24067;&#24335;&#32676;&#26234;&#33021;&#23398;&#20064;&#65288;DSL&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#21644;&#29983;&#29289;&#32676;&#26234;&#33021;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20026;&#22823;&#35268;&#27169;IoT&#22312;&#26080;&#32447;&#32593;&#32476;&#36793;&#32536;&#25552;&#20379;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#31283;&#20581;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20188v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#29289;&#32852;&#32593;&#65288;IoT&#65289;&#30340;&#24555;&#36895;&#22686;&#38271;&#23548;&#33268;&#26234;&#33021;IoT&#35774;&#22791;&#22312;&#26080;&#32447;&#36793;&#32536;&#36827;&#34892;&#21327;&#20316;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#24320;&#21551;&#20102;&#36793;&#32536;&#23398;&#20064;&#30340;&#26032;&#26102;&#20195;&#12290;&#38754;&#23545;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#26080;&#32447;&#32593;&#32476;&#20013;&#36816;&#34892;&#30340;&#22823;&#37327;&#30828;&#20214;&#21463;&#38480;&#21046;&#30340;IoT&#35774;&#22791;&#65292;&#36793;&#32536;&#23398;&#20064;&#38754;&#20020;&#30528;&#36890;&#20449;&#21644;&#35745;&#31639;&#29942;&#39048;&#12289;&#35774;&#22791;&#21644;&#25968;&#25454;&#24322;&#26500;&#24615;&#12289;&#23433;&#20840;&#39118;&#38505;&#12289;&#38544;&#31169;&#27844;&#28431;&#12289;&#38750;&#20984;&#20248;&#21270;&#21644;&#22797;&#26434;&#30340;&#26080;&#32447;&#29615;&#22659;&#31561;&#24040;&#22823;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#31216;&#20026;&#20998;&#24067;&#24335;&#32676;&#26234;&#33021;&#23398;&#20064;&#65288;DSL&#65289;&#30340;&#26032;&#22411;&#26694;&#26550;&#65292;&#23558;&#20154;&#24037;&#26234;&#33021;&#21644;&#29983;&#29289;&#32676;&#26234;&#33021;&#22312;&#25972;&#20307;&#19978;&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#36816;&#29992;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#21644;&#36890;&#20449;&#25216;&#26415;&#65292;DSL&#20026;&#26080;&#32447;&#32593;&#32476;&#36793;&#32536;&#30340;&#22823;&#35268;&#27169;IoT&#25552;&#20379;&#20102;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#21644;&#31283;&#20581;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20188v1 Announce Type: cross  Abstract: The rapid growth of Internet of Things (IoT) has led to the widespread deployment of smart IoT devices at wireless edge for collaborative machine learning tasks, ushering in a new era of edge learning. With a huge number of hardware-constrained IoT devices operating in resource-limited wireless networks, edge learning encounters substantial challenges, including communication and computation bottlenecks, device and data heterogeneity, security risks, privacy leakages, non-convex optimization, and complex wireless environments. To address these issues, this article explores a novel framework known as distributed swarm learning (DSL), which combines artificial intelligence and biological swarm intelligence in a holistic manner. By harnessing advanced signal processing and communications, DSL provides efficient solutions and robust tools for large-scale IoT at the edge of wireless networks.
&lt;/p&gt;</description></item><item><title>HARMamba&#21033;&#29992;&#26356;&#36731;&#37327;&#32423;&#30340;&#36873;&#25321;&#24615;SSM&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#26550;&#26500;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#36164;&#28304;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.20183</link><description>&lt;p&gt;
HARMamba: &#22522;&#20110;&#21452;&#21521;&#36873;&#25321;&#24615;SSM&#30340;&#39640;&#25928;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
HARMamba: Efficient Wearable Sensor Human Activity Recognition Based on Bidirectional Selective SSM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20183
&lt;/p&gt;
&lt;p&gt;
HARMamba&#21033;&#29992;&#26356;&#36731;&#37327;&#32423;&#30340;&#36873;&#25321;&#24615;SSM&#20316;&#20026;&#22522;&#30784;&#27169;&#22411;&#26550;&#26500;&#65292;&#20197;&#35299;&#20915;&#35745;&#31639;&#36164;&#28304;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#27963;&#21160;&#24863;&#30693;&#39046;&#22495;&#30340;&#37325;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#39640;&#25928;&#30340;&#30828;&#20214;&#24863;&#30693;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;Mamba&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#26696;&#20986;&#29616;&#12290;HARMamba&#24341;&#20837;&#20102;&#26356;&#36731;&#37327;&#32423;&#30340;&#36873;&#25321;&#24615;SSM&#20316;&#20026;&#27963;&#21160;&#35782;&#21035;&#30340;&#22522;&#26412;&#27169;&#22411;&#26550;&#26500;&#65292;&#20197;&#35299;&#20915;&#31995;&#32479;&#35745;&#31639;&#36127;&#36733;&#21644;&#20869;&#23384;&#20351;&#29992;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20183v1 Announce Type: cross  Abstract: Wearable sensor human activity recognition (HAR) is a crucial area of research in activity sensing. While transformer-based temporal deep learning models have been extensively studied and implemented, their large number of parameters present significant challenges in terms of system computing load and memory usage, rendering them unsuitable for real-time mobile activity recognition applications. Recently, an efficient hardware-aware state space model (SSM) called Mamba has emerged as a promising alternative. Mamba demonstrates strong potential in long sequence modeling, boasts a simpler network architecture, and offers an efficient hardware-aware design. Leveraging SSM for activity recognition represents an appealing avenue for exploration. In this study, we introduce HARMamba, which employs a more lightweight selective SSM as the foundational model architecture for activity recognition. The goal is to address the computational resourc
&lt;/p&gt;</description></item><item><title>&#38656;&#35201;&#22312;&#20154;&#24037;&#31995;&#32479;&#20013;&#24179;&#34913;&#35752;&#35770;&#24847;&#35782;&#30340;&#21487;&#33021;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#24847;&#35782;&#30340;&#32500;&#24230;&#21644;&#29305;&#24449;&#26469;&#36827;&#34892;&#35752;&#35770;&#30340;&#24517;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.20177</link><description>&lt;p&gt;
&#20154;&#24037;&#24847;&#35782;&#12290;&#19968;&#20123;&#36923;&#36753;&#21644;&#27010;&#24565;&#21021;&#27493;
&lt;/p&gt;
&lt;p&gt;
Artificial consciousness. Some logical and conceptual preliminaries
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20177
&lt;/p&gt;
&lt;p&gt;
&#38656;&#35201;&#22312;&#20154;&#24037;&#31995;&#32479;&#20013;&#24179;&#34913;&#35752;&#35770;&#24847;&#35782;&#30340;&#21487;&#33021;&#23454;&#29616;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#24847;&#35782;&#30340;&#32500;&#24230;&#21644;&#29305;&#24449;&#26469;&#36827;&#34892;&#35752;&#35770;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20177v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20154;&#24037;&#24847;&#35782;&#22312;&#29702;&#35770;&#19978;&#26159;&#21542;&#21487;&#33021;&#65311;&#26159;&#21542;&#21512;&#20046;&#24773;&#29702;&#65311;&#22914;&#26524;&#26159;&#65292;&#37027;&#20040;&#25216;&#26415;&#19978;&#21487;&#34892;&#21527;&#65311;&#35201;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26377;&#24517;&#35201;&#22880;&#23450;&#19968;&#20123;&#22522;&#30784;&#65292;&#38416;&#26126;&#20154;&#24037;&#24847;&#35782;&#20135;&#29983;&#30340;&#36923;&#36753;&#21644;&#32463;&#39564;&#26465;&#20214;&#20197;&#21450;&#28041;&#21450;&#30340;&#30456;&#20851;&#26415;&#35821;&#30340;&#21547;&#20041;&#12290;&#24847;&#35782;&#26159;&#19968;&#20010;&#22810;&#20041;&#35789;&#65306;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#65292;&#21253;&#25324;&#31070;&#32463;&#31185;&#23398;&#12289;&#20154;&#24037;&#26234;&#33021;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#21644;&#21746;&#23398;&#31561;&#65292;&#26377;&#26102;&#20250;&#20351;&#29992;&#19981;&#21516;&#26415;&#35821;&#26469;&#25351;&#31216;&#30456;&#21516;&#29616;&#35937;&#65292;&#25110;&#32773;&#20351;&#29992;&#30456;&#21516;&#26415;&#35821;&#26469;&#25351;&#31216;&#19981;&#21516;&#29616;&#35937;&#12290;&#20107;&#23454;&#19978;&#65292;&#22914;&#26524;&#25105;&#20204;&#24819;&#25506;&#35752;&#20154;&#24037;&#24847;&#35782;&#65292;&#23601;&#38656;&#35201;&#24688;&#24403;&#30028;&#23450;&#20851;&#38190;&#27010;&#24565;&#12290;&#22312;&#27492;&#65292;&#32463;&#36807;&#19968;&#20123;&#36923;&#36753;&#21644;&#27010;&#24565;&#21021;&#27493;&#24037;&#20316;&#21518;&#65292;&#25105;&#20204;&#35748;&#20026;&#26377;&#24517;&#35201;&#20351;&#29992;&#24847;&#35782;&#30340;&#32500;&#24230;&#21644;&#29305;&#24449;&#36827;&#34892;&#24179;&#34913;&#35752;&#35770;&#65292;&#25506;&#35752;&#23427;&#20204;&#22312;&#20154;&#24037;&#31995;&#32479;&#20013;&#30340;&#21487;&#33021;&#23454;&#20363;&#21270;&#25110;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20177v1 Announce Type: new  Abstract: Is artificial consciousness theoretically possible? Is it plausible? If so, is it technically feasible? To make progress on these questions, it is necessary to lay some groundwork clarifying the logical and empirical conditions for artificial consciousness to arise and the meaning of relevant terms involved. Consciousness is a polysemic word: researchers from different fields, including neuroscience, Artificial Intelligence, robotics, and philosophy, among others, sometimes use different terms in order to refer to the same phenomena or the same terms to refer to different phenomena. In fact, if we want to pursue artificial consciousness, a proper definition of the key concepts is required. Here, after some logical and conceptual preliminaries, we argue for the necessity of using dimensions and profiles of consciousness for a balanced discussion about their possible instantiation or realisation in artificial systems. Our primary goal in t
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#19982;Fine-tuned&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#23186;&#20307;&#20559;&#35265;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT&#22312;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#21644;&#25991;&#26412;&#32423;&#19978;&#19979;&#25991;&#20559;&#35265;&#26041;&#38754;&#34920;&#29616;&#19968;&#33268;&#65292;&#20294;&#22312;&#26816;&#27979;&#34394;&#20551;&#26032;&#38395;&#12289;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#35748;&#30693;&#20559;&#35265;&#26041;&#38754;&#21017;&#23384;&#22312;&#22256;&#38590;&#12290;</title><link>https://arxiv.org/abs/2403.20158</link><description>&lt;p&gt;
ChatGPT&#19982;&#23186;&#20307;&#20559;&#35265;&#65306;GPT-3.5&#21644;Fine-tuned&#35821;&#35328;&#27169;&#22411;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
ChatGPT v.s. Media Bias: A Comparative Study of GPT-3.5 and Fine-tuned Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20158
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#19982;Fine-tuned&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#27979;&#23186;&#20307;&#20559;&#35265;&#26041;&#38754;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;ChatGPT&#22312;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#21644;&#25991;&#26412;&#32423;&#19978;&#19979;&#25991;&#20559;&#35265;&#26041;&#38754;&#34920;&#29616;&#19968;&#33268;&#65292;&#20294;&#22312;&#26816;&#27979;&#34394;&#20551;&#26032;&#38395;&#12289;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#35748;&#30693;&#20559;&#35265;&#26041;&#38754;&#21017;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25105;&#20204;&#24555;&#36895;&#21457;&#23637;&#30340;&#25968;&#23383;&#39046;&#22495;&#20013;&#65292;&#36776;&#21035;&#23186;&#20307;&#20559;&#35265;&#30340;&#33021;&#21147;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#22609;&#36896;&#20844;&#20247;&#24773;&#32490;&#24182;&#24433;&#21709;&#20851;&#38190;&#20915;&#31574;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#20855;&#26377;&#24191;&#27867;&#23454;&#29992;&#24615;&#65292;&#24341;&#21457;&#20102;&#23545;&#23427;&#20204;&#22312;&#23186;&#20307;&#20559;&#35265;&#26816;&#27979;&#20013;&#26377;&#25928;&#24615;&#30340;&#25506;&#31350;&#12290;ChatGPT&#33021;&#21542;&#26816;&#27979;&#23186;&#20307;&#20559;&#35265;&#65311;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#21033;&#29992;&#23186;&#20307;&#20559;&#35265;&#35782;&#21035;&#22522;&#20934;&#65288;MBIB&#65289;&#26469;&#35780;&#20272;ChatGPT&#22312;&#21306;&#20998;&#20845;&#31181;&#23186;&#20307;&#20559;&#35265;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#19982;Fine-tuned&#27169;&#22411;&#65288;&#22914;BART&#12289;ConvBERT&#21644;GPT-2&#65289;&#36827;&#34892;&#23545;&#27604;&#12290;&#30740;&#31350;&#32467;&#26524;&#21576;&#29616;&#20102;&#19968;&#31181;&#20108;&#20998;&#27861;&#65306;ChatGPT&#22312;&#26816;&#27979;&#20167;&#24680;&#35328;&#35770;&#21644;&#25991;&#26412;&#32423;&#19978;&#19979;&#25991;&#20559;&#35265;&#26041;&#38754;&#19982;Fine-tuned&#27169;&#22411;&#34920;&#29616;&#19968;&#33268;&#65292;&#20294;&#22312;&#20854;&#20182;&#20559;&#35265;&#26816;&#27979;&#30340;&#26356;&#24494;&#22937;&#35201;&#32032;&#19978;&#65292;&#21363;&#34394;&#20551;&#26032;&#38395;&#12289;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#35748;&#30693;&#20559;&#35265;&#26041;&#38754;&#21017;&#38754;&#20020;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20158v1 Announce Type: cross  Abstract: In our rapidly evolving digital sphere, the ability to discern media bias becomes crucial as it can shape public sentiment and influence pivotal decisions. The advent of large language models (LLMs), such as ChatGPT, noted for their broad utility in various natural language processing (NLP) tasks, invites exploration of their efficacy in media bias detection. Can ChatGPT detect media bias? This study seeks to answer this question by leveraging the Media Bias Identification Benchmark (MBIB) to assess ChatGPT's competency in distinguishing six categories of media bias, juxtaposed against fine-tuned models such as BART, ConvBERT, and GPT-2. The findings present a dichotomy: ChatGPT performs at par with fine-tuned models in detecting hate speech and text-level context bias, yet faces difficulties with subtler elements of other bias detections, namely, fake news, racial, gender, and cognitive biases.
&lt;/p&gt;</description></item><item><title>CAESAR&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#25910;&#25947;&#24863;&#30693;&#37319;&#26679;&#21644;&#31579;&#36873;&#26426;&#21046;&#65292;&#26377;&#25928;&#22686;&#24378;&#20102;&#20010;&#20307;&#20195;&#29702;&#22312;&#19981;&#21516;MDPs&#19978;&#30340;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.20156</link><description>&lt;p&gt;
&#22312;&#24322;&#26500;MDPs&#20013;&#36890;&#36807;&#25910;&#25947;&#24863;&#30693;&#37319;&#26679;&#19982;&#31579;&#36873;&#22686;&#24378;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#30340;CAESAR&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
CAESAR: Enhancing Federated RL in Heterogeneous MDPs through Convergence-Aware Sampling with Screening
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20156
&lt;/p&gt;
&lt;p&gt;
CAESAR&#31639;&#27861;&#36890;&#36807;&#32467;&#21512;&#25910;&#25947;&#24863;&#30693;&#37319;&#26679;&#21644;&#31579;&#36873;&#26426;&#21046;&#65292;&#26377;&#25928;&#22686;&#24378;&#20102;&#20010;&#20307;&#20195;&#29702;&#22312;&#19981;&#21516;MDPs&#19978;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20540;&#20026;&#22522;&#30784;&#20195;&#29702;&#22312;&#19981;&#21516;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDPs&#65289;&#20043;&#38388;&#36816;&#34892;&#26102;&#30340;&#32852;&#37030;&#24378;&#21270;&#23398;&#20064;&#65288;FedRL&#65289;&#12290;&#29616;&#26377;&#30340;FedRL&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#20195;&#29702;&#30340;&#20540;&#20989;&#25968;&#36827;&#34892;&#24179;&#22343;&#26469;&#25913;&#21892;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#24322;&#26500;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#32858;&#21512;&#31574;&#30053;&#22312;&#20195;&#29702;&#25910;&#25947;&#21040;&#19981;&#21516;&#30340;&#26368;&#20248;&#20540;&#20989;&#25968;&#26102;&#26159;&#27425;&#20248;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#35774;&#35745;&#29992;&#20110;&#22686;&#24378;&#20010;&#20307;&#20195;&#29702;&#36328;&#21508;&#31181;MDPs&#23398;&#20064;&#30340;Convergence-AwarE SAmpling with scReening&#65288;CAESAR&#65289;&#32858;&#21512;&#26041;&#26696;&#12290;CAESAR&#26159;&#26381;&#21153;&#22120;&#20351;&#29992;&#30340;&#19968;&#31181;&#32858;&#21512;&#31574;&#30053;&#65292;&#32467;&#21512;&#20102;&#25910;&#25947;&#24863;&#30693;&#37319;&#26679;&#21644;&#31579;&#36873;&#26426;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#23398;&#20064;&#30456;&#21516;MDP&#20013;&#20195;&#29702;&#25910;&#25947;&#21040;&#30456;&#21516;&#26368;&#20248;&#20540;&#20989;&#25968;&#30340;&#20107;&#23454;&#65292;CAESAR&#20351;&#24471;&#33021;&#22815;&#20174;&#26356;&#29087;&#32451;&#30340;&#21516;&#34892;&#37027;&#37324;&#26377;&#36873;&#25321;&#22320;&#21560;&#25910;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20156v1 Announce Type: cross  Abstract: In this study, we delve into Federated Reinforcement Learning (FedRL) in the context of value-based agents operating across diverse Markov Decision Processes (MDPs). Existing FedRL methods typically aggregate agents' learning by averaging the value functions across them to improve their performance. However, this aggregation strategy is suboptimal in heterogeneous environments where agents converge to diverse optimal value functions. To address this problem, we introduce the Convergence-AwarE SAmpling with scReening (CAESAR) aggregation scheme designed to enhance the learning of individual agents across varied MDPs. CAESAR is an aggregation strategy used by the server that combines convergence-aware sampling with a screening mechanism. By exploiting the fact that agents learning in identical MDPs are converging to the same optimal value function, CAESAR enables the selective assimilation of knowledge from more proficient counterparts, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#25955;&#24335;&#28608;&#21169;&#26426;&#21046;&#65292;&#26088;&#22312;&#22312;&#36710;&#32852;&#32593;&#29615;&#22659;&#20013;&#20026;&#31227;&#21160;AIGC&#26381;&#21153;&#20998;&#37197;&#30340;&#20379;&#38656;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.20151</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#25955;&#24335;&#36710;&#32852;&#32593;&#31227;&#21160;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#26381;&#21153;&#28608;&#21169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Learning-based Incentive Mechanism for Mobile AIGC Service in Decentralized Internet of Vehicles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#25955;&#24335;&#28608;&#21169;&#26426;&#21046;&#65292;&#26088;&#22312;&#22312;&#36710;&#32852;&#32593;&#29615;&#22659;&#20013;&#20026;&#31227;&#21160;AIGC&#26381;&#21153;&#20998;&#37197;&#30340;&#20379;&#38656;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20151v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#25351;&#30340;&#26159;&#21033;&#29992;AI&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#21270;&#20869;&#23481;&#29983;&#25104;&#30340;&#33539;&#24335;&#12290;&#36710;&#32852;&#32593;&#65288;IoV&#65289;&#32593;&#32476;&#20013;&#30340;&#31227;&#21160;AIGC&#26381;&#21153;&#27604;&#20256;&#32479;&#22522;&#20110;&#20113;&#30340;AIGC&#26381;&#21153;&#20855;&#26377;&#35832;&#22810;&#20248;&#21183;&#65292;&#21253;&#25324;&#22686;&#24378;&#30340;&#32593;&#32476;&#25928;&#29575;&#12289;&#26356;&#22909;&#30340;&#21487;&#37325;&#26500;&#24615;&#65292;&#20197;&#21450;&#26356;&#24378;&#30340;&#25968;&#25454;&#23433;&#20840;&#21644;&#38544;&#31169;&#24615;&#12290;&#28982;&#32780;&#65292;AIGC&#26381;&#21153;&#25552;&#20379;&#32463;&#24120;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#36164;&#28304;&#21463;&#38480;&#30340;&#36335;&#36793;&#21333;&#20803;&#65288;RSUs&#65289;&#38754;&#20020;&#30528;&#22312;&#19981;&#38477;&#20302;&#25972;&#20307;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#32500;&#25252;&#22810;&#26679;&#21270;AIGC&#26381;&#21153;&#27744;&#24182;&#28385;&#36275;&#25152;&#26377;&#29992;&#25143;&#26381;&#21153;&#35831;&#27714;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#31227;&#21160;AIGC&#26381;&#21153;&#20998;&#37197;&#30340;&#20998;&#25955;&#28608;&#21169;&#26426;&#21046;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25214;&#21040;RSUs&#19978;AIGC&#26381;&#21153;&#20379;&#24212;&#21644;IoV&#29615;&#22659;&#20013;&#29992;&#25143;&#26381;&#21153;&#38656;&#27714;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#20248;&#21270;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20151v1 Announce Type: new  Abstract: Artificial Intelligence-Generated Content (AIGC) refers to the paradigm of automated content generation utilizing AI models. Mobile AIGC services in the Internet of Vehicles (IoV) network have numerous advantages over traditional cloud-based AIGC services, including enhanced network efficiency, better reconfigurability, and stronger data security and privacy. Nonetheless, AIGC service provisioning frequently demands significant resources. Consequently, resource-constrained roadside units (RSUs) face challenges in maintaining a heterogeneous pool of AIGC services and addressing all user service requests without degrading overall performance. Therefore, in this paper, we propose a decentralized incentive mechanism for mobile AIGC service allocation, employing multi-agent deep reinforcement learning to find the balance between the supply of AIGC services on RSUs and user demand for services within the IoV context, optimizing user experience
&lt;/p&gt;</description></item><item><title>TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.20150</link><description>&lt;p&gt;
TFB&#65306;&#38754;&#21521;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#20840;&#38754;&#19988;&#20844;&#24179;&#30340;&#22522;&#20934;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
TFB: Towards Comprehensive and Fair Benchmarking of Time Series Forecasting Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20150
&lt;/p&gt;
&lt;p&gt;
TFB&#36890;&#36807;&#35299;&#20915;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#12289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#20197;&#21450;&#19981;&#19968;&#33268;&#12289;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#31561;&#38382;&#39064;&#65292;&#25512;&#21160;&#20102;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#22522;&#20934;&#27604;&#36739;&#30340;&#26368;&#26032;&#25216;&#26415;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#20250;&#22312;&#32463;&#27982;&#12289;&#20132;&#36890;&#12289;&#20581;&#24247;&#21644;&#33021;&#28304;&#31561;&#19981;&#21516;&#39046;&#22495;&#20013;&#20135;&#29983;&#65292;&#23545;&#26410;&#26469;&#25968;&#20540;&#30340;&#39044;&#27979;&#22312;&#35768;&#22810;&#37325;&#35201;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#19981;&#20986;&#25152;&#26009;&#65292;&#35768;&#22810;&#39044;&#27979;&#26041;&#27861;&#34987;&#25552;&#20986;&#12290;&#20026;&#20102;&#30830;&#20445;&#36827;&#23637;&#65292;&#26377;&#24517;&#35201;&#33021;&#22815;&#20197;&#20840;&#38754;&#19988;&#21487;&#38752;&#30340;&#26041;&#24335;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;&#21644;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TFB&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;TSF&#65289;&#26041;&#27861;&#22522;&#20934;&#27979;&#35797;&#12290;TFB&#36890;&#36807;&#35299;&#20915;&#19982;&#25968;&#25454;&#38598;&#12289;&#27604;&#36739;&#26041;&#27861;&#21644;&#35780;&#20272;&#31649;&#36947;&#30456;&#20851;&#30340;&#32570;&#28857;&#65292;&#25512;&#21160;&#20102;&#26368;&#26032;&#25216;&#26415;&#30340;&#21457;&#23637;&#65306;1&#65289;&#25968;&#25454;&#39046;&#22495;&#35206;&#30422;&#19981;&#36275;&#65292;2&#65289;&#23545;&#20256;&#32479;&#26041;&#27861;&#30340;&#21051;&#26495;&#21360;&#35937;&#65292;3&#65289;&#19981;&#19968;&#33268;&#21644;&#19981;&#28789;&#27963;&#30340;&#27969;&#31243;&#12290;&#20026;&#20102;&#33719;&#24471;&#26356;&#22909;&#30340;&#39046;&#22495;&#35206;&#30422;&#29575;&#65292;&#25105;&#20204;&#21253;&#25324;&#20102;&#26469;&#33258;10&#20010;&#19981;&#21516;&#39046;&#22495;&#30340;&#25968;&#25454;&#38598;&#65306;&#20132;&#36890;&#12289;&#30005;&#21147;&#12289;&#33021;&#28304;&#12289;&#29615;&#22659;&#12289;&#33258;&#28982;&#12289;&#32463;&#27982;&#12289;&#32929;&#31080;&#24066;&#22330;&#12289;&#38134;&#34892;&#12289;&#20581;&#24247;&#21644;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#19968;&#20010;&#26102;&#38388;&#24207;&#21015;&#29305;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20150v1 Announce Type: cross  Abstract: Time series are generated in diverse domains such as economic, traffic, health, and energy, where forecasting of future values has numerous important applications. Not surprisingly, many forecasting methods are being proposed. To ensure progress, it is essential to be able to study and compare such methods empirically in a comprehensive and reliable manner. To achieve this, we propose TFB, an automated benchmark for Time Series Forecasting (TSF) methods. TFB advances the state-of-the-art by addressing shortcomings related to datasets, comparison methods, and evaluation pipelines: 1) insufficient coverage of data domains, 2) stereotype bias against traditional methods, and 3) inconsistent and inflexible pipelines. To achieve better domain coverage, we include datasets from 10 different domains: traffic, electricity, energy, the environment, nature, economic, stock markets, banking, health, and the web. We also provide a time series char
&lt;/p&gt;</description></item><item><title>LLM&#20013;&#20986;&#29616;&#30340;&#31163;&#32676;&#20540;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;Block&#28014;&#28857;&#65288;BFP&#65289;&#26684;&#24335;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#22359;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25512;&#29702;&#38656;&#27714;&#20013;&#30340;&#30828;&#20214;&#25903;&#25345;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.20137</link><description>&lt;p&gt;
LLM&#20013;&#24102;&#26377;&#31163;&#32676;&#20540;&#30340;&#20934;&#30830;&#22359;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Accurate Block Quantization in LLMs with Outliers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20137
&lt;/p&gt;
&lt;p&gt;
LLM&#20013;&#20986;&#29616;&#30340;&#31163;&#32676;&#20540;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#20837;Block&#28014;&#28857;&#65288;BFP&#65289;&#26684;&#24335;&#65292;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#22359;&#37327;&#21270;&#65292;&#35299;&#20915;&#20102;&#22823;&#35268;&#27169;&#25512;&#29702;&#38656;&#27714;&#20013;&#30340;&#30828;&#20214;&#25903;&#25345;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#20010;&#26376;&#26469;&#65292;&#23545;LLMs&#36827;&#34892;&#26497;&#22823;&#35268;&#27169;&#25512;&#29702;&#30340;&#38656;&#27714;&#21576;&#29616;&#20986;&#24040;&#22823;&#22686;&#38271;&#12290;&#36825;&#20984;&#26174;&#20102;&#19987;&#29992;&#30828;&#20214;&#20005;&#37325;&#30701;&#32570;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#26080;&#27861;&#39640;&#25928;&#24555;&#36895;&#22320;&#22788;&#29702;&#30456;&#20851;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#31227;&#21160;&#12290;&#38382;&#39064;&#21152;&#21095;&#20110;&#25152;&#22788;&#29702;&#24207;&#21015;&#38271;&#24230;&#19981;&#26029;&#22686;&#38271;&#65292;&#22240;&#20026;&#36825;&#20123;&#24207;&#21015;&#38656;&#35201;&#19982;&#24207;&#21015;&#38271;&#24230;&#25104;&#27604;&#20363;&#30340;KV-cache&#30340;&#39640;&#25928;&#29255;&#19978;&#23384;&#20648;&#12290;&#20026;&#20102;&#20351;&#25152;&#38656;&#35745;&#31639;&#21487;&#34892;&#24182;&#23558;&#30456;&#20851;&#25968;&#25454;&#25918;&#20837;&#21487;&#29992;&#20869;&#23384;&#20013;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#37327;&#21270;&#25216;&#26415;&#65292;&#20801;&#35768;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;&#20934;&#30830;&#37327;&#21270;&#12290;&#22312;&#36825;&#26041;&#38754;&#30340;&#19968;&#20010;&#20027;&#35201;&#26368;&#36817;&#31361;&#30772;&#26159;&#24341;&#20837;&#19968;&#32452;&#20855;&#26377;&#20849;&#20139;&#27604;&#20363;&#22240;&#23376;&#30340;&#23614;&#25968;&#22359;&#30340;Block&#28014;&#28857;&#65288;BFP&#65289;&#26684;&#24335;&#65292;&#36825;&#20123;&#25216;&#26415;&#20351;&#24471;&#20855;&#26377;&#39640;&#25928;&#30828;&#20214;&#25903;&#25345;&#30340;&#24352;&#37327;&#25805;&#20316;&#30340;&#35760;&#24518;&#12289;&#21151;&#32791;&#21644;&#35745;&#31639;&#21464;&#24471;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20137v1 Announce Type: new  Abstract: The demand for inference on extremely large scale LLMs has seen enormous growth in the recent months. It made evident the colossal shortage of dedicated hardware capable of efficient and fast processing of the involved compute and memory movement. The problem is aggravated by the exploding raise in the lengths of the sequences being processed, since those require efficient on-chip storage of the KV-cache of size proportional to the sequence length. To make the required compute feasible and fit the involved data into available memory, numerous quantization techniques have been proposed that allow accurate quantization for both weights and activations. One of the main recent breakthroughs in this direction was introduction of the family of Block Floating Point (BFP) formats characterized by a block of mantissas with a shared scale factor. These enable memory- power-, and compute- efficient hardware support of the tensor operations and prov
&lt;/p&gt;</description></item><item><title>&#38646;&#26679;&#26412;&#26816;&#27979;&#22120;&#36890;&#24120;&#29420;&#31435;&#20998;&#26512;AI&#29983;&#25104;&#25991;&#26412;&#65292;&#32780;&#24573;&#30053;&#20102;&#21407;&#22987;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#21487;&#33021;&#23548;&#33268;&#22312;&#21487;&#33021;&#24615;&#35780;&#20272;&#20013;&#23384;&#22312;&#24046;&#24322;</title><link>https://arxiv.org/abs/2403.20127</link><description>&lt;p&gt;
&#25552;&#31034;&#23545;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#38646;&#26679;&#26412;&#26816;&#27979;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Impact of Prompts on Zero-Shot Detection of AI-Generated Text
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20127
&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#26816;&#27979;&#22120;&#36890;&#24120;&#29420;&#31435;&#20998;&#26512;AI&#29983;&#25104;&#25991;&#26412;&#65292;&#32780;&#24573;&#30053;&#20102;&#21407;&#22987;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#21487;&#33021;&#23548;&#33268;&#22312;&#21487;&#33021;&#24615;&#35780;&#20272;&#20013;&#23384;&#22312;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#34429;&#28982;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#29616;&#22312;&#24050;&#32463;&#24191;&#27867;&#65292;&#20294;&#20854;&#34987;&#28389;&#29992;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#29983;&#25104;&#20551;&#26032;&#38395;&#21644;&#29359;&#19979;&#21117;&#31363;&#34892;&#20026;&#65292;&#24341;&#36215;&#20102;&#37325;&#22823;&#20851;&#27880;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#26816;&#27979;&#22120;&#26469;&#35780;&#20272;&#32473;&#23450;&#25991;&#26412;&#26159;&#20154;&#31867;&#29983;&#25104;&#36824;&#26159;AI&#29983;&#25104;&#30340;&#33021;&#21147;&#12290;&#22312;&#35768;&#22810;&#26041;&#27861;&#20013;&#65292;&#38646;&#26679;&#26412;&#26816;&#27979;&#22120;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23427;&#20204;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#36890;&#24120;&#22522;&#20110;&#21487;&#33021;&#24615;&#12290;&#22312;&#22522;&#20110;&#32842;&#22825;&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#29992;&#25143;&#36890;&#24120;&#36755;&#20837;&#25552;&#31034;&#24182;&#21033;&#29992;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#38646;&#26679;&#26412;&#26816;&#27979;&#22120;&#36890;&#24120;&#29420;&#31435;&#20998;&#26512;&#36825;&#20123;&#25991;&#26412;&#65292;&#24573;&#30053;&#20102;&#21407;&#22987;&#25552;&#31034;&#30340;&#24433;&#21709;&#12290;&#21487;&#20197;&#24819;&#35937;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#33021;&#23548;&#33268;&#25991;&#26412;&#29983;&#25104;&#38454;&#27573;&#21644;&#26816;&#27979;&#38454;&#27573;&#30340;&#21487;&#33021;&#24615;&#35780;&#20272;&#20043;&#38388;&#23384;&#22312;&#24046;&#24322;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#20173;&#28982;&#26377;&#19968;&#20010;&#26410;&#32463;&#39564;&#35777;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20127v1 Announce Type: new  Abstract: In recent years, there have been significant advancements in the development of Large Language Models (LLMs). While their practical applications are now widespread, their potential for misuse, such as generating fake news and committing plagiarism, has posed significant concerns. To address this issue, detectors have been developed to evaluate whether a given text is human-generated or AI-generated. Among others, zero-shot detectors stand out as effective approaches that do not require additional training data and are often likelihood-based. In chat-based applications, users commonly input prompts and utilize the AI-generated texts. However, zero-shot detectors typically analyze these texts in isolation, neglecting the impact of the original prompts. It is conceivable that this approach may lead to a discrepancy in likelihood assessments between the text generation phase and the detection phase. So far, there remains an unverified gap co
&lt;/p&gt;</description></item><item><title>Mol-AIR &#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#20869;&#22312;&#22870;&#21169;&#30340;&#20998;&#23376;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#21644;&#23398;&#20064;&#30340;&#20869;&#22312;&#22870;&#21169;&#20248;&#21183;&#65292;&#22312;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#24615;&#36136;&#30340;&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20109</link><description>&lt;p&gt;
Mol-AIR&#65306;&#20351;&#29992;&#33258;&#36866;&#24212;&#20869;&#22312;&#22870;&#21169;&#30340;&#20998;&#23376;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#30446;&#26631;&#23548;&#21521;&#30340;&#20998;&#23376;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Mol-AIR: Molecular Reinforcement Learning with Adaptive Intrinsic Rewards for Goal-directed Molecular Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20109
&lt;/p&gt;
&lt;p&gt;
Mol-AIR &#25552;&#20986;&#20102;&#20351;&#29992;&#33258;&#36866;&#24212;&#20869;&#22312;&#22870;&#21169;&#30340;&#20998;&#23376;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#21382;&#21490;&#21644;&#23398;&#20064;&#30340;&#20869;&#22312;&#22870;&#21169;&#20248;&#21183;&#65292;&#22312;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#24615;&#36136;&#30340;&#20998;&#23376;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20248;&#21270;&#21457;&#29616;&#20855;&#26377;&#26399;&#26395;&#24615;&#36136;&#30340;&#20998;&#23376;&#32467;&#26500;&#30340;&#25216;&#26415;&#22312;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#33647;&#29289;&#21457;&#29616;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#23558;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#32467;&#21512;&#24050;&#32463;&#25104;&#20026;&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#24615;&#36136;&#30340;&#20998;&#23376;&#30340;&#26377;&#25928;&#31574;&#30053;&#12290;&#23613;&#31649;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#25506;&#32034;&#24222;&#22823;&#30340;&#21270;&#23398;&#31354;&#38388;&#21644;&#20248;&#21270;&#29305;&#23450;&#21270;&#23398;&#24615;&#36136;&#26041;&#38754;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Mol-AIR&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#20351;&#29992;&#33258;&#36866;&#24212;&#20869;&#22312;&#22870;&#21169;&#36827;&#34892;&#26377;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#20998;&#23376;&#29983;&#25104;&#12290;Mol-AIR&#21033;&#29992;&#22522;&#20110;&#21382;&#21490;&#30340;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#20869;&#22312;&#22870;&#21169;&#30340;&#20248;&#21183;&#65292;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#33976;&#39311;&#32593;&#32476;&#21644;&#22522;&#20110;&#35745;&#25968;&#30340;&#31574;&#30053;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;Mol-AIR&#23637;&#29616;&#20102;&#36229;&#20986;&#29616;&#26377;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#22312;&#29983;&#25104;&#20855;&#26377;&#26399;&#26395;&#24615;&#36136;&#30340;&#20998;&#23376;&#26041;&#38754;&#65292;&#26080;&#38656;&#20219;&#20309;&#20808;&#39564;&#30693;&#35782;&#65292;&#21253;&#25324;pena
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20109v1 Announce Type: cross  Abstract: Optimizing techniques for discovering molecular structures with desired properties is crucial in artificial intelligence(AI)-based drug discovery. Combining deep generative models with reinforcement learning has emerged as an effective strategy for generating molecules with specific properties. Despite its potential, this approach is ineffective in exploring the vast chemical space and optimizing particular chemical properties. To overcome these limitations, we present Mol-AIR, a reinforcement learning-based framework using adaptive intrinsic rewards for effective goal-directed molecular generation. Mol-AIR leverages the strengths of both history-based and learning-based intrinsic rewards by exploiting random distillation network and counting-based strategies. In benchmark tests, Mol-AIR demonstrates superior performance over existing approaches in generating molecules with desired properties without any prior knowledge, including pena
&lt;/p&gt;</description></item><item><title>ITCMA&#26159;&#22522;&#20110;&#35745;&#31639;&#24847;&#35782;&#32467;&#26500;&#30340;&#29983;&#25104;&#24335;Agent&#65292;&#36890;&#36807;&#32771;&#34385;Agent&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#21644;&#25512;&#29702;&#65292;&#22686;&#24378;&#20102;LLMs&#29702;&#35299;&#38544;&#21547;&#25351;&#20196;&#21644;&#24212;&#29992;&#24120;&#35782;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#22312;Alfworld&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#20110;SOTA&#12290;</title><link>https://arxiv.org/abs/2403.20097</link><description>&lt;p&gt;
&#22522;&#20110;&#35745;&#31639;&#24847;&#35782;&#32467;&#26500;&#30340;&#29983;&#25104;&#24335;Agent&#65306;ITCMA
&lt;/p&gt;
&lt;p&gt;
ITCMA: A Generative Agent Based on a Computational Consciousness Structure
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20097
&lt;/p&gt;
&lt;p&gt;
ITCMA&#26159;&#22522;&#20110;&#35745;&#31639;&#24847;&#35782;&#32467;&#26500;&#30340;&#29983;&#25104;&#24335;Agent&#65292;&#36890;&#36807;&#32771;&#34385;Agent&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#21644;&#25512;&#29702;&#65292;&#22686;&#24378;&#20102;LLMs&#29702;&#35299;&#38544;&#21547;&#25351;&#20196;&#21644;&#24212;&#29992;&#24120;&#35782;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#22312;Alfworld&#29615;&#22659;&#20013;&#34920;&#29616;&#20248;&#20110;SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#38656;&#35201;&#29702;&#35299;&#38544;&#21547;&#25351;&#20196;&#21644;&#24212;&#29992;&#24120;&#35782;&#30693;&#35782;&#30340;&#20219;&#21153;&#20013;&#20173;&#28982;&#38754;&#20020;&#25361;&#25112;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;LLMs&#21487;&#33021;&#38656;&#35201;&#22810;&#27425;&#23581;&#35797;&#25165;&#33021;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#30340;&#34920;&#29616;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#20135;&#29983;&#19981;&#20934;&#30830;&#30340;&#21709;&#24212;&#25110;&#25512;&#29702;&#65292;&#24433;&#21709;&#23427;&#20204;&#30340;&#38271;&#26399;&#19968;&#33268;&#24615;&#21644;&#34892;&#20026;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20869;&#37096;&#26102;&#38388;&#24847;&#35782;&#26426;&#22120;&#65288;ITCM&#65289;&#65292;&#19968;&#20010;&#35745;&#31639;&#24847;&#35782;&#32467;&#26500;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;ITCM&#30340;Agent&#65288;ITCMA&#65289;&#65292;&#25903;&#25345;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#29983;&#25104;&#34892;&#20026;&#21644;&#25512;&#29702;&#12290;ITCMA&#36890;&#36807;&#32771;&#34385;Agent&#19982;&#29615;&#22659;&#30340;&#20114;&#21160;&#21644;&#25512;&#29702;&#65292;&#22686;&#24378;&#20102;LLMs&#29702;&#35299;&#38544;&#21547;&#25351;&#20196;&#21644;&#24212;&#29992;&#24120;&#35782;&#30693;&#35782;&#30340;&#33021;&#21147;&#12290;&#22312;Alfworld&#29615;&#22659;&#20013;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#32463;&#36807;&#35757;&#32451;&#30340;ITCMA&#22312;&#24050;&#30693;&#38598;&#19978;&#27604;&#26368;&#20808;&#36827;&#25216;&#26415;&#65288;SOTA&#65289;&#34920;&#29616;&#25552;&#39640;&#20102;9%&#12290;&#21363;&#20351;&#26410;&#32463;&#35757;&#32451;&#30340;ITCMA&#20063;&#36798;&#21040;&#20102;96%&#30340;&#20219;&#21153;&#23436;&#25104;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20097v1 Announce Type: new  Abstract: Large Language Models (LLMs) still face challenges in tasks requiring understanding implicit instructions and applying common-sense knowledge. In such scenarios, LLMs may require multiple attempts to achieve human-level performance, potentially leading to inaccurate responses or inferences in practical environments, affecting their long-term consistency and behavior. This paper introduces the Internal Time-Consciousness Machine (ITCM), a computational consciousness structure. We further propose the ITCM-based Agent (ITCMA), which supports behavior generation and reasoning in open-world settings. ITCMA enhances LLMs' ability to understand implicit instructions and apply common-sense knowledge by considering agents' interaction and reasoning with the environment. Evaluations in the Alfworld environment show that trained ITCMA outperforms the state-of-the-art (SOTA) by 9% on the seen set. Even untrained ITCMA achieves a 96% task completion 
&lt;/p&gt;</description></item><item><title>AI&#27861;&#26696;&#21487;&#33021;&#25104;&#20026;&#24357;&#21512;&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#24459;&#30340;&#37325;&#35201;&#19968;&#29615;&#65292;&#36890;&#36807;&#23558;&#38750;&#27495;&#35270;&#36131;&#20219;&#36716;&#31227;&#21040;AI&#27169;&#22411;&#35774;&#35745;&#38454;&#27573;&#65292;&#25512;&#21160;&#20559;&#35265;&#26816;&#27979;&#21644;&#20559;&#35265;&#26657;&#27491;&#30340;&#30456;&#20851;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2403.20089</link><description>&lt;p&gt;
AI&#27861;&#26696;&#23545;&#38750;&#27495;&#35270;&#27861;&#24459;&#21644;&#31639;&#27861;&#20844;&#24179;&#24615;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Implications of the AI Act for Non-Discrimination Law and Algorithmic Fairness
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20089
&lt;/p&gt;
&lt;p&gt;
AI&#27861;&#26696;&#21487;&#33021;&#25104;&#20026;&#24357;&#21512;&#31639;&#27861;&#20844;&#24179;&#24615;&#21644;&#27431;&#30431;&#38750;&#27495;&#35270;&#27861;&#24459;&#30340;&#37325;&#35201;&#19968;&#29615;&#65292;&#36890;&#36807;&#23558;&#38750;&#27495;&#35270;&#36131;&#20219;&#36716;&#31227;&#21040;AI&#27169;&#22411;&#35774;&#35745;&#38454;&#27573;&#65292;&#25512;&#21160;&#20559;&#35265;&#26816;&#27979;&#21644;&#20559;&#35265;&#26657;&#27491;&#30340;&#30456;&#20851;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#20844;&#24179;&#24615;&#30340;&#35805;&#39064;&#65292;&#27491;&#22914;&#22312;FATE&#65288;AI&#20844;&#24179;&#24615;&#12289;&#36131;&#20219;&#24615;&#12289;&#36879;&#26126;&#24615;&#21644;&#20262;&#29702;&#65289;&#31038;&#21306;&#20013;&#35752;&#35770;&#30340;&#37027;&#26679;&#65292;&#22312;&#36807;&#21435;&#20960;&#24180;&#20013;&#24341;&#21457;&#20102;&#26377;&#24847;&#20041;&#30340;&#35752;&#35770;&#12290;&#28982;&#32780;&#65292;&#20174;&#27861;&#24459;&#35282;&#24230;&#26469;&#30475;&#65292;&#29305;&#21035;&#26159;&#20174;&#27431;&#30431;&#27861;&#24459;&#30340;&#35282;&#24230;&#65292;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#24453;&#35299;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#31639;&#27861;&#20844;&#24179;&#24615;&#26088;&#22312;&#22312;&#35774;&#35745;&#38454;&#27573;&#20943;&#36731;&#32467;&#26500;&#19981;&#24179;&#31561;&#65292;&#20294;&#27431;&#27954;&#30340;&#38750;&#27495;&#35270;&#27861;&#24459;&#21017;&#26159;&#38024;&#23545;AI&#27169;&#22411;&#37096;&#32626;&#21518;&#30340;&#20010;&#21035;&#27495;&#35270;&#26696;&#20214;&#37327;&#36523;&#23450;&#21046;&#30340;&#12290;AI&#27861;&#26696;&#21487;&#33021;&#26159;&#26397;&#30528;&#24357;&#21512;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#24040;&#22823;&#19968;&#27493;&#65292;&#36890;&#36807;&#23558;&#38750;&#27495;&#35270;&#36131;&#20219;&#36716;&#31227;&#21040;AI&#27169;&#22411;&#35774;&#35745;&#38454;&#27573;&#12290;&#22522;&#20110;&#23545;AI&#27861;&#26696;&#30340;&#32508;&#21512;&#38405;&#35835;&#65292;&#25105;&#20204;&#35780;&#35770;&#20102;&#27861;&#24459;&#21644;&#25216;&#26415;&#25191;&#34892;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#22312;&#20559;&#35265;&#26816;&#27979;&#21644;&#20559;&#35265;&#26657;&#27491;&#26041;&#38754;&#30340;&#23454;&#38469;&#24433;&#21709;&#65292;&#20197;&#20415;&#25351;&#23450;&#21644;&#31526;&#21512;&#29305;&#23450;&#30340;&#25216;&#26415;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20089v1 Announce Type: new  Abstract: The topic of fairness in AI, as debated in the FATE (Fairness, Accountability, Transparency, and Ethics in AI) communities, has sparked meaningful discussions in the past years. However, from a legal perspective, particularly from European Union law, many open questions remain. Whereas algorithmic fairness aims to mitigate structural inequalities at the design level, European non-discrimination law is tailored to individual cases of discrimination after an AI model has been deployed. The AI Act might present a tremendous step towards bridging these two concepts by shifting non-discrimination responsibilities into the design stage of AI models. Based on an integrative reading of the AI Act, we comment on legal as well as technical enforcement problems and propose practical implications on bias detection and bias correction in order to specify and comply with specific technical requirements.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#25509;&#21463;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#27169;&#24577;&#20998;&#31163;&#21644;&#37325;&#26500;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20058</link><description>&lt;p&gt;
&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#21644;&#28145;&#24230;&#25972;&#21512;&#30340;&#33041;&#20195;&#35874;&#12289;&#34880;&#28082;&#21160;&#21147;&#23398;&#21644;&#28748;&#27880;&#32593;&#32476;&#24443;&#24213;&#25913;&#21464;&#30142;&#30149;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
Revolutionizing Disease Diagnosis with simultaneous functional PET/MR and Deeply Integrated Brain Metabolic, Hemodynamic, and Perfusion Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20058
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22522;&#20110;AI&#30340;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#21033;&#29992;&#21516;&#26102;&#21151;&#33021;PET/MR&#25216;&#26415;&#65292;&#33021;&#22815;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#25509;&#21463;&#21333;&#27169;&#24577;&#21644;&#22810;&#27169;&#24577;&#36755;&#20837;&#65292;&#20855;&#26377;&#21019;&#26032;&#30340;&#27169;&#24577;&#20998;&#31163;&#21644;&#37325;&#26500;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21516;&#26102;&#21151;&#33021;PET/MR&#65288;sf-PET/MR&#65289;&#26159;&#19968;&#31181;&#23574;&#31471;&#30340;&#22810;&#27169;&#24335;&#31070;&#32463;&#24433;&#20687;&#25216;&#26415;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#20250;&#65292;&#21487;&#20197;&#21516;&#26102;&#30417;&#27979;&#21644;&#25972;&#21512;&#30001;&#26102;&#31354;&#21327;&#21464;&#20195;&#35874;&#27963;&#21160;&#12289;&#31070;&#32463;&#27963;&#21160;&#21644;&#33041;&#34880;&#27969;&#65288;&#28748;&#27880;&#65289;&#26500;&#24314;&#30340;&#22810;&#26041;&#38754;&#22823;&#33041;&#32593;&#32476;&#12290;&#34429;&#28982;&#22312;&#31185;&#23398;/&#20020;&#24202;&#20215;&#20540;&#19978;&#24456;&#39640;&#65292;&#20294;PET/MR&#30828;&#20214;&#30340;&#21487;&#21450;&#24615;&#19981;&#36275;&#38459;&#30861;&#20102;&#20854;&#24212;&#29992;&#65292;&#26356;&#19981;&#29992;&#35828;&#29616;&#20195;&#22522;&#20110;AI&#30340;PET/MR&#34701;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24320;&#21457;&#19968;&#20010;&#22522;&#20110;AI&#30340;&#20020;&#24202;&#21487;&#34892;&#30142;&#30149;&#35786;&#26029;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22522;&#20110;&#20840;&#38754;&#30340;sf-PET/MR&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#20855;&#26377;&#20801;&#35768;&#21333;&#27169;&#24577;&#36755;&#20837;&#65288;&#20363;&#22914;&#65292;&#20165;PET&#65289;&#20197;&#21450;&#24378;&#21046;&#22810;&#27169;&#24577;&#20934;&#30830;&#24615;&#30340;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MX-ARM&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#19987;&#23478;&#28151;&#21512;&#23545;&#40784;&#21644;&#37325;&#26500;&#27169;&#22411;&#12290;&#23427;&#26159;&#27169;&#24577;&#21487;&#20998;&#31163;&#21644;&#21487;&#20132;&#25442;&#30340;&#65292;&#21160;&#24577;&#20998;&#37197;&#19981;&#21516;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;"&#28151;&#21512;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20058v1 Announce Type: cross  Abstract: Simultaneous functional PET/MR (sf-PET/MR) presents a cutting-edge multimodal neuroimaging technique. It provides an unprecedented opportunity for concurrently monitoring and integrating multifaceted brain networks built by spatiotemporally covaried metabolic activity, neural activity, and cerebral blood flow (perfusion). Albeit high scientific/clinical values, short in hardware accessibility of PET/MR hinders its applications, let alone modern AI-based PET/MR fusion models. Our objective is to develop a clinically feasible AI-based disease diagnosis model trained on comprehensive sf-PET/MR data with the power of, during inferencing, allowing single modality input (e.g., PET only) as well as enforcing multimodal-based accuracy. To this end, we propose MX-ARM, a multimodal MiXture-of-experts Alignment and Reconstruction Model. It is modality detachable and exchangeable, allocating different multi-layer perceptrons dynamically ("mixture 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#31616;&#21333;&#30340;&#21103;&#35789;&#21024;&#38500;&#31574;&#30053;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22686;&#24378;&#25991;&#26412;&#25968;&#25454;&#65292;&#20445;&#30041;&#35821;&#20041;&#65292;&#36866;&#29992;&#20110;&#21333;&#19968;&#25991;&#26412;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.20015</link><description>&lt;p&gt;
&#21103;&#35789;&#26159;&#20851;&#38190;&#65306;&#20351;&#29992;&#21103;&#35789;&#21024;&#38500;&#36827;&#34892;&#31616;&#21333;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Adverb Is the Key: Simple Text Data Augmentation with Adverb Deletion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20015
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#31616;&#21333;&#30340;&#21103;&#35789;&#21024;&#38500;&#31574;&#30053;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#33021;&#26377;&#25928;&#22686;&#24378;&#25991;&#26412;&#25968;&#25454;&#65292;&#20445;&#30041;&#35821;&#20041;&#65292;&#36866;&#29992;&#20110;&#21333;&#19968;&#25991;&#26412;&#20998;&#31867;&#21644;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#39046;&#22495;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#34987;&#24191;&#27867;&#37319;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25104;&#26412;&#25928;&#30410;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#23384;&#22312;&#21487;&#33021;&#20002;&#22833;&#32473;&#23450;&#25991;&#26412;&#30340;&#21407;&#22987;&#35821;&#20041;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25991;&#26412;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#31616;&#21333;&#22320;&#21024;&#38500;&#22312;&#21477;&#23376;&#20013;&#36215;&#36741;&#21161;&#20316;&#29992;&#30340;&#21103;&#35789;&#26469;&#36991;&#20813;&#36825;&#31181;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#20840;&#38754;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#19981;&#20165;&#36866;&#29992;&#20110;&#21333;&#19968;&#25991;&#26412;&#20998;&#31867;&#65292;&#36824;&#36866;&#29992;&#20110;&#38656;&#35201;&#35821;&#20041;&#20445;&#30041;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12290;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#29992;&#20110;&#21487;&#37325;&#29616;&#24615;&#30340;&#28304;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20015v1 Announce Type: cross  Abstract: In the field of text data augmentation, rule-based methods are widely adopted for real-world applications owing to their cost-efficiency. However, conventional rule-based approaches suffer from the possibility of losing the original semantics of the given text. We propose a novel text data augmentation strategy that avoids such phenomena through a straightforward deletion of adverbs, which play a subsidiary role in the sentence. Our comprehensive experiments demonstrate the efficiency and effectiveness of our proposed approach for not just single text classification, but also natural language inference that requires semantic preservation. We publicly released our source code for reproducibility.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PURPLE&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#28436;&#31034;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;SQL&#29983;&#25104;&#20013;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.20014</link><description>&lt;p&gt;
PURPLE: &#35753;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25104;&#20026;&#26356;&#22909;&#30340;SQL&#32534;&#20889;&#22120;
&lt;/p&gt;
&lt;p&gt;
PURPLE: Making a Large Language Model a Better SQL Writer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20014
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PURPLE&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#28436;&#31034;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;SQL&#29983;&#25104;&#20013;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25216;&#26415;&#22312;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#65288;NL2SQL&#65289;&#32763;&#35793;&#20013;&#36215;&#30528;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#35757;&#32451;&#30340;LLMs&#20855;&#26377;&#24378;&#22823;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#22522;&#26412;SQL&#29983;&#25104;&#33021;&#21147;&#65292;&#26080;&#38656;&#38024;&#23545;NL2SQL&#20219;&#21153;&#36827;&#34892;&#39069;&#22806;&#35843;&#25972;&#12290;&#29616;&#26377;&#22522;&#20110;LLMs&#30340;NL2SQL&#26041;&#27861;&#35797;&#22270;&#36890;&#36807;&#24378;&#35843;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#26469;&#25552;&#39640;&#32763;&#35793;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#22312;&#32452;&#32455;&#22797;&#26434;&#30340;&#36923;&#36753;&#36816;&#31639;&#31526;&#32452;&#21512;&#26041;&#38754;&#30340;&#30693;&#35782;&#65292;LLMs&#26377;&#26102;&#20250;&#29983;&#25104;&#19981;&#21512;&#36866;&#30340;SQL&#12290;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#26159;&#21521;LLMs&#36755;&#20837;&#28436;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#25968;&#25454;&#24211;&#30340;&#24050;&#30693;NL2SQL&#32763;&#35793;&#12290;LLMs&#21487;&#20197;&#20174;&#36755;&#20837;&#28436;&#31034;&#20013;&#23398;&#20064;&#22914;&#20309;&#20026;&#32473;&#23450;&#20219;&#21153;&#32452;&#32455;&#36816;&#31639;&#31526;&#32452;&#21512;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PURPLE&#65288;Pre-trained models Utilized to Retrieve Prompts for Logical Enhancement&#65289;&#65292;&#36890;&#36807;&#26816;&#32034;&#28436;&#31034;&#26469;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20014v1 Announce Type: cross  Abstract: Large Language Model (LLM) techniques play an increasingly important role in Natural Language to SQL (NL2SQL) translation. LLMs trained by extensive corpora have strong natural language understanding and basic SQL generation abilities without additional tuning specific to NL2SQL tasks. Existing LLMs-based NL2SQL approaches try to improve the translation by enhancing the LLMs with an emphasis on user intention understanding. However, LLMs sometimes fail to generate appropriate SQL due to their lack of knowledge in organizing complex logical operator composition. A promising method is to input the LLMs with demonstrations, which include known NL2SQL translations from various databases. LLMs can learn to organize operator compositions from the input demonstrations for the given task. In this paper, we propose PURPLE (Pre-trained models Utilized to Retrieve Prompts for Logical Enhancement), which improves accuracy by retrieving demonstrati
&lt;/p&gt;</description></item><item><title>&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#24425;&#21098;&#36148;&#30340;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36880;&#28176;&#22686;&#21152;&#22686;&#24378;&#22270;&#20687;&#30340;&#22122;&#22768;&#21644;&#38590;&#24230;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.20012</link><description>&lt;p&gt;
&#22810;&#24425;&#30340;&#21098;&#36148;&#65306;&#20351;&#29992;&#35838;&#31243;&#23398;&#20064;&#22686;&#24378;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Colorful Cutout: Enhancing Image Data Augmentation with Curriculum Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20012
&lt;/p&gt;
&lt;p&gt;
&#37319;&#29992;&#35838;&#31243;&#23398;&#20064;&#30340;&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#24425;&#21098;&#36148;&#30340;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#65292;&#36880;&#28176;&#22686;&#21152;&#22686;&#24378;&#22270;&#20687;&#30340;&#22122;&#22768;&#21644;&#38590;&#24230;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35757;&#32451;&#30340;&#19968;&#31181;&#27491;&#21017;&#21270;&#31574;&#30053;&#65292;&#23427;&#22686;&#24378;&#20102;&#27867;&#21270;&#33021;&#21147;&#65292;&#38450;&#27490;&#36807;&#25311;&#21512;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#12290;&#20316;&#32773;&#22312;&#26412;&#30740;&#31350;&#20013;&#37319;&#29992;&#35838;&#31243;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#22270;&#20687;&#25968;&#25454;&#22686;&#24378;&#65292;&#24182;&#25552;&#20986;&#20102;&#22810;&#24425;&#30340;&#21098;&#36148;&#65292;&#36880;&#28176;&#22686;&#21152;&#20102;&#22686;&#24378;&#22270;&#20687;&#20013;&#24341;&#20837;&#30340;&#22122;&#22768;&#21644;&#38590;&#24230;&#12290;&#23454;&#39564;&#32467;&#26524;&#20984;&#26174;&#20102;&#35838;&#31243;&#25968;&#25454;&#22686;&#24378;&#29992;&#20110;&#22270;&#20687;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#12290;&#20316;&#32773;&#20844;&#24320;&#21457;&#24067;&#20102;&#30740;&#31350;&#20195;&#30721;&#65292;&#20197;&#25552;&#39640;&#30740;&#31350;&#30340;&#21487;&#37325;&#29616;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20012v1 Announce Type: cross  Abstract: Data augmentation is one of the regularization strategies for the training of deep learning models, which enhances generalizability and prevents overfitting, leading to performance improvement. Although researchers have proposed various data augmentation techniques, they often lack consideration for the difficulty of augmented data. Recently, another line of research suggests incorporating the concept of curriculum learning with data augmentation in the field of natural language processing. In this study, we adopt curriculum data augmentation for image data augmentation and propose colorful cutout, which gradually increases the noise and difficulty introduced in the augmented image. Our experimental results highlight the possibility of curriculum data augmentation for image data. We publicly released our source code to improve the reproducibility of our study.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#35270;&#35273;&#12289;&#26412;&#20307;&#24863;&#30693;&#21644;&#35821;&#35328;&#30340;&#22823;&#33041;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#21644;&#20027;&#21160;&#25512;&#26029;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#33258;&#30001;&#33021;&#21407;&#29702;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#32452;&#21512;&#24615;&#21644;&#24863;&#35273;&#36816;&#21160;&#25216;&#33021;&#30340;&#32852;&#21512;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.19995</link><description>&lt;p&gt;
&#36890;&#36807;&#20132;&#20114;&#23398;&#20064;&#35821;&#35328;&#21644;&#26426;&#22120;&#20154;&#21160;&#20316;&#23454;&#29616;&#32452;&#21512;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Development of Compositionality and Generalization through Interactive Learning of Language and Action of Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19995
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#35270;&#35273;&#12289;&#26412;&#20307;&#24863;&#30693;&#21644;&#35821;&#35328;&#30340;&#22823;&#33041;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#39044;&#27979;&#32534;&#30721;&#21644;&#20027;&#21160;&#25512;&#26029;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;&#33258;&#30001;&#33021;&#21407;&#29702;&#65292;&#23454;&#29616;&#20102;&#35821;&#35328;&#32452;&#21512;&#24615;&#21644;&#24863;&#35273;&#36816;&#21160;&#25216;&#33021;&#30340;&#32852;&#21512;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#25797;&#38271;&#23558;&#23398;&#21040;&#30340;&#34892;&#20026;&#24212;&#29992;&#20110;&#26410;&#23398;&#20064;&#36807;&#30340;&#24773;&#22659;&#12290;&#36825;&#31181;&#27867;&#21270;&#34892;&#20026;&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#25105;&#20204;&#33021;&#22815;&#23558;&#25972;&#20307;&#20998;&#35299;&#25104;&#21487;&#37325;&#22797;&#21033;&#29992;&#30340;&#37096;&#20998;&#30340;&#33021;&#21147;&#65292;&#21363;&#32452;&#21512;&#24615;&#12290;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#26159;&#28041;&#21450;&#36825;&#31181;&#29305;&#24449;&#12290;&#8220;&#22312;&#20010;&#20307;&#21482;&#23398;&#20064;&#37096;&#20998;&#35821;&#35328;&#32452;&#21512;&#21450;&#20854;&#30456;&#24212;&#30340;&#24863;&#35273;&#36816;&#21160;&#27169;&#24335;&#26102;&#65292;&#22914;&#20309;&#36890;&#36807;&#32852;&#24819;&#23398;&#20064;&#21516;&#26102;&#21457;&#23637;&#35821;&#35328;&#30340;&#32452;&#21512;&#24615;&#21644;&#24863;&#35273;&#36816;&#21160;&#25216;&#33021;&#65311;&#8221;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#35270;&#35273;&#12289;&#26412;&#20307;&#24863;&#30693;&#21644;&#35821;&#35328;&#30340;&#22823;&#33041;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23558;&#20854;&#32435;&#20837;&#22522;&#20110;&#33258;&#30001;&#33021;&#21407;&#29702;&#30340;&#39044;&#27979;&#32534;&#30721;&#21644;&#20027;&#21160;&#25512;&#26029;&#26694;&#26550;&#20013;&#12290;&#36890;&#36807;&#19982;&#26426;&#22120;&#20154;&#25163;&#33218;&#36827;&#34892;&#30340;&#21508;&#31181;&#27169;&#25311;&#23454;&#39564;&#35780;&#20272;&#20102;&#36825;&#20010;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#21644;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23398;&#20064;&#20013;&#23545;&#20110;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19995v1 Announce Type: new  Abstract: Humans excel at applying learned behavior to unlearned situations. A crucial component of this generalization behavior is our ability to compose/decompose a whole into reusable parts, an attribute known as compositionality. One of the fundamental questions in robotics concerns this characteristic. "How can linguistic compositionality be developed concomitantly with sensorimotor skills through associative learning, particularly when individuals only learn partial linguistic compositions and their corresponding sensorimotor patterns?" To address this question, we propose a brain-inspired neural network model that integrates vision, proprioception, and language into a framework of predictive coding and active inference, based on the free-energy principle. The effectiveness and capabilities of this model were assessed through various simulation experiments conducted with a robot arm. Our results show that generalization in learning to unlear
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;MindArm&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#22823;&#33041;&#20449;&#21495;&#32763;&#35793;&#25104;&#20551;&#32930;&#36816;&#21160;&#65292;&#24110;&#21161;&#24739;&#32773;&#25191;&#34892;&#21508;&#31181;&#27963;&#21160;</title><link>https://arxiv.org/abs/2403.19992</link><description>&lt;p&gt;
MindArm: &#26426;&#26800;&#26234;&#33021;&#38750;&#20405;&#20837;&#24335;&#31070;&#32463;&#39537;&#21160;&#20551;&#32930;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MindArm: Mechanized Intelligent Non-Invasive Neuro-Driven Prosthetic Arm System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19992
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;MindArm&#65292;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23558;&#22823;&#33041;&#20449;&#21495;&#32763;&#35793;&#25104;&#20551;&#32930;&#36816;&#21160;&#65292;&#24110;&#21161;&#24739;&#32773;&#25191;&#34892;&#21508;&#31181;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#65292;&#27531;&#30142;&#25110;&#38590;&#20197;&#31227;&#21160;&#25163;&#33218;&#30340;&#20154;&#65288;&#31616;&#31216;&#8220;&#24739;&#32773;&#8221;&#65289;&#22312;&#26377;&#25928;&#35299;&#20915;&#29983;&#29702;&#38480;&#21046;&#26041;&#38754;&#26377;&#38750;&#24120;&#26377;&#38480;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20027;&#35201;&#26159;&#30001;&#20110;&#20004;&#20010;&#21407;&#22240;&#65306;&#19968;&#26159;&#20687;&#20197;&#24605;&#32500;&#25511;&#21046;&#20026;&#20027;&#30340;&#20551;&#32930;&#35774;&#22791;&#36890;&#24120;&#38750;&#24120;&#26114;&#36149;&#24182;&#38656;&#35201;&#26114;&#36149;&#30340;&#32500;&#25252;&#65307;&#20108;&#26159;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#38656;&#35201;&#26114;&#36149;&#30340;&#20405;&#20837;&#24615;&#33041;&#37096;&#25163;&#26415;&#65292;&#36825;&#31181;&#25163;&#26415;&#39118;&#38505;&#39640;&#65292;&#26114;&#36149;&#19988;&#32500;&#25252;&#22256;&#38590;&#12290;&#22240;&#27492;&#65292;&#24403;&#21069;&#30340;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#24182;&#19981;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#36130;&#21153;&#32972;&#26223;&#30340;&#25152;&#26377;&#24739;&#32773;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#25104;&#26412;&#25216;&#26415;&#35299;&#20915;&#26041;&#26696;&#65292;&#21517;&#20026;MindArm&#65292;&#21363;&#19968;&#31181;&#26426;&#26800;&#26234;&#33021;&#38750;&#20405;&#20837;&#24335;&#31070;&#32463;&#39537;&#21160;&#20551;&#32930;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;MindArm&#31995;&#32479;&#37319;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#24341;&#25806;&#23558;&#22823;&#33041;&#20449;&#21495;&#32763;&#35793;&#25104;&#39044;&#26399;&#30340;&#20551;&#32930;&#36816;&#21160;&#65292;&#20174;&#32780;&#24110;&#21161;&#24739;&#32773;&#23454;&#26045;&#35768;&#22810;&#27963;&#21160;&#65292;&#23613;&#31649;&#20182;&#20204;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19992v1 Announce Type: new  Abstract: Currently, people with disability or difficulty to move their arms (referred to as "patients") have very limited technological solutions to efficiently address their physiological limitations. It is mainly due to two reasons: (1) the non-invasive solutions like mind-controlled prosthetic devices are typically very costly and require expensive maintenance; and (2) other solutions require costly invasive brain surgery, which is high risk to perform, expensive, and difficult to maintain. Therefore, current technological solutions are not accessible for all patients with different financial backgrounds. Toward this, we propose a low-cost technological solution called MindArm, a mechanized intelligent non-invasive neuro-driven prosthetic arm system. Our MindArm system employs a deep neural network (DNN) engine to translate brain signals into the intended prosthetic arm motion, thereby helping patients to perform many activities despite their 
&lt;/p&gt;</description></item><item><title>&#36866;&#37197;&#22120;&#35843;&#25972;&#26041;&#27861;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#36739;&#20248;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#22686;&#37327;&#35843;&#25972;&#20849;&#20139;&#36866;&#37197;&#22120;&#21644;&#21033;&#29992;&#23384;&#20648;&#21407;&#22411;&#36827;&#34892;&#29305;&#24449;&#37319;&#26679;&#21644;&#26356;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19979</link><description>&lt;p&gt;
&#35821;&#20041;&#36716;&#31227;&#22686;&#37327;&#36866;&#37197;&#22120;&#35843;&#25972;&#26159;&#19968;&#31181;&#25345;&#32493;&#30340; ViTransformer
&lt;/p&gt;
&lt;p&gt;
Semantically-Shifted Incremental Adapter-Tuning is A Continual ViTransformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19979
&lt;/p&gt;
&lt;p&gt;
&#36866;&#37197;&#22120;&#35843;&#25972;&#26041;&#27861;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#36739;&#20248;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#22686;&#37327;&#35843;&#25972;&#20849;&#20139;&#36866;&#37197;&#22120;&#21644;&#21033;&#29992;&#23384;&#20648;&#21407;&#22411;&#36827;&#34892;&#29305;&#24449;&#37319;&#26679;&#21644;&#26356;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#23398;&#20064;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;CIL&#65289;&#26088;&#22312;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#25345;&#32493;&#23398;&#20064;&#26032;&#30340;&#31867;&#21035;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#22312;&#25345;&#32493;&#23398;&#20064;&#32972;&#26223;&#19979;&#30340;&#19981;&#21516;&#21442;&#25968;&#39640;&#25928;&#35843;&#25972;&#65288;PET&#65289;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36866;&#37197;&#22120;&#35843;&#25972;&#34920;&#29616;&#20248;&#20110;&#22522;&#20110;&#25552;&#31034;&#30340;&#26041;&#27861;&#65292;&#29978;&#33267;&#22312;&#27599;&#20010;&#23398;&#20064;&#20250;&#35805;&#20013;&#27809;&#26377;&#21442;&#25968;&#25193;&#23637;&#30340;&#24773;&#20917;&#19979;&#20063;&#22914;&#27492;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22686;&#37327;&#35843;&#25972;&#20849;&#20139;&#36866;&#37197;&#22120;&#32780;&#19981;&#26045;&#21152;&#21442;&#25968;&#26356;&#26032;&#32422;&#26463;&#65292;&#22686;&#24378;&#39592;&#24178;&#30340;&#23398;&#20064;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;&#23384;&#20648;&#30340;&#21407;&#22411;&#20013;&#25277;&#21462;&#29305;&#24449;&#26679;&#26412;&#26469;&#37325;&#26032;&#35757;&#32451;&#32479;&#19968;&#30340;&#20998;&#31867;&#22120;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#25105;&#20204;&#20272;&#35745;&#26087;&#21407;&#22411;&#30340;&#35821;&#20041;&#36716;&#31227;&#65292;&#32780;&#26080;&#27861;&#35775;&#38382;&#36807;&#21435;&#30340;&#26679;&#26412;&#65292;&#24182;&#36880;&#20010;&#20250;&#35805;&#26356;&#26032;&#23384;&#20648;&#30340;&#21407;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#27169;&#22411;&#30340;&#25193;&#23637;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19979v1 Announce Type: cross  Abstract: Class-incremental learning (CIL) aims to enable models to continuously learn new classes while overcoming catastrophic forgetting. The introduction of pre-trained models has brought new tuning paradigms to CIL. In this paper, we revisit different parameter-efficient tuning (PET) methods within the context of continual learning. We observe that adapter tuning demonstrates superiority over prompt-based methods, even without parameter expansion in each learning session. Motivated by this, we propose incrementally tuning the shared adapter without imposing parameter update constraints, enhancing the learning capacity of the backbone. Additionally, we employ feature sampling from stored prototypes to retrain a unified classifier, further improving its performance. We estimate the semantic shift of old prototypes without access to past samples and update stored prototypes session by session. Our proposed method eliminates model expansion and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26500;&#24314;&#29305;&#23450;&#20110;&#20195;&#29702;&#30340;&#25968;&#25454;&#24182;&#32454;&#35843;&#27169;&#22411;&#20197;&#21450;&#35774;&#35745;&#33021;&#22815;&#26377;&#25928;&#28608;&#27963;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#22686;&#24378;&#20302;&#21442;&#25968;LLMs&#30340;&#36890;&#29992;&#20195;&#29702;&#21151;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19962</link><description>&lt;p&gt;
&#36890;&#36807;&#35843;&#25972;&#21644;&#22810;&#25903;&#36335;&#25512;&#29702;&#22686;&#24378;&#20302;&#21442;&#25968;LLMs&#30340;&#36890;&#29992;&#20195;&#29702;&#21151;&#33021;
&lt;/p&gt;
&lt;p&gt;
Enhancing the General Agent Capabilities of Low-Parameter LLMs through Tuning and Multi-Branch Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19962
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26500;&#24314;&#29305;&#23450;&#20110;&#20195;&#29702;&#30340;&#25968;&#25454;&#24182;&#32454;&#35843;&#27169;&#22411;&#20197;&#21450;&#35774;&#35745;&#33021;&#22815;&#26377;&#25928;&#28608;&#27963;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#26041;&#27861;&#26469;&#22686;&#24378;&#20302;&#21442;&#25968;LLMs&#30340;&#36890;&#29992;&#20195;&#29702;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19962v1 &#22768;&#26126;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#24320;&#28304;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#34920;&#29616;&#20986;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#38750;&#24120;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#23427;&#20204;&#29992;&#20316;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#22797;&#26434;&#38382;&#39064;&#30340;&#20195;&#29702;&#26102;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#36828;&#36828;&#19981;&#21450;ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#21830;&#29992;&#27169;&#22411;&#12290;&#20316;&#20026;&#26234;&#33021;&#20195;&#29702;&#65292;LLMs&#38656;&#35201;&#20855;&#22791;&#20219;&#21153;&#35268;&#21010;&#12289;&#38271;&#26399;&#35760;&#24518;&#20197;&#21450;&#21033;&#29992;&#22806;&#37096;&#24037;&#20855;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#30340;&#33021;&#21147;&#12290;&#21508;&#31181;&#26041;&#27861;&#24050;&#34987;&#25552;&#20986;&#26469;&#22686;&#24378;LLMs&#30340;&#20195;&#29702;&#33021;&#21147;&#12290;&#19968;&#26041;&#38754;&#65292;&#26377;&#20123;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#29305;&#23450;&#20110;&#20195;&#29702;&#30340;&#25968;&#25454;&#21644;&#24494;&#35843;&#27169;&#22411;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#19968;&#20123;&#26041;&#27861;&#38598;&#20013;&#20110;&#35774;&#35745;&#33021;&#26377;&#25928;&#28608;&#27963;LLMs&#25512;&#29702;&#33021;&#21147;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;7B&#21644;13B&#27169;&#22411;&#19978;&#21516;&#26102;&#25506;&#35752;&#20102;&#36825;&#20004;&#31181;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;GPT-4&#26500;&#24314;&#29305;&#23450;&#20110;&#20195;&#29702;&#25968;&#25454;&#30340;&#20840;&#38754;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19962v1 Announce Type: cross  Abstract: Open-source pre-trained Large Language Models (LLMs) exhibit strong language understanding and generation capabilities, making them highly successful in a variety of tasks. However, when used as agents for dealing with complex problems in the real world, their performance is far inferior to large commercial models such as ChatGPT and GPT-4. As intelligent agents, LLMs need to have the capabilities of task planning, long-term memory, and the ability to leverage external tools to achieve satisfactory performance. Various methods have been proposed to enhance the agent capabilities of LLMs. On the one hand, methods involve constructing agent-specific data and fine-tuning the models. On the other hand, some methods focus on designing prompts that effectively activate the reasoning abilities of the LLMs. We explore both strategies on the 7B and 13B models. We propose a comprehensive method for constructing agent-specific data using GPT-4. T
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#24037;&#19994;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#28151;&#20957;&#22303;&#23380;&#20013;&#23436;&#25104;&#25554;&#38144;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#36731;&#24494;&#23558;&#25554;&#38144;&#19982;&#22681;&#38754;&#20998;&#31163;&#30340;&#31574;&#30053;&#65292;&#22312;&#26080;&#38656;&#35299;&#26512;&#24314;&#27169;&#25110;&#25511;&#21046;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#25214;&#21040;&#28151;&#20957;&#22303;&#23380;&#12290;</title><link>https://arxiv.org/abs/2403.19946</link><description>&lt;p&gt;
&#29992;&#20110;&#28151;&#20957;&#22303;&#23380;&#30340;&#25554;&#38144;&#20219;&#21153;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
A Peg-in-hole Task Strategy for Holes in Concrete
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19946
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#24037;&#19994;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#28151;&#20957;&#22303;&#23380;&#20013;&#23436;&#25104;&#25554;&#38144;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#36731;&#24494;&#23558;&#25554;&#38144;&#19982;&#22681;&#38754;&#20998;&#31163;&#30340;&#31574;&#30053;&#65292;&#22312;&#26080;&#38656;&#35299;&#26512;&#24314;&#27169;&#25110;&#25511;&#21046;&#21442;&#25968;&#35843;&#25972;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#22320;&#25214;&#21040;&#28151;&#20957;&#22303;&#23380;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#24037;&#19994;&#26426;&#22120;&#20154;&#33021;&#22815;&#23436;&#25104;&#28151;&#20957;&#22303;&#23380;&#20013;&#25554;&#38144;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#22312;&#25628;&#32034;&#20301;&#32622;&#38388;&#31227;&#21160;&#26102;&#36731;&#24494;&#23558;&#25554;&#38144;&#19982;&#22681;&#38754;&#20998;&#31163;&#65292;&#20197;&#36991;&#20813;&#28151;&#20957;&#22303;&#39640;&#25705;&#25830;&#31995;&#25968;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#23427;&#20351;&#29992;&#32463;&#36807;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#26377;&#25928;&#22320;&#21457;&#29616;&#20855;&#26377;&#21487;&#21464;&#24418;&#29366;&#21644;&#34920;&#38754;&#22788;&#29702;&#30340;&#23380;(&#30001;&#20110;&#28151;&#20957;&#22303;&#33030;&#24615;)&#65292;&#32780;&#26080;&#38656;&#35299;&#26512;&#24314;&#27169;&#25110;&#25511;&#21046;&#21442;&#25968;&#35843;&#25972;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#25554;&#38144;&#26397;&#21521;&#22681;&#38754;&#34920;&#38754;&#30340;&#20301;&#31227;&#65292;&#20197;&#21450;&#21147;&#21644;&#21147;&#30697;&#20316;&#20026;DNN&#30340;&#36755;&#20837;&#20043;&#19968;&#12290;&#30001;&#20110;&#25554;&#38144;&#38752;&#36817;&#23380;&#26102;&#20301;&#31227;&#20250;&#22686;&#21152;(&#30001;&#20110;&#28151;&#20957;&#22303;&#20013;&#23380;&#30340;&#20498;&#35282;&#24418;&#29366;)&#65292;&#22240;&#27492;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#36755;&#20837;DNN&#30340;&#26377;&#29992;&#21442;&#25968;&#12290;&#36890;&#36807;&#23545;DNN&#36827;&#34892;500&#27425;&#23380;&#30340;&#35757;&#32451;&#24182;&#23581;&#35797;&#21457;&#29616;12&#20010;&#26410;&#30693;&#23380;&#26469;&#35780;&#20272;&#20102;&#35813;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19946v1 Announce Type: cross  Abstract: A method that enables an industrial robot to accomplish the peg-in-hole task for holes in concrete is proposed. The proposed method involves slightly detaching the peg from the wall, when moving between search positions, to avoid the negative influence of the concrete's high friction coefficient. It uses a deep neural network (DNN), trained via reinforcement learning, to effectively find holes with variable shape and surface finish (due to the brittle nature of concrete) without analytical modeling or control parameter tuning. The method uses displacement of the peg toward the wall surface, in addition to force and torque, as one of the inputs of the DNN. Since the displacement increases as the peg gets closer to the hole (due to the chamfered shape of holes in concrete), it is a useful parameter for inputting in the DNN. The proposed method was evaluated by training the DNN on a hole 500 times and attempting to find 12 unknown holes. 
&lt;/p&gt;</description></item><item><title>TDANet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#21435;&#22122;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#25913;&#21892;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19943</link><description>&lt;p&gt;
TDANet&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#21435;&#22122;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#25925;&#38556;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
TDANet: A Novel Temporal Denoise Convolutional Neural Network With Attention for Fault Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19943
&lt;/p&gt;
&lt;p&gt;
TDANet&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26102;&#38388;&#21435;&#22122;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#25913;&#21892;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25925;&#38556;&#35786;&#26029;&#22312;&#32500;&#25252;&#26426;&#26800;&#31995;&#32479;&#30340;&#36816;&#34892;&#23436;&#25972;&#24615;&#65292;&#36991;&#20813;&#30001;&#20110;&#24847;&#22806;&#25925;&#38556;&#36896;&#25104;&#30340;&#37325;&#22823;&#25439;&#22833;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TDANet&#30340;&#26102;&#38388;&#21435;&#22122;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#26088;&#22312;&#25913;&#21892;&#22122;&#22768;&#29615;&#22659;&#19979;&#30340;&#25925;&#38556;&#35786;&#26029;&#24615;&#33021;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#20449;&#21495;&#30340;&#21608;&#26399;&#24615;&#36136;&#23558;&#19968;&#32500;&#20449;&#21495;&#36716;&#25442;&#20026;&#20108;&#32500;&#24352;&#37327;&#65292;&#37319;&#29992;&#22810;&#23610;&#24230;2D&#21367;&#31215;&#26680;&#20174;&#21608;&#26399;&#20869;&#37096;&#21644;&#36328;&#21608;&#26399;&#25552;&#21462;&#20449;&#21495;&#20449;&#24687;&#12290;&#36825;&#31181;&#26041;&#27861;&#33021;&#26377;&#25928;&#22320;&#35782;&#21035;&#20449;&#21495;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19943v1 Announce Type: cross  Abstract: Fault diagnosis plays a crucial role in maintaining the operational integrity of mechanical systems, preventing significant losses due to unexpected failures. As intelligent manufacturing and data-driven approaches evolve, Deep Learning (DL) has emerged as a pivotal technique in fault diagnosis research, recognized for its ability to autonomously extract complex features. However, the practical application of current fault diagnosis methods is challenged by the complexity of industrial environments. This paper proposed the Temporal Denoise Convolutional Neural Network With Attention (TDANet), designed to improve fault diagnosis performance in noise environments. This model transforms one-dimensional signals into two-dimensional tensors based on their periodic properties, employing multi-scale 2D convolution kernels to extract signal information both within and across periods. This method enables effective identification of signal chara
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diverse Feature Learning (DFL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#37325;&#35201;&#29305;&#24449;&#20445;&#30041;&#31639;&#27861;&#21644;&#26032;&#29305;&#24449;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#33976;&#39311;&#21644;&#37325;&#32622;&#26469;&#35299;&#20915;&#27169;&#22411;&#23398;&#20064;&#22810;&#26679;&#29305;&#24449;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19941</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#33976;&#39311;&#21644;&#37325;&#32622;&#23454;&#29616;&#22810;&#26679;&#29305;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Diverse Feature Learning by Self-distillation and Reset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19941
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Diverse Feature Learning (DFL)&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#37325;&#35201;&#29305;&#24449;&#20445;&#30041;&#31639;&#27861;&#21644;&#26032;&#29305;&#24449;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#33258;&#33976;&#39311;&#21644;&#37325;&#32622;&#26469;&#35299;&#20915;&#27169;&#22411;&#23398;&#20064;&#22810;&#26679;&#29305;&#24449;&#26102;&#36935;&#21040;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#35770;&#25991;&#35299;&#20915;&#20102;&#27169;&#22411;&#38590;&#20197;&#23398;&#20064;&#22810;&#26679;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#26159;&#23427;&#20204;&#35201;&#20040;&#24536;&#35760;&#20102;&#20808;&#21069;&#23398;&#20064;&#30340;&#29305;&#24449;&#65292;&#35201;&#20040;&#26080;&#27861;&#23398;&#20064;&#26032;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Diverse Feature Learning (DFL)&#30340;&#26041;&#27861;&#65292;&#23427;&#23558;&#37325;&#35201;&#29305;&#24449;&#20445;&#30041;&#31639;&#27861;&#19982;&#26032;&#29305;&#24449;&#23398;&#20064;&#31639;&#27861;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#22320;&#65292;&#20026;&#20102;&#20445;&#30041;&#37325;&#35201;&#29305;&#24449;&#65292;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#36873;&#25321;&#26377;&#24847;&#20041;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#36890;&#36807;&#33258;&#33976;&#39311;&#22312;&#38598;&#25104;&#27169;&#22411;&#20013;&#36827;&#34892;&#12290;&#20026;&#20102;&#23398;&#20064;&#26032;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#23450;&#26399;&#37325;&#26032;&#21021;&#22987;&#21270;&#27169;&#22411;&#30340;&#37325;&#32622;&#26041;&#27861;&#12290;&#36890;&#36807;&#22312;&#22270;&#20687;&#20998;&#31867;&#19978;&#23545;&#21508;&#31181;&#27169;&#22411;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#33258;&#33976;&#39311;&#21644;&#37325;&#32622;&#20043;&#38388;&#30340;&#21327;&#21516;&#25928;&#24212;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19941v1 Announce Type: new  Abstract: Our paper addresses the problem of models struggling to learn diverse features, due to either forgetting previously learned features or failing to learn new ones. To overcome this problem, we introduce Diverse Feature Learning (DFL), a method that combines an important feature preservation algorithm with a new feature learning algorithm. Specifically, for preserving important features, we utilize self-distillation in ensemble models by selecting the meaningful model weights observed during training. For learning new features, we employ reset that involves periodically re-initializing part of the model. As a result, through experiments with various models on the image classification, we have identified the potential for synergistic effects between self-distillation and reset.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;Mamba&#26694;&#26550;&#25972;&#21512;&#21040;Decision Transformer&#26550;&#26500;&#20013;&#65292;&#25552;&#20986;&#20102;Decision Mamba&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#20915;&#31574;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#34920;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#23545;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;</title><link>https://arxiv.org/abs/2403.19925</link><description>&lt;p&gt;
&#20915;&#31574;&#24040;&#34770;&#65306;&#36890;&#36807;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decision Mamba: Reinforcement Learning via Sequence Modeling with Selective State Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;Mamba&#26694;&#26550;&#25972;&#21512;&#21040;Decision Transformer&#26550;&#26500;&#20013;&#65292;&#25552;&#20986;&#20102;Decision Mamba&#65292;&#36890;&#36807;&#22312;&#19981;&#21516;&#20915;&#31574;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#65292;&#34920;&#26126;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#23545;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Decision Transformer&#26159;&#19968;&#31181;&#23558;Transformer&#26550;&#26500;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#23427;&#20381;&#36182;&#22240;&#26524;&#33258;&#27880;&#24847;&#21147;&#26469;&#27169;&#25311;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#22870;&#21169;&#24207;&#21015;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;Mamba&#26694;&#26550;&#30340;&#25972;&#21512;&#65292;&#35813;&#26694;&#26550;&#20197;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#24207;&#21015;&#24314;&#27169;&#33021;&#21147;&#32780;&#38395;&#21517;&#65292;&#23558;&#20854;&#25972;&#21512;&#21040;Decision Transformer&#26550;&#26500;&#20013;&#65292;&#20851;&#27880;&#22312;&#39034;&#24207;&#20915;&#31574;&#20219;&#21153;&#20013;&#28508;&#22312;&#30340;&#24615;&#33021;&#22686;&#24378;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#21508;&#31181;&#20915;&#31574;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#31995;&#21015;&#23454;&#39564;&#26469;&#31995;&#32479;&#35780;&#20272;&#36825;&#31181;&#25972;&#21512;&#65292;&#23558;&#20462;&#25913;&#21518;&#30340;Decision Transformer&#65292;Decision Mamba&#65292;&#19982;&#20256;&#32479;&#23545;&#24212;&#29289;&#36827;&#34892;&#27604;&#36739;&#12290;&#36825;&#39033;&#24037;&#20316;&#20419;&#36827;&#20102;&#39034;&#24207;&#20915;&#31574;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#24433;&#21709;&#20854;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19925v1 Announce Type: cross  Abstract: Decision Transformer, a promising approach that applies Transformer architectures to reinforcement learning, relies on causal self-attention to model sequences of states, actions, and rewards. While this method has shown competitive results, this paper investigates the integration of the Mamba framework, known for its advanced capabilities in efficient and effective sequence modeling, into the Decision Transformer architecture, focusing on the potential performance enhancements in sequential decision-making tasks. Our study systematically evaluates this integration by conducting a series of experiments across various decision-making environments, comparing the modified Decision Transformer, Decision Mamba, with its traditional counterpart. This work contributes to the advancement of sequential decision-making models, suggesting that the architecture and training methodology of neural networks can significantly impact their performance 
&lt;/p&gt;</description></item><item><title>CtRL-Sim&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Nocturne&#27169;&#25311;&#22120;&#20013;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.19918</link><description>&lt;p&gt;
CtRL-Sim&#65306;&#20351;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#21453;&#24212;&#24615;&#21487;&#25511;&#39550;&#39542;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
CtRL-Sim: Reactive and Controllable Driving Agents with Offline Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19918
&lt;/p&gt;
&lt;p&gt;
CtRL-Sim&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;Nocturne&#27169;&#25311;&#22120;&#20013;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CtRL-Sim&#65292;&#19968;&#31181;&#21033;&#29992;&#29289;&#29702;&#22686;&#24378;&#30340;Nocturne&#27169;&#25311;&#22120;&#20013;&#30340;&#22238;&#25253;&#26465;&#20214;&#21270;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26469;&#39640;&#25928;&#29983;&#25104;&#21453;&#24212;&#24615;&#21644;&#21487;&#25511;&#20132;&#36890;&#20195;&#29702;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;Nocturne&#27169;&#25311;&#22120;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#30340;&#39550;&#39542;&#25968;&#25454;&#65292;&#20197;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19918v1 Announce Type: cross  Abstract: Evaluating autonomous vehicle stacks (AVs) in simulation typically involves replaying driving logs from real-world recorded traffic. However, agents replayed from offline data do not react to the actions of the AV, and their behaviour cannot be easily controlled to simulate counterfactual scenarios. Existing approaches have attempted to address these shortcomings by proposing methods that rely on heuristics or learned generative models of real-world data but these approaches either lack realism or necessitate costly iterative sampling procedures to control the generated behaviours. In this work, we take an alternative approach and propose CtRL-Sim, a method that leverages return-conditioned offline reinforcement learning within a physics-enhanced Nocturne simulator to efficiently generate reactive and controllable traffic agents. Specifically, we process real-world driving data through the Nocturne simulator to generate a diverse offli
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;MANGO&#22522;&#20934;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#22238;&#31572;&#28041;&#21450;&#26144;&#23556;&#21644;&#23548;&#33322;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>https://arxiv.org/abs/2403.19913</link><description>&lt;p&gt;
MANGO&#65306;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19913
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;MANGO&#22522;&#20934;&#65292;&#21457;&#29616;&#21363;&#20351;&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#22312;&#22238;&#31572;&#28041;&#21450;&#26144;&#23556;&#21644;&#23548;&#33322;&#30340;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;ChatGPT&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26368;&#36817;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MANGO&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#23427;&#20204;&#25191;&#34892;&#22522;&#20110;&#25991;&#26412;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#21253;&#25324;&#26469;&#33258;&#19968;&#22871;&#25991;&#26412;&#28216;&#25103;&#30340;53&#20010;&#36855;&#23467;&#65306;&#27599;&#20010;&#36855;&#23467;&#37117;&#19982;&#19968;&#20010;&#28216;&#35272;&#35828;&#26126;&#37197;&#23545;&#65292;&#20854;&#20013;&#21253;&#21547;&#27599;&#20010;&#20301;&#32622;&#30340;&#35775;&#38382;&#20294;&#19981;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#30340;&#36335;&#24452;&#12290;&#20219;&#21153;&#26159;&#38382;&#31572;&#65306;&#23545;&#20110;&#27599;&#20010;&#36855;&#23467;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35835;&#21462;&#28216;&#35272;&#35828;&#26126;&#24182;&#22238;&#31572;&#25968;&#30334;&#20010;&#26144;&#23556;&#21644;&#23548;&#33322;&#38382;&#39064;&#65292;&#20363;&#22914;&#8220;&#20320;&#24212;&#35813;&#20174;&#25151;&#23376;&#35199;&#37096;&#22914;&#20309;&#21435;&#38401;&#27004;&#65311;&#8221;&#21644;&#8220;&#22914;&#26524;&#25105;&#20204;&#20174;&#22320;&#19979;&#23460;&#21521;&#21271;&#21644;&#19996;&#36208;&#65292;&#25105;&#20204;&#20250;&#22312;&#21738;&#37324;&#65311;&#8221;&#12290;&#23613;&#31649;&#36825;&#20123;&#38382;&#39064;&#23545;&#20154;&#31867;&#26469;&#35828;&#24456;&#23481;&#26131;&#65292;&#20294;&#20107;&#23454;&#35777;&#26126;&#65292;&#36804;&#20170;&#20026;&#27490;&#26368;&#22909;&#30340;&#35821;&#35328;&#27169;&#22411;GPT-4&#29978;&#33267;&#22312;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#24378;&#22823;&#30340;&#26144;&#23556;&#21644;&#23548;&#33322;&#33021;&#21147;&#23558;&#26377;&#21033;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19913v1 Announce Type: cross  Abstract: Large language models such as ChatGPT and GPT-4 have recently achieved astonishing performance on a variety of natural language processing tasks. In this paper, we propose MANGO, a benchmark to evaluate their capabilities to perform text-based mapping and navigation. Our benchmark includes 53 mazes taken from a suite of textgames: each maze is paired with a walkthrough that visits every location but does not cover all possible paths. The task is question-answering: for each maze, a large language model reads the walkthrough and answers hundreds of mapping and navigation questions such as "How should you go to Attic from West of House?" and "Where are we if we go north and east from Cellar?". Although these questions are easy to humans, it turns out that even GPT-4, the best-to-date language model, performs poorly at answering them. Further, our experiments suggest that a strong mapping and navigation ability would benefit large languag
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ORAL&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#19990;&#30028;&#22270;&#23398;&#20064;&#20013;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#21407;&#22411;&#23398;&#20064;&#21644;&#21407;&#22411;&#27880;&#24847;&#21147;&#32593;&#32476;&#35299;&#20915;&#20102;&#26032;&#31867;&#21035;&#21644;&#24050;&#30693;&#31867;&#21035;&#33410;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19907</link><description>&lt;p&gt;
&#36229;&#36234;&#24050;&#30693;&#65306;&#24320;&#25918;&#19990;&#30028;&#22270;&#23398;&#20064;&#20013;&#30340;&#26032;&#31867;&#21035;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Beyond the Known: Novel Class Discovery for Open-world Graph Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19907
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ORAL&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#24320;&#25918;&#19990;&#30028;&#22270;&#23398;&#20064;&#20013;&#21457;&#29616;&#26032;&#31867;&#21035;&#65292;&#36890;&#36807;&#21322;&#30417;&#30563;&#21407;&#22411;&#23398;&#20064;&#21644;&#21407;&#22411;&#27880;&#24847;&#21147;&#32593;&#32476;&#35299;&#20915;&#20102;&#26032;&#31867;&#21035;&#21644;&#24050;&#30693;&#31867;&#21035;&#33410;&#28857;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#19978;&#30340;&#33410;&#28857;&#20998;&#31867;&#22312;&#35768;&#22810;&#24212;&#29992;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#24320;&#25918;&#22330;&#26223;&#20013;&#26631;&#35760;&#33021;&#21147;&#26377;&#38480;&#19988;&#21457;&#23637;&#36805;&#36895;&#65292;&#26410;&#26631;&#35760;&#27979;&#35797;&#33410;&#28857;&#19978;&#21487;&#33021;&#20250;&#20986;&#29616;&#26032;&#31867;&#21035;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#22270;&#19978;&#30340;&#26032;&#31867;&#21035;&#21457;&#29616;&#20851;&#27880;&#36739;&#23569;&#12290;&#30001;&#20110;&#22270;&#20013;&#30340;&#26032;&#31867;&#21035;&#21644;&#24050;&#30693;&#31867;&#21035;&#33410;&#28857;&#30001;&#36793;&#30456;&#20851;&#32852;&#65292;&#22240;&#27492;&#24403;&#24212;&#29992;&#28040;&#24687;&#20256;&#36882;GNN&#26102;&#23427;&#20204;&#30340;&#34920;&#31034;&#26159;&#19981;&#21487;&#21306;&#20998;&#30340;&#12290;&#27492;&#22806;&#65292;&#26032;&#31867;&#21035;&#32570;&#20047;&#26631;&#31614;&#20449;&#24687;&#26469;&#25351;&#23548;&#23398;&#20064;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;Open-world gRAph neuraL network (ORAL)&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;ORAL&#39318;&#20808;&#36890;&#36807;&#21322;&#30417;&#30563;&#21407;&#22411;&#23398;&#20064;&#26816;&#27979;&#31867;&#21035;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#36890;&#36807;&#21407;&#22411;&#27880;&#24847;&#21147;&#32593;&#32476;&#28040;&#38500;&#31867;&#38388;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#20026;&#19981;&#21516;&#31867;&#21035;&#25552;&#20379;&#29420;&#29305;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20805;&#20998;&#25506;&#32034;mu
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19907v1 Announce Type: cross  Abstract: Node classification on graphs is of great importance in many applications. Due to the limited labeling capability and evolution in real-world open scenarios, novel classes can emerge on unlabeled testing nodes. However, little attention has been paid to novel class discovery on graphs. Discovering novel classes is challenging as novel and known class nodes are correlated by edges, which makes their representations indistinguishable when applying message passing GNNs. Furthermore, the novel classes lack labeling information to guide the learning process. In this paper, we propose a novel method Open-world gRAph neuraL network (ORAL) to tackle these challenges. ORAL first detects correlations between classes through semi-supervised prototypical learning. Inter-class correlations are subsequently eliminated by the prototypical attention network, leading to distinctive representations for different classes. Furthermore, to fully explore mu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#36827;&#34892;&#20998;&#31867;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#31995;&#32479;&#65292;&#36890;&#36807;&#24494;&#35843;&#25216;&#26415;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#33258;&#21160;&#20998;&#31867;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19905</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of Diabetic Retinopathy using Pre-Trained Deep Learning Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23545;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#36827;&#34892;&#20998;&#31867;&#30340;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#31995;&#32479;&#65292;&#36890;&#36807;&#24494;&#35843;&#25216;&#26415;&#35757;&#32451;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#33258;&#21160;&#20998;&#31867;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#34920;&#26126;&#65292;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65288;DR&#65289;&#26159;&#20840;&#29699;&#23548;&#33268;&#22833;&#26126;&#30340;&#20027;&#35201;&#21407;&#22240;&#65292;&#23588;&#20854;&#24433;&#21709;20-70&#23681;&#20043;&#38388;&#30340;&#20010;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35745;&#31639;&#26426;&#36741;&#21161;&#35786;&#26029;&#65288;CAD&#65289;&#31995;&#32479;&#65292;&#29992;&#20110;&#33258;&#21160;&#23558;&#35270;&#32593;&#33180;&#22270;&#20687;&#20998;&#31867;&#20026;&#20116;&#20010;&#19981;&#21516;&#31867;&#21035;&#65306;&#27491;&#24120;&#12289;&#36731;&#24230;&#12289;&#20013;&#24230;&#12289;&#37325;&#24230;&#21644;&#22686;&#27542;&#24615;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65288;PDR&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#32467;&#21512;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#36890;&#36807;&#24494;&#35843;&#25216;&#26415;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20998;&#36776;&#29575;&#20026;350x350x3&#21644;224x224x3&#30340;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#22270;&#20687;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#22312;Kaggle&#24179;&#21488;&#19978;&#21033;&#29992;4&#20010;CPU&#12289;17 GB RAM&#21644;1 GB&#30828;&#30424;&#36164;&#28304;&#33719;&#24471;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;CNN&#12289;MobileNet&#12289;VGG-16&#12289;InceptionV3&#21644;InceptionResNetV2&#27169;&#22411;&#30340;&#36798;&#21040;&#30340;&#26354;&#32447;&#19979;&#38754;&#31215;&#65288;AUC&#65289;&#20540;&#20998;&#21035;&#20026;0.50&#12289;0.70&#12289;0.53&#12289;0.63&#21644;0&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19905v1 Announce Type: cross  Abstract: Diabetic Retinopathy (DR) stands as the leading cause of blindness globally, particularly affecting individuals between the ages of 20 and 70. This paper presents a Computer-Aided Diagnosis (CAD) system designed for the automatic classification of retinal images into five distinct classes: Normal, Mild, Moderate, Severe, and Proliferative Diabetic Retinopathy (PDR). The proposed system leverages Convolutional Neural Networks (CNNs) employing pre-trained deep learning models. Through the application of fine-tuning techniques, our model is trained on fundus images of diabetic retinopathy with resolutions of 350x350x3 and 224x224x3. Experimental results obtained on the Kaggle platform, utilizing resources comprising 4 CPUs, 17 GB RAM, and 1 GB Disk, demonstrate the efficacy of our approach. The achieved Area Under the Curve (AUC) values for CNN, MobileNet, VGG-16, InceptionV3, and InceptionResNetV2 models are 0.50, 0.70, 0.53, 0.63, and 0
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;-&#22522;&#30784;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#20581;&#22766;&#24615;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#20010;&#20840;&#38754;&#30340;&#31995;&#32479;&#26469;&#22686;&#24378;&#27169;&#22411;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#30340;&#20581;&#22766;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19889</link><description>&lt;p&gt;
&#26397;&#21521;&#19968;&#20010;&#24378;&#22823;&#30340;&#22522;&#20110;&#26816;&#32034;&#30340;&#25688;&#35201;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Towards a Robust Retrieval-Based Summarization System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19889
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;-&#22522;&#30784;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#20581;&#22766;&#24615;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#21644;&#19968;&#20010;&#20840;&#38754;&#30340;&#31995;&#32479;&#26469;&#22686;&#24378;&#27169;&#22411;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#30340;&#20581;&#22766;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25551;&#36848;&#20102;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;-&#22522;&#30784;&#25688;&#35201;&#20219;&#21153;&#20013;&#30340;&#20581;&#22766;&#24615;&#36827;&#34892;&#30340;&#35843;&#26597;&#12290;&#34429;&#28982;LLMs&#25552;&#20379;&#20102;&#25688;&#35201;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#22797;&#26434;&#30340;&#23454;&#38469;&#22330;&#26223;&#20013;&#30340;&#34920;&#29616;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#36129;&#29486;&#26159;LogicSumm&#65292;&#36825;&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#29616;&#23454;&#22330;&#26223;&#65292;&#29992;&#26469;&#35780;&#20272;LLMs&#22312;RAG&#22522;&#30784;&#25688;&#35201;&#36807;&#31243;&#20013;&#30340;&#20581;&#22766;&#24615;&#12290;&#26681;&#25454;LogicSumm&#35782;&#21035;&#20986;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;SummRAG&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#21019;&#24314;&#35757;&#32451;&#23545;&#35805;&#24182;&#24494;&#35843;&#27169;&#22411;&#65292;&#20197;&#22686;&#24378;&#22312;LogicSumm&#22330;&#26223;&#20013;&#30340;&#20581;&#22766;&#24615;&#12290;SummRAG&#26159;&#25105;&#20204;&#23450;&#20041;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#27979;&#35797;LLM&#33021;&#21147;&#30340;&#30446;&#26631;&#30340;&#19968;&#20010;&#31034;&#20363;&#65292;&#32780;&#19981;&#26159;&#19968;&#21171;&#27704;&#36920;&#22320;&#35299;&#20915;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;SummRAG&#30340;&#24378;&#22823;&#65292;&#23637;&#31034;&#20102;&#36923;&#36753;&#36830;&#36143;&#24615;&#21644;&#25688;&#35201;&#36136;&#37327;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19889v1 Announce Type: cross  Abstract: This paper describes an investigation of the robustness of large language models (LLMs) for retrieval augmented generation (RAG)-based summarization tasks. While LLMs provide summarization capabilities, their performance in complex, real-world scenarios remains under-explored. Our first contribution is LogicSumm, an innovative evaluation framework incorporating realistic scenarios to assess LLM robustness during RAG-based summarization. Based on limitations identified by LogiSumm, we then developed SummRAG, a comprehensive system to create training dialogues and fine-tune a model to enhance robustness within LogicSumm's scenarios. SummRAG is an example of our goal of defining structured methods to test the capabilities of an LLM, rather than addressing issues in a one-off fashion. Experimental results confirm the power of SummRAG, showcasing improved logical coherence and summarization quality. Data, corresponding model weights, and Py
&lt;/p&gt;</description></item><item><title>MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.19888</link><description>&lt;p&gt;
MambaMixer&#65306;&#20855;&#26377;&#21452;&#37325;&#26631;&#35760;&#21644;&#36890;&#36947;&#36873;&#25321;&#30340;&#39640;&#25928;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19888
&lt;/p&gt;
&lt;p&gt;
MambaMixer&#26159;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#23545;&#38271;&#24207;&#21015;&#24314;&#27169;&#20855;&#26377;&#28508;&#22312;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#20027;&#35201;&#20381;&#36182;&#20110;Transformers&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#24615;&#24182;&#19988;&#33021;&#22815;&#23454;&#29616;&#22823;&#35268;&#27169;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26550;&#26500;&#20013;&#30340;&#27880;&#24847;&#21147;&#27169;&#22359;&#23637;&#29616;&#20986;&#36755;&#20837;&#22823;&#23567;&#30340;&#20108;&#27425;&#26102;&#38388;&#21644;&#31354;&#38388;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#23613;&#31649;&#26368;&#36817;&#26377;&#23581;&#35797;&#20026;&#22810;&#32500;&#25968;&#25454;&#35774;&#35745;&#39640;&#25928;&#26377;&#25928;&#30340;&#26550;&#26500;&#20027;&#24178;&#65292;&#20363;&#22914;&#22270;&#20687;&#21644;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#20294;&#29616;&#26377;&#27169;&#22411;&#35201;&#20040;&#26159;&#25968;&#25454;&#29420;&#31435;&#30340;&#65292;&#35201;&#20040;&#26080;&#27861;&#20801;&#35768;&#36328;&#32500;&#24230;&#21644;&#20869;&#37096;&#32500;&#24230;&#20043;&#38388;&#30340;&#36890;&#20449;&#12290;&#26368;&#36817;&#65292;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSMs&#65289;&#65292;&#23588;&#20854;&#26159;&#20855;&#26377;&#39640;&#25928;&#30828;&#20214;&#24863;&#30693;&#23454;&#29616;&#30340;&#36873;&#25321;&#24615;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#23637;&#29616;&#20986;&#20102;&#29992;&#20110;&#38271;&#24207;&#21015;&#24314;&#27169;&#30340;&#28508;&#22312;&#20248;&#21183;&#12290;&#21463;&#21040;SSMs&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MambaMixer&#65292;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#25968;&#25454;&#20381;&#36182;&#26435;&#37325;&#30340;&#26550;&#26500;&#65292;&#20351;&#29992;&#36328;&#26631;&#35760;&#21644;&#36890;&#36947;&#30340;&#21452;&#37325;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;&#36873;&#25321;&#24615;&#26631;&#35760;&#21644;&#36890;&#36947;&#28151;&#21512;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19888v1 Announce Type: cross  Abstract: Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. M
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#24182;&#25913;&#36827;&#20102;AND*&#25191;&#34892;&#30340;&#25919;&#31574;&#31354;&#38388;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#25919;&#31574;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#25919;&#31574;&#31561;&#20215;&#24615;&#20462;&#21098;&#25919;&#31574;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#20351;AND*&#22312;&#35299;&#20915;FOND&#20219;&#21153;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.19883</link><description>&lt;p&gt;
&#25919;&#31574;&#31354;&#38388;&#25628;&#32034;: &#31561;&#20215;&#24615;&#12289;&#25913;&#36827;&#21644;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Policy-Space Search: Equivalences, Improvements, and Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19883
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#24182;&#25913;&#36827;&#20102;AND*&#25191;&#34892;&#30340;&#25919;&#31574;&#31354;&#38388;&#25628;&#32034;&#30340;&#24615;&#33021;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#25919;&#31574;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#27010;&#24565;&#65292;&#24182;&#21033;&#29992;&#25919;&#31574;&#31561;&#20215;&#24615;&#20462;&#21098;&#25919;&#31574;&#25628;&#32034;&#31354;&#38388;&#65292;&#20174;&#32780;&#20351;AND*&#22312;&#35299;&#20915;FOND&#20219;&#21153;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#20840;&#21487;&#35266;&#23519;&#30340;&#38750;&#30830;&#23450;&#24615;&#65288;FOND&#65289;&#35268;&#21010;&#26159;&#20154;&#24037;&#26234;&#33021;&#35745;&#21010;&#20013;&#19981;&#30830;&#23450;&#24615;&#30340;&#26680;&#24515;&#12290;&#23427;&#36890;&#36807;&#20855;&#26377;&#38750;&#30830;&#23450;&#24615;&#25928;&#26524;&#30340;&#21160;&#20316;&#26469;&#24314;&#27169;&#19981;&#30830;&#23450;&#24615;&#12290;AND*&#65288;Messa&#21644;Pereira&#65292;2023&#65289;&#26159;&#19968;&#20010;&#27867;&#21270;&#20102;A* (Hart&#31561;&#20154;&#65292;1968) &#29992;&#20110;FOND&#35268;&#21010;&#30340;FOND&#35268;&#21010;&#22120;&#12290; &#26412;&#25991;&#30740;&#31350;&#24182;&#25913;&#36827;&#20102;AND*&#25191;&#34892;&#30340;&#25919;&#31574;&#31354;&#38388;&#25628;&#32034;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#36807;&#31243;&#65292;&#20165;&#32473;&#23450;&#24212;&#26144;&#23556;&#30340;&#29366;&#24577;&#38598;&#21363;&#21487;&#26500;&#36896;&#20986;&#35299;&#20915;&#26041;&#26696;&#25919;&#31574;&#12290; &#36825;&#20010;&#36807;&#31243;&#65292;&#19982;&#23545;FOND&#25919;&#31574;&#32467;&#26500;&#30340;&#26356;&#22909;&#29702;&#35299;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#19977;&#20010;&#25919;&#31574;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#27010;&#24565;&#12290; &#25105;&#20204;&#20351;&#29992;&#25919;&#31574;&#31561;&#20215;&#24615;&#26469;&#20462;&#21098;&#25919;&#31574;&#25628;&#32034;&#31354;&#38388;&#30340;&#19968;&#37096;&#20998;&#65292;&#20351;AND*&#22312;&#35299;&#20915;FOND&#20219;&#21153;&#26102;&#26356;&#21152;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19883v1 Announce Type: new  Abstract: Fully-observable non-deterministic (FOND) planning is at the core of artificial intelligence planning with uncertainty. It models uncertainty through actions with non-deterministic effects. A* with Non-Determinism (AND*) (Messa and Pereira, 2023) is a FOND planner that generalizes A* (Hart et al., 1968) for FOND planning. It searches for a solution policy by performing an explicit heuristic search on the policy space of the FOND task. In this paper, we study and improve the performance of the policy-space search performed by AND*. We present a polynomial-time procedure that constructs a solution policy given just the set of states that should be mapped. This procedure, together with a better understanding of the structure of FOND policies, allows us to present three concepts of equivalences between policies. We use policy equivalences to prune part of the policy search space, making AND* substantially more effective in solving FOND tasks
&lt;/p&gt;</description></item><item><title>IME&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#23436;&#25104;&#20219;&#21153;&#65292;&#23558;&#30693;&#35782;&#22270;&#24314;&#27169;&#20026;&#22810;&#26354;&#29575;&#31354;&#38388;&#65292;&#21253;&#25324;&#36229;&#29699;&#38754;&#12289;&#21452;&#26354;&#32447;&#21644;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65292;&#24182;&#34701;&#21512;&#31354;&#38388;&#20849;&#20139;&#23646;&#24615;&#21644;&#31354;&#38388;&#29305;&#23450;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.19881</link><description>&lt;p&gt;
IME&#65306;&#38598;&#25104;&#22810;&#26354;&#29575;&#20849;&#20139;&#21644;&#29305;&#23450;&#23884;&#20837;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#23436;&#25104;
&lt;/p&gt;
&lt;p&gt;
IME: Integrating Multi-curvature Shared and Specific Embedding for Temporal Knowledge Graph Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19881
&lt;/p&gt;
&lt;p&gt;
IME&#27169;&#22411;&#29992;&#20110;&#26102;&#38388;&#30693;&#35782;&#22270;&#23436;&#25104;&#20219;&#21153;&#65292;&#23558;&#30693;&#35782;&#22270;&#24314;&#27169;&#20026;&#22810;&#26354;&#29575;&#31354;&#38388;&#65292;&#21253;&#25324;&#36229;&#29699;&#38754;&#12289;&#21452;&#26354;&#32447;&#21644;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#65292;&#24182;&#34701;&#21512;&#31354;&#38388;&#20849;&#20139;&#23646;&#24615;&#21644;&#31354;&#38388;&#29305;&#23450;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#30693;&#35782;&#22270;&#65288;TKG&#65289;&#34701;&#21512;&#20102;&#26102;&#38388;&#32500;&#24230;&#65292;&#20801;&#35768;&#31934;&#30830;&#25429;&#25417;&#30693;&#35782;&#30340;&#28436;&#21270;&#24182;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#36890;&#24120;&#65292;TKG&#21253;&#21547;&#22797;&#26434;&#30340;&#20960;&#20309;&#32467;&#26500;&#65292;&#21508;&#31181;&#20960;&#20309;&#32467;&#26500;&#20132;&#32455;&#22312;&#19968;&#36215;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#23436;&#25104;&#65288;TKGC&#65289;&#26041;&#27861;&#35201;&#20040;&#23558;TKG&#24314;&#27169;&#22312;&#19968;&#20010;&#31354;&#38388;&#20013;&#65292;&#35201;&#20040;&#24573;&#30053;&#19981;&#21516;&#26354;&#29575;&#31354;&#38388;&#30340;&#24322;&#36136;&#24615;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#25429;&#25417;&#36825;&#20123;&#22797;&#26434;&#20960;&#20309;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38598;&#25104;&#22810;&#26354;&#29575;&#20849;&#20139;&#21644;&#29305;&#23450;&#23884;&#20837;&#65288;IME&#65289;&#27169;&#22411;&#29992;&#20110;TKGC&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;IME&#23558;TKG&#24314;&#27169;&#20026;&#22810;&#26354;&#29575;&#31354;&#38388;&#65292;&#21253;&#25324;&#36229;&#29699;&#38754;&#12289;&#21452;&#26354;&#32447;&#21644;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#12290;&#38543;&#21518;&#65292;IME&#34701;&#21512;&#20102;&#20004;&#20010;&#20851;&#38190;&#23646;&#24615;&#65292;&#21363;&#31354;&#38388;&#20849;&#20139;&#23646;&#24615;&#21644;&#31354;&#38388;&#29305;&#23450;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19881v1 Announce Type: new  Abstract: Temporal Knowledge Graphs (TKGs) incorporate a temporal dimension, allowing for a precise capture of the evolution of knowledge and reflecting the dynamic nature of the real world. Typically, TKGs contain complex geometric structures, with various geometric structures interwoven. However, existing Temporal Knowledge Graph Completion (TKGC) methods either model TKGs in a single space or neglect the heterogeneity of different curvature spaces, thus constraining their capacity to capture these intricate geometric structures. In this paper, we propose a novel Integrating Multi-curvature shared and specific Embedding (IME) model for TKGC tasks. Concretely, IME models TKGs into multi-curvature spaces, including hyperspherical, hyperbolic, and Euclidean spaces. Subsequently, IME incorporates two key properties, namely space-shared property and space-specific property. The space-shared property facilitates the learning of commonalities across di
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;</title><link>https://arxiv.org/abs/2403.19871</link><description>&lt;p&gt;
&#36890;&#36807;&#32531;&#24930;&#21464;&#21270;&#30340;&#24207;&#21015;&#23454;&#29616;&#31283;&#23450;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Towards Stable Machine Learning Model Retraining via Slowly Varying Sequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19871
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#20445;&#25345;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265;&#20026;&#37325;&#28857;&#65292;&#22312;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#23454;&#29616;&#27604;&#36138;&#23146;&#35757;&#32451;&#26356;&#24378;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#26159;&#23454;&#38469;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#37096;&#32626;&#30340;&#37325;&#35201;&#20219;&#21153;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#36138;&#23146;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#32780;&#19981;&#32771;&#34385;&#36890;&#36807;&#19981;&#21516;&#30340;&#37325;&#26032;&#35757;&#32451;&#28436;&#21464;&#26469;&#20445;&#25345;&#35757;&#32451;&#27169;&#22411;&#32467;&#26500;&#30340;&#31283;&#23450;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#28151;&#21512;&#25972;&#25968;&#20248;&#21270;&#31639;&#27861;&#65292;&#20840;&#38754;&#32771;&#34385;&#20102;&#36890;&#36807;&#19981;&#21516;&#30340;&#25968;&#25454;&#25209;&#27425;&#26356;&#26032;&#37325;&#26032;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20391;&#37325;&#20110;&#20445;&#30041;&#19968;&#33268;&#30340;&#20998;&#26512;&#27934;&#35265; - &#36825;&#23545;&#20110;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#12289;&#23454;&#26045;&#31616;&#26131;&#24615;&#21644;&#19982;&#29992;&#25143;&#24314;&#31435;&#20449;&#20219;&#33267;&#20851;&#37325;&#35201; - &#36890;&#36807;&#20351;&#29992;&#21487;&#20197;&#30452;&#25509;&#32435;&#20837;&#20248;&#21270;&#38382;&#39064;&#30340;&#33258;&#23450;&#20041;&#23450;&#20041;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#30495;&#23454;&#30340;&#29983;&#20135;&#26696;&#20363;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#27604;&#36138;&#23146;&#35757;&#32451;&#27169;&#22411;&#26356;&#24378;&#30340;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#22312;&#27169;&#22411;&#24615;&#33021;&#19978;&#26377;&#23567;&#24133;&#12289;&#21487;&#25511;&#30340;&#29306;&#29298;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19871v1 Announce Type: cross  Abstract: Retraining machine learning models remains an important task for real-world machine learning model deployment. Existing methods focus largely on greedy approaches to find the best-performing model without considering the stability of trained model structures across different retraining evolutions. In this study, we develop a mixed integer optimization algorithm that holistically considers the problem of retraining machine learning models across different data batch updates. Our method focuses on retaining consistent analytical insights - which is important to model interpretability, ease of implementation, and fostering trust with users - by using custom-defined distance metrics that can be directly incorporated into the optimization problem. Importantly, our method shows stronger stability than greedily trained models with a small, controllable sacrifice in model performance in a real-world production case study. Finally, important an
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#27969;&#23398;&#20064;&#20013;&#35745;&#31639;&#20915;&#31574;&#26641;&#26368;&#20339;&#20998;&#21106;&#28857;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27969;&#24335;&#35745;&#31639;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#39640;&#25928;&#36816;&#34892;</title><link>https://arxiv.org/abs/2403.19867</link><description>&lt;p&gt;
&#22312;&#27969;&#24335;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#25214;&#21040;&#20915;&#31574;&#26641;&#20998;&#21106;&#28857;
&lt;/p&gt;
&lt;p&gt;
Finding Decision Tree Splits in Streaming and Massively Parallel Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19867
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22312;&#25968;&#25454;&#27969;&#23398;&#20064;&#20013;&#35745;&#31639;&#20915;&#31574;&#26641;&#26368;&#20339;&#20998;&#21106;&#28857;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#27969;&#24335;&#35745;&#31639;&#21644;&#22823;&#35268;&#27169;&#24182;&#34892;&#27169;&#22411;&#20013;&#39640;&#25928;&#36816;&#34892;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#27969;&#31639;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20915;&#31574;&#26641;&#23398;&#20064;&#20013;&#30340;&#26368;&#20248;&#20998;&#21106;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#35266;&#27979;&#25968;&#25454;&#27969;$x_i$&#21450;&#20854;&#26631;&#31614;$y_i$&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#23558;&#25968;&#25454;&#20998;&#20026;&#20004;&#32452;&#30340;&#26368;&#20339;&#20998;&#21106;&#28857;$j$&#65292;&#20351;&#24471;&#22343;&#26041;&#35823;&#24046;&#65288;&#22238;&#24402;&#38382;&#39064;&#65289;&#25110;&#35823;&#20998;&#31867;&#29575;&#65288;&#20998;&#31867;&#38382;&#39064;&#65289;&#26368;&#23567;&#21270;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22810;&#31181;&#24555;&#36895;&#30340;&#25968;&#25454;&#27969;&#31639;&#27861;&#65292;&#36825;&#20123;&#31639;&#27861;&#22312;&#36825;&#20123;&#38382;&#39064;&#20013;&#20351;&#29992;&#20122;&#32447;&#24615;&#31354;&#38388;&#21644;&#23569;&#37327;&#27425;&#25968;&#30340;&#36941;&#21382;&#12290;&#36825;&#20123;&#31639;&#27861;&#36824;&#21487;&#20197;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#24182;&#34892;&#35745;&#31639;&#27169;&#22411;&#20013;&#12290;&#23613;&#31649;&#19981;&#33021;&#30452;&#25509;&#27604;&#36739;&#65292;&#20294;&#25105;&#20204;&#30340;&#24037;&#20316;&#19982;Domingos&#21644;Hulten&#30340;&#24320;&#21019;&#24615;&#24037;&#20316;&#65288;KDD 2000&#65289;&#30456;&#20114;&#34917;&#20805;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19867v1 Announce Type: cross  Abstract: In this work, we provide data stream algorithms that compute optimal splits in decision tree learning. In particular, given a data stream of observations $x_i$ and their labels $y_i$, the goal is to find the optimal split point $j$ that divides the data into two sets such that the mean squared error (for regression) or misclassification rate (for classification) is minimized. We provide various fast streaming algorithms that use sublinear space and a small number of passes for these problems. These algorithms can also be extended to the massively parallel computation model. Our work, while not directly comparable, complements the seminal work of Domingos and Hulten (KDD 2000).
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#29983;&#25104;&#21644;&#21033;&#29992;&#21512;&#25104;&#22270;&#20687;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#26725;&#25509;&#36801;&#31227;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#20197;&#35299;&#20915;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.19866</link><description>&lt;p&gt;
&#21512;&#25104;&#22270;&#20687;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#26377;&#29992;&#21527;&#65311;&#20851;&#20110;&#25968;&#25454;&#29983;&#25104;&#12289;&#25968;&#37327;&#21644;&#21033;&#29992;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Is Synthetic Image Useful for Transfer Learning? An Investigation into Data Generation, Volume, and Utilization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19866
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#29983;&#25104;&#21644;&#21033;&#29992;&#21512;&#25104;&#22270;&#20687;&#22312;&#36801;&#31227;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#65292;&#25552;&#20986;&#20102;&#26725;&#25509;&#36801;&#31227;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#20197;&#35299;&#20915;&#21512;&#25104;&#22270;&#20687;&#19982;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#20998;&#24067;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#22270;&#20687;&#25968;&#25454;&#29983;&#25104;&#20195;&#34920;&#20102;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#36884;&#24452;&#65292;&#29305;&#21035;&#26159;&#22312;&#36801;&#31227;&#23398;&#20064;&#39046;&#22495;&#65292;&#22240;&#20026;&#22312;&#29305;&#23450;&#39046;&#22495;&#33719;&#21462;&#30495;&#23454;&#22270;&#20687;&#21487;&#33021;&#20250;&#30001;&#20110;&#38544;&#31169;&#21644;&#30693;&#35782;&#20135;&#26435;&#32771;&#34385;&#32780;&#21464;&#24471;&#20195;&#20215;&#39640;&#26114;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#20174;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#20013;&#33719;&#21462;&#30340;&#21512;&#25104;&#22270;&#20687;&#30340;&#29983;&#25104;&#21644;&#21033;&#29992;&#65292;&#20197;&#20419;&#36827;&#36801;&#31227;&#23398;&#20064;&#33539;&#24335;&#12290;&#23613;&#31649;&#29983;&#25104;&#30340;&#22270;&#20687;&#20855;&#26377;&#39640;&#24230;&#30340;&#35270;&#35273;&#36924;&#30495;&#24230;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#23558;&#36825;&#20123;&#22270;&#20687;&#22825;&#30495;&#22320;&#32435;&#20837;&#29616;&#26377;&#30340;&#30495;&#23454;&#22270;&#20687;&#25968;&#25454;&#38598;&#24182;&#19981;&#33021;&#22987;&#32456;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#65292;&#36825;&#26159;&#30001;&#20110;&#21512;&#25104;&#22270;&#20687;&#21644;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#22266;&#26377;&#30340;&#20998;&#24067;&#24046;&#24322;&#25152;&#33268;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#26725;&#25509;&#36801;&#31227;&#8221;&#30340;&#26032;&#39062;&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#39318;&#20808;&#21033;&#29992;&#21512;&#25104;&#22270;&#20687;&#23545;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#20197;&#25552;&#39640;&#20854;&#21487;&#36801;&#31227;&#24615;&#65292;&#38543;&#21518;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#36827;&#34892;&#24555;&#36895;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19866v1 Announce Type: cross  Abstract: Synthetic image data generation represents a promising avenue for training deep learning models, particularly in the realm of transfer learning, where obtaining real images within a specific domain can be prohibitively expensive due to privacy and intellectual property considerations. This work delves into the generation and utilization of synthetic images derived from text-to-image generative models in facilitating transfer learning paradigms. Despite the high visual fidelity of the generated images, we observe that their naive incorporation into existing real-image datasets does not consistently enhance model performance due to the inherent distribution gap between synthetic and real images. To address this issue, we introduce a novel two-stage framework called bridged transfer, which initially employs synthetic images for fine-tuning a pre-trained model to improve its transferability and subsequently uses real data for rapid adaptat
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#19990;&#30028;&#30693;&#35782;&#26469;&#35782;&#21035;&#38271;&#26399;&#26102;&#31354;&#20256;&#24863;&#22120;&#36712;&#36857;&#20013;&#30340;&#22797;&#26434;&#20107;&#20214;&#12290;</title><link>https://arxiv.org/abs/2403.19857</link><description>&lt;p&gt;
LLMSense&#65306;&#21033;&#29992;LLMs&#36827;&#34892;&#26102;&#31354;&#20256;&#24863;&#22120;&#36712;&#36857;&#30340;&#39640;&#32423;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
LLMSense: Harnessing LLMs for High-level Reasoning Over Spatiotemporal Sensor Traces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#19990;&#30028;&#30693;&#35782;&#26469;&#35782;&#21035;&#38271;&#26399;&#26102;&#31354;&#20256;&#24863;&#22120;&#36712;&#36857;&#20013;&#30340;&#22797;&#26434;&#20107;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#20851;&#20110;&#20256;&#24863;&#31995;&#32479;&#20013;&#26426;&#22120;&#23398;&#20064;&#30340;&#30740;&#31350;&#37117;&#38598;&#20013;&#22312;&#22788;&#29702;&#30701;&#26102;&#38388;&#31383;&#20869;&#30340;&#21407;&#22987;&#20256;&#24863;&#25968;&#25454;&#30340;&#20302;&#23618;&#24863;&#30693;&#20219;&#21153;&#19978;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#65292;&#22914;&#20154;&#31867;&#20363;&#34892;&#24314;&#27169;&#21644;&#21344;&#29992;&#36319;&#36394;&#65292;&#38656;&#35201;&#39640;&#32423;&#25512;&#29702;&#33021;&#21147;&#26469;&#29702;&#35299;&#27010;&#24565;&#65292;&#24182;&#26681;&#25454;&#38271;&#26399;&#20256;&#24863;&#22120;&#36712;&#36857;&#20570;&#20986;&#25512;&#26029;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#22788;&#29702;&#36825;&#20123;&#22797;&#26434;&#20219;&#21153;&#30340;&#26041;&#27861;&#30001;&#20110;&#35757;&#32451;&#26679;&#26412;&#26377;&#38480;&#21644;&#20256;&#24863;&#22120;&#36712;&#36857;&#30340;&#39640;&#32500;&#29305;&#24615;&#32780;&#38590;&#20197;&#27867;&#21270;&#65292;&#38656;&#35201;&#34701;&#21512;&#20154;&#31867;&#30693;&#35782;&#26469;&#35774;&#35745;&#22522;&#26412;&#27169;&#22411;&#25110;&#36923;&#36753;&#25512;&#29702;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;&#25105;&#20204;&#33021;&#21542;&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#19990;&#30028;&#30693;&#35782;&#26469;&#35782;&#21035;&#38271;&#26399;&#26102;&#31354;&#20256;&#24863;&#22120;&#36712;&#36857;&#20013;&#30340;&#22797;&#26434;&#20107;&#20214;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#39640;&#32423;&#25512;&#29702;&#20219;&#21153;&#19978;LLMs&#30340;&#26377;&#25928;&#25552;&#31034;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19857v1 Announce Type: new  Abstract: Most studies on machine learning in sensing systems focus on low-level perception tasks that process raw sensory data within a short time window. However, many practical applications, such as human routine modeling and occupancy tracking, require high-level reasoning abilities to comprehend concepts and make inferences based on long-term sensor traces. Existing machine learning-based approaches for handling such complex tasks struggle to generalize due to the limited training samples and the high dimensionality of sensor traces, necessitating the integration of human knowledge for designing first-principle models or logic reasoning methods. We pose a fundamental question: Can we harness the reasoning capabilities and world knowledge of Large Language Models (LLMs) to recognize complex events from long-term spatiotemporal sensor traces? To answer this question, we design an effective prompting framework for LLMs on high-level reasoning ta
&lt;/p&gt;</description></item><item><title>&#26500;&#24314;&#22522;&#20110;&#24052;&#35199;&#21382;&#21490;&#35789;&#20856;&#21644;&#32500;&#22522;&#25968;&#25454;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#20016;&#23500;&#33889;&#33796;&#29273;&#25991;&#26412;&#20449;&#24687;&#25552;&#21462;&#65292;&#22635;&#34917;&#24052;&#35199;&#21629;&#21517;&#23454;&#20307;&#22312;&#32500;&#22522;&#25968;&#25454;&#20013;&#32570;&#22833;&#30340;&#27010;&#24565;&#12290;</title><link>https://arxiv.org/abs/2403.19856</link><description>&lt;p&gt;
&#36808;&#21521;&#24052;&#35199;&#21382;&#21490;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Towards a Brazilian History Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19856
&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#22522;&#20110;&#24052;&#35199;&#21382;&#21490;&#35789;&#20856;&#21644;&#32500;&#22522;&#25968;&#25454;&#30340;&#30693;&#35782;&#22270;&#35889;&#65292;&#20197;&#20016;&#23500;&#33889;&#33796;&#29273;&#25991;&#26412;&#20449;&#24687;&#25552;&#21462;&#65292;&#22635;&#34917;&#24052;&#35199;&#21629;&#21517;&#23454;&#20307;&#22312;&#32500;&#22522;&#25968;&#25454;&#20013;&#32570;&#22833;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#31616;&#30701;&#30340;&#35770;&#25991;&#25551;&#36848;&#20102;&#22312;&#22522;&#20110;&#24052;&#35199;&#21382;&#21490;&#35789;&#20856;&#65288;DHBB&#65289;&#21644;&#32500;&#22522;&#30334;&#31185;/&#32500;&#22522;&#25968;&#25454;&#26500;&#24314;&#24052;&#35199;&#21382;&#21490;&#30693;&#35782;&#22270;&#35889;&#39033;&#30446;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24052;&#35199;&#21629;&#21517;&#23454;&#20307;&#65288;&#20154;&#29289;&#12289;&#22320;&#28857;&#12289;&#32452;&#32455;&#12289;&#25919;&#27835;&#20107;&#20214;&#21644;&#36816;&#21160;&#65289;&#30340;&#22823;&#22411;&#23384;&#20648;&#24211;&#23545;&#20174;&#33889;&#33796;&#29273;&#25991;&#26412;&#20013;&#25552;&#21462;&#20449;&#24687;&#23558;&#26159;&#26377;&#30410;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DHBB&#20013;&#25551;&#36848;&#30340;&#35768;&#22810;&#26415;&#35821;/&#23454;&#20307;&#22312;&#32500;&#22522;&#25968;&#25454;&#20013;&#27809;&#26377;&#30456;&#24212;&#30340;&#27010;&#24565;&#65288;&#25110;Q&#39033;&#65289;&#65292;&#21518;&#32773;&#26159;&#19982;&#32500;&#22522;&#30334;&#31185;&#30456;&#20851;&#30340;&#26368;&#22823;&#32467;&#26500;&#21270;&#23454;&#20307;&#25968;&#25454;&#24211;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20174;DHBB&#20013;&#25552;&#21462;&#20449;&#24687;&#30340;&#20808;&#21069;&#24037;&#20316;&#65292;&#24182;&#27010;&#36848;&#20102;&#26500;&#24314;&#22522;&#20110;&#32500;&#22522;&#25968;&#25454;&#30340;&#21382;&#21490;&#30693;&#35782;&#22270;&#35889;&#30340;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19856v1 Announce Type: new  Abstract: This short paper describes the first steps in a project to construct a knowledge graph for Brazilian history based on the Brazilian Dictionary of Historical Biographies (DHBB) and Wikipedia/Wikidata. We contend that large repositories of Brazilian-named entities (people, places, organizations, and political events and movements) would be beneficial for extracting information from Portuguese texts. We show that many of the terms/entities described in the DHBB do not have corresponding concepts (or Q items) in Wikidata, the largest structured database of entities associated with Wikipedia. We describe previous work on extracting information from the DHBB and outline the steps to construct a Wikidata-based historical knowledge graph.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26356;&#20808;&#36827;&#30340;&#26234;&#33021;&#20316;&#29289;&#31649;&#29702;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26469;&#20248;&#21270;&#20316;&#29289;&#31649;&#29702;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2403.19839</link><description>&lt;p&gt;
&#26032;&#30340;&#20892;&#23398;&#23478;&#65306;&#35821;&#35328;&#27169;&#22411;&#26159;&#20316;&#29289;&#31649;&#29702;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
The New Agronomists: Language Models are Experts in Crop Management
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19839
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26356;&#20808;&#36827;&#30340;&#26234;&#33021;&#20316;&#29289;&#31649;&#29702;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#26469;&#20248;&#21270;&#20316;&#29289;&#31649;&#29702;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#29289;&#31649;&#29702;&#22312;&#20915;&#23450;&#20316;&#29289;&#20135;&#37327;&#12289;&#32463;&#27982;&#30408;&#21033;&#21644;&#29615;&#22659;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#22312;&#20197;&#24448;&#30740;&#31350;&#22522;&#30784;&#19978;&#24341;&#20837;&#20102;&#19968;&#20010;&#26356;&#20808;&#36827;&#30340;&#26234;&#33021;&#20316;&#29289;&#31649;&#29702;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#29420;&#29305;&#22320;&#23558;&#24378;&#21270;&#23398;&#20064;&#12289;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#21644;&#30001;&#20915;&#31574;&#25903;&#25345;&#31995;&#32479;&#20026;&#20892;&#19994;&#25216;&#26415;&#36716;&#31227;&#65288;DSSAT&#65289;&#23454;&#29616;&#30340;&#20316;&#29289;&#27169;&#25311;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;Q&#32593;&#32476;&#65292;&#26469;&#35757;&#32451;&#22788;&#29702;&#27169;&#25311;&#22120;&#20013;&#20247;&#22810;&#29366;&#24577;&#21464;&#37327;&#20316;&#20026;&#35266;&#27979;&#30340;&#31649;&#29702;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19839v1 Announce Type: cross  Abstract: Crop management plays a crucial role in determining crop yield, economic profitability, and environmental sustainability. Despite the availability of management guidelines, optimizing these practices remains a complex and multifaceted challenge. In response, previous studies have explored using reinforcement learning with crop simulators, typically employing simple neural-network-based reinforcement learning (RL) agents. Building on this foundation, this paper introduces a more advanced intelligent crop management system. This system uniquely combines RL, a language model (LM), and crop simulations facilitated by the Decision Support System for Agrotechnology Transfer (DSSAT). We utilize deep RL, specifically a deep Q-network, to train management policies that process numerous state variables from the simulator as observations. A novel aspect of our approach is the conversion of these state variables into more informative language, fac
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#36731;&#37327;&#32423;&#12289;&#22810;&#24103;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; EM-VLM4AD&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#35270;&#35273;&#38382;&#31572;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#20869;&#23384;&#21644;&#28014;&#28857;&#36816;&#31639;&#38656;&#27714;&#33267;&#23569;&#20943;&#23569;&#21313;&#20493;&#65292;&#24182;&#19988;&#22312;BLEU-4&#12289;METEOR&#12289;CIDEr&#21644;ROGUE&#20998;&#25968;&#19978;&#22343;&#21462;&#24471;&#26356;&#39640;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.19838</link><description>&lt;p&gt;
&#22810;&#24103;&#12289;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Multi-Frame, Lightweight &amp; Efficient Vision-Language Models for Question Answering in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19838
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#36731;&#37327;&#32423;&#12289;&#22810;&#24103;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; EM-VLM4AD&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#35270;&#35273;&#38382;&#31572;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#20869;&#23384;&#21644;&#28014;&#28857;&#36816;&#31639;&#38656;&#27714;&#33267;&#23569;&#20943;&#23569;&#21313;&#20493;&#65292;&#24182;&#19988;&#22312;BLEU-4&#12289;METEOR&#12289;CIDEr&#21644;ROGUE&#20998;&#25968;&#19978;&#22343;&#21462;&#24471;&#26356;&#39640;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;MMLMs&#65289;&#24050;&#32463;&#22312;&#33258;&#21160;&#39550;&#39542;&#30740;&#31350;&#20013;&#21464;&#24471;&#31361;&#20986;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#20132;&#36890;&#22330;&#26223;&#22270;&#20687;&#21644;&#20854;&#20182;&#25968;&#25454;&#27169;&#24577;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#25512;&#29702;&#21644;&#21709;&#24212;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#23433;&#20840;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#26041;&#27861;&#20351;&#29992;&#26114;&#36149;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39592;&#24178;&#21644;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#20351;&#24471;&#36825;&#20123;&#31995;&#32479;&#19981;&#36866;&#21512;&#20855;&#26377;&#20005;&#26684;&#20869;&#23384;&#38480;&#21046;&#21644;&#38656;&#35201;&#24555;&#36895;&#25512;&#29702;&#26102;&#38388;&#30340;&#23454;&#26102;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#20808;&#21069;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;EM-VLM4AD&#65292;&#19968;&#31181;&#39640;&#25928;&#12289;&#36731;&#37327;&#32423;&#12289;&#22810;&#24103;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25191;&#34892;&#33258;&#21160;&#39550;&#39542;&#30340;&#35270;&#35273;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19838v1 Announce Type: cross  Abstract: Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) have become prominent in autonomous driving research, as these models can provide interpretable textual reasoning and responses for end-to-end autonomous driving safety tasks using traffic scene images and other data modalities. However, current approaches to these systems use expensive large language model (LLM) backbones and image encoders, making such systems unsuitable for real-time autonomous driving systems where tight memory constraints exist and fast inference time is necessary. To address these previous issues, we develop EM-VLM4AD, an efficient, lightweight, multi-frame vision language model which performs Visual Question Answering for autonomous driving. In comparison to previous approaches, EM-VLM4AD requires at least 10 times less memory and floating point operations, while also achieving higher BLEU-4, METEOR, CIDEr, and ROGUE scores than the existing b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.19837</link><description>&lt;p&gt;
&#36890;&#36807;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#22522;&#20110;&#27010;&#24565;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Concept-based Analysis of Neural Networks via Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19837
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#36879;&#38236;&#65292;&#36890;&#36807;&#20854;&#38544;&#21547;&#30340;&#39640;&#23618;&#27425;&#27010;&#24565;&#26469;&#36827;&#34892;&#23545;&#35270;&#35273;&#27169;&#22411;&#30340;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#24418;&#24335;&#21270;&#20998;&#26512;&#38750;&#24120;&#21487;&#21462;&#65292;&#20294;&#30001;&#20110;&#38590;&#20197;&#34920;&#36798;&#35270;&#35273;&#20219;&#21153;&#30340;&#24418;&#24335;&#21270;&#35268;&#33539;&#20197;&#21450;&#32570;&#20047;&#39640;&#25928;&#30340;&#39564;&#35777;&#31243;&#24207;&#65292;&#36825;&#26159;&#38750;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#26032;&#20852;&#30340;&#22810;&#27169;&#24577;&#12289;&#35270;&#35273;&#35821;&#35328;&#12289;&#22522;&#30784;&#27169;&#22411;&#65288;VLMs&#65289;&#20316;&#20026;&#19968;&#31181;&#36890;&#36807;&#20854;&#21487;&#20197;&#25512;&#29702;&#35270;&#35273;&#27169;&#22411;&#30340;&#36879;&#38236;&#12290;VLMs&#24050;&#32463;&#22312;&#22823;&#37327;&#22270;&#20687;&#21450;&#20854;&#25991;&#26412;&#25551;&#36848;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#65292;&#22240;&#27492;&#38544;&#24335;&#22320;&#20102;&#35299;&#25551;&#36848;&#36825;&#20123;&#22270;&#20687;&#30340;&#39640;&#23618;&#27425;&#12289;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#21517;&#20026;$\texttt{Con}_{\texttt{spec}}$&#30340;&#36923;&#36753;&#35268;&#33539;&#35821;&#35328;&#65292;&#26088;&#22312;&#20415;&#20110;&#25353;&#29031;&#36825;&#20123;&#27010;&#24565;&#32534;&#20889;&#35268;&#33539;&#12290;&#20026;&#20102;&#23450;&#20041;&#21644;&#24418;&#24335;&#21270;&#26816;&#26597;$\texttt{Con}_{\texttt{spec}}$&#35268;&#33539;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#19968;&#20010;VLM&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#32534;&#30721;&#21644;&#39640;&#25928;&#26816;&#26597;&#35270;&#35273;&#27169;&#22411;&#30340;&#33258;&#28982;&#35821;&#35328;&#23646;&#24615;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;te
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19837v1 Announce Type: cross  Abstract: Formal analysis of vision-based deep neural networks (DNNs) is highly desirable but it is very challenging due to the difficulty of expressing formal specifications for vision tasks and the lack of efficient verification procedures. In this paper, we propose to leverage emerging multimodal, vision-language, foundation models (VLMs) as a lens through which we can reason about vision models. VLMs have been trained on a large body of images accompanied by their textual description, and are thus implicitly aware of high-level, human-understandable concepts describing the images. We describe a logical specification language $\texttt{Con}_{\texttt{spec}}$ designed to facilitate writing specifications in terms of these concepts. To define and formally check $\texttt{Con}_{\texttt{spec}}$ specifications, we leverage a VLM, which provides a means to encode and efficiently check natural-language properties of vision models. We demonstrate our te
&lt;/p&gt;</description></item><item><title>ChatTracer&#26159;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#23454;&#26102;&#34013;&#29273;&#35774;&#22791;&#36861;&#36394;&#31995;&#32479;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21487;&#38752;&#39640;&#25928;&#30340;BLE&#25968;&#25454;&#21253;&#20998;&#32452;&#31639;&#27861;&#21644;&#32467;&#21512;&#20102;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#31574;&#30053;&#30340;LLM&#24494;&#35843;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.19833</link><description>&lt;p&gt;
ChatTracer&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#23454;&#26102;&#34013;&#29273;&#35774;&#22791;&#36861;&#36394;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
ChatTracer: Large Language Model Powered Real-time Bluetooth Device Tracking System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19833
&lt;/p&gt;
&lt;p&gt;
ChatTracer&#26159;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#23454;&#26102;&#34013;&#29273;&#35774;&#22791;&#36861;&#36394;&#31995;&#32479;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21487;&#38752;&#39640;&#25928;&#30340;BLE&#25968;&#25454;&#21253;&#20998;&#32452;&#31639;&#27861;&#21644;&#32467;&#21512;&#20102;&#30417;&#30563;&#24494;&#35843;&#21644;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#31574;&#30053;&#30340;LLM&#24494;&#35843;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;OpenAI ChatGPT&#21644;Google Bard&#25152;&#23637;&#31034;&#30340;&#65292;&#24050;&#32463;&#25913;&#21464;&#20102;&#25105;&#20204;&#19982;&#32593;&#32476;&#25216;&#26415;&#20114;&#21160;&#30340;&#26041;&#24335;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;LLM&#19982;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#65288;WSN&#65289;&#30456;&#36830;&#25509;&#30340;&#21487;&#33021;&#24615;&#12290;&#19968;&#20010;&#25104;&#21151;&#30340;&#35774;&#35745;&#19981;&#20165;&#20250;&#23558;LLM&#30340;&#30693;&#35782;&#39046;&#22495;&#24310;&#20280;&#21040;&#29289;&#29702;&#19990;&#30028;&#65292;&#32780;&#19988;&#23558;&#24443;&#24213;&#38761;&#26032;&#20154;&#20204;&#19982;WSN&#30340;&#20114;&#21160;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatTracer&#65292;&#19968;&#20010;&#30001;LLM&#39537;&#21160;&#30340;&#23454;&#26102;&#34013;&#29273;&#35774;&#22791;&#36861;&#36394;&#31995;&#32479;&#12290;ChatTracer&#30001;&#19977;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;&#19968;&#31995;&#21015;&#34013;&#29273;&#21957;&#25506;&#33410;&#28857;&#12289;&#19968;&#20010;&#25968;&#25454;&#24211;&#21644;&#19968;&#20010;&#32463;&#36807;&#31934;&#35843;&#30340;LLM&#12290;ChatTracer&#30340;&#35774;&#35745;&#22522;&#20110;&#25105;&#20204;&#30340;&#23454;&#39564;&#35266;&#23519;&#65292;&#21830;&#29992;&#30340;&#33529;&#26524;/&#23433;&#21331;&#35774;&#22791;&#21363;&#20351;&#22312;&#31354;&#38386;&#29366;&#24577;&#19979;&#20063;&#20250;&#27599;&#20998;&#38047;&#24191;&#25773;&#25968;&#30334;&#20010;BLE&#25968;&#25454;&#21253;&#12290;&#23427;&#30340;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#65306;i&#65289;&#19968;&#20010;&#21487;&#38752;&#39640;&#25928;&#30340;BLE&#25968;&#25454;&#21253;&#20998;&#32452;&#31639;&#27861;&#65307;&#21644;ii&#65289;&#19968;&#20010;&#32467;&#21512;&#20102;&#30417;&#30563;&#24494;&#35843;&#65288;SFT&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#24494;&#35843;&#31574;&#30053;&#30340;LLM&#24494;&#35843;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19833v1 Announce Type: cross  Abstract: Large language models (LLMs), exemplified by OpenAI ChatGPT and Google Bard, have transformed the way we interact with cyber technologies. In this paper, we study the possibility of connecting LLM with wireless sensor networks (WSN). A successful design will not only extend LLM's knowledge landscape to the physical world but also revolutionize human interaction with WSN. To the end, we present ChatTracer, an LLM-powered real-time Bluetooth device tracking system. ChatTracer comprises three key components: an array of Bluetooth sniffing nodes, a database, and a fine-tuned LLM. ChatTracer was designed based on our experimental observation that commercial Apple/Android devices always broadcast hundreds of BLE packets per minute even in their idle status. Its novelties lie in two aspects: i) a reliable and efficient BLE packet grouping algorithm; and ii) an LLM fine-tuning strategy that combines both supervised fine-tuning (SFT) and reinfo
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#24605;&#32771;&#20102;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24230;&#37327;&#65292;&#21457;&#29616;&#24182;&#25913;&#36827;&#20102;PAvPU&#26694;&#26550;&#20013;&#30340;&#26680;&#24515;&#32570;&#38519;&#12290;</title><link>https://arxiv.org/abs/2403.19826</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
Segmentation Re-thinking Uncertainty Estimation Metrics for Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19826
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20102;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24230;&#37327;&#65292;&#21457;&#29616;&#24182;&#25913;&#36827;&#20102;PAvPU&#26694;&#26550;&#20013;&#30340;&#26680;&#24515;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#65292;&#35821;&#20041;&#20998;&#21106;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#24212;&#29992;&#65292;&#23558;&#22270;&#20687;&#30340;&#27599;&#20010;&#20687;&#32032;&#20998;&#31867;&#20026;&#19981;&#21516;&#30340;&#35821;&#20041;&#31867;&#21035;&#12290;&#36825;&#39033;&#20219;&#21153;&#36890;&#36807;&#21253;&#21547;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26469;&#36229;&#36234;&#20256;&#32479;&#30340;&#20934;&#30830;&#24230;&#24230;&#37327;&#65292;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26159;&#35780;&#20272;&#27599;&#20010;&#20998;&#21106;&#39044;&#27979;&#21487;&#38752;&#24615;&#30340;&#20851;&#38190;&#25351;&#26631;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#35782;&#21035;&#20986;PAvPU&#65288;Patch Accuracy versus Patch Uncertainty&#65289;&#26694;&#26550;&#20013;&#30340;&#19977;&#20010;&#26680;&#24515;&#32570;&#38519;&#65292;&#24182;&#25552;&#20986;&#26088;&#22312;&#25913;&#36827;&#35813;&#24230;&#37327;&#30340;&#24378;&#22823;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#26088;&#22312;&#22686;&#24378;&#21487;&#38752;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19826v1 Announce Type: new  Abstract: In the domain of computer vision, semantic segmentation emerges as a fundamental application within machine learning, wherein individual pixels of an image are classified into distinct semantic categories. This task transcends traditional accuracy metrics by incorporating uncertainty quantification, a critical measure for assessing the reliability of each segmentation prediction. Such quantification is instrumental in facilitating informed decision-making, particularly in applications where precision is paramount. Within this nuanced framework, the metric known as PAvPU (Patch Accuracy versus Patch Uncertainty) has been developed as a specialized tool for evaluating entropy-based uncertainty in image segmentation tasks. However, our investigation identifies three core deficiencies within the PAvPU framework and proposes robust solutions aimed at refining the metric. By addressing these issues, we aim to enhance the reliability and applic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#20197;&#21450;&#22522;&#20110;&#32763;&#35793;&#30340;&#30417;&#30563;&#20013;&#38388;&#35757;&#32451;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;Librispeech&#21644;SUPERB&#19978;&#30456;&#23545;&#25552;&#39640;&#39640;&#36798;38.45%&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.19822</link><description>&lt;p&gt;
&#22810;&#38454;&#27573;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#29992;&#20110;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multi-Stage Multi-Modal Pre-Training for Automatic Speech Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19822
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#20197;&#21450;&#22522;&#20110;&#32763;&#35793;&#30340;&#30417;&#30563;&#20013;&#38388;&#35757;&#32451;&#26041;&#27861;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;Librispeech&#21644;SUPERB&#19978;&#30456;&#23545;&#25552;&#39640;&#39640;&#36798;38.45%&#30340;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#34920;&#26126;&#65292;&#19982;&#38543;&#26426;&#21021;&#22987;&#21270;&#27169;&#22411;&#30456;&#27604;&#65292;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#24615;&#33021;&#65292;&#21363;&#20351;&#27169;&#22411;&#22312;&#21333;&#27169;&#24577;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;ASR&#20219;&#21153;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#38454;&#27573;&#39044;&#35757;&#32451;&#65292;&#21363;&#20351;&#29992;&#21333;&#20010;&#26080;&#30417;&#30563;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#28982;&#21518;&#22312;&#19979;&#28216;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22810;&#27169;&#24577;&#21644;&#22810;&#20219;&#21153;&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#19982;&#22522;&#20110;&#32763;&#35793;&#30340;&#30417;&#30563;&#20013;&#38388;&#35757;&#32451;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#65292;&#36825;&#31181;&#22810;&#38454;&#27573;&#26041;&#27861;&#22312;Librispeech&#21644;SUPERB&#19978;&#30456;&#23545;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#30340;&#25913;&#36827;&#26368;&#39640;&#36798;38.45&#65285;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20998;&#20139;&#20102;&#36873;&#25321;&#39044;&#35757;&#32451;&#26041;&#27861;&#21644;&#25968;&#25454;&#38598;&#30340;&#19968;&#20123;&#37325;&#35201;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19822v1 Announce Type: cross  Abstract: Recent advances in machine learning have demonstrated that multi-modal pre-training can improve automatic speech recognition (ASR) performance compared to randomly initialized models, even when models are fine-tuned on uni-modal tasks. Existing multi-modal pre-training methods for the ASR task have primarily focused on single-stage pre-training where a single unsupervised task is used for pre-training followed by fine-tuning on the downstream task. In this work, we introduce a novel method combining multi-modal and multi-task unsupervised pre-training with a translation-based supervised mid-training approach. We empirically demonstrate that such a multi-stage approach leads to relative word error rate (WER) improvements of up to 38.45% over baselines on both Librispeech and SUPERB. Additionally, we share several important findings for choosing pre-training methods and datasets.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#26041;&#27861;&#65292;&#35780;&#20272;&#21307;&#30103;&#35786;&#26029;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#21307;&#23398;&#25351;&#21335;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#30830;&#23450;&#33008;&#33146;&#30284;&#27835;&#30103;&#20851;&#38190;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#20351;&#29992;&#30456;&#20284;&#24230;&#34913;&#37327;&#30340;&#35299;&#37322;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19820</link><description>&lt;p&gt;
&#35780;&#20272;&#21307;&#30103;&#35786;&#26029;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#65306;&#19968;&#31181;&#20154;&#26426;&#21327;&#20316;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evaluating Explanatory Capabilities of Machine Learning Models in Medical Diagnostics: A Human-in-the-Loop Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19820
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#26426;&#21327;&#20316;&#26041;&#27861;&#65292;&#35780;&#20272;&#21307;&#30103;&#35786;&#26029;&#20013;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;&#21307;&#23398;&#25351;&#21335;&#21644;&#29305;&#24449;&#37325;&#35201;&#24615;&#30830;&#23450;&#33008;&#33146;&#30284;&#27835;&#30103;&#20851;&#38190;&#29305;&#24449;&#30340;&#26041;&#27861;&#65292;&#21516;&#26102;&#25506;&#32034;&#20102;&#20351;&#29992;&#30456;&#20284;&#24230;&#34913;&#37327;&#30340;&#35299;&#37322;&#32467;&#26524;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#20915;&#31574;&#26641;&#12289;&#38543;&#26426;&#26862;&#26519;&#21644;XGBoost&#27169;&#22411;&#22312;&#33008;&#33146;&#30284;&#25968;&#25454;&#38598;&#19978;&#30340;&#35299;&#37322;&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30740;&#31350;&#12290;&#25105;&#20204;&#21033;&#29992;&#20154;&#26426;&#21327;&#20316;&#25216;&#26415;&#21644;&#21307;&#23398;&#25351;&#21335;&#20316;&#20026;&#39046;&#22495;&#30693;&#35782;&#26469;&#28304;&#65292;&#20197;&#30830;&#23450;&#19982;&#21046;&#23450;&#33008;&#33146;&#30284;&#27835;&#30103;&#30456;&#20851;&#30340;&#19981;&#21516;&#29305;&#24449;&#30340;&#37325;&#35201;&#24615;&#12290;&#36825;&#20123;&#29305;&#24449;&#19981;&#20165;&#29992;&#20316;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#38477;&#32500;&#26041;&#27861;&#65292;&#36824;&#29992;&#20316;&#35780;&#20272;&#19981;&#21516;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#30340;&#26041;&#24335;&#65292;&#20351;&#29992;&#26080;&#30693;&#21644;&#38750;&#26080;&#30693;&#30340;&#35299;&#37322;&#24615;&#25216;&#26415;&#12290;&#20026;&#20102;&#20415;&#20110;&#35299;&#37322;&#32467;&#26524;&#30340;&#35299;&#37322;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#35832;&#22914;&#21152;&#26435;&#26480;&#21345;&#30456;&#20284;&#31995;&#25968;&#20043;&#31867;&#30340;&#30456;&#20284;&#24230;&#34913;&#37327;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#19981;&#20165;&#26159;&#36873;&#25321;&#24615;&#33021;&#26368;&#20339;&#30340;&#27169;&#22411;&#65292;&#36824;&#35201;&#36873;&#25321;&#33021;&#22815;&#26368;&#22909;&#35299;&#37322;&#20854;&#32467;&#35770;&#24182;&#19982;&#20043;&#20445;&#25345;&#19968;&#33268;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19820v1 Announce Type: cross  Abstract: This paper presents a comprehensive study on the evaluation of explanatory capabilities of machine learning models, with a focus on Decision Trees, Random Forest and XGBoost models using a pancreatic cancer dataset. We use Human-in-the-Loop related techniques and medical guidelines as a source of domain knowledge to establish the importance of the different features that are relevant to establish a pancreatic cancer treatment. These features are not only used as a dimensionality reduction approach for the machine learning models, but also as way to evaluate the explainability capabilities of the different models using agnostic and non-agnostic explainability techniques. To facilitate interpretation of explanatory results, we propose the use of similarity measures such as the Weighted Jaccard Similarity coefficient. The goal is to not only select the best performing model but also the one that can best explain its conclusions and aligns
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#28145;&#24230;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#25928;&#21033;&#29992;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#12290;</title><link>https://arxiv.org/abs/2403.19802</link><description>&lt;p&gt;
&#24320;&#21457;&#21307;&#30103;&#20445;&#20581;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Developing Healthcare Language Model Embedding Spaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19802
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#23545;&#27604;&#23398;&#20064;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#21307;&#30103;&#20445;&#20581;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#26377;&#25928;&#21033;&#29992;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#20943;&#23569;&#20102;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#22312;&#35832;&#22914;&#19987;&#27880;&#20110;&#21307;&#30103;&#20445;&#20581;&#25991;&#26412;&#20043;&#31867;&#30340;&#36328;&#39046;&#22495;&#25968;&#25454;&#38598;&#19978;&#32463;&#24120;&#38754;&#20020;&#22256;&#38590;&#12290;&#25105;&#20204;&#25506;&#32034;&#19987;&#38376;&#30340;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#20197;&#35843;&#25972;&#36739;&#23567;&#30340;LLMs&#20197;&#36866;&#24212;&#19981;&#21516;&#30340;&#21307;&#30103;&#20445;&#20581;&#25968;&#25454;&#38598;&#12290;&#35780;&#20272;&#20102;&#19977;&#31181;&#26041;&#27861;&#65306;&#20256;&#32479;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#12289;&#29992;&#20110;&#26080;&#30417;&#30563;&#25991;&#26412;&#34920;&#31034;&#30340;&#28145;&#24230;&#23545;&#27604;&#23398;&#20064; (DeCLUTR) &#21644;&#19968;&#31181;&#21033;&#29992;&#21307;&#30103;&#20445;&#20581;&#29615;&#22659;&#20013;&#30340;&#20803;&#25968;&#25454;&#31867;&#21035;&#30340;&#26032;&#39062;&#39044;&#35757;&#32451;&#30446;&#26631;&#12290;&#23545;&#27599;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#19979;&#28216;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#24182;&#23545;&#29983;&#25104;&#30340;&#23884;&#20837;&#31354;&#38388;&#36827;&#34892;&#20102;&#39069;&#22806;&#20998;&#26512;&#12290;&#23545;&#27604;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#25552;&#20379;&#24378;&#22823;&#24615;&#33021;&#65292;&#24182;&#19988;&#38656;&#35201;&#36739;&#23569;&#30340;&#27169;&#22411;&#21442;&#25968;&#26356;&#26032;&#12290;&#34429;&#28982;&#22522;&#20110;&#20803;&#25968;&#25454;&#30340;&#39044;&#35757;&#32451;&#24182;&#26410;&#36827;&#19968;&#27493;&#25552;&#39640;&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#20294;&#23427;&#30830;&#23454;&#20135;&#29983;&#20102;&#26377;&#36259;&#30340;&#23884;&#20837;&#32858;&#31867;&#21487;&#20998;&#24615;&#12290;&#25152;&#26377;&#39046;&#22495;&#30340;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19802v1 Announce Type: cross  Abstract: Pre-trained Large Language Models (LLMs) often struggle on out-of-domain datasets like healthcare focused text. We explore specialized pre-training to adapt smaller LLMs to different healthcare datasets. Three methods are assessed: traditional masked language modeling, Deep Contrastive Learning for Unsupervised Textual Representations (DeCLUTR), and a novel pre-training objective utilizing metadata categories from the healthcare settings. These schemes are evaluated on downstream document classification tasks for each dataset, with additional analysis of the resultant embedding spaces. Contrastively trained models outperform other approaches on the classification tasks, delivering strong performance from limited labeled data and with fewer model parameter updates required. While metadata-based pre-training does not further improve classifications across the datasets, it yields interesting embedding cluster separability. All domain adap
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Gegenbauer-based graph convolutional (GegenConv)&#31639;&#23376;&#65292;&#29992;&#20110;&#25552;&#39640;&#26102;&#21464;&#20449;&#21495;&#37325;&#26500;&#30340;&#20934;&#30830;&#24615;</title><link>https://arxiv.org/abs/2403.19800</link><description>&lt;p&gt;
Gegenbauer&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#21464;&#20449;&#21495;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
Gegenbauer Graph Neural Networks for Time-varying Signal Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19800
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Gegenbauer-based graph convolutional (GegenConv)&#31639;&#23376;&#65292;&#29992;&#20110;&#25552;&#39640;&#26102;&#21464;&#20449;&#21495;&#37325;&#26500;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#26500;&#26102;&#21464;&#22270;&#20449;&#21495;&#65288;&#25110;&#22270;&#26102;&#38388;&#24207;&#21015;&#25554;&#34917;&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#21644;&#20449;&#21495;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20174;&#20256;&#24863;&#22120;&#32593;&#32476;&#20013;&#30340;&#32570;&#22833;&#25968;&#25454;&#25554;&#34917;&#21040;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#12290;&#20934;&#30830;&#25429;&#25417;&#36825;&#20123;&#20449;&#21495;&#22266;&#26377;&#30340;&#26102;&#31354;&#20449;&#24687;&#23545;&#20110;&#26377;&#25928;&#35299;&#20915;&#36825;&#20123;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#26102;&#38388;&#24046;&#30340;&#24179;&#28369;&#24615;&#20551;&#35774;&#21644;&#31616;&#21333;&#30340;&#20984;&#20248;&#21270;&#25216;&#26415;&#65292;&#23384;&#22312;&#22266;&#26377;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#23398;&#20064;&#27169;&#22359;&#20197;&#22686;&#24378;&#19979;&#28216;&#20219;&#21153;&#20934;&#30830;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#22522;&#20110;Gegenbauer&#22810;&#39033;&#24335;&#29702;&#35770;&#30340;Gegenbauer-based graph convolutional&#65288;GegenConv&#65289;&#31639;&#23376;&#65292;&#36825;&#26159;&#20256;&#32479;&#20999;&#27604;&#38634;&#22827;&#22270;&#21367;&#31215;&#30340;&#25512; generalization&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19800v1 Announce Type: cross  Abstract: Reconstructing time-varying graph signals (or graph time-series imputation) is a critical problem in machine learning and signal processing with broad applications, ranging from missing data imputation in sensor networks to time-series forecasting. Accurately capturing the spatio-temporal information inherent in these signals is crucial for effectively addressing these tasks. However, existing approaches relying on smoothness assumptions of temporal differences and simple convex optimization techniques have inherent limitations. To address these challenges, we propose a novel approach that incorporates a learning module to enhance the accuracy of the downstream task. To this end, we introduce the Gegenbauer-based graph convolutional (GegenConv) operator, which is a generalization of the conventional Chebyshev graph convolution by leveraging the theory of Gegenbauer polynomials. By deviating from traditional convex problems, we expand t
&lt;/p&gt;</description></item><item><title>MAPL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;Model Agnostic Peer-to-peer Learning&#65292;&#36890;&#36807;&#28857;&#23545;&#28857;&#36890;&#20449;&#22312;&#37051;&#36817;&#23458;&#25143;&#31471;&#20043;&#38388;&#21516;&#26102;&#23398;&#20064;&#24322;&#36136;&#20010;&#24615;&#21270;&#27169;&#22411;&#21644;&#21327;&#20316;&#22270;&#65292;&#22312;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#21327;&#20316;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20102;MAPL&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.19792</link><description>&lt;p&gt;
MAPL: &#27169;&#22411;&#26080;&#20851;&#30340;&#28857;&#23545;&#28857;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MAPL: Model Agnostic Peer-to-peer Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19792
&lt;/p&gt;
&lt;p&gt;
MAPL&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;Model Agnostic Peer-to-peer Learning&#65292;&#36890;&#36807;&#28857;&#23545;&#28857;&#36890;&#20449;&#22312;&#37051;&#36817;&#23458;&#25143;&#31471;&#20043;&#38388;&#21516;&#26102;&#23398;&#20064;&#24322;&#36136;&#20010;&#24615;&#21270;&#27169;&#22411;&#21644;&#21327;&#20316;&#22270;&#65292;&#22312;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26377;&#25928;&#30340;&#21327;&#20316;&#65292;&#24182;&#19988;&#23454;&#39564;&#35777;&#26126;&#20102;MAPL&#22312;&#24615;&#33021;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21435;&#20013;&#24515;&#21270;&#29615;&#22659;&#20013;&#65292;&#24322;&#36136;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;&#22312;&#25991;&#29486;&#20013;&#26159;&#19968;&#20010;&#30456;&#24403;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#12290;&#20026;&#20102;&#20174;&#32467;&#26500;&#19978;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#28857;&#23545;&#28857;&#23398;&#20064;&#65288;&#31616;&#31216;MAPL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36807;&#37051;&#36817;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#28857;&#23545;&#28857;&#36890;&#20449;&#21516;&#26102;&#23398;&#20064;&#24322;&#36136;&#20010;&#24615;&#21270;&#27169;&#22411;&#21644;&#21327;&#20316;&#22270;&#30340;&#26032;&#26041;&#27861;&#12290;MAPL&#30001;&#20004;&#20010;&#20027;&#35201;&#27169;&#22359;&#32452;&#25104;&#65306;&#65288;i&#65289;&#26412;&#22320;&#32423;&#21035;&#30340;&#20010;&#24615;&#21270;&#27169;&#22411;&#23398;&#20064;&#65288;PML&#65289;&#65292;&#21033;&#29992;&#23458;&#25143;&#31471;&#20869;&#37096;&#21644;&#23458;&#25143;&#31471;&#38388;&#23545;&#27604;&#25439;&#22833;&#30340;&#32452;&#21512;&#65307;&#65288;ii&#65289;&#32593;&#32476;&#33539;&#22260;&#30340;&#21435;&#20013;&#24515;&#21270;&#21327;&#20316;&#22270;&#23398;&#20064;&#65288;CGL&#65289;&#65292;&#26681;&#25454;&#26412;&#22320;&#20219;&#21153;&#30456;&#20284;&#24615;&#20197;&#38544;&#31169;&#20445;&#25252;&#30340;&#26041;&#24335;&#21160;&#24577;&#22320;&#20248;&#21270;&#21327;&#20316;&#26435;&#37325;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;MAPL&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#19988;&#19982;&#20854;&#38598;&#20013;&#24335;&#27169;&#22411;&#26080;&#20851;&#30340;&#23545;&#24212;&#29289;&#30456;&#27604;&#65292;MAPL&#34920;&#29616;&#20986;&#31454;&#20105;&#21147;&#65288;&#25110;&#32773;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26356;&#20248;&#65289;&#65292;&#32780;&#19988;&#19981;&#20381;&#36182;&#20110;&#20219;&#20309;&#20013;&#24515;&#26381;&#21153;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19792v1 Announce Type: cross  Abstract: Effective collaboration among heterogeneous clients in a decentralized setting is a rather unexplored avenue in the literature. To structurally address this, we introduce Model Agnostic Peer-to-peer Learning (coined as MAPL) a novel approach to simultaneously learn heterogeneous personalized models as well as a collaboration graph through peer-to-peer communication among neighboring clients. MAPL is comprised of two main modules: (i) local-level Personalized Model Learning (PML), leveraging a combination of intra- and inter-client contrastive losses; (ii) network-wide decentralized Collaborative Graph Learning (CGL) dynamically refining collaboration weights in a privacy-preserving manner based on local task similarities. Our extensive experimentation demonstrates the efficacy of MAPL and its competitive (or, in most cases, superior) performance compared to its centralized model-agnostic counterparts, without relying on any central ser
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#20020;&#24202;&#25968;&#25454;&#65292;&#24110;&#21161;&#33521;&#22269;&#22269;&#23478;&#21355;&#29983;&#26381;&#21153;&#20307;&#31995;(NHS)&#35299;&#20915;&#19987;&#31185;&#31934;&#31070;&#20445;&#20581;&#38271;&#31561;&#24453;&#21517;&#21333;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.19790</link><description>&lt;p&gt;
&#20026;&#25968;&#23383;&#21021;&#32423;&#20445;&#20581;&#24037;&#20316;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Bespoke Large Language Models for Digital Triage Assistance in Mental Health Care
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19790
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22788;&#29702;&#24515;&#29702;&#20581;&#24247;&#25252;&#29702;&#20013;&#30340;&#38750;&#32467;&#26500;&#21270;&#20020;&#24202;&#25968;&#25454;&#65292;&#24110;&#21161;&#33521;&#22269;&#22269;&#23478;&#21355;&#29983;&#26381;&#21153;&#20307;&#31995;(NHS)&#35299;&#20915;&#19987;&#31185;&#31934;&#31070;&#20445;&#20581;&#38271;&#31561;&#24453;&#21517;&#21333;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20195;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#23545;&#22788;&#29702;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHRs&#65289;&#20013;&#21253;&#21547;&#30340;&#38750;&#32467;&#26500;&#21270;&#21465;&#36848;&#24615;&#33258;&#30001;&#25991;&#26412;&#20020;&#24202;&#25968;&#25454;&#20855;&#26377;&#23454;&#29992;&#24615;&#65292;&#36825;&#23545;&#20110;&#31934;&#31070;&#20581;&#24247;&#39046;&#22495;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#24120;&#35268;&#25910;&#38598;&#30340;&#24739;&#32773;&#25968;&#25454;&#32570;&#20047;&#32467;&#26500;&#21270;&#30340;&#26426;&#22120;&#21487;&#35835;&#20869;&#23481;&#12290;&#33521;&#22269;&#22269;&#23478;&#21355;&#29983;&#26381;&#21153;&#20307;&#31995;&#65288;NHS&#65289;&#38754;&#20020;&#30340;&#19968;&#39033;&#37325;&#35201;&#38382;&#39064;&#26159;&#19987;&#31185;&#31934;&#31070;&#20445;&#20581;&#30340;&#38271;&#31561;&#24453;&#21517;&#21333;&#12290;&#26681;&#25454;NHS&#30340;&#25968;&#25454;&#65292;2023&#24180;&#27599;&#20010;&#26376;&#37117;&#26377;37&#19975;&#33267;47&#19975;&#20154;&#27425;&#21521;&#27425;&#32423;&#31934;&#31070;&#20445;&#20581;&#26381;&#21153;&#25552;&#20986;&#20010;&#20307;&#21270;&#30340;&#26032;&#25512;&#33616;&#12290;&#25512;&#33616;&#24517;&#39035;&#30001;&#20020;&#24202;&#21307;&#29983;&#36827;&#34892;&#31579;&#36873;&#65292;&#20351;&#29992;&#24739;&#32773;EHR&#20013;&#21253;&#21547;&#30340;&#20020;&#24202;&#20449;&#24687;&#65292;&#20197;&#20570;&#20986;&#20851;&#20110;&#26368;&#36866;&#21512;&#30340;&#31934;&#31070;&#20445;&#20581;&#22242;&#38431;&#35780;&#20272;&#21644;&#21487;&#33021;&#27835;&#30103;&#36825;&#20123;&#24739;&#32773;&#30340;&#20915;&#23450;&#12290;&#36890;&#36807;&#25668;&#20837;&#28508;&#22312;&#24222;&#22823;&#30340;&#20020;&#24202;&#31508;&#35760;&#65292;&#26377;&#25928;&#22320;&#25512;&#33616;&#30456;&#20851;&#22242;&#38431;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19790v1 Announce Type: new  Abstract: Contemporary large language models (LLMs) may have utility for processing unstructured, narrative free-text clinical data contained in electronic health records (EHRs) -- a particularly important use-case for mental health where a majority of routinely-collected patient data lacks structured, machine-readable content.   A significant problem for the the United Kingdom's National Health Service (NHS) are the long waiting lists for specialist mental healthcare. According to NHS data, in each month of 2023, there were between 370,000 and 470,000 individual new referrals into secondary mental healthcare services. Referrals must be triaged by clinicians, using clinical information contained in the patient's EHR to arrive at a decision about the most appropriate mental healthcare team to assess and potentially treat these patients.   The ability to efficiently recommend a relevant team by ingesting potentially voluminous clinical notes could h
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22810;&#23610;&#24230;&#23618;&#27425;&#20449;&#24687;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#37319;&#29992;&#20998;&#23618;&#20381;&#36182;&#25439;&#22833;&#26469;&#25552;&#39640;&#24847;&#22270;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#35013;&#37197;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36739;&#20248;&#30340;&#24847;&#22270;&#35782;&#21035;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19770</link><description>&lt;p&gt;
&#20998;&#23618;&#28145;&#24230;&#23398;&#20064;&#29992;&#20110;&#35013;&#37197;&#20219;&#21153;&#20013;&#36828;&#31243;&#25805;&#32437;&#25805;&#20316;&#24847;&#22270;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Deep Learning for Intention Estimation of Teleoperation Manipulation in Assembly Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19770
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#22810;&#23610;&#24230;&#23618;&#27425;&#20449;&#24687;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36890;&#36807;&#37319;&#29992;&#20998;&#23618;&#20381;&#36182;&#25439;&#22833;&#26469;&#25552;&#39640;&#24847;&#22270;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#35013;&#37197;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#36739;&#20248;&#30340;&#24847;&#22270;&#35782;&#21035;&#21644;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#26426;&#21327;&#20316;&#20013;&#65292;&#20849;&#20139;&#25511;&#21046;&#20026;&#36828;&#31243;&#25805;&#20316;&#26426;&#22120;&#20154;&#25805;&#32437;&#25552;&#20379;&#20102;&#25552;&#21319;&#21046;&#36896;&#21644;&#35013;&#37197;&#24037;&#33402;&#25928;&#29575;&#30340;&#26426;&#20250;&#12290;&#22312;&#25191;&#34892;&#29992;&#25143;&#24847;&#22270;&#19978;&#65292;&#26426;&#22120;&#20154;&#38656;&#35201;&#36741;&#21161;&#12290;&#20026;&#27492;&#65292;&#38656;&#35201;&#40065;&#26834;&#21644;&#21363;&#26102;&#30340;&#24847;&#22270;&#20272;&#35745;&#65292;&#20381;&#36182;&#20110;&#34892;&#20026;&#35266;&#23519;&#12290;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#32423;&#21035;&#30340;&#24847;&#22270;&#20272;&#35745;&#25216;&#26415;&#65292;&#21363;&#20302;&#32423;&#21160;&#20316;&#21644;&#39640;&#32423;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#25972;&#21512;&#22810;&#23610;&#24230;&#23618;&#27425;&#20449;&#24687;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;&#25105;&#20204;&#37319;&#29992;&#20998;&#23618;&#20381;&#36182;&#25439;&#22833;&#26469;&#25552;&#39640;&#25972;&#20307;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#31383;&#21475;&#26041;&#27861;&#65292;&#20026;&#36755;&#20837;&#25968;&#25454;&#20998;&#37197;&#36866;&#24403;&#30340;&#20998;&#23618;&#39044;&#27979;&#31383;&#21475;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#36755;&#20837;&#30340;&#39044;&#27979;&#33021;&#21147;&#36827;&#34892;&#20998;&#26512;&#65292;&#35777;&#26126;&#20102;&#28145;&#24230;&#20998;&#23618;&#27169;&#22411;&#22312;&#39044;&#27979;&#20934;&#30830;&#24230;&#21644;&#25552;&#21069;&#24847;&#22270;&#35782;&#21035;&#26041;&#38754;&#30340;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#23454;&#29616;&#22312;&#19968;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19770v1 Announce Type: cross  Abstract: In human-robot collaboration, shared control presents an opportunity to teleoperate robotic manipulation to improve the efficiency of manufacturing and assembly processes. Robots are expected to assist in executing the user's intentions. To this end, robust and prompt intention estimation is needed, relying on behavioral observations. The framework presents an intention estimation technique at hierarchical levels i.e., low-level actions and high-level tasks, by incorporating multi-scale hierarchical information in neural networks. Technically, we employ hierarchical dependency loss to boost overall accuracy. Furthermore, we propose a multi-window method that assigns proper hierarchical prediction windows of input data. An analysis of the predictive power with various inputs demonstrates the predominance of the deep hierarchical model in the sense of prediction accuracy and early intention identification. We implement the algorithm on a
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#26469;&#29983;&#25104;&#23545;&#29031;&#24615;&#35299;&#37322;POMDP&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;Search and Rescue&#65288;SAR&#65289;&#29615;&#22659;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#26469;&#35752;&#35770;&#19982;&#20043;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.19760</link><description>&lt;p&gt;
&#21033;&#29992;&#21453;&#20107;&#23454;&#36335;&#24452;&#35299;&#37322;POMDP&#31574;&#30053;&#30340;&#23545;&#29031;&#24615;
&lt;/p&gt;
&lt;p&gt;
Leveraging Counterfactual Paths for Contrastive Explanations of POMDP Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#21453;&#20107;&#23454;&#36335;&#24452;&#26469;&#29983;&#25104;&#23545;&#29031;&#24615;&#35299;&#37322;POMDP&#31574;&#30053;&#65292;&#24182;&#36890;&#36807;&#20998;&#26512;Search and Rescue&#65288;SAR&#65289;&#29615;&#22659;&#20013;&#30340;&#26696;&#20363;&#30740;&#31350;&#26469;&#35752;&#35770;&#19982;&#20043;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#31867;&#36234;&#26469;&#36234;&#20381;&#36182;&#33258;&#20027;&#31995;&#32479;&#65292;&#30830;&#20445;&#36825;&#20123;&#31995;&#32479;&#30340;&#36879;&#26126;&#24615;&#23545;&#20110;&#23427;&#20204;&#25345;&#32493;&#34987;&#37319;&#32435;&#32780;&#35328;&#33267;&#20851;&#37325;&#35201;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#23545;&#20195;&#29702;&#34892;&#20026;&#30340;&#35299;&#37322;&#26469;&#20943;&#23569;&#22256;&#24785;&#65292;&#22521;&#20859;&#20154;&#20204;&#23545;&#31995;&#32479;&#30340;&#20449;&#20219;&#12290;&#37096;&#20998;&#21487;&#35266;&#23519;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#33021;&#22815;&#25512;&#29702;&#36716;&#21464;&#21644;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#30340;&#28789;&#27963;&#26694;&#26550;&#65292;&#21516;&#26102;&#20063;&#36866;&#21512;&#35299;&#37322;&#12290;&#36825;&#39033;&#24037;&#20316;&#30740;&#31350;&#20102;&#20351;&#29992;&#29992;&#25143;&#25552;&#20379;&#30340;&#21453;&#20107;&#23454;&#26469;&#29983;&#25104;POMDP&#31574;&#30053;&#30340;&#23545;&#29031;&#24615;&#35299;&#37322;&#12290;&#29305;&#24449;&#26399;&#26395;&#34987;&#29992;&#20316;&#23545;&#27604;&#36825;&#20123;&#31574;&#30053;&#30340;&#24615;&#33021;&#30340;&#25163;&#27573;&#12290;&#25105;&#20204;&#22312;&#25628;&#32034;&#19982;&#25937;&#25588;&#65288;SAR&#65289;&#29615;&#22659;&#20013;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#20004;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#20998;&#26512;&#21644;&#35752;&#35770;&#20102;&#30456;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19760v1 Announce Type: new  Abstract: As humans come to rely on autonomous systems more, ensuring the transparency of such systems is important to their continued adoption. Explainable Artificial Intelligence (XAI) aims to reduce confusion and foster trust in systems by providing explanations of agent behavior. Partially observable Markov decision processes (POMDPs) provide a flexible framework capable of reasoning over transition and state uncertainty, while also being amenable to explanation. This work investigates the use of user-provided counterfactuals to generate contrastive explanations of POMDP policies. Feature expectations are used as a means of contrasting the performance of these policies. We demonstrate our approach in a Search and Rescue (SAR) setting. We analyze and discuss the associated challenges through two case studies.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;2024&#24180;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#30340;&#37327;&#23376;&#35745;&#31639;&#24212;&#29992;&#65292;&#22312;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#20102;&#35832;&#22914;&#35789;&#23884;&#20837;&#12289;&#24207;&#21015;&#27169;&#22411;&#12289;&#27880;&#24847;&#21147;&#21644;&#35821;&#27861;&#20998;&#26512;&#31561;NLP&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#35774;&#35745;&#26469;&#22788;&#29702;&#25991;&#26412;&#32534;&#30721;&#65292;&#24182;&#25506;&#35752;&#20102;&#37327;&#23376;&#29702;&#35770;&#23545;&#8220;&#19981;&#30830;&#23450;&#24615;&#26159;&#20160;&#20040;&#65311;&#8221;&#21644;&#8220;&#26234;&#33021;&#26159;&#20160;&#20040;&#65311;&#8221;&#31561;&#26680;&#24515;&#38382;&#39064;&#30340;&#20851;&#38190;&#36129;&#29486;&#12290;</title><link>https://arxiv.org/abs/2403.19758</link><description>&lt;p&gt;
2024&#24180;&#30340;&#33258;&#28982;&#35821;&#35328;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#37327;&#23376;&#35745;&#31639;&#65306;QNLP&#20013;&#30340;&#30740;&#31350;&#35201;&#28857;&#21644;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Natural Language, AI, and Quantum Computing in 2024: Research Ingredients and Directions in QNLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19758
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;2024&#24180;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#20013;&#30340;&#37327;&#23376;&#35745;&#31639;&#24212;&#29992;&#65292;&#22312;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#20351;&#29992;&#20102;&#35832;&#22914;&#35789;&#23884;&#20837;&#12289;&#24207;&#21015;&#27169;&#22411;&#12289;&#27880;&#24847;&#21147;&#21644;&#35821;&#27861;&#20998;&#26512;&#31561;NLP&#25216;&#26415;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#35774;&#35745;&#26469;&#22788;&#29702;&#25991;&#26412;&#32534;&#30721;&#65292;&#24182;&#25506;&#35752;&#20102;&#37327;&#23376;&#29702;&#35770;&#23545;&#8220;&#19981;&#30830;&#23450;&#24615;&#26159;&#20160;&#20040;&#65311;&#8221;&#21644;&#8220;&#26234;&#33021;&#26159;&#20160;&#20040;&#65311;&#8221;&#31561;&#26680;&#24515;&#38382;&#39064;&#30340;&#20851;&#38190;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19758v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25277;&#35937;&#65306;&#35821;&#35328;&#22788;&#29702;&#26159;&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#30340;&#20851;&#38190;&#65292;&#21516;&#26102;&#37327;&#23376;&#35745;&#31639;&#20063;&#24320;&#22987;&#24212;&#29992;&#12290;&#36825;&#24341;&#36215;&#20102;&#37327;&#23376;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#26497;&#22823;&#20852;&#36259;&#65292;&#20986;&#29616;&#20102;&#20960;&#20010;&#26089;&#26399;&#25552;&#26696;&#21644;&#23454;&#39564;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;NLP&#30456;&#20851;&#25216;&#26415;&#65292;&#21253;&#25324;&#35789;&#23884;&#20837;&#12289;&#24207;&#21015;&#27169;&#22411;&#12289;&#27880;&#24847;&#21147;&#21644;&#35821;&#27861;&#20998;&#26512;&#26159;&#22914;&#20309;&#24212;&#29992;&#20110;&#37327;&#23376;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#35774;&#35745;&#29992;&#20110;&#25991;&#26412;&#32534;&#30721;&#30340;&#22522;&#26412;&#20219;&#21153;&#65288;&#22312;&#20869;&#23384;&#20013;&#34920;&#31034;&#19968;&#20010;&#23383;&#31526;&#20018;&#65289;&#65292;&#36825;&#22312;&#20197;&#21069;&#27809;&#26377;&#35814;&#32454;&#35752;&#35770;&#36807;&#12290;&#38500;&#20102;&#25512;&#21160;&#26032;&#25216;&#26415;&#65292;&#37327;&#23376;&#29702;&#35770;&#36824;&#23545;&#8220;&#19981;&#30830;&#23450;&#24615;&#26159;&#20160;&#20040;&#65311;&#8221;&#21644;&#8220;&#26234;&#33021;&#26159;&#20160;&#20040;&#65311;&#8221;&#31561;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20570;&#20986;&#20102;&#20851;&#38190;&#36129;&#29486;&#12290;&#38543;&#30528;&#36825;&#20123;&#38382;&#39064;&#22312;&#20154;&#24037;&#31995;&#32479;&#20013;&#21464;&#24471;&#24840;&#21457;&#32039;&#36843;&#65292;&#26412;&#25991;&#36824;&#32771;&#34385;&#20102;&#19968;&#20123;&#20107;&#23454;&#27010;&#24565;&#21270;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19758v1 Announce Type: cross  Abstract: Language processing is at the heart of current developments in artificial intelligence, and quantum computers are becoming available at the same time. This has led to great interest in quantum natural language processing, and several early proposals and experiments. This paper surveys the state of this area, showing how NLP-related techniques including word embeddings, sequential models, attention, and grammatical parsing have been used in quantum language processing. We introduce a new quantum design for the basic task of text encoding (representing a string of characters in memory), which has not been addressed in detail before.   As well as motivating new technologies, quantum theory has made key contributions to the challenging questions of 'What is uncertainty?' and 'What is intelligence?' As these questions are taking on fresh urgency with artificial systems, the paper also considers some of the ways facts are conceptualized and 
&lt;/p&gt;</description></item><item><title>&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20026;&#21355;&#26143;&#20013;&#19968;&#20123;&#38590;&#20197;&#24314;&#27169;&#30340;&#24322;&#24120;&#21152;&#36895;&#24230;&#24773;&#20917;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;</title><link>https://arxiv.org/abs/2403.19736</link><description>&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21355;&#26143;&#29366;&#24577;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Physics-Informed Neural Networks for Satellite State Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19736
&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20026;&#21355;&#26143;&#20013;&#19968;&#20123;&#38590;&#20197;&#24314;&#27169;&#30340;&#24322;&#24120;&#21152;&#36895;&#24230;&#24773;&#20917;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#12298;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21355;&#26143;&#29366;&#24577;&#20272;&#35745;&#12299;&#35770;&#25991;&#32763;&#35793;&#25688;&#35201;&#65306;&#22826;&#31354;&#39046;&#22495;&#24847;&#35782;&#65288;SDA&#65289;&#32676;&#20307;&#36890;&#36807;&#23558;&#36712;&#36947;&#29366;&#24577;&#25311;&#21512;&#21040;&#22826;&#31354;&#30417;&#35270;&#32593;&#32476;&#65288;SSN&#65289;&#35266;&#27979;&#21040;&#30340;&#21355;&#26143;&#26469;&#23450;&#26399;&#36319;&#36394;&#21457;&#23556;&#21355;&#26143;&#12290;&#20026;&#20102;&#25311;&#21512;&#36825;&#26679;&#30340;&#36712;&#36947;&#65292;&#38656;&#35201;&#20934;&#30830;&#30340;&#20316;&#29992;&#20110;&#21355;&#26143;&#30340;&#21147;&#27169;&#22411;&#12290;&#36807;&#21435;&#20960;&#21313;&#24180;&#65292;&#20026;&#21355;&#26143;&#29366;&#24577;&#20272;&#35745;&#21644;&#20256;&#25773;&#24320;&#21457;&#20102;&#39640;&#36136;&#37327;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20272;&#31639;&#21644;&#20256;&#25773;&#38750;&#26426;&#21160;&#21355;&#26143;&#30340;&#36712;&#36947;&#29366;&#24577;&#26041;&#38754;&#34920;&#29616;&#38750;&#24120;&#20986;&#33394;&#65307;&#28982;&#32780;&#65292;&#21355;&#26143;&#21487;&#33021;&#36935;&#21040;&#20960;&#31867;&#19981;&#22826;&#22909;&#24314;&#27169;&#30340;&#24322;&#24120;&#21152;&#36895;&#24230;&#65292;&#27604;&#22914;&#20351;&#29992;&#20302;&#25512;&#21147;&#30005;&#25512;&#36827;&#26469;&#20462;&#25913;&#36712;&#36947;&#30340;&#21355;&#26143;&#12290;&#23545;&#20110;&#36825;&#20123;&#31867;&#21035;&#30340;&#21355;&#26143;&#65292;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26159;&#19968;&#20010;&#23453;&#36149;&#30340;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#23558;&#29289;&#29702;&#27169;&#22411;&#19982;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;DNN&#26159;&#39640;&#24230;&#34920;&#29616;&#21147;&#21644;&#22810;&#21151;&#33021;&#30340;&#20989;&#25968;&#36924;&#36817;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19736v1 Announce Type: cross  Abstract: The Space Domain Awareness (SDA) community routinely tracks satellites in orbit by fitting an orbital state to observations made by the Space Surveillance Network (SSN). In order to fit such orbits, an accurate model of the forces that are acting on the satellite is required. Over the past several decades, high-quality, physics-based models have been developed for satellite state estimation and propagation. These models are exceedingly good at estimating and propagating orbital states for non-maneuvering satellites; however, there are several classes of anomalous accelerations that a satellite might experience which are not well-modeled, such as satellites that use low-thrust electric propulsion to modify their orbit. Physics-Informed Neural Networks (PINNs) are a valuable tool for these classes of satellites as they combine physics models with Deep Neural Networks (DNNs), which are highly expressive and versatile function approximator
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#35821;&#21475;&#35821;&#29702;&#35299;&#30340;&#26032;&#35821;&#20041;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;MEDIA&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#26631;&#27880;&#24847;&#22270;&#26469;&#25193;&#23637;&#20854;&#29992;&#36884;&#21644;&#20351;&#29992;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.19727</link><description>&lt;p&gt;
&#29992;&#20110;&#27861;&#35821;&#21475;&#35821;&#29702;&#35299;&#30340;&#26032;&#35821;&#20041;&#20219;&#21153;MEDIA&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
New Semantic Task for the French Spoken Language Understanding MEDIA Benchmark
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19727
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#27861;&#35821;&#21475;&#35821;&#29702;&#35299;&#30340;&#26032;&#35821;&#20041;&#20219;&#21153;&#65292;&#36890;&#36807;&#22312;MEDIA&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#26631;&#27880;&#24847;&#22270;&#26469;&#25193;&#23637;&#20854;&#29992;&#36884;&#21644;&#20351;&#29992;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24847;&#22270;&#20998;&#31867;&#21644;&#27133;&#22635;&#20805;&#26159;&#21475;&#35821;&#29702;&#35299;&#65288;SLU&#65289;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#22312;&#22823;&#22810;&#25968;SLU&#31995;&#32479;&#20013;&#65292;&#36825;&#20123;&#20219;&#21153;&#30001;&#29420;&#31435;&#27169;&#22359;&#23454;&#29616;&#12290;&#36817;&#21313;&#20116;&#24180;&#26469;&#65292;&#25552;&#20986;&#20102;&#21516;&#26102;&#23454;&#29616;&#36825;&#20004;&#20010;&#20219;&#21153;&#24182;&#21033;&#29992;&#23427;&#20204;&#30456;&#20114;&#22686;&#24378;&#30340;&#27169;&#22411;&#12290;&#19968;&#20010;&#20351;&#29992;&#32852;&#21512;&#27169;&#22411;&#30340;&#22810;&#35821;&#35328;&#27169;&#22359;&#34987;&#35774;&#24819;&#29992;&#20110;&#20026;&#19968;&#20010;&#27431;&#27954;&#39033;&#30446;HumanE-AI-Net&#21019;&#24314;&#19968;&#20010;&#26053;&#28216;&#23545;&#35805;&#31995;&#32479;&#12290;&#24314;&#35758;&#32467;&#21512;&#22810;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;MEDIA&#25968;&#25454;&#38598;&#65292;&#26469;&#35757;&#32451;&#36825;&#20010;&#32852;&#21512;&#27169;&#22411;&#12290;MEDIA SLU&#25968;&#25454;&#38598;&#26159;&#30001;ELRA&#20174;2005&#24180;&#24320;&#22987;&#20998;&#21457;&#30340;&#27861;&#35821;&#25968;&#25454;&#38598;&#65292;&#20027;&#35201;&#34987;&#27861;&#22269;&#30740;&#31350;&#30028;&#20351;&#29992;&#65292;&#33258;2020&#24180;&#36215;&#29992;&#20110;&#23398;&#26415;&#30740;&#31350;&#20813;&#36153;&#20351;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#23427;&#21482;&#22312;&#27133;&#19978;&#26631;&#27880;&#32780;&#19981;&#26631;&#27880;&#24847;&#22270;&#12290;&#24050;&#26500;&#24314;&#20102;&#19968;&#20010;&#24102;&#26377;&#24847;&#22270;&#26631;&#27880;&#30340;&#22686;&#24378;&#29256;&#26412;&#30340;MEDIA&#65292;&#20197;&#25193;&#23637;&#20854;&#29992;&#36884;&#21040;&#26356;&#22810;&#20219;&#21153;&#21644;&#29992;&#20363;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#29992;&#20110;&#33719;&#24471;&#27492;&#22686;&#24378;&#29256;&#26412;&#30340;&#21322;&#33258;&#21160;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19727v1 Announce Type: cross  Abstract: Intent classification and slot-filling are essential tasks of Spoken Language Understanding (SLU). In most SLUsystems, those tasks are realized by independent modules. For about fifteen years, models achieving both of themjointly and exploiting their mutual enhancement have been proposed. A multilingual module using a joint modelwas envisioned to create a touristic dialogue system for a European project, HumanE-AI-Net. A combination ofmultiple datasets, including the MEDIA dataset, was suggested for training this joint model. The MEDIA SLU datasetis a French dataset distributed since 2005 by ELRA, mainly used by the French research community and free foracademic research since 2020. Unfortunately, it is annotated only in slots but not intents. An enhanced version ofMEDIA annotated with intents has been built to extend its use to more tasks and use cases. This paper presents thesemi-automatic methodology used to obtain this enhanced ver
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#29983;&#29289;&#21307;&#23398;&#27861;&#35821;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23545;&#20960;&#31181;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2403.19726</link><description>&lt;p&gt;
&#27861;&#35821;&#20020;&#24202;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#22522;&#20934;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Benchmark Evaluation of Clinical Named Entity Recognition in French
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35780;&#20272;&#20102;&#29983;&#29289;&#21307;&#23398;&#27861;&#35821;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#23545;&#20960;&#31181;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32972;&#26223;&#65306;&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#65288;MLMs&#65289;&#21560;&#24341;&#20102;&#25345;&#32493;&#30340;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#36890;&#36807;&#22312;&#29305;&#23450;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#35757;&#32451;&#25110;&#24494;&#35843;&#26469;&#36866;&#24212;&#19981;&#21516;&#30340;&#35821;&#35328;&#21644;&#23376;&#22495;&#65292;&#21516;&#26102;&#20445;&#25345;&#27604;&#29616;&#20195;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26356;&#36731;&#12290;&#26368;&#36817;&#65292;&#38024;&#23545;&#27861;&#35821;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#21457;&#24067;&#20102;&#20960;&#20010;MLMs&#65292;&#24182;&#19988;&#23454;&#39564;&#34920;&#26126;&#23427;&#20204;&#20248;&#20110;&#26631;&#20934;&#27861;&#35821;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#25552;&#20379;&#23545;&#21516;&#19968;&#35821;&#26009;&#24211;&#19978;&#25152;&#26377;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#30340;&#31995;&#32479;&#35780;&#20272;&#12290;&#30446;&#26631;&#65306;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#29983;&#29289;&#21307;&#23398;&#27861;&#35821;&#25513;&#30721;&#35821;&#35328;&#27169;&#22411;&#22312;&#20020;&#24202;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;&#26041;&#27861;&#21644;&#26448;&#26009;&#65306;&#25105;&#20204;&#35780;&#20272;&#20102;&#29983;&#29289;&#21307;&#23398;&#27169;&#22411;CamemBERT-bio&#21644;DrBERT&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#26631;&#20934;&#27861;&#35821;&#27169;&#22411;CamemBERT&#12289;FlauBERT&#21644;FrALBERT&#20197;&#21450;&#22810;&#35821;&#35328;mBERT&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19726v1 Announce Type: cross  Abstract: Background: Transformer-based language models have shown strong performance on many Natural LanguageProcessing (NLP) tasks. Masked Language Models (MLMs) attract sustained interest because they can be adaptedto different languages and sub-domains through training or fine-tuning on specific corpora while remaining lighterthan modern Large Language Models (LLMs). Recently, several MLMs have been released for the biomedicaldomain in French, and experiments suggest that they outperform standard French counterparts. However, nosystematic evaluation comparing all models on the same corpora is available. Objective: This paper presentsan evaluation of masked language models for biomedical French on the task of clinical named entity recognition.Material and methods: We evaluate biomedical models CamemBERT-bio and DrBERT and compare them tostandard French models CamemBERT, FlauBERT and FrALBERT as well as multilingual mBERT using three publicall
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#20843;&#31181;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#31867;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21478;&#22806;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#26356;&#30701;&#12289;&#35789;&#27719;&#21464;&#21270;&#26356;&#23569;&#65292;&#32780;&#20154;&#31867;&#29983;&#25104;&#20869;&#23481;&#21017;&#20351;&#29992;&#20102;&#19968;&#20123;&#29305;&#23450;&#39046;&#22495;&#30456;&#20851;&#20851;&#38190;&#35789;&#34987;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24573;&#30053;&#20102;&#12290;</title><link>https://arxiv.org/abs/2403.19725</link><description>&lt;p&gt;
MUGC&#65306;&#26426;&#22120;&#29983;&#25104;&#19982;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MUGC: Machine Generated versus User Generated Content Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#20843;&#31181;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#22312;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#31867;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#36827;&#34892;&#20102;&#27604;&#36739;&#35780;&#20272;&#65292;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#22312;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#20855;&#26377;&#36739;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#21478;&#22806;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#26356;&#30701;&#12289;&#35789;&#27719;&#21464;&#21270;&#26356;&#23569;&#65292;&#32780;&#20154;&#31867;&#29983;&#25104;&#20869;&#23481;&#21017;&#20351;&#29992;&#20102;&#19968;&#20123;&#29305;&#23450;&#39046;&#22495;&#30456;&#20851;&#20851;&#38190;&#35789;&#34987;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24573;&#30053;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20808;&#36827;&#30340;&#29616;&#20195;&#31995;&#32479;&#22914;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#19981;&#26029;&#22686;&#24378;&#20854;&#20135;&#29983;&#20196;&#20154;&#20449;&#26381;&#21644;&#36924;&#30495;&#20869;&#23481;&#30340;&#33021;&#21147;&#65292;&#21306;&#20998;&#29992;&#25143;&#29983;&#25104;&#19982;&#26426;&#22120;&#29983;&#25104;&#20869;&#23481;&#30340;&#38656;&#27714;&#21464;&#24471;&#36234;&#26469;&#36234;&#26126;&#26174;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;&#20843;&#31181;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#35780;&#20272;&#65292;&#20197;&#21306;&#20998;&#36328;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#65288;&#35799;&#27468;&#12289;&#25688;&#35201;&#21644;&#35770;&#25991;&#65289;&#20013;&#30340;&#26426;&#22120;&#29983;&#25104;&#21644;&#20154;&#31867;&#29983;&#25104;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20256;&#32479;&#26041;&#27861;&#22312;&#35782;&#21035;&#26426;&#22120;&#29983;&#25104;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#21453;&#26144;&#20102;&#20687;RoBERT&#36825;&#26679;&#30340;&#28909;&#38376;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#27880;&#24847;&#21040;&#65292;&#19982;&#20154;&#31867;&#29983;&#25104;&#20869;&#23481;&#30456;&#27604;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#24448;&#24448;&#26356;&#30701;&#65292;&#35789;&#27719;&#21464;&#21270;&#26356;&#23569;&#12290;&#34429;&#28982;&#20154;&#31867;&#24120;&#29992;&#30340;&#29305;&#23450;&#39046;&#22495;&#30456;&#20851;&#20851;&#38190;&#35789;&#34987;&#24403;&#21069;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24573;&#30053;&#20102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19725v1 Announce Type: cross  Abstract: As advanced modern systems like deep neural networks (DNNs) and generative AI continue to enhance their capabilities in producing convincing and realistic content, the need to distinguish between user-generated and machine generated content is becoming increasingly evident. In this research, we undertake a comparative evaluation of eight traditional machine-learning algorithms to distinguish between machine-generated and human-generated data across three diverse datasets: Poems, Abstracts, and Essays. Our results indicate that traditional methods demonstrate a high level of accuracy in identifying machine-generated data, reflecting the documented effectiveness of popular pre-trained models like RoBERT. We note that machine-generated texts tend to be shorter and exhibit less word variety compared to human-generated content. While specific domain-related keywords commonly utilized by humans, albeit disregarded by current LLMs (Large Lang
&lt;/p&gt;</description></item><item><title>HGT&#26694;&#26550;&#32467;&#21512;&#20102;&#24322;&#36136;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#21644;&#22810;&#31890;&#24230;&#33258;&#30417;&#30563;HG&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.19723</link><description>&lt;p&gt;
HGT&#65306;&#21033;&#29992;&#24322;&#36136;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
HGT: Leveraging Heterogeneous Graph-enhanced Large Language Models for Few-shot Complex Table Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19723
&lt;/p&gt;
&lt;p&gt;
HGT&#26694;&#26550;&#32467;&#21512;&#20102;&#24322;&#36136;&#22270;&#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#36719;&#25552;&#31034;&#21644;&#22810;&#31890;&#24230;&#33258;&#30417;&#30563;HG&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#23454;&#29616;&#20102;&#23569;&#26679;&#26412;&#22797;&#26434;&#34920;&#26684;&#29702;&#35299;&#20219;&#21153;&#30340;&#26368;&#26032;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#29702;&#35299; (TU) &#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#38754;&#20020;&#25163;&#21160;&#26631;&#35760;&#34920;&#26684;&#30340;&#31232;&#32570;&#24615;&#21644;&#22797;&#26434;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; HGT &#26694;&#26550;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#24322;&#36136;&#22270; (HG) &#22686;&#24378;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM)&#65292;&#29992;&#20110;&#35299;&#20915;&#23569;&#26679;&#26412; TU &#20219;&#21153;&#12290;&#23427;&#36890;&#36807;&#36719;&#25552;&#31034;&#21644;&#25351;&#23548;&#36716;&#25442;&#23558;&#34920;&#26684;&#35821;&#20041;&#19982;LLM&#30340;&#21442;&#25968;&#21270;&#30693;&#35782;&#23545;&#40784;&#65292;&#24182;&#36890;&#36807;&#28041;&#21450;&#19977;&#31181;&#26032;&#30340;&#22810;&#31890;&#24230;&#33258;&#30417;&#30563;HG&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#22810;&#20219;&#21153;&#39044;&#35757;&#32451;&#26041;&#26696;&#22788;&#29702;&#22797;&#26434;&#34920;&#26684;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#36890;&#36807;&#23454;&#35777;&#26041;&#27861;&#23637;&#31034;&#20102;HGT&#30340;&#26377;&#25928;&#24615;&#65292;&#34920;&#26126;&#23427;&#22312;&#23569;&#26679;&#26412;&#22797;&#26434;TU&#26041;&#38754;&#30340;&#34920;&#29616;&#20248;&#20110;SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19723v1 Announce Type: cross  Abstract: Table understanding (TU) has achieved promising advancements, but it faces the challenges of the scarcity of manually labeled tables and the presence of complex table structures.To address these challenges, we propose HGT, a framework with a heterogeneous graph (HG)-enhanced large language model (LLM) to tackle few-shot TU tasks.It leverages the LLM by aligning the table semantics with the LLM's parametric knowledge through soft prompts and instruction turning and deals with complex tables by a multi-task pre-training scheme involving three novel multi-granularity self-supervised HG pre-training objectives.We empirically demonstrate the effectiveness of HGT, showing that it outperforms the SOTA for few-shot complex TU on several benchmarks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#31283;&#20581;&#20027;&#25104;&#20998;&#20998;&#26512;(RPCA)&#36827;&#34892;&#22122;&#22768;&#38477;&#20302;&#21644;&#24322;&#24120;&#20540;&#25490;&#38500;&#65292;&#20197;&#21450;&#20248;&#21270;&#20256;&#24863;&#22120;&#25918;&#32622;(OSP)&#36827;&#34892;&#26377;&#25928;&#25968;&#25454;&#21387;&#32553;&#21644;&#23384;&#20648;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20248;&#21270;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#30340;&#22823;&#25968;&#25454;&#39044;&#27979;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.19721</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#31283;&#20581;&#39044;&#27979;&#20998;&#26512;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Computationally and Memory-Efficient Robust Predictive Analytics Using Big Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19721
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#31283;&#20581;&#20027;&#25104;&#20998;&#20998;&#26512;(RPCA)&#36827;&#34892;&#22122;&#22768;&#38477;&#20302;&#21644;&#24322;&#24120;&#20540;&#25490;&#38500;&#65292;&#20197;&#21450;&#20248;&#21270;&#20256;&#24863;&#22120;&#25918;&#32622;(OSP)&#36827;&#34892;&#26377;&#25928;&#25968;&#25454;&#21387;&#32553;&#21644;&#23384;&#20648;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20248;&#21270;&#35745;&#31639;&#21644;&#20869;&#23384;&#25928;&#29575;&#30340;&#22823;&#25968;&#25454;&#39044;&#27979;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24403;&#21069;&#25968;&#25454;&#23494;&#38598;&#30340;&#26102;&#20195;&#65292;&#22823;&#25968;&#25454;&#24050;&#32463;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#37325;&#35201;&#36164;&#20135;&#65292;&#20026;&#24320;&#21457;&#22522;&#20110;&#25968;&#25454;&#30340;&#27169;&#22411;&#24182;&#28145;&#20837;&#25506;&#32034;&#21508;&#31181;&#26410;&#30693;&#39046;&#22495;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#25968;&#25454;&#36827;&#34892;&#25968;&#25454;&#19981;&#30830;&#23450;&#24615;&#12289;&#23384;&#20648;&#38480;&#21046;&#21644;&#39044;&#27979;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#21033;&#29992;&#31283;&#20581;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;RPCA&#65289;&#26377;&#25928;&#38477;&#22122;&#21644;&#21435;&#38500;&#31163;&#32676;&#20540;&#65292;&#20197;&#21450;&#20248;&#21270;&#20256;&#24863;&#22120;&#25918;&#32622;&#65288;OSP&#65289;&#23454;&#29616;&#39640;&#25928;&#25968;&#25454;&#21387;&#32553;&#21644;&#23384;&#20648;&#12290;&#25152;&#25552;&#20986;&#30340;OSP&#25216;&#26415;&#33021;&#22815;&#23454;&#29616;&#25968;&#25454;&#21387;&#32553;&#65292;&#21516;&#26102;&#20943;&#23569;&#23384;&#20648;&#38656;&#27714;&#65292;&#19988;&#20449;&#24687;&#25439;&#22833;&#19981;&#22823;&#12290;&#34429;&#28982;RPCA&#20026;&#39640;&#32500;&#25968;&#25454;&#31649;&#29702;&#25552;&#20379;&#20102;&#27604;&#20256;&#32479;&#20027;&#25104;&#20998;&#20998;&#26512;&#65288;PCA&#65289;&#26356;&#22909;&#30340;&#36873;&#25321;&#65292;&#20294;&#26412;&#30740;&#31350;&#30340;&#33539;&#22260;&#21017;&#25193;&#23637;&#20102;&#20854;&#24212;&#29992;&#33539;&#22260;&#65292;&#19987;&#27880;&#20110;&#36866;&#29992;&#20110;&#23454;&#26102;&#28023;&#37327;&#25968;&#25454;&#38598;&#30340;&#31283;&#20581;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19721v1 Announce Type: cross  Abstract: In the current data-intensive era, big data has become a significant asset for Artificial Intelligence (AI), serving as a foundation for developing data-driven models and providing insight into various unknown fields. This study navigates through the challenges of data uncertainties, storage limitations, and predictive data-driven modeling using big data. We utilize Robust Principal Component Analysis (RPCA) for effective noise reduction and outlier elimination, and Optimal Sensor Placement (OSP) for efficient data compression and storage. The proposed OSP technique enables data compression without substantial information loss while simultaneously reducing storage needs. While RPCA offers an enhanced alternative to traditional Principal Component Analysis (PCA) for high-dimensional data management, the scope of this work extends its utilization, focusing on robust, data-driven modeling applicable to huge data sets in real-time. For tha
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#20132;&#20114;&#26085;&#24535;&#30340;&#29992;&#25143;&#37325;&#32452;&#25968;&#25454;&#26469;&#24320;&#21457;&#33258;&#21160;&#25552;&#31034;&#37325;&#32452;&#27169;&#22411;&#65292;CAPR&#26694;&#26550;&#21019;&#26032;&#24615;&#22320;&#23558;&#29992;&#25143;&#33021;&#21147;&#25972;&#21512;&#21040;&#25552;&#31034;&#37325;&#32452;&#36807;&#31243;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.19716</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#33021;&#21147;&#24863;&#30693;&#25552;&#31034;&#37325;&#32452;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Capability-aware Prompt Reformulation Learning for Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19716
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#20132;&#20114;&#26085;&#24535;&#30340;&#29992;&#25143;&#37325;&#32452;&#25968;&#25454;&#26469;&#24320;&#21457;&#33258;&#21160;&#25552;&#31034;&#37325;&#32452;&#27169;&#22411;&#65292;CAPR&#26694;&#26550;&#21019;&#26032;&#24615;&#22320;&#23558;&#29992;&#25143;&#33021;&#21147;&#25972;&#21512;&#21040;&#25552;&#31034;&#37325;&#32452;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#33402;&#26415;&#21019;&#20316;&#39046;&#22495;&#20013;&#30340;&#38761;&#21629;&#24615;&#24037;&#20855;&#65292;&#20026;&#23558;&#25991;&#26412;&#25552;&#31034;&#36716;&#21270;&#20026;&#35270;&#35273;&#33402;&#26415;&#25552;&#20379;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#20415;&#21033;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#31995;&#32479;&#30340;&#25928;&#21147;&#19982;&#29992;&#25143;&#25552;&#20379;&#30340;&#25552;&#31034;&#36136;&#37327;&#23494;&#20999;&#30456;&#20851;&#65292;&#36825;&#24120;&#24120;&#23545;&#19981;&#29087;&#24713;&#25552;&#31034;&#21046;&#20316;&#30340;&#29992;&#25143;&#26500;&#25104;&#25361;&#25112;&#12290;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#20132;&#20114;&#26085;&#24535;&#30340;&#29992;&#25143;&#37325;&#32452;&#25968;&#25454;&#26469;&#24320;&#21457;&#33258;&#21160;&#25552;&#31034;&#37325;&#32452;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#26085;&#24535;&#30340;&#28145;&#20837;&#20998;&#26512;&#34920;&#26126;&#65292;&#29992;&#25143;&#25552;&#31034;&#30340;&#37325;&#32452;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#20010;&#20307;&#29992;&#25143;&#30340;&#33021;&#21147;&#65292;&#23548;&#33268;&#37325;&#32452;&#23545;&#30340;&#36136;&#37327;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#20026;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#20123;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#33021;&#21147;&#24863;&#30693;&#25552;&#31034;&#37325;&#32452;&#65288;CAPR&#65289;&#26694;&#26550;&#12290;CAPR&#21019;&#26032;&#24615;&#22320;&#36890;&#36807;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#23558;&#29992;&#25143;&#33021;&#21147;&#25972;&#21512;&#21040;&#37325;&#32452;&#36807;&#31243;&#20013;&#65306;&#26377;&#26465;&#20214;&#30340;&#25552;&#31034;&#37325;&#32452;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19716v1 Announce Type: cross  Abstract: Text-to-image generation systems have emerged as revolutionary tools in the realm of artistic creation, offering unprecedented ease in transforming textual prompts into visual art. However, the efficacy of these systems is intricately linked to the quality of user-provided prompts, which often poses a challenge to users unfamiliar with prompt crafting. This paper addresses this challenge by leveraging user reformulation data from interaction logs to develop an automatic prompt reformulation model. Our in-depth analysis of these logs reveals that user prompt reformulation is heavily dependent on the individual user's capability, resulting in significant variance in the quality of reformulation pairs. To effectively use this data for training, we introduce the Capability-aware Prompt Reformulation (CAPR) framework. CAPR innovatively integrates user capability into the reformulation process through two key components: the Conditional Refo
&lt;/p&gt;</description></item><item><title>STRUM-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#23646;&#24615;&#21270;&#12289;&#32467;&#26500;&#21270;&#21644;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#24182;&#31361;&#20986;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#25110;&#22266;&#23450;&#23646;&#24615;&#21015;&#34920;&#65292;&#20855;&#26377;&#39640;&#21534;&#21520;&#37327;&#21644;&#23567;&#20307;&#31215;&#12290;</title><link>https://arxiv.org/abs/2403.19710</link><description>&lt;p&gt;
STRUM-LLM: &#23646;&#24615;&#21270;&#21644;&#32467;&#26500;&#21270;&#23545;&#27604;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
STRUM-LLM: Attributed and Structured Contrastive Summarization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19710
&lt;/p&gt;
&lt;p&gt;
STRUM-LLM&#25552;&#20986;&#20102;&#19968;&#31181;&#29983;&#25104;&#23646;&#24615;&#21270;&#12289;&#32467;&#26500;&#21270;&#21644;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#25688;&#35201;&#30340;&#26041;&#27861;&#65292;&#35782;&#21035;&#24182;&#31361;&#20986;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#25110;&#22266;&#23450;&#23646;&#24615;&#21015;&#34920;&#65292;&#20855;&#26377;&#39640;&#21534;&#21520;&#37327;&#21644;&#23567;&#20307;&#31215;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#32463;&#24120;&#22312;&#20004;&#20010;&#36873;&#39033;&#65288;A vs B&#65289;&#20043;&#38388;&#20570;&#20915;&#31574;&#26102;&#24863;&#21040;&#22256;&#38590;&#65292;&#22240;&#20026;&#36825;&#36890;&#24120;&#38656;&#35201;&#22312;&#22810;&#20010;&#32593;&#39029;&#19978;&#36827;&#34892;&#32791;&#26102;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STRUM-LLM&#65292;&#36890;&#36807;&#29983;&#25104;&#24102;&#23646;&#24615;&#12289;&#32467;&#26500;&#21270;&#21644;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#25688;&#35201;&#65292;&#31361;&#20986;&#20004;&#20010;&#36873;&#39033;&#20043;&#38388;&#30340;&#20851;&#38190;&#24046;&#24322;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#12290;STRUM-LLM&#35782;&#21035;&#20102;&#26377;&#24110;&#21161;&#30340;&#23545;&#27604;&#65306;&#20004;&#20010;&#36873;&#39033;&#22312;&#21738;&#20123;&#29305;&#23450;&#23646;&#24615;&#19978;&#26377;&#26174;&#33879;&#24046;&#24322;&#65292;&#20197;&#21450;&#26368;&#26377;&#21487;&#33021;&#24433;&#21709;&#29992;&#25143;&#20915;&#31574;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#65292;&#24182;&#19981;&#38656;&#35201;&#20219;&#20309;&#20154;&#24037;&#26631;&#35760;&#30340;&#25968;&#25454;&#25110;&#22266;&#23450;&#23646;&#24615;&#21015;&#34920;&#20316;&#20026;&#30417;&#30563;&#12290;STRUM-LLM&#23558;&#25152;&#26377;&#25552;&#21462;&#30340;&#20869;&#23481;&#23646;&#24615;&#21270;&#65292;&#20197;&#21450;&#25991;&#26412;&#35777;&#25454;&#65292;&#19988;&#19981;&#38480;&#21046;&#20854;&#22788;&#29702;&#30340;&#36755;&#20837;&#26469;&#28304;&#30340;&#38271;&#24230;&#12290;STRUM-LLM Distilled&#30340;&#21534;&#21520;&#37327;&#27604;&#20855;&#26377;&#30456;&#20284;&#24615;&#33021;&#30340;&#27169;&#22411;&#39640;100&#20493;&#65292;&#21516;&#26102;&#20307;&#31215;&#23567;10&#20493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19710v1 Announce Type: cross  Abstract: Users often struggle with decision-making between two options (A vs B), as it usually requires time-consuming research across multiple web pages. We propose STRUM-LLM that addresses this challenge by generating attributed, structured, and helpful contrastive summaries that highlight key differences between the two options. STRUM-LLM identifies helpful contrast: the specific attributes along which the two options differ significantly and which are most likely to influence the user's decision. Our technique is domain-agnostic, and does not require any human-labeled data or fixed attribute list as supervision. STRUM-LLM attributes all extractions back to the input sources along with textual evidence, and it does not have a limit on the length of input sources that it can process. STRUM-LLM Distilled has 100x more throughput than the models with comparable performance while being 10x smaller. In this paper, we provide extensive evaluations
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36882;&#24402;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#36866;&#37197;&#22330;&#26223;&#19979;&#38477;&#20302;&#27599;&#20010;&#20219;&#21153;&#30340;&#21442;&#25968;&#24320;&#38144;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#36866;&#37197;&#22120;&#26041;&#27861;&#21644;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#22522;&#32447;</title><link>https://arxiv.org/abs/2403.19709</link><description>&lt;p&gt;
&#39640;&#25928;&#22810;&#20219;&#21153;&#35843;&#25972;&#22823;&#22411;&#35821;&#38899;&#27169;&#22411;&#30340;&#20998;&#23618;&#36882;&#24402;&#36866;&#37197;&#22120;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Recurrent Adapters for Efficient Multi-Task Adaptation of Large Speech Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19709
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#36882;&#24402;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#36866;&#37197;&#22330;&#26223;&#19979;&#38477;&#20302;&#27599;&#20010;&#20219;&#21153;&#30340;&#21442;&#25968;&#24320;&#38144;&#65292;&#21516;&#26102;&#20445;&#25345;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20248;&#20110;&#20808;&#21069;&#30340;&#36866;&#37197;&#22120;&#26041;&#27861;&#21644;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21442;&#25968;&#39640;&#25928;&#30340;&#36866;&#37197;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#35757;&#32451;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#30340;&#20851;&#38190;&#26426;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#36866;&#37197;&#22120;&#27169;&#22359;&#65292;&#22312;&#22823;&#35268;&#27169;&#22810;&#20219;&#21153;&#36866;&#37197;&#22330;&#26223;&#19979;&#20855;&#26377;&#26356;&#22909;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#36866;&#37197;&#22120;&#22312;&#36866;&#37197;&#22120;&#21442;&#25968;&#20998;&#37197;&#26041;&#38754;&#26159;&#20998;&#23618;&#30340;&#12290;&#36866;&#37197;&#22120;&#30001;&#19968;&#20010;&#20849;&#20139;&#30340;&#25511;&#21046;&#32593;&#32476;&#21644;&#22810;&#20010;&#20219;&#21153;&#32423;&#36866;&#37197;&#22120;&#22836;&#32452;&#25104;&#65292;&#20197;&#20943;&#23569;&#27599;&#20010;&#20219;&#21153;&#30340;&#21442;&#25968;&#24320;&#38144;&#65292;&#32780;&#19981;&#20250;&#24433;&#21709;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#36866;&#37197;&#22120;&#36824;&#26159;&#36882;&#24402;&#30340;&#65292;&#22240;&#27492;&#25972;&#20010;&#36866;&#37197;&#22120;&#21442;&#25968;&#22312;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19981;&#21516;&#23618;&#20043;&#38388;&#34987;&#37325;&#29992;&#12290;&#25105;&#20204;&#30340;&#20998;&#23618;&#36882;&#24402;&#36866;&#37197;&#22120;&#65288;HRA&#65289;&#22312;&#21333;&#20219;&#21153;&#21644;&#22810;&#20219;&#21153;&#36866;&#37197;&#35774;&#32622;&#20013;&#37117;&#20248;&#20110;&#20808;&#21069;&#30340;&#22522;&#20110;&#36866;&#37197;&#22120;&#30340;&#26041;&#27861;&#20197;&#21450;&#23436;&#25972;&#27169;&#22411;&#24494;&#35843;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19709v1 Announce Type: cross  Abstract: Parameter efficient adaptation methods have become a key mechanism to train large pre-trained models for downstream tasks. However, their per-task parameter overhead is considered still high when the number of downstream tasks to adapt for is large. We introduce an adapter module that has a better efficiency in large scale multi-task adaptation scenario. Our adapter is hierarchical in terms of how the adapter parameters are allocated. The adapter consists of a single shared controller network and multiple task-level adapter heads to reduce the per-task parameter overhead without performance regression on downstream tasks. The adapter is also recurrent so the entire adapter parameters are reused across different layers of the pre-trained model. Our Hierarchical Recurrent Adapter (HRA) outperforms the previous adapter-based approaches as well as full model fine-tuning baseline in both single and multi-task adaptation settings when evalua
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#21363;&#20351;&#32570;&#20047;&#35270;&#35273;&#36755;&#20837;&#65292;&#21033;&#29992;&#25152;&#26377;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24674;&#22797;&#22823;&#37096;&#20998;VLM&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#35821;&#35328;&#36890;&#36807;&#25552;&#20379;&#23545;&#20808;&#21069;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#35775;&#38382;&#26469;&#23545;&#23398;&#20064;&#26032;&#20219;&#21153;&#26377;&#36129;&#29486;</title><link>https://arxiv.org/abs/2403.19669</link><description>&lt;p&gt;
&#20998;&#26512;&#35821;&#35328;&#21644;&#35270;&#35273;&#22312;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Analyzing the Roles of Language and Vision in Learning from Limited Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19669
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#21457;&#29616;&#21363;&#20351;&#32570;&#20047;&#35270;&#35273;&#36755;&#20837;&#65292;&#21033;&#29992;&#25152;&#26377;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#24674;&#22797;&#22823;&#37096;&#20998;VLM&#30340;&#24615;&#33021;&#65292;&#34920;&#26126;&#35821;&#35328;&#36890;&#36807;&#25552;&#20379;&#23545;&#20808;&#21069;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#35775;&#38382;&#26469;&#23545;&#23398;&#20064;&#26032;&#20219;&#21153;&#26377;&#36129;&#29486;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19669v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#35821;&#35328;&#26159;&#21542;&#26377;&#21161;&#20110;&#29702;&#35299;&#35270;&#35273;&#19990;&#30028;&#65311;&#23454;&#38469;&#35266;&#23519;&#19990;&#30028;&#38656;&#35201;&#30475;&#21040;&#23454;&#38469;&#24773;&#20917;&#65292;&#32780;&#19981;&#26159;&#29992;&#25991;&#23383;&#25551;&#36848;&#21527;&#65311;&#20851;&#20110;&#26234;&#33021;&#26412;&#36136;&#30340;&#36825;&#20123;&#22522;&#26412;&#38382;&#39064;&#24456;&#38590;&#22238;&#31572;&#65292;&#22240;&#20026;&#25105;&#20204;&#21482;&#26377;&#19968;&#20010;&#26234;&#33021;&#31995;&#32479;&#30340;&#20363;&#23376;&#8212;&#8212;&#20154;&#31867;&#8212;&#8212;&#20197;&#21450;&#26377;&#38480;&#30340;&#29420;&#31435;&#35821;&#35328;&#25110;&#35270;&#35273;&#30340;&#26696;&#20363;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#20986;&#22797;&#26434;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#25506;&#32034;&#35821;&#35328;&#21644;&#35270;&#35273;&#23545;&#20110;&#23398;&#20064;&#19990;&#30028;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#20174;&#36825;&#20123;&#27169;&#22411;&#30340;&#35748;&#30693;&#26550;&#26500;&#20013;&#20999;&#38500;&#32452;&#20214;&#65292;&#20197;&#30830;&#23450;&#23427;&#20204;&#23545;&#20174;&#26377;&#38480;&#25968;&#25454;&#20013;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21033;&#29992;&#25152;&#26377;&#32452;&#20214;&#30340;&#35821;&#35328;&#27169;&#22411;&#24674;&#22797;&#20102;&#22823;&#37096;&#20998;VLM&#30340;&#24615;&#33021;&#65292;&#23613;&#31649;&#23427;&#32570;&#20047;&#35270;&#35273;&#36755;&#20837;&#65292;&#32780;&#35821;&#35328;&#20284;&#20046;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#23545;&#20808;&#21069;&#30693;&#35782;&#21644;&#25512;&#29702;&#30340;&#35775;&#38382;&#26469;&#23454;&#29616;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19669v1 Announce Type: cross  Abstract: Does language help make sense of the visual world? How important is it to actually see the world rather than having it described with words? These basic questions about the nature of intelligence have been difficult to answer because we only had one example of an intelligent system -- humans -- and limited access to cases that isolated language or vision. However, the development of sophisticated Vision-Language Models (VLMs) by artificial intelligence researchers offers us new opportunities to explore the contributions that language and vision make to learning about the world. We ablate components from the cognitive architecture of these models to identify their contributions to learning new tasks from limited data. We find that a language model leveraging all components recovers a majority of a VLM's performance, despite its lack of visual input, and that language seems to allow this by providing access to prior knowledge and reasoni
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#26816;&#27979;&#24182;&#32416;&#27491;&#22269;&#23478;&#26292;&#21147;&#27515;&#20129;&#25253;&#21578;&#31995;&#32479;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#33258;&#26432;&#21361;&#26426;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.19432</link><description>&lt;p&gt;
&#36890;&#36807;&#27515;&#22240;&#35843;&#26597;&#31508;&#35760;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#26816;&#27979;&#25581;&#31034;&#33258;&#26432;&#21407;&#22240;&#30340;&#35823;&#24402;&#22240;
&lt;/p&gt;
&lt;p&gt;
Uncovering Misattributed Suicide Causes through Annotation Inconsistency Detection in Death Investigation Notes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19432
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#26816;&#27979;&#24182;&#32416;&#27491;&#22269;&#23478;&#26292;&#21147;&#27515;&#20129;&#25253;&#21578;&#31995;&#32479;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#25552;&#39640;&#20102;&#33258;&#26432;&#21361;&#26426;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#20934;&#30830;&#24615;&#23545;&#31185;&#23398;&#30740;&#31350;&#21644;&#25919;&#31574;&#21046;&#23450;&#33267;&#20851;&#37325;&#35201;&#12290;&#22269;&#23478;&#26292;&#21147;&#27515;&#20129;&#25253;&#21578;&#31995;&#32479;&#65288;NVDRS&#65289;&#25968;&#25454;&#34987;&#24191;&#27867;&#29992;&#20110;&#21457;&#29616;&#27515;&#20129;&#30340;&#27169;&#24335;&#21644;&#21407;&#22240;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;NVDRS&#20869;&#23384;&#22312;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#21487;&#33021;&#24433;&#21709;&#38169;&#35823;&#30340;&#33258;&#26432;&#21407;&#22240;&#24402;&#22240;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#39564;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#26041;&#27861;&#26469;&#26816;&#27979;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#37319;&#29992;&#31867;&#20284;&#20132;&#21449;&#39564;&#35777;&#30340;&#33539;&#24335;&#26469;&#35782;&#21035;&#26377;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;2003&#24180;&#33267;2020&#24180;&#38388;&#20174;NVDRS&#20013;&#30340;267,804&#36215;&#33258;&#26432;&#27515;&#20129;&#26696;&#20363;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23558;&#30446;&#26631;&#24030;&#30340;&#25968;&#25454;&#32435;&#20837;&#35757;&#32451;&#33258;&#26432;&#21361;&#26426;&#20998;&#31867;&#22120;&#65292;&#20351;&#24471;&#22312;&#30446;&#26631;&#24030;&#27979;&#35797;&#38598;&#19978;&#30340;F-1&#20998;&#25968;&#22686;&#21152;&#20102;5.4&#65285;&#65292;&#22312;&#20854;&#20182;&#24030;&#27979;&#35797;&#38598;&#19978;&#38477;&#20302;&#20102;1.1&#65285;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;NVDRS&#27515;&#22240;&#35843;&#26597;&#31508;&#35760;&#20013;&#30340;&#27880;&#37322;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#38382;&#39064;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19432v1 Announce Type: cross  Abstract: Data accuracy is essential for scientific research and policy development. The National Violent Death Reporting System (NVDRS) data is widely used for discovering the patterns and causes of death. Recent studies suggested the annotation inconsistencies within the NVDRS and the potential impact on erroneous suicide-cause attributions. We present an empirical Natural Language Processing (NLP) approach to detect annotation inconsistencies and adopt a cross-validation-like paradigm to identify problematic instances. We analyzed 267,804 suicide death incidents between 2003 and 2020 from the NVDRS. Our results showed that incorporating the target state's data into training the suicide-crisis classifier brought an increase of 5.4% to the F-1 score on the target state's test set and a decrease of 1.1% on other states' test set. To conclude, we demonstrated the annotation inconsistencies in NVDRS's death investigation notes, identified problema
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#26469;&#33258;&#25105;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#38382;&#32773;&#36890;&#36807;&#35810;&#38382;&#35282;&#33394;&#25198;&#28436;&#32773;&#26469;&#24341;&#20986;&#20559;&#22909;&#65292;&#20174;&#32780;&#36845;&#20195;&#24494;&#35843;&#20197;&#22686;&#21152;&#20219;&#21153;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#27010;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.19154</link><description>&lt;p&gt;
STaR-GATE: &#25945;&#25480;&#35821;&#35328;&#27169;&#22411;&#35810;&#38382;&#28548;&#28165;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
STaR-GATE: Teaching Language Models to Ask Clarifying Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19154
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22870;&#21169;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#26469;&#33258;&#25105;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#25552;&#38382;&#32773;&#36890;&#36807;&#35810;&#38382;&#35282;&#33394;&#25198;&#28436;&#32773;&#26469;&#24341;&#20986;&#20559;&#22909;&#65292;&#20174;&#32780;&#36845;&#20195;&#24494;&#35843;&#20197;&#22686;&#21152;&#20219;&#21153;&#39640;&#36136;&#37327;&#21709;&#24212;&#30340;&#27010;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#25552;&#31034;&#35821;&#35328;&#27169;&#22411;&#23436;&#25104;&#20219;&#21153;&#26102;&#65292;&#29992;&#25143;&#36890;&#24120;&#20250;&#36951;&#28431;&#37325;&#35201;&#30340;&#32454;&#33410;&#12290;&#34429;&#28982;&#25552;&#38382;&#21487;&#20197;&#35299;&#20915;&#36825;&#31181;&#27495;&#20041;&#65292;&#20294;&#27169;&#22411;&#24448;&#24448;&#24456;&#38590;&#25552;&#20986;&#22909;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#22870;&#21169;&#27169;&#22411;&#29983;&#25104;&#26377;&#29992;&#38382;&#39064;&#26469;&#33258;&#25105;&#25913;&#36827;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#26041;&#27861;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;STaR-GATE&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;25,500&#20010;&#29420;&#29305;&#20154;&#29289;-&#20219;&#21153;&#25552;&#31034;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20197;&#27169;&#25311;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;--&#25552;&#38382;&#32773;--&#19982;&#19968;&#20010;&#20854;&#20559;&#22909;&#26410;&#30693;&#30340;&#35282;&#33394;&#25198;&#28436;&#32773;&#20043;&#38388;&#30340;&#23545;&#35805;&#12290;&#36890;&#36807;&#25552;&#38382;&#65292;&#25552;&#38382;&#32773;&#20174;&#35282;&#33394;&#25198;&#28436;&#32773;&#37027;&#37324;&#24341;&#20986;&#20559;&#22909;&#12290;&#25552;&#38382;&#32773;&#22312;&#37027;&#20123;&#22686;&#21152;&#39640;&#36136;&#37327;&#21709;&#24212;&#27010;&#29575;&#30340;&#38382;&#39064;&#19978;&#36827;&#34892;&#36845;&#20195;&#24494;&#35843;&#65292;&#36825;&#20123;&#38382;&#39064;&#26159;&#30001;&#20855;&#26377;&#23545;&#35282;&#33394;&#25198;&#28436;&#32773;&#35775;&#38382;&#26435;&#38480;&#30340;&#39044;&#35328;&#32773;&#29983;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19154v1 Announce Type: cross  Abstract: When prompting language models to complete a task, users often leave important aspects unsaid. While asking questions could resolve this ambiguity \citep[GATE;][]{li2023eliciting}, models often struggle to ask good questions. We explore a language model's ability to self-improve \citep[STaR;][]{zelikman2022star} by rewarding the model for generating useful questions -- a simple method we dub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task prompts to simulate conversations between a pretrained language model -- the \texttt{Questioner} -- and a \texttt{Roleplayer} whose preferences are unknown to the \texttt{Questioner}. By asking questions, the \texttt{Questioner} elicits preferences from the \texttt{Roleplayer}. The \texttt{Questioner} is iteratively finetuned on questions that increase the probability of high-quality responses to the task, which are generated by an \texttt{Oracle} with access to the \texttt{Ro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#20013;&#24515;&#30340;&#24314;&#31569;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;&#65292;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.19060</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#26045;&#24037;&#26426;&#22120;&#20154;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;
&lt;/p&gt;
&lt;p&gt;
Towards Human-Centered Construction Robotics: An RL-Driven Companion Robot For Contextually Assisting Carpentry Workers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19060
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#20013;&#24515;&#30340;&#24314;&#31569;&#26426;&#22120;&#20154;&#26041;&#27861;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#21161;&#25163;&#26426;&#22120;&#20154;&#20026;&#26408;&#24037;&#21171;&#21160;&#32773;&#25552;&#20379;&#29615;&#22659;&#19978;&#19979;&#25991;&#21327;&#21161;&#65292;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#20010;&#20805;&#28385;&#27963;&#21147;&#30340;&#24314;&#31569;&#34892;&#19994;&#20013;&#65292;&#20256;&#32479;&#30340;&#26426;&#22120;&#20154;&#38598;&#25104;&#20027;&#35201;&#38598;&#20013;&#22312;&#33258;&#21160;&#21270;&#29305;&#23450;&#20219;&#21153;&#65292;&#36890;&#24120;&#24573;&#30053;&#20102;&#24314;&#31569;&#24037;&#20316;&#27969;&#31243;&#20013;&#20154;&#31867;&#22240;&#32032;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#21270;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#20154;&#20026;&#26412;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#8220;&#24037;&#20316;&#20276;&#20387;&#28459;&#28216;&#22120;&#8221;&#65292;&#26088;&#22312;&#21327;&#21161;&#24314;&#31569;&#24037;&#20154;&#23436;&#25104;&#20854;&#29616;&#26377;&#23454;&#36341;&#65292;&#26088;&#22312;&#22686;&#24378;&#23433;&#20840;&#24615;&#21644;&#24037;&#20316;&#27969;&#31243;&#30340;&#27969;&#30021;&#24615;&#65292;&#21516;&#26102;&#23562;&#37325;&#24314;&#31569;&#21171;&#21160;&#30340;&#25216;&#26415;&#24615;&#36136;&#12290;&#25105;&#20204;&#23545;&#22312;&#26408;&#24037;&#27169;&#26495;&#24037;&#31243;&#20013;&#37096;&#32626;&#26426;&#22120;&#20154;&#31995;&#32479;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;&#19968;&#20010;&#21407;&#22411;&#65292;&#36890;&#36807;&#29615;&#22659;&#30456;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#39537;&#21160;&#27169;&#22359;&#21270;&#26694;&#26550;&#65292;&#37325;&#28857;&#24378;&#35843;&#20102;&#22312;&#21160;&#24577;&#29615;&#22659;&#20013;&#30340;&#26426;&#21160;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#33298;&#36866;&#30340;&#24037;&#20154;-&#26426;&#22120;&#20154;&#21327;&#20316;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25512;&#36827;&#20102;&#26426;&#22120;&#20154;&#22312;&#24314;&#31569;&#20013;&#30340;&#24212;&#29992;&#65292;&#20513;&#23548;&#21327;&#20316;&#27169;&#22411;&#65292;&#20854;&#20013;&#33258;&#36866;&#24212;&#26426;&#22120;&#20154;&#25903;&#25345;&#32780;&#19981;&#26159;&#21462;&#20195;&#20154;&#31867;&#65292;&#24378;&#35843;&#20102;&#20132;&#20114;&#24335;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19060v1 Announce Type: cross  Abstract: In the dynamic construction industry, traditional robotic integration has primarily focused on automating specific tasks, often overlooking the complexity and variability of human aspects in construction workflows. This paper introduces a human-centered approach with a ``work companion rover" designed to assist construction workers within their existing practices, aiming to enhance safety and workflow fluency while respecting construction labor's skilled nature. We conduct an in-depth study on deploying a robotic system in carpentry formwork, showcasing a prototype that emphasizes mobility, safety, and comfortable worker-robot collaboration in dynamic environments through a contextual Reinforcement Learning (RL)-driven modular framework. Our research advances robotic applications in construction, advocating for collaborative models where adaptive robots support rather than replace humans, underscoring the potential for an interactive a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Gamba&#65292;&#19968;&#31181;&#21333;&#35270;&#22270;3D&#37325;&#24314;&#27169;&#22411;&#65292;&#21019;&#26032;&#22320;&#32467;&#21512;&#20102;&#22823;&#37327;&#30340;3D&#39640;&#26031;&#28857;&#36827;&#34892;&#39640;&#25928;&#37325;&#24314;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26364;&#24052;&#30340;&#39034;&#24207;&#32593;&#32476;&#65292;&#20419;&#36827;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.18795</link><description>&lt;p&gt;
Gamba&#65306;&#23558;&#39640;&#26031;&#39128;&#28857;&#19982;&#26364;&#24052;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#21333;&#35270;&#22270;3D&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Gamba: Marry Gaussian Splatting with Mamba for single view 3D reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18795
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Gamba&#65292;&#19968;&#31181;&#21333;&#35270;&#22270;3D&#37325;&#24314;&#27169;&#22411;&#65292;&#21019;&#26032;&#22320;&#32467;&#21512;&#20102;&#22823;&#37327;&#30340;3D&#39640;&#26031;&#28857;&#36827;&#34892;&#39640;&#25928;&#37325;&#24314;&#65292;&#24182;&#24341;&#20837;&#20102;&#22522;&#20110;&#26364;&#24052;&#30340;&#39034;&#24207;&#32593;&#32476;&#65292;&#20419;&#36827;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#25512;&#29702;&#65292;&#23454;&#29616;&#20102;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#33268;&#21147;&#20110;&#35299;&#20915;&#20174;&#21333;&#20010;&#22270;&#20687;&#39640;&#25928;&#37325;&#24314;3D&#36164;&#20135;&#30340;&#25361;&#25112;&#65292;&#38543;&#30528;&#23545;&#33258;&#21160;&#21270;3D&#20869;&#23481;&#21019;&#24314;&#27969;&#27700;&#32447;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Gamba&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#20174;&#21333;&#35270;&#22270;&#22270;&#20687;&#36827;&#34892;&#25674;&#20313;&#21270;3D&#37325;&#24314;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#20004;&#20010;&#20027;&#35201;&#35265;&#35299;&#65306;(1) 3D&#34920;&#31034;&#65306;&#21033;&#29992;&#22823;&#37327;3D&#39640;&#26031;&#26469;&#36827;&#34892;&#39640;&#25928;&#30340;3D&#39640;&#26031;&#39128;&#28857;&#36807;&#31243;&#65307;(2) &#20027;&#24178;&#35774;&#35745;&#65306;&#24341;&#20837;&#22522;&#20110;&#26364;&#24052;&#30340;&#39034;&#24207;&#32593;&#32476;&#65292;&#20419;&#36827;&#20381;&#36182;&#19978;&#19979;&#25991;&#30340;&#25512;&#29702;&#65292;&#24182;&#20855;&#26377;&#19982;&#24207;&#21015;&#65288;&#26631;&#35760;&#65289;&#38271;&#24230;&#30340;&#32447;&#24615;&#21487;&#25193;&#23637;&#24615;&#65292;&#36866;&#24212;&#22823;&#37327;&#39640;&#26031;&#28857;&#12290;Gamba&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#27491;&#21017;&#21270;&#31561;&#26041;&#38754;&#34701;&#20837;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18795v1 Announce Type: cross  Abstract: We tackle the challenge of efficiently reconstructing a 3D asset from a single image with growing demands for automated 3D content creation pipelines. Previous methods primarily rely on Score Distillation Sampling (SDS) and Neural Radiance Fields (NeRF). Despite their significant success, these approaches encounter practical limitations due to lengthy optimization and considerable memory usage. In this report, we introduce Gamba, an end-to-end amortized 3D reconstruction model from single-view images, emphasizing two main insights: (1) 3D representation: leveraging a large number of 3D Gaussians for an efficient 3D Gaussian splatting process; (2) Backbone design: introducing a Mamba-based sequential network that facilitates context-dependent reasoning and linear scalability with the sequence (token) length, accommodating a substantial number of Gaussians. Gamba incorporates significant advancements in data preprocessing, regularization
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#32593;&#32476;&#22914;Deep Sets&#21644;Transformers&#30340;&#20986;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#22522;&#20110;&#38598;&#21512;&#30340;&#25968;&#25454;&#22788;&#29702;&#30340;&#36827;&#23637;</title><link>https://arxiv.org/abs/2403.17410</link><description>&lt;p&gt;
&#35770;&#25490;&#21015;&#19981;&#21464;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
On permutation-invariant neural networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17410
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#22914;Deep Sets&#21644;Transformers&#30340;&#20986;&#29616;&#26174;&#33879;&#25512;&#21160;&#20102;&#22522;&#20110;&#38598;&#21512;&#30340;&#25968;&#25454;&#22788;&#29702;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#36890;&#24120;&#22312;&#20551;&#35774;&#36755;&#20837;&#25968;&#25454;&#36981;&#24490;&#22522;&#20110;&#21521;&#37327;&#30340;&#26684;&#24335;&#30340;&#21069;&#25552;&#19979;&#35774;&#35745;&#65292;&#30528;&#37325;&#20110;&#22522;&#20110;&#21521;&#37327;&#30340;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#38656;&#27714;&#28041;&#21450;&#22522;&#20110;&#38598;&#21512;&#30340;&#20219;&#21153;&#30340;&#22686;&#38271;&#65292;&#30740;&#31350;&#30028;&#23545;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#20852;&#36259;&#21457;&#29983;&#20102;&#33539;&#24335;&#36716;&#21464;&#12290;&#36817;&#24180;&#26469;&#65292;Deep Sets&#21644;Transformers&#31561;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#20986;&#29616;&#22312;&#22788;&#29702;&#22522;&#20110;&#38598;&#21512;&#30340;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#36825;&#20123;&#26550;&#26500;&#19987;&#38376;&#35774;&#35745;&#20026;&#33258;&#28982;&#23481;&#32435;&#38598;&#21512;&#20316;&#20026;&#36755;&#20837;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#34920;&#31034;&#21644;&#22788;&#29702;&#38598;&#21512;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#36817;&#24180;&#26469;&#20986;&#29616;&#20102;&#22823;&#37327;&#33268;&#21147;&#20110;&#25506;&#32034;&#21644;&#21033;&#29992;&#36825;&#20123;&#26550;&#26500;&#33021;&#21147;&#30340;&#30740;&#31350;&#21162;&#21147;&#65292;&#20197;&#36924;&#36817;&#38598;&#21512;&#20989;&#25968;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#36825;&#39033;&#32508;&#21512;&#35843;&#26597;&#26088;&#22312;&#27010;&#36848;th
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17410v1 Announce Type: cross  Abstract: Conventional machine learning algorithms have traditionally been designed under the assumption that input data follows a vector-based format, with an emphasis on vector-centric paradigms. However, as the demand for tasks involving set-based inputs has grown, there has been a paradigm shift in the research community towards addressing these challenges. In recent years, the emergence of neural network architectures such as Deep Sets and Transformers has presented a significant advancement in the treatment of set-based data. These architectures are specifically engineered to naturally accommodate sets as input, enabling more effective representation and processing of set structures. Consequently, there has been a surge of research endeavors dedicated to exploring and harnessing the capabilities of these architectures for various tasks involving the approximation of set functions. This comprehensive survey aims to provide an overview of th
&lt;/p&gt;</description></item><item><title>DASA&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#25910;&#25947;&#36895;&#24230;&#20165;&#20381;&#36182;&#20110;&#28151;&#21512;&#26102;&#38388;&#21644;&#24179;&#22343;&#24310;&#36831;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#22312;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#23454;&#29616;N&#20493;&#30340;&#25910;&#25947;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.17247</link><description>&lt;p&gt;
DASA: &#24310;&#36831;&#33258;&#36866;&#24212;&#22810;&#26234;&#33021;&#20307;&#38543;&#26426;&#36924;&#36817;
&lt;/p&gt;
&lt;p&gt;
DASA: Delay-Adaptive Multi-Agent Stochastic Approximation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.17247
&lt;/p&gt;
&lt;p&gt;
DASA&#31639;&#27861;&#26159;&#31532;&#19968;&#20010;&#25910;&#25947;&#36895;&#24230;&#20165;&#20381;&#36182;&#20110;&#28151;&#21512;&#26102;&#38388;&#21644;&#24179;&#22343;&#24310;&#36831;&#30340;&#31639;&#27861;&#65292;&#21516;&#26102;&#22312;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#23454;&#29616;N&#20493;&#30340;&#25910;&#25947;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#35774;&#32622;&#65292;&#20854;&#20013;$N$&#20010;&#26234;&#33021;&#20307;&#26088;&#22312;&#36890;&#36807;&#24182;&#34892;&#25805;&#20316;&#24182;&#19982;&#20013;&#22830;&#26381;&#21153;&#22120;&#36890;&#20449;&#26469;&#21152;&#36895;&#19968;&#20010;&#24120;&#35265;&#30340;&#38543;&#26426;&#36924;&#36817;&#65288;SA&#65289;&#38382;&#39064;&#12290;&#25105;&#20204;&#20551;&#23450;&#19978;&#34892;&#20256;&#36755;&#21040;&#26381;&#21153;&#22120;&#30340;&#20256;&#36755;&#21463;&#21040;&#24322;&#27493;&#21644;&#28508;&#22312;&#26080;&#30028;&#26102;&#21464;&#24310;&#36831;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#20943;&#36731;&#24310;&#36831;&#21644;&#33853;&#21518;&#32773;&#30340;&#24433;&#21709;&#65292;&#21516;&#26102;&#21448;&#33021;&#33719;&#24471;&#20998;&#24067;&#24335;&#35745;&#31639;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DASA&#30340;&#24310;&#36831;&#33258;&#36866;&#24212;&#22810;&#26234;&#33021;&#20307;&#38543;&#26426;&#36924;&#36817;&#31639;&#27861;&#12290;&#25105;&#20204;&#23545;DASA&#36827;&#34892;&#20102;&#26377;&#38480;&#26102;&#38388;&#20998;&#26512;&#65292;&#20551;&#35774;&#26234;&#33021;&#20307;&#30340;&#38543;&#26426;&#35266;&#27979;&#36807;&#31243;&#26159;&#29420;&#31435;&#39532;&#23572;&#31185;&#22827;&#38142;&#12290;&#19982;&#29616;&#26377;&#32467;&#26524;&#30456;&#27604;&#65292;DASA&#26159;&#31532;&#19968;&#20010;&#20854;&#25910;&#25947;&#36895;&#24230;&#20165;&#21462;&#20915;&#20110;&#28151;&#21512;&#26102;&#38388;$tmix$&#21644;&#24179;&#22343;&#24310;&#36831;$\tau_{avg}$&#65292;&#21516;&#26102;&#22312;&#39532;&#23572;&#31185;&#22827;&#37319;&#26679;&#19979;&#23454;&#29616;N&#20493;&#30340;&#25910;&#25947;&#21152;&#36895;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#23545;&#20110;&#21508;&#31181;SA&#24212;&#29992;&#26159;&#30456;&#20851;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.17247v1 Announce Type: new  Abstract: We consider a setting in which $N$ agents aim to speedup a common Stochastic Approximation (SA) problem by acting in parallel and communicating with a central server. We assume that the up-link transmissions to the server are subject to asynchronous and potentially unbounded time-varying delays. To mitigate the effect of delays and stragglers while reaping the benefits of distributed computation, we propose \texttt{DASA}, a Delay-Adaptive algorithm for multi-agent Stochastic Approximation. We provide a finite-time analysis of \texttt{DASA} assuming that the agents' stochastic observation processes are independent Markov chains. Significantly advancing existing results, \texttt{DASA} is the first algorithm whose convergence rate depends only on the mixing time $\tmix$ and on the average delay $\tau_{avg}$ while jointly achieving an $N$-fold convergence speedup under Markovian sampling. Our work is relevant for various SA applications, inc
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#8220;&#23398;&#20064;&#25351;&#23548;&#8221;&#65288;LTG&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#19987;&#23478;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#26426;&#22120;&#20915;&#31574;&#21644;&#38754;&#20020;&#26080;&#21161;&#20110;&#27169;&#22411;&#25918;&#24323;&#30340;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16501</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#20154;&#31867;&#20915;&#31574;&#32773;
&lt;/p&gt;
&lt;p&gt;
Learning To Guide Human Decision Makers With Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16501
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#8220;&#23398;&#20064;&#25351;&#23548;&#8221;&#65288;LTG&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#19987;&#23478;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#26426;&#22120;&#20915;&#31574;&#21644;&#38754;&#20020;&#26080;&#21161;&#20110;&#27169;&#22411;&#25918;&#24323;&#30340;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#20197;&#21327;&#21161;&#20154;&#31867;&#36827;&#34892;&#39640;&#39118;&#38505;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#34920;&#29616;&#20986;&#20852;&#36259;&#65292;&#27604;&#22914;&#21307;&#23398;&#35786;&#26029;&#65292;&#26088;&#22312;&#25552;&#39640;&#20915;&#31574;&#36136;&#37327;&#21644;&#20943;&#23569;&#35748;&#30693;&#36127;&#25285;&#12290;&#20027;&#27969;&#26041;&#27861;&#26159;&#23558;&#19987;&#23478;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21512;&#20316;&#65292;&#23558;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#19979;&#25918;&#65292;&#35753;&#21069;&#32773;&#19987;&#27880;&#20110;&#38656;&#35201;&#20182;&#20204;&#20851;&#27880;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#20013;&#65292;&#36825;&#31181;&#8220;&#36131;&#20219;&#20998;&#24037;&#8221;&#35774;&#32622;&#26159;&#19981;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16501v1 Announce Type: new  Abstract: There is increasing interest in developing AIs for assisting human decision making in \textit{high-stakes} tasks, such as medical diagnosis, for the purpose of improving decision quality and reducing cognitive strain.   %   Mainstream approaches team up an expert with a machine learning model to which safer decisions are offloaded, thus letting the former focus on cases that demand their attention.   %   This \textit{separation of responsibilities} setup, however, is inadequate for high-stakes scenarios. On the one hand, the expert may end up over-relying on the machine's decisions due to \textit{anchoring bias}, thus losing the human oversight that is increasingly being required by regulatory agencies to ensure trustworthy AI. On the other hand, the expert is left entirely unassisted on the (typically hardest) decisions on which the model abstained.   %   As a remedy, we introduce \textit{learning to guide} (LTG), an alternative framewo
&lt;/p&gt;</description></item><item><title>FedAC&#26694;&#26550;&#36890;&#36807;&#35299;&#32806;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#19981;&#21516;&#32858;&#21512;&#26041;&#27861;&#20026;&#27599;&#20010;&#23376;&#27169;&#22359;&#25552;&#20379;&#20840;&#23616;&#30693;&#35782;&#65292;&#24341;&#20837;&#32463;&#27982;&#39640;&#25928;&#30340;&#22312;&#32447;&#27169;&#22411;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21450;&#38598;&#32676;&#25968;&#37327;&#24494;&#35843;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16460</link><description>&lt;p&gt;
FedAC&#65306;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#20998;&#31751;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedAC: A Adaptive Clustered Federated Learning Framework for Heterogeneous Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16460
&lt;/p&gt;
&lt;p&gt;
FedAC&#26694;&#26550;&#36890;&#36807;&#35299;&#32806;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#19981;&#21516;&#32858;&#21512;&#26041;&#27861;&#20026;&#27599;&#20010;&#23376;&#27169;&#22359;&#25552;&#20379;&#20840;&#23616;&#30693;&#35782;&#65292;&#24341;&#20837;&#32463;&#27982;&#39640;&#25928;&#30340;&#22312;&#32447;&#27169;&#22411;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21450;&#38598;&#32676;&#25968;&#37327;&#24494;&#35843;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#20998;&#31751;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedAC&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#32806;&#31070;&#32463;&#32593;&#32476;&#24182;&#21033;&#29992;&#19981;&#21516;&#30340;&#32858;&#21512;&#26041;&#27861;&#20026;&#27599;&#20010;&#23376;&#27169;&#22359;&#26377;&#25928;&#22320;&#23558;&#20840;&#23616;&#30693;&#35782;&#25972;&#21512;&#21040;&#31751;&#20869;&#23398;&#20064;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65307;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38477;&#32500;&#30340;&#32463;&#27982;&#39640;&#25928;&#30340;&#22312;&#32447;&#27169;&#22411;&#30456;&#20284;&#24230;&#24230;&#37327;&#65307;&#24182;&#19988;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#25913;&#36827;&#22797;&#26434;&#24615;&#30340;&#38598;&#32676;&#25968;&#37327;&#24494;&#35843;&#27169;&#22359;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36866;&#24212;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16460v1 Announce Type: cross  Abstract: Clustered federated learning (CFL) is proposed to mitigate the performance deterioration stemming from data heterogeneity in federated learning (FL) by grouping similar clients for cluster-wise model training. However, current CFL methods struggle due to inadequate integration of global and intra-cluster knowledge and the absence of an efficient online model similarity metric, while treating the cluster count as a fixed hyperparameter limits flexibility and robustness. In this paper, we propose an adaptive CFL framework, named FedAC, which (1) efficiently integrates global knowledge into intra-cluster learning by decoupling neural networks and utilizing distinct aggregation methods for each submodule, significantly enhancing performance; (2) includes a costeffective online model similarity metric based on dimensionality reduction; (3) incorporates a cluster number fine-tuning module for improved adaptability and scalability in complex,
&lt;/p&gt;</description></item><item><title>MedPromptX&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#22522;&#30784;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#34917;&#20805;&#32570;&#22833;&#30340;EHR&#20449;&#24687;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#20294;&#36873;&#25321;&#26368;&#20339;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#39640;&#36136;&#37327;&#20505;&#36873;&#32773;&#20173;&#26377;&#24453;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2403.15585</link><description>&lt;p&gt;
MedPromptX&#65306;&#22522;&#20110;&#29616;&#23454;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15585
&lt;/p&gt;
&lt;p&gt;
MedPromptX&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#22522;&#30784;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#34917;&#20805;&#32570;&#22833;&#30340;EHR&#20449;&#24687;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#20294;&#36873;&#25321;&#26368;&#20339;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#39640;&#36136;&#37327;&#20505;&#36873;&#32773;&#20173;&#26377;&#24453;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#37096;X&#32447;&#22270;&#20687;&#36890;&#24120;&#29992;&#20110;&#39044;&#27979;&#24613;&#24615;&#21644;&#24930;&#24615;&#24515;&#32954;&#30142;&#30149;&#65292;&#20294;&#26159;&#23558;&#23427;&#20204;&#19982;&#32467;&#26500;&#21270;&#20020;&#24202;&#25968;&#25454;&#25972;&#21512;&#30340;&#21162;&#21147;&#38754;&#20020;&#30528;&#22240;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#19981;&#23436;&#25972;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;MedPromptX&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#65288;FP&#65289;&#21644;&#35270;&#35273;&#22522;&#30784;&#65288;VG&#65289;&#30456;&#32467;&#21512;&#65292;&#23558;&#22270;&#20687;&#19982;EHR&#25968;&#25454;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;&#30340;&#27169;&#22411;&#12290;&#39044;&#35757;&#32451;&#30340;MLLM&#34987;&#29992;&#26469;&#34917;&#20805;&#32570;&#22833;&#30340;EHR&#20449;&#24687;&#65292;&#25552;&#20379;&#23545;&#24739;&#32773;&#30149;&#21490;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#23569;&#26679;&#26412;&#25552;&#31034;&#20943;&#23569;&#20102;&#23545;MLLM&#30340;&#22823;&#37327;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#26377;&#25928;&#35299;&#20915;&#20102;&#24187;&#35273;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#26368;&#20339;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#36807;&#31243;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#20505;&#36873;&#32773;&#21487;&#33021;&#36807;&#20110;&#32321;&#29712;&#65292;&#20294;&#23427;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#30528;&#28145;&#36828;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#26469;&#21160;&#24577;&#22320;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15585v1 Announce Type: cross  Abstract: Chest X-ray images are commonly used for predicting acute and chronic cardiopulmonary conditions, but efforts to integrate them with structured clinical data face challenges due to incomplete electronic health records (EHR). This paper introduces \textbf{MedPromptX}, the first model to integrate multimodal large language models (MLLMs), few-shot prompting (FP) and visual grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A pre-trained MLLM is utilized to complement the missing EHR information, providing a comprehensive understanding of patients' medical history. Additionally, FP reduces the necessity for extensive training of MLLMs while effectively tackling the issue of hallucination. Nevertheless, the process of determining the optimal number of few-shot examples and selecting high-quality candidates can be burdensome, yet it profoundly influences model performance. Hence, we propose a new technique that dynam
&lt;/p&gt;</description></item><item><title>WoLF&#26694;&#26550;&#25552;&#20986;&#20102;&#23545;&#20110;CXR&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#20351;&#29992;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#12289;&#37325;&#26500;&#25253;&#21578;&#20197;&#25552;&#20379;&#26356;&#26377;&#32452;&#32455;&#30340;&#20449;&#24687;&#12289;&#20197;&#21450;&#25913;&#36827;&#29983;&#25104;&#31572;&#26696;&#30340;&#32454;&#33268;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.15456</link><description>&lt;p&gt;
WoLF: &#29992;&#20110;&#33016;&#37096;X&#32447;&#22270;&#29702;&#35299;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
WoLF: Large Language Model Framework for CXR Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15456
&lt;/p&gt;
&lt;p&gt;
WoLF&#26694;&#26550;&#25552;&#20986;&#20102;&#23545;&#20110;CXR&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#20351;&#29992;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#12289;&#37325;&#26500;&#25253;&#21578;&#20197;&#25552;&#20379;&#26356;&#26377;&#32452;&#32455;&#30340;&#20449;&#24687;&#12289;&#20197;&#21450;&#25913;&#36827;&#29983;&#25104;&#31572;&#26696;&#30340;&#32454;&#33268;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#21462;&#24471;&#20102;&#23545;&#33016;&#37096;X&#32447;&#22270;(CXR)&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#30528;&#26041;&#27861;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35270;&#35273;&#38382;&#31572;(VQA)&#21644;CXR&#25253;&#21578;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CXR&#29702;&#35299;&#26694;&#26550;&#20173;&#23384;&#22312;&#20960;&#20010;&#31243;&#24207;&#19978;&#30340;&#32570;&#38519;&#12290;(1)&#20197;&#24448;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;CXR&#25253;&#21578;&#65292;&#36825;&#23545;&#20110;&#20840;&#38754;&#30340;&#35270;&#35273;&#38382;&#31572;(VQA)&#26469;&#35828;&#26159;&#19981;&#22815;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#22914;&#29992;&#33647;&#21382;&#21490;&#21644;&#20808;&#21069;&#30340;&#35786;&#26029;&#26102;&#12290;(2)&#20197;&#24448;&#30340;&#26041;&#27861;&#20351;&#29992;&#26410;&#32463;&#22788;&#29702;&#30340;CXR&#25253;&#21578;&#65292;&#36825;&#20123;&#25253;&#21578;&#24448;&#24448;&#32467;&#26500;&#38543;&#24847;&#12290;&#34429;&#28982;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29702;&#35299;&#21508;&#31181;&#25991;&#26412;&#26684;&#24335;&#65292;&#20294;&#20026;&#20102;&#25552;&#20379;&#26356;&#28165;&#26224;&#12289;&#26377;&#32452;&#32455;&#30340;&#22522;&#20110;&#35299;&#21078;&#23398;&#30340;&#20449;&#24687;&#65292;&#37325;&#26500;&#25253;&#21578;&#21487;&#33021;&#20250;&#22686;&#24378;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;(3)&#30446;&#21069;&#29992;&#20110;CXR-VQA&#30340;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#24378;&#35843;&#35821;&#35328;&#27491;&#30830;&#24615;&#65292;&#32570;&#20047;&#23545;&#29983;&#25104;&#31572;&#26696;&#30340;&#24494;&#22937;&#35780;&#20272;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15456v1 Announce Type: new  Abstract: Significant methodological strides have been made toward Chest X-ray (CXR) understanding via modern vision-language models (VLMs), demonstrating impressive Visual Question Answering (VQA) and CXR report generation abilities. However, existing CXR understanding frameworks still possess several procedural caveats. (1) Previous methods solely use CXR reports, which are insufficient for comprehensive Visual Question Answering (VQA), especially when additional health-related data like medication history and prior diagnoses are needed. (2) Previous methods use raw CXR reports, which are often arbitrarily structured. While modern language models can understand various text formats, restructuring reports for clearer, organized anatomy-based information could enhance their usefulness. (3) Current evaluation methods for CXR-VQA primarily emphasize linguistic correctness, lacking the capability to offer nuanced assessments of the generated answers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.09732</link><description>&lt;p&gt;
PET-SQL&#65306;&#19968;&#20010;&#24102;&#26377;&#20132;&#21449;&#19968;&#33268;&#24615;&#30340;&#22686;&#24378;&#25552;&#31034;&#30340;&#20004;&#38454;&#27573;&#25991;&#26412;&#21040;SQL&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
PET-SQL: A Prompt-enhanced Two-stage Text-to-SQL Framework with Cross-consistency
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09732
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#21644;&#23569;&#26679;&#26412;&#28436;&#31034;&#65292;&#35299;&#20915;&#20102;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#29992;&#25143;&#24847;&#22270;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25991;&#26412;&#21040;SQL&#65288;Text2SQL&#65289;&#39046;&#22495;&#30340;&#36827;&#23637;&#24378;&#35843;&#21050;&#28608;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#26524;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#22312;&#22788;&#29702;&#20887;&#38271;&#30340;&#25968;&#25454;&#24211;&#20449;&#24687;&#21644;&#22797;&#26434;&#30340;&#29992;&#25143;&#24847;&#22270;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#20197;&#22686;&#24378;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#33258;&#28982;&#35821;&#35328;&#21040;SQL&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25552;&#31034;&#34920;&#31034;&#65292;&#31216;&#20026;&#21442;&#32771;&#22686;&#24378;&#34920;&#31034;&#65292;&#20854;&#20013;&#21253;&#25324;&#27169;&#24335;&#20449;&#24687;&#21644;&#20174;&#34920;&#26684;&#38543;&#26426;&#25277;&#26679;&#30340;&#21333;&#20803;&#26684;&#20540;&#65292;&#20197;&#25351;&#23548;LLM&#29983;&#25104;SQL&#26597;&#35810;&#12290;&#28982;&#21518;&#65292;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#26816;&#32034;&#38382;&#39064;-SQL&#23545;&#20316;&#20026;&#23569;&#37327;&#28436;&#31034;&#65292;&#20419;&#20351;LLM&#29983;&#25104;&#21021;&#27493;SQL&#65288;PreSQL&#65289;&#12290;&#20043;&#21518;&#65292;&#35299;&#26512;PreSQL&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#36827;&#34892;&#27169;&#24335;&#38142;&#25509;&#65292;&#21487;&#20197;&#26174;&#33879;&#21387;&#32553;&#26377;&#29992;&#20449;&#24687;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;&#21033;&#29992;&#38142;&#25509;&#30340;&#27169;&#24335;&#65292;&#25105;&#20204;&#31616;&#21270;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09732v1 Announce Type: cross  Abstract: Recent advancements in Text-to-SQL (Text2SQL) emphasize stimulating the large language models (LLM) on in-context learning, achieving significant results. Nevertheless, they face challenges when dealing with verbose database information and complex user intentions. This paper presents a two-stage framework to enhance the performance of current LLM-based natural language to SQL systems. We first introduce a novel prompt representation, called reference-enhanced representation, which includes schema information and randomly sampled cell values from tables to instruct LLMs in generating SQL queries. Then, in the first stage, question-SQL pairs are retrieved as few-shot demonstrations, prompting the LLM to generate a preliminary SQL (PreSQL). After that, the mentioned entities in PreSQL are parsed to conduct schema linking, which can significantly compact the useful information. In the second stage, with the linked schema, we simplify the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;STREAM&#65292;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#24403;&#21069;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#23545;&#20110;&#26102;&#31354;&#29305;&#24615;&#30340;&#19981;&#36275;</title><link>https://arxiv.org/abs/2403.09669</link><description>&lt;p&gt;
STREAM&#65306;&#29992;&#20110;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#31354;&#35780;&#20272;&#21644;&#20998;&#26512;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;
STREAM: Spatio-TempoRal Evaluation and Analysis Metric for Video Generative Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09669
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;STREAM&#65292;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#65292;&#24357;&#34917;&#20102;&#24403;&#21069;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#20013;&#23545;&#20110;&#26102;&#31354;&#29305;&#24615;&#30340;&#19981;&#36275;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#36924;&#30495;&#22810;&#26679;&#30340;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24471;&#30410;&#20110;&#21508;&#31181;&#35780;&#20272;&#24230;&#37327;&#30340;&#20840;&#38754;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#30701;&#35270;&#39057;&#29255;&#27573;&#26102;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#65292;&#32570;&#20047;&#25552;&#20379;&#25913;&#36827;&#35265;&#35299;&#30340;&#24037;&#20855;&#12290;&#30446;&#21069;&#30340;&#35270;&#39057;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#26159;&#36890;&#36807;&#23558;&#23884;&#20837;&#35270;&#39057;&#23884;&#20837;&#32593;&#32476;&#26469;&#31616;&#21333;&#35843;&#25972;&#22270;&#20687;&#24230;&#37327;&#26041;&#27861;&#32780;&#24471;&#21040;&#30340;&#65292;&#36825;&#21487;&#33021;&#20302;&#20272;&#20102;&#35270;&#39057;&#30340;&#29420;&#29305;&#29305;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#24191;&#27867;&#20351;&#29992;&#30340;Frechet Video Distance (FVD) &#22312;&#31354;&#38388;&#26041;&#38754;&#30340;&#37325;&#35270;&#31243;&#24230;&#35201;&#22823;&#20110;&#35270;&#39057;&#30340;&#26102;&#38388;&#33258;&#28982;&#24615;&#65292;&#19988;&#21463;&#21040;&#25152;&#20351;&#29992;&#30340;&#23884;&#20837;&#32593;&#32476;&#36755;&#20837;&#22823;&#23567;&#30340;&#38480;&#21046;&#65292;&#20165;&#38480;&#20110;16&#24103;&#35270;&#39057;&#12290;&#27492;&#22806;&#65292;&#23427;&#34920;&#29616;&#20986;&#30456;&#24403;&#22823;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#24182;&#19982;&#20154;&#31867;&#35780;&#20272;&#23384;&#22312;&#24046;&#24322;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STREAM&#65292;&#19968;&#31181;&#26032;&#30340;&#35270;&#39057;&#35780;&#20272;&#24230;&#37327;&#26041;&#27861;&#65292;&#29420;&#29305;&#22320;&#35774;&#35745;&#20197;&#35299;&#20915;&#24403;&#21069;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#35780;&#20272;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09669v1 Announce Type: cross  Abstract: Image generative models have made significant progress in generating realistic and diverse images, supported by comprehensive guidance from various evaluation metrics. However, current video generative models struggle to generate even short video clips, with limited tools that provide insights for improvements. Current video evaluation metrics are simple adaptations of image metrics by switching the embeddings with video embedding networks, which may underestimate the unique characteristics of video. Our analysis reveals that the widely used Frechet Video Distance (FVD) has a stronger emphasis on the spatial aspect than the temporal naturalness of video and is inherently constrained by the input size of the embedding networks used, limiting it to 16 frames. Additionally, it demonstrates considerable instability and diverges from human evaluations. To address the limitations, we propose STREAM, a new video evaluation metric uniquely des
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26126;&#31034;&#24847;&#35782;&#26657;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.09488</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#32416;&#27491;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;
&lt;/p&gt;
&lt;p&gt;
Rectifying Demonstration Shortcut in In-Context Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09488
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#30340;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26126;&#31034;&#24847;&#35782;&#26657;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#22815;&#21033;&#29992;&#23427;&#20204;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#65292;&#20165;&#20973;&#23569;&#37327;&#28436;&#31034;&#20415;&#33021;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;LLMs&#24120;&#24120;&#20381;&#36182;&#20110;&#23427;&#20204;&#23545;&#28436;&#31034;&#30340;&#39044;&#20808;&#35757;&#32451;&#30340;&#35821;&#20041;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#26681;&#25454;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#32487;&#32493;&#36827;&#34892;ICL&#39044;&#27979;&#12290;&#26412;&#25991;&#23558;&#36825;&#19968;&#29616;&#35937;&#31216;&#20026;&#8220;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#8221;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#20110;&#25913;&#36827;&#39044;&#23450;&#20041;&#20219;&#21153;&#30340;ICL&#39044;&#27979;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#32416;&#27491;&#28436;&#31034;&#24555;&#25463;&#26041;&#24335;&#65292;&#20174;&#32780;&#20351;LLM&#33021;&#22815;&#26377;&#25928;&#22320;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#26032;&#30340;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#12290;&#20026;&#23454;&#29616;&#27492;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26126;&#31034;&#24847;&#35782;&#30340;&#26657;&#20934;&#26041;&#27861;&#65306;In-Context Calibration&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#35774;&#32622;&#20013;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65306;&#65288;1&#65289;&#20351;&#29992;&#26631;&#20934;&#26631;&#31614;&#31354;&#38388;&#30340;&#21407;&#22987;ICL&#20219;&#21153;&#20197;&#21450;&#65288;2&#65289;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#20854;&#20013;&#26631;&#31614;&#31354;&#38388;&#34987;&#35821;&#20041;&#26080;&#20851;&#30340;&#26631;&#35760;&#26367;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09488v1 Announce Type: cross  Abstract: Large language models (LLMs) are able to solve various tasks with only a few demonstrations utilizing their in-context learning (ICL) abilities. However, LLMs often rely on their pre-trained semantic priors of demonstrations rather than on the input-label relationships to proceed with ICL prediction. In this work, we term this phenomenon as the `Demonstration Shortcut'. While previous works have primarily focused on improving ICL prediction results for predefined tasks, we aim to rectify the Demonstration Shortcut, thereby enabling the LLM to effectively learn new input-label relationships from demonstrations. To achieve this, we introduce In-Context Calibration, a demonstration-aware calibration method. We evaluate the effectiveness of the proposed method in two settings: (1) the Original ICL Task using the standard label space and (2) the Task Learning setting, where the label space is replaced with semantically unrelated tokens. In 
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;ChatGPT&#22312;&#25552;&#20379;&#23450;&#21046;&#21270;&#30340;&#35821;&#22659;&#29305;&#23450;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#21487;&#20197;&#36741;&#21161;&#26415;&#35821;&#23398;&#23478;&#36827;&#34892;&#26415;&#35821;&#32534;&#32386;&#65292;&#23454;&#29616;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#32467;&#21512;&#12290;</title><link>https://arxiv.org/abs/2402.16139</link><description>&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#23545;&#26415;&#35821;&#23450;&#20041;&#30340;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
What Generative Artificial Intelligence Means for Terminological Definitions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16139
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#22914;ChatGPT&#22312;&#25552;&#20379;&#23450;&#21046;&#21270;&#30340;&#35821;&#22659;&#29305;&#23450;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#23384;&#22312;&#25361;&#25112;&#65292;&#21487;&#20197;&#36741;&#21161;&#26415;&#35821;&#23398;&#23478;&#36827;&#34892;&#26415;&#35821;&#32534;&#32386;&#65292;&#23454;&#29616;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#23545;&#26415;&#35821;&#23450;&#20041;&#30340;&#21019;&#24314;&#21644;&#28040;&#36153;&#30340;&#24433;&#21709;&#12290;&#20687;ChatGPT&#36825;&#26679;&#30340;GenAI&#24037;&#20855;&#19982;&#20256;&#32479;&#26415;&#35821;&#36164;&#28304;&#30456;&#27604;&#65292;&#24102;&#26469;&#20102;&#19968;&#31995;&#21015;&#30410;&#22788;&#21644;&#25361;&#25112;&#12290;ChatGPT&#22312;&#20197;&#20132;&#20114;&#24335;&#21644;&#23450;&#21046;&#21270;&#30340;&#26041;&#24335;&#25552;&#20379;&#29305;&#23450;&#35821;&#22659;&#21547;&#20041;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#35782;&#21035;&#36164;&#28304;&#20013;&#30340;&#26415;&#35821;&#23450;&#20041;&#21487;&#33021;&#20250;&#22240;&#20854;&#21487;&#38752;&#24615;&#32780;&#32487;&#32493;&#23384;&#22312;&#12290;&#20174;&#26415;&#35821;&#23398;&#23478;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#35832;&#22914;ChatGPT&#20043;&#31867;&#30340;&#24037;&#20855;&#20351;&#24471;AI&#36741;&#21161;&#30340;&#26415;&#35821;&#32534;&#32386;&#25104;&#20026;&#21487;&#33021;&#65292;&#21253;&#25324;&#21518;&#26399;&#32534;&#36753;&#26415;&#35821;&#32534;&#32386;&#65292;&#23558;AI&#25928;&#29575;&#19982;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26356;&#24555;&#36895;&#30340;&#23450;&#20041;&#21019;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16139v1 Announce Type: cross  Abstract: This paper examines the impact of Generative Artificial Intelligence (GenAI) on the creation and consumption of terminological definitions. GenAI tools like ChatGPT present a mix of benefits and drawbacks compared to traditional terminological resources. ChatGPT excels in providing context-specific meanings in an interactive and customized fashion but faces challenges with accuracy. Terminological definitions in recognized resources will likely survive because of their reliability. From the point of view of the terminologist, tools like ChatGPT enable AI-assisted terminography, including post-editing terminography, as an approach blending AI efficiency with human expertise for faster definition creation.
&lt;/p&gt;</description></item><item><title>PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#20026;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#31283;&#20581;&#38450;&#24481;</title><link>https://arxiv.org/abs/2402.12168</link><description>&lt;p&gt;
&#38024;&#23545;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#30340;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Defending Against Weight-Poisoning Backdoor Attacks for Parameter-Efficient Fine-Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12168
&lt;/p&gt;
&lt;p&gt;
PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#20026;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#25552;&#20379;&#31283;&#20581;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38024;&#23545;&#35821;&#35328;&#27169;&#22411;&#24212;&#29992;&#25552;&#20986;&#24182;&#25104;&#21151;&#23454;&#26045;&#20102;&#21508;&#31181;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#38754;&#23545;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#26102;&#65292;&#20165;&#26356;&#26032;&#26377;&#38480;&#27169;&#22411;&#21442;&#25968;&#30340;PEFT&#26159;&#21542;&#26500;&#25104;&#23433;&#20840;&#28431;&#27934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PEFT&#30456;&#23545;&#20110;&#20840;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#26356;&#23481;&#26131;&#21463;&#21040;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#39044;&#23450;&#20041;&#30340;&#35302;&#21457;&#22120;&#20173;&#28982;&#26131;&#21463;&#21033;&#29992;&#65292;&#39044;&#23450;&#20041;&#30340;&#30446;&#26631;&#22312;&#24494;&#35843;&#21518;&#20381;&#28982;&#20445;&#25345;&#39640;&#32622;&#20449;&#24230;&#12290;&#21463;&#21040;&#36825;&#19968;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21033;&#29992;PEFT&#30340;&#27602;&#21270;&#26679;&#26412;&#35782;&#21035;&#27169;&#22359;&#65288;PSIM&#65289;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#35782;&#21035;&#21463;&#27745;&#26579;&#26679;&#26412;&#65292;&#25552;&#20379;&#38024;&#23545;&#26435;&#37325;&#25237;&#27602;&#21518;&#38376;&#25915;&#20987;&#30340;&#31283;&#20581;&#38450;&#24481;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;PEFT&#35757;&#32451;PSIM&#65292;&#24102;&#26377;&#38543;&#26426;&#37325;&#32622;&#26679;&#26412;&#26631;&#31614;&#12290;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12168v1 Announce Type: cross  Abstract: Recently, various parameter-efficient fine-tuning (PEFT) strategies for application to language models have been proposed and successfully implemented. However, this raises the question of whether PEFT, which only updates a limited set of model parameters, constitutes security vulnerabilities when confronted with weight-poisoning backdoor attacks. In this study, we show that PEFT is more susceptible to weight-poisoning backdoor attacks compared to the full-parameter fine-tuning method, with pre-defined triggers remaining exploitable and pre-defined targets maintaining high confidence, even after fine-tuning. Motivated by this insight, we developed a Poisoned Sample Identification Module (PSIM) leveraging PEFT, which identifies poisoned samples through confidence, providing robust defense against weight-poisoning backdoor attacks. Specifically, we leverage PEFT to train the PSIM with randomly reset sample labels. During the inference pr
&lt;/p&gt;</description></item><item><title>MultiCorrupt&#26159;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#22312;&#21313;&#31181;&#19981;&#21516;&#25968;&#25454;&#25439;&#22351;&#31867;&#22411;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2402.11677</link><description>&lt;p&gt;
MultiCorrupt&#65306;&#19968;&#31181;&#29992;&#20110;3D&#29289;&#20307;&#26816;&#27979;&#30340;LiDAR-&#30456;&#26426;&#34701;&#21512;&#30340;&#22810;&#27169;&#24577;&#40065;&#26834;&#24615;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
MultiCorrupt: A Multi-Modal Robustness Dataset and Benchmark of LiDAR-Camera Fusion for 3D Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11677
&lt;/p&gt;
&lt;p&gt;
MultiCorrupt&#26159;&#29992;&#20110;&#35780;&#20272;&#22810;&#27169;&#24577;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#22312;&#21313;&#31181;&#19981;&#21516;&#25968;&#25454;&#25439;&#22351;&#31867;&#22411;&#19979;&#30340;&#40065;&#26834;&#24615;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;3D&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#20934;&#25968;&#25454;&#38598;&#22914;nuScenes&#19978;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#20294;&#23427;&#20204;&#23545;&#23494;&#38598;&#37319;&#26679;&#30340;LiDAR&#28857;&#20113;&#21644;&#31934;&#24515;&#26657;&#20934;&#30340;&#20256;&#24863;&#22120;&#38453;&#21015;&#20381;&#36182;&#24615;&#20351;&#24471;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MultiCorrupt&#65292;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#22810;&#27169;&#24577;3D&#30446;&#26631;&#26816;&#27979;&#22120;&#23545;&#21313;&#31181;&#19981;&#21516;&#31867;&#22411;&#25968;&#25454;&#25439;&#22351;&#30340;&#40065;&#26834;&#24615;&#30340;&#32508;&#21512;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11677v1 Announce Type: cross  Abstract: Multi-modal 3D object detection models for automated driving have demonstrated exceptional performance on computer vision benchmarks like nuScenes. However, their reliance on densely sampled LiDAR point clouds and meticulously calibrated sensor arrays poses challenges for real-world applications. Issues such as sensor misalignment, miscalibration, and disparate sampling frequencies lead to spatial and temporal misalignment in data from LiDAR and cameras. Additionally, the integrity of LiDAR and camera data is often compromised by adverse environmental conditions such as inclement weather, leading to occlusions and noise interference. To address this challenge, we introduce MultiCorrupt, a comprehensive benchmark designed to evaluate the robustness of multi-modal 3D object detectors against ten distinct types of corruptions. We evaluate five state-of-the-art multi-modal detectors on MultiCorrupt and analyze their performance in terms of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21453;&#21465;&#20107;&#65292;&#36890;&#36807;5&#20010;&#26041;&#38754;&#20174;&#19987;&#38376; NGO &#25351;&#21335;&#20013;&#25552;&#21462;&#23450;&#20041;&#30340;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#20197;&#24448;&#35780;&#20272;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11676</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21453;&#21465;&#20107;&#35780;&#20272;&#30340;&#22810;&#26041;&#38754;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Multi-Aspect Framework for Counter Narrative Evaluation using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11676
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26041;&#38754;&#26694;&#26550;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#21453;&#21465;&#20107;&#65292;&#36890;&#36807;5&#20010;&#26041;&#38754;&#20174;&#19987;&#38376; NGO &#25351;&#21335;&#20013;&#25552;&#21462;&#23450;&#20041;&#30340;&#20869;&#23481;&#65292;&#20197;&#35299;&#20915;&#20197;&#24448;&#35780;&#20272;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21465;&#20107;&#26159;&#23545;&#20167;&#24680;&#35328;&#35770;&#32972;&#26223;&#30340;&#30693;&#24773;&#22238;&#24212;&#65292;&#26088;&#22312;&#39539;&#26021;&#20167;&#24680;&#20027;&#24352;&#24182;&#21270;&#35299;&#20914;&#31361;&#65292;&#24050;&#25104;&#20026;&#19968;&#31181;&#26377;&#25928;&#30340;&#20167;&#24680;&#35328;&#35770;&#24178;&#39044;&#31574;&#30053;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#25552;&#20986;&#20102;&#33258;&#21160;&#29983;&#25104;&#21453;&#21465;&#20107;&#30340;&#26041;&#27861;&#26469;&#36741;&#21161;&#25163;&#21160;&#24178;&#39044;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#30340;&#35780;&#20272;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#21457;&#23637;&#12290;&#20808;&#21069;&#29992;&#20110;&#21453;&#21465;&#20107;&#35780;&#20272;&#30340;&#33258;&#21160;&#24230;&#37327;&#26631;&#20934;&#32570;&#20047;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#19968;&#33268;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#20381;&#36182;&#20110;&#34920;&#38754;&#21442;&#32771;&#27604;&#36739;&#65292;&#32780;&#19981;&#26159;&#23558;&#21453;&#21465;&#20107;&#36136;&#37327;&#30340;&#20851;&#38190;&#26041;&#38754;&#32435;&#20837;&#35780;&#20272;&#26631;&#20934;&#12290;&#20026;&#35299;&#20915;&#20808;&#21069;&#30340;&#35780;&#20272;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#20419;&#20351;LLM&#25552;&#20379;&#29983;&#25104;&#30340;&#21453;&#21465;&#20107;&#20505;&#36873;&#30340;&#24471;&#20998;&#21644;&#21453;&#39304;&#65292;&#20351;&#29992;&#20102;&#26469;&#33258;&#19987;&#38376;NGO&#30340;&#21453;&#21465;&#20107;&#25351;&#21335;&#20013;&#25552;&#21462;&#30340;5&#20010;&#23450;&#20041;&#30340;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11676v1 Announce Type: cross  Abstract: Counter narratives - informed responses to hate speech contexts designed to refute hateful claims and de-escalate encounters - have emerged as an effective hate speech intervention strategy. While previous work has proposed automatic counter narrative generation methods to aid manual interventions, the evaluation of these approaches remains underdeveloped. Previous automatic metrics for counter narrative evaluation lack alignment with human judgment as they rely on superficial reference comparisons instead of incorporating key aspects of counter narrative quality as evaluation criteria. To address prior evaluation limitations, we propose a novel evaluation framework prompting LLMs to provide scores and feedback for generated counter narrative candidates using 5 defined aspects derived from guidelines from counter narrative specialized NGOs. We found that LLM evaluators achieve strong alignment to human-annotated scores and feedback and
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2402.02977</link><description>&lt;p&gt;
&#21464;&#20998;&#27969;&#27169;&#22411;&#65306;&#20197;&#20320;&#30340;&#39118;&#26684;&#27969;&#21160;
&lt;/p&gt;
&lt;p&gt;
Variational Flow Models: Flowing in Your Style
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02977
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21464;&#20998;&#27969;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31995;&#32479;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20351;&#24471;&#24555;&#36895;&#37319;&#26679;&#25104;&#20026;&#21487;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#23545;"&#21518;&#39564;&#27969;"&#27169;&#22411;&#36827;&#34892;&#21464;&#20998;&#25512;&#29702;&#35299;&#37322;&#30340;&#26041;&#27861;&#8212;&#8212;&#29992;&#20197;&#23558;"&#27010;&#29575;&#27969;"&#25512;&#24191;&#21040;&#26356;&#24191;&#27867;&#30340;&#38543;&#26426;&#36807;&#31243;&#31867;&#21035;&#65292;&#19981;&#24517;&#23616;&#38480;&#20110;&#25193;&#25955;&#36807;&#31243;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#32467;&#26524;&#31216;&#20026;"&#21464;&#20998;&#27969;&#27169;&#22411;"&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#23558;&#30001;&#26041;&#31243;Xt = at * X0 + st * X1&#25152;&#25551;&#36848;&#30340;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#36716;&#21270;&#20026;&#30452;&#32447;&#24658;&#36895;(SC)&#27969;&#65292;&#31867;&#20284;&#20110;&#30699;&#27491;&#27969;&#12290;&#36825;&#31181;&#36716;&#21270;&#20351;&#24471;&#21487;&#20197;&#24555;&#36895;&#27839;&#30528;&#21407;&#22987;&#30340;&#21518;&#39564;&#27969;&#36827;&#34892;&#37319;&#26679;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#19968;&#20010;&#26032;&#30340;SC&#27969;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#28789;&#27963;&#24615;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#36716;&#25442;&#25193;&#23637;&#21040;&#20004;&#20010;&#19981;&#21516;"&#32447;&#24615;"&#38543;&#26426;&#36807;&#31243;&#30340;&#21518;&#39564;&#27969;&#20043;&#38388;&#36827;&#34892;&#20114;&#30456;&#36716;&#21270;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21487;&#20197;&#23558;&#39640;&#38454;&#25968;&#20540;&#35299;&#27861;&#36731;&#26494;&#38598;&#25104;&#21040;&#36716;&#25442;&#21518;&#30340;SC&#27969;&#20013;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#37319;&#26679;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a variational inference interpretation for models of "posterior flows" - generalizations of "probability flows" to a broader class of stochastic processes not necessarily diffusion processes. We coin the resulting models as "Variational Flow Models". Additionally, we propose a systematic training-free method to transform the posterior flow of a "linear" stochastic process characterized by the equation Xt = at * X0 + st * X1 into a straight constant-speed (SC) flow, reminiscent of Rectified Flow. This transformation facilitates fast sampling along the original posterior flow without training a new model of the SC flow. The flexibility of our approach allows us to extend our transformation to inter-convert two posterior flows from distinct "linear" stochastic processes. Moreover, we can easily integrate high-order numerical solvers into the transformed SC flow, further enhancing sampling accuracy and efficiency. Rigorous theoretical analysis and extensive experimental result
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22806;&#37096;&#24605;&#32771;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#36890;&#20449;&#21327;&#35758;&#21644;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#28216;&#25103;&#25512;&#29702;&#12289;&#35821;&#38899;&#29983;&#25104;&#21644;&#22312;&#32447;&#35780;&#20272;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.02330</link><description>&lt;p&gt;
&#22312;&#29436;&#20154;&#28216;&#25103;&#20013;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhance Reasoning for Large Language Models in the Game Werewolf
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22806;&#37096;&#24605;&#32771;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#25512;&#29702;&#33021;&#21147;&#65292;&#36890;&#36807;&#24341;&#20837;&#36890;&#20449;&#21327;&#35758;&#21644;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#28216;&#25103;&#25512;&#29702;&#12289;&#35821;&#38899;&#29983;&#25104;&#21644;&#22312;&#32447;&#35780;&#20272;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21019;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#22806;&#37096;&#24605;&#32771;&#27169;&#22359;&#30456;&#32467;&#21512;&#65292;&#20197;&#22686;&#24378;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#19982;&#36890;&#36807;prompt&#24037;&#31243;&#22686;&#21152;LLM&#19981;&#21516;&#65292;&#24605;&#32771;&#32773;&#30452;&#25509;&#21033;&#29992;&#25968;&#25454;&#24211;&#20013;&#30340;&#30693;&#35782;&#65292;&#24182;&#37319;&#29992;&#21508;&#31181;&#20248;&#21270;&#25216;&#26415;&#12290;&#35813;&#26694;&#26550;&#24418;&#25104;&#20102;&#19968;&#20010;&#25512;&#29702;&#23618;&#27425;&#32467;&#26500;&#65292;&#22312;&#20854;&#20013;LLM&#22788;&#29702;&#30452;&#35266;&#30340;&#31995;&#32479;1&#20219;&#21153;&#65292;&#22914;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65292;&#32780;&#24605;&#32771;&#32773;&#19987;&#27880;&#20110;&#38656;&#35201;&#22797;&#26434;&#30340;&#36923;&#36753;&#20998;&#26512;&#21644;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#35748;&#30693;&#31995;&#32479;2&#20219;&#21153;&#12290;&#25105;&#20204;&#20197;&#38656;&#35201;&#21452;&#31995;&#32479;&#25512;&#29702;&#30340;9&#20154;&#29436;&#20154;&#28216;&#25103;&#20026;&#20363;&#20171;&#32461;&#20102;&#35813;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LLM&#21644;&#24605;&#32771;&#32773;&#20043;&#38388;&#30340;&#36890;&#20449;&#21327;&#35758;&#65292;&#24182;&#20351;&#29992;&#26469;&#33258;18800&#20010;&#20154;&#31867;&#20250;&#35805;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#25968;&#25454;&#35757;&#32451;&#20102;&#24605;&#32771;&#32773;&#12290;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26694;&#26550;&#22312;&#28436;&#32462;&#25512;&#29702;&#12289;&#35821;&#38899;&#29983;&#25104;&#21644;&#22312;&#32447;&#28216;&#25103;&#35780;&#20272;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24494;&#35843;6B LLM&#65292;&#36229;&#36234;&#20102;GPT4&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents an innovative framework that integrates Large Language Models (LLMs) with an external Thinker module to enhance the reasoning capabilities of LLM-based agents. Unlike augmenting LLMs with prompt engineering, Thinker directly harnesses knowledge from databases and employs various optimization techniques. The framework forms a reasoning hierarchy where LLMs handle intuitive System-1 tasks such as natural language processing, while the Thinker focuses on cognitive System-2 tasks that require complex logical analysis and domain-specific knowledge. Our framework is presented using a 9-player Werewolf game that demands dual-system reasoning. We introduce a communication protocol between LLMs and the Thinker, and train the Thinker using data from 18800 human sessions and reinforcement learning. Experiments demonstrate the framework's effectiveness in deductive reasoning, speech generation, and online game evaluation. Additionally, we fine-tune a 6B LLM to surpass GPT4 when
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SERNet-Former&#30340;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#21644;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#26469;&#25913;&#21892;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#34701;&#21512;&#35821;&#20041;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2401.15741</link><description>&lt;p&gt;
SERNet-Former: &#24102;&#26377;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#21644;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#30340;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SERNet-Former: Semantic Segmentation by Efficient Residual Network with Attention-Boosting Gates and Attention-Fusion Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15741
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SERNet-Former&#30340;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#21644;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#26469;&#25913;&#21892;&#35821;&#20041;&#20998;&#21106;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#34701;&#21512;&#35821;&#20041;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35821;&#20041;&#20998;&#21106;&#39046;&#22495;&#65292;&#25913;&#21892;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#25928;&#29575;&#38656;&#35201;&#35299;&#20915;&#19981;&#26029;&#22686;&#38271;&#30340;&#35745;&#31639;&#25104;&#26412;&#20197;&#21450;&#20174;&#20840;&#23616;&#21644;&#23616;&#37096;&#19978;&#34701;&#21512;&#35821;&#20041;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;&#22522;&#20110;&#26368;&#36817;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#30340;&#25104;&#21151;&#21644;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24102;&#26377;&#29420;&#29305;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#12290;&#36890;&#36807;&#24341;&#20837;&#27880;&#24847;&#21147;&#22686;&#24378;&#38376;&#65288;AbGs&#65289;&#21644;&#27880;&#24847;&#21147;&#22686;&#24378;&#27169;&#22359;&#65288;AbMs&#65289;&#65292;&#30446;&#26631;&#26159;&#22312;&#32534;&#30721;&#22120;&#20013;&#23558;&#22522;&#20110;&#29305;&#24449;&#30340;&#35821;&#20041;&#20449;&#24687;&#19982;&#39640;&#25928;&#21097;&#20313;&#32593;&#32476;&#30340;&#20840;&#23616;&#19978;&#19979;&#25991;&#30456;&#32467;&#21512;&#12290;&#21516;&#26102;&#65292;&#22312;&#35299;&#30721;&#22120;&#37096;&#20998;&#37319;&#29992;&#20102;&#21463;&#21040;AbM&#21551;&#21457;&#30340;&#39069;&#22806;&#27880;&#24847;&#21147;&#34701;&#21512;&#32593;&#32476;&#65288;AfNs&#65289;&#12290;AfNs&#26088;&#22312;&#36890;&#36807;&#22312;&#35299;&#30721;&#22120;&#37096;&#20998;&#37096;&#32626;&#39069;&#22806;&#30340;&#21367;&#31215;&#23618;&#65292;&#25913;&#21892;&#35821;&#20041;&#20449;&#24687;&#30340;&#36880;&#19968;&#36716;&#25442;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#23558;&#32593;&#32476;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;CamVid&#21644;Cityscapes&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Improving the efficiency of state-of-the-art methods in semantic segmentation requires overcoming the increasing computational cost as well as issues such as fusing semantic information from global and local contexts. Based on the recent success and problems that convolutional neural networks (CNNs) encounter in semantic segmentation, this research proposes an encoder-decoder architecture with a unique efficient residual network. Attention-boosting gates (AbGs) and attention-boosting modules (AbMs) are deployed by aiming to fuse the feature-based semantic information with the global context of the efficient residual network in the encoder. Respectively, the decoder network is developed with the additional attention-fusion networks (AfNs) inspired by AbM. AfNs are designed to improve the efficiency in the one-to-one conversion of the semantic information by deploying additional convolution layers in the decoder part. Our network is tested on the challenging CamVid and Cityscapes dataset
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20351;&#29992;&#37325;&#36848;&#25216;&#26415;&#25913;&#36827;&#23545;&#35805;&#24335;&#38382;&#31572;&#24615;&#33021;&#30340;CornNet&#27169;&#22411;</title><link>https://arxiv.org/abs/2312.17269</link><description>&lt;p&gt;
&#20351;&#29992;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#37325;&#36848;&#36827;&#34892;&#23545;&#35805;&#24335;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Conversational Question Answering with Reformulations over Knowledge Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.17269
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20351;&#29992;&#37325;&#36848;&#25216;&#26415;&#25913;&#36827;&#23545;&#35805;&#24335;&#38382;&#31572;&#24615;&#33021;&#30340;CornNet&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#24335;&#38382;&#31572;&#65288;convQA&#65289;&#26159;&#20851;&#20110;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#30340;&#22810;&#36718;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#30340;&#22238;&#31572;&#12290;&#30446;&#21069;&#30340;convQA&#26041;&#27861;&#36890;&#24120;&#22312;&#38590;&#20197;&#29702;&#35299;&#30340;&#38382;&#31572;&#37197;&#23545;&#26041;&#38754;&#36935;&#21040;&#22256;&#38590;&#12290;&#36825;&#20123;&#36755;&#20837;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#22312;&#23545;&#35805;&#21382;&#21490;&#30340;&#22522;&#30784;&#19978;&#24456;&#23481;&#26131;&#29702;&#35299;&#65292;&#20294;&#23545;&#20110;&#26426;&#22120;&#26469;&#35828;&#24456;&#38590;&#35299;&#37322;&#65292;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;convQA&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#27169;&#22411;CornNet&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#30340;&#38382;&#39064;&#37325;&#36848;&#26469;&#25552;&#39640;convQA&#24615;&#33021;&#12290;CornNet&#37319;&#29992;&#25945;&#24072;-&#23398;&#29983;&#26550;&#26500;&#65292;&#20854;&#20013;&#25945;&#24072;&#27169;&#22411;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#37325;&#36848;&#26469;&#23398;&#20064;&#38382;&#39064;&#34920;&#31034;&#65292;&#23398;&#29983;&#27169;&#22411;&#36890;&#36807;LLMs&#29983;&#25104;&#30340;&#37325;&#36848;&#26469;&#27169;&#20223;&#25945;&#24072;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#28982;&#21518;&#65292;RL&#27169;&#22411;&#20351;&#29992;&#23398;&#21040;&#30340;&#38382;&#39064;&#34920;&#31034;&#26469;&#22312;KG&#20013;&#23450;&#20301;&#27491;&#30830;&#31572;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.17269v2 Announce Type: replace-cross  Abstract: Conversational question answering (convQA) over knowledge graphs (KGs) involves answering multi-turn natural language questions about information contained in a KG. State-of-the-art methods of ConvQA often struggle with inexplicit question-answer pairs. These inputs are easy for human beings to understand given a conversation history, but hard for a machine to interpret, which can degrade ConvQA performance. To address this problem, we propose a reinforcement learning (RL) based model, CornNet, which utilizes question reformulations generated by large language models (LLMs) to improve ConvQA performance. CornNet adopts a teacher-student architecture where a teacher model learns question representations using human writing reformulations, and a student model to mimic the teacher model's output via reformulations generated by LLMs. The learned question representation is then used by an RL model to locate the correct answer in a K
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;&#26412;&#20307;&#24863;&#30693;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#27169;&#22411;&#65292;&#36890;&#36807;SAP&#21644;DRL&#31574;&#30053;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#40065;&#26834;&#22320;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29031;&#26126;&#21644;&#23380;&#38754;&#26465;&#20214;&#19979;&#30340;&#25554;&#38144;&#23380;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2312.16438</link><description>&lt;p&gt;
&#35270;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;&#26412;&#20307;&#24863;&#30693;&#25968;&#25454;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#19981;&#21516;&#26465;&#20214;&#19979;&#40065;&#26834;&#30340;&#25554;&#38144;&#23380;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Visual Spatial Attention and Proprioceptive Data-Driven Reinforcement Learning for Robust Peg-in-Hole Task Under Variable Conditions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.16438
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#35273;&#31354;&#38388;&#27880;&#24847;&#21147;&#21644;&#26412;&#20307;&#24863;&#30693;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#27169;&#22411;&#65292;&#36890;&#36807;SAP&#21644;DRL&#31574;&#30053;&#36827;&#34892;&#31471;&#21040;&#31471;&#35757;&#32451;&#65292;&#40065;&#26834;&#22320;&#35299;&#20915;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29031;&#26126;&#21644;&#23380;&#38754;&#26465;&#20214;&#19979;&#30340;&#25554;&#38144;&#23380;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38170;&#26643;&#25554;&#20837;&#26159;&#24314;&#31569;&#39046;&#22495;&#20013;&#36827;&#34892;&#28151;&#20957;&#22303;&#23380;&#25554;&#38144;&#20219;&#21153;&#65292;&#24050;&#32463;&#20570;&#20986;&#20102;&#21162;&#21147;&#33258;&#21160;&#21270;&#36825;&#39033;&#20219;&#21153;&#65292;&#20294;&#26159;&#21464;&#21270;&#22810;&#31471;&#30340;&#20809;&#29031;&#21644;&#23380;&#34920;&#38754;&#26465;&#20214;&#20197;&#21450;&#23545;&#30701;&#26242;&#35774;&#32622;&#21644;&#20219;&#21153;&#25191;&#34892;&#26102;&#38388;&#30340;&#38656;&#27714;&#20351;&#24471;&#33258;&#21160;&#21270;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#35270;&#35273;&#21644;&#26412;&#20307;&#24863;&#30693;&#25968;&#25454;&#39537;&#21160;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#27169;&#22411;&#65292;&#29992;&#20110;&#40065;&#26834;&#22320;&#24212;&#23545;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29031;&#26126;&#21644;&#23380;&#38754;&#26465;&#20214;&#12290;&#35813;&#27169;&#22411;&#30001;&#31354;&#38388;&#27880;&#24847;&#28857;&#32593;&#32476;&#65288;SAP&#65289;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#31574;&#30053;&#32452;&#25104;&#65292;&#20108;&#32773;&#20849;&#21516;&#31471;&#21040;&#31471;&#22320;&#36827;&#34892;&#35757;&#32451;&#20197;&#25511;&#21046;&#26426;&#22120;&#20154;&#12290;&#35813;&#27169;&#22411;&#20197;&#31163;&#32447;&#26041;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26679;&#26412;&#39640;&#25928;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#24182;&#26368;&#23567;&#21270;&#22312;&#23558;&#27169;&#22411;&#36716;&#31227;&#21040;&#29616;&#23454;&#19990;&#30028;&#26102;&#30340;&#29616;&#23454;&#24046;&#36317;&#12290;&#36890;&#36807;&#23545;&#19968;&#21488;&#24037;&#19994;&#26426;&#22120;&#20154;&#22312;12&#20010;&#26410;&#30693;&#23380;&#20013;&#36827;&#34892;&#20219;&#21153;&#25191;&#34892;&#30340;&#35780;&#20272;&#65292;&#22987;&#20110;16&#20010;&#19981;&#21516;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.16438v2 Announce Type: replace-cross  Abstract: Anchor-bolt insertion is a peg-in-hole task performed in the construction field for holes in concrete. Efforts have been made to automate this task, but the variable lighting and hole surface conditions, as well as the requirements for short setup and task execution time make the automation challenging. In this study, we introduce a vision and proprioceptive data-driven robot control model for this task that is robust to challenging lighting and hole surface conditions. This model consists of a spatial attention point network (SAP) and a deep reinforcement learning (DRL) policy that are trained jointly end-to-end to control the robot. The model is trained in an offline manner, with a sample-efficient framework designed to reduce training time and minimize the reality gap when transferring the model to the physical world. Through evaluations with an industrial robot performing the task in 12 unknown holes, starting from 16 diffe
&lt;/p&gt;</description></item><item><title>GlitchBench&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#28216;&#25103;&#36136;&#37327;&#20445;&#35777;&#20219;&#21153;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#25361;&#25112;LMMs&#22312;&#26816;&#27979;&#21644;&#35299;&#37322;&#24322;&#24120;&#20107;&#20214;&#26041;&#38754;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.05291</link><description>&lt;p&gt;
GlitchBench&#65306;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#33021;&#22815;&#26816;&#27979;&#35270;&#39057;&#28216;&#25103;&#28431;&#27934;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
GlitchBench: Can large multimodal models detect video game glitches?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.05291
&lt;/p&gt;
&lt;p&gt;
GlitchBench&#26159;&#19968;&#20010;&#22522;&#20110;&#35270;&#39057;&#28216;&#25103;&#36136;&#37327;&#20445;&#35777;&#20219;&#21153;&#30340;&#26032;&#22522;&#20934;&#65292;&#26088;&#22312;&#25361;&#25112;LMMs&#22312;&#26816;&#27979;&#21644;&#35299;&#37322;&#24322;&#24120;&#20107;&#20214;&#26041;&#38754;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#65288;LMMs&#65289;&#24050;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21457;&#23637;&#32780;&#26469;&#65292;&#20197;&#25972;&#21512;&#22810;&#31181;&#36755;&#20837;&#27169;&#24577;&#65292;&#22914;&#35270;&#35273;&#36755;&#20837;&#12290;&#36825;&#31181;&#25972;&#21512;&#22686;&#24378;&#20102;LLMs&#22312;&#38656;&#35201;&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#20219;&#21153;&#19978;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23588;&#20854;&#26159;&#22312;&#28041;&#21450;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26102;&#65292;&#23427;&#20204;&#22686;&#24378;&#30340;&#33021;&#21147;&#30340;&#31243;&#24230;&#21644;&#38480;&#21046;&#23578;&#26410;&#23436;&#20840;&#34987;&#29702;&#35299;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;GlitchBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#65292;&#28304;&#33258;&#20110;&#35270;&#39057;&#28216;&#25103;&#36136;&#37327;&#20445;&#35777;&#20219;&#21153;&#65292;&#26088;&#22312;&#27979;&#35797;&#21644;&#35780;&#20272;LMMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#22522;&#20934;&#26159;&#20174;&#21508;&#31181;&#35270;&#39057;&#28216;&#25103;&#20013;&#30340;&#19981;&#23547;&#24120;&#21644;&#20986;&#29616;&#25925;&#38556;&#30340;&#22330;&#26223;&#31934;&#24515;&#31574;&#21010;&#32780;&#25104;&#65292;&#26088;&#22312;&#25361;&#25112;LMMs&#22312;&#26816;&#27979;&#21644;&#35299;&#37322;&#38750;&#21516;&#23547;&#24120;&#20107;&#20214;&#26041;&#38754;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#22810;&#20010;&#26368;&#20808;&#36827;&#30340;LMMs&#65292;&#24182;&#23637;&#31034;&#20102;GlitchBench&#20026;&#36825;&#20123;&#27169;&#22411;&#25552;&#20986;&#20102;&#26032;&#25361;&#25112;&#12290; &#20195;&#30721;&#21644;&#25968;&#25454;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#25214;&#21040;&#65306;https://glitchb
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.05291v2 Announce Type: replace-cross  Abstract: Large multimodal models (LMMs) have evolved from large language models (LLMs) to integrate multiple input modalities, such as visual inputs. This integration augments the capacity of LLMs for tasks requiring visual comprehension and reasoning. However, the extent and limitations of their enhanced abilities are not fully understood, especially when it comes to real-world tasks. To address this gap, we introduce GlitchBench, a novel benchmark derived from video game quality assurance tasks, to test and evaluate the reasoning capabilities of LMMs. Our benchmark is curated from a variety of unusual and glitched scenarios from video games and aims to challenge both the visual and linguistic reasoning powers of LMMs in detecting and interpreting out-of-the-ordinary events. We evaluate multiple state-of-the-art LMMs, and we show that GlitchBench presents a new challenge for these models. Code and data are available at: https://glitchb
&lt;/p&gt;</description></item><item><title>&#24555;&#36895;&#30005;&#26426;&#36866;&#24212;&#24615;&#65288;RMA&#65289;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30340;&#27867;&#21270;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#28145;&#24230;&#24863;&#30693;&#24320;&#21457;&#20102;&#38024;&#23545;&#21508;&#31181;&#25805;&#32437;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;</title><link>https://arxiv.org/abs/2312.04670</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#25805;&#32437;&#33218;&#30340;&#24555;&#36895;&#30005;&#26426;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;
Rapid Motor Adaptation for Robotic Manipulator Arms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04670
&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#30005;&#26426;&#36866;&#24212;&#24615;&#65288;RMA&#65289;&#20026;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#30340;&#27867;&#21270;&#25552;&#20986;&#20102;&#35299;&#20915;&#26041;&#26696;&#65292;&#21033;&#29992;&#28145;&#24230;&#24863;&#30693;&#24320;&#21457;&#20102;&#38024;&#23545;&#21508;&#31181;&#25805;&#32437;&#20219;&#21153;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#36890;&#29992;&#26426;&#22120;&#20154;&#25805;&#20316;&#25216;&#33021;&#26159;&#20307;&#29616;&#20154;&#24037;&#26234;&#33021;&#30340;&#26680;&#24515;&#25361;&#25112;&#12290;&#36825;&#21253;&#25324;&#36328;&#36234;&#21508;&#31181;&#20219;&#21153;&#37197;&#32622;&#30340;&#27867;&#21270;&#65292;&#28085;&#30422;&#20102;&#23545;&#35937;&#24418;&#29366;&#12289;&#23494;&#24230;&#12289;&#25705;&#25830;&#31995;&#25968;&#30340;&#21464;&#21270;&#20197;&#21450;&#22806;&#37096;&#24178;&#25200;&#65292;&#22914;&#26045;&#21152;&#22312;&#26426;&#22120;&#20154;&#36523;&#19978;&#30340;&#21147;&#12290;&#24555;&#36895;&#30005;&#26426;&#36866;&#24212;&#24615;&#65288;Rapid Motor Adaptation&#65292;RMA&#65289;&#20026;&#36825;&#19968;&#25361;&#25112;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#35748;&#20026;&#65292;&#24433;&#21709;&#26234;&#33021;&#20307;&#20219;&#21153;&#34920;&#29616;&#30340;&#22522;&#26412;&#38544;&#21464;&#37327;&#65292;&#22914;&#23545;&#35937;&#36136;&#37327;&#21644;&#24418;&#29366;&#65292;&#21487;&#20197;&#20174;&#26234;&#33021;&#20307;&#30340;&#21160;&#20316;&#21644;&#26412;&#20307;&#24863;&#24615;&#21382;&#21490;&#20013;&#26377;&#25928;&#22320;&#25512;&#26029;&#20986;&#26469;&#12290;&#25105;&#20204;&#20511;&#37492;&#20102;&#22312;&#31227;&#21160;&#21644;&#25163;&#20869;&#26059;&#36716;&#20013;&#30340;RMA&#65292;&#21033;&#29992;&#28145;&#24230;&#24863;&#30693;&#26469;&#24320;&#21457;&#38024;&#23545;&#21508;&#31181;&#25805;&#32437;&#20219;&#21153;&#30340;&#24555;&#36895;&#30005;&#26426;&#36866;&#24212;&#24615;&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;Maniskill2&#22522;&#20934;&#27979;&#35797;&#20013;&#23545;&#25105;&#20204;&#30340;&#20195;&#29702;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#36825;&#20123;&#20219;&#21153;&#21253;&#25324;&#20174;YCB&#21644;EGAD&#25968;&#25454;&#38598;&#20013;&#25968;&#30334;&#20010;&#23545;&#35937;&#30340;&#21462;&#25918;&#25805;&#20316;&#65292;&#20197;&#21450;&#31934;&#30830;&#23450;&#20301;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04670v2 Announce Type: replace-cross  Abstract: Developing generalizable manipulation skills is a core challenge in embodied AI. This includes generalization across diverse task configurations, encompassing variations in object shape, density, friction coefficient, and external disturbances such as forces applied to the robot. Rapid Motor Adaptation (RMA) offers a promising solution to this challenge. It posits that essential hidden variables influencing an agent's task performance, such as object mass and shape, can be effectively inferred from the agent's action and proprioceptive history. Drawing inspiration from RMA in locomotion and in-hand rotation, we use depth perception to develop agents tailored for rapid motor adaptation in a variety of manipulation tasks. We evaluated our agents on four challenging tasks from the Maniskill2 benchmark, namely pick-and-place operations with hundreds of objects from the YCB and EGAD datasets, peg insertion with precise position and 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#20801;&#35768;&#20174;&#21333;&#20010;&#36830;&#32493;&#35270;&#39057;&#27969;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#20687;&#32032;&#32423;&#24314;&#27169;&#23454;&#29616;&#39044;&#35757;&#32451;&#21644;&#21333;&#27969;&#35780;&#20272;&#20043;&#38388;&#30340;&#28789;&#27963;&#20999;&#25442;&#65292;&#24182;&#33719;&#24471;&#20102;&#22823;&#37327;&#21333;&#27969;&#23398;&#20064;&#30340;&#25910;&#30410;&#12290;</title><link>https://arxiv.org/abs/2312.00598</link><description>&lt;p&gt;
&#20174;&#21333;&#20010;&#36830;&#32493;&#35270;&#39057;&#27969;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from One Continuous Video Stream
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.00598
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22312;&#32447;&#23398;&#20064;&#26694;&#26550;&#65292;&#20801;&#35768;&#20174;&#21333;&#20010;&#36830;&#32493;&#35270;&#39057;&#27969;&#20013;&#23398;&#20064;&#65292;&#36890;&#36807;&#20687;&#32032;&#32423;&#24314;&#27169;&#23454;&#29616;&#39044;&#35757;&#32451;&#21644;&#21333;&#27969;&#35780;&#20272;&#20043;&#38388;&#30340;&#28789;&#27963;&#20999;&#25442;&#65292;&#24182;&#33719;&#24471;&#20102;&#22823;&#37327;&#21333;&#27969;&#23398;&#20064;&#30340;&#25910;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#38024;&#23545;&#20174;&#21333;&#20010;&#36830;&#32493;&#35270;&#39057;&#27969;&#20013;&#36827;&#34892;&#22312;&#32447;&#23398;&#20064;&#30340;&#26694;&#26550;--&#23601;&#20687;&#20154;&#31867;&#21644;&#21160;&#29289;&#23398;&#20064;&#30340;&#26041;&#24335;&#19968;&#26679;&#65292;&#26080;&#38656;&#23567;&#25209;&#37327;&#12289;&#25968;&#25454;&#22686;&#24378;&#25110;&#27927;&#29260;&#12290;&#30001;&#20110;&#36830;&#32493;&#35270;&#39057;&#24103;&#20043;&#38388;&#30340;&#39640;&#30456;&#20851;&#24615;&#65292;&#36825;&#24102;&#26469;&#20102;&#24456;&#22823;&#30340;&#25361;&#25112;&#65292;&#24182;&#19988;&#22312;&#36825;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20351;&#25105;&#20204;&#39318;&#27425;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#19968;&#20027;&#39064;&#65292;&#21253;&#25324;&#20102;&#30001;&#20004;&#20010;&#29616;&#26377;&#35270;&#39057;&#25968;&#25454;&#38598;&#32452;&#25104;&#30340;&#19968;&#32452;&#27969;&#21644;&#20219;&#21153;&#65292;&#20197;&#21450;&#32771;&#34385;&#20102;&#36866;&#24212;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#35780;&#20272;&#30340;&#26041;&#27861;&#35770;&#12290;&#25105;&#20204;&#37319;&#29992;&#20687;&#32032;&#32423;&#24314;&#27169;&#20316;&#20026;&#19968;&#31181;&#23454;&#29992;&#19988;&#28789;&#27963;&#30340;&#26041;&#24335;&#65292;&#21487;&#20197;&#22312;&#39044;&#35757;&#32451;&#21644;&#21333;&#20010;&#27969;&#35780;&#20272;&#20043;&#38388;&#20197;&#21450;&#22312;&#20219;&#24847;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#20999;&#25442;&#65292;&#32780;&#26080;&#38656;&#25913;&#21464;&#27169;&#22411;&#65292;&#24182;&#22987;&#32456;&#20351;&#29992;&#30456;&#21516;&#30340;&#20687;&#32032;&#25439;&#22833;&#12290;&#20511;&#21161;&#36825;&#20010;&#26694;&#26550;&#65292;&#25105;&#20204;&#36890;&#36807;&#39044;&#35757;&#32451;&#19968;&#32452;&#26032;&#39062;&#30340;&#26410;&#26469;&#39044;&#27979;&#20219;&#21153;&#33719;&#24471;&#20102;&#22823;&#37327;&#21333;&#27969;&#23398;&#20064;&#30340;&#25910;&#30410;&#65292;foun
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.00598v2 Announce Type: replace-cross  Abstract: We introduce a framework for online learning from a single continuous video stream -- the way people and animals learn, without mini-batches, data augmentation or shuffling. This poses great challenges given the high correlation between consecutive video frames and there is very little prior work on it. Our framework allows us to do a first deep dive into the topic and includes a collection of streams and tasks composed from two existing video datasets, plus methodology for performance evaluation that considers both adaptation and generalization. We employ pixel-to-pixel modelling as a practical and flexible way to switch between pre-training and single-stream evaluation as well as between arbitrary tasks, without ever requiring changes to models and always using the same pixel loss. Equipped with this framework we obtained large single-stream learning gains from pre-training with a novel family of future prediction tasks, foun
&lt;/p&gt;</description></item><item><title>TransNeXt&#25552;&#20986;&#20102;Aggregated Attention&#65292;&#19968;&#31181;&#20223;&#29983;&#35774;&#35745;&#30340;&#20196;&#29260;&#28151;&#21512;&#22120;&#65292;&#36890;&#36807;&#27169;&#25311;&#29983;&#29289;&#35270;&#35273;&#21644;&#36830;&#32493;&#30524;&#21160;&#65292;&#20351;&#24471;&#27599;&#20010;&#29305;&#24449;&#22270;&#19978;&#30340;&#20196;&#29260;&#20855;&#26377;&#20840;&#23616;&#24863;&#30693;&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#21472;&#21152;&#23618;&#36827;&#34892;&#20449;&#24687;&#20132;&#25442;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#28145;&#24230;&#36864;&#21270;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#30340;&#35270;&#30693;&#35273;&#12290;</title><link>https://arxiv.org/abs/2311.17132</link><description>&lt;p&gt;
TransNeXt&#65306;Vision Transformers&#30340;&#40065;&#26834;&#20239;&#33033;&#35270;&#30693;&#35273;
&lt;/p&gt;
&lt;p&gt;
TransNeXt: Robust Foveal Visual Perception for Vision Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17132
&lt;/p&gt;
&lt;p&gt;
TransNeXt&#25552;&#20986;&#20102;Aggregated Attention&#65292;&#19968;&#31181;&#20223;&#29983;&#35774;&#35745;&#30340;&#20196;&#29260;&#28151;&#21512;&#22120;&#65292;&#36890;&#36807;&#27169;&#25311;&#29983;&#29289;&#35270;&#35273;&#21644;&#36830;&#32493;&#30524;&#21160;&#65292;&#20351;&#24471;&#27599;&#20010;&#29305;&#24449;&#22270;&#19978;&#30340;&#20196;&#29260;&#20855;&#26377;&#20840;&#23616;&#24863;&#30693;&#65292;&#19988;&#19981;&#20381;&#36182;&#20110;&#21472;&#21152;&#23618;&#36827;&#34892;&#20449;&#24687;&#20132;&#25442;&#65292;&#26377;&#25928;&#36991;&#20813;&#20102;&#28145;&#24230;&#36864;&#21270;&#65292;&#23454;&#29616;&#20102;&#33258;&#28982;&#30340;&#35270;&#30693;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#27531;&#24046;&#36830;&#25509;&#20013;&#30340;&#28145;&#24230;&#36864;&#21270;&#25928;&#24212;&#65292;&#35768;&#22810;&#20381;&#36182;&#20110;&#21472;&#21152;&#23618;&#36827;&#34892;&#20449;&#24687;&#20132;&#25442;&#30340;&#39640;&#25928;Vision Transformers&#27169;&#22411;&#36890;&#24120;&#26080;&#27861;&#24418;&#25104;&#36275;&#22815;&#30340;&#20449;&#24687;&#28151;&#21512;&#65292;&#23548;&#33268;&#35270;&#30693;&#35273;&#19981;&#33258;&#28982;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32858;&#21512;&#27880;&#24847;&#21147;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20223;&#29983;&#35774;&#35745;&#30340;&#20196;&#29260;&#28151;&#21512;&#22120;&#65292;&#27169;&#25311;&#29983;&#29289;&#35270;&#35273;&#21644;&#36830;&#32493;&#30524;&#21160;&#65292;&#21516;&#26102;&#20351;&#29305;&#24449;&#22270;&#19978;&#30340;&#27599;&#20010;&#20196;&#29260;&#20855;&#26377;&#20840;&#23616;&#24863;&#30693;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#21487;&#23398;&#20064;&#30340;&#20196;&#29260;&#32435;&#20837;&#24120;&#35268;&#26597;&#35810;&#21644;&#23494;&#38053;&#20013;&#65292;&#36827;&#19968;&#27493;&#20351;&#20146;&#21644;&#30697;&#38453;&#30340;&#29983;&#25104;&#22810;&#26679;&#21270;&#65292;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#26597;&#35810;&#21644;&#23494;&#38053;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#21472;&#21152;&#36827;&#34892;&#20449;&#24687;&#20132;&#25442;&#65292;&#22240;&#27492;&#26377;&#25928;&#36991;&#20813;&#20102;&#28145;&#24230;&#36864;&#21270;&#65292;&#24182;&#23454;&#29616;&#20102;&#33258;&#28982;&#30340;&#35270;&#30693;&#35273;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Convolutional GLU&#65292;&#36825;&#26159;&#19968;&#31181;&#36890;&#36947;&#28151;&#21512;&#22120;&#65292;&#25645;&#36215;&#20102;&#35270;&#30693;&#35273;&#20013;&#30340;&#26029;&#23618;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17132v2 Announce Type: replace-cross  Abstract: Due to the depth degradation effect in residual connections, many efficient Vision Transformers models that rely on stacking layers for information exchange often fail to form sufficient information mixing, leading to unnatural visual perception. To address this issue, in this paper, we propose Aggregated Attention, a biomimetic design-based token mixer that simulates biological foveal vision and continuous eye movement while enabling each token on the feature map to have a global perception. Furthermore, we incorporate learnable tokens that interact with conventional queries and keys, which further diversifies the generation of affinity matrices beyond merely relying on the similarity between queries and keys. Our approach does not rely on stacking for information exchange, thus effectively avoiding depth degradation and achieving natural visual perception. Additionally, we propose Convolutional GLU, a channel mixer that bridg
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#20351;&#29992;VLM&#21644;&#26174;&#33879;&#24615;&#20002;&#24323;&#26469;&#35299;&#20915;&#36807;&#24230;&#20998;&#21106;&#21644;&#27424;&#20998;&#21106;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2311.17095</link><description>&lt;p&gt;
&#20174;&#29616;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;&#30340;&#32039;&#24613;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Emergent Open-Vocabulary Semantic Segmentation from Off-the-shelf Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17095
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#65292;&#36890;&#36807;&#20351;&#29992;VLM&#21644;&#26174;&#33879;&#24615;&#20002;&#24323;&#26469;&#35299;&#20915;&#36807;&#24230;&#20998;&#21106;&#21644;&#27424;&#20998;&#21106;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20013;&#65292;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#23398;&#20064;&#38544;&#24335;&#23558;&#22270;&#20687;&#21306;&#22495;&#19982;&#35789;&#27719;&#20851;&#32852;&#36215;&#26469;&#65292;&#36825;&#23545;&#20110;&#35832;&#22914;&#35270;&#35273;&#38382;&#31572;&#31561;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#20851;&#32852;&#36827;&#34892;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26497;&#20854;&#26377;&#25928;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#25216;&#26415;&#65292;Plug-and-Play Open-Vocabulary Semantic Segmentation (PnP-OVSS)&#12290;PnP-OVSS&#21033;&#29992;&#20855;&#26377;&#30452;&#25509;&#25991;&#26412;&#21040;&#22270;&#20687;&#20132;&#21449;&#27880;&#24847;&#21147;&#21644;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#25439;&#22833;&#30340;VLM&#12290;&#20026;&#20102;&#22312;&#36807;&#24230;&#20998;&#21106;&#21644;&#27424;&#20998;&#21106;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26174;&#33879;&#24615;&#20002;&#24323;&#65288;Salience Dropout&#65289;&#65307;&#36890;&#36807;&#36845;&#20195;&#20002;&#24323;&#27169;&#22411;&#26368;&#20851;&#27880;&#30340;&#34917;&#19969;&#65292;&#25105;&#20204;&#33021;&#22815;&#26356;&#22909;&#22320;&#35299;&#20915;&#25972;&#20010;&#20998;&#21106;&#25513;&#27169;&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17095v2 Announce Type: replace-cross  Abstract: From image-text pairs, large-scale vision-language models (VLMs) learn to implicitly associate image regions with words, which prove effective for tasks like visual question answering. However, leveraging the learned association for open-vocabulary semantic segmentation remains a challenge. In this paper, we propose a simple, yet extremely effective, training-free technique, Plug-and-Play Open-Vocabulary Semantic Segmentation (PnP-OVSS) for this task. PnP-OVSS leverages a VLM with direct text-to-image cross-attention and an image-text matching loss. To balance between over-segmentation and under-segmentation, we introduce Salience Dropout; by iteratively dropping patches that the model is most attentive to, we are able to better resolve the entire extent of the segmentation mask. \shortname{} does not require any neural network training and performs hyperparameter tuning without the need for any segmentation annotations, even f
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#65292;&#20197;&#20811;&#26381;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.17076</link><description>&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#32452;&#21512;&#24335;&#24605;&#32500;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Compositional Chain-of-Thought Prompting for Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.17076
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#65292;&#20197;&#20811;&#26381;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#22823;&#30340;&#35270;&#35273;&#39592;&#24178;&#21644;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#25512;&#29702;&#30340;&#32467;&#21512;&#24050;&#32463;&#23548;&#33268;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMM)&#25104;&#20026;&#24403;&#21069;&#24191;&#27867;&#35270;&#35273;&#21644;&#35821;&#35328;(VL)&#20219;&#21153;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;LMM&#20173;&#28982;&#38590;&#20197;&#25429;&#25417;&#21040;&#32452;&#21512;&#35270;&#35273;&#25512;&#29702;&#26041;&#38754;&#30340;&#32454;&#33410;&#65292;&#27604;&#22914;&#23545;&#35937;&#20043;&#38388;&#30340;&#23646;&#24615;&#21644;&#20851;&#31995;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#21033;&#29992;&#22330;&#26223;&#22270;(SGs)&#8212;&#8212;&#23545;&#35937;&#21450;&#20854;&#20851;&#31995;&#21644;&#23646;&#24615;&#30340;&#24418;&#24335;&#21270;&#34920;&#36798;&#65292;&#23427;&#24050;&#34987;&#24191;&#27867;&#29992;&#20316;&#35270;&#35273;&#21644;&#25991;&#26412;&#39046;&#22495;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#28982;&#32780;&#65292;&#22330;&#26223;&#22270;&#25968;&#25454;&#38656;&#35201;&#22330;&#26223;&#22270;&#27880;&#37322;&#65292;&#36825;&#31181;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#39640;&#26114;&#65292;&#22240;&#27492;&#38590;&#20197;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#22330;&#26223;&#22270;&#25968;&#25454;&#24494;&#35843;LMM&#21487;&#33021;&#23548;&#33268;&#39044;&#35757;&#32451;&#30446;&#26631;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38382;&#39064;&#65292;&#21463;&#21040;&#24605;&#32500;&#38142;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38646;&#26679;&#24335;&#24605;&#32500;&#25552;&#31034;&#65288;CCoT&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.17076v2 Announce Type: replace-cross  Abstract: The combination of strong visual backbones and Large Language Model (LLM) reasoning has led to Large Multimodal Models (LMMs) becoming the current standard for a wide range of vision and language (VL) tasks. However, recent research has shown that even the most advanced LMMs still struggle to capture aspects of compositional visual reasoning, such as attributes and relationships between objects. One solution is to utilize scene graphs (SGs)--a formalization of objects and their relations and attributes that has been extensively used as a bridge between the visual and textual domains. Yet, scene graph data requires scene graph annotations, which are expensive to collect and thus not easily scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic forgetting of the pretraining objective. To overcome this, inspired by chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a novel zero-sho
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#38169;&#35823;&#32416;&#27491;&#30340;&#25968;&#25454;&#23545;&#26469;&#25913;&#36827;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25552;&#21319;&#20165;&#20351;&#29992;CoT&#36827;&#34892;&#24494;&#35843;&#21518;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.20689</link><description>&lt;p&gt;
&#23398;&#20064;&#20174;&#38169;&#35823;&#20013;&#20351;LLM&#25104;&#20026;&#26356;&#22909;&#30340;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Learning From Mistakes Makes LLM Better Reasoner
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.20689
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#20174;&#38169;&#35823;&#20013;&#23398;&#20064;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#23398;&#20064;&#30340;&#36807;&#31243;&#65292;&#24182;&#36890;&#36807;&#24341;&#20837;&#38169;&#35823;&#32416;&#27491;&#30340;&#25968;&#25454;&#23545;&#26469;&#25913;&#36827;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#25345;&#32493;&#25552;&#21319;&#20165;&#20351;&#29992;CoT&#36827;&#34892;&#24494;&#35843;&#21518;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#21331;&#36234;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20026;&#20102;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;LLM&#26159;&#21542;&#21487;&#20197;&#23398;&#20064;&#20174;&#38169;&#35823;&#20013;&#33719;&#30410;&#65288;LEMA&#65289;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#32771;&#34385;&#19968;&#20010;&#26410;&#33021;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#30340;&#20154;&#31867;&#23398;&#29983;&#65292;&#20182;&#20250;&#20174;&#33258;&#24049;&#29359;&#30340;&#38169;&#35823;&#20013;&#23398;&#20064;&#65292;&#24182;&#32416;&#27491;&#23427;&#12290;&#27169;&#20223;&#36825;&#31181;&#38169;&#35823;&#39537;&#21160;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;LEMA&#22312;LLM&#30340;&#24494;&#35843;&#36807;&#31243;&#20013;&#24341;&#20837;&#20102;&#38169;&#35823;&#32416;&#27491;&#30340;&#25968;&#25454;&#23545;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#26469;&#33258;&#21508;&#31181;LLM&#30340;&#38169;&#35823;&#25512;&#29702;&#36335;&#24452;&#65292;&#28982;&#21518;&#20351;&#29992;GPT-4&#20316;&#20026;&#8220;&#32416;&#27491;&#32773;&#8221;&#26469;&#35782;&#21035;&#38169;&#35823;&#27493;&#39588;&#65292;&#35299;&#37322;&#38169;&#35823;&#21407;&#22240;&#65292;&#32416;&#27491;&#38169;&#35823;&#24182;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#32416;&#27491;&#30340;&#36827;&#21270;&#31574;&#30053;&#65292;&#26377;&#25928;&#22320;&#25193;&#23637;&#20102;&#29983;&#25104;&#32416;&#27491;&#25968;&#25454;&#30340;&#38382;&#39064;&#38598;&#12290;&#22312;&#21508;&#31181;LLM&#21644;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LEMA&#22987;&#32456;&#21487;&#20197;&#25552;&#21319;&#20165;&#20351;&#29992;CoT&#30340;&#24494;&#35843;&#12290;&#25105;&#20204;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) recently exhibited remarkable reasoning capabilities on solving math problems. To further improve their reasoning capabilities, this work explores whether LLMs can LEarn from MistAkes (LEMA), akin to the human learning process. Consider a human student who failed to solve a math problem, he will learn from what mistake he has made and how to correct it. Mimicking this error-driven learning process, LEMA incorporates mistake-correction data pairs during fine-tuning LLMs. Specifically, we first collect inaccurate reasoning paths from various LLMs, and then employ GPT-4 as a "corrector" to identify the mistake step, explain the reason for the mistake, correct the mistake and generate the final answer. In addition, we apply a correction-centric evolution strategy that effectively expands the question set for generating correction data. Experiments across various LLMs and reasoning tasks show that \textsc{LeMa} consistently improves CoT-alone fine-tuning. Our fu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#35856;&#27874;&#33258;&#35843;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#22312;&#22810;&#37197;&#20307;&#23545;&#25509;&#21644;&#32467;&#21512;&#20301;&#28857;&#35774;&#35745;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#29983;&#25104;&#36807;&#31243;&#21644;&#35774;&#35745;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2310.05764</link><description>&lt;p&gt;
&#22810;&#37197;&#20307;&#23545;&#25509;&#21644;&#32467;&#21512;&#20301;&#28857;&#35774;&#35745;&#30340;&#35856;&#27874;&#33258;&#35843;&#27969;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Harmonic Self-Conditioned Flow Matching for Multi-Ligand Docking and Binding Site Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05764
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#31181;&#35856;&#27874;&#33258;&#35843;&#27969;&#21305;&#37197;&#26041;&#27861;&#65292;&#22312;&#22810;&#37197;&#20307;&#23545;&#25509;&#21644;&#32467;&#21512;&#20301;&#28857;&#35774;&#35745;&#20013;&#34920;&#29616;&#20986;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#29983;&#25104;&#36807;&#31243;&#21644;&#35774;&#35745;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37327;&#34507;&#30333;&#36136;&#21151;&#33021;&#38656;&#35201;&#19982;&#23567;&#20998;&#23376;&#32467;&#21512;&#65292;&#21253;&#25324;&#37238;&#20652;&#21270;&#12290;&#22240;&#27492;&#65292;&#20026;&#23567;&#20998;&#23376;&#35774;&#35745;&#32467;&#21512;&#21475;&#34955;&#20855;&#26377;&#20174;&#33647;&#29289;&#21512;&#25104;&#21040;&#33021;&#37327;&#23384;&#20648;&#31561;&#22810;&#31181;&#24433;&#21709;&#28145;&#36828;&#30340;&#24212;&#29992;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;HarmonicFlow&#65292;&#36825;&#26159;&#19968;&#20010;&#25913;&#36827;&#30340;&#22522;&#20110;&#33258;&#35843;&#27969;&#21305;&#37197;&#30446;&#26631;&#30340;3D&#34507;&#30333;&#36136;-&#37197;&#20307;&#32467;&#21512;&#32467;&#26500;&#29983;&#25104;&#36807;&#31243;&#12290;FlowSite&#23558;&#36825;&#31181;&#27969;&#27169;&#22411;&#25193;&#23637;&#21040;&#32852;&#21512;&#29983;&#25104;&#34507;&#30333;&#36136;&#21475;&#34955;&#30340;&#31163;&#25955;&#27531;&#22522;&#31867;&#22411;&#21644;&#20998;&#23376;&#30340;&#32467;&#21512;3D&#32467;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;HarmonicFlow&#22312;&#21475;&#34955;&#32423;&#23545;&#25509;&#20013;&#22312;&#31616;&#21333;&#24615;&#12289;&#26222;&#36866;&#24615;&#21644;&#24179;&#22343;&#26679;&#26412;&#36136;&#37327;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#29983;&#25104;&#36807;&#31243;&#12290;&#20511;&#21161;&#20110;&#36825;&#31181;&#32467;&#26500;&#24314;&#27169;&#65292;FlowSite&#35774;&#35745;&#30340;&#32467;&#21512;&#20301;&#28857;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05764v3 Announce Type: replace-cross  Abstract: A significant amount of protein function requires binding small molecules, including enzymatic catalysis. As such, designing binding pockets for small molecules has several impactful applications ranging from drug synthesis to energy storage. Towards this goal, we first develop HarmonicFlow, an improved generative process over 3D protein-ligand binding structures based on our self-conditioned flow matching objective. FlowSite extends this flow model to jointly generate a protein pocket's discrete residue types and the molecule's binding 3D structure. We show that HarmonicFlow improves upon state-of-the-art generative processes for docking in simplicity, generality, and average sample quality in pocket-level docking. Enabled by this structure modeling, FlowSite designs binding sites substantially better than baseline approaches.
&lt;/p&gt;</description></item><item><title>&#20998;&#35789;&#22120;&#26159;&#35270;&#35273;&#29983;&#25104;&#30340;&#20851;&#38190;&#65292;&#26032;&#30340;&#35270;&#39057;&#20998;&#35789;&#22120;MAGVIT-v2&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#19978;&#32988;&#36807;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#35270;&#39057;&#21387;&#32553;&#21644;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2310.05737</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20987;&#36133;&#25193;&#25955;&#27169;&#22411;--&#20998;&#35789;&#22120;&#26159;&#35270;&#35273;&#29983;&#25104;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05737
&lt;/p&gt;
&lt;p&gt;
&#20998;&#35789;&#22120;&#26159;&#35270;&#35273;&#29983;&#25104;&#30340;&#20851;&#38190;&#65292;&#26032;&#30340;&#35270;&#39057;&#20998;&#35789;&#22120;MAGVIT-v2&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#19978;&#32988;&#36807;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#35270;&#39057;&#21387;&#32553;&#21644;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20027;&#23548;&#27169;&#22411;&#65292;&#20294;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#19981;&#22914;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#21033;&#29992;LLMs&#36827;&#34892;&#35270;&#35273;&#29983;&#25104;&#65292;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#20214;&#26159;&#35270;&#35273;&#20998;&#35789;&#22120;&#65292;&#23427;&#23558;&#20687;&#32032;&#31354;&#38388;&#36755;&#20837;&#26144;&#23556;&#21040;&#36866;&#21512;LLM&#23398;&#20064;&#30340;&#31163;&#25955;&#26631;&#35760;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MAGVIT-v2&#65292;&#19968;&#20010;&#35270;&#39057;&#20998;&#35789;&#22120;&#65292;&#26088;&#22312;&#20351;&#29992;&#20849;&#21516;&#30340;&#26631;&#35760;&#35789;&#27719;&#20026;&#35270;&#39057;&#21644;&#22270;&#20687;&#29983;&#25104;&#31616;&#27905;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#26631;&#35760;&#12290;&#37197;&#22791;&#20102;&#36825;&#20010;&#26032;&#30340;&#20998;&#35789;&#22120;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#22312;&#26631;&#20934;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#22522;&#20934;&#19978;&#20248;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#21253;&#25324;ImageNet&#21644;Kinetics&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#35789;&#22120;&#22312;&#20004;&#39033;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#20808;&#21069;&#34920;&#29616;&#26368;&#20339;&#30340;&#35270;&#39057;&#20998;&#35789;&#22120;&#65306;(1)&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#65292;&#35270;&#39057;&#21387;&#32553;&#19982;&#19979;&#19968;&#20195;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;(VCC)&#30456;&#23218;&#32654;&#65292;(2)&#23398;&#20064;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05737v2 Announce Type: replace-cross  Abstract: While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#35843;&#25972;&#65288;DAT&#65289;&#26041;&#27861;&#65292;&#20197;&#20351;&#24471;&#38754;&#21521;&#35821;&#20041;&#20998;&#21106;&#30340;&#36830;&#32493;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;CTTA&#65289;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26356;&#21152;&#39640;&#25928;&#21644;&#23454;&#29992;&#12290;</title><link>https://arxiv.org/abs/2309.13604</link><description>&lt;p&gt;
&#38754;&#21521;&#35821;&#20041;&#20998;&#21106;&#30340;&#20998;&#24067;&#24863;&#30693;&#36830;&#32493;&#27979;&#35797;&#26102;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Distribution-Aware Continual Test-Time Adaptation for Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13604
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#35843;&#25972;&#65288;DAT&#65289;&#26041;&#27861;&#65292;&#20197;&#20351;&#24471;&#38754;&#21521;&#35821;&#20041;&#20998;&#21106;&#30340;&#36830;&#32493;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;CTTA&#65289;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#26356;&#21152;&#39640;&#25928;&#21644;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#36890;&#24120;&#38754;&#20020;&#21160;&#24577;&#21644;&#19981;&#26029;&#21464;&#21270;&#30340;&#29615;&#22659;&#65292;&#36830;&#32493;&#27979;&#35797;&#26102;&#36866;&#24212;&#65288;CTTA&#65289;&#34987;&#25552;&#20986;&#20316;&#20026;&#19968;&#31181;&#23558;&#37096;&#32626;&#27169;&#22411;&#36716;&#31227;&#21040;&#19981;&#26029;&#21464;&#21270;&#30340;&#30446;&#26631;&#22495;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36861;&#27714;&#38271;&#26399;&#36866;&#24212;&#24448;&#24448;&#20250;&#24341;&#20837;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#35823;&#24046;&#32047;&#31215;&#38382;&#39064;&#65292;&#36825;&#20123;&#38382;&#39064;&#38459;&#30861;&#20102;CTTA&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#30340;&#23454;&#26045;&#12290;&#26368;&#36817;&#65292;&#29616;&#26377;&#30340;CTTA&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#20110;&#21033;&#29992;&#22823;&#37096;&#20998;&#21442;&#25968;&#36890;&#36807;&#33258;&#35757;&#32451;&#26469;&#36866;&#24212;&#30446;&#26631;&#39046;&#22495;&#30693;&#35782;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#24448;&#24448;&#20250;&#30001;&#20110;&#22024;&#26434;&#30340;&#20266;&#26631;&#31614;&#32780;&#21152;&#21095;&#35823;&#24046;&#32047;&#31215;&#30340;&#25361;&#25112;&#65292;&#24182;&#30001;&#20110;&#25972;&#20010;&#27169;&#22411;&#26356;&#26032;&#25152;&#24102;&#26469;&#30340;&#27785;&#37325;&#35745;&#31639;&#25104;&#26412;&#32780;&#24102;&#26469;&#23454;&#38469;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24863;&#30693;&#35843;&#25972;&#65288;DAT&#65289;&#26041;&#27861;&#65292;&#20351;&#24471;&#35821;&#20041;&#20998;&#21106;CTTA&#22312;&#30495;&#23454;&#24212;&#29992;&#20013;&#26082;&#39640;&#25928;&#21448;&#23454;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13604v2 Announce Type: replace-cross  Abstract: Since autonomous driving systems usually face dynamic and ever-changing environments, continual test-time adaptation (CTTA) has been proposed as a strategy for transferring deployed models to continually changing target domains. However, the pursuit of long-term adaptation often introduces catastrophic forgetting and error accumulation problems, which impede the practical implementation of CTTA in the real world. Recently, existing CTTA methods mainly focus on utilizing a majority of parameters to fit target domain knowledge through self-training. Unfortunately, these approaches often amplify the challenge of error accumulation due to noisy pseudo-labels, and pose practical limitations stemming from the heavy computational costs associated with entire model updates. In this paper, we propose a distribution-aware tuning (DAT) method to make the semantic segmentation CTTA efficient and practical in real-world applications. DAT ad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Differentiable Frank-Wolfe Layer&#65288;DFWLayer&#65289;&#30340;&#21487;&#24494;&#23618;&#65292;&#36890;&#36807;&#25512;&#20986;Frank-Wolfe&#26041;&#27861;&#65292;&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#33539;&#25968;&#32422;&#26463;&#30340;&#22823;&#35268;&#27169;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#35299;&#21644;&#26799;&#24230;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#31454;&#20105;&#24615;&#12290;</title><link>https://arxiv.org/abs/2308.10806</link><description>&lt;p&gt;
DFWLayer: &#21487;&#24494;&#21270;&#30340;Frank-Wolfe&#20248;&#21270;&#23618;
&lt;/p&gt;
&lt;p&gt;
DFWLayer: Differentiable Frank-Wolfe Optimization Layer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Differentiable Frank-Wolfe Layer&#65288;DFWLayer&#65289;&#30340;&#21487;&#24494;&#23618;&#65292;&#36890;&#36807;&#25512;&#20986;Frank-Wolfe&#26041;&#27861;&#65292;&#26377;&#25928;&#22788;&#29702;&#20855;&#26377;&#33539;&#25968;&#32422;&#26463;&#30340;&#22823;&#35268;&#27169;&#20984;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#22312;&#35299;&#21644;&#26799;&#24230;&#20934;&#30830;&#24615;&#26041;&#38754;&#34920;&#29616;&#31454;&#20105;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#21516;iable&#20248;&#21270;&#22240;&#20854;&#22312;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#22522;&#30784;&#20316;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#36890;&#36807;&#25512;&#20986;Frank-Wolfe&#26041;&#27861;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#24494;&#23618;&#65292;&#21629;&#21517;&#20026;Differentiable Frank-Wolfe Layer&#65288;DFWLayer&#65289;&#65292;&#35813;&#26041;&#27861;&#26159;&#19968;&#31181;&#21487;&#20197;&#22312;&#19981;&#36827;&#34892;&#25237;&#24433;&#21644;Hessian&#30697;&#38453;&#35745;&#31639;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#32422;&#26463;&#20248;&#21270;&#38382;&#39064;&#30340;&#33879;&#21517;&#20248;&#21270;&#31639;&#27861;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#22788;&#29702;&#20855;&#26377;&#33539;&#25968;&#32422;&#26463;&#30340;&#22823;&#35268;&#27169;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DFWLayer&#19981;&#20165;&#22312;&#35299;&#21644;&#26799;&#24230;&#31934;&#24230;&#19978;&#34920;&#29616;&#31454;&#20105;&#24615;&#65292;&#32780;&#19988;&#22987;&#32456;&#36981;&#23432;&#32422;&#26463;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10806v2 Announce Type: replace-cross  Abstract: Differentiable optimization has received a significant amount of attention due to its foundational role in the domain of machine learning based on neural networks. This paper proposes a differentiable layer, named Differentiable Frank-Wolfe Layer (DFWLayer), by rolling out the Frank-Wolfe method, a well-known optimization algorithm which can solve constrained optimization problems without projections and Hessian matrix computations, thus leading to an efficient way of dealing with large-scale convex optimization problems with norm constraints. Experimental results demonstrate that the DFWLayer not only attains competitive accuracy in solutions and gradients but also consistently adheres to constraints.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;S3PRL&#24037;&#20855;&#21253;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21457;&#29616;SpecAugment&#30053;&#24494;&#25552;&#39640;&#20102;HuBERT&#21644;wav2vec&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#39640;&#26031;&#22122;&#22768;&#21644;&#36895;&#24230;&#25200;&#21160;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#32463;&#36807;&#22686;&#24378;&#30340;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#26356;&#20026;&#31283;&#20581;&#12290;</title><link>https://arxiv.org/abs/2303.00510</link><description>&lt;p&gt;
&#20351;&#29992;S3PRL&#24037;&#20855;&#21253;&#27604;&#36739;&#35821;&#38899;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Speech Data Augmentation Methods Using S3PRL Toolkit
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2303.00510
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;S3PRL&#24037;&#20855;&#21253;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21457;&#29616;SpecAugment&#30053;&#24494;&#25552;&#39640;&#20102;HuBERT&#21644;wav2vec&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#65292;&#32780;&#20351;&#29992;&#39640;&#26031;&#22122;&#22768;&#21644;&#36895;&#24230;&#25200;&#21160;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#32463;&#36807;&#22686;&#24378;&#30340;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#26356;&#20026;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#24050;&#30693;&#21487;&#20197;&#25552;&#39640;&#35821;&#38899;&#22788;&#29702;&#20219;&#21153;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;S3PRL&#24037;&#20855;&#21253;&#24635;&#32467;&#24182;&#27604;&#36739;&#19981;&#21516;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#20351;&#29992;HuBERT&#21644;wav2vec&#32467;&#21512;&#19981;&#21516;&#22686;&#24378;&#25216;&#26415;&#65288;SpecAugment&#12289;&#39640;&#26031;&#22122;&#22768;&#12289;&#36895;&#24230;&#25200;&#21160;&#65289;&#36827;&#34892;&#38899;&#32032;&#35782;&#21035;&#65288;PR&#65289;&#21644;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#20219;&#21153;&#12290;&#25105;&#20204;&#26681;&#25454;&#38899;&#32032;&#38169;&#35823;&#29575;&#65288;PER&#65289;&#21644;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#35780;&#20272;&#27169;&#22411;&#24615;&#33021;&#12290;&#20174;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;SpecAugment&#22312;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#30053;&#24494;&#25552;&#39640;&#20102;HuBERT&#21644;wav2vec&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#39640;&#26031;&#22122;&#22768;&#21644;&#36895;&#24230;&#25200;&#21160;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#32463;&#36807;&#22686;&#24378;&#30340;&#27979;&#35797;&#38598;&#19978;&#34920;&#29616;&#26356;&#20026;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2303.00510v2 Announce Type: replace-cross  Abstract: Data augmentations are known to improve robustness in speech-processing tasks. In this study, we summarize and compare different data augmentation strategies using S3PRL toolkit. We explore how HuBERT and wav2vec perform using different augmentation techniques (SpecAugment, Gaussian Noise, Speed Perturbation) for Phoneme Recognition (PR) and Automatic Speech Recognition (ASR) tasks. We evaluate model performance in terms of phoneme error rate (PER) and word error rate (WER). From the experiments, we observed that SpecAugment slightly improves the performance of HuBERT and wav2vec on the original dataset. Also, we show that models trained using the Gaussian Noise and Speed Perturbation dataset are more robust when tested with augmented test sets.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20998;&#31867;&#27861;&#65292;&#27010;&#36848;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#65292;&#35843;&#26597;&#20102;&#32467;&#26500;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#24182;&#25506;&#35752;&#20102;&#23545;&#37325;&#35201;&#22270;&#31867;&#30340;&#25193;&#23637;&#65292;&#22914;3D&#20998;&#23376;&#22270;&#12290;</title><link>https://arxiv.org/abs/2302.04181</link><description>&lt;p&gt;
&#20851;&#27880;&#22270;&#36716;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Attending to Graph Transformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.04181
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20998;&#31867;&#27861;&#65292;&#27010;&#36848;&#20102;&#20854;&#29702;&#35770;&#24615;&#36136;&#65292;&#35843;&#26597;&#20102;&#32467;&#26500;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#24182;&#25506;&#35752;&#20102;&#23545;&#37325;&#35201;&#22270;&#31867;&#30340;&#25193;&#23637;&#65292;&#22914;3D&#20998;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29992;&#20110;&#22270;&#24418;&#30340;&#36716;&#25442;&#22120;&#26550;&#26500;&#20316;&#20026;&#26426;&#22120;&#23398;&#20064;&#30340;&#26367;&#20195;&#25216;&#26415;&#20986;&#29616;&#65292;&#20363;&#22914;&#65288;&#28040;&#24687;&#20256;&#36882;&#65289;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#23427;&#20204;&#24050;&#32463;&#23637;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#23454;&#35777;&#32467;&#26524;&#65292;&#20363;&#22914;&#22312;&#20998;&#23376;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#65292;&#36890;&#24120;&#24402;&#22240;&#20110;&#20854;&#32469;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32570;&#28857;&#65292;&#22914;&#36807;&#24230;&#24179;&#28369;&#21644;&#36807;&#24230;&#21387;&#32553;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22270;&#36716;&#25442;&#22120;&#26550;&#26500;&#30340;&#20998;&#31867;&#27861;&#65292;&#20026;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#24102;&#26469;&#20102;&#19968;&#20123;&#31209;&#24207;&#12290;&#25105;&#20204;&#27010;&#36848;&#23427;&#20204;&#30340;&#29702;&#35770;&#24615;&#36136;&#65292;&#35843;&#26597;&#32467;&#26500;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#24182;&#35752;&#35770;&#20102;&#23545;&#37325;&#35201;&#22270;&#31867;&#30340;&#25193;&#23637;&#65292;&#20363;&#22914;3D&#20998;&#23376;&#22270;&#12290;&#22312;&#23454;&#35777;&#26041;&#38754;&#65292;&#25105;&#20204;&#30740;&#31350;&#22270;&#36716;&#25442;&#22120;&#22914;&#20309;&#24674;&#22797;&#21508;&#31181;&#22270;&#23646;&#24615;&#65292;&#22914;&#20309;&#22788;&#29702;&#24322;&#24615;&#22270;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#21487;&#20197;&#38450;&#27490;&#36807;&#24230;&#21387;&#32553;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#21644;&#30740;&#31350;&#26041;&#21521;&#65292;&#20197;&#20419;&#36827;&#26410;&#26469;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.04181v3 Announce Type: replace-cross  Abstract: Recently, transformer architectures for graphs emerged as an alternative to established techniques for machine learning with graphs, such as (message-passing) graph neural networks. So far, they have shown promising empirical results, e.g., on molecular prediction datasets, often attributed to their ability to circumvent graph neural networks' shortcomings, such as over-smoothing and over-squashing. Here, we derive a taxonomy of graph transformer architectures, bringing some order to this emerging field. We overview their theoretical properties, survey structural and positional encodings, and discuss extensions for important graph classes, e.g., 3D molecular graphs. Empirically, we probe how well graph transformers can recover various graph properties, how well they can deal with heterophilic graphs, and to what extent they prevent over-squashing. Further, we outline open challenges and research direction to stimulate future wo
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#37327;&#23376;&#33455;&#29255;&#26550;&#26500;&#19978;&#34920;&#29616;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#25104;&#26412;&#20989;&#25968;&#24448;&#24448;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#24179;&#22343;&#20540;&#12290;</title><link>https://arxiv.org/abs/2212.14426</link><description>&lt;p&gt;
&#38480;&#21046;&#22312;&#33455;&#29255;&#26550;&#26500;&#19978;&#20445;&#25345;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;
&lt;/p&gt;
&lt;p&gt;
Restricting to the chip architecture maintains the quantum neural network accuracy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2212.14426
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#26088;&#22312;&#25506;&#35752;&#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#22312;&#37327;&#23376;&#33455;&#29255;&#26550;&#26500;&#19978;&#34920;&#29616;&#20986;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#21457;&#29616;&#25104;&#26412;&#20989;&#25968;&#24448;&#24448;&#20250;&#25910;&#25947;&#21040;&#19968;&#20010;&#24179;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22122;&#22768;&#20013;&#31561;&#35268;&#27169;&#37327;&#23376;&#35774;&#22791;&#30340;&#26102;&#20195;&#65292;&#21464;&#20998;&#37327;&#23376;&#31639;&#27861;&#65288;VQAs&#65289;&#20316;&#20026;&#26500;&#24314;&#37327;&#23376;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26174;&#33879;&#31574;&#30053;&#12290;&#36825;&#20123;&#27169;&#22411;&#21253;&#25324;&#37327;&#23376;&#37096;&#20998;&#21644;&#32463;&#20856;&#37096;&#20998;&#12290;&#37327;&#23376;&#37096;&#20998;&#36890;&#36807;&#21442;&#25968;&#21270; $U$ &#26469;&#34920;&#24449;&#65292;&#36890;&#24120;&#30001;&#21508;&#31181;&#37327;&#23376;&#38376;&#30340;&#32452;&#21512;&#24471;&#21040;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#32463;&#20856;&#37096;&#20998;&#28041;&#21450;&#19968;&#20010;&#20248;&#21270;&#22120;&#65292;&#35843;&#33410; $U$ &#30340;&#21442;&#25968;&#20197;&#26368;&#23567;&#21270;&#25104;&#26412;&#20989;&#25968; $C$&#12290;&#23613;&#31649;VQAs&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#65292;&#27604;&#22914;&#30830;&#23450;&#26368;&#20339;&#38376;&#24207;&#21015;&#12289;&#35774;&#35745;&#39640;&#25928;&#30340;&#21442;&#25968;&#20248;&#21270;&#31574;&#30053;&#12289;&#36873;&#25321;&#21512;&#36866;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#20197;&#21450;&#20102;&#35299;&#37327;&#23376;&#33455;&#29255;&#26550;&#26500;&#23545;&#26368;&#32456;&#32467;&#26524;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#26368;&#21518;&#19968;&#20010;&#38382;&#39064;&#65292;&#24182;&#24378;&#35843;&#65292;&#19968;&#33324;&#24773;&#20917;&#19979;&#65292;&#25104;&#26412;&#20989;&#25968;&#20542;&#21521;&#20110;&#25910;&#25947;&#21040;&#19968;&#20010;&#24179;&#22343;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2212.14426v2 Announce Type: replace-cross  Abstract: In the era of noisy intermediate-scale quantum devices, variational quantum algorithms (VQAs) stand as a prominent strategy for constructing quantum machine learning models. These models comprise both a quantum and a classical component. The quantum facet is characterized by a parametrization $U$, typically derived from the composition of various quantum gates. On the other hand, the classical component involves an optimizer that adjusts the parameters of $U$ to minimize a cost function $C$. Despite the extensive applications of VQAs, several critical questions persist, such as determining the optimal gate sequence, devising efficient parameter optimization strategies, selecting appropriate cost functions, and understanding the influence of quantum chip architectures on the final results. This article aims to address the last question, emphasizing that, in general, the cost function tends to converge towards an average value as
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25361;&#25112;-&#21709;&#24212;&#26041;&#24335;&#65292;&#38024;&#23545;AI Real-Time Deepfakes&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#23454;&#26102;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2210.06186</link><description>&lt;p&gt;
GOTCHA&#65306;&#36890;&#36807;&#25361;&#25112;-&#21709;&#24212;&#23454;&#29616;&#23454;&#26102;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
GOTCHA: Real-Time Video Deepfake Detection via Challenge-Response
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2210.06186
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25361;&#25112;-&#21709;&#24212;&#26041;&#24335;&#65292;&#38024;&#23545;AI Real-Time Deepfakes&#30340;&#38480;&#21046;&#65292;&#25552;&#20986;&#23454;&#26102;&#35270;&#39057;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;AI-enabled Real-Time Deepfakes&#65288;RTDFs&#65289;&#30340;&#20852;&#36215;&#65292;&#22312;&#32447;&#35270;&#39057;&#20114;&#21160;&#30340;&#23436;&#25972;&#24615;&#24050;&#25104;&#20026;&#19968;&#20010;&#26085;&#30410;&#20196;&#20154;&#25285;&#24551;&#30340;&#38382;&#39064;&#12290;RTDFs&#29616;&#22312;&#20351;&#24471;&#22312;&#23454;&#26102;&#35270;&#39057;&#20114;&#21160;&#20013;&#23558;&#20882;&#21517;&#39030;&#26367;&#32773;&#30340;&#33080;&#26367;&#25442;&#20026;&#20854;&#21463;&#23475;&#32773;&#25104;&#20026;&#21487;&#33021;&#12290;&#36825;&#31181;&#28145;&#24230;&#20266;&#36896;&#30340;&#36827;&#27493;&#20063;&#20419;&#20351;&#26816;&#27979;&#36798;&#21040;&#21516;&#26679;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#28145;&#24230;&#20266;&#36896;&#26816;&#27979;&#25216;&#26415;&#26159;&#24322;&#27493;&#30340;&#65292;&#22240;&#27492;&#19981;&#36866;&#29992;&#20110;RTDFs&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23454;&#26102;&#29615;&#22659;&#20013;&#24314;&#31435;&#30495;&#23454;&#24615;&#30340;&#25361;&#25112;-&#21709;&#24212;&#26041;&#27861;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#35762;&#35805;&#22836;&#37096;&#39118;&#26684;&#30340;&#35270;&#39057;&#20114;&#21160;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;RTDF&#29983;&#25104;&#31649;&#36947;&#22266;&#26377;&#38480;&#21046;&#30340;&#25361;&#25112;&#20998;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;&#25910;&#38598;&#19968;&#20010;&#21253;&#21547;&#20843;&#20010;&#25361;&#25112;&#30340;&#29420;&#29305;&#25968;&#25454;&#38598;&#26469;&#35780;&#20272;&#20998;&#31867;&#20013;&#30340;&#20195;&#34920;&#24615;&#31034;&#20363;&#65292;&#36825;&#20123;&#25361;&#25112;&#19968;&#33268;&#21644;&#26126;&#26174;&#22320;&#38477;&#20302;&#20102;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#20266;&#36896;&#29983;&#25104;&#22120;&#30340;&#36136;&#37327;&#12290;&#36825;&#20123;&#32467;&#26524;&#24471;&#21040;&#20102;&#20154;&#31867;&#30340;&#35777;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2210.06186v3 Announce Type: replace-cross  Abstract: With the rise of AI-enabled Real-Time Deepfakes (RTDFs), the integrity of online video interactions has become a growing concern. RTDFs have now made it feasible to replace an imposter's face with their victim in live video interactions. Such advancement in deepfakes also coaxes detection to rise to the same standard. However, existing deepfake detection techniques are asynchronous and hence ill-suited for RTDFs. To bridge this gap, we propose a challenge-response approach that establishes authenticity in live settings. We focus on talking-head style video interaction and present a taxonomy of challenges that specifically target inherent limitations of RTDF generation pipelines. We evaluate representative examples from the taxonomy by collecting a unique dataset comprising eight challenges, which consistently and visibly degrades the quality of state-of-the-art deepfake generators. These results are corroborated both by humans 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23494;&#24230;&#20272;&#35745;&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#20219;&#21153;&#20998;&#24067;&#65292;&#20174;&#32780;&#23545;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#25152;&#38656;&#35757;&#32451;&#20219;&#21153;&#25968;&#25552;&#20986;&#20102;&#26032;&#30340;&#30028;&#38480;&#20998;&#26512;&#26041;&#27861;</title><link>https://arxiv.org/abs/2206.10716</link><description>&lt;p&gt;
&#20855;&#26377;&#26377;&#38480;&#35757;&#32451;&#20219;&#21153;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;--&#19968;&#31181;&#23494;&#24230;&#20272;&#35745;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Meta Reinforcement Learning with Finite Training Tasks -- a Density Estimation Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.10716
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#20272;&#35745;&#25216;&#26415;&#30452;&#25509;&#23398;&#20064;&#20219;&#21153;&#20998;&#24067;&#65292;&#20174;&#32780;&#23545;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#25152;&#38656;&#35757;&#32451;&#20219;&#21153;&#25968;&#25552;&#20986;&#20102;&#26032;&#30340;&#30028;&#38480;&#20998;&#26512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20803;&#24378;&#21270;&#23398;&#20064;(meta RL)&#20013;&#65292;&#19968;&#20010;&#26234;&#33021;&#20307;&#36890;&#36807;&#19968;&#32452;&#35757;&#32451;&#20219;&#21153;&#23398;&#20064;&#22914;&#20309;&#24555;&#36895;&#35299;&#20915;&#19968;&#20010;&#26032;&#20219;&#21153;&#65292;&#26032;&#20219;&#21153;&#26469;&#33258;&#30456;&#21516;&#30340;&#20219;&#21153;&#20998;&#24067;&#12290;&#20248;&#21270;&#30340;&#20803;RL&#31574;&#30053;&#65292;&#21363;&#36125;&#21494;&#26031;&#26368;&#20248;&#34892;&#20026;&#65292;&#26159;&#26126;&#30830;&#23450;&#20041;&#30340;&#65292;&#24182;&#20445;&#35777;&#26399;&#26395;&#19979;&#30456;&#23545;&#20110;&#20219;&#21153;&#20998;&#24067;&#30340;&#26368;&#20248;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25506;&#35752;&#30340;&#38382;&#39064;&#26159;&#38656;&#35201;&#22810;&#23569;&#20010;&#35757;&#32451;&#20219;&#21153;&#25165;&#33021;&#20445;&#35777;&#20197;&#39640;&#27010;&#29575;&#36817;&#20284;&#22320;&#33719;&#24471;&#26368;&#20248;&#34892;&#20026;&#12290;&#26368;&#36817;&#30340;&#24037;&#20316;&#20026;&#27169;&#22411;&#26080;&#20851;&#35774;&#32622;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#36825;&#26679;&#30340;PAC&#20998;&#26512;&#65292;&#20854;&#20013;&#20174;&#35757;&#32451;&#20219;&#21153;&#20013;&#23398;&#20064;&#20102;&#19968;&#31181;&#21382;&#21490;&#30456;&#20851;&#30340;&#31574;&#30053;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#65306;&#30452;&#25509;&#23398;&#20064;&#20219;&#21153;&#20998;&#24067;&#65292;&#20351;&#29992;&#23494;&#24230;&#20272;&#35745;&#25216;&#26415;&#65292;&#28982;&#21518;&#22312;&#23398;&#20064;&#30340;&#20219;&#21153;&#20998;&#24067;&#19978;&#35757;&#32451;&#31574;&#30053;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#23548;&#33268;&#20381;&#36182;&#20110;&#20219;&#21153;&#20998;&#24067;&#32500;&#24230;&#30340;&#30028;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20219;&#21153;&#20998;&#24067;&#32500;&#24230;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.10716v2 Announce Type: replace-cross  Abstract: In meta reinforcement learning (meta RL), an agent learns from a set of training tasks how to quickly solve a new task, drawn from the same task distribution. The optimal meta RL policy, a.k.a. the Bayes-optimal behavior, is well defined, and guarantees optimal reward in expectation, taken with respect to the task distribution. The question we explore in this work is how many training tasks are required to guarantee approximately optimal behavior with high probability. Recent work provided the first such PAC analysis for a model-free setting, where a history-dependent policy was learned from the training tasks. In this work, we propose a different approach: directly learn the task distribution, using density estimation techniques, and then train a policy on the learned task distribution. We show that our approach leads to bounds that depend on the dimension of the task distribution. In particular, in settings where the task dis
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; QAGCN &#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38382;&#39064;&#36827;&#34892;&#24863;&#30693;&#26469;&#23454;&#29616;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#65292;&#20174;&#32780;&#22238;&#31572;&#22810;&#20851;&#31995;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#26174;&#24335;&#22810;&#27493;&#25512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26356;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#26131;&#20110;&#37319;&#29992;&#12290;</title><link>https://arxiv.org/abs/2206.01818</link><description>&lt;p&gt;
QAGCN&#65306;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#36827;&#34892;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#22238;&#31572;&#22810;&#20851;&#31995;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
QAGCN: Answering Multi-Relation Questions via Single-Step Implicit Reasoning over Knowledge Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2206.01818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; QAGCN &#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#38382;&#39064;&#36827;&#34892;&#24863;&#30693;&#26469;&#23454;&#29616;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#65292;&#20174;&#32780;&#22238;&#31572;&#22810;&#20851;&#31995;&#38382;&#39064;&#65292;&#30456;&#27604;&#20110;&#26174;&#24335;&#22810;&#27493;&#25512;&#29702;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26356;&#31616;&#21333;&#12289;&#39640;&#25928;&#19988;&#26131;&#20110;&#37319;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20851;&#31995;&#38382;&#39064;&#22238;&#31572;&#65288;QA&#65289;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36890;&#24120;&#38656;&#35201;&#22312;&#30001;&#22810;&#20010;&#20851;&#31995;&#32452;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#38271;&#26102;&#38388;&#25512;&#29702;&#38142;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#65292;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#26126;&#26174;&#20351;&#29992;&#20102;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#26174;&#24335;&#22810;&#27493;&#25512;&#29702;&#26041;&#27861;&#65292;&#24182;&#23637;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#36880;&#27493;&#26631;&#31614;&#20256;&#25773;&#30340;&#26041;&#27861;&#20197;&#21450;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#27983;&#35272;&#30693;&#35782;&#22270;&#35889;&#19977;&#20803;&#32452;&#30340;&#26041;&#27861;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#24369;&#28857;&#26159;&#23427;&#20204;&#30340;&#25512;&#29702;&#26426;&#21046;&#36890;&#24120;&#22797;&#26434;&#19988;&#38590;&#20197;&#23454;&#29616;&#25110;&#35757;&#32451;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#20026;&#21487;&#20197;&#36890;&#36807;&#31471;&#21040;&#31471;&#21333;&#27493;&#38544;&#24335;&#25512;&#29702;&#23454;&#29616;&#22810;&#20851;&#31995;QA&#65292;&#36825;&#31181;&#26041;&#27861;&#26356;&#31616;&#21333;&#12289;&#26356;&#39640;&#25928;&#19988;&#26356;&#26131;&#20110;&#37319;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; QAGCN -- &#19968;&#31181;&#22522;&#20110;&#38382;&#39064;&#24847;&#35782;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#21463;&#25511;&#38382;&#39064;&#30456;&#20851;&#20449;&#24687;&#20256;&#25773;&#30340;GCN&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2206.01818v3 Announce Type: replace  Abstract: Multi-relation question answering (QA) is a challenging task, where given questions usually require long reasoning chains in KGs that consist of multiple relations. Recently, methods with explicit multi-step reasoning over KGs have been prominently used in this task and have demonstrated promising performance. Examples include methods that perform stepwise label propagation through KG triples and methods that navigate over KG triples based on reinforcement learning. A main weakness of these methods is that their reasoning mechanisms are usually complex and difficult to implement or train. In this paper, we argue that multi-relation QA can be achieved via end-to-end single-step implicit reasoning, which is simpler, more efficient, and easier to adopt. We propose QAGCN -- a Question-Aware Graph Convolutional Network (GCN)-based method that includes a novel GCN architecture with controlled question-dependent message propagation for the 
&lt;/p&gt;</description></item><item><title>MADRID&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#22330;&#26223;&#26469;&#25581;&#31034;&#39044;&#35757;&#32451;&#22810;Agent&#31574;&#30053;&#30340;&#25112;&#30053;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;&#36951;&#25022;&#20540;&#34913;&#37327;&#28431;&#27934;&#30340;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2401.13460</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#26679;&#24615;&#21551;&#31034;&#30340;&#22810;Agent&#35786;&#26029;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Diagnostics for Robustness via Illuminated Diversity. (arXiv:2401.13460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.13460
&lt;/p&gt;
&lt;p&gt;
MADRID&#26159;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#22810;&#26679;&#21270;&#30340;&#23545;&#25239;&#22330;&#26223;&#26469;&#25581;&#31034;&#39044;&#35757;&#32451;&#22810;Agent&#31574;&#30053;&#30340;&#25112;&#30053;&#28431;&#27934;&#65292;&#24182;&#36890;&#36807;&#36951;&#25022;&#20540;&#34913;&#37327;&#28431;&#27934;&#30340;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24555;&#36895;&#21457;&#23637;&#30340;&#22810;Agent&#31995;&#32479;&#39046;&#22495;&#20013;&#65292;&#30830;&#20445;&#22312;&#38476;&#29983;&#21644;&#25932;&#23545;&#29615;&#22659;&#20013;&#30340;&#31283;&#20581;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#36825;&#20123;&#31995;&#32479;&#22312;&#29087;&#24713;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#26032;&#24773;&#20917;&#19979;&#24448;&#24448;&#20250;&#22240;&#20026;&#35757;&#32451;&#38454;&#27573;&#30340;&#36807;&#25311;&#21512;&#32780;&#22833;&#36133;&#12290;&#22312;&#26082;&#21253;&#21547;&#21512;&#20316;&#21448;&#21253;&#21547;&#31454;&#20105;&#34892;&#20026;&#30340;&#29615;&#22659;&#20013;&#65292;&#36825;&#19968;&#38382;&#39064;&#23588;&#20026;&#31361;&#20986;&#65292;&#20307;&#29616;&#20102;&#36807;&#25311;&#21512;&#21644;&#27867;&#21270;&#25361;&#25112;&#30340;&#21452;&#37325;&#24615;&#36136;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36890;&#36807;&#22810;&#26679;&#24615;&#21551;&#31034;&#30340;&#22810;Agent&#31283;&#20581;&#24615;&#35786;&#26029;&#65288;MADRID&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29983;&#25104;&#22810;Agent&#31574;&#30053;&#20013;&#26292;&#38706;&#25112;&#30053;&#28431;&#27934;&#30340;&#22810;&#26679;&#21270;&#23545;&#25239;&#22330;&#26223;&#30340;&#26032;&#26041;&#27861;&#12290;MADRID&#21033;&#29992;&#24320;&#25918;&#24335;&#23398;&#20064;&#30340;&#27010;&#24565;&#65292;&#23548;&#33322;&#23545;&#25239;&#29615;&#22659;&#30340;&#24191;&#38420;&#31354;&#38388;&#65292;&#20351;&#29992;&#30446;&#26631;&#31574;&#30053;&#30340;&#36951;&#25022;&#20540;&#26469;&#34913;&#37327;&#36825;&#20123;&#29615;&#22659;&#30340;&#28431;&#27934;&#12290;&#25105;&#20204;&#22312;11vs11&#29256;&#30340;Google Research Football&#19978;&#35780;&#20272;&#20102;MADRID&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the rapidly advancing field of multi-agent systems, ensuring robustness in unfamiliar and adversarial settings is crucial. Notwithstanding their outstanding performance in familiar environments, these systems often falter in new situations due to overfitting during the training phase. This is especially pronounced in settings where both cooperative and competitive behaviours are present, encapsulating a dual nature of overfitting and generalisation challenges. To address this issue, we present Multi-Agent Diagnostics for Robustness via Illuminated Diversity (MADRID), a novel approach for generating diverse adversarial scenarios that expose strategic vulnerabilities in pre-trained multi-agent policies. Leveraging the concepts from open-ended learning, MADRID navigates the vast space of adversarial settings, employing a target policy's regret to gauge the vulnerabilities of these settings. We evaluate the effectiveness of MADRID on the 11vs11 version of Google Research Football, one o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#19968;GPU&#19978;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#19988;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;</title><link>http://arxiv.org/abs/2312.10144</link><description>&lt;p&gt;
&#21333;&#19968;GPU&#19978;&#30340;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Multimodal Fusion on a Single GPU. (arXiv:2312.10144v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.10144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21333;&#19968;GPU&#19978;&#36827;&#34892;&#25968;&#25454;&#39640;&#25928;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#23545;&#40784;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#19988;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23545;&#40784;&#30340;&#30446;&#26631;&#26159;&#23398;&#20064;&#20849;&#20139;&#22810;&#27169;&#24577;&#36755;&#20837;&#20043;&#38388;&#30340;&#21333;&#19968;&#28508;&#22312;&#31354;&#38388;&#12290;&#22312;&#36825;&#20010;&#39046;&#22495;&#20013;&#65292;&#26368;&#24378;&#22823;&#30340;&#27169;&#22411;&#36890;&#24120;&#26159;&#20351;&#29992;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#35745;&#31639;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#30340;&#65292;&#22240;&#27492;&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#30340;&#25104;&#26412;&#38750;&#24120;&#39640;&#26114;&#12290;&#25105;&#20204;&#25512;&#27979;&#65292;&#29616;&#26377;&#30340;&#22312;&#22823;&#37327;&#21333;&#27169;&#24577;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#24212;&#35813;&#33021;&#22815;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#20174;&#21333;&#27169;&#24577;&#27169;&#22411;&#20013;&#21019;&#24314;&#22810;&#27169;&#24577;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FuseMix&#65292;&#19968;&#31181;&#22810;&#27169;&#24577;&#22686;&#24378;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#20219;&#24847;&#39044;&#35757;&#32451;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25805;&#20316;&#12290;&#36890;&#36807;&#20351;&#29992;FuseMix&#36827;&#34892;&#22810;&#27169;&#24577;&#23545;&#40784;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21644;&#38899;&#39057;-&#25991;&#26412;&#26816;&#32034;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#32780;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#65306;&#20363;&#22914;&#65292;&#25105;&#20204;&#22312;Flickr30K&#30340;&#25991;&#26412;-&#22270;&#20687;&#26816;&#32034;&#20219;&#21153;&#20013;&#27604;CLIP&#30340;&#24615;&#33021;&#25552;&#39640;&#20102;&#32422;600&#20493;&#65292;&#32780;&#35745;&#31639;&#21644;&#25968;&#25454;&#37327;&#20943;&#23569;&#20102;&#25968;&#20010;&#25968;&#37327;&#32423;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of multimodal alignment is to learn a single latent space that is shared between multimodal inputs. The most powerful models in this space have been trained using massive datasets of paired inputs and large-scale computational resources, making them prohibitively expensive to train in many practical scenarios. We surmise that existing unimodal encoders pre-trained on large amounts of unimodal data should provide an effective bootstrap to create multimodal models from unimodal ones at much lower costs. We therefore propose FuseMix, a multimodal augmentation scheme that operates on the latent spaces of arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal alignment, we achieve competitive performance -- and in certain cases outperform state-of-the art methods -- in both image-text and audio-text retrieval, with orders of magnitude less compute and data: for example, we outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \! 600\times$ fewer GP
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;DialogBench&#65292;&#19968;&#20010;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;28&#20010;LLMs&#30340;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#23545;&#25552;&#21319;&#24615;&#33021;&#25928;&#26524;&#26174;&#33879;&#12290;</title><link>http://arxiv.org/abs/2311.01677</link><description>&lt;p&gt;
DialogBench: &#23558;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#36827;&#34892;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
DialogBench: Evaluating LLMs as Human-like Dialogue Systems. (arXiv:2311.01677v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;DialogBench&#65292;&#19968;&#20010;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;&#65292;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;28&#20010;LLMs&#30340;&#24191;&#27867;&#27979;&#35797;&#65292;&#21457;&#29616;&#25351;&#23548;&#24494;&#35843;&#23545;&#25552;&#21319;&#24615;&#33021;&#25928;&#26524;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26032;&#30340;&#23545;&#35805;&#33021;&#21147;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#31361;&#30772;&#65292;&#21047;&#26032;&#20102;&#20154;&#20204;&#23545;&#23545;&#35805;&#31995;&#32479;&#30340;&#21360;&#35937;&#12290;&#23545;&#35805;&#31995;&#32479;&#38271;&#26399;&#20197;&#26469;&#30340;&#30446;&#26631;&#26159;&#36275;&#22815;&#20687;&#20154;&#31867;&#65292;&#20197;&#20415;&#36890;&#36807;&#28385;&#36275;&#20132;&#27969;&#12289;&#24773;&#24863;&#21644;&#31038;&#20132;&#24402;&#23646;&#30340;&#38656;&#35201;&#19982;&#29992;&#25143;&#24314;&#31435;&#38271;&#26399;&#32852;&#31995;&#12290;&#22240;&#27492;&#65292;&#36843;&#20999;&#38656;&#35201;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DialogBench&#65292;&#19968;&#20010;&#23545;&#35805;&#35780;&#20272;&#22522;&#20934;&#65292;&#30446;&#21069;&#21253;&#21547;12&#20010;&#23545;&#35805;&#20219;&#21153;&#65292;&#35780;&#20272;LLMs&#20316;&#20026;&#20154;&#31867;&#23545;&#35805;&#31995;&#32479;&#24212;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#20351;&#29992;GPT-4&#29983;&#25104;&#27599;&#20010;&#20219;&#21153;&#30340;&#35780;&#20272;&#23454;&#20363;&#12290;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#24191;&#27867;&#20351;&#29992;&#30340;&#35774;&#35745;&#21407;&#21017;&#35774;&#35745;&#22522;&#26412;&#25552;&#31034;&#65292;&#24182;&#36827;&#19968;&#27493;&#20943;&#36731;&#29616;&#26377;&#30340;&#20559;&#35265;&#65292;&#29983;&#25104;&#26356;&#39640;&#36136;&#37327;&#30340;&#35780;&#20272;&#23454;&#20363;&#12290;&#25105;&#20204;&#23545;28&#20010;LLMs&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#65288;&#21253;&#25324;&#39044;&#35757;&#32451;&#21644;&#30417;&#30563;&#25351;&#23548;&#35843;&#20248;&#65289;&#65292;&#32467;&#26524;&#26174;&#31034;&#25351;&#23548;&#24494;&#35843;&#25928;&#30410;&#26174;&#33879;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have achieved remarkable breakthroughs in new dialogue capabilities, refreshing human's impressions on dialogue systems. The long-standing goal of dialogue systems is to be human-like enough to establish long-term connections with users by satisfying the need for communication, affection and social belonging. Therefore, there has been an urgent need to evaluate LLMs as human-like dialogue systems. In this paper, we propose DialogBench, a dialogue evaluation benchmark that currently contains $12$ dialogue tasks to assess the capabilities of LLMs as human-like dialogue systems should have. Specifically, we prompt GPT-4 to generate evaluation instances for each task. We first design the basic prompt based on widely-used design principles and further mitigate the existing biases to generate higher-quality evaluation instances. Our extensive test over $28$ LLMs (including pre-trained and supervised instruction-tuning) shows that instruction fine-tuning benefits 
&lt;/p&gt;</description></item><item><title>ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2310.13001</link><description>&lt;p&gt;
&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65288;ConFIRM&#65289;
&lt;/p&gt;
&lt;p&gt;
Conversational Financial Information Retrieval Model (ConFIRM). (arXiv:2310.13001v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13001
&lt;/p&gt;
&lt;p&gt;
ConFIRM&#26159;&#19968;&#31181;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#36890;&#36807;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#21644;&#35780;&#20272;&#21442;&#25968;&#24494;&#35843;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#20026;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#25552;&#20379;&#20102;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#21033;&#29992;&#23427;&#20204;&#22312;&#37329;&#34701;&#31561;&#19987;&#38376;&#39046;&#22495;&#30340;&#26032;&#20852;&#29305;&#24615;&#20855;&#26377;&#25506;&#32034;&#30340;&#20215;&#20540;&#12290;&#28982;&#32780;&#65292;&#37329;&#34701;&#31561;&#21463;&#30417;&#31649;&#39046;&#22495;&#20855;&#26377;&#29420;&#29305;&#30340;&#32422;&#26463;&#26465;&#20214;&#65292;&#38656;&#35201;&#20855;&#22791;&#38024;&#23545;&#35813;&#39046;&#22495;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;ConFIRM&#65292;&#19968;&#31181;&#22522;&#20110;LLM&#30340;&#20250;&#35805;&#24335;&#37329;&#34701;&#20449;&#24687;&#26816;&#32034;&#27169;&#22411;&#65292;&#29992;&#20110;&#26597;&#35810;&#24847;&#22270;&#20998;&#31867;&#21644;&#30693;&#35782;&#24211;&#26631;&#35760;&#12290;ConFIRM&#21253;&#25324;&#20004;&#20010;&#27169;&#22359;&#65306;1&#65289;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#39046;&#22495;&#29305;&#23450;&#38382;&#31572;&#23545;&#30340;&#26041;&#27861;&#65292;&#20197;&#21450;2&#65289;&#35780;&#20272;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#36827;&#34892;&#26597;&#35810;&#20998;&#31867;&#20219;&#21153;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;4000&#22810;&#20010;&#26679;&#26412;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#21333;&#29420;&#30340;&#27979;&#35797;&#38598;&#19978;&#35780;&#20272;&#20102;&#20934;&#30830;&#24615;&#12290;ConFIRM&#23454;&#29616;&#20102;&#36229;&#36807;90%&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#23545;&#20110;&#31526;&#21512;&#30417;&#31649;&#35201;&#27714;&#33267;&#20851;&#37325;&#35201;&#12290;ConFIRM&#25552;&#20379;&#20102;&#19968;&#31181;&#25968;&#25454;&#39640;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#29992;&#20110;&#25552;&#21462;&#37329;&#34701;&#23545;&#35805;&#31995;&#32479;&#30340;&#31934;&#30830;&#26597;&#35810;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the exponential growth in large language models (LLMs), leveraging their emergent properties for specialized domains like finance merits exploration. However, regulated fields such as finance pose unique constraints, requiring domain-optimized frameworks. We present ConFIRM, an LLM-based conversational financial information retrieval model tailored for query intent classification and knowledge base labeling.  ConFIRM comprises two modules:  1) a method to synthesize finance domain-specific question-answer pairs, and  2) evaluation of parameter efficient fine-tuning approaches for the query classification task. We generate a dataset of over 4000 samples, assessing accuracy on a separate test set.  ConFIRM achieved over 90% accuracy, essential for regulatory compliance. ConFIRM provides a data-efficient solution to extract precise query intent for financial dialog systems.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GTA&#65289;&#65292;&#29992;&#20110;&#23558;&#20960;&#20309;&#32467;&#26500;&#32534;&#30721;&#20026;&#30456;&#23545;&#21464;&#25442;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22810;&#35270;&#22270;Transformer&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.10375</link><description>&lt;p&gt;
GTA&#65306;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#22810;&#35270;&#22270;Transformer&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
GTA: A Geometry-Aware Attention Mechanism for Multi-View Transformers. (arXiv:2310.10375v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.10375
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65288;GTA&#65289;&#65292;&#29992;&#20110;&#23558;&#20960;&#20309;&#32467;&#26500;&#32534;&#30721;&#20026;&#30456;&#23545;&#21464;&#25442;&#65292;&#20174;&#32780;&#25913;&#36827;&#20102;&#22810;&#35270;&#22270;Transformer&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;transformers&#23545;&#36755;&#20837;&#26631;&#35760;&#30340;&#25490;&#21015;&#20855;&#26377;&#31561;&#21464;&#24615;&#65292;&#23545;&#26631;&#35760;&#30340;&#20301;&#32622;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#23545;&#35768;&#22810;&#20219;&#21153;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29616;&#26377;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#26368;&#21021;&#26159;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#35774;&#35745;&#30340;&#65292;&#23545;&#20110;&#36890;&#24120;&#22312;&#20854;&#25968;&#25454;&#20013;&#34920;&#29616;&#20986;&#19981;&#21516;&#32467;&#26500;&#29305;&#24615;&#30340;&#35270;&#35273;&#20219;&#21153;&#26469;&#35828;&#65292;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#20540;&#24471;&#24576;&#30097;&#12290;&#25105;&#20204;&#35748;&#20026;&#29616;&#26377;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#26696;&#23545;&#20110;3D&#35270;&#35273;&#20219;&#21153;&#26469;&#35828;&#26159;&#27425;&#20248;&#30340;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#23562;&#37325;&#20854;&#24213;&#23618;&#30340;3D&#20960;&#20309;&#32467;&#26500;&#12290;&#22522;&#20110;&#36825;&#20010;&#20551;&#35774;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20960;&#20309;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23427;&#23558;&#26631;&#35760;&#30340;&#20960;&#20309;&#32467;&#26500;&#32534;&#30721;&#20026;&#30001;&#26597;&#35810;&#21644;&#38190;&#20540;&#23545;&#20043;&#38388;&#30340;&#20960;&#20309;&#20851;&#31995;&#25152;&#30830;&#23450;&#30340;&#30456;&#23545;&#21464;&#25442;&#12290;&#36890;&#36807;&#22312;&#31232;&#30095;&#23485;&#22522;&#32447;&#22810;&#35270;&#22270;&#35774;&#32622;&#20013;&#35780;&#20272;&#22810;&#20010;&#26032;&#39062;&#35270;&#22270;&#21512;&#25104;&#65288;NVS&#65289;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#8212;&#8212;&#20960;&#20309;&#21464;&#25442;&#27880;&#24847;&#21147;&#65288;GTA&#65289;&#22914;&#20309;&#25552;&#39640;&#20102;&#26368;&#20808;&#36827;&#30340;Transformer&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
As transformers are equivariant to the permutation of input tokens, encoding the positional information of tokens is necessary for many tasks. However, since existing positional encoding schemes have been initially designed for NLP tasks, their suitability for vision tasks, which typically exhibit different structural properties in their data, is questionable. We argue that existing positional encoding schemes are suboptimal for 3D vision tasks, as they do not respect their underlying 3D geometric structure. Based on this hypothesis, we propose a geometry-aware attention mechanism that encodes the geometric structure of tokens as relative transformation determined by the geometric relationship between queries and key-value pairs. By evaluating on multiple novel view synthesis (NVS) datasets in the sparse wide-baseline multi-view setting, we show that our attention, called Geometric Transform Attention (GTA), improves learning efficiency and performance of state-of-the-art transformer-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35299;&#26512;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#32452;&#20214;&#65292;&#25581;&#31034;&#20102;&#22270;&#20687;&#34920;&#31034;&#30340;&#26500;&#25104;&#26041;&#24335;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#34920;&#31034;&#35299;&#37322;&#20102;&#20854;&#21508;&#20010;&#37096;&#20998;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#29702;&#35299;&#27880;&#24847;&#21147;&#22836;&#21644;&#22270;&#20687;&#22359;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#30340;&#20462;&#22797;&#21644;&#25913;&#36827;&#65292;&#21253;&#25324;&#28040;&#38500;&#35823;&#29305;&#24449;&#21644;&#26500;&#24314;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#22120;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2310.05916</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#25991;&#26412;&#30340;&#20998;&#35299;&#35299;&#37322;CLIP&#22270;&#20687;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Interpreting CLIP's Image Representation via Text-Based Decomposition. (arXiv:2310.05916v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.05916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35299;&#26512;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#30340;&#32452;&#20214;&#65292;&#25581;&#31034;&#20102;&#22270;&#20687;&#34920;&#31034;&#30340;&#26500;&#25104;&#26041;&#24335;&#65292;&#24182;&#21033;&#29992;&#25991;&#26412;&#34920;&#31034;&#35299;&#37322;&#20102;&#20854;&#21508;&#20010;&#37096;&#20998;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#29702;&#35299;&#27880;&#24847;&#21147;&#22836;&#21644;&#22270;&#20687;&#22359;&#65292;&#20316;&#32773;&#23454;&#29616;&#20102;&#23545;&#27169;&#22411;&#30340;&#20462;&#22797;&#21644;&#25913;&#36827;&#65292;&#21253;&#25324;&#28040;&#38500;&#35823;&#29305;&#24449;&#21644;&#26500;&#24314;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#22120;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20010;&#21035;&#27169;&#22411;&#32452;&#20214;&#23545;&#26368;&#32456;&#34920;&#31034;&#30340;&#24433;&#21709;&#65292;&#25506;&#35752;&#20102;CLIP&#22270;&#20687;&#32534;&#30721;&#22120;&#12290;&#25105;&#20204;&#23558;&#22270;&#20687;&#34920;&#31034;&#20998;&#35299;&#20026;&#21508;&#20010;&#22270;&#20687;&#22359;&#12289;&#27169;&#22411;&#23618;&#21644;&#27880;&#24847;&#21147;&#22836;&#30340;&#27714;&#21644;&#65292;&#24182;&#20351;&#29992;CLIP&#30340;&#25991;&#26412;&#34920;&#31034;&#26469;&#35299;&#37322;&#36825;&#20123;&#27714;&#21644;&#39033;&#12290;&#36890;&#36807;&#35299;&#37322;&#27880;&#24847;&#21147;&#22836;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#23547;&#25214;&#33021;&#22815;&#36328;&#36234;&#36755;&#20986;&#31354;&#38388;&#30340;&#25991;&#26412;&#34920;&#31034;&#26469;&#34920;&#24449;&#27599;&#20010;&#22836;&#30340;&#20316;&#29992;&#65292;&#25581;&#31034;&#20986;&#35768;&#22810;&#22836;&#30340;&#29305;&#23450;&#23646;&#24615;&#35282;&#33394;&#65288;&#20363;&#22914;&#20301;&#32622;&#25110;&#24418;&#29366;&#65289;&#12290;&#25509;&#19979;&#26469;&#65292;&#36890;&#36807;&#35299;&#37322;&#22270;&#20687;&#22359;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;CLIP&#20013;&#30340;&#32039;&#23494;&#31354;&#38388;&#23450;&#20301;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;&#36825;&#19968;&#29702;&#35299;&#28040;&#38500;&#20102;CLIP&#20013;&#30340;&#35823;&#29305;&#24449;&#65292;&#24182;&#21019;&#24314;&#20102;&#19968;&#20010;&#24378;&#22823;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#20998;&#21106;&#22120;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#25193;&#23637;&#30340;&#23545;Transformer&#27169;&#22411;&#30340;&#29702;&#35299;&#26159;&#21487;&#23454;&#29616;&#30340;&#65292;&#24182;&#21487;&#29992;&#20110;&#20462;&#22797;&#21644;&#25913;&#36827;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero-shot image segmenter. Our results indicate that a scalable understanding of transformer models is attainable and can be used to repair and improve models.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21629;&#21517;&#24815;&#20363;&#21644;&#30456;&#20851;&#32570;&#38519;&#65292;&#20026;&#25105;&#20204;&#20102;&#35299;&#30740;&#31350;&#21040;&#23454;&#36341;&#36807;&#31243;&#25552;&#20379;&#20102;&#30693;&#35782;&#21644;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2310.01642</link><description>&lt;p&gt;
&#25506;&#32034;Hugging Face&#21644;&#20854;&#20182;&#27169;&#22411;&#20179;&#24211;&#20013;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21629;&#21517;&#24815;&#20363;&#65288;&#21450;&#32570;&#38519;&#65289;
&lt;/p&gt;
&lt;p&gt;
Exploring Naming Conventions (and Defects) of Pre-trained Deep Learning Models in Hugging Face and Other Model Hubs. (arXiv:2310.01642v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01642
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#31995;&#32479;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21629;&#21517;&#24815;&#20363;&#21644;&#30456;&#20851;&#32570;&#38519;&#65292;&#20026;&#25105;&#20204;&#20102;&#35299;&#30740;&#31350;&#21040;&#23454;&#36341;&#36807;&#31243;&#25552;&#20379;&#20102;&#30693;&#35782;&#21644;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#21019;&#26032;&#19981;&#26029;&#25512;&#36827;&#65292;&#35768;&#22810;&#24037;&#31243;&#24072;&#24076;&#26395;&#23558;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65288;PTMs&#65289;&#20316;&#20026;&#35745;&#31639;&#31995;&#32479;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;PTMs&#26159;&#30740;&#31350;&#21040;&#23454;&#36341;&#30340;&#27969;&#31243;&#30340;&#19968;&#37096;&#20998;&#65306;&#30740;&#31350;&#20154;&#21592;&#21457;&#24067;PTMs&#65292;&#24037;&#31243;&#24072;&#26681;&#25454;&#36136;&#37327;&#25110;&#24615;&#33021;&#36827;&#34892;&#35843;&#25972;&#24182;&#37096;&#32626;&#12290;&#22914;&#26524;PTM&#30340;&#20316;&#32773;&#20026;&#20854;&#36873;&#25321;&#36866;&#24403;&#30340;&#21517;&#31216;&#65292;&#21487;&#20197;&#20419;&#36827;&#27169;&#22411;&#30340;&#21457;&#29616;&#21644;&#22797;&#29992;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25253;&#36947;&#20102;&#27169;&#22411;&#21517;&#31216;&#24182;&#19981;&#24635;&#26159;&#36873;&#25321;&#24471;&#24456;&#22909;&#65292;&#26377;&#26102;&#29978;&#33267;&#26159;&#38169;&#35823;&#30340;&#12290;PTM&#21253;&#30340;&#21629;&#21517;&#24815;&#20363;&#21644;&#21629;&#21517;&#32570;&#38519;&#23578;&#26410;&#24471;&#21040;&#31995;&#32479;&#30340;&#30740;&#31350;&#65292;&#20102;&#35299;&#23427;&#20204;&#23558;&#22686;&#21152;&#25105;&#20204;&#23545;PTM&#21253;&#30340;&#30740;&#31350;&#21040;&#23454;&#36341;&#36807;&#31243;&#36816;&#20316;&#26041;&#24335;&#30340;&#35748;&#35782;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102;&#23545;PTM&#21629;&#21517;&#24815;&#20363;&#21450;&#30456;&#20851;&#21629;&#21517;&#32570;&#38519;&#30340;&#39318;&#27425;&#30740;&#31350;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;PTM&#21253;&#21517;&#31216;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#21253;&#25324;&#20803;&#25968;&#25454;&#20013;&#30340;&#21253;&#21517;&#31216;&#21644;&#22768;&#26126;&#30340;&#26550;&#26500;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31532;&#19968;&#39033;&#26088;&#22312;&#25551;&#36848;PTM&#21629;&#21517;&#24615;&#36136;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
As innovation in deep learning continues, many engineers want to adopt Pre-Trained deep learning Models (PTMs) as components in computer systems. PTMs are part of a research-to-practice pipeline: researchers publish PTMs, which engineers adapt for quality or performance and then deploy. If PTM authors choose appropriate names for their PTMs, it could facilitate model discovery and reuse. However, prior research has reported that model names are not always well chosen, and are sometimes erroneous. The naming conventions and naming defects for PTM packages have not been systematically studied - understanding them will add to our knowledge of how the research-to-practice process works for PTM packages  In this paper, we report the first study of PTM naming conventions and the associated PTM naming defects. We define the components of a PTM package name, comprising the package name and claimed architecture from the metadata. We present the first study focused on characterizing the nature o
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#23454;&#20363;&#20998;&#21106;&#30340;&#32958;&#33039;&#27963;&#26816;&#32467;&#26500;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#32479;&#35745;&#35299;&#21078;&#32467;&#26500;&#19978;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20174;&#32780;&#20943;&#23569;&#24037;&#20316;&#37327;&#21644;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.17166</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#38598;&#23454;&#20363;&#20998;&#21106;&#22312;&#32958;&#33039;&#27963;&#26816;&#32467;&#26500;&#35780;&#20272;&#26041;&#38754;&#30340;&#36827;&#23637;
&lt;/p&gt;
&lt;p&gt;
Advances in Kidney Biopsy Structural Assessment through Dense Instance Segmentation. (arXiv:2309.17166v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.17166
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#38598;&#23454;&#20363;&#20998;&#21106;&#30340;&#32958;&#33039;&#27963;&#26816;&#32467;&#26500;&#35780;&#20272;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#33258;&#21160;&#32479;&#35745;&#35299;&#21078;&#32467;&#26500;&#19978;&#30340;&#32479;&#35745;&#25968;&#25454;&#65292;&#20174;&#32780;&#20943;&#23569;&#24037;&#20316;&#37327;&#21644;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32958;&#33039;&#27963;&#26816;&#26159;&#32958;&#33039;&#30142;&#30149;&#35786;&#26029;&#30340;&#37329;&#26631;&#20934;&#12290;&#19987;&#23478;&#32958;&#33039;&#30149;&#29702;&#23398;&#23478;&#21046;&#23450;&#30340;&#30149;&#21464;&#35780;&#20998;&#26159;&#21322;&#23450;&#37327;&#30340;&#65292;&#24182;&#19988;&#23384;&#22312;&#39640;&#30340;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;&#22240;&#27492;&#65292;&#36890;&#36807;&#23545;&#20998;&#21106;&#30340;&#35299;&#21078;&#23545;&#35937;&#36827;&#34892;&#33258;&#21160;&#32479;&#35745;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#24037;&#20316;&#37327;&#21644;&#36825;&#31181;&#35266;&#23519;&#32773;&#38388;&#21464;&#24322;&#24615;&#12290;&#28982;&#32780;&#65292;&#27963;&#26816;&#30340;&#23454;&#20363;&#20998;&#21106;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#21407;&#22240;&#26377;&#65306;&#65288;a&#65289;&#24179;&#22343;&#25968;&#37327;&#36739;&#22823;&#65288;&#32422;300&#33267;1000&#20010;&#65289;&#23494;&#38598;&#25509;&#35302;&#30340;&#35299;&#21078;&#32467;&#26500;&#65292;&#65288;b&#65289;&#20855;&#26377;&#22810;&#20010;&#31867;&#21035;&#65288;&#33267;&#23569;3&#20010;&#65289;&#65292;&#65288;c&#65289;&#23610;&#23544;&#21644;&#24418;&#29366;&#21508;&#24322;&#12290;&#30446;&#21069;&#20351;&#29992;&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#19981;&#33021;&#20197;&#39640;&#25928;&#36890;&#29992;&#30340;&#26041;&#24335;&#21516;&#26102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#38170;&#28857;&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#23558;&#25193;&#25955;&#27169;&#22411;&#12289;&#21464;&#25442;&#22120;&#27169;&#22359;&#21644;RCNN&#65288;&#21306;&#22495;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65289;&#32467;&#21512;&#36215;&#26469;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#19968;&#21488;NVIDIA GeForce RTX 3090 GPU&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#20294;&#21487;&#20197;&#25552;&#20379;&#21487;&#35266;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The kidney biopsy is the gold standard for the diagnosis of kidney diseases. Lesion scores made by expert renal pathologists are semi-quantitative and suffer from high inter-observer variability. Automatically obtaining statistics per segmented anatomical object, therefore, can bring significant benefits in reducing labor and this inter-observer variability. Instance segmentation for a biopsy, however, has been a challenging problem due to (a) the on average large number (around 300 to 1000) of densely touching anatomical structures, (b) with multiple classes (at least 3) and (c) in different sizes and shapes. The currently used instance segmentation models cannot simultaneously deal with these challenges in an efficient yet generic manner. In this paper, we propose the first anchor-free instance segmentation model that combines diffusion models, transformer modules, and RCNNs (regional convolution neural networks). Our model is trained on just one NVIDIA GeForce RTX 3090 GPU, but can 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#20559;&#22909;&#26041;&#27861;&#8212;&#8212;&#20851;&#31995;&#29942;&#39048;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#35825;&#23548;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#20154;&#31867;&#24605;&#32500;&#21644;&#22823;&#33041;&#20013;&#25277;&#35937;&#27010;&#24565;&#20064;&#24471;&#20013;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.06629</link><description>&lt;p&gt;
&#20316;&#20026;&#26377;&#25928;&#25277;&#35937;&#30340;&#24402;&#32435;&#20559;&#22909;&#30340;&#20851;&#31995;&#29942;&#39048;
&lt;/p&gt;
&lt;p&gt;
The Relational Bottleneck as an Inductive Bias for Efficient Abstraction. (arXiv:2309.06629v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06629
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24402;&#32435;&#20559;&#22909;&#26041;&#27861;&#8212;&#8212;&#20851;&#31995;&#29942;&#39048;&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#35825;&#23548;&#25277;&#35937;&#27010;&#24565;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#20854;&#22312;&#20154;&#31867;&#24605;&#32500;&#21644;&#22823;&#33041;&#20013;&#25277;&#35937;&#27010;&#24565;&#20064;&#24471;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35748;&#30693;&#31185;&#23398;&#30340;&#19968;&#20010;&#26680;&#24515;&#25361;&#25112;&#26159;&#35299;&#37322;&#22914;&#20309;&#20174;&#26377;&#38480;&#32463;&#39564;&#20013;&#33719;&#21462;&#25277;&#35937;&#27010;&#24565;&#12290;&#36825;&#19968;&#21162;&#21147;&#24120;&#24120;&#34987;&#25551;&#36848;&#20026;&#32463;&#39564;&#20027;&#20041;&#21644;&#22825;&#36171;&#20027;&#20041;&#26041;&#27861;&#20043;&#38388;&#30340;&#20108;&#20998;&#27861;&#65292;&#26368;&#36817;&#20027;&#35201;&#20307;&#29616;&#22312;&#26377;&#20851;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#35748;&#30693;&#27169;&#22411;&#30340;&#20105;&#35770;&#20013;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#19968;&#31181;&#26368;&#36817;&#20852;&#36215;&#30340;&#24037;&#20316;&#32447;&#36335;&#65292;&#35813;&#32447;&#36335;&#36890;&#36807;&#21033;&#29992;&#25105;&#20204;&#31216;&#20043;&#20026;&#20851;&#31995;&#29942;&#39048;&#30340;&#24402;&#32435;&#20559;&#22909;&#65292;&#25552;&#20986;&#20102;&#36825;&#20123;&#26041;&#27861;&#30340;&#19968;&#31181;&#26032;&#30340;&#35843;&#21644;&#26041;&#24335;&#12290;&#25105;&#20204;&#22238;&#39038;&#20102;&#19968;&#31995;&#21015;&#37319;&#29992;&#36825;&#31181;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340;&#26041;&#24335;&#19979;&#35825;&#23548;&#20986;&#25277;&#35937;&#30340;&#27169;&#22411;&#65292;&#24378;&#35843;&#20102;&#23427;&#20204;&#20316;&#20026;&#20154;&#31867;&#24605;&#32500;&#21644;&#22823;&#33041;&#20013;&#25277;&#35937;&#27010;&#24565;&#20064;&#24471;&#30340;&#20505;&#36873;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
A central challenge for cognitive science is to explain how abstract concepts are acquired from limited experience. This effort has often been framed in terms of a dichotomy between empiricist and nativist approaches, most recently embodied by debates concerning deep neural networks and symbolic cognitive models. Here, we highlight a recently emerging line of work that suggests a novel reconciliation of these approaches, by exploiting an inductive bias that we term the relational bottleneck. We review a family of models that employ this approach to induce abstractions in a data-efficient manner, emphasizing their potential as candidate models for the acquisition of abstract concepts in the human mind and brain.
&lt;/p&gt;</description></item><item><title>RLSynC&#26159;&#19968;&#31181;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#21516;&#26102;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#35780;&#20272;&#21453;&#24212;&#29289;&#30340;&#21512;&#25104;&#33021;&#21147;&#26469;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;</title><link>http://arxiv.org/abs/2309.02671</link><description>&lt;p&gt;
RLSynC: &#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#21512;&#25104;&#26041;&#27861;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
RLSynC: Offline-Online Reinforcement Learning for Synthon Completion. (arXiv:2309.02671v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02671
&lt;/p&gt;
&lt;p&gt;
RLSynC&#26159;&#19968;&#31181;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;&#23427;&#20351;&#29992;&#22810;&#20010;&#20195;&#29702;&#21516;&#26102;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#65292;&#24182;&#36890;&#36807;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#35780;&#20272;&#21453;&#24212;&#29289;&#30340;&#21512;&#25104;&#33021;&#21147;&#26469;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36870;&#21521;&#21512;&#25104;&#26159;&#30830;&#23450;&#33021;&#22815;&#21453;&#24212;&#24418;&#25104;&#25152;&#38656;&#20135;&#29289;&#30340;&#19968;&#32452;&#21453;&#24212;&#29289;&#20998;&#23376;&#30340;&#36807;&#31243;&#12290;&#21322;&#27169;&#26495;&#21270;&#36870;&#21521;&#21512;&#25104;&#26041;&#27861;&#39318;&#20808;&#39044;&#27979;&#20135;&#29289;&#20013;&#30340;&#21453;&#24212;&#20013;&#24515;&#65292;&#28982;&#21518;&#23558;&#29983;&#25104;&#30340;&#21512;&#25104;&#29289;&#37325;&#26032;&#34917;&#20840;&#25104;&#21453;&#24212;&#29289;&#12290;&#36825;&#20123;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#24517;&#35201;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#39640;&#23454;&#29992;&#24615;&#65292;&#20197;&#25351;&#23548;&#21512;&#25104;&#35268;&#21010;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#31163;&#32447;-&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;RLSynC&#65292;&#29992;&#20110;&#21322;&#27169;&#26495;&#21270;&#26041;&#27861;&#20013;&#30340;&#21512;&#25104;&#29289;&#34917;&#20840;&#12290;RLSynC&#20026;&#27599;&#20010;&#21512;&#25104;&#29289;&#20998;&#37197;&#19968;&#20010;&#20195;&#29702;&#65292;&#25152;&#26377;&#20195;&#29702;&#37117;&#36890;&#36807;&#21516;&#27493;&#36827;&#34892;&#36880;&#27493;&#34892;&#21160;&#65292;&#23436;&#25104;&#21512;&#25104;&#29289;&#30340;&#34917;&#20840;&#12290;RLSynC&#36890;&#36807;&#21516;&#26102;&#36827;&#34892;&#31163;&#32447;&#35757;&#32451;&#21644;&#22312;&#32447;&#20132;&#20114;&#26469;&#23398;&#20064;&#31574;&#30053;&#65292;&#20174;&#32780;&#21487;&#20197;&#25506;&#32034;&#26032;&#30340;&#21453;&#24212;&#31354;&#38388;&#12290;RLSynC&#20351;&#29992;&#27491;&#21521;&#21512;&#25104;&#27169;&#22411;&#26469;&#35780;&#20272;&#39044;&#27979;&#30340;&#21453;&#24212;&#29289;&#22312;&#21512;&#25104;&#20135;&#29289;&#26102;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#25351;&#23548;&#34892;&#21160;&#25628;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Retrosynthesis is the process of determining the set of reactant molecules that can react to form a desired product. Semi-template-based retrosynthesis methods, which imitate the reverse logic of synthesis reactions, first predict the reaction centers in the products, and then complete the resulting synthons back into reactants. These methods enable necessary interpretability and high practical utility to inform synthesis planning. We develop a new offline-online reinforcement learning method RLSynC for synthon completion in semi-template-based methods. RLSynC assigns one agent to each synthon, all of which complete the synthons by conducting actions step by step in a synchronized fashion. RLSynC learns the policy from both offline training episodes and online interactions which allow RLSynC to explore new reaction spaces. RLSynC uses a forward synthesis model to evaluate the likelihood of the predicted reactants in synthesizing a product, and thus guides the action search. We compare 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;</title><link>http://arxiv.org/abs/2308.12044</link><description>&lt;p&gt;
&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A multiobjective continuation method to compute the regularization path of deep neural networks. (arXiv:2308.12044v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#24310;&#32493;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27491;&#21017;&#21270;&#36335;&#24452;&#65292;&#20197;&#35299;&#20915;DNNs&#20013;&#31232;&#30095;&#24615;&#21644;&#25968;&#20540;&#25928;&#29575;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#20013;&#38750;&#24120;&#29702;&#24819;&#30340;&#29305;&#24449;&#65292;&#22240;&#20026;&#23427;&#30830;&#20445;&#20102;&#25968;&#20540;&#25928;&#29575;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;(&#30001;&#20110;&#30456;&#20851;&#29305;&#24449;&#30340;&#25968;&#37327;&#36739;&#23569;)&#21644;&#40065;&#26834;&#24615;&#12290;&#22312;&#22522;&#20110;&#32447;&#24615;&#27169;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20013;&#65292;&#20247;&#25152;&#21608;&#30693;&#22312;$\ell^1$&#33539;&#25968;(&#21363;&#38646;&#26435;&#37325;)&#30340;&#26368;&#31232;&#30095;&#35299;&#21644;&#38750;&#27491;&#21017;&#21270;&#35299;&#20043;&#38388;&#23384;&#22312;&#19968;&#26465;&#36830;&#25509;&#36335;&#24452;&#65292;&#36825;&#26465;&#36335;&#24452;&#34987;&#31216;&#20026;&#27491;&#21017;&#21270;&#36335;&#24452;&#12290;&#26368;&#36817;&#65292;&#36890;&#36807;&#23558;&#32463;&#39564;&#25439;&#22833;&#21644;&#31232;&#30095;&#24615;($\ell^1$&#33539;&#25968;)&#20316;&#20026;&#20004;&#20010;&#20914;&#31361;&#30340;&#26631;&#20934;&#65292;&#24182;&#35299;&#20915;&#30001;&#27492;&#20135;&#29983;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#39318;&#27425;&#23581;&#35797;&#23558;&#27491;&#21017;&#21270;&#36335;&#24452;&#30340;&#27010;&#24565;&#25193;&#23637;&#21040;DNNs&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;$\ell^1$&#33539;&#25968;&#30340;&#19981;&#20809;&#28369;&#24615;&#21644;&#21442;&#25968;&#25968;&#37327;&#30340;&#39640;&#24230;&#65292;&#20174;&#35745;&#31639;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#26041;&#27861;&#24182;&#19981;&#26159;&#24456;&#26377;&#25928;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#21487;&#20197;&#36817;&#20284;&#35745;&#31639;&#25972;&#20010;&#24085;&#32047;&#25176;&#26354;&#32447;
&lt;/p&gt;
&lt;p&gt;
Sparsity is a highly desired feature in deep neural networks (DNNs) since it ensures numerical efficiency, improves the interpretability of models (due to the smaller number of relevant features), and robustness. In machine learning approaches based on linear models, it is well known that there exists a connecting path between the sparsest solution in terms of the $\ell^1$ norm (i.e., zero weights) and the non-regularized solution, which is called the regularization path. Very recently, there was a first attempt to extend the concept of regularization paths to DNNs by means of treating the empirical loss and sparsity ($\ell^1$ norm) as two conflicting criteria and solving the resulting multiobjective optimization problem. However, due to the non-smoothness of the $\ell^1$ norm and the high number of parameters, this approach is not very efficient from a computational perspective. To overcome this limitation, we present an algorithm that allows for the approximation of the entire Pareto
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09591</link><description>&lt;p&gt;
&#26799;&#24230;&#21453;&#20987;&#65306;&#22914;&#20309;&#28388;&#38500;&#39640;&#39057;&#29575;&#25552;&#39640;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Gradient strikes back: How filtering out high frequencies improves explanations. (arXiv:2307.09591v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09591
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#19982;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#19981;&#21516;&#30340;&#39640;&#39057;&#20869;&#23481;&#65292;&#28388;&#38500;&#39640;&#39057;&#29575;&#21487;&#20197;&#25552;&#39640;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26032;&#22411;&#22522;&#20110;&#39044;&#27979;&#30340;&#23646;&#24615;&#26041;&#27861;&#30340;&#21457;&#23637;&#36805;&#29467;&#65292;&#36880;&#28176;&#21462;&#20195;&#20102;&#26087;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20026;&#20309;&#20248;&#20110;&#26799;&#24230;&#22411;&#26041;&#27861;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#20174;&#32463;&#39564;&#35266;&#23519;&#24320;&#22987;&#65306;&#36825;&#20004;&#31181;&#26041;&#27861;&#20135;&#29983;&#30340;&#23646;&#24615;&#22270;&#20855;&#26377;&#38750;&#24120;&#19981;&#21516;&#30340;&#21151;&#29575;&#35889;&#65292;&#26799;&#24230;&#22411;&#26041;&#27861;&#25581;&#31034;&#20102;&#27604;&#39044;&#27979;&#22411;&#26041;&#27861;&#26356;&#22810;&#30340;&#39640;&#39057;&#20869;&#23481;&#12290;&#36825;&#19968;&#35266;&#23519;&#24341;&#21457;&#20102;&#22810;&#20010;&#38382;&#39064;&#65306;&#36825;&#31181;&#39640;&#39057;&#20449;&#24687;&#30340;&#26469;&#28304;&#26159;&#20160;&#20040;&#65292;&#23427;&#26159;&#21542;&#30495;&#27491;&#21453;&#26144;&#20102;&#31995;&#32479;&#25152;&#20316;&#20986;&#30340;&#20915;&#31574;&#65311;&#26368;&#21518;&#65292;&#20026;&#20160;&#20040;&#22312;&#22810;&#20010;&#35780;&#20215;&#25351;&#26631;&#19979;&#65292;&#39044;&#27979;&#22411;&#26041;&#27861;&#20013;&#32570;&#20047;&#39640;&#39057;&#20449;&#24687;&#23558;&#20135;&#29983;&#26356;&#22909;&#30340;&#21487;&#35299;&#37322;&#24615;&#20998;&#25968;&#65311;&#25105;&#20204;&#20998;&#26512;&#20102;&#19977;&#20010;&#20195;&#34920;&#24615;&#30340;&#35270;&#35273;&#20998;&#31867;&#27169;&#22411;&#30340;&#26799;&#24230;&#65292;&#24182;&#35266;&#23519;&#21040;&#23427;&#21253;&#21547;&#26469;&#33258;&#39640;&#39057;&#30340;&#22122;&#22768;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an explosion in the development of novel prediction-based attribution methods, which have slowly been supplanting older gradient-based methods to explain the decisions of deep neural networks. However, it is still not clear why prediction-based methods outperform gradient-based ones. Here, we start with an empirical observation: these two approaches yield attribution maps with very different power spectra, with gradient-based methods revealing more high-frequency content than prediction-based methods. This observation raises multiple questions: What is the source of this high-frequency information, and does it truly reflect decisions made by the system? Lastly, why would the absence of high-frequency information in prediction-based methods yield better explainability scores along multiple metrics? We analyze the gradient of three representative visual classification models and observe that it contains noisy information emanating from high-frequencies. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;NILM&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#35745;&#31639;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#22686;&#24378;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#22312;&#34394;&#25311;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.09244</link><description>&lt;p&gt;
&#38754;&#21521;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#21487;&#25345;&#32493;&#28145;&#24230;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Sustainable Deep Learning for Multi-Label Classification on NILM. (arXiv:2307.09244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;NILM&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#35745;&#31639;&#21644;&#33021;&#28304;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#23545;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#30340;&#22686;&#24378;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#27979;&#35797;&#26041;&#27861;&#65292;&#21487;&#20197;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#22312;&#34394;&#25311;&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38750;&#20405;&#20837;&#24335;&#36127;&#36733;&#30417;&#27979;&#65288;NILM&#65289;&#26159;&#20174;&#21333;&#20010;&#35745;&#37327;&#28857;&#33719;&#21462;&#23478;&#24237;&#25110;&#20225;&#19994;&#24635;&#30005;&#21147;&#28040;&#32791;&#30340;&#30005;&#22120;&#32423;&#25968;&#25454;&#30340;&#36807;&#31243;&#12290;&#30005;&#22120;&#32423;&#25968;&#25454;&#21487;&#20197;&#30452;&#25509;&#29992;&#20110;&#38656;&#27714;&#21709;&#24212;&#24212;&#29992;&#12289;&#33021;&#28304;&#31649;&#29702;&#31995;&#32479;&#20197;&#21450;&#25552;&#39640;&#33021;&#25928;&#21644;&#20943;&#23569;&#30899;&#36275;&#36857;&#30340;&#24847;&#35782;&#25552;&#39640;&#21644;&#28608;&#21169;&#12290;&#26368;&#36817;&#65292;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#25216;&#26415;&#22312;NILM&#20998;&#31867;&#20013;&#21464;&#24471;&#38750;&#24120;&#27969;&#34892;&#65292;&#24182;&#35777;&#26126;&#22312;&#22686;&#38271;&#30340;&#22797;&#26434;&#24615;&#19979;&#23545;NILM&#20998;&#31867;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#38543;&#30528;&#22797;&#26434;&#24230;&#30340;&#22686;&#21152;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;&#25805;&#20316;&#36807;&#31243;&#20013;&#38754;&#20020;&#30528;&#26174;&#33879;&#30340;&#35745;&#31639;&#21644;&#33021;&#28304;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;DL&#27169;&#22411;&#65292;&#26088;&#22312;&#36890;&#36807;&#25552;&#39640;&#35745;&#31639;&#21644;&#33021;&#28304;&#25928;&#29575;&#26469;&#22686;&#24378;NILM&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20351;&#29992;&#20174;&#27979;&#37327;&#25968;&#25454;&#38598;&#21512;&#25104;&#30340;&#25968;&#25454;&#27604;&#36739;&#19981;&#21516;&#27169;&#22411;&#30340;&#27979;&#35797;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Non-intrusive load monitoring (NILM) is the process of obtaining appliance-level data from a single metering point, measuring total electricity consumption of a household or a business. Appliance-level data can be directly used for demand response applications and energy management systems as well as for awareness raising and motivation for improvements in energy efficiency and reduction in the carbon footprint. Recently, classical machine learning and deep learning (DL) techniques became very popular and proved as highly effective for NILM classification, but with the growing complexity these methods are faced with significant computational and energy demands during both their training and operation. In this paper, we introduce a novel DL model aimed at enhanced multi-label classification of NILM with improved computation and energy efficiency. We also propose a testing methodology for comparison of different models using data synthesized from the measurement datasets so as to better 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21453;&#20107;&#23454;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#19968;&#23450;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#24448;&#24448;&#20063;&#20381;&#36182;&#20110;&#29421;&#31364;&#12289;&#38590;&#20197;&#36716;&#31227;&#30340;&#36807;&#31243;&#65292;&#36825;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#35299;&#37322;&#21644;&#29702;&#35299;&#26377;&#30528;&#37325;&#35201;&#30340;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.02477</link><description>&lt;p&gt;
&#25512;&#29702;&#36824;&#26159;&#32972;&#35829;&#65311;&#36890;&#36807;&#21453;&#20107;&#23454;&#20219;&#21153;&#25506;&#32034;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks. (arXiv:2307.02477v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02477
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21453;&#20107;&#23454;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#20855;&#22791;&#19968;&#23450;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#20013;&#24448;&#24448;&#20063;&#20381;&#36182;&#20110;&#29421;&#31364;&#12289;&#38590;&#20197;&#36716;&#31227;&#30340;&#36807;&#31243;&#65292;&#36825;&#23545;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#35299;&#37322;&#21644;&#29702;&#35299;&#26377;&#30528;&#37325;&#35201;&#30340;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#34920;&#29616;&#34920;&#26126;&#23427;&#20204;&#20855;&#22791;&#19968;&#23450;&#31243;&#24230;&#30340;&#25277;&#35937;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#20123;&#33021;&#21147;&#26159;&#36890;&#29992;&#19988;&#21487;&#36716;&#31227;&#30340;&#65292;&#36824;&#26159;&#19987;&#38376;&#38024;&#23545;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#36935;&#21040;&#30340;&#29305;&#23450;&#20219;&#21153;&#65311;&#20026;&#20102;&#20998;&#24320;&#36825;&#20123;&#25928;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22522;&#20110;&#8220;&#21453;&#20107;&#23454;&#8221;&#20219;&#21153;&#21464;&#31181;&#65292;&#36825;&#20123;&#21464;&#31181;&#19982;&#25903;&#25745;&#26631;&#20934;&#20219;&#21153;&#30340;&#40664;&#35748;&#20551;&#35774;&#26377;&#25152;&#20559;&#31163;&#12290;&#22312;&#19968;&#22871;&#21253;&#21547;11&#20010;&#20219;&#21153;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#21453;&#20107;&#23454;&#21464;&#31181;&#30340;&#38750;&#24179;&#20961;&#24615;&#33021;&#65292;&#20294;&#19982;&#40664;&#35748;&#26465;&#20214;&#30456;&#27604;&#65292;&#24615;&#33021;&#26174;&#33879;&#32780;&#25345;&#32493;&#22320;&#19979;&#38477;&#12290;&#36825;&#34920;&#26126;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#20855;&#22791;&#25277;&#35937;&#20219;&#21153;&#27714;&#35299;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#20063;&#20381;&#36182;&#20110;&#29421;&#31364;&#12289;&#38590;&#20197;&#36716;&#31227;&#30340;&#20219;&#21153;&#27714;&#35299;&#36807;&#31243;&#12290;&#36825;&#20123;&#32467;&#26524;&#20419;&#20351;&#25105;&#20204;&#23545;&#35821;&#35328;&#27169;&#22411;&#24615;&#33021;&#36827;&#34892;&#26356;&#21152;&#35880;&#24910;&#30340;&#35299;&#37322;&#65292;&#20197;&#21306;&#20998;&#36825;&#20123;&#34892;&#20026;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to a degree, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#26679;&#26412;&#38598;&#20248;&#21270;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#27700;&#19979;&#33322;&#34892;&#22120;&#33337;&#20307;&#65292;&#20854;&#20013;&#20195;&#29702;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#20248;&#21270;&#26356;&#21152;&#24555;&#36895;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2304.12420</link><description>&lt;p&gt;
&#27700;&#19979;&#33322;&#34892;&#22120;&#33337;&#20307;&#30340;&#26679;&#26412;&#39640;&#25928;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#35774;&#35745;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Sample-Efficient and Surrogate-Based Design Optimization of Underwater Vehicle Hulls. (arXiv:2304.12420v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12420
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20351;&#29992;&#20102;&#39640;&#25928;&#30340;&#26679;&#26412;&#38598;&#20248;&#21270;&#21644;&#22522;&#20110;&#20195;&#29702;&#30340;&#26041;&#27861;&#26469;&#35774;&#35745;&#27700;&#19979;&#33322;&#34892;&#22120;&#33337;&#20307;&#65292;&#20854;&#20013;&#20195;&#29702;&#27169;&#22411;&#26174;&#33879;&#25552;&#39640;&#20102;&#35745;&#31639;&#25928;&#29575;&#65292;&#20351;&#20248;&#21270;&#26356;&#21152;&#24555;&#36895;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#29702;&#27169;&#25311;&#26159;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;(CAD)&#20248;&#21270;&#36807;&#31243;&#20013;&#30340;&#19968;&#20010;&#35745;&#31639;&#29942;&#39048;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20351;&#31934;&#30830;(&#35745;&#31639;&#26114;&#36149;)&#30340;&#27169;&#25311;&#21487;&#29992;&#20110;&#35774;&#35745;&#20248;&#21270;&#20013;&#65292;&#38656;&#35201;&#19968;&#20010;&#39640;&#26679;&#26412;&#25928;&#29575;&#30340;&#20248;&#21270;&#26694;&#26550;&#25110;&#24555;&#36895;&#30340;&#25968;&#25454;&#39537;&#21160;&#20195;&#29702;(&#20195;&#29702;&#27169;&#22411;)&#26469;&#20195;&#26367;&#38271;&#26102;&#38388;&#36816;&#34892;&#30340;&#27169;&#25311;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#26368;&#36817;&#20248;&#21270;&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#30340;&#36827;&#23637;&#26469;&#35299;&#20915;&#36825;&#20004;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20197;&#35774;&#35745;&#19968;&#20010;&#26368;&#20339;&#30340;&#26080;&#20154;&#27700;&#19979;&#33322;&#34892;&#22120;(UUV)&#12290;&#25105;&#20204;&#39318;&#20808;&#30740;&#31350;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#20248;&#21270;&#25216;&#26415;&#22312;&#20248;&#21270;&#24490;&#29615;&#20013;&#19982;&#26631;&#20934;&#35745;&#31639;&#27969;&#20307;&#21147;&#23398;(CFD)&#27714;&#35299;&#22120;&#30456;&#32467;&#21512;&#26102;&#30340;&#26679;&#26412;&#25928;&#29575;&#21644;&#25910;&#25947;&#34892;&#20026;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNN)&#30340;&#20195;&#29702;&#27169;&#22411;&#26469;&#36924;&#36817;&#21542;&#21017;&#36890;&#36807;CFD&#27714;&#35299;&#22120;&#36827;&#34892;&#35745;&#31639;&#30340;&#38459;&#21147;&#12290;&#20195;&#29702;&#27169;&#22411;&#36827;&#32780;&#29992;&#20110;&#26679;&#26412;&#39640;&#25928;&#30340;&#20248;&#21270;&#26694;&#26550;&#20013;&#65292;&#35813;&#26694;&#26550;&#22312;&#19981;&#20351;&#29992;&#20195;&#29702;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#26631;&#20934;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Physics simulations are a computational bottleneck in computer-aided design (CAD) optimization processes. Hence, in order to make accurate (computationally expensive) simulations feasible for use in design optimization, one requires either an optimization framework that is highly sample-efficient or fast data-driven proxies (surrogate models) for long running simulations. In this work, we leverage recent advances in optimization and artificial intelligence (AI) to address both of these potential solutions, in the context of designing an optimal unmanned underwater vehicle (UUV). We first investigate and compare the sample efficiency and convergence behavior of different optimization techniques with a standard computational fluid dynamics (CFD) solver in the optimization loop. We then develop a deep neural network (DNN) based surrogate model to approximate drag forces that would otherwise be computed via direct numerical simulation with the CFD solver. The surrogate model is in turn use
&lt;/p&gt;</description></item><item><title>&#23433;&#20840;&#21487;&#35299;&#37322;&#26426;&#22120;&#20154;&#35268;&#21010;&#26041;&#27861;&#65288;SEP&#65289;&#25193;&#23637;&#20102;&#21487;&#35299;&#37322;&#35268;&#21010;&#65292;&#25903;&#25345;&#23433;&#20840;&#30028;&#38480;&#30340;&#35268;&#23450;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#21644;&#21487;&#35299;&#37322;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.03773</link><description>&lt;p&gt;
&#23433;&#20840;&#21487;&#35299;&#37322;&#26426;&#22120;&#20154;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe Explicable Robot Planning. (arXiv:2304.03773v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03773
&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#21487;&#35299;&#37322;&#26426;&#22120;&#20154;&#35268;&#21010;&#26041;&#27861;&#65288;SEP&#65289;&#25193;&#23637;&#20102;&#21487;&#35299;&#37322;&#35268;&#21010;&#65292;&#25903;&#25345;&#23433;&#20840;&#30028;&#38480;&#30340;&#35268;&#23450;&#65292;&#20197;&#23454;&#29616;&#23433;&#20840;&#21644;&#21487;&#35299;&#37322;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#30340;&#26399;&#26395;&#28304;&#33258;&#20110;&#20182;&#20204;&#23545;&#20854;&#20182;&#20154;&#21644;&#19990;&#30028;&#30340;&#20102;&#35299;&#12290;&#22312;&#28041;&#21450;&#21040;&#20154;&#26426;&#20132;&#20114;&#30340;&#24773;&#20917;&#19979;&#65292;&#23545;&#26426;&#22120;&#20154;&#30340;&#20102;&#35299;&#21487;&#33021;&#19982;&#29616;&#23454;&#19981;&#31526;&#65292;&#23548;&#33268;&#26426;&#22120;&#20154;&#19981;&#33021;&#28385;&#36275;&#20154;&#20204;&#30340;&#26399;&#26395;&#12290;&#21487;&#35299;&#37322;&#35268;&#21010;&#34987;&#24341;&#20837;&#20316;&#20026;&#19968;&#31181;&#26032;&#39062;&#30340;&#35268;&#21010;&#26041;&#27861;&#65292;&#20197;&#21327;&#35843;&#20154;&#31867;&#26399;&#26395;&#21644;&#26368;&#20248;&#26426;&#22120;&#20154;&#34892;&#20026;&#65292;&#36827;&#34892;&#26356;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#20154;&#20915;&#31574;&#12290;&#19968;&#20010;&#20851;&#38190;&#30340;&#38382;&#39064;&#23578;&#26410;&#24471;&#21040;&#35299;&#20915;&#65292;&#37027;&#23601;&#26159;&#22312;&#21487;&#35299;&#37322;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#23433;&#20840;&#30340;&#21487;&#35299;&#37322;&#34892;&#20026;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#23433;&#20840;&#21487;&#35299;&#37322;&#35268;&#21010;&#65288;SEP&#65289;&#65292;&#23427;&#25193;&#23637;&#20102;&#21487;&#35299;&#37322;&#35268;&#21010;&#65292;&#25903;&#25345;&#23433;&#20840;&#30028;&#38480;&#30340;&#35268;&#23450;&#12290; SEP&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#31181;&#31574;&#30053;&#65292;&#29983;&#25104;&#25509;&#36817;&#20110;&#20154;&#31867;&#26399;&#26395;&#30340;&#34892;&#20026;&#65292;&#21516;&#26102;&#28385;&#36275;&#23433;&#20840;&#32422;&#26463;&#30340;&#35201;&#27714;&#12290;&#36825;&#26159;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;SEP&#30340;&#35299;&#20915;&#26041;&#26696;&#20301;&#20110;&#24085;&#32047;&#25176;&#21069;&#27839;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20999;&#23454;&#21487;&#34892;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#19981;&#29306;&#29298;&#20219;&#20309;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#30340;&#21069;&#25552;&#19979;&#65292;&#20135;&#29983;&#20102;&#23433;&#20840;&#24615;&#21644;&#35299;&#37322;&#24615;&#20043;&#38388;&#30340;&#19968;&#20010;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human expectations stem from their knowledge of the others and the world. Where human-robot interaction is concerned, such knowledge about the robot may be inconsistent with the ground truth, resulting in the robot not meeting its expectations. Explicable planning was previously introduced as a novel planning approach to reconciling human expectations and the optimal robot behavior for more interpretable robot decision-making. One critical issue that remains unaddressed is safety during explicable decision-making which can lead to explicable behaviors that are unsafe. We propose Safe Explicable Planning (SEP), which extends explicable planning to support the specification of a safety bound. The objective of SEP is to find a policy that generates a behavior close to human expectations while satisfying the safety constraints introduced by the bound, which is a special case of multi-objective optimization where the solution to SEP lies on the Pareto frontier. Under such a formulation, we 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#27425;&#23398;&#20064;&#30340;&#21382;&#21490;&#25163;&#31295;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861; OTS&#65292; &#23588;&#20854;&#23545;&#20110;&#20302;&#36164;&#28304;&#26816;&#27979;&#20219;&#21153;&#65292;&#20351;&#29992;&#26032;&#22411;&#30340;&#8220;&#29615;&#24418;&#25439;&#22833;&#8221;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#20102;&#26816;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#21019;&#24314;&#20102;&#21253;&#21547;&#21476;&#20195;&#19996;&#24052;&#35937;&#24418;&#25991;&#23383;&#30340;&#25163;&#31295;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.00746</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#27425;&#23398;&#20064;&#30340;&#21382;&#21490;&#25163;&#31295;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861; OTS
&lt;/p&gt;
&lt;p&gt;
OTS: A One-shot Learning Approach for Text Spotting in Historical Manuscripts. (arXiv:2304.00746v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00746
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#27425;&#23398;&#20064;&#30340;&#21382;&#21490;&#25163;&#31295;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861; OTS&#65292; &#23588;&#20854;&#23545;&#20110;&#20302;&#36164;&#28304;&#26816;&#27979;&#20219;&#21153;&#65292;&#20351;&#29992;&#26032;&#22411;&#30340;&#8220;&#29615;&#24418;&#25439;&#22833;&#8221;&#25439;&#22833;&#20989;&#25968;&#25552;&#39640;&#20102;&#26816;&#27979;&#33021;&#21147;&#65292;&#21516;&#26102;&#21019;&#24314;&#20102;&#21253;&#21547;&#21476;&#20195;&#19996;&#24052;&#35937;&#24418;&#25991;&#23383;&#30340;&#25163;&#31295;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21382;&#21490;&#25163;&#31295;&#22788;&#29702;&#38754;&#20020;&#26377;&#38480;&#30340;&#27880;&#37322;&#35757;&#32451;&#25968;&#25454;&#21644;&#26032;&#31867;&#21035;&#20986;&#29616;&#31561;&#25361;&#25112;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#27425;&#23398;&#20064;&#30340;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861; OTS&#65292;&#36890;&#36807;&#20165;&#19968;&#20010;&#27880;&#37322;&#26679;&#26412;&#65292;&#20934;&#30830;&#21487;&#38752;&#22320;&#26816;&#27979;&#20986;&#26032;&#39062;&#23383;&#31526;&#12290;&#28789;&#24863;&#28304;&#33258;&#35748;&#30693;&#30740;&#31350;&#65292;&#24341;&#20837;&#31354;&#38388;&#23545;&#40784;&#27169;&#22359;&#65292;&#22522;&#20110;&#19968;&#20010;&#25903;&#25345;&#22270;&#20687;&#21457;&#29616;&#12289;&#20851;&#27880;&#21644;&#23398;&#20064;&#26597;&#35810;&#22270;&#20687;&#20013;&#26368;&#20855;&#26377;&#21306;&#21035;&#24615;&#30340;&#31354;&#38388;&#21306;&#22495;&#12290;&#23588;&#20854;&#26159;&#65292;&#38024;&#23545;&#20302;&#36164;&#28304;&#26816;&#27979;&#20219;&#21153;&#36890;&#24120;&#38754;&#20020;&#26679;&#26412;&#19981;&#24179;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#29615;&#24418;&#25439;&#22833;&#8221;&#30340;&#26032;&#22411;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#20351;&#36317;&#31163;&#24230;&#37327;&#30340;&#23884;&#20837;&#31354;&#38388;&#26356;&#20855;&#26377;&#21306;&#20998;&#24615;&#12290;&#35813;&#26041;&#27861;&#39640;&#25928;&#65292;&#21482;&#38656;&#35201;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65292;&#20855;&#26377;&#22788;&#29702;&#26032;&#39062;&#23383;&#31526;&#21644;&#31526;&#21495;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#20026;&#20102;&#22686;&#24378;&#25968;&#25454;&#38598;&#30340;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#21476;&#20195;&#19996;&#24052;&#35937;&#24418;&#25991;&#23383;&#65288;DBH&#65289;&#30340;&#25163;&#31295;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Historical manuscript processing poses challenges like limited annotated training data and novel class emergence. To address this, we propose a novel One-shot learning-based Text Spotting (OTS) approach that accurately and reliably spots novel characters with just one annotated support sample. Drawing inspiration from cognitive research, we introduce a spatial alignment module that finds, focuses on, and learns the most discriminative spatial regions in the query image based on one support image. Especially, since the low-resource spotting task often faces the problem of example imbalance, we propose a novel loss function called torus loss which can make the embedding space of distance metric more discriminative. Our approach is highly efficient and requires only a few training samples while exhibiting the remarkable ability to handle novel characters, and symbols. To enhance dataset diversity, a new manuscript dataset that contains the ancient Dongba hieroglyphics (DBH) is created. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2303.06530</link><description>&lt;p&gt;
&#22312;&#32852;&#37030;&#28145;&#24230;&#23398;&#20064;&#20013;&#20248;&#21270;&#25209;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
Making Batch Normalization Great in Federated Deep Learning. (arXiv:2303.06530v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#20351;&#29992;&#25209;&#26631;&#20934;&#21270;&#21644;&#32676;&#32452;&#24402;&#19968;&#21270;&#30340;&#25928;&#26524;&#65292;&#21457;&#29616;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;&#25209;&#26631;&#20934;&#21270;&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;&#32852;&#37030;&#23398;&#20064;&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper studies the use of batch normalization and group normalization in federated learning, and finds that with proper treatments, batch normalization can be highly competitive across a wide range of federated learning settings, and this requires no additional training or communication costs.
&lt;/p&gt;
&lt;p&gt;
&#25209;&#26631;&#20934;&#21270;&#65288;BN&#65289;&#36890;&#24120;&#29992;&#20110;&#29616;&#20195;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#20013;&#65292;&#20197;&#25552;&#39640;&#31283;&#23450;&#24615;&#24182;&#21152;&#36895;&#38598;&#20013;&#24335;&#35757;&#32451;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#22312;&#20855;&#26377;&#38750;IID&#20998;&#25955;&#25968;&#25454;&#30340;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20013;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#35266;&#23519;&#21040;&#20351;&#29992;BN&#36827;&#34892;&#35757;&#32451;&#21487;&#33021;&#20250;&#30001;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#30340;BN&#32479;&#35745;&#19981;&#21305;&#37197;&#32780;&#38459;&#30861;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#32676;&#32452;&#24402;&#19968;&#21270;&#65288;GN&#65289;&#26356;&#24120;&#29992;&#20110;FL&#20316;&#20026;BN&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25105;&#20204;&#22312;&#21508;&#31181;FL&#35774;&#32622;&#19979;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;BN&#21644;GN&#20043;&#38388;&#27809;&#26377;&#19968;&#33268;&#30340;&#20248;&#32988;&#32773;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;FL&#20013;&#24402;&#19968;&#21270;&#23618;&#30340;&#20351;&#29992;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36866;&#24403;&#30340;&#22788;&#29702;&#19979;&#65292;BN&#21487;&#20197;&#22312;&#24191;&#27867;&#30340;FL&#35774;&#32622;&#20013;&#20855;&#26377;&#24456;&#39640;&#30340;&#31454;&#20105;&#21147;&#65292;&#32780;&#19988;&#36825;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#35757;&#32451;&#25110;&#36890;&#20449;&#25104;&#26412;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#30740;&#31350;&#21487;&#20197;&#25104;&#20026;FL&#26410;&#26469;&#23454;&#38469;&#20351;&#29992;&#21644;&#29702;&#35770;&#20998;&#26512;&#30340;&#26377;&#20215;&#20540;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Batch Normalization (BN) is commonly used in modern deep neural networks (DNNs) to improve stability and speed up convergence during centralized training. In federated learning (FL) with non-IID decentralized data, previous works observed that training with BN could hinder performance due to the mismatch of the BN statistics between training and testing. Group Normalization (GN) is thus more often used in FL as an alternative to BN. However, from our empirical study across various FL settings, we see no consistent winner between BN and GN. This leads us to revisit the use of normalization layers in FL. We find that with proper treatments, BN can be highly competitive across a wide range of FL settings, and this requires no additional training or communication costs. We hope that our study could serve as a valuable reference for future practical usage and theoretical analysis in FL.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#23545;&#27604;&#25439;&#22833;&#21644;&#20351;&#29992;PU&#29305;&#23450;&#32858;&#31867;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#22312;PU&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20102;&#20248;&#31168;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2206.01206</link><description>&lt;p&gt;
&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Positive Unlabeled Contrastive Learning. (arXiv:2206.01206v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.01206
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#23545;&#27604;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25193;&#23637;&#23545;&#27604;&#25439;&#22833;&#21644;&#20351;&#29992;PU&#29305;&#23450;&#32858;&#31867;&#26041;&#26696;&#65292;&#35813;&#26041;&#27861;&#22312;PU&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#20102;&#20248;&#31168;&#30340;&#34920;&#31034;&#65292;&#24182;&#22312;&#22810;&#20010;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#26080;&#26631;&#31614;&#25968;&#25454;&#65292;&#28982;&#21518;&#22312;&#26631;&#35760;&#25968;&#25454;&#19978;&#36827;&#34892;&#30417;&#30563;&#24494;&#35843;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#20174;&#26377;&#38480;&#26631;&#35760;&#26679;&#26412;&#20013;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26041;&#27861;&#25193;&#23637;&#21040;&#32463;&#20856;&#30340;&#27491;&#26679;&#26412;&#26410;&#26631;&#35760;&#65288;PU&#65289;&#35774;&#32622;&#65292;&#20854;&#20013;&#30340;&#20219;&#21153;&#26159;&#20165;&#36890;&#36807;&#19968;&#20123;&#26631;&#35760;&#20026;&#27491;&#26679;&#26412;&#21644;&#65288;&#36890;&#24120;&#65289;&#22823;&#37327;&#30340;&#26410;&#26631;&#35760;&#26679;&#26412;&#65288;&#21487;&#20197;&#26159;&#27491;&#26679;&#26412;&#25110;&#36127;&#26679;&#26412;&#65289;&#26469;&#23398;&#20064;&#20108;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#26631;&#20934;infoNCE&#23545;&#27604;&#25439;&#22833;&#30340;&#23478;&#26063;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25193;&#23637;&#65292;&#36866;&#29992;&#20110;PU&#35774;&#32622;&#65307;&#24182;&#19988;&#35777;&#26126;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#26080;&#30417;&#30563;&#21644;&#26377;&#30417;&#30563;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#23398;&#20064;&#21040;&#20102;&#26356;&#22909;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26032;&#30340;PU&#29305;&#23450;&#32858;&#31867;&#26041;&#26696;&#20026;&#26410;&#26631;&#35760;&#26679;&#26412;&#26500;&#24314;&#20266;&#26631;&#31614;&#65307;&#36825;&#20123;&#20266;&#26631;&#31614;&#21487;&#20197;&#29992;&#26469;&#35757;&#32451;&#26368;&#32456;&#30340;&#65288;&#27491;&#26679;&#26412; vs. &#36127;&#26679;&#26412;&#65289;&#20998;&#31867;&#22120;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#26631;&#20934;PU&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#30340;PU&#26041;&#27861;&#65292;&#24182;&#19988;&#19981;&#38656;&#35201;&#20219;&#20309;&#31867;&#21035;&#30340;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised pretraining on unlabeled data followed by supervised fine-tuning on labeled data is a popular paradigm for learning from limited labeled examples. We extend this paradigm to the classical positive unlabeled (PU) setting, where the task is to learn a binary classifier given only a few labeled positive samples, and (often) a large amount of unlabeled samples (which could be positive or negative).  We first propose a simple extension of standard infoNCE family of contrastive losses, to the PU setting; and show that this learns superior representations, as compared to existing unsupervised and supervised approaches. We then develop a simple methodology to pseudo-label the unlabeled samples using a new PU-specific clustering scheme; these pseudo-labels can then be used to train the final (positive vs. negative) classifier. Our method handily outperforms state-of-the-art PU methods over several standard PU benchmark datasets, while not requiring a-priori knowledge of any clas
&lt;/p&gt;</description></item></channel></rss>