<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#23558;CVE&#28431;&#27934;&#35760;&#24405;&#26144;&#23556;&#21040;MITRE CWE&#24369;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2304.11130</link><description>&lt;p&gt;
&#33258;&#21160;&#23558;CVE&#28431;&#27934;&#35760;&#24405;&#26144;&#23556;&#21040;MITRE CWE&#24369;&#28857;
&lt;/p&gt;
&lt;p&gt;
Automated Mapping of CVE Vulnerability Records to MITRE CWE Weaknesses. (arXiv:2304.11130v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11130
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#23558;CVE&#28431;&#27934;&#35760;&#24405;&#26144;&#23556;&#21040;MITRE CWE&#24369;&#28857;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20960;&#24180;&#65292;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#21644;&#22810;&#26679;&#24615;&#30340;&#22686;&#21152;&#23548;&#33268;&#20102;&#28431;&#27934;&#25253;&#21578;&#21644;&#20998;&#26512;&#30340;&#22686;&#21152;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#35768;&#22810;&#38750;&#33829;&#21033;&#32452;&#32455;&#22312;&#36825;&#19968;&#39046;&#22495;&#23835;&#36215;&#65292;&#22914;MITRE&#21644;OSWAP&#65292;&#20182;&#20204;&#19968;&#30452;&#22312;&#31215;&#26497;&#36861;&#36394;&#28431;&#27934;&#65292;&#24182;&#20197;&#26631;&#20934;&#21270;&#26684;&#24335;&#21457;&#24067;&#38450;&#24481;&#24314;&#35758;&#12290;&#30001;&#20110;&#25163;&#21160;&#29983;&#20135;&#36825;&#31181;&#26684;&#24335;&#30340;&#25968;&#25454;&#38750;&#24120;&#32791;&#26102;&#65292;&#22240;&#27492;&#19968;&#20123;&#25552;&#35758;&#35797;&#22270;&#33258;&#21160;&#21270;&#35813;&#36807;&#31243;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#37319;&#29992;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#35299;&#20915;&#27492;&#38382;&#39064;&#30340;&#19968;&#20010;&#37325;&#35201;&#38556;&#30861;&#26159;&#32570;&#20047;&#20844;&#24320;&#30340;&#19987;&#19994;&#25968;&#25454;&#38598;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23558;CVE&#35760;&#24405;&#26144;&#23556;&#21040;MITRE CWE&#24369;&#28857;&#65292;&#24182;&#21521;&#30740;&#31350;&#31038;&#21306;&#21457;&#24067;&#20102;&#19968;&#20010;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;4,012&#26465;&#35760;&#24405;&#12290;&#22312;&#32771;&#34385;&#21040;&#20154;&#22312;&#24490;&#29615;&#26694;&#26550;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23558;&#38382;&#39064;&#35270;&#20026;&#25490;&#21517;&#20219;&#21153;&#65292;&#24182;&#26088;&#22312;&#37319;&#29992;&#24378;&#21270;&#23398;&#20064;&#26469;&#21033;&#29992;&#20854;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a proliferation of cyber-security threats and diversity has been on the rise culminating in an increase in their reporting and analysis. To counter that, many non-profit organizations have emerged in this domain, such as MITRE and OSWAP, which have been actively tracking vulnerabilities, and publishing defense recommendations in standardized formats. As producing data in such formats manually is very time-consuming, there have been some proposals to automate the process. Unfortunately, a major obstacle to adopting supervised machine learning for this problem has been the lack of publicly available specialized datasets. Here, we aim to bridge this gap. In particular, we focus on mapping CVE records into MITRE CWE Weaknesses, and we release to the research community a manually annotated dataset of 4,012 records for this task. With a human-in-the-loop framework in mind, we approach the problem as a ranking task and aim to incorporate reinforced learning to make use of the
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.11127</link><description>&lt;p&gt;
&#26641;&#29366;Parzen&#20272;&#35745;&#22120;&#65306;&#29702;&#35299;&#20854;&#31639;&#27861;&#32452;&#25104;&#37096;&#20998;&#21450;&#20854;&#22312;&#25552;&#39640;&#23454;&#35777;&#34920;&#29616;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Tree-structured Parzen estimator: Understanding its algorithm components and their roles for better empirical performance. (arXiv:2304.11127v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11127
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861; Tree-structured Parzen estimator (TPE)&#65292;&#24182;&#23545;&#20854;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#21644;&#31639;&#27861;&#30452;&#35273;&#36827;&#34892;&#20102;&#35752;&#35770;&#21644;&#20998;&#26512;&#65292;&#25552;&#20379;&#20102;&#19968;&#32452;&#25512;&#33616;&#35774;&#32622;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#25552;&#39640;TPE&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#39046;&#22495;&#20013;&#26368;&#36817;&#30340;&#36827;&#23637;&#35201;&#27714;&#26356;&#21152;&#22797;&#26434;&#30340;&#23454;&#39564;&#35774;&#35745;&#12290;&#36825;&#31181;&#22797;&#26434;&#30340;&#23454;&#39564;&#36890;&#24120;&#26377;&#35768;&#22810;&#21442;&#25968;&#65292;&#38656;&#35201;&#21442;&#25968;&#35843;&#25972;&#12290;Tree-structured Parzen estimator (TPE) &#26159;&#19968;&#31181;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#65292;&#22312;&#26368;&#36817;&#30340;&#21442;&#25968;&#35843;&#25972;&#26694;&#26550;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#23613;&#31649;&#23427;&#24456;&#21463;&#27426;&#36814;&#65292;&#20294;&#25511;&#21046;&#21442;&#25968;&#30340;&#35282;&#33394;&#21644;&#31639;&#27861;&#30452;&#35273;&#23578;&#26410;&#24471;&#21040;&#35752;&#35770;&#12290;&#22312;&#26412;&#25945;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#30830;&#23450;&#27599;&#20010;&#25511;&#21046;&#21442;&#25968;&#30340;&#20316;&#29992;&#20197;&#21450;&#23427;&#20204;&#23545;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#24433;&#21709;&#65292;&#20351;&#29992;&#22810;&#31181;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#23558;&#20174;&#21078;&#26512;&#30740;&#31350;&#20013;&#24471;&#20986;&#30340;&#25512;&#33616;&#35774;&#32622;&#19982;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#25512;&#33616;&#35774;&#32622;&#25552;&#39640;&#20102;TPE&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;TPE&#23454;&#29616;&#21487;&#22312;https://github.com/nabenabe0928/tpe/tree/single-opt&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in many domains require more and more complicated experiment design. Such complicated experiments often have many parameters, which necessitate parameter tuning. Tree-structured Parzen estimator (TPE), a Bayesian optimization method, is widely used in recent parameter tuning frameworks. Despite its popularity, the roles of each control parameter and the algorithm intuition have not been discussed so far. In this tutorial, we will identify the roles of each control parameter and their impacts on hyperparameter optimization using a diverse set of benchmarks. We compare our recommended setting drawn from the ablation study with baseline methods and demonstrate that our recommended setting improves the performance of TPE. Our TPE implementation is available at https://github.com/nabenabe0928/tpe/tree/single-opt.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#35821;&#20041;&#23398;&#12289;&#26412;&#20307;&#35770;&#21644;&#35299;&#37322;&#20013;&#30340;&#27010;&#24565;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26412;&#20307;&#25286;&#21253;&#30340;&#35299;&#37322;&#27010;&#24565;&#65292;&#21363;&#36890;&#36807;&#25581;&#31034;&#31526;&#21495;&#39046;&#22495;&#25551;&#36848;&#30340;&#30495;&#30456;&#21046;&#36896;&#32773;&#26469;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#31561;&#65307;&#36824;&#35752;&#35770;&#20102;&#26412;&#20307;&#35770;&#39537;&#21160;&#30340;&#27010;&#24565;&#27169;&#22411;&#22312;&#21508;&#31181;AI&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.11124</link><description>&lt;p&gt;
&#35821;&#20041;&#23398;&#12289;&#26412;&#20307;&#35770;&#19982;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Semantics, Ontology and Explanation. (arXiv:2304.11124v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11124
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#35821;&#20041;&#23398;&#12289;&#26412;&#20307;&#35770;&#21644;&#35299;&#37322;&#20013;&#30340;&#27010;&#24565;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#26412;&#20307;&#25286;&#21253;&#30340;&#35299;&#37322;&#27010;&#24565;&#65292;&#21363;&#36890;&#36807;&#25581;&#31034;&#31526;&#21495;&#39046;&#22495;&#25551;&#36848;&#30340;&#30495;&#30456;&#21046;&#36896;&#32773;&#26469;&#35299;&#37322;&#27010;&#24565;&#27169;&#22411;&#31561;&#65307;&#36824;&#35752;&#35770;&#20102;&#26412;&#20307;&#35770;&#39537;&#21160;&#30340;&#27010;&#24565;&#27169;&#22411;&#22312;&#21508;&#31181;AI&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#35821;&#20041;&#8221;&#21644;&#8220;&#26412;&#20307;&#35770;&#8221;&#36825;&#20004;&#20010;&#35789;&#36234;&#26469;&#36234;&#19982;&#8220;&#35299;&#37322;&#8221;&#19968;&#36215;&#20986;&#29616;&#65292;&#19981;&#20165;&#22312;&#31185;&#23398;&#25991;&#29486;&#20013;&#65292;&#32780;&#19988;&#22312;&#32452;&#32455;&#20132;&#27969;&#20013;&#20063;&#39057;&#32321;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#26415;&#35821;&#37117;&#34987;&#22823;&#37327;&#36229;&#36733;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#23427;&#20204;&#20043;&#38388;&#30340;&#24378;&#20851;&#32852;&#24615;&#65292;&#29305;&#21035;&#26159;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#31181;&#35299;&#37322;&#27010;&#24565;&#65292;&#31216;&#20026;&#26412;&#20307;&#25286;&#21253;&#65292;&#26088;&#22312;&#36890;&#36807;&#25581;&#31034;&#20854;&#20551;&#23450;&#30340;&#30495;&#30456;&#21046;&#36896;&#32773;&#8212;&#8212;&#21363;&#20351;&#37027;&#20123;&#25551;&#36848;&#20013;&#30340;&#21629;&#39064;&#20026;&#30495;&#30340;&#26412;&#20307;&#35770;&#23454;&#20307;&#65292;&#26469;&#35299;&#37322;&#31526;&#21495;&#39046;&#22495;&#25551;&#36848;&#65288;&#27010;&#24565;&#27169;&#22411;&#12289;&#30693;&#35782;&#22270;&#21644;&#36923;&#36753;&#35268;&#33539;&#65289;&#12290;&#20026;&#20102;&#35828;&#26126;&#36825;&#20010;&#24819;&#27861;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#20851;&#31995;&#30340;&#26412;&#20307;&#35770;&#29702;&#35770;&#65292;&#36890;&#36807;&#25581;&#31034;&#26631;&#20934;&#24314;&#27169;&#35821;&#35328;UML&#20013;&#32534;&#30721;&#30340;&#38750;&#24120;&#31616;&#21333;&#30340;&#31526;&#21495;&#27169;&#22411;&#30340;&#38544;&#34255;&#35821;&#20041;&#26469;&#35299;&#37322;&#23427;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#30001;&#26412;&#24418;&#24335;&#30340;&#35299;&#37322;&#36896;&#25104;&#30340;&#26412;&#20307;&#35770;&#39537;&#21160;&#30340;&#27010;&#24565;&#27169;&#22411;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#21508;&#31181;&#24418;&#24335;&#30340;&#22522;&#20110;AI&#30340;&#20915;&#31574;&#21046;&#23450;&#20013;&#25152;&#21457;&#25381;&#30340;&#20851;&#38190;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The terms 'semantics' and 'ontology' are increasingly appearing together with 'explanation', not only in the scientific literature, but also in organizational communication. However, all of these terms are also being significantly overloaded. In this paper, we discuss their strong relation under particular interpretations. Specifically, we discuss a notion of explanation termed ontological unpacking, which aims at explaining symbolic domain descriptions (conceptual models, knowledge graphs, logical specifications) by revealing their ontological commitment in terms of their assumed truthmakers, i.e., the entities in one's ontology that make the propositions in those descriptions true. To illustrate this idea, we employ an ontological theory of relations to explain (by revealing the hidden semantics of) a very simple symbolic model encoded in the standard modeling language UML. We also discuss the essential role played by ontology-driven conceptual models (resulting from this form of exp
&lt;/p&gt;</description></item><item><title>&#20013;&#22269;&#21644;&#32654;&#22269;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21512;&#20316;&#33021;&#20135;&#29983;&#26356;&#22823;&#24433;&#21709;&#21147;&#65292;&#26368;&#36817;&#25968;&#25454;&#26174;&#31034;&#20004;&#22269;&#33258;2000&#24180;&#26469;&#19968;&#30452;&#22788;&#20110;&#39046;&#23548;&#22320;&#20301;&#65292;&#32780;&#22823;&#22810;&#25968;&#20154;&#25165;&#27969;&#22833;&#22312;&#20004;&#22269;&#20043;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.11123</link><description>&lt;p&gt;
&#20013;&#32654;&#21512;&#20316;&#26102;&#65292;&#20013;&#22269;&#21644;&#32654;&#22269;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#33021;&#22815;&#20135;&#29983;&#26356;&#22823;&#30340;&#24433;&#21709;&#21147;
&lt;/p&gt;
&lt;p&gt;
China and the U.S. produce more impactful AI research when collaborating together. (arXiv:2304.11123v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11123
&lt;/p&gt;
&lt;p&gt;
&#20013;&#22269;&#21644;&#32654;&#22269;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21512;&#20316;&#33021;&#20135;&#29983;&#26356;&#22823;&#24433;&#21709;&#21147;&#65292;&#26368;&#36817;&#25968;&#25454;&#26174;&#31034;&#20004;&#22269;&#33258;2000&#24180;&#26469;&#19968;&#30452;&#22788;&#20110;&#39046;&#23548;&#22320;&#20301;&#65292;&#32780;&#22823;&#22810;&#25968;&#20154;&#25165;&#27969;&#22833;&#22312;&#20004;&#22269;&#20043;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#25104;&#20026;&#39072;&#35206;&#24615;&#25216;&#26415;&#65292;&#26377;&#26395;&#20026;&#25484;&#25569;&#20854;&#21147;&#37327;&#30340;&#22269;&#23478;&#24102;&#26469;&#26174;&#33879;&#30340;&#32463;&#27982;&#21644;&#25112;&#30053;&#20248;&#21183;&#12290;&#26368;&#36817;&#65292;&#20013;&#22269;&#25512;&#21160;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#37319;&#29992;&#65292;&#27491;&#22312;&#25361;&#25112;&#32654;&#22269;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#20840;&#29699;&#39046;&#23548;&#22320;&#20301;&#12290;&#32771;&#34385;&#21040;&#20154;&#24037;&#26234;&#33021;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#20197;&#21450;&#20004;&#22269;&#20043;&#38388;&#28608;&#28872;&#30340;&#22320;&#32536;&#25919;&#27835;&#32039;&#24352;&#23616;&#21183;&#65292;&#24050;&#32463;&#21046;&#23450;&#20102;&#19968;&#20123;&#25919;&#31574;&#65292;&#20197;&#38450;&#27490;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#23478;&#31227;&#27665;&#21040;&#23545;&#26041;&#22269;&#23478;&#25110;&#19982;&#20043;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20154;&#25165;&#27969;&#22833;&#21644;&#36328;&#22659;&#21512;&#20316;&#30340;&#31243;&#24230;&#36824;&#27809;&#26377;&#34987;&#23436;&#20840;&#20102;&#35299;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36229;&#36807;350,000&#21517;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#23478;&#21644;5,000,000&#31687;&#20154;&#24037;&#26234;&#33021;&#25991;&#29486;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#21457;&#29616;&#33258;2000&#24180;&#20197;&#26469;&#65292;&#20013;&#22269;&#21644;&#32654;&#22269;&#22312;&#24433;&#21709;&#21147;&#12289;&#21019;&#26032;&#24615;&#12289;&#29983;&#20135;&#21147;&#21644;&#21171;&#21160;&#21147;&#26041;&#38754;&#19968;&#30452;&#22788;&#20110;&#39046;&#20808;&#22320;&#20301;&#12290;&#22823;&#22810;&#25968;&#31227;&#27665;&#21040;&#20013;&#22269;&#30340;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#23478;&#26469;&#33258;&#32654;&#22269;&#65292;&#32780;&#31227;&#27665;&#21040;&#32654;&#22269;&#30340;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#23478;&#26469;&#33258;&#20013;&#22269;&#65292;&#20984;&#26174;&#20986;&#26126;&#26174;&#30340;&#20154;&#25165;&#27969;&#22833;&#29616;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) has become a disruptive technology, promising to grant a significant economic and strategic advantage to the nations that harness its power. China, with its recent push towards AI adoption, is challenging the U.S.'s position as the global leader in this field. Given AI's massive potential, as well as the fierce geopolitical tensions between the two nations, a number of policies have been put in place that discourage AI scientists from migrating to, or collaborating with, the other country. However, the extents of such brain drain and cross-border collaboration are not fully understood. Here, we analyze a dataset of over 350,000 AI scientists and 5,000,000 AI papers. We find that, since the year 2000, China and the U.S. have been leading the field in terms of impact, novelty, productivity, and workforce. Most AI scientists who migrate to China come from the U.S., and most who migrate to the U.S. come from China, highlighting a notable brain drain in both dir
&lt;/p&gt;</description></item><item><title>BoDiffusion&#26159;&#19968;&#31181;&#24212;&#29992;&#20110;&#20840;&#36523;&#20154;&#20307;&#36816;&#21160;&#21512;&#25104;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#36319;&#36394;&#36755;&#20837;&#29983;&#25104;&#24179;&#28369;&#36924;&#30495;&#30340;&#23436;&#25972;&#20840;&#36523;&#36816;&#21160;&#24207;&#21015;&#65292;&#35813;&#26041;&#27861;&#22312;&#36924;&#30495;&#24230;&#21644;&#37325;&#24314;&#35823;&#24046;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.11118</link><description>&lt;p&gt;
BoDiffusion&#65306;&#24212;&#29992;&#20110;&#20840;&#36523;&#20154;&#20307;&#36816;&#21160;&#21512;&#25104;&#30340;&#31232;&#30095;&#35266;&#27979;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
BoDiffusion: Diffusing Sparse Observations for Full-Body Human Motion Synthesis. (arXiv:2304.11118v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11118
&lt;/p&gt;
&lt;p&gt;
BoDiffusion&#26159;&#19968;&#31181;&#24212;&#29992;&#20110;&#20840;&#36523;&#20154;&#20307;&#36816;&#21160;&#21512;&#25104;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#31232;&#30095;&#36319;&#36394;&#36755;&#20837;&#29983;&#25104;&#24179;&#28369;&#36924;&#30495;&#30340;&#23436;&#25972;&#20840;&#36523;&#36816;&#21160;&#24207;&#21015;&#65292;&#35813;&#26041;&#27861;&#22312;&#36924;&#30495;&#24230;&#21644;&#37325;&#24314;&#35823;&#24046;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#29616;&#23454;&#24212;&#29992;&#38656;&#35201;&#36319;&#36394;&#29992;&#25143;&#30340;&#20840;&#36523;&#36816;&#21160;&#20197;&#23454;&#29616;&#27785;&#28024;&#24335;&#20307;&#39564;&#12290;&#28982;&#32780;&#65292;&#20856;&#22411;&#30340;&#22836;&#25140;&#24335;&#35774;&#22791;&#21482;&#33021;&#36319;&#36394;&#22836;&#37096;&#21644;&#25163;&#37096;&#36816;&#21160;&#65292;&#23548;&#33268;&#30001;&#20110;&#19979;&#21322;&#36523;&#23039;&#24577;&#30340;&#21464;&#24322;&#24615;&#32780;&#23545;&#23436;&#25972;&#30340;&#20840;&#36523;&#36816;&#21160;&#37325;&#24314;&#23384;&#22312;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;BoDiffusion&#8212;&#8212;&#19968;&#31181;&#29992;&#20110;&#36816;&#21160;&#21512;&#25104;&#30340;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;&#65292;&#20197;&#24212;&#23545;&#36825;&#31181;&#27424;&#32422;&#26463;&#30340;&#37325;&#24314;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#31354;&#26465;&#20214;&#26041;&#26696;&#65292;&#20351;BoDiffusion&#33021;&#22815;&#21033;&#29992;&#31232;&#30095;&#36319;&#36394;&#36755;&#20837;&#21516;&#26102;&#29983;&#25104;&#24179;&#28369;&#12289;&#36924;&#30495;&#30340;&#23436;&#25972;&#20840;&#36523;&#36816;&#21160;&#24207;&#21015;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#21033;&#29992;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#23558;&#20840;&#36523;&#36319;&#36394;&#24314;&#27169;&#20026;&#26465;&#20214;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#21160;&#20316;&#25429;&#25417;&#25968;&#25454;&#38598;AMASS&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20840;&#36523;&#36816;&#21160;&#36924;&#30495;&#24230;&#21644;&#20851;&#33410;&#37325;&#24314;&#35823;&#24046;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mixed reality applications require tracking the user's full-body motion to enable an immersive experience. However, typical head-mounted devices can only track head and hand movements, leading to a limited reconstruction of full-body motion due to variability in lower body configurations. We propose BoDiffusion -- a generative diffusion model for motion synthesis to tackle this under-constrained reconstruction problem. We present a time and space conditioning scheme that allows BoDiffusion to leverage sparse tracking inputs while generating smooth and realistic full-body motion sequences. To the best of our knowledge, this is the first approach that uses the reverse diffusion process to model full-body tracking as a conditional sequence generation task. We conduct experiments on the large-scale motion-capture dataset AMASS and show that our approach outperforms the state-of-the-art approaches by a significant margin in terms of full-body motion realism and joint reconstruction error.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;Graph-ToolFormer&#26694;&#26550;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#29616;&#26377;LLMs&#22312;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#24369;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.11116</link><description>&lt;p&gt;
Graph-ToolFormer: &#36890;&#36807;ChatGPT&#22686;&#24378;&#30340;&#25552;&#31034;&#65292;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Graph-ToolFormer: To Empower LLMs with Graph Reasoning Ability via Prompt Augmented by ChatGPT. (arXiv:2304.11116v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;Graph-ToolFormer&#26694;&#26550;&#36171;&#20104;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#35299;&#20915;&#29616;&#26377;LLMs&#22312;&#25191;&#34892;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23384;&#22312;&#30340;&#22266;&#26377;&#24369;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#33021;&#22815;&#23545;&#22797;&#26434;&#22270;&#24418;&#25968;&#25454;&#36827;&#34892;&#25512;&#29702;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#12290;&#24403;&#21069;&#65292;LLMs&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#38750;&#24120;&#20986;&#33394;&#30340;&#34920;&#29616;&#65292;&#36825;&#20123;&#25193;&#23637;&#20063;&#24050;&#34987;&#24212;&#29992;&#20110;&#30740;&#31350;&#20855;&#26377;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#35270;&#35273;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#65292;&#29616;&#26377;&#30340;LLMs&#30001;&#20110;&#22312;&#25191;&#34892;&#22810;&#27493;&#36923;&#36753;&#25512;&#29702;&#12289;&#31934;&#30830;&#30340;&#25968;&#23398;&#35745;&#31639;&#20197;&#21450;&#23545;&#31354;&#38388;&#21644;&#26102;&#38388;&#22240;&#32032;&#30340;&#24863;&#30693;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#22266;&#26377;&#24369;&#28857;&#65292;&#22240;&#27492;&#21576;&#29616;&#20986;&#38750;&#24120;&#20005;&#37325;&#30340;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#23558;&#35843;&#26597;&#25506;&#32034;&#36171;&#20104;&#29616;&#26377;LLMs&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#30340;&#21407;&#29702;&#12289;&#26041;&#27861;&#21644;&#31639;&#27861;&#65292;&#36825;&#23558;&#23545;LLMs&#21644;&#22270;&#24418;&#23398;&#20064;&#30340;&#24403;&#21069;&#30740;&#31350;&#20135;&#29983;&#24040;&#22823;&#24433;&#21709;&#12290;&#21463;&#26368;&#26032;&#30340;ChatGPT&#21644;Toolformer&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Graph-ToolFormer&#65288;&#38754;&#21521;&#22270;&#24418;&#25512;&#29702;&#30340;Toolformer&#65289;&#26694;&#26550;&#65292;&#36890;&#36807;ChatGPT&#22686;&#24378;&#30340;&#25552;&#31034;&#26469;&#25945;&#23548;LLMs&#33258;&#36523;&#65292;&#26088;&#22312;&#22521;&#20859;&#20182;&#20204;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we aim to develop a large language model (LLM) with the reasoning ability on complex graph data. Currently, LLMs have achieved very impressive performance on various natural language learning tasks, extensions of which have also been applied to study the vision tasks with multi-modal data. However, when it comes to the graph learning tasks, existing LLMs present very serious flaws due to their several inherited weaknesses in performing {multi-step logic reasoning}, {precise mathematical calculation} and {perception about the spatial and temporal factors}.  To address such challenges, in this paper, we will investigate the principles, methodologies and algorithms to empower existing LLMs with graph reasoning ability, which will have tremendous impacts on the current research of both LLMs and graph learning. Inspired by the latest ChatGPT and Toolformer models, we propose the Graph-ToolFormer (Graph Reasoning oriented Toolformer) framework to teach LLMs themselves with pro
&lt;/p&gt;</description></item><item><title>&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26045;&#21152;&#28966;&#34385;&#33021;&#24433;&#21709;&#23427;&#20204;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;&#65292;&#36825;&#38656;&#35201;&#26356;&#22810;&#36947;&#24503;&#32771;&#34385;&#21644;&#30417;&#31649;&#12290;</title><link>http://arxiv.org/abs/2304.11111</link><description>&lt;p&gt;
&#24341;&#21457;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#28966;&#34385;&#20250;&#22686;&#21152;&#23427;&#20204;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Inducing anxiety in large language models increases exploration and bias. (arXiv:2304.11111v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11111
&lt;/p&gt;
&lt;p&gt;
&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26045;&#21152;&#28966;&#34385;&#33021;&#24433;&#21709;&#23427;&#20204;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;&#65292;&#36825;&#38656;&#35201;&#26356;&#22810;&#36947;&#24503;&#32771;&#34385;&#21644;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#25913;&#21464;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#65292;&#24341;&#21457;&#20844;&#20247;&#30340;&#36777;&#35770;&#12290;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#19981;&#20165;&#20309;&#26102;&#33021;&#22815;&#27491;&#24120;&#24037;&#20316;&#21644;&#25104;&#21151;&#65292;&#20063;&#20026;&#20160;&#20040;&#20250;&#22833;&#36133;&#21644;&#34892;&#20026;&#22833;&#24120;&#65292;&#20855;&#26377;&#24040;&#22823;&#30340;&#31038;&#20250;&#24847;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#35745;&#31639;&#31934;&#31070;&#30149;&#23398;&#30340;&#35270;&#35282;&#36716;&#21521;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30528;&#30524;&#20110;Generative Pre-Trained Transformer 3.5&#65292;&#24182;&#23558;&#20854;&#32622;&#20110;&#31934;&#31070;&#30149;&#23398;&#20013;&#24120;&#35265;&#30340;&#20219;&#21153;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;GPT-3.5&#23545;&#24120;&#35265;&#30340;&#28966;&#34385;&#38382;&#21367;&#20570;&#20986;&#26377;&#21147;&#30340;&#21453;&#24212;&#65292;&#20135;&#29983;&#27604;&#20154;&#31867;&#20027;&#20307;&#26356;&#39640;&#30340;&#28966;&#34385;&#20998;&#25968;&#12290;&#27492;&#22806;&#65292;&#20351;&#29992;&#24773;&#32490;&#24863;&#24212;&#25552;&#31034;&#21487;&#20197;&#21487;&#39044;&#27979;&#22320;&#25913;&#21464;GPT-3.5&#30340;&#21453;&#24212;&#12290;&#24773;&#24863;&#24863;&#24212;&#19981;&#20165;&#24433;&#21709;GPT-3.5&#22312;&#34913;&#37327;&#25506;&#32034;&#20915;&#31574;-making&#30340;&#35748;&#30693;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#65292;&#36824;&#24433;&#21709;&#20854;&#22312;&#20043;&#21069;&#24314;&#31435;&#30340;&#34913;&#37327;&#31181;&#26063;&#20027;&#20041;&#21644;&#22833;&#33021;&#20027;&#20041;&#31561;&#20559;&#35265;&#30340;&#20219;&#21153;&#20013;&#30340;&#34892;&#20026;&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;GPT-3.5&#22312;&#21463;&#21040;&#28966;&#34385;&#35825;&#23548;&#26102;&#21576;&#29616;&#20986;&#26126;&#26174;&#30340;&#25506;&#32034;&#24615;&#21644;&#20559;&#35265;&#22686;&#21152;&#65292;&#34920;&#26126;&#20854;&#36755;&#20986;&#23481;&#26131;&#21463;&#21040;&#24773;&#24863;&#25805;&#32437;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#32467;&#26524;&#31361;&#26174;&#20102;&#22312;&#35821;&#35328;&#27169;&#22411;&#30340;&#24320;&#21457;&#21644;&#20351;&#29992;&#36807;&#31243;&#20013;&#38656;&#35201;&#26356;&#22810;&#30340;&#36947;&#24503;&#32771;&#34385;&#21644;&#30417;&#31649;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are transforming research on machine learning while galvanizing public debates. Understanding not only when these models work well and succeed but also why they fail and misbehave is of great societal relevance. We propose to turn the lens of computational psychiatry, a framework used to computationally describe and modify aberrant behavior, to the outputs produced by these models. We focus on the Generative Pre-Trained Transformer 3.5 and subject it to tasks commonly studied in psychiatry. Our results show that GPT-3.5 responds robustly to a common anxiety questionnaire, producing higher anxiety scores than human subjects. Moreover, GPT-3.5's responses can be predictably changed by using emotion-inducing prompts. Emotion-induction not only influences GPT-3.5's behavior in a cognitive task measuring exploratory decision-making but also influences its behavior in a previously-established task measuring biases such as racism and ableism. Crucially, GPT-3.5 shows a s
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ChatABL&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLM&#25972;&#21512;&#21040;&#24402;&#32435;&#23398;&#20064;ABL&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#23454;&#29616;&#20132;&#20114;&#24335;&#23398;&#20064;&#65292;&#26088;&#22312;&#20197;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#21644;&#26131;&#29702;&#35299;&#30340;&#26041;&#24335;&#32479;&#19968;&#24863;&#30693;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatABL&#21487;&#20197;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#22320;&#23398;&#20064;&#35299;&#20915;&#24402;&#32435;&#25512;&#29702;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.11107</link><description>&lt;p&gt;
ChatABL&#65306;&#36890;&#36807;&#19982;ChatGPT&#30340;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#23454;&#29616;&#24402;&#32435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ChatABL: Abductive Learning via Natural Language Interaction with ChatGPT. (arXiv:2304.11107v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11107
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;ChatABL&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLM&#25972;&#21512;&#21040;&#24402;&#32435;&#23398;&#20064;ABL&#26694;&#26550;&#20013;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#23545;&#35805;&#23454;&#29616;&#20132;&#20114;&#24335;&#23398;&#20064;&#65292;&#26088;&#22312;&#20197;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#21644;&#26131;&#29702;&#35299;&#30340;&#26041;&#24335;&#32479;&#19968;&#24863;&#30693;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;ChatABL&#21487;&#20197;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#22320;&#23398;&#20064;&#35299;&#20915;&#24402;&#32435;&#25512;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#25968;&#23398;&#33021;&#21147;&#26041;&#38754;&#30340;&#37325;&#22823;&#28508;&#21147;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#19982;&#20154;&#31867;&#33258;&#28982;&#35821;&#35328;&#19968;&#33268;&#30340;&#26377;&#20215;&#20540;&#30340;&#25512;&#29702;&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#20043;&#38388;&#24213;&#23618;&#20449;&#24687;&#27969;&#30340;&#19981;&#20860;&#23481;&#24615;&#65292;LLM&#30446;&#21069;&#38590;&#20197;&#22312;&#24863;&#30693;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#65292;&#20351;&#24471;&#23454;&#29616;&#33258;&#20027;&#20219;&#21153;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29992;&#20110;&#23558;&#24863;&#30693;&#21644;&#25512;&#29702;&#20004;&#31181;&#33021;&#21147;&#25972;&#21512;&#30340;&#24402;&#32435;&#23398;&#20064;(ABL)&#26694;&#26550;&#22312;&#19981;&#23436;&#25972;&#20107;&#23454;&#30340;&#36870;&#21521;&#35299;&#23494;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#21463;&#21040;&#36923;&#36753;&#25512;&#29702;&#35268;&#21017;&#30340;&#35821;&#20041;&#29702;&#35299;&#19981;&#36275;&#20197;&#21450;&#20381;&#36182;&#20110;&#22797;&#26434;&#30340;&#39046;&#22495;&#30693;&#35782;&#34920;&#31034;&#30340;&#38480;&#21046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;(ChatABL)&#65292;&#23558;LLM&#25972;&#21512;&#21040;ABL&#26694;&#26550;&#20013;&#65292;&#26088;&#22312;&#20197;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#21644;&#26131;&#29702;&#35299;&#30340;&#26041;&#24335;&#32479;&#19968;&#36825;&#19977;&#31181;&#33021;&#21147;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21033;&#29992;LLM&#30340;&#20248;&#21183;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#27969;&#36827;&#34892;&#20132;&#20114;&#24335;&#23398;&#20064;&#65292;&#23454;&#29616;&#24402;&#32435;&#25512;&#29702;&#12290;ChatABL&#26694;&#26550;&#26088;&#22312;&#20351;LLM&#22312;&#24863;&#30693;&#21644;&#25512;&#29702;&#33021;&#21147;&#20043;&#38388;&#24314;&#31435;&#26725;&#26753;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#21644;&#31526;&#21495;&#25512;&#29702;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#29616;&#26377;&#30340;ABL&#27169;&#22411;&#30456;&#27604;&#65292;ChatABL&#21487;&#20197;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#22320;&#23398;&#20064;&#35299;&#20915;&#24402;&#32435;&#25512;&#29702;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as ChatGPT have recently demonstrated significant potential in mathematical abilities, providing valuable reasoning paradigm consistent with human natural language. However, LLMs currently have difficulty in bridging perception, language understanding and reasoning capabilities due to incompatibility of the underlying information flow among them, making it challenging to accomplish tasks autonomously. On the other hand, abductive learning (ABL) frameworks for integrating the two abilities of perception and reasoning has seen significant success in inverse decipherment of incomplete facts, but it is limited by the lack of semantic understanding of logical reasoning rules and the dependence on complicated domain knowledge representation. This paper presents a novel method (ChatABL) for integrating LLMs into the ABL framework, aiming at unifying the three abilities in a more user-friendly and understandable manner. The proposed method uses the strengths o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23631;&#34109;&#27010;&#24565;&#30340;&#23433;&#20840;&#25506;&#32034;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#39564;&#35777;&#31574;&#30053;&#22238;&#28378;&#65292;&#19981;&#20381;&#36182;&#20110;&#23433;&#20840;&#30456;&#20851;&#25277;&#35937;&#25110;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#12290;&#22312;&#23569;&#37327;Atari&#28216;&#25103;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#26377;&#25928;&#24615;&#39640;&#65292;&#20943;&#23569;&#20102;&#23433;&#20840;&#36829;&#35268;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.11104</link><description>&lt;p&gt;
Atari&#26234;&#33021;&#20307;&#30340;&#36817;&#20284;&#23631;&#34109;&#20351;&#24471;&#23433;&#20840;&#25506;&#32034;&#25104;&#20026;&#21487;&#33021;
&lt;/p&gt;
&lt;p&gt;
Approximate Shielding of Atari Agents for Safe Exploration. (arXiv:2304.11104v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23631;&#34109;&#27010;&#24565;&#30340;&#23433;&#20840;&#25506;&#32034;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#19990;&#30028;&#27169;&#22411;&#39564;&#35777;&#31574;&#30053;&#22238;&#28378;&#65292;&#19981;&#20381;&#36182;&#20110;&#23433;&#20840;&#30456;&#20851;&#25277;&#35937;&#25110;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#12290;&#22312;&#23569;&#37327;Atari&#28216;&#25103;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#26377;&#25928;&#24615;&#39640;&#65292;&#20943;&#23569;&#20102;&#23433;&#20840;&#36829;&#35268;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21463;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#24179;&#34913;&#25506;&#32034;&#19982;&#20445;&#23432;&#24615;&#26159;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#23436;&#25104;&#26377;&#24847;&#20041;&#20219;&#21153;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23631;&#34109;&#27010;&#24565;&#30340;&#23433;&#20840;&#25506;&#32034;&#30340;&#21407;&#29702;&#24615;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#22522;&#20110;&#28508;&#22312;&#23631;&#34109;&#36825;&#19968;&#24819;&#27861;&#65292;&#21033;&#29992;&#19990;&#30028;&#27169;&#22411;&#22312;&#23398;&#20064;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#39564;&#35777;&#31574;&#30053;&#22238;&#28378;&#12290;&#19982;&#20043;&#21069;&#30340;&#23631;&#34109;&#26041;&#27861;&#19981;&#21516;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20381;&#36182;&#20110;&#23433;&#20840;&#30456;&#20851;&#25277;&#35937;&#25110;&#39640;&#20445;&#30495;&#24230;&#27169;&#25311;&#22120;&#12290;&#25105;&#20204;&#30340;&#26032;&#31639;&#27861;&#22312;&#27492;&#22522;&#30784;&#19978;&#20351;&#29992;&#23433;&#20840;&#25209;&#21028;&#21644;&#20854;&#20182;&#38468;&#21152;&#29305;&#24615;&#26469;&#25552;&#39640;&#31639;&#27861;&#30340;&#31283;&#23450;&#24615;&#21644;&#36828;&#35265;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#23567;&#32452;&#21253;&#21547;&#29366;&#24577;&#30456;&#20851;&#23433;&#20840;&#26631;&#31614;&#30340;Atari&#28216;&#25103;&#19978;&#36816;&#34892;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#36817;&#20284;&#23631;&#34109;&#31639;&#27861;&#26377;&#25928;&#38477;&#20302;&#20102;&#23433;&#20840;&#36829;&#35268;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Balancing exploration and conservatism in the constrained setting is an important problem if we are to use reinforcement learning for meaningful tasks in the real world. In this paper, we propose a principled algorithm for safe exploration based on the concept of shielding. Previous approaches to shielding assume access to a safety-relevant abstraction of the environment or a high-fidelity simulator. Instead, our work is based on latent shielding another approach that leverages world models to verify policy roll-outs in the latent space of a learned dynamics model. Our novel algorithm builds on this previous work, using safety critics and other additional features to improve the stability and farsightedness of the algorithm. We demonstrate the effectiveness of our approach by running experiments on a small set of Atari games with state dependent safety labels. We present preliminary results that show our approximate shielding algorithm effectively reduces the rate of safety violation
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19981;&#38656;&#35201;&#35757;&#32451;&#65292;&#22522;&#20110;&#31616;&#21333;&#26144;&#23556;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32534;&#30721;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#35821;&#20041;&#19978;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#26144;&#23556;&#21040;&#21516;&#19968;&#31354;&#38388;&#65292;&#24182;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2304.11095</link><description>&lt;p&gt;
&#19981;&#38656;&#35201;&#35757;&#32451;&#65292;&#36328;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#26159;&#21542;&#21487;&#34892;&#65311;
&lt;/p&gt;
&lt;p&gt;
Is Cross-modal Information Retrieval Possible without Training?. (arXiv:2304.11095v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11095
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#19981;&#38656;&#35201;&#35757;&#32451;&#65292;&#22522;&#20110;&#31616;&#21333;&#26144;&#23556;&#30340;&#36328;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#32534;&#30721;&#34920;&#31034;&#12290;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#22312;&#35821;&#20041;&#19978;&#23558;&#19981;&#21516;&#27169;&#24577;&#30340;&#25968;&#25454;&#26144;&#23556;&#21040;&#21516;&#19968;&#31354;&#38388;&#65292;&#24182;&#22312;&#25991;&#26412;&#21644;&#22270;&#20687;&#20043;&#38388;&#36798;&#21040;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#32534;&#30721;&#30340;&#34920;&#31034;(&#20363;&#22914;BERT&#25991;&#26412;&#23884;&#20837;&#65292;&#22270;&#20687;&#30340;&#20498;&#25968;&#31532;&#20108;&#20010;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23618;&#28608;&#27963;)&#20256;&#36882;&#20102;&#19968;&#32452;&#26377;&#30410;&#30340;&#20449;&#24687;&#26816;&#32034;&#29305;&#24449;&#12290;&#32473;&#23450;&#25968;&#25454;&#27169;&#24577;&#30340;&#23884;&#20837;&#23384;&#22312;&#33258;&#24049;&#30340;&#39640;&#32500;&#31354;&#38388;&#20013;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#30340;&#26144;&#23556;&#36827;&#34892;&#35821;&#20041;&#23545;&#40784;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26469;&#33258;&#26368;&#23567;&#20108;&#20056;&#27861;&#21644;&#22855;&#24322;&#20540;&#20998;&#35299; (SVD) &#30340;&#31616;&#21333;&#26144;&#23556;&#20316;&#20026;Procrustes&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#23454;&#29616;&#36328;&#27169;&#24577;&#20449;&#24687;&#26816;&#32034;&#30340;&#25163;&#27573;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#32473;&#23450;&#19968;&#20010;&#27169;&#24577;&#20013;&#30340;&#20449;&#24687;&#65292;&#20363;&#22914;&#25991;&#26412;&#65292;&#35813;&#26144;&#23556;&#21487;&#20197;&#24110;&#21161;&#25105;&#20204;&#22312;&#21478;&#19968;&#20010;&#27169;&#24577;&#20013;&#25214;&#21040;&#19982;&#20854;&#35821;&#20041;&#30456;&#24403;&#30340;&#25968;&#25454;&#39033;&#65292;&#20363;&#22914;&#22270;&#20687;&#12290;&#20351;&#29992;&#29616;&#25104;&#30340;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#25105;&#20204;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#22270;&#20687;&#21040;&#25991;&#26412;&#30340;&#26816;&#32034;&#20219;&#21153;&#20013;&#23581;&#35797;&#20102;&#19978;&#36848;&#31616;&#21333;&#30340;&#36328;&#27169;&#24577;&#26144;&#23556;&#12290;&#23613;&#31649;&#31616;&#21333;&#65292;&#25105;&#20204;&#30340;&#26144;&#23556;&#34920;&#29616;&#20986;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Encoded representations from a pretrained deep learning model (e.g., BERT text embeddings, penultimate CNN layer activations of an image) convey a rich set of features beneficial for information retrieval. Embeddings for a particular modality of data occupy a high-dimensional space of its own, but it can be semantically aligned to another by a simple mapping without training a deep neural net. In this paper, we take a simple mapping computed from the least squares and singular value decomposition (SVD) for a solution to the Procrustes problem to serve a means to cross-modal information retrieval. That is, given information in one modality such as text, the mapping helps us locate a semantically equivalent data item in another modality such as image. Using off-the-shelf pretrained deep learning models, we have experimented the aforementioned simple cross-modal mappings in tasks of text-to-image and image-to-text retrieval. Despite simplicity, our mappings perform reasonably well reachin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#26412;&#22303;&#30340;&#35270;&#35282;&#25506;&#35752;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#21628;&#21505;&#22312;&#31639;&#27861;&#20013;&#32435;&#20837;&#26412;&#22320;&#30693;&#35782;&#21644;&#29702;&#35299;&#20197;&#30830;&#20445;&#20844;&#27491;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#31038;&#20250;&#26102;&#12290;</title><link>http://arxiv.org/abs/2304.11094</link><description>&lt;p&gt;
&#21435;&#20559;&#35265;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65306;&#19968;&#20010;&#26412;&#22303;&#30340;&#23450;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Effectiveness of Debiasing Techniques: An Indigenous Qualitative Analysis. (arXiv:2304.11094v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11094
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#26412;&#22303;&#30340;&#35270;&#35282;&#25506;&#35752;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21435;&#20559;&#35265;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65292;&#21628;&#21505;&#22312;&#31639;&#27861;&#20013;&#32435;&#20837;&#26412;&#22320;&#30693;&#35782;&#21644;&#29702;&#35299;&#20197;&#30830;&#20445;&#20844;&#27491;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#31038;&#20250;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#26412;&#22303;&#30340;&#35270;&#35282;&#65292;&#25506;&#35752;&#20102;&#38024;&#23545;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#21435;&#20559;&#35265;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;&#30446;&#21069;&#34913;&#37327;&#19982;&#21435;&#20559;&#35265;PLMs&#20351;&#29992;&#30340;&#25216;&#26415;&#23384;&#22312;&#32654;&#22269;&#31181;&#26063;&#20559;&#35265;&#30340;&#20542;&#21521;&#65292;&#24182;&#19988;&#20381;&#36182;&#20110;&#39044;&#23450;&#20041;&#30340;&#20559;&#35265;&#23646;&#24615;&#65288;&#20363;&#22914;&#8220;&#40657;&#20154;&#8221;&#19982;&#8220;&#30333;&#20154;&#8221;&#65289;&#12290;&#26377;&#20123;&#25216;&#26415;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38598;&#21644;&#36827;&#19968;&#27493;&#30340;&#39044;&#35757;&#32451;&#12290;&#36825;&#26679;&#30340;&#25216;&#26415;&#24182;&#19981;&#33021;&#25429;&#25417;&#20854;&#20182;&#22269;&#23478;&#20013;&#34987;&#36739;&#23569;&#20195;&#34920;&#30340;&#22303;&#33879;&#20154;&#21475;&#65292;&#20363;&#22914;&#26032;&#35199;&#20848;&#30340;&#27611;&#21033;&#20154;&#12290;&#24517;&#39035;&#32435;&#20837;&#26412;&#22320;&#30340;&#30693;&#35782;&#21644;&#29702;&#35299;&#65292;&#20197;&#30830;&#20445;&#20844;&#27491;&#30340;&#31639;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#38754;&#23545;&#36164;&#28304;&#21463;&#38480;&#30340;&#31038;&#20250;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
An indigenous perspective on the effectiveness of debiasing techniques for pre-trained language models (PLMs) is presented in this paper. The current techniques used to measure and debias PLMs are skewed towards the US racial biases and rely on pre-defined bias attributes (e.g. "black" vs "white"). Some require large datasets and further pre-training. Such techniques are not designed to capture the underrepresented indigenous populations in other countries, such as M\=aori in New Zealand. Local knowledge and understanding must be incorporated to ensure unbiased algorithms, especially when addressing a resource-restricted society.
&lt;/p&gt;</description></item><item><title>&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#36890;&#36807;DPCC&#21019;&#36896;&#20986;&#33021;&#22815;&#19982;&#29992;&#25143;&#36827;&#34892;&#35270;&#21548;&#20132;&#20114;&#30340;&#28145;&#24230;&#20010;&#24615;&#21270;&#25968;&#23383;&#35282;&#33394;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36817;10k&#20010;&#35805;&#35821;&#21644;6&#20010;&#23567;&#26102;&#38899;&#39057;/&#35270;&#39057;&#30340;&#35282;&#33394;&#20013;&#24515;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2304.11093</link><description>&lt;p&gt;
Hi Sheldon! &#20174;&#30005;&#35270;&#21095;&#20013;&#21019;&#24314;&#28145;&#24230;&#20010;&#24615;&#21270;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hi Sheldon! Creating Deep Personalized Characters from TV Shows. (arXiv:2304.11093v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11093
&lt;/p&gt;
&lt;p&gt;
&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#36890;&#36807;DPCC&#21019;&#36896;&#20986;&#33021;&#22815;&#19982;&#29992;&#25143;&#36827;&#34892;&#35270;&#21548;&#20132;&#20114;&#30340;&#28145;&#24230;&#20010;&#24615;&#21270;&#25968;&#23383;&#35282;&#33394;&#65292;&#24182;&#25910;&#38598;&#20102;&#19968;&#20010;&#21253;&#21547;&#36817;10k&#20010;&#35805;&#35821;&#21644;6&#20010;&#23567;&#26102;&#38899;&#39057;/&#35270;&#39057;&#30340;&#35282;&#33394;&#20013;&#24515;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24819;&#35937;&#19968;&#19979;&#65292;&#20320;&#21487;&#20197;&#19982;&#19968;&#20010;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25968;&#23383;&#35282;&#33394;&#36827;&#34892;&#35270;&#21548;&#20132;&#20114;&#65292;&#20854;&#22806;&#35980;&#21644;&#20010;&#24615;&#19982;&#12298;&#29983;&#27963;&#22823;&#29190;&#28856;&#12299;&#20013;&#30340;Sheldon&#20960;&#20046;&#19968;&#27169;&#19968;&#26679;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#31070;&#22855;&#30340;&#35270;&#21548;&#20132;&#20114;&#22330;&#26223;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;"Deep Personalized Character Creation&#65288;DPCC&#65289;"&#30340;&#21019;&#26032;&#20219;&#21153;&#65306;&#20174;&#30005;&#35270;&#21095;&#31561;&#22810;&#27169;&#24577;&#25968;&#25454;&#20013;&#21019;&#36896;&#20986;&#20010;&#24615;&#21270;&#35282;&#33394;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#21333;&#19968;&#25110;&#22810;&#20010;&#27169;&#24335;&#30340;&#25991;&#26412;&#12289;&#38899;&#39057;&#25110;&#35270;&#39057;&#36755;&#20837;&#65292;DPCC&#26088;&#22312;&#29983;&#25104;&#19982;&#26576;&#20010;&#29305;&#23450;&#35282;&#33394;&#65288;&#22914;Sheldon&#65289;&#30340;&#20010;&#24615;&#29305;&#28857;&#38750;&#24120;&#21305;&#37197;&#19988;&#36136;&#37327;&#39640;&#30340;&#22810;&#27169;&#24577;&#65288;&#25991;&#26412;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#65289;&#21709;&#24212;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#21019;&#26032;&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25910;&#38598;&#20102;&#19968;&#20010;&#21517;&#20026;"Deep Personalized Character Dataset&#65288;DPCD&#65289;"&#30340;&#35282;&#33394;&#20013;&#24515;&#22810;&#27169;&#24577;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;~10k&#20010;&#35805;&#35821;&#21644;~6&#20010;&#23567;&#26102;&#30340;&#38899;&#39057;/&#35270;&#39057;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imagine an interesting multimodal interactive scenario that you can see, hear, and chat with an AI-generated digital character, who is capable of behaving like Sheldon from The Big Bang Theory, as a DEEP copy from appearance to personality. Towards this fantastic multimodal chatting scenario, we propose a novel task, named Deep Personalized Character Creation (DPCC): creating multimodal chat personalized characters from multimodal data such as TV shows. Specifically, given a single- or multi-modality input (text, audio, video), the goal of DPCC is to generate a multi-modality (text, audio, video) response, which should be well-matched the personality of a specific character such as Sheldon, and of high quality as well. To support this novel task, we further collect a character centric multimodal dialogue dataset, named Deep Personalized Character Dataset (DPCD), from TV shows. DPCD contains character-specific multimodal dialogue data of ~10k utterances and ~6 hours of audio/video per c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.11090</link><description>&lt;p&gt;
&#22312;ChatGPT&#26102;&#20195;&#36808;&#21521;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#30340;&#21442;&#32771;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Towards Responsible AI in the Era of ChatGPT: A Reference Architecture for Designing Foundation Model-based AI Systems. (arXiv:2304.11090v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#65292;&#37325;&#28857;&#20851;&#27880;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#31561;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#12289;Bard&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#25512;&#20986;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24341;&#36215;&#20102;&#24040;&#22823;&#20851;&#27880;&#12290;&#22522;&#30784;&#27169;&#22411;&#23558;&#25104;&#20026;&#26410;&#26469;&#22823;&#22810;&#25968;AI&#31995;&#32479;&#30340;&#22522;&#30784;&#26500;&#24314;&#22359;&#30340;&#36235;&#21183;&#27491;&#22312;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;&#23558;&#22522;&#30784;&#27169;&#22411;&#32435;&#20837;AI&#31995;&#32479;&#24341;&#21457;&#20102;&#23545;&#36127;&#36131;&#20219;AI&#30340;&#37325;&#22823;&#20851;&#27880;&#65292;&#36825;&#26159;&#30001;&#20110;&#20854;&#40657;&#21283;&#23376;&#24615;&#36136;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;&#36229;&#32423;&#26234;&#33021;&#24341;&#36215;&#30340;&#12290;&#27492;&#22806;&#65292;&#22522;&#30784;&#27169;&#22411;&#30340;&#22686;&#38271;&#33021;&#21147;&#26368;&#32456;&#21487;&#33021;&#20250;&#21534;&#22124;AI&#31995;&#32479;&#30340;&#20854;&#20182;&#32452;&#20214;&#65292;&#24341;&#20837;&#26550;&#26500;&#35774;&#35745;&#20013;&#30340;&#36816;&#21160;&#36793;&#30028;&#21644;&#25509;&#21475;&#28436;&#21464;&#25361;&#25112;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#27169;&#24335;&#20026;&#23548;&#21521;&#30340;&#36127;&#36131;&#20219;AI-by-design&#21442;&#32771;&#26550;&#26500;&#65292;&#29992;&#20110;&#35774;&#35745;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#12290;&#29305;&#21035;&#22320;&#65292;&#26412;&#25991;&#39318;&#20808;&#21576;&#29616;&#20102;&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#22312;&#26550;&#26500;&#28436;&#36827;&#26041;&#38754;&#30340;&#21457;&#23637;&#65292;&#20174;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#36830;&#25509;&#22120;"&#21040;"&#22522;&#30784;&#27169;&#22411;&#20316;&#20026;&#21333;&#29255;&#26426;&#26680;"&#12290;&#28982;&#21518;&#65292;&#23427;&#25552;&#20986;&#20102;&#19968;&#20010;&#21442;&#32771;&#26550;&#26500;&#65292;&#21253;&#25324;&#20116;&#20010;&#31867;&#21035;&#30340;&#27169;&#24335;&#65292;&#37325;&#28857;&#20851;&#27880;&#20851;&#38190;&#35774;&#35745;&#20803;&#32032;&#65292;&#20363;&#22914;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#21442;&#32771;&#26550;&#26500;&#20026;&#35774;&#35745;&#36127;&#36131;&#20219;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#20102;&#31995;&#32479;&#21270;&#21644;&#36879;&#26126;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The release of ChatGPT, Bard, and other large language model (LLM)-based chatbots has drawn huge attention on foundations models worldwide. There is a growing trend that foundation models will serve as the fundamental building blocks for most of the future AI systems. However, incorporating foundation models in AI systems raises significant concerns about responsible AI due to their black box nature and rapidly advancing super-intelligence. Additionally, the foundation model's growing capabilities can eventually absorb the other components of AI systems, introducing the moving boundary and interface evolution challenges in architecture design. To address these challenges, this paper proposes a pattern-oriented responsible-AI-by-design reference architecture for designing foundation model-based AI systems. Specially, the paper first presents an architecture evolution of AI systems in the era of foundation models, from "foundation-model-as-a-connector" to "foundation-model-as-a-monolithi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21033;&#29992;&#26032;&#38395;&#26631;&#39064;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#24773;&#24863;&#29305;&#24449;&#26469;&#23545;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#12290;</title><link>http://arxiv.org/abs/2304.11088</link><description>&lt;p&gt;
&#20351;&#29992;&#26032;&#38395;&#26631;&#39064;&#26469;&#20998;&#26512;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;
&lt;/p&gt;
&lt;p&gt;
Profiling the news spreading barriers using news headlines. (arXiv:2304.11088v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#26032;&#38395;&#26631;&#39064;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#24773;&#24863;&#29305;&#24449;&#26469;&#23545;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#36827;&#34892;&#20998;&#31867;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#26631;&#39064;&#21487;&#20197;&#26159;&#26816;&#27979;&#26032;&#38395;&#23186;&#20307;&#20013;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#30340;&#22909;&#25968;&#25454;&#28304;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#26412;&#25991;&#21033;&#29992;&#22522;&#20110;&#25512;&#29702;&#30340;&#27169;&#22411;COMET&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#26032;&#38395;&#26631;&#39064;&#30340;&#24773;&#24863;&#29305;&#24449;&#26469;&#23545;&#38556;&#30861;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#25991;&#21270;&#12289;&#32463;&#27982;&#12289;&#25919;&#27835;&#12289;&#35821;&#35328;&#21644;&#22320;&#29702;&#31561;&#20116;&#31181;&#38556;&#30861;&#65292;&#20197;&#21450;&#21253;&#25324;&#20581;&#24247;&#12289;&#36816;&#21160;&#12289;&#31185;&#23398;&#12289;&#23089;&#20048;&#12289;&#28216;&#25103;&#12289;&#20303;&#25151;&#12289;&#31038;&#20250;&#12289;&#36141;&#29289;&#12289;&#35745;&#31639;&#26426;&#21644;&#21830;&#19994;&#31561;&#19981;&#21516;&#31867;&#22411;&#30340;&#26032;&#38395;&#26631;&#39064;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#26032;&#38395;&#20986;&#29256;&#21830;&#30340;&#20803;&#25968;&#25454;&#33258;&#21160;&#25910;&#38598;&#21644;&#26631;&#35760;&#26032;&#38395;&#26631;&#39064;&#65292;&#20197;&#27492;&#26469;&#26816;&#27979;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20256;&#32479;&#30340;&#25991;&#26412;&#20998;&#31867;&#26041;&#27861;&#12289;&#28145;&#24230;&#23398;&#20064;&#21644;&#22522;&#20110;transformer&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21033;&#29992;&#25512;&#29702;&#20026;&#22522;&#30784;&#30340;&#35821;&#20041;&#30693;&#35782;&#21644;&#24773;&#24863;&#29305;&#24449;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#26816;&#27979;&#26032;&#38395;&#20256;&#25773;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
News headlines can be a good data source for detecting the news spreading barriers in news media, which may be useful in many real-world applications. In this paper, we utilize semantic knowledge through the inference-based model COMET and sentiments of news headlines for barrier classification. We consider five barriers including cultural, economic, political, linguistic, and geographical, and different types of news headlines including health, sports, science, recreation, games, homes, society, shopping, computers, and business. To that end, we collect and label the news headlines automatically for the barriers using the metadata of news publishers. Then, we utilize the extracted commonsense inferences and sentiments as features to detect the news spreading barriers. We compare our approach to the classical text classification methods, deep learning, and transformer-based methods. The results show that the proposed approach using inferences-based semantic knowledge and sentiment offe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#20026;AI&#36719;&#20214;&#20135;&#21697;&#30340;&#24320;&#21457;&#20154;&#21592;&#12289;&#35774;&#35745;&#24072;&#12289;&#31649;&#29702;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#20171;&#32461;AI&#20135;&#21697;&#23433;&#20840;&#30340;&#22522;&#30784;&#65292;&#35753;&#20182;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#20135;&#21697;&#30340;&#23041;&#32961;&#24182;&#36991;&#20813;&#24120;&#35265;&#38519;&#38449;&#12290;</title><link>http://arxiv.org/abs/2304.11087</link><description>&lt;p&gt;
AI&#20135;&#21697;&#23433;&#20840;&#65306;&#24320;&#21457;&#32773;&#20837;&#38376;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
AI Product Security: A Primer for Developers. (arXiv:2304.11087v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11087
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20026;AI&#36719;&#20214;&#20135;&#21697;&#30340;&#24320;&#21457;&#20154;&#21592;&#12289;&#35774;&#35745;&#24072;&#12289;&#31649;&#29702;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#20171;&#32461;AI&#20135;&#21697;&#23433;&#20840;&#30340;&#22522;&#30784;&#65292;&#35753;&#20182;&#20204;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#20135;&#21697;&#30340;&#23041;&#32961;&#24182;&#36991;&#20813;&#24120;&#35265;&#38519;&#38449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#20037;&#20197;&#21069;&#65292;AI&#23433;&#20840;&#23601;&#24847;&#21619;&#30528;&#22914;&#20309;&#21033;&#29992;AI&#22686;&#24378;&#32593;&#32476;&#23433;&#20840;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#65292;&#21363;&#23433;&#20840;&#30340;AI&#12290;&#33258;&#20174;Ian Goodfellow&#21450;&#20854;&#22242;&#38431;&#24191;&#27867;&#24212;&#29992;&#23545;&#25239;&#24615;&#25915;&#20987;&#20110;&#26426;&#22120;&#23398;&#20064;&#20043;&#21518;&#65292;AI&#23433;&#20840;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#20063;&#26159;AI&#23433;&#20840;&#30340;&#19968;&#37096;&#20998;&#12290;&#20102;&#35299;&#26426;&#22120;&#23398;&#20064;&#20135;&#21697;&#30340;&#23041;&#32961;&#24182;&#36991;&#20813;AI&#20135;&#21697;&#24320;&#21457;&#20013;&#30340;&#24120;&#35265;&#38519;&#38449;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#38754;&#21521;AI&#36719;&#20214;&#20135;&#21697;&#30340;&#24320;&#21457;&#20154;&#21592;&#12289;&#35774;&#35745;&#24072;&#12289;&#31649;&#29702;&#32773;&#21644;&#30740;&#31350;&#20154;&#21592;&#12290;
&lt;/p&gt;
&lt;p&gt;
Not too long ago, AI security used to mean the research and practice of how AI can empower cybersecurity, that is, AI for security. Ever since Ian Goodfellow and his team popularized adversarial attacks on machine learning, security for AI became an important concern and also part of AI security. It is imperative to understand the threats to machine learning products and avoid common pitfalls in AI product development. This article is addressed to developers, designers, managers and researchers of AI software products.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ICICLE&#26234;&#33021;&#29615;&#22659;&#35745;&#31639;&#23398;&#20064;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25152;&#24320;&#21457;&#30340;Jupyter Notebooks&#21644;Python&#21629;&#20196;&#34892;&#23458;&#25143;&#31471;&#65292;&#20351;&#29992;ICICLE&#36523;&#20221;&#39564;&#35777;&#26426;&#21046;&#35775;&#38382;ICICLE&#36164;&#28304;&#21644;&#26381;&#21153;&#12290;&#20351;&#29992;&#20102;Tapis&#26694;&#26550;&#21644;Neo4j&#23558;&#25968;&#25454;&#32452;&#32455;&#25104;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#65292;&#24182;&#28436;&#31034;&#20102;&#20351;&#29992;&#35813;&#36719;&#20214;&#30340;&#20960;&#20010;&#23458;&#25143;&#31471;&#65292;&#39564;&#35777;&#20102;&#20351;&#29992;Tapis&#21644;Neo4j&#24320;&#21457;&#35748;&#35777;&#23458;&#25143;&#31471;&#35775;&#38382;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#26381;&#21153;&#30340;&#21487;&#34892;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11086</link><description>&lt;p&gt;
ICICLE CI&#26381;&#21153;&#35748;&#35777;&#30340;&#23458;&#25143;&#31471;&#21644;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;--REHS&#35745;&#21010;&#26368;&#32456;&#25253;&#21578;&#65288;2022&#24180;6&#26376;-8&#26376;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Development of Authenticated Clients and Applications for ICICLE CI Services -- Final Report for the REHS Program, June-August, 2022. (arXiv:2304.11086v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11086
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;ICICLE&#26234;&#33021;&#29615;&#22659;&#35745;&#31639;&#23398;&#20064;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25152;&#24320;&#21457;&#30340;Jupyter Notebooks&#21644;Python&#21629;&#20196;&#34892;&#23458;&#25143;&#31471;&#65292;&#20351;&#29992;ICICLE&#36523;&#20221;&#39564;&#35777;&#26426;&#21046;&#35775;&#38382;ICICLE&#36164;&#28304;&#21644;&#26381;&#21153;&#12290;&#20351;&#29992;&#20102;Tapis&#26694;&#26550;&#21644;Neo4j&#23558;&#25968;&#25454;&#32452;&#32455;&#25104;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#65292;&#24182;&#28436;&#31034;&#20102;&#20351;&#29992;&#35813;&#36719;&#20214;&#30340;&#20960;&#20010;&#23458;&#25143;&#31471;&#65292;&#39564;&#35777;&#20102;&#20351;&#29992;Tapis&#21644;Neo4j&#24320;&#21457;&#35748;&#35777;&#23458;&#25143;&#31471;&#35775;&#38382;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#26381;&#21153;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ICICLE&#26234;&#33021;&#29615;&#22659;&#35745;&#31639;&#23398;&#20064;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#25152;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#24314;&#31435;&#19979;&#19968;&#20195;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#65292;&#20351;&#20154;&#24037;&#26234;&#33021;&#26356;&#20855;&#26222;&#36866;&#24615;&#24182;&#25512;&#21160;&#20854;&#22312;&#26356;&#22823;&#33539;&#22260;&#20869;&#30340;&#27665;&#20027;&#21270;&#12290;&#26412;&#30740;&#31350;&#25551;&#36848;&#20102;&#25105;&#20204;&#24320;&#21457;&#30340;Jupyter Notebooks&#21644;Python&#21629;&#20196;&#34892;&#23458;&#25143;&#31471;&#65292;&#36825;&#20123;&#23458;&#25143;&#31471;&#20351;&#29992;ICICLE&#36523;&#20221;&#39564;&#35777;&#26426;&#21046;&#35775;&#38382;ICICLE&#36164;&#28304;&#21644;&#26381;&#21153;&#12290;&#25105;&#20204;&#20351;&#29992;&#25903;&#25345;&#31185;&#23398;&#23478;&#35775;&#38382;&#12289;&#21033;&#29992;&#21644;&#31649;&#29702;&#22810;&#38498;&#26657;&#36164;&#28304;&#21644;&#26381;&#21153;&#30340;Tapis&#26694;&#26550;&#26469;&#36830;&#25509;&#23458;&#25143;&#31471;&#12290;&#25105;&#20204;&#20351;&#29992;Neo4j&#23558;&#25968;&#25454;&#32452;&#32455;&#25104;&#19968;&#20010;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;KG&#25176;&#31649;&#22312;Tapis Pod&#19978;&#65292;Tapis Pod&#25552;&#20379;&#19968;&#20010;&#29305;&#21035;&#20026;Neo4j KG&#32780;&#21046;&#20316;&#30340;&#27169;&#26495;&#36827;&#34892;&#25345;&#20037;&#25968;&#25454;&#23384;&#20648;&#12290;&#20026;&#20102;&#28436;&#31034;&#25105;&#20204;&#36719;&#20214;&#30340;&#21151;&#33021;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20960;&#20010;&#23458;&#25143;&#31471;&#65306;Jupyter&#31508;&#35760;&#26412;&#36523;&#20221;&#39564;&#35777;&#12289;&#20351;&#29992;ICICLE&#36164;&#28304;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#65292;&#20197;&#21450;&#29992;&#20110;&#35775;&#38382;ICICLE&#26381;&#21153;&#30340;&#21629;&#20196;&#34892;&#30028;&#38754;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#35777;&#26126;&#20102;&#20351;&#29992;Tapis&#21644;Neo4j&#24320;&#21457;&#35775;&#38382;&#32593;&#32476;&#22522;&#30784;&#35774;&#26045;&#26381;&#21153;&#30340;&#35748;&#35777;&#23458;&#25143;&#31471;&#30340;&#21487;&#34892;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Artificial Intelligence (AI) institute for Intelligent Cyberinfrastructure with Computational Learning in the Environment (ICICLE) is funded by the NSF to build the next generation of Cyberinfrastructure to render AI more accessible to everyone and drive its further democratization in the larger society. We describe our efforts to develop Jupyter Notebooks and Python command line clients that would access these ICICLE resources and services using ICICLE authentication mechanisms. To connect our clients, we used Tapis, which is a framework that supports computational research to enable scientists to access, utilize, and manage multi-institution resources and services. We used Neo4j to organize data into a knowledge graph (KG). We then hosted the KG on a Tapis Pod, which offers persistent data storage with a template made specifically for Neo4j KGs. In order to demonstrate the capabilities of our software, we developed several clients: Jupyter notebooks authentication, Neural Network
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.11082</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;
&lt;/p&gt;
&lt;p&gt;
Fundamental Limitations of Alignment in Large Language Models. (arXiv:2304.11082v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#29702;&#35770;&#26041;&#27861;&#8212;&#8212;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;&#65288;BEB&#65289;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23545;&#40784;&#30340;&#22522;&#26412;&#38480;&#21046;&#65292;&#24182;&#35777;&#26126;&#20219;&#20309;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#26681;&#38500;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#65292;&#36825;&#23545;&#20110;&#38450;&#27490;&#24694;&#24847;&#25915;&#20987;&#26159;&#19981;&#23433;&#20840;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#19982;&#20154;&#20132;&#20114;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#35201;&#26041;&#38754;&#26159;&#23545;&#40784;&#20854;&#34892;&#20026;&#65292;&#20351;&#20854;&#23545;&#20854;&#20154;&#31867;&#29992;&#25143;&#26377;&#29992;&#19988;&#26080;&#23475;&#12290;&#36825;&#36890;&#24120;&#36890;&#36807;&#35843;&#25972;&#27169;&#22411;&#30340;&#26041;&#24335;&#26469;&#23454;&#29616;&#65292;&#20197;&#22686;&#24378;&#25152;&#38656;&#30340;&#34892;&#20026;&#24182;&#25233;&#21046;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#34892;&#20026;&#26399;&#26395;&#36793;&#30028;(BEB)&#30340;&#29702;&#35770;&#26041;&#27861;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#27491;&#24335;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#20960;&#20010;&#20869;&#22312;&#29305;&#24449;&#21644;&#23545;&#40784;&#30340;&#38480;&#21046;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35777;&#26126;&#23545;&#20110;&#20219;&#20309;&#20855;&#26377;&#34987;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#30340;&#26377;&#38480;&#27010;&#29575;&#30340;&#34892;&#20026;&#65292;&#37117;&#23384;&#22312;&#21487;&#20197;&#35302;&#21457;&#27169;&#22411;&#36755;&#20986;&#27492;&#34892;&#20026;&#30340;&#25552;&#31034;&#65292;&#20854;&#27010;&#29575;&#38543;&#25552;&#31034;&#30340;&#38271;&#24230;&#22686;&#21152;&#32780;&#22686;&#21152;&#12290;&#36825;&#24847;&#21619;&#30528;&#20219;&#20309;&#20943;&#24369;&#19981;&#24076;&#26395;&#30340;&#34892;&#20026;&#20294;&#26410;&#23558;&#20854;&#23436;&#20840;&#28040;&#38500;&#30340;&#23545;&#40784;&#36807;&#31243;&#37117;&#26080;&#27861;&#25269;&#24481;&#38024;&#23545;&#24615;&#25915;&#20987;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#31034;&#20102;&#39046;&#20808;&#30340;
&lt;/p&gt;
&lt;p&gt;
An important aspect in developing language models that interact with humans is aligning their behavior to be useful and unharmful for their human users. This is usually achieved by tuning the model in a way that enhances desired behaviors and inhibits undesired ones, a process referred to as alignment. In this paper, we propose a theoretical approach called Behavior Expectation Bounds (BEB) which allows us to formally investigate several inherent characteristics and limitations of alignment in large language models. Importantly, we prove that for any behavior that has a finite probability of being exhibited by the model, there exist prompts that can trigger the model into outputting this behavior, with probability that increases with the length of the prompt. This implies that any alignment process that attenuates undesired behavior but does not remove it altogether, is not safe against adversarial prompting attacks. Furthermore, our framework hints at the mechanism by which leading al
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#24076;&#20271;&#26469;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25152;&#38656;&#30340;&#26368;&#22823;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;HeDC4&#65292;&#20197;&#21450;&#20004;&#31181;&#34920;&#29616;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65306;&#29992;&#20110;&#26631;&#20934;&#38271;&#24230;&#36755;&#20837;&#30340;HeRo&#21644;&#29992;&#20110;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;LongHeRo&#12290;&#20004;&#20010;&#27169;&#22411;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11077</link><description>&lt;p&gt;
HeRo: RoBERTa&#21644; Longformer&#30340;&#24076;&#20271;&#26469;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HeRo: RoBERTa and Longformer Hebrew Language Models. (arXiv:2304.11077v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#24076;&#20271;&#26469;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#25152;&#38656;&#30340;&#26368;&#22823;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;HeDC4&#65292;&#20197;&#21450;&#20004;&#31181;&#34920;&#29616;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65306;&#29992;&#20110;&#26631;&#20934;&#38271;&#24230;&#36755;&#20837;&#30340;HeRo&#21644;&#29992;&#20110;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;LongHeRo&#12290;&#20004;&#20010;&#27169;&#22411;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22635;&#34917;&#20102;&#24076;&#20271;&#26469;&#35821;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#29616;&#26377;&#36164;&#28304;&#30340;&#31354;&#30333;&#65292;&#25552;&#20379;&#36804;&#20170;&#26368;&#22823;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;HeDC4&#12289;&#29992;&#20110;&#26631;&#20934;&#38271;&#24230;&#36755;&#20837;&#30340;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;HeRo&#20197;&#21450;&#29992;&#20110;&#38271;&#36755;&#20837;&#24207;&#21015;&#30340;&#39640;&#25928;transformer LongHeRo. HeRo &#27169;&#22411;&#22312;&#24773;&#24863;&#20998;&#26512;&#12289;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#38382;&#31572;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32780; LongHeRo &#27169;&#22411;&#22312;&#30001;&#38271;&#25991;&#26723;&#32452;&#25104;&#30340;&#25991;&#26723;&#20998;&#31867;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290; &#36825;&#20004;&#20010;&#27169;&#22411;&#22343;&#21576;&#29616;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#26816;&#26597;&#28857;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we fill in an existing gap in resources available to the Hebrew NLP community by providing it with the largest so far pre-train dataset HeDC4, a state-of-the-art pre-trained language model HeRo for standard length inputs and an efficient transformer LongHeRo for long input sequences. The HeRo model was evaluated on the sentiment analysis, the named entity recognition, and the question answering tasks while the LongHeRo model was evaluated on the document classification task with a dataset composed of long documents. Both HeRo and LongHeRo presented state-of-the-art performance. The dataset and model checkpoints used in this work are publicly available.
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#36136;&#30097;&#31867;ChatGPT&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#33021;&#20445;&#35777;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#24456;&#22810;&#26032;&#19968;&#20195;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24320;&#28436;&#31034;&#20013;&#23384;&#22312;&#20107;&#23454;&#38169;&#35823;&#65292;&#21628;&#21505;&#31185;&#30740;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#20107;&#23454;&#27491;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11076</link><description>&lt;p&gt;
&#8220;&#31867;ChatGPT&#29983;&#25104;&#27169;&#22411;&#33021;&#21542;&#20445;&#35777;&#20107;&#23454;&#20934;&#30830;&#24615;&#65311;&#26032;&#19968;&#20195;&#25628;&#32034;&#24341;&#25806;&#30340;&#22833;&#35823;&#8221;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT-like Generative Models Guarantee Factual Accuracy? On the Mistakes of New Generation Search Engines. (arXiv:2304.11076v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11076
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#36136;&#30097;&#31867;ChatGPT&#29983;&#25104;&#27169;&#22411;&#26159;&#21542;&#33021;&#20445;&#35777;&#20107;&#23454;&#20934;&#30830;&#24615;&#65292;&#24456;&#22810;&#26032;&#19968;&#20195;&#25628;&#32034;&#24341;&#25806;&#30340;&#20844;&#24320;&#28436;&#31034;&#20013;&#23384;&#22312;&#20107;&#23454;&#38169;&#35823;&#65292;&#21628;&#21505;&#31185;&#30740;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25552;&#39640;&#36879;&#26126;&#24230;&#21644;&#20107;&#23454;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20687;OpenAI&#30340;ChatGPT&#36825;&#26679;&#30340;&#22823;&#22411;&#23545;&#35805;&#22411;AI&#27169;&#22411;&#23637;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#25105;&#20204;&#36136;&#30097;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#20445;&#35777;&#20107;&#23454;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#65292;&#24494;&#36719;&#21644;&#35895;&#27468;&#31561;&#31185;&#25216;&#20844;&#21496;&#23459;&#24067;&#20102;&#26088;&#22312;&#23558;&#25628;&#32034;&#24341;&#25806;&#19982;&#23545;&#35805;&#22411;AI&#30456;&#32467;&#21512;&#30340;&#26032;&#26381;&#21153;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#21457;&#29616;&#20844;&#24320;&#28436;&#31034;&#20013;&#23384;&#22312;&#35768;&#22810;&#38169;&#35823;&#65292;&#36825;&#34920;&#26126;&#25105;&#20204;&#19981;&#24212;&#36731;&#26131;&#30456;&#20449;AI&#27169;&#22411;&#30340;&#20107;&#23454;&#20027;&#24352;&#12290;&#25105;&#20204;&#24076;&#26395;&#21628;&#21505;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25913;&#21892;AI&#27169;&#22411;&#30340;&#36879;&#26126;&#24230;&#21644;&#20107;&#23454;&#27491;&#30830;&#24615;&#65292;&#32780;&#19981;&#26159;&#25209;&#35780;&#29305;&#23450;&#30340;&#27169;&#22411;&#25110;&#20844;&#21496;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although large conversational AI models such as OpenAI's ChatGPT have demonstrated great potential, we question whether such models can guarantee factual accuracy. Recently, technology companies such as Microsoft and Google have announced new services which aim to combine search engines with conversational AI. However, we have found numerous mistakes in the public demonstrations that suggest we should not easily trust the factual claims of the AI models. Rather than criticizing specific models or companies, we hope to call on researchers and developers to improve AI models' transparency and factual correctness.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#28431;&#27934;&#22270;&#30340;&#26080;&#20559;Transformer&#28304;&#20195;&#30721;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#28304;&#20195;&#30721;&#20013;&#24471;&#21040;&#30340;&#35821;&#20041;&#28431;&#27934;&#22270;&#65288;SVG&#65289;&#34920;&#31034;&#26469;&#35299;&#20915;&#24403;&#21069;&#28431;&#27934;&#31579;&#36873;&#25216;&#26415;&#23545;&#20110;&#35782;&#21035;&#26032;&#28431;&#27934;&#25110;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#28431;&#27934;&#21644;&#20998;&#31867;&#26041;&#38754;&#30340;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.11072</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#28431;&#27934;&#22270;&#30340;&#26080;&#20559;Transformer&#28304;&#20195;&#30721;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Unbiased Transformer Source Code Learning with Semantic Vulnerability Graph. (arXiv:2304.11072v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11072
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#20041;&#28431;&#27934;&#22270;&#30340;&#26080;&#20559;Transformer&#28304;&#20195;&#30721;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20174;&#28304;&#20195;&#30721;&#20013;&#24471;&#21040;&#30340;&#35821;&#20041;&#28431;&#27934;&#22270;&#65288;SVG&#65289;&#34920;&#31034;&#26469;&#35299;&#20915;&#24403;&#21069;&#28431;&#27934;&#31579;&#36873;&#25216;&#26415;&#23545;&#20110;&#35782;&#21035;&#26032;&#28431;&#27934;&#25110;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#28431;&#27934;&#21644;&#20998;&#31867;&#26041;&#38754;&#30340;&#25928;&#26524;&#19981;&#20339;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#65292;&#24320;&#28304;&#36719;&#20214;&#31995;&#32479;&#24050;&#32463;&#25104;&#20026;&#23041;&#32961;&#34892;&#20026;&#32773;&#30340;&#29454;&#29289;&#12290;&#23613;&#31649;&#24320;&#28304;&#31038;&#21306;&#24555;&#36895;&#37319;&#21462;&#25514;&#26045;&#20462;&#34917;&#28431;&#27934;&#65292;&#20294;&#20195;&#30721;&#28431;&#27934;&#31579;&#36873;&#24212;&#35813;&#25104;&#20026;&#25935;&#25463;&#36719;&#20214;&#24320;&#21457;&#30340;&#19968;&#37096;&#20998;&#65292;&#20197;&#20415;&#20174;&#19968;&#24320;&#22987;&#23601;&#33021;&#22815;&#35782;&#21035;&#26032;&#28431;&#27934;&#25110;&#21521;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#28431;&#27934;&#21644;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#28431;&#27934;&#31579;&#36873;&#25216;&#26415;&#23545;&#20110;&#35782;&#21035;&#26032;&#28431;&#27934;&#25110;&#20026;&#24320;&#21457;&#20154;&#21592;&#25552;&#20379;&#20195;&#30721;&#28431;&#27934;&#21644;&#20998;&#31867;&#26041;&#38754;&#30340;&#25928;&#26524;&#19981;&#20339;&#12290;&#27492;&#22806;&#65292;&#29992;&#20110;&#28431;&#27934;&#23398;&#20064;&#30340;&#25968;&#25454;&#38598;&#30001;&#20110;&#25915;&#20987;&#32773;&#37096;&#32626;&#30340;&#26032;&#25915;&#20987;&#31574;&#30053;&#32780;&#23637;&#31034;&#20986;&#19982;&#23454;&#38469;&#27979;&#35797;&#20998;&#24067;&#30340;&#24046;&#24322;&#65292;&#23548;&#33268;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#38459;&#30861;&#25110;&#20559;&#20506;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32852;&#21512;&#25554;&#20540;&#22810;&#20219;&#21153;&#26080;&#20559;&#28431;&#27934;&#20998;&#31867;&#22120;&#65292;&#21253;&#25324;Transformer "RoBERTa"&#21644;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;GCN&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35757;&#32451;&#36807;&#31243;&#65292;&#21033;&#29992;&#20174;&#28304;&#20195;&#30721;&#20013;&#24471;&#21040;&#30340;&#35821;&#20041;&#28431;&#27934;&#22270;&#65288;SVG&#65289;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#26159;&#30001;&#20195;&#30721;&#32467;&#26500;&#21644;&#20989;&#25968;&#21019;&#24314;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#35299;&#20915;&#20102;&#24615;&#33021;&#20559;&#24046;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#39640;&#20102;&#35782;&#21035;&#26032;&#28431;&#27934;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the years, open-source software systems have become prey to threat actors. Even as open-source communities act quickly to patch the breach, code vulnerability screening should be an integral part of agile software development from the beginning. Unfortunately, current vulnerability screening techniques are ineffective at identifying novel vulnerabilities or providing developers with code vulnerability and classification. Furthermore, the datasets used for vulnerability learning often exhibit distribution shifts from the real-world testing distribution due to novel attack strategies deployed by adversaries and as a result, the machine learning model's performance may be hindered or biased. To address these issues, we propose a joint interpolated multitasked unbiased vulnerability classifier comprising a transformer "RoBERTa" and graph convolution neural network (GCN). We present a training process utilizing a semantic vulnerability graph (SVG) representation from source code, creat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#30340;&#30740;&#31350;&#20102;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#20110;&#25903;&#25345;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#25152;&#25552;&#20379;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2304.11065</link><description>&lt;p&gt;
&#23545;&#35805;&#36807;&#31243;&#24314;&#27169;&#65306;&#29616;&#29366;&#12289;&#24212;&#29992;&#21644;&#23454;&#36341;&#24433;&#21709;&#30340;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Conversational Process Modelling: State of the Art, Applications, and Implications in Practice. (arXiv:2304.11065v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#30340;&#30740;&#31350;&#20102;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#20110;&#25903;&#25345;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#25152;&#25552;&#20379;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#24182;&#25512;&#23548;&#20986;&#20102;&#22312;&#23454;&#36341;&#20013;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;Chatbots&#31561;&#32842;&#22825;&#26426;&#22120;&#20154;&#24341;&#36215;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#23545;&#20110;BPM&#24212;&#29992;&#26469;&#35828;&#65292;&#22914;&#20309;&#24212;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#26469;&#29983;&#25104;&#21830;&#19994;&#20215;&#20540;&#36890;&#24120;&#26159;&#19981;&#26126;&#30830;&#30340;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#31995;&#32479;&#22320;&#20998;&#26512;&#29616;&#26377;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#23545;&#20110;&#25903;&#25345;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#20316;&#20026;&#38754;&#21521;&#27969;&#31243;&#30340;&#33021;&#21147;&#30340;&#25903;&#25345;&#12290;&#35813;&#30740;&#31350;&#35782;&#21035;&#20102;&#27839;&#27969;&#31243;&#29983;&#21629;&#21608;&#26399;&#30340;&#24212;&#29992;&#22330;&#26223;&#65292;&#28982;&#21518;&#36827;&#34892;&#20102;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#12290;&#24471;&#20986;&#30340;&#20998;&#31867;&#23398;&#29992;&#20316;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#30340;&#24212;&#29992;&#22330;&#26223;&#30340;&#35782;&#21035;&#65292;&#21253;&#25324;&#27969;&#31243;&#25551;&#36848;&#30340;&#37322;&#20041;&#21644;&#25913;&#36827;&#12290;&#24212;&#29992;&#22330;&#26223;&#22522;&#20110;&#39640;&#31561;&#25945;&#32946;&#39046;&#22495;&#30340;&#23454;&#38469;&#27979;&#35797;&#38598;&#23545;&#29616;&#26377;&#32842;&#22825;&#26426;&#22120;&#20154;&#36827;&#34892;&#35780;&#20272;&#12290;&#35813;&#27979;&#35797;&#38598;&#21253;&#21547;&#27969;&#31243;&#25551;&#36848;&#21450;&#20854;&#23545;&#24212;&#30340;&#27969;&#31243;&#27169;&#22411;&#65292;&#20197;&#21450;&#27169;&#22411;&#36136;&#37327;&#30340;&#35780;&#20272;&#12290;&#22522;&#20110;&#25991;&#29486;&#21644;&#24212;&#29992;&#22330;&#26223;&#20998;&#26512;&#65292;&#24471;&#20986;&#20102;&#20851;&#20110;&#22312;&#23545;&#35805;&#24335;&#27969;&#31243;&#24314;&#27169;&#20013;&#20351;&#29992;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Chatbots such as ChatGPT have caused a tremendous hype lately. For BPM applications, it is often not clear how to apply chatbots to generate business value. Hence, this work aims at the systematic analysis of existing chatbots for their support of conversational process modelling as process-oriented capability. Application scenarios are identified along the process life cycle. Then a systematic literature review on conversational process modelling is performed. The resulting taxonomy serves as input for the identification of application scenarios for conversational process modelling, including paraphrasing and improvement of process descriptions. The application scenarios are evaluated for existing chatbots based on a real-world test set from the higher education domain. It contains process descriptions as well as corresponding process models, together with an assessment of the model quality. Based on the literature and application scenario analyses, recommendations for the usage (prac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#21160;&#20316;&#32479;&#19968;&#22312;&#21333;&#20010;&#31574;&#30053;&#20013;&#65292;&#21033;&#29992; transformer &#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20132;&#26367;&#20351;&#29992;&#21160;&#20316;&#30340;&#25991;&#26412;&#26631;&#39064;&#12290;&#22312; BabyAI &#20219;&#21153;&#20013;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#25512;&#29702;&#31574;&#30053;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#26631;&#39064;&#30340;&#22522;&#20934;&#32447;&#12290;</title><link>http://arxiv.org/abs/2304.11063</link><description>&lt;p&gt;
&#24910;&#24605;&#20043;&#21518;&#20877;&#34892;&#21160;&#65306;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#21160;&#20316;&#32479;&#19968;&#30340;&#20132;&#38169;&#31574;&#30053;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Think Before You Act: Unified Policy for Interleaving Language Reasoning with Actions. (arXiv:2304.11063v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23558;&#35821;&#35328;&#25512;&#29702;&#19982;&#21160;&#20316;&#32479;&#19968;&#22312;&#21333;&#20010;&#31574;&#30053;&#20013;&#65292;&#21033;&#29992; transformer &#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20132;&#26367;&#20351;&#29992;&#21160;&#20316;&#30340;&#25991;&#26412;&#26631;&#39064;&#12290;&#22312; BabyAI &#20219;&#21153;&#20013;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#25512;&#29702;&#31574;&#30053;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#26631;&#39064;&#30340;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#26377;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#30340; transformer &#27169;&#22411;&#30340;&#25104;&#21151;&#35757;&#32451;&#20026;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#24102;&#26469;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26426;&#20250;&#65292;Decision Transformer &#26159;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#36808;&#20986;&#30340;&#19968;&#27493;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#22312;&#31163;&#32447;&#25968;&#25454;&#19978;&#35757;&#32451;&#31867;&#20284;&#30340;&#19979;&#19968;&#27493;&#39044;&#27979;&#30446;&#26631;&#30340; transformers&#65307;&#32780;&#36825;&#20010;&#39046;&#22495;&#30340;&#21478;&#19968;&#20010;&#37325;&#35201;&#21457;&#23637;&#26159;&#36817;&#26399;&#20986;&#29616;&#20102;&#20174;&#20114;&#32852;&#32593;&#25910;&#38598;&#32780;&#26469;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#20363;&#22914;&#30001;&#35270;&#39057;&#25945;&#31243;&#21644;&#23383;&#24149;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#30340;&#20154;&#20204;&#35762;&#36848;&#20182;&#20204;&#27491;&#22312;&#20570;&#30340;&#20107;&#24773;&#12290;&#20026;&#20102;&#21033;&#29992;&#36825;&#31181;&#35821;&#35328;&#32452;&#20214;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35821;&#35328;&#25512;&#29702;&#21644;&#21160;&#20316;&#32479;&#19968;&#22312;&#21333;&#20010;&#31574;&#30053;&#20013;&#30340;&#26032;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#22686;&#21152;&#20102;&#19968;&#20010;&#24102;&#26377;&#21333;&#35789;&#36755;&#20986;&#30340; transformer &#31574;&#30053;&#65292;&#20197;&#20415;&#23427;&#21487;&#20197;&#29983;&#25104;&#20132;&#26367;&#20351;&#29992;&#21160;&#20316;&#30340;&#25991;&#26412;&#26631;&#39064;&#12290;&#22312;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340; BabyAI &#20219;&#21153;&#20013;&#27979;&#35797;&#65292;&#25105;&#20204;&#30340;&#25512;&#29702;&#31574;&#30053;&#22312;&#25551;&#36848;&#19979;&#19968;&#20010;&#23376;&#30446;&#26631;&#30340;&#26631;&#39064;&#19978;&#65292;&#22987;&#32456;&#20248;&#20110;&#27809;&#26377;&#26631;&#39064;&#30340;&#22522;&#20934;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;
The success of transformer models trained with a language modeling objective brings a promising opportunity to the reinforcement learning framework. Decision Transformer is a step towards this direction, showing how to train transformers with a similar next-step prediction objective on offline data. Another important development in this area is the recent emergence of large-scale datasets collected from the internet, such as the ones composed of tutorial videos with captions where people talk about what they are doing. To take advantage of this language component, we propose a novel method for unifying language reasoning with actions in a single policy. Specifically, we augment a transformer policy with word outputs, so it can generate textual captions interleaved with actions. When tested on the most challenging task in BabyAI, with captions describing next subgoals, our reasoning policy consistently outperforms the caption-free baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25193;&#23637;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#26377;&#26395;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#24182;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2304.11062</link><description>&lt;p&gt;
&#21033;&#29992;RMT&#23558;Transformer&#25193;&#23637;&#21040;100&#19975;&#20010;&#26631;&#35760;&#21450;&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Scaling Transformer to 1M tokens and beyond with RMT. (arXiv:2304.11062v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;&#25104;&#21151;&#25193;&#23637;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#26377;&#26395;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#24182;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;&#25193;&#23637;BERT&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#26041;&#27861;&#65292;BERT&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#26368;&#26377;&#25928;&#30340;&#22522;&#20110;Transformer&#27169;&#22411;&#20043;&#19968;&#12290;&#36890;&#36807;&#21033;&#29992;&#24490;&#29615;&#35760;&#24518;Transformer&#26550;&#26500;&#65292;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#27169;&#22411;&#30340;&#26377;&#25928;&#19978;&#19979;&#25991;&#38271;&#24230;&#22686;&#21152;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;200&#19975;&#20010;&#26631;&#35760;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#39640;&#30340;&#20869;&#23384;&#26816;&#32034;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20801;&#35768;&#23384;&#20648;&#21644;&#22788;&#29702;&#26412;&#22320;&#21644;&#20840;&#23616;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#24490;&#29615;&#23454;&#29616;&#36755;&#20837;&#24207;&#21015;&#21508;&#37096;&#20998;&#20043;&#38388;&#30340;&#20449;&#24687;&#27969;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#38271;&#26399;&#20381;&#36182;&#22788;&#29702;&#65292;&#24182;&#33021;&#22815;&#20026;&#20869;&#23384;&#23494;&#38598;&#22411;&#24212;&#29992;&#31243;&#24207;&#23454;&#29616;&#22823;&#35268;&#27169;&#19978;&#19979;&#25991;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This technical report presents the application of a recurrent memory to extend the context length of BERT, one of the most effective Transformer-based models in natural language processing. By leveraging the Recurrent Memory Transformer architecture, we have successfully increased the model's effective context length to an unprecedented two million tokens, while maintaining high memory retrieval accuracy. Our method allows for the storage and processing of both local and global information and enables information flow between segments of the input sequence through the use of recurrence. Our experiments demonstrate the effectiveness of our approach, which holds significant potential to enhance long-term dependency handling in natural language understanding and generation tasks as well as enable large-scale context processing for memory-intensive applications.
&lt;/p&gt;</description></item><item><title>CEIL&#26159;&#19968;&#31181;&#36845;&#20195;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#31867;&#30446;&#26631;&#26469;&#25913;&#36827;&#29305;&#24449;&#34920;&#31034;&#21644;&#25991;&#26412;&#32858;&#31867;&#25928;&#26524;&#65292;&#33021;&#22815;&#26222;&#36866;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#25991;&#26412;&#32858;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.11061</link><description>&lt;p&gt;
CEIL&#65306;&#19968;&#31181;&#36890;&#29992;&#30340;&#20998;&#31867;&#22686;&#24378;&#36845;&#20195;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#25991;&#26412;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
CEIL: A General Classification-Enhanced Iterative Learning Framework for Text Clustering. (arXiv:2304.11061v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11061
&lt;/p&gt;
&lt;p&gt;
CEIL&#26159;&#19968;&#31181;&#36845;&#20195;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#31867;&#30446;&#26631;&#26469;&#25913;&#36827;&#29305;&#24449;&#34920;&#31034;&#21644;&#25991;&#26412;&#32858;&#31867;&#25928;&#26524;&#65292;&#33021;&#22815;&#26222;&#36866;&#20110;&#19981;&#21516;&#39046;&#22495;&#30340;&#25991;&#26412;&#32858;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#26368;&#22522;&#26412;&#30340;&#25361;&#25112;&#20043;&#19968;&#65292;&#25991;&#26412;&#32858;&#31867;&#26088;&#22312;&#23558;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#25991;&#26412;&#27573;&#20998;&#32452;&#65292;&#32780;&#19981;&#20381;&#36182;&#20110;&#20154;&#24037;&#27880;&#37322;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#30340;&#36805;&#36895;&#21457;&#23637;&#65292;&#28145;&#24230;&#32858;&#31867;&#22312;&#20256;&#32479;&#32858;&#31867;&#26041;&#27861;&#19978;&#21462;&#24471;&#20102;&#26174;&#33879;&#20248;&#21183;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#28145;&#24230;&#25991;&#26412;&#32858;&#31867;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#22312;&#19968;&#33324;&#39046;&#22495;&#39044;&#35757;&#32451;&#30340;&#34920;&#31034;&#65292;&#36825;&#21487;&#33021;&#19981;&#26159;&#22312;&#29305;&#23450;&#30446;&#26631;&#39046;&#22495;&#32858;&#31867;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CEIL&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#30701;&#25991;&#26412;&#32858;&#31867;&#20998;&#31867;&#22686;&#24378;&#36845;&#20195;&#23398;&#20064;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#20998;&#31867;&#30446;&#26631;&#24341;&#20837;&#36845;&#20195;&#22320;&#25913;&#36827;&#29305;&#24449;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#26222;&#36941;&#25552;&#39640;&#32858;&#31867;&#24615;&#33021;&#12290;&#22312;&#27599;&#20010;&#36845;&#20195;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#37319;&#29992;&#35821;&#35328;&#27169;&#22411;&#26816;&#32034;&#21021;&#22987;&#25991;&#26412;&#34920;&#31034;&#65292;&#28982;&#21518;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#20998;&#31867;&#19981;&#21487;&#20998;&#21106;&#20998;&#24067;&#65288;CDC&#65289;&#27169;&#22359;&#25910;&#38598;&#32858;&#31867;&#32467;&#26524;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20998;&#31867;&#27169;&#22359;&#23558;&#20998;&#31867;&#30446;&#26631;&#25972;&#21512;&#21040;&#32858;&#31867;&#26694;&#26550;&#20013;&#65292;&#24182;&#20351;&#29992;&#24471;&#21040;&#30340;&#25913;&#36827;&#29305;&#24449;&#34920;&#31034;&#22312;&#19979;&#19968;&#27425;&#36845;&#20195;&#20013;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;CEIL&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;&#20854;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#31639;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text clustering, as one of the most fundamental challenges in unsupervised learning, aims at grouping semantically similar text segments without relying on human annotations. With the rapid development of deep learning, deep clustering has achieved significant advantages over traditional clustering methods. Despite the effectiveness, most existing deep text clustering methods rely heavily on representations pre-trained in general domains, which may not be the most suitable solution for clustering in specific target domains. To address this issue, we propose CEIL, a novel Classification-Enhanced Iterative Learning framework for short text clustering, which aims at generally promoting the clustering performance by introducing a classification objective to iteratively improve feature representations. In each iteration, we first adopt a language model to retrieve the initial text representations, from which the clustering results are collected using our proposed Category Disentangled Contr
&lt;/p&gt;</description></item><item><title>SkillGPT&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;&#30340;RESTful API&#26381;&#21153;&#65292;&#36890;&#36807;&#25688;&#35201;&#21644;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.11060</link><description>&lt;p&gt;
SkillGPT: &#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;&#30340;RESTful API&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
SkillGPT: a RESTful API service for skill extraction and standardization using a Large Language Model. (arXiv:2304.11060v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11060
&lt;/p&gt;
&lt;p&gt;
SkillGPT&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270;&#30340;RESTful API&#26381;&#21153;&#65292;&#36890;&#36807;&#25688;&#35201;&#21644;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102; SkillGPT&#65292;&#19968;&#31181;&#21033;&#29992;&#24320;&#28304;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#36827;&#34892;&#20174;&#33258;&#30001;&#39118;&#26684;&#32844;&#20301;&#25551;&#36848;&#21644;&#29992;&#25143;&#36164;&#26009;&#20013;&#36827;&#34892;&#25216;&#33021;&#25552;&#21462;&#21644;&#26631;&#20934;&#21270; (SES) &#30340;&#24037;&#20855;&#12290;&#19982;&#22823;&#22810;&#25968;&#31867;&#20284;&#20219;&#21153;&#30340;&#20197;&#21069;&#26041;&#27861;&#19981;&#21516;&#65292;SkillGPT &#30452;&#25509;&#20351;&#29992;&#26368;&#26032;&#30340;&#23545;&#35805; LLM &#36827;&#34892;&#26631;&#20934;&#25216;&#33021;&#30340;&#25552;&#31034;&#65292;&#36890;&#36807;&#25688;&#35201;&#21644;&#21521;&#37327;&#30456;&#20284;&#24615;&#25628;&#32034;&#26469;&#24179;&#34913;&#36895;&#24230;&#21644;&#20934;&#30830;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#20813;&#36153; SkillGPT &#35753;&#29992;&#25143;&#33021;&#22815;&#39640;&#25928;&#21487;&#38752;&#22320;&#36827;&#34892;&#23545;&#35805;&#22411; SES&#12290;
&lt;/p&gt;
&lt;p&gt;
We present SkillGPT, a tool for skill extraction and standardization (SES) from free-style job descriptions and user profiles with an open-source Large Language Model (LLM) as backbone. Most previous methods for similar tasks either need supervision or rely on heavy data-preprocessing and feature engineering. Directly prompting the latest conversational LLM for standard skills, however, is slow, costly and inaccurate. In contrast, SkillGPT utilizes a LLM to perform its tasks in steps via summarization and vector similarity search, to balance speed with precision. The backbone LLM of SkillGPT is based on Llama, free for academic use and thus useful for exploratory research and prototype development. Hence, our cost-free SkillGPT gives users the convenience of conversational SES, efficiently and reliably.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#29615;&#22659;&#8212;&#8212;CyberBattleSim&#65292;&#35774;&#35745;&#29992;&#20110;RL&#32593;&#32476;&#25805;&#20316;&#20195;&#29702;&#30340;&#35757;&#32451;&#12290;&#26412;&#25991;&#30528;&#37325;&#25253;&#36947;&#20102;&#23545;&#38450;&#24481;&#22411;&#34013;&#33394;&#20195;&#29702;&#35757;&#32451;&#30340;&#25913;&#36827;&#65292;&#32467;&#26524;&#34920;&#26126;&#32418;&#33394;&#20195;&#29702;&#19982;&#34013;&#33394;&#20195;&#29702;&#32852;&#21512;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#34013;&#33394;&#20195;&#29702;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.11052</link><description>&lt;p&gt;
RL&#32593;&#32476;&#25805;&#20316;&#20195;&#29702;&#30340;&#22810;&#26234;&#33021;&#20307;&#32593;&#32476;&#25112;&#20223;&#30495;
&lt;/p&gt;
&lt;p&gt;
A Multiagent CyberBattleSim for RL Cyber Operation Agents. (arXiv:2304.11052v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11052
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#29615;&#22659;&#8212;&#8212;CyberBattleSim&#65292;&#35774;&#35745;&#29992;&#20110;RL&#32593;&#32476;&#25805;&#20316;&#20195;&#29702;&#30340;&#35757;&#32451;&#12290;&#26412;&#25991;&#30528;&#37325;&#25253;&#36947;&#20102;&#23545;&#38450;&#24481;&#22411;&#34013;&#33394;&#20195;&#29702;&#35757;&#32451;&#30340;&#25913;&#36827;&#65292;&#32467;&#26524;&#34920;&#26126;&#32418;&#33394;&#20195;&#29702;&#19982;&#34013;&#33394;&#20195;&#29702;&#32852;&#21512;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#34013;&#33394;&#20195;&#29702;&#30340;&#38450;&#24481;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30828;&#21270;&#32593;&#32476;&#36164;&#20135;&#26082;&#33267;&#20851;&#37325;&#35201;&#65292;&#21448;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#30340;&#20154;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31561;&#25216;&#26415;&#24050;&#32463;&#23637;&#29616;&#20102;&#22312;&#33258;&#21160;&#21270;&#20219;&#21153;&#26041;&#38754;&#30340;&#24040;&#22823;&#28508;&#21147;&#65292;&#21487;&#20197;&#33258;&#20027;&#23436;&#25104;&#20154;&#21147;&#26080;&#27861;&#32988;&#20219;&#30340;&#37325;&#22797;&#24615;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24320;&#21457;&#33258;&#20027;RL&#20195;&#29702;&#38656;&#35201;&#19968;&#20010;&#23545;&#25239;&#35757;&#32451;&#29615;&#22659;&#65292;&#21487;&#20197;&#24555;&#36895;&#35780;&#20272;&#21508;&#31181;&#24773;&#20917;&#65292;&#24182;&#38024;&#23545;&#19981;&#21516;&#22330;&#26223;&#36827;&#34892;&#35757;&#32451;&#12290;CyberBattleSim&#20415;&#26159;&#36825;&#26679;&#19968;&#20010;&#38024;&#23545;&#32418;&#33394;&#20195;&#29702;&#65288;&#21363;&#25915;&#20987;&#32773;&#65289;&#30340;&#35757;&#32451;&#29615;&#22659;&#65292;&#22312;&#20854;&#22522;&#30784;&#19978;&#28155;&#21152;&#20102;&#38024;&#23545;&#34013;&#33394;&#20195;&#29702;&#65288;&#21363;&#38450;&#24481;&#32773;&#65289;&#30340;&#35757;&#32451;&#25511;&#21046;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;&#36825;&#20123;&#25913;&#36827;&#65292;&#20197;&#21450;&#22312;&#20351;&#29992;&#36825;&#20123;&#25913;&#36827;&#21518;&#38024;&#23545;&#34013;&#33394;&#20195;&#29702;&#35757;&#32451;&#26102;&#25152;&#33719;&#24471;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#38024;&#23545;&#34013;&#33394;&#20195;&#29702;&#30340;&#35757;&#32451;&#30830;&#23454;&#33021;&#22815;&#22686;&#24378;&#20854;&#23545;&#25239;&#25915;&#20987;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#21644;&#32418;&#33394;&#20195;&#29702;&#19968;&#36215;&#35757;&#32451;&#26102;&#20854;&#25928;&#26524;&#26356;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hardening cyber physical assets is both crucial and labor-intensive. Recently, Machine Learning (ML) in general and Reinforcement Learning RL) more specifically has shown great promise to automate tasks that otherwise would require significant human insight/intelligence. The development of autonomous RL agents requires a suitable training environment that allows us to quickly evaluate various alternatives, in particular how to arrange training scenarios that pit attackers and defenders against each other. CyberBattleSim is a training environment that supports the training of red agents, i.e., attackers. We added the capability to train blue agents, i.e., defenders. The paper describes our changes and reports on the results we obtained when training blue agents, either in isolation or jointly with red agents. Our results show that training a blue agent does lead to stronger defenses against attacks. In particular, training a blue agent jointly with a red agent increases the blue agent's
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#31038;&#20132;&#21270;&#20154;&#24418;&#26234;&#33021;&#31995;&#32479;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20154;&#31867;&#22768;&#38899;&#30340;&#24773;&#24863;&#35821;&#20041;&#65292;&#23454;&#29616;&#31867;&#20154;&#23545;&#35805;&#12290;</title><link>http://arxiv.org/abs/2304.11046</link><description>&lt;p&gt;
&#24773;&#24863;&#31038;&#20132;&#21270;&#20154;&#24418;&#26234;&#33021;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Affective social anthropomorphic intelligent system. (arXiv:2304.11046v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#24773;&#24863;&#31038;&#20132;&#21270;&#20154;&#24418;&#26234;&#33021;&#31995;&#32479;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#29702;&#35299;&#20154;&#31867;&#22768;&#38899;&#30340;&#24773;&#24863;&#35821;&#20041;&#65292;&#23454;&#29616;&#31867;&#20154;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30340;&#23545;&#35805;&#39118;&#26684;&#21487;&#20197;&#36890;&#36807;&#24189;&#40664;&#24863;&#12289;&#20010;&#24615;&#21644;&#35821;&#35843;&#26469;&#34913;&#37327;&#12290;&#36825;&#20123;&#29305;&#24449;&#24050;&#32463;&#25104;&#20026;&#23545;&#35805;&#26234;&#33021;&#34394;&#25311;&#21161;&#25163;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#26234;&#33021;&#34394;&#25311;&#21161;&#25163;&#65288;IVAs&#65289;&#26080;&#27861;&#35299;&#37322;&#20154;&#31867;&#22768;&#38899;&#30340;&#24773;&#24863;&#35821;&#20041;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#24418;&#26234;&#33021;&#31995;&#32479;&#65292;&#21487;&#20197;&#36890;&#36807;&#34920;&#36798;&#24773;&#24863;&#21644;&#20010;&#24615;&#26469;&#36827;&#34892;&#31867;&#20154;&#23545;&#35805;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35821;&#38899;&#39118;&#26684;&#36716;&#25442;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#29305;&#23450;&#24773;&#24863;&#30340;&#23646;&#24615;&#26144;&#23556;&#20986;&#26469;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#23558;&#26102;&#38388;&#38899;&#39057;&#27874;&#24418;&#25968;&#25454;&#36716;&#25442;&#20026;&#39057;&#22495;&#25968;&#25454;&#65288;Mel-Spectrogram&#65289;&#65292;&#21019;&#24314;&#20102;&#31163;&#25955;&#30340;&#38899;&#39057;&#29305;&#24449;&#27169;&#24335;&#65292;&#22914;&#38899;&#31526;&#12289;&#38899;&#35843;&#12289;&#33410;&#22863;&#12289;&#26059;&#24459;&#31561;&#31561;&#12290;&#24182;&#20351;&#29992;&#19968;&#20010;&#22806;&#37096;CNN-Transformer-Encoder&#27169;&#22411;&#26469;&#39044;&#27979;&#22768;&#38899;&#20013;&#30340;&#19971;&#31181;&#19981;&#21516;&#30340;&#24773;&#24863;&#29366;&#24577;&#12290;&#21516;&#26102;&#65292;&#35813;&#27169;&#22411;&#23558;&#35821;&#38899;&#36755;&#20837;&#21040;&#19968;&#20010;RNN&#27169;&#22411;&#65288;Deep-speech&#65289;&#20013;&#65292;&#29983;&#25104;&#23545;&#38899;&#39057;&#30340;&#25991;&#26412;&#36716;&#24405;&#12290;&#28982;&#21518;&#65292;&#36716;&#24405;&#25991;&#26412;&#23558;&#36890;&#36807;&#19968;&#20010;transformer&#35299;&#30721;&#22120;&#19982;&#30456;&#24212;&#24773;&#24863;&#30340;&#21709;&#24212;&#19968;&#36215;&#20135;&#29983;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human conversational styles are measured by the sense of humor, personality, and tone of voice. These characteristics have become essential for conversational intelligent virtual assistants. However, most of the state-of-the-art intelligent virtual assistants (IVAs) are failed to interpret the affective semantics of human voices. This research proposes an anthropomorphic intelligent system that can hold a proper human-like conversation with emotion and personality. A voice style transfer method is also proposed to map the attributes of a specific emotion. Initially, the frequency domain data (Mel-Spectrogram) is created by converting the temporal audio wave data, which comprises discrete patterns for audio features such as notes, pitch, rhythm, and melody. A collateral CNN-Transformer-Encoder is used to predict seven different affective states from voice. The voice is also fed parallelly to the deep-speech, an RNN model that generates the text transcription from the spectrogram. Then t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#21453;&#39304;&#25511;&#21046;&#24518;&#38459;&#22120;&#32534;&#31243;&#30005;&#36335;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#26597;&#25214;&#34920;&#30340;&#32534;&#31243;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21333;&#21521;&#39034;&#24207;&#36807;&#31243;&#20013;&#25191;&#34892;&#24518;&#38459;&#22120;&#30340;&#32534;&#31243;&#21644;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31639;&#27861;&#20013;&#39057;&#32321;&#20999;&#25442;&#24102;&#26469;&#30340;&#39640;&#21160;&#24577;&#21151;&#29575;&#21644;&#38271;&#32534;&#31243;&#26102;&#38388;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2304.11030</link><description>&lt;p&gt;
&#29992;&#20110;&#27169;&#25311;&#20869;&#23481;&#21487;&#23547;&#22336;&#23384;&#20648;&#22120;&#30340;&#27169;&#25311;&#21453;&#39304;&#25511;&#21046;&#24518;&#38459;&#22120;&#32534;&#31243;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Analog Feedback-Controlled Memristor programming Circuit for analog Content Addressable Memory. (arXiv:2304.11030v1 [cs.ET])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#21453;&#39304;&#25511;&#21046;&#24518;&#38459;&#22120;&#32534;&#31243;&#30005;&#36335;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#26597;&#25214;&#34920;&#30340;&#32534;&#31243;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21333;&#21521;&#39034;&#24207;&#36807;&#31243;&#20013;&#25191;&#34892;&#24518;&#38459;&#22120;&#30340;&#32534;&#31243;&#21644;&#39564;&#35777;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#31639;&#27861;&#20013;&#39057;&#32321;&#20999;&#25442;&#24102;&#26469;&#30340;&#39640;&#21160;&#24577;&#21151;&#29575;&#21644;&#38271;&#32534;&#31243;&#26102;&#38388;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#32852;&#24819;&#23384;&#20648;&#22120;&#31361;&#30772;&#34920;&#26126;&#65292;&#30789;&#23384;&#20648;&#22120;&#27491;&#36234;&#26469;&#36234;&#25509;&#36817;&#20154;&#31867;&#35760;&#24518;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#33021;&#22815;&#35835;&#20889;&#27169;&#25311;&#20540;&#30340;&#24518;&#38459;&#22120;&#20869;&#23481;&#21487;&#23547;&#22336;&#23384;&#20648;&#22120;&#65288;CAM&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24518;&#38459;&#22120;&#32534;&#31243;&#31639;&#27861;&#38656;&#35201;&#39057;&#32321;&#22320;&#22312;&#39564;&#35777;&#21644;&#32534;&#31243;&#24518;&#38459;&#22120;&#23548;&#32435;&#20043;&#38388;&#36827;&#34892;&#20999;&#25442;&#65292;&#36825;&#24102;&#26469;&#35768;&#22810;&#32570;&#38519;&#65292;&#22914;&#39640;&#21160;&#24577;&#21151;&#29575;&#21644;&#38271;&#30340;&#32534;&#31243;&#26102;&#38388;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#21453;&#39304;&#25511;&#21046;&#24518;&#38459;&#22120;&#32534;&#31243;&#30005;&#36335;&#65292;&#35813;&#30005;&#36335;&#37319;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#26597;&#25214;&#34920;&#30340;&#32534;&#31243;&#31639;&#27861;&#12290;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#21333;&#21521;&#39034;&#24207;&#36807;&#31243;&#20013;&#25191;&#34892;&#24518;&#38459;&#22120;&#30340;&#32534;&#31243;&#21644;&#39564;&#35777;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23558;&#21333;&#20010;&#25152;&#25552;&#20986;&#30340;&#32534;&#31243;&#30005;&#36335;&#19982;8&#20010;&#27169;&#25311;CAM&#65288;aCAM&#65289;&#21333;&#20803;&#38598;&#25104;&#65292;&#20197;&#26500;&#24314;aCAM&#38453;&#21015;&#12290;&#25105;&#20204;&#22312;TSMC 28nm&#24037;&#33402;&#19978;&#36827;&#34892;&#20102;SPICE&#20223;&#30495;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65306;1. A
&lt;/p&gt;
&lt;p&gt;
Recent breakthroughs in associative memories suggest that silicon memories are coming closer to human memories, especially for memristive Content Addressable Memories (CAMs) which are capable to read and write in analog values. However, the Program-Verify algorithm, the state-of-the-art memristor programming algorithm, requires frequent switching between verifying and programming memristor conductance, which brings many defects such as high dynamic power and long programming time. Here, we propose an analog feedback-controlled memristor programming circuit that makes use of a novel look-up table-based (LUT-based) programming algorithm. With the proposed algorithm, the programming and the verification of a memristor can be performed in a single-direction sequential process. Besides, we also integrated a single proposed programming circuit with eight analog CAM (aCAM) cells to build an aCAM array. We present SPICE simulations on TSMC 28nm process. The theoretical analysis shows that 1. A
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FARM&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22788;&#29702;&#23454;&#26102;&#25968;&#25454;&#27969;&#24182;&#25552;&#20379;&#24179;&#34913;&#30340;&#30456;&#20851;&#24615;&#24230;&#37327;&#65292;&#36827;&#32780;&#30830;&#23450;&#22806;&#37096;&#25968;&#25454;&#22312;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11028</link><description>&lt;p&gt;
&#39044;&#27979;&#20013;&#30340;&#22806;&#22312;&#25968;&#25454;&#65306;&#19968;&#31181;&#29992;&#20110;&#20851;&#32852;&#35780;&#20272;&#30340;FARM&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Exogenous Data in Forecasting: FARM -- An Approach for Relevance Evaluation. (arXiv:2304.11028v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11028
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FARM&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#22788;&#29702;&#23454;&#26102;&#25968;&#25454;&#27969;&#24182;&#25552;&#20379;&#24179;&#34913;&#30340;&#30456;&#20851;&#24615;&#24230;&#37327;&#65292;&#36827;&#32780;&#30830;&#23450;&#22806;&#37096;&#25968;&#25454;&#22312;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22806;&#22312;&#25968;&#25454;&#34987;&#35748;&#20026;&#22312;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#38024;&#23545;&#24688;&#24403;&#30340;&#36873;&#25321;&#65292;&#20840;&#38754;&#30340;&#30456;&#20851;&#24615;&#20998;&#26512;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#31532;&#19968;&#27493;&#65292;&#20174;&#22806;&#22312;&#25968;&#25454;&#19982;&#21442;&#32771;&#26102;&#38388;&#24207;&#21015;&#30340;&#30456;&#20284;&#24615;&#24320;&#22987;&#12290;&#21463;&#29616;&#26377;&#26102;&#38388;&#24207;&#21015;&#30456;&#20284;&#24615;&#25351;&#26631;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FARM&#65288;&#21069;&#21521;&#35282;&#30456;&#20851;&#24230;&#37327;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#22788;&#29702;&#23454;&#26102;&#25968;&#25454;&#27969;&#12290;&#25105;&#20204;&#30340;&#21069;&#21521;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#31181;&#35282;&#24230;&#29305;&#24449;&#65292;&#35813;&#29305;&#24449;&#21033;&#29992;&#21518;&#32493;&#25968;&#25454;&#28857;&#30340;&#21464;&#21270;&#27604;&#36739;&#26469;&#23545;&#40784;&#32463;&#36807;&#26102;&#38388;&#21464;&#24418;&#30340;&#24207;&#21015;&#12290;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#32467;&#21512;&#20102;&#26412;&#22320;&#21644;&#20840;&#23616;&#25351;&#26631;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#24179;&#34913;&#30340;&#30456;&#20851;&#24615;&#24230;&#37327;&#12290;&#36825;&#23548;&#33268;&#23558;&#37096;&#20998;&#12289;&#20013;&#38388;&#21305;&#37197;&#20063;&#35270;&#20026;&#22806;&#22312;&#25968;&#25454;&#24207;&#21015;&#37325;&#35201;&#25351;&#26631;&#30340;&#32771;&#34385;&#22240;&#32032;&#12290;&#20316;&#20026;&#31532;&#19968;&#27493;&#39564;&#35777;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#25105;&#20204;&#30340;FARM&#26041;&#27861;&#23545;&#21512;&#25104;&#20294;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;&#20449;&#21495;&#21644;&#30495;&#23454;&#19990;&#30028;&#26102;&#38388;&#24207;&#21015;&#35760;&#24405;&#30340;&#24212;&#29992;&#12290;&#21516;&#26102;&#23637;&#31034;&#20102;FARM&#26041;&#27861;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24230;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exogenous data is believed to play a key role for increasing forecasting accuracy. For an appropriate selection, a throughout relevance analysis is a fundamental first step, starting from the exogenous data similarity with the reference time series. Inspired by existing metrics for time series similarity, we introduce a new approach named FARM - Forward Angular Relevance Measure, able to effectively deal with real-time data streams. Our forward method relies on an angular feature that compares changes in subsequent data points to align time-warped series in an efficient way. The proposed algorithm combines local and global measures to provide a balanced relevance measure. This results in considering also partial, intermediate matches as relevant indicators for exogenous data series significance. As a first validation step, we present the application of our FARM approach to both synthetic but representative signals and real-world time series recordings. While demonstrating the improved 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;RoboGPT&#31995;&#32479;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#36741;&#21161;&#26045;&#24037;&#35013;&#37197;&#20013;&#30340;&#33258;&#21160;&#21270;&#39034;&#24207;&#35268;&#21010;&#12290;&#35813;&#31995;&#32479;&#21487;&#20811;&#26381;&#20854;&#20182;&#26041;&#27861;&#22312;&#36866;&#24212;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.11018</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#36741;&#21161;&#26045;&#24037;&#35013;&#37197;&#30340;&#33258;&#21160;&#21270;&#39034;&#24207;&#35268;&#21010;&#65306;&#22522;&#20110;ChatGPT&#30340;RoboGPT
&lt;/p&gt;
&lt;p&gt;
Robot-Enabled Construction Assembly with Automated Sequence Planning based on ChatGPT: RoboGPT. (arXiv:2304.11018v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11018
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;RoboGPT&#31995;&#32479;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#36741;&#21161;&#26045;&#24037;&#35013;&#37197;&#20013;&#30340;&#33258;&#21160;&#21270;&#39034;&#24207;&#35268;&#21010;&#12290;&#35813;&#31995;&#32479;&#21487;&#20811;&#26381;&#20854;&#20182;&#26041;&#27861;&#22312;&#36866;&#24212;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#22312;&#26045;&#24037;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#25104;&#20026;&#35299;&#20915;&#35832;&#22810;&#25361;&#25112;&#30340;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#22914;&#25104;&#26412;&#19978;&#28072;&#12289;&#21171;&#21160;&#21147;&#30701;&#32570;&#20197;&#21450;&#23545;&#23433;&#20840;&#12289;&#39640;&#25928;&#26045;&#24037;&#24037;&#33402;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#36825;&#20123;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#20840;&#37096;&#28508;&#21147;&#38754;&#20020;&#30340;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#23545;&#26045;&#24037;&#20219;&#21153;&#36827;&#34892;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#39034;&#24207;&#35268;&#21010;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#21253;&#25324;&#25968;&#23398;&#21644;&#21551;&#21457;&#24335;&#25216;&#26415;&#25110;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#36866;&#24212;&#33021;&#21147;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#25193;&#23637;&#24403;&#21069;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#39034;&#24207;&#29702;&#35299;&#33021;&#21147;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#20808;&#36827;&#25512;&#29702;&#33021;&#21147;&#30340;RoboGPT&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#36741;&#21161;&#26045;&#24037;&#35013;&#37197;&#20013;&#30340;&#33258;&#21160;&#21270;&#39034;&#24207;&#35268;&#21010;&#12290;&#35813;&#31995;&#32479;&#37319;&#29992;&#20102;ChatGPT&#26500;&#24314;&#26045;&#24037;&#39034;&#24207;&#35268;&#21010;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20854;&#21487;&#34892;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Robot-based assembly in construction has emerged as a promising solution to address numerous challenges such as increasing costs, labor shortages, and the demand for safe and efficient construction processes. One of the main obstacles in realizing the full potential of these robotic systems is the need for effective and efficient sequence planning for construction tasks. Current approaches, including mathematical and heuristic techniques or machine learning methods, face limitations in their adaptability and scalability to dynamic construction environments. To expand the ability of the current robot system in sequential understanding, this paper introduces RoboGPT, a novel system that leverages the advanced reasoning capabilities of ChatGPT, a large language model, for automated sequence planning in robot-based assembly applied to construction tasks. The proposed system adapts ChatGPT for construction sequence planning and demonstrate its feasibility and effectiveness through experimen
&lt;/p&gt;</description></item><item><title>DIN-SQL&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#65292;&#20351;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.11015</link><description>&lt;p&gt;
DIN-SQL: &#33258;&#32416;&#27491;&#30340;&#25991;&#26412;&#21040;SQL&#20998;&#35299;&#24335;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
DIN-SQL: Decomposed In-Context Learning of Text-to-SQL with Self-Correction. (arXiv:2304.11015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11015
&lt;/p&gt;
&lt;p&gt;
DIN-SQL&#36890;&#36807;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#23427;&#20204;&#30340;&#34920;&#29616;&#65292;&#20351;&#20934;&#30830;&#24615;&#36229;&#36807;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22797;&#26434;&#30340;&#25991;&#26412;&#21040;SQL&#20219;&#21153;&#20998;&#35299;&#20026;&#36739;&#23567;&#30340;&#23376;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#20998;&#35299;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23613;&#31649;SQL&#26597;&#35810;&#20855;&#26377;&#22768;&#26126;&#24335;&#32467;&#26500;&#65292;&#20294;&#21487;&#20197;&#23558;&#20854;&#20998;&#35299;&#20026;&#23376;&#38382;&#39064;&#65292;&#24182;&#23558;&#36825;&#20123;&#23376;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#39304;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#23427;&#20204;&#30340;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#31283;&#23450;&#25552;&#39640;&#19977;&#31181;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#29616;&#65292;&#22823;&#32422;&#25552;&#39640;&#20102;10&#65285;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#25512;&#21521;&#26368;&#26032;&#27700;&#24179;&#65292;&#24182;&#22312;Holdout Spider&#25968;&#25454;&#38598;&#19978;&#29978;&#33267;&#36229;&#36807;&#20102;&#32463;&#36807;&#31934;&#35843;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of decomposing a complex text-to-sql task into smaller sub-tasks and how such a decomposition can significantly improve the performance of Large Language Models (LLMs) in the reasoning process. There is currently a significant gap between the performance of fine-tuned models and prompting approaches using LLMs on challenging text-to-sql datasets such as Spider. We show that SQL queries, despite their declarative structure, can be broken down into sub-problems and the solutions of those sub-problems can be fed into LLMs to significantly improve their performance. Our experiments with three LLMs show that this approach consistently improves their performance by roughly 10%, pushing the accuracy of LLMs towards state-of-the-art, and even beating large fine-tuned models on the holdout Spider dataset.
&lt;/p&gt;</description></item><item><title>&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#38376;&#25915;&#20987;&#19968;&#30452;&#26159;&#19968;&#20010;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#25913;&#36827;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#25915;&#20987;&#32773;&#33021;&#21147;&#65292;&#22312;&#33021;&#21147;&#21463;&#38480;&#22330;&#26223;&#19979;&#36824;&#27809;&#26377;&#25214;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#27492;&#22806;&#65292;&#27169;&#22411;&#40065;&#26834;&#24615;&#20173;&#28982;&#20540;&#24471;&#20851;&#27880;&#12290;</title><link>http://arxiv.org/abs/2304.10985</link><description>&lt;p&gt;
&#22312;&#33021;&#21147;&#21463;&#38480;&#22330;&#26223;&#19979;&#21551;&#21160;&#24378;&#38887;&#21518;&#38376;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Launching a Robust Backdoor Attack under Capability Constrained Scenarios. (arXiv:2304.10985v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10985
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21518;&#38376;&#25915;&#20987;&#19968;&#30452;&#26159;&#19968;&#20010;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#29616;&#26377;&#30340;&#25913;&#36827;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#25915;&#20987;&#32773;&#33021;&#21147;&#65292;&#22312;&#33021;&#21147;&#21463;&#38480;&#22330;&#26223;&#19979;&#36824;&#27809;&#26377;&#25214;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#21150;&#27861;&#65292;&#27492;&#22806;&#65292;&#27169;&#22411;&#40065;&#26834;&#24615;&#20173;&#28982;&#20540;&#24471;&#20851;&#27880;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#20851;&#38190;&#39046;&#22495;&#30340;&#24212;&#29992;&#19981;&#26029;&#22686;&#21152;&#65292;&#20154;&#20204;&#24320;&#22987;&#25285;&#24515;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#12290;&#30001;&#20110;&#32570;&#20047;&#36879;&#26126;&#24230;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#21518;&#38376;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#27745;&#26579;&#30340;&#21518;&#38376;&#27169;&#22411;&#22312;&#26222;&#36890;&#29615;&#22659;&#19979;&#21487;&#33021;&#34920;&#29616;&#27491;&#24120;&#65292;&#20294;&#24403;&#36755;&#20837;&#21253;&#21547;&#35302;&#21457;&#22120;&#26102;&#65292;&#20250;&#26174;&#31034;&#20986;&#24694;&#24847;&#34892;&#20026;&#12290;&#30446;&#21069;&#23545;&#21518;&#38376;&#25915;&#20987;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#25913;&#21892;&#35302;&#21457;&#22120;&#30340;&#31192;&#23494;&#24615;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#38656;&#35201;&#24378;&#22823;&#30340;&#25915;&#20987;&#32773;&#33021;&#21147;&#65292;&#20363;&#22914;&#23545;&#27169;&#22411;&#32467;&#26500;&#30340;&#20102;&#35299;&#25110;&#23545;&#35757;&#32451;&#36807;&#31243;&#30340;&#25511;&#21046;&#12290;&#30001;&#20110;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#25915;&#20987;&#32773;&#30340;&#33021;&#21147;&#21463;&#21040;&#38480;&#21046;&#65292;&#36825;&#20123;&#25915;&#20987;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#38382;&#39064;&#36824;&#26410;&#24471;&#21040;&#20805;&#20998;&#20851;&#27880;&#12290;&#20363;&#22914;&#65292;&#27169;&#22411;&#33976;&#39311;&#24120;&#29992;&#20110;&#31616;&#21270;&#27169;&#22411;&#22823;&#23567;&#65292;&#20294;&#38543;&#30528;&#21442;&#25968;&#25968;&#37327;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20197;&#21069;&#30340;&#35768;&#22810;&#21518;&#38376;&#25915;&#20987;&#22312;&#27169;&#22411;&#33976;&#39311;&#21518;&#22343;&#22833;&#36133;;&#22270;&#20687;&#22686;&#24378;&#25805;&#20316;&#21487;&#20197;&#30772;&#22351;&#35302;&#21457;&#22120;&#65292;&#20174;&#32780;&#20351;&#21518;&#38376;&#25915;&#20987;&#22833;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
As deep neural networks continue to be used in critical domains, concerns over their security have emerged. Deep learning models are vulnerable to backdoor attacks due to the lack of transparency. A poisoned backdoor model may perform normally in routine environments, but exhibit malicious behavior when the input contains a trigger. Current research on backdoor attacks focuses on improving the stealthiness of triggers, and most approaches require strong attacker capabilities, such as knowledge of the model structure or control over the training process. These attacks are impractical since in most cases the attacker's capabilities are limited. Additionally, the issue of model robustness has not received adequate attention. For instance, model distillation is commonly used to streamline model size as the number of parameters grows exponentially, and most of previous backdoor attacks failed after model distillation; the image augmentation operations can destroy the trigger and thus disabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20449;&#24687;&#39281;&#21644;&#25209;&#27425;&#32622;&#20449;&#26641;&#31639;&#27861;&#35299;&#20915;&#20102;&#36816;&#21160;&#21644;&#20256;&#24863;&#22120;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#36816;&#21160;&#35268;&#21010;&#65292;&#20808;&#26500;&#24314;&#26631;&#20934;&#36712;&#36857;&#30340;&#22270;&#65292;&#20877;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#21551;&#21457;&#24335;&#20195;&#20215;&#35745;&#31639;&#35299;&#20915;&#21407;&#38382;&#39064;&#65292;&#26368;&#21518;&#36890;&#36807;&#25628;&#32034;&#26631;&#20934;&#36712;&#36857;&#30340;&#26641;&#26469;&#29983;&#25104;&#32622;&#20449;&#24230;&#26641;&#12290;</title><link>http://arxiv.org/abs/2304.10984</link><description>&lt;p&gt;
IBBT: &#20449;&#24687;&#39281;&#21644;&#25209;&#27425;&#32622;&#20449;&#26641;&#29992;&#20110;&#19981;&#30830;&#23450;&#24773;&#20917;&#19979;&#30340;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
IBBT: Informed Batch Belief Trees for Motion Planning Under Uncertainty. (arXiv:2304.10984v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#20449;&#24687;&#39281;&#21644;&#25209;&#27425;&#32622;&#20449;&#26641;&#31639;&#27861;&#35299;&#20915;&#20102;&#36816;&#21160;&#21644;&#20256;&#24863;&#22120;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#36816;&#21160;&#35268;&#21010;&#65292;&#20808;&#26500;&#24314;&#26631;&#20934;&#36712;&#36857;&#30340;&#22270;&#65292;&#20877;&#21033;&#29992;&#25152;&#25552;&#20986;&#30340;&#21551;&#21457;&#24335;&#20195;&#20215;&#35745;&#31639;&#35299;&#20915;&#21407;&#38382;&#39064;&#65292;&#26368;&#21518;&#36890;&#36807;&#25628;&#32034;&#26631;&#20934;&#36712;&#36857;&#30340;&#26641;&#26469;&#29983;&#25104;&#32622;&#20449;&#24230;&#26641;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20449;&#24687;&#39281;&#21644;&#25209;&#27425;&#32622;&#20449;&#26641;&#65288;IBBT&#65289;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36816;&#21160;&#21644;&#20256;&#24863;&#22120;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#23558;&#38543;&#26426;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#20998;&#20026;&#30830;&#23450;&#24615;&#36816;&#21160;&#35268;&#21010;&#38382;&#39064;&#21644;&#22270;&#25628;&#38382;&#39064;&#20004;&#37096;&#20998;&#65292;&#20351;&#29992;&#22522;&#20110;&#37319;&#26679;&#30340;&#26041;&#27861;&#26500;&#24314;&#19968;&#20010;&#26631;&#20934;&#36712;&#36857;&#30340;&#22270;&#65292;&#24182;&#35745;&#31639;&#20986;&#21407;&#38382;&#39064;&#30340;&#20449;&#24687;&#21551;&#21457;&#24335;&#20195;&#20215;&#12290;&#26368;&#21518;&#65292;&#36890;&#36807;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#21551;&#21457;&#24335;&#31574;&#30053;&#65292;&#25628;&#32034;&#26631;&#20934;&#36712;&#36857;&#30340;&#22270;&#26469;&#29983;&#25104;&#32622;&#20449;&#24230;&#26641;&#12290;IBBT&#37319;&#29992;&#25152;&#25552;&#20986;&#30340;&#31574;&#30053;&#65292;&#20132;&#26367;&#36827;&#34892;&#29366;&#24577;&#37319;&#26679;&#65292;&#26631;&#20934;&#36712;&#36857;&#22270;&#26500;&#24314;&#65292;&#21551;&#21457;&#24335;&#35745;&#31639;&#21644;&#22270;&#25628;&#20197;&#25214;&#21040;&#32622;&#20449;&#24230;&#31354;&#38388;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;IBBT&#26159;&#19968;&#31181;&#20219;&#20309;&#26102;&#20505;&#30340;&#22686;&#37327;&#31639;&#27861;&#65292;&#38543;&#30528;&#25209;&#27425;&#26679;&#26412;&#25968;&#37327;&#30340;&#19981;&#26029;&#22686;&#21152;&#65292;&#31639;&#27861;&#25214;&#21040;&#30340;&#36816;&#21160;&#35268;&#21010;&#20250;&#36880;&#28176;&#36235;&#21521;&#20110;&#26368;&#20248;&#32467;&#26524;&#65292;&#22240;&#27492;&#25928;&#29575;&#24456;&#39640;&#65292;&#19988;&#21487;&#20197;&#22312;&#19981;&#21516;&#26679;&#26412;&#38598;&#20043;&#38388;&#37325;&#22797;&#21033;&#29992;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we propose the Informed Batch Belief Trees (IBBT) algorithm for motion planning under motion and sensing uncertainties. The original stochastic motion planning problem is divided into a deterministic motion planning problem and a graph search problem. We solve the deterministic planning problem using sampling-based methods such as PRM or RRG to construct a graph of nominal trajectories. Then, an informed cost-to-go heuristic for the original problem is computed based on the nominal trajectory graph. Finally, we grow a belief tree by searching over the graph using the proposed heuristic. IBBT interleaves between batch state sampling, nominal trajectory graph construction, heuristic computing, and search over the graph to find belief space motion plans. IBBT is an anytime, incremental algorithm. With an increasing number of batches of samples added to the graph, the algorithm finds motion plans that converge to the optimal one. IBBT is efficient by reusing results between s
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEIA&#30340;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#30001;&#36229;&#36807;6&#30334;&#19975;&#20010;&#33258;&#27880;&#37322;&#25991;&#26412;&#24086;&#23376;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#21033;&#29992;&#25513;&#34109;&#21333;&#35789;&#30340;&#26041;&#27861;&#22686;&#24378;&#27169;&#22411;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#24773;&#24863;&#21333;&#35789;&#30340;&#23398;&#20064;&#65292;&#24182;&#22312;&#19977;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#32422;73&#30340;&#23439;F1&#20540;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.10973</link><description>&lt;p&gt;
LEIA&#65306;&#35821;&#35328;&#23884;&#20837;&#29992;&#20110;&#24773;&#24863;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
LEIA: Linguistic Embeddings for the Identification of Affect. (arXiv:2304.10973v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10973
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LEIA&#30340;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#30001;&#36229;&#36807;6&#30334;&#19975;&#20010;&#33258;&#27880;&#37322;&#25991;&#26412;&#24086;&#23376;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#21033;&#29992;&#25513;&#34109;&#21333;&#35789;&#30340;&#26041;&#27861;&#22686;&#24378;&#27169;&#22411;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#24773;&#24863;&#21333;&#35789;&#30340;&#23398;&#20064;&#65292;&#24182;&#22312;&#19977;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#32422;73&#30340;&#23439;F1&#20540;&#65292;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20135;&#29983;&#20102;&#22823;&#37327;&#25991;&#26412;&#25968;&#25454;&#65292;&#20351;&#24471;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#20998;&#26512;&#24773;&#24863;&#21464;&#24471;&#26356;&#21152;&#23481;&#26131;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#22312;&#30001;&#35835;&#32773;&#29983;&#25104;&#30340;&#23567;&#22411;&#32780;&#26114;&#36149;&#30340;&#25991;&#26412;&#27880;&#37322;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20123;&#35835;&#32773;&#29468;&#27979;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#20013;&#34920;&#36798;&#30340;&#24773;&#24863;&#12290;&#36825;&#24433;&#21709;&#20102;&#24773;&#24863;&#35782;&#21035;&#26041;&#27861;&#30340;&#36136;&#37327;&#65292;&#22240;&#20026;&#23384;&#22312;&#35757;&#32451;&#25968;&#25454;&#22823;&#23567;&#38480;&#21046;&#21644;&#29992;&#20110;&#27169;&#22411;&#24320;&#21457;&#30340;&#26631;&#31614;&#29983;&#20135;&#20013;&#30340;&#22122;&#22768;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LEIA&#65292;&#36825;&#26159;&#19968;&#31181;&#25991;&#26412;&#24773;&#24863;&#35782;&#21035;&#27169;&#22411;&#65292;&#23427;&#22522;&#20110;&#30001;&#36229;&#36807;6&#30334;&#19975;&#20010;&#24086;&#23376;&#32452;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#20854;&#20013;&#36825;&#20123;&#24086;&#23376;&#20855;&#26377;&#33258;&#27880;&#37322;&#30340;&#24773;&#24863;&#26631;&#31614;&#65292;&#21253;&#25324;&#24555;&#20048;&#12289;&#20146;&#24773;&#12289;&#24754;&#20260;&#12289;&#24868;&#24594;&#21644;&#24656;&#24807;&#12290;LEIA&#22522;&#20110;&#19968;&#31181;&#25513;&#34109;&#21333;&#35789;&#30340;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#39044;&#35757;&#32451;&#36807;&#31243;&#20013;&#23545;&#24773;&#24863;&#21333;&#35789;&#30340;&#23398;&#20064;&#12290;LEIA&#22312;&#19977;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#32422;73&#30340;&#23439;F1&#20540;&#65292;&#20248;&#20110;&#20854;&#20182;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#26041;&#27861;&#65292;&#24182;&#22312;&#24378;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;LEIA&#21487;&#20197;&#27010;&#25324;&#19981;&#21516;&#30340;&#24086;&#23376;&#12289;&#29992;&#25143;&#21644;&#26102;&#38388;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
The wealth of text data generated by social media has enabled new kinds of analysis of emotions with language models. These models are often trained on small and costly datasets of text annotations produced by readers who guess the emotions expressed by others in social media posts. This affects the quality of emotion identification methods due to training data size limitations and noise in the production of labels used in model development. We present LEIA, a model for emotion identification in text that has been trained on a dataset of more than 6 million posts with self-annotated emotion labels for happiness, affection, sadness, anger, and fear. LEIA is based on a word masking method that enhances the learning of emotion words during model pre-training. LEIA achieves macro-F1 values of approximately 73 on three in-domain test datasets, outperforming other supervised and unsupervised methods in a strong benchmark that shows that LEIA generalizes across posts, users, and time periods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;GATv2&#30340;&#21487;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#20102;&#20840;&#38754;&#25512;&#23548;&#65292;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#30340;&#24615;&#33021;&#34920;&#29616;&#19981;&#19968;&#33268;&#30340;&#21407;&#22240;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10939</link><description>&lt;p&gt;
&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#20013;&#21487;&#23398;&#20064;&#21442;&#25968;&#30340;&#26799;&#24230;&#25512;&#23548;
&lt;/p&gt;
&lt;p&gt;
Gradient Derivation for Learnable Parameters in Graph Attention Networks. (arXiv:2304.10939v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;GATv2&#30340;&#21487;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#20102;&#20840;&#38754;&#25512;&#23548;&#65292;&#22312;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#30340;&#24615;&#33021;&#34920;&#29616;&#19981;&#19968;&#33268;&#30340;&#21407;&#22240;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#30340;&#24191;&#27867;&#23454;&#29616;&#20043;&#19968;&#8212;&#8212;GATv2&#30340;&#21487;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#26799;&#24230;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#25512;&#23548;&#12290;&#34429;&#28982;GAT&#24050;&#34987;&#35777;&#26126;&#26159;&#22788;&#29702;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#20294;&#26159;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#30340;&#24615;&#33021;&#34920;&#29616;&#19981;&#19968;&#33268;&#65292;&#20854;&#21407;&#22240;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#38382;&#39064;&#12290;&#30001;&#20110;&#26799;&#24230;&#27969;&#20026;&#32479;&#35745;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21160;&#24577;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#27934;&#35265;&#65292;&#22240;&#27492;&#26412;&#25991;&#25512;&#23548;&#20102;GATv2&#30340;&#21487;&#35757;&#32451;&#27169;&#22411;&#21442;&#25968;&#30340;&#26799;&#24230;&#12290;&#36825;&#20123;&#26799;&#24230;&#25512;&#23548;&#34917;&#20805;&#20102;[2]&#30340;&#24037;&#20316;&#65292;&#21518;&#32773;&#35843;&#26597;&#20102;GATv2&#30340;&#28508;&#22312;&#38519;&#38449;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work provides a comprehensive derivation of the parameter gradients for GATv2 [4], a widely used implementation of Graph Attention Networks (GATs). GATs have proven to be powerful frameworks for processing graph-structured data and, hence, have been used in a range of applications. However, the achieved performance by these attempts has been found to be inconsistent across different datasets and the reasons for this remains an open research question. As the gradient flow provides valuable insights into the training dynamics of statistically learning models, this work obtains the gradients for the trainable model parameters of GATv2. The gradient derivations supplement the efforts of [2], where potential pitfalls of GATv2 are investigated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#25105;&#30417;&#30563;&#27169;&#22411;&#38519;&#20837;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23558;&#37492;&#21035;&#22120;&#32435;&#20837;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#65292;&#24110;&#21161;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10914</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#25239;&#20223;&#30495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Adversarial Imitation Learning. (arXiv:2304.10914v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10914
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35299;&#20915;&#33258;&#25105;&#30417;&#30563;&#27169;&#22411;&#38519;&#20837;&#22351;&#23616;&#37096;&#26368;&#23567;&#20540;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#23558;&#37492;&#21035;&#22120;&#32435;&#20837;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#65292;&#24110;&#21161;&#23398;&#20064;&#65292;&#24182;&#35299;&#20915;&#20102;&#24120;&#35265;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34892;&#20026;&#20811;&#38534;&#26159;&#19968;&#31181;&#36890;&#36807;&#19987;&#23478;&#28436;&#31034;&#26469;&#25945;&#25480;&#26234;&#33021;&#20307;&#22914;&#20309;&#34892;&#20026;&#30340;&#20223;&#30495;&#23398;&#20064;&#25216;&#26415;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#33258;&#25105;&#30417;&#30563;&#30340;&#23436;&#20840;&#21487;&#35266;&#23519;&#26410;&#26631;&#35760;&#29366;&#24577;&#30340;&#24555;&#29031;&#26469;&#23558;&#29366;&#24577;&#23545;&#35299;&#30721;&#20026;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25216;&#26415;&#37319;&#29992;&#30340;&#36845;&#20195;&#23398;&#20064;&#26041;&#26696;&#23481;&#26131;&#38519;&#20837;&#22351;&#30340;&#23616;&#37096;&#26368;&#23567;&#20540;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#30446;&#26631;&#24863;&#30693;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#38656;&#35201;&#20154;&#24037;&#20171;&#20837;&#26469;&#39564;&#35777;&#26234;&#33021;&#20307;&#26159;&#21542;&#36798;&#21040;&#20102;&#30446;&#26631;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#37492;&#21035;&#22120;&#32435;&#20837;&#21407;&#22987;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25552;&#20379;&#20102;&#20004;&#20010;&#20851;&#38190;&#20248;&#21183;&#65292;&#24182;&#30452;&#25509;&#35299;&#20915;&#20102;&#20197;&#21069;&#30340;&#19968;&#20010;&#23398;&#20064;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#23427;&#19981;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#12290;&#20854;&#27425;&#65292;&#23427;&#36890;&#36807;&#25351;&#23548;&#22522;&#20110;&#19987;&#23478;&#36712;&#36857;&#30340;&#29366;&#24577;&#36716;&#25442;&#30340;&#20989;&#25968;&#36924;&#36817;&#26469;&#24110;&#21161;&#23398;&#20064;&#12290;&#31532;&#19977;&#65292;&#37492;&#21035;&#22120;&#35299;&#20915;&#20102;&#31574;&#30053;&#27169;&#22411;&#24120;&#35265;&#30340;&#23398;&#20064;&#38382;&#39064;&#65292;&#21363;&#26377;&#26102;&#20250;&#25191;&#34892;&#8220;&#26080;&#21160;&#20316;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Behavioural cloning is an imitation learning technique that teaches an agent how to behave via expert demonstrations. Recent approaches use self-supervision of fully-observable unlabelled snapshots of the states to decode state pairs into actions. However, the iterative learning scheme employed by these techniques is prone to get trapped into bad local minima. Previous work uses goal-aware strategies to solve this issue. However, this requires manual intervention to verify whether an agent has reached its goal. We address this limitation by incorporating a discriminator into the original framework, offering two key advantages and directly solving a learning problem previous work had. First, it disposes of the manual intervention requirement. Second, it helps in learning by guiding function approximation based on the state transition of the expert's trajectories. Third, the discriminator solves a learning issue commonly present in the policy model, which is to sometimes perform a `no ac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#21644;&#36816;&#31639;&#31526;&#30340;&#20248;&#32570;&#28857;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#38024;&#23545;&#20415;&#25658;&#35745;&#31639;&#24179;&#21488;&#30340;&#30828;&#20214;&#21152;&#36895;&#26041;&#26696;&#65292;&#24182;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#23618;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;</title><link>http://arxiv.org/abs/2304.10891</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21450;&#20854;&#30828;&#20214;&#21152;&#36895;&#20998;&#26512;&#65306;&#32508;&#36848; (arXiv:2304.10891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
Transformer-based models and hardware acceleration analysis in autonomous driving: A survey. (arXiv:2304.10891v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#24212;&#29992;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#20307;&#31995;&#32467;&#26500;&#21644;&#36816;&#31639;&#31526;&#30340;&#20248;&#32570;&#28857;&#65292;&#37325;&#28857;&#35752;&#35770;&#20102;&#38024;&#23545;&#20415;&#25658;&#35745;&#31639;&#24179;&#21488;&#30340;&#30828;&#20214;&#21152;&#36895;&#26041;&#26696;&#65292;&#24182;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;Transformer&#30340;&#23618;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Transformer&#26550;&#26500;&#22312;&#21508;&#31181;&#33258;&#21160;&#39550;&#39542;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#23558;&#20854;&#19987;&#38376;&#29992;&#20110;&#20415;&#25658;&#24335;&#35745;&#31639;&#24179;&#21488;&#30340;&#30828;&#20214;&#21152;&#36895;&#24050;&#25104;&#20026;&#23454;&#38469;&#37096;&#32626;&#22312;&#30495;&#23454;&#33258;&#21160;&#27773;&#36710;&#20013;&#30340;&#19979;&#19968;&#27493;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#32508;&#36848;&#35770;&#25991;&#25552;&#20379;&#20102;&#38024;&#23545;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#20840;&#38754;&#27010;&#36848;&#12289;&#22522;&#20934;&#21644;&#20998;&#26512;&#65292;&#20363;&#22914;&#36710;&#36947;&#26816;&#27979;&#12289;&#20998;&#21106;&#12289;&#36319;&#36394;&#12289;&#35268;&#21010;&#21644;&#20915;&#31574;&#21046;&#23450;&#12290;&#25105;&#20204;&#23457;&#26597;&#20102;&#19981;&#21516;&#30340;&#20307;&#31995;&#32467;&#26500;&#65292;&#29992;&#20110;&#32452;&#32455;Transformer&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#65292;&#20363;&#22914;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#21644;&#20165;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#32570;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#28145;&#20837;&#35752;&#35770;&#20102;Transformer&#30456;&#20851;&#30340;&#36816;&#31639;&#31526;&#21450;&#20854;&#30828;&#20214;&#21152;&#36895;&#26041;&#26696;&#65292;&#32771;&#34385;&#21040;&#20851;&#38190;&#22240;&#32032;&#65292;&#22914;&#37327;&#21270;&#21644;&#36816;&#34892;&#26102;&#12290;&#25105;&#20204;&#29305;&#21035;&#22312;&#31227;&#21160;&#21644;&#26700;&#38754;&#24179;&#21488;&#19978;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#23618;&#19982;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#30340;&#36816;&#31639;&#31526;&#36827;&#34892;&#20102;&#23545;&#27604;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#32508;&#36848;&#35770;&#25991;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#25552;&#20379;&#20102;&#31995;&#32479;&#30340;&#25351;&#21335;&#65292;&#20197;&#20102;&#35299;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#21450;&#20854;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#30828;&#20214;&#21152;&#36895;&#30340;&#24403;&#21069;&#36827;&#23637;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architectures have exhibited promising performance in various autonomous driving applications in recent years. On the other hand, its dedicated hardware acceleration on portable computational platforms has become the next critical step for practical deployment in real autonomous vehicles. This survey paper provides a comprehensive overview, benchmark, and analysis of Transformer-based models specifically tailored for autonomous driving tasks such as lane detection, segmentation, tracking, planning, and decision-making. We review different architectures for organizing Transformer inputs and outputs, such as encoder-decoder and encoder-only structures, and explore their respective advantages and disadvantages. Furthermore, we discuss Transformer-related operators and their hardware acceleration schemes in depth, taking into account key factors such as quantization and runtime. We specifically illustrate the operator level comparison between layers from convolutional neural ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#25512;&#26029;&#21160;&#24577;&#31995;&#32479;&#21442;&#25968;&#20449;&#24687;&#24182;&#20174;&#20043;&#21069;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#26426;&#22120;&#20154;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#23558;&#35813;&#31639;&#27861;&#19982;Adversarial Motion Priors&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#20581;&#22766;&#12289;&#28789;&#27963;&#12289;&#33258;&#28982;&#30340;&#27493;&#24577;&#65292;&#21487;&#29992;&#20110;&#31359;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#24418;&#12290;</title><link>http://arxiv.org/abs/2304.10888</link><description>&lt;p&gt;
&#37326;&#22806;&#29615;&#22659;&#19979;&#30340;AMP&#65306;&#23398;&#20064;&#20581;&#22766;&#12289;&#28789;&#27963;&#12289;&#33258;&#28982;&#30340;&#26377;&#33151;&#31227;&#21160;&#25216;&#33021;
&lt;/p&gt;
&lt;p&gt;
AMP in the wild: Learning robust, agile, natural legged locomotion skills. (arXiv:2304.10888v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#21487;&#25512;&#26029;&#21160;&#24577;&#31995;&#32479;&#21442;&#25968;&#20449;&#24687;&#24182;&#20174;&#20043;&#21069;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#26426;&#22120;&#20154;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#23558;&#35813;&#31639;&#27861;&#19982;Adversarial Motion Priors&#30456;&#32467;&#21512;&#65292;&#23454;&#29616;&#20102;&#22312;&#20223;&#30495;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#20581;&#22766;&#12289;&#28789;&#27963;&#12289;&#33258;&#28982;&#30340;&#27493;&#24577;&#65292;&#21487;&#29992;&#20110;&#31359;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#24418;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#19968;&#20010;&#23398;&#20064;&#25511;&#21046;&#22120;&#20174;&#20223;&#30495;&#29615;&#22659;&#36716;&#31227;&#21040;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#38656;&#35201;&#19981;&#20165;&#33021;&#22815;&#35782;&#21035;&#31995;&#32479;&#65292;&#32780;&#19988;&#36824;&#38656;&#35201;&#20934;&#30830;&#22320;&#20272;&#35745;&#26426;&#22120;&#20154;&#30340;&#29366;&#24577;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#19981;&#20165;&#21487;&#20197;&#25512;&#26029;&#21160;&#24577;&#31995;&#32479;&#21442;&#25968;&#20449;&#24687;&#65292;&#36824;&#21487;&#20197;&#20174;&#20043;&#21069;&#30340;&#35266;&#23519;&#25968;&#25454;&#20013;&#20272;&#35745;&#26426;&#22120;&#20154;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#12290;&#25105;&#20204;&#23558;&#35813;&#31639;&#27861;&#19982;Adversarial Motion Priors&#30456;&#32467;&#21512;&#65292;&#22312;&#20223;&#30495;&#21644;&#22312;Unitree A1&#22235;&#36275;&#26426;&#22120;&#20154;&#30495;&#23454;&#19990;&#30028;&#20013;&#23454;&#29616;&#20102;&#20581;&#22766;&#12289;&#28789;&#27963;&#12289;&#33258;&#28982;&#30340;&#27493;&#24577;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#22522;&#20934;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#31639;&#27861;&#33021;&#22815;&#20197;&#26356;&#20302;&#30340;&#21151;&#32791;&#31359;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22320;&#24418;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#23450;&#24615;&#21644;&#23450;&#37327;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The successful transfer of a learned controller from simulation to the real world for a legged robot requires not only the ability to identify the system, but also accurate estimation of the robot's state. In this paper, we propose a novel algorithm that can infer not only information about the parameters of the dynamic system, but also estimate important information about the robot's state from previous observations. We integrate our algorithm with Adversarial Motion Priors and achieve a robust, agile, and natural gait in both simulation and on a Unitree A1 quadruped robot in the real world. Empirical results demonstrate that our proposed algorithm enables traversing challenging terrains with lower power consumption compared to the baselines. Both qualitative and quantitative results are presented in this paper.
&lt;/p&gt;</description></item><item><title>Metropolis&#31639;&#27861;&#30340;&#23616;&#37096;&#25628;&#32034;&#31574;&#30053;&#24182;&#19981;&#24635;&#26159;&#20248;&#20110;&#36827;&#21270;&#31639;&#27861;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#23436;&#21892;&#12290;</title><link>http://arxiv.org/abs/2304.10848</link><description>&lt;p&gt;
Metropolis&#31639;&#27861;&#22312;&#22788;&#29702;&#23616;&#37096;&#26368;&#20248;&#26102;&#30340;&#25928;&#26524;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How Well Does the Metropolis Algorithm Cope With Local Optima?. (arXiv:2304.10848v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10848
&lt;/p&gt;
&lt;p&gt;
Metropolis&#31639;&#27861;&#30340;&#23616;&#37096;&#25628;&#32034;&#31574;&#30053;&#24182;&#19981;&#24635;&#26159;&#20248;&#20110;&#36827;&#21270;&#31639;&#27861;&#65292;&#36824;&#38656;&#35201;&#36827;&#19968;&#27493;&#25913;&#36827;&#21644;&#23436;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Metropolis&#31639;&#27861;&#65288;MA&#65289;&#26159;&#19968;&#31181;&#32463;&#20856;&#30340;&#38543;&#26426;&#23616;&#37096;&#25628;&#32034;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;&#23427;&#36890;&#36807;&#20598;&#23572;&#25509;&#21463;&#27425;&#20248;&#35299;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#24182;&#20197;&#20005;&#26684;&#30340;&#26041;&#24335;&#29702;&#35299;&#36825;&#31181;&#33021;&#21147;&#65292;&#25105;&#20204;&#23545;CLIFF&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;MA&#36827;&#34892;&#20102;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#12290;&#38500;&#20102;&#19968;&#20010;&#23616;&#37096;&#26368;&#20248;&#35299;&#22806;&#65292;cliff&#20989;&#25968;&#21521;&#20840;&#23616;&#26368;&#20248;&#35299;&#21333;&#35843;&#36882;&#22686;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#20248;&#21270;cliff&#20989;&#25968;&#65292;MA&#21482;&#38656;&#35201;&#19968;&#27425;&#25509;&#21463;&#19968;&#20010;&#21155;&#36136;&#35299;&#12290;&#23613;&#31649;&#30475;&#36215;&#26469;&#36825;&#26159;MA&#20174;&#20854;&#20027;&#35201;&#24037;&#20316;&#21407;&#29702;&#20013;&#33719;&#21033;&#30340;&#29702;&#24819;&#22522;&#20934;&#27979;&#35797;&#65292;&#20294;&#25105;&#20204;&#30340;&#25968;&#23398;&#36816;&#34892;&#26102;&#38388;&#20998;&#26512;&#34920;&#26126;&#36825;&#19968;&#24076;&#26395;&#24182;&#27809;&#26377;&#23454;&#29616;&#12290;&#21363;&#20351;&#22312;&#26368;&#20248;&#28201;&#24230;&#19979;&#65288;MA&#30340;&#21807;&#19968;&#21442;&#25968;&#65289;&#65292;MA&#20248;&#21270;&#22823;&#22810;&#25968;cliff&#20989;&#25968;&#30340;&#25928;&#29575;&#20063;&#19981;&#22914;&#31616;&#21333;&#30340;&#31934;&#33521;&#36827;&#21270;&#31639;&#27861;&#65288;EAs&#65289;&#65292;&#21518;&#32773;&#21482;&#33021;&#36890;&#36807;&#29983;&#25104;&#21487;&#33021;&#30456;&#36317;&#24456;&#36828;&#30340;&#20248;&#31168;&#35299;&#26469;&#31163;&#24320;&#23616;&#37096;&#26368;&#20248;&#35299;&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#23545;MA&#20026;&#20160;&#20040;&#26377;&#25928;&#30340;&#29702;&#35299;&#38656;&#35201;&#36827;&#19968;&#27493;&#23436;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Metropolis algorithm (MA) is a classic stochastic local search heuristic. It avoids getting stuck in local optima by occasionally accepting inferior solutions. To better and in a rigorous manner understand this ability, we conduct a mathematical runtime analysis of the MA on the CLIFF benchmark. Apart from one local optimum, cliff functions are monotonically increasing towards the global optimum. Consequently, to optimize a cliff function, the MA only once needs to accept an inferior solution. Despite seemingly being an ideal benchmark for the MA to profit from its main working principle, our mathematical runtime analysis shows that this hope does not come true. Even with the optimal temperature (the only parameter of the MA), the MA optimizes most cliff functions less efficiently than simple elitist evolutionary algorithms (EAs), which can only leave the local optimum by generating a superior solution possibly far away. This result suggests that our understanding of why the MA is 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37051;&#22495;&#24863;&#30693;&#23376;&#22270;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#22122;&#22768;&#65292;&#25552;&#39640;&#23376;&#22270;&#30340;&#21484;&#22238;&#29575;&#65292;&#20174;&#32780;&#21487;&#20197;&#25512;&#21160;&#36828;&#36317;&#31163;&#30340;&#33410;&#28857;&#21521;&#30456;&#21516;&#30340;&#20013;&#24515;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2304.10831</link><description>&lt;p&gt;
&#23398;&#20064;&#22914;&#20309;&#20351;&#29992;&#26356;&#22909;&#30340;&#23376;&#22270;&#36827;&#34892;&#20154;&#33080;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Learn to Cluster Faces with Better Subgraphs. (arXiv:2304.10831v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10831
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37051;&#22495;&#24863;&#30693;&#23376;&#22270;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#22122;&#22768;&#65292;&#25552;&#39640;&#23376;&#22270;&#30340;&#21484;&#22238;&#29575;&#65292;&#20174;&#32780;&#21487;&#20197;&#25512;&#21160;&#36828;&#36317;&#31163;&#30340;&#33410;&#28857;&#21521;&#30456;&#21516;&#30340;&#20013;&#24515;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#32858;&#31867;&#21487;&#20197;&#20026;&#28023;&#37327;&#26080;&#26631;&#31614;&#20154;&#33080;&#25968;&#25454;&#25552;&#20379;&#20266;&#26631;&#31614;&#65292;&#24182;&#25552;&#39640;&#19981;&#21516;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#29616;&#26377;&#30340;&#32858;&#31867;&#26041;&#27861;&#36890;&#24120;&#26159;&#22312;&#23376;&#22270;&#20869;&#32858;&#21512;&#29305;&#24449;&#65292;&#36825;&#20123;&#23376;&#22270;&#36890;&#24120;&#22522;&#20110;&#32479;&#19968;&#30340;&#38408;&#20540;&#25110;&#23398;&#20064;&#24471;&#21040;&#30340;&#25130;&#27490;&#20301;&#32622;&#23454;&#29616;&#12290;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#23376;&#22270;&#30340;&#21484;&#22238;&#29575;&#65292;&#20174;&#32780;&#38477;&#20302;&#32858;&#31867;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#37051;&#22495;&#24863;&#30693;&#23376;&#22270;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#20943;&#23569;&#22122;&#22768;&#65292;&#25552;&#39640;&#23376;&#22270;&#30340;&#21484;&#22238;&#29575;&#65292;&#20174;&#32780;&#21487;&#20197;&#25512;&#21160;&#36828;&#36317;&#31163;&#30340;&#33410;&#28857;&#21521;&#30456;&#21516;&#30340;&#20013;&#24515;&#25910;&#25947;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#20004;&#20010;&#32452;&#20214;&#65292;&#21363;&#20351;&#29992;&#37051;&#23621;&#30340;&#23884;&#20837;&#26469;&#22686;&#24378;&#20154;&#33080;&#23884;&#20837;&#65292;&#24182;&#23545;&#33410;&#28857;&#23545;&#36827;&#34892;&#23553;&#38381;&#23376;&#22270;&#26500;&#24314;&#20197;&#25552;&#21462;&#32467;&#26500;&#20449;&#24687;&#12290;&#23558;&#23884;&#20837;&#32452;&#21512;&#36215;&#26469;&#65292;&#39044;&#27979;&#25152;&#26377;&#33410;&#28857;&#23545;&#30340;&#38142;&#25509;&#27010;&#29575;&#20197;&#26367;&#25442;&#20313;&#24358;&#30456;&#20284;&#24230;&#26469;&#29983;&#25104;&#26032;&#30340;&#23376;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face clustering can provide pseudo-labels to the massive unlabeled face data and improve the performance of different face recognition models. The existing clustering methods generally aggregate the features within subgraphs that are often implemented based on a uniform threshold or a learned cutoff position. This may reduce the recall of subgraphs and hence degrade the clustering performance. This work proposed an efficient neighborhood-aware subgraph adjustment method that can significantly reduce the noise and improve the recall of the subgraphs, and hence can drive the distant nodes to converge towards the same centers. More specifically, the proposed method consists of two components, i.e. face embeddings enhancement using the embeddings from neighbors, and enclosed subgraph construction of node pairs for structural information extraction. The embeddings are combined to predict the linkage probabilities for all node pairs to replace the cosine similarities to produce new subgraphs
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2304.10819</link><description>&lt;p&gt;
&#21487;&#25511;&#30340;&#20449;&#20219;&#26435;&#34913;&#19979;&#30340;&#21512;&#25104;&#25968;&#25454;&#23457;&#35745;&#19982;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Auditing and Generating Synthetic Data with Controllable Trust Trade-offs. (arXiv:2304.10819v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10819
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#33021;&#22815;&#20197;&#20840;&#38754;&#30340;&#26041;&#24335;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#21644;AI&#27169;&#22411;&#30340;&#20855;&#20307;&#25928;&#26524;&#65292;&#21253;&#25324;&#20559;&#35265;&#21644;&#27495;&#35270;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#22312;&#22810;&#20010;&#29992;&#20363;&#20013;&#65292;&#23457;&#35745;&#26694;&#26550;&#24179;&#34913;&#20102;&#20449;&#20219;&#21644;&#25928;&#29992;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#23454;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#24448;&#24448;&#23384;&#22312;&#20559;&#24046;&#12289;&#19981;&#24179;&#34913;&#65292;&#24182;&#19988;&#26377;&#27844;&#38706;&#25935;&#24863;&#21644;&#38544;&#31169;&#20449;&#24687;&#30340;&#39118;&#38505;&#12290;&#36825;&#19968;&#20107;&#23454;&#24341;&#21457;&#20102;&#21019;&#24314;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#24819;&#27861;&#65292;&#20197;&#20943;&#36731;&#30495;&#23454;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#39118;&#38505;&#12289;&#20559;&#35265;&#12289;&#20260;&#23475;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#36825;&#20010;&#27010;&#24565;&#20381;&#36182;&#20110;&#29983;&#25104;AI&#27169;&#22411;&#65292;&#20197;&#20135;&#29983;&#19981;&#20559;&#25191;&#12289;&#20445;&#25252;&#38544;&#31169;&#30340;&#21512;&#25104;&#25968;&#25454;&#65292;&#21516;&#26102;&#24544;&#23454;&#20110;&#30495;&#23454;&#25968;&#25454;&#12290;&#22312;&#36825;&#31181;&#26032;&#33539;&#24335;&#20013;&#65292;&#25105;&#20204;&#22914;&#20309;&#30693;&#36947;&#36825;&#31181;&#26041;&#27861;&#26159;&#21542;&#20817;&#29616;&#20102;&#20854;&#25215;&#35834;&#65311;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23457;&#35745;&#26694;&#26550;&#65292;&#25552;&#20379;&#20102;&#23545;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22522;&#20110;&#23427;&#20204;&#35757;&#32451;&#30340;AI&#27169;&#22411;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#22260;&#32469;&#20559;&#35265;&#21644;&#27495;&#35270;&#30340;&#39044;&#38450;&#12289;&#23545;&#30495;&#23454;&#25968;&#25454;&#30340;&#24544;&#23454;&#31243;&#24230;&#12289;&#25928;&#29992;&#12289;&#40065;&#26834;&#24615;&#21644;&#38544;&#31169;&#20445;&#25252;&#12290;&#25105;&#20204;&#36890;&#36807;&#23457;&#35745;&#22810;&#20010;&#29983;&#25104;&#27169;&#22411;&#22312;&#19981;&#21516;&#29992;&#20363;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#25945;&#32946;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#38134;&#34892;&#12289;&#20154;&#21147;&#36164;&#28304;&#65292;&#20197;&#21450;&#20174;&#34920;&#26684;&#65292;&#26102;&#38388;&#24207;&#21015;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#19981;&#21516;&#27169;&#24577;&#12290;&#25105;&#20204;&#30340;&#29992;&#20363;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#20013;&#24179;&#34913;&#20449;&#20219;&#21644;&#25928;&#29992;&#30340;&#26435;&#34913;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data collected from the real world tends to be biased, unbalanced, and at risk of exposing sensitive and private information. This reality has given rise to the idea of creating synthetic datasets to alleviate risk, bias, harm, and privacy concerns inherent in the real data. This concept relies on Generative AI models to produce unbiased, privacy-preserving synthetic data while being true to the real data. In this new paradigm, how can we tell if this approach delivers on its promises? We present an auditing framework that offers a holistic assessment of synthetic datasets and AI models trained on them, centered around bias and discrimination prevention, fidelity to the real data, utility, robustness, and privacy preservation. We showcase our framework by auditing multiple generative models on diverse use cases, including education, healthcare, banking, human resources, and across different modalities, from tabular, to time-series, to natural language. Our use cases demonstrate the imp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;Segment Anything&#27169;&#22411;&#29992;&#20110;&#23569;&#26679;&#26412;&#29289;&#20307;&#35745;&#25968;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#24182;&#19982;&#20854;&#20182;&#35745;&#25968;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;SAM&#30340;&#34920;&#29616;&#20173;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23567;&#22411;&#21644;&#25317;&#25380;&#30340;&#29289;&#20307;&#12290;</title><link>http://arxiv.org/abs/2304.10817</link><description>&lt;p&gt;
SAM&#33021;&#22815;&#35745;&#25968;&#25152;&#26377;&#29289;&#20307;&#21527;&#65311;&#23545;SAM&#35745;&#25968;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can SAM Count Anything? An Empirical Study on SAM Counting. (arXiv:2304.10817v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;Segment Anything&#27169;&#22411;&#29992;&#20110;&#23569;&#26679;&#26412;&#29289;&#20307;&#35745;&#25968;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#24182;&#19982;&#20854;&#20182;&#35745;&#25968;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;SAM&#30340;&#34920;&#29616;&#20173;&#38656;&#36827;&#19968;&#27493;&#24494;&#35843;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23567;&#22411;&#21644;&#25317;&#25380;&#30340;&#29289;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Meta AI&#26368;&#36817;&#21457;&#24067;&#20102;Segment Anything&#27169;&#22411;(SAM)&#65292;&#30001;&#20110;&#22312;&#31867;&#26080;&#20851;&#20998;&#21106;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#35813;&#27169;&#22411;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;SAM&#29992;&#20110;&#23569;&#26679;&#26412;&#29289;&#20307;&#35745;&#25968;&#30340;&#25361;&#25112;&#24615;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#28041;&#21450;&#36890;&#36807;&#25552;&#20379;&#20960;&#20010;&#31034;&#20363;&#30340;&#36793;&#30028;&#26694;&#26469;&#35745;&#25968;&#26410;&#35265;&#36807;&#31867;&#21035;&#30340;&#29289;&#20307;&#12290;&#25105;&#20204;&#23558;SAM&#30340;&#24615;&#33021;&#19982;&#20854;&#20182;&#23569;&#26679;&#26412;&#35745;&#25968;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#21457;&#29616;&#22312;&#27809;&#26377;&#36827;&#19968;&#27493;&#24494;&#35843;&#30340;&#24773;&#20917;&#19979;&#65292;&#23427;&#30340;&#34920;&#29616;&#20173;&#28982;&#19981;&#20196;&#20154;&#28385;&#24847;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23567;&#22411;&#21644;&#25317;&#25380;&#30340;&#29289;&#20307;&#12290;&#20195;&#30721;&#21487;&#22312;\url{https://github.com/Vision-Intelligence-and-Robots-Group/count-anything}&#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta AI recently released the Segment Anything model (SAM), which has garnered attention due to its impressive performance in class-agnostic segmenting. In this study, we explore the use of SAM for the challenging task of few-shot object counting, which involves counting objects of an unseen category by providing a few bounding boxes of examples. We compare SAM's performance with other few-shot counting methods and find that it is currently unsatisfactory without further fine-tuning, particularly for small and crowded objects. Code can be found at \url{https://github.com/Vision-Intelligence-and-Robots-Group/count-anything}.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35774;&#35745;&#26377;&#24847;&#20041;&#21644;&#21487;&#35299;&#37322;&#30340;&#25552;&#31034;&#38598;&#65292;&#25552;&#39640;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10805</link><description>&lt;p&gt;
RPLKG: &#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
RPLKG: Robust Prompt Learning with Knowledge Graph. (arXiv:2304.10805v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#21160;&#35774;&#35745;&#26377;&#24847;&#20041;&#21644;&#21487;&#35299;&#37322;&#30340;&#25552;&#31034;&#38598;&#65292;&#25552;&#39640;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#21487;&#36801;&#31227;&#30340;&#65292;&#24182;&#19988;&#23545;&#26410;&#30693;&#25968;&#25454;&#38598;&#20855;&#26377;&#24456;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26368;&#36817;&#65292;&#35832;&#22914;CLIP&#20043;&#31867;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#26174;&#30528;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#28982;&#32780;&#65292;&#24403;&#26631;&#35760;&#25968;&#25454;&#38598;&#26377;&#38480;&#26102;&#65292;&#26032;&#25968;&#25454;&#38598;&#25110;&#39046;&#22495;&#30340;&#27867;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#23567;&#26679;&#26412;&#23398;&#20064;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24050;&#32463;&#36827;&#34892;&#20102;&#21508;&#31181;&#21162;&#21147;&#65292;&#22914;&#25552;&#31034;&#23398;&#20064;&#21644;&#36866;&#37197;&#22120;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#23569;&#26679;&#26412;&#33258;&#36866;&#24212;&#26041;&#27861;&#19981;&#20855;&#22791;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#39640;&#35745;&#31639;&#25104;&#26412;&#26469;&#36827;&#34892;&#33258;&#36866;&#24212;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#40065;&#26834;&#25552;&#31034;&#23398;&#20064;&#65288;RPLKG&#65289;&#12290;&#22522;&#20110;&#30693;&#35782;&#22270;&#35889;&#65292;&#25105;&#20204;&#33258;&#21160;&#35774;&#35745;&#20986;&#21508;&#31181;&#21487;&#35299;&#37322;&#21644;&#26377;&#24847;&#20041;&#30340;&#25552;&#31034;&#38598;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19968;&#27425;&#27491;&#21521;&#20256;&#36882;&#21518;&#33719;&#24471;&#25552;&#31034;&#38598;&#30340;&#32531;&#23384;&#23884;&#20837;&#12290;&#20043;&#21518;&#65292;&#27169;&#22411;&#20351;&#29992;GumbelSoftmax&#20248;&#21270;&#25552;&#31034;&#36873;&#25321;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale pre-trained models have been known that they are transferable, and they generalize well on the unseen dataset. Recently, multimodal pre-trained models such as CLIP show significant performance improvement in diverse experiments. However, when the labeled dataset is limited, the generalization of a new dataset or domain is still challenging. To improve the generalization performance on few-shot learning, there have been diverse efforts, such as prompt learning and adapter. However, the current few-shot adaptation methods are not interpretable, and they require a high computation cost for adaptation. In this study, we propose a new method, robust prompt learning with knowledge graph (RPLKG). Based on the knowledge graph, we automatically design diverse interpretable and meaningful prompt sets. Our model obtains cached embeddings of prompt sets after one forwarding from a large pre-trained model. After that, model optimizes the prompt selection processes with GumbelSoftmax. In
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CLASP&#30340;&#26041;&#27861;&#65292;&#23558;&#35821;&#35328;&#12289;&#21160;&#20316;&#21644;&#29366;&#24577;&#20449;&#24687;&#32479;&#19968;&#21040;&#20849;&#20139;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#12290;&#36890;&#36807;&#20998;&#24067;&#24335;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#19981;&#21516;&#30340;&#25991;&#26412;&#21629;&#20196;&#19982;&#21333;&#20010;&#34892;&#20026;&#30456;&#20851;&#32852;&#65292;&#24182;&#21453;&#20043;&#20134;&#28982;&#12290;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25991;&#26412;&#34892;&#20026;&#26816;&#32034;&#12289;&#20026;&#26410;&#35265;&#26426;&#22120;&#20154;&#34892;&#20026;&#21152;&#26631;&#39064;&#20197;&#21450;&#23398;&#20064;&#19968;&#20010;&#34892;&#20026;&#20808;&#39564;&#30693;&#35782;&#20197;&#36827;&#34892;&#35821;&#35328;&#20381;&#23384;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10782</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#23545;&#27604;&#35821;&#35328;&#12289;&#21160;&#20316;&#21644;&#29366;&#24577;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language, Action, and State Pre-training for Robot Learning. (arXiv:2304.10782v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;CLASP&#30340;&#26041;&#27861;&#65292;&#23558;&#35821;&#35328;&#12289;&#21160;&#20316;&#21644;&#29366;&#24577;&#20449;&#24687;&#32479;&#19968;&#21040;&#20849;&#20139;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#12290;&#36890;&#36807;&#20998;&#24067;&#24335;&#32534;&#30721;&#22120;&#23454;&#29616;&#20102;&#19981;&#21516;&#30340;&#25991;&#26412;&#21629;&#20196;&#19982;&#21333;&#20010;&#34892;&#20026;&#30456;&#20851;&#32852;&#65292;&#24182;&#21453;&#20043;&#20134;&#28982;&#12290;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#25991;&#26412;&#34892;&#20026;&#26816;&#32034;&#12289;&#20026;&#26410;&#35265;&#26426;&#22120;&#20154;&#34892;&#20026;&#21152;&#26631;&#39064;&#20197;&#21450;&#23398;&#20064;&#19968;&#20010;&#34892;&#20026;&#20808;&#39564;&#30693;&#35782;&#20197;&#36827;&#34892;&#35821;&#35328;&#20381;&#23384;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#23558;&#35821;&#35328;&#12289;&#21160;&#20316;&#21644;&#29366;&#24577;&#20449;&#24687;&#32479;&#19968;&#21040;&#20849;&#20139;&#30340;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#20197;&#20419;&#36827;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#19968;&#31995;&#21015;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;&#23545;&#27604;&#35821;&#35328;&#12289;&#21160;&#20316;&#21644;&#29366;&#24577;&#39044;&#35757;&#32451;&#65288;CLASP&#65289;&#65292;&#25193;&#23637;&#20102;CLIP&#20844;&#24335;&#65292;&#32467;&#21512;&#20998;&#24067;&#24335;&#23398;&#20064;&#65292;&#25429;&#25417;&#34892;&#20026;&#25991;&#26412;&#23545;&#40784;&#20013;&#30340;&#22266;&#26377;&#22797;&#26434;&#24615;&#21644;&#19968;&#23545;&#22810;&#20851;&#31995;&#12290;&#36890;&#36807;&#20026;&#25991;&#26412;&#21644;&#34892;&#20026;&#32534;&#30721;&#22120;&#25552;&#20379;&#20998;&#24067;&#36755;&#20986;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#26377;&#25928;&#22320;&#23558;&#19981;&#21516;&#30340;&#25991;&#26412;&#21629;&#20196;&#19982;&#21333;&#20010;&#34892;&#20026;&#30456;&#20851;&#32852;&#65292;&#24182;&#21453;&#20043;&#20134;&#28982;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20197;&#19979;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#23454;&#29992;&#24615;&#65306;&#38646;&#26679;&#26412;&#25991;&#26412;&#34892;&#20026;&#26816;&#32034;&#12289;&#20026;&#26410;&#35265;&#26426;&#22120;&#20154;&#34892;&#20026;&#21152;&#26631;&#39064;&#20197;&#21450;&#23398;&#20064;&#19968;&#20010;&#34892;&#20026;&#20808;&#39564;&#30693;&#35782;&#20197;&#36827;&#34892;&#35821;&#35328;&#20381;&#23384;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#20998;&#24067;&#24335;&#32534;&#30721;&#22120;&#22312;&#26410;&#35265;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#26816;&#32034;&#21644;&#26631;&#39064;&#24615;&#33021;&#65292;&#20197;&#21450;&#20174;&#25991;&#26412;&#21629;&#20196;&#29983;&#25104;&#26377;&#24847;&#20041;&#30340;&#25506;&#32034;&#24615;&#34892;&#20026;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a method for unifying language, action, and state information in a shared embedding space to facilitate a range of downstream tasks in robot learning. Our method, Contrastive Language, Action, and State Pre-training (CLASP), extends the CLIP formulation by incorporating distributional learning, capturing the inherent complexities and one-to-many relationships in behaviour-text alignment. By employing distributional outputs for both text and behaviour encoders, our model effectively associates diverse textual commands with a single behaviour and vice-versa. We demonstrate the utility of our method for the following downstream tasks: zero-shot text-behaviour retrieval, captioning unseen robot behaviours, and learning a behaviour prior for language-conditioned reinforcement learning. Our distributional encoders exhibit superior retrieval and captioning performance on unseen datasets, and the ability to generate meaningful exploratory behaviours from textual com
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;DEIR&#65292;&#20511;&#21161;&#21306;&#20998;&#24615;&#27169;&#22411;&#23454;&#29616;&#29702;&#35770;&#19978;&#23548;&#20986;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#33021;&#22815;&#39640;&#25928;&#19988;&#40065;&#26834;&#22320;&#36827;&#34892;&#25506;&#32034;&#65292;&#36866;&#29992;&#20110;&#38754;&#23545;&#22806;&#37096;&#22870;&#21169;&#31232;&#30095;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2304.10770</link><description>&lt;p&gt;
DEIR: &#22522;&#20110;&#21306;&#20998;&#24615;&#27169;&#22411;&#30340;&#24773;&#33410;&#20869;&#22312;&#22870;&#21169;&#65292;&#39640;&#25928;&#19988;&#40065;&#26834;&#30340;&#25506;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DEIR: Efficient and Robust Exploration through Discriminative-Model-Based Episodic Intrinsic Rewards. (arXiv:2304.10770v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10770
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#25506;&#32034;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;DEIR&#65292;&#20511;&#21161;&#21306;&#20998;&#24615;&#27169;&#22411;&#23454;&#29616;&#29702;&#35770;&#19978;&#23548;&#20986;&#30340;&#20869;&#22312;&#22870;&#21169;&#65292;&#33021;&#22815;&#39640;&#25928;&#19988;&#40065;&#26834;&#22320;&#36827;&#34892;&#25506;&#32034;&#65292;&#36866;&#29992;&#20110;&#38754;&#23545;&#22806;&#37096;&#22870;&#21169;&#31232;&#30095;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#26159;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#26041;&#38754;&#65292;&#20854;&#26377;&#25928;&#24615;&#20851;&#38190;&#22320;&#24433;&#21709;&#30528;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#38754;&#23545;&#31232;&#30095;&#30340;&#22806;&#37096;&#22870;&#21169;&#26102;&#26356;&#20026;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20174;&#35266;&#27979;&#20013;&#20272;&#35745;&#26032;&#39062;&#24615;&#30340;&#20869;&#22312;&#22870;&#21169;&#21487;&#20197;&#26377;&#25928;&#40723;&#21169;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#30340;&#38543;&#26426;&#24615;&#20197;&#21450;&#20195;&#29702;&#30340;&#34892;&#20026;&#21487;&#33021;&#20250;&#24433;&#21709;&#35266;&#23519;&#32467;&#26524;&#65292;&#22240;&#27492;&#19968;&#20010;&#35266;&#27979;&#30340;&#26032;&#39062;&#24615;&#19982;&#25506;&#32034;&#20043;&#38388;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#20934;&#30830;&#20272;&#35745;&#25506;&#32034;&#34892;&#20026;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DEIR&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#25105;&#20204;&#20174;&#26465;&#20214;&#20114;&#20449;&#24687;&#39033;&#20013;&#29702;&#35770;&#19978;&#23548;&#20986;&#20869;&#22312;&#22870;&#21169;&#65292;&#35813;&#22870;&#21169;&#20027;&#35201;&#19982;&#20195;&#29702;&#30340;&#25506;&#32034;&#34892;&#20026;&#25152;&#36129;&#29486;&#30340;&#26032;&#39062;&#24615;&#25104;&#27604;&#20363;&#65292;&#24182;&#20511;&#21161;&#21306;&#20998;&#24615;&#30340;&#21069;&#21521;&#27169;&#22411;&#23454;&#29616;&#22870;&#21169;&#12290;&#25105;&#20204;&#22312;MiniGrid&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;&#26631;&#20934;&#21644;&#30828;&#26680;&#25506;&#32034;&#28216;&#25103;&#65292;&#22312;&#32467;&#26524;&#19978;DEIR&#27604;&#22522;&#32447;&#23398;&#20064;&#26356;&#24555;&#24182;&#19988;&#20855;&#26377;&#26356;&#39640;&#30340;&#25104;&#21151;&#29575;&#21644;&#40065;&#26834;&#24615;&#65292;&#36866;&#24212;&#29615;&#22659;&#21160;&#24577;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exploration is a fundamental aspect of reinforcement learning (RL), and its effectiveness crucially decides the performance of RL algorithms, especially when facing sparse extrinsic rewards. Recent studies showed the effectiveness of encouraging exploration with intrinsic rewards estimated from novelty in observations. However, there is a gap between the novelty of an observation and an exploration in general, because the stochasticity in the environment as well as the behavior of an agent may affect the observation. To estimate exploratory behaviors accurately, we propose DEIR, a novel method where we theoretically derive an intrinsic reward from a conditional mutual information term that principally scales with the novelty contributed by agent explorations, and materialize the reward with a discriminative forward model. We conduct extensive experiments in both standard and hardened exploration games in MiniGrid to show that DEIR quickly learns a better policy than baselines. Our eval
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#38454;&#27573;&#26041;&#27861;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#25104;&#21151;&#21033;&#29992;&#20102;&#23569;&#37327;&#26377;&#38480;&#30340;&#26631;&#35760;&#30446;&#26631;&#26679;&#26412;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.10762</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#21452;&#38454;&#27573;&#26041;&#27861;&#65306;&#23454;&#29616;&#26631;&#35760;&#30446;&#26631;&#26679;&#26412;&#20215;&#20540;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Realizing the Value of Labeled Target Samples: a Two-Stage Approach for Semi-Supervised Domain Adaptation. (arXiv:2304.10762v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#38454;&#27573;&#26041;&#27861;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#25104;&#21151;&#21033;&#29992;&#20102;&#23569;&#37327;&#26377;&#38480;&#30340;&#26631;&#35760;&#30446;&#26631;&#26679;&#26412;&#65292;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;SSDA&#65289;&#26159;&#19968;&#20010;&#26032;&#20852;&#30340;&#30740;&#31350;&#20027;&#39064;&#65292;&#23427;&#26159;&#20174;&#24191;&#27867;&#30740;&#31350;&#30340;&#26080;&#30417;&#30563;&#39046;&#22495;&#36866;&#24212;&#65288;UDA&#65289;&#25193;&#23637;&#32780;&#26469;&#65292;&#36890;&#36807;&#36827;&#19968;&#27493;&#26631;&#35760;&#19968;&#20123;&#30446;&#26631;&#26679;&#26412;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#21363;&#20351;&#29992;&#26631;&#35760;&#30340;&#28304;&#26679;&#26412;&#12289;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#26679;&#26412;&#20197;&#21450;&#23569;&#37327;&#26631;&#35760;&#30340;&#30446;&#26631;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#12290;&#19982;UDA&#30456;&#27604;&#65292;SSDA&#30340;&#20851;&#38190;&#22312;&#20110;&#22914;&#20309;&#26368;&#26377;&#25928;&#22320;&#21033;&#29992;&#19968;&#20123;&#26377;&#38480;&#30340;&#26631;&#35760;&#30446;&#26631;&#26679;&#26412;&#12290;&#29616;&#26377;&#30340;SSDA&#26041;&#27861;&#23558;&#23569;&#25968;&#23453;&#36149;&#30340;&#26631;&#35760;&#30446;&#26631;&#26679;&#26412;&#31616;&#21333;&#21512;&#24182;&#21040;&#22823;&#37327;&#30340;&#26631;&#35760;&#28304;&#26679;&#26412;&#20013;&#25110;&#36827;&#19968;&#27493;&#23545;&#40784;&#23427;&#20204;&#65292;&#36825;&#20250;&#21066;&#24369;&#26631;&#35760;&#30446;&#26631;&#26679;&#26412;&#30340;&#20215;&#20540;&#65292;&#22240;&#27492;&#20173;&#28982;&#20250;&#24471;&#21040;&#26377;&#20559;&#24046;&#30340;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;SSDA&#20998;&#35299;&#20026;UDA&#38382;&#39064;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#26631;&#35760;&#28304;&#21644;&#26410;&#26631;&#35760;&#30446;&#26631;&#26679;&#26412;&#23398;&#20064;UDA&#27169;&#22411;&#65292;&#28982;&#21518;&#20877;&#20351;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30446;&#26631;&#26679;&#26412;&#20197;&#21322;&#30417;&#30563;&#30340;&#26041;&#24335;&#35843;&#25972;&#25152;&#23398;&#30340;UDA&#27169;&#22411;&#12290;&#36890;&#36807;&#21033;&#29992;&#26631;&#35760;&#28304;&#26679;&#26412;&#65292;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#21322;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-Supervised Domain Adaptation (SSDA) is a recently emerging research topic that extends from the widely-investigated Unsupervised Domain Adaptation (UDA) by further having a few target samples labeled, i.e., the model is trained with labeled source samples, unlabeled target samples as well as a few labeled target samples. Compared with UDA, the key to SSDA lies how to most effectively utilize the few labeled target samples. Existing SSDA approaches simply merge the few precious labeled target samples into vast labeled source samples or further align them, which dilutes the value of labeled target samples and thus still obtains a biased model. To remedy this, in this paper, we propose to decouple SSDA as an UDA problem and a semi-supervised learning problem where we first learn an UDA model using labeled source and unlabeled target samples and then adapt the learned UDA model in a semi-supervised way using labeled and unlabeled target samples. By utilizing the labeled source samples
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#33041;&#30005;&#22270;&#31995;&#32479;&#20013;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;AI&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20854;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#20998;&#31867;&#27861;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#40065;&#26834;AI&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.10755</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;&#33041;&#30005;&#22270;AI&#31995;&#32479;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Interpretable and Robust AI in EEG Systems: A Survey. (arXiv:2304.10755v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10755
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#36817;&#24180;&#26469;&#33041;&#30005;&#22270;&#31995;&#32479;&#20013;&#21487;&#35299;&#37322;&#21644;&#40065;&#26834;&#30340;AI&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#20854;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#20998;&#31867;&#27861;&#65292;&#35814;&#32454;&#20171;&#32461;&#20102;&#40065;&#26834;AI&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#26102;&#20195;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#30340;&#23494;&#20999;&#32806;&#21512;&#26497;&#22823;&#22320;&#25512;&#21160;&#20102;&#20154;&#26426;&#20132;&#20114;&#65288;HCI&#65289;&#25216;&#26415;&#30340;&#21457;&#23637;&#12290;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;EEG&#31995;&#32479;&#65292;&#22522;&#20110;AI&#30340;EEG&#31995;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#21464;&#24471;&#23588;&#20026;&#20851;&#38190;&#12290;&#21487;&#35299;&#37322;&#24615;&#33021;&#22815;&#38416;&#37322;AI&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#65292;&#22240;&#27492;&#21487;&#20197;&#33719;&#24471;&#29992;&#25143;&#30340;&#20449;&#20219;&#12290;&#40065;&#26834;&#24615;&#21017;&#21453;&#26144;&#20102;AI&#23545;&#25239;&#25915;&#20987;&#21644;&#25200;&#21160;&#30340;&#21487;&#38752;&#24615;&#65292;&#36825;&#23545;&#20110;&#25935;&#24863;&#21644;&#33030;&#24369;&#30340;EEG&#20449;&#21495;&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#22240;&#27492;&#65292;EEG&#31995;&#32479;&#20013;AI&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#24615;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#19988;&#26368;&#36817;&#30340;&#30740;&#31350;&#21462;&#24471;&#20102;&#24040;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#20173;&#28982;&#27809;&#26377;&#32508;&#36848;&#12290;&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#24615;&#20998;&#31867;&#27861;&#65292;&#36890;&#36807;&#29305;&#24449;&#21270;&#27169;&#22411;&#12289;&#25968;&#25454;&#21644;&#36755;&#20986;&#35299;&#37322;&#24615;&#65292;&#24635;&#32467;&#20102;&#33041;&#30005;&#22270;&#31995;&#32479;&#20013;&#35299;&#37322;&#24615;&#21644;&#40065;&#26834;&#30340;AI&#25216;&#26415;&#65292;&#24182;&#35814;&#32454;&#20171;&#32461;&#20102;&#40065;&#26834;AI&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#23545;&#25239;&#25915;&#20987;&#21644;&#38450;&#24481;&#12289;&#36801;&#31227;&#23398;&#20064;&#21644;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#19968;&#39046;&#22495;&#26410;&#26469;&#30340;&#26041;&#21521;&#21644;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
The close coupling of artificial intelligence (AI) and electroencephalography (EEG) has substantially advanced human-computer interaction (HCI) technologies in the AI era. Different from traditional EEG systems, the interpretability and robustness of AI-based EEG systems are becoming particularly crucial. The interpretability clarifies the inner working mechanisms of AI models and thus can gain the trust of users. The robustness reflects the AI's reliability against attacks and perturbations, which is essential for sensitive and fragile EEG signals. Thus the interpretability and robustness of AI in EEG systems have attracted increasing attention, and their research has achieved great progress recently. However, there is still no survey covering recent advances in this field. In this paper, we present the first comprehensive survey and summarize the interpretable and robust AI techniques for EEG systems. Specifically, we first propose a taxonomy of interpretability by characterizing it 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#39044;&#27979;&#36941;&#21382;&#24615;&#65288;forecast ergodicity&#65289;&#30340;&#27010;&#24565;&#65292;&#21363;&#20174;&#36807;&#21435;&#25968;&#25454;&#20013;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#33021;&#21147;&#30340;&#24230;&#37327;&#12290;&#25991;&#31456;&#20351;&#29992;&#31639;&#27861;&#22797;&#26434;&#24615;&#27169;&#25311;&#36825;&#20010;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2304.10752</link><description>&lt;p&gt;
&#39044;&#27979;&#36941;&#21382;&#24615;&#65306;&#20351;&#29992;&#31639;&#27861;&#20449;&#24687;&#35770;&#36827;&#34892;&#39044;&#27979;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Forecast Ergodicity: Prediction Modeling Using Algorithmic Information Theory. (arXiv:2304.10752v1 [cs.IT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10752
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35752;&#35770;&#20102;&#39044;&#27979;&#36941;&#21382;&#24615;&#65288;forecast ergodicity&#65289;&#30340;&#27010;&#24565;&#65292;&#21363;&#20174;&#36807;&#21435;&#25968;&#25454;&#20013;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#33021;&#21147;&#30340;&#24230;&#37327;&#12290;&#25991;&#31456;&#20351;&#29992;&#31639;&#27861;&#22797;&#26434;&#24615;&#27169;&#25311;&#36825;&#20010;&#33021;&#21147;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#26234;&#33021;&#30340;&#33021;&#21147;&#21463;&#21040;&#36807;&#21435;&#25968;&#25454;&#39044;&#27979;&#26410;&#26469;&#30340;&#28508;&#21147;&#30340;&#38480;&#21046;&#12290;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#29992;&#20110;&#21457;&#29616;&#21487;&#29992;&#25968;&#25454;&#20013;&#30340;&#32467;&#26500;&#65292;&#20197;&#20415;&#39044;&#27979;&#26410;&#26469;&#12290;&#20294;&#36825;&#20123;&#32467;&#26500;&#24517;&#39035;&#39318;&#20808;&#23384;&#22312;&#20110;&#21487;&#29992;&#25968;&#25454;&#20013;&#65292;&#24182;&#19988;&#22312;&#26410;&#26469;&#20063;&#24517;&#39035;&#36866;&#29992;&#12290;&#39044;&#27979;&#36941;&#21382;&#24615;&#26159;&#20174;&#36807;&#21435;&#25968;&#25454;&#20013;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#30340;&#33021;&#21147;&#30340;&#24230;&#37327;&#12290;&#25105;&#20204;&#36890;&#36807;&#21487;&#29992;&#25968;&#25454;&#30340;&#31639;&#27861;&#22797;&#26434;&#24615;&#26469;&#27169;&#25311;&#36825;&#20010;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
The capabilities of machine intelligence are bounded by the potential of data from the past to forecast the future. Deep learning tools are used to find structures in the available data to make predictions about the future. Such structures have to be present in the available data in the first place and they have to be applicable in the future. Forecast ergodicity is a measure of the ability to forecast future events from data in the past. We model this bound by the algorithmic complexity of the available data.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#20114;&#21160;&#21453;&#39304;&#19982;&#20195;&#29702;&#20132;&#20114;&#26469;&#25552;&#39640;&#21327;&#20316;&#29615;&#22659;&#19979;&#22522;&#20110;&#23454;&#22320;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.10750</link><description>&lt;p&gt;
&#36890;&#36807;&#20114;&#21160;&#21453;&#39304;&#19982;&#20195;&#29702;&#20132;&#20114;&#26469;&#25552;&#39640;&#21327;&#20316;&#29615;&#22659;&#19979;&#22522;&#20110;&#23454;&#22320;&#29702;&#35299;&#65288;Grounded Language Understanding&#65289;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Grounded Language Understanding in a Collaborative Environment by Interacting with Agents Through Help Feedback. (arXiv:2304.10750v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10750
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#20114;&#21160;&#21453;&#39304;&#19982;&#20195;&#29702;&#20132;&#20114;&#26469;&#25552;&#39640;&#21327;&#20316;&#29615;&#22659;&#19979;&#22522;&#20110;&#23454;&#22320;&#29702;&#35299;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#36890;&#24120;&#34987;&#35270;&#20026;&#21333;&#27493;&#38382;&#39064;&#12290;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#65292;&#20195;&#29702;&#25509;&#25910;&#19968;&#20010;&#25351;&#20196;&#65292;&#25191;&#34892;&#23427;&#65292;&#28982;&#21518;&#26681;&#25454;&#26368;&#32456;&#32467;&#26524;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#35821;&#35328;&#26412;&#36136;&#19978;&#26159;&#20132;&#20114;&#24335;&#30340;&#65292;&#25105;&#20204;&#20027;&#24352;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#30340;&#21327;&#20316;&#20063;&#24212;&#26159;&#20132;&#20114;&#24335;&#30340;&#65292;&#20154;&#31867;&#30417;&#30563;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#24037;&#20316;&#65292;&#24182;&#25552;&#20379;&#20195;&#29702;&#21487;&#20197;&#29702;&#35299;&#21644;&#21033;&#29992;&#30340;&#21453;&#39304;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;Help Feedback&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many approaches to Natural Language Processing (NLP) tasks often treat them as single-step problems, where an agent receives an instruction, executes it, and is evaluated based on the final outcome. However, human language is inherently interactive, as evidenced by the back-and-forth nature of human conversations. In light of this, we posit that human-AI collaboration should also be interactive, with humans monitoring the work of AI agents and providing feedback that the agent can understand and utilize. Further, the AI agent should be able to detect when it needs additional information and proactively ask for help. Enabling this scenario would lead to more natural, efficient, and engaging human-AI collaborations.  In this work, we explore these directions using the challenging task defined by the IGLU competition, an interactive grounded language understanding task in a MineCraft-like world. We explore multiple types of help players can give to the AI to guide it and analyze the impac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;--&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#24494;&#35266;&#12289;&#20013;&#35266;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#33041;&#25299;&#25169;&#32467;&#26500;&#12290;MSE-NAS&#21487;&#20197;&#24110;&#21161;SNN&#23454;&#29616;&#22810;&#30005;&#36335;&#27169;&#24335;&#30340;&#33258;&#32452;&#32455;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#24615;&#30340;&#36328;&#27169;&#24335;&#36830;&#25509;&#26469;&#20248;&#21270;&#32593;&#32476;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10749</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#29992;&#20110;&#28145;&#24230;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Evolutionary Neural Architecture Search for Deep Spiking Neural Networks. (arXiv:2304.10749v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;--&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#24494;&#35266;&#12289;&#20013;&#35266;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#33041;&#25299;&#25169;&#32467;&#26500;&#12290;MSE-NAS&#21487;&#20197;&#24110;&#21161;SNN&#23454;&#29616;&#22810;&#30005;&#36335;&#27169;&#24335;&#30340;&#33258;&#32452;&#32455;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#24615;&#30340;&#36328;&#27169;&#24335;&#36830;&#25509;&#26469;&#20248;&#21270;&#32593;&#32476;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19981;&#20165;&#22240;&#20854;&#31163;&#25955;&#20449;&#21495;&#22788;&#29702;&#30340;&#33021;&#28304;&#25928;&#29575;&#21331;&#36234;&#65292;&#32780;&#19988;&#22240;&#20854;&#22825;&#28982;&#36866;&#21512;&#20110;&#38598;&#25104;&#22810;&#23610;&#24230;&#29983;&#29289;&#21487;&#22609;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;SNN&#30452;&#25509;&#37319;&#29992;&#25104;&#29087;&#30340;DNN&#32467;&#26500;&#65292;&#24456;&#23569;&#33258;&#21160;&#35774;&#35745;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#29992;&#20110;SNN&#12290;&#20154;&#31867;&#22823;&#33041;&#31070;&#32463;&#27169;&#24335;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#27169;&#22359;&#21270;&#30340;&#21306;&#22495;&#32467;&#26500;&#21644;&#20840;&#23616;&#24615;&#30340;&#36328;&#33041;&#21306;&#36830;&#25509;&#26159;&#33258;&#28982;&#36827;&#21270;&#30340;&#20135;&#29289;&#65292;&#21487;&#20197;&#20316;&#20026;&#35774;&#35745;&#22522;&#20110;&#33041;&#30340;SNN&#26550;&#26500;&#30340;&#23436;&#32654;&#21442;&#32771;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;MSE-NAS&#65289;&#65292;&#21516;&#26102;&#32771;&#34385;&#24494;&#35266;&#12289;&#20013;&#35266;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#33041;&#25299;&#25169;&#20316;&#20026;&#36827;&#21270;&#25628;&#32034;&#31354;&#38388;&#12290; MSE-NAS&#36890;&#36807;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#38388;&#25509;&#26041;&#24335;&#65292;&#36827;&#21270;&#21333;&#20010;&#31070;&#32463;&#20803;&#25805;&#20316;&#65292;&#22810;&#20010;&#30005;&#36335;&#27169;&#24335;&#30340;&#33258;&#32452;&#32455;&#38598;&#25104;&#20197;&#21450;&#36328;&#27169;&#24335;&#30340;&#20840;&#23616;&#36830;&#36890;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have received considerable attention not only for their superiority in energy efficient with discrete signal processing, but also for their natural suitability to integrate multi-scale biological plasticity. However, most SNNs directly adopt the structure of the well-established DNN, rarely automatically design Neural Architecture Search (NAS) for SNNs. The neural motifs topology, modular regional structure and global cross-brain region connection of the human brain are the product of natural evolution and can serve as a perfect reference for designing brain-inspired SNN architecture. In this paper, we propose a Multi-Scale Evolutionary Neural Architecture Search (MSE-NAS) for SNN, simultaneously considering micro-, meso- and macro-scale brain topologies as the evolutionary search space. MSE-NAS evolves individual neuron operation, self-organized integration of multiple circuit motifs, and global connectivity across motifs through a brain-inspired indirec
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FedEx&#26694;&#26550;&#65292;&#21033;&#29992;&#26080;&#20154;&#26426;&#31561;&#31227;&#21160;&#20256;&#36755;&#22120;&#24314;&#31435;&#38388;&#25509;&#36890;&#20449;&#36890;&#36947;&#65292;&#22312;&#35299;&#20915;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#26080;&#27861;&#30452;&#25509;&#36890;&#20449;&#30340;&#38382;&#39064;&#26102;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23458;&#25143;&#31471;&#20998;&#37197;&#21644;&#26080;&#20154;&#26426;&#36335;&#24452;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#26368;&#23567;&#21270;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2304.10744</link><description>&lt;p&gt;
&#22522;&#20110;&#38388;&#25509;&#36890;&#20449;&#30340;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23458;&#25143;&#31471;&#20998;&#37197;&#21644;&#26080;&#20154;&#26426;&#36335;&#24452;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Joint Client Assignment and UAV Route Planning for Indirect-Communication Federated Learning. (arXiv:2304.10744v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10744
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;FedEx&#26694;&#26550;&#65292;&#21033;&#29992;&#26080;&#20154;&#26426;&#31561;&#31227;&#21160;&#20256;&#36755;&#22120;&#24314;&#31435;&#38388;&#25509;&#36890;&#20449;&#36890;&#36947;&#65292;&#22312;&#35299;&#20915;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#26080;&#27861;&#30452;&#25509;&#36890;&#20449;&#30340;&#38382;&#39064;&#26102;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23458;&#25143;&#31471;&#20998;&#37197;&#21644;&#26080;&#20154;&#26426;&#36335;&#24452;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#26368;&#23567;&#21270;&#25972;&#20307;&#35757;&#32451;&#26102;&#38388;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#21019;&#24314;&#29992;&#20110;&#24378;&#22823;&#24212;&#29992;&#31243;&#24207;&#30340;&#20849;&#20139;&#27169;&#22411;&#65292;&#21516;&#26102;&#20801;&#35768;&#25968;&#25454;&#20445;&#30041;&#22312;&#35774;&#22791;&#19978;&#65292;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#25968;&#25454;&#38544;&#31169;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#38477;&#20302;&#30340;&#24310;&#36831;&#31561;&#20248;&#28857;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#31995;&#32479;&#20013;&#65292;&#23458;&#25143;&#31471;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#30340;&#30452;&#25509;&#36890;&#20449;&#21487;&#33021;&#19981;&#21487;&#33021;&#65292;&#20363;&#22914;&#27809;&#26377;&#36866;&#24403;&#36890;&#20449;&#22522;&#30784;&#35774;&#26045;&#30340;&#36828;&#31243;&#22320;&#21306;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550; FedEx&#65288;&#36890;&#36807;&#27169;&#22411;&#20256;&#36882;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#65289;&#65292;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#31227;&#21160;&#20256;&#36755;&#22120;&#65292;&#22914;&#26080;&#20154;&#26426;&#65292;&#24314;&#31435;&#20102;&#26381;&#21153;&#22120;&#21644;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#38388;&#25509;&#36890;&#20449;&#36890;&#36947;&#12290;&#36825;&#20123;&#20256;&#36755;&#22120;&#20316;&#20026;&#20013;&#20171;&#65292;&#20801;&#35768;&#27169;&#22411;&#20449;&#24687;&#30340;&#20132;&#25442;&#12290;&#38388;&#25509;&#36890;&#20449;&#30340;&#20351;&#29992;&#20026;&#25910;&#25947;&#20998;&#26512;&#21644;&#20248;&#21270;&#24102;&#26469;&#20102;&#26032;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20256;&#36755;&#22120;&#36816;&#21160;&#24341;&#20837;&#30340;&#24310;&#36831;&#23545;&#20840;&#23616;&#27169;&#22411;&#20998;&#21457;&#21644;&#26412;&#22320;&#27169;&#22411;&#25910;&#38598;&#37117;&#20250;&#20135;&#29983;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32852;&#21512;&#23458;&#25143;&#31471;&#20998;&#37197;&#21644;&#26080;&#20154;&#26426;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#31227;&#21160;&#20256;&#36755;&#22120;&#30340;&#20351;&#29992;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#24635;&#20307;&#35757;&#32451;&#26102;&#38388;&#21644;&#36890;&#20449;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated Learning (FL) is a machine learning approach that enables the creation of shared models for powerful applications while allowing data to remain on devices. This approach provides benefits such as improved data privacy, security, and reduced latency. However, in some systems, direct communication between clients and servers may not be possible, such as remote areas without proper communication infrastructure. To overcome this challenge, a new framework called FedEx (Federated Learning via Model Express Delivery) is proposed. This framework employs mobile transporters, such as UAVs, to establish indirect communication channels between the server and clients. These transporters act as intermediaries and allow for model information exchange. The use of indirect communication presents new challenges for convergence analysis and optimization, as the delay introduced by the transporters' movement creates issues for both global model dissemination and local model collection. To addre
&lt;/p&gt;</description></item><item><title>KitchenScale&#26159;&#19968;&#20010;&#32463;&#36807;Fine-tuned&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#21487;&#26681;&#25454;&#39135;&#35889;&#19978;&#19979;&#25991;&#39044;&#27979;&#30446;&#26631;&#25104;&#20998;&#30340;&#25968;&#37327;&#21644;&#27979;&#37327;&#21333;&#20301;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#31163;&#25955;&#28508;&#22312;&#25351;&#25968;&#65288;DExp&#65289;&#26041;&#27861;&#22788;&#29702;&#39135;&#35889;&#35821;&#26009;&#24211;&#20013;&#25968;&#23383;&#23610;&#24230;&#30340;&#39640;&#26041;&#24046;&#65292;&#23581;&#35797;&#20174;&#39135;&#35889;&#25991;&#26412;&#21040;PLMs&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#22312;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#21644;&#25512;&#33616;&#31034;&#20363;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;KitchenScale&#20855;&#26377;&#27867;&#21270;&#24615;&#24182;&#21487;&#20197;&#29702;&#35299;&#21508;&#31181;&#39135;&#35889;&#35821;&#22659;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#26469;&#20026;&#29992;&#25143;&#25552;&#20379;&#25152;&#38656;&#30340;&#39135;&#21697;&#37327;&#30340;&#37197;&#26041;&#29305;&#23450;&#30340;&#27979;&#37327;&#21333;&#20301;&#12290;</title><link>http://arxiv.org/abs/2304.10739</link><description>&lt;p&gt;
KitchenScale: &#20174;&#39135;&#35889;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#39044;&#27979;&#25104;&#20998;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
KitchenScale: Learning to predict ingredient quantities from recipe contexts. (arXiv:2304.10739v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10739
&lt;/p&gt;
&lt;p&gt;
KitchenScale&#26159;&#19968;&#20010;&#32463;&#36807;Fine-tuned&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#21487;&#26681;&#25454;&#39135;&#35889;&#19978;&#19979;&#25991;&#39044;&#27979;&#30446;&#26631;&#25104;&#20998;&#30340;&#25968;&#37327;&#21644;&#27979;&#37327;&#21333;&#20301;&#12290;&#35813;&#27169;&#22411;&#37319;&#29992;&#31163;&#25955;&#28508;&#22312;&#25351;&#25968;&#65288;DExp&#65289;&#26041;&#27861;&#22788;&#29702;&#39135;&#35889;&#35821;&#26009;&#24211;&#20013;&#25968;&#23383;&#23610;&#24230;&#30340;&#39640;&#26041;&#24046;&#65292;&#23581;&#35797;&#20174;&#39135;&#35889;&#25991;&#26412;&#21040;PLMs&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#22312;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#21644;&#25512;&#33616;&#31034;&#20363;&#19978;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;KitchenScale&#20855;&#26377;&#27867;&#21270;&#24615;&#24182;&#21487;&#20197;&#29702;&#35299;&#21508;&#31181;&#39135;&#35889;&#35821;&#22659;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#26469;&#20026;&#29992;&#25143;&#25552;&#20379;&#25152;&#38656;&#30340;&#39135;&#21697;&#37327;&#30340;&#37197;&#26041;&#29305;&#23450;&#30340;&#27979;&#37327;&#21333;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28921;&#39274;&#23454;&#36341;&#20013;&#65292;&#30830;&#23450;&#25104;&#20998;&#30340;&#36866;&#24403;&#37327;&#23545;&#20110;&#20016;&#23500;&#21475;&#24863;&#21644;&#20419;&#36827;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;KitchenScale&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;Fine-tuned&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#65292;&#26681;&#25454;&#39135;&#35889;&#19978;&#19979;&#25991;&#39044;&#27979;&#30446;&#26631;&#25104;&#20998;&#30340;&#25968;&#37327;&#21644;&#27979;&#37327;&#21333;&#20301;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#35757;&#32451;&#25105;&#20204;&#30340;KitchenScale&#27169;&#22411;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#21253;&#25324;&#19977;&#20010;&#23376;&#20219;&#21153;&#30340;&#25104;&#20998;&#37327;&#39044;&#27979;&#20219;&#21153;&#65292;&#36825;&#20123;&#23376;&#20219;&#21153;&#26159;&#25104;&#20998;&#27979;&#37327;&#31867;&#22411;&#20998;&#31867;&#12289;&#21333;&#20301;&#20998;&#31867;&#21644;&#25968;&#37327;&#22238;&#24402;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#20174;&#39135;&#35889;&#25991;&#26412;&#21040;PLMs&#30340;&#28921;&#39274;&#30693;&#35782;&#30340;&#36716;&#31227;&#23398;&#20064;&#12290;&#25105;&#20204;&#37319;&#29992;&#20102;&#31163;&#25955;&#28508;&#22312;&#25351;&#25968;&#65288;DExp&#65289;&#26041;&#27861;&#26469;&#24212;&#23545;&#39135;&#35889;&#35821;&#26009;&#24211;&#20013;&#25968;&#23383;&#23610;&#24230;&#30340;&#39640;&#26041;&#24046;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#26032;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;&#21644;&#25512;&#33616;&#31034;&#20363;&#36827;&#34892;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;KitchenScale&#29702;&#35299;&#21508;&#31181;&#39135;&#35889;&#35821;&#22659;&#20197;&#21450;&#22312;&#39044;&#27979;&#25104;&#20998;&#37327;&#26041;&#38754;&#20855;&#26377;&#27867;&#21270;&#24615;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;Web&#24212;&#29992;&#31243;&#24207;&#65292;&#20026;&#29992;&#25143;&#25552;&#20379;&#25152;&#38656;&#30340;&#29992;&#20110;&#25152;&#38656;&#20154;&#25968;&#39135;&#21697;&#37327;&#30340;&#37197;&#26041;&#29305;&#23450;&#30340;&#27979;&#37327;&#21333;&#20301;&#12290;
&lt;/p&gt;
&lt;p&gt;
Determining proper quantities for ingredients is an essential part of cooking practice from the perspective of enriching tastiness and promoting healthiness. We introduce KitchenScale, a fine-tuned Pre-trained Language Model (PLM) that predicts a target ingredient's quantity and measurement unit given its recipe context. To effectively train our KitchenScale model, we formulate an ingredient quantity prediction task that consists of three sub-tasks which are ingredient measurement type classification, unit classification, and quantity regression task. Furthermore, we utilized transfer learning of cooking knowledge from recipe texts to PLMs. We adopted the Discrete Latent Exponent (DExp) method to cope with high variance of numerical scales in recipe corpora. Experiments with our newly constructed dataset and recommendation examples demonstrate KitchenScale's understanding of various recipe contexts and generalizability in predicting ingredient quantities. We implemented a web applicati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;</title><link>http://arxiv.org/abs/2304.10727</link><description>&lt;p&gt;
RoCOCO&#65306;&#31283;&#20581;&#30340;&#22522;&#20934;MS-COCO&#35780;&#20272;&#22270;&#25991;&#21305;&#37197;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
RoCOCO: Robust Benchmark MS-COCO to Stress-test Robustness of Image-Text Matching Models. (arXiv:2304.10727v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10727
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#36890;&#36807;&#23558;&#19968;&#20123;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#65292;&#22312;MS COCO&#25968;&#25454;&#38598;&#19978;&#20026;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#40065;&#26834;&#24615;&#27979;&#35797;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#30340;&#19981;&#36275;&#20043;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#35270;&#35273;&#35821;&#20041;&#23884;&#20837;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;MS COCO 5K&#27979;&#35797;&#38598;&#19978;&#22270;&#25991;&#21305;&#37197;&#65288;ITM&#65289;&#30340;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#24403;&#23558;&#36825;&#20123;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#29992;&#20110;&#23454;&#38469;&#24212;&#29992;&#26102;&#65292;&#23427;&#20204;&#30340;&#40065;&#26834;&#24615;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#22522;&#20934;&#26469;&#27979;&#35797;ITM&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#21508;&#31181;&#8220;&#24858;&#24324;&#8221;&#30340;&#22270;&#29255;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#26816;&#32034;&#27744;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25554;&#20837;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#26469;&#26356;&#25913;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#26367;&#25442;&#21517;&#35789;&#26469;&#26356;&#25913;&#26631;&#39064;&#65292;&#20174;&#32780;&#25913;&#21464;&#21477;&#23376;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20165;&#20165;&#23558;&#36825;&#20123;&#26032;&#21019;&#24314;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#28155;&#21152;&#21040;&#27979;&#35797;&#38598;&#20013;&#23601;&#21487;&#20197;&#38477;&#20302;&#21508;&#31181;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#24615;&#33021;&#65288;&#20363;&#22914;&#65292;&#22312;BLIP&#20013;&#20174;81.9&#65285;&#38477;&#33267;64.5&#65285;&#65292;&#22312;VSE&#8734;&#20013;&#20174;66.1&#65285;&#38477;&#33267;37.5&#65285;&#65289;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#21457;&#29616;&#33021;&#20026;&#25552;&#39640;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#21644;&#35774;&#35745;&#26356;&#22810;&#26679;&#21270;&#30340;&#21387;&#21147;&#27979;&#35797;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, large-scale vision-language pre-training models and visual semantic embedding methods have significantly improved image-text matching (ITM) accuracy on MS COCO 5K test set. However, it is unclear how robust these state-of-the-art (SOTA) models are when using them in the wild. In this paper, we propose a novel evaluation benchmark to stress-test the robustness of ITM models. To this end, we add various fooling images and captions to a retrieval pool. Specifically, we change images by inserting unrelated images, and change captions by substituting a noun, which can change the meaning of a sentence. We discover that just adding these newly created images and captions to the test set can degrade performances (i.e., Recall@1) of a wide range of SOTA models (e.g., 81.9% $\rightarrow$ 64.5% in BLIP, 66.1% $\rightarrow$ 37.5% in VSE$\infty$). We expect that our findings can provide insights for improving the robustness of the vision-language models and devising more diverse stress-te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20132;&#36890;&#36335;&#32593;&#20013;&#32570;&#23569;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#34917;&#20805;&#27969;&#37327;&#29366;&#24577;&#25110;&#29366;&#24577;&#21644;&#21160;&#20316;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#25511;&#21046;&#21644;&#26465;&#20214;&#34701;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.10722</link><description>&lt;p&gt;
&#32570;&#22833;&#25968;&#25454;&#19979;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning Approaches for Traffic Signal Control under Missing Data. (arXiv:2304.10722v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#20132;&#36890;&#36335;&#32593;&#20013;&#32570;&#23569;&#20256;&#24863;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#34917;&#20805;&#27969;&#37327;&#29366;&#24577;&#25110;&#29366;&#24577;&#21644;&#21160;&#20316;&#26469;&#23454;&#29616;&#33258;&#36866;&#24212;&#25511;&#21046;&#21644;&#26465;&#20214;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#24050;&#32463;&#21462;&#24471;&#20102;&#27604;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#20132;&#36890;&#29366;&#24577;&#30340;&#32570;&#22833;&#21487;&#33021;&#32463;&#24120;&#21457;&#29983;&#65292;&#20351;&#24471;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#32570;&#23569;&#20256;&#24863;&#22120;&#30340;&#36335;&#32593;&#19978;&#26080;&#27861;&#24212;&#29992;&#12290;&#26412;&#25991;&#26088;&#22312;&#25511;&#21046;&#20132;&#36890;&#20449;&#21495;&#22312;&#29616;&#23454;&#29615;&#22659;&#19979;&#30340;&#35774;&#32622;&#20013;&#65292;&#20854;&#20013;&#19968;&#20123;&#36335;&#21475;&#27809;&#26377;&#23433;&#35013;&#20256;&#24863;&#22120;&#65292;&#22240;&#27492;&#21608;&#22260;&#27809;&#26377;&#30452;&#25509;&#35266;&#23519;&#25968;&#25454;&#12290;&#22312;&#25105;&#20204;&#25152;&#30693;&#36947;&#30340;&#33539;&#22260;&#20869;&#65292;&#25105;&#20204;&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#38382;&#39064;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#35299;&#20915;&#26041;&#26696;&#65306;&#31532;&#19968;&#31181;&#26041;&#26696;&#34917;&#20805;&#27969;&#37327;&#29366;&#24577;&#20197;&#23454;&#29616;&#33258;&#36866;&#24212;&#25511;&#21046;&#65292;&#31532;&#20108;&#31181;&#26041;&#26696;&#34917;&#20805;&#29366;&#24577;&#21644;&#21160;&#20316;&#20197;&#36827;&#34892;&#26465;&#20214;&#34701;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The emergence of reinforcement learning (RL) methods in traffic signal control tasks has achieved better performance than conventional rule-based approaches. Most RL approaches require the observation of the environment for the agent to decide which action is optimal for a long-term reward. However, in real-world urban scenarios, missing observation of traffic states may frequently occur due to the lack of sensors, which makes existing RL methods inapplicable on road networks with missing observation. In this work, we aim to control the traffic signals in a real-world setting, where some of the intersections in the road network are not installed with sensors and thus with no direct observations around them. To the best of our knowledge, we are the first to use RL methods to tackle the traffic signal control problem in this real-world setting. Specifically, we propose two solutions: the first one imputes the traffic states to enable adaptive control, and the second one imputes both stat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#65288;AdvIB&#65289;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#28909;&#25104;&#20687;&#31995;&#32479;&#25191;&#34892;&#38544;&#34109;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#25104;&#21151;&#35825;&#23548;&#30446;&#26631;&#32418;&#22806;&#26816;&#27979;&#22120;&#22312;&#29289;&#29702;&#22330;&#26223;&#20013;&#23545;&#23545;&#35937;&#25110;&#20154;&#31867;&#36827;&#34892;&#35823;&#20998;&#31867;&#65292;&#24182;&#19988;&#19981;&#34987;&#20154;&#31867;&#23519;&#35273;&#12290;</title><link>http://arxiv.org/abs/2304.10712</link><description>&lt;p&gt;
&#29289;&#29702;&#19990;&#30028;&#20013;&#24858;&#24324;&#28909;&#32418;&#22806;&#25506;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Fooling Thermal Infrared Detectors in Physical World. (arXiv:2304.10712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#8212;&#8212;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#65288;AdvIB&#65289;&#65292;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#28909;&#25104;&#20687;&#31995;&#32479;&#25191;&#34892;&#38544;&#34109;&#30340;&#40657;&#30418;&#25915;&#20987;&#65292;&#25104;&#21151;&#35825;&#23548;&#30446;&#26631;&#32418;&#22806;&#26816;&#27979;&#22120;&#22312;&#29289;&#29702;&#22330;&#26223;&#20013;&#23545;&#23545;&#35937;&#25110;&#20154;&#31867;&#36827;&#34892;&#35823;&#20998;&#31867;&#65292;&#24182;&#19988;&#19981;&#34987;&#20154;&#31867;&#23519;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32418;&#22806;&#25104;&#20687;&#31995;&#32479;&#22312;&#34892;&#20154;&#26816;&#27979;&#21644;&#33258;&#21160;&#39550;&#39542;&#31561;&#26041;&#38754;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#23433;&#20840;&#24615;&#33021;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#30740;&#31350;&#25506;&#32034;&#20102;&#32418;&#22806;&#25104;&#20687;&#31995;&#32479;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#30340;&#23433;&#20840;&#24615;&#12290;&#36807;&#21435;&#30340;&#30740;&#31350;&#20351;&#29992;&#29289;&#29702;&#24178;&#25200;&#65292;&#22914;&#23567;&#28783;&#27873;&#21644;&#28909;&#8220;QR&#20195;&#30721;&#8221;&#26469;&#25915;&#20987;&#32418;&#22806;&#25104;&#20687;&#25506;&#27979;&#22120;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#24456;&#23481;&#26131;&#34987;&#23519;&#35273;&#65292;&#32570;&#20047;&#38544;&#31192;&#24615;&#12290;&#20854;&#20182;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#28909;&#21644;&#20919;&#22359;&#26469;&#27450;&#39575;&#32418;&#22806;&#25104;&#20687;&#25506;&#27979;&#22120;&#65292;&#20294;&#36825;&#31181;&#26041;&#27861;&#22312;&#20174;&#22810;&#20010;&#35282;&#24230;&#25191;&#34892;&#25915;&#20987;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29289;&#29702;&#25915;&#20987;&#26041;&#27861;&#65292;&#31216;&#20026;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#65288;AdvIB&#65289;&#12290;&#36890;&#36807;&#20248;&#21270;&#23545;&#25239;&#24615;&#32418;&#22806;&#22359;&#30340;&#29289;&#29702;&#21442;&#25968;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#20174;&#22810;&#20010;&#35282;&#24230;&#23545;&#28909;&#25104;&#20687;&#31995;&#32479;&#25191;&#34892;&#38544;&#34109;&#30340;&#40657;&#30418;&#25915;&#20987;&#12290;&#25105;&#20204;&#26681;&#25454;&#20854;&#26377;&#25928;&#24615;&#12289;&#38544;&#31192;&#24615;&#21644;&#31283;&#20581;&#24615;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;AdvIB&#21487;&#20197;&#25104;&#21151;&#35825;&#23548;&#30446;&#26631;&#32418;&#22806;&#26816;&#27979;&#22120;&#22312;&#29289;&#29702;&#22330;&#26223;&#20013;&#23545;&#23545;&#35937;&#25110;&#20154;&#31867;&#36827;&#34892;&#35823;&#20998;&#31867;&#65292;&#24182;&#19988;&#19981;&#34987;&#20154;&#31867;&#23519;&#35273;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#24378;&#35843;&#20102;&#22312;&#32418;&#22806;&#25104;&#20687;&#31995;&#32479;&#20013;&#25552;&#39640;&#23433;&#20840;&#25514;&#26045;&#30340;&#24517;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Infrared imaging systems have a vast array of potential applications in pedestrian detection and autonomous driving, and their safety performance is of great concern. However, few studies have explored the safety of infrared imaging systems in real-world settings. Previous research has used physical perturbations such as small bulbs and thermal "QR codes" to attack infrared imaging detectors, but such methods are highly visible and lack stealthiness. Other researchers have used hot and cold blocks to deceive infrared imaging detectors, but this method is limited in its ability to execute attacks from various angles. To address these shortcomings, we propose a novel physical attack called adversarial infrared blocks (AdvIB). By optimizing the physical parameters of the adversarial infrared blocks, this method can execute a stealthy black-box attack on thermal imaging system from various angles. We evaluate the proposed method based on its effectiveness, stealthiness, and robustness. Our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#26631;&#31614;&#22686;&#24378;&#30340;&#26032;&#22411;MIML&#26694;&#26550;GLEMIML&#65292;&#36890;&#36807;&#25552;&#39640;&#26631;&#31614;&#37325;&#35201;&#24615;&#26469;&#25552;&#39640;MIML&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10705</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26631;&#31614;&#22686;&#24378;&#30340;&#22810;&#31034;&#20363;&#22810;&#26631;&#31614;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Graph based Label Enhancement for Multi-instance Multi-label learning. (arXiv:2304.10705v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#26631;&#31614;&#22686;&#24378;&#30340;&#26032;&#22411;MIML&#26694;&#26550;GLEMIML&#65292;&#36890;&#36807;&#25552;&#39640;&#26631;&#31614;&#37325;&#35201;&#24615;&#26469;&#25552;&#39640;MIML&#30340;&#20998;&#31867;&#24615;&#33021;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#31034;&#20363;&#22810;&#26631;&#31614;&#65288;MIML&#65289;&#23398;&#20064;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#39046;&#22495;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#65292;&#20854;&#20013;&#19968;&#20010;&#22270;&#20687;&#21253;&#21547;&#21516;&#26102;&#19982;&#22810;&#20010;&#36923;&#36753;&#26631;&#31614;&#30456;&#20851;&#30340;&#22810;&#20010;&#31034;&#20363;&#12290;&#29616;&#26377; MIML &#20013;&#30340;&#30456;&#20851;&#26631;&#31614;&#37117;&#34987;&#20551;&#23450;&#20026;&#20855;&#26377;&#30456;&#31561;&#24847;&#20041;&#30340;&#36923;&#36753;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#22312; MIML &#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#27599;&#20010;&#26631;&#31614;&#23545;&#20110;&#27599;&#20010;&#22810;&#31034;&#20363;&#32972;&#21253;&#65288;&#22914;&#19968;&#20010;&#22270;&#20687;&#65289;&#30340;&#37325;&#35201;&#24615;&#24046;&#24322;&#26174;&#33879;&#12290;&#24573;&#30053;&#26631;&#31614;&#30340;&#37325;&#35201;&#24615;&#23558;&#22823;&#22823;&#25439;&#22833;&#23545;&#35937;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20351; MIML &#22312;&#22797;&#26434;&#22330;&#26223;&#20013;&#30340;&#23398;&#20064;&#24615;&#33021;&#19981;&#20339;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#26631;&#31614;&#22686;&#24378;&#30340;&#26032;&#22411; MIML &#26694;&#26550;&#65292;&#21363; GLEMIML&#65292;&#36890;&#36807;&#21033;&#29992;&#26631;&#31614;&#37325;&#35201;&#24615;&#26469;&#25552;&#39640; MIML &#30340;&#20998;&#31867;&#24615;&#33021;&#12290;GLEMIML &#39318;&#20808;&#36890;&#36807;&#24314;&#31435;&#22270;&#26469;&#35782;&#21035;&#23454;&#20363;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#28982;&#21518;&#36890;&#36807;&#37051;&#22495;&#20256;&#25773;&#23558;&#20174;&#29305;&#24449;&#31354;&#38388;&#20013;&#25366;&#25496;&#30340;&#38544;&#24335;&#20449;&#24687;&#36801;&#31227;&#21040;&#26631;&#31614;&#31354;&#38388;&#20013;&#12290;&#26368;&#21518;&#65292;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#26631;&#31614;&#22686;&#24378;&#31574;&#30053;&#65292;&#20197;&#22686;&#24378;&#19981;&#21516;&#37325;&#35201;&#24615;&#32423;&#21035;&#30340;&#23454;&#20363;&#26631;&#31614;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340; MIML &#26041;&#27861;&#30456;&#27604;&#65292;GLEMIML &#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#37117;&#24471;&#21040;&#20102;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-instance multi-label (MIML) learning is widely applicated in numerous domains, such as the image classification where one image contains multiple instances correlated with multiple logic labels simultaneously. The related labels in existing MIML are all assumed as logical labels with equal significance. However, in practical applications in MIML, significance of each label for multiple instances per bag (such as an image) is significant different. Ignoring labeling significance will greatly lose the semantic information of the object, so that MIML is not applicable in complex scenes with a poor learning performance. To this end, this paper proposed a novel MIML framework based on graph label enhancement, namely GLEMIML, to improve the classification performance of MIML by leveraging label significance. GLEMIML first recognizes the correlations among instances by establishing the graph and then migrates the implicit information mined from the feature space to the label space via n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; InterSAD &#26041;&#27861;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#25311;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#26377;&#25928;&#30340;&#28608;&#27963;&#20449;&#21495;&#21644;&#23454;&#26102;&#20132;&#20114;&#65292;&#35299;&#20915;&#20102;&#31995;&#32479;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10704</link><description>&lt;p&gt;
&#20132;&#20114;&#24335;&#31995;&#32479;&#32423;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interactive System-wise Anomaly Detection. (arXiv:2304.10704v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; InterSAD &#26041;&#27861;&#65292;&#21033;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#25311;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#24182;&#36890;&#36807;&#25214;&#21040;&#26377;&#25928;&#30340;&#28608;&#27963;&#20449;&#21495;&#21644;&#23454;&#26102;&#20132;&#20114;&#65292;&#35299;&#20915;&#20102;&#31995;&#32479;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#22522;&#30784;&#24615;&#30340;&#20316;&#29992;&#65292;&#20854;&#30446;&#30340;&#26159;&#25214;&#21040;&#21253;&#21547;&#19981;&#21516;&#20110;&#22823;&#22810;&#25968;&#30340;&#29305;&#24449;&#27169;&#24335;&#30340;&#25968;&#25454;&#23454;&#20363;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#24456;&#38590;&#22788;&#29702;&#20854;&#20013;&#23454;&#20363;&#26159;&#31995;&#32479;&#30340;&#24773;&#20917;&#65292;&#22240;&#20026;&#31995;&#32479;&#30340;&#29305;&#24449;&#19981;&#26131;&#35266;&#23519;&#20316;&#20026;&#25968;&#25454;&#12290;&#38656;&#35201;&#36866;&#24403;&#30340;&#20132;&#20114;&#26469;&#19982;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#24182;&#35782;&#21035;&#37027;&#20123;&#20855;&#26377;&#24322;&#24120;&#21709;&#24212;&#30340;&#31995;&#32479;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20132;&#20114;&#24335;&#31995;&#32479;&#32423;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#26469;&#27169;&#25311;&#20132;&#20114;&#24335;&#31995;&#32479;&#65292;&#24182;&#23450;&#20041;&#20102;&#31995;&#32479;&#32423;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#25214;&#21040;&#26377;&#25928;&#30340;&#28608;&#27963;&#20449;&#21495;&#26469;&#19982;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#36890;&#36807;&#23454;&#26102;&#20132;&#20114;&#30830;&#20445;&#31283;&#23450;&#30340;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection, where data instances are discovered containing feature patterns different from the majority, plays a fundamental role in various applications. However, it is challenging for existing methods to handle the scenarios where the instances are systems whose characteristics are not readily observed as data. Appropriate interactions are needed to interact with the systems and identify those with abnormal responses. Detecting system-wise anomalies is a challenging task due to several reasons including: how to formally define the system-wise anomaly detection problem; how to find the effective activation signal for interacting with systems to progressively collect the data and learn the detector; how to guarantee stable training in such a non-stationary scenario with real-time interactions? To address the challenges, we propose InterSAD (Interactive System-wise Anomaly Detection). Specifically, first, we adopt Markov decision process to model the interactive systems, and defi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#23548;&#38142;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#30340;&#25512;&#29702;&#38142;&#35780;&#20272;&#26694;&#26550;ReCEval&#65292;&#29992;&#20197;&#35780;&#20272;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#22320;&#35780;&#20272;&#25512;&#29702;&#38142;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.10703</link><description>&lt;p&gt;
&#36890;&#36807;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#35780;&#20272;&#25512;&#29702;&#38142;&#30340;ReCEval
&lt;/p&gt;
&lt;p&gt;
ReCEval: Evaluating Reasoning Chains via Correctness and Informativeness. (arXiv:2304.10703v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25512;&#23548;&#38142;&#27491;&#30830;&#24615;&#21644;&#20449;&#24687;&#37327;&#30340;&#25512;&#29702;&#38142;&#35780;&#20272;&#26694;&#26550;ReCEval&#65292;&#29992;&#20197;&#35780;&#20272;&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#12290;&#35813;&#26694;&#26550;&#33021;&#22815;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#22320;&#35780;&#20272;&#25512;&#29702;&#38142;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27493;&#25512;&#29702;&#33021;&#21147;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#20013;&#37117;&#26159;&#22522;&#30784;&#65292;&#20294;&#20160;&#20040;&#26500;&#25104;&#22909;&#30340;&#25512;&#29702;&#38142;&#20197;&#21450;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#23578;&#19981;&#28165;&#26970;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20165;&#20851;&#27880;&#25512;&#29702;&#38142;&#26159;&#21542;&#23548;&#33268;&#27491;&#30830;&#30340;&#32467;&#35770;&#65292;&#20294;&#36825;&#31181;&#20197;&#31572;&#26696;&#20026;&#23548;&#21521;&#30340;&#35266;&#28857;&#21487;&#33021;&#20250;&#23558;&#22909;&#30340;&#25512;&#29702;&#36136;&#37327;&#19982;&#20854;&#20182;&#29992;&#20110;&#39044;&#27979;&#31572;&#26696;&#30340;&#20551;&#25463;&#24452;&#28151;&#28102;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#23558;&#25512;&#29702;&#38142;&#35270;&#20026;&#25512;&#23548;&#26368;&#32456;&#31572;&#26696;&#30340;&#38750;&#27491;&#24335;&#35777;&#26126;&#65292;&#36890;&#36807;&#35780;&#20272;&#25512;&#29702;&#38142;&#30340;&#20004;&#20010;&#20851;&#38190;&#29305;&#24615;&#8212;&#8212;&#65288;1&#65289;&#27491;&#30830;&#24615;&#65292;&#21363;&#27599;&#20010;&#27493;&#39588;&#22522;&#20110;&#27493;&#39588;&#65292;&#21069;&#32622;&#27493;&#39588;&#21644;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#21253;&#21547;&#30340;&#20449;&#24687;&#36827;&#34892;&#26377;&#25928;&#25512;&#29702;&#65292;&#20197;&#21450;&#65288;2&#65289;&#20449;&#24687;&#37327;&#65292;&#21363;&#27599;&#20010;&#27493;&#39588;&#25552;&#20379;&#26032;&#20449;&#24687;&#26377;&#21161;&#20110;&#25512;&#23548;&#29983;&#25104;&#30340;&#31572;&#26696;&#8212;&#8212;&#25105;&#20204;&#25552;&#20986;&#20102;ReCEval&#65288;&#25512;&#29702;&#38142;&#35780;&#20272;&#65289;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#21644;&#20449;&#24687;&#29702;&#35770;&#27979;&#37327;&#23454;&#29616;&#20102;ReCEval&#12290;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#35780;&#20272;&#25512;&#29702;&#38142;&#26041;&#38754;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#21152;&#23458;&#35266;&#12289;&#31995;&#32479;&#21644;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-step reasoning ability is fundamental to many natural language tasks, yet it is unclear what constitutes a good reasoning chain and how to evaluate them. Most existing methods focus solely on whether the reasoning chain leads to the correct conclusion, but this answer-oriented view may confound the quality of reasoning with other spurious shortcuts to predict the answer. To bridge this gap, we evaluate reasoning chains by viewing them as informal proofs that derive the final answer. Specifically, we propose ReCEval (Reasoning Chain Evaluation), a framework that evaluates reasoning chains through two key properties: (1) correctness, i.e., each step makes a valid inference based on the information contained within the step, preceding steps, and input context, and (2) informativeness, i.e., each step provides new information that is helpful towards deriving the generated answer. We implement ReCEval using natural language inference models and information-theoretic measures. On multi
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#35789;&#27719;&#22806;&#38382;&#39064;&#30340;&#20004;&#31181;&#31574;&#30053;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#26356;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#35821;&#20041;&#27169;&#22411;&#65292;&#26088;&#22312;&#20811;&#26381;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21644;&#22522;&#20110;&#35268;&#21017;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2304.10663</link><description>&lt;p&gt;
&#20803;&#35821;&#20041;&#23398;&#65306;&#36808;&#21521;&#26356;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Meta Semantics: Towards better natural language understanding and reasoning. (arXiv:2304.10663v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10663
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35299;&#20915;&#35789;&#27719;&#22806;&#38382;&#39064;&#30340;&#20004;&#31181;&#31574;&#30053;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#26356;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#35821;&#20041;&#27169;&#22411;&#65292;&#26088;&#22312;&#20811;&#26381;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#21644;&#22522;&#20110;&#35268;&#21017;&#26041;&#27861;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#20043;&#19968;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22359;&#65288;LLM&#65289;&#26041;&#27861;&#65292;&#22914;ChatGPT&#21644;GPT-3&#65292;&#20855;&#26377;&#37319;&#29992;&#38750;&#27491;&#24335;&#25991;&#26412;&#30340;&#24378;&#22823;&#28789;&#27963;&#24615;&#65292;&#20294;&#22312;&#36923;&#36753;&#25512;&#23548;&#19978;&#36739;&#24369;&#65292;&#24182;&#19988;&#21463;&#21040;&#35789;&#27719;&#22806;&#65288;OOV&#65289;&#38382;&#39064;&#30340;&#24433;&#21709;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#22914;Mathematica&#12289;&#35821;&#20041;&#32593;&#32476;&#21644;Lean&#65292;&#22312;&#25512;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26080;&#27861;&#22788;&#29702;&#22797;&#26434;&#21644;&#26131;&#21464;&#30340;&#38750;&#27491;&#24335;&#25991;&#26412;&#12290;&#21463;&#21040;&#35821;&#29992;&#23398;&#21644;&#32467;&#26500;&#20027;&#20041;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#35299;&#20915;OOV&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#35821;&#20041;&#27169;&#22411;&#65292;&#29992;&#20110;&#26356;&#22909;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language understanding is one of the most challenging topics in artificial intelligence. Deep neural network methods, particularly large language module (LLM) methods such as ChatGPT and GPT-3, have powerful flexibility to adopt informal text but are weak on logical deduction and suffer from the out-of-vocabulary (OOV) problem. On the other hand, rule-based methods such as Mathematica, Semantic web, and Lean, are excellent in reasoning but cannot handle the complex and changeable informal text. Inspired by pragmatics and structuralism, we propose two strategies to solve the OOV problem and a semantic model for better natural language understanding and reasoning.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;ChatGPT&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#26377;&#23475;&#35780;&#35770;&#30340;&#21487;&#34892;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#21487;&#20197;&#36798;&#21040;&#32422;80%&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10619</link><description>&lt;p&gt;
&#8220;HOT&#8221; ChatGPT&#65306;ChatGPT&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#26816;&#27979;&#21644;&#35782;&#21035;&#20196;&#20154;&#35752;&#21388;&#12289;&#20196;&#20154;&#19981;&#24742;&#21644;&#26377;&#23475;&#35780;&#35770;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
"HOT" ChatGPT: The promise of ChatGPT in detecting and discriminating hateful, offensive, and toxic comments on social media. (arXiv:2304.10619v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10619
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;ChatGPT&#25506;&#31350;&#20102;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#26377;&#23475;&#35780;&#35770;&#30340;&#21487;&#34892;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;ChatGPT&#21487;&#20197;&#36798;&#21040;&#32422;80%&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#21361;&#23475;&#24615;&#20869;&#23481;&#30340;&#23384;&#22312;&#23545;&#22312;&#32447;&#31038;&#21306;&#21644;&#21442;&#19982;&#20135;&#29983;&#20102;&#36127;&#38754;&#24433;&#21709;&#12290;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#20043;&#19968;&#26159;&#24320;&#21457;&#38656;&#35201;&#20154;&#24037;&#26631;&#27880;&#30340;&#26816;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#36825;&#26679;&#30340;&#27169;&#22411;&#38656;&#35201;&#26333;&#38706;&#26631;&#27880;&#32773;&#20110;&#26377;&#23475;&#21644;&#20882;&#29359;&#24615;&#20869;&#23481;&#30340;&#20219;&#21153;&#65292;&#21487;&#33021;&#38656;&#35201;&#22823;&#37327;&#30340;&#26102;&#38388;&#21644;&#25104;&#26412;&#12290;&#29983;&#25104;&#24335;AI&#27169;&#22411;&#26377;&#28508;&#21147;&#29702;&#35299;&#21644;&#26816;&#27979;&#26377;&#23475;&#20869;&#23481;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20010;&#28508;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#65292;&#24182;&#23558;&#20854;&#24615;&#33021;&#19982;MTurker&#27880;&#37322;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#36825;&#20123;&#27880;&#37322;&#19982;&#26377;&#23475;&#20869;&#23481;&#30456;&#20851;&#30340;&#19977;&#20010;&#32463;&#24120;&#35752;&#35770;&#30340;&#27010;&#24565;&#65306;&#20196;&#20154;&#35752;&#21388;&#12289;&#20196;&#20154;&#19981;&#24742;&#21644;&#26377;&#23475;&#65288;HOT&#65289;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20116;&#20010;&#25552;&#31034;&#19982;ChatGPT&#36827;&#34892;&#20132;&#20114;&#65292;&#24182;&#36827;&#34892;&#20102;&#22235;&#20010;&#23454;&#39564;&#26469;&#24341;&#20986;HOT&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;MTurker&#27880;&#37322;&#30456;&#27604;&#65292;ChatGPT&#21487;&#20197;&#36798;&#21040;&#32422;80&#65285;&#30340;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#19982;HOT&#35780;&#35770;&#30456;&#27604;&#65292;&#27169;&#22411;&#23545;&#38750;HOT&#35780;&#35770;&#30340;&#20998;&#31867;&#26356;&#21152;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harmful content is pervasive on social media, poisoning online communities and negatively impacting participation. A common approach to address this issue is to develop detection models that rely on human annotations. However, the tasks required to build such models expose annotators to harmful and offensive content and may require significant time and cost to complete. Generative AI models have the potential to understand and detect harmful content. To investigate this potential, we used ChatGPT and compared its performance with MTurker annotations for three frequently discussed concepts related to harmful content: Hateful, Offensive, and Toxic (HOT). We designed five prompts to interact with ChatGPT and conducted four experiments eliciting HOT classifications. Our results show that ChatGPT can achieve an accuracy of approximately 80% when compared to MTurker annotations. Specifically, the model displays a more consistent classification for non-HOT comments than HOT comments compared 
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27880;&#24847;&#21147;&#26465;&#20214;&#33258;&#32534;&#30721;&#22120;(AF-CA)&#65292;&#29992;&#20110;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#65292;&#22312;&#22788;&#29702;&#22122;&#22768;&#26102;&#26356;&#21487;&#38752;&#65292;&#21487;&#20197;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.10614</link><description>&lt;p&gt;
&#26080;&#27880;&#24847;&#21147;&#26465;&#20214;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#21152;&#23494;&#36135;&#24065;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
An Attention Free Conditional Autoencoder For Anomaly Detection in Cryptocurrencies. (arXiv:2304.10614v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27880;&#24847;&#21147;&#26465;&#20214;&#33258;&#32534;&#30721;&#22120;(AF-CA)&#65292;&#29992;&#20110;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#65292;&#22312;&#22788;&#29702;&#22122;&#22768;&#26102;&#26356;&#21487;&#38752;&#65292;&#21487;&#20197;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26102;&#38388;&#24207;&#21015;&#20013;&#35782;&#21035;&#24322;&#24120;&#24456;&#22256;&#38590;&#65292;&#23588;&#20854;&#26159;&#22312;&#23384;&#22312;&#22823;&#37327;&#22122;&#22768;&#30340;&#24773;&#20917;&#19979;&#12290;&#38477;&#22122;&#25216;&#26415;&#21487;&#20197;&#21435;&#38500;&#22122;&#22768;&#65292;&#20294;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#20449;&#24687;&#30340;&#26174;&#33879;&#25439;&#22833;&#12290;&#20026;&#20102;&#26816;&#27979;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#27880;&#24847;&#21147;&#26465;&#20214;&#33258;&#32534;&#30721;&#22120;(AF-CA)&#12290;&#25105;&#20204;&#20174;&#33258;&#32534;&#30721;&#22120;&#26465;&#20214;&#27169;&#22411;&#24320;&#22987;&#65292;&#28155;&#21152;&#20102;&#19968;&#20010;&#26080;&#27880;&#24847;&#21147;LSTM&#23618;\cite{inzirillo2022attention}&#65292;&#20197;&#20351;&#24322;&#24120;&#26816;&#27979;&#33021;&#21147;&#26356;&#21487;&#38752;&#65292;&#24182;&#22686;&#21152;&#24322;&#24120;&#26816;&#27979;&#30340;&#21151;&#29575;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;Attention Free Conditional Autoencoder&#19982;LSTM Autoencoder&#30340;&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#26126;&#26174;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#35299;&#37322;&#33021;&#21147;&#65292;&#22240;&#27492;&#21487;&#20197;&#26356;&#22909;&#22320;&#26816;&#27979;&#22122;&#22768;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is difficult to identify anomalies in time series, especially when there is a lot of noise. Denoising techniques can remove the noise but this technique can cause a significant loss of information. To detect anomalies in the time series we have proposed an attention free conditional autoencoder (AF-CA). We started from the autoencoder conditional model on which we added an Attention-Free LSTM layer \cite{inzirillo2022attention} in order to make the anomaly detection capacity more reliable and to increase the power of anomaly detection. We compared the results of our Attention Free Conditional Autoencoder with those of an LSTM Autoencoder and clearly improved the explanatory power of the model and therefore the detection of anomaly in noisy time series.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; Text2Seg &#30340;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#27969;&#31243;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#21644;&#25991;&#26412;&#24341;&#23548;&#65292;&#21462;&#24471;&#20102;&#21021;&#27493;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.10597</link><description>&lt;p&gt;
Text2Seg: &#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Text2Seg: Remote Sensing Image Semantic Segmentation via Text-Guided Visual Foundation Models. (arXiv:2304.10597v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; Text2Seg &#30340;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#27969;&#31243;&#65292;&#21033;&#29992;&#22810;&#20010;&#22522;&#30784;&#27169;&#22411;&#21644;&#25991;&#26412;&#24341;&#23548;&#65292;&#21462;&#24471;&#20102;&#21021;&#27493;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289;&#65292;&#22914; GPT-4 &#21644; LLaMA&#65292;&#22312;&#38646;&#26679;&#26412;&#23398;&#20064;&#26041;&#26696;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#21560;&#24341;&#20102;&#22823;&#37327;&#20851;&#27880;&#12290;&#31867;&#20284;&#22320;&#65292;&#22312;&#35270;&#35273;&#23398;&#20064;&#39046;&#22495;&#65292;Grounding DINO &#21644; Segment Anything Model&#65288;SAM&#65289;&#31561;&#27169;&#22411;&#22312;&#24320;&#25918;&#24335;&#26816;&#27979;&#21644;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#36827;&#27493;&#12290;&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#36965;&#24863;&#39046;&#22495;&#65292;&#20854;&#20013;&#22270;&#29255;&#19982;&#20256;&#32479;&#22330;&#26223;&#20013;&#30340;&#22270;&#29255;&#26126;&#26174;&#19981;&#21516;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27969;&#31243;&#65292;&#21033;&#29992;&#22810;&#20010; FMs&#65292;&#20197;&#25991;&#26412;&#25552;&#31034;&#20026;&#25351;&#23548;&#65292;&#20419;&#36827;&#36965;&#24863;&#22270;&#20687;&#35821;&#20041;&#20998;&#21106;&#20219;&#21153;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026; Text2Seg &#12290;&#35813;&#31649;&#36947;&#22312;&#22810;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#36965;&#24863;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20379;&#21021;&#27493;&#32467;&#26524;&#20197;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in foundation models (FMs), such as GPT-4 and LLaMA, have attracted significant attention due to their exceptional performance in zero-shot learning scenarios. Similarly, in the field of visual learning, models like Grounding DINO and the Segment Anything Model (SAM) have exhibited remarkable progress in open-set detection and instance segmentation tasks. It is undeniable that these FMs will profoundly impact a wide range of real-world visual learning tasks, ushering in a new paradigm shift for developing such models. In this study, we concentrate on the remote sensing domain, where the images are notably dissimilar from those in conventional scenarios. We developed a pipeline that leverages multiple FMs to facilitate remote sensing image semantic segmentation tasks guided by text prompt, which we denote as Text2Seg. The pipeline is benchmarked on several widely-used remote sensing datasets, and we present preliminary results to demonstrate its effectiveness. Throug
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#32593;&#32476;&#20998;&#26512;&#25581;&#31034;&#20102;&#22609;&#36896;AI&#21457;&#23637;&#26684;&#23616;&#21644;&#39046;&#22495;&#25104;&#29087;&#24230;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;AI&#20844;&#20849;&#25919;&#31574;&#30340;&#25968;&#25454;&#39537;&#21160;&#24102;&#26469;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2304.10596</link><description>&lt;p&gt;
&#21033;&#29992;&#32593;&#32476;&#20998;&#26512;&#26469;&#20174;&#21360;&#24230;&#19987;&#21033;&#20013;&#25506;&#31350;&#20154;&#24037;&#26234;&#33021;&#25919;&#31574;&#30340;&#34701;&#21512;&#19982;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Enhancing Artificial intelligence Policies with Fusion and Forecasting: Insights from Indian Patents Using Network Analysis. (arXiv:2304.10596v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10596
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#32593;&#32476;&#20998;&#26512;&#25581;&#31034;&#20102;&#22609;&#36896;AI&#21457;&#23637;&#26684;&#23616;&#21644;&#39046;&#22495;&#25104;&#29087;&#24230;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#20026;AI&#20844;&#20849;&#25919;&#31574;&#30340;&#25968;&#25454;&#39537;&#21160;&#24102;&#26469;&#20102;&#26032;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21033;&#29992;&#32593;&#32476;&#20998;&#26512;&#30340;&#20013;&#24515;&#24230;&#27979;&#37327;&#12289;&#32858;&#31867;&#31995;&#25968;&#21644;&#34701;&#21512;&#24230;&#27979;&#37327;&#31561;&#26041;&#27861;&#65292;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#30340;&#20114;&#32852;&#20114;&#36890;&#21644;&#30456;&#20114;&#20381;&#23384;&#12290;&#36890;&#36807;&#22312;&#19981;&#21516;&#26102;&#38388;&#31383;&#21475;&#20869;&#20998;&#26512;&#21644;&#37327;&#21270;&#36825;&#20123;&#25216;&#26415;&#30340;&#37325;&#35201;&#24615;&#65292;&#25581;&#31034;&#20102;&#22609;&#36896;AI&#21457;&#23637;&#26684;&#23616;&#21644;&#39046;&#22495;&#25104;&#29087;&#24230;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#35813;&#30740;&#31350;&#32467;&#26524;&#23545;&#26410;&#26469;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#21644;&#36827;&#27493;&#20855;&#26377;&#37325;&#22823;&#24847;&#20041;&#65292;&#24182;&#28165;&#26224;&#22320;&#20102;&#35299;&#20102;&#34701;&#21512;&#25216;&#26415;&#30340;&#20851;&#38190;&#25216;&#26415;&#39046;&#22495;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36890;&#36807;&#25552;&#20379;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#35270;&#35282;&#65292;&#20026;AI&#20844;&#20849;&#25919;&#31574;&#30740;&#31350;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#27880;&#24847;&#26412;&#30740;&#31350;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#21628;&#21505;&#36827;&#19968;&#27493;&#30740;&#31350;&#26469;&#24314;&#31435;&#36825;&#20123;&#32467;&#26524;&#30340;&#22522;&#30784;&#12290;&#36890;&#36807;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#24076;&#26395;&#25351;&#23548;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a study of the interconnectivity and interdependence of various Artificial intelligence (AI) technologies through the use of centrality measures, clustering coefficients, and degree of fusion measures. By analyzing the technologies through different time windows and quantifying their importance, we have revealed important insights into the crucial components shaping the AI landscape and the maturity level of the domain. The results of this study have significant implications for future development and advancements in artificial intelligence and provide a clear understanding of key technology areas of fusion. Furthermore, this paper contributes to AI public policy research by offering a data-driven perspective on the current state and future direction of the field. However, it is important to acknowledge the limitations of this research and call for further studies to build on these results. With these findings, we hope to inform and guide future research in the fiel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#30340;&#31526;&#21495;&#12289;&#20122;&#31526;&#21495;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65288;SDP&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#26080;&#35770;&#26159;&#22522;&#20110;&#33258;&#21160;&#21270;&#35268;&#21010;&#65288;AP&#65289;&#36824;&#26159;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#37117;&#28085;&#30422;&#20102;&#35299;&#20915;SDP&#30340;&#26041;&#27861;&#21644;&#23398;&#20064;&#20854;&#32467;&#26500;&#30340;&#26041;&#38754;&#12290;&#23545;&#20110;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#20063;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;</title><link>http://arxiv.org/abs/2304.10590</link><description>&lt;p&gt;
&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#30340;&#31526;&#21495;&#12289;&#20122;&#31526;&#21495;&#21644;&#28151;&#21512;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Symbolic, Subsymbolic and Hybrid Methods for Sequential Decision Making. (arXiv:2304.10590v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10590
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#30340;&#31526;&#21495;&#12289;&#20122;&#31526;&#21495;&#21644;&#28151;&#21512;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65288;SDP&#65289;&#20013;&#30340;&#38382;&#39064;&#12290;&#26080;&#35770;&#26159;&#22522;&#20110;&#33258;&#21160;&#21270;&#35268;&#21010;&#65288;AP&#65289;&#36824;&#26159;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#65292;&#37117;&#28085;&#30422;&#20102;&#35299;&#20915;SDP&#30340;&#26041;&#27861;&#21644;&#23398;&#20064;&#20854;&#32467;&#26500;&#30340;&#26041;&#38754;&#12290;&#23545;&#20110;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#20063;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#65288;SDM&#65289;&#39046;&#22495;&#25552;&#20379;&#20102;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#36807;&#31243;&#65288;SDP&#65289;&#30340;&#24037;&#20855;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#24517;&#39035;&#20570;&#20986;&#19968;&#31995;&#21015;&#20915;&#31574;&#20197;&#23436;&#25104;&#20219;&#21153;&#25110;&#23454;&#29616;&#30446;&#26631;&#12290;&#21382;&#21490;&#19978;&#65292;&#20004;&#31181;&#31454;&#20105;&#30340;SDM&#33539;&#20363;&#20027;&#23548;&#20102;&#35813;&#39046;&#22495;&#12290;&#33258;&#21160;&#21270;&#35268;&#21010;&#65288;AP&#65289;&#25552;&#20986;&#36890;&#36807;&#23545;&#19990;&#30028;&#27169;&#22411;&#30340;&#25512;&#29702;&#36807;&#31243;&#35299;&#20915;SDP&#65292;&#36890;&#24120;&#20351;&#29992;&#31526;&#21495;&#34920;&#31034;&#12290;&#30456;&#21453;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21017;&#25552;&#20986;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;SDP&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19981;&#38656;&#35201;&#19990;&#30028;&#27169;&#22411;&#65292;&#24182;&#20197;&#20122;&#31526;&#21495;&#24418;&#24335;&#34920;&#31034;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#12290;&#26412;&#32508;&#36848;&#22312;&#21327;&#35843;&#20004;&#31181;&#26041;&#27861;&#30340;&#22522;&#30784;&#19978;&#65292;&#23545;SDM&#30340;&#31526;&#21495;&#12289;&#20122;&#31526;&#21495;&#21644;&#28151;&#21512;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#36848;&#12290;&#25105;&#20204;&#28085;&#30422;&#20102;&#35299;&#20915;SDP&#30340;&#26041;&#27861;&#65288;&#20363;&#22914;AP&#12289;RL&#21644;&#23398;&#20064;&#35268;&#21010;&#30340;&#25216;&#26415;&#65289;&#20197;&#21450;&#23398;&#20064;&#20854;&#32467;&#26500;&#30340;&#26041;&#38754;&#65288;&#20363;&#22914;&#19990;&#30028;&#27169;&#22411;&#12289;&#29366;&#24577;&#19981;&#21464;&#37327;&#21644;&#22320;&#26631;&#65289;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#35813;&#39046;&#22495;&#20013;&#27809;&#26377;&#20854;&#20182;&#32508;&#36848;&#25552;&#20379;&#30456;&#21516;&#30340;&#33539;&#22260;&#12290;&#20316;&#20026;&#39069;&#22806;&#30340;&#36129;&#29486;&#65292;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#21508;&#31181;&#26041;&#27861;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#65292;&#20363;&#22914;&#22914;&#20309;&#23558;&#26041;&#27861;&#25193;&#23637;&#21040;&#26356;&#22823;&#12289;&#26356;&#22797;&#26434;&#30340;&#39046;&#22495;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of Sequential Decision Making (SDM) provides tools for solving Sequential Decision Processes (SDPs), where an agent must make a series of decisions in order to complete a task or achieve a goal. Historically, two competing SDM paradigms have view for supremacy. Automated Planning (AP) proposes to solve SDPs by performing a reasoning process over a model of the world, often represented symbolically. Conversely, Reinforcement Learning (RL) proposes to learn the solution of the SDP from data, without a world model, and represent the learned knowledge subsymbolically. In the spirit of reconciliation, we provide a review of symbolic, subsymbolic and hybrid methods for SDM. We cover both methods for solving SDPs (e.g., AP, RL and techniques that learn to plan) and for learning aspects of their structure (e.g., world models, state invariants and landmarks). To the best of our knowledge, no other review in the field provides the same scope. As an additional contribution, we discuss w
&lt;/p&gt;</description></item><item><title>&#35813;&#31687;&#35770;&#25991;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24212;&#29992;&#20110;&#25968;&#30334;&#19975;&#31687;&#25991;&#29486;&#26469;&#20272;&#35745;&#20154;&#24037;&#26234;&#33021;&#30340;&#30452;&#25509;&#20351;&#29992;&#21644;&#28508;&#22312;&#21463;&#30410;&#65292;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#20284;&#20046;&#22312;&#25152;&#26377;&#31185;&#23398;&#39046;&#22495;&#20013;&#26222;&#36941;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#35770;&#25991;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.10578</link><description>&lt;p&gt;
&#37327;&#21270;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#30410;&#22788;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Benefit of Artificial Intelligence for Scientific Research. (arXiv:2304.10578v1 [cs.DL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10578
&lt;/p&gt;
&lt;p&gt;
&#35813;&#31687;&#35770;&#25991;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24212;&#29992;&#20110;&#25968;&#30334;&#19975;&#31687;&#25991;&#29486;&#26469;&#20272;&#35745;&#20154;&#24037;&#26234;&#33021;&#30340;&#30452;&#25509;&#20351;&#29992;&#21644;&#28508;&#22312;&#21463;&#30410;&#65292;&#21457;&#29616;&#20154;&#24037;&#26234;&#33021;&#22312;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#20284;&#20046;&#22312;&#25152;&#26377;&#31185;&#23398;&#39046;&#22495;&#20013;&#26222;&#36941;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#35770;&#25991;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#19981;&#26029;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#38761;&#21629;&#26377;&#21487;&#33021;&#25913;&#21464;&#20960;&#20046;&#25152;&#26377;&#30340;&#34892;&#19994;&#12290;&#38543;&#30528;AI&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#31934;&#24230;&#12289;&#40065;&#26834;&#24615;&#21644;&#24212;&#29992;&#33539;&#22260;&#30340;&#22686;&#21152;&#65292;AI&#21487;&#33021;&#20250;&#22312;&#35768;&#22810;&#26377;&#20215;&#20540;&#30340;&#20219;&#21153;&#19978;&#36229;&#36807;&#29978;&#33267;&#21462;&#20195;&#20154;&#31867;&#19987;&#23478;&#12290;&#23613;&#31649;&#20154;&#20204;&#19981;&#26029;&#21162;&#21147;&#30740;&#31350;AI&#23545;&#21171;&#21160;&#21147;&#21644;&#32463;&#27982;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#23427;&#22312;&#20419;&#36827;&#31185;&#23398;&#21457;&#29616;&#21644;&#36827;&#27493;&#26041;&#38754;&#30340;&#26368;&#36817;&#25104;&#21151;&#65292;&#20294;&#25105;&#20204;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#30340;&#29702;&#35299;&#65292;&#21363;AI&#30340;&#36827;&#27493;&#22914;&#20309;&#22312;&#19981;&#21516;&#23398;&#31185;&#21644;&#39046;&#22495;&#20013;&#21463;&#30410;&#20110;&#31185;&#23398;&#30740;&#31350;&#12290;&#22312;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#34913;&#37327;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#24212;&#29992;&#20110;87.6&#30334;&#19975;&#31687;&#35770;&#25991;&#21644;7.1&#30334;&#19975;&#20221;&#19987;&#21033;&#65292;&#26469;&#20272;&#35745;AI&#30340;&#30452;&#25509;&#20351;&#29992;&#21644;&#28508;&#22312;&#21463;&#30410;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;AI&#22312;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#20284;&#20046;&#22312;&#25152;&#26377;&#31185;&#23398;&#39046;&#22495;&#20013;&#26222;&#36941;&#65292;&#29305;&#21035;&#26159;&#33258;2015&#24180;&#20197;&#26469;&#24320;&#22987;&#36805;&#36895;&#22686;&#38271;&#65292;&#24182;&#19988;&#20351;&#29992;AI&#30340;&#35770;&#25991;&#20855;&#26377;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#65292;&#26356;&#21487;&#33021;&#22312;&#20869;&#22806;&#37096;&#34987;&#39640;&#24230;&#24341;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ongoing artificial intelligence (AI) revolution has the potential to change almost every line of work. As AI capabilities continue to improve in accuracy, robustness, and reach, AI may outperform and even replace human experts across many valuable tasks. Despite enormous efforts devoted to understanding AI's impact on labor and the economy and its recent success in accelerating scientific discovery and progress, we lack a systematic understanding of how advances in AI may benefit scientific research across disciplines and fields. Here we develop a measurement framework to estimate both the direct use of AI and the potential benefit of AI in scientific research by applying natural language processing techniques to 87.6 million publications and 7.1 million patents. We find that the use of AI in research appears widespread throughout the sciences, growing especially rapidly since 2015, and papers that use AI exhibit an impact premium, more likely to be highly cited both within and out
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37325;&#26032;&#35299;&#37322;&#38544;&#24335;Q&#23398;&#20064;(IQL)&#20316;&#20026;Actor-Critic&#26041;&#27861;&#65292;&#25552;&#20986;&#20351;&#29992;&#25193;&#25955;&#34892;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#26435;&#37325;&#26469;&#24179;&#34913;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20998;&#27495;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#21644;&#22810;&#23792;&#29305;&#24449;&#30340;Actor&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.10573</link><description>&lt;p&gt;
IDQL: &#20316;&#20026;&#19968;&#31181;&#25193;&#25955;&#31574;&#30053;&#30340;Actor-Critic&#26041;&#27861;&#30340;&#38544;&#24335;Q&#23398;&#20064;&#12290; (arXiv:2304.10573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
IDQL: Implicit Q-Learning as an Actor-Critic Method with Diffusion Policies. (arXiv:2304.10573v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10573
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37325;&#26032;&#35299;&#37322;&#38544;&#24335;Q&#23398;&#20064;(IQL)&#20316;&#20026;Actor-Critic&#26041;&#27861;&#65292;&#25552;&#20986;&#20351;&#29992;&#25193;&#25955;&#34892;&#20026;&#31574;&#30053;&#21644;&#35780;&#21028;&#22120;&#26435;&#37325;&#26469;&#24179;&#34913;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20998;&#27495;&#12290;&#36825;&#20010;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#21644;&#22810;&#23792;&#29305;&#24449;&#30340;Actor&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#31163;&#32447;RL&#26041;&#27861;&#38656;&#35201;&#27491;&#30830;&#22788;&#29702;&#36229;&#20986;&#20998;&#24067;&#30340;&#34892;&#20026;&#12290;&#38544;&#24335;Q&#23398;&#20064;&#65288;IQL&#65289;&#36890;&#36807;&#20165;&#20351;&#29992;&#25968;&#25454;&#38598;&#34892;&#21160;&#36890;&#36807;&#20462;&#25913;&#21518;&#30340;Bellman Backup&#26469;&#35757;&#32451;Q&#20989;&#25968;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#20294;&#26159;&#65292;&#19981;&#28165;&#26970;&#21738;&#20010;&#31574;&#30053;&#23454;&#38469;&#19978;&#23454;&#29616;&#20102;&#27492;&#38544;&#21547;&#35757;&#32451;&#30340;Q&#20989;&#25968;&#25152;&#20195;&#34920;&#30340;&#20540;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;IQL&#37325;&#26032;&#35299;&#37322;&#20026;Actor-Critic&#26041;&#27861;&#65292;&#36890;&#36807;&#24191;&#20041;&#21270;&#35780;&#21028;&#30446;&#26631;&#24182;&#23558;&#20854;&#36830;&#25509;&#21040;&#34892;&#20026;&#35268;&#33539;&#21270;&#30340;&#38544;&#24335;Actor&#26469;&#23454;&#29616;&#12290;&#36825;&#31181;&#27867;&#21270;&#26174;&#31034;&#20102;&#24341;&#20837;&#30340;Actor&#22914;&#20309;&#24179;&#34913;&#22870;&#21169;&#26368;&#22823;&#21270;&#21644;&#19982;&#34892;&#20026;&#31574;&#30053;&#30340;&#20998;&#27495;&#65292;&#20855;&#20307;&#30340;&#25439;&#22833;&#36873;&#25321;&#20915;&#23450;&#20102;&#36825;&#31181;&#26435;&#34913;&#30340;&#24615;&#36136;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#36825;&#20010;Actor&#21487;&#20197;&#34920;&#29616;&#20986;&#22797;&#26434;&#21644;&#22810;&#23792;&#30340;&#29305;&#24449;&#65292;&#36825;&#34920;&#26126;&#20102;&#21033;&#29992;&#20248;&#21183;&#21152;&#26435;&#22238;&#24402;&#65288;AWR&#65289;&#20013;&#20351;&#29992;&#30340;&#26465;&#20214;&#39640;&#26031;Actor&#30340;&#25311;&#21512;&#38382;&#39064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#26469;&#33258;&#21442;&#25968;&#21270;&#25193;&#25955;&#34892;&#20026;&#31574;&#30053;&#30340;&#26679;&#26412;&#21644;&#30001;&#35780;&#21028;&#22120;&#35745;&#31639;&#30340;&#26435;&#37325;&#65292;&#28982;&#21518;&#23558;&#20854;&#23548;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Effective offline RL methods require properly handling out-of-distribution actions. Implicit Q-learning (IQL) addresses this by training a Q-function using only dataset actions through a modified Bellman backup. However, it is unclear which policy actually attains the values represented by this implicitly trained Q-function. In this paper, we reinterpret IQL as an actor-critic method by generalizing the critic objective and connecting it to a behavior-regularized implicit actor. This generalization shows how the induced actor balances reward maximization and divergence from the behavior policy, with the specific loss choice determining the nature of this tradeoff. Notably, this actor can exhibit complex and multimodal characteristics, suggesting issues with the conditional Gaussian actor fit with advantage weighted regression (AWR) used in prior methods. Instead, we propose using samples from a diffusion parameterized behavior policy and weights computed from the critic to then importa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Z3&#27714;&#35299;&#22120;&#23545;&#20840;&#23616;&#40065;&#26834;&#24615;&#21487;&#39564;&#35777;&#26694;&#26550;DeepGlobal&#36827;&#34892;&#26356;&#26126;&#30830;&#30340;&#23450;&#20041;&#21644;&#20248;&#21270;&#30340;&#24037;&#20316;&#65292;&#26469;&#24314;&#31435;FNN&#30340;&#24418;&#24335;&#21270;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.10558</link><description>&lt;p&gt;
&#20351;&#29992;Z3&#36827;&#34892;FNN&#20840;&#23616;&#40065;&#26834;&#24615;&#30340;&#24418;&#24335;&#21270;&#24314;&#27169;&#21644;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Using Z3 for Formal Modeling and Verification of FNN Global Robustness. (arXiv:2304.10558v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20351;&#29992;Z3&#27714;&#35299;&#22120;&#23545;&#20840;&#23616;&#40065;&#26834;&#24615;&#21487;&#39564;&#35777;&#26694;&#26550;DeepGlobal&#36827;&#34892;&#26356;&#26126;&#30830;&#30340;&#23450;&#20041;&#21644;&#20248;&#21270;&#30340;&#24037;&#20316;&#65292;&#26469;&#24314;&#31435;FNN&#30340;&#24418;&#24335;&#21270;&#27169;&#22411;&#65292;&#20197;&#23454;&#29616;&#26356;&#26377;&#25928;&#30340;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65288;FNN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23545;&#23545;&#25239;&#26679;&#26412;&#24456;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#25216;&#26415;&#26469;&#39564;&#35777;FNN&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#20294;&#22823;&#22810;&#25968;&#25216;&#26415;&#37117;&#38598;&#20013;&#22312;&#38024;&#23545;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#23616;&#37096;&#25200;&#21160;&#37051;&#22495;&#30340;&#40065;&#26834;&#24615;&#39564;&#35777;&#19978;&#12290;&#20840;&#23616;&#40065;&#26834;&#24615;&#20998;&#26512;&#20173;&#23384;&#22312;&#36739;&#22823;&#30340;&#30740;&#31350;&#31354;&#30333;&#12290;DeepGlobal&#26159;&#19968;&#31181;&#20840;&#23616;&#40065;&#26834;&#24615;&#21487;&#39564;&#35777;&#26694;&#26550;&#65292;&#26088;&#22312;&#30830;&#23450;FNN&#30340;&#25152;&#26377;&#21487;&#33021;&#30340;&#23545;&#25239;&#21361;&#38505;&#21306;&#22495;&#65288;ADR&#65289;&#65292;&#19981;&#38480;&#20110;&#27979;&#35797;&#38598;&#20013;&#30340;&#25968;&#25454;&#26679;&#26412;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;DeepGlobal&#30340;&#23436;&#25972;&#35268;&#33539;&#21644;&#23454;&#29616;&#65292;&#21033;&#29992;SMT&#27714;&#35299;&#22120;Z3&#36827;&#34892;&#26356;&#26126;&#30830;&#30340;&#23450;&#20041;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#39033;&#25913;&#36827;&#20197;&#36827;&#34892;&#26356;&#39640;&#25928;&#30340;&#39564;&#35777;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#23454;&#29616;&#21644;&#25913;&#36827;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23545;&#19968;&#32452;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#20102;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
While Feedforward Neural Networks (FNNs) have achieved remarkable success in various tasks, they are vulnerable to adversarial examples. Several techniques have been developed to verify the adversarial robustness of FNNs, but most of them focus on robustness verification against the local perturbation neighborhood of a single data point. There is still a large research gap in global robustness analysis. The global-robustness verifiable framework DeepGlobal has been proposed to identify \textit{all} possible Adversarial Dangerous Regions (ADRs) of FNNs, not limited to data samples in a test set. In this paper, we propose a complete specification and implementation of DeepGlobal utilizing the SMT solver Z3 for more explicit definition, and propose several improvements to DeepGlobal for more efficient verification. To evaluate the effectiveness of our implementation and improvements, we conduct extensive experiments on a set of benchmark datasets. Visualization of our experiment results s
&lt;/p&gt;</description></item><item><title>Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.10557</link><description>&lt;p&gt;
Transformer&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
An Introduction to Transformers. (arXiv:2304.10557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10557
&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12290;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;Transformer&#30340;&#20171;&#32461;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#32570;&#23569;&#23545;&#20854;&#26550;&#26500;&#30340;&#31934;&#30830;&#25968;&#23398;&#25551;&#36848;&#65292;&#20854;&#35774;&#35745;&#36873;&#25321;&#30340;&#30452;&#35273;&#20063;&#24120;&#24120;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#30740;&#31350;&#36335;&#24452;&#30340;&#26354;&#25240;&#65292;Transformer&#37096;&#20214;&#30340;&#35299;&#37322;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer is a neural network component that can be used to learn useful representations of sequences or sets of datapoints. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21327;&#21516;&#21830;&#21153;&#26234;&#33021;&#30340;&#34394;&#25311;&#21161;&#25163;&#21442;&#32771;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#21644;&#21327;&#20316;&#12290;&#35813;&#27169;&#22411;&#21487;&#29992;&#20110;&#25903;&#25345;&#21830;&#19994;&#20998;&#26512;&#20013;&#30340;&#21508;&#31181;&#27963;&#21160;&#65292;&#20419;&#36827;&#26356;&#22909;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2304.10556</link><description>&lt;p&gt;
&#19968;&#31181;&#21327;&#21516;&#21830;&#21153;&#26234;&#33021;&#34394;&#25311;&#21161;&#25163;&#21442;&#32771;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Reference Model for Collaborative Business Intelligence Virtual Assistants. (arXiv:2304.10556v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10556
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21327;&#21516;&#21830;&#21153;&#26234;&#33021;&#30340;&#34394;&#25311;&#21161;&#25163;&#21442;&#32771;&#27169;&#22411;&#65292;&#26088;&#22312;&#25552;&#39640;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#30693;&#35782;&#20849;&#20139;&#21644;&#21327;&#20316;&#12290;&#35813;&#27169;&#22411;&#21487;&#29992;&#20110;&#25903;&#25345;&#21830;&#19994;&#20998;&#26512;&#20013;&#30340;&#21508;&#31181;&#27963;&#21160;&#65292;&#20419;&#36827;&#26356;&#22909;&#30340;&#20915;&#31574;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#21830;&#21153;&#20998;&#26512;(CBA)&#26159;&#19968;&#31181;&#26041;&#27861;&#35770;&#65292;&#23427;&#28041;&#21450;&#23558;&#19981;&#21516;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#65292;&#21253;&#25324;&#19994;&#21153;&#29992;&#25143;&#12289;&#20998;&#26512;&#24072;&#21644;&#25216;&#26415;&#19987;&#23478;&#65292;&#32858;&#38598;&#22312;&#19968;&#36215;&#65292;&#21327;&#21516;&#20998;&#26512;&#25968;&#25454;&#24182;&#33719;&#21462;&#23545;&#19994;&#21153;&#36816;&#33829;&#30340;&#27934;&#23519;&#12290;CBA&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#40723;&#21169;&#30693;&#35782;&#20849;&#20139;&#21644;&#21327;&#20316;&#65292;&#20174;&#32780;&#21487;&#20197;&#26356;&#20840;&#38754;&#22320;&#29702;&#35299;&#25968;&#25454;&#24182;&#20570;&#20986;&#26356;&#22909;&#30340;&#20915;&#31574;&#12290;CBA&#36890;&#24120;&#38656;&#35201;&#36827;&#34892;&#21508;&#31181;&#27963;&#21160;&#65292;&#21253;&#25324;&#25968;&#25454;&#25910;&#38598;&#21644;&#20998;&#26512;&#12289;&#22836;&#33041;&#39118;&#26292;&#12289;&#38382;&#39064;&#35299;&#20915;&#12289;&#20915;&#31574;&#21644;&#30693;&#35782;&#20849;&#20139;&#12290;&#36825;&#20123;&#27963;&#21160;&#21487;&#20197;&#36890;&#36807;&#21508;&#31181;&#28192;&#36947;&#36827;&#34892;&#65292;&#20363;&#22914;&#38754;&#23545;&#38754;&#20250;&#35758;&#12289;&#34394;&#25311;&#21327;&#20316;&#24037;&#20855;&#25110;&#22312;&#32447;&#35770;&#22363;&#12290;&#26412;&#25991;&#35752;&#35770;&#20102;&#34394;&#25311;&#21327;&#20316;&#24037;&#20855;&#20316;&#20026;&#21830;&#21153;&#26234;&#33021;(BI)&#24179;&#21488;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#21327;&#21516;&#21830;&#21153;&#26234;&#33021;(CBI)&#24037;&#20855;&#21464;&#24471;&#26356;&#21152;&#29992;&#25143;&#21451;&#22909;&#12289;&#21487;&#35775;&#38382;&#21644;&#28789;&#27963;&#65292;&#21487;&#23454;&#29616;&#36328;&#22242;&#38431;&#21327;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Collaborative Business Analysis (CBA) is a methodology that involves bringing together different stakeholders, including business users, analysts, and technical specialists, to collaboratively analyze data and gain insights into business operations. The primary objective of CBA is to encourage knowledge sharing and collaboration between the different groups involved in business analysis, as this can lead to a more comprehensive understanding of the data and better decision-making. CBA typically involves a range of activities, including data gathering and analysis, brainstorming, problem-solving, decision-making and knowledge sharing. These activities may take place through various channels, such as in-person meetings, virtual collaboration tools or online forums. This paper deals with virtual collaboration tools as an important part of Business Intelligence (BI) platform. Collaborative Business Intelligence (CBI) tools are becoming more user-friendly, accessible, and flexible, allowing
&lt;/p&gt;</description></item><item><title>&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#32593;&#32476;&#30340;&#34920;&#29616;</title><link>http://arxiv.org/abs/2304.10553</link><description>&lt;p&gt;
&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
Sparsity in neural networks can improve their privacy. (arXiv:2304.10553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10553
&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#30340;&#38544;&#31169;&#65292;&#24182;&#19988;&#33021;&#22815;&#20445;&#25345;&#32593;&#32476;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#31232;&#30095;&#24615;&#22914;&#20309;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#31232;&#30095;&#24615;&#33021;&#22815;&#25552;&#39640;&#32593;&#32476;&#30340;&#38544;&#31169;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#22312;&#20219;&#21153;&#19978;&#30340;&#30456;&#20284;&#34920;&#29616;&#12290;&#36825;&#20010;&#23454;&#35777;&#30740;&#31350;&#23436;&#21892;&#21644;&#25193;&#23637;&#20102;&#29616;&#26377;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article measures how sparsity can make neural networks more robust to membership inference attacks. The obtained empirical results show that sparsity improves the privacy of the network, while preserving comparable performances on the task at hand. This empirical study completes and extends existing literature.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;IDS&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#22810;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;&#34701;&#21512;&#21644;/&#25110;&#36866;&#24212;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38750;&#24120;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;</title><link>http://arxiv.org/abs/2304.10550</link><description>&lt;p&gt;
&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Transfer Learning Applications in Intrusion Detection Systems: A Comprehensive Review. (arXiv:2304.10550v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#22238;&#39038;&#20102;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#22312;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;IDS&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65292;&#35813;&#25216;&#26415;&#21033;&#29992;&#22810;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;&#34701;&#21512;&#21644;/&#25110;&#36866;&#24212;&#20197;&#25552;&#39640;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#65292;&#23588;&#20854;&#26159;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#38750;&#24120;&#23569;&#30340;&#24773;&#20917;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#29699;&#33539;&#22260;&#20869;&#65292;&#22806;&#37096;&#20114;&#32852;&#32593;&#36234;&#26469;&#36234;&#22810;&#22320;&#19982;&#24403;&#20195;&#24037;&#19994;&#25511;&#21046;&#31995;&#32479;&#30456;&#36830;&#25509;&#12290;&#22240;&#27492;&#65292;&#26377;&#19968;&#20010;&#36843;&#20999;&#30340;&#38656;&#27714;&#20445;&#25252;&#32593;&#32476;&#20813;&#21463;&#21508;&#31181;&#23041;&#32961;&#12290;&#21487;&#20197;&#20351;&#29992;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#26469;&#20445;&#25252;&#24037;&#19994;&#27963;&#21160;&#30340;&#20851;&#38190;&#22522;&#30784;&#35774;&#26045;&#65292;&#36825;&#26159;&#19968;&#31181;&#39044;&#38450;&#24615;&#25514;&#26045;&#26426;&#21046;&#65292;&#29992;&#20110;&#35782;&#21035;&#26032;&#30340;&#21361;&#38505;&#23041;&#32961;&#21644;&#25932;&#23545;&#27963;&#21160;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#22312;&#35768;&#22810;&#31181;&#24037;&#19994;&#25511;&#21046;&#32593;&#32476;&#20013;&#21019;&#24314;IDS&#30340;&#26368;&#26032;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#65292;&#29305;&#21035;&#20391;&#37325;&#20110;&#22522;&#20110;IDS&#30340;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#65288;DTL&#65289;&#12290;DTL&#21487;&#20197;&#30475;&#20316;&#26159;&#23558;&#26469;&#33258;&#22810;&#20010;&#39046;&#22495;&#30340;&#30693;&#35782;&#34701;&#21512;&#21644;/&#25110;&#36866;&#24212;&#20197;&#22686;&#24378;&#30446;&#26631;&#20219;&#21153;&#30340;&#24615;&#33021;&#30340;&#19968;&#31181;&#20449;&#24687;&#34701;&#21512;&#12290;&#37325;&#28857;&#26159;&#24403;&#30446;&#26631;&#22495;&#20013;&#30340;&#26631;&#35760;&#25968;&#25454;&#24456;&#23569;&#26102;&#65292;DTL&#21487;&#20197;&#24110;&#21161;&#25552;&#39640;IDS&#30340;&#24615;&#33021;&#12290;&#32771;&#34385;&#20102;2015&#24180;&#20043;&#21518;&#30340;&#20986;&#29256;&#29289;&#12290;&#36825;&#20123;&#36873;&#23450;&#30340;&#20986;&#29256;&#29289;&#34987;&#20998;&#20026;&#19977;&#31867;&#65306;&#20165;DTL&#21644;&#20165;IDS&#65292;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#30340;IDS&#65292;&#20197;&#21450;&#22522;&#20110;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#30340;IDS&#12290;&#35813;&#30740;&#31350;&#20840;&#38754;&#22238;&#39038;&#20102;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#28145;&#24230;&#36801;&#31227;&#23398;&#20064;&#24212;&#29992;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Globally, the external Internet is increasingly being connected to the contemporary industrial control system. As a result, there is an immediate need to protect the network from several threats. The key infrastructure of industrial activity may be protected from harm by using an intrusion detection system (IDS), a preventive measure mechanism, to recognize new kinds of dangerous threats and hostile activities. The most recent artificial intelligence (AI) techniques used to create IDS in many kinds of industrial control networks are examined in this study, with a particular emphasis on IDS-based deep transfer learning (DTL). This latter can be seen as a type of information fusion that merge, and/or adapt knowledge from multiple domains to enhance the performance of the target task, particularly when the labeled data in the target domain is scarce. Publications issued after 2015 were taken into account. These selected publications were divided into three categories: DTL-only and IDS-onl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#20559;&#24207;&#25968;&#25454;&#28145;&#24230;&#20989;&#25968;&#30340;&#32972;&#26223;&#19979;Blocher&#31561;&#20154;[2023]&#20013;&#20171;&#32461;&#30340;&#26080;&#20132;&#36890;&#29992;&#38598;&#21512;&#20855;&#26377;&#36830;&#36890;&#24615;&#23646;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.10549</link><description>&lt;p&gt;
&#20851;&#20110;&#26080;&#20132;&#38598;&#20559;&#24207;&#36890;&#29992;&#38598;&#21512;&#30340;&#36830;&#36890;&#24615;&#23646;&#24615;&#30340;&#27880;&#35760;
&lt;/p&gt;
&lt;p&gt;
A note on the connectedness property of union-free generic sets of partial orders. (arXiv:2304.10549v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#20559;&#24207;&#25968;&#25454;&#28145;&#24230;&#20989;&#25968;&#30340;&#32972;&#26223;&#19979;Blocher&#31561;&#20154;[2023]&#20013;&#20171;&#32461;&#30340;&#26080;&#20132;&#36890;&#29992;&#38598;&#21512;&#20855;&#26377;&#36830;&#36890;&#24615;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30701;&#25991;&#25551;&#36848;&#24182;&#35777;&#26126;&#20102;&#22312;&#20559;&#24207;&#25968;&#25454;&#28145;&#24230;&#20989;&#25968;&#30340;&#32972;&#26223;&#19979;Blocher&#31561;&#20154;[2023]&#24341;&#20837;&#30340;&#36830;&#36890;&#24615;&#23646;&#24615;&#12290; &#36830;&#36890;&#24615;&#23646;&#24615;&#20026;&#26080;&#20132;&#36890;&#29992;&#38598;&#21512;&#25552;&#20379;&#20102;&#32467;&#26500;&#24615;&#30340;&#28145;&#20837;&#35748;&#35782;&#12290;&#36825;&#20123;&#38598;&#21512;&#26159;&#22312;Blocher&#31561;&#20154;[2023]&#20013;&#20171;&#32461;&#30340;&#65292;&#23427;&#20204;&#20351;&#29992;&#22312;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#29702;&#35770;&#20013;&#33258;&#28982;&#20986;&#29616;&#30340;&#25152;&#26377;&#20559;&#24207;&#38598;&#21512;&#19978;&#30340;&#38381;&#21253;&#36816;&#31639;&#36827;&#34892;&#23450;&#20041;&#12290;&#22312;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;&#30340;&#35821;&#35328;&#20013;&#65292;&#36830;&#36890;&#24615;&#30340;&#23646;&#24615;&#21487;&#20197;&#29983;&#21160;&#22320;&#34987;&#35777;&#26126;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#22312;Blocher&#31561;&#20154;[2023]&#20013;&#25105;&#20204;&#27809;&#26377;&#35752;&#35770;&#24418;&#24335;&#27010;&#24565;&#20998;&#26512;,&#22240;&#27492;&#25105;&#20204;&#25226;&#35777;&#26126;&#25918;&#21040;&#20102;&#36825;&#37324;&#12290;
&lt;/p&gt;
&lt;p&gt;
This short note describes and proves a connectedness property which was introduced in Blocher et al. [2023] in the context of data depth functions for partial orders. The connectedness property gives a structural insight into union-free generic sets. These sets, presented in Blocher et al. [2023], are defined by using a closure operator on the set of all partial orders which naturally appears within the theory of formal concept analysis. In the language of formal concept analysis, the property of connectedness can be vividly proven. However, since within Blocher et al. [2023] we did not discuss formal concept analysis, we outsourced the proof to this note.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25903;&#25345;&#23450;&#24615;&#20998;&#26512;&#20013;&#30340;&#28436;&#32462;&#32534;&#30721;&#12290;&#36890;&#36807;&#32467;&#21512;GPT-3&#21644;&#19987;&#23478;&#32534;&#20889;&#30340;&#32534;&#30721;&#26412;&#65292;&#30740;&#31350;&#20154;&#21592;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#19982;&#19987;&#23478;&#32534;&#30721;&#32467;&#26524;&#30456;&#36817;&#30340;&#26631;&#35760;&#32467;&#26524;&#65292;&#24182;&#19988;&#36824;&#20801;&#35768;&#36827;&#34892;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#32534;&#30721;&#26412;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.10548</link><description>&lt;p&gt;
&#32467;&#21512;&#32534;&#30721;&#26412;&#21644;GPT-3&#25903;&#25345;&#23450;&#24615;&#20998;&#26512;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Supporting Qualitative Analysis with Large Language Models: Combining Codebook with GPT-3 for Deductive Coding. (arXiv:2304.10548v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25903;&#25345;&#23450;&#24615;&#20998;&#26512;&#20013;&#30340;&#28436;&#32462;&#32534;&#30721;&#12290;&#36890;&#36807;&#32467;&#21512;GPT-3&#21644;&#19987;&#23478;&#32534;&#20889;&#30340;&#32534;&#30721;&#26412;&#65292;&#30740;&#31350;&#20154;&#21592;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#19982;&#19987;&#23478;&#32534;&#30721;&#32467;&#26524;&#30456;&#36817;&#30340;&#26631;&#35760;&#32467;&#26524;&#65292;&#24182;&#19988;&#36824;&#20801;&#35768;&#36827;&#34892;&#39640;&#25928;&#21644;&#26377;&#25928;&#30340;&#32534;&#30721;&#26412;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25991;&#26412;&#20869;&#23481;&#36827;&#34892;&#23450;&#24615;&#20998;&#26512;&#36890;&#36807;&#32473;&#25968;&#25454;&#25171;&#19978;&#26631;&#31614;&#25581;&#31034;&#20102;&#20016;&#23500;&#32780;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22788;&#29702;&#22823;&#22411;&#25968;&#25454;&#38598;&#26102;&#65292;&#36825;&#20010;&#36807;&#31243;&#24448;&#24448;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#20154;&#21147;&#36164;&#28304;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24037;&#20855;&#23637;&#31034;&#20102;&#20854;&#23454;&#29992;&#24615;&#65292;&#20294;&#30740;&#31350;&#20154;&#21592;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#29616;&#25104;&#30340;&#20154;&#24037;&#26234;&#33021;&#36164;&#28304;&#21644;&#25216;&#26415;&#65292;&#26356;&#19981;&#24517;&#35828;&#25361;&#25112;&#37027;&#20123;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#20102;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#25903;&#25345;&#28436;&#32462;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21487;&#33021;&#24615;&#12290;&#28436;&#32462;&#32534;&#30721;&#26159;&#23450;&#24615;&#20998;&#26512;&#30340;&#20027;&#35201;&#31867;&#21035;&#20043;&#19968;&#65292;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#39044;&#20808;&#30830;&#23450;&#30340;&#32534;&#30721;&#26412;&#23558;&#25968;&#25454;&#26631;&#35760;&#21040;&#19968;&#32452;&#22266;&#23450;&#30340;&#32534;&#30721;&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#22522;&#20110;&#22909;&#22855;&#24515;&#39537;&#21160;&#30340;&#38382;&#39064;&#32534;&#30721;&#20219;&#21153;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#36890;&#36807;&#23558;GPT-3&#19982;&#19987;&#23478;&#21046;&#23450;&#30340;&#32534;&#30721;&#26412;&#30456;&#32467;&#21512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#19987;&#23478;&#32534;&#30721;&#32467;&#26524;&#23454;&#29616;&#20102;&#20844;&#24179;&#21040;&#30456;&#24403;&#22823;&#30340;&#19968;&#33268;&#24615;&#65292;&#24182;&#20801;&#35768;&#26377;&#25928;&#22320;&#36827;&#34892;&#32534;&#30721;&#26412;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#32467;&#21512;LLM&#21644;&#28436;&#32462;&#32534;&#30721;&#26159;&#23450;&#24615;&#20998;&#26512;&#30340;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#65292;&#24182;&#20855;&#26377;&#28508;&#22312;&#30340;&#23454;&#36341;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Qualitative analysis of textual contents unpacks rich and valuable information by assigning labels to the data. However, this process is often labor-intensive, particularly when working with large datasets. While recent AI-based tools demonstrate utility, researchers may not have readily available AI resources and expertise, let alone be challenged by the limited generalizability of those task-specific models. In this study, we explored the use of large language models (LLMs) in supporting deductive coding, a major category of qualitative analysis where researchers use pre-determined codebooks to label the data into a fixed set of codes. Instead of training task-specific models, a pre-trained LLM could be used directly for various tasks without fine-tuning through prompt learning. Using a curiosity-driven questions coding task as a case study, we found, by combining GPT-3 with expert-drafted codebooks, our proposed approach achieved fair to substantial agreements with expert-coded resu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#24320;&#21457;&#19968;&#20010;AI&#27169;&#22411;&#35774;&#35745;&#31354;&#38388;&#30340;&#21547;&#20041;&#65292;&#25552;&#20986;&#20004;&#20010;&#35774;&#35745;&#31354;&#38388;&#19982;&#29983;&#25104;AI&#27169;&#22411;&#30456;&#20851;&#65306;&#31532;&#19968;&#20010;&#32771;&#34385;HCI&#22914;&#20309;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;;&#31532;&#20108;&#20010;&#32771;&#34385;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#24433;&#21709;HCI&#12290;</title><link>http://arxiv.org/abs/2304.10547</link><description>&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
The Design Space of Generative Models. (arXiv:2304.10547v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#24320;&#21457;&#19968;&#20010;AI&#27169;&#22411;&#35774;&#35745;&#31354;&#38388;&#30340;&#21547;&#20041;&#65292;&#25552;&#20986;&#20004;&#20010;&#35774;&#35745;&#31354;&#38388;&#19982;&#29983;&#25104;AI&#27169;&#22411;&#30456;&#20851;&#65306;&#31532;&#19968;&#20010;&#32771;&#34385;HCI&#22914;&#20309;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;;&#31532;&#20108;&#20010;&#32771;&#34385;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#24433;&#21709;HCI&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Card&#31561;&#20154;&#30340;&#32463;&#20856;&#35770;&#25991;"&#36755;&#20837;&#35774;&#22791;&#30340;&#35774;&#35745;&#31354;&#38388;"&#24314;&#31435;&#20102;&#35774;&#35745;&#31354;&#38388;&#20316;&#20026;HCI&#20998;&#26512;&#21644;&#21457;&#26126;&#24037;&#20855;&#30340;&#20215;&#20540;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20026;&#26032;&#20852;&#30340;&#39044;&#35757;&#32451;&#29983;&#25104;AI&#27169;&#22411;&#24320;&#21457;&#35774;&#35745;&#31354;&#38388;&#26159;&#25903;&#25345;&#23427;&#20204;&#34701;&#20837;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#31995;&#32479;&#21644;&#23454;&#36341;&#25152;&#24517;&#38656;&#30340;&#12290;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19982;&#29983;&#25104;AI&#27169;&#22411;&#30456;&#20851;&#30340;&#20004;&#20010;&#35774;&#35745;&#31354;&#38388;&#26469;&#25506;&#35752;&#24320;&#21457;AI&#27169;&#22411;&#35774;&#35745;&#31354;&#38388;&#30340;&#21547;&#20041;&#65306;&#31532;&#19968;&#20010;&#32771;&#34385;HCI&#22914;&#20309;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#65288;&#21363;&#27169;&#22411;&#30340;&#30028;&#38754;&#65289;&#65292;&#31532;&#20108;&#20010;&#32771;&#34385;&#29983;&#25104;&#27169;&#22411;&#22914;&#20309;&#24433;&#21709;HCI&#65288;&#21363;&#27169;&#22411;&#20316;&#20026;HCI&#21407;&#22411;&#26448;&#26009;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Card et al.'s classic paper "The Design Space of Input Devices" established the value of design spaces as a tool for HCI analysis and invention. We posit that developing design spaces for emerging pre-trained, generative AI models is necessary for supporting their integration into human-centered systems and practices. We explore what it means to develop an AI model design space by proposing two design spaces relating to generative AI models: the first considers how HCI can impact generative models (i.e., interfaces for models) and the second considers how generative models can impact HCI (i.e., models as an HCI prototyping material).
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#21019;&#24314;&#34913;&#37327;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#12290;&#20182;&#20204;&#20351;&#29992;&#20102;&#34892;&#20026;&#35266;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#38382;&#39064;&#20197;&#34913;&#37327;&#19981;&#21516;&#27700;&#24179;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27700;&#24179;&#21644;&#26041;&#27861;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.10327</link><description>&lt;p&gt;
&#21521;&#30528;&#20154;&#31867;&#21644;&#26426;&#22120;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Towards a Benchmark for Scientific Understanding in Humans and Machines. (arXiv:2304.10327v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10327
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#21019;&#24314;&#34913;&#37327;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#12290;&#20182;&#20204;&#20351;&#29992;&#20102;&#34892;&#20026;&#35266;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#32452;&#38382;&#39064;&#20197;&#34913;&#37327;&#19981;&#21516;&#27700;&#24179;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#36825;&#20010;&#26694;&#26550;&#21487;&#20197;&#24110;&#21161;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27700;&#24179;&#21644;&#26041;&#27861;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31185;&#23398;&#29702;&#35299;&#26159;&#31185;&#23398;&#30340;&#22522;&#26412;&#30446;&#26631;&#65292;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#35299;&#37322;&#19990;&#30028;&#12290;&#30446;&#21069;&#36824;&#27809;&#26377;&#22909;&#30340;&#26041;&#27861;&#26469;&#34913;&#37327;&#20195;&#29702;&#20154;&#30340;&#31185;&#23398;&#29702;&#35299;&#65292;&#26080;&#35770;&#23427;&#20204;&#26159;&#20154;&#31867;&#36824;&#26159;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#32570;&#20047;&#28165;&#26224;&#30340;&#22522;&#20934;&#65292;&#38590;&#20197;&#35780;&#20272;&#21644;&#27604;&#36739;&#19981;&#21516;&#27700;&#24179;&#21644;&#26041;&#27861;&#30340;&#31185;&#23398;&#29702;&#35299;&#12290;&#22312;&#27492;&#36335;&#32447;&#22270;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21033;&#29992;&#31185;&#23398;&#21746;&#23398;&#24037;&#20855;&#21019;&#24314;&#31185;&#23398;&#29702;&#35299;&#30340;&#22522;&#20934;&#12290;&#25105;&#20204;&#37319;&#29992;&#34892;&#20026;&#35266;&#24565;&#65292;&#35748;&#20026;&#30495;&#27491;&#30340;&#29702;&#35299;&#24212;&#35813;&#34987;&#35748;&#20026;&#26159;&#25191;&#34892;&#26576;&#20123;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#32771;&#34385;&#19968;&#32452;&#38382;&#39064;&#26469;&#25193;&#23637;&#36825;&#20010;&#27010;&#24565;&#65292;&#36825;&#20123;&#38382;&#39064;&#21487;&#20197;&#34913;&#37327;&#19981;&#21516;&#27700;&#24179;&#30340;&#31185;&#23398;&#29702;&#35299;&#65292;&#21253;&#25324;&#20449;&#24687;&#26816;&#32034;&#65292;&#23433;&#25490;&#20449;&#24687;&#20197;&#29983;&#25104;&#35299;&#37322;&#30340;&#33021;&#21147;&#20197;&#21450;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#25512;&#26029;&#20107;&#29289;&#20250;&#26377;&#21738;&#20123;&#19981;&#21516;&#12290;Scientific Understanding Benchmark&#65288;SUB&#65289;&#30001;
&lt;/p&gt;
&lt;p&gt;
Scientific understanding is a fundamental goal of science, allowing us to explain the world. There is currently no good way to measure the scientific understanding of agents, whether these be humans or Artificial Intelligence systems. Without a clear benchmark, it is challenging to evaluate and compare different levels of and approaches to scientific understanding. In this Roadmap, we propose a framework to create a benchmark for scientific understanding, utilizing tools from philosophy of science. We adopt a behavioral notion according to which genuine understanding should be recognized as an ability to perform certain tasks. We extend this notion by considering a set of questions that can gauge different levels of scientific understanding, covering information retrieval, the capability to arrange information to produce an explanation, and the ability to infer how things would be different under different circumstances. The Scientific Understanding Benchmark (SUB), which is formed by 
&lt;/p&gt;</description></item><item><title>LARD&#25968;&#25454;&#38598;&#26159;&#29992;&#20110;&#30528;&#38470;&#21644;&#36827;&#22330;&#38454;&#27573;&#30340;&#36305;&#36947;&#26816;&#27979;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#33322;&#25293;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#23427;&#30001;&#22823;&#37327;&#21512;&#25104;&#22270;&#20687;&#21644;&#19968;&#20123;&#20154;&#24037;&#26631;&#27880;&#30340;&#23454;&#38469;&#30528;&#38470;&#38236;&#22836;&#22270;&#20687;&#32452;&#25104;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#29983;&#25104;&#22120;&#20197;&#20135;&#29983;&#27492;&#31867;&#21512;&#25104;&#22270;&#20687;&#24182;&#33258;&#21160;&#27880;&#37322;&#36305;&#36947;&#25296;&#35282;&#28857;&#12290;</title><link>http://arxiv.org/abs/2304.09938</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#35273;&#30340;&#30528;&#38470;&#25968;&#25454;&#38598;-- LARD&#65288;Landing Approach Runway Detection Dataset&#65289;
&lt;/p&gt;
&lt;p&gt;
LARD -- Landing Approach Runway Detection -- Dataset for Vision Based Landing. (arXiv:2304.09938v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09938
&lt;/p&gt;
&lt;p&gt;
LARD&#25968;&#25454;&#38598;&#26159;&#29992;&#20110;&#30528;&#38470;&#21644;&#36827;&#22330;&#38454;&#27573;&#30340;&#36305;&#36947;&#26816;&#27979;&#20219;&#21153;&#30340;&#39640;&#36136;&#37327;&#33322;&#25293;&#22270;&#20687;&#25968;&#25454;&#38598;&#12290;&#23427;&#30001;&#22823;&#37327;&#21512;&#25104;&#22270;&#20687;&#21644;&#19968;&#20123;&#20154;&#24037;&#26631;&#27880;&#30340;&#23454;&#38469;&#30528;&#38470;&#38236;&#22836;&#22270;&#20687;&#32452;&#25104;&#65292;&#21516;&#26102;&#36824;&#25552;&#20379;&#20102;&#29983;&#25104;&#22120;&#20197;&#20135;&#29983;&#27492;&#31867;&#21512;&#25104;&#22270;&#20687;&#24182;&#33258;&#21160;&#27880;&#37322;&#36305;&#36947;&#25296;&#35282;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#33258;&#20027;&#31995;&#32479;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#65292;&#25910;&#38598;&#36275;&#22815;&#19988;&#20195;&#34920;&#24615;&#30340;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#25104;&#20026;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#23613;&#31649;&#33322;&#31354;&#33322;&#22825;&#39046;&#22495;&#30340;&#33258;&#20027;&#30528;&#38470;&#31995;&#32479;&#20855;&#26377;&#24378;&#28872;&#30340;&#23454;&#29992;&#21644;&#21830;&#19994;&#20215;&#20540;&#65292;&#20294;&#32570;&#20047;&#24320;&#28304;&#30340;&#33322;&#31354;&#24433;&#20687;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#36136;&#37327;&#30340;&#33322;&#25293;&#22270;&#20687;&#25968;&#25454;&#38598;-LARD&#65292;&#29992;&#20110;&#30528;&#38470;&#21644;&#36827;&#22330;&#38454;&#27573;&#30340;&#36305;&#36947;&#26816;&#27979;&#20219;&#21153;&#12290;&#25968;&#25454;&#38598;&#22823;&#37096;&#20998;&#30001;&#21512;&#25104;&#22270;&#20687;&#32452;&#25104;&#65292;&#20294;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#20154;&#24037;&#26631;&#27880;&#30340;&#23454;&#38469;&#30528;&#38470;&#38236;&#22836;&#22270;&#20687;&#65292;&#20197;&#25193;&#23637;&#26816;&#27979;&#20219;&#21153;&#21040;&#26356;&#29616;&#23454;&#30340;&#22330;&#26223;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#29983;&#25104;&#22120;&#65292;&#21487;&#20197;&#20135;&#29983;&#36825;&#26679;&#30340;&#21512;&#25104;&#21069;&#35270;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#20960;&#20309;&#21464;&#25442;&#23454;&#29616;&#36305;&#36947;&#25296;&#35282;&#28857;&#30340;&#33258;&#21160;&#27880;&#37322;&#12290;&#35813;&#25968;&#25454;&#38598;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#22914;&#25968;&#25454;&#38598;&#36136;&#37327;&#20998;&#26512;&#25110;&#24320;&#21457;&#27169;&#22411;&#24212;&#23545;&#26816;&#27979;&#20219;&#21153;&#65292;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the interest in autonomous systems continues to grow, one of the major challenges is collecting sufficient and representative real-world data. Despite the strong practical and commercial interest in autonomous landing systems in the aerospace field, there is a lack of open-source datasets of aerial images. To address this issue, we present a dataset-lard-of high-quality aerial images for the task of runway detection during approach and landing phases. Most of the dataset is composed of synthetic images but we also provide manually labelled images from real landing footages, to extend the detection task to a more realistic setting. In addition, we offer the generator which can produce such synthetic front-view images and enables automatic annotation of the runway corners through geometric transformations. This dataset paves the way for further research such as the analysis of dataset quality or the development of models to cope with the detection tasks. Find data, code and more up-to
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#32858;&#31867;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2304.09868</link><description>&lt;p&gt;
&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Accelerate Support Vector Clustering via Spectrum-Preserving Data Compression?. (arXiv:2304.09868v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09868
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#32858;&#31867;&#36895;&#24230;&#32780;&#19981;&#29306;&#29298;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#26159;&#19968;&#31181;&#37325;&#35201;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#20294;&#26159;&#30001;&#20110;&#20854;&#35745;&#31639;&#26114;&#36149;&#30340;&#31751;&#20998;&#37197;&#27493;&#39588;&#65292;&#23427;&#38754;&#20020;&#30528;&#21487;&#20280;&#32553;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20445;&#30041;&#35889;&#30340;&#25968;&#25454;&#21387;&#32553;&#26469;&#21152;&#36895;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#21387;&#32553;&#25104;&#23569;&#37327;&#35889;&#34920;&#31034;&#30340;&#32858;&#21512;&#25968;&#25454;&#28857;&#65292;&#28982;&#21518;&#22312;&#21387;&#32553;&#21518;&#30340;&#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#26631;&#20934;&#30340;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#65292;&#26368;&#21518;&#23558;&#21387;&#32553;&#25968;&#25454;&#38598;&#30340;&#32858;&#31867;&#32467;&#26524;&#26144;&#23556;&#22238;&#21407;&#22987;&#25968;&#25454;&#38598;&#20197;&#21457;&#29616;&#31751;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#36739;&#20110;&#26631;&#20934;&#25903;&#25345;&#21521;&#37327;&#32858;&#31867;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#36895;&#24230;&#65292;&#32780;&#19981;&#20250;&#25439;&#22833;&#32858;&#31867;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support vector clustering is an important clustering method. However, it suffers from a scalability issue due to its computational expensive cluster assignment step. In this paper we accelertate the support vector clustering via spectrum-preserving data compression. Specifically, we first compress the original data set into a small amount of spectrally representative aggregated data points. Then, we perform standard support vector clustering on the compressed data set. Finally, we map the clustering results of the compressed data set back to discover the clusters in the original data set. Our extensive experimental results on real-world data set demonstrate dramatically speedups over standard support vector clustering without sacrificing clustering quality.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#21644;&#23454;&#26045;&#20102;&#19968;&#20010;&#21517;&#20026;MATURE-HEALTH&#30340;&#20581;&#24247;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#39044;&#27979;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#24182;&#25512;&#33616;&#33829;&#20859;&#24179;&#34913;&#30340;&#39135;&#29289;&#65292;&#20174;&#32780;&#22686;&#21152;&#26089;&#26399;&#26816;&#27979;&#30142;&#30149;&#30340;&#26426;&#20250;&#24182;&#38450;&#27490;&#20581;&#24247;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.09099</link><description>&lt;p&gt;
MATURE-HEALTH: MAndatory FeaTURE&#36873;&#25321;&#30340;&#20581;&#24247;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
MATURE-HEALTH: HEALTH Recommender System for MAndatory FeaTURE choices. (arXiv:2304.09099v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#21644;&#23454;&#26045;&#20102;&#19968;&#20010;&#21517;&#20026;MATURE-HEALTH&#30340;&#20581;&#24247;&#25512;&#33616;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#39044;&#27979;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#24182;&#25512;&#33616;&#33829;&#20859;&#24179;&#34913;&#30340;&#39135;&#29289;&#65292;&#20174;&#32780;&#22686;&#21152;&#26089;&#26399;&#26816;&#27979;&#30142;&#30149;&#30340;&#26426;&#20250;&#24182;&#38450;&#27490;&#20581;&#24247;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24179;&#34913;&#30005;&#35299;&#36136;&#23545;&#20110;&#20154;&#20307;&#22120;&#23448;&#30340;&#36866;&#24403;&#21151;&#33021;&#33267;&#20851;&#37325;&#35201;&#21644;&#24517;&#19981;&#21487;&#23569;&#65292;&#22240;&#20026;&#30005;&#35299;&#36136;&#22833;&#34913;&#21487;&#33021;&#26159;&#28508;&#22312;&#30149;&#29702;&#29983;&#29702;&#23398;&#21457;&#23637;&#30340;&#25351;&#31034;&#12290;&#39640;&#25928;&#30417;&#27979;&#30005;&#35299;&#36136;&#22833;&#34913;&#19981;&#20165;&#21487;&#20197;&#22686;&#21152;&#30142;&#30149;&#26089;&#26399;&#26816;&#27979;&#30340;&#26426;&#20250;&#65292;&#32780;&#19988;&#21487;&#20197;&#36890;&#36807;&#20005;&#26684;&#36981;&#24490;&#33829;&#20859;&#25511;&#21046;&#39278;&#39135;&#20197;&#24179;&#34913;&#30005;&#35299;&#36136;&#20174;&#32780;&#38450;&#27490;&#20581;&#24247;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#25512;&#33616;&#31995;&#32479;MATURE Health&#65292;&#35813;&#31995;&#32479;&#39044;&#27979;&#34880;&#28082;&#20013;&#24517;&#38656;&#30005;&#35299;&#36136;&#21644;&#20854;&#20182;&#29289;&#36136;&#30340;&#19981;&#24179;&#34913;&#65292;&#28982;&#21518;&#25512;&#33616;&#21547;&#26377;&#24179;&#34913;&#33829;&#20859;&#30340;&#39135;&#29289;&#65292;&#20197;&#36991;&#20813;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#30340;&#21457;&#29983;&#12290;&#35813;&#27169;&#22411;&#32771;&#34385;&#21040;&#29992;&#25143;&#26368;&#36817;&#30340;&#23454;&#39564;&#23460;&#32467;&#26524;&#21644;&#27599;&#26085;&#39135;&#29289;&#25668;&#20837;&#37327;&#26469;&#39044;&#27979;&#30005;&#35299;&#36136;&#19981;&#24179;&#34913;&#12290;MATURE Health&#20381;&#36182;&#20110;MATURE Food&#31639;&#27861;&#25512;&#33616;&#39135;&#29289;&#65292;&#21518;&#32773;&#20165;&#25512;&#33616;&#37027;&#20123;
&lt;/p&gt;
&lt;p&gt;
Balancing electrolytes is utmost important and essential for appropriate functioning of organs in human body as electrolytes imbalance can be an indication of the development of underlying pathophysiology. Efficient monitoring of electrolytes imbalance not only can increase the chances of early detection of disease, but also prevents the further deterioration of the health by strictly following nutrient controlled diet for balancing the electrolytes post disease detection. In this research, a recommender system MATURE Health is proposed and implemented, which predicts the imbalance of mandatory electrolytes and other substances presented in blood and recommends the food items with the balanced nutrients to avoid occurrence of the electrolytes imbalance. The proposed model takes user most recent laboratory results and daily food intake into account to predict the electrolytes imbalance. MATURE Health relies on MATURE Food algorithm to recommend food items as latter recommends only those
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#21644;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#30446;&#24405;&#20197;&#24110;&#21161;&#20154;&#20204;&#25552;&#39640;&#23545;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.08275</link><description>&lt;p&gt;
&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#26045;&#65306;&#20262;&#29702;&#26041;&#38754;&#30340;&#32039;&#24352;&#21644;&#26435;&#34913;
&lt;/p&gt;
&lt;p&gt;
Implementing Responsible AI: Tensions and Trade-Offs Between Ethics Aspects. (arXiv:2304.08275v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08275
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#21407;&#21017;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#21644;&#26435;&#34913;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#30446;&#24405;&#20197;&#24110;&#21161;&#20154;&#20204;&#25552;&#39640;&#23545;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20154;&#24037;&#26234;&#33021;/&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#28389;&#29992;&#21644;&#19981;&#24403;&#20351;&#29992;&#24341;&#36215;&#30340;&#25285;&#24551;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#36127;&#36131;&#20219;&#20154;&#24037;&#26234;&#33021;&#30340;&#20262;&#29702;&#21407;&#21017;&#12290;&#36825;&#20123;&#20934;&#21017;&#30340;&#22522;&#26412;&#26041;&#38754;&#21253;&#25324;&#38544;&#31169;&#12289;&#20934;&#30830;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#31283;&#20581;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#38754;&#20043;&#38388;&#23384;&#22312;&#28508;&#22312;&#30340;&#32039;&#24352;&#20851;&#31995;&#65292;&#36825;&#32473;&#23547;&#27714;&#36981;&#24490;&#36825;&#20123;&#21407;&#21017;&#30340;AI/ML&#24320;&#21457;&#32773;&#24102;&#26469;&#20102;&#22256;&#38590;&#12290;&#20363;&#22914;&#65292;&#25552;&#39640;AI/ML&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21487;&#33021;&#20250;&#38477;&#20302;&#20854;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#27719;&#32534;&#21644;&#35752;&#35770;10&#20010;&#31361;&#20986;&#30340;&#32039;&#24352;&#20851;&#31995;&#12289;&#26435;&#34913;&#21644;&#20854;&#20182;&#22522;&#26412;&#26041;&#38754;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#20415;&#22312;&#25345;&#32493;&#21162;&#21147;&#23558;&#36825;&#20123;&#21407;&#21017;&#36716;&#21270;&#20026;&#23454;&#36341;&#30340;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#23545;&#21487;&#33021;&#20986;&#29616;&#30340;&#20262;&#29702;&#21407;&#21017;&#26041;&#38754;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#65292;&#24182;&#36890;&#36807;&#22312;&#24191;&#27867;&#25991;&#29486;&#20013;&#30340;&#25903;&#25345;&#36827;&#34892;&#21452;&#38754;&#20114;&#21160;&#30340;&#37325;&#28857;&#35752;&#35770;&#12290;&#36825;&#20010;&#30446;&#24405;&#23545;&#20110;&#25552;&#39640;&#20154;&#20204;&#23545;&#20262;&#29702;&#20934;&#21017;&#26041;&#38754;&#20043;&#38388;&#21487;&#33021;&#30456;&#20114;&#20316;&#29992;&#30340;&#35748;&#35782;&#20197;&#21450;&#20419;&#36827;&#35774;&#35745;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#20570;&#20986;&#26377;&#20805;&#20998;&#20381;&#25454;&#30340;&#21028;&#26029;&#21487;&#33021;&#26377;&#25152;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many sets of ethics principles for responsible AI have been proposed to allay concerns about misuse and abuse of AI/ML systems. The underlying aspects of such sets of principles include privacy, accuracy, fairness, robustness, explainability, and transparency. However, there are potential tensions between these aspects that pose difficulties for AI/ML developers seeking to follow these principles. For example, increasing the accuracy of an AI/ML system may reduce its explainability. As part of the ongoing effort to operationalise the principles into practice, in this work we compile and discuss a catalogue of 10 notable tensions, trade-offs and other interactions between the underlying aspects. We primarily focus on two-sided interactions, drawing on support spread across a diverse literature. This catalogue can be helpful in raising awareness of the possible interactions between aspects of ethics principles, as well as facilitating well-supported judgements by the designers and develo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#22312;&#20013;&#25991;&#35821;&#35328;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2304.07987</link><description>&lt;p&gt;
&#20013;&#25991;&#24320;&#25918;&#24335;&#25351;&#20196;&#24191;&#20041;&#35821;&#35328;&#27169;&#22411;&#65306;&#21021;&#27493;&#21457;&#24067;
&lt;/p&gt;
&lt;p&gt;
Chinese Open Instruction Generalist: A Preliminary Release. (arXiv:2304.07987v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#26088;&#22312;&#36890;&#36807;&#36866;&#24212;&#19981;&#21516;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#20197;&#22635;&#34917;&#25351;&#20196;&#35843;&#25972;&#25216;&#26415;&#22312;&#20013;&#25991;&#35821;&#35328;&#39046;&#22495;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#26500;&#24314;&#24191;&#20041;&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#38543;&#30528;InstructGPT&#21644;ChatGPT&#30340;&#21457;&#24067;&#65292;&#23427;&#24050;&#32463;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#23613;&#31649;&#33521;&#35821;&#20026;&#22522;&#30784;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;&#36824;&#26410;&#25506;&#32034;&#33521;&#35821;&#20026;&#22522;&#30784;&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#35821;&#20219;&#21153;&#19978;&#26159;&#21542;&#21487;&#20197;&#20687;&#33521;&#35821;&#20219;&#21153;&#37027;&#26679;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25351;&#20196;&#35843;&#25972;&#26469;&#25191;&#34892;&#65292;&#20197;&#21450;&#25105;&#20204;&#22914;&#20309;&#26500;&#24314;&#25152;&#38656;&#30340;&#35821;&#26009;&#24211;&#36827;&#34892;&#35843;&#25972;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#39033;&#30446;&#65292;&#35797;&#22270;&#36890;&#36807;&#36866;&#24212;4&#20010;&#23376;&#20219;&#21153;&#30340;&#22266;&#26377;&#29305;&#24615;&#65292;&#37319;&#29992;&#21508;&#31181;&#26041;&#27861;&#21019;&#24314;&#19968;&#20010;&#20013;&#25991;&#25351;&#20196;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#32422;20&#19975;&#20010;&#20013;&#25991;&#25351;&#20196;&#35843;&#25972;&#26679;&#26412;&#65292;&#24182;&#36827;&#34892;&#20102;&#20154;&#24037;&#26816;&#26597;&#20197;&#30830;&#20445;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#33521;&#25991;&#21644;&#20013;&#25991;&#25351;&#20196;&#35821;&#26009;&#24211;&#65292;&#24182;&#23545;&#19968;&#20123;&#28508;&#22312;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#31616;&#35201;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instruction tuning is widely recognized as a key technique for building generalist language models, which has attracted the attention of researchers and the public with the release of InstructGPT~\citep{ouyang2022training} and ChatGPT\footnote{\url{https://chat.openai.com/}}. Despite impressive progress in English-oriented large-scale language models (LLMs), it is still under-explored whether English-based foundation LLMs can perform similarly on multilingual tasks compared to English tasks with well-designed instruction tuning and how we can construct the corpora needed for the tuning.  To remedy this gap, we propose the project as an attempt to create a Chinese instruction dataset by various methods adapted to the intrinsic characteristics of 4 sub-tasks. We collect around 200k Chinese instruction tuning samples, which have been manually checked to guarantee high quality. We also summarize the existing English and Chinese instruction corpora and briefly describe some potential applic
&lt;/p&gt;</description></item><item><title>TPU v4&#26159;&#19968;&#27454;&#25903;&#25345;&#23884;&#20837;&#24335;&#30828;&#20214;&#30340;&#21487;&#37325;&#26500;&#20809;&#23398;&#36229;&#32423;&#35745;&#31639;&#26426;&#65292;&#37319;&#29992;&#20809;&#23398;&#30005;&#36335;&#20132;&#25442;&#26426;&#37325;&#26032;&#37197;&#32622;&#20114;&#36830;&#25299;&#25169;&#65292;&#25552;&#39640;&#35268;&#27169;&#12289;&#21487;&#29992;&#24615;&#12289;&#21033;&#29992;&#29575;&#12289;&#27169;&#22359;&#21270;&#12289;&#37096;&#32626;&#12289;&#23433;&#20840;&#12289;&#21151;&#29575;&#21644;&#24615;&#33021;&#65292;&#23427;&#36890;&#36807;SparseCores&#21152;&#36895;&#23884;&#20837;&#24335;&#27169;&#22411;&#65292;&#24615;&#33021;&#20248;&#36234;&#65292;&#21151;&#32791;&#20302;&#12290;</title><link>http://arxiv.org/abs/2304.01433</link><description>&lt;p&gt;
TPU v4&#65306;&#19968;&#27454;&#25903;&#25345;&#23884;&#20837;&#24335;&#30828;&#20214;&#30340;&#21487;&#37325;&#26500;&#20809;&#23398;&#36229;&#32423;&#35745;&#31639;&#26426;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings. (arXiv:2304.01433v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01433
&lt;/p&gt;
&lt;p&gt;
TPU v4&#26159;&#19968;&#27454;&#25903;&#25345;&#23884;&#20837;&#24335;&#30828;&#20214;&#30340;&#21487;&#37325;&#26500;&#20809;&#23398;&#36229;&#32423;&#35745;&#31639;&#26426;&#65292;&#37319;&#29992;&#20809;&#23398;&#30005;&#36335;&#20132;&#25442;&#26426;&#37325;&#26032;&#37197;&#32622;&#20114;&#36830;&#25299;&#25169;&#65292;&#25552;&#39640;&#35268;&#27169;&#12289;&#21487;&#29992;&#24615;&#12289;&#21033;&#29992;&#29575;&#12289;&#27169;&#22359;&#21270;&#12289;&#37096;&#32626;&#12289;&#23433;&#20840;&#12289;&#21151;&#29575;&#21644;&#24615;&#33021;&#65292;&#23427;&#36890;&#36807;SparseCores&#21152;&#36895;&#23884;&#20837;&#24335;&#27169;&#22411;&#65292;&#24615;&#33021;&#20248;&#36234;&#65292;&#21151;&#32791;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#21019;&#26032;&#65292;&#29983;&#20135;&#24037;&#20316;&#36127;&#36733;&#21457;&#29983;&#20102;&#26681;&#26412;&#24615;&#21644;&#36805;&#36895;&#30340;&#21464;&#21270;&#12290;TPU v4&#26159;&#35895;&#27468;&#30340;&#31532;&#20116;&#20195;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#26550;&#26500;&#65288;DSA&#65289;&#65292;&#26159;&#20854;&#31532;&#19977;&#20010;&#29992;&#20110;&#22788;&#29702;&#27492;&#31867;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36229;&#32423;&#35745;&#31639;&#26426;&#12290;&#20809;&#23398;&#30005;&#36335;&#20132;&#25442;&#26426;&#65288;OCS&#65289;&#21160;&#24577;&#37325;&#26032;&#37197;&#32622;&#20854;&#20114;&#36830;&#25299;&#25169;&#65292;&#20197;&#25552;&#39640;&#35268;&#27169;&#12289;&#21487;&#29992;&#24615;&#12289;&#21033;&#29992;&#29575;&#12289;&#27169;&#22359;&#21270;&#12289;&#37096;&#32626;&#12289;&#23433;&#20840;&#12289;&#21151;&#29575;&#21644;&#24615;&#33021;&#12290;&#37096;&#32626;&#33258;2020&#24180;&#20197;&#26469;&#65292;TPU v4&#36229;&#32423;&#35745;&#31639;&#26426;&#30340;&#34920;&#29616;&#20248;&#20110;TPU v3&#65292;&#21516;&#26102;&#24615;&#33021;/Watt&#25552;&#39640;&#20102;2.7&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
In response to innovations in machine learning (ML) models, production workloads changed radically and rapidly. TPU v4 is the fifth Google domain specific architecture (DSA) and its third supercomputer for such ML models. Optical circuit switches (OCSes) dynamically reconfigure its interconnect topology to improve scale, availability, utilization, modularity, deployment, security, power, and performance; users can pick a twisted 3D torus topology if desired. Much cheaper, lower power, and faster than Infiniband, OCSes and underlying optical components are &lt;5% of system cost and &lt;3% of system power. Each TPU v4 includes SparseCores, dataflow processors that accelerate models that rely on embeddings by 5x-7x yet use only 5% of die area and power. Deployed since 2020, TPU v4 outperforms TPU v3 by 2.1x and improves performance/Watt by 2.7x. The TPU v4 supercomputer is 4x larger at 4096 chips and thus ~10x faster overall, which along with OCS flexibility helps large language models. For sim
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WAE-MDP&#30340;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;RL&#31574;&#30053;&#20013;&#25552;&#21462;&#24418;&#24335;&#21487;&#39564;&#35777;&#25511;&#21046;&#22120;&#65292;&#24182;&#19988;&#20855;&#26377;&#24179;&#34913;&#25511;&#21046;&#24615;&#33021;&#21644;&#23433;&#20840;&#20043;&#38388;&#30340;&#25928;&#26524;&#21644;&#35299;&#20915;&#19968;&#20123;&#23398;&#20064;&#32570;&#38519;&#30340;&#22810;&#26041;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2303.12558</link><description>&lt;p&gt;
Wasserstein&#33258;&#32534;&#30721;MDPs&#65306;&#20855;&#26377;&#22810;&#26041;&#20445;&#35777;&#30340;&#39640;&#25928;RL&#31574;&#30053;&#27491;&#24335;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Wasserstein Auto-encoded MDPs: Formal Verification of Efficiently Distilled RL Policies with Many-sided Guarantees. (arXiv:2303.12558v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12558
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;WAE-MDP&#30340;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#20219;&#20309;RL&#31574;&#30053;&#20013;&#25552;&#21462;&#24418;&#24335;&#21487;&#39564;&#35777;&#25511;&#21046;&#22120;&#65292;&#24182;&#19988;&#20855;&#26377;&#24179;&#34913;&#25511;&#21046;&#24615;&#33021;&#21644;&#23433;&#20840;&#20043;&#38388;&#30340;&#25928;&#26524;&#21644;&#35299;&#20915;&#19968;&#20123;&#23398;&#20064;&#32570;&#38519;&#30340;&#22810;&#26041;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26377;&#35768;&#22810;&#25104;&#21151;&#26696;&#20363;&#65292;&#20294;&#36890;&#36807;&#36825;&#20123;&#20808;&#36827;&#25216;&#26415;&#23398;&#20064;&#30340;&#20915;&#31574;&#32773;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#30340;&#22823;&#35268;&#27169;&#37096;&#32626;&#21463;&#21040;&#27491;&#24335;&#20445;&#35777;&#19981;&#36275;&#30340;&#38459;&#30861;&#12290;&#21464;&#20998;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;VAE-MDPs&#65289;&#26159;&#31163;&#25955;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#20174;&#20219;&#20309;RL&#31574;&#30053;&#20013;&#25552;&#21462;&#24418;&#24335;&#21487;&#39564;&#35777;&#25511;&#21046;&#22120;&#30340;&#21487;&#38752;&#26694;&#26550;&#12290;&#34429;&#28982;&#30456;&#20851;&#20445;&#35777;&#28085;&#30422;&#20102;&#23454;&#38469;&#38382;&#39064;&#30340;&#28385;&#36275;&#24615;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#65292;&#20294;VAE&#26041;&#27861;&#22240;&#32570;&#20047;&#25277;&#35937;&#21644;&#34920;&#31034;&#20445;&#35777;&#20197;&#25903;&#25345;&#28508;&#22312;&#26368;&#20248;&#21270;&#32780;&#36973;&#21463;&#22810;&#31181;&#23398;&#20064;&#32570;&#38519;&#65288;&#21518;&#39564;&#23849;&#22604;&#65292;&#23398;&#20064;&#36895;&#24230;&#24930;&#65292;&#21160;&#21147;&#23398;&#20272;&#35745;&#19981;&#33391;&#65289;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Wasserstein&#33258;&#32534;&#30721;MDP&#65288;WAE-MDP&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#28508;&#22312;&#31354;&#38388;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25191;&#34892;&#21407;&#22987;&#31574;&#30053;&#30340;&#26234;&#33021;&#20307;&#34892;&#20026;&#21644;&#25552;&#21462;&#20986;&#30340;&#31574;&#30053;&#20043;&#38388;&#30340;&#26368;&#20248;&#36716;&#36816;&#30340;&#24809;&#32602;&#24418;&#24335;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#20197;&#26377;&#21033;&#20110;&#25511;&#21046;&#24615;&#33021;&#21644;&#23433;&#20840;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#21516;&#26102;&#20943;&#36731;&#20102;&#19978;&#36848;&#30340;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#25512;&#23548;&#20102;&#20851;&#20110;&#24615;&#33021;&#21644;&#23433;&#20840;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;RL&#22522;&#20934;&#19978;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although deep reinforcement learning (DRL) has many success stories, the large-scale deployment of policies learned through these advanced techniques in safety-critical scenarios is hindered by their lack of formal guarantees. Variational Markov Decision Processes (VAE-MDPs) are discrete latent space models that provide a reliable framework for distilling formally verifiable controllers from any RL policy. While the related guarantees address relevant practical aspects such as the satisfaction of performance and safety properties, the VAE approach suffers from several learning flaws (posterior collapse, slow learning speed, poor dynamics estimates), primarily due to the absence of abstraction and representation guarantees to support latent optimization. We introduce the Wasserstein auto-encoded MDP (WAE-MDP), a latent space model that fixes those issues by minimizing a penalized form of the optimal transport between the behaviors of the agent executing the original policy and the disti
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#26694;&#26550;&#65292;&#21517;&#20026;Q4RealBPP&#65292;&#21487;&#20197;&#32771;&#34385;&#30495;&#23454;&#19990;&#30028;&#30340;&#38480;&#21046;&#29305;&#24449;&#65292;&#35299;&#20915;&#19977;&#32500;&#35013;&#31665;&#38382;&#39064;&#65292;&#25903;&#25345;&#24037;&#19994;&#21644;&#29289;&#27969;&#34892;&#19994;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2303.01977</link><description>&lt;p&gt;
&#29992;&#37327;&#23376;&#36864;&#28779;&#22120;&#35299;&#20915;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#35013;&#31665;&#38382;&#39064;&#30340;&#28151;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Hybrid Approach for Solving Real-World Bin Packing Problem Instances Using Quantum Annealers. (arXiv:2303.01977v2 [cs.ET] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01977
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#37327;&#23376;-&#32463;&#20856;&#28151;&#21512;&#26694;&#26550;&#65292;&#21517;&#20026;Q4RealBPP&#65292;&#21487;&#20197;&#32771;&#34385;&#30495;&#23454;&#19990;&#30028;&#30340;&#38480;&#21046;&#29305;&#24449;&#65292;&#35299;&#20915;&#19977;&#32500;&#35013;&#31665;&#38382;&#39064;&#65292;&#25903;&#25345;&#24037;&#19994;&#21644;&#29289;&#27969;&#34892;&#19994;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#22320;&#23558;&#29289;&#21697;&#35013;&#20837;&#31665;&#23376;&#26159;&#19968;&#39033;&#24120;&#35265;&#30340;&#26085;&#24120;&#20219;&#21153;&#65292;&#20063;&#34987;&#31216;&#20026;&#8220;&#35013;&#31665;&#38382;&#39064;&#8221;&#12290;&#30001;&#20110;&#34892;&#19994;&#21644;&#29289;&#27969;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#23427;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#33258;&#20960;&#21313;&#24180;&#20197;&#26469;&#65292;&#35768;&#22810;&#21464;&#31181;&#24050;&#34987;&#25552;&#20986;&#65292;&#20854;&#20013;&#19977;&#32500;&#35013;&#31665;&#38382;&#39064;&#26159;&#26368;&#25509;&#36817;&#23454;&#38469;&#29992;&#20363;&#30340;&#19968;&#20010;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#28151;&#21512;&#30340;&#37327;&#23376;-&#32463;&#20856;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#32771;&#34385;&#19981;&#21516;&#29616;&#23454;&#29305;&#24449;&#30340;&#19977;&#32500;&#35013;&#31665;&#38382;&#39064;&#65288;Q4RealBPP&#65289;&#65292;&#20363;&#22914;&#65306;&#65288;i&#65289;&#21253;&#35013;&#21644;&#31665;&#23376;&#23610;&#23544;&#65292;&#65288;ii&#65289;&#36229;&#37325;&#38480;&#21046;&#65292;&#65288;iii&#65289;&#29289;&#21697;&#31867;&#21035;&#20043;&#38388;&#30340;&#20146;&#21644;&#21147;&#21644;&#65288;iv&#65289;&#29289;&#21697;&#25490;&#24207;&#30340;&#20559;&#22909;&#12290;Q4RealBPP&#20801;&#35768;&#35299;&#20915;&#32771;&#34385;&#21040;&#24037;&#19994;&#21644;&#29289;&#27969;&#37096;&#38376;&#24191;&#27867;&#35780;&#20215;&#30340;&#38480;&#21046;&#30340;3dBPP&#30340;&#29616;&#23454;&#23548;&#21521;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Efficient packing of items into bins is a common daily task. Known as Bin Packing Problem, it has been intensively studied in the field of artificial intelligence, thanks to the wide interest from industry and logistics. Since decades, many variants have been proposed, with the three-dimensional Bin Packing Problem as the closest one to real-world use cases. We introduce a hybrid quantum-classical framework for solving real-world three-dimensional Bin Packing Problems (Q4RealBPP), considering different realistic characteristics, such as: i) package and bin dimensions, ii) overweight restrictions, iii) affinities among item categories and iv) preferences for item ordering. Q4RealBPP permits the solving of real-world oriented instances of 3dBPP, contemplating restrictions well appreciated by industrial and logistics sectors.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#30340;&#20844;&#27491;&#26234;&#24935;&#22478;&#24066;&#25919;&#31574;&#35843;&#25972;&#21644;&#29983;&#25104;&#26041;&#27861;Fairguard&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#38745;&#24577;&#29983;&#25104;&#21644;&#21160;&#24577;&#35843;&#33410;&#65292;&#32531;&#35299;&#30001;&#22810;&#31181;&#25968;&#25454;&#21644;&#31639;&#27861;&#20559;&#35265;&#23548;&#33268;&#30340;&#19981;&#20844;&#27491;&#39044;&#27979;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.11137</link><description>&lt;p&gt;
Fairguard: &#22312;&#26234;&#24935;&#22478;&#24066;&#20013;&#21033;&#29992;&#22522;&#20110;&#36923;&#36753;&#30340;&#20844;&#27491;&#35268;&#21017;
&lt;/p&gt;
&lt;p&gt;
Fairguard: Harness Logic-based Fairness Rules in Smart Cities. (arXiv:2302.11137v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#30340;&#20844;&#27491;&#26234;&#24935;&#22478;&#24066;&#25919;&#31574;&#35843;&#25972;&#21644;&#29983;&#25104;&#26041;&#27861;Fairguard&#65292;&#36890;&#36807;&#20004;&#20010;&#38454;&#27573;&#30340;&#38745;&#24577;&#29983;&#25104;&#21644;&#21160;&#24577;&#35843;&#33410;&#65292;&#32531;&#35299;&#30001;&#22810;&#31181;&#25968;&#25454;&#21644;&#31639;&#27861;&#20559;&#35265;&#23548;&#33268;&#30340;&#19981;&#20844;&#27491;&#39044;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#24935;&#22478;&#24066;&#36816;&#34892;&#22312;&#35745;&#31639;&#39044;&#27979;&#26694;&#26550;&#19978;&#65292;&#25910;&#38598;&#12289;&#25972;&#21512;&#21644;&#21033;&#29992;&#22823;&#35268;&#27169;&#20256;&#24863;&#22120;&#32593;&#32476;&#30340;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26694;&#26550;&#23481;&#26131;&#21463;&#21040;&#22810;&#31181;&#25968;&#25454;&#21644;&#31639;&#27861;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#36825;&#32463;&#24120;&#23548;&#33268;&#19981;&#20844;&#27491;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#26412;&#25991;&#39318;&#20808;&#36890;&#36807;&#30740;&#31350;&#30000;&#32435;&#35199;&#24030;&#26597;&#22612;&#21162;&#21152;&#30340;&#30495;&#23454;&#22478;&#24066;&#25968;&#25454;&#65292;&#23637;&#31034;&#20102;&#20559;&#35265;&#22312;&#24494;&#35266;&#23618;&#38754;&#19978;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#19978;&#20173;&#28982;&#23384;&#22312;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#31181;&#20559;&#35265;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Fairguard&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#24494;&#35266;&#23618;&#38754;&#26102;&#38388;&#36923;&#36753;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#31354;&#38388;&#22495;&#20013;&#36827;&#34892;&#20844;&#27491;&#30340;&#26234;&#24935;&#22478;&#24066;&#25919;&#31574;&#35843;&#25972;&#21644;&#29983;&#25104;&#12290;Fairguard&#26694;&#26550;&#30001;&#20004;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#38745;&#24577;&#29983;&#25104;&#22120;&#65292;&#33021;&#22815;&#36890;&#36807;&#26368;&#23567;&#21270;&#25152;&#36873;&#23646;&#24615;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#26465;&#20214;&#26469;&#20943;&#23569;&#25968;&#25454;&#20559;&#35265;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#30830;&#20445;&#39044;&#27979;&#31639;&#27861;&#30340;&#20844;&#27491;&#24615;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21160;&#24577;&#32452;&#20214;&#26469;&#35843;&#33410;&#39044;&#27979;&#32467;&#26524;&#65292;&#24182;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#29983;&#25104;&#26410;&#26469;&#30340;&#20844;&#27491;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Smart cities operate on computational predictive frameworks that collect, aggregate, and utilize data from large-scale sensor networks. However, these frameworks are prone to multiple sources of data and algorithmic bias, which often lead to unfair prediction results. In this work, we first demonstrate that bias persists at a micro-level both temporally and spatially by studying real city data from Chattanooga, TN. To alleviate the issue of such bias, we introduce Fairguard, a micro-level temporal logic-based approach for fair smart city policy adjustment and generation in complex temporal-spatial domains. The Fairguard framework consists of two phases: first, we develop a static generator that is able to reduce data bias based on temporal logic conditions by minimizing correlations between selected attributes. Then, to ensure fairness in predictive algorithms, we design a dynamic component to regulate prediction results and generate future fair predictions by harnessing logic rules. E
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#36136;&#37327;&#39184;&#21381;&#35780;&#35770;&#29983;&#25104;&#34394;&#20551;&#35780;&#35770;&#24182;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39044;&#27979;&#34394;&#20551;&#35780;&#35770;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#19978;&#23545;&#36825;&#20123;&#35780;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#27492;&#31867;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#38754;&#20020;&#30340;&#25345;&#32493;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2302.07731</link><description>&lt;p&gt;
AI&#23545;&#25239;AI&#65306;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#25171;&#20987;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#39184;&#21381;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
Combat AI With AI: Counteract Machine-Generated Fake Restaurant Reviews on Social Media. (arXiv:2302.07731v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#39640;&#36136;&#37327;&#39184;&#21381;&#35780;&#35770;&#29983;&#25104;&#34394;&#20551;&#35780;&#35770;&#24182;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39044;&#27979;&#34394;&#20551;&#35780;&#35770;&#30340;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#25506;&#32034;&#20102;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#19978;&#23545;&#36825;&#20123;&#35780;&#35770;&#36827;&#34892;&#20998;&#26512;&#65292;&#27492;&#31867;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#26159;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#38754;&#20020;&#30340;&#25345;&#32493;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#29983;&#25104;&#27169;&#22411;&#65288;&#22914;GPT&#65289;&#30340;&#21457;&#23637;&#20351;&#24471;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#21046;&#36896;&#20986;&#38590;&#20197;&#21306;&#20998;&#30340;&#34394;&#20551;&#39038;&#23458;&#35780;&#35770;&#65292;&#20174;&#32780;&#23545;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26816;&#27979;&#36825;&#20123;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#36896;&#25104;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;Yelp&#39564;&#35777;&#30340;&#39640;&#36136;&#37327;&#30340;&#31934;&#33521;&#39184;&#21381;&#35780;&#35770;&#26469;&#29983;&#25104;OpenAI GPT&#35780;&#35770;&#29983;&#25104;&#22120;&#30340;&#34394;&#20551;&#35780;&#35770;&#65292;&#24182;&#26368;&#32456;&#24494;&#35843;GPT&#36755;&#20986;&#26816;&#27979;&#22120;&#26469;&#39044;&#27979;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#30340;&#34394;&#20551;&#35780;&#35770;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#39044;&#27979;&#38750;&#31934;&#33521;&#35780;&#35770;&#65292;&#24182;&#22312;&#20960;&#20010;&#32500;&#24230;&#65288;&#22914;&#35780;&#35770;&#12289;&#29992;&#25143;&#21644;&#39184;&#21381;&#29305;&#24449;&#20197;&#21450;&#20889;&#20316;&#39118;&#26684;&#65289;&#19978;&#35782;&#21035;&#27169;&#24335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#27491;&#22312;&#19981;&#26029;&#38754;&#20020;&#26426;&#22120;&#29983;&#25104;&#30340;&#34394;&#20551;&#35780;&#35770;&#30340;&#25361;&#25112;&#65292;&#23613;&#31649;&#20182;&#20204;&#21487;&#33021;&#23454;&#26045;&#26816;&#27979;&#31995;&#32479;&#20197;&#36807;&#28388;&#20986;&#21487;&#30097;&#30340;&#35780;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in generative models such as GPT may be used to fabricate indistinguishable fake customer reviews at a much lower cost, thus posing challenges for social media platforms to detect these machine-generated fake reviews. We propose to leverage the high-quality elite restaurant reviews verified by Yelp to generate fake reviews from the OpenAI GPT review creator and ultimately fine-tune a GPT output detector to predict fake reviews that significantly outperform existing solutions. We further apply the model to predict non-elite reviews and identify the patterns across several dimensions, such as review, user and restaurant characteristics, and writing style. We show that social media platforms are continuously challenged by machine-generated fake reviews, although they may implement detection systems to filter out suspicious reviews.
&lt;/p&gt;</description></item><item><title>&#21453;&#21319;&#32423;&#25110;&#27010;&#25324;&#26159;&#24402;&#32435;&#25512;&#29702;&#20013;&#20351;&#29992;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#26159;&#23450;&#29702;&#35777;&#26126;&#30340;&#21452;&#37325;&#25805;&#20316;&#20043;&#19968;&#12290;&#35813;&#35843;&#26597;&#25253;&#21578;&#23545;&#21453;&#21319;&#32423;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#24402;&#32435;&#21644;&#24635;&#32467;&#12290;</title><link>http://arxiv.org/abs/2302.00277</link><description>&lt;p&gt;
&#21453;&#21319;&#32423;&#19982;&#27010;&#25324;&#65306;&#19968;&#20221;&#35843;&#26597;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Anti-unification and Generalization: A Survey. (arXiv:2302.00277v3 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00277
&lt;/p&gt;
&lt;p&gt;
&#21453;&#21319;&#32423;&#25110;&#27010;&#25324;&#26159;&#24402;&#32435;&#25512;&#29702;&#20013;&#20351;&#29992;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#26159;&#23450;&#29702;&#35777;&#26126;&#30340;&#21452;&#37325;&#25805;&#20316;&#20043;&#19968;&#12290;&#35813;&#35843;&#26597;&#25253;&#21578;&#23545;&#21453;&#21319;&#32423;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#36827;&#34892;&#20102;&#31995;&#32479;&#24402;&#32435;&#21644;&#24635;&#32467;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21319;&#32423;&#65288;AU&#65289;&#21448;&#31216;&#27010;&#25324;&#65292;&#26159;&#24402;&#32435;&#25512;&#29702;&#20013;&#20351;&#29992;&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#26159;&#23450;&#29702;&#35777;&#26126;&#22522;&#30784;&#19978;&#30340;&#21452;&#37325;&#25805;&#20316;&#20043;&#19968;&#12290; AI&#21644;&#30456;&#20851;&#31038;&#21306;&#23545;AU&#30340;&#20852;&#36259;&#26085;&#30410;&#22686;&#38271;&#65292;&#20294;&#27809;&#26377;&#31995;&#32479;&#30740;&#31350;&#35813;&#27010;&#24565;&#65292;&#20063;&#27809;&#26377;&#29616;&#26377;&#24037;&#20316;&#30340;&#35843;&#26597;&#65292;&#35843;&#26597;&#24448;&#24448;&#20250;&#37319;&#29992;&#29305;&#23450;&#20110;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#32780;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#24050;&#32463;&#34987;&#29616;&#26377;&#26041;&#27861;&#35206;&#30422;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20221;&#26377;&#20851;AU&#30740;&#31350;&#21450;&#20854;&#24212;&#29992;&#30340;&#35843;&#26597;&#25253;&#21578;&#65292;&#20197;&#21450;&#19968;&#31181;&#23558;&#29616;&#26377;&#21644;&#26410;&#26469;&#21457;&#23637;&#20998;&#31867;&#30340;&#36890;&#29992;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anti-unification (AU), also known as generalization, is a fundamental operation used for inductive inference and is the dual operation to unification, an operation at the foundation of theorem proving. Interest in AU from the AI and related communities is growing, but without a systematic study of the concept, nor surveys of existing work, investigations7 often resort to developing application-specific methods that may be covered by existing approaches. We provide the first survey of AU research and its applications, together with a general framework for categorizing existing and future developments.
&lt;/p&gt;</description></item><item><title>PyExperimenter&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#31639;&#27861;&#23454;&#35777;&#30740;&#31350;&#35774;&#35745;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#31616;&#21270;&#23454;&#39564;&#35774;&#32622;&#12289;&#25191;&#34892;&#12289;&#25991;&#26723;&#32534;&#20889;&#19982;&#32467;&#26524;&#35780;&#20272;&#65292;&#33410;&#30465;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2301.06348</link><description>&lt;p&gt;
PyExperimenter&#65306;&#31616;&#21333;&#22320;&#20998;&#21457;&#23454;&#39564;&#24182;&#36319;&#36394;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
PyExperimenter: Easily distribute experiments and track results. (arXiv:2301.06348v2 [cs.DC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06348
&lt;/p&gt;
&lt;p&gt;
PyExperimenter&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#31639;&#27861;&#23454;&#35777;&#30740;&#31350;&#35774;&#35745;&#30340;&#24037;&#20855;&#65292;&#33021;&#22815;&#31616;&#21270;&#23454;&#39564;&#35774;&#32622;&#12289;&#25191;&#34892;&#12289;&#25991;&#26723;&#32534;&#20889;&#19982;&#32467;&#26524;&#35780;&#20272;&#65292;&#33410;&#30465;&#22823;&#37327;&#25163;&#21160;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PyExperimenter&#26159;&#19968;&#20010;&#24037;&#20855;&#65292;&#26088;&#22312;&#31616;&#21270;&#31639;&#27861;&#23454;&#35777;&#30740;&#31350;&#30340;&#35774;&#32622;&#12289;&#25991;&#26723;&#32534;&#20889;&#12289;&#25191;&#34892;&#21644;&#38543;&#21518;&#30340;&#32467;&#26524;&#35780;&#20272;&#65292;&#24182;&#29305;&#21035;&#35774;&#35745;&#20026;&#26174;&#33879;&#20943;&#23569;&#25152;&#28041;&#21450;&#30340;&#25163;&#21160;&#24037;&#20316;&#12290;&#23427;&#26088;&#22312;&#34987;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#20351;&#29992;&#65292;&#20294;&#24182;&#19981;&#20165;&#38480;&#20110;&#27492;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
PyExperimenter is a tool to facilitate the setup, documentation, execution, and subsequent evaluation of results from an empirical study of algorithms and in particular is designed to reduce the involved manual effort significantly. It is intended to be used by researchers in the field of artificial intelligence, but is not limited to those.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.06751</link><description>&lt;p&gt;
&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#21152;&#36895;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#30340;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Speeding up Multi-objective Non-hierarchical Hyperparameter Optimization by Task Similarity-Based Meta-Learning for the Tree-structured Parzen Estimator. (arXiv:2212.06751v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.06751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20219;&#21153;&#30456;&#20284;&#24230;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#21152;&#36895;&#26641;&#24418;&#32467;&#26500;Parzen&#20272;&#35745;&#20013;&#30340;&#22810;&#30446;&#26631;&#38750;&#20998;&#23618;&#36229;&#21442;&#25968;&#26368;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#26159;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#24615;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#23454;&#36341;&#32773;&#36890;&#24120;&#38754;&#20020;&#22810;&#20010;&#26041;&#38754;&#30340;&#26435;&#34913;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#26102;&#38388;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#30340;&#39640;&#35745;&#31639;&#38656;&#27714;&#21644;&#23545;&#39640;&#25928;&#36229;&#21442;&#25968;&#20248;&#21270;&#30340;&#19981;&#26029;&#22686;&#38271;&#38656;&#27714;&#19979;&#65292;&#21152;&#36895;&#22810;&#30446;&#26631;&#20248;&#21270;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23558;TPE&#30340;&#25910;&#36141;&#20989;&#25968;&#25193;&#23637;&#21040;&#20803;&#23398;&#20064;&#35774;&#32622;&#20013;&#65292;&#20351;&#29992;&#30001;&#20219;&#21153;&#20043;&#38388;&#39030;&#32423;&#22495;&#20043;&#38388;&#30340;&#37325;&#21472;&#24230;&#23450;&#20041;&#30340;&#20219;&#21153;&#30456;&#20284;&#24615;&#12290;&#25105;&#20204;&#20063;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#24182;&#35299;&#20915;&#20102;&#20219;&#21153;&#30456;&#20284;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#34920;&#26684;HPO&#22522;&#20934;&#19978;&#21152;&#36895;&#20102;MO-TPE&#65292;&#24182;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#36194;&#24471;AutoML 2022&#26469;&#24471;&#21040;&#22806;&#37096;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is a vital step in improving performance in deep learning (DL). Practitioners are often faced with the trade-off between multiple criteria, such as accuracy and latency. Given the high computational needs of DL and the growing demand for efficient HPO, the acceleration of multi-objective (MO) optimization becomes ever more important. Despite the significant body of work on meta-learning for HPO, existing methods are inapplicable to MO tree-structured Parzen estimator (MO-TPE), a simple yet powerful MO-HPO algorithm. In this paper, we extend TPE's acquisition function to the meta-learning setting using a task similarity defined by the overlap of top domains between tasks. We also theoretically analyze and address the limitations of our task similarity. In the experiments, we demonstrate that our method speeds up MO-TPE on tabular HPO benchmarks and attains state-of-the-art performance. Our method was also validated externally by winning the AutoML 2022 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28040;&#38500;OCV&#65288;Aliasing&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#22797;&#25968;&#21367;&#31215;&#65292;&#21516;&#26102;&#37319;&#29992;Gabor&#26679;&#24335;&#30340;&#21367;&#31215;&#26680;&#65292;&#21487;&#20197;&#25552;&#39640;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.00394</link><description>&lt;p&gt;
&#20174;CNN&#21040;&#22522;&#20110;&#22797;&#23567;&#27874;&#30340;&#24179;&#31227;&#19981;&#21464;&#21452;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
From CNNs to Shift-Invariant Twin Models Based on Complex Wavelets. (arXiv:2212.00394v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00394
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28040;&#38500;OCV&#65288;Aliasing&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#22797;&#25968;&#21367;&#31215;&#65292;&#21516;&#26102;&#37319;&#29992;Gabor&#26679;&#24335;&#30340;&#21367;&#31215;&#26680;&#65292;&#21487;&#20197;&#25552;&#39640;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25239;&#28151;&#21472;&#26041;&#27861;&#26469;&#22686;&#21152;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24179;&#31227;&#19981;&#21464;&#24615;&#21644;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#29992;&#8220;&#22797;&#20540;&#21367;&#31215;+&#27169;&#36816;&#31639;&#8221;&#65288;$\mathbb{C}$Mod&#65289;&#20195;&#26367;&#31532;&#19968;&#23618;&#30340;&#8220;&#23454;&#20540;&#21367;&#31215;+&#26368;&#22823;&#27744;&#21270;&#8221;&#65288;$\mathbb{R}$Max&#65289;&#65292;&#22240;&#20026;&#23427;&#31283;&#23450;&#20110;&#24179;&#31227;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#22768;&#31216;&#24403;&#21367;&#31215;&#26680;&#26159;&#24102;&#36890;&#21644;&#23450;&#21521;&#30340;&#65288;&#31867;&#20284;&#20110;Gabor&#28388;&#27874;&#22120;&#65289;&#26102;&#65292;$\mathbb{C}$Mod&#21644;$\mathbb{R}$Max&#20135;&#29983;&#21487;&#27604;&#36739;&#30340;&#36755;&#20986;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;$\mathbb{C}$Mod&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;$\mathbb{R}$Max&#30340;&#31283;&#23450;&#26367;&#20195;&#21697;&#12290;&#22240;&#27492;&#65292;&#22312;&#25239;&#28151;&#21472;&#20043;&#21069;&#65292;&#25105;&#20204;&#24378;&#21046;&#21367;&#31215;&#26680;&#37319;&#29992;&#36825;&#31181;Gabor&#26679;&#24335;&#30340;&#32467;&#26500;&#12290;&#30456;&#24212;&#30340;&#26550;&#26500;&#31216;&#20026;&#25968;&#23398;&#21452;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20351;&#29992;&#19968;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#25968;&#23398;&#36816;&#31639;&#31526;&#26469;&#27169;&#25311;&#21407;&#22987;&#30340;&#33258;&#30001;&#35757;&#32451;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30340;&#25239;&#28151;&#21472;&#26041;&#27861;&#22312;Imagenet&#21644;CIFAR-10&#20998;&#31867;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#27604;&#20043;&#21069;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a novel antialiasing method to increase shift invariance and prediction accuracy in convolutional neural networks. Specifically, we replace the first-layer combination "real-valued convolutions + max pooling" ($\mathbb{R}$Max) by "complex-valued convolutions + modulus" ($\mathbb{C}$Mod), which is stable to translations. To justify our approach, we claim that $\mathbb{C}$Mod and $\mathbb{R}$Max produce comparable outputs when the convolution kernel is band-pass and oriented (Gabor-like filter). In this context, $\mathbb{C}$Mod can be considered as a stable alternative to $\mathbb{R}$Max. Thus, prior to antialiasing, we force the convolution kernels to adopt such a Gabor-like structure. The corresponding architecture is called mathematical twin, because it employs a well-defined mathematical operator to mimic the behavior of the original, freely-trained model. Our antialiasing approach achieves superior accuracy on ImageNet and CIFAR-10 classification tasks, compared to prior 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#26041;&#27861;&#65292;&#26159;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#30340;&#32422;&#26463;&#38480;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#25490;&#21517;&#12290;</title><link>http://arxiv.org/abs/2211.14411</link><description>&lt;p&gt;
c-TPE:&#22522;&#20110;&#26641;&#24418;&#32467;&#26500;&#30340;&#24102;&#19981;&#31561;&#24335;&#32422;&#26463;&#30340;&#24085;&#25463;&#26031;&#29305;&#20272;&#35745;&#22120;&#29992;&#20110;&#26114;&#36149;&#36229;&#21442;&#25968;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
c-TPE: Tree-structured Parzen Estimator with Inequality Constraints for Expensive Hyperparameter Optimization. (arXiv:2211.14411v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14411
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#26041;&#27861;&#65292;&#26159;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#21487;&#26377;&#25928;&#22788;&#29702;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#30340;&#32422;&#26463;&#38480;&#21046;&#65292;&#23454;&#39564;&#35777;&#26126;&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#24615;&#33021;&#25490;&#21517;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#23545;&#20110;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#30340;&#24378;&#22823;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#23454;&#38469;&#24212;&#29992;&#36890;&#24120;&#20250;&#22312;&#24615;&#33021;&#35201;&#27714;&#20043;&#19978;&#26045;&#21152;&#19968;&#20123;&#38480;&#21046;&#65292;&#20363;&#22914;&#20869;&#23384;&#20351;&#29992;&#25110;&#24310;&#36831;&#31561;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32422;&#26463;TPE&#65288;c-TPE&#65289;&#65292;&#36825;&#26159;&#24191;&#27867;&#20351;&#29992;&#30340;&#22810;&#21151;&#33021;&#36125;&#21494;&#26031;&#20248;&#21270;&#26041;&#27861;&#8212;&#8212;&#26641;&#24418;Parzen&#20272;&#35745;&#22120;&#65288;TPE&#65289;&#30340;&#25193;&#23637;&#65292;&#20197;&#22788;&#29702;&#36825;&#20123;&#32422;&#26463;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#25193;&#23637;&#19981;&#20165;&#26159;&#31616;&#21333;&#22320;&#23558;&#29616;&#26377;&#25910;&#30410;&#20989;&#25968;&#21644;&#21407;&#22987;TPE&#32452;&#21512;&#36215;&#26469;&#65292;&#32780;&#26159;&#21253;&#25324;&#20462;&#25913;&#26469;&#35299;&#20915;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#20174;&#32463;&#39564;&#21644;&#29702;&#35770;&#19978;&#28145;&#20837;&#20998;&#26512;&#36825;&#20123;&#20462;&#25913;&#65292;&#25552;&#20379;&#20102;&#26377;&#20851;&#23427;&#20204;&#22914;&#20309;&#26377;&#25928;&#22320;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#30340;&#35265;&#35299;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;c-TPE&#22312;81&#20010;&#26114;&#36149;&#30340;HPO&#35774;&#32622;&#20013;&#34920;&#29616;&#20986;&#26368;&#20339;&#30340;&#24179;&#22343;&#25490;&#21517;&#24615;&#33021;&#65292;&#20855;&#26377;&#32479;&#35745;&#26174;&#30528;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hyperparameter optimization (HPO) is crucial for strong performance of deep learning algorithms and real-world applications often impose some constraints, such as memory usage, or latency on top of the performance requirement. In this work, we propose constrained TPE (c-TPE), an extension of the widely-used versatile Bayesian optimization method, tree-structured Parzen estimator (TPE), to handle these constraints. Our proposed extension goes beyond a simple combination of an existing acquisition function and the original TPE, and instead includes modifications that address issues that cause poor performance. We thoroughly analyze these modifications both empirically and theoretically, providing insights into how they effectively overcome these challenges. In the experiments, we demonstrate that c-TPE exhibits the best average rank performance among existing methods with statistical significance on 81 expensive HPO settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;SSET&#30340;&#32463;&#39564;&#22238;&#25918;&#37319;&#26679;&#26041;&#27861;&#65292;&#23558;&#32531;&#20914;&#21306;&#20998;&#20026;&#20107;&#20214;&#34920;&#65292;&#24182;&#37319;&#29992;&#29616;&#26377;&#30340;&#20248;&#20808;&#37319;&#26679;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#23398;&#20064;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2211.00576</link><description>&lt;p&gt;
&#26377;&#25928;&#32463;&#39564;&#22238;&#25918;&#30340;&#20107;&#20214;&#34920;
&lt;/p&gt;
&lt;p&gt;
Event Tables for Efficient Experience Replay. (arXiv:2211.00576v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00576
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;SSET&#30340;&#32463;&#39564;&#22238;&#25918;&#37319;&#26679;&#26041;&#27861;&#65292;&#23558;&#32531;&#20914;&#21306;&#20998;&#20026;&#20107;&#20214;&#34920;&#65292;&#24182;&#37319;&#29992;&#29616;&#26377;&#30340;&#20248;&#20808;&#37319;&#26679;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#23398;&#20064;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32463;&#39564;&#22238;&#25918;(ER)&#26159;&#35768;&#22810;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(RL)&#31995;&#32479;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20174;ER&#32531;&#20914;&#21306;&#36827;&#34892;&#32479;&#19968;&#37319;&#26679;&#21487;&#33021;&#23548;&#33268;&#32531;&#24930;&#30340;&#25910;&#25947;&#21644;&#19981;&#31283;&#23450;&#30340;&#28176;&#36817;&#34892;&#20026;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20107;&#20214;&#34920;&#30340;&#20998;&#23618;&#37319;&#26679;&#65288;SSET&#65289;&#65292;&#23558;ER&#32531;&#20914;&#21306;&#20998;&#20026;&#20107;&#20214;&#34920;&#65292;&#27599;&#20010;&#20107;&#20214;&#34920;&#25429;&#33719;&#20102;&#26368;&#20248;&#34892;&#20026;&#30340;&#37325;&#35201;&#23376;&#24207;&#21015;&#12290;&#25105;&#20204;&#35777;&#26126;&#30456;&#23545;&#20110;&#20256;&#32479;&#30340;&#32479;&#19968;&#32531;&#20914;&#21306;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#29702;&#35770;&#20248;&#21183;&#65292;&#24182;&#23558;SSET&#19982;&#29616;&#26377;&#30340;&#20248;&#20808;&#37319;&#26679;&#31574;&#30053;&#32467;&#21512;&#36215;&#26469;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#23398;&#20064;&#36895;&#24230;&#21644;&#31283;&#23450;&#24615;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;MiniGrid&#39046;&#22495;&#65292;&#22522;&#20934;RL&#29615;&#22659;&#20197;&#21450;&#39640;&#20445;&#30495;&#24230;&#30340;&#27773;&#36710;&#36187;&#36710;&#27169;&#25311;&#22120;&#20013;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;SSET&#30456;&#23545;&#20110;&#29616;&#26377;ER&#32531;&#20914;&#21306;&#37319;&#26679;&#26041;&#27861;&#30340;&#20248;&#21183;&#21644;&#22810;&#21151;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experience replay (ER) is a crucial component of many deep reinforcement learning (RL) systems. However, uniform sampling from an ER buffer can lead to slow convergence and unstable asymptotic behaviors. This paper introduces Stratified Sampling from Event Tables (SSET), which partitions an ER buffer into Event Tables, each capturing important subsequences of optimal behavior. We prove a theoretical advantage over the traditional monolithic buffer approach and combine SSET with an existing prioritized sampling strategy to further improve learning speed and stability. Empirical results in challenging MiniGrid domains, benchmark RL environments, and a high-fidelity car racing simulator demonstrate the advantages and versatility of SSET over existing ER buffer sampling approaches.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2206.06420</link><description>&lt;p&gt;
GraphMLP&#65306;&#19968;&#31181;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#30340;&#22270;&#24418;MLP&#24335;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
GraphMLP: A Graph MLP-Like Architecture for 3D Human Pose Estimation. (arXiv:2206.06420v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.06420
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GraphMLP&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#23427;&#23558;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#36824;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#25104;&#21151;&#22320;&#36827;&#34892;&#20102;&#26102;&#38388;&#21160;&#21147;&#23398;&#30340;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#27809;&#26377;&#33258;&#25105;&#27880;&#24847;&#21147;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#35270;&#35273;&#34920;&#31034;&#26041;&#38754;&#30340;&#31454;&#20105;&#24615;&#32467;&#26524;&#65292;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;MLP&#27169;&#22411;&#24182;&#19981;&#25797;&#38271;&#25429;&#25417;&#23616;&#37096;&#32454;&#33410;&#65292;&#20063;&#32570;&#20047;&#26377;&#20851;&#20154;&#20307;&#26500;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#29992;&#20110;&#39592;&#39612;&#34920;&#31034;&#23398;&#20064;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22270;&#24418;&#22686;&#24378;&#30340;MLP&#24335;&#26550;&#26500;&#65292;&#31216;&#20026;GraphMLP&#65292;&#23427;&#32467;&#21512;&#20102;MLP&#21644;&#22270;&#24418;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#22312;&#20840;&#23616;-&#23616;&#37096;-&#22270;&#24418;&#32479;&#19968;&#26550;&#26500;&#20013;&#29992;&#20110;3D&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#12290;GraphMLP&#23558;&#20154;&#20307;&#30340;&#22270;&#24418;&#32467;&#26500;&#32435;&#20837;MLP&#27169;&#22411;&#20013;&#65292;&#20197;&#28385;&#36275;3D&#20154;&#20307;&#23039;&#24577;&#30340;&#39046;&#22495;&#29305;&#23450;&#38656;&#27714;&#65292;&#21516;&#26102;&#20801;&#35768;&#23616;&#37096;&#21644;&#20840;&#23616;&#30340;&#31354;&#38388;&#20132;&#20114;&#20316;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;GraphMLP&#28789;&#27963;&#39640;&#25928;&#22320;&#25193;&#23637;&#21040;&#35270;&#39057;&#39046;&#22495;&#65292;&#24182;&#23637;&#31034;&#20102;&#21487;&#20197;&#20197;&#21487;&#24573;&#30053;&#30340;&#35745;&#31639;&#20195;&#20215;&#26469;&#26377;&#25928;&#22320;&#24314;&#27169;&#22797;&#26434;&#30340;&#26102;&#38388;&#21160;&#21147;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern multi-layer perceptron (MLP) models have shown competitive results in learning visual representations without self-attention. However, existing MLP models are not good at capturing local details and lack prior knowledge of human body configurations, which limits their modeling power for skeletal representation learning. To address these issues, we propose a simple yet effective graph-reinforced MLP-Like architecture, named GraphMLP, that combines MLPs and graph convolutional networks (GCNs) in a global-local-graphical unified architecture for 3D human pose estimation. GraphMLP incorporates the graph structure of human bodies into an MLP model to meet the domain-specific demand of the 3D human pose, while allowing for both local and global spatial interactions. Furthermore, we propose to flexibly and efficiently extend the GraphMLP to the video domain and show that complex temporal dynamics can be effectively modeled in a simple way with negligible computational cost gains in the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TraCoCo&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#25968;&#25454;&#35270;&#22270;&#30340;&#19981;&#21516;&#31354;&#38388;&#19978;&#19979;&#25991;&#26469;&#25200;&#21160;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#21487;&#35270;&#21270;&#23545;&#35937;&#20013;&#23398;&#20064;&#20998;&#21106;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#30340;&#32763;&#35793;&#19968;&#33268;&#21322;&#30417;&#30563;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2203.14523</link><description>&lt;p&gt;
&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#30340;&#32763;&#35793;&#19968;&#33268;&#21322;&#30417;&#30563;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Translation Consistent Semi-supervised Segmentation for 3D Medical Images. (arXiv:2203.14523v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.14523
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TraCoCo&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#21464;&#36755;&#20837;&#25968;&#25454;&#35270;&#22270;&#30340;&#19981;&#21516;&#31354;&#38388;&#19978;&#19979;&#25991;&#26469;&#25200;&#21160;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#21487;&#35270;&#21270;&#23545;&#35937;&#20013;&#23398;&#20064;&#20998;&#21106;&#27169;&#24335;&#65292;&#23454;&#29616;&#20102;&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#30340;&#32763;&#35793;&#19968;&#33268;&#21322;&#30417;&#30563;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#65292;&#20294;&#20854;&#20381;&#36182;&#20110;&#28085;&#30422;&#22823;&#37327;&#20307;&#32032;&#30340;&#27880;&#37322;&#25968;&#25454;&#65292;&#36825;&#26159;&#19968;&#20010;&#38656;&#35201;&#35299;&#20915;&#30340;&#21155;&#21183;&#65292;&#22240;&#20026;&#33719;&#21462;&#36825;&#31181;&#27880;&#37322;&#30340;&#25104;&#26412;&#24456;&#39640;&#12290;&#21322;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#36890;&#36807;&#20351;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#21644;&#23569;&#37327;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#26469;&#35757;&#32451;&#27169;&#22411;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26368;&#25104;&#21151;&#30340;SSL&#26041;&#27861;&#22522;&#20110;&#19968;&#33268;&#24615;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#26368;&#23567;&#21270;&#20174;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#25200;&#21160;&#35270;&#22270;&#33719;&#24471;&#30340;&#27169;&#22411;&#21709;&#24212;&#20043;&#38388;&#30340;&#36317;&#31163;&#26469;&#23454;&#29616;&#12290;&#36825;&#20123;&#25200;&#21160;&#36890;&#24120;&#20250;&#20445;&#25345;&#35270;&#22270;&#20043;&#38388;&#30340;&#31354;&#38388;&#36755;&#20837;&#19978;&#19979;&#25991;&#30456;&#24403;&#19968;&#33268;&#65292;&#36825;&#21487;&#33021;&#20250;&#20351;&#27169;&#22411;&#20174;&#31354;&#38388;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#23398;&#20064;&#20998;&#21106;&#27169;&#24335;&#65292;&#32780;&#19981;&#26159;&#20174;&#20998;&#21106;&#23545;&#35937;&#26412;&#36523;&#20013;&#23398;&#20064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#32763;&#35793;&#19968;&#33268;&#21327;&#21516;&#35757;&#32451;&#65288;TraCoCo&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#19968;&#33268;&#24615;&#23398;&#20064;SSL&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#25913;&#21464;&#19981;&#21516;&#30340;&#31354;&#38388;&#36755;&#20837;&#19978;&#19979;&#25991;&#26469;&#25200;&#21160;&#36755;&#20837;&#25968;&#25454;&#35270;&#22270;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#20174;&#21487;&#35270;&#21270;&#23545;&#35937;&#20013;&#23398;&#20064;&#20998;&#21106;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
3D medical image segmentation methods have been successful, but their dependence on large amounts of voxel-level annotated data is a disadvantage that needs to be addressed given the high cost to obtain such annotation. Semi-supervised learning (SSL) solve this issue by training models with a large unlabelled and a small labelled dataset. The most successful SSL approaches are based on consistency learning that minimises the distance between model responses obtained from perturbed views of the unlabelled data. These perturbations usually keep the spatial input context between views fairly consistent, which may cause the model to learn segmentation patterns from the spatial input contexts instead of the segmented objects. In this paper, we introduce the Translation Consistent Co-training (TraCoCo) which is a consistency learning SSL method that perturbs the input data views by varying their spatial input context, allowing the model to learn segmentation patterns from visual objects. Fur
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#39046;&#22495;&#22270;&#23545;&#39046;&#22495;&#30456;&#37051;&#24615;&#36827;&#34892;&#32534;&#30721;&#65292;&#25918;&#23485;&#20102;&#39046;&#22495;&#36866;&#24212;&#30340;&#32479;&#19968;&#23545;&#40784;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38750;&#24179;&#20961;&#30340;&#23545;&#40784;&#65292;&#24182;&#25104;&#21151;&#22320;&#34701;&#21512;&#20102;&#39046;&#22495;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2202.03628</link><description>&lt;p&gt;
&#22270;&#20851;&#31995;&#39046;&#22495;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Graph-Relational Domain Adaptation. (arXiv:2202.03628v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.03628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#39046;&#22495;&#22270;&#23545;&#39046;&#22495;&#30456;&#37051;&#24615;&#36827;&#34892;&#32534;&#30721;&#65292;&#25918;&#23485;&#20102;&#39046;&#22495;&#36866;&#24212;&#30340;&#32479;&#19968;&#23545;&#40784;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#38750;&#24179;&#20961;&#30340;&#23545;&#40784;&#65292;&#24182;&#25104;&#21151;&#22320;&#34701;&#21512;&#20102;&#39046;&#22495;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#24448;&#24448;&#23558;&#27599;&#20010;&#39046;&#22495;&#31561;&#21516;&#23545;&#24453;&#24182;&#23436;&#32654;&#23545;&#40784;&#65292;&#24573;&#30053;&#20102;&#19981;&#21516;&#39046;&#22495;&#20043;&#38388;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#22240;&#27492;&#23545;&#20110;&#30456;&#37051;&#39046;&#22495;&#21487;&#33021;&#26377;&#21033;&#65292;&#20294;&#23545;&#20110;&#36828;&#31163;&#39046;&#22495;&#21017;&#21487;&#33021;&#26080;&#30410;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#39046;&#22495;&#22270;&#23545;&#39046;&#22495;&#30456;&#37051;&#24615;&#36827;&#34892;&#32534;&#30721;&#65292;&#20363;&#22914;&#20197;&#32654;&#22269;&#19981;&#21516;&#24030;&#20026;&#39046;&#22495;&#21019;&#24314;&#30340;&#29366;&#24577;&#22270;&#65292;&#20351;&#24471;&#39046;&#22495;&#21487;&#20197;&#26681;&#25454;&#22270;&#32467;&#26500;&#28789;&#27963;&#23545;&#40784;&#65292;&#20174;&#32780;&#25918;&#23485;&#20102;&#36825;&#31181;&#32479;&#19968;&#30340;&#23545;&#40784;&#26041;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#22270;&#21028;&#21035;&#22120;&#23558;&#29616;&#26377;&#30340;&#23545;&#25239;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#20102;&#25512;&#24191;&#65292;&#24182;&#20351;&#29992;&#32534;&#30721;&#26465;&#20214;&#22270;&#23884;&#20837;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;&#22343;&#34913;&#29366;&#24577;&#19979;&#65292;&#24403;&#22270;&#26159;&#19968;&#20010;&#22242;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20250;&#24674;&#22797;&#32463;&#20856;&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#24182;&#20026;&#20854;&#20182;&#31867;&#22411;&#30340;&#22270;&#23454;&#29616;&#20102;&#38750;&#24179;&#20961;&#30340;&#23545;&#40784;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25104;&#21151;&#22320;&#25512;&#24191;&#32479;&#19968;&#30340;&#23545;&#40784;&#26041;&#27861;&#65292;&#24182;&#33258;&#28982;&#22320;&#34701;&#21512;&#20102;&#39046;&#22495;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing domain adaptation methods tend to treat every domain equally and align them all perfectly. Such uniform alignment ignores topological structures among different domains; therefore it may be beneficial for nearby domains, but not necessarily for distant domains. In this work, we relax such uniform alignment by using a domain graph to encode domain adjacency, e.g., a graph of states in the US with each state as a domain and each edge indicating adjacency, thereby allowing domains to align flexibly based on the graph structure. We generalize the existing adversarial learning framework with a novel graph discriminator using encoding-conditioned graph embeddings. Theoretical analysis shows that at equilibrium, our method recovers classic domain adaptation when the graph is a clique, and achieves non-trivial alignment for other types of graphs. Empirical results show that our approach successfully generalizes uniform alignment, naturally incorporates domain information represented b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#22810;&#31181;&#26469;&#28304;&#30340;&#33258;&#21160;SATD&#35782;&#21035;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25214;&#21040;&#20195;&#30721;/&#35774;&#35745;&#20538;&#21153;&#12289;&#38656;&#27714;&#20538;&#21153;&#12289;&#25991;&#26723;&#20538;&#21153;&#21644;&#27979;&#35797;&#20538;&#21153;&#12290;</title><link>http://arxiv.org/abs/2202.02387</link><description>&lt;p&gt;
&#26469;&#28304;&#22810;&#20803;&#21270;&#30340;&#33258;&#25105;&#25215;&#35748;&#25216;&#26415;&#20538;&#21153;&#30340;&#33258;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Automatic Identification of Self-Admitted Technical Debt from Four Different Sources. (arXiv:2202.02387v5 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.02387
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#22810;&#31181;&#26469;&#28304;&#30340;&#33258;&#21160;SATD&#35782;&#21035;&#26041;&#27861;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#25214;&#21040;&#20195;&#30721;/&#35774;&#35745;&#20538;&#21153;&#12289;&#38656;&#27714;&#20538;&#21153;&#12289;&#25991;&#26723;&#20538;&#21153;&#21644;&#27979;&#35797;&#20538;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25216;&#26415;&#20538;&#21153;&#26159;&#25351;&#20026;&#36798;&#21040;&#30701;&#26399;&#30446;&#26631;&#32780;&#37319;&#21462;&#25463;&#24452;&#65292;&#32780;&#29306;&#29298;&#36719;&#20214;&#31995;&#32479;&#30340;&#38271;&#26399;&#21487;&#32500;&#25252;&#24615;&#21644;&#21487;&#21457;&#23637;&#24615;&#12290;&#22823;&#37096;&#20998;&#25216;&#26415;&#20538;&#21153;&#26159;&#30001;&#24320;&#21457;&#20154;&#21592;&#33258;&#24049;&#26126;&#30830;&#25253;&#21578;&#30340;&#65307;&#36825;&#36890;&#24120;&#34987;&#31216;&#20026;&#33258;&#25105;&#25215;&#35748;&#30340;&#25216;&#26415;&#20538;&#21153;&#65288;SATD&#65289;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#37325;&#28857;&#26159;&#20174;&#28304;&#20195;&#30721;&#27880;&#37322;&#21644;&#38382;&#39064;&#36319;&#36394;&#22120;&#20013;&#35782;&#21035;SATD&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27809;&#26377;&#21487;&#33258;&#21160;&#20174;&#20854;&#20182;&#26469;&#28304;&#65288;&#22914;&#25552;&#20132;&#28040;&#24687;&#21644;&#25289;&#35831;&#27714;&#65289;&#25110;&#36890;&#36807;&#22810;&#31181;&#26469;&#28304;&#32452;&#21512;&#33258;&#21160;&#35782;&#21035;SATD&#30340;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#24182;&#35780;&#20272;&#20102;&#19968;&#31181;&#33258;&#21160;SATD&#35782;&#21035;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25972;&#21512;&#20102;&#22235;&#20010;&#26469;&#28304;&#65306;&#28304;&#20195;&#30721;&#27880;&#37322;&#12289;&#25552;&#20132;&#28040;&#24687;&#12289;&#25289;&#35831;&#27714;&#21644;&#38382;&#39064;&#36319;&#36394;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#20934;&#26041;&#27861;&#65292;&#22312;&#35782;&#21035;&#22235;&#31181;&#31867;&#22411;&#30340;SATD&#65288;&#21363;&#20195;&#30721;/&#35774;&#35745;&#20538;&#21153;&#12289;&#38656;&#27714;&#20538;&#21153;&#12289;&#25991;&#26723;&#20538;&#21153;&#21644;&#27979;&#35797;&#20538;&#21153;&#65289;&#26102;&#36798;&#21040;&#20102;&#24179;&#22343;F1&#24471;&#20998;0.611&#12290;
&lt;/p&gt;
&lt;p&gt;
Technical debt refers to taking shortcuts to achieve short-term goals while sacrificing the long-term maintainability and evolvability of software systems. A large part of technical debt is explicitly reported by the developers themselves; this is commonly referred to as Self-Admitted Technical Debt or SATD. Previous work has focused on identifying SATD from source code comments and issue trackers. However, there are no approaches available for automatically identifying SATD from other sources such as commit messages and pull requests, or by combining multiple sources. Therefore, we propose and evaluate an approach for automated SATD identification that integrates four sources: source code comments, commit messages, pull requests, and issue tracking systems. Our findings show that our approach outperforms baseline approaches and achieves an average F1-score of 0.611 when detecting four types of SATD (i.e., code/design debt, requirement debt, documentation debt, and test debt) from the 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31995;&#32479;&#23398;&#20064;&#26426;&#21046;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#36830;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#23398;&#21040;&#30340;&#30693;&#35782;&#25299;&#23637;&#21040;&#26032;&#39046;&#22495;&#12290;&#27492;&#27169;&#22411;&#32467;&#21512;&#22810;&#27169;&#24577;&#20998;&#24067;&#31354;&#38388;&#21644;&#20266;&#25490;&#32451;&#35760;&#24518;&#26426;&#21046;&#65292;&#21487;&#29992;&#20110;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;</title><link>http://arxiv.org/abs/2110.04662</link><description>&lt;p&gt;
&#22522;&#20110;&#35748;&#30693;&#30340;&#22686;&#37327;&#28418;&#31227;&#27010;&#24565;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cognitively Inspired Learning of Incremental Drifting Concepts. (arXiv:2110.04662v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.04662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31995;&#32479;&#23398;&#20064;&#26426;&#21046;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#36830;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#23398;&#21040;&#30340;&#30693;&#35782;&#25299;&#23637;&#21040;&#26032;&#39046;&#22495;&#12290;&#27492;&#27169;&#22411;&#32467;&#21512;&#22810;&#27169;&#24577;&#20998;&#24067;&#31354;&#38388;&#21644;&#20266;&#25490;&#32451;&#35760;&#24518;&#26426;&#21046;&#65292;&#21487;&#29992;&#20110;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#19981;&#26029;&#23558;&#33258;&#24049;&#23398;&#21040;&#30340;&#30693;&#35782;&#25299;&#23637;&#21040;&#26032;&#30340;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#23398;&#20064;&#26032;&#30340;&#27010;&#24565;&#26102;&#19981;&#20250;&#23545;&#20197;&#21069;&#23398;&#20064;&#30340;&#32463;&#39564;&#26377;&#20219;&#20309;&#24178;&#25200;&#12290;&#30456;&#21453;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#36830;&#32493;&#30340;&#23398;&#20064;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#22240;&#20026;&#36755;&#20837;&#25968;&#25454;&#30340;&#20998;&#24067;&#20250;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#21464;&#21270;&#12290;&#21463;&#31070;&#32463;&#31995;&#32479;&#23398;&#20064;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#35745;&#31639;&#27169;&#22411;&#65292;&#20351;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#36830;&#32493;&#23398;&#20064;&#29615;&#22659;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#65292;&#24182;&#23558;&#20854;&#23398;&#21040;&#30340;&#30693;&#35782;&#25299;&#23637;&#21040;&#26032;&#39046;&#22495;&#12290;&#25105;&#20204;&#20381;&#38752;&#24182;&#34892;&#20998;&#24067;&#22788;&#29702;&#29702;&#35770;&#65292;&#22312;&#19968;&#20010;&#22810;&#27169;&#24577;&#20998;&#24067;&#31354;&#38388;&#20013;&#65292;&#29992;&#20869;&#37096;&#25968;&#25454;&#34920;&#31034;&#22312;&#38544;&#34255;&#32593;&#32476;&#23618;&#20013;&#24314;&#27169;&#25277;&#35937;&#27010;&#24565;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#21033;&#29992;&#34917;&#20805;&#23398;&#20064;&#31995;&#32479;&#29702;&#35770;&#65292;&#36890;&#36807;&#23454;&#29616;&#20266;&#25490;&#32451;&#26469;&#20026;&#27169;&#22411;&#37197;&#22791;&#35760;&#24518;&#26426;&#21046;&#65292;&#20197;&#20811;&#26381;&#28798;&#38590;&#24615;&#36951;&#24536;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#20266;&#25968;&#25454;&#28857;&#36827;&#34892;&#32463;&#39564;&#22238;&#25918;&#21644;&#30693;&#35782;&#31215;&#32047;&#65292;&#36825;&#20123;&#28857;&#23558;&#34987;&#29992;&#20316;&#26032;&#25968;&#25454;&#30340;&#35757;&#32451;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans continually expand their learned knowledge to new domains and learn new concepts without any interference with past learned experiences. In contrast, machine learning models perform poorly in a continual learning setting, where input data distribution changes over time. Inspired by the nervous system learning mechanisms, we develop a computational model that enables a deep neural network to learn new concepts and expand its learned knowledge to new domains incrementally in a continual learning setting. We rely on the Parallel Distributed Processing theory to encode abstract concepts in an embedding space in terms of a multimodal distribution. This embedding space is modeled by internal data representations in a hidden network layer. We also leverage the Complementary Learning Systems theory to equip the model with a memory mechanism to overcome catastrophic forgetting through implementing pseudo-rehearsal. Our model can generate pseudo-data points for experience replay and accum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#65292;&#22312;MeToo&#36816;&#21160;&#30456;&#20851;&#30340;&#25512;&#25991;&#20013;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#65292;&#24182;&#22312;IEEEMBigMM 2020&#22823;&#36187;&#20013;&#33719;&#24471;&#20102;&#31532;5&#21517;&#30340;&#22909;&#25104;&#32489;&#12290;</title><link>http://arxiv.org/abs/2104.05331</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#27169;&#24577;&#26694;&#26550;&#30340;MeToo&#25512;&#25991;&#24773;&#24863;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
MeToo Tweets Sentiment Analysis Using Multi Modal frameworks. (arXiv:2104.05331v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.05331
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#22810;&#20010;&#27169;&#22411;&#65292;&#22312;MeToo&#36816;&#21160;&#30456;&#20851;&#30340;&#25512;&#25991;&#20013;&#36827;&#34892;&#24773;&#24863;&#20998;&#31867;&#65292;&#24182;&#22312;IEEEMBigMM 2020&#22823;&#36187;&#20013;&#33719;&#24471;&#20102;&#31532;5&#21517;&#30340;&#22909;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25105;&#20204;&#38024;&#23545;IEEEMBigMM 2020&#22823;&#25361;&#25112;&#36187;&#65288;BMGC&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#19982;MeToo&#36816;&#21160;&#30456;&#20851;&#30340;&#25512;&#25991;&#30340;&#24773;&#24863;&#12290;&#35813;&#27169;&#22411;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12289;&#21452;&#21521;LSTM&#21644;DNN&#30340;&#38598;&#25104;&#65292;&#20197;&#36827;&#34892;&#26368;&#32456;&#20998;&#31867;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#27169;&#22411;&#21644;&#32467;&#26524;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#25104;&#32489;&#20026;0.51491&#65292;&#22312;10&#20010;&#22242;&#38431;&#20013;&#25490;&#21517;&#31532;5&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, We present our approach for IEEEBigMM 2020, Grand Challenge (BMGC), Identifying senti-ments from tweets related to the MeToo movement. The modelis based on an ensemble of Convolutional Neural Network,Bidirectional LSTM and a DNN for final classification. Thispaper is aimed at providing a detailed analysis of the modeland the results obtained. We have ranked 5th out of 10 teamswith a score of 0.51491
&lt;/p&gt;</description></item></channel></rss>