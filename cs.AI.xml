<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#39033;&#30740;&#31350;&#23558;&#22270;&#20449;&#21495;&#22788;&#29702;&#24341;&#20837;&#21040;&#23545;&#24212;&#22270;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#35889;&#37319;&#26679;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.08770</link><description>&lt;p&gt;
FastMAC: &#23545;&#24212;&#22270;&#30340;&#38543;&#26426;&#35889;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
FastMAC: Stochastic Spectral Sampling of Correspondence Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08770
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#23558;&#22270;&#20449;&#21495;&#22788;&#29702;&#24341;&#20837;&#21040;&#23545;&#24212;&#22270;&#39046;&#22495;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#35889;&#37319;&#26679;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#23545;&#24212;&#65292;&#21363;&#19968;&#23545;3D&#28857;&#65292;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#26159;&#19968;&#20010;&#22522;&#26412;&#27010;&#24565;&#12290;&#19968;&#32452;3D&#23545;&#24212;&#65292;&#24403;&#20855;&#26377;&#20860;&#23481;&#24615;&#36793;&#32536;&#26102;&#65292;&#24418;&#25104;&#19968;&#20010;&#23545;&#24212;&#22270;&#12290;&#22312;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;3D&#28857;&#20113;&#37197;&#20934;&#26041;&#27861;&#20013;&#65292;&#35813;&#22270;&#26159;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#12290;&#25105;&#20204;&#23558;&#22270;&#20449;&#21495;&#22788;&#29702;&#24341;&#20837;&#21040;&#23545;&#24212;&#22270;&#39046;&#22495;&#65292;&#21033;&#29992;&#23545;&#24212;&#22270;&#19978;&#30340;&#24191;&#20041;&#24230;&#20449;&#21495;&#65292;&#24182;&#36861;&#27714;&#20445;&#30041;&#27492;&#20449;&#21495;&#39640;&#39057;&#32452;&#20214;&#30340;&#37319;&#26679;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#30830;&#23450;&#24615;&#37319;&#26679;&#20013;&#32791;&#26102;&#30340;&#22855;&#24322;&#20540;&#20998;&#35299;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#31181;&#38543;&#26426;&#36817;&#20284;&#37319;&#26679;&#31574;&#30053;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#26041;&#27861;&#30340;&#26680;&#24515;&#26159;&#23545;&#24212;&#22270;&#30340;&#38543;&#26426;&#35889;&#37319;&#26679;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08770v1 Announce Type: cross  Abstract: 3D correspondence, i.e., a pair of 3D points, is a fundamental concept in computer vision. A set of 3D correspondences, when equipped with compatibility edges, forms a correspondence graph. This graph is a critical component in several state-of-the-art 3D point cloud registration approaches, e.g., the one based on maximal cliques (MAC). However, its properties have not been well understood. So we present the first study that introduces graph signal processing into the domain of correspondence graph. We exploit the generalized degree signal on correspondence graph and pursue sampling strategies that preserve high-frequency components of this signal. To address time-consuming singular value decomposition in deterministic sampling, we resort to a stochastic approximate sampling strategy. As such, the core of our method is the stochastic spectral sampling of correspondence graph. As an application, we build a complete 3D registration algor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08763</link><description>&lt;p&gt;
&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#21487;&#25193;&#23637;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Simple and Scalable Strategies to Continually Pre-train Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08763
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#31616;&#21333;&#21644;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#35843;&#25972;&#12289;&#37325;&#25918;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#37325;&#26032;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#65292;&#25345;&#32493;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#21305;&#37197;&#23436;&#20840;&#37325;&#26032;&#35757;&#32451;&#26102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#22312;&#25968;&#21313;&#20159;&#30340;&#26631;&#35760;&#19978;&#36827;&#34892;&#24120;&#35268;&#39044;&#35757;&#32451;&#65292;&#19968;&#26086;&#26377;&#26032;&#25968;&#25454;&#21487;&#29992;&#23601;&#37325;&#26032;&#24320;&#22987;&#35813;&#36807;&#31243;&#12290;&#19968;&#20010;&#26356;&#26377;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#25345;&#32493;&#39044;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#19982;&#37325;&#26032;&#35757;&#32451;&#30456;&#27604;&#33021;&#33410;&#30465;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#28982;&#32780;&#65292;&#26032;&#25968;&#25454;&#24341;&#36215;&#30340;&#20998;&#24067;&#36716;&#31227;&#36890;&#24120;&#20250;&#23548;&#33268;&#22312;&#20197;&#21069;&#25968;&#25454;&#19978;&#38477;&#20302;&#24615;&#33021;&#25110;&#26080;&#27861;&#36866;&#24212;&#26032;&#25968;&#25454;&#12290;&#22312;&#26412;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#19968;&#31181;&#31616;&#21333;&#19988;&#21487;&#25193;&#23637;&#30340;&#23398;&#20064;&#29575;&#65288;LR&#65289;&#37325;&#26032;&#21319;&#28201;&#12289;LR&#37325;&#26032;&#34928;&#20943;&#21644;&#37325;&#25918;&#19978;&#19968;&#25968;&#25454;&#30340;&#32452;&#21512;&#36275;&#20197;&#19982;&#23436;&#20840;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#22312;&#25152;&#26377;&#21487;&#29992;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#65292;&#20174;&#26368;&#32456;&#25439;&#22833;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#35780;&#20272;&#22522;&#20934;&#30340;&#35282;&#24230;&#34913;&#37327;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#20004;&#20010;&#24120;&#29992;&#30340;LLM&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#65288;&#33521;&#35821;&#8594;&#33521;&#35821;&#65289;&#20043;&#38388;&#30340;&#24369;&#20294;&#29616;&#23454;&#30340;&#20998;&#24067;&#36716;&#31227;&#20197;&#21450;&#26356;&#24378;&#28872;&#30340;&#20998;&#24067;&#36716;&#31227;&#65288;&#33521;&#35821;&#8594;&#24503;&#35821;&#65289;&#19979;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08763v1 Announce Type: cross  Abstract: Large language models (LLMs) are routinely pre-trained on billions of tokens, only to start the process over again once new data becomes available. A much more efficient solution is to continually pre-train these models, saving significant compute compared to re-training. However, the distribution shift induced by new data typically results in degraded performance on previous data or poor adaptation to the new data. In this work, we show that a simple and scalable combination of learning rate (LR) re-warming, LR re-decaying, and replay of previous data is sufficient to match the performance of fully re-training from scratch on all available data, as measured by final loss and language model (LM) evaluation benchmarks. Specifically, we show this for a weak but realistic distribution shift between two commonly used LLM pre-training datasets (English$\rightarrow$English) and a stronger distribution shift (English$\rightarrow$German) at th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#27861;DAM&#65292;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#26377;&#25928;&#36866;&#24212;&#19981;&#26029;&#21040;&#26469;&#30340;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#38598;&#36755;&#20837;&#65292;&#24182;&#20801;&#35768;&#22312;&#31867;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.08755</link><description>&lt;p&gt;
DAM:&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;
&lt;/p&gt;
&lt;p&gt;
DAM: Dynamic Adapter Merging for Continual Video QA Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08755
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#23398;&#20064;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#27861;DAM&#65292;&#33021;&#22815;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#12289;&#26377;&#25928;&#36866;&#24212;&#19981;&#26029;&#21040;&#26469;&#30340;&#25968;&#25454;&#38598;&#12289;&#22788;&#29702;&#26410;&#30693;&#25968;&#25454;&#38598;&#36755;&#20837;&#65292;&#24182;&#20801;&#35768;&#22312;&#31867;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#20043;&#38388;&#20849;&#20139;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21442;&#25968;&#39640;&#25928;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#25345;&#32493;&#35270;&#39057;&#38382;&#31572;&#65288;VidQA&#65289;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;DAM&#65292;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26469;&#65288;i&#65289;&#20943;&#36731;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#65288;ii&#65289;&#23454;&#29616;&#23545;&#25345;&#32493;&#21040;&#36798;&#30340;&#25968;&#25454;&#38598;&#30340;&#39640;&#25928;&#36866;&#24212;&#65292;&#65288;iii&#65289;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#22788;&#29702;&#26469;&#33258;&#26410;&#30693;&#25968;&#25454;&#38598;&#30340;&#36755;&#20837;&#65292;&#65288;iv&#65289;&#23454;&#29616;&#36328;&#30456;&#20284;&#25968;&#25454;&#38598;&#39046;&#22495;&#30340;&#30693;&#35782;&#20849;&#20139;&#12290;&#22312;&#32473;&#23450;&#19968;&#32452;&#25345;&#32493;&#27969;&#24335;&#20256;&#36755;&#30340;VidQA&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20026;&#27599;&#20010;&#25968;&#25454;&#38598;&#39034;&#24207;&#35757;&#32451;&#29305;&#23450;&#20110;&#25968;&#25454;&#38598;&#30340;&#36866;&#37197;&#22120;&#65292;&#21516;&#26102;&#20923;&#32467;&#22823;&#22411;&#39044;&#35757;&#32451;&#35270;&#39057;&#35821;&#35328;&#39592;&#24178;&#30340;&#21442;&#25968;&#12290;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#65292;&#32473;&#23450;&#26469;&#33258;&#26410;&#30693;&#39046;&#22495;&#30340;&#35270;&#39057;&#38382;&#39064;&#31034;&#20363;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#39318;&#20808;&#20351;&#29992;&#25152;&#25552;&#20986;&#30340;&#38750;&#21442;&#25968;&#36335;&#30001;&#22120;&#20989;&#25968;&#35745;&#31639;&#27599;&#20010;&#36866;&#37197;&#22120;&#30340;&#27010;&#29575;&#65292;&#21453;&#26144;&#20986;&#35813;&#36866;&#37197;&#22120;&#19982;&#24403;&#21069;&#35270;&#39057;&#38382;&#39064;&#36755;&#20837;&#23454;&#20363;&#30340;&#30456;&#20851;&#24615;&#12290;&#38543;&#21518;&#65292;&#25152;&#25552;&#20986;&#30340;&#21160;&#24577;&#36866;&#37197;&#22120;&#21512;&#24182;&#26041;&#26696;&#32858;&#21512;&#25152;&#26377;&#36866;&#37197;&#22120;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08755v1 Announce Type: cross  Abstract: We present a parameter-efficient method for continual video question-answering (VidQA) learning. Our method, named DAM, uses the proposed Dynamic Adapter Merging to (i) mitigate catastrophic forgetting, (ii) enable efficient adaptation to continually arriving datasets, (iii) handle inputs from unknown datasets during inference, and (iv) enable knowledge sharing across similar dataset domains. Given a set of continually streaming VidQA datasets, we sequentially train dataset-specific adapters for each dataset while freezing the parameters of a large pretrained video-language backbone. During inference, given a video-question sample from an unknown domain, our method first uses the proposed non-parametric router function to compute a probability for each adapter, reflecting how relevant that adapter is to the current video-question input instance. Subsequently, the proposed dynamic adapter merging scheme aggregates all the adapter weight
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#25351;&#23548;&#35774;&#35745;&#25552;&#31034;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20135;&#29983;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.08743</link><description>&lt;p&gt;
&#23558;LLMs&#24341;&#23548;&#21040;&#26080;&#20559;&#21709;&#24212;&#65306;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Steering LLMs Towards Unbiased Responses: A Causality-Guided Debiasing Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#25351;&#23548;&#35774;&#35745;&#25552;&#31034;&#26469;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#20135;&#29983;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24456;&#23481;&#26131;&#20135;&#29983;&#20559;&#35265;&#21644;&#27495;&#35270;&#24615;&#30340;&#21709;&#24212;&#12290;&#30001;&#20110;LLMs&#28041;&#21450;&#21040;&#37325;&#35201;&#30340;&#20915;&#31574;&#21046;&#23450;&#65288;&#20363;&#22914;&#25307;&#32856;&#21644;&#21307;&#30103;&#20445;&#20581;&#65289;&#65292;&#24320;&#21457;&#20943;&#36731;&#36825;&#20123;&#20559;&#35265;&#30340;&#31574;&#30053;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#31038;&#20250;&#20559;&#35265;&#65292;&#35299;&#20915;&#20102;&#20154;&#21475;&#32479;&#35745;&#20449;&#24687;&#19982;LLM&#36755;&#20986;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#30340;&#21435;&#20559;&#20542;&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;LLMs&#36755;&#20837;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20197;&#21450;LLM&#25512;&#29702;&#30340;&#20869;&#37096;&#25512;&#29702;&#36807;&#31243;&#30340;&#22240;&#26524;&#29702;&#35299;&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#25351;&#23548;&#21435;&#20559;&#20542;LLM&#36755;&#20986;&#30340;&#25552;&#31034;&#35774;&#35745;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;&#21435;&#20559;&#25351;&#31034;&#26041;&#27861;&#65292;&#22914;&#25233;&#21046;&#25351;&#20196;&#21644;&#19978;&#19979;&#25991;&#23545;&#27604;&#20363;&#23376;&#65292;&#24182;&#36890;&#36807;&#40723;&#21169;&#26080;&#20559;&#25512;&#29702;&#30340;&#26041;&#27861;&#65292;&#21551;&#31034;&#20102;&#26032;&#30340;&#21435;&#20559;&#20542;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#24378;&#22823;&#23454;&#35777;&#34920;&#29616;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08743v1 Announce Type: cross  Abstract: Large language models (LLMs) can easily generate biased and discriminative responses. As LLMs tap into consequential decision-making (e.g., hiring and healthcare), it is of crucial importance to develop strategies to mitigate these biases. This paper focuses on social bias, tackling the association between demographic information and LLM outputs. We propose a causality-guided debiasing framework that utilizes causal understandings of (1) the data-generating process of the training corpus fed to LLMs, and (2) the internal reasoning process of LLM inference, to guide the design of prompts for debiasing LLM outputs through selection mechanisms. Our framework unifies existing de-biasing prompting approaches such as inhibitive instructions and in-context contrastive examples, and sheds light on new ways of debiasing by encouraging bias-free reasoning. Our strong empirical performance on real-world datasets demonstrates that our framework pr
&lt;/p&gt;</description></item><item><title>&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21160;&#24577;&#21442;&#25968;&#20998;&#24067;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#29305;&#21035;&#26159;&#21449;&#20998;&#25928;&#24212;&#65292;&#33021;&#24110;&#21161;&#29702;&#35299;&#27169;&#22411;&#36136;&#37327;&#65292;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#21644;&#35780;&#20272;&#24037;&#20316;&#65292;&#21516;&#26102;&#23454;&#35777;&#26174;&#31034;&#31232;&#30095;&#26435;&#37325;&#26377;&#25928;&#24615;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;</title><link>https://arxiv.org/abs/2403.08739</link><description>&lt;p&gt;
&#20998;&#21449;&#36335;&#24452;&#30340;&#33457;&#22253;&#65306;&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21160;&#24577;&#21442;&#25968;&#20998;&#24067;
&lt;/p&gt;
&lt;p&gt;
The Garden of Forking Paths: Observing Dynamic Parameters Distribution in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08739
&lt;/p&gt;
&lt;p&gt;
&#35266;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21160;&#24577;&#21442;&#25968;&#20998;&#24067;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#29305;&#21035;&#26159;&#21449;&#20998;&#25928;&#24212;&#65292;&#33021;&#24110;&#21161;&#29702;&#35299;&#27169;&#22411;&#36136;&#37327;&#65292;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#21644;&#35780;&#20272;&#24037;&#20316;&#65292;&#21516;&#26102;&#23454;&#35777;&#26174;&#31034;&#31232;&#30095;&#26435;&#37325;&#26377;&#25928;&#24615;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29702;&#35299;Transformer&#26550;&#26500;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#21331;&#36234;&#24615;&#33021;&#32972;&#21518;&#21407;&#22240;&#26041;&#38754;&#20173;&#23384;&#22312;&#24040;&#22823;&#24046;&#36317;&#12290;&#23588;&#20854;&#26159;&#19968;&#20010;&#23578;&#26410;&#34987;&#25506;&#32034;&#30340;&#39046;&#22495;&#28041;&#21450;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21442;&#25968;&#20998;&#24067;&#22914;&#20309;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#26426;&#26800;&#25551;&#36848;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24314;&#35758;&#35266;&#23519;&#27169;&#22411;&#21442;&#25968;&#30340;&#32479;&#35745;&#20998;&#24067;&#38543;&#26102;&#38388;&#28436;&#21464;&#30340;&#26041;&#24335;&#65292;&#23588;&#20854;&#26159;&#23545;&#21449;&#20998;&#24433;&#21709;&#65292;&#21487;&#20197;&#24110;&#21161;&#29702;&#35299;&#27169;&#22411;&#36136;&#37327;&#65292;&#28508;&#22312;&#22320;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#21644;&#35780;&#20272;&#24037;&#20316;&#65292;&#24182;&#20174;&#23454;&#35777;&#19978;&#23637;&#31034;&#20102;&#31232;&#30095;&#26435;&#37325;&#26377;&#25928;&#24615;&#32972;&#21518;&#30340;&#21407;&#22240;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08739v1 Announce Type: cross  Abstract: A substantial gap persists in understanding the reasons behind the exceptional performance of the Transformer architecture in NLP. A particularly unexplored area involves the mechanistic description of how the distribution of parameters evolves over time during training. In this work we suggest that looking at the time evolution of the statistic distribution of model parameters, and specifically at bifurcation effects, can help understanding the model quality, potentially reducing training costs and evaluation efforts and empirically showing the reasons behind the effectiveness of weights sparsification.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29615;&#22659;&#25193;&#25955;&#21518;&#39564;&#37319;&#26679;&#35299;&#20915;&#36870;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22312;&#21463;&#25439;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22270;&#20687;&#24674;&#22797;&#21644;MRI&#27169;&#22411;&#35757;&#32451;&#20013;&#21462;&#24471;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08728</link><description>&lt;p&gt;
&#20351;&#29992;&#29615;&#22659;&#25193;&#25955;&#21518;&#39564;&#37319;&#26679;&#65306;&#22312;&#21463;&#25439;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#35299;&#20915;&#36870;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Ambient Diffusion Posterior Sampling: Solving Inverse Problems with Diffusion Models trained on Corrupted Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08728
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#29615;&#22659;&#25193;&#25955;&#21518;&#39564;&#37319;&#26679;&#35299;&#20915;&#36870;&#38382;&#39064;&#30340;&#26694;&#26550;&#65292;&#33021;&#22312;&#21463;&#25439;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#22312;&#22270;&#20687;&#24674;&#22797;&#21644;MRI&#27169;&#22411;&#35757;&#32451;&#20013;&#21462;&#24471;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#20351;&#29992;&#20174;&#32447;&#24615;&#21463;&#25439;&#25968;&#25454;&#20013;&#23398;&#20064;&#30340;&#25193;&#25955;&#27169;&#22411;&#35299;&#20915;&#36870;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;Ambient Diffusion Posterior Sampling (A-DPS)&#65292;&#21033;&#29992;&#19968;&#20010;&#39044;&#20808;&#22312;&#19968;&#31181;&#31867;&#22411;&#30340;&#25439;&#22351;&#25968;&#25454;&#19978;&#36827;&#34892;&#36807;&#35757;&#32451;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#22312;&#21487;&#33021;&#26469;&#33258;&#19981;&#21516;&#21069;&#21521;&#36807;&#31243;&#65288;&#20363;&#22914;&#22270;&#20687;&#27169;&#31946;&#65289;&#30340;&#27979;&#37327;&#26465;&#20214;&#19979;&#25191;&#34892;&#21518;&#39564;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#33258;&#28982;&#22270;&#20687;&#25968;&#25454;&#38598;&#65288;CelebA&#12289;FFHQ &#21644; AFHQ&#65289;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102; A-DPS &#26377;&#26102;&#22312;&#36895;&#24230;&#21644;&#24615;&#33021;&#19978;&#37117;&#33021;&#32988;&#36807;&#22312;&#28165;&#27905;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20960;&#20010;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25193;&#23637;&#20102;&#29615;&#22659;&#25193;&#25955;&#26694;&#26550;&#65292;&#20197;&#20165;&#35775;&#38382;&#20613;&#37324;&#21494;&#23376;&#37319;&#26679;&#30340;&#22810;&#32447;&#22280; MRI &#27979;&#37327;&#25968;&#25454;&#26469;&#35757;&#32451; MRI &#27169;&#22411;&#65292;&#20854;&#21152;&#36895;&#22240;&#23376;&#20026;&#19981;&#21516;&#30340;&#21152;&#36895;&#22240;&#23376;&#65288;R=2&#12289;4&#12289;6&#12289;8&#65289;&#12290;&#25105;&#20204;&#20877;&#27425;&#35266;&#23519;&#21040;&#65292;&#22312;&#39640;&#24230;&#23376;&#37319;&#26679;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#26356;&#36866;&#29992;&#20110;&#35299;&#20915;&#39640;&#21152;&#36895; MRI &#36870;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08728v1 Announce Type: cross  Abstract: We provide a framework for solving inverse problems with diffusion models learned from linearly corrupted data. Our method, Ambient Diffusion Posterior Sampling (A-DPS), leverages a generative model pre-trained on one type of corruption (e.g. image inpainting) to perform posterior sampling conditioned on measurements from a potentially different forward process (e.g. image blurring). We test the efficacy of our approach on standard natural image datasets (CelebA, FFHQ, and AFHQ) and we show that A-DPS can sometimes outperform models trained on clean data for several image restoration tasks in both speed and performance. We further extend the Ambient Diffusion framework to train MRI models with access only to Fourier subsampled multi-coil MRI measurements at various acceleration factors (R=2, 4, 6, 8). We again observe that models trained on highly subsampled data are better priors for solving inverse problems in the high acceleration r
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#22312;&#19968;&#23618;Softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#19978;&#25351;&#25968;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#27969;&#65292;&#21457;&#29616;&#22312;&#28176;&#36827;&#26368;&#23567;&#21270;&#25439;&#22833;&#20540;&#26102;&#38544;&#24335;&#26368;&#23567;&#21270;&#20102;&#20851;&#38190;&#21644;&#26597;&#35810;&#26435;&#37325;&#30697;&#38453;&#20056;&#31215;&#30340;&#26680;&#33539;&#25968;&#65292;&#36825;&#31181;&#38544;&#24335;&#27491;&#21017;&#21270;&#21487;&#36890;&#36807;&#19982;&#27880;&#24847;&#21147;&#26435;&#37325;&#30456;&#20851;&#30340;SVM&#38382;&#39064;&#25551;&#36848;&#12290;</title><link>https://arxiv.org/abs/2403.08699</link><description>&lt;p&gt;
&#19968;&#23618;Softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#19978;&#26799;&#24230;&#27969;&#30340;&#38544;&#24335;&#27491;&#21017;&#21270;
&lt;/p&gt;
&lt;p&gt;
Implicit Regularization of Gradient Flow on One-Layer Softmax Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08699
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#22312;&#19968;&#23618;Softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#19978;&#25351;&#25968;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#27969;&#65292;&#21457;&#29616;&#22312;&#28176;&#36827;&#26368;&#23567;&#21270;&#25439;&#22833;&#20540;&#26102;&#38544;&#24335;&#26368;&#23567;&#21270;&#20102;&#20851;&#38190;&#21644;&#26597;&#35810;&#26435;&#37325;&#30697;&#38453;&#20056;&#31215;&#30340;&#26680;&#33539;&#25968;&#65292;&#36825;&#31181;&#38544;&#24335;&#27491;&#21017;&#21270;&#21487;&#36890;&#36807;&#19982;&#27880;&#24847;&#21147;&#26435;&#37325;&#30456;&#20851;&#30340;SVM&#38382;&#39064;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#19968;&#23618;Softmax&#27880;&#24847;&#21147;&#27169;&#22411;&#19978;&#25351;&#25968;&#25439;&#22833;&#20989;&#25968;&#30340;&#26799;&#24230;&#27969;&#65292;&#20854;&#20013;&#20851;&#38190;&#21644;&#26597;&#35810;&#26435;&#37325;&#30697;&#38453;&#26159;&#20998;&#21035;&#35757;&#32451;&#30340;&#12290;&#22312;&#25968;&#25454;&#21487;&#20998;&#24615;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#26799;&#24230;&#27969;&#36798;&#21040;&#26368;&#23567;&#25439;&#22833;&#20540;&#26102;&#65292;&#23427;&#36827;&#19968;&#27493;&#38544;&#24335;&#22320;&#26368;&#23567;&#21270;&#20102;&#20851;&#38190;&#21644;&#26597;&#35810;&#26435;&#37325;&#30697;&#38453;&#20056;&#31215;&#30340;&#26680;&#33539;&#25968;&#12290;&#36825;&#31181;&#38544;&#24335;&#27491;&#21017;&#21270;&#21487;&#20197;&#36890;&#36807;&#19982;&#27880;&#24847;&#21147;&#26435;&#37325;&#30456;&#20851;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#38382;&#39064;&#26469;&#25551;&#36848;&#12290;&#36825;&#19968;&#21457;&#29616;&#19982;&#20808;&#21069;&#30340;&#32467;&#26524;&#24418;&#25104;&#23545;&#27604;&#65292;&#20808;&#21069;&#30340;&#32467;&#26524;&#26174;&#31034;&#24403;&#23558;&#20851;&#38190;&#21644;&#26597;&#35810;&#30697;&#38453;&#21512;&#24182;&#20026;&#21333;&#20010;&#26435;&#37325;&#30697;&#38453;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#26799;&#24230;&#19979;&#38477;&#20250;&#22312;&#20056;&#31215;&#26435;&#37325;&#30697;&#38453;&#19978;&#23454;&#26045;&#38544;&#24335;&#27491;&#21017;&#21270;&#65292;&#26368;&#23567;&#21270;&#24343;&#32599;&#36125;&#23612;&#20044;&#26031;&#33539;&#25968;&#12290;&#23545;&#20110;&#23545;&#35282;&#20851;&#38190;&#21644;&#26597;&#35810;&#30697;&#38453;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#24314;&#31435;&#22312;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#26415;&#21644;&#21033;&#29992;&#19982;&#20998;&#31867;&#20219;&#21153;&#30456;&#20851;&#30340;SVM&#30340;&#36817;&#20284;KKT&#26465;&#20214;&#30340;&#22522;&#30784;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08699v1 Announce Type: cross  Abstract: We study gradient flow on the exponential loss for a classification problem with a one-layer softmax attention model, where the key and query weight matrices are trained separately. Under a separability assumption on the data, we show that when gradient flow achieves the minimal loss value, it further implicitly minimizes the nuclear norm of the product of the key and query weight matrices. Such implicit regularization can be described by a Support Vector Machine (SVM) problem with respect to the attention weights. This finding contrasts with prior results showing that the gradient descent induces an implicit regularization on the Frobenius norm on the product weight matrix when the key and query matrices are combined into a single weight matrix for training. For diagonal key and query matrices, our analysis builds upon the reparameterization technique and exploits approximate KKT conditions of the SVM associated with the classificatio
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23383;&#31526;&#21305;&#37197;&#23454;&#29616;&#26631;&#35760;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#23545;&#40784;&#30340;&#24773;&#26223;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#31354;&#26684;&#21069;&#32512;&#21644;&#37096;&#20998;&#32553;&#36827;&#31561;&#24494;&#22937;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.08688</link><description>&lt;p&gt;
&#36890;&#36807;&#23383;&#31526;&#21305;&#37197;&#23454;&#29616;&#26631;&#35760;&#23545;&#40784;&#29992;&#20110;&#23376;&#35789;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Token Alignment via Character Matching for Subword Completion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08688
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23383;&#31526;&#21305;&#37197;&#23454;&#29616;&#26631;&#35760;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#26174;&#33879;&#25913;&#36827;&#20102;&#29983;&#25104;&#27169;&#22411;&#22312;&#22788;&#29702;&#37096;&#20998;&#26631;&#35760;&#23545;&#40784;&#30340;&#24773;&#26223;&#20013;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#31354;&#26684;&#21069;&#32512;&#21644;&#37096;&#20998;&#32553;&#36827;&#31561;&#24494;&#22937;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#65292;&#20294;&#36890;&#24120;&#38590;&#20197;&#22788;&#29702;&#19982;&#37096;&#20998;&#26631;&#35760;&#23545;&#40784;&#30340;&#25552;&#31034;&#12290;&#36825;&#31181;&#22256;&#38590;&#28304;&#33258;&#26631;&#35760;&#21270;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#37096;&#20998;&#26631;&#35760;&#20250;&#33073;&#31163;&#20998;&#24067;&#65292;&#23548;&#33268;&#19981;&#27491;&#30830;&#25110;&#33618;&#35884;&#30340;&#36755;&#20986;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#25216;&#26415;&#65292;&#29992;&#20110;&#20943;&#36731;&#29983;&#25104;&#27169;&#22411;&#20013;&#25991;&#26412;&#34917;&#20840;&#26102;&#30340;&#26631;&#35760;&#21270;&#38382;&#39064;&#65292;&#21363;&#20351;&#22312;&#24120;&#35268;&#38750;&#23376;&#35789;&#24773;&#20917;&#19979;&#20063;&#33021;&#20445;&#25345;&#24615;&#33021;&#12290;&#35813;&#26041;&#27861;&#31216;&#20026;&#26631;&#35760;&#23545;&#40784;&#65292;&#28041;&#21450;&#22238;&#28335;&#21040;&#26368;&#21518;&#23436;&#25972;&#26631;&#35760;&#65292;&#24182;&#30830;&#20445;&#27169;&#22411;&#29983;&#25104;&#19982;&#25552;&#31034;&#23545;&#40784;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#37096;&#20998;&#26631;&#35760;&#22330;&#26223;&#20013;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#31354;&#26684;&#21069;&#32512;&#21644;&#37096;&#20998;&#32553;&#36827;&#31561;&#24494;&#22937;&#24773;&#20917;&#65292;&#20165;&#22686;&#21152;&#20102;&#23569;&#37327;&#26102;&#38388;&#12290;&#26412;&#25991;&#35814;&#32454;&#20171;&#32461;&#30340;&#25216;&#26415;&#21644;&#20998;&#26512;&#26377;&#21161;&#20110;&#22312;&#22788;&#29702;&#37096;&#20998;&#36755;&#20837;&#26041;&#38754;&#19981;&#26029;&#25512;&#36827;&#29983;&#25104;&#27169;&#22411;&#65292;&#23545;&#35832;&#22914;&#30340;&#24212;&#29992;&#20855;&#26377;&#30456;&#20851;&#24847;&#20041;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08688v1 Announce Type: cross  Abstract: Generative models, widely utilized in various applications, can often struggle with prompts corresponding to partial tokens. This struggle stems from tokenization, where partial tokens fall out of distribution during inference, leading to incorrect or nonsensical outputs. This paper examines a technique to alleviate the tokenization artifact on text completion in generative models, maintaining performance even in regular non-subword cases. The method, termed token alignment, involves backtracking to the last complete tokens and ensuring the model's generation aligns with the prompt. This approach showcases marked improvement across many partial token scenarios, including nuanced cases like space-prefix and partial indentation, with only a minor time increase. The technique and analysis detailed in this paper contribute to the continuous advancement of generative models in handling partial inputs, bearing relevance for applications like
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#20004;&#31181;&#26368;&#36817;&#23545;&#40784;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#27867;&#21270;&#29256;&#26412;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2403.08635</link><description>&lt;p&gt;
&#36890;&#36807;&#22312;&#32447;&#20559;&#22909;&#20248;&#21270;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Human Alignment of Large Language Models through Online Preference Optimisation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08635
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#20004;&#31181;&#26368;&#36817;&#23545;&#40784;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#27867;&#21270;&#29256;&#26412;&#65292;&#26377;&#21161;&#20110;&#23454;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#30340;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20986;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#23545;&#20110;&#30830;&#20445;&#29992;&#25143;&#20307;&#39564;&#30340;&#26377;&#29992;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#24841;&#24742;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#20154;&#31867;&#23545;&#40784;&#24050;&#32463;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20986;&#29616;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20363;&#22914;&#24378;&#21270;&#23398;&#20064;&#26469;&#33258;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#12289;&#30452;&#25509;&#31574;&#30053;&#20248;&#21270;&#65288;DPO&#65289;&#21644;&#24207;&#21015;&#20284;&#28982;&#26657;&#20934;&#65288;SLiC&#65289;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20004;&#31181;&#26368;&#36817;&#23545;&#40784;&#26041;&#27861;&#20043;&#38388;&#30340;&#31561;&#20215;&#24615;&#65292;&#21363;&#36523;&#20221;&#31574;&#30053;&#20248;&#21270;&#65288;IPO&#65289;&#21644;&#32435;&#20160;&#38236;&#20687;&#19979;&#38477;&#65288;Nash-MD&#65289;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;IPO&#30340;&#19968;&#31181;&#27867;&#21270;&#29256;&#26412;&#65292;&#21517;&#20026;IPO-MD&#65292;&#23427;&#21033;&#29992;&#20102;Nash-MD&#25552;&#20986;&#30340;&#27491;&#21017;&#21270;&#25277;&#26679;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08635v1 Announce Type: cross  Abstract: Ensuring alignment of language models' outputs with human preferences is critical to guarantee a useful, safe, and pleasant user experience. Thus, human alignment has been extensively studied recently and several methods such as Reinforcement Learning from Human Feedback (RLHF), Direct Policy Optimisation (DPO) and Sequence Likelihood Calibration (SLiC) have emerged. In this paper, our contribution is two-fold. First, we show the equivalence between two recent alignment methods, namely Identity Policy Optimisation (IPO) and Nash Mirror Descent (Nash-MD). Second, we introduce a generalisation of IPO, named IPO-MD, that leverages the regularised sampling approach proposed by Nash-MD.   This equivalence may seem surprising at first sight, since IPO is an offline method whereas Nash-MD is an online method using a preference model. However, this equivalence can be proven when we consider the online version of IPO, that is when both generati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;Verifix&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#27714;</title><link>https://arxiv.org/abs/2403.08618</link><description>&lt;p&gt;
Verifix: &#21518;&#35757;&#32451;&#26657;&#27491;&#20197;&#25913;&#21892;&#20855;&#26377;&#32463;&#36807;&#39564;&#35777;&#26679;&#26412;&#30340;&#26631;&#31614;&#22122;&#22768;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Verifix: Post-Training Correction to Improve Label Noise Robustness with Verified Samples
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08618
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#30340;&#26032;&#33539;&#24335;&#65292;&#36890;&#36807;&#22855;&#24322;&#20540;&#20998;&#35299;&#31639;&#27861;Verifix&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#31614;&#38169;&#35823;&#65292;&#21363;&#35757;&#32451;&#26679;&#26412;&#20855;&#26377;&#19981;&#27491;&#30830;&#30340;&#26631;&#31614;&#65292;&#21487;&#33021;&#20005;&#37325;&#25439;&#23475;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;&#38169;&#35823;&#24448;&#24448;&#26469;&#33258;&#38750;&#19987;&#23478;&#26631;&#27880;&#25110;&#25932;&#23545;&#25915;&#20987;&#12290;&#33719;&#21462;&#22823;&#22411;&#12289;&#23436;&#20840;&#26631;&#35760;&#30340;&#25968;&#25454;&#38598;&#25104;&#26412;&#39640;&#65292;&#24403;&#26377;&#24178;&#20928;&#30340;&#25968;&#25454;&#38598;&#21487;&#29992;&#26102;&#65292;&#37325;&#26032;&#35757;&#32451;&#22823;&#22411;&#27169;&#22411;&#23601;&#21464;&#24471;&#35745;&#31639;&#26114;&#36149;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21518;&#35757;&#32451;&#26657;&#27491;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#21021;&#22987;&#35757;&#32451;&#21518;&#35843;&#25972;&#27169;&#22411;&#21442;&#25968;&#20197;&#20943;&#36731;&#26631;&#31614;&#22122;&#22768;&#30340;&#26032;&#33539;&#24335;&#65292;&#28040;&#38500;&#20102;&#37325;&#26032;&#35757;&#32451;&#30340;&#38656;&#35201;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Verifix&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;SVD&#65289;&#30340;&#26032;&#31639;&#27861;&#65292;&#21033;&#29992;&#19968;&#20010;&#23567;&#30340;&#12289;&#32463;&#36807;&#39564;&#35777;&#30340;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21333;&#20010;&#26356;&#26032;&#26657;&#27491;&#27169;&#22411;&#26435;&#37325;&#12290;Verifix&#20351;&#29992;SVD&#20272;&#35745;&#24178;&#20928;&#28608;&#27963;&#31354;&#38388;&#65292;&#28982;&#21518;&#23558;&#27169;&#22411;&#30340;&#26435;&#37325;&#25237;&#24433;&#21040;&#36825;&#20010;&#31354;&#38388;&#19978;&#65292;&#20197;&#25233;&#21046;&#23545;&#24212;&#20110;&#25439;&#22351;&#25968;&#25454;&#30340;&#28608;&#27963;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Verifix&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08618v1 Announce Type: cross  Abstract: Label corruption, where training samples have incorrect labels, can significantly degrade the performance of machine learning models. This corruption often arises from non-expert labeling or adversarial attacks. Acquiring large, perfectly labeled datasets is costly, and retraining large models from scratch when a clean dataset becomes available is computationally expensive. To address this challenge, we propose Post-Training Correction, a new paradigm that adjusts model parameters after initial training to mitigate label noise, eliminating the need for retraining. We introduce Verifix, a novel Singular Value Decomposition (SVD) based algorithm that leverages a small, verified dataset to correct the model weights using a single update. Verifix uses SVD to estimate a Clean Activation Space and then projects the model's weights onto this space to suppress activations corresponding to corrupted data. We demonstrate Verifix's effectiveness 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21551;&#21457;&#24335;&#29305;&#24449;&#21644;&#23398;&#20064;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31038;&#20132;&#32593;&#32476;&#20013;&#32570;&#22833;&#38142;&#25509;&#30340;&#39044;&#27979;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.08613</link><description>&lt;p&gt;
&#21033;&#29992;&#34920;&#31034;&#23398;&#20064;&#21644;&#22522;&#20110;&#21551;&#21457;&#24335;&#29305;&#24449;&#30340;&#26041;&#27861;&#36827;&#34892;&#31038;&#20132;&#32593;&#32476;&#30340;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Link Prediction for Social Networks using Representation Learning and Heuristic-based Features
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08613
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#21551;&#21457;&#24335;&#29305;&#24449;&#21644;&#23398;&#20064;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#31038;&#20132;&#32593;&#32476;&#20013;&#32570;&#22833;&#38142;&#25509;&#30340;&#39044;&#27979;&#20219;&#21153;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#24615;&#33021;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31038;&#20132;&#32593;&#32476;&#30340;&#35268;&#27169;&#21644;&#30456;&#20851;&#24615;&#21576;&#25351;&#25968;&#22686;&#38271;&#30340;&#24773;&#20917;&#19979;&#65292;&#39044;&#27979;&#31038;&#20132;&#32593;&#32476;&#20013;&#32570;&#22833;&#30340;&#38142;&#25509;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#21508;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#65292;&#20197;&#29983;&#25104;&#31038;&#20132;&#32593;&#32476;&#20013;&#33410;&#28857;&#21644;&#36793;&#30340;&#34920;&#31034;&#65292;&#20174;&#32780;&#24110;&#21161;&#39044;&#27979;&#32570;&#22833;&#30340;&#38142;&#25509;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21313;&#31181;&#29305;&#24449;&#25552;&#21462;&#25216;&#26415;&#30340;&#32467;&#26524;&#65292;&#36825;&#20123;&#25216;&#26415;&#20998;&#20026;&#32467;&#26500;&#23884;&#20837;&#12289;&#22522;&#20110;&#37051;&#23621;&#30340;&#23884;&#20837;&#12289;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#22270;&#21551;&#21457;&#24335;&#12290;&#25509;&#30528;&#65292;&#25105;&#20204;&#20351;&#29992;&#38598;&#25104;&#20998;&#31867;&#22120;&#21644;&#23450;&#21046;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#22522;&#20110;&#21551;&#21457;&#24335;&#29305;&#24449;&#21644;&#23398;&#20064;&#34920;&#31034;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#31038;&#20132;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08613v1 Announce Type: cross  Abstract: The exponential growth in scale and relevance of social networks enable them to provide expansive insights. Predicting missing links in social networks efficiently can help in various modern-day business applications ranging from generating recommendations to influence analysis. Several categories of solutions exist for the same. Here, we explore various feature extraction techniques to generate representations of nodes and edges in a social network that allow us to predict missing links. We compare the results of using ten feature extraction techniques categorized across Structural embeddings, Neighborhood-based embeddings, Graph Neural Networks, and Graph Heuristics, followed by modeling with ensemble classifiers and custom Neural Networks. Further, we propose combining heuristic-based features and learned representations that demonstrate improved performance for the link prediction task on social network datasets. Using this method 
&lt;/p&gt;</description></item><item><title>MedInsight&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#21462;&#19982;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30456;&#20851;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#23558;&#20854;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20837;&#32467;&#21512;&#65292;&#29983;&#25104;&#20016;&#23500;&#30340;&#12289;&#38024;&#23545;&#24739;&#32773;&#30340;&#21709;&#24212;</title><link>https://arxiv.org/abs/2403.08607</link><description>&lt;p&gt;
MedInsight&#65306;&#29992;&#20110;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21307;&#30103;&#21709;&#24212;&#30340;&#22810;&#28304;&#19978;&#19979;&#25991;&#22686;&#24378;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
MedInsight: A Multi-Source Context Augmentation Framework for Generating Patient-Centric Medical Responses using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08607
&lt;/p&gt;
&lt;p&gt;
MedInsight&#26159;&#19968;&#31181;&#26816;&#32034;&#22686;&#24378;&#26694;&#26550;&#65292;&#36890;&#36807;&#20174;&#22810;&#20010;&#26469;&#28304;&#25552;&#21462;&#19982;&#24739;&#32773;&#21307;&#30103;&#35760;&#24405;&#30456;&#20851;&#30340;&#32972;&#26223;&#20449;&#24687;&#65292;&#23558;&#20854;&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36755;&#20837;&#32467;&#21512;&#65292;&#29983;&#25104;&#20016;&#23500;&#30340;&#12289;&#38024;&#23545;&#24739;&#32773;&#30340;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29983;&#25104;&#31867;&#20154;&#21709;&#24212;&#26041;&#38754;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32570;&#20047;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#32780;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#20855;&#26377;&#19978;&#19979;&#25991;&#21644;&#20840;&#38754;&#24615;&#21709;&#24212;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#30456;&#20851;&#24615;&#21644;&#20840;&#38754;&#24615;&#30340;&#20197;&#24739;&#32773;&#20026;&#20013;&#24515;&#30340;&#21709;&#24212;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MedInsight&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#22810;&#20010;&#26469;&#28304;&#30340;&#30456;&#20851;&#32972;&#26223;&#20449;&#24687;&#22686;&#24378;LLM&#36755;&#20837;&#65288;&#25552;&#31034;&#65289;&#12290;MedInsight&#20174;&#24739;&#32773;&#30340;&#30149;&#21382;&#25110;&#20250;&#35786;&#35760;&#24405;&#20013;&#25552;&#21462;&#30456;&#20851;&#35814;&#32454;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#26681;&#25454;&#24739;&#32773;&#30340;&#20581;&#24247;&#21382;&#21490;&#21644;&#29366;&#20917;&#65292;&#38598;&#25104;&#26469;&#33258;&#26435;&#23041;&#21307;&#23398;&#25945;&#31185;&#20070;&#21644;&#31574;&#21010;&#30340;&#32593;&#32476;&#36164;&#28304;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#23558;&#24739;&#32773;&#35760;&#24405;&#19982;&#30456;&#20851;&#21307;&#23398;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#22686;&#24378;&#19978;&#19979;&#25991;&#65292;MedInsight&#29983;&#25104;&#20016;&#23500;&#30340;&#12289;&#29305;&#23450;&#20110;&#24739;&#32773;&#30340;&#21709;&#24212;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08607v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown impressive capabilities in generating human-like responses. However, their lack of domain-specific knowledge limits their applicability in healthcare settings, where contextual and comprehensive responses are vital. To address this challenge and enable the generation of patient-centric responses that are contextually relevant and comprehensive, we propose MedInsight:a novel retrieval augmented framework that augments LLM inputs (prompts) with relevant background information from multiple sources. MedInsight extracts pertinent details from the patient's medical record or consultation transcript. It then integrates information from authoritative medical textbooks and curated web resources based on the patient's health history and condition. By constructing an augmented context combining the patient's record with relevant medical knowledge, MedInsight generates enriched, patient-specific responses t
&lt;/p&gt;</description></item><item><title>LLMs&#20511;&#21161;Reasoning-Path-Editing (Readi)&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#24544;&#23454;&#22320;&#25512;&#29702;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#20010;KGQA&#21644;TableQA&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.08593</link><description>&lt;p&gt;
&#24403;&#38656;&#35201;&#26102;&#32473;&#25105;&#25171;&#30005;&#35805;&#65306;LLM&#21487;&#20197;&#39640;&#25928;&#32780;&#24544;&#23454;&#22320;&#25512;&#29702;&#32467;&#26500;&#21270;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Call Me When Necessary: LLMs can Efficiently and Faithfully Reason over Structured Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08593
&lt;/p&gt;
&lt;p&gt;
LLMs&#20511;&#21161;Reasoning-Path-Editing (Readi)&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#39640;&#25928;&#19988;&#24544;&#23454;&#22320;&#25512;&#29702;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#22810;&#20010;KGQA&#21644;TableQA&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24050;&#32463;&#23637;&#31034;&#20986;&#22312;&#25512;&#29702;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#65292;&#20363;&#22914;&#30693;&#35782;&#22270;&#35889;&#21644;&#34920;&#26684;&#12290;&#36825;&#20123;&#20219;&#21153;&#36890;&#24120;&#38656;&#35201;&#22810;&#36339;&#25512;&#29702;&#65292;&#21363;&#23558;&#33258;&#28982;&#35821;&#35328;&#35805;&#35821;&#19982;&#29615;&#22659;&#20013;&#30340;&#23454;&#20363;&#21305;&#37197;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#21033;&#29992;LLMs&#36880;&#27493;&#26500;&#24314;&#25512;&#29702;&#36335;&#24452;&#65292;&#20854;&#20013;LLMs&#36890;&#36807;&#19982;&#29615;&#22659;&#36880;&#27493;&#20132;&#20114;&#26469;&#35843;&#29992;&#24037;&#20855;&#25110;&#36873;&#25321;&#27169;&#24335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;Reasoning-Path-Editing&#65288;Readi&#65289;&#65292;&#22312;&#20854;&#20013;LLMs&#21487;&#20197;&#39640;&#25928;&#19988;&#24544;&#23454;&#22320;&#22312;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#36827;&#34892;&#25512;&#29702;&#12290;&#22312;Readi&#20013;&#65292;LLMs&#22312;&#32473;&#23450;&#26597;&#35810;&#26102;&#26368;&#21021;&#29983;&#25104;&#19968;&#20010;&#25512;&#29702;&#36335;&#24452;&#65292;&#21482;&#26377;&#22312;&#24517;&#35201;&#26102;&#25165;&#32534;&#36753;&#36335;&#24452;&#12290;&#25105;&#20204;&#23558;&#36335;&#24452;&#23454;&#20363;&#21270;&#21040;&#32467;&#26500;&#21270;&#29615;&#22659;&#19978;&#65292;&#24182;&#22312;&#20986;&#29616;&#38382;&#39064;&#26102;&#25552;&#20379;&#21453;&#39304;&#20197;&#32534;&#36753;&#36335;&#24452;&#12290;&#23545;&#19977;&#20010;KGQA&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;TableQA&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;Readi&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#33879;&#36229;&#36234;&#20102;&#25152;&#26377;&#22522;&#20110;LLM&#30340;&#26041;&#27861;&#65288;&#22312;WebQ&#19978;&#25552;&#39640;&#20102;9.1&#65285;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08593v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have shown potential in reasoning over structured environments, e.g., knowledge graph and table. Such tasks typically require multi-hop reasoning, i.e., match natural language utterance with instances in the environment. Previous methods leverage LLMs to incrementally build a reasoning path, where the LLMs either invoke tools or pick up schemas by step-by-step interacting with the environment. We propose Reasoning-Path-Editing (Readi), a novel framework where LLMs can efficiently and faithfully reason over structured environments. In Readi, LLMs initially generate a reasoning path given a query, and edit the path only when necessary. We instantiate the path on structured environments and provide feedback to edit the path if anything goes wrong. Experimental results on three KGQA datasets and two TableQA datasets show the effectiveness of Readi, significantly surpassing all LLM-based methods (by 9.1% on WebQ
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#37327;&#21270;&#24615;&#21035;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#25552;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.08564</link><description>&lt;p&gt;
&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;
&lt;/p&gt;
&lt;p&gt;
Non-discrimination Criteria for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#22312;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#35782;&#21035;&#21644;&#37327;&#21270;&#24615;&#21035;&#20559;&#35265;&#65292;&#25552;&#20986;&#20102;&#19977;&#20010;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#38750;&#27495;&#35270;&#26631;&#20934;&#24182;&#35774;&#35745;&#20102;&#30456;&#24212;&#30340;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#32463;&#21382;&#20102;&#24555;&#36895;&#21457;&#23637;&#12290;&#38543;&#30528;&#36825;&#20123;&#27169;&#22411;&#36234;&#26469;&#36234;&#26222;&#36941;&#22320;&#25552;&#20379;&#32473;&#20844;&#20247;&#20351;&#29992;&#65292;&#20154;&#20204;&#24320;&#22987;&#25285;&#24515;&#22312;&#24212;&#29992;&#20013;&#24310;&#32493;&#21644;&#25918;&#22823;&#26377;&#23475;&#20559;&#35265;&#30340;&#38382;&#39064;&#12290;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#21487;&#33021;&#23545;&#20854;&#38024;&#23545;&#30340;&#20010;&#20154;&#36896;&#25104;&#20260;&#23475;&#21644;&#38480;&#21046;&#65292;&#26080;&#35770;&#26159;&#30001;&#35823;&#20256;&#36824;&#26159;&#27495;&#35270;&#25152;&#26500;&#25104;&#12290;&#35782;&#21035;&#24615;&#21035;&#20559;&#35265;&#20316;&#20026;&#19968;&#31181;&#26222;&#36941;&#30340;&#31038;&#20250;&#26500;&#36896;&#65292;&#26412;&#25991;&#30740;&#31350;&#22914;&#20309;&#21457;&#29616;&#21644;&#37327;&#21270;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#23384;&#22312;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#19977;&#20010;&#26469;&#33258;&#20998;&#31867;&#30340;&#33879;&#21517;&#38750;&#27495;&#35270;&#26631;&#20934;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#31867;&#27604;&#65292;&#21363;&#29420;&#31435;&#24615;&#12289;&#20998;&#31163;&#24615;&#21644;&#20805;&#20998;&#24615;&#12290;&#20026;&#20102;&#23637;&#31034;&#36825;&#20123;&#26631;&#20934;&#30340;&#20316;&#29992;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#38024;&#23545;&#27599;&#20010;&#26631;&#20934;&#30340;&#25552;&#31034;&#65292;&#37325;&#28857;&#20851;&#27880;&#32844;&#19994;&#24615;&#21035;&#21051;&#26495;&#21360;&#35937;&#65292;&#20855;&#20307;&#21033;&#29992;&#21307;&#23398;&#27979;&#35797;&#26469;&#22312;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#32972;&#26223;&#20013;&#24341;&#20837;&#22522;&#26412;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08564v1 Announce Type: cross  Abstract: Within recent years, generative AI, such as large language models, has undergone rapid development. As these models become increasingly available to the public, concerns arise about perpetuating and amplifying harmful biases in applications. Gender stereotypes can be harmful and limiting for the individuals they target, whether they consist of misrepresentation or discrimination. Recognizing gender bias as a pervasive societal construct, this paper studies how to uncover and quantify the presence of gender biases in generative language models. In particular, we derive generative AI analogues of three well-known non-discrimination criteria from classification, namely independence, separation and sufficiency. To demonstrate these criteria in action, we design prompts for each of the criteria with a focus on occupational gender stereotype, specifically utilizing the medical test to introduce the ground truth in the generative AI context. 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20174;&#32467;&#26500;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#32422;&#26463;&#30340;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#65292;&#21457;&#29616;&#20102;&#22270;&#30340;&#32467;&#26500;&#29305;&#24615;&#21644;&#23398;&#20064;&#25152;&#38656;&#27979;&#35797;&#25968;&#37327;&#20043;&#38388;&#30340;&#37325;&#35201;&#20851;&#31995;</title><link>https://arxiv.org/abs/2403.08562</link><description>&lt;p&gt;
&#32467;&#26500;&#35270;&#35282;&#19979;&#22522;&#20110;&#32422;&#26463;&#30340;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Structural perspective on constraint-based learning of Markov networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08562
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20174;&#32467;&#26500;&#35282;&#24230;&#30740;&#31350;&#20102;&#22522;&#20110;&#32422;&#26463;&#30340;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#65292;&#21457;&#29616;&#20102;&#22270;&#30340;&#32467;&#26500;&#29305;&#24615;&#21644;&#23398;&#20064;&#25152;&#38656;&#27979;&#35797;&#25968;&#37327;&#20043;&#38388;&#30340;&#37325;&#35201;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#26159;&#27010;&#29575;&#22270;&#27169;&#22411;&#65292;&#20351;&#29992;&#26080;&#21521;&#22270;&#26469;&#34920;&#31034;&#21464;&#37327;&#20043;&#38388;&#30340;&#26465;&#20214;&#29420;&#31435;&#20851;&#31995;&#12290;&#25105;&#20204;&#20851;&#27880;&#32422;&#26463;-based&#32467;&#26500;&#23398;&#20064;&#65292;&#36890;&#36807;&#25191;&#34892;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#26080;&#21521;&#22270;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#20851;&#20110;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#32422;&#26463;-based&#23398;&#20064;&#30340;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#30340;&#29702;&#35770;&#38480;&#21046;&#65306;&#27979;&#35797;&#25968;&#37327;&#21644;&#26465;&#20214;&#35774;&#32622;&#30340;&#22823;&#23567;&#12290;&#36825;&#20123;&#30028;&#38480;&#25581;&#31034;&#20102;&#22270;&#30340;&#32467;&#26500;&#29305;&#24615;&#19982;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#32593;&#32476;&#25152;&#38656;&#27979;&#35797;&#37327;&#20043;&#38388;&#30340;&#26377;&#36259;&#20114;&#21160;&#12290;&#25105;&#20204;&#24037;&#20316;&#30340;&#20986;&#21457;&#28857;&#26159;&#22270;&#21442;&#25968;&#26368;&#22823;&#25104;&#23545;&#36830;&#36890;&#24615; $\kappa$&#65292;&#21363;&#65292;&#22270;&#20013;&#36830;&#25509;&#19968;&#23545;&#39030;&#28857;&#30340;&#26368;&#22823;&#25968;&#37327;&#30340;&#39030;&#28857;&#19981;&#30456;&#20132;&#36335;&#24452;&#65292;&#36127;&#36131;&#29420;&#31435;&#24615;&#27979;&#35797;&#25152;&#38656;&#30340;&#22823;&#23567;&#65292;&#20197;&#23398;&#20064;&#22270;&#12290;&#19968;&#26041;&#38754;, &#25105;&#20204;&#34920;&#26126;&#33267;&#23569; s&#229;orest
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08562v1 Announce Type: cross  Abstract: Markov networks are probabilistic graphical models that employ undirected graphs to depict conditional independence relationships among variables. Our focus lies in constraint-based structure learning, which entails learning the undirected graph from data through the execution of conditional independence tests. We establish theoretical limits concerning two critical aspects of constraint-based learning of Markov networks: the number of tests and the sizes of the conditioning sets. These bounds uncover an exciting interplay between the structural properties of the graph and the amount of tests required to learn a Markov network. The starting point of our work is that the graph parameter maximum pairwise connectivity, $\kappa$, that is, the maximum number of vertex-disjoint paths connecting a pair of vertices in the graph, is responsible for the sizes of independence tests required to learn the graph. On one hand, we show that at least o
&lt;/p&gt;</description></item><item><title>SM4Depth&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#21333;&#20803;&#21644;&#28145;&#24230;&#38388;&#38548;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21333;&#30446;&#24230;&#37327;&#28145;&#24230;&#20272;&#35745;&#20013;&#30340;&#30456;&#26426;&#25935;&#24863;&#24615;&#12289;&#22330;&#26223;&#31934;&#24230;&#19981;&#19968;&#33268;&#21644;&#25968;&#25454;&#20381;&#36182;&#24615;&#31561;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08556</link><description>&lt;p&gt;
SM4Depth: &#19968;&#31181;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#23454;&#29616;&#36328;&#22810;&#25668;&#20687;&#22836;&#21644;&#22330;&#26223;&#30340;&#26080;&#32541;&#21333;&#30446;&#24230;&#37327;&#28145;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
SM4Depth: Seamless Monocular Metric Depth Estimation across Multiple Cameras and Scenes by One Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08556
&lt;/p&gt;
&lt;p&gt;
SM4Depth&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#39044;&#22788;&#29702;&#21333;&#20803;&#21644;&#28145;&#24230;&#38388;&#38548;&#31163;&#25955;&#21270;&#30340;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#21333;&#30446;&#24230;&#37327;&#28145;&#24230;&#20272;&#35745;&#20013;&#30340;&#30456;&#26426;&#25935;&#24863;&#24615;&#12289;&#22330;&#26223;&#31934;&#24230;&#19981;&#19968;&#33268;&#21644;&#25968;&#25454;&#20381;&#36182;&#24615;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#30446;&#24230;&#37327;&#28145;&#24230;&#20272;&#35745;&#65288;MMDE&#65289;&#30340;&#27867;&#21270;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#36890;&#36807;&#32467;&#21512;&#30456;&#23545;&#28145;&#24230;&#21644;&#24230;&#37327;&#28145;&#24230;&#25110;&#23545;&#40784;&#36755;&#20837;&#22270;&#20687;&#28966;&#36317;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#38754;&#20020;&#30528;&#22312;&#30456;&#26426;&#12289;&#22330;&#26223;&#21644;&#25968;&#25454;&#32423;&#21035;&#19978;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#23545;&#19981;&#21516;&#25668;&#20687;&#22836;&#30340;&#25935;&#24863;&#24615;&#65307;&#65288;2&#65289;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#31934;&#24230;&#19981;&#19968;&#33268;&#65307;&#65288;3&#65289;&#20381;&#36182;&#22823;&#35268;&#27169;&#35757;&#32451;&#25968;&#25454;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#32541;&#30340;MMDE&#26041;&#27861;SM4Depth&#65292;&#20197;&#22312;&#21333;&#20010;&#32593;&#32476;&#20869;&#35299;&#20915;&#19978;&#36848;&#25152;&#26377;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08556v1 Announce Type: cross  Abstract: The generalization of monocular metric depth estimation (MMDE) has been a longstanding challenge. Recent methods made progress by combining relative and metric depth or aligning input image focal length. However, they are still beset by challenges in camera, scene, and data levels: (1) Sensitivity to different cameras; (2) Inconsistent accuracy across scenes; (3) Reliance on massive training data. This paper proposes SM4Depth, a seamless MMDE method, to address all the issues above within a single network. First, we reveal that a consistent field of view (FOV) is the key to resolve ``metric ambiguity'' across cameras, which guides us to propose a more straightforward preprocessing unit. Second, to achieve consistently high accuracy across scenes, we explicitly model the metric scale determination as discretizing the depth interval into bins and propose variation-based unnormalized depth bins. This method bridges the depth gap of divers
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FedDM&#65292;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#37030;&#30693;&#35782;&#22270;&#20013;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.08554</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#32852;&#37030;&#30693;&#35782;&#22270;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Federated Knowledge Graph Unlearning via Diffusion Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08554
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FedDM&#65292;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#32852;&#37030;&#30693;&#35782;&#22270;&#20013;&#30340;&#26426;&#22120;&#21435;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08554v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#36890;&#36807;&#20419;&#36827;&#27169;&#22411;&#20849;&#20139;&#21644;&#21327;&#20316;&#65292;&#21516;&#26102;&#32500;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#25512;&#21160;&#20102;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#21457;&#23637;&#21644;&#24212;&#29992;&#12290;&#30693;&#35782;&#22270;&#65288;KG&#65289;&#23884;&#20837;&#34920;&#31034;&#36890;&#36807;&#23558;&#23454;&#20307;&#21644;&#20851;&#31995;&#26144;&#23556;&#21040;&#21521;&#37327;&#31354;&#38388;&#65292;&#20026;&#30693;&#35782;&#25512;&#29702;&#21644;&#24212;&#29992;&#25552;&#20379;&#22522;&#30784;&#12290;&#32852;&#37030;&#30693;&#35782;&#22270;&#23884;&#20837;&#33021;&#22815;&#21033;&#29992;&#26469;&#33258;&#19981;&#21516;&#23458;&#25143;&#31471;&#30340;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25252;&#26412;&#22320;&#25968;&#25454;&#30340;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#20445;&#25252;&#31561;&#38656;&#27714;&#20197;&#21450;&#38656;&#35201;&#36866;&#24212;&#21160;&#24577;&#25968;&#25454;&#21464;&#21270;&#65292;&#26426;&#22120;&#21435;&#23398;&#20064;&#65288;MU&#65289;&#30340;&#30740;&#31350;&#24471;&#20197;&#23637;&#24320;&#12290;&#28982;&#32780;&#65292;&#22312;&#24536;&#35760;&#29305;&#23450;&#36951;&#24536;&#25968;&#25454;&#23545;&#27169;&#22411;&#30340;&#24433;&#21709;&#30340;&#21516;&#26102;&#65292;&#20445;&#25345;KG&#23884;&#20837;&#27169;&#22411;&#30340;&#24615;&#33021;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;FedDM&#65292;&#19968;&#20010;&#38024;&#23545;&#32852;&#37030;&#30693;&#35782;&#22270;&#20013;&#26426;&#22120;&#21435;&#23398;&#20064;&#32780;&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#26694;&#26550;&#12290;&#36890;&#36807;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#65292;&#25105;&#20204;&#29983;&#25104;&#24102;&#26377;&#22122;&#22768;&#30340;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08554v1 Announce Type: cross  Abstract: Federated learning (FL) promotes the development and application of artificial intelligence technologies by enabling model sharing and collaboration while safeguarding data privacy. Knowledge graph (KG) embedding representation provides a foundation for knowledge reasoning and applications by mapping entities and relations into vector space. Federated KG embedding enables the utilization of knowledge from diverse client sources while safeguarding the privacy of local data. However, due to demands such as privacy protection and the need to adapt to dynamic data changes, investigations into machine unlearning (MU) have been sparked. However, it is challenging to maintain the performance of KG embedding models while forgetting the influence of specific forgotten data on the model. In this paper, we propose FedDM, a novel framework tailored for machine unlearning in federated knowledge graphs. Leveraging diffusion models, we generate noisy
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#23454;&#29616;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#65292;&#22312;GPU&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#30456;&#21305;&#25932;&#12290;</title><link>https://arxiv.org/abs/2403.08551</link><description>&lt;p&gt;
&#39640;&#26031;&#22270;&#20687;&#65306;&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#36827;&#34892;1000&#24103;&#27599;&#31186;&#30340;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
GaussianImage: 1000 FPS Image Representation and Compression by 2D Gaussian Splatting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08551
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#23454;&#29616;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#65292;&#22312;GPU&#20869;&#23384;&#21344;&#29992;&#38477;&#20302;&#30340;&#24773;&#20917;&#19979;&#65292;&#25552;&#20379;&#20102;&#26356;&#24555;&#30340;&#28210;&#26579;&#36895;&#24230;&#65292;&#24182;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#30456;&#21305;&#25932;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#22312;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#25552;&#20379;&#20102;&#39640;&#35270;&#35273;&#36136;&#37327;&#21644;&#24555;&#36895;&#28210;&#26579;&#36895;&#24230;&#65292;&#27599;&#31186;10-1000&#24103;&#65292;&#20551;&#35774;&#26377;&#36275;&#22815;&#30340;GPU&#36164;&#28304;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#35201;&#27714;&#24120;&#24120;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#20869;&#23384;&#26377;&#38480;&#30340;&#20302;&#31471;&#35774;&#22791;&#19978;&#30340;&#20351;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;2D&#39640;&#26031;&#21943;&#28034;&#36827;&#34892;&#22270;&#20687;&#34920;&#31034;&#21644;&#21387;&#32553;&#30340;&#24320;&#21019;&#24615;&#33539;&#24335;&#65292;&#21517;&#20026;GaussianImage&#12290;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;2D&#39640;&#26031;&#26469;&#34920;&#31034;&#22270;&#20687;&#65292;&#20854;&#20013;&#27599;&#20010;&#39640;&#26031;&#20855;&#26377;8&#20010;&#21442;&#25968;&#65292;&#21253;&#25324;&#20301;&#32622;&#12289;&#21327;&#26041;&#24046;&#21644;&#39068;&#33394;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#32047;&#31215;&#27714;&#21644;&#30340;&#26032;&#39062;&#28210;&#26579;&#31639;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;GPU&#20869;&#23384;&#33267;&#23569;&#38477;&#20302;3&#20493;&#65292;&#25311;&#21512;&#26102;&#38388;&#24555;5&#20493;&#65292;&#19981;&#20165;&#22312;&#34920;&#31034;&#24615;&#33021;&#19978;&#19982;INR&#65288;&#20363;&#22914;WIRE&#65292;I-NGP&#65289;&#19981;&#30456;&#19978;&#19979;&#65292;&#32780;&#19988;&#26080;&#35770;&#21442;&#25968;&#22823;&#23567;&#22914;&#20309;&#37117;&#33021;&#25552;&#20379;1500-2000&#24103;&#27599;&#31186;&#30340;&#26356;&#24555;&#28210;&#26579;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08551v1 Announce Type: cross  Abstract: Implicit neural representations (INRs) recently achieved great success in image representation and compression, offering high visual quality and fast rendering speeds with 10-1000 FPS, assuming sufficient GPU resources are available. However, this requirement often hinders their use on low-end devices with limited memory. In response, we propose a groundbreaking paradigm of image representation and compression by 2D Gaussian Splatting, named GaussianImage. We first introduce 2D Gaussian to represent the image, where each Gaussian has 8 parameters including position, covariance and color. Subsequently, we unveil a novel rendering algorithm based on accumulated summation. Remarkably, our method with a minimum of 3$\times$ lower GPU memory usage and 5$\times$ faster fitting time not only rivals INRs (e.g., WIRE, I-NGP) in representation performance, but also delivers a faster rendering speed of 1500-2000 FPS regardless of parameter size. 
&lt;/p&gt;</description></item><item><title>HOLMES&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#26631;&#31614;&#20998;&#35299;&#20026;&#19968;&#32452;&#30456;&#20851;&#27010;&#24565;&#24182;&#25552;&#20379;&#37096;&#20214;&#32423;&#35299;&#37322;&#65292;&#26469;&#24110;&#21161;&#29702;&#35299;&#21644;&#35299;&#37322;&#21367;&#31215;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.08536</link><description>&lt;p&gt;
HOLMES: &#22522;&#20110;HOLonym-MEronym&#30340;&#35821;&#20041;&#26816;&#26597;&#25216;&#26415;&#29992;&#20110;&#21367;&#31215;&#22270;&#20687;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
HOLMES: HOLonym-MEronym based Semantic inspection for Convolutional Image Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08536
&lt;/p&gt;
&lt;p&gt;
HOLMES&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#36890;&#36807;&#23558;&#26631;&#31614;&#20998;&#35299;&#20026;&#19968;&#32452;&#30456;&#20851;&#27010;&#24565;&#24182;&#25552;&#20379;&#37096;&#20214;&#32423;&#35299;&#37322;&#65292;&#26469;&#24110;&#21161;&#29702;&#35299;&#21644;&#35299;&#37322;&#21367;&#31215;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#22914;&#20170;&#26159;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#39318;&#36873;&#27169;&#22411;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#33258;&#21160;&#21270;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#29305;&#24449;&#25552;&#21462;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#33719;&#24471;&#30340;&#30693;&#35782;&#23436;&#20840;&#26159;&#20122;&#31526;&#21495;&#30340;&#65292;&#22240;&#27492;&#38590;&#20197;&#29702;&#35299;&#21644;&#35299;&#37322;&#32473;&#26368;&#32456;&#29992;&#25143;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HOLMES (HOLonym-MEronym based Semantic inspection)&#30340;&#26032;&#25216;&#26415;&#65292;&#23427;&#23558;&#26631;&#31614;&#20998;&#35299;&#20026;&#19968;&#32452;&#30456;&#20851;&#27010;&#24565;&#65292;&#24182;&#20026;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#25552;&#20379;&#37096;&#20214;&#32423;&#35299;&#37322;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;HOLMES&#21033;&#29992;&#26412;&#20307;&#35770;&#12289;&#32593;&#32476;&#25235;&#21462;&#21644;&#36801;&#31227;&#23398;&#20064;&#26469;&#33258;&#21160;&#26500;&#24314;&#32473;&#23450;holonym (&#31867;&#21035;)&#30340;meronym (&#37096;&#20214;)&#26816;&#27979;&#22120;&#12290;&#28982;&#21518;&#65292;&#23427;&#22312;meronym&#32423;&#21035;&#29983;&#25104;&#28909;&#22270;&#65292;&#26368;&#21518;&#36890;&#36807;&#20351;&#29992;&#36974;&#25377;&#22270;&#20687;&#23545;CNN&#30340;holonym&#36827;&#34892;&#25506;&#27979;&#65292;&#31361;&#20986;&#27599;&#20010;&#37096;&#20214;&#23545;&#20998;&#31867;&#36755;&#20986;&#30340;&#37325;&#35201;&#24615;&#12290;&#19982;&#26368;&#20808;&#36827;&#30340;&#26174;&#33879;&#24615;&#26041;&#27861;&#30456;&#27604;&#65292;HOLMES
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08536v1 Announce Type: cross  Abstract: Convolutional Neural Networks (CNNs) are nowadays the model of choice in Computer Vision, thanks to their ability to automatize the feature extraction process in visual tasks. However, the knowledge acquired during training is fully subsymbolic, and hence difficult to understand and explain to end users. In this paper, we propose a new technique called HOLMES (HOLonym-MEronym based Semantic inspection) that decomposes a label into a set of related concepts, and provides component-level explanations for an image classification model. Specifically, HOLMES leverages ontologies, web scraping and transfer learning to automatically construct meronym (parts)-based detectors for a given holonym (class). Then, it produces heatmaps at the meronym level and finally, by probing the holonym CNN with occluded images, it highlights the importance of each part on the classification output. Compared to state-of-the-art saliency methods, HOLMES takes a 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#24320;&#21457;&#20102;&#20351;&#29992;CNN&#12289;Transformer&#21644;&#24490;&#29615;&#32593;&#32476;&#23545;&#29482;&#30340;&#25915;&#20987;&#24615;&#36827;&#34892;&#20998;&#31867;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20154;&#24037;&#20998;&#26512;&#21160;&#29289;&#34892;&#20026;&#21487;&#33021;&#23384;&#22312;&#30340;&#38169;&#35823;&#21644;&#32791;&#26102;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08528</link><description>&lt;p&gt;
&#20351;&#29992;CNN&#12289;Transformer&#21644;&#24490;&#29615;&#32593;&#32476;&#23545;&#29482;&#30340;&#25915;&#20987;&#24615;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Pig aggression classification using CNN, Transformers and Recurrent Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08528
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#24320;&#21457;&#20102;&#20351;&#29992;CNN&#12289;Transformer&#21644;&#24490;&#29615;&#32593;&#32476;&#23545;&#29482;&#30340;&#25915;&#20987;&#24615;&#36827;&#34892;&#20998;&#31867;&#30340;&#25216;&#26415;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20154;&#24037;&#20998;&#26512;&#21160;&#29289;&#34892;&#20026;&#21487;&#33021;&#23384;&#22312;&#30340;&#38169;&#35823;&#21644;&#32791;&#26102;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#21457;&#33021;&#29992;&#20110;&#20998;&#26512;&#21644;&#26816;&#27979;&#21160;&#29289;&#34892;&#20026;&#30340;&#25216;&#26415;&#26159;&#30044;&#29287;&#19994;&#30340;&#20851;&#38190;&#27963;&#21160;&#65292;&#21487;&#20197;&#30417;&#27979;&#21387;&#21147;&#21644;&#21160;&#29289;&#31119;&#21033;&#65292;&#24182;&#26377;&#21161;&#20110;&#20892;&#22330;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#24320;&#21457;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#24110;&#21161;&#39282;&#20859;&#21592;&#20570;&#20986;&#25913;&#21892;&#29983;&#20135;&#34920;&#29616;&#21644;&#38477;&#20302;&#25104;&#26412;&#30340;&#20915;&#31574;&#65292;&#22240;&#20026;&#21160;&#29289;&#34892;&#20026;&#30001;&#20154;&#31867;&#20998;&#26512;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#21644;&#32791;&#26102;&#12290;&#29482;&#30340;&#25915;&#20987;&#24615;&#26159;&#19968;&#20010;&#34987;&#30740;&#31350;&#30340;&#34892;&#20026;&#26679;&#26412;&#65292;&#36890;&#36807;&#21160;&#29289;&#20998;&#31867;&#21644;&#35782;&#21035;&#26469;&#38477;&#20302;&#20854;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#36825;&#19968;&#36807;&#31243;&#32321;&#29712;&#12289;&#23481;&#26131;&#20986;&#38169;&#65292;&#21487;&#20197;&#36890;&#36807;&#33258;&#21160;&#21270;&#20943;&#23569;&#65292;&#36890;&#36807;&#22312;&#21463;&#25511;&#29615;&#22659;&#19979;&#25429;&#25417;&#35270;&#39057;&#36827;&#34892;&#35270;&#35273;&#20998;&#31867;&#20197;&#23454;&#29616;&#12290;&#25429;&#33719;&#30340;&#35270;&#39057;&#21487;&#29992;&#20110;&#35757;&#32451;&#65292;&#32467;&#26524;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#20998;&#31867;&#65292;&#37319;&#29992;&#31070;&#32463;&#32593;&#32476;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08528v1 Announce Type: cross  Abstract: The development of techniques that can be used to analyze and detect animal behavior is a crucial activity for the livestock sector, as it is possible to monitor the stress and animal welfare and contributes to decision making in the farm. Thus, the development of applications can assist breeders in making decisions to improve production performance and reduce costs, once the animal behavior is analyzed by humans and this can lead to susceptible errors and time consumption. Aggressiveness in pigs is an example of behavior that is studied to reduce its impact through animal classification and identification. However, this process is laborious and susceptible to errors, which can be reduced through automation by visually classifying videos captured in controlled environment. The captured videos can be used for training and, as a result, for classification through computer vision and artificial intelligence, employing neural network techn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiPrompT&#30340;&#35299;&#32806;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36866;&#24212;&#25552;&#31034;&#26469;&#35299;&#20915;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.08506</link><description>&lt;p&gt;
DiPrompT: &#22810;&#28508;&#22312;&#39046;&#22495;&#27867;&#21270;&#30340;&#35299;&#32806;&#25552;&#31034;&#35843;&#25972;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;
&lt;/p&gt;
&lt;p&gt;
DiPrompT: Disentangled Prompt Tuning for Multiple Latent Domain Generalization in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08506
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DiPrompT&#30340;&#35299;&#32806;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#36866;&#24212;&#25552;&#31034;&#26469;&#35299;&#20915;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#23545;&#39046;&#22495;&#27867;&#21270;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08506v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495;  &#25688;&#35201;: &#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#23398;&#20064;&#33539;&#24335;&#65292;&#21487;&#20197;&#20174;&#20998;&#25955;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#65292;&#32780;&#32852;&#37030;&#39046;&#22495;&#27867;&#21270;&#36827;&#19968;&#27493;&#32771;&#34385;&#27979;&#35797;&#25968;&#25454;&#38598;&#65288;&#30446;&#26631;&#39046;&#22495;&#65289;&#19981;&#23384;&#22312;&#20110;&#20998;&#25955;&#30340;&#35757;&#32451;&#25968;&#25454;&#65288;&#28304;&#39046;&#22495;&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;FL&#26041;&#27861;&#20551;&#35774;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#20102;&#39046;&#22495;&#26631;&#31614;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#35780;&#20272;&#23545;&#39046;&#22495;&#25968;&#37327;&#26045;&#21152;&#26126;&#30830;&#30340;&#32422;&#26463;&#65292;&#36825;&#20123;&#32422;&#26463;&#24517;&#39035;&#20005;&#26684;&#21305;&#37197;&#23458;&#25143;&#31471;&#30340;&#25968;&#37327;&#12290;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#20013;&#20247;&#22810;&#36793;&#32536;&#35774;&#22791;&#30340;&#34987;&#20302;&#25928;&#21033;&#29992;&#20197;&#21450;&#39069;&#22806;&#30340;&#36328;&#23458;&#25143;&#31471;&#39046;&#22495;&#27880;&#37322;&#65292;&#36825;&#20123;&#38480;&#21046;&#21487;&#33021;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#65292;&#24182;&#28041;&#21450;&#28508;&#22312;&#30340;&#38544;&#31169;&#27844;&#28431;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#32780;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#35299;&#32806;&#25552;&#31034;&#35843;&#25972;&#65288;DiPrompT&#65289;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20998;&#24067;&#24335;&#23398;&#20064;&#36866;&#24212;&#25552;&#31034;&#26469;&#22788;&#29702;&#19978;&#36848;&#38480;&#21046;&#65292;&#26469;&#23454;&#29616;&#39046;&#22495;&#27867;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#20004;&#31181;&#25552;&#31034;&#31867;&#22411;&#65292;&#21363;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08506v1 Announce Type: cross  Abstract: Federated learning (FL) has emerged as a powerful paradigm for learning from decentralized data, and federated domain generalization further considers the test dataset (target domain) is absent from the decentralized training data (source domains). However, most existing FL methods assume that domain labels are provided during training, and their evaluation imposes explicit constraints on the number of domains, which must strictly match the number of clients. Because of the underutilization of numerous edge devices and additional cross-client domain annotations in the real world, such restrictions may be impractical and involve potential privacy leaks. In this paper, we propose an efficient and novel approach, called Disentangled Prompt Tuning (DiPrompT), a method that tackles the above restrictions by learning adaptive prompts for domain generalization in a distributed manner. Specifically, we first design two types of prompts, i.e., 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#65292;&#20351;&#24471;&#26080;&#38656;&#39069;&#22806;Transformer&#35299;&#30721;&#22120;&#23601;&#33021;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08505</link><description>&lt;p&gt;
&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#21464;&#21387;&#22120;&#29992;&#20110;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Content-aware Masked Image Modeling Transformer for Stereo Image Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08505
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#65292;&#20351;&#24471;&#26080;&#38656;&#39069;&#22806;Transformer&#35299;&#30721;&#22120;&#23601;&#33021;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#22522;&#20110;&#23398;&#20064;&#30340;&#31435;&#20307;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#37319;&#29992;&#20102;&#22797;&#26434;&#30340;&#36716;&#25442;&#26041;&#27861;&#65292;&#20294;&#22312;&#32534;&#30721;&#28508;&#22312;&#34920;&#31034;&#26102;&#21364;&#37319;&#29992;&#20102;&#20174;&#21333;&#20010;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#23548;&#20986;&#30340;&#31616;&#21333;&#29109;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29109;&#27169;&#22411;&#38590;&#20197;&#26377;&#25928;&#25429;&#25417;&#31435;&#20307;&#22270;&#20687;&#22266;&#26377;&#30340;&#31354;&#38388;-&#35270;&#24046;&#29305;&#24449;&#65292;&#23548;&#33268;&#20122;&#26368;&#20248;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CAMSIC&#30340;&#31435;&#20307;&#22270;&#20687;&#21387;&#32553;&#26694;&#26550;&#12290; CAMSIC &#29420;&#31435;&#22320;&#23558;&#27599;&#20010;&#22270;&#20687;&#36716;&#25442;&#20026;&#28508;&#22312;&#34920;&#31034;&#65292;&#24182;&#37319;&#29992;&#24378;&#22823;&#30340;&#26080;&#35299;&#30721;&#22120;&#21464;&#21387;&#22120;&#29109;&#27169;&#22411;&#26469;&#25429;&#25417;&#31354;&#38388;&#21644;&#35270;&#24046;&#20381;&#36182;&#20851;&#31995;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;&#25513;&#30721;&#22270;&#20687;&#24314;&#27169;&#65288;MIM&#65289;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#38754;&#21521;&#20869;&#23481;&#24863;&#30693;&#30340;MIM&#20419;&#36827;&#20102;&#20808;&#39564;&#20449;&#24687;&#19982;&#20272;&#35745;&#20196;&#29260;&#20043;&#38388;&#30340;&#39640;&#25928;&#21452;&#21521;&#20132;&#20114;&#65292;&#33258;&#28982;&#22320;&#28040;&#38500;&#20102;&#39069;&#22806;&#30340;Transformer&#35299;&#30721;&#22120;&#30340;&#38656;&#27714;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#31435;&#20307;&#22270;&#20687;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#29575;&#22833;&#30495;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08505v1 Announce Type: cross  Abstract: Existing learning-based stereo image codec adopt sophisticated transformation with simple entropy models derived from single image codecs to encode latent representations. However, those entropy models struggle to effectively capture the spatial-disparity characteristics inherent in stereo images, which leads to suboptimal rate-distortion results. In this paper, we propose a stereo image compression framework, named CAMSIC. CAMSIC independently transforms each image to latent representation and employs a powerful decoder-free Transformer entropy model to capture both spatial and disparity dependencies, by introducing a novel content-aware masked image modeling (MIM) technique. Our content-aware MIM facilitates efficient bidirectional interaction between prior information and estimated tokens, which naturally obviates the need for an extra Transformer decoder. Experiments show that our stereo image codec achieves state-of-the-art rate-d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36974;&#34109;&#29983;&#25104;&#25925;&#20107;&#21464;&#25442;&#22120;&#65292;&#21033;&#29992;&#23383;&#31526;&#24341;&#23548;&#21644;&#26631;&#39064;&#22686;&#24378;&#26469;&#23454;&#29616;&#19968;&#33268;&#24615;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.08502</link><description>&lt;p&gt;
&#20855;&#26377;&#23383;&#31526;&#24341;&#23548;&#21644;&#26631;&#39064;&#22686;&#24378;&#30340;&#36974;&#34109;&#29983;&#25104;&#25925;&#20107;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Masked Generative Story Transformer with Character Guidance and Caption Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08502
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36974;&#34109;&#29983;&#25104;&#25925;&#20107;&#21464;&#25442;&#22120;&#65292;&#21033;&#29992;&#23383;&#31526;&#24341;&#23548;&#21644;&#26631;&#39064;&#22686;&#24378;&#26469;&#23454;&#29616;&#19968;&#33268;&#24615;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Story Visualization (SV)&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#29983;&#25104;&#35270;&#35273;&#20219;&#21153;&#65292;&#26082;&#35201;&#27714;&#29983;&#25104;&#22270;&#20687;&#24207;&#21015;&#30340;&#35270;&#35273;&#36136;&#37327;&#65292;&#21448;&#35201;&#27714;&#19981;&#21516;&#24103;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#24182;&#34892;&#21464;&#25442;&#22120;&#26041;&#27861;&#65292;&#36890;&#36807;&#36807;&#21435;&#21644;&#26410;&#26469;&#30340;&#26631;&#39064;&#20043;&#38388;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26469;&#23454;&#29616;&#19968;&#33268;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23383;&#31526;&#24341;&#23548;&#25216;&#26415;&#65292;&#36890;&#36807;&#22312;logit&#31354;&#38388;&#20013;&#24418;&#25104;&#25991;&#26412;&#26465;&#20214;&#21644;&#23383;&#31526;&#26465;&#20214;logits&#30340;&#32452;&#21512;&#65292;&#20197;&#38544;&#24335;&#22320;&#32858;&#28966;&#20110;&#35282;&#33394;&#30340;&#29983;&#25104;&#12290;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#19968;&#31181;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#25191;&#34892;&#30340;&#26631;&#39064;&#22686;&#24378;&#25216;&#26415;&#65292;&#20197;&#22686;&#24378;&#25105;&#20204;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08502v1 Announce Type: cross  Abstract: Story Visualization (SV) is a challenging generative vision task, that requires both visual quality and consistency between different frames in generated image sequences. Previous approaches either employ some kind of memory mechanism to maintain context throughout an auto-regressive generation of the image sequence, or model the generation of the characters and their background separately, to improve the rendering of characters. On the contrary, we embrace a completely parallel transformer-based approach, exclusively relying on Cross-Attention with past and future captions to achieve consistency. Additionally, we propose a Character Guidance technique to focus on the generation of characters in an implicit manner, by forming a combination of text-conditional and character-conditional logits in the logit space. We also employ a caption-augmentation technique, carried out by a Large Language Model (LLM), to enhance the robustness of our
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20013;&#30340;&#20877;&#29616;&#24615;&#21644;&#20960;&#20309;&#20869;&#22312;&#32500;&#24230;&#24615;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20877;&#29616;&#24615;&#26412;&#20307;&#35770;&#65292;&#20197;&#21450;&#25506;&#35752;&#20102;&#32500;&#24230;&#35781;&#21650;&#23545;&#25968;&#25454;&#25910;&#38598;&#12289;&#34920;&#31034;&#21644;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.08438</link><description>&lt;p&gt;
&#20877;&#29616;&#24615;&#21644;&#20960;&#20309;&#20869;&#22312;&#32500;&#24230;&#24615;&#65306;&#23545;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Reproducibility and Geometric Intrinsic Dimensionality: An Investigation on Graph Neural Network Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08438
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#20013;&#30340;&#20877;&#29616;&#24615;&#21644;&#20960;&#20309;&#20869;&#22312;&#32500;&#24230;&#24615;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20877;&#29616;&#24615;&#26412;&#20307;&#35770;&#65292;&#20197;&#21450;&#25506;&#35752;&#20102;&#32500;&#24230;&#35781;&#21650;&#23545;&#25968;&#25454;&#25910;&#38598;&#12289;&#34920;&#31034;&#21644;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20013;&#22797;&#21046;&#21644;&#21487;&#20877;&#29616;&#24615;&#30340;&#22256;&#38590;&#36817;&#24180;&#26469;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#30340;&#35805;&#39064;&#12290;&#30830;&#20445;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#38656;&#35201;&#21487;&#20877;&#29616;&#24615;&#65292;&#36890;&#36807;&#20351;&#29992;&#30456;&#21516;&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#39564;&#35777;&#30740;&#31350;&#32467;&#26524;&#30340;&#21487;&#38752;&#24615;&#12290;&#36825;&#20419;&#36827;&#20102;&#24320;&#25918;&#21644;&#21487;&#35775;&#38382;&#30340;&#30740;&#31350;&#12289;&#31283;&#20581;&#30340;&#23454;&#39564;&#24037;&#20316;&#27969;&#31243;&#20197;&#21450;&#26032;&#21457;&#29616;&#30340;&#24555;&#36895;&#25972;&#21512;&#12290;&#35780;&#20272;&#30740;&#31350;&#20986;&#29256;&#29289;&#25903;&#25345;&#20877;&#29616;&#24615;&#30340;&#31243;&#24230;&#26159;&#26412;&#25991;&#30340;&#19968;&#20010;&#30446;&#26631;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20877;&#29616;&#24615;&#26412;&#20307;&#35770;&#65292;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#12290;&#22312;&#36825;&#20123;&#21162;&#21147;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36716;&#21521;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#21478;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#21363;&#32500;&#24230;&#35781;&#21650;&#65292;&#23427;&#22312;&#25968;&#25454;&#25910;&#38598;&#12289;&#34920;&#31034;&#21644;&#20998;&#26512;&#26041;&#38754;&#24102;&#26469;&#25361;&#25112;&#65292;&#20351;&#24471;&#26356;&#38590;&#25214;&#21040;&#20195;&#34920;&#24615;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08438v1 Announce Type: cross  Abstract: Difficulties in replication and reproducibility of empirical evidences in machine learning research have become a prominent topic in recent years. Ensuring that machine learning research results are sound and reliable requires reproducibility, which verifies the reliability of research findings using the same code and data. This promotes open and accessible research, robust experimental workflows, and the rapid integration of new findings. Evaluating the degree to which research publications support these different aspects of reproducibility is one goal of the present work. For this we introduce an ontology of reproducibility in machine learning and apply it to methods for graph neural networks. Building on these efforts we turn towards another critical challenge in machine learning, namely the curse of dimensionality, which poses challenges in data collection, representation, and analysis, making it harder to find representative data 
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#25628;&#32034;&#26041;&#27861;&#20248;&#21270;LLM&#22312;&#25925;&#20107;&#28857;&#20272;&#35745;&#20013;&#30340;&#34920;&#29616;&#65292;&#20351;&#20854;&#24179;&#22343;&#20272;&#35745;&#24615;&#33021;&#25552;&#39640;&#20102;59.34%&#12290;</title><link>https://arxiv.org/abs/2403.08430</link><description>&lt;p&gt;
&#22522;&#20110;&#25628;&#32034;&#30340;&#23545;LLM&#23398;&#20064;&#26679;&#26412;&#36827;&#34892;&#20248;&#21270;&#20197;&#36827;&#34892;&#25925;&#20107;&#28857;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Search-based Optimisation of LLM Learning Shots for Story Point Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08430
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#25628;&#32034;&#26041;&#27861;&#20248;&#21270;LLM&#22312;&#25925;&#20107;&#28857;&#20272;&#35745;&#20013;&#30340;&#34920;&#29616;&#65292;&#20351;&#20854;&#24179;&#22343;&#20272;&#35745;&#24615;&#33021;&#25552;&#39640;&#20102;59.34%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29992;&#20110;&#25191;&#34892;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#30340;&#19968;&#31181;&#26041;&#24335;&#26159;&#22312;&#35201;&#27714;&#23427;&#20204;&#36827;&#34892;&#39044;&#27979;&#20043;&#21069;&#25552;&#20379;&#19968;&#20123;&#31034;&#20363;&#12290;&#36825;&#26159;&#19968;&#31181;&#31216;&#20026;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#20803;&#23398;&#20064;&#36807;&#31243;&#12290;&#26412;&#25991;&#20351;&#29992;&#21487;&#29992;&#30340;&#22522;&#20110;&#25628;&#32034;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#25968;&#37327;&#21644;&#32452;&#21512;&#31034;&#20363;&#65292;&#21487;&#20197;&#25552;&#39640;LLM&#22312;&#29992;&#20110;&#20272;&#35745;&#26032;&#30340;&#25935;&#25463;&#20219;&#21153;&#30340;&#25925;&#20107;&#28857;&#26102;&#30340;&#20272;&#35745;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#30340;SBSE&#25216;&#26415;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#19978;&#24179;&#22343;&#25552;&#39640;&#20102;59.34%&#30340;LLM&#20272;&#35745;&#24615;&#33021;&#65288;&#20197;&#20272;&#35745;&#30340;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#20026;&#25351;&#26631;&#65289;&#65292;&#30456;&#23545;&#20110;&#38646;&#26679;&#26412;&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08430v1 Announce Type: cross  Abstract: One of the ways Large Language Models (LLMs) are used to perform machine learning tasks is to provide them with a few examples before asking them to produce a prediction. This is a meta-learning process known as few-shot learning. In this paper, we use available Search-Based methods to optimise the number and combination of examples that can improve an LLM's estimation performance, when it is used to estimate story points for new agile tasks. Our preliminary results show that our SBSE technique improves the estimation performance of the LLM by 59.34% on average (in terms of mean absolute error of the estimation) over three datasets against a zero-shot setting.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21327;&#21161;&#36827;&#34892;&#20195;&#30721;&#23457;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;&#26631;&#35760;&#23433;&#20840;&#28431;&#27934;&#20195;&#30721;&#21644;&#25191;&#34892;&#36719;&#20214;&#21151;&#33021;&#39564;&#35777;&#20004;&#20010;&#20219;&#21153;&#65292;&#32467;&#26524;&#26174;&#31034;&#19987;&#26377;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;</title><link>https://arxiv.org/abs/2403.08429</link><description>&lt;p&gt;
&#20351;&#29992;LLMs&#36827;&#34892;&#36719;&#20214;&#28431;&#27934;&#21644;&#21151;&#33021;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Software Vulnerability and Functionality Assessment using LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08429
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#22914;&#20309;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21327;&#21161;&#36827;&#34892;&#20195;&#30721;&#23457;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;&#26631;&#35760;&#23433;&#20840;&#28431;&#27934;&#20195;&#30721;&#21644;&#25191;&#34892;&#36719;&#20214;&#21151;&#33021;&#39564;&#35777;&#20004;&#20010;&#20219;&#21153;&#65292;&#32467;&#26524;&#26174;&#31034;&#19987;&#26377;&#27169;&#22411;&#34920;&#29616;&#20248;&#20110;&#24320;&#28304;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20195;&#30721;&#23457;&#26597;&#22312;&#36719;&#20214;&#24320;&#21457;&#36807;&#31243;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36827;&#34892;&#20195;&#30721;&#23457;&#26597;&#21487;&#33021;&#20250;&#24456;&#32321;&#29712;&#19988;&#26114;&#36149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#21487;&#20197;&#24110;&#21161;&#36827;&#34892;&#20195;&#30721;&#23457;&#26597;&#65292;&#37325;&#28857;&#20851;&#27880;&#20004;&#39033;&#25105;&#20204;&#35748;&#20026;&#23545;&#33391;&#22909;&#23457;&#26597;&#33267;&#20851;&#37325;&#35201;&#30340;&#20219;&#21153;&#65306;&#65288;i&#65289;&#26631;&#35760;&#20855;&#26377;&#23433;&#20840;&#28431;&#27934;&#30340;&#20195;&#30721;&#21644;&#65288;ii&#65289;&#25191;&#34892;&#36719;&#20214;&#21151;&#33021;&#39564;&#35777;&#65292;&#21363;&#30830;&#20445;&#20195;&#30721;&#31526;&#21512;&#20854;&#39044;&#26399;&#21151;&#33021;&#12290;&#20026;&#20102;&#27979;&#35797;&#22312;&#36825;&#20004;&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#20351;&#29992;&#38646;&#38236;&#20687;&#21644;&#38142;&#24335;&#25552;&#31034;&#26469;&#33719;&#24471;&#26368;&#32456;&#30340;&#8220;&#25209;&#20934;&#25110;&#25298;&#32477;&#8221;&#24314;&#35758;&#12290;&#20316;&#20026;&#25968;&#25454;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#37324;&#31243;&#30865;&#24335;&#30340;&#20195;&#30721;&#29983;&#25104;&#25968;&#25454;&#38598;&#65288;HumanEval&#21644;MBPP&#65289;&#65292;&#20197;&#21450;&#26469;&#33258;&#36890;&#29992;&#24369;&#28857;&#26522;&#20030;&#65288;CWE&#65289;&#30340;&#24102;&#26377;&#23433;&#20840;&#28431;&#27934;&#30340;&#19987;&#23478;&#32534;&#20889;&#20195;&#30721;&#29255;&#27573;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32771;&#34385;&#20102;&#26469;&#33258;OpenAI&#30340;&#19977;&#20010;&#19987;&#26377;&#27169;&#22411;&#21644;&#36739;&#23567;&#30340;&#24320;&#28304;LLMs&#30340;&#32452;&#21512;&#12290;&#25105;&#20204;&#21457;&#29616;&#21069;&#32773;&#34920;&#29616;&#20248;&#20110;&#21518;&#32773;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08429v1 Announce Type: cross  Abstract: While code review is central to the software development process, it can be tedious and expensive to carry out. In this paper, we investigate whether and how Large Language Models (LLMs) can aid with code reviews. Our investigation focuses on two tasks that we argue are fundamental to good reviews: (i) flagging code with security vulnerabilities and (ii) performing software functionality validation, i.e., ensuring that code meets its intended functionality. To test performance on both tasks, we use zero-shot and chain-of-thought prompting to obtain final ``approve or reject'' recommendations. As data, we employ seminal code generation datasets (HumanEval and MBPP) along with expert-written code snippets with security vulnerabilities from the Common Weakness Enumeration (CWE). Our experiments consider a mixture of three proprietary models from OpenAI and smaller open-source LLMs. We find that the former outperforms the latter by a large
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340; LDVC &#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36335;&#30001;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#35821;&#20041;&#21644;&#35270;&#35273;&#20449;&#24687;&#26356;&#22909;&#30340;&#23545;&#40784;</title><link>https://arxiv.org/abs/2403.08426</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#35270;&#35273;&#19968;&#33268;&#24615;&#26041;&#27861;&#29992;&#20110;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Language-Driven Visual Consensus for Zero-Shot Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08426
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340; LDVC &#26041;&#27861;&#36890;&#36807;&#24341;&#20837;&#36335;&#30001;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#20013;&#30340;&#35821;&#20041;&#21644;&#35270;&#35273;&#20449;&#24687;&#26356;&#22909;&#30340;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#35270;&#35273;&#29305;&#24449;&#19982;&#31867;&#21035;&#23884;&#20837;&#23545;&#40784;&#65292;&#20511;&#21161;&#21464;&#25442;&#22120;&#35299;&#30721;&#22120;&#29983;&#25104;&#35821;&#20041;&#25513;&#27169;&#65292;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#25512;&#21160;&#20102;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#30340;&#21457;&#23637;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36825;&#19968;&#33539;&#24335;&#20869;&#30340;&#26041;&#27861;&#36935;&#21040;&#20102;&#19968;&#20123;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#24050;&#35265;&#31867;&#21035;&#19978;&#36807;&#24230;&#25311;&#21512;&#21644;&#25513;&#27169;&#20013;&#30340;&#23567;&#30862;&#29255;&#21270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#39537;&#21160;&#30340;&#35270;&#35273;&#19968;&#33268;&#24615;&#65288;LDVC&#65289;&#26041;&#27861;&#65292;&#20419;&#36827;&#20102;&#35821;&#20041;&#21644;&#35270;&#35273;&#20449;&#24687;&#30340;&#25913;&#36827;&#23545;&#40784;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;&#31867;&#21035;&#23884;&#20837;&#20316;&#20026;&#38170;&#28857;&#65292;&#24341;&#23548;&#35270;&#35273;&#29305;&#24449;&#26397;&#21521;&#31867;&#21035;&#23884;&#20837;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#36991;&#20813;&#30001;&#20110;&#35270;&#35273;&#37096;&#20998;&#30340;&#20887;&#20313;&#24615;&#32780;&#23548;&#33268;&#30340;&#22024;&#26434;&#23545;&#40784;&#65292;&#25105;&#20204;&#23558;&#36335;&#30001;&#27880;&#24847;&#24341;&#20837;&#21040;&#33258;&#27880;&#24847;&#21147;&#20013;&#65292;&#29992;&#20110;&#25214;&#21040;&#35270;&#35273;&#19968;&#33268;&#24615;&#65292;&#20174;&#32780;&#22686;&#24378;&#21516;&#19968;&#29289;&#20307;&#20869;&#30340;&#35821;&#20041;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08426v1 Announce Type: cross  Abstract: The pre-trained vision-language model, exemplified by CLIP, advances zero-shot semantic segmentation by aligning visual features with class embeddings through a transformer decoder to generate semantic masks. Despite its effectiveness, prevailing methods within this paradigm encounter challenges, including overfitting on seen classes and small fragmentation in masks. To mitigate these issues, we propose a Language-Driven Visual Consensus (LDVC) approach, fostering improved alignment of semantic and visual information.Specifically, we leverage class embeddings as anchors due to their discrete and abstract nature, steering vision features toward class embeddings. Moreover, to circumvent noisy alignments from the vision part due to its redundant nature, we introduce route attention into self-attention for finding visual consensus, thereby enhancing semantic consistency within the same object. Equipped with a vision-language prompting stra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23450;&#20041;&#20102;&#35268;&#26684;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#21363;&#31995;&#32479;&#36807;&#24230;&#20851;&#27880;&#25351;&#23450;&#25351;&#26631;&#32780;&#25439;&#23475;&#20102;&#39640;&#32423;&#35201;&#27714;&#21644;&#20219;&#21153;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08425</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#35268;&#26684;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Specification Overfitting in Artificial Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23450;&#20041;&#20102;&#35268;&#26684;&#36807;&#24230;&#25311;&#21512;&#38382;&#39064;&#65292;&#21363;&#31995;&#32479;&#36807;&#24230;&#20851;&#27880;&#25351;&#23450;&#25351;&#26631;&#32780;&#25439;&#23475;&#20102;&#39640;&#32423;&#35201;&#27714;&#21644;&#20219;&#21153;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#32463;&#24120;&#34987;&#25209;&#35780;&#23384;&#22312;&#22266;&#26377;&#30340;&#20559;&#35265;&#65292;&#20197;&#21450;&#32570;&#20047;&#25511;&#21046;&#12289;&#38382;&#36131;&#21644;&#36879;&#26126;&#24230;&#65292;&#30417;&#31649;&#26426;&#26500;&#22240;&#27492;&#38590;&#20197;&#25511;&#21046;&#36825;&#31181;&#25216;&#26415;&#30340;&#28508;&#22312;&#36127;&#38754;&#24433;&#21709;&#12290;&#39640;&#32423;&#35201;&#27714;&#65292;&#22914;&#20844;&#24179;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#38656;&#35201;&#34987;&#24418;&#24335;&#21270;&#20026;&#20855;&#20307;&#30340;&#35268;&#26684;&#24230;&#37327;&#65292;&#32780;&#36825;&#20123;&#24230;&#37327;&#26159;&#25429;&#25417;&#22522;&#26412;&#35201;&#27714;&#30340;&#29420;&#31435;&#26041;&#38754;&#30340;&#19981;&#23436;&#32654;&#20195;&#29702;&#12290;&#37492;&#20110;&#19981;&#21516;&#25351;&#26631;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#30340;&#26435;&#34913;&#21450;&#20854;&#23545;&#36807;&#24230;&#20248;&#21270;&#30340;&#33030;&#24369;&#24615;&#65292;&#23558;&#35268;&#26684;&#24230;&#37327;&#25972;&#21512;&#21040;&#31995;&#32479;&#24320;&#21457;&#36807;&#31243;&#20013;&#24182;&#19981;&#26159;&#19968;&#20214;&#31616;&#21333;&#30340;&#20107;&#24773;&#12290;&#26412;&#25991;&#23450;&#20041;&#20102;&#35268;&#26684;&#36807;&#24230;&#25311;&#21512;&#65292;&#21363;&#31995;&#32479;&#36807;&#24230;&#20391;&#37325;&#20110;&#25351;&#23450;&#30340;&#24230;&#37327;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#39640;&#32423;&#35201;&#27714;&#21644;&#20219;&#21153;&#24615;&#33021;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#25991;&#29486;&#35843;&#30740;&#65292;&#23545;&#30740;&#31350;&#20154;&#21592;&#22914;&#20309;&#25552;&#20986;&#12289;&#27979;&#37327;&#21644;&#20248;&#21270;&#35268;&#26684;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08425v1 Announce Type: new  Abstract: Machine learning (ML) and artificial intelligence (AI) approaches are often criticized for their inherent bias and for their lack of control, accountability, and transparency. Consequently, regulatory bodies struggle with containing this technology's potential negative side effects. High-level requirements such as fairness and robustness need to be formalized into concrete specification metrics, imperfect proxies that capture isolated aspects of the underlying requirements. Given possible trade-offs between different metrics and their vulnerability to over-optimization, integrating specification metrics in system development processes is not trivial. This paper defines specification overfitting, a scenario where systems focus excessively on specified metrics to the detriment of high-level requirements and task performance. We present an extensive literature survey to categorize how researchers propose, measure, and optimize specification
&lt;/p&gt;</description></item><item><title>Tastle&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#36234;&#29425;&#26694;&#26550;&#65292;&#37319;&#29992;&#24694;&#24847;&#20869;&#23481;&#38544;&#34255;&#21644;&#20869;&#23384;&#37325;&#26500;&#20197;&#21450;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32418;&#38431;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.08424</link><description>&lt;p&gt;
Tastle: &#20026;&#33258;&#21160;&#36234;&#29425;&#25915;&#20987;&#24178;&#25200;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Tastle: Distract Large Language Models for Automatic Jailbreak Attack
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08424
&lt;/p&gt;
&lt;p&gt;
Tastle&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#36234;&#29425;&#26694;&#26550;&#65292;&#37319;&#29992;&#24694;&#24847;&#20869;&#23481;&#38544;&#34255;&#21644;&#20869;&#23384;&#37325;&#26500;&#20197;&#21450;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32418;&#38431;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#24180;&#26469;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;&#22312;LLMs&#20844;&#24320;&#21457;&#24067;&#20043;&#21069;&#65292;&#20154;&#20204;&#24050;&#32463;&#20570;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#26469;&#23558;&#23427;&#20204;&#30340;&#34892;&#20026;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#23545;&#40784;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#30830;&#20445;&#23427;&#20204;&#30340;&#26377;&#30410;&#24615;&#12289;&#35802;&#23454;&#24615;&#21644;&#26080;&#23475;&#24615;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#32463;&#36807;&#32454;&#33268;&#23545;&#40784;&#30340;LLMs&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24694;&#24847;&#25805;&#32437;&#65292;&#22914;&#36234;&#29425;&#65292;&#23548;&#33268;&#24847;&#22806;&#30340;&#34892;&#20026;&#12290;&#36234;&#29425;&#26159;&#26377;&#24847;&#24320;&#21457;&#24694;&#24847;&#25552;&#31034;&#65292;&#20174;LLM&#23433;&#20840;&#38480;&#21046;&#20013;&#36867;&#33073;&#20197;&#29983;&#25104;&#26410;&#32463;&#23457;&#26597;&#30340;&#26377;&#23475;&#20869;&#23481;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#36234;&#29425;&#26041;&#27861;&#26469;&#23545;LLMs&#36827;&#34892;&#32418;&#38431;&#25915;&#20987;&#65292;&#20294;&#23427;&#20204;&#22312;&#25928;&#26524;&#21644;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#36935;&#21040;&#20102;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Tastle&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#40657;&#30418;&#36234;&#29425;&#26694;&#26550;&#65292;&#29992;&#20110;&#33258;&#21160;&#23545;LLMs&#36827;&#34892;&#32418;&#38431;&#25915;&#20987;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#24694;&#24847;&#20869;&#23481;&#38544;&#34255;&#21644;&#20869;&#23384;&#37325;&#26500;&#65292;&#24182;&#32467;&#21512;&#36845;&#20195;&#20248;&#21270;&#31639;&#27861;&#26469;&#36234;&#29425;LLMs&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08424v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved significant advances in recent days. Extensive efforts have been made before the public release of LLMs to align their behaviors with human values. The primary goal of alignment is to ensure their helpfulness, honesty and harmlessness. However, even meticulously aligned LLMs remain vulnerable to malicious manipulations such as jailbreaking, leading to unintended behaviors. The jailbreak is to intentionally develop a malicious prompt that escapes from the LLM security restrictions to produce uncensored detrimental contents. Previous works explore different jailbreak methods for red teaming LLMs, yet they encounter challenges regarding to effectiveness and scalability. In this work, we propose Tastle, a novel black-box jailbreak framework for automated red teaming of LLMs. We designed malicious content concealing and memory reframing with an iterative optimization algorithm to jailbreak LLMs, mo
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#22240;&#26524;&#24615;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#27169;&#22411;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#22797;&#26434;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#26426;&#29702;&#65292;&#25552;&#39640;&#20102;&#28779;&#28798;&#27169;&#24335;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08414</link><description>&lt;p&gt;
&#29992;&#20110;&#28779;&#28798;&#21361;&#38505;&#39044;&#27979;&#30340;&#22240;&#26524;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Causal Graph Neural Networks for Wildfire Danger Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08414
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#22240;&#26524;&#24615;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#27169;&#22411;&#33021;&#22815;&#26174;&#24335;&#22320;&#24314;&#27169;&#22797;&#26434;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#26426;&#29702;&#65292;&#25552;&#39640;&#20102;&#28779;&#28798;&#27169;&#24335;&#39044;&#27979;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28779;&#28798;&#39044;&#27979;&#22240;&#22825;&#27668;&#26465;&#20214;&#12289;&#26893;&#34987;&#31867;&#22411;&#21644;&#20154;&#31867;&#27963;&#21160;&#31561;&#19981;&#21516;&#22240;&#32032;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#32780;&#21464;&#24471;&#38590;&#20197;&#39044;&#27979;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23637;&#29616;&#20102;&#30452;&#25509;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#22788;&#29702;&#36825;&#31181;&#22797;&#26434;&#24615;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25903;&#25345;&#20851;&#38190;&#20915;&#31574;&#65292;&#25105;&#20204;&#35748;&#20026;&#25105;&#20204;&#38656;&#35201;&#36866;&#21512;&#27491;&#30830;&#21407;&#22240;&#30340;&#27169;&#22411;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;&#23398;&#21040;&#30340;&#38544;&#24335;&#35268;&#21017;&#24212;&#35813;&#20197;&#25512;&#21160;&#28779;&#28798;&#30340;&#22522;&#26412;&#36807;&#31243;&#20026;&#22522;&#30784;&#12290;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#22240;&#26524;&#24615;&#19982;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#22270;&#23398;&#20064;&#26174;&#24335;&#22320;&#23545;&#22797;&#26434;&#21464;&#37327;&#20043;&#38388;&#30340;&#22240;&#26524;&#26426;&#29702;&#24314;&#27169;&#12290;&#22240;&#26524;&#37051;&#25509;&#30697;&#38453;&#32771;&#34385;&#20102;&#19981;&#21516;&#21464;&#37327;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#24182;&#28040;&#38500;&#20102;&#39640;&#24230;&#30456;&#20851;&#24433;&#21709;&#20043;&#38388;&#30340;&#34394;&#20551;&#32852;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#36890;&#36807;&#22312;&#27431;&#27954;&#21271;&#37096;&#21644;&#22320;&#20013;&#28023;&#29983;&#29289;&#32676;&#31995;&#20013;&#20248;&#36234;&#24615;&#33021;&#39044;&#27979;&#28779;&#28798;&#27169;&#24335;&#32780;&#24471;&#21040;&#35777;&#26126;&#12290;&#25910;&#30410;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08414v1 Announce Type: cross  Abstract: Wildfire forecasting is notoriously hard due to the complex interplay of different factors such as weather conditions, vegetation types and human activities. Deep learning models show promise in dealing with this complexity by learning directly from data. However, to inform critical decision making, we argue that we need models that are right for the right reasons; that is, the implicit rules learned should be grounded by the underlying processes driving wildfires. In that direction, we propose integrating causality with Graph Neural Networks (GNNs) that explicitly model the causal mechanism among complex variables via graph learning. The causal adjacency matrix considers the synergistic effect among variables and removes the spurious links from highly correlated impacts. Our methodology's effectiveness is demonstrated through superior performance forecasting wildfire patterns in the European boreal and mediterranean biome. The gain is
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#29702;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#23398;&#20064;&#22914;&#20309;&#22312;&#28151;&#21512;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#20013;&#26368;&#20339;&#20998;&#37197;&#20915;&#31574;&#36131;&#20219;&#65292;&#24182;&#26368;&#23567;&#21270;&#19981;&#33391;&#22242;&#38431;&#34892;&#20026;&#23548;&#33268;&#30340;&#22996;&#27966;&#21464;&#26356;&#27425;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.08386</link><description>&lt;p&gt;
&#20248;&#21270;&#39118;&#38505;&#25935;&#24863;&#30340;&#20154;&#24037;&#26234;&#33021;&#28151;&#21512;&#22242;&#38431;
&lt;/p&gt;
&lt;p&gt;
Optimizing Risk-averse Human-AI Hybrid Teams
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08386
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#29702;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#23398;&#20064;&#22914;&#20309;&#22312;&#28151;&#21512;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#20013;&#26368;&#20339;&#20998;&#37197;&#20915;&#31574;&#36131;&#20219;&#65292;&#24182;&#26368;&#23567;&#21270;&#19981;&#33391;&#22242;&#38431;&#34892;&#20026;&#23548;&#33268;&#30340;&#22996;&#27966;&#21464;&#26356;&#27425;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#39044;&#35745;&#38543;&#30528;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#25152;&#35859;&#30340;&#28151;&#21512;&#22242;&#38431;&#20013;&#21512;&#20316;&#30340;&#22686;&#21152;&#65292;&#36825;&#31181;&#21512;&#20316;&#30340;&#39057;&#29575;&#23558;&#22686;&#21152;&#12290;&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#29087;&#32451;&#24230;&#25552;&#39640;&#21644;&#20854;&#37319;&#29992;&#21464;&#24471;&#26356;&#21152;&#24191;&#27867;&#65292;&#39044;&#35745;&#21327;&#20316;&#23558;&#22686;&#21152;&#12290;&#20294;&#26159;&#65292;&#23427;&#20204;&#30340;&#34892;&#20026;&#24182;&#38750;&#26080;&#35823;&#65292;&#20174;&#32780;&#20351;&#28151;&#21512;&#22242;&#38431;&#25104;&#20026;&#19968;&#31181;&#38750;&#24120;&#21512;&#36866;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#25913;&#36827;&#36825;&#20123;&#30001;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#32452;&#25104;&#30340;&#22242;&#38431;&#30340;&#32489;&#25928;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#28151;&#21512;&#22242;&#38431;&#65292;&#25105;&#20204;&#23558;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#37117;&#31216;&#20026;&#20195;&#29702;&#12290;&#20026;&#20102;&#25552;&#39640;&#22242;&#38431;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32463;&#29702;&#65292;&#35813;&#32463;&#29702;&#36890;&#36807;&#26631;&#20934;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#26696;&#23398;&#20064;&#22914;&#20309;&#22312;&#19968;&#27573;&#26102;&#38388;&#20869;&#26368;&#22909;&#22320;&#22996;&#27966;&#20915;&#31574;&#36131;&#20219;&#32473;&#20219;&#20309;&#19968;&#20010;&#20195;&#29702;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24341;&#23548;&#32463;&#29702;&#30340;&#23398;&#20064;&#65292;&#35753;&#20182;&#20204;&#26368;&#23567;&#21270;&#22240;&#19981;&#33391;&#22242;&#38431;&#34892;&#20026;&#32780;&#23548;&#33268;&#30340;&#22996;&#27966;&#21464;&#26356;&#27425;&#25968;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#31649;&#29702;&#32773;&#32489;&#25928;&#30340;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08386v1 Announce Type: new  Abstract: We anticipate increased instances of humans and AI systems working together in what we refer to as a hybrid team. The increase in collaboration is expected as AI systems gain proficiency and their adoption becomes more widespread. However, their behavior is not error-free, making hybrid teams a very suitable solution. As such, we consider methods for improving performance for these teams of humans and AI systems. For hybrid teams, we will refer to both the humans and AI systems as agents. To improve team performance over that seen for agents operating individually, we propose a manager which learns, through a standard Reinforcement Learning scheme, how to best delegate, over time, the responsibility of taking a decision to any of the agents. We further guide the manager's learning so they also minimize how many changes in delegation are made resulting from undesirable team behavior. We demonstrate the optimality of our manager's performa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#20113;&#36801;&#31227;&#20013;SQL&#25968;&#25454;&#24211;&#26041;&#35328;&#30340;&#36716;&#25442;&#22256;&#38590;&#65292;&#23613;&#31649;&#26377;&#19968;&#20123;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;&#36716;&#25442;&#26041;&#35328;&#65292;&#20294;&#24182;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#38382;&#39064;&#65292;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#12290;</title><link>https://arxiv.org/abs/2403.08375</link><description>&lt;p&gt;
&#20113;&#36801;&#31227;&#20013;SQL&#26041;&#35328;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Translating between SQL Dialects for Cloud Migration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08375
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#20113;&#36801;&#31227;&#20013;SQL&#25968;&#25454;&#24211;&#26041;&#35328;&#30340;&#36716;&#25442;&#22256;&#38590;&#65292;&#23613;&#31649;&#26377;&#19968;&#20123;&#24037;&#20855;&#21487;&#20197;&#24110;&#21161;&#36716;&#25442;&#26041;&#35328;&#65292;&#20294;&#24182;&#19981;&#33021;&#23436;&#20840;&#35299;&#20915;&#38382;&#39064;&#65292;&#38656;&#35201;&#20154;&#24037;&#24178;&#39044;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#29616;&#22330;&#21040;&#20113;&#30340;&#31995;&#32479;&#36801;&#31227;&#26159;&#35768;&#22810;&#24037;&#19994;&#26426;&#26500;&#30340;&#37325;&#35201;&#24037;&#20316;&#12290;&#36825;&#31181;&#20113;&#36801;&#31227;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#23558;&#25968;&#25454;&#24211;&#36716;&#31227;&#21040;&#22312;&#32447;&#20027;&#26426;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;SQL&#25968;&#25454;&#24211;&#30340;&#36801;&#31227;&#22256;&#38590;&#12290;&#23613;&#31649;SQL&#26159;&#23384;&#20648;&#25968;&#25454;&#24211;&#31243;&#24207;&#30340;&#26174;&#33879;&#26041;&#27861;&#20043;&#19968;&#65292;&#20294;&#23384;&#22312;&#30528;&#22823;&#37327;&#19981;&#21516;&#30340;SQL&#26041;&#35328;&#65288;&#20363;&#22914;MySQL&#65292;Postgres&#31561;&#65289;&#65292;&#24403;&#29616;&#22330;&#30340;SQL&#26041;&#35328;&#19982;&#20113;&#19978;&#25176;&#31649;&#30340;&#26041;&#35328;&#19981;&#21516;&#26102;&#65292;&#36825;&#21487;&#33021;&#20250;&#20351;&#36801;&#31227;&#22797;&#26434;&#21270;&#12290;&#19968;&#20123;&#24120;&#35265;&#20113;&#25552;&#20379;&#21830;&#22914;AWS&#21644;Azure&#25552;&#20379;&#20102;&#24037;&#20855;&#26469;&#24110;&#21161;&#22312;&#26041;&#35328;&#20043;&#38388;&#36827;&#34892;&#36716;&#25442;&#65292;&#20197;&#20943;&#36731;&#22823;&#37096;&#20998;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#24037;&#20855;&#24182;&#19981;&#25104;&#21151;&#22320;&#36716;&#25442; 100% &#30340;&#20195;&#30721;&#12290;&#22240;&#27492;&#65292;&#36719;&#20214;&#24037;&#31243;&#24072;&#24517;&#39035;&#25163;&#21160;&#36716;&#25442;&#26410;&#32763;&#35793;&#25968;&#25454;&#24211;&#30340;&#20854;&#20313;&#37096;&#20998;&#12290;&#23545;&#20110;&#22823;&#22411;&#32452;&#32455;&#65292;&#36825;&#39033;&#20219;&#21153;&#24456;&#24555;&#21464;&#24471;&#26840;&#25163;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08375v1 Announce Type: cross  Abstract: Migrations of systems from on-site premises to the cloud has been a fundamental endeavor by many industrial institutions. A crucial component of such cloud migrations is the transition of databases to be hosted online. In this work, we consider the difficulties of this migration for SQL databases. While SQL is one of the prominent methods for storing database procedures, there are a plethora of different SQL dialects (e.g., MySQL, Postgres, etc.) which can complicate migrations when the on-premise SQL dialect differs to the dialect hosted on the cloud. Tools exist by common cloud provides such as AWS and Azure to aid in translating between dialects in order to mitigate the majority of the difficulties. However, these tools do not successfully translate $100\%$ of the code. Consequently, software engineers must manually convert the remainder of the untranslated database. For large organizations, this task quickly becomes intractable and
&lt;/p&gt;</description></item><item><title>SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.08370</link><description>&lt;p&gt;
SMART: &#29992;&#20110;&#25351;&#20196;&#35843;&#25972;&#30340;&#23376;&#27169;&#22359;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
SMART: Submodular Data Mixture Strategy for Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08370
&lt;/p&gt;
&lt;p&gt;
SMART&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#65292;&#24182;&#22312;&#24494;&#35843;&#20013;&#37325;&#26032;&#20998;&#37197;&#39044;&#31639;&#65292;&#20174;&#32780;&#22312;&#25351;&#20196;&#35843;&#25972;&#20219;&#21153;&#20013;&#21462;&#24471;&#26126;&#26174;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25351;&#20196;&#35843;&#25972;&#28041;&#21450;&#22312;&#19968;&#32452;&#20197;&#25351;&#20196;&#26684;&#24335;&#21270;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#27169;&#22411;&#23545;&#26410;&#35265;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#24494;&#35843;&#36807;&#31243;&#20013;&#24179;&#34913;&#19981;&#21516;&#20219;&#21153;&#27604;&#20363;&#30340;&#37325;&#35201;&#24615;&#65292;&#20294;&#25214;&#21040;&#21512;&#36866;&#30340;&#24179;&#34913;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30446;&#21069;&#38500;&#20102;&#25163;&#21160;&#35843;&#25972;&#25110;&#20381;&#36182;&#20174;&#19994;&#32773;&#30340;&#30452;&#35273;&#22806;&#65292;&#23578;&#26080;&#31995;&#32479;&#26041;&#27861;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;SMART&#65288;Submodular data Mixture strAtegy for instRuction Tuning&#65289;- &#19968;&#31181;&#21033;&#29992;&#23376;&#27169;&#22359;&#20989;&#25968;&#20026;&#20219;&#21153;&#20998;&#37197;&#37325;&#35201;&#24615;&#20998;&#25968;&#30340;&#26032;&#39062;&#25968;&#25454;&#28151;&#21512;&#31574;&#30053;&#65292;&#28982;&#21518;&#29992;&#36825;&#20123;&#20998;&#25968;&#26469;&#30830;&#23450;&#28151;&#21512;&#26435;&#37325;&#12290;&#32473;&#23450;&#24494;&#35843;&#39044;&#31639;&#65292;SMART&#37325;&#26032;&#20998;&#37197;&#20219;&#21153;&#38388;&#30340;&#39044;&#31639;&#65292;&#24182;&#20174;&#27599;&#20010;&#20219;&#21153;&#20013;&#36873;&#25321;&#38750;&#20887;&#20313;&#26679;&#26412;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SMART&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#22914;&#20363;&#23376;&#27604;&#20363;&#28151;&#21512;&#21644;&#22343;&#31561;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08370v1 Announce Type: cross  Abstract: Instruction Tuning involves finetuning a language model on a collection of instruction-formatted datasets in order to enhance the generalizability of the model to unseen tasks. Studies have shown the importance of balancing different task proportions during finetuning, but finding the right balance remains challenging. Unfortunately, there's currently no systematic method beyond manual tuning or relying on practitioners' intuition. In this paper, we introduce SMART (Submodular data Mixture strAtegy for instRuction Tuning) - a novel data mixture strategy which makes use of a submodular function to assign importance scores to tasks which are then used to determine the mixture weights. Given a fine-tuning budget, SMART redistributes the budget among tasks and selects non-redundant samples from each task. Experimental results demonstrate that SMART significantly outperforms traditional methods such as examples proportional mixing and equal
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#38271;&#23614;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#29305;&#24449;&#32479;&#35745;&#20998;&#24320;&#24335;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#24335;&#35299;&#20915;&#23614;&#37096;&#31867;&#21035;&#31232;&#30095;&#20998;&#24067;&#23548;&#33268;&#30340;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.08364</link><description>&lt;p&gt;
&#38024;&#23545;&#38271;&#23614;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#29305;&#24449;&#32479;&#35745;&#20998;&#24320;&#24335;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Decoupled Federated Learning on Long-Tailed and Non-IID data with Feature Statistics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08364
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38024;&#23545;&#38271;&#23614;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#30340;&#29305;&#24449;&#32479;&#35745;&#20998;&#24320;&#24335;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#38454;&#27573;&#26041;&#24335;&#35299;&#20915;&#23614;&#37096;&#31867;&#21035;&#31232;&#30095;&#20998;&#24067;&#23548;&#33268;&#30340;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26088;&#22312;&#22686;&#24378;&#25968;&#25454;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#24615;&#65292;&#20294;&#22312;&#22788;&#29702;&#38271;&#23614;&#21644;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#24322;&#26500;&#25968;&#25454;&#26102;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#24773;&#26223;&#65292;&#21363;&#23614;&#37096;&#31867;&#21035;&#22312;&#23569;&#25968;&#23458;&#25143;&#31471;&#19978;&#31232;&#30095;&#20998;&#24067;&#65292;&#23548;&#33268;&#20351;&#29992;&#36825;&#20123;&#31867;&#21035;&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#23458;&#25143;&#31471;&#32858;&#21512;&#36807;&#31243;&#20013;&#34987;&#36873;&#25321;&#30340;&#27010;&#29575;&#36739;&#20302;&#65292;&#20174;&#32780;&#23548;&#33268;&#25910;&#25947;&#36895;&#24230;&#36739;&#24930;&#21644;&#27169;&#22411;&#24615;&#33021;&#36739;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#29305;&#24449;&#32479;&#35745;&#30340;&#20004;&#38454;&#27573;&#20998;&#24320;&#24335;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65288;DFL-FS&#65289;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#26381;&#21153;&#22120;&#36890;&#36807;&#33945;&#29256;&#23616;&#37096;&#29305;&#24449;&#32479;&#35745;&#32858;&#31867;&#20272;&#35745;&#23458;&#25143;&#31471;&#30340;&#31867;&#21035;&#35206;&#30422;&#20998;&#24067;&#65292;&#20197;&#21152;&#24555;&#25910;&#25947;&#36895;&#24230;&#21644;&#22686;&#24378;&#29305;&#24449;&#23398;&#20064;&#32780;&#19981;&#27844;&#38706;&#38544;&#31169;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;DFL-FS&#22522;&#20110;&#20840;&#23616;&#29305;&#24449;&#32479;&#35745;&#37319;&#29992;&#32852;&#37030;&#29305;&#24449;&#20877;&#29983;&#65292;&#24182;&#21033;&#29992;&#37325;&#37319;&#26679;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08364v1 Announce Type: cross  Abstract: Federated learning is designed to enhance data security and privacy, but faces challenges when dealing with heterogeneous data in long-tailed and non-IID distributions. This paper explores an overlooked scenario where tail classes are sparsely distributed over a few clients, causing the models trained with these classes to have a lower probability of being selected during client aggregation, leading to slower convergence rates and poorer model performance. To address this issue, we propose a two-stage Decoupled Federated learning framework using Feature Statistics (DFL-FS). In the first stage, the server estimates the client's class coverage distributions through masked local feature statistics clustering to select models for aggregation to accelerate convergence and enhance feature learning without privacy leakage. In the second stage, DFL-FS employs federated feature regeneration based on global feature statistics and utilizes resamp
&lt;/p&gt;</description></item><item><title>&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#65292;&#20026;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.08352</link><description>&lt;p&gt;
&#21033;&#29992;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#21450;&#19982;&#20256;&#32479;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#24615;&#33021;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Data augmentation with automated machine learning: approaches and performance comparison with classical data augmentation methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08352
&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#36807;&#31243;&#65292;&#20026;&#25913;&#21892;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#25552;&#20379;&#20102;&#26356;&#39640;&#25928;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#34987;&#35748;&#20026;&#26159;&#24120;&#29992;&#20110;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27867;&#21270;&#24615;&#33021;&#30340;&#26368;&#37325;&#35201;&#30340;&#27491;&#21017;&#21270;&#25216;&#26415;&#12290;&#23427;&#20027;&#35201;&#28041;&#21450;&#24212;&#29992;&#36866;&#24403;&#30340;&#25968;&#25454;&#36716;&#25442;&#25805;&#20316;&#65292;&#20197;&#21019;&#24314;&#20855;&#26377;&#25152;&#38656;&#23646;&#24615;&#30340;&#26032;&#25968;&#25454;&#26679;&#26412;&#12290;&#23613;&#31649;&#20854;&#26377;&#25928;&#24615;&#65292;&#36825;&#19968;&#36807;&#31243;&#36890;&#24120;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#25163;&#21160;&#21019;&#24314;&#21644;&#27979;&#35797;&#19981;&#21516;&#20505;&#36873;&#22686;&#24378;&#21450;&#20854;&#36229;&#21442;&#25968;&#38656;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#12290;&#33258;&#21160;&#21270;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26088;&#22312;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#65288;AutoML&#65289;&#21407;&#21017;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#20110;AutoML&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#30340;&#20840;&#38754;&#35843;&#26597;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20351;&#29992;AutoML&#23454;&#29616;&#25968;&#25454;&#22686;&#24378;&#30340;&#21508;&#31181;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#25805;&#20316;&#12289;&#25968;&#25454;&#38598;&#25104;&#21644;&#25968;&#25454;&#21512;&#25104;&#25216;&#26415;&#12290;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08352v1 Announce Type: cross  Abstract: Data augmentation is arguably the most important regularization technique commonly used to improve generalization performance of machine learning models. It primarily involves the application of appropriate data transformation operations to create new data samples with desired properties. Despite its effectiveness, the process is often challenging because of the time-consuming trial and error procedures for creating and testing different candidate augmentations and their hyperparameters manually. Automated data augmentation methods aim to automate the process. State-of-the-art approaches typically rely on automated machine learning (AutoML) principles. This work presents a comprehensive survey of AutoML-based data augmentation techniques. We discuss various approaches for accomplishing data augmentation with AutoML, including data manipulation, data integration and data synthesis techniques. We present extensive discussion of technique
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25972;&#21512;&#21040;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;(TSC)&#31995;&#32479;&#20013;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;TSC&#31995;&#32479;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#22330;&#26223;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#20351;&#24471;LLMs&#19982;&#19968;&#31995;&#21015;&#24863;&#30693;&#21644;&#20915;&#31574;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25552;&#21319;TSC&#31995;&#32479;&#23545;&#22478;&#24066;&#20132;&#36890;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#30340;&#31649;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08337</link><description>&lt;p&gt;
LLM&#36741;&#21161;&#19979;&#30340;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65306;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22797;&#26434;&#22478;&#24066;&#29615;&#22659;&#20013;&#23454;&#29616;&#20154;&#31867;&#20223;&#29983;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
LLM-Assisted Light: Leveraging Large Language Model Capabilities for Human-Mimetic Traffic Signal Control in Complex Urban Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08337
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25972;&#21512;&#21040;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;(TSC)&#31995;&#32479;&#20013;&#30340;&#21019;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#20256;&#32479;TSC&#31995;&#32479;&#22312;&#36866;&#24212;&#19981;&#29087;&#24713;&#22330;&#26223;&#26041;&#38754;&#30340;&#38480;&#21046;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#20351;&#24471;LLMs&#19982;&#19968;&#31995;&#21015;&#24863;&#30693;&#21644;&#20915;&#31574;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25552;&#21319;TSC&#31995;&#32479;&#23545;&#22478;&#24066;&#20132;&#36890;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#30340;&#31649;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#37117;&#24066;&#22320;&#21306;&#30340;&#20132;&#36890;&#25317;&#22581;&#26159;&#19968;&#20010;&#20855;&#26377;&#28145;&#36828;&#32463;&#27982;&#12289;&#29615;&#22659;&#21644;&#31038;&#20250;&#24433;&#21709;&#30340;&#24040;&#22823;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#26377;&#25928;&#30340;&#25317;&#22581;&#31649;&#29702;&#33267;&#20851;&#37325;&#35201;&#65292;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;(TSC)&#31995;&#32479;&#22312;&#36825;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#20026;&#20102;&#22238;&#24212;&#20256;&#32479;TSC&#31995;&#32479;&#22312;&#31649;&#29702;&#22478;&#24066;&#20132;&#36890;&#27969;&#21160;&#30340;&#22797;&#26434;&#24615;&#21644;&#21464;&#24322;&#24615;&#26041;&#38754;&#32463;&#24120;&#34920;&#29616;&#20986;&#30340;&#19981;&#36275;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#26041;&#27861;&#65292;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#25972;&#21512;&#21040;TSC&#20013;&#65292;&#21033;&#29992;&#20854;&#20808;&#36827;&#30340;&#25512;&#29702;&#21644;&#20915;&#31574;&#33021;&#21147;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28151;&#21512;&#26694;&#26550;&#65292;&#23558;LLMs&#19982;&#19968;&#22871;&#24863;&#30693;&#21644;&#20915;&#31574;&#24037;&#20855;&#30456;&#32467;&#21512;&#65292;&#26377;&#21161;&#20110;&#25506;&#35752;&#38745;&#24577;&#21644;&#21160;&#24577;&#20132;&#36890;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08337v1 Announce Type: cross  Abstract: Traffic congestion in metropolitan areas presents a formidable challenge with far-reaching economic, environmental, and societal ramifications. Therefore, effective congestion management is imperative, with traffic signal control (TSC) systems being pivotal in this endeavor. Conventional TSC systems, designed upon rule-based algorithms or reinforcement learning (RL), frequently exhibit deficiencies in managing the complexities and variabilities of urban traffic flows, constrained by their limited capacity for adaptation to unfamiliar scenarios. In response to these limitations, this work introduces an innovative approach that integrates Large Language Models (LLMs) into TSC, harnessing their advanced reasoning and decision-making faculties. Specifically, a hybrid framework that augments LLMs with a suite of perception and decision-making tools is proposed, facilitating the interrogation of both the static and dynamic traffic informatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#31232;&#30095;&#21407;&#21017;&#65292;&#24314;&#31435;&#20102;&#20004;&#20010;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65292;&#20026;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#35774;&#32622;&#20102;&#22522;&#30784;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.08335</link><description>&lt;p&gt;
&#37096;&#20998;&#21487;&#35266;&#27979;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#31232;&#30095;&#21407;&#21017;
&lt;/p&gt;
&lt;p&gt;
A Sparsity Principle for Partially Observable Causal Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08335
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#37096;&#20998;&#21487;&#35266;&#27979;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#30340;&#31232;&#30095;&#21407;&#21017;&#65292;&#24314;&#31435;&#20102;&#20004;&#20010;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65292;&#20026;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#21644;&#20998;&#27573;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#35774;&#32622;&#20102;&#22522;&#30784;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;&#26088;&#22312;&#20174;&#24863;&#30693;&#25968;&#25454;&#20013;&#35782;&#21035;&#39640;&#23618;&#27425;&#30340;&#22240;&#26524;&#21464;&#37327;&#12290;&#26412;&#25991;&#32771;&#34385;&#37096;&#20998;&#35266;&#27979;&#35774;&#32622;&#65292;&#20854;&#20013;&#27599;&#27425;&#27979;&#37327;&#20165;&#25552;&#20379;&#20851;&#20110;&#28508;&#22312;&#22240;&#26524;&#29366;&#24577;&#23376;&#38598;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20174;&#25968;&#25454;&#38598;&#20013;&#19981;&#37197;&#23545;&#35266;&#23519;&#23398;&#20064;&#65292;&#20854;&#20013;&#23384;&#22312;&#23454;&#20363;&#30456;&#20851;&#30340;&#37096;&#20998;&#21487;&#35266;&#27979;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20026;&#35813;&#35774;&#32622;&#24314;&#31435;&#20004;&#20010;&#21487;&#35782;&#21035;&#24615;&#32467;&#26524;&#65306;&#19968;&#20010;&#26159;&#20851;&#20110;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#30340;&#32467;&#26524;&#65292;&#26080;&#38656;&#23545;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#20570;&#21442;&#25968;&#20551;&#35774;&#65292;&#21478;&#19968;&#20010;&#26159;&#23545;&#20855;&#26377;&#39640;&#26031;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#30340;&#20998;&#27573;&#32447;&#24615;&#28151;&#21512;&#20989;&#25968;&#30340;&#32467;&#26524;&#12290;&#22522;&#20110;&#36825;&#20123;&#35265;&#35299;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#29992;&#20110;&#20272;&#35745;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08335v1 Announce Type: cross  Abstract: Causal representation learning aims at identifying high-level causal variables from perceptual data. Most methods assume that all latent causal variables are captured in the high-dimensional observations. We instead consider a partially observed setting, in which each measurement only provides information about a subset of the underlying causal state. Prior work has studied this setting with multiple domains or views, each depending on a fixed subset of latents. Here, we focus on learning from unpaired observations from a dataset with an instance-dependent partial observability pattern. Our main contribution is to establish two identifiability results for this setting: one for linear mixing functions without parametric assumptions on the underlying causal model, and one for piecewise linear mixing functions with Gaussian latent causal variables. Based on these insights, we propose two methods for estimating the underlying causal variab
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#35757;&#32451;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.08333</link><description>&lt;p&gt;
&#24555;&#36895;&#25512;&#26029;&#22522;&#20110;&#31227;&#38500;&#30340;&#33410;&#28857;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Fast Inference of Removal-Based Node Influence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08333
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#27979;&#37327;&#35757;&#32451;&#22909;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#65292;&#20197;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#34987;&#24191;&#27867;&#29992;&#20110;&#25429;&#33719;&#22270;&#20013;&#20449;&#24687;&#20256;&#25773;&#27169;&#24335;&#12290;&#34429;&#28982;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#65292;&#20294;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#36235;&#21183;&#26085;&#30410;&#21463;&#21040;&#20851;&#27880;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#33410;&#28857;&#24433;&#21709;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#34913;&#37327;&#35757;&#32451;&#22909;&#30340;GNN&#27169;&#22411;&#22312;&#31227;&#38500;&#33410;&#28857;&#21518;&#30340;&#39044;&#27979;&#21464;&#21270;&#12290;&#19968;&#20010;&#30495;&#23454;&#24212;&#29992;&#26159;&#65292;&#8220;&#22312;&#39044;&#27979;Twitter&#36134;&#25143;&#26497;&#24615;&#30340;&#20219;&#21153;&#20013;&#65292;&#22914;&#26524;&#31227;&#38500;&#29305;&#23450;&#36134;&#25143;&#65292;&#20854;&#20182;&#36134;&#25143;&#30340;&#26497;&#24615;&#20250;&#22914;&#20309;&#25913;&#21464;&#65311;&#8221;&#25105;&#20204;&#23558;GNN&#20316;&#20026;&#19968;&#20010;&#20195;&#29702;&#27169;&#22411;&#65292;&#20854;&#39044;&#27979;&#21487;&#20197;&#27169;&#25311;&#31227;&#38500;&#33410;&#28857;&#24341;&#36215;&#30340;&#33410;&#28857;&#25110;&#36793;&#30340;&#21464;&#21270;&#12290;&#20026;&#20102;&#33719;&#24471;&#27599;&#20010;&#33410;&#28857;&#30340;&#24433;&#21709;&#65292;&#19968;&#31181;&#30452;&#25509;&#30340;&#26041;&#27861;&#26159;&#20132;&#26367;&#31227;&#38500;&#27599;&#20010;&#33410;&#28857;&#65292;&#24182;&#22312;&#20462;&#25913;&#21518;&#30340;&#22270;&#19978;&#24212;&#29992;&#35757;&#32451;&#22909;&#30340;GNN&#12290;&#36825;&#26159;&#21487;&#38752;&#30340;&#20294;&#32791;&#26102;&#65292;&#22240;&#27492;&#25105;&#20204;&#38656;&#35201;&#19968;&#31181;&#39640;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08333v1 Announce Type: cross  Abstract: Graph neural networks (GNNs) are widely utilized to capture the information spreading patterns in graphs. While remarkable performance has been achieved, there is a new trending topic of evaluating node influence. We propose a new method of evaluating node influence, which measures the prediction change of a trained GNN model caused by removing a node. A real-world application is, "In the task of predicting Twitter accounts' polarity, had a particular account been removed, how would others' polarity change?". We use the GNN as a surrogate model whose prediction could simulate the change of nodes or edges caused by node removal. To obtain the influence for every node, a straightforward way is to alternately remove every node and apply the trained GNN on the modified graph. It is reliable but time-consuming, so we need an efficient method. The related lines of work, such as graph adversarial attack and counterfactual explanation, cannot 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#33258;&#22238;&#24402;&#39044;&#27979;&#22810;&#29305;&#24449;&#20998;&#25968;&#30340;&#26041;&#27861;&#65288;ArTS&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;T5&#26469;&#32467;&#21512;&#35299;&#30721;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#20013;&#22810;&#20998;&#25968;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.08332</link><description>&lt;p&gt;
&#33258;&#22238;&#24402;&#24471;&#20998;&#29983;&#25104;&#29992;&#20110;&#22810;&#29305;&#24449;&#20316;&#25991;&#35780;&#20998;
&lt;/p&gt;
&lt;p&gt;
Autoregressive Score Generation for Multi-trait Essay Scoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08332
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#33258;&#22238;&#24402;&#39044;&#27979;&#22810;&#29305;&#24449;&#20998;&#25968;&#30340;&#26041;&#27861;&#65288;ArTS&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;T5&#26469;&#32467;&#21512;&#35299;&#30721;&#36807;&#31243;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#20013;&#22810;&#20998;&#25968;&#39044;&#27979;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20165;&#32534;&#30721;&#22120;&#39044;&#35757;&#32451;&#27169;&#22411;&#22914;BERT&#24050;&#25104;&#21151;&#24212;&#29992;&#20110;&#33258;&#21160;&#20316;&#25991;&#35780;&#20998;&#65288;AES&#65289;&#20013;&#65292;&#29992;&#20110;&#39044;&#27979;&#21333;&#19968;&#25972;&#20307;&#20998;&#25968;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#23578;&#26410;&#25506;&#32034;&#36825;&#20123;&#27169;&#22411;&#22312;&#22810;&#29305;&#24449;AES&#20013;&#30340;&#24212;&#29992;&#65292;&#21487;&#33021;&#26159;&#30001;&#20110;&#20026;&#27599;&#20010;&#29305;&#24449;&#22797;&#21046;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#30340;&#25928;&#29575;&#20302;&#19979;&#12290;&#25105;&#20204;&#31361;&#30772;&#20102;&#29616;&#26377;&#20165;&#20351;&#29992;&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#22238;&#24402;&#39044;&#27979;&#22810;&#29305;&#24449;&#20998;&#25968;&#65288;ArTS&#65289;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;T5&#26469;&#32467;&#21512;&#19968;&#20010;&#35299;&#30721;&#36807;&#31243;&#12290;&#19982;&#20808;&#21069;&#30340;&#22238;&#24402;&#25110;&#20998;&#31867;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;AES&#20026;&#19968;&#20010;&#24471;&#20998;&#29983;&#25104;&#20219;&#21153;&#65292;&#20801;&#35768;&#21333;&#20010;&#27169;&#22411;&#39044;&#27979;&#22810;&#20010;&#20998;&#25968;&#12290;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#65292;&#38543;&#21518;&#30340;&#29305;&#24449;&#39044;&#27979;&#21487;&#20197;&#36890;&#36807;&#22312;&#20808;&#21069;&#30340;&#29305;&#24449;&#20998;&#25968;&#19978;&#36827;&#34892;&#26465;&#20214;&#21270;&#32780;&#21463;&#30410;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;ArTS&#30340;&#26377;&#25928;&#24615;&#65292;&#26174;&#31034;&#20102;&#22312;&#25552;&#31034;&#21644;&#29305;&#24449;&#26041;&#38754;&#24179;&#22343;&#25552;&#39640;5%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08332v1 Announce Type: cross  Abstract: Recently, encoder-only pre-trained models such as BERT have been successfully applied in automated essay scoring (AES) to predict a single overall score. However, studies have yet to explore these models in multi-trait AES, possibly due to the inefficiency of replicating BERT-based models for each trait. Breaking away from the existing sole use of encoder, we propose an autoregressive prediction of multi-trait scores (ArTS), incorporating a decoding process by leveraging the pre-trained T5. Unlike prior regression or classification methods, we redefine AES as a score-generation task, allowing a single model to predict multiple scores. During decoding, the subsequent trait prediction can benefit by conditioning on the preceding trait scores. Experimental results proved the efficacy of ArTS, showing over 5% average improvements in both prompts and traits.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#28145;&#20837;&#20998;&#26512;&#20102;LLMs&#22312;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25506;&#35752;&#20102;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#23545;&#20854;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLMs&#31283;&#20581;&#24615;&#31574;&#30053;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.08319</link><description>&lt;p&gt;
LLMs&#30340;&#30693;&#35782;&#20914;&#31361;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Knowledge Conflicts for LLMs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08319
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#28145;&#20837;&#20998;&#26512;&#20102;LLMs&#22312;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20914;&#31361;&#65292;&#25506;&#35752;&#20102;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#23545;&#20854;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#30340;&#37325;&#35201;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#25913;&#36827;LLMs&#31283;&#20581;&#24615;&#31574;&#30053;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#30693;&#35782;&#20914;&#31361;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#31361;&#20986;&#20102;&#24403;&#23427;&#20204;&#34701;&#21512;&#19978;&#19979;&#25991;&#21644;&#21442;&#25968;&#21270;&#30693;&#35782;&#26102;&#25152;&#36935;&#21040;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;&#25105;&#20204;&#20851;&#27880;&#19977;&#31867;&#30693;&#35782;&#20914;&#31361;&#65306;&#19978;&#19979;&#25991;-&#35760;&#24518;&#20914;&#31361;&#12289;&#36328;&#19978;&#19979;&#25991;&#20914;&#31361;&#21644;&#20869;&#37096;&#35760;&#24518;&#20914;&#31361;&#12290;&#36825;&#20123;&#20914;&#31361;&#21487;&#33021;&#20250;&#26174;&#33879;&#24433;&#21709;LLMs&#30340;&#21487;&#20449;&#24230;&#21644;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65292;&#22122;&#38899;&#21644;&#38169;&#35823;&#20449;&#24687;&#24456;&#24120;&#35265;&#12290;&#36890;&#36807;&#23545;&#36825;&#20123;&#20914;&#31361;&#36827;&#34892;&#20998;&#31867;&#65292;&#25506;&#35752;&#20854;&#21407;&#22240;&#65292;&#30740;&#31350;LLMs&#22312;&#36825;&#20123;&#20914;&#31361;&#19979;&#30340;&#34892;&#20026;&#65292;&#24182;&#22238;&#39038;&#21487;&#29992;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26412;&#35843;&#26597;&#26088;&#22312;&#20026;&#25913;&#36827;LLMs&#30340;&#31283;&#20581;&#24615;&#31574;&#30053;&#25552;&#20379;&#21551;&#31034;&#65292;&#20174;&#32780;&#25104;&#20026;&#25512;&#21160;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#39046;&#22495;&#30740;&#31350;&#30340;&#23453;&#36149;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08319v1 Announce Type: cross  Abstract: This survey provides an in-depth analysis of knowledge conflicts for large language models (LLMs), highlighting the complex challenges they encounter when blending contextual and parametric knowledge. Our focus is on three categories of knowledge conflicts: context-memory, inter-context, and intra-memory conflict. These conflicts can significantly impact the trustworthiness and performance of LLMs, especially in real-world applications where noise and misinformation are common. By categorizing these conflicts, exploring the causes, examining the behaviors of LLMs under such conflicts, and reviewing available solutions, this survey aims to shed light on strategies for improving the robustness of LLMs, thereby serving as a valuable resource for advancing research in this evolving area.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;StreamingDialogue&#65292;&#36890;&#36807;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65292;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23569;&#65292;&#24182;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;</title><link>https://arxiv.org/abs/2403.08312</link><description>&lt;p&gt;
&#36890;&#36807;&#26368;&#23567;&#25439;&#22833;&#36827;&#34892;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;StreamingDialogue&#65306;&#38271;&#23545;&#35805;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
StreamingDialogue: Prolonged Dialogue Learning via Long Context Compression with Minimal Losses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08312
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;StreamingDialogue&#65292;&#36890;&#36807;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65292;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#20943;&#23569;&#65292;&#24182;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26631;&#20934;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#22788;&#29702;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#26102;&#36935;&#21040;&#20102;&#25928;&#29575;&#21644;&#19968;&#33268;&#24615;&#38382;&#39064;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#35266;&#23519;&#65292;&#23545;&#35805;&#19978;&#19979;&#25991;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;&#24182;&#19988;&#23545;&#35805;&#20013;&#30340;&#29305;&#27530;&#26631;&#35760;\textit{End-of-Utterance} (EoU) &#26377;&#32858;&#21512;&#20449;&#24687;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#23558;EoU&#26631;&#35760;&#31216;&#20026;"&#20250;&#35805;&#27880;&#24847;&#21147;&#27719;&#38598;&#28857;"&#65288;conv-attn sinks&#65289;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;StreamingDialogue&#65292;&#23558;&#38271;&#23545;&#35805;&#21382;&#21490;&#21387;&#32553;&#20026;conv-attn&#27785;&#28857;&#65292;&#24182;&#26368;&#23567;&#21270;&#25439;&#22833;&#65292;&#20174;&#32780;&#20351;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#27785;&#28857;&#25968;&#37327;&#65288;&#21363;&#35805;&#35821;&#25968;&#37327;&#65289;&#30340;&#24179;&#26041;&#25104;&#27491;&#27604;&#12290;&#24403;&#21069;&#30340;LLMs&#24050;&#32463;&#23637;&#31034;&#20102;&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;&#33021;&#21147;&#65292;&#20363;&#22914;&#65292;&#31383;&#21475;&#22823;&#23567;&#36798;&#21040;200k&#29978;&#33267;&#26356;&#22823;&#12290;&#36890;&#36807;&#23558;&#35805;&#35821;&#21387;&#32553;&#20026;EoUs&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#28508;&#21147;&#22788;&#29702;&#36229;&#36807;200k&#26465;&#35805;&#35821;&#65292;&#23454;&#29616;&#38271;&#26102;&#38388;&#23545;&#35805;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08312v1 Announce Type: cross  Abstract: Standard Large Language Models (LLMs) struggle with handling dialogues with long contexts due to efficiency and consistency issues. According to our observation, dialogue contexts are highly structured, and the special token of \textit{End-of-Utterance} (EoU) in dialogues has the potential to aggregate information. We refer to the EoU tokens as ``conversational attention sinks'' (conv-attn sinks). Accordingly, we introduce StreamingDialogue, which compresses long dialogue history into conv-attn sinks with minimal losses, and thus reduces computational complexity quadratically with the number of sinks (i.e., the number of utterances). Current LLMs already demonstrate the ability to handle long context window, e.g., a window size of 200k or more. To this end, by compressing utterances into EoUs, our method has the potential to handle more than 200k of utterances, resulting in a prolonged dialogue learning. In order to minimize informatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;HRLAIF&#26041;&#27861;&#26469;&#25913;&#21892;&#24320;&#25918;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#21709;&#24212;&#24110;&#21161;&#24615;&#65292;&#36890;&#36807;&#22686;&#24378;AI&#27880;&#37322;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#40065;&#26834;&#24615;</title><link>https://arxiv.org/abs/2403.08309</link><description>&lt;p&gt;
HRLAIF: &#36890;&#36807;AI&#21453;&#39304;&#25913;&#36827;&#24320;&#25918;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24110;&#21161;&#24615;&#21644;&#26080;&#23475;&#24615;
&lt;/p&gt;
&lt;p&gt;
HRLAIF: Improvements in Helpfulness and Harmlessness in Open-domain Reinforcement Learning From AI Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;HRLAIF&#26041;&#27861;&#26469;&#25913;&#21892;&#24320;&#25918;&#22495;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#27169;&#22411;&#21709;&#24212;&#24110;&#21161;&#24615;&#65292;&#36890;&#36807;&#22686;&#24378;AI&#27880;&#37322;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20174;AI&#21453;&#39304;&#65288;RLAIF&#65289;&#30456;&#27604;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65288;RLHF&#65289;&#20855;&#26377;&#26356;&#30701;&#30340;&#27880;&#37322;&#21608;&#26399;&#21644;&#26356;&#20302;&#30340;&#25104;&#26412;&#20248;&#21183;&#65292;&#20351;&#20854;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#35757;&#32451;&#30340;&#24555;&#36895;&#31574;&#30053;&#36845;&#20195;&#38454;&#27573;&#38750;&#24120;&#39640;&#25928;&#12290;&#20351;&#29992;ChatGPT&#20316;&#20026;&#26631;&#27880;&#21592;&#65292;&#22312;RLAIF&#35757;&#32451;&#20013;&#20026;&#24320;&#25918;&#22495;&#25552;&#31034;&#25552;&#20379;&#21453;&#39304;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20154;&#31867;&#35780;&#20272;&#32773;&#23545;&#27169;&#22411;&#21709;&#24212;&#30340;&#20559;&#22909;&#32988;&#29575;&#22686;&#21152;&#65292;&#20294;&#35780;&#20272;&#32773;&#30340;&#28385;&#24847;&#24230;&#19979;&#38477;&#12290;&#20998;&#26512;&#34920;&#26126;&#65292;&#28385;&#24847;&#24230;&#19979;&#38477;&#20027;&#35201;&#26159;&#22240;&#20026;&#19968;&#20123;&#21709;&#24212;&#21464;&#24471;&#19981;&#22815;&#26377;&#24110;&#21161;&#65292;&#29305;&#21035;&#26159;&#22312;&#27491;&#30830;&#24615;&#21644;&#30495;&#23454;&#24615;&#26041;&#38754;&#65292;&#31361;&#26174;&#20102;&#22522;&#26412;RLAIF&#30340;&#23454;&#38469;&#23616;&#38480;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28151;&#21512;&#24378;&#21270;&#23398;&#20064;&#20174;AI&#21453;&#39304;&#65288;HRLAIF&#65289;&#12290;&#35813;&#26041;&#27861;&#22686;&#24378;&#20102;AI&#27880;&#37322;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#27169;&#22411;&#30340;&#24110;&#21161;&#24615;&#26356;&#21152;&#31283;&#20581;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08309v1 Announce Type: cross  Abstract: Reinforcement Learning from AI Feedback (RLAIF) has the advantages of shorter annotation cycles and lower costs over Reinforcement Learning from Human Feedback (RLHF), making it highly efficient during the rapid strategy iteration periods of large language model (LLM) training. Using ChatGPT as a labeler to provide feedback on open-domain prompts in RLAIF training, we observe an increase in human evaluators' preference win ratio for model responses, but a decrease in evaluators' satisfaction rate. Analysis suggests that the decrease in satisfaction rate is mainly due to some responses becoming less helpful, particularly in terms of correctness and truthfulness, highlighting practical limitations of basic RLAIF. In this paper, we propose Hybrid Reinforcement Learning from AI Feedback (HRLAIF). This method enhances the accuracy of AI annotations for responses, making the model's helpfulness more robust in training process. Additionally, 
&lt;/p&gt;</description></item><item><title>AutoDev&#26159;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;AI&#39537;&#21160;&#36719;&#20214;&#24320;&#21457;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22797;&#26434;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#30340;&#33258;&#20027;&#35268;&#21010;&#21644;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.08299</link><description>&lt;p&gt;
AutoDev&#65306;&#33258;&#21160;&#21270;AI&#39537;&#21160;&#24320;&#21457;
&lt;/p&gt;
&lt;p&gt;
AutoDev: Automated AI-Driven Development
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08299
&lt;/p&gt;
&lt;p&gt;
AutoDev&#26159;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;AI&#39537;&#21160;&#36719;&#20214;&#24320;&#21457;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22797;&#26434;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#30340;&#33258;&#20027;&#35268;&#21010;&#21644;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#39046;&#22495;&#38543;&#30528;AI&#21161;&#25163;&#30340;&#20986;&#29616;&#65288;&#22914;GitHub Copilot&#65289;&#32780;&#21457;&#29983;&#20102;&#33539;&#24335;&#36716;&#21464;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#24182;&#26410;&#20805;&#20998;&#21033;&#29992;IDE&#20013;&#21487;&#29992;&#30340;&#25152;&#26377;&#28508;&#22312;&#21151;&#33021;&#65292;&#22914;&#26500;&#24314;&#12289;&#27979;&#35797;&#12289;&#25191;&#34892;&#20195;&#30721;&#12289;git&#25805;&#20316;&#31561;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#21463;&#21040;&#20854;&#26377;&#38480;&#33021;&#21147;&#30340;&#38480;&#21046;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#22312;&#22522;&#20110;&#32842;&#22825;&#30340;&#30028;&#38754;&#20013;&#24314;&#35758;&#20195;&#30721;&#29255;&#27573;&#21644;&#25991;&#20214;&#25805;&#20316;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#32570;&#21475;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AutoDev&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;AI&#39537;&#21160;&#36719;&#20214;&#24320;&#21457;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#29616;&#22797;&#26434;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#30340;&#33258;&#20027;&#35268;&#21010;&#21644;&#25191;&#34892;&#12290;AutoDev&#20351;&#29992;&#25143;&#33021;&#22815;&#23450;&#20041;&#22797;&#26434;&#30340;&#36719;&#20214;&#24037;&#31243;&#30446;&#26631;&#65292;&#24182;&#23558;&#36825;&#20123;&#30446;&#26631;&#20998;&#37197;&#32473;AutoDev&#30340;&#33258;&#20027;AI&#20195;&#29702;&#20197;&#23454;&#29616;&#12290;&#36825;&#20123;AI&#20195;&#29702;&#21487;&#20197;&#22312;&#20195;&#30721;&#24211;&#19978;&#25191;&#34892;&#21508;&#31181;&#25805;&#20316;&#65292;&#21253;&#25324;&#25991;&#20214;&#32534;&#36753;&#12289;&#26816;&#32034;&#12289;&#26500;&#24314;&#36807;&#31243;&#12289;&#25191;&#34892;&#12289;&#27979;&#35797;&#21644;git&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08299v1 Announce Type: cross  Abstract: The landscape of software development has witnessed a paradigm shift with the advent of AI-powered assistants, exemplified by GitHub Copilot. However, existing solutions are not leveraging all the potential capabilities available in an IDE such as building, testing, executing code, git operations, etc. Therefore, they are constrained by their limited capabilities, primarily focusing on suggesting code snippets and file manipulation within a chat-based interface. To fill this gap, we present AutoDev, a fully automated AI-driven software development framework, designed for autonomous planning and execution of intricate software engineering tasks. AutoDev enables users to define complex software engineering objectives, which are assigned to AutoDev's autonomous AI Agents to achieve. These AI agents can perform diverse operations on a codebase, including file editing, retrieval, build processes, execution, testing, and git operations. They
&lt;/p&gt;</description></item><item><title>Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.08295</link><description>&lt;p&gt;
Gemma&#65306;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#30340;&#24320;&#25918;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Gemma: Open Models Based on Gemini Research and Technology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08295
&lt;/p&gt;
&lt;p&gt;
Gemma&#26159;&#22522;&#20110;Gemini&#30740;&#31350;&#21644;&#25216;&#26415;&#25152;&#26500;&#24314;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#65292;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Gemma&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;Gemini&#27169;&#22411;&#30740;&#31350;&#21644;&#25216;&#26415;&#26500;&#24314;&#30340;&#36731;&#37327;&#32423;&#12289;&#26368;&#20808;&#36827;&#30340;&#24320;&#25918;&#27169;&#22411;&#31995;&#21015;&#12290;Gemma&#27169;&#22411;&#22312;&#35821;&#35328;&#29702;&#35299;&#12289;&#25512;&#29702;&#21644;&#23433;&#20840;&#24615;&#31561;&#23398;&#26415;&#22522;&#20934;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#21457;&#24067;&#20102;&#20004;&#20010;&#35268;&#27169;&#30340;&#27169;&#22411;&#65288;20&#20159;&#21644;70&#20159;&#21442;&#25968;&#65289;&#65292;&#24182;&#25552;&#20379;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#26816;&#26597;&#28857;&#12290;Gemma&#22312;18&#20010;&#22522;&#20110;&#25991;&#26412;&#30340;&#20219;&#21153;&#20013;&#65292;&#26377;11&#20010;&#20219;&#21153;&#20248;&#20110;&#31867;&#20284;&#35268;&#27169;&#30340;&#24320;&#25918;&#27169;&#22411;&#65292;&#24182;&#23545;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#21644;&#36131;&#20219;&#26041;&#38754;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21516;&#26102;&#35814;&#32454;&#25551;&#36848;&#20102;&#27169;&#22411;&#24320;&#21457;&#36807;&#31243;&#12290;&#25105;&#20204;&#30456;&#20449;&#36127;&#36131;&#20219;&#22320;&#21457;&#24067;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20110;&#25552;&#39640;&#21069;&#27839;&#27169;&#22411;&#30340;&#23433;&#20840;&#24615;&#65292;&#24182;&#23454;&#29616;&#19979;&#19968;&#27874;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21019;&#26032;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08295v1 Announce Type: cross  Abstract: This work introduces Gemma, a family of lightweight, state-of-the art open models built from the research and technology used to create Gemini models. Gemma models demonstrate strong performance across academic benchmarks for language understanding, reasoning, and safety. We release two sizes of models (2 billion and 7 billion parameters), and provide both pretrained and fine-tuned checkpoints. Gemma outperforms similarly sized open models on 11 out of 18 text-based tasks, and we present comprehensive evaluations of safety and responsibility aspects of the models, alongside a detailed description of model development. We believe the responsible release of LLMs is critical for improving the safety of frontier models, and for enabling the next wave of LLM innovations.
&lt;/p&gt;</description></item><item><title>GPST&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#23454;&#29616;&#23545;&#21407;&#22987;&#25991;&#26412;&#30340;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;&#20043;&#21069;SLM&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#21516;&#31561;&#35268;&#27169;&#30340;GPT-2&#12290;</title><link>https://arxiv.org/abs/2403.08293</link><description>&lt;p&gt;
&#29983;&#25104;&#39044;&#35757;&#32451;&#32467;&#26500;&#21270;Transformer&#65306;&#35268;&#27169;&#21270;&#30340;&#26080;&#30417;&#30563;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Pretrained Structured Transformers: Unsupervised Syntactic Language Models at Scale
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08293
&lt;/p&gt;
&lt;p&gt;
GPST&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#32852;&#21512;&#35757;&#32451;&#20004;&#20010;&#27169;&#22411;&#23454;&#29616;&#23545;&#21407;&#22987;&#25991;&#26412;&#30340;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#65292;&#20811;&#26381;&#20102;&#20043;&#21069;SLM&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#30340;&#38480;&#21046;&#65292;&#23637;&#31034;&#20102;&#22312;&#22810;&#20010;&#20219;&#21153;&#20013;&#20248;&#20110;&#21516;&#31561;&#35268;&#27169;&#30340;GPT-2&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21477;&#27861;&#35821;&#35328;&#27169;&#22411;&#65288;SLM&#65289;&#20197;&#20174;&#24038;&#21040;&#21491;&#30340;&#26041;&#24335;&#36880;&#27493;&#29983;&#25104;&#24102;&#26377;&#20854;&#21477;&#27861;&#26641;&#30340;&#21477;&#23376;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29983;&#25104;&#39044;&#35757;&#32451;&#32467;&#26500;&#21270;Transformer&#65288;GPST&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#35268;&#27169;&#21270;&#30340;&#26080;&#30417;&#30563;SLM&#65292;&#33021;&#22815;&#22312;&#21407;&#22987;&#25991;&#26412;&#19978;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;&#39640;&#24182;&#34892;&#39044;&#35757;&#32451;&#12290;GPST&#35268;&#36991;&#20102;&#20043;&#21069;SLM&#30340;&#19968;&#20123;&#38480;&#21046;&#65292;&#27604;&#22914;&#20381;&#36182;&#20110;&#40644;&#37329;&#26641;&#21644;&#39034;&#24207;&#35757;&#32451;&#12290;&#23427;&#30001;&#20004;&#20010;&#32452;&#20214;&#32452;&#25104;&#65292;&#19968;&#20010;&#36890;&#24120;&#30340;SLM&#21463;&#21333;&#21521;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#30340;&#30417;&#30563;&#65292;&#20197;&#21450;&#19968;&#20010;&#39069;&#22806;&#30340;&#32452;&#21512;&#27169;&#22411;&#65292;&#29992;&#20110;&#24341;&#23548;&#21477;&#27861;&#35299;&#26512;&#26641;&#24182;&#35745;&#31639;&#25104;&#20998;&#34920;&#31034;&#65292;&#21463;&#21452;&#21521;&#35821;&#35328;&#24314;&#27169;&#25439;&#22833;&#30340;&#30417;&#30563;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#26367;&#20195;&#26041;&#26696;&#65292;&#20197;&#23454;&#29616;&#20004;&#20010;&#27169;&#22411;&#30340;&#32852;&#21512;&#24182;&#34892;&#35757;&#32451;&#65292;&#37319;&#29992;&#30828;EM&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#22312;OpenWebText&#19978;&#23545;GPST&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#35813;&#35821;&#26009;&#24211;&#21253;&#25324;90&#20159;&#20010;token&#65292;&#24182;&#23637;&#31034;&#20102;GPST&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#30340;&#20248;&#36234;&#24615;&#65292;&#28085;&#30422;&#20102;&#19982;GPT-2&#30456;&#24403;&#35268;&#27169;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08293v1 Announce Type: cross  Abstract: A syntactic language model (SLM) incrementally generates a sentence with its syntactic tree in a left-to-right manner. We present Generative Pretrained Structured Transformers (GPST), an unsupervised SLM at scale capable of being pre-trained from scratch on raw texts with high parallelism. GPST circumvents the limitations of previous SLMs such as relying on gold trees and sequential training. It consists of two components, a usual SLM supervised by a uni-directional language modeling loss, and an additional composition model, which induces syntactic parse trees and computes constituent representations, supervised by a bi-directional language modeling loss. We propose a representation surrogate to enable joint parallel training of the two models in a hard-EM fashion. We pre-train GPST on OpenWebText, a corpus with $9$ billion tokens, and demonstrate the superiority of GPST over GPT-2 with a comparable size in numerous tasks covering bot
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#37197;&#23545;&#22238;&#24402;&#65288;WCR&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#31163;&#25955;&#30340;&#32858;&#21512;&#25968;&#25454;&#20013;&#26126;&#30830;&#25581;&#31034;&#20855;&#26377;L&#233;vy&#22122;&#22768;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#65292;&#22635;&#34917;&#20102;&#22312;&#25552;&#21462;&#20855;&#26377;L&#233;vy&#22122;&#22768;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.08292</link><description>&lt;p&gt;
&#20351;&#29992;&#24369;&#37197;&#23545;&#22238;&#24402;&#25512;&#26029;&#20855;&#26377;L&#233;vy&#22122;&#22768;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Weak Collocation Regression for Inferring Stochastic Dynamics with L\'{e}vy Noise
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#37197;&#23545;&#22238;&#24402;&#65288;WCR&#65289;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#31163;&#25955;&#30340;&#32858;&#21512;&#25968;&#25454;&#20013;&#26126;&#30830;&#25581;&#31034;&#20855;&#26377;L&#233;vy&#22122;&#22768;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#65292;&#22635;&#34917;&#20102;&#22312;&#25552;&#21462;&#20855;&#26377;L&#233;vy&#22122;&#22768;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#38543;&#26426;&#31995;&#32479;&#30340;&#35266;&#27979;&#12289;&#23454;&#39564;&#21644;&#27169;&#25311;&#25968;&#25454;&#24555;&#36895;&#22686;&#21152;&#65292;&#20154;&#20204;&#20026;&#20102;&#25214;&#21040;&#36825;&#20123;&#31995;&#32479;&#28436;&#21270;&#32972;&#21518;&#30340;&#25511;&#21046;&#35268;&#24459;&#36827;&#34892;&#20102;&#22823;&#37327;&#21162;&#21147;&#12290;&#23613;&#31649;&#38750;&#39640;&#26031;&#27874;&#21160;&#22312;&#35768;&#22810;&#29289;&#29702;&#29616;&#35937;&#20013;&#26377;&#30528;&#24191;&#27867;&#30340;&#24212;&#29992;&#65292;&#20294;&#29992;&#20110;&#25552;&#21462;&#20855;&#26377;L&#233;vy&#22122;&#22768;&#30340;&#38543;&#26426;&#21160;&#21147;&#23398;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#30456;&#23545;&#36739;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24369;&#37197;&#23545;&#22238;&#24402;&#65288;WCR&#65289;&#26041;&#27861;&#65292;&#26469;&#26126;&#30830;&#25581;&#31034;&#26410;&#30693;&#30340;&#38543;&#26426;&#21160;&#21147;&#31995;&#32479;&#65292;&#21363;&#20855;&#26377;$\alpha$-&#31283;&#23450;L&#233;vy&#22122;&#22768;&#21644;&#39640;&#26031;&#22122;&#22768;&#30340;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;SDE&#65289;&#65292;&#20174;&#31163;&#25955;&#30340;&#32858;&#21512;&#25968;&#25454;&#20013;&#12290;&#27492;&#26041;&#27861;&#21033;&#29992;&#27010;&#29575;&#20998;&#24067;&#20989;&#25968;&#30340;&#28436;&#21270;&#26041;&#31243;&#65292;&#21363;&#31119;&#20811;-&#26222;&#26391;&#20811;&#65288;FP&#65289;&#26041;&#31243;&#12290;&#36890;&#36807;FP&#26041;&#31243;&#30340;&#24369;&#24418;&#24335;&#65292;WCR&#26500;&#24314;&#20102;&#19968;&#20010;&#26410;&#30693;&#21442;&#25968;&#30340;&#32447;&#24615;&#31995;&#32479;&#65292;&#20854;&#20013;&#25152;&#26377;&#31215;&#20998;&#37117;&#36890;&#36807;&#33945;&#29305;&#21345;&#32599;&#26041;&#27861;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08292v1 Announce Type: cross  Abstract: With the rapid increase of observational, experimental and simulated data for stochastic systems, tremendous efforts have been devoted to identifying governing laws underlying the evolution of these systems. Despite the broad applications of non-Gaussian fluctuations in numerous physical phenomena, the data-driven approaches to extracting stochastic dynamics with L\'{e}vy noise are relatively few. In this work, we propose a Weak Collocation Regression (WCR) to explicitly reveal unknown stochastic dynamical systems, i.e., the Stochastic Differential Equation (SDE) with both $\alpha$-stable L\'{e}vy noise and Gaussian noise, from discrete aggregate data. This method utilizes the evolution equation of the probability distribution function, i.e., the Fokker-Planck (FP) equation. With the weak form of the FP equation, the WCR constructs a linear system of unknown parameters where all integrals are evaluated by Monte Carlo method with the ob
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22768;&#26126;&#24615;&#12289;&#32479;&#19968;API&#30340;Python&#24211;&#65292;&#36890;&#36807;&#31616;&#27905;&#30340;API&#35843;&#29992;&#31616;&#21270;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27969;&#31243;</title><link>https://arxiv.org/abs/2403.08291</link><description>&lt;p&gt;
CleanAgent&#65306;&#22522;&#20110;LLM&#20195;&#29702;&#33258;&#21160;&#21270;&#25968;&#25454;&#26631;&#20934;&#21270;
&lt;/p&gt;
&lt;p&gt;
CleanAgent: Automating Data Standardization with LLM-based Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08291
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#20855;&#26377;&#22768;&#26126;&#24615;&#12289;&#32479;&#19968;API&#30340;Python&#24211;&#65292;&#36890;&#36807;&#31616;&#27905;&#30340;API&#35843;&#29992;&#31616;&#21270;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#26631;&#20934;&#21270;&#26159;&#25968;&#25454;&#31185;&#23398;&#29983;&#21629;&#21608;&#26399;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#19968;&#37096;&#20998;&#12290;&#34429;&#28982;&#35832;&#22914;Pandas&#20043;&#31867;&#30340;&#24037;&#20855;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21151;&#33021;&#65292;&#20294;&#23427;&#20204;&#30340;&#22797;&#26434;&#24615;&#20197;&#21450;&#38656;&#35201;&#23450;&#21046;&#20195;&#30721;&#20197;&#36866;&#24212;&#19981;&#21516;&#21015;&#31867;&#22411;&#30340;&#25163;&#21160;&#25805;&#20316;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#24050;&#32463;&#23637;&#29616;&#20986;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#20195;&#30721;&#29983;&#25104;&#33258;&#21160;&#21270;&#27492;&#36807;&#31243;&#30340;&#28508;&#21147;&#65292;&#20294;&#20173;&#38656;&#35201;&#19987;&#19994;&#31243;&#24230;&#30340;&#32534;&#31243;&#30693;&#35782;&#21644;&#25345;&#32493;&#20114;&#21160;&#20197;&#36827;&#34892;&#21450;&#26102;&#30340;&#23436;&#21892;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#25552;&#20986;&#19968;&#20010;&#20855;&#26377;&#22768;&#26126;&#24615;&#12289;&#32479;&#19968;API&#30340;Python&#24211;&#65292;&#29992;&#20110;&#26631;&#20934;&#21270;&#21015;&#31867;&#22411;&#65292;&#36890;&#36807;&#31616;&#27905;&#30340;API&#35843;&#29992;&#31616;&#21270;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#27969;&#31243;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;Dataprep.Clean&#65292;&#20316;&#20026;Dataprep&#24211;&#30340;&#19968;&#20010;&#32452;&#20214;&#65292;&#36890;&#36807;&#19968;&#34892;&#20195;&#30721;&#23454;&#29616;&#29305;&#23450;&#21015;&#31867;&#22411;&#30340;&#26631;&#20934;&#21270;&#65292;&#26497;&#22823;&#38477;&#20302;&#20102;&#22797;&#26434;&#24615;&#12290;&#28982;&#21518;&#25105;&#20204;&#20171;&#32461;&#20102;CleanAgen
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08291v1 Announce Type: cross  Abstract: Data standardization is a crucial part in data science life cycle. While tools like Pandas offer robust functionalities, their complexity and the manual effort required for customizing code to diverse column types pose significant challenges. Although large language models (LLMs) like ChatGPT have shown promise in automating this process through natural language understanding and code generation, it still demands expert-level programming knowledge and continuous interaction for prompt refinement. To solve these challenges, our key idea is to propose a Python library with declarative, unified APIs for standardizing column types, simplifying the code generation of LLM with concise API calls. We first propose Dataprep.Clean which is written as a component of the Dataprep Library, offers a significant reduction in complexity by enabling the standardization of specific column types with a single line of code. Then we introduce the CleanAgen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08281</link><description>&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#35821;&#35328;&#27169;&#22411;&#21516;&#26102;&#25484;&#25569;&#25991;&#26412;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;
&lt;/p&gt;
&lt;p&gt;
Mastering Text, Code and Math Simultaneously via Fusing Highly Specialized Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08281
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#39640;&#24230;&#19987;&#19994;&#21270;&#30340;&#35821;&#35328;&#12289;&#20195;&#30721;&#21644;&#25968;&#23398;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;UltraFuser&#30340;&#34701;&#21512;&#26694;&#26550;&#65292;&#24341;&#20837;&#20102;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#65292;&#24182;&#35774;&#35745;&#20102;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#65292;&#20197;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#21462;&#24471;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#12289;&#32534;&#31243;&#20195;&#30721;&#21644;&#25968;&#23398;&#31526;&#21495;&#30340;&#24213;&#23618;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#24040;&#22823;&#65292;&#23545;&#20110;&#37027;&#20123;&#21162;&#21147;&#21516;&#26102;&#22312;&#19977;&#20010;&#39046;&#22495;&#23454;&#29616;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25552;&#20986;&#20102;&#22797;&#26434;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#30452;&#25509;&#34701;&#21512;&#24050;&#32463;&#39640;&#24230;&#19987;&#19994;&#21270;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#34701;&#21512;&#26694;&#26550;UltraFuser&#21253;&#25324;&#19977;&#20010;&#24050;&#32463;&#22312;&#35821;&#35328;&#12289;&#32534;&#30721;&#21644;&#25968;&#23398;&#19978;&#24471;&#21040;&#20805;&#20998;&#35757;&#32451;&#30340;&#19987;&#23478;&#12290;&#24341;&#20837;&#20102;&#19968;&#20010;&#26631;&#35760;&#32423;&#21035;&#30340;&#38376;&#25511;&#26426;&#21046;&#26469;&#28151;&#21512;&#19987;&#23478;&#30340;&#36755;&#20986;&#12290;&#35774;&#35745;&#20102;&#19968;&#20010;&#20276;&#38543;&#24179;&#34913;&#37319;&#26679;&#30340;&#20004;&#38454;&#27573;&#35757;&#32451;&#31574;&#30053;&#20197;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#20026;&#20102;&#26377;&#25928;&#35757;&#32451;&#34701;&#21512;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08281v1 Announce Type: cross  Abstract: Underlying data distributions of natural language, programming code, and mathematical symbols vary vastly, presenting a complex challenge for large language models (LLMs) that strive to achieve high performance across all three domains simultaneously. Achieving a very high level of proficiency for an LLM within a specific domain often requires extensive training with relevant corpora, which is typically accompanied by a sacrifice in performance in other domains. In this paper, we propose to fuse models that are already highly-specialized directly. The proposed fusing framework, UltraFuser, consists of three distinct specialists that are already sufficiently trained on language, coding, and mathematics. A token-level gating mechanism is introduced to blend the specialists' outputs. A two-stage training strategy accompanied by balanced sampling is designed to ensure stability. To effectively train the fused model, we further construct a 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U^2-Net&#30340;&#23481;&#22120;&#21160;&#24577;&#28082;&#20301;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;SAM&#27169;&#22411;&#29983;&#25104;&#21021;&#22987;&#25968;&#25454;&#38598;&#24182;&#32467;&#21512;SemiReward&#26694;&#26550;&#36807;&#28388;&#20266;&#26631;&#31614;&#22270;&#20687;&#65292;&#20351;&#29992;U^2-Net&#25552;&#21462;&#25513;&#27169;&#22270;&#20687;&#24182;&#36827;&#34892;&#24418;&#24577;&#23398;&#22788;&#29702;&#65292;&#26368;&#32456;&#36890;&#36807;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#28082;&#20301;&#20998;&#31867;</title><link>https://arxiv.org/abs/2403.08273</link><description>&lt;p&gt;
LiqD&#65306;&#19968;&#31181;&#29992;&#20110;Tricky&#23567;&#23481;&#22120;&#30340;&#21160;&#24577;&#28082;&#20301;&#26816;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LiqD: A Dynamic Liquid Level Detection Model under Tricky Small Containers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08273
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U^2-Net&#30340;&#23481;&#22120;&#21160;&#24577;&#28082;&#20301;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;SAM&#27169;&#22411;&#29983;&#25104;&#21021;&#22987;&#25968;&#25454;&#38598;&#24182;&#32467;&#21512;SemiReward&#26694;&#26550;&#36807;&#28388;&#20266;&#26631;&#31614;&#22270;&#20687;&#65292;&#20351;&#29992;U^2-Net&#25552;&#21462;&#25513;&#27169;&#22270;&#20687;&#24182;&#36827;&#34892;&#24418;&#24577;&#23398;&#22788;&#29702;&#65292;&#26368;&#32456;&#36890;&#36807;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#28082;&#20301;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#21644;&#24037;&#19994;&#29983;&#20135;&#20013;&#65292;&#20934;&#30830;&#26816;&#27979;&#23481;&#22120;&#20013;&#28082;&#20301;&#30340;&#21464;&#21270;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#25509;&#35302;&#24335;&#27979;&#37327;&#26041;&#27861;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#32780;&#26032;&#20852;&#30340;&#38750;&#25509;&#35302;&#24335;&#22270;&#20687;&#22788;&#29702;&#25216;&#26415;&#26174;&#31034;&#20986;&#33391;&#22909;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;U^2-Net&#30340;&#23481;&#22120;&#21160;&#24577;&#28082;&#20301;&#26816;&#27979;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;SAM&#27169;&#22411;&#29983;&#25104;&#21021;&#22987;&#25968;&#25454;&#38598;&#65292;&#28982;&#21518;&#36890;&#36807;SemiReward&#26694;&#26550;&#35780;&#20272;&#21644;&#36807;&#28388;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#22270;&#20687;&#65292;&#26500;&#24314;&#19987;&#23646;&#25968;&#25454;&#38598;&#12290;&#27169;&#22411;&#20351;&#29992;U^2-Net&#20174;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#23481;&#22120;&#30340;&#25513;&#27169;&#22270;&#20687;&#65292;&#24182;&#21033;&#29992;&#24418;&#24577;&#23398;&#22788;&#29702;&#26469;&#34917;&#20607;&#25513;&#27169;&#32570;&#38519;&#12290;&#38543;&#21518;&#65292;&#27169;&#22411;&#35745;&#31639;&#21516;&#19968;&#20301;&#32622;&#30456;&#37051;&#35270;&#39057;&#24103;&#22270;&#20687;&#20043;&#38388;&#30340;&#28784;&#24230;&#24046;&#24322;&#65292;&#36890;&#36807;&#35774;&#32622;&#24046;&#24322;&#38408;&#20540;&#20998;&#21106;&#28082;&#20301;&#21464;&#21270;&#21306;&#22495;&#65292;&#26368;&#32456;&#20351;&#29992;&#36731;&#37327;&#32423;&#31070;&#32463;&#32593;&#32476;&#23545;&#28082;&#20301;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08273v1 Announce Type: cross  Abstract: In daily life and industrial production, it is crucial to accurately detect changes in liquid level in containers. Traditional contact measurement methods have some limitations, while emerging non-contact image processing technology shows good application prospects. This paper proposes a container dynamic liquid level detection model based on U^2-Net. This model uses the SAM model to generate an initial data set, and then evaluates and filters out high-quality pseudo-label images through the SemiReward framework to build an exclusive data set. The model uses U^2-Net to extract mask images of containers from the data set, and uses morphological processing to compensate for mask defects. Subsequently, the model calculates the grayscale difference between adjacent video frame images at the same position, segments the liquid level change area by setting a difference threshold, and finally uses a lightweight neural network to classify the l
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#26410;&#35265;&#33337;&#33334;&#31867;&#21035;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08271</link><description>&lt;p&gt;
&#29992;&#20110;&#32454;&#31890;&#24230;&#33337;&#33334;&#20998;&#31867;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#25928;&#25552;&#31034;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Efficient Prompt Tuning of Large Vision-Language Model for Fine-Grained Ship Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#26410;&#35265;&#33337;&#33334;&#31867;&#21035;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#20013;&#30340;&#32454;&#31890;&#24230;&#33337;&#33334;&#20998;&#31867; (RS-FGSC) &#30001;&#20110;&#31867;&#21035;&#20043;&#38388;&#30340;&#39640;&#30456;&#20284;&#24615;&#20197;&#21450;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#21487;&#29992;&#24615;&#32780;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#38480;&#21046;&#20102;&#20256;&#32479;&#30417;&#30563;&#20998;&#31867;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#26368;&#36817;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411; (VLMs) &#22312;&#23569;&#26679;&#26412;&#25110;&#38646;&#26679;&#26412;&#23398;&#20064;&#20013;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#29702;&#35299;&#22270;&#20687;&#20869;&#23481;&#26041;&#38754;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25366;&#25496;&#20102;VLMs&#30340;&#28508;&#21147;&#65292;&#20197;&#25552;&#39640;&#26410;&#35265;&#33337;&#33334;&#31867;&#21035;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;&#22312;&#30001;&#20110;&#25104;&#26412;&#25110;&#38544;&#31169;&#38480;&#21046;&#32780;&#25968;&#25454;&#21463;&#38480;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#30452;&#25509;&#20026;RS-FGSC&#24494;&#35843;VLMs&#36890;&#24120;&#20250;&#36935;&#21040;&#36807;&#25311;&#21512;&#21487;&#35265;&#31867;&#30340;&#25361;&#25112;&#65292;&#23548;&#33268;&#23545;&#26410;&#35265;&#31867;&#30340;&#27867;&#21270;&#19981;&#20339;&#65292;&#31361;&#20986;&#20102;&#21306;&#20998;&#22797;&#26434;&#32972;&#26223;&#21644;&#25429;&#25417;&#29420;&#29305;&#33337;&#33334;&#29305;&#24449;&#30340;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08271v1 Announce Type: cross  Abstract: Fine-grained ship classification in remote sensing (RS-FGSC) poses a significant challenge due to the high similarity between classes and the limited availability of labeled data, limiting the effectiveness of traditional supervised classification methods. Recent advancements in large pre-trained Vision-Language Models (VLMs) have demonstrated impressive capabilities in few-shot or zero-shot learning, particularly in understanding image content. This study delves into harnessing the potential of VLMs to enhance classification accuracy for unseen ship categories, which holds considerable significance in scenarios with restricted data due to cost or privacy constraints. Directly fine-tuning VLMs for RS-FGSC often encounters the challenge of overfitting the seen classes, resulting in suboptimal generalization to unseen classes, which highlights the difficulty in differentiating complex backgrounds and capturing distinct ship features. To 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#39640;&#36136;&#37327;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#37197;&#32622;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#32570;&#20047;&#21487;&#38752;&#27604;&#36739;&#21644;&#21487;&#37325;&#29616;&#24615;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08265</link><description>&lt;p&gt;
&#38543;&#26426;&#25628;&#32034;&#20316;&#20026;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#25628;&#32034;&#30340;&#22522;&#20934;&#32447;
&lt;/p&gt;
&lt;p&gt;
Random Search as a Baseline for Sparse Neural Network Architecture Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08265
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#22522;&#20110;&#38543;&#26426;&#25628;&#32034;&#30340;&#22522;&#32447;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#39640;&#36136;&#37327;&#30340;&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#37197;&#32622;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#32570;&#20047;&#21487;&#38752;&#27604;&#36739;&#21644;&#21487;&#37325;&#29616;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31232;&#30095;&#31070;&#32463;&#32593;&#32476;&#22312;&#21442;&#25968;&#25928;&#29575;&#26356;&#39640;&#30340;&#24773;&#20917;&#19979;&#23637;&#29616;&#20986;&#19982;&#23494;&#38598;&#32593;&#32476;&#31867;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#36825;&#20419;&#20351;&#35768;&#22810;&#24037;&#20316;&#23398;&#20064;&#12289;&#35825;&#23548;&#25110;&#25628;&#32034;&#24615;&#33021;&#39640;&#30340;&#31232;&#30095;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#36136;&#37327;&#25110;&#25928;&#29575;&#30340;&#25552;&#21319;&#20540;&#24471;&#27880;&#24847;&#65292;&#20294;&#26631;&#20934;&#22522;&#32447;&#32570;&#20047;&#65292;&#22240;&#27492;&#22952;&#30861;&#20102;&#26041;&#27861;&#20043;&#38388;&#30340;&#21487;&#38752;&#27604;&#36739;&#21644;&#21487;&#37325;&#29616;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#35780;&#20272;&#26041;&#27861;&#21644;&#19968;&#20010;&#31616;&#21333;&#30340;&#38543;&#26426;&#25628;&#32034;&#22522;&#32447;&#26041;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#33391;&#22909;&#30340;&#31232;&#30095;&#37197;&#32622;&#12290;&#25105;&#20204;&#22312;&#36807;&#24230;&#21442;&#25968;&#21270;&#32593;&#32476;&#30340;&#33410;&#28857;&#31354;&#38388;&#19978;&#24212;&#29992;&#38543;&#26426;&#25628;&#32034;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#22312;&#25439;&#22833;&#26223;&#35266;&#20013;&#20301;&#32622;&#26356;&#26377;&#20248;&#21183;&#30340;&#26356;&#22909;&#21021;&#22987;&#21270;&#30340;&#31232;&#30095;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#35760;&#24405;&#20102;&#19981;&#21516;&#31232;&#30095;&#31243;&#24230;&#19979;&#31232;&#30095;&#32593;&#32476;&#30340;&#35757;&#32451;&#21518;&#24615;&#33021;&#65292;&#24182;&#19982;&#23427;&#20204;&#30340;&#23436;&#20840;&#36830;&#25509;&#29238;&#32593;&#32476;&#20197;&#21450;&#38543;&#26426;&#31232;&#30095;&#37197;&#32622;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08265v1 Announce Type: cross  Abstract: Sparse neural networks have shown similar or better generalization performance than their dense counterparts while having higher parameter efficiency. This has motivated a number of works to learn, induce, or search for high performing sparse networks. While reports of quality or efficiency gains are impressive, standard baselines are lacking, therefore hindering having reliable comparability and reproducibility across methods. In this work, we provide an evaluation approach and a naive Random Search baseline method for finding good sparse configurations. We apply Random Search on the node space of an overparameterized network with the goal of finding better initialized sparse sub-networks that are positioned more advantageously in the loss landscape. We record sparse network post-training performances at various levels of sparsity and compare against both their fully connected parent networks and random sparse configurations at the sa
&lt;/p&gt;</description></item><item><title>GPT-Onto-CAABAC&#26694;&#26550;&#25972;&#21512;&#20102;GPT&#12289;&#26412;&#20307;&#35770;&#21644;CAABAC&#65292;&#21160;&#24577;&#35299;&#37322;&#31574;&#30053;&#24182;&#25552;&#20379;&#23450;&#21046;&#30340;&#35775;&#38382;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;EHR&#23433;&#20840;&#24615;&#65292;&#36866;&#24212;&#22797;&#26434;&#30340;&#27861;&#35268;&#21644;&#24773;&#22659;&#35201;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.08264</link><description>&lt;p&gt;
GPT&#12289;&#26412;&#20307;&#35770;&#21644;CAABAC&#65306;&#20197;&#21512;&#35268;&#24615;&#12289;&#29615;&#22659;&#21644;&#23646;&#24615;&#20026;&#25903;&#26609;&#30340;&#19977;&#26041;&#20010;&#24615;&#21270;&#35775;&#38382;&#25511;&#21046;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GPT, Ontology, and CAABAC: A Tripartite Personalized Access Control Model Anchored by Compliance, Context and Attribute
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08264
&lt;/p&gt;
&lt;p&gt;
GPT-Onto-CAABAC&#26694;&#26550;&#25972;&#21512;&#20102;GPT&#12289;&#26412;&#20307;&#35770;&#21644;CAABAC&#65292;&#21160;&#24577;&#35299;&#37322;&#31574;&#30053;&#24182;&#25552;&#20379;&#23450;&#21046;&#30340;&#35775;&#38382;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;EHR&#23433;&#20840;&#24615;&#65292;&#36866;&#24212;&#22797;&#26434;&#30340;&#27861;&#35268;&#21644;&#24773;&#22659;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#23383;&#21307;&#30103;&#30340;&#21457;&#23637;&#65292;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#30340;&#23433;&#20840;&#24615;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;GPT-Onto-CAABAC&#26694;&#26550;&#65292;&#23558;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#36716;&#25442;&#22120;&#65288;GPT&#65289;&#12289;&#21307;&#30103;&#27861;&#24459;&#26412;&#20307;&#21644;&#19978;&#19979;&#25991;&#24863;&#30693;&#23646;&#24615;&#35775;&#38382;&#25511;&#21046;&#65288;CAABAC&#65289;&#38598;&#25104;&#65292;&#20197;&#22686;&#24378;EHR&#35775;&#38382;&#23433;&#20840;&#24615;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#65292;GPT-Onto-CAABAC&#21160;&#24577;&#35299;&#37322;&#31574;&#30053;&#65292;&#24182;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#21307;&#30103;&#21644;&#27861;&#24459;&#29615;&#22659;&#65292;&#25552;&#20379;&#23450;&#21046;&#30340;&#35775;&#38382;&#25511;&#21046;&#35299;&#20915;&#26041;&#26696;&#12290;&#36890;&#36807;&#32463;&#39564;&#35780;&#20272;&#65292;&#35813;&#26694;&#26550;&#34987;&#35777;&#26126;&#22312;&#36890;&#36807;&#23558;&#35775;&#38382;&#20915;&#31574;&#19982;&#22797;&#26434;&#30340;&#30417;&#31649;&#21644;&#24773;&#22659;&#35201;&#27714;&#20934;&#30830;&#23545;&#40784;&#32780;&#25552;&#39640;EHR&#23433;&#20840;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26694;&#26550;&#22312;&#38656;&#35201;&#28385;&#36275;&#20005;&#26684;&#21512;&#35268;&#24615;&#21644;&#36866;&#24212;&#24615;&#26631;&#20934;&#30340;&#34892;&#19994;&#20013;&#20855;&#26377;&#26356;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08264v1 Announce Type: cross  Abstract: As digital healthcare evolves, the security of electronic health records (EHR) becomes increasingly crucial. This study presents the GPT-Onto-CAABAC framework, integrating Generative Pretrained Transformer (GPT), medical-legal ontologies and Context-Aware Attribute-Based Access Control (CAABAC) to enhance EHR access security. Unlike traditional models, GPT-Onto-CAABAC dynamically interprets policies and adapts to changing healthcare and legal environments, offering customized access control solutions. Through empirical evaluation, this framework is shown to be effective in improving EHR security by accurately aligning access decisions with complex regulatory and situational requirements. The findings suggest its broader applicability in sectors where access control must meet stringent compliance and adaptability standards.
&lt;/p&gt;</description></item><item><title>CoroNetGAN&#36890;&#36807;&#36229;&#32593;&#32476;&#32467;&#21512;&#21487;&#24494;&#21098;&#26525;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;GAN&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#21487;&#25511;&#30340;&#21387;&#32553;&#20248;&#21183;&#24182;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;</title><link>https://arxiv.org/abs/2403.08261</link><description>&lt;p&gt;
CoroNetGAN&#65306;&#36890;&#36807;&#36229;&#32593;&#32476;&#25511;&#21046;GAN&#30340;&#21098;&#26525;
&lt;/p&gt;
&lt;p&gt;
CoroNetGAN: Controlled Pruning of GANs via Hypernetworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08261
&lt;/p&gt;
&lt;p&gt;
CoroNetGAN&#36890;&#36807;&#36229;&#32593;&#32476;&#32467;&#21512;&#21487;&#24494;&#21098;&#26525;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21387;&#32553;GAN&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#21487;&#25511;&#30340;&#21387;&#32553;&#20248;&#21183;&#24182;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#24050;&#34987;&#35777;&#26126;&#22312;&#35768;&#22810;&#29983;&#25104;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#24615;&#33021;&#65292;&#24182;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#37096;&#32626;GAN&#30340;&#38656;&#27714;&#21069;&#25152;&#26410;&#26377;&#65292;&#36825;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#65292;&#22240;&#20026;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#28041;&#21450;&#20102;&#22823;&#37327;&#21442;&#25968;&#12290;&#36825;&#23548;&#33268;&#20851;&#27880;&#28857;&#38598;&#20013;&#22312;&#21387;&#32553;GAN&#39046;&#22495;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#20316;&#21697;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#65292;&#20294;&#23384;&#22312;&#30528;&#38656;&#35201;&#25945;&#24072;&#20381;&#36182;&#30340;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#27809;&#26377;&#33021;&#21147;&#25511;&#21046;&#21387;&#32553;&#31243;&#24230;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20351;&#29992;&#36229;&#32593;&#32476;&#36890;&#36807;&#21487;&#24494;&#21098;&#26525;&#26041;&#27861;&#21387;&#32553;GAN&#30340;CoroNet-GAN&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#25552;&#20379;&#20102;&#21487;&#25511;&#21046;&#30340;&#21387;&#32553;&#20248;&#21183;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#34920;&#29616;&#20986;&#26174;&#33879;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#30340;&#29305;&#28857;&#12290;&#22312;&#21508;&#31181;&#26465;&#20214;GAN&#26550;&#26500;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08261v1 Announce Type: cross  Abstract: Generative Adversarial Networks (GANs) have proven to exhibit remarkable performance and are widely used across many generative computer vision applications. However, the unprecedented demand for the deployment of GANs on resource-constrained edge devices still poses a challenge due to huge number of parameters involved in the generation process. This has led to focused attention on the area of compressing GANs. Most of the existing works use knowledge distillation with the overhead of teacher dependency. Moreover, there is no ability to control the degree of compression in these methods. Hence, we propose CoroNet-GAN for compressing GAN using the combined strength of differentiable pruning method via hypernetworks. The proposed method provides the advantage of performing controllable compression while training along with reducing training time by a substantial factor. Experiments have been done on various conditional GAN architectures
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#32676;&#20307;&#20869;&#31038;&#20250;&#35268;&#33539;&#20986;&#29616;&#30340;&#29983;&#25104;&#24335;Agent&#26550;&#26500;CRSEC&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08251</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;Agent&#31038;&#20250;&#20013;&#31038;&#20250;&#35268;&#33539;&#30340;&#20986;&#29616;
&lt;/p&gt;
&lt;p&gt;
Emergence of Social Norms in Large Language Model-based Agent Societies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08251
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#32676;&#20307;&#20869;&#31038;&#20250;&#35268;&#33539;&#20986;&#29616;&#30340;&#29983;&#25104;&#24335;Agent&#26550;&#26500;CRSEC&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20250;&#35268;&#33539;&#30340;&#20986;&#29616;&#21560;&#24341;&#20102;&#31038;&#20250;&#31185;&#23398;&#12289;&#35748;&#30693;&#31185;&#23398;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#31561;&#21508;&#20010;&#39046;&#22495;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#36171;&#20104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;Agent&#32676;&#20307;&#20869;&#31038;&#20250;&#35268;&#33539;&#20986;&#29616;&#30340;&#29983;&#25104;&#24335;Agent&#26550;&#26500;CRSEC&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#21253;&#25324;&#22235;&#20010;&#27169;&#22359;&#65306;Creation &amp; Representation&#12289;Spreading&#12289;Evaluation&#21644;Compliance&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#22788;&#29702;&#20102;&#20960;&#20010;&#20851;&#38190;&#26041;&#38754;&#30340;&#32039;&#24613;&#36807;&#31243;&#65306;(i)&#31038;&#20250;&#35268;&#33539;&#30340;&#26469;&#28304;&#65292;(ii)&#23427;&#20204;&#22914;&#20309;&#34987;&#27491;&#24335;&#34920;&#31034;&#65292;(iii)&#23427;&#20204;&#22914;&#20309;&#36890;&#36807;Agent&#30340;&#20132;&#27969;&#21644;&#35266;&#23519;&#20256;&#25773;&#65292;(iv)&#22914;&#20309;&#36890;&#36807;&#21512;&#29702;&#26816;&#26597;&#36827;&#34892;&#26816;&#26597;&#24182;&#22312;&#38271;&#26399;&#20869;&#36827;&#34892;&#32508;&#21512;&#65292;(v)&#22914;&#20309;&#34987;&#32435;&#20837;Agent&#30340;&#35745;&#21010;&#21644;&#34892;&#21160;&#20013;&#12290;&#25105;&#20204;&#22312;Smallville&#27801;&#30418;&#28216;&#25103;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26550;&#26500;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08251v1 Announce Type: cross  Abstract: The emergence of social norms has attracted much interest in a wide array of disciplines, ranging from social science and cognitive science to artificial intelligence. In this paper, we propose the first generative agent architecture that empowers the emergence of social norms within a population of large language model-based agents. Our architecture, named CRSEC, consists of four modules: Creation &amp; Representation, Spreading, Evaluation, and Compliance. Our architecture addresses several important aspects of the emergent processes all in one: (i) where social norms come from, (ii) how they are formally represented, (iii) how they spread through agents' communications and observations, (iv) how they are examined with a sanity check and synthesized in the long term, and (v) how they are incorporated into agents' planning and actions. Our experiments deployed in the Smallville sandbox game environment demonstrate the capability of our ar
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#20223;&#29983;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#23454;&#26102;&#29983;&#25104;&#26080;&#30896;&#25937;&#25588;&#36335;&#24452;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#24555;&#36895;&#21709;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#25552;&#39640;&#20102;&#25937;&#25588;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.08238</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#20223;&#29983;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#23454;&#26102;&#26080;&#30896;&#25937;&#25588;&#30340;&#21019;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Novel Feature Learning-based Bio-inspired Neural Network for Real-time Collision-free Rescue of Multi-Robot Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08238
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#20223;&#29983;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#23454;&#26102;&#29983;&#25104;&#26080;&#30896;&#25937;&#25588;&#36335;&#24452;&#65292;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#24555;&#36895;&#21709;&#24212;&#29615;&#22659;&#21464;&#21270;&#65292;&#25552;&#39640;&#20102;&#25937;&#25588;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#28798;&#23475;&#21644;&#22478;&#24066;&#20107;&#25925;&#25512;&#21160;&#20102;&#23545;&#25937;&#25588;&#26426;&#22120;&#20154;&#30340;&#38656;&#27714;&#65292;&#20197;&#25552;&#20379;&#26356;&#23433;&#20840;&#12289;&#26356;&#24555;&#36895;&#12289;&#26356;&#39640;&#25928;&#30340;&#25937;&#25588;&#36335;&#24452;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29305;&#24449;&#23398;&#20064;&#30340;&#20223;&#29983;&#31070;&#32463;&#32593;&#32476;&#65288;FLBBINN&#65289;&#65292;&#29992;&#20110;&#22312;&#22797;&#26434;&#21644;&#21160;&#24577;&#29615;&#22659;&#20013;&#24555;&#36895;&#29983;&#25104;&#21551;&#21457;&#24335;&#25937;&#25588;&#36335;&#24452;&#65292;&#22240;&#20026;&#20256;&#32479;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#23545;&#31361;&#21457;&#29615;&#22659;&#21464;&#21270;&#23454;&#26102;&#21709;&#24212;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#31070;&#32463;&#21160;&#21147;&#23398;&#27169;&#22411;&#32467;&#21512;&#20102;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#29615;&#22659;&#20449;&#24687;&#25913;&#36827;&#36335;&#24452;&#35268;&#21010;&#31574;&#30053;&#12290;&#36890;&#36807;&#26426;&#22120;&#20154;&#23039;&#21183;&#21644;&#31070;&#32463;&#27963;&#21160;&#30340;&#21160;&#24577;&#26223;&#35266;&#20135;&#29983;&#20219;&#21153;&#20998;&#37197;&#21644;&#26080;&#30896;&#25937;&#25588;&#36712;&#36857;&#12290;&#20351;&#29992;&#21452;&#36890;&#36947;&#23610;&#24230;&#36807;&#28388;&#22120;&#12289;&#31070;&#32463;&#27963;&#21160;&#36890;&#36947;&#21644;&#20108;&#27425;&#36317;&#31163;&#34701;&#21512;&#26469;&#25552;&#21462;&#21644;&#36807;&#28388;&#29305;&#24449;&#31070;&#32463;&#20803;&#12290;&#29305;&#24449;&#23398;&#20064;&#36807;&#31243;&#23436;&#25104;&#21518;&#65292;&#22522;&#20110;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#29305;&#24449;&#30697;&#38453;&#34987;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08238v1 Announce Type: cross  Abstract: Natural disasters and urban accidents drive the demand for rescue robots to provide safer, faster, and more efficient rescue trajectories. In this paper, a feature learning-based bio-inspired neural network (FLBBINN) is proposed to quickly generate a heuristic rescue path in complex and dynamic environments, as traditional approaches usually cannot provide a satisfactory solution to real-time responses to sudden environmental changes. The neurodynamic model is incorporated into the feature learning method that can use environmental information to improve path planning strategies. Task assignment and collision-free rescue trajectory are generated through robot poses and the dynamic landscape of neural activity. A dual-channel scale filter, a neural activity channel, and a secondary distance fusion are employed to extract and filter feature neurons. After completion of the feature learning process, a neurodynamics-based feature matrix is
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#32771;&#34385;&#20102;&#22312;&#26082;&#26377;&#30495;&#23454;&#19987;&#23478;&#21448;&#26377;&#23545;&#25239;&#24615;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#30340;&#20108;&#20803;&#20915;&#31574;&#32858;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35774;&#35745;&#40065;&#26834;&#32858;&#21512;&#22120;&#20197;&#26368;&#23567;&#21270;&#36951;&#25022;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#30495;&#23454;&#19987;&#23478;&#26159;&#23545;&#31216;&#30340;&#19988;&#23545;&#25239;&#24615;&#19987;&#23478;&#19981;&#22826;&#22810;&#26102;&#65292;&#25130;&#23614;&#22343;&#20540;&#26159;&#26368;&#20248;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.08222</link><description>&lt;p&gt;
&#20855;&#26377;&#23545;&#25239;&#24615;&#19987;&#23478;&#30340;&#40065;&#26834;&#20915;&#31574;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
Robust Decision Aggregation with Adversarial Experts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08222
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#32771;&#34385;&#20102;&#22312;&#26082;&#26377;&#30495;&#23454;&#19987;&#23478;&#21448;&#26377;&#23545;&#25239;&#24615;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#30340;&#20108;&#20803;&#20915;&#31574;&#32858;&#21512;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#35774;&#35745;&#40065;&#26834;&#32858;&#21512;&#22120;&#20197;&#26368;&#23567;&#21270;&#36951;&#25022;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#24403;&#30495;&#23454;&#19987;&#23478;&#26159;&#23545;&#31216;&#30340;&#19988;&#23545;&#25239;&#24615;&#19987;&#23478;&#19981;&#22826;&#22810;&#26102;&#65292;&#25130;&#23614;&#22343;&#20540;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#26082;&#26377;&#30495;&#23454;&#19987;&#23478;&#21448;&#26377;&#23545;&#25239;&#24615;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#30340;&#20108;&#20803;&#20915;&#31574;&#32858;&#21512;&#38382;&#39064;&#12290;&#30495;&#23454;&#19987;&#23478;&#23558;&#20250;&#22914;&#23454;&#25253;&#21578;&#20182;&#20204;&#30340;&#31169;&#20154;&#20449;&#21495;&#65292;&#24182;&#33719;&#24471;&#36866;&#24403;&#30340;&#28608;&#21169;&#65292;&#32780;&#23545;&#25239;&#24615;&#19987;&#23478;&#21487;&#20197;&#20219;&#24847;&#25253;&#21578;&#12290;&#20915;&#31574;&#32773;&#38656;&#35201;&#35774;&#35745;&#19968;&#20010;&#40065;&#26834;&#30340;&#32858;&#21512;&#22120;&#65292;&#26681;&#25454;&#19987;&#23478;&#30340;&#25253;&#21578;&#26469;&#39044;&#27979;&#19990;&#30028;&#30340;&#30495;&#23454;&#29366;&#24577;&#12290;&#20915;&#31574;&#32773;&#19981;&#20102;&#35299;&#20855;&#20307;&#30340;&#20449;&#24687;&#32467;&#26500;&#65292;&#21363;&#20449;&#21495;&#12289;&#29366;&#24577;&#20197;&#21450;&#23545;&#25239;&#24615;&#19987;&#23478;&#30340;&#31574;&#30053;&#30340;&#32852;&#21512;&#20998;&#24067;&#12290;&#25105;&#20204;&#24076;&#26395;&#25214;&#21040;&#22312;&#26368;&#22351;&#20449;&#24687;&#32467;&#26500;&#19979;&#26368;&#23567;&#21270;&#36951;&#25022;&#30340;&#26368;&#20248;&#32858;&#21512;&#22120;&#12290;&#36951;&#25022;&#34987;&#23450;&#20041;&#20026;&#32858;&#21512;&#22120;&#21644;&#19968;&#20010;&#22522;&#20934;&#20043;&#38388;&#30340;&#26399;&#26395;&#25439;&#22833;&#24046;&#65292;&#35813;&#22522;&#20934;&#26681;&#25454;&#32852;&#21512;&#20998;&#24067;&#21644;&#30495;&#23454;&#19987;&#23478;&#30340;&#25253;&#21578;&#20570;&#20986;&#26368;&#20248;&#20915;&#31574;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#24403;&#30495;&#23454;&#19987;&#23478;&#26159;&#23545;&#31216;&#30340;&#19988;&#23545;&#25239;&#24615;&#19987;&#23478;&#19981;&#22826;&#22810;&#26102;&#65292;&#25130;&#23614;&#22343;&#20540;&#26159;&#26368;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08222v1 Announce Type: cross  Abstract: We consider a binary decision aggregation problem in the presence of both truthful and adversarial experts. The truthful experts will report their private signals truthfully with proper incentive, while the adversarial experts can report arbitrarily. The decision maker needs to design a robust aggregator to forecast the true state of the world based on the reports of experts. The decision maker does not know the specific information structure, which is a joint distribution of signals, states, and strategies of adversarial experts. We want to find the optimal aggregator minimizing regret under the worst information structure. The regret is defined by the difference in expected loss between the aggregator and a benchmark who makes the optimal decision given the joint distribution and reports of truthful experts.   We prove that when the truthful experts are symmetric and adversarial experts are not too numerous, the truncated mean is opt
&lt;/p&gt;</description></item><item><title>&#23558;&#21452;&#32534;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#33719;&#24471;&#30340;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#21333;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#30340;logit&#33976;&#39311;&#21644;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08215</link><description>&lt;p&gt;
LIX&#65306;&#23558;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
LIX: Implicitly Infusing Spatial Geometric Prior Knowledge into Visual Semantic Segmentation for Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08215
&lt;/p&gt;
&lt;p&gt;
&#23558;&#21452;&#32534;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#33719;&#24471;&#30340;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#21333;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#65292;&#36890;&#36807;&#26032;&#30340;logit&#33976;&#39311;&#21644;&#29305;&#24449;&#33976;&#39311;&#26041;&#27861;&#65292;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#34701;&#21512;&#32593;&#32476;&#22312;&#35270;&#35273;&#35821;&#20041;&#20998;&#21106;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#24403;&#32570;&#20047;&#31354;&#38388;&#20960;&#20309;&#25968;&#25454;&#26102;&#65292;&#21452;&#32534;&#30721;&#22120;&#21464;&#24471;&#26080;&#25928;&#12290;&#23558;&#21452;&#32534;&#30721;&#22120;&#25945;&#24072;&#27169;&#22411;&#33719;&#24471;&#30340;&#31354;&#38388;&#20960;&#20309;&#20808;&#39564;&#30693;&#35782;&#38544;&#24335;&#27880;&#20837;&#21333;&#32534;&#30721;&#22120;&#23398;&#29983;&#27169;&#22411;&#26159;&#19968;&#20010;&#23454;&#29992;&#20294;&#19981;&#22826;&#25506;&#32034;&#30340;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#20010;&#20027;&#39064;&#65292;&#24182;&#37319;&#29992;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Learning to Infuse "X" (LIX) &#26694;&#26550;&#65292;&#22312;logit&#33976;&#39311;&#21644;&#29305;&#24449;&#33976;&#39311;&#26041;&#38754;&#36827;&#34892;&#20102;&#26032;&#39062;&#36129;&#29486;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25968;&#23398;&#35777;&#26126;&#65292;&#24378;&#35843;&#22312;&#35299;&#32806;&#30693;&#35782;&#33976;&#39311;&#20013;&#20351;&#29992;&#21333;&#19968;&#22266;&#23450;&#26435;&#37325;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;logit&#26234;&#33021;&#21160;&#24577;&#26435;&#37325;&#25511;&#21046;&#22120;&#20316;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#37325;&#26032;&#26657;&#20934;&#30340;&#29305;&#24449;&#33976;&#39311;&#31639;&#27861;&#65292;&#21253;&#25324;&#20004;&#31181;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08215v1 Announce Type: cross  Abstract: Despite the impressive performance achieved by data-fusion networks with duplex encoders for visual semantic segmentation, they become ineffective when spatial geometric data are not available. Implicitly infusing the spatial geometric prior knowledge acquired by a duplex-encoder teacher model into a single-encoder student model is a practical, albeit less explored research avenue. This paper delves into this topic and resorts to knowledge distillation approaches to address this problem. We introduce the Learning to Infuse "X" (LIX) framework, with novel contributions in both logit distillation and feature distillation aspects. We present a mathematical proof that underscores the limitation of using a single fixed weight in decoupled knowledge distillation and introduce a logit-wise dynamic weight controller as a solution to this issue. Furthermore, we develop an adaptively-recalibrated feature distillation algorithm, including two tec
&lt;/p&gt;</description></item><item><title>P2LHAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-to-Label Seq2Seq&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#20013;&#21516;&#26102;&#23454;&#29616;&#20154;&#31867;&#27963;&#21160;&#30340;&#20998;&#21106;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;</title><link>https://arxiv.org/abs/2403.08214</link><description>&lt;p&gt;
P2LHAP&#65306;&#22522;&#20110;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#30340;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#12289;&#20998;&#21106;&#21644;&#39044;&#27979;&#30340;Patch-to-Label Seq2Seq Transformer
&lt;/p&gt;
&lt;p&gt;
P2LHAP:Wearable sensor-based human activity recognition, segmentation and forecast through Patch-to-Label Seq2Seq Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08214
&lt;/p&gt;
&lt;p&gt;
P2LHAP&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-to-Label Seq2Seq&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#20013;&#21516;&#26102;&#23454;&#29616;&#20154;&#31867;&#27963;&#21160;&#30340;&#20998;&#21106;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24456;&#38590;&#21516;&#26102;&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#20998;&#21106;&#12289;&#35782;&#21035;&#21644;&#39044;&#27979;&#20154;&#31867;&#27963;&#21160;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#21307;&#30103;&#20445;&#20581;&#21644;&#36741;&#21161;&#29983;&#27963;&#31561;&#39046;&#22495;&#30340;&#23454;&#29992;&#24615;&#65292;&#32780;&#36825;&#20123;&#39046;&#22495;&#23545;&#20110;&#23454;&#26102;&#29702;&#35299;&#27491;&#22312;&#36827;&#34892;&#21644;&#21363;&#23558;&#21457;&#29983;&#30340;&#27963;&#21160;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;P2LHAP&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;Patch-to-Label Seq2Seq&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#19968;&#20010;&#39640;&#25928;&#30340;&#21333;&#19968;&#20219;&#21153;&#27169;&#22411;&#20013;&#35299;&#20915;&#36825;&#19977;&#20010;&#20219;&#21153;&#12290;P2LHAP&#23558;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#21010;&#20998;&#20026;&#19968;&#31995;&#21015;&#8220;&#34917;&#19969;&#8221;&#65292;&#20316;&#20026;&#36755;&#20837;&#26631;&#35760;&#65292;&#24182;&#36755;&#20986;&#19968;&#31995;&#21015;&#21253;&#25324;&#39044;&#27979;&#30340;&#26410;&#26469;&#27963;&#21160;&#22312;&#20869;&#30340;&#34917;&#19969;&#32423;&#27963;&#21160;&#26631;&#31614;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21608;&#22260;&#34917;&#19969;&#26631;&#31614;&#30340;&#29420;&#29305;&#24179;&#28369;&#25216;&#26415;&#65292;&#21487;&#20934;&#30830;&#35782;&#21035;&#27963;&#21160;&#36793;&#30028;&#12290;&#27492;&#22806;&#65292;P2LHAP&#36890;&#36807;&#20256;&#24863;&#22120;&#20449;&#21495;&#36890;&#36947;&#29420;&#31435;&#30340;Transformer&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#23398;&#20064;&#34917;&#19969;&#32423;&#34920;&#31034;&#12290;&#25152;&#26377;&#36890;&#36947;&#22312;&#25152;&#26377;&#24207;&#21015;&#19978;&#20849;&#20139;&#23884;&#20837;&#21644;Transformer&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08214v1 Announce Type: cross  Abstract: Traditional deep learning methods struggle to simultaneously segment, recognize, and forecast human activities from sensor data. This limits their usefulness in many fields such as healthcare and assisted living, where real-time understanding of ongoing and upcoming activities is crucial. This paper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles all three tasks in a efficient single-task model. P2LHAP divides sensor data streams into a sequence of "patches", served as input tokens, and outputs a sequence of patch-level activity labels including the predicted future activities. A unique smoothing technique based on surrounding patch labels, is proposed to identify activity boundaries accurately. Additionally, P2LHAP learns patch-level representation by sensor signal channel-independent Transformer encoders and decoders. All channels share embedding and Transformer weights across all sequences. Evaluated on thre
&lt;/p&gt;</description></item><item><title>&#23545;&#27604;&#25552;&#31034;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#33391;&#65292;&#36824;&#21487;&#20197;&#19982;&#29616;&#26377;&#25552;&#31034;&#26041;&#27861;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08211</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#23545;&#27604;&#25512;&#29702;&#32773;
&lt;/p&gt;
&lt;p&gt;
Large Language Models are Contrastive Reasoners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08211
&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#25552;&#31034;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#19981;&#20165;&#22312;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#33391;&#65292;&#36824;&#21487;&#20197;&#19982;&#29616;&#26377;&#25552;&#31034;&#26041;&#27861;&#25972;&#21512;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#31034;&#26041;&#27861;&#22312;&#22686;&#24378;&#39044;&#35757;&#32451;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#23545;&#27604;&#25552;&#31034;&#65288;CP&#65289;&#22914;&#20309;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25191;&#34892;&#22797;&#26434;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#31616;&#21333;&#22320;&#22312;LLMs&#25552;&#20379;&#31572;&#26696;&#20043;&#21069;&#28155;&#21152;"&#35753;&#25105;&#20204;&#32473;&#20986;&#19968;&#20010;&#27491;&#30830;&#31572;&#26696;&#21644;&#19968;&#20010;&#38169;&#35823;&#31572;&#26696;"&#26469;&#28436;&#31034;LLMs&#26159;&#20307;&#38754;&#30340;&#23545;&#27604;&#25512;&#29702;&#32773;&#12290;&#23545;&#20004;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#38646;&#36801;&#31227;&#23545;&#27604;&#25552;&#31034;&#25552;&#21319;&#20102;&#22312;&#19968;&#31995;&#21015;&#31639;&#26415;&#12289;&#24120;&#35782;&#21644;&#31526;&#21495;&#25512;&#29702;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#19981;&#38656;&#35201;&#25163;&#24037;&#21046;&#20316;&#30340;&#23569;&#37327;&#36801;&#31227;&#31034;&#20363;&#65292;&#27604;&#22914;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;GPT-4&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;&#22312;GSM8K&#19978;&#30340;&#20934;&#30830;&#29575;&#20174;35.9%&#21040;88.8%&#20197;&#21450;AQUA-RAT&#20174;41.3%&#21040;62.2%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#22823;&#22810;&#25968;&#31639;&#26415;&#21644;&#24120;&#35782;&#25512;&#29702;&#20219;&#21153;&#20013;&#32988;&#36807;&#38646;&#36801;&#31227;CoT&#21644;&#23569;&#37327;&#36801;&#31227;CoT&#65292;&#36824;&#21487;&#20197;&#19982;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#26080;&#32541;&#25972;&#21512;&#65292;&#20174;&#32780;&#23454;&#29616;&#25913;&#36827;&#25110;&#32773;&#31454;&#20105;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08211v1 Announce Type: cross  Abstract: Prompting methods play a crucial role in enhancing the capabilities of pre-trained large language models (LLMs). We explore how contrastive prompting (CP) significantly improves the ability of large language models to perform complex reasoning. We demonstrate that LLMs are decent contrastive reasoners by simply adding "Let's give a correct and a wrong answer." before LLMs provide answers. Experiments on two large language models show that zero-shot contrastive prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks without any hand-crafted few-shot examples, such as increasing the accuracy on GSM8K from 35.9% to 88.8% and AQUA-RAT from 41.3% to 62.2% with the state-of-the-art GPT-4 model. Our method not only surpasses zero-shot CoT and few-shot CoT in most arithmetic and commonsense reasoning tasks but also can seamlessly integrate with existing prompting methods, resulting in improved or comp
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;&#65288;DSPNs&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;GPC-ready&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#23376;&#27169;&#20989;&#25968;&#23398;&#20064;&#20013;&#30340;&#20004;&#22823;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.08199</link><description>&lt;p&gt;
&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Submodular Peripteral Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08199
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;&#65288;DSPNs&#65289;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;GPC-ready&#31574;&#30053;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#23376;&#27169;&#20989;&#25968;&#23398;&#20064;&#20013;&#30340;&#20004;&#22823;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#27169;&#20989;&#25968;&#23545;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#36890;&#24120;&#32570;&#20047;&#23454;&#29992;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#33719;&#21462;&#23427;&#20204;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#28145;&#24230;&#23376;&#27169;&#36870;&#28857;&#32593;&#32476;&#65288;DSPNs&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#23376;&#27169;&#20989;&#25968;&#21442;&#25968;&#21270;&#26063;&#65292;&#24182;&#25552;&#20986;&#20102;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#21551;&#21457;&#30340;GPC-ready&#31574;&#30053;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#36830;&#25509;&#24182;&#35299;&#20915;&#19978;&#36848;&#20004;&#20010;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08199v1 Announce Type: cross  Abstract: Submodular functions, crucial for various applications, often lack practical learning methods for their acquisition. Seemingly unrelated, learning a scaling from oracles offering graded pairwise preferences (GPC) is underexplored, despite a rich history in psychometrics. In this paper, we introduce deep submodular peripteral networks (DSPNs), a novel parametric family of submodular functions, and methods for their training using a contrastive-learning inspired GPC-ready strategy to connect and then tackle both of the above challenges. We introduce newly devised GPC-style "peripteral" loss which leverages numerically graded relationships between pairs of objects (sets in our case). Unlike traditional contrastive learning, our method utilizes graded comparisons, extracting more nuanced information than just binary-outcome comparisons, and contrasts sets of any size (not just two). We also define a novel suite of automatic sampling strate
&lt;/p&gt;</description></item><item><title>PAGE&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26234;&#33021;&#21307;&#30103;&#30340;&#39046;&#22495;&#22686;&#37327;&#36866;&#24212;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#20110;&#20445;&#30041;&#25968;&#25454;&#25110;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#29983;&#25104;&#22238;&#25918;&#65292;&#26377;&#25928;&#24179;&#34913;&#39046;&#22495;&#36866;&#24212;&#21644;&#30693;&#35782;&#20445;&#30041;&#65292;&#24182;&#32467;&#21512;&#25193;&#23637;&#30340;&#24402;&#32435;&#30830;&#35748;&#39044;&#27979;&#26041;&#27861;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#30142;&#30149;&#26816;&#27979;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.08197</link><description>&lt;p&gt;
PAGE: &#20855;&#26377;&#38754;&#21521;&#36807;&#21435;&#26080;&#20851;&#29983;&#25104;&#22238;&#25918;&#30340;&#39046;&#22495;&#22686;&#37327;&#36866;&#24212;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
PAGE: Domain-Incremental Adaptation with Past-Agnostic Generative Replay for Smart Healthcare
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08197
&lt;/p&gt;
&lt;p&gt;
PAGE&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#26234;&#33021;&#21307;&#30103;&#30340;&#39046;&#22495;&#22686;&#37327;&#36866;&#24212;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#19981;&#20381;&#36182;&#20110;&#20445;&#30041;&#25968;&#25454;&#25110;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#29983;&#25104;&#22238;&#25918;&#65292;&#26377;&#25928;&#24179;&#34913;&#39046;&#22495;&#36866;&#24212;&#21644;&#30693;&#35782;&#20445;&#30041;&#65292;&#24182;&#32467;&#21512;&#25193;&#23637;&#30340;&#24402;&#32435;&#30830;&#35748;&#39044;&#27979;&#26041;&#27861;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#30142;&#30149;&#26816;&#27979;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;PAGE&#65292;&#19968;&#31181;&#20855;&#26377;&#38754;&#21521;&#36807;&#21435;&#26080;&#20851;&#29983;&#25104;&#22238;&#25918;&#30340;&#39046;&#22495;&#22686;&#37327;&#36866;&#24212;&#31574;&#30053;&#65292;&#29992;&#20110;&#26234;&#33021;&#21307;&#30103;&#12290;PAGE&#33021;&#22815;&#23454;&#29616;&#29983;&#25104;&#22238;&#25918;&#65292;&#32780;&#26080;&#38656;&#20445;&#30041;&#26469;&#33258;&#20808;&#21069;&#39046;&#22495;&#30340;&#20219;&#20309;&#25968;&#25454;&#25110;&#20449;&#24687;&#12290;&#22312;&#36866;&#24212;&#26032;&#39046;&#22495;&#26102;&#65292;&#23427;&#21033;&#29992;&#26469;&#33258;&#26032;&#20998;&#24067;&#21644;&#24403;&#21069;&#27169;&#22411;&#30340;&#30495;&#23454;&#25968;&#25454;&#26469;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#20445;&#30041;&#20808;&#21069;&#39046;&#22495;&#30340;&#23398;&#20064;&#30693;&#35782;&#12290;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#37325;&#26032;&#25773;&#25918;&#21512;&#25104;&#25968;&#25454;&#21644;&#26032;&#23454;&#38469;&#25968;&#25454;&#65292;PAGE&#22312;&#39046;&#22495;&#36866;&#24212;&#21644;&#30693;&#35782;&#20445;&#30041;&#20043;&#38388;&#21462;&#24471;&#33391;&#22909;&#24179;&#34913;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23558;&#25193;&#23637;&#30340;&#24402;&#32435;&#30830;&#35748;&#39044;&#27979;&#65288;EICP&#65289;&#26041;&#27861;&#25972;&#21512;&#21040;PAGE&#20013;&#65292;&#20026;&#27599;&#20010;&#26816;&#27979;&#32467;&#26524;&#29983;&#25104;&#32622;&#20449;&#24230;&#20998;&#25968;&#21644;&#21487;&#20449;&#24230;&#20540;&#12290;&#36825;&#20351;&#24471;&#39044;&#27979;&#32467;&#26524;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#24182;&#20026;&#26234;&#33021;&#21307;&#30103;&#24212;&#29992;&#20013;&#30340;&#30142;&#30149;&#26816;&#27979;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;PAGE&#22312;&#39046;&#22495;&#22686;&#37327;&#30142;&#30149;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08197v1 Announce Type: cross  Abstract: We propose PAGE, a domain-incremental adaptation strategy with past-agnostic generative replay for smart healthcare. PAGE enables generative replay without the aid of any preserved data or information from prior domains. When adapting to a new domain, it exploits real data from the new distribution and the current model to generate synthetic data that retain the learned knowledge of previous domains. By replaying the synthetic data with the new real data during training, PAGE achieves a good balance between domain adaptation and knowledge retention. In addition, we incorporate an extended inductive conformal prediction (EICP) method into PAGE to produce a confidence score and a credibility value for each detection result. This makes the predictions interpretable and provides statistical guarantees for disease detection in smart healthcare applications. We demonstrate PAGE's effectiveness in domain-incremental disease detection with thr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19987;&#20026;FEVER&#20219;&#21153;&#23450;&#21046;&#30340;&#20004;&#31181;&#20219;&#21153;&#29305;&#23450;&#30446;&#26631;&#20989;&#25968;&#65292;&#30456;&#27604;&#26631;&#20934;&#20132;&#21449;&#29109;&#22312;&#20107;&#23454;&#39564;&#35777;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#31867;&#21035;&#21152;&#26435;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.08174</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20107;&#23454;&#39564;&#35777;&#30340;&#25439;&#22833;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Rethinking Loss Functions for Fact Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08174
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19987;&#20026;FEVER&#20219;&#21153;&#23450;&#21046;&#30340;&#20004;&#31181;&#20219;&#21153;&#29305;&#23450;&#30446;&#26631;&#20989;&#25968;&#65292;&#30456;&#27604;&#26631;&#20934;&#20132;&#21449;&#29109;&#22312;&#20107;&#23454;&#39564;&#35777;&#20013;&#34920;&#29616;&#26356;&#22909;&#65292;&#24182;&#36890;&#36807;&#31616;&#21333;&#31867;&#21035;&#21152;&#26435;&#36827;&#19968;&#27493;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;FEVER&#20849;&#20139;&#20219;&#21153;&#20013;&#20107;&#23454;&#39564;&#35777;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;&#34429;&#28982;&#20132;&#21449;&#29109;&#25439;&#22833;&#26159;&#35757;&#32451;&#21028;&#20915;&#39044;&#27979;&#22120;&#30340;&#26631;&#20934;&#30446;&#26631;&#65292;&#20294;&#23427;&#26410;&#33021;&#25429;&#25417;&#21040;FEVER&#21028;&#20915;&#31867;&#21035;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#38024;&#23545;FEVER&#30340;&#29305;&#23450;&#20219;&#21153;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#65292;&#25152;&#25552;&#20986;&#30340;&#30446;&#26631;&#20989;&#25968;&#20248;&#20110;&#26631;&#20934;&#30340;&#20132;&#21449;&#29109;&#12290;&#24403;&#36825;&#20123;&#30446;&#26631;&#19982;&#31616;&#21333;&#30340;&#31867;&#21035;&#21152;&#26435;&#30456;&#32467;&#21512;&#26102;&#65292;&#24615;&#33021;&#36827;&#19968;&#27493;&#25552;&#39640;&#65292;&#26377;&#25928;&#22320;&#20811;&#26381;&#20102;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#19981;&#24179;&#34913;&#12290;&#28304;&#20195;&#30721;&#21487;&#22312;https://github.com/yuta-mukobara/RLF-KGAT &#19978;&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08174v1 Announce Type: cross  Abstract: We explore loss functions for fact verification in the FEVER shared task. While the cross-entropy loss is a standard objective for training verdict predictors, it fails to capture the heterogeneity among the FEVER verdict classes. In this paper, we develop two task-specific objectives tailored to FEVER. Experimental results confirm that the proposed objective functions outperform the standard cross-entropy. Performance is further improved when these objectives are combined with simple class weighting, which effectively overcomes the imbalance in the training data. The souce code is available at https://github.com/yuta-mukobara/RLF-KGAT
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#26631;&#30340;&#20154;&#33080;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38754;&#37096;&#22320;&#26631;&#23450;&#20301;&#30340;&#34917;&#19969;&#26469;&#23398;&#20064;&#26356;&#36866;&#21512;&#20110;&#20154;&#33080;&#35782;&#21035;&#30340;&#20851;&#38190;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.08161</link><description>&lt;p&gt;
&#22522;&#20110;&#22320;&#26631;&#30340;&#20154;&#33080;&#33258;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#20154;&#33080;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
LAFS: Landmark-based Facial Self-supervised Learning for Face Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22320;&#26631;&#30340;&#20154;&#33080;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38754;&#37096;&#22320;&#26631;&#23450;&#20301;&#30340;&#34917;&#19969;&#26469;&#23398;&#20064;&#26356;&#36866;&#21512;&#20110;&#20154;&#33080;&#35782;&#21035;&#30340;&#20851;&#38190;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#23398;&#20064;&#21487;&#20197;&#36866;&#24212;&#35757;&#32451;&#26377;&#25928;&#20154;&#33080;&#35782;&#21035;&#27169;&#22411;&#30340;&#20154;&#33080;&#34920;&#31034;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#20047;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#12290;&#19982;&#29616;&#26377;&#30340;&#24102;&#26631;&#31614;&#20154;&#33080;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#23384;&#22312;&#30528;&#19968;&#20010;&#25968;&#37327;&#19978;&#22823;&#24471;&#22810;&#30340;&#26410;&#26631;&#35760;&#20154;&#33080;&#12290;&#25105;&#20204;&#36890;&#36807;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25506;&#32034;&#36825;&#20123;&#26410;&#26631;&#35760;&#38754;&#37096;&#22270;&#20687;&#30340;&#23398;&#20064;&#31574;&#30053;&#65292;&#20197;&#36716;&#31227;&#24191;&#20041;&#20154;&#33080;&#35782;&#21035;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;&#26368;&#36817;&#30340;&#19968;&#20010;&#21457;&#29616;&#30340;&#21551;&#21457;&#65292;&#21363;&#38754;&#37096;&#26174;&#33879;&#21306;&#22495;&#23545;&#20110;&#20154;&#33080;&#35782;&#21035;&#33267;&#20851;&#37325;&#35201;&#65292;&#19982;&#21033;&#29992;&#38543;&#26426;&#35009;&#21098;&#30340;&#22270;&#20687;&#22359;&#36827;&#34892;&#39044;&#35757;&#32451;&#22686;&#24191;&#30456;&#21453;&#65292;&#25105;&#20204;&#21033;&#29992;&#36890;&#36807;&#25552;&#21462;&#30340;&#38754;&#37096;&#22320;&#26631;&#23450;&#20301;&#30340;&#34917;&#19969;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861; - &#21363;&#22522;&#20110;&#22320;&#26631;&#30340;&#20154;&#33080;&#33258;&#30417;&#30563;&#23398;&#20064;(LAndmark-based Facial Self-supervised learning LAFS)&#33021;&#22815;&#23398;&#20064;&#26356;&#36866;&#21512;&#20154;&#33080;&#35782;&#21035;&#30340;&#20851;&#38190;&#34920;&#31034;&#12290;&#25105;&#20204;&#36824;&#34701;&#20837;&#20102;&#20004;&#31181;&#29305;&#23450;&#20110;&#22320;&#26631;&#30340;&#22686;&#24191;&#65292;&#24341;&#20837;&#20102;&#26356;&#22810;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08161v1 Announce Type: cross  Abstract: In this work we focus on learning facial representations that can be adapted to train effective face recognition models, particularly in the absence of labels. Firstly, compared with existing labelled face datasets, a vastly larger magnitude of unlabeled faces exists in the real world. We explore the learning strategy of these unlabeled facial images through self-supervised pretraining to transfer generalized face recognition performance. Moreover, motivated by one recent finding, that is, the face saliency area is critical for face recognition, in contrast to utilizing random cropped blocks of images for constructing augmentations in pretraining, we utilize patches localized by extracted facial landmarks. This enables our method - namely LAndmark-based Facial Self-supervised learning LAFS), to learn key representation that is more critical for face recognition. We also incorporate two landmark-specific augmentations which introduce mo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#24191;&#20041;Needle&#38382;&#39064;&#19978;&#30740;&#31350;&#20102;&#38543;&#26426;&#23616;&#37096;&#25628;&#32034;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30830;&#23450;&#21442;&#25968;$k$&#24433;&#21709;&#30340;&#19979;&#38480;&#26041;&#27861;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#39044;&#26399;&#36816;&#34892;&#26102;&#38388;&#19978;&#38480;&#12290;</title><link>https://arxiv.org/abs/2403.08153</link><description>&lt;p&gt;
&#22312;&#24191;&#20041;Needle&#38382;&#39064;&#19978;&#30340;&#38543;&#26426;&#23616;&#37096;&#25628;&#32034;&#36816;&#34892;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;
The Runtime of Random Local Search on the Generalized Needle Problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08153
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#24191;&#20041;Needle&#38382;&#39064;&#19978;&#30740;&#31350;&#20102;&#38543;&#26426;&#23616;&#37096;&#25628;&#32034;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#30830;&#23450;&#21442;&#25968;$k$&#24433;&#21709;&#30340;&#19979;&#38480;&#26041;&#27861;&#65292;&#24182;&#25913;&#36827;&#20102;&#29616;&#26377;&#30340;&#39044;&#26399;&#36816;&#34892;&#26102;&#38388;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#24037;&#20316;&#20013;&#65292;C. Doerr&#21644;Krejca&#65288;2023&#24180;&#12298;&#36827;&#21270;&#35745;&#31639;&#20132;&#26131;&#12299;&#65289;&#35777;&#26126;&#20102;&#38543;&#26426;&#23616;&#37096;&#25628;&#32034;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#24191;&#20041;Needle&#20989;&#25968;&#19978;&#30340;&#39044;&#26399;&#36816;&#34892;&#26102;&#38388;&#30340;&#19978;&#38480;&#12290;&#22522;&#20110;&#36825;&#20123;&#19978;&#38480;&#65292;&#20182;&#20204;&#20197;&#19968;&#31181;&#19981;&#23436;&#20840;&#20005;&#35880;&#30340;&#26041;&#24335;&#25512;&#26029;&#20102;&#38024;&#21322;&#24452;$k$&#23545;&#36816;&#34892;&#26102;&#38388;&#30340;&#24040;&#22823;&#24433;&#21709;&#12290;&#22312;&#36825;&#31687;&#31616;&#30701;&#30340;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#28155;&#21152;&#20102;&#30830;&#23450;&#21442;&#25968;$k$&#23545;&#36816;&#34892;&#26102;&#38388;&#24433;&#21709;&#25152;&#38656;&#30340;&#32570;&#22833;&#19979;&#38480;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#20010;&#23545;&#39044;&#26399;&#36816;&#34892;&#26102;&#38388;&#30340;&#31934;&#30830;&#25551;&#36848;&#65292;&#36825;&#20063;&#26174;&#33879;&#25913;&#36827;&#20102;C. Doerr&#21644;Krejca&#32473;&#20986;&#30340;&#19978;&#38480;&#12290;&#25105;&#20204;&#36824;&#25551;&#36848;&#20102;&#39044;&#26399;&#36816;&#34892;&#26102;&#38388;&#30340;&#28176;&#36817;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08153v1 Announce Type: cross  Abstract: In their recent work, C. Doerr and Krejca (Transactions on Evolutionary Computation, 2023) proved upper bounds on the expected runtime of the randomized local search heuristic on generalized Needle functions. Based on these upper bounds, they deduce in a not fully rigorous manner a drastic influence of the needle radius $k$ on the runtime.   In this short article, we add the missing lower bound necessary to determine the influence of parameter $k$ on the runtime. To this aim, we derive an exact description of the expected runtime, which also significantly improves the upper bound given by C. Doerr and Krejca. We also describe asymptotic estimates of the expected runtime.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#19981;&#26029;&#22686;&#38271;&#30340;&#33021;&#32791;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#32593;&#32476;&#23610;&#23544;&#12289;&#35745;&#31639;&#21644;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#30340;&#33021;&#32791;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.08151</link><description>&lt;p&gt;
&#34913;&#37327;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#33021;&#32791;&#21644;&#25928;&#29575;&#65306;&#23454;&#35777;&#20998;&#26512;&#19982;&#35774;&#35745;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
Measuring the Energy Consumption and Efficiency of Deep Neural Networks: An Empirical Analysis and Design Recommendations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08151
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#38024;&#23545;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#19981;&#26029;&#22686;&#38271;&#30340;&#33021;&#32791;&#38382;&#39064;&#36827;&#34892;&#20102;&#23454;&#35777;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#32771;&#34385;&#32593;&#32476;&#23610;&#23544;&#12289;&#35745;&#31639;&#21644;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#30340;&#33021;&#32791;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#35268;&#27169;&#31070;&#32463;&#32593;&#32476;&#26085;&#30410;&#22686;&#38271;&#30340;&#33021;&#32791;&#38382;&#39064;&#65288;&#25152;&#35859;&#30340;&#8220;&#32418;&#33394;AI&#8221;&#36235;&#21183;&#65289;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#33410;&#28857;&#32423;&#29926;&#29305;&#34920;&#27979;&#37327;&#20102;&#35757;&#32451;&#21508;&#31181;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#23454;&#38469;&#33021;&#32791;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;BUTTER-E&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;BUTTER&#23454;&#35777;&#28145;&#24230;&#23398;&#20064;&#25968;&#25454;&#38598;&#30340;&#19968;&#20010;&#25193;&#20805;&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;63,527&#20010;&#21333;&#29420;&#23454;&#39564;&#36816;&#34892;&#30340;&#33021;&#32791;&#21644;&#24615;&#33021;&#25968;&#25454;&#65292;&#28085;&#30422;&#20102;30,582&#20010;&#19981;&#21516;&#30340;&#37197;&#32622;&#65306;13&#20010;&#25968;&#25454;&#38598;&#12289;20&#20010;&#22823;&#23567;&#65288;&#21487;&#35757;&#32451;&#21442;&#25968;&#25968;&#37327;&#65289;&#12289;8&#20010;&#32593;&#32476;&#8220;&#24418;&#29366;&#8221;&#21644;14&#20010;&#28145;&#24230;&#65292;&#20197;&#21450;&#22312;CPU&#21644;GPU&#30828;&#20214;&#19978;&#20351;&#29992;&#33410;&#28857;&#32423;&#29926;&#29305;&#34920;&#25910;&#38598;&#30340;&#25968;&#25454;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#25581;&#31034;&#20102;&#25968;&#25454;&#38598;&#22823;&#23567;&#12289;&#32593;&#32476;&#32467;&#26500;&#21644;&#33021;&#32791;&#20043;&#38388;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#24182;&#31361;&#20986;&#20102;&#32531;&#23384;&#25928;&#24212;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33021;&#32791;&#27169;&#22411;&#65292;&#32771;&#34385;&#20102;&#32593;&#32476;&#22823;&#23567;&#12289;&#35745;&#31639;&#21644;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#25581;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08151v1 Announce Type: cross  Abstract: Addressing the so-called ``Red-AI'' trend of rising energy consumption by large-scale neural networks, this study investigates the actual energy consumption, as measured by node-level watt-meters, of training various fully connected neural network architectures. We introduce the BUTTER-E dataset, an augmentation to the BUTTER Empirical Deep Learning dataset, containing energy consumption and performance data from 63,527 individual experimental runs spanning 30,582 distinct configurations: 13 datasets, 20 sizes (number of trainable parameters), 8 network ``shapes'', and 14 depths on both CPU and GPU hardware collected using node-level watt-meters. This dataset reveals the complex relationship between dataset size, network structure, and energy use, and highlights the impact of cache effects. We propose a straightforward and effective energy model that accounts for network size, computing, and memory hierarchy. Our analysis also uncovers
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26500;&#24314;&#31995;&#32479;&#65292;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#21019;&#24314;&#35774;&#35745;&#21345;&#29255;&#65292;&#24110;&#21161;&#35774;&#35745;&#24072;&#26356;&#22909;&#29702;&#35299;&#35774;&#35745;&#21551;&#31034;&#24182;&#25552;&#39640;&#27807;&#36890;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.08137</link><description>&lt;p&gt;
&#20174;&#35770;&#25991;&#21040;&#21345;&#29255;&#65306;&#21033;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#25913;&#21464;&#35774;&#35745;&#21551;&#31034;
&lt;/p&gt;
&lt;p&gt;
From Paper to Card: Transforming Design Implications with Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08137
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#26500;&#24314;&#31995;&#32479;&#65292;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#21019;&#24314;&#35774;&#35745;&#21345;&#29255;&#65292;&#24110;&#21161;&#35774;&#35745;&#24072;&#26356;&#22909;&#29702;&#35299;&#35774;&#35745;&#21551;&#31034;&#24182;&#25552;&#39640;&#27807;&#36890;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#27969;&#35774;&#35745;&#21551;&#31034;&#22312;&#20154;&#26426;&#20132;&#20114;&#39046;&#22495;&#20013;&#21457;&#34920;&#23398;&#26415;&#35770;&#25991;&#26102;&#24456;&#24120;&#35265;&#65292;&#28982;&#32780;&#36825;&#20123;&#35770;&#25991;&#24456;&#23569;&#34987;&#35774;&#35745;&#24072;&#20204;&#38405;&#35835;&#21644;&#20351;&#29992;&#12290;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#35774;&#35745;&#21345;&#29255;&#20316;&#20026;&#19968;&#31181;&#32763;&#35793;&#36164;&#28304;&#65292;&#20197;&#26356;&#26131;&#28040;&#21270;&#21644;&#33719;&#21462;&#30340;&#26041;&#24335;&#20256;&#36798;&#35770;&#25991;&#20013;&#30340;&#26377;&#20215;&#20540;&#35265;&#35299;&#65292;&#20197;&#21327;&#21161;&#35774;&#35745;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#35774;&#35745;&#21345;&#29255;&#21487;&#33021;&#32791;&#26102;&#65292;&#32780;&#20316;&#32773;&#21487;&#33021;&#32570;&#20047;&#21046;&#20316;&#21345;&#29255;&#30340;&#36164;&#28304;&#21644;&#30693;&#35782;&#12290;&#36890;&#36807;&#36845;&#20195;&#35774;&#35745;&#36807;&#31243;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#20351;&#29992;LLM&#21644;&#25991;&#26412;&#36716;&#22270;&#20687;&#27169;&#22411;&#20174;&#23398;&#26415;&#35770;&#25991;&#20013;&#24110;&#21161;&#21019;&#24314;&#35774;&#35745;&#21345;&#29255;&#12290;&#25105;&#20204;&#23545;&#35774;&#35745;&#24072;&#65288;N=21&#65289;&#21644;&#25152;&#36873;&#35770;&#25991;&#30340;&#20316;&#32773;&#65288;N=12&#65289;&#36827;&#34892;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;&#35774;&#35745;&#24072;&#35748;&#20026;&#25105;&#20204;&#35774;&#35745;&#21345;&#29255;&#20013;&#30340;&#35774;&#35745;&#21551;&#31034;&#27604;&#38405;&#35835;&#21407;&#22987;&#35770;&#25991;&#25991;&#26412;&#26356;&#20855;&#21551;&#21457;&#24615;&#21644;&#29983;&#25104;&#24615;&#65292;&#32780;&#20316;&#32773;&#21017;&#35748;&#20026;&#25105;&#20204;&#30340;&#31995;&#32479;&#26159;&#20256;&#36798;&#20182;&#20204;&#35774;&#35745;&#21551;&#31034;&#30340;&#26377;&#25928;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08137v1 Announce Type: cross  Abstract: Communicating design implications is common within the HCI community when publishing academic papers, yet these papers are rarely read and used by designers. One solution is to use design cards as a form of translational resource that communicates valuable insights from papers in a more digestible and accessible format to assist in design processes. However, creating design cards can be time-consuming, and authors may lack the resources/know-how to produce cards. Through an iterative design process, we built a system that helps create design cards from academic papers using an LLM and text-to-image model. Our evaluation with designers (N=21) and authors of selected papers (N=12) revealed that designers perceived the design implications from our design cards as more inspiring and generative, compared to reading original paper texts, and the authors viewed our system as an effective way of communicating their design implications. We also
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RoboCertProb&#65292;&#29992;&#20110;&#25351;&#23450;&#22312;RoboChart&#20013;&#24314;&#27169;&#30340;&#27010;&#29575;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#23450;&#37327;&#23646;&#24615;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;PCTL*&#30340;&#35821;&#20041;&#21644;&#39532;&#23572;&#21487;&#22827;&#35821;&#20041;&#65288;DTMCs&#21644;MDPs&#65289;&#65292;&#25903;&#25345;&#23545;&#36825;&#20123;&#23646;&#24615;&#36827;&#34892;&#35299;&#37322;&#21644;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.08136</link><description>&lt;p&gt;
RoboCertProb&#65306;&#27010;&#29575;RoboChart&#27169;&#22411;&#30340;&#23646;&#24615;&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
RoboCertProb: Property Specification for Probabilistic RoboChart Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RoboCertProb&#65292;&#29992;&#20110;&#25351;&#23450;&#22312;RoboChart&#20013;&#24314;&#27169;&#30340;&#27010;&#29575;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#23450;&#37327;&#23646;&#24615;&#65292;&#24182;&#36890;&#36807;&#22522;&#20110;PCTL*&#30340;&#35821;&#20041;&#21644;&#39532;&#23572;&#21487;&#22827;&#35821;&#20041;&#65288;DTMCs&#21644;MDPs&#65289;&#65292;&#25903;&#25345;&#23545;&#36825;&#20123;&#23646;&#24615;&#36827;&#34892;&#35299;&#37322;&#21644;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
RoboChart&#26159;RoboStar&#26694;&#26550;&#20013;&#30340;&#26680;&#24515;&#31526;&#21495;&#65292;&#23558;&#29616;&#20195;&#24314;&#27169;&#21644;&#24418;&#24335;&#21270;&#39564;&#35777;&#25216;&#26415;&#24341;&#20837;&#36719;&#20214;&#24037;&#31243;&#39046;&#22495;&#30340;&#26426;&#22120;&#20154;&#25216;&#26415;&#20013;&#12290;&#23427;&#26159;&#19968;&#31181;&#38754;&#21521;&#26426;&#22120;&#20154;&#30340;&#23450;&#26102;&#21644;&#27010;&#29575;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#65292;&#25552;&#20379;&#31867;&#20284;UML&#30340;&#26550;&#26500;&#21644;&#29366;&#24577;&#26426;&#24314;&#27169;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;RoboCertProb&#65292;&#29992;&#20110;&#25351;&#23450;&#22312;RoboChart&#20013;&#24314;&#27169;&#30340;&#27010;&#29575;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#23450;&#37327;&#23646;&#24615;&#12290;RoboCertProb&#30340;&#35821;&#20041;&#22522;&#20110;PCTL*&#12290;&#20026;&#20102;&#22312;RoboChart&#27169;&#22411;&#19978;&#35299;&#37322;RoboCertProb&#65292;&#25105;&#20204;&#20026;RoboChart&#25552;&#20379;&#20102;&#19968;&#20010;&#39532;&#23572;&#21487;&#22827;&#35821;&#20041;&#65288;DTMC&#21644;MDPs&#65289;&#65292;&#35813;&#35821;&#20041;&#26469;&#33258;&#20110;RoboChart&#21040;PRISM&#35821;&#35328;&#30340;&#29616;&#26377;&#36716;&#25442;&#35821;&#20041;&#12290;&#38500;&#20102;&#23646;&#24615;&#35268;&#33539;&#65292;RoboCertProb&#36824;&#20801;&#35768;&#25105;&#20204;&#22312;RoboChart&#27169;&#22411;&#20013;&#37197;&#32622;&#26494;&#25955;&#24120;&#37327;&#21644;&#26410;&#25351;&#23450;&#30340;&#20989;&#25968;&#21644;&#25805;&#20316;&#12290;&#23427;&#20351;&#25105;&#20204;&#33021;&#22815;&#35774;&#32622;&#29615;&#22659;&#36755;&#20837;&#20197;&#39564;&#35777;&#22312;&#27010;&#29575;&#27169;&#22411;&#26816;&#26597;&#22120;&#20013;&#19981;&#30452;&#25509;&#25903;&#25345;&#30340;&#21453;&#24212;&#24335;&#27010;&#29575;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08136v1 Announce Type: cross  Abstract: RoboChart is a core notation in the RoboStar framework which brings modern modelling and formal verification technologies into software engineering for robotics. It is a timed and probabilistic domain-specific language for robotics and provides a UML-like architectural and state machine modelling. This work presents RoboCertProb for specifying quantitative properties of probabilistic robotic systems modelled in RoboChart. RoboCertProb's semantics is based on PCTL*. To interpret RoboCertProb over RoboChart models, we give a Markov semantics (DTMCs and MDPs) to RoboChart, derived from its existing transformation semantics to the PRISM language. In addition to property specification, RoboCertProb also entitles us to configure loose constants and unspecified functions and operations in RoboChart models. It allows us to set up environmental inputs to verify reactive probabilistic systems not directly supported in probabilistic model checker
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;CSI&#19978;&#37319;&#26679;&#26694;&#26550;&#65292;&#21033;&#29992;&#29289;&#29702;&#21407;&#29702;&#21644;&#23398;&#20064;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;MIMO FDD&#31995;&#32479;&#20013;&#30001;&#27424;&#37319;&#26679;&#23548;&#33268;&#30340;&#28151;&#21472;&#25928;&#24212;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.08133</link><description>&lt;p&gt;
&#29289;&#29702;&#21551;&#21457;&#30340;&#28145;&#24230;&#23398;&#20064;&#25239;&#28151;&#21472;&#26694;&#26550;&#22312;&#39640;&#25928;&#20449;&#36947;&#29366;&#24577;&#21453;&#39304;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Physics-Inspired Deep Learning Anti-Aliasing Framework in Efficient Channel State Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;CSI&#19978;&#37319;&#26679;&#26694;&#26550;&#65292;&#21033;&#29992;&#29289;&#29702;&#21407;&#29702;&#21644;&#23398;&#20064;&#26041;&#27861;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#22312;&#22823;&#35268;&#27169;MIMO FDD&#31995;&#32479;&#20013;&#30001;&#27424;&#37319;&#26679;&#23548;&#33268;&#30340;&#28151;&#21472;&#25928;&#24212;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#22522;&#31449;&#22788;&#30340;&#19979;&#34892;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#23545;&#20110;&#20248;&#21270;&#22823;&#35268;&#27169;MIMO FDD&#31995;&#32479;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#28145;&#24230;&#23398;&#20064;&#26550;&#26500;&#25104;&#21151;&#22320;&#20419;&#36827;&#20102;UE&#31471;&#30340;CSI&#21453;&#39304;&#21644;gNB&#31471;&#30340;&#24674;&#22797;&#65292;&#20294;&#22312;CSI&#21453;&#39304;&#20043;&#21069;&#20986;&#29616;&#30340;&#27424;&#37319;&#26679;&#38382;&#39064;&#32463;&#24120;&#34987;&#24573;&#35270;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;CSI&#19978;&#37319;&#26679;&#26694;&#26550;&#65292;&#20316;&#20026;&#22788;&#29702;&#30001;&#27424;&#37319;&#26679;&#24341;&#36215;&#30340;&#38388;&#38553;&#30340;&#21518;&#22788;&#29702;&#35299;&#20915;&#26041;&#26696;&#12290;&#21033;&#29992;&#31163;&#25955;&#20613;&#31435;&#21494;&#21464;&#25442;&#31227;&#20301;&#23450;&#29702;&#21644;&#22810;&#24452;&#20114;&#26131;&#24615;&#30340;&#29289;&#29702;&#21407;&#29702;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26377;&#25928;&#22320;&#21033;&#29992;&#19978;&#34892;CSI&#26469;&#20943;&#36731;&#28151;&#21472;&#25928;&#24212;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21457;&#23637;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08133v1 Announce Type: cross  Abstract: Acquiring downlink channel state information (CSI) at the base station is vital for optimizing performance in massive Multiple input multiple output (MIMO) Frequency-Division Duplexing (FDD) systems. While deep learning architectures have been successful in facilitating UE-side CSI feedback and gNB-side recovery, the undersampling issue prior to CSI feedback is often overlooked. This issue, which arises from low density pilot placement in current standards, results in significant aliasing effects in outdoor channels and consequently limits CSI recovery performance. To this end, this work introduces a new CSI upsampling framework at the gNB as a post-processing solution to address the gaps caused by undersampling. Leveraging the physical principles of discrete Fourier transform shifting theorem and multipath reciprocity, our framework effectively uses uplink CSI to mitigate aliasing effects. We further develop a learning-based method th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24433;&#21709;&#20989;&#25968;&#21644;&#20998;&#24067;&#29420;&#31435;&#21407;&#21017;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#26426;&#22120;&#36951;&#24536;&#20013;&#38750;&#22343;&#21248;&#29305;&#24449;&#21644;&#26631;&#31614;&#21024;&#38500;&#30340;&#25361;&#25112;&#65292;&#20445;&#25252;&#38544;&#31169;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;</title><link>https://arxiv.org/abs/2403.08124</link><description>&lt;p&gt;
&#22312;&#26426;&#22120;&#29305;&#24449;&#21644;&#26631;&#31614;&#30340;&#36951;&#24536;&#20013;&#26397;&#21521;&#29420;&#31435;&#20934;&#21017;
&lt;/p&gt;
&lt;p&gt;
Towards Independence Criterion in Machine Unlearning of Features and Labels
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08124
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#24433;&#21709;&#20989;&#25968;&#21644;&#20998;&#24067;&#29420;&#31435;&#21407;&#21017;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#26426;&#22120;&#36951;&#24536;&#20013;&#38750;&#22343;&#21248;&#29305;&#24449;&#21644;&#26631;&#31614;&#21024;&#38500;&#30340;&#25361;&#25112;&#65292;&#20445;&#25252;&#38544;&#31169;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#28145;&#20837;&#25506;&#35752;&#20102;&#38754;&#20020;&#20998;&#24067;&#36716;&#31227;&#26102;&#30340;&#26426;&#22120;&#36951;&#24536;&#22797;&#26434;&#24615;&#65292;&#29305;&#21035;&#20851;&#27880;&#38750;&#22343;&#21248;&#29305;&#24449;&#21644;&#26631;&#31614;&#21024;&#38500;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#38543;&#30528;GDPR&#31561;&#27861;&#35268;&#24378;&#35843;&#25968;&#25454;&#38544;&#31169;&#21644;&#34987;&#36951;&#24536;&#30340;&#26435;&#21033;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#38754;&#20020;&#30528;&#36951;&#24536;&#25935;&#24863;&#20449;&#24687;&#32780;&#19981;&#25439;&#23475;&#20854;&#23436;&#25972;&#24615;&#25110;&#24615;&#33021;&#30340;&#33392;&#24040;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#24433;&#21709;&#20989;&#25968;&#21644;&#20998;&#24067;&#29420;&#31435;&#21407;&#21017;&#26469;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#30340;&#26032;&#26041;&#27861;&#12290;&#36890;&#36807;&#25552;&#20986;&#19968;&#20010;&#20840;&#38754;&#30340;&#26426;&#22120;&#36951;&#24536;&#26694;&#26550;&#65292;&#25105;&#20204;&#26088;&#22312;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#22312;&#19981;&#21516;&#20998;&#24067;&#19979;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#21644;&#36866;&#24212;&#24615;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#26377;&#21161;&#20110;&#39640;&#25928;&#22320;&#21024;&#38500;&#25968;&#25454;&#65292;&#36824;&#21160;&#24577;&#35843;&#25972;&#27169;&#22411;&#20197;&#20445;&#25345;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08124v1 Announce Type: cross  Abstract: This work delves into the complexities of machine unlearning in the face of distributional shifts, particularly focusing on the challenges posed by non-uniform feature and label removal. With the advent of regulations like the GDPR emphasizing data privacy and the right to be forgotten, machine learning models face the daunting task of unlearning sensitive information without compromising their integrity or performance. Our research introduces a novel approach that leverages influence functions and principles of distributional independence to address these challenges. By proposing a comprehensive framework for machine unlearning, we aim to ensure privacy protection while maintaining model performance and adaptability across varying distributions. Our method not only facilitates efficient data removal but also dynamically adjusts the model to preserve its generalization capabilities. Through extensive experimentation, we demonstrate the
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25351;&#20986;&#22312;&#26500;&#24314;&#22810;&#20445;&#30495;&#24230;&#20195;&#29702;&#27169;&#22411;&#26102;&#65292;&#26377;&#23475;&#25968;&#25454;&#28304;&#30340;&#29305;&#24449;&#21270;&#26377;&#21161;&#20110;&#25351;&#23548;&#20174;&#19994;&#32773;&#22312;&#36873;&#25321;&#26102;&#20309;&#26102;&#24573;&#30053;&#26576;&#20010;&#25968;&#25454;&#28304;&#12290;</title><link>https://arxiv.org/abs/2403.08118</link><description>&lt;p&gt;
&#30740;&#31350;&#26500;&#24314;&#22810;&#20445;&#30495;&#24230;&#20195;&#29702;&#27169;&#22411;&#26102;&#26377;&#23475;&#25968;&#25454;&#26469;&#28304;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Characterising harmful data sources when constructing multi-fidelity surrogate models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08118
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25351;&#20986;&#22312;&#26500;&#24314;&#22810;&#20445;&#30495;&#24230;&#20195;&#29702;&#27169;&#22411;&#26102;&#65292;&#26377;&#23475;&#25968;&#25454;&#28304;&#30340;&#29305;&#24449;&#21270;&#26377;&#21161;&#20110;&#25351;&#23548;&#20174;&#19994;&#32773;&#22312;&#36873;&#25321;&#26102;&#20309;&#26102;&#24573;&#30053;&#26576;&#20010;&#25968;&#25454;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#24403;&#24212;&#29992;&#20110;&#24037;&#19994;&#35774;&#35745;&#38382;&#39064;&#30340;&#24314;&#27169;&#21644;&#20248;&#21270;&#20013;&#65292;&#20195;&#29702;&#24314;&#27169;&#25216;&#26415;&#21463;&#21040;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#24403;&#35780;&#20272;&#29305;&#23450;&#35774;&#35745;&#30340;&#24615;&#33021;&#25104;&#26412;&#24456;&#39640;&#26102;&#65292;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#20197;&#20195;&#26367;&#21487;&#29992;&#30340;&#39640;&#25104;&#26412;&#26469;&#28304;&#26469;&#26597;&#35810;&#21487;&#20197;&#38477;&#20302;&#24635;&#25104;&#26412;&#12290;&#26500;&#24314;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#21487;&#20197;&#21033;&#29992;&#20854;&#20182;&#20415;&#23452;&#19988;&#19981;&#22826;&#20934;&#30830;&#30340;&#20449;&#24687;&#28304;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#20449;&#24687;&#28304;&#30340;&#23384;&#22312;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#26500;&#24314;&#27169;&#22411;&#26102;&#24212;&#35813;&#20351;&#29992;&#21738;&#20123;&#20449;&#24687;&#28304;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#23581;&#35797;&#23545;&#26377;&#23475;&#25968;&#25454;&#28304;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#20197;&#25351;&#23548;&#20174;&#19994;&#32773;&#20309;&#26102;&#24573;&#30053;&#26576;&#20010;&#20449;&#24687;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08118v1 Announce Type: cross  Abstract: Surrogate modelling techniques have seen growing attention in recent years when applied to both modelling and optimisation of industrial design problems. These techniques are highly relevant when assessing the performance of a particular design carries a high cost, as the overall cost can be mitigated via the construction of a model to be queried in lieu of the available high-cost source. The construction of these models can sometimes employ other sources of information which are both cheaper and less accurate. The existence of these sources however poses the question of which sources should be used when constructing a model. Recent studies have attempted to characterise harmful data sources to guide practitioners in choosing when to ignore a certain source. These studies have done so in a synthetic setting, characterising sources using a large amount of data that is not available in practice. Some of these studies have also been shown
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#20174;&#27861;&#24459;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#30740;&#31350;&#20013;&#30830;&#23450;&#20449;&#24687;&#20844;&#27491;&#24615;&#12289;&#34920;&#29616;&#20844;&#27491;&#24615;&#21644;&#36947;&#24503;/&#20262;&#29702;&#19982;&#38544;&#31169;&#25919;&#31574;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#25919;&#31574;&#30340;&#36873;&#39033;&#12290;</title><link>https://arxiv.org/abs/2403.08115</link><description>&lt;p&gt;
&#27861;&#24459;&#32422;&#26463;&#20294;&#19981;&#20844;&#24179;&#65311;&#26397;&#21521;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#20844;&#24179;&#24615;&#30340;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Legally Binding but Unfair? Towards Assessing Fairness of Privacy Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08115
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#30340;&#20844;&#24179;&#24615;&#65292;&#36890;&#36807;&#20174;&#27861;&#24459;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#30740;&#31350;&#20013;&#30830;&#23450;&#20449;&#24687;&#20844;&#27491;&#24615;&#12289;&#34920;&#29616;&#20844;&#27491;&#24615;&#21644;&#36947;&#24503;/&#20262;&#29702;&#19982;&#38544;&#31169;&#25919;&#31574;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#33258;&#21160;&#35780;&#20272;&#25919;&#31574;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#31169;&#25919;&#31574;&#24212;&#24403;&#21578;&#30693;&#25968;&#25454;&#20027;&#20307;&#20854;&#25968;&#25454;&#20445;&#25252;&#26435;&#21033;&#65292;&#35299;&#37322;&#25968;&#25454;&#25511;&#21046;&#32773;&#30340;&#25968;&#25454;&#31649;&#29702;&#23454;&#36341;&#65292;&#24182;&#20351;&#20445;&#30041;&#26399;&#38480;&#25110;&#25968;&#25454;&#36716;&#31227;&#32473;&#31532;&#19977;&#26041;&#31561;&#20107;&#23454;&#36879;&#26126;&#21270;&#12290;&#38544;&#31169;&#25919;&#31574;&#21482;&#26377;&#22312;&#25968;&#25454;&#20027;&#20307;&#27491;&#30830;&#24863;&#30693;&#12289;&#35299;&#37322;&#12289;&#29702;&#35299;&#21644;&#20449;&#20219;&#26102;&#25165;&#33021;&#23454;&#29616;&#20854;&#30446;&#30340;&#12290;&#20854;&#20013;&#65292;&#36825;&#35201;&#27714;&#38544;&#31169;&#25919;&#31574;&#20197;&#20844;&#24179;&#30340;&#26041;&#24335;&#32534;&#20889;&#65292;&#20363;&#22914;&#19981;&#20351;&#29992;&#26497;&#31471;&#30340;&#26415;&#35821;&#65292;&#19981;&#38656;&#35201;&#29305;&#23450;&#30340;&#25945;&#32946;&#31243;&#24230;&#65292;&#25110;&#19981;&#20551;&#35774;&#29305;&#23450;&#30340;&#31038;&#20250;&#32972;&#26223;&#12290;&#22312;&#36825;&#20221;&#36827;&#34892;&#20013;&#30340;&#24037;&#20316;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25105;&#20204;&#35780;&#20272;&#38544;&#31169;&#25919;&#31574;&#20844;&#24179;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20174;&#22522;&#26412;&#27861;&#24459;&#26469;&#28304;&#21644;&#20844;&#24179;&#24615;&#30740;&#31350;&#20013;&#30830;&#23450;&#20449;&#24687;&#20844;&#27491;&#24615;&#12289;&#34920;&#29616;&#20844;&#27491;&#24615;&#21644;&#36947;&#24503;/&#20262;&#29702;&#19982;&#38544;&#31169;&#25919;&#31574;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#21160;&#35780;&#20272;&#25919;&#31574;&#30340;&#36873;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08115v1 Announce Type: cross  Abstract: Privacy policies are expected to inform data subjects about their data protection rights. They should explain the data controller's data management practices, and make facts such as retention periods or data transfers to third parties transparent. Privacy policies only fulfill their purpose, if they are correctly perceived, interpreted, understood, and trusted by the data subject. Amongst others, this requires that a privacy policy is written in a fair way, e.g., it does not use polarizing terms, does not require a certain education, or does not assume a particular social background. In this work-in-progress paper, we outline our approach to assessing fairness in privacy policies. To this end, we identify from fundamental legal sources and fairness research, how the dimensions informational fairness, representational fairness and ethics/morality are related to privacy policies. We propose options to automatically assess policies in the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22240;&#26524;&#36335;&#24452;&#22270;&#25972;&#21512;&#21040;&#20154;&#31867;&#20013;&#24515;&#35774;&#35745;&#20013;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19987;&#29992;&#25554;&#20214;&#29992;&#20110;&#22312;&#32447;&#21327;&#20316;&#30333;&#26495;&#24179;&#21488;&#65292;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616;&#20854;&#25903;&#25345;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#20998;&#25955;&#21644;&#38598;&#20013;&#38454;&#27573;&#65292;&#20943;&#23569;&#35774;&#35745;&#24072;&#30340;&#35748;&#30693;&#36127;&#33655;&#24182;&#22686;&#21152;&#21019;&#36896;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08111</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#35774;&#35745;&#30340;AI&#36741;&#21161;&#22240;&#26524;&#36335;&#24452;&#22270;
&lt;/p&gt;
&lt;p&gt;
AI-Assisted Causal Pathway Diagram for Human-Centered Design
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08111
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#22240;&#26524;&#36335;&#24452;&#22270;&#25972;&#21512;&#21040;&#20154;&#31867;&#20013;&#24515;&#35774;&#35745;&#20013;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19987;&#29992;&#25554;&#20214;&#29992;&#20110;&#22312;&#32447;&#21327;&#20316;&#30333;&#26495;&#24179;&#21488;&#65292;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#21457;&#29616;&#20854;&#25903;&#25345;&#35774;&#35745;&#36807;&#31243;&#20013;&#30340;&#20998;&#25955;&#21644;&#38598;&#20013;&#38454;&#27573;&#65292;&#20943;&#23569;&#35774;&#35745;&#24072;&#30340;&#35748;&#30693;&#36127;&#33655;&#24182;&#22686;&#21152;&#21019;&#36896;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22240;&#26524;&#36335;&#24452;&#22270;&#65288;CPD&#65289;&#25972;&#21512;&#21040;&#20154;&#31867;&#20013;&#24515;&#35774;&#35745;&#65288;HCD&#65289;&#20013;&#65292;&#30740;&#31350;&#36825;&#20123;&#22270;&#22914;&#20309;&#22686;&#24378;&#35774;&#35745;&#36807;&#31243;&#30340;&#26089;&#26399;&#38454;&#27573;&#12290;&#24320;&#21457;&#20102;&#19987;&#29992;&#30340;CPD&#25554;&#20214;&#65292;&#29992;&#20110;&#22312;&#32447;&#21327;&#20316;&#30333;&#26495;&#24179;&#21488;Miro&#65292;&#20197;&#31616;&#21270;&#22270;&#30340;&#21019;&#24314;&#24182;&#25552;&#20379;&#23454;&#26102;&#30340;AI&#39537;&#21160;&#25351;&#23548;&#12290;&#36890;&#36807;&#19982;&#35774;&#35745;&#24072;&#65288;N=20&#65289;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;CPD&#30340;&#20998;&#25903;&#21644;&#20854;&#23545;&#22240;&#26524;&#20851;&#31995;&#30340;&#24378;&#35843;&#22312;&#35774;&#35745;&#36807;&#31243;&#30340;&#20998;&#25955;&#21644;&#38598;&#20013;&#38454;&#27573;&#37117;&#24471;&#21040;&#20102;&#25903;&#25345;&#12290;CPD&#20063;&#21487;&#20197;&#20419;&#36827;&#21033;&#30410;&#30456;&#20851;&#32773;&#20043;&#38388;&#30340;&#27807;&#36890;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#25554;&#20214;&#26174;&#33879;&#20943;&#23569;&#20102;&#35774;&#35745;&#24072;&#30340;&#35748;&#30693;&#36127;&#33655;&#65292;&#24182;&#22312;&#22836;&#33041;&#39118;&#26292;&#36807;&#31243;&#20013;&#22686;&#21152;&#20102;&#20182;&#20204;&#30340;&#21019;&#36896;&#21147;&#65292;&#31361;&#26174;&#20102;AI&#36741;&#21161;&#24037;&#20855;&#22312;&#25903;&#25345;&#21019;&#36896;&#24615;&#24037;&#20316;&#21644;&#22522;&#20110;&#35777;&#25454;&#30340;&#35774;&#35745;&#20013;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08111v1 Announce Type: cross  Abstract: This paper explores the integration of causal pathway diagrams (CPD) into human-centered design (HCD), investigating how these diagrams can enhance the early stages of the design process. A dedicated CPD plugin for the online collaborative whiteboard platform Miro was developed to streamline diagram creation and offer real-time AI-driven guidance. Through a user study with designers (N=20), we found that CPD's branching and its emphasis on causal connections supported both divergent and convergent processes during design. CPD can also facilitate communication among stakeholders. Additionally, we found our plugin significantly reduces designers' cognitive workload and increases their creativity during brainstorming, highlighting the implications of AI-assisted tools in supporting creative work and evidence-based designs.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;Transformer&#27169;&#22411;&#21644;Context-Reverso&#25968;&#25454;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#28165;&#26224;&#24230;&#30340;&#21477;&#23376;</title><link>https://arxiv.org/abs/2403.08103</link><description>&lt;p&gt;
&#21033;&#29992;Context-Reverso&#25968;&#25454;&#20351;&#29992;Transformer&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#28165;&#26224;&#24230;&#30340;&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;
Contextual Clarity: Generating Sentences with Transformer Models using Context-Reverso Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08103
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;Transformer&#27169;&#22411;&#21644;Context-Reverso&#25968;&#25454;&#29983;&#25104;&#20855;&#26377;&#19978;&#19979;&#25991;&#28165;&#26224;&#24230;&#30340;&#21477;&#23376;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#20016;&#23500;&#30340;&#26102;&#20195;&#65292;&#25552;&#20379;&#19982;&#19978;&#19979;&#25991;&#30456;&#20851;&#19988;&#31616;&#27905;&#30340;&#20449;&#24687;&#23545;&#29992;&#25143;&#33267;&#20851;&#37325;&#35201;&#12290;&#20851;&#38190;&#35789;&#19978;&#19979;&#25991;(KIC)&#29983;&#25104;&#26159;&#22312;&#19968;&#20123;&#24212;&#29992;&#20013;&#25198;&#28436;&#33267;&#20851;&#37325;&#35201;&#35282;&#33394;&#30340;&#20219;&#21153;&#65292;&#27604;&#22914;&#25628;&#32034;&#24341;&#25806;&#12289;&#20010;&#20154;&#21161;&#25163;&#21644;&#20869;&#23481;&#25688;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;T5 transformer&#27169;&#22411;&#29983;&#25104;&#32473;&#23450;&#20851;&#38190;&#35789;&#30340;&#26126;&#30830;&#19988;&#31616;&#27905;&#21477;&#23376;&#19978;&#19979;&#25991;&#30340;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#20174;Context-Reverso API&#33719;&#21462;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08103v1 Announce Type: cross  Abstract: In the age of information abundance, the ability to provide users with contextually relevant and concise information is crucial. Keyword in Context (KIC) generation is a task that plays a vital role in and generation applications, such as search engines, personal assistants, and content summarization. In this paper, we present a novel approach to generating unambiguous and brief sentence-contexts for given keywords using the T5 transformer model, leveraging data obtained from the Context-Reverso API. The code is available at https://github.com/Rusamus/word2context/tree/main .
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#33258;&#27880;&#24847;&#21147;&#23398;&#20064;&#21040;&#19968;&#20010;&#33258;&#21160;&#26426;&#65292;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#29983;&#25104;&#26631;&#35760;&#30340;&#20004;&#20010;&#19981;&#21516;&#27493;&#39588;&#26159;&#65306;&#30828;&#26816;&#32034;&#21644;&#36719;&#32452;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.08081</link><description>&lt;p&gt;
&#20855;&#26377;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#30340;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Mechanics of Next Token Prediction with Self-Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08081
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#33258;&#27880;&#24847;&#21147;&#23398;&#20064;&#21040;&#19968;&#20010;&#33258;&#21160;&#26426;&#65292;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#29983;&#25104;&#26631;&#35760;&#30340;&#20004;&#20010;&#19981;&#21516;&#27493;&#39588;&#26159;&#65306;&#30828;&#26816;&#32034;&#21644;&#36719;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#65292;&#20197;&#39044;&#27979;&#32473;&#23450;&#36755;&#20837;&#24207;&#21015;&#30340;&#19979;&#19968;&#20010;&#26631;&#35760;&#12290;&#23613;&#31649;&#35757;&#32451;&#30446;&#26631;&#31616;&#21333;&#65292;&#20294;&#23427;&#20204;&#24050;&#32463;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#38761;&#21629;&#24615;&#36827;&#23637;&#12290;&#36825;&#19968;&#25104;&#21151;&#30340;&#22522;&#30784;&#26159;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#19968;&#20010;&#21333;&#29420;&#30340;&#33258;&#27880;&#24847;&#21147;&#23618;&#20174;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20013;&#23398;&#21040;&#20102;&#20160;&#20040;&#65311;&#25105;&#20204;&#23637;&#31034;&#65306;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#33258;&#27880;&#24847;&#21147;&#23398;&#20064;&#21040;&#19968;&#20010;&#33258;&#21160;&#26426;&#65292;&#35813;&#33258;&#21160;&#26426;&#36890;&#36807;&#20004;&#20010;&#19981;&#21516;&#30340;&#27493;&#39588;&#29983;&#25104;&#19979;&#19968;&#20010;&#26631;&#35760;&#65306;(1) &#30828;&#26816;&#32034;&#65306;&#22312;&#32473;&#23450;&#36755;&#20837;&#24207;&#21015;&#30340;&#24773;&#20917;&#19979;&#65292;&#33258;&#27880;&#24847;&#21147;&#31934;&#30830;&#36873;&#25321;&#19982;&#19978;&#19968;&#20010;&#36755;&#20837;&#26631;&#35760;&#30456;&#20851;&#30340;&#39640;&#20248;&#20808;&#32423;&#36755;&#20837;&#26631;&#35760;&#12290;(2) &#36719;&#32452;&#21512;&#65306;&#28982;&#21518;&#65292;&#23427;&#21019;&#24314;&#39640;&#20248;&#20808;&#32423;&#26631;&#35760;&#30340;&#20984;&#32452;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08081v1 Announce Type: cross  Abstract: Transformer-based language models are trained on large datasets to predict the next token given an input sequence. Despite this simple training objective, they have led to revolutionary advances in natural language processing. Underlying this success is the self-attention mechanism. In this work, we ask: $\textit{What}$ $\textit{does}$ $\textit{a}$ $\textit{single}$ $\textit{self-attention}$ $\textit{layer}$ $\textit{learn}$ $\textit{from}$ $\textit{next-token}$ $\textit{prediction?}$ We show that training self-attention with gradient descent learns an automaton which generates the next token in two distinct steps: $\textbf{(1)}$ $\textbf{Hard}$ $\textbf{retrieval:}$ Given input sequence, self-attention precisely selects the $\textit{high-priority}$ $\textit{input}$ $\textit{tokens}$ associated with the last input token. $\textbf{(2)}$ $\textbf{Soft}$ $\textbf{composition:}$ It then creates a convex combination of the high-priority tok
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#27969;&#24418;&#23398;&#20064;&#30340;&#20013;&#38388;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#20174;&#22810;&#20010;&#27169;&#24577;&#20013;&#25429;&#33719;&#21327;&#21516;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#32500;&#24230;&#38477;&#20302;&#20248;&#21270;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#25552;&#39640;&#21387;&#21147;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.08077</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#27969;&#24418;&#23398;&#20064;&#30340;&#22810;&#27169;&#24577;&#20013;&#38388;&#34701;&#21512;&#32593;&#32476;&#29992;&#20110;&#21387;&#21147;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
A Multimodal Intermediate Fusion Network with Manifold Learning for Stress Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08077
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#27969;&#24418;&#23398;&#20064;&#30340;&#20013;&#38388;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#65292;&#20174;&#22810;&#20010;&#27169;&#24577;&#20013;&#25429;&#33719;&#21327;&#21516;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#32500;&#24230;&#38477;&#20302;&#20248;&#21270;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#25552;&#39640;&#21387;&#21147;&#26816;&#27979;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20943;&#23569;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20174;&#22810;&#20010;&#27169;&#24577;&#20013;&#25429;&#33719;&#21327;&#21516;&#29305;&#24449;&#65292;&#24182;&#19988;&#19982;&#21333;&#27169;&#24577;&#26041;&#27861;&#30456;&#27604;&#65292;&#20855;&#26377;&#25552;&#39640;&#21387;&#21147;&#26816;&#27979;&#20934;&#30830;&#24615;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20934;&#30830;&#24615;&#25552;&#21319;&#36890;&#24120;&#26469;&#33258;&#20110;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#21407;&#22240;&#26159;&#39640;&#32500;&#29305;&#24449;&#31354;&#38388;&#65292;&#23588;&#20854;&#23545;&#20110;&#20013;&#38388;&#34701;&#21512;&#12290;&#38477;&#32500;&#26159;&#20248;&#21270;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#19968;&#31181;&#26041;&#24335;&#65292;&#36890;&#36807;&#31616;&#21270;&#25968;&#25454;&#24182;&#20351;&#29305;&#24449;&#26356;&#26131;&#20110;&#22788;&#29702;&#21644;&#20998;&#26512;&#65292;&#20174;&#32780;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20855;&#26377;&#22522;&#20110;&#27969;&#24418;&#23398;&#20064;&#30340;&#32500;&#24230;&#38477;&#20302;&#30340;&#20013;&#38388;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#12290;&#22810;&#27169;&#24577;&#32593;&#32476;&#36890;&#36807;1D-CNN&#21644;2D-CNN&#20174;&#29983;&#29289;&#29305;&#24449;&#20449;&#21495;&#21644;&#38754;&#37096;&#26631;&#35760;&#29983;&#25104;&#29420;&#31435;&#34920;&#31034;&#12290;&#26368;&#21518;&#65292;&#36825;&#20123;&#29305;&#24449;&#34987;&#34701;&#21512;&#24182;&#39304;&#36865;&#32473;&#21478;&#19968;&#20010;1D-CNN&#23618;&#65292;&#28982;&#21518;&#26159;&#19968;&#20010;&#20840;&#36830;&#25509;&#30340;&#31264;&#23494;&#23618;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#21508;&#31181;&#32500;&#24230;&#20943;&#23569;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08077v1 Announce Type: cross  Abstract: Multimodal deep learning methods capture synergistic features from multiple modalities and have the potential to improve accuracy for stress detection compared to unimodal methods. However, this accuracy gain typically comes from high computational cost due to the high-dimensional feature spaces, especially for intermediate fusion. Dimensionality reduction is one way to optimize multimodal learning by simplifying data and making the features more amenable to processing and analysis, thereby reducing computational complexity. This paper introduces an intermediate multimodal fusion network with manifold learning-based dimensionality reduction. The multimodal network generates independent representations from biometric signals and facial landmarks through 1D-CNN and 2D-CNN. Finally, these features are fused and fed to another 1D-CNN layer, followed by a fully connected dense layer. We compared various dimensionality reduction techniques f
&lt;/p&gt;</description></item><item><title>FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.08059</link><description>&lt;p&gt;
FluoroSAM: &#29992;&#20110;X&#20809;&#22270;&#20687;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FluoroSAM: A Language-aligned Foundation Model for X-ray Image Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08059
&lt;/p&gt;
&lt;p&gt;
FluoroSAM&#26159;&#29992;&#20110;X&#20809;&#22270;&#20687;&#30340;&#20998;&#21106;&#30340;&#35821;&#35328;&#23545;&#40784;&#22522;&#30784;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;X&#20809;&#25104;&#20687;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#36866;&#29992;&#24615;&#30340;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;X&#20809;&#22270;&#20687;&#20998;&#21106;&#23558;&#21152;&#36895;&#35786;&#26029;&#21644;&#20171;&#20837;&#31934;&#20934;&#21307;&#23398;&#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#21457;&#23637;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#36866;&#29992;&#20110;&#35299;&#20915;&#29305;&#23450;&#22270;&#20687;&#20998;&#26512;&#38382;&#39064;&#30340;&#29305;&#23450;&#20219;&#21153;&#27169;&#22411;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#25928;&#29992;&#21463;&#38480;&#20110;&#29305;&#23450;&#20219;&#21153;&#39046;&#22495;&#65292;&#35201;&#25299;&#23637;&#21040;&#26356;&#24191;&#27867;&#30340;&#24212;&#29992;&#21017;&#38656;&#35201;&#39069;&#22806;&#30340;&#25968;&#25454;&#12289;&#26631;&#31614;&#21644;&#37325;&#26032;&#35757;&#32451;&#24037;&#20316;&#12290;&#26368;&#36817;&#65292;&#22522;&#30784;&#27169;&#22411;&#65288;FMs&#65289; - &#35757;&#32451;&#22312;&#22823;&#37327;&#39640;&#24230;&#21464;&#21270;&#25968;&#25454;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22240;&#27492;&#20351;&#24471;&#24191;&#27867;&#36866;&#29992;&#24615;&#25104;&#20026;&#21487;&#33021; - &#24050;&#32463;&#25104;&#20026;&#33258;&#21160;&#22270;&#20687;&#20998;&#26512;&#30340;&#26377;&#24076;&#26395;&#30340;&#24037;&#20855;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;FMs&#32858;&#28966;&#20110;&#23545;&#35937;&#34987;&#26126;&#26174;&#21487;&#35265;&#36793;&#30028;&#28165;&#26224;&#23450;&#20041;&#30340;&#22330;&#26223;&#21644;&#27169;&#24335;&#65292;&#22914;&#20869;&#31397;&#38236;&#25163;&#26415;&#24037;&#20855;&#20998;&#21106;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;X&#20809;&#25104;&#20687;&#36890;&#24120;&#27809;&#26377;&#25552;&#20379;&#36825;&#31181;&#28165;&#26224;&#30340;&#36793;&#30028;&#25110;&#32467;&#26500;&#20808;&#39564;&#12290;&#22312;X&#20809;&#22270;&#20687;&#24418;&#25104;&#26399;&#38388;&#65292;&#22797;&#26434;&#30340;&#19977;&#32500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08059v1 Announce Type: cross  Abstract: Automated X-ray image segmentation would accelerate research and development in diagnostic and interventional precision medicine. Prior efforts have contributed task-specific models capable of solving specific image analysis problems, but the utility of these models is restricted to their particular task domain, and expanding to broader use requires additional data, labels, and retraining efforts. Recently, foundation models (FMs) -- machine learning models trained on large amounts of highly variable data thus enabling broad applicability -- have emerged as promising tools for automated image analysis. Existing FMs for medical image analysis focus on scenarios and modalities where objects are clearly defined by visually apparent boundaries, such as surgical tool segmentation in endoscopy. X-ray imaging, by contrast, does not generally offer such clearly delineated boundaries or structure priors. During X-ray image formation, complex 3D
&lt;/p&gt;</description></item><item><title>TutoAI &#26159;&#19968;&#20010;&#36328;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29289;&#29702;&#20219;&#21153;&#19978;&#21033;&#29992;AI&#36741;&#21161;&#28151;&#21512;&#23186;&#20307;&#25945;&#31243;&#21019;&#24314;&#65292;&#36890;&#36807;&#35843;&#26597;&#24120;&#35265;&#25945;&#31243;&#32452;&#20214;&#12289;&#35780;&#20272;AI&#27169;&#22411;&#25552;&#21462;&#32452;&#20214;&#30340;&#26041;&#27861;&#20197;&#21450;&#35774;&#35745;UI&#25903;&#25345;&#25945;&#31243;&#21019;&#24314;&#30340;&#25351;&#21335;&#65292;&#35777;&#26126;&#20102;&#20854;&#36739;&#22522;&#20934;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#25110;&#30456;&#20284;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.08049</link><description>&lt;p&gt;
TutoAI&#65306;&#29992;&#20110;&#29289;&#29702;&#20219;&#21153;&#30340;&#36328;&#39046;&#22495; AI &#36741;&#21161;&#28151;&#21512;&#23186;&#20307;&#25945;&#31243;&#21019;&#20316;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
TutoAI: A Cross-domain Framework for AI-assisted Mixed-media Tutorial Creation on Physical Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08049
&lt;/p&gt;
&lt;p&gt;
TutoAI &#26159;&#19968;&#20010;&#36328;&#39046;&#22495;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#29289;&#29702;&#20219;&#21153;&#19978;&#21033;&#29992;AI&#36741;&#21161;&#28151;&#21512;&#23186;&#20307;&#25945;&#31243;&#21019;&#24314;&#65292;&#36890;&#36807;&#35843;&#26597;&#24120;&#35265;&#25945;&#31243;&#32452;&#20214;&#12289;&#35780;&#20272;AI&#27169;&#22411;&#25552;&#21462;&#32452;&#20214;&#30340;&#26041;&#27861;&#20197;&#21450;&#35774;&#35745;UI&#25903;&#25345;&#25945;&#31243;&#21019;&#24314;&#30340;&#25351;&#21335;&#65292;&#35777;&#26126;&#20102;&#20854;&#36739;&#22522;&#20934;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#25110;&#30456;&#20284;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#23186;&#20307;&#25945;&#31243;&#23558;&#35270;&#39057;&#12289;&#22270;&#29255;&#12289;&#25991;&#26412;&#21644;&#22270;&#34920;&#25972;&#21512;&#36215;&#26469;&#65292;&#20197;&#25945;&#25480;&#36807;&#31243;&#25216;&#33021;&#65292;&#25552;&#20379;&#27604;&#22522;&#20110;&#26102;&#38388;&#36724;&#30340;&#35270;&#39057;&#26356;&#20855;&#21487;&#27983;&#35272;&#24615;&#30340;&#36873;&#25321;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#21019;&#24314;&#27492;&#31867;&#25945;&#31243;&#26159;&#20047;&#21619;&#30340;&#65292;&#29616;&#26377;&#30340;&#33258;&#21160;&#21270;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#23616;&#38480;&#20110;&#29305;&#23450;&#39046;&#22495;&#12290;&#34429;&#28982; AI &#27169;&#22411;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#22914;&#20309;&#26377;&#25928;&#21033;&#29992;&#23427;&#20204;&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#65292;&#32771;&#34385;&#21040;&#25152;&#28041;&#21450;&#30340;&#22810;&#27169;&#24577;&#25968;&#25454;&#21644;&#27169;&#22411;&#30340;&#24191;&#38420;&#39046;&#22495;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; TutoAI&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#29289;&#29702;&#20219;&#21153;&#30340;&#36328;&#39046;&#22495; AI &#36741;&#21161;&#28151;&#21512;&#23186;&#20307;&#25945;&#31243;&#21019;&#20316;&#30340;&#26694;&#26550;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#35843;&#26597;&#29616;&#26377;&#24037;&#20316;&#65292;&#25105;&#20204;&#25552;&#28860;&#20102;&#24120;&#35265;&#30340;&#25945;&#31243;&#32452;&#20214;&#65307;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#35782;&#21035;&#12289;&#32452;&#35013;&#21644;&#35780;&#20272;&#29992;&#20110;&#32452;&#20214;&#25552;&#21462;&#30340; AI &#27169;&#22411;&#30340;&#26041;&#27861;&#65307;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20026;&#22522;&#20110; AI &#29983;&#25104;&#30340;&#32452;&#20214;&#25903;&#25345;&#25945;&#31243;&#21019;&#24314;&#30340;&#29992;&#25143;&#30028;&#38754;&#65288;UI&#65289;&#35774;&#35745;&#25351;&#21335;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102; TutoAI &#22312;&#36136;&#37327;&#19978;&#36798;&#21040;&#25110;&#39640;&#20110;&#22522;&#20934;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08049v1 Announce Type: cross  Abstract: Mixed-media tutorials, which integrate videos, images, text, and diagrams to teach procedural skills, offer more browsable alternatives than timeline-based videos. However, manually creating such tutorials is tedious, and existing automated solutions are often restricted to a particular domain. While AI models hold promise, it is unclear how to effectively harness their powers, given the multi-modal data involved and the vast landscape of models. We present TutoAI, a cross-domain framework for AI-assisted mixed-media tutorial creation on physical tasks. First, we distill common tutorial components by surveying existing work; then, we present an approach to identify, assemble, and evaluate AI models for component extraction; finally, we propose guidelines for designing user interfaces (UI) that support tutorial creation based on AI-generated components. We show that TutoAI has achieved higher or similar quality compared to a baseline mo
&lt;/p&gt;</description></item><item><title>&#35813;&#32508;&#36848;&#20998;&#26512;&#20102;&#39135;&#21697;&#19982;&#20892;&#19994;&#39046;&#22495;&#30340;&#32593;&#32476;&#23433;&#20840;&#20107;&#20214;&#24182;&#25253;&#21578;&#20102;&#19981;&#26029;&#22686;&#21152;&#30340;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#39057;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.08036</link><description>&lt;p&gt;
&#39135;&#21697;&#19982;&#20892;&#19994;&#39046;&#22495;&#32593;&#32476;&#23433;&#20840;&#20107;&#20214;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Review of Cybersecurity Incidents in the Food and Agriculture Sector
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#32508;&#36848;&#20998;&#26512;&#20102;&#39135;&#21697;&#19982;&#20892;&#19994;&#39046;&#22495;&#30340;&#32593;&#32476;&#23433;&#20840;&#20107;&#20214;&#24182;&#25253;&#21578;&#20102;&#19981;&#26029;&#22686;&#21152;&#30340;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#39057;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39135;&#21697;&#19982;&#20892;&#19994;&#65288;FA&#65289;&#39046;&#22495;&#23545;&#26032;&#20852;&#25216;&#26415;&#30340;&#26085;&#30410;&#21033;&#29992;&#22686;&#21152;&#20102;&#20943;&#23569;&#32593;&#32476;&#39118;&#38505;&#30340;&#23433;&#20840;&#38656;&#27714;&#12290;&#37492;&#20110;&#36825;&#19968;&#24773;&#20917;&#65292;&#26412;&#25991;&#23457;&#26597;&#20102;FA&#39046;&#22495;&#25259;&#38706;&#21644;&#35760;&#24405;&#30340;&#32593;&#32476;&#23433;&#20840;&#20107;&#20214;&#12290;&#20026;&#27492;&#65292;&#30830;&#23450;&#20102;&#21457;&#29983;&#22312;2011&#24180;7&#26376;&#33267;2023&#24180;4&#26376;&#20043;&#38388;&#30340;30&#36215;&#32593;&#32476;&#23433;&#20840;&#20107;&#20214;&#12290;&#36825;&#20123;&#20107;&#20214;&#30340;&#35814;&#32454;&#20449;&#24687;&#26469;&#28304;&#20110;&#22810;&#20010;&#26469;&#28304;&#65292;&#21253;&#25324;&#65306;&#31169;&#33829;&#34892;&#19994;&#21644;&#32852;&#37030;&#35843;&#26597;&#23616;&#65288;FBI&#65289;&#21457;&#24067;&#30340;&#38378;&#30005;&#36890;&#30693;&#12289;&#21463;&#24433;&#21709;&#32452;&#32455;&#30340;&#20869;&#37096;&#25253;&#21578;&#21644;&#21487;&#33719;&#24471;&#30340;&#23186;&#20307;&#26469;&#28304;&#12290;&#26681;&#25454;&#25552;&#20379;&#30340;&#20449;&#24687;&#65292;&#23545;&#27599;&#19968;&#36215;&#20107;&#20214;&#30340;&#23433;&#20840;&#23041;&#32961;&#27010;&#20917;&#12289;&#21202;&#32034;&#37329;&#39069;&#21644;&#23545;&#32452;&#32455;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#35752;&#35770;&#12290;&#35813;&#32508;&#36848;&#25253;&#21578;&#26174;&#31034;&#32593;&#32476;&#23433;&#20840;&#23041;&#32961;&#23545;FA&#39046;&#22495;&#30340;&#39057;&#29575;&#22686;&#21152;&#12290;&#20026;&#20102;&#20943;&#23569;&#36825;&#20123;&#32593;&#32476;&#39118;&#38505;&#65292;&#27969;&#34892;&#30340;&#32593;&#32476;&#23433;&#20840;&#26694;&#26550;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08036v1 Announce Type: cross  Abstract: The increasing utilization of emerging technologies in the Food &amp; Agriculture (FA) sector has heightened the need for security to minimize cyber risks. Considering this aspect, this manuscript reviews disclosed and documented cybersecurity incidents in the FA sector. For this purpose, thirty cybersecurity incidents were identified, which took place between July 2011 and April 2023. The details of these incidents are reported from multiple sources such as: the private industry and flash notifications generated by the Federal Bureau of Investigation (FBI), internal reports from the affected organizations, and available media sources. Considering the available information, a brief description of the security threat, ransom amount, and impact on the organization are discussed for each incident. This review reports an increased frequency of cybersecurity threats to the FA sector. To minimize these cyber risks, popular cybersecurity framewor
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#22260;&#32469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26816;&#27979;&#21644;&#20998;&#31867;&#20196;&#20154;&#24974;&#24694;&#25110;&#26377;&#27602;&#20869;&#23481;&#26041;&#38754;&#30340;&#20316;&#29992;&#23637;&#24320;&#25991;&#29486;&#32508;&#36848;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#26088;&#22312;&#25581;&#31034;LLM&#22312;&#35782;&#21035;&#20196;&#20154;&#24974;&#24694;&#20869;&#23481;&#26041;&#38754;&#30340;&#33021;&#21147;&#21450;&#20854;&#24433;&#21709;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2403.08035</link><description>&lt;p&gt;
&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#25171;&#20987;&#32593;&#32476;&#20167;&#24680;&#65306;&#25506;&#35752;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;
&lt;/p&gt;
&lt;p&gt;
Harnessing Artificial Intelligence to Combat Online Hate: Exploring the Challenges and Opportunities of Large Language Models in Hate Speech Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08035
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#22260;&#32469;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#26816;&#27979;&#21644;&#20998;&#31867;&#20196;&#20154;&#24974;&#24694;&#25110;&#26377;&#27602;&#20869;&#23481;&#26041;&#38754;&#30340;&#20316;&#29992;&#23637;&#24320;&#25991;&#29486;&#32508;&#36848;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#26088;&#22312;&#25581;&#31034;LLM&#22312;&#35782;&#21035;&#20196;&#20154;&#24974;&#24694;&#20869;&#23481;&#26041;&#38754;&#30340;&#33021;&#21147;&#21450;&#20854;&#24433;&#21709;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#38500;&#20102;&#35821;&#35328;&#29983;&#25104;&#65292;&#36824;&#21253;&#25324;&#32763;&#35793;&#12289;&#25688;&#35201;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#12290;&#19968;&#20010;&#24341;&#20154;&#27880;&#30446;&#30340;&#24212;&#29992;&#26159;&#25991;&#26412;&#20998;&#31867;&#12290;&#22312;&#35782;&#21035;&#20196;&#20154;&#24974;&#24694;&#25110;&#26377;&#27602;&#35328;&#35770;&#30340;&#39046;&#22495;&#20013;&#65292;&#36825;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#26159;&#19968;&#20010;&#20805;&#28385;&#25361;&#25112;&#21644;&#20262;&#29702;&#22256;&#22659;&#30340;&#39046;&#22495;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#26377;&#20004;&#20010;&#30446;&#26631;&#65306;&#39318;&#20808;&#65292;&#25552;&#20379;&#19968;&#20010;&#22260;&#32469;LLMs&#20316;&#20026;&#20998;&#31867;&#22120;&#30340;&#25991;&#29486;&#32508;&#36848;&#65292;&#24378;&#35843;&#23427;&#20204;&#22312;&#26816;&#27979;&#21644;&#20998;&#31867;&#20196;&#20154;&#24974;&#24694;&#25110;&#26377;&#27602;&#20869;&#23481;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#20960;&#31181;LLMs&#22312;&#20998;&#31867;&#20167;&#24680;&#35328;&#35770;&#26041;&#38754;&#30340;&#25928;&#21147;&#65306;&#35782;&#21035;&#21738;&#20123;LLMs&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#20197;&#21450;&#23427;&#20204;&#30340;&#22522;&#26412;&#23646;&#24615;&#21644;&#35757;&#32451;&#12290;&#20511;&#27492;&#27934;&#23519;&#20419;&#25104;LLM&#22312;&#35782;&#21035;&#20196;&#20154;&#24974;&#24694;&#20869;&#23481;&#26041;&#38754;&#30340;&#20248;&#21155;&#24615;&#25152;&#20381;&#36182;&#30340;&#22240;&#32032;&#12290;&#36890;&#36807;&#32467;&#21512;&#20840;&#38754;&#30340;&#25991;&#29486;&#22238;&#39038;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#35770;&#25991;&#26088;&#22312;&#35299;&#24320;&#36825;&#20123;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36776;&#21035;&#20196;&#20154;&#24974;&#24694;&#20869;&#23481;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08035v1 Announce Type: cross  Abstract: Large language models (LLMs) excel in many diverse applications beyond language generation, e.g., translation, summarization, and sentiment analysis. One intriguing application is in text classification. This becomes pertinent in the realm of identifying hateful or toxic speech -- a domain fraught with challenges and ethical dilemmas. In our study, we have two objectives: firstly, to offer a literature review revolving around LLMs as classifiers, emphasizing their role in detecting and classifying hateful or toxic content. Subsequently, we explore the efficacy of several LLMs in classifying hate speech: identifying which LLMs excel in this task as well as their underlying attributes and training. Providing insight into the factors that contribute to an LLM proficiency (or lack thereof) in discerning hateful content. By combining a comprehensive literature review with an empirical analysis, our paper strives to shed light on the capabil
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25913;&#36827;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#36816;&#21160;&#32447;&#32034;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;LG-Traj&#26041;&#27861;&#32467;&#21512;LLMs&#29983;&#25104;&#36807;&#21435;&#21644;&#26410;&#26469;&#36712;&#36857;&#20013;&#30340;&#36816;&#21160;&#32447;&#32034;&#65292;&#20197;&#22686;&#36827;&#23545;&#34892;&#20154;&#36712;&#36857;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.08032</link><description>&lt;p&gt;
LG-Traj: LLM&#24341;&#23548;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
LG-Traj: LLM Guided Pedestrian Trajectory Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08032
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25913;&#36827;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#36816;&#21160;&#32447;&#32034;&#26469;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;LG-Traj&#26041;&#27861;&#32467;&#21512;LLMs&#29983;&#25104;&#36807;&#21435;&#21644;&#26410;&#26469;&#36712;&#36857;&#20013;&#30340;&#36816;&#21160;&#32447;&#32034;&#65292;&#20197;&#22686;&#36827;&#23545;&#34892;&#20154;&#36712;&#36857;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#21160;&#24577;&#29615;&#22659;&#20013;&#34892;&#20154;&#36816;&#21160;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#20173;&#38656;&#35201;&#26356;&#22810;&#25506;&#32034;&#26469;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#36816;&#21160;&#27169;&#24335;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25913;&#36827;&#34892;&#20154;&#36712;&#36857;&#39044;&#27979;&#20219;&#21153;&#30340;&#21487;&#33021;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#36816;&#21160;&#32447;&#32034;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;LG-Traj&#65292;&#19968;&#31181;&#23558;LLMs&#32467;&#21512;&#36215;&#26469;&#20135;&#29983;&#20986;&#34892;&#20154;&#36807;&#21435;/&#35266;&#23519;&#36712;&#36857;&#20013;&#30340;&#36816;&#21160;&#32447;&#32034;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#28151;&#21512;&#23545;&#35757;&#32451;&#25968;&#25454;&#30340;&#26410;&#26469;&#36712;&#36857;&#36827;&#34892;&#32858;&#31867;&#65292;&#20174;&#32780;&#32467;&#21512;&#20102;&#22312;&#34892;&#20154;&#26410;&#26469;&#36712;&#36857;&#20013;&#23384;&#22312;&#30340;&#36816;&#21160;&#32447;&#32034;&#12290;&#36825;&#20123;&#36816;&#21160;&#32447;&#32034;&#65292;&#20877;&#21152;&#19978;&#34892;&#20154;&#22352;&#26631;&#65292;&#26377;&#21161;&#20110;&#26356;&#22909;&#22320;&#29702;&#35299;&#28508;&#22312;&#30340;&#34920;&#31034;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36816;&#29992;&#22855;&#24322;&#20540;&#20998;&#35299;&#26469;&#22686;&#24378;&#35266;&#27979;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08032v1 Announce Type: cross  Abstract: Accurate pedestrian trajectory prediction is crucial for various applications, and it requires a deep understanding of pedestrian motion patterns in dynamic environments. However, existing pedestrian trajectory prediction methods still need more exploration to fully leverage these motion patterns. This paper investigates the possibilities of using Large Language Models (LLMs) to improve pedestrian trajectory prediction tasks by inducing motion cues. We introduce LG-Traj, a novel approach incorporating LLMs to generate motion cues present in pedestrian past/observed trajectories. Our approach also incorporates motion cues present in pedestrian future trajectories by clustering future trajectories of training data using a mixture of Gaussians. These motion cues, along with pedestrian coordinates, facilitate a better understanding of the underlying representation. Furthermore, we utilize singular value decomposition to augment the observe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#32418;&#38431;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#39564;&#39640;&#20809;&#35889;&#22270;&#20687;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#26500;&#24314;&#20986;&#19968;&#20010;&#21482;&#20351;&#29992;1%&#30340;&#36755;&#20837;&#29305;&#24449;&#21363;&#21487;&#36798;&#21040;&#21487;&#27604;&#36739;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.08017</link><description>&lt;p&gt;
&#20351;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#39640;&#20809;&#35889;&#22270;&#20687;&#20998;&#26512;&#32418;&#38431;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Red Teaming Models for Hyperspectral Image Analysis Using Explainable AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08017
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30340;&#32418;&#38431;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#39564;&#39640;&#20809;&#35889;&#22270;&#20687;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#25104;&#21151;&#26500;&#24314;&#20986;&#19968;&#20010;&#21482;&#20351;&#29992;1%&#30340;&#36755;&#20837;&#29305;&#24449;&#21363;&#21487;&#36798;&#21040;&#21487;&#27604;&#36739;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36965;&#24863;&#39046;&#22495;&#35201;&#27714;&#21487;&#38752;&#12289;&#31283;&#20581;&#19988;&#32463;&#36807;&#36136;&#37327;&#20445;&#35777;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#24471;&#32418;&#38431;&#25104;&#20026;&#35782;&#21035;&#21644;&#26292;&#38706;&#28508;&#22312;&#32570;&#38519;&#21644;&#20559;&#35265;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#35770;&#65292;&#29992;&#20110;&#26816;&#26597;&#22312;HYPERVIEW&#25361;&#25112;&#36187;&#20013;&#36816;&#34892;&#22312;&#39640;&#20809;&#35889;&#22270;&#20687;&#19978;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#37325;&#28857;&#20851;&#27880;&#22303;&#22756;&#21442;&#25968;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#30340;&#20107;&#21518;&#35299;&#37322;&#26041;&#27861;&#65292;&#23545;&#36194;&#24471;HYPERVIEW&#25361;&#25112;&#36187;&#24182;&#20316;&#20026;INTUITION-1&#39640;&#20809;&#35889;&#20219;&#21153;&#19978;&#37096;&#32626;&#27169;&#22411;&#28789;&#24863;&#30340;&#34920;&#29616;&#26368;&#20339;&#27169;&#22411;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#25351;&#20986;&#21644;&#39564;&#35777;&#20851;&#38190;&#32570;&#38519;&#65292;&#26500;&#24314;&#20986;&#19968;&#20010;&#21482;&#20351;&#29992;1%&#30340;&#36755;&#20837;&#29305;&#24449;&#21363;&#21487;&#36798;&#21040;&#21487;&#27604;&#36739;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08017v1 Announce Type: cross  Abstract: Remote sensing (RS) applications in the space domain demand machine learning (ML) models that are reliable, robust, and quality-assured, making red teaming a vital approach for identifying and exposing potential flaws and biases. Since both fields advance independently, there is a notable gap in integrating red teaming strategies into RS. This paper introduces a methodology for examining ML models operating on hyperspectral images within the HYPERVIEW challenge, focusing on soil parameters' estimation. We use post-hoc explanation methods from the Explainable AI (XAI) domain to critically assess the best performing model that won the HYPERVIEW challenge and served as an inspiration for the model deployed on board the INTUITION-1 hyperspectral mission. Our approach effectively red teams the model by pinpointing and validating key shortcomings, constructing a model that achieves comparable performance using just 1% of the input features a
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#36755;&#20986;&#20013;&#20197;&#27599;&#23618;&#26377;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#21333;&#35789;&#21644;&#23383;&#31526;&#30340;&#35821;&#35328;ID&#26465;&#20214;&#21270;&#21464;&#21387;&#22120;&#23618;&#65292;&#35813;&#26041;&#27861;&#34429;&#28982;&#26410;&#33021;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#65292;&#20294;&#23637;&#29616;&#20102;&#22312;&#20165;&#20165;&#36890;&#36807;&#21475;&#35821;&#25968;&#25454;&#39044;&#27979;&#27491;&#30830;&#35821;&#35328;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08011</link><description>&lt;p&gt;
&#20351;&#29992;&#38598;&#25104;&#39044;&#27979;&#21475;&#35821;&#35328;&#35782;&#21035;&#30340;&#21476;&#21513;&#25289;&#29305;&#35821;-&#33521;&#35821;&#28151;&#21512;&#35821;&#38899;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Gujarati-English Code-Switching Speech Recognition using ensemble prediction of spoken language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08011
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#36755;&#20986;&#20013;&#20197;&#27599;&#23618;&#26377;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#21333;&#35789;&#21644;&#23383;&#31526;&#30340;&#35821;&#35328;ID&#26465;&#20214;&#21270;&#21464;&#21387;&#22120;&#23618;&#65292;&#35813;&#26041;&#27861;&#34429;&#28982;&#26410;&#33021;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#65292;&#20294;&#23637;&#29616;&#20102;&#22312;&#20165;&#20165;&#36890;&#36807;&#21475;&#35821;&#25968;&#25454;&#39044;&#27979;&#27491;&#30830;&#35821;&#35328;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#35821;&#38899;&#35782;&#21035;&#20013;&#19968;&#20010;&#37325;&#35201;&#19988;&#22256;&#38590;&#30340;&#20219;&#21153;&#26159;&#35782;&#21035;&#35821;&#35328;&#65292;&#22240;&#20026;&#20004;&#31181;&#35821;&#35328;&#20013;&#30340;&#35768;&#22810;&#35789;&#22312;&#26576;&#20123;&#21475;&#38899;&#19979;&#21548;&#36215;&#26469;&#30456;&#20284;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#36890;&#36807;&#22312;&#36755;&#20986;&#20013;&#20197;&#27599;&#23618;&#26377;&#30417;&#30563;&#30340;&#26041;&#24335;&#23545;&#21333;&#35789;&#21644;&#23383;&#31526;&#30340;&#35821;&#35328;ID&#26465;&#20214;&#21270;&#21464;&#21387;&#22120;&#23618;&#65292;&#20197;&#25913;&#21892;&#31471;&#21040;&#31471;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#24341;&#20837;&#35821;&#35328;&#29305;&#23450;&#21442;&#25968;&#21644;&#21487;&#35299;&#37322;&#24615;&#21040;&#22810;&#22836;&#27880;&#24847;&#21147;&#26426;&#21046;&#30340;&#26041;&#27861;&#65292;&#24182;&#23454;&#26045;&#20102;&#19968;&#20010;&#26377;&#21161;&#20110;&#20445;&#25345;&#36755;&#20837;&#23545;&#40784;&#36830;&#32493;&#24615;&#30340;&#26102;&#38388;&#25439;&#22833;&#12290;&#23613;&#31649;&#26080;&#27861;&#26174;&#33879;&#38477;&#20302;&#35789;&#38169;&#35823;&#29575;&#65288;WER&#65289;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20102;&#22312;&#20165;&#20165;&#36890;&#36807;&#21475;&#35821;&#25968;&#25454;&#39044;&#27979;&#27491;&#30830;&#35821;&#35328;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#24207;&#21015;&#20013;&#21024;&#38500;LID&#24341;&#20837;&#20102;&#35821;&#35328;&#39044;&#27979;&#30340;&#27491;&#21017;&#21270;&#65292;&#26377;&#21161;&#20110;&#23545;&#40784;&#38271;&#37325;&#22797;&#30340;&#36755;&#20986;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08011v1 Announce Type: cross  Abstract: An important and difficult task in code-switched speech recognition is to recognize the language, as lots of words in two languages can sound similar, especially in some accents. We focus on improving performance of end-to-end Automatic Speech Recognition models by conditioning transformer layers on language ID of words and character in the output in an per layer supervised manner. To this end, we propose two methods of introducing language specific parameters and explainability in the multi-head attention mechanism, and implement a Temporal Loss that helps maintain continuity in input alignment. Despite being unable to reduce WER significantly, our method shows promise in predicting the correct language from just spoken data. We introduce regularization in the language prediction by dropping LID in the sequence, which helps align long repeated output sequences.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#39044;&#22791;&#24037;&#20316;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#21644;DDIM&#21453;&#28436;&#65292;&#33719;&#21462;&#32534;&#36753;&#26041;&#21521;&#23884;&#20837;&#65292;&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#65292;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#31454;&#20105;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.08004</link><description>&lt;p&gt;
Pix2Pix-OnTheFly: &#21033;&#29992;LLMs&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Pix2Pix-OnTheFly: Leveraging LLMs for Instruction-Guided Image Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.08004
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#30340;&#22270;&#20687;&#32534;&#36753;&#65292;&#22312;&#19981;&#38656;&#35201;&#20219;&#20309;&#39044;&#22791;&#24037;&#20316;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#21644;DDIM&#21453;&#28436;&#65292;&#33719;&#21462;&#32534;&#36753;&#26041;&#21521;&#23884;&#20837;&#65292;&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#65292;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#26368;&#36817;&#32467;&#21512;&#35821;&#35328;&#22788;&#29702;&#21644;&#22270;&#20687;&#22788;&#29702;&#30340;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#23383;&#24149;&#21644;DDIM&#21453;&#28436;&#65292;&#33719;&#21462;&#32534;&#36753;&#26041;&#21521;&#23884;&#20837;&#65292;&#36827;&#34892;&#25351;&#23548;&#22270;&#20687;&#32534;&#36753;&#65292;&#32780;&#26080;&#38656;&#39044;&#22791;&#24037;&#20316;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.08004v1 Announce Type: cross  Abstract: The combination of language processing and image processing keeps attracting increased interest given recent impressive advances that leverage the combined strengths of both domains of research. Among these advances, the task of editing an image on the basis solely of a natural language instruction stands out as a most challenging endeavour. While recent approaches for this task resort, in one way or other, to some form of preliminary preparation, training or fine-tuning, this paper explores a novel approach: We propose a preparation-free method that permits instruction-guided image editing on the fly. This approach is organized along three steps properly orchestrated that resort to image captioning and DDIM inversion, followed by obtaining the edit direction embedding, followed by image editing proper. While dispensing with preliminary preparation, our approach demonstrates to be effective and competitive, outperforming recent, state 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#29983;&#25104;&#24335;&#22686;&#24378;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#36890;&#36807;&#22522;&#20110;&#24819;&#35937;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.07979</link><description>&lt;p&gt;
&#22810;&#26234;&#20307;&#26159;&#21542;&#26790;&#35265;&#30005;&#23376;&#32650;&#65311;&#65306;&#36890;&#36807;&#29983;&#25104;&#24335;&#23398;&#20064;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Do Agents Dream of Electric Sheep?: Improving Generalization in Reinforcement Learning through Generative Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07979
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#24335;&#22686;&#24378;&#25216;&#26415;&#65292;&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#22312;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#20013;&#36890;&#36807;&#22522;&#20110;&#24819;&#35937;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#26469;&#25552;&#39640;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#27867;&#21270;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#24230;&#25311;&#21512;&#30340;&#22823;&#33041;&#20551;&#35774;&#34920;&#26126;&#26790;&#30340;&#21457;&#29983;&#26159;&#20026;&#20102;&#35753;&#20154;&#31867;&#22823;&#33041;&#36827;&#34892;&#27867;&#21270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#35810;&#38382;&#26159;&#21542;&#23545;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;&#20063;&#26159;&#22914;&#27492;&#12290;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#32463;&#39564;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#22522;&#20110;&#24819;&#35937;&#21147;&#30340;&#24378;&#21270;&#23398;&#20064;&#26469;&#22312;&#31867;&#20284;&#26790;&#22659;&#30340;&#24773;&#33410;&#20013;&#35757;&#32451;&#31574;&#30053;&#65292;&#22312;&#37027;&#37324;&#65292;&#23545;&#38750;&#24819;&#35937;&#21147;&#12289;&#39044;&#27979;&#30340;&#36712;&#36857;&#36827;&#34892;&#29983;&#25104;&#24615;&#22686;&#24378;&#12290;&#22312;&#22235;&#20010;ProcGen&#29615;&#22659;&#30340;&#23454;&#39564;&#20013;&#34920;&#26126;&#65292;&#19982;&#32463;&#20856;&#30340;&#24819;&#35937;&#21147;&#21644;&#31163;&#32447;&#35757;&#32451;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#31232;&#30095;&#22870;&#21169;&#29615;&#22659;&#26102;&#21487;&#20197;&#36798;&#21040;&#26356;&#39640;&#30340;&#27867;&#21270;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07979v1 Announce Type: cross  Abstract: The Overfitted Brain hypothesis suggests dreams happen to allow generalization in the human brain. Here, we ask if the same is true for reinforcement learning agents as well. Given limited experience in a real environment, we use imagination-based reinforcement learning to train a policy on dream-like episodes, where non-imaginative, predicted trajectories are modified through generative augmentations. Experiments on four ProcGen environments show that, compared to classic imagination and offline training on collected experience, our method can reach a higher level of generalization when dealing with sparsely rewarded environments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;KnowCoder&#65292;&#19968;&#20010;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#25191;&#34892;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20195;&#30721;&#39118;&#26684;&#30340;&#27169;&#24335;&#34920;&#31034;&#26041;&#27861;&#21644;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;LLMs&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#20934;&#30830;&#25552;&#21462;&#33021;&#21147;</title><link>https://arxiv.org/abs/2403.07969</link><description>&lt;p&gt;
KnowCoder&#65306;&#23558;&#32467;&#26500;&#21270;&#30693;&#35782;&#32534;&#30721;&#21040;LLMs&#20013;&#29992;&#20110;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
KnowCoder: Coding Structured Knowledge into LLMs for Universal Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07969
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;KnowCoder&#65292;&#19968;&#20010;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#25191;&#34892;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#20837;&#20102;&#20195;&#30721;&#39118;&#26684;&#30340;&#27169;&#24335;&#34920;&#31034;&#26041;&#27861;&#21644;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#25552;&#39640;LLMs&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#30340;&#20934;&#30830;&#25552;&#21462;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KnowCoder&#65292;&#36825;&#26159;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#20195;&#30721;&#29983;&#25104;&#36827;&#34892;&#26222;&#36866;&#20449;&#24687;&#25552;&#21462;&#65288;UIE&#65289;&#12290;KnowCoder&#26088;&#22312;&#24320;&#21457;&#19968;&#31181;&#32479;&#19968;&#30340;&#27169;&#24335;&#34920;&#31034;&#65292;&#20351;LLMs&#33021;&#22815;&#36731;&#26494;&#29702;&#35299;&#65292;&#24182;&#19988;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#40723;&#21169;LLMs&#36981;&#24490;&#27169;&#24335;&#24182;&#20934;&#30830;&#25552;&#21462;&#32467;&#26500;&#21270;&#30693;&#35782;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;KnowCoder&#24341;&#20837;&#20102;&#19968;&#31181;&#20195;&#30721;&#39118;&#26684;&#30340;&#27169;&#24335;&#34920;&#31034;&#26041;&#27861;&#65292;&#23558;&#19981;&#21516;&#30340;&#27169;&#24335;&#32479;&#19968;&#36716;&#25442;&#20026;Python&#31867;&#65292;&#20174;&#32780;&#21487;&#20197;&#20197;LLM&#21451;&#22909;&#30340;&#26041;&#24335;&#25429;&#25417;UIE&#20013;&#20219;&#21153;&#20043;&#38388;&#30340;&#32422;&#26463;&#31561;&#22797;&#26434;&#27169;&#24335;&#20449;&#24687;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;30,000&#31181;&#30693;&#35782;&#31867;&#22411;&#30340;&#20195;&#30721;&#39118;&#26684;&#27169;&#24335;&#24211;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;UIE&#20013;&#26368;&#22823;&#30340;&#24211;&#12290;&#20026;&#20102;&#31616;&#21270;LLMs&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;KnowCoder&#21253;&#21547;&#19968;&#20010;&#36890;&#36807;&#20195;&#30721;&#39044;&#35757;&#32451;&#22686;&#24378;&#20854;&#27169;&#24335;&#29702;&#35299;&#33021;&#21147;&#30340;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07969v1 Announce Type: cross  Abstract: In this paper, we propose KnowCoder, a Large Language Model (LLM) to conduct Universal Information Extraction (UIE) via code generation. KnowCoder aims to develop a kind of unified schema representation that LLMs can easily understand and an effective learning framework that encourages LLMs to follow schemas and extract structured knowledge accurately. To achieve these, KnowCoder introduces a code-style schema representation method to uniformly transform different schemas into Python classes, with which complex schema information, such as constraints among tasks in UIE, can be captured in an LLM-friendly manner. We further construct a code-style schema library covering over $\textbf{30,000}$ types of knowledge, which is the largest one for UIE, to the best of our knowledge. To ease the learning process of LLMs, KnowCoder contains a two-phase learning framework that enhances its schema understanding ability via code pretraining and its 
&lt;/p&gt;</description></item><item><title>SGD&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#19968;&#20010;&#26143;&#24418;&#22495;&#65292;&#21253;&#21547;&#19968;&#20010;&#26143;&#24418;&#27169;&#22411;&#65292;&#36890;&#36807;&#20302;&#25439;&#22833;&#25968;&#20540;&#30340;&#36335;&#24452;&#19982;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#32447;&#24615;&#30456;&#36830;&#65292;&#27169;&#38500;&#25490;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.07968</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#26159;&#21542;&#24418;&#25104;&#26143;&#24418;&#21306;&#22495;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Deep Neural Network Solutions Form a Star Domain?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07968
&lt;/p&gt;
&lt;p&gt;
SGD&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#19968;&#20010;&#26143;&#24418;&#22495;&#65292;&#21253;&#21547;&#19968;&#20010;&#26143;&#24418;&#27169;&#22411;&#65292;&#36890;&#36807;&#20302;&#25439;&#22833;&#25968;&#20540;&#30340;&#36335;&#24452;&#19982;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#32447;&#24615;&#30456;&#36830;&#65292;&#27169;&#38500;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Entezari&#31561;&#20154;&#65288;2022&#65289;&#25512;&#27979;&#36890;&#36807;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#21487;&#36798;&#21040;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#20984;&#30340;&#65292;&#32771;&#34385;&#21040;&#25490;&#21015;&#19981;&#21464;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26356;&#21152;&#23485;&#26494;&#30340;&#35266;&#28857;&#65306;SGD&#35299;&#20915;&#26041;&#26696;&#38598;&#26159;&#19968;&#20010;&#26143;&#24418;&#22495;&#65292;&#21253;&#21547;&#19968;&#20010;&#26143;&#24418;&#27169;&#22411;&#65292;&#36890;&#36807;&#20302;&#25439;&#22833;&#25968;&#20540;&#30340;&#36335;&#24452;&#19982;&#20854;&#20182;&#35299;&#20915;&#26041;&#26696;&#32447;&#24615;&#30456;&#36830;&#65292;&#27169;&#38500;&#25490;&#21015;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Starlight&#31639;&#27861;&#65292;&#29992;&#20110;&#25214;&#21040;&#32473;&#23450;&#23398;&#20064;&#20219;&#21153;&#30340;&#26143;&#24418;&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#36825;&#20010;&#26143;&#24418;&#27169;&#22411;&#19982;&#20854;&#20182;&#29420;&#31435;&#25214;&#21040;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#32447;&#24615;&#30456;&#36830;&#30340;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07968v1 Announce Type: cross  Abstract: Entezari et al. (2022) conjectured that neural network solution sets reachable via stochastic gradient descent (SGD) are convex, considering permutation invariances. This means that two independent solutions can be connected by a linear path with low loss, given one of them is appropriately permuted. However, current methods to test this theory often fail to eliminate loss barriers between two independent solutions (Ainsworth et al., 2022; Benzing et al., 2022). In this work, we conjecture that a more relaxed claim holds: the SGD solution set is a star domain that contains a star model that is linearly connected to all the other solutions via paths with low loss values, modulo permutations. We propose the Starlight algorithm that finds a star model of a given learning task. We validate our claim by showing that this star model is linearly connected with other independently found solutions. As an additional benefit of our study, we demo
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#23558;&#26465;&#20214;&#35745;&#31639;&#26041;&#27861;&#24212;&#29992;&#20110;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#21407;&#29702;&#21644;&#24605;&#24819;&#65292;&#24182;&#20171;&#32461;&#20102;&#19987;&#23478;&#28151;&#21512;&#32593;&#32476;&#12289;&#26631;&#35760;&#36873;&#25321;&#26426;&#21046;&#21644;&#25552;&#21069;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#31561;&#19977;&#31181;&#23454;&#29616;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.07965</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26465;&#20214;&#35745;&#31639;: &#21407;&#29702;&#19982;&#30740;&#31350;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Conditional computation in neural networks: principles and research trends
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07965
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#23558;&#26465;&#20214;&#35745;&#31639;&#26041;&#27861;&#24212;&#29992;&#20110;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#21407;&#29702;&#21644;&#24605;&#24819;&#65292;&#24182;&#20171;&#32461;&#20102;&#19987;&#23478;&#28151;&#21512;&#32593;&#32476;&#12289;&#26631;&#35760;&#36873;&#25321;&#26426;&#21046;&#21644;&#25552;&#21069;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#31561;&#19977;&#31181;&#23454;&#29616;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#24635;&#32467;&#20102;&#23558;&#26465;&#20214;&#35745;&#31639;&#26041;&#27861;&#24212;&#29992;&#20110;&#35774;&#35745;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#20852;&#39046;&#22495;&#20013;&#30340;&#21407;&#29702;&#21644;&#24605;&#24819;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#21487;&#20197;&#26681;&#25454;&#36755;&#20837;&#21160;&#24577;&#28608;&#27963;&#25110;&#21435;&#28608;&#27963;&#20854;&#35745;&#31639;&#22270;&#37096;&#20998;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#20363;&#22914;&#65292;&#21160;&#24577;&#36873;&#25321;&#36755;&#20837;&#26631;&#35760;&#12289;&#23618;&#65288;&#25110;&#19968;&#32452;&#23618;&#65289;&#20197;&#21450;&#27599;&#20010;&#23618;&#20869;&#30340;&#23376;&#27169;&#22359;&#65288;&#20363;&#22914;&#65292;&#21367;&#31215;&#28388;&#27874;&#22120;&#20013;&#30340;&#36890;&#36947;&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#19968;&#20010;&#36890;&#29992;&#24418;&#24335;&#26469;&#32479;&#19968;&#25551;&#36848;&#36825;&#20123;&#25216;&#26415;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36825;&#20123;&#21407;&#21017;&#30340;&#19977;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#23454;&#29616;&#65306;&#19987;&#23478;&#28151;&#21512;&#65288;MoEs&#65289;&#32593;&#32476;&#12289;&#26631;&#35760;&#36873;&#25321;&#26426;&#21046;&#21644;&#25552;&#21069;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#12290;&#26412;&#25991;&#26088;&#22312;&#21521;&#36825;&#19968;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#25552;&#20379;&#31867;&#20284;&#25945;&#31243;&#30340;&#20171;&#32461;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#27169;&#22359;&#21270;&#35774;&#35745;&#22312;&#25928;&#29575;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#36801;&#31227;&#23398;&#20064;&#26041;&#38754;&#30340;&#22909;&#22788;&#65292;&#37325;&#28857;&#25918;&#22312;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07965v1 Announce Type: cross  Abstract: This article summarizes principles and ideas from the emerging area of applying \textit{conditional computation} methods to the design of neural networks. In particular, we focus on neural networks that can dynamically activate or de-activate parts of their computational graph conditionally on their input. Examples include the dynamic selection of, e.g., input tokens, layers (or sets of layers), and sub-modules inside each layer (e.g., channels in a convolutional filter). We first provide a general formalism to describe these techniques in an uniform way. Then, we introduce three notable implementations of these principles: mixture-of-experts (MoEs) networks, token selection mechanisms, and early-exit neural networks. The paper aims to provide a tutorial-like introduction to this growing field. To this end, we analyze the benefits of these modular designs in terms of efficiency, explainability, and transfer learning, with a focus on em
&lt;/p&gt;</description></item><item><title>&#25552;&#20379;&#20102;&#19968;&#31181;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#65292;&#20197;&#20195;&#29702;-&#29615;&#36335;&#26041;&#27861;&#21644;&#27169;&#22359;&#21270;&#26550;&#26500;&#20026;&#29305;&#33394;&#65292;&#26088;&#22312;&#24357;&#34917;&#29616;&#26377;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#26381;&#21153;&#30340;&#35774;&#35745;&#32570;&#38519;&#65292;&#21521;&#30005;&#21160;&#20986;&#34892;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#31119;&#21033;&#12290;</title><link>https://arxiv.org/abs/2403.07964</link><description>&lt;p&gt;
&#19968;&#31181;&#24320;&#28304;&#20223;&#30495;&#24179;&#21488;&#30340;&#26368;&#20339;&#35774;&#35745;&#19982;&#23454;&#26045;&#65292;&#29992;&#20110;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#26381;&#21153;
&lt;/p&gt;
&lt;p&gt;
Optimal Design and Implementation of an Open-source Emulation Platform for User-Centric Shared E-mobility Services
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20379;&#20102;&#19968;&#31181;&#24320;&#28304;&#26694;&#26550;&#65292;&#29992;&#20110;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#65292;&#20197;&#20195;&#29702;-&#29615;&#36335;&#26041;&#27861;&#21644;&#27169;&#22359;&#21270;&#26550;&#26500;&#20026;&#29305;&#33394;&#65292;&#26088;&#22312;&#24357;&#34917;&#29616;&#26377;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#26381;&#21153;&#30340;&#35774;&#35745;&#32570;&#38519;&#65292;&#21521;&#30005;&#21160;&#20986;&#34892;&#30740;&#31350;&#31038;&#21306;&#25552;&#20379;&#31119;&#21033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#20132;&#36890;&#25490;&#25918;&#21644;&#27745;&#26579;&#19981;&#26029;&#21152;&#21095;&#30340;&#20840;&#29699;&#25361;&#25112;&#65292;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#26381;&#21153;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#31574;&#30053;&#65292;&#21253;&#25324;&#30005;&#21160;&#27773;&#36710;&#12289;&#30005;&#21160;&#33258;&#34892;&#36710;&#21644;&#30005;&#21160;&#28369;&#26495;&#36710;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#26381;&#21153;&#23384;&#22312;&#20851;&#38190;&#35774;&#35745;&#32570;&#38519;&#65292;&#21253;&#25324;&#26381;&#21153;&#25972;&#21512;&#19981;&#36275;&#12289;&#33021;&#28304;&#28040;&#32791;&#39044;&#27979;&#19981;&#31934;&#30830;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#22320;&#29702;&#35206;&#30422;&#33539;&#22260;&#26377;&#38480;&#65292;&#20197;&#21450;&#22312;&#22810;&#27169;&#24335;&#20132;&#36890;&#32972;&#26223;&#19979;&#23588;&#20854;&#32570;&#20047;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#35270;&#35282;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#30446;&#21069;&#27809;&#26377;&#19968;&#20010;&#25972;&#21512;&#30340;&#24320;&#28304;&#26694;&#26550;&#65292;&#21487;&#20197;&#20026;&#30005;&#21160;&#20986;&#34892;&#30740;&#31350;&#31038;&#21306;&#24102;&#26469;&#30410;&#22788;&#12290;&#26412;&#25991;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25552;&#20379;&#19968;&#20010;&#24320;&#21019;&#24615;&#30340;&#20849;&#20139;&#30005;&#21160;&#20986;&#34892;&#24320;&#28304;&#26694;&#26550;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#37319;&#29992;&#20195;&#29702;-&#29615;&#36335;&#26041;&#27861;&#21644;&#27169;&#22359;&#21270;&#26550;&#26500;&#65292;&#26088;&#22312;&#28385;&#36275;&#19981;&#21516;&#29992;&#25143;&#20559;&#22909;&#65292;&#24182;&#25552;&#20379;&#22686;&#24378;&#30340;&#33258;&#23450;&#20041;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07964v1 Announce Type: new  Abstract: In response to the escalating global challenge of increasing emissions and pollution in transportation, shared electric mobility services, encompassing e-cars, e-bikes, and e-scooters, have emerged as a popular strategy. However, existingshared electric mobility services exhibit critical design deficiencies, including insufficient service integration, imprecise energy consumption forecasting, limited scalability and geographical coverage, and a notable absence of a user-centric perspective, particularly in the context of multi-modal transportation. More importantly, there is no consolidated open-source framework which could benefit the e-mobility research community. This paper aims to bridge this gap by providing a pioneering open-source framework for shared e-mobility. The proposed framework, with an agent-in-the-loop approach and modular architecture, is tailored to diverse user preferences and offers enhanced customization. We demonst
&lt;/p&gt;</description></item><item><title>IG&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#27867;&#21270;&#26426;&#21046;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#30340;&#32593;&#32476;&#27969;&#37327;&#65292;&#24182;&#25581;&#31034;&#22797;&#26434;&#30340;&#20837;&#20405;&#36335;&#24452;&#65292;&#20026;&#32593;&#32476;&#23433;&#20840;&#21462;&#35777;&#25552;&#20379;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.07959</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20934;&#30830;&#26816;&#27979;&#24322;&#24120;&#24182;&#35782;&#21035;&#32593;&#32476;&#20837;&#20405;&#25216;&#26415;&#30340;&#21487;&#35299;&#37322;&#27867;&#21270;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
An Interpretable Generalization Mechanism for Accurately Detecting Anomaly and Identifying Networking Intrusion Techniques
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07959
&lt;/p&gt;
&lt;p&gt;
IG&#26159;&#19968;&#20010;&#21487;&#35299;&#37322;&#27867;&#21270;&#26426;&#21046;&#65292;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#30340;&#32593;&#32476;&#27969;&#37327;&#65292;&#24182;&#25581;&#31034;&#22797;&#26434;&#30340;&#20837;&#20405;&#36335;&#24452;&#65292;&#20026;&#32593;&#32476;&#23433;&#20840;&#21462;&#35777;&#25552;&#20379;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#65288;IDS&#65289;&#20013;&#25972;&#21512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#36890;&#36807;&#31934;&#30830;&#30340;&#29305;&#24449;&#36873;&#25321;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#31995;&#32479;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23545;&#32593;&#32476;&#25915;&#20987;&#30340;&#24443;&#24213;&#29702;&#35299;&#35201;&#27714;IDS&#20869;&#22312;&#21487;&#35299;&#37322;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;&#21487;&#35299;&#37322;&#27867;&#21270;&#26426;&#21046;&#8221;&#65288;IG&#65289;&#65292;&#26088;&#22312;&#24443;&#24213;&#25913;&#21464;IDS&#30340;&#33021;&#21147;&#12290;IG&#33021;&#22815;&#35782;&#21035;&#36830;&#36143;&#27169;&#24335;&#65292;&#20351;&#20854;&#33021;&#22815;&#35299;&#37322;&#21306;&#20998;&#27491;&#24120;&#21644;&#24322;&#24120;&#30340;&#32593;&#32476;&#27969;&#37327;&#12290;&#27492;&#22806;&#65292;&#36830;&#36143;&#27169;&#24335;&#30340;&#32508;&#21512;&#25581;&#31034;&#22797;&#26434;&#30340;&#20837;&#20405;&#36335;&#24452;&#65292;&#20026;&#32593;&#32476;&#23433;&#20840;&#21462;&#35777;&#25552;&#20379;&#20102;&#24517;&#35201;&#30340;&#35265;&#35299;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#25968;&#25454;&#38598;NSL-KDD&#12289;UNSW-NB15&#21644;UKM-IDS20&#30340;&#23454;&#39564;&#65292;IG&#21363;&#20351;&#22312;&#36739;&#20302;&#30340;&#35757;&#32451;-&#27979;&#35797;&#27604;&#29575;&#19979;&#20063;&#33021;&#20934;&#30830;&#12290;&#22312;NSL-KDD&#25968;&#25454;&#38598;&#20013;&#65292;&#24403;&#35757;&#32451;-&#27979;&#35797;&#27604;&#29575;&#20026;10%-90%&#26102;&#65292;IG&#23454;&#29616;&#30340;Precision&#65288;PRE&#65289;=0.93&#12289;Recall&#65288;REC&#65289;=0.94&#21644;Area Under Curve&#65288;AUC&#65289;=0.94&#65307;PRE=0.98...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07959v1 Announce Type: cross  Abstract: Recent advancements in Intrusion Detection Systems (IDS), integrating Explainable AI (XAI) methodologies, have led to notable improvements in system performance via precise feature selection. However, a thorough understanding of cyber-attacks requires inherently explainable decision-making processes within IDS. In this paper, we present the Interpretable Generalization Mechanism (IG), poised to revolutionize IDS capabilities. IG discerns coherent patterns, making it interpretable in distinguishing between normal and anomalous network traffic. Further, the synthesis of coherent patterns sheds light on intricate intrusion pathways, providing essential insights for cybersecurity forensics. By experiments with real-world datasets NSL-KDD, UNSW-NB15, and UKM-IDS20, IG is accurate even at a low ratio of training-to-test. With 10%-to-90%, IG achieves Precision (PRE)=0.93, Recall (REC)=0.94, and Area Under Curve (AUC)=0.94 in NSL-KDD; PRE=0.98
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#24046;&#24322;&#26816;&#27979;&#21644;&#26102;&#38388;&#32784;&#24515;&#20316;&#20026;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#26426;&#21046;&#65292;&#21033;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#20013;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#26469;&#26377;&#25928;&#32456;&#27490;&#25512;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.07958</link><description>&lt;p&gt;
&#26102;&#38388;&#20915;&#31574;&#65306;&#21033;&#29992;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#36827;&#34892;&#26377;&#25928;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Temporal Decisions: Leveraging Temporal Correlation for Efficient Decisions in Early Exit Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07958
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24341;&#20837;&#20102;&#24046;&#24322;&#26816;&#27979;&#21644;&#26102;&#38388;&#32784;&#24515;&#20316;&#20026;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#26426;&#21046;&#65292;&#21033;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#20013;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#26469;&#26377;&#25928;&#32456;&#27490;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#23884;&#20837;&#24335;&#21644;&#29289;&#32852;&#32593;&#24212;&#29992;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#20294;&#26159;&#65292;&#22312;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#37096;&#32626;&#27169;&#22411;&#38754;&#20020;&#36164;&#28304;&#38480;&#21046;&#30340;&#25361;&#25112;&#65292;&#36825;&#21487;&#33021;&#24433;&#21709;&#27169;&#22411;&#30340;&#25512;&#26029;&#20934;&#30830;&#24615;&#21644;&#24310;&#36831;&#12290;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#23427;&#36890;&#36807;&#22312;&#38544;&#34255;&#23618;&#20043;&#38388;&#38468;&#21152;&#30340;&#39069;&#22806;&#20998;&#31867;&#22120;&#21160;&#24577;&#35843;&#25972;&#27169;&#22411;&#28145;&#24230;&#12290;&#28982;&#32780;&#65292;&#23454;&#26102;&#32456;&#27490;&#20915;&#31574;&#26426;&#21046;&#23545;&#31995;&#32479;&#30340;&#25928;&#29575;&#12289;&#24310;&#36831;&#21644;&#25345;&#32493;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#24046;&#24322;&#26816;&#27979;&#21644;&#26102;&#38388;&#32784;&#24515;&#20316;&#20026;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#30340;&#20915;&#31574;&#26426;&#21046;&#12290;&#23427;&#20204;&#21033;&#29992;&#20256;&#24863;&#22120;&#25968;&#25454;&#27969;&#20013;&#23384;&#22312;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#26469;&#26377;&#25928;&#22320;&#32456;&#27490;&#25512;&#26029;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20581;&#24247;&#30417;&#27979;&#12289;&#22270;&#20687;&#20998;&#31867;&#21644;&#21796;&#37266;&#35789;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#21019;&#26032;&#36129;&#29486;&#33021;&#22815;&#20943;&#23569;&#35745;&#31639;&#36127;&#25285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07958v1 Announce Type: cross  Abstract: Deep Learning is becoming increasingly relevant in Embedded and Internet-of-things applications. However, deploying models on embedded devices poses a challenge due to their resource limitations. This can impact the model's inference accuracy and latency. One potential solution are Early Exit Neural Networks, which adjust model depth dynamically through additional classifiers attached between their hidden layers. However, the real-time termination decision mechanism is critical for the system's efficiency, latency, and sustained accuracy.   This paper introduces Difference Detection and Temporal Patience as decision mechanisms for Early Exit Neural Networks. They leverage the temporal correlation present in sensor data streams to efficiently terminate the inference. We evaluate their effectiveness in health monitoring, image classification, and wake-word detection tasks. Our novel contributions were able to reduce the computational foo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#22686;&#24378;&#27969;&#31243;&#65292;&#33021;&#22815;&#23558;&#29616;&#26377;&#27169;&#22411;&#36716;&#25442;&#20026;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#65288;EENN&#65289;&#65292;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#37096;&#32626;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#22312;&#29289;&#32852;&#32593;&#21644;&#22270;&#20687;&#20998;&#31867;&#29992;&#20363;&#19978;&#26174;&#33879;&#20943;&#23569;&#25512;&#26029;&#25805;&#20316;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.07957</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#21518;&#35757;&#32451;&#22686;&#24378;&#26041;&#27861;&#29992;&#20110;&#24322;&#26500;&#21644;&#20998;&#24067;&#24335;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#30340;&#33258;&#36866;&#24212;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Efficient Post-Training Augmentation for Adaptive Inference in Heterogeneous and Distributed IoT Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07957
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#22686;&#24378;&#27969;&#31243;&#65292;&#33021;&#22815;&#23558;&#29616;&#26377;&#27169;&#22411;&#36716;&#25442;&#20026;&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#65288;EENN&#65289;&#65292;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#37096;&#32626;&#25928;&#29575;&#65292;&#23454;&#29616;&#20102;&#22312;&#29289;&#32852;&#32593;&#21644;&#22270;&#20687;&#20998;&#31867;&#29992;&#20363;&#19978;&#26174;&#33879;&#20943;&#23569;&#25512;&#26029;&#25805;&#20316;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#36864;&#20986;&#31070;&#32463;&#32593;&#32476;&#65288;EENN&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#37096;&#32626;&#25928;&#29575;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;EENN&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#24182;&#19988;&#38656;&#35201;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#65292;&#30001;&#20110;&#22823;&#37327;&#39069;&#22806;&#30340;&#35774;&#35745;&#36873;&#25321;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#22686;&#24378;&#27969;&#31243;&#65292;&#19987;&#27880;&#20110;&#23558;&#29616;&#26377;&#27169;&#22411;&#36716;&#25442;&#20026;EENN&#12290;&#23427;&#25191;&#34892;&#20102;&#37096;&#32626;&#21040;&#24322;&#26500;&#25110;&#20998;&#24067;&#24335;&#30828;&#20214;&#30446;&#26631;&#25152;&#38656;&#30340;&#25152;&#26377;&#35774;&#35745;&#20915;&#31574;&#65306;&#25105;&#20204;&#30340;&#26694;&#26550;&#26500;&#24314;&#20102;EENN&#26550;&#26500;&#65292;&#23558;&#20854;&#23376;&#22270;&#26144;&#23556;&#21040;&#30828;&#20214;&#30446;&#26631;&#65292;&#24182;&#37197;&#32622;&#20102;&#20854;&#20915;&#31574;&#26426;&#21046;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#25191;&#34892;&#25152;&#26377;&#36825;&#20123;&#27493;&#39588;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#29289;&#32852;&#32593;&#21644;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#29992;&#20363;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;&#35821;&#38899;&#21629;&#20196;&#26816;&#27979;&#20219;&#21153;&#65292;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#33021;&#22815;&#23558;&#27599;&#27425;&#25512;&#26029;&#30340;&#24179;&#22343;&#25805;&#20316;&#20943;&#23569;&#20102;59.67%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07957v1 Announce Type: cross  Abstract: Early Exit Neural Networks (EENNs) present a solution to enhance the efficiency of neural network deployments. However, creating EENNs is challenging and requires specialized domain knowledge, due to the large amount of additional design choices. To address this issue, we propose an automated augmentation flow that focuses on converting an existing model into an EENN. It performs all required design decisions for the deployment to heterogeneous or distributed hardware targets: Our framework constructs the EENN architecture, maps its subgraphs to the hardware targets, and configures its decision mechanism. To the best of our knowledge, it is the first framework that is able to perform all of these steps.   We evaluated our approach on a collection of Internet-of-Things and standard image classification use cases. For a speech command detection task, our solution was able to reduce the mean operations per inference by 59.67%. For an ECG 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CDCL&#31639;&#27861;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26694;&#26550;DeepCDCL&#65292;&#36890;&#36807;&#24341;&#20837;&#24322;&#27493;&#23376;&#21477;&#23398;&#20064;&#21644;&#31649;&#29702;&#32467;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#26102;&#38388;&#28040;&#32791;&#65292;&#24182;&#22312;ACAS Xu&#21644;MNIST&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;</title><link>https://arxiv.org/abs/2403.07956</link><description>&lt;p&gt;
DeepCDCL: &#22522;&#20110;CDCL&#31639;&#27861;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
DeepCDCL: An CDCL-based Neural Network Verification Framework
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07956
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CDCL&#31639;&#27861;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26694;&#26550;DeepCDCL&#65292;&#36890;&#36807;&#24341;&#20837;&#24322;&#27493;&#23376;&#21477;&#23398;&#20064;&#21644;&#31649;&#29702;&#32467;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#26102;&#38388;&#28040;&#32791;&#65292;&#24182;&#22312;ACAS Xu&#21644;MNIST&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#38754;&#20020;&#36234;&#26469;&#36234;&#22810;&#30340;&#23433;&#20840;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#65292;&#36825;&#26159;&#30001;&#20110;&#23427;&#20204;&#23545;&#24494;&#23567;&#25200;&#21160;&#30340;&#25935;&#24863;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DeepCDCL&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#20914;&#31361;&#39537;&#21160;&#23376;&#21477;&#23398;&#20064;&#65288;CDCL&#65289;&#31639;&#27861;&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#26694;&#26550;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#24322;&#27493;&#23376;&#21477;&#23398;&#20064;&#21644;&#31649;&#29702;&#32467;&#26500;&#65292;&#30456;&#27604;&#30452;&#25509;&#24212;&#29992;CDCL&#26694;&#26550;&#65292;&#20943;&#23569;&#20102;&#20887;&#20313;&#30340;&#26102;&#38388;&#28040;&#32791;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;ACAS Xu&#21644;MNIST&#25968;&#25454;&#38598;&#19978;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#35814;&#32454;&#35780;&#20272;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07956v1 Announce Type: cross  Abstract: Neural networks in safety-critical applications face increasing safety and security concerns due to their susceptibility to little disturbance. In this paper, we propose DeepCDCL, a novel neural network verification framework based on the Conflict-Driven Clause Learning (CDCL) algorithm. We introduce an asynchronous clause learning and management structure, reducing redundant time consumption compared to the direct application of the CDCL framework. Furthermore, we also provide a detailed evaluation of the performance of our approach on the ACAS Xu and MNIST datasets, showing that a significant speed-up is achieved in most cases.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Shortcuts-fused Selective Rationalization (SSR)&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#21033;&#29992;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#26469;&#25552;&#21319;&#29702;&#24615;&#21270;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#32531;&#35299;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#26469;&#32452;&#25104;&#29702;&#24615;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#34917;&#20805;&#24050;&#27880;&#37322;&#29702;&#24615;&#21270;&#25968;&#37327;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07955</link><description>&lt;p&gt;
&#26397;&#30528;&#24544;&#23454;&#35299;&#37322;&#65306;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#21457;&#29616;&#26469;&#22686;&#24378;&#29702;&#24615;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards Faithful Explanations: Boosting Rationalization with Shortcuts Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07955
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Shortcuts-fused Selective Rationalization (SSR)&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#21033;&#29992;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#26469;&#25552;&#21319;&#29702;&#24615;&#21270;&#65292;&#24182;&#36890;&#36807;&#20004;&#31181;&#31574;&#30053;&#32531;&#35299;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#26469;&#32452;&#25104;&#29702;&#24615;&#21270;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#34917;&#20805;&#24050;&#27880;&#37322;&#29702;&#24615;&#21270;&#25968;&#37327;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#31070;&#32463;&#32593;&#32476;&#30340;&#26174;&#33879;&#25104;&#21151;&#24341;&#21457;&#20102;&#26377;&#36873;&#25321;&#24615;&#30340;&#29702;&#24615;&#21270;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;Shortcuts-fused Selective Rationalization (SSR)&#26041;&#27861;&#65292;&#36890;&#36807;&#21457;&#29616;&#21644;&#21033;&#29992;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#26469;&#25552;&#21319;&#29702;&#24615;&#21270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SSR&#39318;&#20808;&#35774;&#35745;&#20102;&#19968;&#31181;&#24555;&#25463;&#26041;&#24335;&#21457;&#29616;&#26041;&#27861;&#26469;&#26816;&#27979;&#20960;&#20010;&#28508;&#22312;&#30340;&#24555;&#25463;&#26041;&#24335;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#24341;&#20837;&#35782;&#21035;&#20986;&#30340;&#24555;&#25463;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#26469;&#32531;&#35299;&#21033;&#29992;&#24555;&#25463;&#26041;&#24335;&#26469;&#32452;&#25104;&#29702;&#24615;&#21270;&#30340;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#20004;&#31181;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#26469;&#24357;&#34917;&#24050;&#27880;&#37322;&#29702;&#24615;&#21270;&#25968;&#37327;&#30340;&#24046;&#36317;&#12290;&#23545;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#32467;&#26524;&#28165;&#26970;&#22320;&#39564;&#35777;&#20102;&#36825;&#19968;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07955v1 Announce Type: cross  Abstract: The remarkable success in neural networks provokes the selective rationalization. It explains the prediction results by identifying a small subset of the inputs sufficient to support them. Since existing methods still suffer from adopting the shortcuts in data to compose rationales and limited large-scale annotated rationales by human, in this paper, we propose a Shortcuts-fused Selective Rationalization (SSR) method, which boosts the rationalization by discovering and exploiting potential shortcuts. Specifically, SSR first designs a shortcuts discovery approach to detect several potential shortcuts. Then, by introducing the identified shortcuts, we propose two strategies to mitigate the problem of utilizing shortcuts to compose rationales. Finally, we develop two data augmentations methods to close the gap in the number of annotated rationales. Extensive experimental results on real-world datasets clearly validate the effectiveness of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#35299;&#24352;&#37327;&#36827;&#19968;&#27493;&#25277;&#35937;&#31232;&#30095;DNN&#21152;&#36895;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#25104;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.07953</link><description>&lt;p&gt;
&#36890;&#36807;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#20998;&#35299;&#23545;&#31232;&#30095;DNN&#21152;&#36895;&#36827;&#34892;&#25277;&#35937;&#21270;
&lt;/p&gt;
&lt;p&gt;
Abstracting Sparse DNN Acceleration via Structured Sparse Tensor Decomposition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#21270;&#20998;&#35299;&#24352;&#37327;&#36827;&#19968;&#27493;&#25277;&#35937;&#31232;&#30095;DNN&#21152;&#36895;&#30340;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23558;&#31232;&#30095;&#24352;&#37327;&#36716;&#25442;&#25104;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#65292;&#20174;&#32780;&#24357;&#21512;&#20102;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#20013;&#21033;&#29992;&#31232;&#30095;&#24615;&#24050;&#25104;&#20026;&#28385;&#36275;&#29616;&#20195;DNN&#26085;&#30410;&#22686;&#38271;&#30340;&#35745;&#31639;&#38656;&#27714;&#30340;&#19968;&#31181;&#20855;&#26377;&#21069;&#26223;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#31232;&#30095;DNN&#21152;&#36895;&#20173;&#28982;&#38754;&#20020;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#12290;&#20026;&#20102;&#26368;&#23567;&#21270;&#31232;&#30095;&#21152;&#36895;&#30340;&#24320;&#38144;&#65292;&#30828;&#20214;&#35774;&#35745;&#24072;&#26368;&#36817;&#25552;&#20986;&#20102;&#32467;&#26500;&#21270;&#31232;&#30095;&#30828;&#20214;&#25903;&#25345;&#65292;&#36825;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#28789;&#27963;&#24615;&#24182;&#38656;&#35201;&#39069;&#22806;&#30340;&#27169;&#22411;&#24494;&#35843;&#12290;&#27492;&#22806;&#65292;&#20026;&#26576;&#20123;&#32467;&#26500;&#21270;&#31232;&#30095;&#30828;&#20214;&#24494;&#35843;&#30340;&#20219;&#20309;&#31232;&#30095;&#27169;&#22411;&#26080;&#27861;&#34987;&#20854;&#20182;&#32467;&#26500;&#21270;&#30828;&#20214;&#21152;&#36895;&#12290;&#20026;&#20102;&#24357;&#21512;&#31232;&#30095;DNN&#27169;&#22411;&#21644;&#30828;&#20214;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#32467;&#26500;&#20998;&#35299;&#30340;&#24352;&#37327;&#36817;&#20284;&#65288;TASD&#65289;&#65292;&#21033;&#29992;&#20102;&#32447;&#24615;&#20195;&#25968;&#20013;&#30340;&#20998;&#37197;&#24615;&#36136;&#23558;&#20219;&#20309;&#31232;&#30095;&#24352;&#37327;&#36716;&#21270;&#20026;&#19968;&#31995;&#21015;&#32467;&#26500;&#21270;&#31232;&#30095;&#24352;&#37327;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#36719;&#20214;&#26694;&#26550;TASDER&#65292;&#36890;&#36807;&#25628;&#32034;&#36880;&#23618;&#39640;&#36136;&#37327;&#30340;&#32467;&#26500;&#21270;&#20998;&#35299;&#26469;&#21152;&#36895;DNNs&#30340;&#26435;&#37325;&#21644;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07953v1 Announce Type: cross  Abstract: Exploiting sparsity in deep neural networks (DNNs) has been a promising area to meet the growing computation need of modern DNNs. However, in practice, sparse DNN acceleration still faces a key challenge. To minimize the overhead of sparse acceleration, hardware designers have proposed structured sparse hardware support recently, which provides limited flexibility and requires extra model fine-tuning. Moreover, any sparse model fine-tuned for certain structured sparse hardware cannot be accelerated by other structured hardware. To bridge the gap between sparse DNN models and hardware, this paper proposes tensor approximation via structured decomposition (TASD), which leverages the distributive property in linear algebra to turn any sparse tensor into a series of structured sparse tensors. Next, we develop a software framework, TASDER, to accelerate DNNs by searching layer-wise, high-quality structured decomposition for both weight and 
&lt;/p&gt;</description></item><item><title>AesopAgent&#26159;&#19968;&#31181;&#22522;&#20110;Agent&#30340;&#25925;&#20107;&#21040;&#35270;&#39057;&#21046;&#20316;&#31995;&#32479;&#65292;&#33021;&#23558;&#29992;&#25143;&#25925;&#20107;&#25552;&#26696;&#36716;&#21270;&#20026;&#21095;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#39057;&#65292;&#24182;&#23558;&#36825;&#20123;&#20869;&#23481;&#38598;&#25104;&#20026;&#35270;&#39057;&#65292;&#21516;&#26102;&#30830;&#20445;&#29983;&#25104;&#30340;&#35270;&#39057;&#20869;&#23481;&#20016;&#23500;&#19988;&#36830;&#36143;&#12290;</title><link>https://arxiv.org/abs/2403.07952</link><description>&lt;p&gt;
AesopAgent: &#22522;&#20110;Agent&#30340;&#25925;&#20107;&#21040;&#35270;&#39057;&#21046;&#20316;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AesopAgent: Agent-driven Evolutionary System on Story-to-Video Production
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07952
&lt;/p&gt;
&lt;p&gt;
AesopAgent&#26159;&#19968;&#31181;&#22522;&#20110;Agent&#30340;&#25925;&#20107;&#21040;&#35270;&#39057;&#21046;&#20316;&#31995;&#32479;&#65292;&#33021;&#23558;&#29992;&#25143;&#25925;&#20107;&#25552;&#26696;&#36716;&#21270;&#20026;&#21095;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#39057;&#65292;&#24182;&#23558;&#36825;&#20123;&#20869;&#23481;&#38598;&#25104;&#20026;&#35270;&#39057;&#65292;&#21516;&#26102;&#30830;&#20445;&#29983;&#25104;&#30340;&#35270;&#39057;&#20869;&#23481;&#20016;&#23500;&#19988;&#36830;&#36143;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Agent&#21644;AIGC&#65288;Artificial Intelligence Generated Content&#65289;&#25216;&#26415;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;AesopAgent&#65292;&#19968;&#31181;&#22522;&#20110;Agent&#30340;&#25925;&#20107;&#21040;&#35270;&#39057;&#21046;&#20316;&#28436;&#36827;&#31995;&#32479;&#12290;AesopAgent&#26159;Agent&#25216;&#26415;&#22312;&#22810;&#27169;&#24577;&#20869;&#23481;&#29983;&#25104;&#26041;&#38754;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#35813;&#31995;&#32479;&#22312;&#32479;&#19968;&#26694;&#26550;&#20869;&#38598;&#25104;&#20102;&#22810;&#31181;&#29983;&#25104;&#33021;&#21147;&#65292;&#20351;&#20010;&#20154;&#29992;&#25143;&#21487;&#20197;&#36731;&#26494;&#21033;&#29992;&#36825;&#20123;&#27169;&#22359;&#12290;&#36825;&#19968;&#21019;&#26032;&#31995;&#32479;&#23558;&#29992;&#25143;&#30340;&#25925;&#20107;&#25552;&#26696;&#36716;&#21270;&#20026;&#21095;&#26412;&#12289;&#22270;&#20687;&#21644;&#38899;&#39057;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#22810;&#27169;&#24577;&#20869;&#23481;&#38598;&#25104;&#21040;&#35270;&#39057;&#20013;&#12290;&#27492;&#22806;&#65292;&#21160;&#30011;&#21333;&#20803;&#65288;&#20363;&#22914;Gen-2&#21644;Sora&#65289;&#21487;&#20197;&#20351;&#35270;&#39057;&#26356;&#20855;&#24863;&#26579;&#21147;&#12290;AesopAgent&#31995;&#32479;&#21487;&#20197;&#21327;&#35843;&#35270;&#39057;&#29983;&#25104;&#30340;&#20219;&#21153;&#24037;&#20316;&#27969;&#31243;&#65292;&#30830;&#20445;&#29983;&#25104;&#30340;&#35270;&#39057;&#20869;&#23481;&#20016;&#23500;&#19988;&#36830;&#36143;&#12290;&#35813;&#31995;&#32479;&#20027;&#35201;&#21253;&#21547;&#20004;&#20010;&#23618;&#65292;&#21363;&#27700;&#24179;&#23618;&#21644;&#23454;&#29992;&#23618;&#12290;&#22312;&#27700;&#24179;&#23618;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07952v1 Announce Type: cross  Abstract: The Agent and AIGC (Artificial Intelligence Generated Content) technologies have recently made significant progress. We propose AesopAgent, an Agent-driven Evolutionary System on Story-to-Video Production. AesopAgent is a practical application of agent technology for multimodal content generation. The system integrates multiple generative capabilities within a unified framework, so that individual users can leverage these modules easily. This innovative system would convert user story proposals into scripts, images, and audio, and then integrate these multimodal contents into videos. Additionally, the animating units (e.g., Gen-2 and Sora) could make the videos more infectious. The AesopAgent system could orchestrate task workflow for video generation, ensuring that the generated video is both rich in content and coherent. This system mainly contains two layers, i.e., the Horizontal Layer and the Utility Layer. In the Horizontal Layer,
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#26412;&#35770;&#25991;&#23558;&#31639;&#27861;&#35270;&#35282;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#35748;&#35782;&#35770;&#65292;&#25361;&#25112;&#20256;&#32479;&#26041;&#27861;&#22312;&#20449;&#24687;&#19981;&#23436;&#22791;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#65292;&#20026;&#20010;&#20307;&#22914;&#20309;&#28436;&#21270;&#20854;&#20449;&#24565;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.07949</link><description>&lt;p&gt;
&#31639;&#27861;&#36125;&#21494;&#26031;&#35748;&#35782;&#35770;
&lt;/p&gt;
&lt;p&gt;
Algorithmic Bayesian Epistemology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07949
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#26412;&#35770;&#25991;&#23558;&#31639;&#27861;&#35270;&#35282;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#35748;&#35782;&#35770;&#65292;&#25361;&#25112;&#20256;&#32479;&#26041;&#27861;&#22312;&#20449;&#24687;&#19981;&#23436;&#22791;&#24773;&#20917;&#19979;&#30340;&#24212;&#29992;&#65292;&#20026;&#20010;&#20307;&#22914;&#20309;&#28436;&#21270;&#20854;&#20449;&#24565;&#25552;&#20379;&#20102;&#26032;&#30340;&#29702;&#35770;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35770;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#31639;&#27861;&#35270;&#35282;&#30340;&#19968;&#20010;&#26041;&#38754;&#26159;&#20851;&#27880;&#20854;&#20182;&#31185;&#23398;&#23398;&#31185;&#30340;&#35270;&#35282;&#65292;&#37325;&#28857;&#25918;&#22312;&#28385;&#36275;&#23454;&#38469;&#32422;&#26463;&#26465;&#20214;&#30340;&#20196;&#20154;&#28385;&#24847;&#30340;&#35299;&#20915;&#26041;&#26696;&#19978;&#65292;&#32780;&#19981;&#26159;&#24573;&#30053;&#36825;&#20123;&#32422;&#26463;&#26465;&#20214;&#30340;&#26368;&#20248;&#35299;&#20915;&#26041;&#26696;&#12290;&#31639;&#27861;&#35270;&#35282;&#20026;&#35768;&#22810;&#23398;&#26415;&#39046;&#22495;&#65292;&#21253;&#25324;&#20998;&#23376;&#29983;&#29289;&#23398;&#12289;&#29983;&#24577;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#12289;&#37327;&#23376;&#29289;&#29702;&#23398;&#12289;&#32463;&#27982;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#65292;&#25552;&#20379;&#20102;&#29420;&#29305;&#32780;&#37325;&#35201;&#30340;&#35270;&#35282;&#12290;&#26412;&#35770;&#25991;&#23558;&#31639;&#27861;&#35270;&#35282;&#24212;&#29992;&#20110;&#36125;&#21494;&#26031;&#35748;&#35782;&#35770;&#12290;&#20256;&#32479;&#30340;&#36125;&#21494;&#26031;&#35748;&#35782;&#35770;&#20026;&#20010;&#20307;&#22312;&#25509;&#25910;&#26032;&#20449;&#24687;&#21518;&#22914;&#20309;&#28436;&#21270;&#20854;&#20449;&#24565;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20551;&#23450;&#20102;&#36825;&#31181;&#20449;&#24687;&#30340;&#35814;&#23613;&#27169;&#22411;&#65292;&#21253;&#25324;&#19981;&#21516;&#35777;&#25454;&#20043;&#38388;&#30340;&#30456;&#20851;&#32467;&#26500;&#12290;&#20107;&#23454;&#19978;&#65292;&#20010;&#20307;&#21487;&#33021;&#32570;&#20047;&#36825;&#26679;&#30340;&#35814;&#23613;&#27169;&#22411;&#65292;&#20294;&#20173;&#38656;&#35201;&#24418;&#25104;&#20449;&#24565;&#12290;&#38500;&#27492;&#20043;&#22806;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07949v1 Announce Type: cross  Abstract: One aspect of the algorithmic lens in theoretical computer science is a view on other scientific disciplines that focuses on satisfactory solutions that adhere to real-world constraints, as opposed to solutions that would be optimal ignoring such constraints. The algorithmic lens has provided a unique and important perspective on many academic fields, including molecular biology, ecology, neuroscience, quantum physics, economics, and social science.   This thesis applies the algorithmic lens to Bayesian epistemology. Traditional Bayesian epistemology provides a comprehensive framework for how an individual's beliefs should evolve upon receiving new information. However, these methods typically assume an exhaustive model of such information, including the correlation structure between different pieces of evidence. In reality, individuals might lack such an exhaustive model, while still needing to form beliefs. Beyond such informational 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#39057;&#29983;&#25104;AI&#20195;&#29702;&#65292;&#36890;&#36807;&#32467;&#21512;&#25991;&#26412;&#25552;&#31034;&#21644;&#22270;&#20687;&#36755;&#20837;&#65292;&#21033;&#29992;Sora&#21551;&#21457;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#25216;&#26415;&#26500;&#24314;&#20016;&#23500;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35270;&#39057;&#29983;&#25104;&#20013;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#21160;&#20316;&#24179;&#28369;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.07944</link><description>&lt;p&gt;
WorldGPT&#65306;&#21463;Sora&#21551;&#21457;&#30340;&#35270;&#39057;AI&#20195;&#29702;&#20316;&#20026;&#25991;&#26412;&#21644;&#22270;&#20687;&#36755;&#20837;&#30340;&#20016;&#23500;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
WorldGPT: A Sora-Inspired Video AI Agent as Rich World Models from Text and Image Inputs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35270;&#39057;&#29983;&#25104;AI&#20195;&#29702;&#65292;&#36890;&#36807;&#32467;&#21512;&#25991;&#26412;&#25552;&#31034;&#21644;&#22270;&#20687;&#36755;&#20837;&#65292;&#21033;&#29992;Sora&#21551;&#21457;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#25216;&#26415;&#26500;&#24314;&#20016;&#23500;&#30340;&#19990;&#30028;&#27169;&#22411;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#35270;&#39057;&#29983;&#25104;&#20013;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#21644;&#21160;&#20316;&#24179;&#28369;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20960;&#31181;&#25991;&#26412;&#21040;&#35270;&#39057;&#25193;&#25955;&#27169;&#22411;&#23637;&#31034;&#20102;&#22312;&#21512;&#25104;&#39640;&#36136;&#37327;&#35270;&#39057;&#20869;&#23481;&#26041;&#38754;&#20986;&#33394;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20445;&#25345;&#29983;&#25104;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#19968;&#33268;&#24615;&#24182;&#30830;&#20445;&#21160;&#20316;&#24179;&#28369;&#20173;&#28982;&#26159;&#19968;&#20010;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#35270;&#39057;&#29983;&#25104;AI&#20195;&#29702;&#65292;&#21033;&#29992;&#21463;Sora&#21551;&#21457;&#30340;&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#21147;&#37327;&#26469;&#22522;&#20110;&#25991;&#26412;&#25552;&#31034;&#21644;&#38468;&#24102;&#22270;&#20687;&#26500;&#24314;&#29087;&#32451;&#30340;&#19990;&#30028;&#27169;&#22411;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#21253;&#25324;&#20004;&#37096;&#20998;&#65306;&#25552;&#31034;&#22686;&#24378;&#22120;&#21644;&#23436;&#25972;&#35270;&#39057;&#32763;&#35793;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07944v1 Announce Type: cross  Abstract: Several text-to-video diffusion models have demonstrated commendable capabilities in synthesizing high-quality video content. However, it remains a formidable challenge pertaining to maintaining temporal consistency and ensuring action smoothness throughout the generated sequences. In this paper, we present an innovative video generation AI agent that harnesses the power of Sora-inspired multimodal learning to build skilled world models framework based on textual prompts and accompanying images. The framework includes two parts: prompt enhancer and full video translation. The first part employs the capabilities of ChatGPT to meticulously distill and proactively construct precise prompts for each subsequent step, thereby guaranteeing the utmost accuracy in prompt communication and accurate execution in following model operations. The second part employ compatible with existing advanced diffusion techniques to expansively generate and re
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#19982;&#35270;&#39057;&#23545;&#40784;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#22522;&#20934;T2AV-Bench&#65292;&#20197;&#21450;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35270;&#39057;&#23545;&#40784;TTA&#29983;&#25104;&#27169;&#22411;T2AV&#65292;&#25913;&#36827;&#20102;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#34701;&#21512;&#20102;&#35270;&#35273;&#23545;&#40784;&#25991;&#26412;&#23884;&#20837;&#12290;</title><link>https://arxiv.org/abs/2403.07938</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#19982;&#35270;&#39057;&#21516;&#27493;
&lt;/p&gt;
&lt;p&gt;
Text-to-Audio Generation Synchronized with Videos
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07938
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#19982;&#35270;&#39057;&#23545;&#40784;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#22522;&#20934;T2AV-Bench&#65292;&#20197;&#21450;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35270;&#39057;&#23545;&#40784;TTA&#29983;&#25104;&#27169;&#22411;T2AV&#65292;&#25913;&#36827;&#20102;&#20256;&#32479;&#26041;&#27861;&#65292;&#24182;&#34701;&#21512;&#20102;&#35270;&#35273;&#23545;&#40784;&#25991;&#26412;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20154;&#20204;&#23545;&#25991;&#26412;&#21040;&#38899;&#39057;&#65288;TTA&#65289;&#29983;&#25104;&#30340;&#20851;&#27880;&#26085;&#30410;&#21152;&#24378;&#65292;&#30740;&#31350;&#20154;&#21592;&#21162;&#21147;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#21512;&#25104;&#38899;&#39057;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#34429;&#28982;&#21033;&#29992;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#38899;&#39057;&#21644;&#25991;&#26412;&#23884;&#20837;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20294;&#22312;&#20445;&#25345;&#29983;&#25104;&#30340;&#38899;&#39057;&#19982;&#20854;&#35270;&#39057;&#20043;&#38388;&#26080;&#32541;&#21516;&#27493;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#36825;&#32463;&#24120;&#23548;&#33268;&#21487;&#23519;&#35273;&#30340;&#38899;&#39057;-&#35270;&#35273;&#19981;&#21305;&#37197;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#19982;&#35270;&#39057;&#23545;&#40784;&#30340;&#25991;&#26412;&#21040;&#38899;&#39057;&#29983;&#25104;&#30340;&#24320;&#21019;&#24615;&#22522;&#20934;&#65292;&#21517;&#20026;T2AV-Bench&#12290;&#35813;&#22522;&#20934;&#36890;&#36807;&#19977;&#20010;&#19987;&#38376;&#29992;&#20110;&#35780;&#20272;&#35270;&#35273;&#23545;&#40784;&#21644;&#26102;&#38388;&#19968;&#33268;&#24615;&#30340;&#26032;&#39062;&#25351;&#26631;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#20026;&#20102;&#34917;&#20805;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#35270;&#39057;&#23545;&#40784;TTA&#29983;&#25104;&#27169;&#22411;&#65292;&#21363;T2AV&#12290;&#36229;&#36234;&#20256;&#32479;&#26041;&#27861;&#65292;T2AV&#36890;&#36807;&#23558;&#35270;&#35273;&#23545;&#40784;&#25991;&#26412;&#23884;&#20837;&#38598;&#25104;&#20026;&#20854;c
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07938v1 Announce Type: cross  Abstract: In recent times, the focus on text-to-audio (TTA) generation has intensified, as researchers strive to synthesize audio from textual descriptions. However, most existing methods, though leveraging latent diffusion models to learn the correlation between audio and text embeddings, fall short when it comes to maintaining a seamless synchronization between the produced audio and its video. This often results in discernible audio-visual mismatches. To bridge this gap, we introduce a groundbreaking benchmark for Text-to-Audio generation that aligns with Videos, named T2AV-Bench. This benchmark distinguishes itself with three novel metrics dedicated to evaluating visual alignment and temporal consistency. To complement this, we also present a simple yet effective video-aligned TTA generation model, namely T2AV. Moving beyond traditional methods, T2AV refines the latent diffusion approach by integrating visual-aligned text embeddings as its c
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#20551;&#21160;&#20316;&#30340;&#39318;&#27425;&#24418;&#24335;&#21270;&#12289;&#23454;&#29616;&#21644;&#23450;&#37327;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22870;&#21169;&#25910;&#30410;&#12289;&#22686;&#21152;&#28216;&#25103;&#22810;&#26679;&#24615;&#65292;&#19988;&#26102;&#38388;&#28040;&#32791;&#26041;&#38754;&#24320;&#38144;&#24456;&#23567;&#12290;</title><link>https://arxiv.org/abs/2403.07932</link><description>&lt;p&gt;
&#22810;&#20154;&#28216;&#25103;&#20013;&#30340;&#20551;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Feint in Multi-Player Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07932
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#20154;&#28216;&#25103;&#20013;&#20551;&#21160;&#20316;&#30340;&#39318;&#27425;&#24418;&#24335;&#21270;&#12289;&#23454;&#29616;&#21644;&#23450;&#37327;&#35780;&#20272;&#65292;&#24182;&#35777;&#26126;&#20854;&#33021;&#22815;&#26174;&#33879;&#25552;&#39640;&#22870;&#21169;&#25910;&#30410;&#12289;&#22686;&#21152;&#28216;&#25103;&#22810;&#26679;&#24615;&#65292;&#19988;&#26102;&#38388;&#28040;&#32791;&#26041;&#38754;&#24320;&#38144;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#23545;&#22810;&#20154;&#28216;&#25103;&#20013;&#30340;&#20551;&#21160;&#20316;&#36827;&#34892;&#20102;&#39318;&#27425;&#24418;&#24335;&#21270;&#12289;&#23454;&#29616;&#21644;&#23450;&#37327;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#22810;&#20154;&#28216;&#25103;&#30340;&#35282;&#24230;&#23545;&#20551;&#21160;&#20316;&#36827;&#34892;&#20102;&#24418;&#24335;&#21270;&#65292;&#28041;&#21450;&#21040;&#26102;&#38388;&#12289;&#31354;&#38388;&#21644;&#23427;&#20204;&#30340;&#38598;&#20307;&#24433;&#21709;&#12290;&#35813;&#24418;&#24335;&#21270;&#24314;&#31435;&#22312;&#38750;&#20256;&#36882;&#24615;&#20027;&#21160;&#39532;&#23572;&#21487;&#22827;&#28216;&#25103;&#27169;&#22411;&#20043;&#19978;&#65292;&#20854;&#20013;&#20551;&#21160;&#20316;&#33021;&#22815;&#20135;&#29983;&#21487;&#35266;&#30340;&#24433;&#21709;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#22810;&#20195;&#29702;&#24314;&#27169;&#30340;&#26368;&#26032;&#36827;&#23637;&#19979;&#65288;&#21363;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65289;&#22312;&#22810;&#20154;&#28216;&#25103;&#20013;&#23454;&#26045;&#20551;&#21160;&#20316;&#30340;&#23454;&#38469;&#32454;&#33410;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23450;&#37327;&#26816;&#39564;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;&#26377;&#25928;&#24615;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#20551;&#21160;&#20316;&#35774;&#35745;&#21487;&#20197;&#65288;1&#65289;&#26174;&#33879;&#25552;&#39640;&#28216;&#25103;&#20013;&#30340;&#22870;&#21169;&#25910;&#30410;&#65307;&#65288;2&#65289;&#26174;&#33879;&#25552;&#39640;&#22810;&#20154;&#28216;&#25103;&#30340;&#22810;&#26679;&#24615;&#65307;&#20197;&#21450;&#65288;3&#65289;&#20165;&#22312;&#26102;&#38388;&#28040;&#32791;&#26041;&#38754;&#20135;&#29983;&#21487;&#24573;&#30053;&#30340;&#24320;&#38144;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#25105;&#20204;&#30340;&#20551;&#21160;&#20316;&#35774;&#35745;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07932v1 Announce Type: cross  Abstract: This paper introduces the first formalization, implementation and quantitative evaluation of Feint in Multi-Player Games. Our work first formalizes Feint from the perspective of Multi-Player Games, in terms of the temporal, spatial, and their collective impacts. The formalization is built upon Non-transitive Active Markov Game Model, where Feint can have a considerable amount of impacts. Then, our work considers practical implementation details of Feint in Multi-Player Games, under the state-of-the-art progress of multi-agent modeling to date (namely Multi-Agent Reinforcement Learning). Finally, our work quantitatively examines the effectiveness of our design, and the results show that our design of Feint can (1) greatly improve the reward gains from the game; (2) significantly improve the diversity of Multi-Player Games; and (3) only incur negligible overheads in terms of time consumption. We conclude that our design of Feint is effec
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36523;&#20221;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#36890;&#36807;&#36523;&#20221;&#35270;&#35282;&#20419;&#36827;AI&#22810;&#26679;&#24615;&#22312;&#21019;&#36896;&#32773;&#12289;&#21019;&#20316;&#29289;&#21644;&#21518;&#26524;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07924</link><description>&lt;p&gt;
AI&#36523;&#20221;&#30340;&#24847;&#20041;&#65306;&#21019;&#36896;&#32773;&#12289;&#21019;&#20316;&#29289;&#21644;&#21518;&#26524;&#30340;&#21547;&#20041;
&lt;/p&gt;
&lt;p&gt;
Implications of Identity of AI: Creators, Creations, and Consequences
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07924
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36523;&#20221;&#38382;&#39064;&#65292;&#24378;&#35843;&#20102;&#36890;&#36807;&#36523;&#20221;&#35270;&#35282;&#20419;&#36827;AI&#22810;&#26679;&#24615;&#22312;&#21019;&#36896;&#32773;&#12289;&#21019;&#20316;&#29289;&#21644;&#21518;&#26524;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#39046;&#22495;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#20855;&#26377;&#26174;&#33879;&#28508;&#21147;&#25913;&#21464;&#31038;&#20250;&#12290;&#28982;&#32780;&#65292;&#23427;&#38754;&#20020;&#30528;&#19968;&#20010;&#26126;&#26174;&#30340;&#25361;&#25112;&#65306;&#32570;&#20047;&#22810;&#26679;&#24615;&#65292;&#36825;&#26159;STEM&#39046;&#22495;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#26412;&#31435;&#22330;&#35770;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#36523;&#20221;&#30340;&#20132;&#21449;&#28857;&#65292;&#20316;&#20026;&#29702;&#35299;AI&#21457;&#23637;&#21644;&#24212;&#29992;&#20013;&#30340;&#20559;&#35265;&#12289;&#19981;&#24179;&#31561;&#21644;&#36947;&#24503;&#32771;&#34385;&#30340;&#36884;&#24452;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#32500;&#24230;&#30340;AI&#36523;&#20221;&#23450;&#20041;&#65292;&#28085;&#30422;&#20854;&#21019;&#36896;&#32773;&#12289;&#24212;&#29992;&#20197;&#21450;&#23427;&#20204;&#30340;&#26356;&#24191;&#27867;&#24433;&#21709;&#12290;&#29702;&#35299;AI&#30340;&#36523;&#20221;&#28041;&#21450;&#20998;&#26512;&#21442;&#19982;AI&#21457;&#23637;&#30340;&#21508;&#31181;&#20010;&#20307;&#12289;&#25152;&#20135;&#29983;&#30340;&#25216;&#26415;&#20197;&#21450;&#31038;&#20250;&#12289;&#36947;&#24503;&#21644;&#24515;&#29702;&#24433;&#21709;&#12290;&#22312;&#25506;&#35752;AI&#36523;&#20221;&#29983;&#24577;&#31995;&#32479;&#21450;&#20854;&#31038;&#20250;&#21160;&#24577;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#24378;&#35843;&#20102;&#36890;&#36807;&#36523;&#20221;&#35270;&#35282;&#36328;&#19977;&#20010;&#32500;&#24230;&#8212;&#8212;&#21019;&#36896;&#32773;&#12289;&#21019;&#20316;&#29289;&#21644;&#21518;&#26524;&#65292;&#22312;AI&#20013;&#38656;&#35201;&#22810;&#26679;&#24615;&#30340;&#24517;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07924v1 Announce Type: cross  Abstract: The field of Artificial Intelligence (AI) is rapidly advancing, with significant potential to transform society. However, it faces a notable challenge: lack of diversity, a longstanding issue in STEM fields. In this context, This position paper examines the intersection of AI and identity as a pathway to understand biases, inequalities, and ethical considerations in AI development and deployment. We present a multifaceted definition of AI identity, which encompasses its creators, applications, and their broader impacts. Understanding AI's identity involves analyzing the diverse individuals involved in AI's development, the technologies produced, and the social, ethical, and psychological implications. After exploring the AI identity ecosystem and its societal dynamics, We propose a framework that highlights the need for diversity in AI across three dimensions: Creators, Creations, and Consequences through the lens of identity. This pap
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#36793;&#32536;&#35745;&#31639;&#30340;&#20248;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#20113;&#36793;&#21327;&#21516;&#21644;&#21160;&#24577;&#36164;&#28304;&#20998;&#37197;&#23454;&#29616;&#24037;&#19994;&#30446;&#26631;&#30340;&#30417;&#25511;&#21644;&#20248;&#21270;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#33410;&#30465;&#20102;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.07923</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#19982;&#36793;&#32536;&#35745;&#31639;&#22312;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#30340;&#23454;&#26102;&#30417;&#25511;&#19982;&#25511;&#21046;&#20248;&#21270;&#34701;&#21512;
&lt;/p&gt;
&lt;p&gt;
The Fusion of Deep Reinforcement Learning and Edge Computing for Real-time Monitoring and Control Optimization in IoT Environments
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#36793;&#32536;&#35745;&#31639;&#30340;&#20248;&#21270;&#25511;&#21046;&#31995;&#32479;&#65292;&#36890;&#36807;&#20113;&#36793;&#21327;&#21516;&#21644;&#21160;&#24577;&#36164;&#28304;&#20998;&#37197;&#23454;&#29616;&#24037;&#19994;&#30446;&#26631;&#30340;&#30417;&#25511;&#21644;&#20248;&#21270;&#65292;&#26174;&#33879;&#25552;&#21319;&#20102;&#31995;&#32479;&#24615;&#33021;&#65292;&#24182;&#33410;&#30465;&#20102;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#24037;&#19994;&#29289;&#32852;&#32593;&#29615;&#22659;&#20013;&#23545;&#23454;&#26102;&#24615;&#33021;&#21644;&#25511;&#21046;&#36136;&#37327;&#30340;&#38656;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#36793;&#32536;&#35745;&#31639;&#30340;&#20248;&#21270;&#25511;&#21046;&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#21033;&#29992;&#20113;&#36793;&#21327;&#21516;&#65292;&#37096;&#32626;&#36731;&#37327;&#32423;&#31574;&#30053;&#32593;&#32476;&#22312;&#36793;&#32536;&#65292;&#39044;&#27979;&#31995;&#32479;&#29366;&#24577;&#65292;&#24182;&#20197;&#39640;&#39057;&#29575;&#36755;&#20986;&#25511;&#21046;&#65292;&#23454;&#29616;&#24037;&#19994;&#30446;&#26631;&#30340;&#30417;&#25511;&#21644;&#20248;&#21270;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#21160;&#24577;&#36164;&#28304;&#20998;&#37197;&#26426;&#21046;&#65292;&#20197;&#30830;&#20445;&#36793;&#32536;&#35745;&#31639;&#36164;&#28304;&#30340;&#21512;&#29702;&#35843;&#24230;&#65292;&#23454;&#29616;&#20840;&#23616;&#20248;&#21270;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#20943;&#23569;&#20102;&#20113;&#36793;&#36890;&#20449;&#24310;&#36831;&#65292;&#21152;&#24555;&#20102;&#23545;&#24322;&#24120;&#24773;&#20917;&#30340;&#21709;&#24212;&#65292;&#38477;&#20302;&#20102;&#31995;&#32479;&#25925;&#38556;&#29575;&#65292;&#24310;&#38271;&#20102;&#35774;&#22791;&#24179;&#22343;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#33410;&#30465;&#20102;&#25163;&#21160;&#32500;&#25252;&#21644;&#26356;&#25442;&#25104;&#26412;&#12290;&#36825;&#30830;&#20445;&#20102;&#23454;&#26102;&#21644;&#31283;&#23450;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07923v1 Announce Type: cross  Abstract: In response to the demand for real-time performance and control quality in industrial Internet of Things (IoT) environments, this paper proposes an optimization control system based on deep reinforcement learning and edge computing. The system leverages cloud-edge collaboration, deploys lightweight policy networks at the edge, predicts system states, and outputs controls at a high frequency, enabling monitoring and optimization of industrial objectives. Additionally, a dynamic resource allocation mechanism is designed to ensure rational scheduling of edge computing resources, achieving global optimization. Results demonstrate that this approach reduces cloud-edge communication latency, accelerates response to abnormal situations, reduces system failure rates, extends average equipment operating time, and saves costs for manual maintenance and replacement. This ensures real-time and stable control.
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#26469;&#22312;&#35745;&#31639;&#39044;&#31639;&#20869;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;MeRino&#27169;&#22411;&#65292;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23637;&#29616;&#20986;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#31454;&#20105;&#24615;&#33021;&#30340;&#29305;&#28857;</title><link>https://arxiv.org/abs/2403.07921</link><description>&lt;p&gt;
Merino&#65306;&#22522;&#20110;&#29109;&#39537;&#21160;&#30340;IoT&#35774;&#22791;&#19978;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Merino: Entropy-driven Design for Generative Language Models on IoT Devices
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07921
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#26469;&#22312;&#35745;&#31639;&#39044;&#31639;&#20869;&#65292;&#25104;&#21151;&#35774;&#35745;&#20102;MeRino&#27169;&#22411;&#65292;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23637;&#29616;&#20986;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#31454;&#20105;&#24615;&#33021;&#30340;&#29305;&#28857;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#29616;&#20195;&#26102;&#20195;&#30340;&#38761;&#21629;&#24615;&#36827;&#27493;&#65292;&#28982;&#32780;&#65292;&#30452;&#25509;&#37096;&#32626;LLMs&#22312;&#36164;&#28304;&#21463;&#38480;&#30340;&#30828;&#20214;&#19978;&#65292;&#27604;&#22914;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#35774;&#22791;&#65292;&#30001;&#20110;&#20854;&#39640;&#35745;&#31639;&#25104;&#26412;&#32780;&#21464;&#24471;&#22256;&#38590;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20449;&#24687;&#29109;&#26694;&#26550;&#65292;&#29992;&#20110;&#35774;&#35745;&#25163;&#26426;&#21451;&#22909;&#30340;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35774;&#35745;&#33539;&#24335;&#26159;&#22312;&#32473;&#23450;&#30340;&#35745;&#31639;&#39044;&#31639;&#20869;&#26368;&#22823;&#21270;transformer&#35299;&#30721;&#22120;&#30340;&#29109;&#12290;&#25972;&#20010;&#35774;&#35745;&#36807;&#31243;&#28041;&#21450;&#35299;&#20915;&#19968;&#20010;&#25968;&#23398;&#35268;&#21010;&#65288;MP&#65289;&#38382;&#39064;&#65292;&#21487;&#20197;&#22312;&#20960;&#20998;&#38047;&#20869;&#22312;CPU&#19978;&#23436;&#25104;&#65292;&#20351;&#20854;&#20960;&#20046;&#26159;&#38646;&#25104;&#26412;&#30340;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;&#27169;&#22411;MeRino&#65292;&#22312;&#20061;&#20010;NLP&#19979;&#28216;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#23545;&#25239;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#33258;&#22238;&#24402;transformer&#27169;&#22411;&#30340;&#31454;&#20105;&#24615;&#34920;&#29616;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;MeRino&#22312;&#31227;&#21160;&#35774;&#32622;&#19979;&#33719;&#24471;&#20102;&#31867;&#20284;&#25110;&#26356;&#22909;&#30340;&#38646;&#24615;&#33021;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07921v1 Announce Type: cross  Abstract: Generative Large Language Models (LLMs) stand as a revolutionary advancement in the modern era of artificial intelligence (AI). However, directly deploying LLMs in resource-constrained hardware, such as Internet-of-Things (IoT) devices, is difficult due to their high computational cost. In this paper, we propose a novel information-entropy framework for designing mobile-friendly generative language models. Our key design paradigm is to maximize the entropy of transformer decoders within the given computational budgets. The whole design procedure involves solving a mathematical programming (MP) problem, which can be done on the CPU within minutes, making it nearly zero-cost. We evaluate our designed models, termed MeRino, across nine NLP downstream tasks, showing their competitive performance against the state-of-the-art autoregressive transformer models under the mobile setting. Notably, MeRino achieves similar or better zero performan
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ProtLLM&#65292;&#19968;&#31181;&#20855;&#26377;&#29420;&#29305;&#21160;&#24577;&#34507;&#30333;&#36136;&#35013;&#37197;&#26426;&#21046;&#21450;&#34507;&#30333;&#36136;&#20316;&#20026;&#21333;&#35789;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#35821;&#35328;LLM&#65292;&#24182;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;</title><link>https://arxiv.org/abs/2403.07920</link><description>&lt;p&gt;
ProtLLM&#65306;&#19968;&#31181;&#20855;&#26377;&#34507;&#30333;&#36136;&#20316;&#20026;&#21333;&#35789;&#39044;&#35757;&#32451;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#35821;&#35328;LLM
&lt;/p&gt;
&lt;p&gt;
ProtLLM: An Interleaved Protein-Language LLM with Protein-as-Word Pre-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07920
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ProtLLM&#65292;&#19968;&#31181;&#20855;&#26377;&#29420;&#29305;&#21160;&#24577;&#34507;&#30333;&#36136;&#35013;&#37197;&#26426;&#21046;&#21450;&#34507;&#30333;&#36136;&#20316;&#20026;&#21333;&#35789;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#35821;&#35328;LLM&#65292;&#24182;&#26500;&#24314;&#20102;&#22823;&#35268;&#27169;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#25991;&#26412;&#25968;&#25454;&#38598;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;ProtLLM&#65292;&#19968;&#31181;&#22810;&#21151;&#33021;&#30340;&#36328;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#29992;&#20110;&#22788;&#29702;&#26082;&#26377;&#34507;&#30333;&#36136;&#20026;&#20013;&#24515;&#21448;&#26377;&#34507;&#30333;&#36136;-&#35821;&#35328;&#20219;&#21153;&#12290;ProtLLM&#20855;&#26377;&#29420;&#29305;&#30340;&#21160;&#24577;&#34507;&#30333;&#36136;&#35013;&#37197;&#26426;&#21046;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#19982;&#20219;&#24847;&#25968;&#37327;&#30340;&#34507;&#30333;&#36136;&#20132;&#32455;&#22312;&#19968;&#36215;&#30340;&#22797;&#26434;&#36755;&#20837;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34507;&#30333;&#36136;&#20316;&#20026;&#21333;&#35789;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#26469;&#35757;&#32451;ProtLLM&#12290;&#36890;&#36807;&#24320;&#21457;&#19987;&#38376;&#30340;&#34507;&#30333;&#36136;&#35789;&#27719;&#34920;&#65292;&#25105;&#20204;&#36171;&#20104;&#35813;&#27169;&#22411;&#19981;&#20165;&#39044;&#27979;&#33258;&#28982;&#35821;&#35328;&#32780;&#19988;&#39044;&#27979;&#26469;&#33258;&#22823;&#37327;&#20505;&#36873;&#34507;&#30333;&#36136;&#30340;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#20132;&#32455;&#24335;&#34507;&#30333;&#36136;-&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;InterPT&#65292;&#29992;&#20110;&#39044;&#35757;&#32451;&#12290;&#35813;&#25968;&#25454;&#38598;&#20840;&#38754;&#28085;&#30422;&#20102;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65288;&#22914;&#34507;&#30333;&#36136;&#27880;&#37322;&#65289;&#21644;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#65288;&#22914;&#29983;&#29289;&#30740;&#31350;&#35770;&#25991;&#65289;&#65292;&#20174;&#32780;&#36171;&#20104;ProtLLM&#29702;&#35299;&#34507;&#30333;&#36136;&#30340;&#20851;&#38190;&#30693;&#35782;&#12290;&#25105;&#20204;&#22312;&#32463;&#20856;&#25968;&#25454;&#38598;&#19978;&#23545;ProtLLM&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07920v1 Announce Type: cross  Abstract: We propose ProtLLM, a versatile cross-modal large language model (LLM) for both protein-centric and protein-language tasks. ProtLLM features a unique dynamic protein mounting mechanism, enabling it to handle complex inputs where the natural language text is interspersed with an arbitrary number of proteins. Besides, we propose the protein-as-word language modeling approach to train ProtLLM. By developing a specialized protein vocabulary, we equip the model with the capability to predict not just natural language but also proteins from a vast pool of candidates. Additionally, we construct a large-scale interleaved protein-text dataset, named InterPT, for pre-training. This dataset comprehensively encompasses both (1) structured data sources like protein annotations and (2) unstructured data sources like biological research papers, thereby endowing ProtLLM with crucial knowledge for understanding proteins. We evaluate ProtLLM on classic 
&lt;/p&gt;</description></item><item><title>&#24320;&#25918;&#22522;&#37329;&#20250;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#21487;&#29992;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#24102;&#26469;&#20102;&#37325;&#22823;&#21033;&#30410;&#65292;&#20294;&#20063;&#23384;&#22312;&#36793;&#38469;&#39118;&#38505;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#26469;&#35780;&#20272;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07918</link><description>&lt;p&gt;
&#35770;&#24320;&#25918;&#22522;&#37329;&#20250;&#27169;&#22411;&#30340;&#31038;&#20250;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the Societal Impact of Open Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07918
&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#22522;&#37329;&#20250;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#21487;&#29992;&#30340;&#27169;&#22411;&#26435;&#37325;&#65292;&#24102;&#26469;&#20102;&#37325;&#22823;&#21033;&#30410;&#65292;&#20294;&#20063;&#23384;&#22312;&#36793;&#38469;&#39118;&#38505;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#26469;&#35780;&#20272;&#20854;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07918v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#22522;&#37329;&#20250;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#25216;&#26415;&#65306;&#23427;&#20204;&#22914;&#20309;&#20844;&#24320;&#21457;&#24067;&#30452;&#25509;&#24433;&#21709;&#20102;&#23427;&#20204;&#30340;&#31038;&#20250;&#24433;&#21709;&#12290; &#22312;&#36825;&#31687;&#31435;&#22330;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20851;&#27880;&#24320;&#25918;&#22522;&#37329;&#20250;&#27169;&#22411;&#65292;&#22312;&#36825;&#37324;&#23450;&#20041;&#20026;&#20855;&#26377;&#24191;&#27867;&#21487;&#29992;&#27169;&#22411;&#26435;&#37325;&#30340;&#27169;&#22411;&#65288;&#20363;&#22914;Llama 2&#12289;Stable Diffusion XL&#65289;&#12290; &#25105;&#20204;&#30830;&#23450;&#20102;&#24320;&#25918;&#22522;&#37329;&#20250;&#27169;&#22411;&#30340;&#20116;&#20010;&#29420;&#29305;&#23646;&#24615;&#65288;&#20363;&#22914;&#26356;&#22823;&#30340;&#21487;&#23450;&#21046;&#24615;&#12289;&#36739;&#24046;&#30340;&#30417;&#25511;&#65289;&#65292;&#36825;&#20123;&#23646;&#24615;&#23548;&#33268;&#20102;&#23427;&#20204;&#30340;&#21033;&#30410;&#21644;&#39118;&#38505;&#12290; &#24320;&#25918;&#22522;&#37329;&#20250;&#27169;&#22411;&#25552;&#20379;&#20102;&#37325;&#22823;&#30340;&#21033;&#30410;&#65292;&#20294;&#20063;&#26377;&#19968;&#20123;&#38480;&#21046;&#65292;&#28085;&#30422;&#20102;&#21019;&#26032;&#12289;&#31454;&#20105;&#12289;&#20915;&#31574;&#26435;&#30340;&#20998;&#37197;&#20197;&#21450;&#36879;&#26126;&#24230;&#12290;&#20026;&#20102;&#29702;&#35299;&#20854;&#34987;&#28389;&#29992;&#30340;&#39118;&#38505;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#20998;&#26512;&#20854;&#36793;&#38469;&#39118;&#38505;&#30340;&#39118;&#38505;&#35780;&#20272;&#26694;&#26550;&#12290; &#22312;&#20960;&#20010;&#28389;&#29992;&#21521;&#37327;&#65288;&#20363;&#22914;&#32593;&#32476;&#25915;&#20987;&#12289;&#29983;&#29289;&#27494;&#22120;&#65289;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#30446;&#21069;&#30340;&#30740;&#31350;&#19981;&#36275;&#20197;&#26377;&#25928;&#22320;&#34920;&#24449;&#24320;&#25918;&#22522;&#37329;&#20250;&#27169;&#22411;&#30456;&#23545;&#20110;&#29616;&#26377;&#25216;&#26415;&#30340;&#36793;&#38469;&#39118;&#38505;&#12290; &#35813;&#26694;&#26550;&#26377;&#21161;&#20110;&#25193;&#23637;&#35770;&#25991;&#20013;&#24314;&#35758;&#30340;&#20998;&#26512;&#20197;&#20272;&#35745;&#26032;&#25216;&#26415;&#30340;&#23433;&#20840;&#24615;&#36793;&#38469;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07918v1 Announce Type: cross  Abstract: Foundation models are powerful technologies: how they are released publicly directly shapes their societal impact. In this position paper, we focus on open foundation models, defined here as those with broadly available model weights (e.g. Llama 2, Stable Diffusion XL). We identify five distinctive properties (e.g. greater customizability, poor monitoring) of open foundation models that lead to both their benefits and risks. Open foundation models present significant benefits, with some caveats, that span innovation, competition, the distribution of decision-making power, and transparency. To understand their risks of misuse, we design a risk assessment framework for analyzing their marginal risk. Across several misuse vectors (e.g. cyberattacks, bioweapons), we find that current research is insufficient to effectively characterize the marginal risk of open foundation models relative to pre-existing technologies. The framework helps ex
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20013;&#65292;&#34701;&#21512;&#20102;&#20135;&#19994;&#32423;&#26041;&#27861;&#21644;&#37327;&#21270;&#37329;&#34701;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#39046;&#22495;&#26041;&#27861;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;</title><link>https://arxiv.org/abs/2403.07916</link><description>&lt;p&gt;
&#25512;&#36827;&#25237;&#36164;&#21069;&#27839;&#65306;&#38754;&#21521;&#20135;&#19994;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Advancing Investment Frontiers: Industry-grade Deep Reinforcement Learning for Portfolio Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07916
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20013;&#65292;&#34701;&#21512;&#20102;&#20135;&#19994;&#32423;&#26041;&#27861;&#21644;&#37327;&#21270;&#37329;&#34701;&#65292;&#25552;&#20986;&#20102;&#32467;&#21512;&#22810;&#39046;&#22495;&#26041;&#27861;&#30340;&#29420;&#29305;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#22312;&#36164;&#20135;&#31867;&#21035;&#26080;&#20851;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#20013;&#30340;&#24212;&#29992;&#65292;&#23558;&#20135;&#19994;&#32423;&#26041;&#27861;&#19982;&#37327;&#21270;&#37329;&#34701;&#30456;&#32467;&#21512;&#12290;&#22312;&#36825;&#19968;&#34701;&#21512;&#30340;&#26680;&#24515;&#26159;&#25105;&#20204;&#30340;&#24378;&#22823;&#26694;&#26550;&#65292;&#19981;&#20165;&#23558;&#20808;&#36827;&#30340;DRL&#31639;&#27861;&#19982;&#29616;&#20195;&#35745;&#31639;&#25216;&#26415;&#30456;&#32467;&#21512;&#65292;&#36824;&#24378;&#35843;&#20005;&#26684;&#30340;&#32479;&#35745;&#20998;&#26512;&#12289;&#36719;&#20214;&#24037;&#31243;&#21644;&#30417;&#31649;&#21512;&#35268;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#31532;&#19968;&#39033;&#23558;&#37329;&#34701;&#24378;&#21270;&#23398;&#20064;&#19982;&#26426;&#22120;&#20154;&#23398;&#21644;&#25968;&#23398;&#29289;&#29702;&#20013;&#30340;&#20174;&#27169;&#25311;&#21040;&#30495;&#23454;&#26041;&#27861;&#30456;&#32467;&#21512;&#30340;&#30740;&#31350;&#65292;&#20026;&#25105;&#20204;&#30340;&#26694;&#26550;&#21644;&#35770;&#25454;&#24102;&#26469;&#20102;&#29420;&#29305;&#30340;&#35270;&#35282;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26368;&#32456;&#20171;&#32461;&#20102;AlphaOptimizerNet&#65292;&#19968;&#31181;&#20855;&#26377;&#19987;&#26377;&#26435;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#65288;&#20197;&#21450;&#30456;&#24212;&#30340;&#24211;&#65289;&#12290;AlphaOptimizerNet&#26159;&#20174;&#26368;&#26032;&#30340;&#25991;&#29486;&#21644;&#25105;&#20204;&#29420;&#29305;&#30340;&#36328;&#23398;&#31185;&#26041;&#27861;&#30340;&#32508;&#21512;&#20013;&#21457;&#23637;&#32780;&#26469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07916v1 Announce Type: new  Abstract: This research paper delves into the application of Deep Reinforcement Learning (DRL) in asset-class agnostic portfolio optimization, integrating industry-grade methodologies with quantitative finance. At the heart of this integration is our robust framework that not only merges advanced DRL algorithms with modern computational techniques but also emphasizes stringent statistical analysis, software engineering and regulatory compliance. To the best of our knowledge, this is the first study integrating financial Reinforcement Learning with sim-to-real methodologies from robotics and mathematical physics, thus enriching our frameworks and arguments with this unique perspective. Our research culminates with the introduction of AlphaOptimizerNet, a proprietary Reinforcement Learning agent (and corresponding library). Developed from a synthesis of state-of-the-art (SOTA) literature and our unique interdisciplinary methodology, AlphaOptimizerNe
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#21046;&#26469;&#35780;&#20272;&#21307;&#30103;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#12289;&#26377;&#29992;&#21644;&#21487;&#38752;AI&#27169;&#22411;&#65292;&#24357;&#21512;AI&#27169;&#22411;&#24320;&#21457;&#19982;&#23454;&#38469;&#21463;&#30410;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;</title><link>https://arxiv.org/abs/2403.07911</link><description>&lt;p&gt;
&#31449;&#22312;FURM&#22522;&#30784;&#19978;-&#35780;&#20272;&#21307;&#30103;&#31995;&#32479;&#20013;&#20844;&#24179;&#12289;&#26377;&#29992;&#21644;&#21487;&#38752;AI&#27169;&#22411;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Standing on FURM ground -- A framework for evaluating Fair, Useful, and Reliable AI Models in healthcare systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07911
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#21046;&#26469;&#35780;&#20272;&#21307;&#30103;&#31995;&#32479;&#20013;&#30340;&#20844;&#24179;&#12289;&#26377;&#29992;&#21644;&#21487;&#38752;AI&#27169;&#22411;&#65292;&#24357;&#21512;AI&#27169;&#22411;&#24320;&#21457;&#19982;&#23454;&#38469;&#21463;&#30410;&#20043;&#38388;&#30340;&#40511;&#27807;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25351;&#23548;&#24739;&#32773;&#25252;&#29702;&#25110;&#25805;&#20316;&#27969;&#31243;&#30340;&#24433;&#21709;&#21462;&#20915;&#20110;AI&#27169;&#22411;&#30340;&#36755;&#20986;&#12289;&#22522;&#20110;&#35813;&#36755;&#20986;&#30340;&#20915;&#31574;&#21327;&#35758;&#20197;&#21450;&#30456;&#20851;&#21033;&#30410;&#30456;&#20851;&#32773;&#37319;&#21462;&#24517;&#35201;&#21518;&#32493;&#34892;&#21160;&#30340;&#33021;&#21147;&#12290;&#22312;&#37096;&#32626;&#21069;&#20272;&#35745;&#36825;&#31181;&#30456;&#20114;&#20316;&#29992;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#37096;&#32626;&#21518;&#23454;&#26102;&#30740;&#31350;&#23427;&#65292;&#23545;&#20110;&#24357;&#21512;AI&#27169;&#22411;&#24320;&#21457;&#19982;&#21487;&#23454;&#29616;&#21033;&#30410;&#20043;&#38388;&#30340;&#40511;&#27807;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#26031;&#22374;&#31119;&#21307;&#30103;&#20445;&#20581;&#30340;&#25968;&#25454;&#31185;&#23398;&#22242;&#38431;&#24320;&#21457;&#20102;&#19968;&#31181;&#26426;&#21046;&#65292;&#36890;&#36807;&#36827;&#34892;&#20262;&#29702;&#23457;&#26597;&#20197;&#35782;&#21035;&#28508;&#22312;&#30340;&#20215;&#20540;&#19981;&#21305;&#37197;&#12289;&#20223;&#30495;&#20272;&#31639;&#26377;&#29992;&#24615;&#12289;&#36130;&#21153;&#39044;&#27979;&#35780;&#20272;&#21487;&#25345;&#32493;&#24615;&#65292;&#20197;&#21450;&#20998;&#26512;&#30830;&#23450;IT&#21487;&#34892;&#24615;&#12289;&#35774;&#35745;&#37096;&#32626;&#31574;&#30053;&#65292;&#24182;&#24314;&#35758;&#21069;&#30651;&#24615;&#30417;&#27979;&#21644;&#35780;&#20272;&#35745;&#21010;&#26469;&#35782;&#21035;&#20844;&#24179;&#12289;&#26377;&#29992;&#21644;&#21487;&#38752;&#30340;AI&#27169;&#22411;&#65288;FURM&#65289;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#23545;FURM&#35780;&#20272;&#36827;&#34892;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07911v1 Announce Type: cross  Abstract: The impact of using artificial intelligence (AI) to guide patient care or operational processes is an interplay of the AI model's output, the decision-making protocol based on that output, and the capacity of the stakeholders involved to take the necessary subsequent action. Estimating the effects of this interplay before deployment, and studying it in real time afterwards, are essential to bridge the chasm between AI model development and achievable benefit. To accomplish this, the Data Science team at Stanford Health Care has developed a mechanism to identify fair, useful and reliable AI models (FURM) by conducting an ethical review to identify potential value mismatches, simulations to estimate usefulness, financial projections to assess sustainability, as well as analyses to determine IT feasibility, design a deployment strategy, and recommend a prospective monitoring and evaluation plan. We report on FURM assessments done to evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20219;&#21153;&#35843;&#24230;&#26041;&#26696;&#65292;&#26088;&#22312;&#23454;&#29616;&#22823;&#35268;&#27169;&#20113;&#35745;&#31639;&#31995;&#32479;&#20013;&#20219;&#21153;&#35843;&#24230;&#30340;&#26368;&#20248;&#21033;&#29992;&#21644;&#26368;&#22823;&#25191;&#34892;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.07905</link><description>&lt;p&gt;
&#29992;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#22686;&#24378;Kubernetes&#33258;&#21160;&#35843;&#24230;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#20113;&#35745;&#31639;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Enhancing Kubernetes Automated Scheduling with Deep Learning and Reinforcement Techniques for Large-Scale Cloud Computing Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07905
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20219;&#21153;&#35843;&#24230;&#26041;&#26696;&#65292;&#26088;&#22312;&#23454;&#29616;&#22823;&#35268;&#27169;&#20113;&#35745;&#31639;&#31995;&#32479;&#20013;&#20219;&#21153;&#35843;&#24230;&#30340;&#26368;&#20248;&#21033;&#29992;&#21644;&#26368;&#22823;&#25191;&#34892;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20113;&#35745;&#31639;&#24212;&#29992;&#35268;&#27169;&#30340;&#25345;&#32493;&#25193;&#22823;&#65292;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#31561;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#36880;&#28176;&#25104;&#20026;&#35299;&#20915;&#22823;&#35268;&#27169;&#20113;&#35745;&#31639;&#31995;&#32479;&#33258;&#21160;&#20219;&#21153;&#35843;&#24230;&#30340;&#20851;&#38190;&#24037;&#20855;&#12290;&#38024;&#23545;&#22823;&#35268;&#27169;&#20113;&#35745;&#31639;&#31995;&#32479;&#20013;&#20219;&#21153;&#35843;&#24230;&#30340;&#22797;&#26434;&#24615;&#21644;&#23454;&#26102;&#24615;&#38656;&#27714;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#21160;&#20219;&#21153;&#35843;&#24230;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#23454;&#26102;&#30417;&#27979;&#21644;&#39044;&#27979;&#20113;&#35745;&#31639;&#31995;&#32479;&#20013;&#30340;&#21442;&#25968;&#65292;&#20197;&#33719;&#21462;&#31995;&#32479;&#29366;&#24577;&#20449;&#24687;&#12290;&#28982;&#21518;&#65292;&#32467;&#21512;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26681;&#25454;&#23454;&#26102;&#31995;&#32479;&#29366;&#24577;&#21644;&#20219;&#21153;&#29305;&#24449;&#21160;&#24577;&#35843;&#25972;&#20219;&#21153;&#35843;&#24230;&#31574;&#30053;&#65292;&#23454;&#29616;&#31995;&#32479;&#36164;&#28304;&#30340;&#26368;&#20339;&#21033;&#29992;&#21644;&#20219;&#21153;&#25191;&#34892;&#25928;&#29575;&#30340;&#26368;&#22823;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07905v1 Announce Type: cross  Abstract: With the continuous expansion of the scale of cloud computing applications, artificial intelligence technologies such as Deep Learning and Reinforcement Learning have gradually become the key tools to solve the automated task scheduling of large-scale cloud computing systems. Aiming at the complexity and real-time requirement of task scheduling in large-scale cloud computing system, this paper proposes an automatic task scheduling scheme based on deep learning and reinforcement learning. Firstly, the deep learning technology is used to monitor and predict the parameters in the cloud computing system in real time to obtain the system status information. Then, combined with reinforcement learning algorithm, the task scheduling strategy is dynamically adjusted according to the real-time system state and task characteristics to achieve the optimal utilization of system resources and the maximum of task execution efficiency. This paper veri
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#21512;&#35268;&#21644;&#30417;&#30563;&#30340;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#65292;&#24378;&#35843;&#20102;DSA&#21644;AIA&#30417;&#31649;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#30417;&#31649;&#31354;&#30333;&#65292;&#24182;&#35201;&#27714;AIA&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#31038;&#20250;&#20844;&#27665;&#25552;&#20379;&#25968;&#25454;&#21644;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;</title><link>https://arxiv.org/abs/2403.07904</link><description>&lt;p&gt;
&#27491;&#35270;&#30417;&#31649;&#31354;&#30333;&#65306;&#36890;&#36807;&#32435;&#20837;&#31038;&#20250;&#20844;&#27665;&#25171;&#36896;&#36229;&#36234;AIA&#30340;&#27431;&#30431;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Addressing the Regulatory Gap: Moving Towards an EU AI Audit Ecosystem Beyond the AIA by Including Civil Society
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07904
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#21512;&#35268;&#21644;&#30417;&#30563;&#30340;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#65292;&#24378;&#35843;&#20102;DSA&#21644;AIA&#30417;&#31649;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#30417;&#31649;&#31354;&#30333;&#65292;&#24182;&#35201;&#27714;AIA&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#31038;&#20250;&#20844;&#27665;&#25552;&#20379;&#25968;&#25454;&#21644;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#27954;&#31435;&#27861;&#26426;&#26500;&#25552;&#20986;&#20102;&#25968;&#23383;&#26381;&#21153;&#27861;&#26696;&#65288;DSA&#65289;&#21644;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#65288;AIA&#65289;&#26469;&#35268;&#33539;&#24179;&#21488;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20135;&#21697;&#12290;&#26412;&#25991;&#23457;&#26597;&#20102;&#31532;&#19977;&#26041;&#23457;&#35745;&#22312;&#36825;&#20004;&#39033;&#27861;&#24459;&#20013;&#30340;&#22320;&#20301;&#20197;&#21450;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#25552;&#20379;&#27169;&#22411;&#21644;&#25968;&#25454;&#30340;&#35775;&#38382;&#26435;&#38480;&#12290;&#36890;&#36807;&#32771;&#34385;&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#20013;&#31532;&#19977;&#26041;&#23457;&#35745;&#21644;&#31532;&#19977;&#26041;&#25968;&#25454;&#35775;&#38382;&#30340;&#20215;&#20540;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#20010;&#30417;&#31649;&#31354;&#30333;&#65292;&#21363;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12299;&#27809;&#26377;&#20026;&#30740;&#31350;&#20154;&#21592;&#21644;&#31038;&#20250;&#20844;&#27665;&#25552;&#20379;&#25968;&#25454;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#23545;&#25991;&#29486;&#30340;&#36129;&#29486;&#21253;&#25324;&#65306;&#65288;1&#65289;&#23450;&#20041;&#20102;&#19968;&#20010;&#34701;&#21512;&#21512;&#35268;&#21644;&#30417;&#30563;&#30340;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#12290;&#65288;2&#65289;&#24378;&#35843;&#20102;DSA&#21644;AIA&#30417;&#31649;&#26694;&#26550;&#20013;&#23384;&#22312;&#30340;&#30417;&#31649;&#31354;&#30333;&#65292;&#38459;&#30861;&#20102;AI&#23457;&#35745;&#29983;&#24577;&#31995;&#32479;&#30340;&#24314;&#31435;&#12290;&#65288;3&#65289;&#24378;&#35843;&#30740;&#31350;&#21644;&#31038;&#20250;&#20844;&#27665;&#30340;&#31532;&#19977;&#26041;&#23457;&#35745;&#24517;&#39035;&#25104;&#20026;&#35813;&#29983;&#24577;&#31995;&#32479;&#30340;&#19968;&#37096;&#20998;&#65292;&#24182;&#35201;&#27714;AIA&#21253;&#25324;&#25968;&#25454;&#21644;&#27169;&#22411;&#35775;&#38382;&#26435;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07904v1 Announce Type: cross  Abstract: The European legislature has proposed the Digital Services Act (DSA) and Artificial Intelligence Act (AIA) to regulate platforms and Artificial Intelligence (AI) products. We review to what extent third-party audits are part of both laws and to what extent access to models and data is provided. By considering the value of third-party audits and third-party data access in an audit ecosystem, we identify a regulatory gap in that the Artificial Intelligence Act does not provide access to data for researchers and civil society. Our contributions to the literature include: (1) Defining an AI audit ecosystem that incorporates compliance and oversight. (2) Highlighting a regulatory gap within the DSA and AIA regulatory framework, preventing the establishment of an AI audit ecosystem. (3) Emphasizing that third-party audits by research and civil society must be part of that ecosystem and demand that the AIA include data and model access for ce
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20048;&#35266;&#30340;&#21069;&#30651;&#24615;&#39046;&#23548;&#32773;&#31639;&#27861;&#65288;OFTRL&#65289;&#21644;&#36866;&#24403;&#30340;&#25968;&#20540;&#26356;&#26032;&#31243;&#24207;&#65292;&#22312;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#25214;&#21040;&#20102;$\widetilde{O}(T^{-1})$-approximate&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#65292;&#36825;&#22312;$T$&#27425;&#36845;&#20195;&#20869;&#24471;&#20197;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.07890</link><description>&lt;p&gt;
$\widetilde{O}(T^{-1})$ &#25910;&#25947;&#21040;&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#22312;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
$\widetilde{O}(T^{-1})$ Convergence to (Coarse) Correlated Equilibria in Full-Information General-Sum Markov Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07890
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#20048;&#35266;&#30340;&#21069;&#30651;&#24615;&#39046;&#23548;&#32773;&#31639;&#27861;&#65288;OFTRL&#65289;&#21644;&#36866;&#24403;&#30340;&#25968;&#20540;&#26356;&#26032;&#31243;&#24207;&#65292;&#22312;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#25214;&#21040;&#20102;$\widetilde{O}(T^{-1})$-approximate&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#65292;&#36825;&#22312;$T$&#27425;&#36845;&#20195;&#20869;&#24471;&#20197;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
No-regret&#23398;&#20064;&#19982;&#21338;&#24328;&#35770;&#23494;&#20999;&#30456;&#20851;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#38750;&#32806;&#21512;&#30340;&#26080;&#24724;&#23398;&#20064;&#21160;&#24577;&#65292;&#24403;&#25152;&#26377;&#29609;&#23478;&#22312;&#27491;&#21017;&#24418;&#24335;&#28216;&#25103;&#20013;&#37319;&#29992;&#26102;&#65292;&#20197;$\widetilde{O}(T^{-1})$&#30340;&#25509;&#36817;&#26368;&#20248;&#36895;&#29575;&#25910;&#25947;&#21040;&#21508;&#31181;&#22343;&#34913;&#35299;&#65292;&#36825;&#26174;&#30528;&#25913;&#36827;&#20102;&#32463;&#20856;&#26080;&#24724;&#23398;&#20064;&#32773;&#30340;$O(1/\sqrt{T})$&#36895;&#29575;&#12290;&#28982;&#32780;&#65292;&#22312;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#31867;&#20284;&#30340;&#25910;&#25947;&#32467;&#26524;&#24456;&#23569;&#35265;&#65292;&#36825;&#26159;&#19968;&#20010;&#26356;&#36890;&#29992;&#30340;&#35774;&#32622;&#65292;&#20026;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#20048;&#35266;&#30340;&#21069;&#30651;&#24615;&#39046;&#23548;&#32773;&#31639;&#27861;&#65288;OFTRL&#65289;&#65292;&#36830;&#21516;&#36866;&#24403;&#30340;&#25968;&#20540;&#26356;&#26032;&#31243;&#24207;&#65292;&#21487;&#20197;&#22312;$T$&#27425;&#36845;&#20195;&#20869;&#25214;&#21040;&#20840;&#20449;&#24687;&#19968;&#33324;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#30340;$\widetilde{O}(T^{-1})$&#36817;&#20284;&#65288;&#31895;&#31961;&#65289;&#30456;&#20851;&#22343;&#34913;&#12290;&#25968;&#20540;&#32467;&#26524;&#20063;&#21253;&#25324;&#20197;&#35777;&#23454;&#25105;&#20204;&#30340;&#29702;&#35770;&#21457;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07890v1 Announce Type: cross  Abstract: No-regret learning has a long history of being closely connected to game theory. Recent works have devised uncoupled no-regret learning dynamics that, when adopted by all the players in normal-form games, converge to various equilibrium solutions at a near-optimal rate of $\widetilde{O}(T^{-1})$, a significant improvement over the $O(1/\sqrt{T})$ rate of classic no-regret learners. However, analogous convergence results are scarce in Markov games, a more generic setting that lays the foundation for multi-agent reinforcement learning. In this work, we close this gap by showing that the optimistic-follow-the-regularized-leader (OFTRL) algorithm, together with appropriate value update procedures, can find $\widetilde{O}(T^{-1})$-approximate (coarse) correlated equilibria in full-information general-sum Markov games within $T$ iterations. Numerical results are also included to corroborate our theoretical findings.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#21435;&#20559;&#32622;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#25913;&#21892;&#22312;&#23376;&#32676;&#20307;&#19978;&#30340;&#26368;&#22351;&#24773;&#20917;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.07888</link><description>&lt;p&gt;
&#36328;&#27169;&#24577;&#21435;&#20559;&#35265;: &#20351;&#29992;&#35821;&#35328;&#20943;&#36731;&#24433;&#20687;&#20013;&#30340;&#23376;&#32676;&#20307;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
Cross-modality debiasing: using language to mitigate sub-population shifts in imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07888
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#21435;&#20559;&#32622;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#25913;&#21892;&#22312;&#23376;&#32676;&#20307;&#19978;&#30340;&#26368;&#22351;&#24773;&#20917;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23376;&#32676;&#20307;&#36716;&#21464;&#26159;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30340;&#39046;&#22495;&#36716;&#21464;&#65292;&#31361;&#26174;&#20102;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#20043;&#38388;&#29305;&#23450;&#23376;&#32676;&#20307;&#25110;&#20154;&#21475;&#30340;&#25968;&#25454;&#20998;&#24067;&#30340;&#21464;&#21270;&#12290;&#23376;&#32676;&#20307;&#36716;&#21464;&#21344;&#25454;&#20102;&#31639;&#27861;&#20559;&#35265;&#30340;&#19968;&#20010;&#37325;&#35201;&#26469;&#28304;&#65292;&#24182;&#38656;&#35201;&#20998;&#24067;&#40065;&#26834;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#22810;&#27169;&#24577;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;CLIP&#65292;&#20855;&#26377;&#22266;&#26377;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#65292;&#20294;&#36825;&#31181;&#40065;&#26834;&#24615;&#23545;&#21442;&#25968;&#24494;&#35843;&#26159;&#33030;&#24369;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#19981;&#21516;&#27169;&#24577;&#20043;&#38388;&#30340;&#40065;&#26834;&#24615;&#36830;&#25509;&#65292;&#37325;&#26032;&#22609;&#36896;&#19968;&#20010;&#27169;&#24577;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#12290;&#20855;&#20307;&#22320;&#65292;&#22312;CLIP&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;&#19978;&#19979;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#26469;&#21435;&#20559;&#32622;&#22270;&#20687;&#29305;&#24449;&#34920;&#31034;&#65292;&#20197;&#25913;&#21892;&#22312;&#23376;&#32676;&#20307;&#19978;&#30340;&#26368;&#22351;&#24773;&#20917;&#34920;&#29616;&#12290;&#25105;&#20204;&#30340;&#24191;&#27867;&#23454;&#35777;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#36827;&#34892;&#21435;&#20559;&#35265;&#22788;&#29702;&#30340;&#22270;&#20687;&#34920;&#31034;&#33021;&#22815;&#22312;&#23376;&#32676;&#20307;&#19978;&#25913;&#21892;&#26368;&#22351;&#24773;&#20917;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07888v1 Announce Type: cross  Abstract: Sub-population shift is a specific type of domain shift that highlights changes in data distribution within specific sub-groups or populations between training and testing. Sub-population shift accounts for a significant source of algorithmic bias and calls for distributional robustness. Recent studies found inherent distributional robustness in multi-modality foundation models, such as the vision-language model CLIP, yet this robustness is vulnerable through parameter fine-tuning. In this paper, we propose leveraging the connection of robustness among different modalities and reshaping the distributional robustness of one modality with another. Specifically, in the context of the distributional robustness of CLIP, we propose to leverage natural language inputs to debias the image feature representations, to improve worst-case performance on sub-populations. Our extensive empirical studies show that image representations debiased by na
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65288;NSI&#65289;&#65292;&#36890;&#36807;&#27133;&#34920;&#31034;&#23398;&#20064;&#25509;&#22320;&#21644;&#29983;&#25104;&#29289;&#20307;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#35821;&#20041;&#32467;&#21512;&#21040;&#25277;&#35937;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.07887</link><description>&lt;p&gt;
&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65306;&#22312;&#26032;&#20852;&#30340;&#27133;&#34920;&#31034;&#20013;&#25509;&#22320;&#23545;&#35937;&#35821;&#20041;
&lt;/p&gt;
&lt;p&gt;
Neural Slot Interpreters: Grounding Object Semantics in Emergent Slot Representations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07887
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65288;NSI&#65289;&#65292;&#36890;&#36807;&#27133;&#34920;&#31034;&#23398;&#20064;&#25509;&#22320;&#21644;&#29983;&#25104;&#29289;&#20307;&#35821;&#20041;&#65292;&#23454;&#29616;&#20102;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#35821;&#20041;&#32467;&#21512;&#21040;&#25277;&#35937;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#20013;&#24515;&#26041;&#27861;&#22312;&#23558;&#21407;&#22987;&#24863;&#30693;&#26080;&#30417;&#30563;&#20998;&#35299;&#20026;&#20016;&#23500;&#30340;&#31867;&#20284;&#29289;&#20307;&#30340;&#25277;&#35937;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#23558;&#29616;&#23454;&#19990;&#30028;&#30340;&#29289;&#20307;&#35821;&#20041;&#25509;&#22320;&#21040;&#23398;&#21040;&#30340;&#25277;&#35937;&#20013;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#19979;&#28216;&#29702;&#35299;&#24212;&#29992;&#20013;&#30340;&#37319;&#29992;&#12290;&#25105;&#20204;&#25552;&#20986;&#31070;&#32463;&#27133;&#35299;&#37322;&#22120;&#65288;NSI&#65289;&#65292;&#23427;&#36890;&#36807;&#27133;&#34920;&#31034;&#23398;&#20064;&#25509;&#22320;&#21644;&#29983;&#25104;&#29289;&#20307;&#35821;&#20041;&#12290;NSI&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#31867;&#20284;XML&#30340;&#32534;&#31243;&#35821;&#35328;&#65292;&#23427;&#20351;&#29992;&#31616;&#21333;&#30340;&#35821;&#27861;&#35268;&#21017;&#23558;&#22330;&#26223;&#30340;&#29289;&#20307;&#35821;&#20041;&#32452;&#32455;&#25104;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#31243;&#24207;&#21407;&#35821;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#23545;&#40784;&#27169;&#22411;&#23398;&#20064;&#36890;&#36807;&#20849;&#20139;&#23884;&#20837;&#31354;&#38388;&#19978;&#30340;&#21452;&#23618;&#23545;&#27604;&#23398;&#20064;&#30446;&#26631;&#23558;&#31243;&#24207;&#21407;&#35821;&#25509;&#22320;&#21040;&#27133;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26500;&#24314;NSI&#31243;&#24207;&#29983;&#25104;&#27169;&#22411;&#65292;&#21033;&#29992;&#23545;&#40784;&#27169;&#22411;&#25512;&#26029;&#30340;&#23494;&#38598;&#20851;&#32852;&#20174;&#27133;&#29983;&#25104;&#20197;&#29289;&#20307;&#20026;&#20013;&#24515;&#30340;&#31243;&#24207;&#12290;&#22312;&#21452;&#27169;&#24335;&#26816;&#32034;&#23454;&#39564;&#20013;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07887v1 Announce Type: cross  Abstract: Object-centric methods have seen significant progress in unsupervised decomposition of raw perception into rich object-like abstractions. However, limited ability to ground object semantics of the real world into the learned abstractions has hindered their adoption in downstream understanding applications. We present the Neural Slot Interpreter (NSI) that learns to ground and generate object semantics via slot representations. At the core of NSI is an XML-like programming language that uses simple syntax rules to organize the object semantics of a scene into object-centric program primitives. Then, an alignment model learns to ground program primitives into slots through a bi-level contrastive learning objective over a shared embedding space. Finally, we formulate the NSI program generator model to use the dense associations inferred from the alignment model to generate object-centric programs from slots. Experiments on bi-modal retrie
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#21704;&#23494;&#39039;&#22270;&#20013;&#23547;&#25214;&#21704;&#23494;&#39039;&#24490;&#29615;&#30340;&#27169;&#22240;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22686;&#24378;&#23616;&#37096;&#25628;&#32034;&#21644;&#21160;&#24577;&#22686;&#24378;&#36755;&#20837;&#22270;&#30340;&#26032;&#39062;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#25104;&#21151;&#35777;&#26126;&#20102;&#21704;&#23494;&#39039;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.07886</link><description>&lt;p&gt;
&#19968;&#20010;&#22312;&#21704;&#23494;&#39039;&#22270;&#20013;&#23547;&#25214;&#21704;&#23494;&#39039;&#24490;&#29615;&#30340;&#27169;&#22240;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Memetic Algorithm To Find a Hamiltonian Cycle in a Hamiltonian Graph
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07886
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#21704;&#23494;&#39039;&#22270;&#20013;&#23547;&#25214;&#21704;&#23494;&#39039;&#24490;&#29615;&#30340;&#27169;&#22240;&#31639;&#27861;&#65292;&#24182;&#24341;&#20837;&#20102;&#22686;&#24378;&#23616;&#37096;&#25628;&#32034;&#21644;&#21160;&#24577;&#22686;&#24378;&#36755;&#20837;&#22270;&#30340;&#26032;&#39062;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#25104;&#21151;&#35777;&#26126;&#20102;&#21704;&#23494;&#39039;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22312;&#21704;&#23494;&#39039;&#22270;&#20013;&#23547;&#25214;&#21704;&#23494;&#39039;&#24490;&#29615;&#30340;&#27169;&#22240;&#31639;&#27861;&#12290;&#36825;&#31181;&#31639;&#27861;&#22522;&#20110;&#19968;&#31181;&#24050;&#34987;&#35777;&#26126;&#26377;&#25928;&#30340;&#35299;&#20915;&#38750;&#23545;&#31216;&#26053;&#34892;&#21830;&#38382;&#39064; (\atspp) &#30340;&#26041;&#27861;&#65292;&#26412;&#25991;&#20013;&#36824;&#36890;&#36807;&#24341;&#20837;&#26356;&#24378;&#22823;&#30340;&#23616;&#37096;&#25628;&#32034;&#26469;&#22686;&#24378;&#36825;&#31181;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#22312;&#32771;&#34385;&#21704;&#23494;&#39039;&#24615;&#30340;&#36755;&#20837;&#22270;&#20013;&#31232;&#30095;&#21270;&#24182;&#22312;&#25628;&#32034;&#36807;&#31243;&#20013;&#21160;&#24577;&#22686;&#24378;&#23427;&#30340;&#26032;&#39062;&#25216;&#26415;&#12290;&#36825;&#31181;&#32508;&#21512;&#21551;&#21457;&#24335;&#26041;&#27861;&#26377;&#21161;&#20110;&#36890;&#36807;&#26356;&#30701;&#30340;&#26102;&#38388;&#25214;&#21040;&#21704;&#23494;&#39039;&#24490;&#29615;&#26469;&#35777;&#26126;&#21704;&#23494;&#39039;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#37319;&#29992;&#20102;&#26368;&#36817;&#20171;&#32461;&#30340;&#20174; \hamcyc &#21040;&#23545;&#31216; \tsp &#30340;&#22810;&#39033;&#24335;&#26102;&#38388;&#24402;&#32422;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#35745;&#31639;&#22270;&#30340;&#20256;&#36882;&#38381;&#21253;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#20803;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#21363;&#22312;&#29702;&#35770;&#19978;&#19981;&#33021;&#20445;&#35777;&#25214;&#21040;&#21704;&#23494;&#39039;&#24490;&#29615;&#65292;&#20294;&#25105;&#20204;&#35266;&#23519;&#21040;&#35813;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#25104;&#21151;&#39564;&#35777;&#20102;&#21704;&#23494;&#39039;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07886v1 Announce Type: cross  Abstract: We present a memetic algorithm (\maa) approach for finding a Hamiltonian cycle in a Hamiltonian graph. The \ma is based on a proven approach to the Asymmetric Travelling Salesman Problem (\atspp) that, in this contribution, is boosted by the introduction of more powerful local searches. Our approach also introduces a novel technique that sparsifies the input graph under consideration for Hamiltonicity and dynamically augments it during the search. Such a combined heuristic approach helps to prove Hamiltonicity by finding a Hamiltonian cycle in less time. In addition, we also employ a recently introduced polynomial-time reduction from the \hamcyc to the Symmetric \tsp, which is based on computing the transitive closure of the graph. Although our approach is a metaheuristic, i.e., it does not give a theoretical guarantee for finding a Hamiltonian cycle, we have observed that the method is successful in practice in verifying the Hamiltoni
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;MOD-CL&#22810;&#26631;&#31614;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#32422;&#26463;&#25439;&#22833;&#25913;&#21892;&#36755;&#20986;&#65292;&#36890;&#36807;&#20462;&#27491;&#22120;&#27169;&#22411;&#21644;&#28151;&#21512;&#22120;&#27169;&#22411;&#29983;&#25104;&#26356;&#21463;&#38480;&#21046;&#30340;&#32467;&#26524;&#65292;&#24182;&#21033;&#29992;Product T-Norm&#23558;&#32422;&#26463;&#25439;&#22833;&#38598;&#25104;&#21040;MOD_YOLO&#26550;&#26500;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20219;&#21153;1&#21644;&#20219;&#21153;2&#30340;&#24471;&#20998;&#12290;</title><link>https://arxiv.org/abs/2403.07885</link><description>&lt;p&gt;
MOD-CL: &#24102;&#32422;&#26463;&#25439;&#22833;&#30340;&#22810;&#26631;&#31614;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
MOD-CL: Multi-label Object Detection with Constrained Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07885
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;MOD-CL&#22810;&#26631;&#31614;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#65292;&#21033;&#29992;&#32422;&#26463;&#25439;&#22833;&#25913;&#21892;&#36755;&#20986;&#65292;&#36890;&#36807;&#20462;&#27491;&#22120;&#27169;&#22411;&#21644;&#28151;&#21512;&#22120;&#27169;&#22411;&#29983;&#25104;&#26356;&#21463;&#38480;&#21046;&#30340;&#32467;&#26524;&#65292;&#24182;&#21033;&#29992;Product T-Norm&#23558;&#32422;&#26463;&#25439;&#22833;&#38598;&#25104;&#21040;MOD_YOLO&#26550;&#26500;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#20219;&#21153;1&#21644;&#20219;&#21153;2&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;MOD-CL&#65292;&#36825;&#26159;&#19968;&#20010;&#21033;&#29992;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#32422;&#26463;&#25439;&#22833;&#26469;&#20135;&#29983;&#26356;&#22909;&#28385;&#36275;&#32473;&#23450;&#38656;&#27714;&#30340;&#36755;&#20986;&#30340;&#22810;&#26631;&#31614;&#30446;&#26631;&#26816;&#27979;&#26694;&#26550;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#26368;&#20808;&#36827;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;YOLOv8&#26500;&#24314;&#30340;&#22810;&#26631;&#31614;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;MOD_YOLO&#65292;&#35813;&#27169;&#22411;&#36817;&#24180;&#26469;&#24050;&#34987;&#21457;&#34920;&#12290;&#22312;&#20219;&#21153;1&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20462;&#27491;&#22120;&#27169;&#22411;&#21644;&#28151;&#21512;&#22120;&#27169;&#22411;&#65292;&#36825;&#20004;&#20010;&#26032;&#27169;&#22411;&#36319;&#22312;&#30446;&#26631;&#26816;&#27979;&#36807;&#31243;&#20043;&#21518;&#65292;&#26088;&#22312;&#29983;&#25104;&#26356;&#21463;&#38480;&#21046;&#30340;&#36755;&#20986;&#12290;&#23545;&#20110;&#20219;&#21153;2&#65292;&#25105;&#20204;&#20351;&#29992;Product T-Norm&#23558;&#32422;&#26463;&#25439;&#22833;&#21512;&#24182;&#21040;MOD_YOLO&#26550;&#26500;&#20013;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#23454;&#29616;&#23545;&#20110;&#25913;&#21892;&#20219;&#21153;1&#21644;&#20219;&#21153;2&#30340;&#24471;&#20998;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07885v1 Announce Type: cross  Abstract: We introduce MOD-CL, a multi-label object detection framework that utilizes constrained loss in the training process to produce outputs that better satisfy the given requirements. In this paper, we use $\mathrm{MOD_{YOLO}}$, a multi-label object detection model built upon the state-of-the-art object detection model YOLOv8, which has been published in recent years. In Task 1, we introduce the Corrector Model and Blender Model, two new models that follow after the object detection process, aiming to generate a more constrained output. For Task 2, constrained losses have been incorporated into the $\mathrm{MOD_{YOLO}}$ architecture using Product T-Norm. The results show that these implementations are instrumental to improving the scores for both Task 1 and Task 2.
&lt;/p&gt;</description></item><item><title>seg-metrics&#26159;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#20998;&#21106;&#24230;&#37327;&#30340;Python&#21253;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30740;&#31350;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#26631;&#20934;&#21270;&#27169;&#22411;&#35780;&#20272;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#25903;&#25345;&#22810;&#31181;&#25991;&#20214;&#26684;&#24335;&#30340;&#29305;&#28857;&#12290;</title><link>https://arxiv.org/abs/2403.07884</link><description>&lt;p&gt;
Seg-metrics: &#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#20998;&#21106;&#24230;&#37327;&#30340;Python&#21253;
&lt;/p&gt;
&lt;p&gt;
Seg-metrics: a Python package to compute segmentation metrics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07884
&lt;/p&gt;
&lt;p&gt;
seg-metrics&#26159;&#19968;&#20010;&#29992;&#20110;&#35745;&#31639;&#20998;&#21106;&#24230;&#37327;&#30340;Python&#21253;&#65292;&#20026;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30740;&#31350;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#26631;&#20934;&#21270;&#27169;&#22411;&#35780;&#20272;&#35299;&#20915;&#26041;&#26696;&#65292;&#20855;&#26377;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#21644;&#25903;&#25345;&#22810;&#31181;&#25991;&#20214;&#26684;&#24335;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;(MIS)&#30740;&#31350;&#20013;&#36873;&#25321;&#24615;&#24378;&#35843;&#25351;&#26631;&#30340;&#20196;&#20154;&#25285;&#24551;&#30340;&#36235;&#21183;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;\texttt{seg-metrics}&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#26631;&#20934;&#21270;MIS&#27169;&#22411;&#35780;&#20272;&#30340;&#24320;&#28304;Python&#21253;&#12290;&#19982;&#29616;&#26377;&#30340;&#21253;&#19981;&#21516;&#65292;\texttt{seg-metrics}&#20026;&#21508;&#31181;&#22522;&#20110;&#37325;&#21472;&#21644;&#36317;&#31163;&#30340;&#24230;&#37327;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#30028;&#38754;&#65292;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;\texttt{seg-metrics}&#25903;&#25345;&#22810;&#31181;&#25991;&#20214;&#26684;&#24335;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;Python&#36719;&#20214;&#21253;&#32034;&#24341;(PyPI)&#36731;&#26494;&#23433;&#35013;&#12290;&#32858;&#28966;&#20110;&#36895;&#24230;&#21644;&#20415;&#21033;&#24615;&#65292;\texttt{seg-metrics}&#25104;&#20026;&#39640;&#25928;MIS&#27169;&#22411;&#35780;&#20272;&#30340;&#23453;&#36149;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07884v1 Announce Type: cross  Abstract: In response to a concerning trend of selectively emphasizing metrics in medical image segmentation (MIS) studies, we introduce \texttt{seg-metrics}, an open-source Python package for standardized MIS model evaluation. Unlike existing packages, \texttt{seg-metrics} offers user-friendly interfaces for various overlap-based and distance-based metrics, providing a comprehensive solution. \texttt{seg-metrics} supports multiple file formats and is easily installable through the Python Package Index (PyPI). With a focus on speed and convenience, \texttt{seg-metrics} stands as a valuable tool for efficient MIS model assessment.
&lt;/p&gt;</description></item><item><title>TRIPS&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#22359;&#36873;&#25321;&#23618;&#65292;&#21160;&#24577;&#35745;&#31639;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#65292;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#65292;&#32780;&#19988;&#19981;&#22686;&#21152;&#39069;&#22806;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.07883</link><description>&lt;p&gt;
&#20351;&#29992;&#19982;&#25991;&#26412;&#30456;&#20851;&#30340;&#22270;&#20687;&#22359;&#36873;&#25321;&#36827;&#34892;&#39640;&#25928;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Efficient Vision-and-Language Pre-training with Text-Relevant Image Patch Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07883
&lt;/p&gt;
&lt;p&gt;
TRIPS&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#39044;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#22359;&#36873;&#25321;&#23618;&#65292;&#21160;&#24577;&#35745;&#31639;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#65292;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#65292;&#32780;&#19988;&#19981;&#22686;&#21152;&#39069;&#22806;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs)&#22312;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#39044;&#35757;&#32451;(VLP)&#27169;&#22411;&#20013;&#21464;&#24471;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;VLP&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;ViTs&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#36825;&#20123;&#21162;&#21147;&#20173;&#28982;&#21463;&#21040;&#30001;&#20110;&#20887;&#38271;&#30340;&#35270;&#35273;&#24207;&#21015;&#24341;&#36215;&#30340;&#35745;&#31639;&#25928;&#29575;&#20302;&#19979;&#30340;&#22256;&#25200;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;VLP&#26041;&#27861;&#65292;&#31216;&#20026;TRIPS&#65292;&#20840;&#31216;&#20026;Text-Relevant Image Patch Selection&#12290;TRIPS&#36890;&#36807;&#22312;&#35270;&#35273;&#20027;&#24178;&#32593;&#32476;&#20013;&#36880;&#27493;&#20943;&#23569;&#35270;&#35273;&#24207;&#21015;&#65292;&#20351;&#29992;&#19968;&#20010;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#22359;&#36873;&#25321;&#23618;&#65292;&#20174;&#32780;&#21152;&#36895;&#35757;&#32451;&#21644;&#25512;&#29702;&#36807;&#31243;&#12290;&#36825;&#20010;&#22359;&#36873;&#25321;&#23618;&#21160;&#24577;&#35745;&#31639;&#25991;&#26412;&#30456;&#20851;&#30340;&#35270;&#35273;&#27880;&#24847;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#20013;&#36890;&#36807;&#25991;&#26412;&#24341;&#23548;&#35782;&#21035;&#20986;&#20851;&#27880;&#30340;&#22270;&#20687;&#35760;&#21495;&#24182;&#34701;&#21512;&#19981;&#20851;&#27880;&#30340;&#35760;&#21495;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;TRIPS&#19981;&#28155;&#21152;&#20219;&#20309;&#39069;&#22806;&#30340;&#21442;&#25968;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#22823;&#22810;&#25968;&#22522;&#20110;ViT&#30340;VLP&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07883v1 Announce Type: cross  Abstract: Vision Transformers (ViTs) have become increasingly popular in large-scale Vision and Language Pre-training (VLP) models. Although previous VLP research has demonstrated the efficacy of ViTs, these efforts still struggle with computational inefficiencies caused by lengthy visual sequences. To address this challenge, we introduce an efficient VLP approach called TRIPS, which stands for Text-Relevant Image Patch Selection. TRIPS progressively reduces the visual sequence using a text-guided patch-selection layer in the visual backbone, thereby accelerating both training and inference processes. This patch-selection layer dynamically computes text-dependent visual attention, enabling it to identify attentive image tokens with text guidance and fuse inattentive ones in an end-to-end fashion. Importantly, TRIPS does not add any extra parameters and generalizes to most ViT-based VLP models. We incorporate TRIPS into three representative VLP m
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20027;&#24352;&#19968;&#20010;AI&#20107;&#20214;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#37325;&#28857;&#20851;&#27880;AI&#20986;&#38169;&#24182;&#24341;&#21457;&#20105;&#35758;&#30340;&#31034;&#20363;&#20197;&#21450;&#23427;&#20204;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#22914;&#20309;&#26500;&#24314;&#65292;&#35748;&#20026;&#36825;&#20123;&#20107;&#20214;&#26159;&#21442;&#19982;AI&#31995;&#32479;&#30340;&#37325;&#35201;&#25163;&#27573;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2403.07879</link><description>&lt;p&gt;
AI&#20107;&#20214;&#21644;&#8220;&#32593;&#32476;&#25925;&#38556;&#8221;: &#30740;&#31350;&#35758;&#31243;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
AI incidents and 'networked trouble': The case for a research agenda
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07879
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20027;&#24352;&#19968;&#20010;AI&#20107;&#20214;&#30340;&#30740;&#31350;&#35758;&#31243;&#65292;&#37325;&#28857;&#20851;&#27880;AI&#20986;&#38169;&#24182;&#24341;&#21457;&#20105;&#35758;&#30340;&#31034;&#20363;&#20197;&#21450;&#23427;&#20204;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#22914;&#20309;&#26500;&#24314;&#65292;&#35748;&#20026;&#36825;&#20123;&#20107;&#20214;&#26159;&#21442;&#19982;AI&#31995;&#32479;&#30340;&#37325;&#35201;&#25163;&#27573;&#65292;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#20204;&#26222;&#36941;&#20851;&#27880;&#20844;&#20247;&#22914;&#20309;&#21442;&#19982;AI&#35774;&#35745;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20027;&#24352;&#19968;&#20010;&#19987;&#27880;&#20110;AI&#20107;&#20214;&#30340;&#30740;&#31350;&#35758;&#31243; - &#21363;AI&#20986;&#38169;&#24182;&#24341;&#21457;&#20105;&#35758;&#30340;&#31034;&#20363; - &#20197;&#21450;&#23427;&#20204;&#22312;&#22312;&#32447;&#29615;&#22659;&#20013;&#22914;&#20309;&#26500;&#24314;&#30340;&#30740;&#31350;&#35758;&#31243;&#12290;&#25105;&#20197;2020&#24180;9&#26376;&#30340;&#19968;&#20010;AI&#20107;&#20214;&#20026;&#20363;&#65292;&#24403;&#26102;&#19968;&#20010;Twitter&#29992;&#25143;&#21019;&#24314;&#20102;&#19968;&#20010;&#8220;&#21487;&#24597;&#30340;&#23454;&#39564;&#8221;&#26469;&#23637;&#31034;Twitter&#31639;&#27861;&#22312;&#35009;&#21098;&#22270;&#20687;&#26102;&#23384;&#22312;&#30340;&#31181;&#26063;&#20559;&#35265;&#12290;&#36825;&#23548;&#33268;Twitter&#19981;&#20165;&#25918;&#24323;&#20102;&#35813;&#31639;&#27861;&#30340;&#20351;&#29992;&#65292;&#36824;&#21542;&#35748;&#20102;&#20351;&#29992;&#20219;&#20309;&#31639;&#27861;&#36827;&#34892;&#35813;&#20219;&#21153;&#30340;&#20915;&#23450;&#12290;&#25105;&#35748;&#20026;&#20687;&#36825;&#26679;&#30340;AI&#20107;&#20214;&#26159;&#21442;&#19982;&#38656;&#35201;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;AI&#31995;&#32479;&#30340;&#37325;&#35201;&#25163;&#27573;&#12290;&#25105;&#35748;&#20026;&#65292;&#36825;&#39033;&#30740;&#31350;&#35758;&#31243;&#24212;&#38598;&#20013;&#20110;&#22914;&#20309;&#36890;&#36807;&#25105;&#31216;&#20043;&#20026;&#8220;&#32593;&#32476;&#25925;&#38556;&#8221;&#30340;&#32593;&#32476;&#21270;&#22312;&#32447;&#34892;&#20026;&#26469;&#26500;&#24314;&#20107;&#20214;&#65292;&#20854;&#20013;&#21442;&#19982;&#30340;&#26684;&#24335;&#20351;&#20010;&#20154;&#21644;&#31639;&#27861;&#20197;&#21508;&#31181;&#26041;&#24335;&#36827;&#34892;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07879v1 Announce Type: cross  Abstract: Against a backdrop of widespread interest in how publics can participate in the design of AI, I argue for a research agenda focused on AI incidents - examples of AI going wrong and sparking controversy - and how they are constructed in online environments. I take up the example of an AI incident from September 2020, when a Twitter user created a 'horrible experiment' to demonstrate the racist bias of Twitter's algorithm for cropping images. This resulted in Twitter not only abandoning its use of that algorithm, but also disavowing its decision to use any algorithm for the task. I argue that AI incidents like this are a significant means for participating in AI systems that require further research. That research agenda, I argue, should focus on how incidents are constructed through networked online behaviours that I refer to as 'networked trouble', where formats for participation enable individuals and algorithms to interact in ways th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35775;&#38382;&#20869;&#23384;&#26102;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.07805</link><description>&lt;p&gt;
&#36229;&#36234;&#27515;&#35760;&#30828;&#32972;&#65306;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Beyond Memorization: The Challenge of Random Memory Access in Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35775;&#38382;&#20869;&#23384;&#26102;&#30340;&#25361;&#25112;&#65292;&#21457;&#29616;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#27169;&#22411;&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#20219;&#21153;&#20013;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#35821;&#35328;&#27169;&#22411;(LMs)&#30340;&#21457;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;NLP&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#23588;&#20854;&#26159;&#22312;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#20013;&#12290;&#28982;&#32780;&#65292;&#22312;&#20854;&#21442;&#25968;&#20869;&#37096;&#30340;&#30693;&#35782;&#23384;&#20648;&#21644;&#20869;&#23384;&#35775;&#38382;&#26426;&#21046;&#20173;&#28982;&#20196;&#20154;&#36153;&#35299;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;GPT-2&#65289;&#26159;&#21542;&#33021;&#22815;&#39034;&#24207;&#25110;&#38543;&#26426;&#22320;&#35775;&#38382;&#20854;&#20869;&#23384;&#12290;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#21512;&#25104;&#20219;&#21153;&#65292;&#28085;&#30422;&#20840;&#38754;&#32972;&#35829;&#12289;&#36873;&#25321;&#24615;&#32972;&#35829;&#21644;&#22522;&#20110;&#38382;&#39064;&#22238;&#31572;&#30340;&#24773;&#26223;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;LMs&#33021;&#22815;&#39034;&#24207;&#35775;&#38382;&#20854;&#20869;&#23384;&#65292;&#21516;&#26102;&#22312;&#38543;&#26426;&#35775;&#38382;&#24050;&#35760;&#24518;&#20869;&#23481;&#26102;&#36935;&#21040;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36890;&#36807;&#32972;&#35829;&#21644;&#32622;&#25442;&#31561;&#25216;&#26415;&#21487;&#20197;&#25552;&#39640;LMs&#30340;&#38543;&#26426;&#20869;&#23384;&#35775;&#38382;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23558;&#36825;&#31181;&#24178;&#39044;&#24212;&#29992;&#20110;&#24320;&#25918;&#22495;&#38382;&#39064;&#22238;&#31572;&#30340;&#29616;&#23454;&#22330;&#26223;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36890;&#36807;&#32972;&#35829;&#26469;&#22686;&#24378;&#38543;&#26426;&#35775;&#38382;&#25216;&#26415;&#23545;&#38382;&#39064;&#22238;&#31572;&#33021;&#21147;&#30340;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07805v1 Announce Type: cross  Abstract: Recent developments in Language Models (LMs) have shown their effectiveness in NLP tasks, particularly in knowledge-intensive tasks. However, the mechanisms underlying knowledge storage and memory access within their parameters remain elusive. In this paper, we investigate whether a generative LM (e.g., GPT-2) is able to access its memory sequentially or randomly. Through carefully-designed synthetic tasks, covering the scenarios of full recitation, selective recitation and grounded question answering, we reveal that LMs manage to sequentially access their memory while encountering challenges in randomly accessing memorized content. We find that techniques including recitation and permutation improve the random memory access capability of LMs. Furthermore, by applying this intervention to realistic scenarios of open-domain question answering, we validate that enhancing random access by recitation leads to notable improvements in questi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#20116;&#31181;&#26174;&#33879;&#30340;&#24037;&#20214;&#65292;&#24182;&#24212;&#29992;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;&#12290;</title><link>https://arxiv.org/abs/2403.07743</link><description>&lt;p&gt;
&#20026;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#37197;&#22791;&#24037;&#20214;&#22788;&#29702;&#27969;&#27700;&#32447;&#65306;&#35745;&#31639;&#19982;&#24615;&#33021;&#26435;&#34913;&#30340;&#23637;&#31034;
&lt;/p&gt;
&lt;p&gt;
Equipping Computational Pathology Systems with Artifact Processing Pipelines: A Showcase for Computation and Performance Trade-offs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07743
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#26041;&#26696;&#65292;&#29992;&#20110;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#20116;&#31181;&#26174;&#33879;&#30340;&#24037;&#20214;&#65292;&#24182;&#24212;&#29992;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32452;&#32455;&#30149;&#29702;&#23398;&#26159;&#30284;&#30151;&#35786;&#26029;&#30340;&#40644;&#37329;&#26631;&#20934;&#65292;&#22312;&#26174;&#24494;&#38236;&#19979;&#36827;&#34892;&#26816;&#26597;&#12290;&#28982;&#32780;&#65292;&#32452;&#32455;&#30149;&#29702;&#23398;&#22788;&#29702;&#36807;&#31243;&#20250;&#20135;&#29983;&#19968;&#20123;&#24037;&#20214;&#65292;&#26368;&#32456;&#20250;&#36716;&#31227;&#21040;&#29627;&#29827;&#36733;&#29627;&#29255;&#30340;&#25968;&#23383;&#21270;&#29256;&#26412;&#65292;&#21363;&#20840;&#29627;&#24187;&#28783;&#29255;&#12290;&#24037;&#20214;&#26159;&#35786;&#26029;&#26080;&#20851;&#30340;&#21306;&#22495;&#65292;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#30340;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#39044;&#27979;&#12290;&#22240;&#27492;&#65292;&#22312;&#35745;&#31639;&#30149;&#29702;&#23398;&#65288;CPATH&#65289;&#31995;&#32479;&#20013;&#26816;&#27979;&#21644;&#25490;&#38500;&#24037;&#20214;&#23545;&#20110;&#21487;&#38752;&#30340;&#33258;&#21160;&#35786;&#26029;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19987;&#23478;&#28151;&#21512;&#65288;MoE&#65289;&#26041;&#26696;&#65292;&#29992;&#20110;&#26816;&#27979;&#21253;&#25324;&#25439;&#22351;&#32452;&#32455;&#12289;&#27169;&#31946;&#12289;&#35126;&#30385;&#32452;&#32455;&#12289;&#27668;&#27873;&#21644;&#22312;WSIs&#20013;&#30340;&#32452;&#32455;&#23398;&#26080;&#20851;&#34880;&#28082;&#31561;&#20116;&#31181;&#26174;&#33879;&#24037;&#20214;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35757;&#32451;&#29420;&#31435;&#30340;&#20108;&#20803;DL&#27169;&#22411;&#20316;&#20026;&#19987;&#23478;&#26469;&#25429;&#25417;&#29305;&#23450;&#30340;&#24037;&#20214;&#24418;&#24577;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#34701;&#21512;&#26426;&#21046;&#26469;&#38598;&#25104;&#23427;&#20204;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#23545;&#26368;&#32456;&#30340;&#27010;&#29575;&#36827;&#34892;&#27010;&#29575;&#38408;&#20540;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07743v1 Announce Type: cross  Abstract: Histopathology is a gold standard for cancer diagnosis under a microscopic examination. However, histological tissue processing procedures result in artifacts, which are ultimately transferred to the digitized version of glass slides, known as whole slide images (WSIs). Artifacts are diagnostically irrelevant areas and may result in wrong deep learning (DL) algorithms predictions. Therefore, detecting and excluding artifacts in the computational pathology (CPATH) system is essential for reliable automated diagnosis. In this paper, we propose a mixture of experts (MoE) scheme for detecting five notable artifacts, including damaged tissue, blur, folded tissue, air bubbles, and histologically irrelevant blood from WSIs. First, we train independent binary DL models as experts to capture particular artifact morphology. Then, we ensemble their predictions using a fusion mechanism. We apply probabilistic thresholding over the final probabilit
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20855;&#36523;&#20195;&#29702;&#30340;&#20004;&#31181;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#65306;&#23398;&#20064;&#26032;&#34892;&#20026;&#21644;&#26032;&#29615;&#22659;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#20449;&#24230;&#24471;&#20998;&#26469;&#26356;&#26032;&#23384;&#20648;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#38656;&#35201;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.07548</link><description>&lt;p&gt;
&#20114;&#21160;&#25351;&#20196;&#36319;&#38543;&#20195;&#29702;&#30340;&#22312;&#32447;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Continual Learning For Interactive Instruction Following Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07548
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;&#20855;&#36523;&#20195;&#29702;&#30340;&#20004;&#31181;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#65306;&#23398;&#20064;&#26032;&#34892;&#20026;&#21644;&#26032;&#29615;&#22659;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#20449;&#24230;&#24471;&#20998;&#26469;&#26356;&#26032;&#23384;&#20648;&#30340;&#20449;&#24687;&#65292;&#20174;&#32780;&#36991;&#20813;&#38656;&#35201;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36890;&#36807;&#35821;&#35328;&#25351;&#20196;&#25191;&#34892;&#26085;&#24120;&#20219;&#21153;&#30340;&#20855;&#36523;&#20195;&#29702;&#23398;&#20064;&#36807;&#31243;&#20013;&#65292;&#25991;&#29486;&#22823;&#37117;&#20551;&#23450;&#20195;&#29702;&#22312;&#24320;&#22987;&#26102;&#23601;&#23398;&#20064;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#30340;&#23398;&#20064;&#22330;&#26223;&#36739;&#19981;&#29616;&#23454;&#65292;&#22240;&#20026;&#26426;&#22120;&#20154;&#20195;&#29702;&#24212;&#35813;&#22312;&#25506;&#32034;&#21644;&#24863;&#30693;&#19990;&#30028;&#30340;&#36807;&#31243;&#20013;&#19981;&#26029;&#22320;&#23398;&#20064;&#12290;&#20026;&#20102;&#26397;&#30528;&#26356;&#30495;&#23454;&#30340;&#20855;&#36523;&#20195;&#29702;&#23398;&#20064;&#22330;&#26223;&#36808;&#36827;&#19968;&#27493;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25345;&#32493;&#23398;&#20064;&#35774;&#32622;&#20379;&#20855;&#36523;&#20195;&#29702;&#20351;&#29992;&#65307;&#23398;&#20064;&#26032;&#34892;&#20026;&#65288;&#34892;&#20026;&#22686;&#37327;&#23398;&#20064;&#65292;Behavior-IL&#65289;&#21644;&#26032;&#29615;&#22659;&#65288;&#29615;&#22659;&#22686;&#37327;&#23398;&#20064;&#65292;Environment-IL&#65289;&#12290;&#22312;&#20219;&#21153;&#20013;&#65292;&#20808;&#21069;&#22522;&#20110;&#8220;&#25968;&#25454;&#20808;&#39564;&#8221;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#32500;&#25252;&#36807;&#21435;&#20219;&#21153;&#30340;logits&#12290;&#28982;&#32780;&#65292;&#23384;&#20648;&#30340;&#20449;&#24687;&#24448;&#24448;&#26159;&#19981;&#20805;&#20998;&#23398;&#20064;&#30340;&#20449;&#24687;&#65292;&#38656;&#35201;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#65292;&#32780;&#36825;&#31181;&#20449;&#24687;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#35758;&#22522;&#20110;&#33258;&#20449;&#24230;&#24471;&#20998;&#32780;&#26080;&#38656;&#20219;&#21153;&#36793;&#30028;&#20449;&#24687;&#26469;&#26356;&#26032;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07548v1 Announce Type: new  Abstract: In learning an embodied agent executing daily tasks via language directives, the literature largely assumes that the agent learns all training data at the beginning. We argue that such a learning scenario is less realistic since a robotic agent is supposed to learn the world continuously as it explores and perceives it. To take a step towards a more realistic embodied agent learning scenario, we propose two continual learning setups for embodied agents; learning new behaviors (Behavior Incremental Learning, Behavior-IL) and new environments (Environment Incremental Learning, Environment-IL) For the tasks, previous 'data prior' based continual learning methods maintain logits for the past tasks. However, the stored information is often insufficiently learned information and requires task boundary information, which might not always be available. Here, we propose to update them based on confidence scores without task boundary information d
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;CSI&#21453;&#39304;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#23450;&#30340;&#36716;&#25442;&#20989;&#25968;&#21644;&#21487;&#35757;&#32451;&#30340;&#30721;&#26412;&#35774;&#35745;&#31574;&#30053;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#20102;CSI&#37325;&#24314;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.07355</link><description>&lt;p&gt;
&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#29992;&#20110;CSI&#21453;&#39304;
&lt;/p&gt;
&lt;p&gt;
Vector Quantization for Deep-Learning-Based CSI Feedback in Massive MIMO Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.07355
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30690;&#37327;&#37327;&#21270;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;MIMO&#31995;&#32479;&#20013;&#30340;CSI&#21453;&#39304;&#65292;&#36890;&#36807;&#24341;&#20837;&#29305;&#23450;&#30340;&#36716;&#25442;&#20989;&#25968;&#21644;&#21487;&#35757;&#32451;&#30340;&#30721;&#26412;&#35774;&#35745;&#31574;&#30053;&#65292;&#38477;&#20302;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#24182;&#25552;&#39640;&#20102;CSI&#37325;&#24314;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#26377;&#38480;&#36895;&#29575;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#20449;&#36947;&#29366;&#24577;&#20449;&#24687;&#65288;CSI&#65289;&#21453;&#39304;&#26041;&#27861;&#65292;&#29992;&#20110;&#22823;&#35268;&#27169;&#22810;&#36755;&#20837;&#22810;&#36755;&#20986;&#65288;MIMO&#65289;&#31995;&#32479;&#12290;&#35813;&#26041;&#27861;&#22312;&#30690;&#37327;&#37327;&#21270;&#21464;&#20998;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;VQ-VAE&#65289;&#26694;&#26550;&#19979;&#25552;&#20379;&#20102;&#28508;&#22312;&#30690;&#37327;&#30340;&#26377;&#38480;&#20301;&#34920;&#31034;&#65292;&#21516;&#26102;&#22522;&#20110;&#24418;&#29366;&#22686;&#30410;&#30690;&#37327;&#37327;&#21270;&#20943;&#23569;&#20102;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22312;&#36825;&#31181;&#26041;&#27861;&#20013;&#65292;&#28508;&#22312;&#30690;&#37327;&#30340;&#24133;&#24230;&#20351;&#29992;&#21512;&#36866;&#30340;&#36716;&#25442;&#20989;&#25968;&#21644;&#38750;&#22343;&#21248;&#26631;&#37327;&#30721;&#26412;&#36827;&#34892;&#37327;&#21270;&#65292;&#32780;&#28508;&#22312;&#30690;&#37327;&#30340;&#26041;&#21521;&#21017;&#20351;&#29992;&#21487;&#35757;&#32451;&#30340;Grassmann&#30721;&#26412;&#36827;&#34892;&#37327;&#21270;&#12290;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#29992;&#20110;&#23884;&#22871;&#30721;&#26412;&#30340;&#30721;&#23383;&#36873;&#25321;&#35268;&#21017;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#65292;&#36824;&#24320;&#21457;&#20102;&#19968;&#31181;&#22810;&#36895;&#29575;&#30721;&#26412;&#35774;&#35745;&#31574;&#30053;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#38477;&#20302;&#20102;&#19982;VQ-VAE&#30456;&#20851;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#21516;&#26102;&#25913;&#21892;&#20102;CSI&#37325;&#24314;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.07355v1 Announce Type: cross  Abstract: This paper presents a finite-rate deep-learning (DL)-based channel state information (CSI) feedback method for massive multiple-input multiple-output (MIMO) systems. The presented method provides a finite-bit representation of the latent vector based on a vector-quantized variational autoencoder (VQ-VAE) framework while reducing its computational complexity based on shape-gain vector quantization. In this method, the magnitude of the latent vector is quantized using a non-uniform scalar codebook with a proper transformation function, while the direction of the latent vector is quantized using a trainable Grassmannian codebook. A multi-rate codebook design strategy is also developed by introducing a codeword selection rule for a nested codebook along with the design of a loss function. Simulation results demonstrate that the proposed method reduces the computational complexity associated with VQ-VAE while improving CSI reconstruction pe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#25490;&#21015;&#36136;&#37327;&#20989;&#25968;&#30340;&#21542;&#23450;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#25910;&#25947;&#24615;&#65292;&#30740;&#31350;&#20102;&#21542;&#23450;&#25805;&#20316;&#21518;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#30456;&#20284;&#24615;&#21464;&#21270;&#36235;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.06483</link><description>&lt;p&gt;
&#25490;&#21015;&#36136;&#37327;&#20989;&#25968;&#30340;&#21542;&#23450;
&lt;/p&gt;
&lt;p&gt;
The negation of permutation mass function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06483
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#25490;&#21015;&#36136;&#37327;&#20989;&#25968;&#30340;&#21542;&#23450;&#26041;&#27861;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#25910;&#25947;&#24615;&#65292;&#30740;&#31350;&#20102;&#21542;&#23450;&#25805;&#20316;&#21518;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#30456;&#20284;&#24615;&#21464;&#21270;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21542;&#23450;&#26159;&#30693;&#35782;&#34920;&#31034;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#35270;&#35282;&#12290;&#29616;&#26377;&#30340;&#21542;&#23450;&#26041;&#27861;&#20027;&#35201;&#24212;&#29992;&#20110;&#27010;&#29575;&#35770;&#12289;&#35777;&#25454;&#29702;&#35770;&#21644;&#22797;&#26434;&#35777;&#25454;&#29702;&#35770;&#12290;&#20316;&#20026;&#35777;&#25454;&#29702;&#35770;&#30340;&#19968;&#31181;&#27867;&#21270;&#65292;&#38543;&#26426;&#25490;&#21015;&#38598;&#21512;&#29702;&#35770;&#21487;&#20197;&#26356;&#31934;&#30830;&#22320;&#34920;&#31034;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#23558;&#21542;&#23450;&#30340;&#27010;&#24565;&#24212;&#29992;&#20110;&#38543;&#26426;&#25490;&#21015;&#38598;&#21512;&#29702;&#35770;&#23578;&#26410;&#34987;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#25490;&#21015;&#36136;&#37327;&#20989;&#25968;&#30340;&#21542;&#23450;&#12290;&#27492;&#22806;&#65292;&#22312;&#21542;&#23450;&#36807;&#31243;&#20013;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#21542;&#23450;&#26041;&#27861;&#30340;&#25910;&#25947;&#24615;&#12290;&#30740;&#31350;&#20102;&#27599;&#20010;&#21542;&#23450;&#25805;&#20316;&#21518;&#19981;&#30830;&#23450;&#24615;&#21644;&#19981;&#30456;&#20284;&#24615;&#30340;&#36235;&#21183;&#12290;&#25968;&#20540;&#20363;&#23376;&#34987;&#29992;&#26469;&#35777;&#26126;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06483v1 Announce Type: new  Abstract: Negation is a important perspective of knowledge representation. Existing negation methods are mainly applied in probability theory, evidence theory and complex evidence theory. As a generalization of evidence theory, random permutation sets theory may represent information more precisely. However, how to apply the concept of negation to random permutation sets theory has not been studied. In this paper, the negation of permutation mass function is proposed. Moreover, in the negation process, the convergence of proposed negation method is verified. The trends of uncertainty and dissimilarity after each negation operation are investigated. Numerical examples are used to demonstrate the rationality of the proposed method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20513;&#23548;&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26500;&#24314;&#36890;&#29992;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#29305;&#23450;&#34920;&#31034;&#26080;&#27861;&#36328;&#36234;&#19981;&#21516;&#32452;&#21512;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.06026</link><description>&lt;p&gt;
&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26500;&#24314;&#36890;&#29992;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Towards a Generic Representation of Cominatorial Problems for Learning-Based Approaches
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20513;&#23548;&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26500;&#24314;&#36890;&#29992;&#34920;&#31034;&#65292;&#20197;&#35299;&#20915;&#29305;&#23450;&#34920;&#31034;&#26080;&#27861;&#36328;&#36234;&#19981;&#21516;&#32452;&#21512;&#38382;&#39064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#20154;&#21592;&#23545;&#20351;&#29992;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#32452;&#21512;&#38382;&#39064;&#20135;&#29983;&#20102;&#20852;&#36259;&#65292;&#26080;&#35770;&#26159;&#20197;&#31471;&#21040;&#31471;&#30340;&#26041;&#24335;&#36824;&#26159;&#19982;&#20256;&#32479;&#20248;&#21270;&#31639;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#26223;&#19979;&#65292;&#25361;&#25112;&#22312;&#20110;&#23558;&#30446;&#26631;&#32452;&#21512;&#38382;&#39064;&#32534;&#30721;&#25104;&#36866;&#29992;&#20110;&#23398;&#20064;&#31639;&#27861;&#30340;&#32467;&#26500;&#12290;&#35768;&#22810;&#29616;&#26377;&#20316;&#21697;&#25552;&#20986;&#20102;&#29305;&#23450;&#20110;&#38382;&#39064;&#30340;&#34920;&#31034;&#65292;&#36890;&#24120;&#20197;&#22270;&#30340;&#24418;&#24335;&#65292;&#20197;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#32570;&#20047;&#27867;&#21270;&#24615;&#65292;&#22240;&#20026;&#34920;&#31034;&#19981;&#33021;&#36731;&#26131;&#20174;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#36716;&#31227;&#21040;&#21478;&#19968;&#20010;&#32452;&#21512;&#38382;&#39064;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#23581;&#35797;&#21435;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#21482;&#25552;&#20379;&#20102;&#37096;&#20998;&#27867;&#21270;&#24615;&#12290;&#37492;&#20110;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#20513;&#23548;&#20026;&#22522;&#20110;&#23398;&#20064;&#26041;&#27861;&#30340;&#32452;&#21512;&#38382;&#39064;&#26397;&#30528;&#23436;&#20840;&#36890;&#29992;&#30340;&#34920;&#31034;&#26041;&#24335;&#36808;&#36827;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06026v1 Announce Type: cross  Abstract: In recent years, there has been a growing interest in using learning-based approaches for solving combinatorial problems, either in an end-to-end manner or in conjunction with traditional optimization algorithms. In both scenarios, the challenge lies in encoding the targeted combinatorial problems into a structure compatible with the learning algorithm. Many existing works have proposed problem-specific representations, often in the form of a graph, to leverage the advantages of \textit{graph neural networks}. However, these approaches lack generality, as the representation cannot be easily transferred from one combinatorial problem to another one. While some attempts have been made to bridge this gap, they still offer a partial generality only. In response to this challenge, this paper advocates for progress toward a fully generic representation of combinatorial problems for learning-based approaches. The approach we propose involves 
&lt;/p&gt;</description></item><item><title>MuseGraph&#23558;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#21644;&#36890;&#29992;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20351;&#29992;</title><link>https://arxiv.org/abs/2403.04780</link><description>&lt;p&gt;
MuseGraph&#65306;&#38754;&#21521;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22270;&#23548;&#21521;&#25351;&#20196;&#35843;&#25972;&#29992;&#20110;&#36890;&#29992;&#22270;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
MuseGraph: Graph-oriented Instruction Tuning of Large Language Models for Generic Graph Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04780
&lt;/p&gt;
&lt;p&gt;
MuseGraph&#23558;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#32467;&#21512;&#36215;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#21644;&#36890;&#29992;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20351;&#29992;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#20016;&#23500;&#23646;&#24615;&#30340;&#22270;&#22312;&#24314;&#27169;&#20114;&#32852;&#23454;&#20307;&#21644;&#25913;&#36827;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#39044;&#27979;&#26041;&#38754;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#36890;&#24120;&#29992;&#20110;&#24314;&#27169;&#24102;&#23646;&#24615;&#30340;&#22270;&#65292;&#20294;&#38656;&#35201;&#22312;&#24212;&#29992;&#20110;&#19981;&#21516;&#22270;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26102;&#36827;&#34892;&#37325;&#26032;&#35757;&#32451;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#24341;&#20837;&#20102;&#26032;&#30340;&#33539;&#20363;&#65292;&#20294;LLMs&#22312;&#22270;&#25366;&#25496;&#20013;&#30340;&#29983;&#25104;&#28508;&#21147;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550; MuseGraph&#65292;&#23427;&#26080;&#32541;&#25972;&#21512;&#20102;GNNs&#21644;LLMs&#30340;&#20248;&#21183;&#65292;&#24182;&#20419;&#36827;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#21644;&#36890;&#29992;&#30340;&#22270;&#25366;&#25496;&#26041;&#27861;&#65292;&#21487;&#36328;&#19981;&#21516;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#20351;&#29992;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25552;&#20986;&#30340;&#33258;&#36866;&#24212;&#36755;&#20837;&#29983;&#25104;&#24341;&#20837;&#19968;&#20010;&#32039;&#20945;&#30340;&#22270;&#25551;&#36848;&#65292;&#20197;&#22312;&#35821;&#35328;&#20196;&#29260;&#38480;&#21046;&#30340;&#32422;&#26463;&#19979;&#23553;&#35013;&#26469;&#33258;&#22270;&#30340;&#20851;&#38190;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04780v1 Announce Type: cross  Abstract: Graphs with abundant attributes are essential in modeling interconnected entities and improving predictions in various real-world applications. Traditional Graph Neural Networks (GNNs), which are commonly used for modeling attributed graphs, need to be re-trained every time when applied to different graph tasks and datasets. Although the emergence of Large Language Models (LLMs) has introduced a new paradigm in natural language processing, the generative potential of LLMs in graph mining remains largely under-explored. To this end, we propose a novel framework MuseGraph, which seamlessly integrates the strengths of GNNs and LLMs and facilitates a more effective and generic approach for graph mining across different tasks and datasets. Specifically, we first introduce a compact graph description via the proposed adaptive input generation to encapsulate key information from the graph under the constraints of language token limitations. T
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#35299;&#35868;&#20219;&#21153;AlgoPuzzleVQA&#65292;&#36890;&#36807;&#31639;&#27861;&#35868;&#39064;&#25361;&#25112;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#35270;&#35273;&#29702;&#35299;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#22797;&#26434;&#31639;&#27861;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#35780;&#20272;&#35270;&#35273;&#25968;&#25454;&#35299;&#37322;&#19982;&#31639;&#27861;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.03864</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#26159;&#35299;&#35868;&#22825;&#25165;&#65311;&#31639;&#27861;&#35868;&#39064;&#25581;&#31034;&#20102;&#22810;&#27169;&#24577;&#25512;&#29702;&#20013;&#30340;&#20005;&#23803;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Are Language Models Puzzle Prodigies? Algorithmic Puzzles Unveil Serious Challenges in Multimodal Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03864
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#35299;&#35868;&#20219;&#21153;AlgoPuzzleVQA&#65292;&#36890;&#36807;&#31639;&#27861;&#35868;&#39064;&#25361;&#25112;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#35270;&#35273;&#29702;&#35299;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#22797;&#26434;&#31639;&#27861;&#25512;&#29702;&#30340;&#33021;&#21147;&#65292;&#26088;&#22312;&#35780;&#20272;&#35270;&#35273;&#25968;&#25454;&#35299;&#37322;&#19982;&#31639;&#27861;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#22810;&#27169;&#24577;&#35299;&#35868;&#20219;&#21153;&#65292;&#23558;&#20854;&#25918;&#22312;&#35270;&#35273;&#38382;&#31572;&#30340;&#32972;&#26223;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;AlgoPuzzleVQA&#65292;&#26088;&#22312;&#25361;&#25112;&#21644;&#35780;&#20272;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#38656;&#35201;&#35270;&#35273;&#29702;&#35299;&#12289;&#35821;&#35328;&#29702;&#35299;&#21644;&#22797;&#26434;&#31639;&#27861;&#25512;&#29702;&#30340;&#31639;&#27861;&#35868;&#39064;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#21019;&#24314;&#20102;&#28085;&#30422;&#24067;&#23572;&#36923;&#36753;&#12289;&#32452;&#21512;&#25968;&#23398;&#12289;&#22270;&#35770;&#12289;&#20248;&#21270;&#12289;&#25628;&#32034;&#31561;&#22810;&#31181;&#25968;&#23398;&#21644;&#31639;&#27861;&#20027;&#39064;&#30340;&#35868;&#39064;&#65292;&#26088;&#22312;&#35780;&#20272;&#35270;&#35273;&#25968;&#25454;&#35299;&#37322;&#19982;&#31639;&#27861;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#20154;&#31867;&#32534;&#20889;&#30340;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#30340;&#12290;&#25105;&#20204;&#25152;&#26377;&#30340;&#35868;&#39064;&#37117;&#26377;&#31934;&#30830;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#20197;&#20174;&#31639;&#27861;&#20013;&#25214;&#21040;&#65292;&#26080;&#38656;&#32321;&#29712;&#30340;&#20154;&#24037;&#35745;&#31639;&#12290;&#36825;&#30830;&#20445;&#20102;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#22312;&#25512;&#29702;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#38598;&#22823;&#23567;&#26041;&#38754;&#21487;&#20197;&#20219;&#24847;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03864v1 Announce Type: cross  Abstract: This paper introduces the novel task of multimodal puzzle solving, framed within the context of visual question-answering. We present a new dataset, AlgoPuzzleVQA designed to challenge and evaluate the capabilities of multimodal language models in solving algorithmic puzzles that necessitate both visual understanding, language understanding, and complex algorithmic reasoning. We create the puzzles to encompass a diverse array of mathematical and algorithmic topics such as boolean logic, combinatorics, graph theory, optimization, search, etc., aiming to evaluate the gap between visual data interpretation and algorithmic problem-solving skills. The dataset is generated automatically from code authored by humans. All our puzzles have exact solutions that can be found from the algorithm without tedious human calculations. It ensures that our dataset can be scaled up arbitrarily in terms of reasoning complexity and dataset size. Our investi
&lt;/p&gt;</description></item><item><title>&#36793;&#32536;&#26234;&#33021;&#32467;&#21512;&#20102;AI&#21644;&#36793;&#32536;&#35745;&#31639;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#22312;FL&#32593;&#32476;&#20013;&#25552;&#20986;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#26694;&#26550;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.03165</link><description>&lt;p&gt;
&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#21644;&#36793;&#32536;&#35745;&#31639;&#23454;&#29616;&#20113;&#35745;&#31639;&#32593;&#32476;&#20013;&#30340;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Leveraging Federated Learning and Edge Computing for Recommendation Systems within Cloud Computing Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03165
&lt;/p&gt;
&lt;p&gt;
&#36793;&#32536;&#26234;&#33021;&#32467;&#21512;&#20102;AI&#21644;&#36793;&#32536;&#35745;&#31639;&#65292;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#23454;&#29616;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#65292;&#22312;FL&#32593;&#32476;&#20013;&#25552;&#20986;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#26694;&#26550;&#20197;&#25552;&#39640;&#36890;&#20449;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#22823;&#35268;&#27169;&#39640;&#25928;&#37096;&#32626;&#65292;AI&#21644;&#36793;&#32536;&#35745;&#31639;&#30340;&#32467;&#21512;&#20135;&#29983;&#20102;&#36793;&#32536;&#26234;&#33021;&#65292;&#21033;&#29992;&#26411;&#31471;&#35774;&#22791;&#21644;&#36793;&#32536;&#26381;&#21153;&#22120;&#30340;&#35745;&#31639;&#21644;&#36890;&#20449;&#33021;&#21147;&#26469;&#26356;&#25509;&#36817;&#25968;&#25454;&#29983;&#25104;&#22320;&#22788;&#29702;&#25968;&#25454;&#12290;&#36793;&#32536;&#26234;&#33021;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#26159;&#38544;&#31169;&#20445;&#25252;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539;&#24335;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#65292;&#20351;&#25968;&#25454;&#25152;&#26377;&#32773;&#33021;&#22815;&#22312;&#26080;&#38656;&#23558;&#21407;&#22987;&#25968;&#25454;&#20256;&#36755;&#33267;&#31532;&#19977;&#26041;&#26381;&#21153;&#22120;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;FL&#32593;&#32476;&#39044;&#35745;&#20250;&#28041;&#21450;&#25968;&#21315;&#20010;&#24322;&#26500;&#20998;&#24067;&#24335;&#35774;&#22791;&#12290;&#22240;&#27492;&#65292;&#36890;&#20449;&#25928;&#29575;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#29942;&#39048;&#12290;&#20026;&#20102;&#20943;&#23569;&#33410;&#28857;&#25925;&#38556;&#21644;&#35774;&#22791;&#36864;&#20986;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#32852;&#37030;&#23398;&#20064;&#65288;HFL&#65289;&#26694;&#26550;&#65292;&#20854;&#20013;&#25351;&#23450;&#30340;&#38598;&#32676;&#39046;&#23548;&#32773;&#36890;&#36807;&#20013;&#38388;&#27169;&#22411;&#32858;&#21512;&#26469;&#25903;&#25345;&#25968;&#25454;&#25152;&#26377;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03165v1 Announce Type: new  Abstract: To enable large-scale and efficient deployment of artificial intelligence (AI), the combination of AI and edge computing has spawned Edge Intelligence, which leverages the computing and communication capabilities of end devices and edge servers to process data closer to where it is generated. A key technology for edge intelligence is the privacy-protecting machine learning paradigm known as Federated Learning (FL), which enables data owners to train models without having to transfer raw data to third-party servers. However, FL networks are expected to involve thousands of heterogeneous distributed devices. As a result, communication efficiency remains a key bottleneck. To reduce node failures and device exits, a Hierarchical Federated Learning (HFL) framework is proposed, where a designated cluster leader supports the data owner through intermediate model aggregation. Therefore, based on the improvement of edge server resource utilizatio
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20809;&#35889;&#21644;&#31354;&#38388;&#22495;&#30340;&#26144;&#23556;&#65292;&#20197;&#39044;&#27979;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#24212;&#21644;&#24418;&#29366;&#25554;&#20540;&#65292;&#30456;&#27604;&#20808;&#21069;&#26041;&#27861;&#65292;&#21462;&#24471;&#26356;&#20934;&#30830;&#12289;&#24179;&#28369;&#30340;&#28857;&#23545;&#24212;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.18920</link><description>&lt;p&gt;
&#20809;&#35889;&#36935;&#35265;&#31354;&#38388;: &#21644;&#35856;3D&#24418;&#29366;&#21305;&#37197;&#21644;&#25554;&#20540;
&lt;/p&gt;
&lt;p&gt;
Spectral Meets Spatial: Harmonising 3D Shape Matching and Interpolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18920
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#32467;&#21512;&#20809;&#35889;&#21644;&#31354;&#38388;&#22495;&#30340;&#26144;&#23556;&#65292;&#20197;&#39044;&#27979;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#24212;&#21644;&#24418;&#29366;&#25554;&#20540;&#65292;&#30456;&#27604;&#20808;&#21069;&#26041;&#27861;&#65292;&#21462;&#24471;&#26356;&#20934;&#30830;&#12289;&#24179;&#28369;&#30340;&#28857;&#23545;&#24212;&#32467;&#26524;&#65292;&#24182;&#19988;&#22312;&#35745;&#31639;&#19978;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;3D&#24418;&#29366;&#21305;&#37197;&#21644;&#25554;&#20540;&#23494;&#20999;&#30456;&#20851;&#65292;&#20294;&#23427;&#20204;&#32463;&#24120;&#34987;&#20998;&#24320;&#30740;&#31350;&#24182;&#20381;&#27425;&#24212;&#29992;&#20110;&#20851;&#32852;&#19981;&#21516;&#30340;3D&#24418;&#29366;&#65292;&#20174;&#32780;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39044;&#27979;3D&#24418;&#29366;&#20043;&#38388;&#30340;&#28857;&#23545;&#24212;&#21644;&#24418;&#29366;&#25554;&#20540;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23558;&#28145;&#24230;&#21151;&#33021;&#26144;&#23556;&#26694;&#26550;&#19982;&#32463;&#20856;&#34920;&#38754;&#21464;&#24418;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22312;&#20809;&#35889;&#21644;&#31354;&#38388;&#22495;&#20013;&#26144;&#23556;&#24418;&#29366;&#12290;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#25972;&#21512;&#31354;&#38388;&#26144;&#23556;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#23545;&#20110;&#20808;&#21069;&#29992;&#20110;&#24418;&#29366;&#21305;&#37197;&#30340;&#21151;&#33021;&#26144;&#23556;&#26041;&#27861;&#33719;&#24471;&#26356;&#31934;&#30830;&#21644;&#24179;&#28369;&#30340;&#28857;&#23545;&#24212;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#36890;&#36807;&#24341;&#20837;&#20809;&#35889;&#26144;&#23556;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25670;&#33073;&#20102;&#36890;&#24120;&#20351;&#29992;&#20294;&#35745;&#31639;&#26114;&#36149;&#30340;&#20165;&#23545;&#36817;&#31561;&#36317;&#24418;&#29366;&#21464;&#24418;&#26377;&#25928;&#30340;&#27979;&#22320;&#36317;&#31163;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18920v1 Announce Type: cross  Abstract: Although 3D shape matching and interpolation are highly interrelated, they are often studied separately and applied sequentially to relate different 3D shapes, thus resulting in sub-optimal performance. In this work we present a unified framework to predict both point-wise correspondences and shape interpolation between 3D shapes. To this end, we combine the deep functional map framework with classical surface deformation models to map shapes in both spectral and spatial domains. On the one hand, by incorporating spatial maps, our method obtains more accurate and smooth point-wise correspondences compared to previous functional map methods for shape matching. On the other hand, by introducing spectral maps, our method gets rid of commonly used but computationally expensive geodesic distance constraints that are only valid for near-isometric shape deformations. Furthermore, we propose a novel test-time adaptation scheme to capture both 
&lt;/p&gt;</description></item><item><title>&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.14989</link><description>&lt;p&gt;
&#20998;&#26512;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;
&lt;/p&gt;
&lt;p&gt;
Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.14989
&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#30340;&#25193;&#23637;&#8212;&#8212;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#31283;&#23450;&#24615;&#21644;&#24615;&#33021;&#26041;&#38754;&#25552;&#20986;&#20102;&#37325;&#35201;&#25351;&#23548;&#65292;&#38656;&#35201;&#35880;&#24910;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#20197;&#20445;&#25345;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#38469;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#19981;&#35268;&#21017;&#37319;&#26679;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#23545;&#20110;&#20551;&#35774;&#19968;&#33268;&#38388;&#38548;&#21644;&#23436;&#25972;&#25968;&#25454;&#30340;&#20256;&#32479;&#26041;&#27861;&#26500;&#25104;&#25361;&#25112;&#12290;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;Neural ODEs&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#19982;&#24120;&#24494;&#20998;&#26041;&#31243;&#27714;&#35299;&#22120;&#32467;&#21512;&#65292;&#36890;&#36807;&#21442;&#25968;&#21270;&#21521;&#37327;&#22330;&#23398;&#20064;&#36830;&#32493;&#28508;&#22312;&#34920;&#31034;&#12290;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65288;Neural SDEs&#65289;&#36890;&#36807;&#24341;&#20837;&#25193;&#25955;&#39033;&#25193;&#23637;&#20102;&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65292;&#28982;&#32780;&#22312;&#22788;&#29702;&#19981;&#35268;&#21017;&#38388;&#38548;&#21644;&#32570;&#22833;&#20540;&#26102;&#65292;&#36825;&#31181;&#28155;&#21152;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#12290;&#22240;&#27492;&#65292;&#20180;&#32454;&#35774;&#35745;&#28418;&#31227;&#21644;&#25193;&#25955;&#20989;&#25968;&#23545;&#20110;&#20445;&#25345;&#31283;&#23450;&#24615;&#21644;&#22686;&#24378;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#31895;&#24515;&#30340;&#36873;&#25321;&#21487;&#33021;&#23548;&#33268;&#20986;&#29616;&#27809;&#26377;&#24378;&#35299;&#12289;&#38543;&#26426;&#30772;&#22351;&#25110;&#19981;&#31283;&#23450;&#30340;Euler&#31163;&#25955;&#21270;&#31561;&#19981;&#21033;&#30340;&#24615;&#36136;&#65292;&#26174;&#33879;&#24433;&#21709;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.14989v1 Announce Type: cross  Abstract: Irregular sampling intervals and missing values in real-world time series data present challenges for conventional methods that assume consistent intervals and complete data. Neural Ordinary Differential Equations (Neural ODEs) offer an alternative approach, utilizing neural networks combined with ODE solvers to learn continuous latent representations through parameterized vector fields. Neural Stochastic Differential Equations (Neural SDEs) extend Neural ODEs by incorporating a diffusion term, although this addition is not trivial, particularly when addressing irregular intervals and missing values. Consequently, careful design of drift and diffusion functions is crucial for maintaining stability and enhancing performance, while incautious choices can result in adverse properties such as the absence of strong solutions, stochastic destabilization, or unstable Euler discretizations, significantly affecting Neural SDEs' performance. In 
&lt;/p&gt;</description></item><item><title>LongAgent&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23558;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#38271;&#25991;&#26412;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.11550</link><description>&lt;p&gt;
LongAgent: &#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23558;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;
&lt;/p&gt;
&lt;p&gt;
LongAgent: Scaling Language Models to 128k Context through Multi-Agent Collaboration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11550
&lt;/p&gt;
&lt;p&gt;
LongAgent&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#23558;&#35821;&#35328;&#27169;&#22411;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;&#65292;&#24182;&#22312;&#38271;&#25991;&#26412;&#22788;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#28508;&#22312;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#29702;&#35299;&#35821;&#35328;&#21644;&#25191;&#34892;&#22797;&#26434;&#25512;&#29702;&#20219;&#21153;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#20855;&#26377;&#38271;&#19978;&#19979;&#25991;&#31383;&#21475;&#30340;LLMs&#20197;&#20854;&#26114;&#36149;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#39640;&#25512;&#29702;&#24310;&#36831;&#32780;&#33261;&#21517;&#26157;&#33879;&#12290;&#21363;&#20351;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#22914;GPT-4&#21644;Claude2&#22312;&#22788;&#29702;&#36229;&#36807;$100k$&#26631;&#35760;&#30340;&#36755;&#20837;&#26102;&#20063;&#32463;&#24120;&#20986;&#38169;&#65292;&#36825;&#31181;&#29616;&#35937;&#20063;&#34987;&#31216;&#20026;\textit{&#20013;&#38388;&#36855;&#22833;}&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#21327;&#20316;&#30340;&#26041;&#27861;\textsc{LongAgent}&#65292;&#23558;LLMs&#65288;&#20363;&#22914;LLaMA&#65289;&#25193;&#23637;&#21040;128K&#19978;&#19979;&#25991;&#65292;&#24182;&#23637;&#31034;&#20986;&#22312;&#38271;&#25991;&#26412;&#22788;&#29702;&#26041;&#38754;&#21487;&#33021;&#20248;&#20110;GPT-4&#30340;&#28508;&#21147;&#12290;&#22312;\textsc{LongAgent}&#20013;&#65292;&#19968;&#20301;&#39046;&#23548;&#32773;&#36127;&#36131;&#29702;&#35299;&#29992;&#25143;&#24847;&#22270;&#24182;&#25351;&#23548;&#22242;&#38431;&#25104;&#21592;&#20174;&#25991;&#26723;&#20013;&#33719;&#21462;&#20449;&#24687;&#12290;&#30001;&#20110;&#25104;&#21592;&#23384;&#22312;&#24187;&#35273;&#65292;&#39046;&#23548;&#32773;&#20174;&#20960;&#21313;&#21040;&#25968;&#30334;&#21517;&#25104;&#21592;&#30340;&#22238;&#24212;&#20013;&#33719;&#21462;&#20934;&#30830;&#20449;&#24687;&#24182;&#38750;&#26131;&#20107;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11550v1 Announce Type: cross  Abstract: Large language models (LLMs) have demonstrated impressive performance in understanding language and executing complex reasoning tasks. However, LLMs with long context windows have been notorious for their expensive training costs and high inference latency. Even the most advanced models such as GPT-4 and Claude2 often make mistakes when processing inputs of over $100k$ tokens, a phenomenon also known as \textit{lost in the middle}. In this paper, we propose \textsc{LongAgent}, a method based on multi-agent collaboration, which scales LLMs (e.g., LLaMA) to a context of 128K and demonstrates potential superiority in long-text processing compared to GPT-4. In \textsc{LongAgent}, a leader is responsible for understanding user intent and directing team members to acquire information from documents. Due to members' hallucinations, it is non-trivial for a leader to obtain accurate information from the responses of dozens to hundreds of member
&lt;/p&gt;</description></item><item><title>&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;</title><link>https://arxiv.org/abs/2402.10659</link><description>&lt;p&gt;
&#22810;&#20010;LLM&#20043;&#38388;&#30340;&#32593;&#32476;&#24418;&#25104;&#19982;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Network Formation and Dynamics Among Multi-LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10659
&lt;/p&gt;
&lt;p&gt;
&#20998;&#26512;&#20102;&#22810;&#20010;LLM&#22312;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#34892;&#20026;&#65292;&#21457;&#29616;&#23427;&#20204;&#22312;&#32473;&#23450;&#32593;&#32476;&#32467;&#26500;&#24182;&#34987;&#35810;&#38382;&#24418;&#25104;&#32593;&#32476;&#20559;&#22909;&#26102;&#34920;&#29616;&#20986;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#30340;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#24433;&#21709;&#34892;&#20026;&#12289;&#20559;&#22909;&#21644;&#20851;&#31995;&#65292;&#22312;&#20154;&#31867;&#31038;&#20250;&#20013;&#23545;&#20449;&#24687;&#21644;&#35268;&#33539;&#30340;&#20256;&#25773;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#31038;&#20132;&#21644;&#19987;&#19994;&#29615;&#22659;&#20013;&#65292;&#29702;&#35299;&#23427;&#20204;&#22312;&#31038;&#20132;&#32593;&#32476;&#21644;&#20114;&#21160;&#32972;&#26223;&#19979;&#30340;&#34892;&#20026;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#20998;&#26512;&#20102;&#26631;&#20934;&#32593;&#32476;&#32467;&#26500;&#21644;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#30340;&#34892;&#20026;&#65292;&#20197;&#30830;&#23450;&#22810;&#20010;LLMs&#30340;&#21160;&#24577;&#26159;&#21542;&#19982;&#20154;&#31867;&#31038;&#20132;&#21160;&#24577;&#19968;&#33268;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21508;&#31181;&#31038;&#20132;&#32593;&#32476;&#21407;&#21017;&#65292;&#21253;&#25324;&#24494;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#20559;&#29233;&#38468;&#30528;&#12289;&#19977;&#35282;&#38381;&#21512;&#21644;&#21516;&#20284;&#24615;&#65292;&#20197;&#21450;&#23439;&#35266;&#23618;&#38754;&#30340;&#27010;&#24565;&#65292;&#22914;&#31038;&#21306;&#32467;&#26500;&#21644;&#23567;&#19990;&#30028;&#29616;&#35937;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#34920;&#26126;&#65292;&#24403;&#21521;LLMs&#25552;&#20379;&#32593;&#32476;&#32467;&#26500;&#24182;&#35810;&#38382;&#23427;&#20204;&#23545;&#32593;&#32476;&#24418;&#25104;&#30340;&#20559;&#22909;&#26102;&#65292;&#23427;&#20204;&#34920;&#29616;&#20986;&#25152;&#26377;&#36825;&#20123;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10659v1 Announce Type: cross  Abstract: Social networks influence behaviors, preferences, and relationships and play a crucial role in the dissemination of information and norms within human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social networks and interactions becomes essential. Our study analyzes the behaviors of standard network structures and real-world networks to determine whether the dynamics of multiple LLMs align with human social dynamics. We explore various social network principles, including micro-level concepts such as preferential attachment, triadic closure, and homophily, as well as macro-level concepts like community structure and the small-world phenomenon. Our findings suggest that LLMs demonstrate all these principles when they are provided with network structures and asked about their preferences regarding network formation. Furtherm
&lt;/p&gt;</description></item><item><title>SGS-SLAM&#26159;&#19968;&#31181;&#22522;&#20110;&#19977;&#32500;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;SLAM&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#36890;&#36947;&#20248;&#21270;&#21644;&#20851;&#38190;&#24103;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#21644;&#31934;&#30830;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2402.03246</link><description>&lt;p&gt;
SGS-SLAM&#65306;&#22522;&#20110;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;SLAM
&lt;/p&gt;
&lt;p&gt;
SGS-SLAM: Semantic Gaussian Splatting For Neural Dense SLAM
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03246
&lt;/p&gt;
&lt;p&gt;
SGS-SLAM&#26159;&#19968;&#31181;&#22522;&#20110;&#19977;&#32500;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;SLAM&#31995;&#32479;&#65292;&#36890;&#36807;&#22810;&#36890;&#36947;&#20248;&#21270;&#21644;&#20851;&#38190;&#24103;&#20248;&#21270;&#65292;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#30340;&#37325;&#24314;&#21644;&#31934;&#30830;&#30340;&#35821;&#20041;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#29702;&#35299;&#22312;&#31264;&#23494;&#21516;&#26102;&#23450;&#20301;&#21644;&#24314;&#22270;&#65288;SLAM&#65289;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#26377;&#21161;&#20110;&#20840;&#38754;&#30340;&#22330;&#26223;&#35299;&#26512;&#12290;&#26368;&#36817;&#23558;&#39640;&#26031;&#28857;&#20113;&#38598;&#25104;&#21040;SLAM&#31995;&#32479;&#20013;&#30340;&#36827;&#23637;&#34920;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#26174;&#24335;&#30340;&#19977;&#32500;&#39640;&#26031;&#34920;&#31034;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#28210;&#26579;&#25928;&#26524;&#12290;&#22522;&#20110;&#36825;&#19968;&#36827;&#23637;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SGS-SLAM&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#19977;&#32500;&#39640;&#26031;&#28857;&#20113;&#30340;&#35821;&#20041;&#31264;&#23494;&#35270;&#35273;SLAM&#31995;&#32479;&#65292;&#23427;&#19981;&#20165;&#25552;&#20379;&#31934;&#30830;&#30340;&#19977;&#32500;&#35821;&#20041;&#20998;&#21106;&#65292;&#36824;&#23454;&#29616;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;&#37325;&#24314;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#24314;&#22270;&#36807;&#31243;&#20013;&#37319;&#29992;&#22810;&#36890;&#36947;&#20248;&#21270;&#65292;&#23558;&#22806;&#35266;&#12289;&#20960;&#20309;&#21644;&#35821;&#20041;&#32422;&#26463;&#19982;&#20851;&#38190;&#24103;&#20248;&#21270;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#37325;&#24314;&#36136;&#37327;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;SGS-SLAM&#22312;&#30456;&#26426;&#20301;&#23039;&#20272;&#35745;&#12289;&#22320;&#22270;&#37325;&#24314;&#21644;&#35821;&#20041;&#20998;&#21106;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#21516;&#26102;&#20445;&#25345;&#23454;&#26102;&#28210;&#26579;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic understanding plays a crucial role in Dense Simultaneous Localization and Mapping (SLAM), facilitating comprehensive scene interpretation. Recent advancements that integrate Gaus- sian Splatting into SLAM systems have demonstrated its effectiveness in generating high-quality renderings through the use of explicit 3D Gaussian representations. Building on this progress, we propose SGS-SLAM, the first semantic dense visual SLAM system grounded in 3D Gaussians, which provides precise 3D semantic segmentation alongside high-fidelity reconstructions. Specifically, we propose to employ multi-channel optimization during the mapping process, integrating appearance, geometric, and semantic constraints with key-frame optimization to enhance reconstruction quality. Extensive experiments demonstrate that SGS-SLAM delivers state-of-the-art performance in camera pose estimation, map reconstruction, and semantic segmentation, outperforming existing methods meanwhile preserving real-time rende
&lt;/p&gt;</description></item><item><title>CERM&#26159;&#19968;&#20010;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#22522;&#20110;&#25991;&#29486;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21457;&#29616;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#29702;&#35299;&#39135;&#21697;&#19982;&#20581;&#24247;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#39135;&#26448;&#33829;&#20859;&#25104;&#20998;&#25110;&#22522;&#20110;&#26631;&#35760;&#25968;&#25454;&#30340;&#35745;&#31639;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#39135;&#35889;&#25512;&#33616;&#21644;&#20998;&#26512;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27169;&#22411;&#65292;&#36890;&#36807;&#25429;&#25417;&#39135;&#26448;&#19982;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#20043;&#38388;&#30340;&#22266;&#26377;&#20851;&#31995;&#65292;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#26469;&#26356;&#22909;&#22320;&#25903;&#25345;&#39135;&#21697;&#30456;&#20851;&#30740;&#31350;&#12290;</title><link>https://arxiv.org/abs/2402.01724</link><description>&lt;p&gt;
CERM: &#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#22522;&#20110;&#25991;&#29486;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
CERM: Context-aware Literature-based Discovery via Sentiment Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01724
&lt;/p&gt;
&lt;p&gt;
CERM&#26159;&#19968;&#20010;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#36827;&#34892;&#22522;&#20110;&#25991;&#29486;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#21457;&#29616;&#30340;&#31995;&#32479;&#65292;&#26088;&#22312;&#29702;&#35299;&#39135;&#21697;&#19982;&#20581;&#24247;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22522;&#20110;&#39135;&#26448;&#33829;&#20859;&#25104;&#20998;&#25110;&#22522;&#20110;&#26631;&#35760;&#25968;&#25454;&#30340;&#35745;&#31639;&#27169;&#22411;&#24050;&#34987;&#29992;&#20110;&#39135;&#35889;&#25512;&#33616;&#21644;&#20998;&#26512;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#27169;&#22411;&#65292;&#36890;&#36807;&#25429;&#25417;&#39135;&#26448;&#19982;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#20043;&#38388;&#30340;&#22266;&#26377;&#20851;&#31995;&#65292;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#26469;&#26356;&#22909;&#22320;&#25903;&#25345;&#39135;&#21697;&#30456;&#20851;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#29983;&#29289;&#21307;&#23398;&#20986;&#29256;&#29289;&#30340;&#20016;&#23500;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#24773;&#24863;&#20998;&#26512;&#20219;&#21153;&#26469;&#29702;&#35299;&#39135;&#21697;&#19982;&#20581;&#24247;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#20043;&#21069;&#23558;&#20581;&#24247;&#32435;&#20837;&#39135;&#35889;&#25512;&#33616;&#21644;&#20998;&#26512;&#31995;&#32479;&#30340;&#23581;&#35797;&#20027;&#35201;&#38598;&#20013;&#22312;&#39135;&#26448;&#33829;&#20859;&#25104;&#20998;&#19978;&#65292;&#25110;&#32773;&#21033;&#29992;&#22522;&#20110;&#26631;&#35760;&#25968;&#25454;&#30340;&#22522;&#26412;&#35745;&#31639;&#27169;&#22411;&#36827;&#34892;&#35757;&#32451;&#12290;&#25429;&#25417;&#39135;&#26448;&#21644;&#29983;&#29289;&#21307;&#23398;&#27010;&#24565;&#20043;&#38388;&#22266;&#26377;&#20851;&#31995;&#30340;&#22686;&#24378;&#27169;&#22411;&#23545;&#20110;&#39135;&#21697;&#30456;&#20851;&#30740;&#31350;&#26356;&#26377;&#30410;&#22788;&#65292;&#37492;&#20110;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#32771;&#34385;&#21040;&#26114;&#36149;&#30340;&#25968;&#25454;&#26631;&#35760;&#36807;&#31243;&#65292;&#36825;&#20123;&#27169;&#22411;&#24212;&#35813;&#26377;&#25928;&#21033;&#29992;&#26631;&#35760;&#21644;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#39033;&#21517;&#20026;&#23454;&#20307;&#20851;&#31995;&#24773;&#24863;&#20998;&#26512;&#65288;ERSA&#65289;&#30340;&#26032;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#22522;&#20110;&#23454;&#20307;&#23545;&#25429;&#25417;&#25991;&#26412;&#30340;&#24773;&#24863;&#12290;ERSA&#25193;&#23637;&#20102;&#24191;&#27867;&#30740;&#31350;&#30340;&#22522;&#20110;&#26041;&#38754;&#30340;&#24773;&#24863;&#20998;&#26512;&#65288;ABSA&#65289;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#24212;&#29992;&#20110;&#29983;&#29289;&#21307;&#23398;&#25991;&#26412;&#30340;ERSA&#20219;&#21153;&#19978;&#65292;&#37325;&#28857;&#20851;&#27880;(entity-ent
&lt;/p&gt;
&lt;p&gt;
Driven by the abundance of biomedical publications, we introduce a sentiment analysis task to understand food-health relationship. Prior attempts to incorporate health into recipe recommendation and analysis systems have primarily focused on ingredient nutritional components or utilized basic computational models trained on curated labeled data. Enhanced models that capture the inherent relationship between food ingredients and biomedical concepts can be more beneficial for food-related research, given the wealth of information in biomedical texts. Considering the costly data labeling process, these models should effectively utilize both labeled and unlabeled data. This paper introduces Entity Relationship Sentiment Analysis (ERSA), a new task that captures the sentiment of a text based on an entity pair. ERSA extends the widely studied Aspect Based Sentiment Analysis (ABSA) task. Specifically, our study concentrates on the ERSA task applied to biomedical texts, focusing on (entity-ent
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22825;&#27668;&#39044;&#27979;&#30340;&#20892;&#19994;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#23391;&#21152;&#25289;&#22269;&#20892;&#19994;&#38754;&#20020;&#30340;&#22825;&#27668;&#19981;&#21033;&#22240;&#32032;&#23545;&#31918;&#39135;&#29983;&#20135;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#30408;&#21033;&#12289;&#21487;&#25345;&#32493;&#21644;&#20892;&#27665;&#21451;&#22909;&#30340;&#20892;&#19994;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2401.11410</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20892;&#19994;&#25512;&#33616;&#31995;&#32479;&#65306;&#19968;&#31181;&#22810;&#21464;&#37327;&#22825;&#27668;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Agricultural Recommendation System based on Deep Learning: A Multivariate Weather Forecasting Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11410
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#22825;&#27668;&#39044;&#27979;&#30340;&#20892;&#19994;&#25512;&#33616;&#31995;&#32479;&#65292;&#26088;&#22312;&#35299;&#20915;&#23391;&#21152;&#25289;&#22269;&#20892;&#19994;&#38754;&#20020;&#30340;&#22825;&#27668;&#19981;&#21033;&#22240;&#32032;&#23545;&#31918;&#39135;&#29983;&#20135;&#30340;&#24433;&#21709;&#65292;&#20197;&#23454;&#29616;&#30408;&#21033;&#12289;&#21487;&#25345;&#32493;&#21644;&#20892;&#27665;&#21451;&#22909;&#30340;&#20892;&#19994;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23391;&#21152;&#25289;&#22269;&#20027;&#35201;&#26159;&#19968;&#20010;&#20892;&#19994;&#22269;&#23478;&#65292;&#20892;&#19994;&#37096;&#38376;&#23545;&#20110;&#21152;&#24555;&#32463;&#27982;&#22686;&#38271;&#21644;&#20445;&#38556;&#20154;&#27665;&#31918;&#39135;&#23433;&#20840;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#23391;&#21152;&#25289;&#22269;&#21171;&#21160;&#23494;&#38598;&#22411;&#20892;&#19994;&#21462;&#24471;&#20102;&#31918;&#39135;&#20135;&#37327;&#31283;&#27493;&#22686;&#38271;&#65292;&#20294;&#24120;&#24120;&#21463;&#21040;&#19981;&#21033;&#22825;&#27668;&#26465;&#20214;&#30340;&#24433;&#21709;&#65292;&#22914;&#26292;&#38632;&#12289;&#20302;&#28201;&#21644;&#24178;&#26097;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#22240;&#32032;&#20005;&#37325;&#24433;&#21709;&#20102;&#31918;&#39135;&#29983;&#20135;&#65292;&#20351;&#24471;&#22269;&#23478;&#30340;&#31918;&#39135;&#23433;&#20840;&#21463;&#21040;&#23041;&#32961;&#12290;&#20026;&#20102;&#23454;&#29616;&#30408;&#21033;&#12289;&#21487;&#25345;&#32493;&#19988;&#20892;&#27665;&#21451;&#22909;&#30340;&#20892;&#19994;&#23454;&#36341;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22825;&#27668;&#39044;&#27979;&#27169;&#22411;&#30340;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#20316;&#29289;&#25512;&#33616;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11410v2 Announce Type: replace-cross  Abstract: Bangladesh is predominantly an agricultural country, where the agrarian sector plays an essential role in accelerating economic growth and enabling the food security of the people. The performance of this sector has an overwhelming impact on the primary macroeconomic objectives like food security, employment generation, poverty alleviation, human resources development, and other economic and social forces. Although Bangladesh's labor-intensive agriculture has achieved steady increases in food grain production, it often suffered from unfavorable weather conditions such as heavy rainfall, low temperature, and drought. Consequently, these factors hinder the production of food substantially, putting the country's overall food security in danger. In order to have a profitable, sustainable, and farmer-friendly agricultural practice, this paper proposes a context-based crop recommendation system powered by a weather forecast model. Wi
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23398;&#20064;&#31867;&#20154;&#30340;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#31526;&#21512;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#25903;&#25345;&#20262;&#29702;&#31561;&#22810;&#26041;&#38754;&#30340;&#20215;&#20540;&#23545;&#40784;&#12290;</title><link>https://arxiv.org/abs/2312.14106</link><description>&lt;p&gt;
&#23398;&#20064;&#31867;&#20154;&#34920;&#31034;&#20197;&#23454;&#29616;&#23398;&#20064;&#31867;&#20154;&#20215;&#20540;&#35266;
&lt;/p&gt;
&lt;p&gt;
Learning Human-like Representations to Enable Learning Human Values
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14106
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23398;&#20064;&#31867;&#20154;&#30340;&#34920;&#31034;&#65292;&#21487;&#20197;&#23454;&#29616;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#31526;&#21512;&#20154;&#31867;&#20215;&#20540;&#35266;&#65292;&#25903;&#25345;&#20262;&#29702;&#31561;&#22810;&#26041;&#38754;&#30340;&#20215;&#20540;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#26500;&#24314;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#30456;&#19968;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#20197;&#36991;&#20813;&#36896;&#25104;&#20260;&#23475;&#25110;&#36829;&#21453;&#31038;&#20250;&#23545;&#21487;&#25509;&#21463;&#34892;&#20026;&#30340;&#26631;&#20934;&#65311;&#25105;&#20204;&#35748;&#20026;&#65292;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#34920;&#24449;&#23545;&#40784;&#26377;&#21161;&#20110;&#20215;&#20540;&#35266;&#30340;&#23545;&#40784;&#12290;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#23398;&#20064;&#31867;&#20154;&#31867;&#23545;&#19990;&#30028;&#30340;&#34920;&#31034;&#20855;&#26377;&#35768;&#22810;&#24050;&#30693;&#22909;&#22788;&#65292;&#21253;&#25324;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12289;&#22686;&#24378;&#23545;&#39046;&#22495;&#36716;&#31227;&#30340;&#31283;&#20581;&#24615;&#21644;&#25552;&#39640;&#23569;&#26679;&#26412;&#23398;&#20064;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#65292;&#36825;&#31181;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#19982;&#20154;&#31867;&#20043;&#38388;&#30340;&#34920;&#31034;&#23545;&#40784;&#20063;&#21487;&#20197;&#25903;&#25345;&#20215;&#20540;&#23545;&#40784;&#65292;&#20351;ML&#31995;&#32479;&#36981;&#24490;&#20154;&#31867;&#20215;&#20540;&#35266;&#21644;&#31038;&#20250;&#35268;&#33539;&#12290;&#25105;&#20204;&#20851;&#27880;&#20262;&#29702;&#23398;&#20316;&#20026;&#20215;&#20540;&#23545;&#40784;&#30340;&#19968;&#20010;&#26041;&#38754;&#65292;&#24182;&#22312;&#22810;&#33218;&#32769;&#34382;&#26426;&#35774;&#32622;&#20013;&#20351;&#29992;&#21508;&#31181;&#26041;&#27861;&#35757;&#32451;ML&#20195;&#29702;&#65292;&#20854;&#20013;&#22870;&#21169;&#21453;&#26144;&#25152;&#36873;&#34892;&#21160;&#30340;&#36947;&#24503;&#21487;&#25509;&#21463;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#21512;&#25104;&#23454;&#39564;&#26469;&#35777;&#26126;&#20195;&#29702;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#34920;&#31034;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14106v2 Announce Type: replace  Abstract: How can we build AI systems that are aligned with human values to avoid causing harm or violating societal standards for acceptable behavior? We argue that representational alignment between humans and AI agents facilitates value alignment. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We propose that this kind of representational alignment between machine learning (ML) models and humans can also support value alignment, allowing ML systems to conform to human values and societal norms. We focus on ethics as one aspect of value alignment and train ML agents using a variety of methods in a multi-armed bandit setting, where rewards reflect the moral acceptability of the chosen action. We use a synthetic experiment to demonstrate that agents' representational alignment with the environment bounds
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20462;&#25913;&#22522;&#20110;&#36712;&#36857;&#25104;&#26412;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#27169;&#20223;&#22909;&#30340;&#36712;&#36857;&#21644;&#36991;&#20813;&#22351;&#30340;&#36712;&#36857;&#26469;&#25913;&#36827;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2312.10385</link><description>&lt;p&gt;
&#27169;&#20223;&#22909;&#30340;&#24182;&#36991;&#20813;&#22351;&#30340;&#65306;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#30340;&#22686;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Imitate the Good and Avoid the Bad: An Incremental Approach to Safe Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.10385
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#20462;&#25913;&#22522;&#20110;&#36712;&#36857;&#25104;&#26412;&#32422;&#26463;&#30340;&#26041;&#27861;&#65292;&#22312;&#23433;&#20840;&#24378;&#21270;&#23398;&#20064;&#20013;&#36890;&#36807;&#27169;&#20223;&#22909;&#30340;&#36712;&#36857;&#21644;&#36991;&#20813;&#22351;&#30340;&#36712;&#36857;&#26469;&#25913;&#36827;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20013;&#25191;&#34892;&#23433;&#20840;&#21160;&#20316;&#30340;&#27969;&#34892;&#26694;&#26550;&#26159;&#32422;&#26463;RL&#65292;&#20854;&#20013;&#21033;&#29992;&#22522;&#20110;&#36712;&#36857;&#30340;&#25104;&#26412;&#32422;&#26463;&#65288;&#25110;&#20854;&#20182;&#25104;&#26412;&#24230;&#37327;&#65289;&#26469;&#25191;&#34892;&#23433;&#20840;&#25805;&#20316;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#22312;&#26368;&#22823;&#21270;&#26399;&#26395;&#22870;&#21169;&#30340;&#21516;&#26102;&#25191;&#34892;&#36825;&#20123;&#32422;&#26463;&#12290;&#26368;&#36817;&#35299;&#20915;&#32422;&#26463;RL&#30340;&#26041;&#27861;&#23558;&#22522;&#20110;&#36712;&#36857;&#30340;&#25104;&#26412;&#32422;&#26463;&#36716;&#25442;&#20026;&#19968;&#20010;&#26367;&#20195;&#38382;&#39064;&#65292;&#21487;&#20197;&#36890;&#36807;&#23545;RL&#26041;&#27861;&#36827;&#34892;&#36731;&#24494;&#20462;&#25913;&#26469;&#35299;&#20915;&#12290;&#36825;&#31867;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#32570;&#28857;&#26159;&#22312;&#27599;&#20010;&#29366;&#24577;&#19978;&#23545;&#25104;&#26412;&#32422;&#26463;&#36827;&#34892;&#36807;&#24230;&#25110;&#19981;&#36275;&#20272;&#35745;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#19981;&#20462;&#25913;&#22522;&#20110;&#36712;&#36857;&#30340;&#25104;&#26412;&#32422;&#26463;&#65292;&#32780;&#26159;&#27169;&#20223;&#8220;&#22909;&#8221;&#36712;&#36857;&#24182;&#36991;&#20813;&#20174;&#36880;&#27493;&#25913;&#36827;&#30340;&#31574;&#30053;&#29983;&#25104;&#30340;&#8220;&#22351;&#8221;&#36712;&#36857;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;oracle&#65292;&#21033;&#29992;&#22870;&#21169;&#38408;&#20540;&#65288;&#38543;&#23398;&#20064;&#21464;&#21270;&#65289;&#21644;&#25972;&#20307;&#25104;&#26412;&#32422;&#26463;&#26469;&#23558;&#36712;&#36857;&#26631;&#35760;&#20026;&#8220;&#22909;&#8221;&#25110;&#8220;&#22351;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.10385v3 Announce Type: replace-cross  Abstract: A popular framework for enforcing safe actions in Reinforcement Learning (RL) is Constrained RL, where trajectory based constraints on expected cost (or other cost measures) are employed to enforce safety and more importantly these constraints are enforced while maximizing expected reward. Most recent approaches for solving Constrained RL convert the trajectory based cost constraint into a surrogate problem that can be solved using minor modifications to RL methods. A key drawback with such approaches is an over or underestimation of the cost constraint at each state. Therefore, we provide an approach that does not modify the trajectory based cost constraint and instead imitates ``good'' trajectories and avoids ``bad'' trajectories generated from incrementally improving policies. We employ an oracle that utilizes a reward threshold (which is varied with learning) and the overall cost constraint to label trajectories as ``good''
&lt;/p&gt;</description></item><item><title>&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;3D&#21407;&#23376;&#31995;&#32479;&#20013;&#20197;&#21033;&#29992;&#29289;&#29702;&#23545;&#31216;&#24615;&#21644;&#21270;&#23398;&#24615;&#36136;&#31561;&#24402;&#32435;&#20559;&#24046;&#26469;&#23398;&#20064;&#20960;&#20309;&#22270;&#20449;&#24687;&#34920;&#31034;&#32780;&#33879;&#31216;&#12290;</title><link>https://arxiv.org/abs/2312.07511</link><description>&lt;p&gt;
&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;3D&#21407;&#23376;&#31995;&#32479;&#20013;&#30340;&#23454;&#36341;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
A Hitchhiker's Guide to Geometric GNNs for 3D Atomic Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07511
&lt;/p&gt;
&lt;p&gt;
&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;3D&#21407;&#23376;&#31995;&#32479;&#20013;&#20197;&#21033;&#29992;&#29289;&#29702;&#23545;&#31216;&#24615;&#21644;&#21270;&#23398;&#24615;&#36136;&#31561;&#24402;&#32435;&#20559;&#24046;&#26469;&#23398;&#20064;&#20960;&#20309;&#22270;&#20449;&#24687;&#34920;&#31034;&#32780;&#33879;&#31216;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#20316;&#20026;&#39318;&#36873;&#30340;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#23853;&#38706;&#22836;&#35282;&#65292;&#25903;&#25345;&#20174;&#34507;&#30333;&#32467;&#26500;&#39044;&#27979;&#21040;&#20998;&#23376;&#27169;&#25311;&#21644;&#26448;&#26009;&#29983;&#25104;&#31561;&#24212;&#29992;&#65292;&#20854;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#35832;&#22914;&#29289;&#29702;&#23545;&#31216;&#24615;&#21644;&#21270;&#23398;&#24615;&#36136;&#20043;&#31867;&#30340;&#24402;&#32435;&#20559;&#24046;&#65292;&#23398;&#20064;&#36825;&#20123;&#20960;&#20309;&#22270;&#30340;&#20449;&#24687;&#34920;&#31034;&#12290;&#22312;&#36825;&#31687;&#20027;&#35266;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#32780;&#33258;&#36275;&#22320;&#27010;&#36848;&#20102;&#29992;&#20110;3D&#21407;&#23376;&#31995;&#32479;&#30340;&#20960;&#20309;&#22270;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07511v2 Announce Type: replace-cross  Abstract: Recent advances in computational modelling of atomic systems, spanning molecules, proteins, and materials, represent them as geometric graphs with atoms embedded as nodes in 3D Euclidean space. In these graphs, the geometric attributes transform according to the inherent physical symmetries of 3D atomic systems, including rotations and translations in Euclidean space, as well as node permutations. In recent years, Geometric Graph Neural Networks have emerged as the preferred machine learning architecture powering applications ranging from protein structure prediction to molecular simulations and material generation. Their specificity lies in the inductive biases they leverage - such as physical symmetries and chemical properties - to learn informative representations of these geometric graphs.   In this opinionated paper, we provide a comprehensive and self-contained overview of the field of Geometric GNNs for 3D atomic systems
&lt;/p&gt;</description></item><item><title>KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2312.06185</link><description>&lt;p&gt;
KnowGPT&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
KnowGPT: Black-Box Knowledge Injection for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06185
&lt;/p&gt;
&lt;p&gt;
KnowGPT&#26159;&#19968;&#31181;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22810;&#33218;&#32769;&#34382;&#26426;&#26500;&#24314;&#26368;&#36866;&#21512;&#27599;&#20010;&#38382;&#39064;&#30340;&#25552;&#31034;&#65292;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#39564;&#35777;&#26126;&#20854;&#26174;&#33879;&#25552;&#21319;&#20102;&#30693;&#35782;&#27880;&#20837;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#65292;&#25552;&#20379;&#20114;&#21160;&#24335;API&#65292;&#21487;&#20197;&#20197;&#20154;&#31867;&#19987;&#23478;&#27700;&#24179;&#22238;&#31572;&#24120;&#35265;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#24403;&#38754;&#20020;&#38656;&#35201;&#29305;&#23450;&#39046;&#22495;&#25110;&#19987;&#19994;&#39046;&#22495;&#30693;&#35782;&#30340;&#38382;&#39064;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#20250;&#32473;&#20986;&#19981;&#20934;&#30830;&#25110;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#65292;&#36825;&#20123;&#30693;&#35782;&#24182;&#26410;&#21253;&#21547;&#22312;&#23427;&#20204;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#26368;&#20808;&#36827;&#30340;LLMs&#24182;&#38750;&#24320;&#28304;&#65292;&#36825;&#20351;&#24471;&#20165;&#20351;&#29992;&#27169;&#22411;API&#27880;&#20837;&#30693;&#35782;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;KnowGPT&#65292;&#19968;&#31181;&#29992;&#20110;LLMs&#22312;&#38382;&#31572;&#20013;&#30340;&#40657;&#30418;&#30693;&#35782;&#27880;&#20837;&#26694;&#26550;&#12290;KnowGPT&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20174;&#30693;&#35782;&#22270;&#20013;&#25552;&#21462;&#30456;&#20851;&#30693;&#35782;&#65292;&#24182;&#20351;&#29992;&#22810;&#33218;&#32769;&#34382;&#26426;&#65288;MAB&#65289;&#20026;&#27599;&#20010;&#38382;&#39064;&#26500;&#24314;&#26368;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;KnowGPT&#26174;&#33879;&#22686;&#24378;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;KnowGPT&#24179;&#22343;&#25913;&#36827;&#20102;23%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06185v2 Announce Type: replace-cross  Abstract: Generative Large Language Models (LLMs), such as ChatGPT, offer interactive APIs that can answer common questions at a human-expert level. However, these models often give inaccurate or incorrect responses when faced with questions requiring domain-specific or professional-specific knowledge not covered in their training corpus. Furthermore, many state-of-the-art LLMs are not open-source, making it challenging to inject knowledge with model APIs only. In this work, we introduce KnowGPT, a black-box knowledge injection framework for LLMs in question answering. KnowGPT leverages deep reinforcement learning (RL) to extract relevant knowledge from Knowledge Graphs (KGs) and use Multi-Armed Bandit (MAB) to construct the most suitable prompt for each question. Our extensive experiments on three benchmark datasets showcase that KnowGPT significantly enhances the existing methods. Notably, KnowGPT achieves an average improvement of 23.
&lt;/p&gt;</description></item><item><title>TimeDRL&#26159;&#19968;&#20010;&#20855;&#26377;&#35299;&#32544;&#21452;&#23618;&#23884;&#20837;&#30340;&#36890;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#38388;&#25139;&#32423;&#21035;&#21644;&#23454;&#20363;&#32423;&#21035;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#35299;&#32544;&#27966;&#29983;&#20197;&#21450;&#26102;&#38388;&#25139;-&#39044;&#27979;&#21644;&#23454;&#20363;-&#23545;&#27604;&#20219;&#21153;&#30340;&#21033;&#29992;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#20016;&#23500;&#34920;&#31034;&#24182;&#35299;&#20915;&#24402;&#32435;&#20559;&#24046;&#30340;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2312.04142</link><description>&lt;p&gt;
TimeDRL&#65306;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#30340;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TimeDRL: Disentangled Representation Learning for Multivariate Time-Series
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.04142
&lt;/p&gt;
&lt;p&gt;
TimeDRL&#26159;&#19968;&#20010;&#20855;&#26377;&#35299;&#32544;&#21452;&#23618;&#23884;&#20837;&#30340;&#36890;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#38388;&#25139;&#32423;&#21035;&#21644;&#23454;&#20363;&#32423;&#21035;&#30340;&#23884;&#20837;&#20043;&#38388;&#30340;&#35299;&#32544;&#27966;&#29983;&#20197;&#21450;&#26102;&#38388;&#25139;-&#39044;&#27979;&#21644;&#23454;&#20363;-&#23545;&#27604;&#20219;&#21153;&#30340;&#21033;&#29992;&#65292;&#23454;&#29616;&#20102;&#23398;&#20064;&#20016;&#23500;&#34920;&#31034;&#24182;&#35299;&#20915;&#24402;&#32435;&#20559;&#24046;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20013;&#65288;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#24037;&#19994;&#65289;&#38750;&#24120;&#20855;&#26377;&#20449;&#24687;&#37327;&#65292;&#20294;&#30001;&#20110;&#32570;&#20047;&#26631;&#31614;&#21644;&#39640;&#32500;&#24230;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26368;&#36817;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30740;&#31350;&#26174;&#31034;&#20102;&#22312;&#23398;&#20064;&#20016;&#23500;&#34920;&#31034;&#32780;&#19981;&#20381;&#36182;&#20110;&#26631;&#31614;&#30340;&#28508;&#21147;&#65292;&#20294;&#23427;&#20204;&#22312;&#23398;&#20064;&#35299;&#32544;&#23884;&#20837;&#21644;&#35299;&#20915;&#24402;&#32435;&#20559;&#24046;&#65288;&#20363;&#22914;&#21464;&#25442;&#19981;&#21464;&#24615;&#65289;&#26041;&#38754;&#36824;&#26377;&#19981;&#36275;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TimeDRL&#65292;&#19968;&#20010;&#20855;&#26377;&#35299;&#32544;&#21452;&#23618;&#23884;&#20837;&#30340;&#36890;&#29992;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#12290;TimeDRL&#30340;&#19977;&#20010;&#26032;&#39062;&#29305;&#24449;&#20026;&#65306;&#65288;i&#65289;&#20351;&#29992;[CLS]&#20196;&#29260;&#31574;&#30053;&#20174;&#25171;&#34917;&#19969;&#30340;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#35299;&#32544;&#26102;&#38388;&#25139;&#32423;&#21644;&#23454;&#20363;&#32423;&#23884;&#20837;&#65307;&#65288;ii&#65289;&#21033;&#29992;&#26102;&#38388;&#25139;&#39044;&#27979;&#21644;&#23454;&#20363;&#23545;&#27604;&#20219;&#21153;&#36827;&#34892;&#35299;&#32544;&#34920;&#31034;&#23398;&#20064;&#65292;&#21069;&#32773;&#20248;&#21270;&#26102;&#38388;&#25139;&#32423;&#21035;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.04142v2 Announce Type: replace-cross  Abstract: Multivariate time-series data in numerous real-world applications (e.g., healthcare and industry) are informative but challenging due to the lack of labels and high dimensionality. Recent studies in self-supervised learning have shown their potential in learning rich representations without relying on labels, yet they fall short in learning disentangled embeddings and addressing issues of inductive bias (e.g., transformation-invariance). To tackle these challenges, we propose TimeDRL, a generic multivariate time-series representation learning framework with disentangled dual-level embeddings. TimeDRL is characterized by three novel features: (i) disentangled derivation of timestamp-level and instance-level embeddings from patched time-series data using a [CLS] token strategy; (ii) utilization of timestamp-predictive and instance-contrastive tasks for disentangled representation learning, with the former optimizing timestamp-lev
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#25366;&#25496;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#26412;&#22320;LLMs&#26469;&#35299;&#20915;&#36890;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#38382;&#39064;&#65292;&#30830;&#20445;&#25968;&#25454;&#23433;&#20840;&#24182;&#36827;&#34892;&#36827;&#19968;&#27493;&#35843;&#25972;</title><link>https://arxiv.org/abs/2312.01678</link><description>&lt;p&gt;
Jellyfish&#65306;&#19968;&#20010;&#29992;&#20110;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Jellyfish: A Large Language Model for Data Preprocessing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.01678
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#25366;&#25496;&#20013;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25351;&#23548;&#35843;&#25972;&#26412;&#22320;LLMs&#26469;&#35299;&#20915;&#36890;&#29992;&#25968;&#25454;&#39044;&#22788;&#29702;&#38382;&#39064;&#65292;&#30830;&#20445;&#25968;&#25454;&#23433;&#20840;&#24182;&#36827;&#34892;&#36827;&#19968;&#27493;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#25968;&#25454;&#25366;&#25496;&#31649;&#36947;&#20013;&#23558;&#21407;&#22987;&#25968;&#25454;&#36716;&#25442;&#20026;&#26377;&#21033;&#20110;&#31616;&#21333;&#22788;&#29702;&#30340;&#24178;&#20928;&#26684;&#24335;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#65288;DP&#65289;&#20013;LLMs&#30340;&#21033;&#29992;&#12290;&#19982;&#20351;&#29992;LLMs&#20026;DP&#35774;&#35745;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#24341;&#36215;&#20102;&#20852;&#36259;&#30456;&#27604;&#65292;&#26368;&#36817;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#20513;&#35758;&#36890;&#24120;&#20381;&#36182;&#20110;GPT API&#65292;&#24341;&#21457;&#20102;&#19981;&#21487;&#36991;&#20813;&#30340;&#25968;&#25454;&#27844;&#38671;&#25285;&#24551;&#12290;&#19982;&#36825;&#20123;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#32771;&#34385;&#23558;&#25351;&#23548;&#35843;&#25972;&#26412;&#22320;LLMs&#65288;7-13B&#27169;&#22411;&#65289;&#20316;&#20026;&#36890;&#29992;DP&#38382;&#35299;&#22120;&#12290;&#25105;&#20204;&#36873;&#25321;&#20102;&#20195;&#34920;&#24615;DP&#20219;&#21153;&#30340;&#22235;&#32452;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#38024;&#23545;DP&#23450;&#21046;&#30340;&#24207;&#21015;&#21270;&#21644;&#30693;&#35782;&#27880;&#20837;&#25216;&#26415;&#26500;&#24314;&#20102;&#25351;&#23548;&#35843;&#25972;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#25351;&#23548;&#35843;&#25972;&#30340;LLMs&#20351;&#29992;&#25143;&#33021;&#22815;&#20026;DP&#25163;&#21160;&#21046;&#23450;&#25351;&#23548;&#12290;&#21516;&#26102;&#65292;&#23427;&#20204;&#21487;&#20197;&#22312;&#26412;&#22320;&#12289;&#21333;&#19968;&#21644;&#20215;&#26684;&#20302;&#24265;&#30340;GPU&#19978;&#36816;&#34892;&#65292;&#30830;&#20445;&#25968;&#25454;&#23433;&#20840;&#24182;&#23454;&#29616;&#36827;&#19968;&#27493;&#35843;&#25972;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#20026;DP&#25351;&#23548;&#26500;&#24314;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.01678v4 Announce Type: replace  Abstract: This paper explores the utilization of LLMs for data preprocessing (DP), a crucial step in the data mining pipeline that transforms raw data into a clean format conducive to easy processing. Whereas the use of LLMs has sparked interest in devising universal solutions to DP, recent initiatives in this domain typically rely on GPT APIs, raising inevitable data breach concerns. Unlike these approaches, we consider instruction-tuning local LLMs (7 - 13B models) as universal DP ask solver. We select a collection of datasets across four representative DP tasks and construct instruction-tuning data using serialization and knowledge injection techniques tailored to DP. As such, the instruction-tuned LLMs empower users to manually craft instructions for DP. Meanwhile, they can operate on a local, single, and low-priced GPU, ensuring data security and enabling further tuning. Our experiments show that our dataset constructed for DP instruction
&lt;/p&gt;</description></item><item><title>MLLMs&#36890;&#36807;&#20026;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#24314;&#31435;&#26356;&#20016;&#23500;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#65292;&#20197;&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#8220;&#25991;&#26412;&#21098;&#20999;&#8221;&#26041;&#27861;&#26469;&#36991;&#20813;&#20559;&#35265;&#24341;&#20837;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.18765</link><description>&lt;p&gt;
MLLMs&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MLLMs-Augmented Visual-Language Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.18765
&lt;/p&gt;
&lt;p&gt;
MLLMs&#36890;&#36807;&#20026;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#24314;&#31435;&#26356;&#20016;&#23500;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#65292;&#20197;&#22686;&#24378;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#65292;&#24182;&#36890;&#36807;&#8220;&#25991;&#26412;&#21098;&#20999;&#8221;&#26041;&#27861;&#26469;&#36991;&#20813;&#20559;&#35265;&#24341;&#20837;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18765v3 &#20844;&#21578;&#31867;&#22411;: replace-cross &#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#22312;&#35768;&#22810;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#25104;&#21151;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#21151;&#20110;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21487;&#20197;&#36890;&#36807;&#20026;&#22270;&#20687;-&#25991;&#26412;&#25968;&#25454;&#38598;&#24314;&#31435;&#26356;&#20016;&#23500;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#26469;&#21152;&#24378;&#35270;&#35273;-&#35821;&#35328;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24456;&#31616;&#21333;&#65292;&#21033;&#29992;MLLMs&#20026;&#27599;&#20010;&#22270;&#20687;&#25193;&#23637;&#22810;&#20010;&#19981;&#21516;&#30340;&#26631;&#39064;&#12290;&#20026;&#20102;&#38450;&#27490;MLLMs&#30340;&#24187;&#35273;&#21644;&#21333;&#35843;&#35821;&#35328;&#39118;&#26684;&#24341;&#20837;&#30340;&#20559;&#35265;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#25991;&#26412;&#21098;&#20999;&#8221;&#26469;&#20445;&#25345;&#25193;&#23637;&#26631;&#39064;&#30340;&#36136;&#37327;&#21644;&#21487;&#29992;&#24615;&#12290;&#22312;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#20013;&#65292;&#22312;&#19981;&#24341;&#20837;&#39069;&#22806;&#30340;&#35757;&#32451;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#35843;&#21644;&#38646;-shot&#35774;&#32622;&#19979;&#19968;&#33268;&#22320;&#22312;Recall@1&#19978;&#33719;&#24471;&#20102;5.6 ~ 35.0&#21644;16.8 ~ 46.1&#30340;&#25913;&#36827;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#33719;&#24471;&#20102;&#19982;&#22312;&#30446;&#26631;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#30456;&#24403;&#30340;&#38646;-shot&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.18765v3 Announce Type: replace-cross  Abstract: Visual-language pre-training has achieved remarkable success in many multi-modal tasks, largely attributed to the availability of large-scale image-text datasets. In this work, we demonstrate that Multi-modal Large Language Models (MLLMs) can enhance visual-language representation learning by establishing richer image-text associations for image-text datasets. Our approach is simple, utilizing MLLMs to extend multiple diverse captions for each image. To prevent the bias introduced by MLLMs' hallucinations and monotonous language styles, we propose "text shearing" to maintain the quality and availability of extended captions. In image-text retrieval, without introducing additional training cost, our method consistently obtains 5.6 ~ 35.0 and 16.8 ~ 46.1 improvement on Recall@1 under the fine-tuning and zero-shot settings, respectively. Notably, we obtain zero-shot results that are comparable to fine-tuning on target datasets, wh
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;</title><link>https://arxiv.org/abs/2311.16480</link><description>&lt;p&gt;
&#30149;&#29702;&#25253;&#21578;&#30340;&#22810;&#23454;&#20363;&#29983;&#25104;&#29992;&#20110;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
WsiCaption: Multiple Instance Generation of Pathology Reports for Gigapixel Whole-Slide Images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16480
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#29983;&#25104;&#21315;&#20159;&#20687;&#32032;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#27169;&#22411;&#33021;&#22815;&#20135;&#29983;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20840;&#20999;&#29255;&#22270;&#20687;&#26159;&#29992;&#20110;&#30284;&#30151;&#35786;&#26029;&#21644;&#27835;&#30103;&#30340;&#25968;&#23383;&#30149;&#29702;&#23398;&#30340;&#22522;&#30784;&#12290;&#25776;&#20889;&#30149;&#29702;&#25253;&#21578;&#23545;&#32463;&#39564;&#19981;&#36275;&#30340;&#30149;&#29702;&#23398;&#23478;&#26469;&#35828;&#26159;&#36153;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#12290;&#20026;&#20102;&#20943;&#23569;&#24037;&#20316;&#37327;&#24182;&#25913;&#21892;&#20020;&#24202;&#33258;&#21160;&#21270;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#29983;&#25104;&#32473;&#23450;&#20840;&#20999;&#29255;&#22270;&#20687;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;&#22312;&#25968;&#25454;&#31471;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#26368;&#22823;&#30340;WSI-&#25991;&#26412;&#25968;&#25454;&#38598;&#65288;TCGA-PathoText&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#35782;&#21035;&#21644;&#28165;&#29702;TCGA&#20013;&#21465;&#36848;&#35786;&#26029;&#24187;&#28783;&#29255;&#30340;&#30149;&#29702;&#25253;&#21578;&#65292;&#25910;&#38598;&#20102;&#36817;1&#19975;&#23545;&#39640;&#36136;&#37327;&#30340;WSI-&#25991;&#26412;&#37197;&#23545;&#65292;&#20379;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#20351;&#29992;&#12290;&#22312;&#27169;&#22411;&#31471;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21487;&#20197;&#20026;&#21315;&#20159;&#20687;&#32032;WSI&#29983;&#25104;&#30149;&#29702;&#25253;&#21578;&#30340;&#22810;&#23454;&#20363;&#29983;&#25104;&#27169;&#22411;&#65288;MI-Gen&#65289;&#12290;&#25105;&#20204;&#22312;TCGA-PathoText&#30340;&#26368;&#22823;&#23376;&#38598;&#19978;&#23545;&#25105;&#20204;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21487;&#20197;&#29983;&#25104;&#21253;&#21547;&#22810;&#20010;&#20020;&#24202;&#32447;&#32034;&#30340;&#30149;&#29702;&#25253;&#21578;&#12290;&#27492;&#22806;&#65292;WSI-&#25991;&#26412;&#39044;&#27979;&#21487;&#34987;&#35270;&#20026;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16480v2 Announce Type: replace-cross  Abstract: Whole slide images are the foundation of digital pathology for the diagnosis and treatment of carcinomas. Writing pathology reports is laborious and error-prone for inexperienced pathologists. To reduce the workload and improve clinical automation, we investigate how to generate pathology reports given whole slide images. On the data end, we curated the largest WSI-text dataset (TCGA-PathoText). In specific, we collected nearly 10000 high-quality WSI-text pairs for visual-language models by recognizing and cleaning pathology reports which narrate diagnostic slides in TCGA. On the model end, we propose the multiple instance generative model (MI-Gen) which can produce pathology reports for gigapixel WSIs. We benchmark our model on the largest subset of TCGA-PathoText. Experimental results show our model can generate pathology reports which contain multiple clinical clues. Furthermore, WSI-text prediction can be seen as an approac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23433;&#20840;&#30340;&#22240;&#26524;&#34920;&#31034;&#26041;&#27861;FUSION&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#32467;&#26500;&#21270;&#24773;&#26223;&#20449;&#24687;&#20419;&#36827;&#27867;&#21270;&#30340;&#31471;&#21040;&#31471;&#39550;&#39542;&#31574;&#30053;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2311.10747</link><description>&lt;p&gt;
&#38754;&#21521;&#23433;&#20840;&#30340;&#22240;&#26524;&#34920;&#31034;&#26041;&#27861;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safety-aware Causal Representation for Trustworthy Offline Reinforcement Learning in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#23433;&#20840;&#30340;&#22240;&#26524;&#34920;&#31034;&#26041;&#27861;FUSION&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#21033;&#29992;&#32467;&#26500;&#21270;&#24773;&#26223;&#20449;&#24687;&#20419;&#36827;&#27867;&#21270;&#30340;&#31471;&#21040;&#31471;&#39550;&#39542;&#31574;&#30053;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#35299;&#20915;&#26469;&#33258;&#31163;&#32447;&#25968;&#25454;&#38598;&#30340;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#26041;&#38754;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#21508;&#31181;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#20445;&#25345;&#23433;&#20840;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#22240;&#20026;&#31163;&#32447;&#25968;&#25454;&#38598;&#20013;&#32570;&#20047;&#38271;&#23614;&#21644;&#26080;&#27861;&#39044;&#35265;&#30340;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;saFety-aware strUctured Scenario representatION (FUSION)&#65292;&#36825;&#26159;&#19968;&#31181;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#24320;&#21019;&#24615;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#32467;&#26500;&#21270;&#22330;&#26223;&#20449;&#24687;&#20419;&#36827;&#23398;&#20064;&#21487;&#27867;&#21270;&#30340;&#31471;&#21040;&#31471;&#39550;&#39542;&#31574;&#30053;&#12290;FUSION&#21033;&#29992;&#20998;&#35299;&#22870;&#21169;&#12289;&#25104;&#26412;&#12289;&#29366;&#24577;&#21644;&#21160;&#20316;&#31354;&#38388;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#22312;&#21160;&#24577;&#20132;&#36890;&#29615;&#22659;&#20013;&#36827;&#34892;&#32467;&#26500;&#21270;&#39034;&#24207;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#20004;&#20010;&#20856;&#22411;&#30495;&#23454;&#19990;&#30028;&#35774;&#23450;&#19979;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10747v3 Announce Type: replace-cross  Abstract: In the domain of autonomous driving, the offline Reinforcement Learning~(RL) approaches exhibit notable efficacy in addressing sequential decision-making problems from offline datasets. However, maintaining safety in diverse safety-critical scenarios remains a significant challenge due to long-tailed and unforeseen scenarios absent from offline datasets. In this paper, we introduce the saFety-aware strUctured Scenario representatION (FUSION), a pioneering representation learning method in offline RL to facilitate the learning of a generalizable end-to-end driving policy by leveraging structured scenario information. FUSION capitalizes on the causal relationships between the decomposed reward, cost, state, and action space, constructing a framework for structured sequential reasoning in dynamic traffic environments. We conduct extensive evaluations in two typical real-world settings of the distribution shift in autonomous vehicl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#24187;&#35273;&#21644;&#19981;&#24544;&#23454;&#25512;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25506;&#27979;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#20197;&#30740;&#31350;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#21542;&#20250;&#37319;&#21462;&#27450;&#39575;&#24615;&#35821;&#20041;&#24555;&#25463;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2311.09702</link><description>&lt;p&gt;
&#25512;&#29702;&#38142;&#19978;&#30340;&#27450;&#39575;&#24615;&#35821;&#20041;&#24555;&#25463;&#26041;&#24335;&#65306;&#27169;&#22411;&#22312;&#27809;&#26377;&#24187;&#35273;&#30340;&#24773;&#20917;&#19979;&#33021;&#36208;&#22810;&#36828;&#65311;
&lt;/p&gt;
&lt;p&gt;
Deceptive Semantic Shortcuts on Reasoning Chains: How Far Can Models Go without Hallucination?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.09702
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#30340;&#24187;&#35273;&#21644;&#19981;&#24544;&#23454;&#25512;&#29702;&#38382;&#39064;&#65292;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#25506;&#27979;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#20197;&#30740;&#31350;LLMs&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#26159;&#21542;&#20250;&#37319;&#21462;&#27450;&#39575;&#24615;&#35821;&#20041;&#24555;&#25463;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36817;&#26399;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#24182;&#22312;&#20247;&#22810;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;LLMs&#23384;&#22312;&#24187;&#35273;&#21644;&#19981;&#24544;&#23454;&#25512;&#29702;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#29305;&#23450;&#31867;&#22411;&#30001;&#35821;&#20041;&#20851;&#32852;&#24341;&#36215;&#30340;&#24187;&#35273;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;LLMs&#22312;&#25552;&#31034;&#20013;&#26159;&#21542;&#20250;&#22240;&#20026;&#26576;&#20123;&#20851;&#38190;&#23383;/&#23454;&#20307;&#20559;&#35265;&#32780;&#37319;&#21462;&#25463;&#24452;&#65292;&#32780;&#19981;&#26159;&#36981;&#24490;&#27491;&#30830;&#30340;&#25512;&#29702;&#36335;&#24452;&#12290;&#20026;&#20102;&#37327;&#21270;&#36825;&#19968;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EureQA&#30340;&#26032;&#22411;&#25506;&#27979;&#26041;&#27861;&#21644;&#22522;&#20934;&#27979;&#35797;&#12290;&#25105;&#20204;&#20174;LLMs&#20250;&#20197;&#32477;&#23545;&#30830;&#23450;&#24615;&#27491;&#30830;&#22238;&#31572;&#30340;&#38382;&#39064;&#24320;&#22987;&#65292;&#28982;&#21518;&#36882;&#24402;&#22320;&#29992;&#35777;&#25454;&#21477;&#23376;&#36974;&#34109;&#37325;&#35201;&#23454;&#20307;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;&#22238;&#31572;&#38382;&#39064;&#20043;&#21069;&#25214;&#21040;&#26681;&#25454;&#35777;&#25454;&#38142;&#26465;&#36974;&#34109;&#30340;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.09702v2 Announce Type: replace-cross  Abstract: Despite the recent advancement in large language models (LLMs) and their high performances across numerous benchmarks, recent research has unveiled that LLMs suffer from hallucinations and unfaithful reasoning. This work studies a specific type of hallucination induced by semantic associations. Specifically, we investigate to what extent LLMs take shortcuts from certain keyword/entity biases in the prompt instead of following the correct reasoning path. To quantify this phenomenon, we propose a novel probing method and benchmark called EureQA. We start from questions that LLMs will answer correctly with utmost certainty, and mask the important entity with evidence sentence recursively, asking models to find masked entities according to a chain of evidence before answering the question.   During the construction of the evidence, we purposefully replace semantic clues (entities) that may lead to the correct answer with distractor
&lt;/p&gt;</description></item><item><title>Agent Lumos&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35268;&#21010;&#27169;&#22359;&#23398;&#20064;&#39640;&#32423;&#23376;&#30446;&#26631;&#29983;&#25104;&#65292;&#35757;&#32451;&#25509;&#22320;&#27169;&#22359;&#23558;&#20854;&#36716;&#21270;&#20026;&#21160;&#20316;&#65292;&#20419;&#36827;&#24191;&#27867;&#20114;&#21160;&#20219;&#21153;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2311.05657</link><description>&lt;p&gt;
Agent Lumos: &#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#35757;&#32451;&#24320;&#28304;&#35821;&#35328;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Agent Lumos: Unified and Modular Training for Open-Source Language Agents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.05657
&lt;/p&gt;
&lt;p&gt;
Agent Lumos&#25552;&#20986;&#20102;&#19968;&#31181;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#35268;&#21010;&#27169;&#22359;&#23398;&#20064;&#39640;&#32423;&#23376;&#30446;&#26631;&#29983;&#25104;&#65292;&#35757;&#32451;&#25509;&#22320;&#27169;&#22359;&#23558;&#20854;&#36716;&#21270;&#20026;&#21160;&#20316;&#65292;&#20419;&#36827;&#24191;&#27867;&#20114;&#21160;&#20219;&#21153;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38381;&#28304;&#20195;&#29702;&#23384;&#22312;&#35832;&#22810;&#38382;&#39064;&#65292;&#22914;&#32570;&#20047;&#36127;&#25285;&#24471;&#36215;&#24615;&#12289;&#36879;&#26126;&#24230;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#30340;&#20114;&#21160;&#20219;&#21153;&#20013;&#12290;&#36825;&#20419;&#20351;&#20102;&#24320;&#28304;&#26367;&#20195;&#26041;&#26696;&#30340;&#21457;&#23637;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102; LUMOS&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#20026;&#35757;&#32451;&#24320;&#28304; LLM-based &#20195;&#29702;&#32780;&#35774;&#35745;&#30340;&#26694;&#26550;&#20043;&#19968;&#12290;LUMOS&#20855;&#26377;&#21487;&#23398;&#20064;&#12289;&#32479;&#19968;&#21644;&#27169;&#22359;&#21270;&#30340;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#25324;&#19968;&#20010;&#23398;&#20064;&#39640;&#32423;&#23376;&#30446;&#26631;&#29983;&#25104;&#30340;&#35268;&#21010;&#27169;&#22359;&#65292;&#20197;&#21450;&#19968;&#20010;&#35757;&#32451;&#26377;&#32032;&#30340;&#25509;&#22320;&#27169;&#22359;&#65292;&#29992;&#20110;&#20351;&#29992;&#25191;&#34892;&#27169;&#22359;&#20013;&#30340;&#21508;&#31181;&#24037;&#20855;&#23558;&#36825;&#20123;&#36716;&#21270;&#20026;&#21160;&#20316;&#12290;&#36825;&#31181;&#35774;&#35745;&#20801;&#35768;&#27169;&#22359;&#21270;&#21319;&#32423;&#65292;&#24182;&#26356;&#24191;&#27867;&#22320;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#20114;&#21160;&#20219;&#21153;&#12290;&#20026;&#20102;&#20419;&#36827;&#36890;&#29992;&#20195;&#29702;&#23398;&#20064;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#28304;&#33258;&#21508;&#31181;&#22797;&#26434;&#20114;&#21160;&#20219;&#21153;&#20013;&#19981;&#21516;&#22320;&#38754;&#30495;&#23454;&#25512;&#29702;&#21407;&#29702;&#30340;&#22823;&#35268;&#27169;&#12289;&#32479;&#19968;&#21644;&#39640;&#36136;&#37327;&#30340;&#35757;&#32451;&#27880;&#37322;&#12290;&#22312;9&#20010;&#25968;&#25454;&#38598;&#19978;&#65292;LUMOS&#34920;&#29616;&#20986;&#20102;&#20960;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#65288;1&#65289;LUMOS&#22312;&#22810;&#20010;&#36739;&#22823;&#30340;&#24320;&#28304;a
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.05657v2 Announce Type: replace  Abstract: Closed-source agents suffer from several issues such as a lack of affordability, transparency, and reproducibility, particularly on complex interactive tasks. This motivates the development of open-source alternatives. We introduce LUMOS, one of the first frameworks for training open-source LLM-based agents. LUMOS features a learnable, unified, and modular architecture with a planning module that learns high-level subgoal generation, and a grounding module trained to translate these into actions using various tools in the execution module. The design allows for modular upgrades and wider applicability to diverse interactive tasks. To foster generalizable agent learning, we collect large-scale, unified, and high-quality training annotations derived from diverse ground-truth reasoning rationales across various complex interactive tasks. On 9 datasets, LUMOS exhibits several key advantages: (1) LUMOS excels multiple larger open-source a
&lt;/p&gt;</description></item><item><title>&#22312;&#22522;&#20110;&#20559;&#22909;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#21270;&#35774;&#35745;&#30340;&#31639;&#27861;&#22312;&#32447;&#24615;MDP&#27169;&#22411;&#19979;&#34920;&#29616;&#20986;&#26679;&#26412;&#39640;&#25928;&#24615;&#21644;&#22810;&#39033;&#24335;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#21270;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#26368;&#23567;&#21270;&#20102;&#26597;&#35810;&#22797;&#26434;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.14554</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#21270;&#20351;&#22522;&#20110;&#20559;&#22909;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#21464;&#24471;&#39640;&#25928;
&lt;/p&gt;
&lt;p&gt;
Making RL with Preference-based Feedback Efficient via Randomization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14554
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#20559;&#22909;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#38543;&#26426;&#21270;&#35774;&#35745;&#30340;&#31639;&#27861;&#22312;&#32447;&#24615;MDP&#27169;&#22411;&#19979;&#34920;&#29616;&#20986;&#26679;&#26412;&#39640;&#25928;&#24615;&#21644;&#22810;&#39033;&#24335;&#36816;&#34892;&#26102;&#38388;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#21270;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#26368;&#23567;&#21270;&#20102;&#26597;&#35810;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#38656;&#35201;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#65292;&#32780;&#19988;&#22312;&#32479;&#35745;&#22797;&#26434;&#24615;&#12289;&#35745;&#31639;&#22797;&#26434;&#24615;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#26041;&#38754;&#38656;&#35201;&#39640;&#25928;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#20351;&#29992;&#20559;&#22909;&#26469;&#34920;&#36798;&#23545;&#36712;&#36857;&#23545;&#30340;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#12290;&#22312;&#32447;&#24615;MDP&#27169;&#22411;&#20013;&#65292;&#36890;&#36807;&#22312;&#31639;&#27861;&#35774;&#35745;&#20013;&#24341;&#20837;&#38543;&#26426;&#21270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20855;&#26377;&#26679;&#26412;&#39640;&#25928;&#24615;&#65288;&#21363;&#20855;&#26377;&#36817;&#20046;&#26368;&#20248;&#30340;&#26368;&#22351;&#24773;&#20917;&#36951;&#25022;&#30028;&#38480;&#65289;&#21644;&#22810;&#39033;&#24335;&#36816;&#34892;&#26102;&#38388;&#65288;&#21363;&#35745;&#31639;&#22797;&#26434;&#24230;&#19982;&#30456;&#20851;&#21442;&#25968;&#26159;&#22810;&#39033;&#24335;&#20851;&#31995;&#65289;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#36827;&#19968;&#27493;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#38543;&#26426;&#21270;&#20027;&#21160;&#23398;&#20064;&#36807;&#31243;&#26368;&#23567;&#21270;&#20102;&#26597;&#35810;&#22797;&#26434;&#24615;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#23637;&#31034;&#20102;&#36951;&#25022;&#30028;&#38480;&#21644;&#26597;&#35810;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#36817;&#20046;&#26368;&#20248;&#25240;&#34935;&#12290;&#20026;&#20102;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#26356;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#36924;&#36817;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21463;&#32447;&#24615;MDP&#27169;&#22411;&#30340;&#38543;&#26426;&#21270;&#31639;&#27861;&#21551;&#21457;&#32780;&#26469;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#38543;&#26426;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14554v2 Announce Type: replace-cross  Abstract: Reinforcement Learning algorithms that learn from human feedback (RLHF) need to be efficient in terms of statistical complexity, computational complexity, and query complexity. In this work, we consider the RLHF setting where the feedback is given in the format of preferences over pairs of trajectories. In the linear MDP model, using randomization in algorithm design, we present an algorithm that is sample efficient (i.e., has near-optimal worst-case regret bounds) and has polynomial running time (i.e., computational complexity is polynomial with respect to relevant parameters). Our algorithm further minimizes the query complexity through a novel randomized active learning procedure. In particular, our algorithm demonstrates a near-optimal tradeoff between the regret bound and the query complexity. To extend the results to more general nonlinear function approximation, we design a model-based randomized algorithm inspired by th
&lt;/p&gt;</description></item><item><title>&#20998;&#35789;&#22120;&#26159;&#35270;&#35273;&#29983;&#25104;&#30340;&#20851;&#38190;&#65292;&#26032;&#30340;&#35270;&#39057;&#20998;&#35789;&#22120;MAGVIT-v2&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#19978;&#32988;&#36807;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#35270;&#39057;&#21387;&#32553;&#21644;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>https://arxiv.org/abs/2310.05737</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#20987;&#36133;&#25193;&#25955;&#27169;&#22411;--&#20998;&#35789;&#22120;&#26159;&#35270;&#35273;&#29983;&#25104;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05737
&lt;/p&gt;
&lt;p&gt;
&#20998;&#35789;&#22120;&#26159;&#35270;&#35273;&#29983;&#25104;&#30340;&#20851;&#38190;&#65292;&#26032;&#30340;&#35270;&#39057;&#20998;&#35789;&#22120;MAGVIT-v2&#20351;&#24471;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;LLMs&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#20219;&#21153;&#19978;&#32988;&#36807;&#25193;&#25955;&#27169;&#22411;&#65292;&#24182;&#22312;&#35270;&#39057;&#21387;&#32553;&#21644;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#35821;&#35328;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#20027;&#23548;&#27169;&#22411;&#65292;&#20294;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#19981;&#22914;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#21033;&#29992;LLMs&#36827;&#34892;&#35270;&#35273;&#29983;&#25104;&#65292;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#32452;&#20214;&#26159;&#35270;&#35273;&#20998;&#35789;&#22120;&#65292;&#23427;&#23558;&#20687;&#32032;&#31354;&#38388;&#36755;&#20837;&#26144;&#23556;&#21040;&#36866;&#21512;LLM&#23398;&#20064;&#30340;&#31163;&#25955;&#26631;&#35760;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;MAGVIT-v2&#65292;&#19968;&#20010;&#35270;&#39057;&#20998;&#35789;&#22120;&#65292;&#26088;&#22312;&#20351;&#29992;&#20849;&#21516;&#30340;&#26631;&#35760;&#35789;&#27719;&#20026;&#35270;&#39057;&#21644;&#22270;&#20687;&#29983;&#25104;&#31616;&#27905;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#26631;&#35760;&#12290;&#37197;&#22791;&#20102;&#36825;&#20010;&#26032;&#30340;&#20998;&#35789;&#22120;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;LLMs&#22312;&#26631;&#20934;&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#22522;&#20934;&#19978;&#20248;&#20110;&#25193;&#25955;&#27169;&#22411;&#65292;&#21253;&#25324;ImageNet&#21644;Kinetics&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#20998;&#35789;&#22120;&#22312;&#20004;&#39033;&#20219;&#21153;&#19978;&#36229;&#36807;&#20102;&#20808;&#21069;&#34920;&#29616;&#26368;&#20339;&#30340;&#35270;&#39057;&#20998;&#35789;&#22120;&#65306;(1)&#26681;&#25454;&#20154;&#31867;&#35780;&#20272;&#65292;&#35270;&#39057;&#21387;&#32553;&#19982;&#19979;&#19968;&#20195;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;(VCC)&#30456;&#23218;&#32654;&#65292;(2)&#23398;&#20064;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05737v2 Announce Type: replace-cross  Abstract: While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RoPE&#22522;&#22806;&#25512;&#30340;&#23610;&#24230;&#24459;&#65292;&#36890;&#36807;&#35843;&#25972; RoPE &#20013;&#30340;&#22522;&#25968;&#21644;&#24494;&#35843;&#25991;&#26412;&#38271;&#24230;&#26469;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22806;&#25512;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2310.05209</link><description>&lt;p&gt;
RoPE&#22522;&#22806;&#25512;&#30340;&#23610;&#24230;&#24459;
&lt;/p&gt;
&lt;p&gt;
Scaling Laws of RoPE-based Extrapolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;RoPE&#22522;&#22806;&#25512;&#30340;&#23610;&#24230;&#24459;&#65292;&#36890;&#36807;&#35843;&#25972; RoPE &#20013;&#30340;&#22522;&#25968;&#21644;&#24494;&#35843;&#25991;&#26412;&#38271;&#24230;&#26469;&#26174;&#33879;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22806;&#25512;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Rotary Position Embedding&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22806;&#25512;&#33021;&#21147;&#26159;&#30446;&#21069;&#22791;&#21463;&#20851;&#27880;&#30340;&#35805;&#39064;&#12290;&#29992;&#20110;&#35299;&#20915;LLMs&#22806;&#25512;&#38382;&#39064;&#30340;&#20027;&#27969;&#26041;&#27861;&#26159;&#36890;&#36807;&#23558;RoPE&#20013;&#30340;10000, $\theta_n={10000}^{-2n/d}$&#65292;&#36825;&#20010;&#26059;&#36716;&#22522;&#25968;&#65292;&#26367;&#25442;&#20026;&#26356;&#22823;&#30340;&#20540;&#65292;&#24182;&#25552;&#20379;&#26356;&#38271;&#30340;&#24494;&#35843;&#25991;&#26412;&#12290;&#26412;&#30740;&#31350;&#39318;&#20808;&#35266;&#23519;&#21040;&#65292;&#22312;&#39044;&#35757;&#32451;&#19978;&#29992;&#36739;&#23567;&#25110;&#36739;&#22823;&#30340;&#22522;&#25968;&#24494;&#35843;RoPE-based LLM&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20854;&#22806;&#25512;&#24615;&#33021;&#12290;&#20043;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RoPE&#22522;&#22806;&#25512;&#30340;&#23610;&#24230;&#24459;&#65292;&#36825;&#26159;&#19968;&#20010;&#20174;&#21608;&#26399;&#24615;&#30340;&#35282;&#24230;&#25551;&#36848;&#22806;&#25512;&#24615;&#33021;&#19982;&#22522;&#20540;&#20197;&#21450;&#35843;&#25972;&#19978;&#19979;&#25991;&#38271;&#24230;&#20043;&#38388;&#20851;&#31995;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;RoPE&#22522;&#22806;&#25512;&#38382;&#39064;&#30340;&#20851;&#38190;&#32500;&#24230;&#20171;&#32461;&#20102;&#20854;&#36215;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05209v2 Announce Type: replace-cross  Abstract: The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \textbf{\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \textbf{\textit{critical dimension for
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30452;&#25509;&#19982;&#23884;&#20837;&#20132;&#20114;&#65292;&#23558;&#25277;&#35937;&#21521;&#37327;&#36716;&#25442;&#20026;&#21487;&#29702;&#35299;&#30340;&#21465;&#36848;&#65292;&#20351;&#24471;&#22797;&#26434;&#23884;&#20837;&#25968;&#25454;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#24191;&#27867;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.04475</link><description>&lt;p&gt;
&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#23494;&#23884;&#20837;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Demystifying Embedding Spaces using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04475
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30452;&#25509;&#19982;&#23884;&#20837;&#20132;&#20114;&#65292;&#23558;&#25277;&#35937;&#21521;&#37327;&#36716;&#25442;&#20026;&#21487;&#29702;&#35299;&#30340;&#21465;&#36848;&#65292;&#20351;&#24471;&#22797;&#26434;&#23884;&#20837;&#25968;&#25454;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#24191;&#27867;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23884;&#20837;&#24050;&#32463;&#25104;&#20026;&#34920;&#31034;&#26377;&#20851;&#23454;&#20307;&#12289;&#27010;&#24565;&#21644;&#20851;&#31995;&#30340;&#22797;&#26434;&#22810;&#26041;&#38754;&#20449;&#24687;&#30340;&#20851;&#38190;&#25163;&#27573;&#65292;&#20197;&#19968;&#31181;&#32039;&#20945;&#19988;&#26377;&#29992;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#36890;&#24120;&#38590;&#20197;&#30452;&#25509;&#35299;&#37322;&#12290;&#23613;&#31649;&#19979;&#28216;&#20219;&#21153;&#21033;&#29992;&#20102;&#36825;&#20123;&#21387;&#32553;&#34920;&#31034;&#65292;&#20294;&#26377;&#24847;&#20041;&#30340;&#35299;&#37322;&#36890;&#24120;&#38656;&#35201;&#20351;&#29992;&#38477;&#32500;&#25110;&#19987;&#38376;&#30340;&#26426;&#22120;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#36827;&#34892;&#21487;&#35270;&#21270;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30452;&#25509;&#19982;&#23884;&#20837;&#20132;&#20114;&#65292;&#23558;&#25277;&#35937;&#21521;&#37327;&#36716;&#25442;&#20026;&#21487;&#29702;&#35299;&#30340;&#21465;&#36848;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20351;&#36825;&#20123;&#23884;&#20837;&#26356;&#20855;&#35299;&#37322;&#24615;&#21644;&#24191;&#27867;&#23454;&#29992;&#24615;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#23558;&#23884;&#20837;&#27880;&#20837;LLMs&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#23545;&#22797;&#26434;&#23884;&#20837;&#25968;&#25454;&#30340;&#26597;&#35810;&#21644;&#25506;&#32034;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#20219;&#21153;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#65306;&#22686;&#24378;&#27010;&#24565;&#28608;&#27963;&#21521;&#37327;&#65288;CAVs&#65289;&#12289;&#20256;&#36798;&#26032;&#39062;&#30340;&#23884;&#20837;&#23454;&#20307;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04475v2 Announce Type: replace-cross  Abstract: Embeddings have become a pivotal means to represent complex, multi-faceted information about entities, concepts, and relationships in a condensed and useful format. Nevertheless, they often preclude direct interpretation. While downstream tasks make use of these compressed representations, meaningful interpretation usually requires visualization using dimensionality reduction or specialized machine learning interpretability methods. This paper addresses the challenge of making such embeddings more interpretable and broadly useful, by employing Large Language Models (LLMs) to directly interact with embeddings -- transforming abstract vectors into understandable narratives. By injecting embeddings into LLMs, we enable querying and exploration of complex embedding data. We demonstrate our approach on a variety of diverse tasks, including: enhancing concept activation vectors (CAVs), communicating novel embedded entities, and decod
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;PECoRe&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#19978;&#19979;&#25991;&#20351;&#29992;&#24773;&#20917;&#65292;&#20174;&#32780;&#35780;&#20272;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;</title><link>https://arxiv.org/abs/2310.01188</link><description>&lt;p&gt;
&#37327;&#21270;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#32972;&#26223;&#20381;&#36182;&#24615;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Plausibility of Context Reliance in Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01188
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;PECoRe&#26694;&#26550;&#65292;&#29992;&#20110;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#30340;&#19978;&#19979;&#25991;&#20351;&#29992;&#24773;&#20917;&#65292;&#20174;&#32780;&#35780;&#20272;&#19978;&#19979;&#25991;&#24863;&#30693;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;PECoRe&#30340;&#31471;&#21040;&#31471;&#21487;&#35299;&#37322;&#24615;&#26694;&#26550;&#65292;&#26088;&#22312;&#37327;&#21270;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20013;&#19978;&#19979;&#25991;&#20351;&#29992;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#27169;&#22411;&#20869;&#37096;&#26469;&#23545;&#27604;&#35782;&#21035;&#29983;&#25104;&#25991;&#26412;&#20013;&#19978;&#19979;&#25991;&#25935;&#24863;&#30340;&#30446;&#26631;&#20196;&#29260;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#35777;&#26126;&#20854;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#32447;&#32034;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#20351;&#29992;PECORE&#26469;&#37327;&#21270;&#20855;&#26377;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;&#65292;&#23558;&#27169;&#22411;&#30340;&#29702;&#30001;&#19982;&#20154;&#31867;&#27880;&#37322;&#22312;&#20960;&#20010;&#23618;&#27425;&#30340;&#35805;&#35821;&#27700;&#24179;&#19978;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01188v2 Announce Type: replace-cross  Abstract: Establishing whether language models can use contextual information in a human-plausible way is important to ensure their trustworthiness in real-world settings. However, the questions of when and which parts of the context affect model generations are typically tackled separately, with current plausibility evaluations being practically limited to a handful of artificial benchmarks. To address this, we introduce Plausibility Evaluation of Context Reliance (PECoRe), an end-to-end interpretability framework designed to quantify context usage in language models' generations. Our approach leverages model internals to (i) contrastively identify context-sensitive target tokens in generated texts and (ii) link them to contextual cues justifying their prediction. We use \pecore to quantify the plausibility of context-aware machine translation models, comparing model rationales with human annotations across several discourse-level pheno
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#32773;&#36890;&#36807;&#35757;&#32451;&#32447;&#24615;Transformer&#27169;&#22411;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#65292;&#21457;&#29616;&#36825;&#31181;&#31616;&#21333;&#30340;&#32447;&#24615;&#21270;&#27169;&#22411;&#33021;&#22815;&#37325;&#29616;Transformer&#35757;&#32451;&#21160;&#24577;&#30340;&#22810;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#34920;&#26126;&#32447;&#24615;&#27880;&#24847;&#21147;&#21487;&#33021;&#26159;&#29702;&#35299;Transformer&#20248;&#21270;&#30340;&#20851;&#38190;&#12290;</title><link>https://arxiv.org/abs/2310.01082</link><description>&lt;p&gt;
&#32447;&#24615;&#27880;&#24847;&#21147;&#21487;&#33021;&#23601;&#26159;&#29702;&#35299;Transformer&#20248;&#21270;&#30340;&#20851;&#38190;
&lt;/p&gt;
&lt;p&gt;
Linear attention is (maybe) all you need (to understand transformer optimization)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.01082
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#32773;&#36890;&#36807;&#35757;&#32451;&#32447;&#24615;Transformer&#27169;&#22411;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#65292;&#21457;&#29616;&#36825;&#31181;&#31616;&#21333;&#30340;&#32447;&#24615;&#21270;&#27169;&#22411;&#33021;&#22815;&#37325;&#29616;Transformer&#35757;&#32451;&#21160;&#24577;&#30340;&#22810;&#20010;&#20851;&#38190;&#26041;&#38754;&#65292;&#34920;&#26126;&#32447;&#24615;&#27880;&#24847;&#21147;&#21487;&#33021;&#26159;&#29702;&#35299;Transformer&#20248;&#21270;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#30340;&#35757;&#32451;&#22240;&#38656;&#35201;&#20180;&#32454;&#35774;&#35745;&#20248;&#21270;&#22120;&#24182;&#20351;&#29992;&#21508;&#31181;&#21551;&#21457;&#24335;&#32780;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#36890;&#36807;&#20180;&#32454;&#30740;&#31350;&#19968;&#20010;&#31616;&#21333;&#20294;&#32463;&#20856;&#30340;&#32447;&#24615;&#21270;&#27973;&#23618;Transformer&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#35299;&#26512;Transformer&#35757;&#32451;&#32454;&#24494;&#20043;&#22788;&#30340;&#36827;&#23637;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35757;&#32451;&#32447;&#24615;Transformer&#26469;&#35299;&#20915;&#22238;&#24402;&#20219;&#21153;&#65292;&#28789;&#24863;&#26469;&#28304;&#20110;J. von Oswald&#31561;&#20154;&#65288;ICML 2023&#65289;&#21644;K. Ahn&#31561;&#20154;&#65288;NeurIPS 2023&#65289;&#12290;&#26368;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#25552;&#20986;&#30340;&#32447;&#24615;&#21270;&#27169;&#22411;&#33021;&#22815;&#37325;&#29616;Transformer&#35757;&#32451;&#21160;&#24577;&#30340;&#20960;&#20010;&#37325;&#35201;&#26041;&#38754;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24471;&#21040;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#32447;&#24615;&#21270;Transformer&#27169;&#22411;&#23454;&#38469;&#19978;&#21487;&#33021;&#26159;&#29702;&#35299;Transformer&#20248;&#21270;&#30340;&#26377;&#20215;&#20540;&#12289;&#29616;&#23454;&#30340;&#25277;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.01082v2 Announce Type: replace-cross  Abstract: Transformer training is notoriously difficult, requiring a careful design of optimizers and use of various heuristics. We make progress towards understanding the subtleties of training Transformers by carefully studying a simple yet canonical linearized shallow Transformer model. Specifically, we train linear Transformers to solve regression tasks, inspired by J.~von Oswald et al.~(ICML 2023), and K.~Ahn et al.~(NeurIPS 2023). Most importantly, we observe that our proposed linearized models can reproduce several prominent aspects of Transformer training dynamics. Consequently, the results obtained in this paper suggest that a simple linearized Transformer model could actually be a valuable, realistic abstraction for understanding Transformer optimization.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#22686;&#24378;MLLMs&#32454;&#31890;&#24230;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#65292;&#21253;&#25324;&#36890;&#36807;&#20302;&#25104;&#26412;&#26500;&#24314;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#21644;&#24341;&#20837;&#33258;&#19968;&#33268;&#30340;&#33258;&#20030;&#26041;&#27861;&#25193;&#23637;&#23494;&#38598;&#30446;&#26631;&#27880;&#37322;&#31561;&#20851;&#38190;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2310.00582</link><description>&lt;p&gt;
Pink: &#25581;&#31034;&#24341;&#29992;&#29702;&#35299;&#22312;&#22810;&#27169;&#24577;LLMs&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00582
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#26694;&#26550;&#26469;&#22686;&#24378;MLLMs&#32454;&#31890;&#24230;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#65292;&#21253;&#25324;&#36890;&#36807;&#20302;&#25104;&#26412;&#26500;&#24314;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#21644;&#24341;&#20837;&#33258;&#19968;&#33268;&#30340;&#33258;&#20030;&#26041;&#27861;&#25193;&#23637;&#23494;&#38598;&#30446;&#26631;&#27880;&#37322;&#31561;&#20851;&#38190;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#32454;&#31890;&#24230;&#22270;&#20687;&#29702;&#35299;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#20173;&#28982;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22686;&#24378;MLLMs&#32454;&#31890;&#24230;&#22270;&#20687;&#29702;&#35299;&#33021;&#21147;&#30340;&#26032;&#26694;&#26550;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#30340;&#27880;&#37322;&#26469;&#20197;&#20302;&#25104;&#26412;&#26500;&#24314;&#25351;&#20196;&#35843;&#20248;&#25968;&#25454;&#38598;&#30340;&#26032;&#26041;&#27861;&#12290;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#33258;&#19968;&#33268;&#30340;&#33258;&#20030;&#26041;&#27861;&#65292;&#23558;&#29616;&#26377;&#30340;&#23494;&#38598;&#30446;&#26631;&#27880;&#37322;&#25193;&#23637;&#20026;&#39640;&#36136;&#37327;&#30340;&#24341;&#29992;&#34920;&#36798;-&#36793;&#30028;&#26694;&#23545;&#12290;&#36825;&#20123;&#26041;&#27861;&#20351;&#24471;&#29983;&#25104;&#21253;&#21547;&#23545;&#32454;&#31890;&#24230;&#22270;&#20687;&#24863;&#30693;&#33267;&#20851;&#37325;&#35201;&#30340;&#24191;&#27867;&#22522;&#26412;&#33021;&#21147;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#25104;&#20026;&#21487;&#33021;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#35748;&#20026;&#22312;&#25351;&#20196;&#35843;&#20248;&#36807;&#31243;&#20013;&#24212;&#35813;&#35843;&#25972;&#35270;&#35273;&#32534;&#30721;&#22120;&#65292;&#20197;&#20943;&#36731;&#23436;&#25104;&#22270;&#20687;&#24863;&#30693;&#21644;&#25351;&#20196;&#29702;&#35299;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00582v3 Announce Type: replace-cross  Abstract: Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities in various multi-modal tasks. Nevertheless, their performance in fine-grained image understanding tasks is still limited. To address this issue, this paper proposes a new framework to enhance the fine-grained image understanding abilities of MLLMs. Specifically, we present a new method for constructing the instruction tuning dataset at a low cost by leveraging annotations in existing datasets. A self-consistent bootstrapping method is also introduced to extend existing dense object annotations into high-quality referring-expression-bounding-box pairs. These methods enable the generation of high-quality instruction data which includes a wide range of fundamental abilities essential for fine-grained image perception. Moreover, we argue that the visual encoder should be tuned during instruction tuning to mitigate the gap between full image perception and 
&lt;/p&gt;</description></item><item><title>CRAFT&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#24037;&#20855;&#21019;&#24314;&#21644;&#26816;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#23450;&#21046;LLMs&#65292;&#20026;&#20854;&#21019;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#24037;&#20855;&#38598;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#38598;&#22686;&#24378;&#20854;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2309.17428</link><description>&lt;p&gt;
CRAFT: &#36890;&#36807;&#21019;&#24314;&#21644;&#26816;&#32034;&#19987;&#19994;&#24037;&#20855;&#38598;&#23450;&#21046;LLMs
&lt;/p&gt;
&lt;p&gt;
CRAFT: Customizing LLMs by Creating and Retrieving from Specialized Toolsets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.17428
&lt;/p&gt;
&lt;p&gt;
CRAFT&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#24037;&#20855;&#21019;&#24314;&#21644;&#26816;&#32034;&#26694;&#26550;&#65292;&#33021;&#22815;&#23450;&#21046;LLMs&#65292;&#20026;&#20854;&#21019;&#24314;&#29305;&#23450;&#20219;&#21153;&#30340;&#24037;&#20855;&#38598;&#65292;&#24182;&#20351;&#29992;&#36825;&#20123;&#24037;&#20855;&#38598;&#22686;&#24378;&#20854;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36890;&#24120;&#36890;&#36807;&#29983;&#25104;&#20195;&#30721;&#29255;&#27573;&#24182;&#36890;&#36807;&#29305;&#23450;&#20219;&#21153;&#30340;&#24212;&#29992;&#31243;&#24207;&#32534;&#31243;&#25509;&#21475;&#65288;API&#65289;&#25191;&#34892;&#23427;&#20204;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;CRAFT&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;LLMs&#21019;&#24314;&#24037;&#20855;&#38598;&#30340;&#36890;&#29992;&#24037;&#20855;&#21019;&#24314;&#21644;&#26816;&#32034;&#26694;&#26550;&#12290;&#23427;&#20026;&#20219;&#21153;&#21019;&#24314;&#20102;&#29305;&#23450;&#30340;&#24037;&#20855;&#38598;&#65292;&#24182;&#20026;LLMs&#37197;&#22791;&#20102;&#19968;&#20010;&#32452;&#20214;&#65292;&#29992;&#20110;&#20174;&#36825;&#20123;&#38598;&#21512;&#20013;&#26816;&#32034;&#24037;&#20855;&#65292;&#20197;&#22686;&#24378;&#23427;&#20204;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.17428v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) are often augmented with tools to solve complex tasks. By generating code snippets and executing them through task-specific Application Programming Interfaces (APIs), they can offload certain functions to dedicated external modules, such as image encoding and performing calculations. However, most existing approaches to augment LLMs with tools are constrained by general-purpose APIs and lack the flexibility for tailoring them to specific tasks. In this work, we present CRAFT, a general tool creation and retrieval framework for LLMs. It creates toolsets specifically curated for the tasks and equips LLMs with a component that retrieves tools from these sets to enhance their capability to solve complex tasks. For each task, we collect specific code solutions by prompting GPT-4 to solve the training examples. Following a validation step ensuring the correctness, these solutions are abstracted into code 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38142;&#24335;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#38754;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#12289;&#20010;&#24615;&#21270;&#25351;&#23548;&#21644;&#28789;&#27963;&#27979;&#39564;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2309.08112</link><description>&lt;p&gt;
&#36890;&#36807;&#38142;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22686;&#24378;&#31169;&#20154;&#36741;&#23548;
&lt;/p&gt;
&lt;p&gt;
Empowering Private Tutoring by Chaining Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.08112
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38142;&#24335;&#36830;&#25509;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#20840;&#38754;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#65292;&#23454;&#29616;&#20102;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#12289;&#20010;&#24615;&#21270;&#25351;&#23548;&#21644;&#28789;&#27963;&#27979;&#39564;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#34987;&#24212;&#29992;&#20110;&#22312;&#32447;&#25945;&#32946;&#30340;&#21508;&#20010;&#26041;&#38754;&#65292;&#20197;&#20419;&#36827;&#25945;&#23398;&#21644;&#23398;&#20064;&#12290;&#28982;&#32780;&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#33268;&#21147;&#20110;&#23436;&#25972;&#30340;AI&#36741;&#23548;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19968;&#20010;&#30001;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#39537;&#21160;&#30340;&#20840;&#38754;&#26234;&#33021;&#36741;&#23548;&#31995;&#32479;&#30340;&#24320;&#21457;&#65292;&#28085;&#30422;&#33258;&#21160;&#35838;&#31243;&#35268;&#21010;&#21644;&#35843;&#25972;&#12289;&#23450;&#21046;&#25351;&#23548;&#20197;&#21450;&#28789;&#27963;&#30340;&#27979;&#39564;&#35780;&#20272;&#12290;&#20026;&#20102;&#20351;&#31995;&#32479;&#33021;&#22815;&#32463;&#21463;&#20303;&#38271;&#26102;&#38388;&#20132;&#20114;&#24182;&#28385;&#36275;&#20010;&#24615;&#21270;&#25945;&#32946;&#30340;&#38656;&#27714;&#65292;&#31995;&#32479;&#34987;&#20998;&#35299;&#20026;&#19977;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#26680;&#24515;&#27969;&#31243;-&#20132;&#20114;&#12289;&#21453;&#24605;&#21644;&#21453;&#24212;&#12290;&#27599;&#20010;&#27969;&#31243;&#37117;&#36890;&#36807;&#38142;&#25509;LLM&#39537;&#21160;&#30340;&#24037;&#20855;&#20197;&#21450;&#21160;&#24577;&#26356;&#26032;&#30340;&#35760;&#24518;&#27169;&#22359;&#26469;&#23454;&#29616;&#12290;&#24037;&#20855;&#26159;LLMs&#65292;&#34987;&#25552;&#31034;&#25191;&#34892;&#19968;&#39033;&#29305;&#23450;&#20219;&#21153;&#65292;&#32780;&#35760;&#24518;&#26159;&#22312;&#25945;&#32946;&#36807;&#31243;&#20013;&#26356;&#26032;&#30340;&#25968;&#25454;&#23384;&#20648;&#12290;&#23398;&#20064;&#26085;&#24535;&#20013;&#30340;&#32479;&#35745;&#32467;&#26524;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.08112v1 Announce Type: cross  Abstract: Artificial intelligence has been applied in various aspects of online education to facilitate teaching and learning. However, few approaches has been made toward a complete AI-powered tutoring system. In this work, we explore the development of a full-fledged intelligent tutoring system powered by state-of-the-art large language models (LLMs), covering automatic course planning and adjusting, tailored instruction, and flexible quiz evaluation. To make the system robust to prolonged interaction and cater to individualized education, the system is decomposed into three inter-connected core processes-interaction, reflection, and reaction. Each process is implemented by chaining LLM-powered tools along with dynamically updated memory modules. Tools are LLMs prompted to execute one specific task at a time, while memories are data storage that gets updated during education process. Statistical results from learning logs demonstrate the effec
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;ChatEDA&#65292;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;AutoMage&#36171;&#33021;&#30340;EDA&#33258;&#20027;&#20195;&#29702;&#65292;&#36890;&#36807;&#26377;&#25928;&#31649;&#29702;&#20219;&#21153;&#35745;&#21010;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#20219;&#21153;&#25191;&#34892;&#65292;&#31616;&#21270;&#20102;&#20174;RTL&#21040;GDSII&#30340;&#35774;&#35745;&#27969;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#20248;&#36234;&#24615;&#12290;</title><link>https://arxiv.org/abs/2308.10204</link><description>&lt;p&gt;
ChatEDA&#65306;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#29992;&#20110;EDA
&lt;/p&gt;
&lt;p&gt;
ChatEDA: A Large Language Model Powered Autonomous Agent for EDA
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10204
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;ChatEDA&#65292;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;AutoMage&#36171;&#33021;&#30340;EDA&#33258;&#20027;&#20195;&#29702;&#65292;&#36890;&#36807;&#26377;&#25928;&#31649;&#29702;&#20219;&#21153;&#35745;&#21010;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#20219;&#21153;&#25191;&#34892;&#65292;&#31616;&#21270;&#20102;&#20174;RTL&#21040;GDSII&#30340;&#35774;&#35745;&#27969;&#31243;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#24615;&#33021;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10204v2 &#20844;&#21578;&#31867;&#22411;&#65306;&#26367;&#25442;-&#20132;&#21449;&#25688;&#35201;&#65306;&#38598;&#25104;&#19968;&#31995;&#21015;&#22797;&#26434;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;&#65288;EDA&#65289;&#24037;&#20855;&#20197;&#22686;&#24378;&#20114;&#25805;&#20316;&#24615;&#26159;&#30005;&#36335;&#35774;&#35745;&#32773;&#20851;&#27880;&#30340;&#37325;&#35201;&#38382;&#39064;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#29702;&#35299;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19982;EDA&#24037;&#20855;&#25509;&#21475;&#30340;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#35770;&#25991;&#20171;&#32461;&#20102;ChatEDA&#65292;&#19968;&#20010;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;AutoMage&#36171;&#33021;&#30340;EDA&#33258;&#20027;&#20195;&#29702;&#65292;&#32467;&#21512;&#20316;&#20026;&#25191;&#34892;&#22120;&#30340;EDA&#24037;&#20855;&#12290;ChatEDA&#36890;&#36807;&#26377;&#25928;&#31649;&#29702;&#20219;&#21153;&#35745;&#21010;&#12289;&#33050;&#26412;&#29983;&#25104;&#21644;&#20219;&#21153;&#25191;&#34892;&#65292;&#31616;&#21270;&#20102;&#20174;&#23492;&#23384;&#22120;&#20256;&#36755;&#32423;&#65288;RTL&#65289;&#21040;&#22270;&#24418;&#25968;&#25454;&#31995;&#32479;&#31532;&#20108;&#29256;&#65288;GDSII&#65289;&#30340;&#35774;&#35745;&#27969;&#31243;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;ChatEDA&#24050;&#32463;&#35777;&#26126;&#20102;&#20854;&#22788;&#29702;&#21508;&#31181;&#38656;&#27714;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#32463;&#36807;&#31934;&#24515;&#35843;&#20248;&#30340;AutoMage&#27169;&#22411;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#65292;&#30456;&#36739;&#20110;GPT-4&#21644;&#20854;&#20182;&#31867;&#20284;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10204v2 Announce Type: replace-cross  Abstract: The integration of a complex set of Electronic Design Automation (EDA) tools to enhance interoperability is a critical concern for circuit designers. Recent advancements in large language models (LLMs) have showcased their exceptional capabilities in natural language processing and comprehension, offering a novel approach to interfacing with EDA tools. This research paper introduces ChatEDA, an autonomous agent for EDA empowered by a large language model, AutoMage, complemented by EDA tools serving as executors. ChatEDA streamlines the design flow from the Register-Transfer Level (RTL) to the Graphic Data System Version II (GDSII) by effectively managing task planning, script generation, and task execution. Through comprehensive experimental evaluations, ChatEDA has demonstrated its proficiency in handling diverse requirements, and our fine-tuned AutoMage model has exhibited superior performance compared to GPT-4 and other simi
&lt;/p&gt;</description></item><item><title>SSMG&#27169;&#22411;&#37319;&#29992;&#29305;&#24449;&#22270;&#20316;&#20026;&#24341;&#23548;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#31354;&#38388;&#35821;&#20041;&#21487;&#25511;&#24615;</title><link>https://arxiv.org/abs/2308.10156</link><description>&lt;p&gt;
SSMG&#65306;&#31354;&#38388;&#35821;&#20041;&#22320;&#22270;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#33258;&#30001;&#24418;&#24335;&#24067;&#23616;&#21040;&#22270;&#20687;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SSMG: Spatial-Semantic Map Guided Diffusion Model for Free-form Layout-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.10156
&lt;/p&gt;
&lt;p&gt;
SSMG&#27169;&#22411;&#37319;&#29992;&#29305;&#24449;&#22270;&#20316;&#20026;&#24341;&#23548;&#65292;&#30456;&#27604;&#20854;&#20182;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#29983;&#25104;&#36136;&#37327;&#21644;&#31354;&#38388;&#35821;&#20041;&#21487;&#25511;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25991;&#26412;&#21040;&#22270;&#20687;(T2I)&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#21363;&#20351;&#26159;&#20887;&#38271;&#22797;&#26434;&#30340;&#25991;&#26412;&#25551;&#36848;&#20173;&#28982;&#38590;&#20197;&#20256;&#36798;&#35814;&#32454;&#30340;&#25511;&#21046;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#24067;&#23616;&#21040;&#22270;&#20687;(L2I)&#29983;&#25104;&#26088;&#22312;&#20174;&#29992;&#25143;&#25351;&#23450;&#30340;&#24067;&#23616;&#29983;&#25104;&#36924;&#30495;&#19988;&#22797;&#26434;&#30340;&#22330;&#26223;&#22270;&#20687;&#65292;&#29616;&#24050;&#24341;&#36215;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#23558;&#24067;&#23616;&#20449;&#24687;&#36716;&#25442;&#20026;&#26631;&#35760;&#25110;RGB&#22270;&#20687;&#65292;&#29992;&#20110;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#26465;&#20214;&#25511;&#21046;&#65292;&#23548;&#33268;&#20010;&#20307;&#23454;&#20363;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#21487;&#25511;&#24615;&#19981;&#36275;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31354;&#38388;&#35821;&#20041;&#22320;&#22270;&#24341;&#23548;(SSMG)&#25193;&#25955;&#27169;&#22411;&#65292;&#37319;&#29992;&#20174;&#24067;&#23616;&#20013;&#33719;&#21462;&#30340;&#29305;&#24449;&#22270;&#20316;&#20026;&#24341;&#23548;&#12290;&#30001;&#20110;&#23500;&#21547;&#35774;&#35745;&#33391;&#22909;&#30340;&#29305;&#24449;&#22270;&#20869;&#30340;&#20016;&#23500;&#31354;&#38388;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;SSMG&#30456;&#27604;&#20043;&#21069;&#30340;&#24037;&#20316;&#23454;&#29616;&#20102;&#26356;&#20248;&#31168;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#20855;&#26377;&#36275;&#22815;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#21487;&#25511;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.10156v2 Announce Type: replace-cross  Abstract: Despite significant progress in Text-to-Image (T2I) generative models, even lengthy and complex text descriptions still struggle to convey detailed controls. In contrast, Layout-to-Image (L2I) generation, aiming to generate realistic and complex scene images from user-specified layouts, has risen to prominence. However, existing methods transform layout information into tokens or RGB images for conditional control in the generative process, leading to insufficient spatial and semantic controllability of individual instances. To address these limitations, we propose a novel Spatial-Semantic Map Guided (SSMG) diffusion model that adopts the feature map, derived from the layout, as guidance. Owing to rich spatial and semantic information encapsulated in well-designed feature maps, SSMG achieves superior generation quality with sufficient spatial and semantic controllability compared to previous works. Additionally, we propose the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MCR-Agent&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#30446;&#26631;&#24182;&#20998;&#21035;&#22788;&#29702;&#23427;&#20204;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#23548;&#33322;&#21644;&#20132;&#20114;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2308.09387</link><description>&lt;p&gt;
&#20114;&#21160;&#25351;&#20196;&#36319;&#38543;&#30340;&#22810;&#23618;&#27425;&#32452;&#21512;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Compositional Reasoning for Interactive Instruction Following
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.09387
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MCR-Agent&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#30446;&#26631;&#24182;&#20998;&#21035;&#22788;&#29702;&#23427;&#20204;&#26469;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#23548;&#33322;&#21644;&#20132;&#20114;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Multi-level Compositional Reasoning Agent&#65288;MCR-Agent&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25191;&#34892;&#23478;&#21153;&#20219;&#21153;&#30340;&#26426;&#22120;&#20154;&#20195;&#29702;&#38754;&#20020;&#30340;&#22797;&#26434;&#29615;&#22659;&#23548;&#33322;&#21644;&#19982;&#29615;&#22659;&#20013;&#29289;&#20307;&#20132;&#20114;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#30446;&#26631;&#65292;&#24182;&#21333;&#29420;&#22788;&#29702;&#23427;&#20204;&#20197;&#36827;&#34892;&#26356;&#22909;&#30340;&#23548;&#33322;&#21644;&#20132;&#20114;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#19977;&#32423;&#21160;&#20316;&#31574;&#30053;&#65292;&#20998;&#21035;&#26159;&#39640;&#32423;&#31574;&#30053;&#32452;&#21512;&#25511;&#21046;&#22120;&#25512;&#26029;&#22522;&#20110;&#35821;&#35328;&#25351;&#20196;&#35201;&#25191;&#34892;&#30340;&#19968;&#31995;&#21015;&#21487;&#29702;&#35299;&#30340;&#23376;&#30446;&#26631;&#65307;&#20013;&#32423;&#31574;&#30053;&#36890;&#36807;&#23548;&#33322;&#20027;&#31574;&#30053;&#23545;&#20195;&#29702;&#30340;&#23548;&#33322;&#36827;&#34892;&#21028;&#21035;&#25511;&#21046;&#65292;&#20174;&#32780;&#20132;&#26367;&#22320;&#25191;&#34892;&#23548;&#33322;&#25805;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.09387v2 Announce Type: replace-cross  Abstract: Robotic agents performing domestic chores by natural language directives are required to master the complex job of navigating environment and interacting with objects in the environments. The tasks given to the agents are often composite thus are challenging as completing them require to reason about multiple subtasks, e.g., bring a cup of coffee. To address the challenge, we propose to divide and conquer it by breaking the task into multiple subgoals and attend to them individually for better navigation and interaction. We call it Multi-level Compositional Reasoning Agent (MCR-Agent). Specifically, we learn a three-level action policy. At the highest level, we infer a sequence of human-interpretable subgoals to be executed based on language instructions by a high-level policy composition controller. At the middle level, we discriminatively control the agent's navigation by a master policy by alternating between a navigation po
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#39318;&#27425;&#35843;&#30740;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;</title><link>https://arxiv.org/abs/2302.06670</link><description>&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#20013;&#21487;&#35299;&#37322;&#30340;&#24322;&#24120;&#26816;&#27979;&#65306;&#19968;&#39033;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
Explainable Anomaly Detection in Images and Videos: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.06670
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20379;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#39318;&#27425;&#35843;&#30740;&#65292;&#20026;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#23454;&#38469;&#24212;&#29992;&#25552;&#20379;&#20102;&#37325;&#35201;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#21644;&#23450;&#20301;&#35270;&#35273;&#25968;&#25454;&#65288;&#21253;&#25324;&#22270;&#20687;&#21644;&#35270;&#39057;&#65289;&#22312;&#26426;&#22120;&#23398;&#20064;&#23398;&#26415;&#30028;&#21644;&#24212;&#29992;&#23454;&#38469;&#22330;&#26223;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#21487;&#35270;&#24322;&#24120;&#26816;&#27979;&#25216;&#26415;&#36805;&#36895;&#21457;&#23637;&#65292;&#20294;&#23545;&#20110;&#36825;&#20123;&#40657;&#30418;&#27169;&#22411;&#30340;&#35299;&#37322;&#20197;&#21450;&#20026;&#20309;&#21487;&#20197;&#21306;&#20998;&#24322;&#24120;&#30340;&#21512;&#29702;&#35299;&#37322;&#21364;&#21313;&#20998;&#31232;&#32570;&#12290;&#26412;&#25991;&#39318;&#27425;&#25552;&#20379;&#20102;&#19968;&#39033;&#38598;&#20013;&#20110;&#21487;&#35299;&#37322;&#35270;&#35273;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#35843;&#30740;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22270;&#20687;&#32423;&#21644;&#35270;&#39057;&#32423;&#24322;&#24120;&#26816;&#27979;&#30340;&#22522;&#26412;&#32972;&#26223;&#12290;&#28982;&#21518;&#65292;&#20316;&#20026;&#26412;&#35843;&#30740;&#30340;&#20027;&#35201;&#20869;&#23481;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38024;&#23545;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#30340;&#20840;&#38754;&#21644;&#35814;&#23613;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#20026;&#20160;&#20040;&#19968;&#20123;&#21487;&#35299;&#37322;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#21487;&#20197;&#24212;&#29992;&#20110;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#32780;&#21478;&#19968;&#20123;&#21017;&#21482;&#33021;&#24212;&#29992;&#20110;&#19968;&#31181;&#27169;&#24577;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24635;&#32467;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.06670v2 Announce Type: replace-cross  Abstract: Anomaly detection and localization of visual data, including images and videos, are of great significance in both machine learning academia and applied real-world scenarios. Despite the rapid development of visual anomaly detection techniques in recent years, the interpretations of these black-box models and reasonable explanations of why anomalies can be distinguished out are scarce. This paper provides the first survey concentrated on explainable visual anomaly detection methods. We first introduce the basic background of image-level and video-level anomaly detection. Then, as the main content of this survey, a comprehensive and exhaustive literature review of explainable anomaly detection methods for both images and videos is presented. Next, we analyze why some explainable anomaly detection methods can be applied to both images and videos and why others can be only applied to one modality. Additionally, we provide summaries
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;&#25856;&#23721;&#24555;&#25346;&#19978;&#23433;&#35013;&#30340;&#21152;&#36895;&#24230;&#20256;&#24863;&#22120;&#37319;&#38598;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#25856;&#23721;&#27963;&#21160;&#20013;&#26816;&#27979;&#25856;&#23721;&#32773;&#19979;&#38477;&#24773;&#20917;&#30340;&#25216;&#26415;&#65292;&#20445;&#25252;&#25856;&#23721;&#32773;&#38544;&#31169;&#21644;&#20581;&#36523;&#25151;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20415;&#21033;&#24615;&#12290;</title><link>https://arxiv.org/abs/2301.10164</link><description>&lt;p&gt;
&#22522;&#20110;&#20256;&#24863;&#22120;&#22686;&#24378;&#24555;&#25346;&#30340;&#26397;&#21521;&#30340;&#25856;&#23721;&#20013;&#19979;&#38477;&#34892;&#20026;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Lowering Detection in Sport Climbing Based on Orientation of the Sensor Enhanced Quickdraw
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.10164
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;&#25856;&#23721;&#24555;&#25346;&#19978;&#23433;&#35013;&#30340;&#21152;&#36895;&#24230;&#20256;&#24863;&#22120;&#37319;&#38598;&#25968;&#25454;&#65292;&#23454;&#29616;&#20102;&#22312;&#25856;&#23721;&#27963;&#21160;&#20013;&#26816;&#27979;&#25856;&#23721;&#32773;&#19979;&#38477;&#24773;&#20917;&#30340;&#25216;&#26415;&#65292;&#20445;&#25252;&#25856;&#23721;&#32773;&#38544;&#31169;&#21644;&#20581;&#36523;&#25151;&#25104;&#26412;&#30340;&#21516;&#26102;&#25552;&#39640;&#20102;&#25928;&#29575;&#21644;&#20415;&#21033;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36319;&#36394;&#25856;&#23721;&#32773;&#30340;&#27963;&#21160;&#20197;&#25913;&#21892;&#26381;&#21153;&#24182;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#20182;&#20204;&#30340;&#22522;&#30784;&#35774;&#26045;&#26159;&#25856;&#23721;&#20581;&#36523;&#25151;&#20851;&#27880;&#30340;&#28966;&#28857;&#12290;&#24517;&#39035;&#20174;&#24320;&#22987;&#20998;&#26512;&#27599;&#20010;&#25856;&#23721;&#27963;&#21160;&#30452;&#21040;&#25856;&#30331;&#32773;&#38477;&#19979;&#26469;&#12290;&#22240;&#27492;&#65292;&#21457;&#29616;&#25856;&#23721;&#32773;&#19979;&#38477;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#36825;&#26631;&#24535;&#30528;&#25856;&#30331;&#32467;&#26463;&#12290;&#24517;&#39035;&#22312;&#20445;&#25252;&#25856;&#23721;&#32773;&#21644;&#20581;&#36523;&#25151;&#25104;&#26412;&#38544;&#31169;&#21644;&#20415;&#21033;&#24615;&#30340;&#21516;&#26102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20026;&#27492;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#30828;&#20214;&#21407;&#22411;&#65292;&#20351;&#29992;&#38468;&#22312;&#22681;&#19978;&#30340;&#25856;&#23721;&#35774;&#22791;&#19978;&#30340;&#21152;&#36895;&#24230;&#20256;&#24863;&#22120;&#25910;&#38598;&#25968;&#25454;&#65292;&#31216;&#20026;&#24555;&#25346;&#65292;&#23427;&#36830;&#25509;&#25856;&#23721;&#32499;&#21644;&#34746;&#26643;&#38170;&#28857;&#12290;&#30456;&#24212;&#30340;&#20256;&#24863;&#22120;&#34987;&#37197;&#32622;&#20026;&#33410;&#33021;&#65292;&#22240;&#27492;&#22312;&#25856;&#23721;&#20581;&#36523;&#25151;&#22823;&#37327;&#20351;&#29992;&#26102;&#22312;&#36153;&#29992;&#21644;&#26356;&#25442;&#25152;&#38656;&#26102;&#38388;&#26041;&#38754;&#21464;&#24471;&#23454;&#29992;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#30828;&#20214;&#35268;&#26684;&#65292;&#24182;&#30740;&#31350;&#20102;&#20256;&#24863;&#22120;&#27979;&#24471;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.10164v2 Announce Type: replace-cross  Abstract: Tracking climbers' activity to improve services and make the best use of their infrastructure is a concern for climbing gyms. Each climbing session must be analyzed from beginning till lowering of the climber. Therefore, spotting the climbers descending is crucial since it indicates when the ascent has come to an end. This problem must be addressed while preserving privacy and convenience of the climbers and the costs of the gyms. To this aim, a hardware prototype is developed to collect data using accelerometer sensors attached to a piece of climbing equipment mounted on the wall, called quickdraw, that connects the climbing rope to the bolt anchors. The corresponding sensors are configured to be energy-efficient, hence become practical in terms of expenses and time consumption for replacement when using in large quantity in a climbing gym. This paper describes hardware specifications, studies data measured by the sensors in u
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20840;&#38754;&#19968;&#33268;&#30340;&#22522;&#20934;&#65292;&#20197;&#24320;&#21457;&#21487;&#37325;&#22797;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#24182;&#35780;&#20272;&#26469;&#33258;&#22810;&#31181;&#26816;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20197;&#23454;&#29616;&#32467;&#26524;&#30340;&#26377;&#21147;&#27604;&#36739;&#12290;</title><link>https://arxiv.org/abs/2203.02115</link><description>&lt;p&gt;
&#26397;&#21521;&#27979;&#35780;&#21644;&#35780;&#20272;Deepfake&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Towards Benchmarking and Evaluating Deepfake Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2203.02115
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#26088;&#22312;&#24314;&#31435;&#19968;&#20010;&#20840;&#38754;&#19968;&#33268;&#30340;&#22522;&#20934;&#65292;&#20197;&#24320;&#21457;&#21487;&#37325;&#22797;&#30340;&#35780;&#20272;&#27969;&#31243;&#65292;&#24182;&#35780;&#20272;&#26469;&#33258;&#22810;&#31181;&#26816;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20197;&#23454;&#29616;&#32467;&#26524;&#30340;&#26377;&#21147;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Deepfake&#26816;&#27979;&#26159;&#36890;&#36807;&#20998;&#26512;&#25805;&#32437;&#21644;&#26410;&#25805;&#32437;&#35270;&#39057;&#20043;&#38388;&#30340;&#24046;&#24322;&#26469;&#33258;&#21160;&#35782;&#21035;&#25805;&#32437;&#23186;&#20307;&#30340;&#36807;&#31243;&#12290;&#29616;&#22312;&#33258;&#28982;&#32780;&#28982;&#22320;&#35810;&#38382;&#21738;&#20123;&#29616;&#26377;&#30340;Deepfake&#26816;&#27979;&#26041;&#27861;&#26159;&#34920;&#29616;&#26368;&#22909;&#30340;&#65292;&#20197;&#30830;&#23450;&#24212;&#35813;&#20851;&#27880;&#21738;&#20123;&#30740;&#31350;&#26041;&#21521;&#24182;&#25552;&#20379;&#23454;&#38469;&#25351;&#23548;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35780;&#20272;&#26465;&#20214;&#22312;&#30740;&#31350;&#20013;&#23384;&#22312;&#19981;&#19968;&#33268;&#24615;&#65292;&#20351;&#29992;&#25991;&#29486;&#20013;&#30340;&#32467;&#26524;&#38590;&#20197;&#36827;&#34892;&#29616;&#26377;&#26816;&#27979;&#26041;&#27861;&#30340;&#21512;&#29702;&#22522;&#20934;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#24314;&#31435;&#19968;&#20010;&#20840;&#38754;&#19988;&#19968;&#33268;&#30340;&#22522;&#20934;&#65292;&#24320;&#21457;&#21487;&#37325;&#22797;&#30340;&#35780;&#20272;&#31243;&#24207;&#65292;&#24182;&#27979;&#37327;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#20197;&#20415;&#32467;&#26524;&#21487;&#20197;&#24471;&#21040;&#21512;&#29702;&#27604;&#36739;&#12290;&#25105;&#20204;&#25910;&#38598;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#30001;13&#31181;&#20197;&#19978;&#19981;&#21516;&#26041;&#27861;&#29983;&#25104;&#30340;&#25805;&#32437;&#26679;&#26412;&#65292;&#20197;&#21450;&#26469;&#33258;11&#31181;&#27969;&#34892;&#30340;&#26816;&#27979;&#26041;&#27861;&#65288;9&#31181;&#31639;&#27861;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2203.02115v2 Announce Type: replace-cross  Abstract: Deepfake detection automatically recognizes the manipulated medias through the analysis of the difference between manipulated and non-altered videos. It is natural to ask which are the top performers among the existing deepfake detection approaches to identify promising research directions and provide practical guidance. Unfortunately, it's difficult to conduct a sound benchmarking comparison of existing detection approaches using the results in the literature because evaluation conditions are inconsistent across studies. Our objective is to establish a comprehensive and consistent benchmark, to develop a repeatable evaluation procedure, and to measure the performance of a range of detection approaches so that the results can be compared soundly. A challenging dataset consisting of the manipulated samples generated by more than 13 different methods has been collected, and 11 popular detection approaches (9 algorithms) from the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#34920;&#21333;&#22635;&#20805;&#20219;&#21153;&#37325;&#26032;&#26500;&#36896;&#20026;&#22810;&#27169;&#24577;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;QA&#31995;&#32479;&#23454;&#29616;&#34920;&#21333;&#20803;&#32032;&#30340;&#22635;&#20805;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#36827;&#19968;&#27493;&#32454;&#21270;&#34920;&#21333;&#22635;&#20805;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2011.12340</link><description>&lt;p&gt;
mForms: &#22810;&#27169;&#24577;&#38382;&#31572;&#24418;&#24335;&#22635;&#20805;
&lt;/p&gt;
&lt;p&gt;
mForms : Multimodal Form-Filling with Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2011.12340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#34920;&#21333;&#22635;&#20805;&#20219;&#21153;&#37325;&#26032;&#26500;&#36896;&#20026;&#22810;&#27169;&#24577;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;QA&#31995;&#32479;&#23454;&#29616;&#34920;&#21333;&#20803;&#32032;&#30340;&#22635;&#20805;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#36827;&#19968;&#27493;&#32454;&#21270;&#34920;&#21333;&#22635;&#20805;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#21333;&#22635;&#20805;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#37325;&#26032;&#26500;&#36896;&#20026;&#22810;&#27169;&#24577;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65288;QA&#65289;&#12290;&#36890;&#36807;&#39318;&#20808;&#23558;GUI&#34920;&#21333;&#19978;&#30340;&#20803;&#32032;&#65288;&#25991;&#26412;&#23383;&#27573;&#12289;&#25353;&#38062;&#12289;&#22270;&#26631;&#31561;&#65289;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#26469;&#23454;&#29616;&#36825;&#31181;&#37325;&#26032;&#26500;&#36896;&#65292;&#36825;&#20123;&#38382;&#39064;&#25429;&#25417;&#20102;&#20803;&#32032;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#12290;&#22312;&#30830;&#23450;&#34920;&#21333;&#20803;&#32032;&#65288;&#38382;&#39064;&#65289;&#21644;&#29992;&#25143;&#35805;&#35821;&#65288;&#31572;&#26696;&#65289;&#20043;&#38388;&#30340;&#21305;&#37197;&#21518;&#65292;&#36890;&#36807;&#19968;&#20010;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#25277;&#21462;&#24335;QA&#31995;&#32479;&#22635;&#20805;&#34920;&#21333;&#20803;&#32032;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;QA&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#29305;&#23450;&#20110;&#34920;&#21333;&#30340;&#35757;&#32451;&#65292;&#36825;&#31181;&#34920;&#21333;&#22635;&#20805;&#26041;&#27861;&#26159;&#38646;-shot &#30340;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#36890;&#36807;&#20351;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#26469;&#32454;&#21270;&#34920;&#21333;&#22635;&#20805;&#30340;&#26041;&#27861;&#65292;&#20197;&#25972;&#21512;&#21487;&#33021;&#22823;&#37327;&#30340;&#36830;&#32493;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#28982;&#35821;&#35328;&#34920;&#21333;&#22635;&#20805;&#25968;&#25454;&#38598;Multimodal Forms&#65288;mForms&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#22810;&#27169;&#24577;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
arXiv:2011.12340v3 Announce Type: replace  Abstract: This paper presents a new approach to form-filling by reformulating the task as multimodal natural language Question Answering (QA). The reformulation is achieved by first translating the elements on the GUI form (text fields, buttons, icons, etc.) to natural language questions, where these questions capture the element's multimodal semantics. After a match is determined between the form element (Question) and the user utterance (Answer), the form element is filled through a pre-trained extractive QA system. By leveraging pre-trained QA models and not requiring form-specific training, this approach to form-filling is zero-shot. The paper also presents an approach to further refine the form-filling by using multi-task training to incorporate a potentially large number of successive tasks. Finally, the paper introduces a multimodal natural language form-filling dataset Multimodal Forms (mForms), as well as a multimodal extension of the
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#25429;&#25417;&#31185;&#23398;&#26415;&#35821;&#20851;&#31995;&#30340;&#26102;&#38388;&#21160;&#24577;</title><link>https://arxiv.org/abs/2010.01916</link><description>&lt;p&gt;
&#36890;&#36807;&#39118;&#38505;&#20272;&#35745;&#23454;&#29616;&#29983;&#29289;&#21307;&#23398;&#20551;&#35774;&#29983;&#25104;&#30340;&#26102;&#38388;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Temporal Positive-unlabeled Learning for Biomedical Hypothesis Generation via Risk Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2010.01916
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#27491;&#26080;&#26631;&#35760;&#23398;&#20064;&#25429;&#25417;&#31185;&#23398;&#26415;&#35821;&#20851;&#31995;&#30340;&#26102;&#38388;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#30149;&#27602;&#12289;&#33647;&#29289;&#21644;&#30151;&#29366;&#31561;&#29983;&#29289;&#21307;&#23398;&#26415;&#35821;&#20043;&#38388;&#30340;&#20851;&#31995;&#23545;&#20110;&#25239;&#20987;&#30142;&#30149;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#23558;&#20551;&#35774;&#29983;&#25104;&#65288;HG&#65289;&#38382;&#39064;&#34920;&#36848;&#20026;&#21160;&#24577;&#23646;&#24615;&#22270;&#19978;&#26410;&#26469;&#36830;&#25509;&#24615;&#39044;&#27979;&#20219;&#21153;&#65292;&#24182;&#36890;&#36807;&#27491;&#26080;&#26631;&#35760;&#65288;PU&#65289;&#23398;&#20064;&#26469;&#25429;&#25417;&#31185;&#23398;&#26415;&#35821;&#20851;&#31995;&#30340;&#26102;&#38388;&#21160;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2010.01916v1 Announce Type: cross  Abstract: Understanding the relationships between biomedical terms like viruses, drugs, and symptoms is essential in the fight against diseases. Many attempts have been made to introduce the use of machine learning to the scientific process of hypothesis generation(HG), which refers to the discovery of meaningful implicit connections between biomedical terms. However, most existing methods fail to truly capture the temporal dynamics of scientific term relations and also assume unobserved connections to be irrelevant (i.e., in a positive-negative (PN) learning setting). To break these limits, we formulate this HG problem as future connectivity prediction task on a dynamic attributed graph via positive-unlabeled (PU) learning. Then, the key is to capture the temporal evolution of node pair (term pair) relations from just the positive and unlabeled data. We propose a variational inference model to estimate the positive prior, and incorporate it in 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#27880;&#24847;&#21147;&#27493;&#34892;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#26469;&#25506;&#32034;&#22270;&#20013;&#30340;&#37051;&#22495;&#65292;&#25214;&#21040;&#36866;&#21512;&#20998;&#31867;&#26410;&#26631;&#35760;&#30446;&#26631;&#33410;&#28857;&#30340;&#36335;&#24452;&#12290;</title><link>https://arxiv.org/abs/1910.10266</link><description>&lt;p&gt;
&#22522;&#20110;&#24490;&#29615;&#27880;&#24847;&#21147;&#27493;&#34892;&#30340;&#21322;&#30417;&#30563;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Recurrent Attention Walk for Semi-supervised Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1910.10266
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#27880;&#24847;&#21147;&#27493;&#34892;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#26469;&#25506;&#32034;&#22270;&#20013;&#30340;&#37051;&#22495;&#65292;&#25214;&#21040;&#36866;&#21512;&#20998;&#31867;&#26410;&#26631;&#35760;&#30446;&#26631;&#33410;&#28857;&#30340;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#29992;&#20110;&#23545;&#24102;&#23646;&#24615;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#65292;&#20854;&#20013;&#33410;&#28857;&#21644;&#36793;&#37117;&#21253;&#21547;&#20869;&#23481;&#20449;&#24687;&#12290;&#26368;&#36817;&#30340;&#26041;&#27861;&#65292;&#22914;&#22270;&#21367;&#31215;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24050;&#34987;&#25552;&#20986;&#26469;&#32452;&#21512;&#19968;&#38454;&#37051;&#23621;&#24182;&#25972;&#21512;&#30456;&#20851;&#37051;&#23621;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#25152;&#26377;&#37051;&#23621;&#32780;&#19981;&#36827;&#34892;&#20808;&#21069;&#21306;&#20998;&#26174;&#24471;&#26114;&#36149;&#65288;&#23588;&#20854;&#26159;&#22312;&#20869;&#23384;&#26041;&#38754;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#22312;&#24378;&#21270;&#23398;&#20064;&#35774;&#32622;&#20013;&#25506;&#32034;&#37051;&#22495;&#65292;&#24182;&#25214;&#21040;&#19968;&#20010;&#32463;&#36807;&#35843;&#25972;&#20197;&#20415;&#23545;&#26410;&#26631;&#35760;&#30446;&#26631;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#30340;&#36335;&#24452;&#12290;&#25105;&#20204;&#35753;&#19968;&#20010;&#20195;&#29702;&#20154;&#65288;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#65289;&#22312;&#22270;&#19978;&#34892;&#36208;&#65292;&#24182;&#20915;&#23450;&#22312;&#21738;&#37324;&#21069;&#36827;&#20197;&#26368;&#22823;&#21270;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#23558;&#22270;&#28459;&#27493;&#23450;&#20041;&#20026;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28789;&#27963;&#22320;&#36866;&#29992;&#20110;&#24037;&#20316;&#22312;&#36716;&#23548;&#21644;&#24402;&#32435;&#35774;&#32622;&#20013;&#12290;&#23545;&#22235;&#20010;&#25968;&#25454;&#38598;&#30340;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#20102;&#25215;&#35748;
&lt;/p&gt;
&lt;p&gt;
arXiv:1910.10266v1 Announce Type: cross  Abstract: In this paper, we study the graph-based semi-supervised learning for classifying nodes in attributed networks, where the nodes and edges possess content information. Recent approaches like graph convolution networks and attention mechanisms have been proposed to ensemble the first-order neighbors and incorporate the relevant neighbors. However, it is costly (especially in memory) to consider all neighbors without a prior differentiation. We propose to explore the neighborhood in a reinforcement learning setting and find a walk path well-tuned for classifying the unlabelled target nodes. We let an agent (of node classification task) walk over the graph and decide where to direct to maximize classification accuracy. We define the graph walk as a partially observable Markov decision process (POMDP). The proposed method is flexible for working in both transductive and inductive setting. Extensive experiments on four datasets demonstrate th
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#26631;&#31614;&#22270;&#36941;&#21382;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#24335;&#30340;&#22270;&#36941;&#21382;&#31574;&#30053;&#23398;&#20064;&#33410;&#28857;&#26631;&#31614;&#19982;&#22270;&#32467;&#26500;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#22810;&#26631;&#31614;&#29305;&#23450;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>https://arxiv.org/abs/1910.09706</link><description>&lt;p&gt;
&#21512;&#20316;&#22270;&#36941;&#21382;&#29992;&#20110;&#21322;&#30417;&#30563;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Collaborative Graph Walk for Semi-supervised Multi-Label Node Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1910.09706
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#26631;&#31614;&#22270;&#36941;&#21382;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#20316;&#24335;&#30340;&#22270;&#36941;&#21382;&#31574;&#30053;&#23398;&#20064;&#33410;&#28857;&#26631;&#31614;&#19982;&#22270;&#32467;&#26500;&#23646;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20197;&#21450;&#22810;&#26631;&#31614;&#29305;&#23450;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#23646;&#24615;&#22270;&#20013;&#30340;&#21322;&#30417;&#30563;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#38382;&#39064;&#12290;&#32463;&#20856;&#30340;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#35299;&#20915;&#26041;&#26696;&#36890;&#24120;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#65292;&#39318;&#20808;&#23398;&#20064;&#33410;&#28857;&#23884;&#20837;&#65292;&#28982;&#21518;&#22312;&#23398;&#20064;&#30340;&#23884;&#20837;&#19978;&#26500;&#24314;&#33410;&#28857;&#20998;&#31867;&#22120;&#12290;&#20026;&#20102;&#25552;&#39640;&#33410;&#28857;&#23884;&#20837;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21512;&#20316;&#22270;&#36941;&#21382;&#26041;&#27861;&#65292;&#31216;&#20026;&#22810;&#26631;&#31614;&#22270;&#36941;&#21382;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#23646;&#24615;&#22270;&#20013;&#20351;&#29992;&#21487;&#29992;&#30340;&#26631;&#31614;&#20998;&#37197;&#26469;&#31934;&#32454;&#35843;&#25972;&#33410;&#28857;&#34920;&#31034;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23558;&#22810;&#26631;&#31614;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#20316;&#20026;&#30001;&#22810;&#20010;&#29305;&#23450;&#20110;&#26631;&#31614;&#30340;&#20195;&#29702;&#36827;&#34892;&#30340;&#21516;&#26102;&#22270;&#36941;&#21382;&#26469;&#34920;&#36848;&#12290;&#27492;&#22806;&#65292;&#26631;&#31614;&#21270;&#22270;&#36941;&#21382;&#30340;&#25919;&#31574;&#20197;&#21327;&#21516;&#26041;&#24335;&#23398;&#20064;&#65292;&#20197;&#39318;&#20808;&#25429;&#25417;&#33410;&#28857;&#26631;&#31614;&#19982;&#22270;&#32467;&#26500;&#23646;&#24615;&#20043;&#38388;&#30340;&#39044;&#27979;&#20851;&#31995;&#65292;&#20854;&#27425;&#26159;&#25429;&#25417;&#22810;&#20010;&#29305;&#23450;&#20110;&#26631;&#31614;&#30340;&#20998;&#31867;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#19968;&#39033;&#20840;&#38754;&#30340;&#23454;&#39564;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
arXiv:1910.09706v2 Announce Type: cross  Abstract: In this work, we study semi-supervised multi-label node classification problem in attributed graphs. Classic solutions to multi-label node classification follow two steps, first learn node embedding and then build a node classifier on the learned embedding. To improve the discriminating power of the node embedding, we propose a novel collaborative graph walk, named Multi-Label-Graph-Walk, to finely tune node representations with the available label assignments in attributed graphs via reinforcement learning. The proposed method formulates the multi-label node classification task as simultaneous graph walks conducted by multiple label-specific agents. Furthermore, policies of the label-wise graph walks are learned in a cooperative way to capture first the predictive relation between node labels and structural attributes of graphs; and second, the correlation among the multiple label-specific classification tasks. A comprehensive experim
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#37117;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#21644;&#31616;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2401.08898</link><description>&lt;p&gt;
&#26725;&#25509;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#65306;&#29702;&#35299;&#33258;&#39044;&#27979;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Bridging State and History Representations: Understanding Self-Predictive RL. (arXiv:2401.08898v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.08898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#29366;&#24577;&#21644;&#21382;&#21490;&#34920;&#31034;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20102;&#36825;&#20123;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#37117;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#65292;&#24182;&#25552;&#20379;&#20102;&#29702;&#35770;&#27934;&#35265;&#21644;&#31616;&#21270;&#31639;&#27861;&#26469;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#31034;&#26159;&#25152;&#26377;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#30340;&#26680;&#24515;&#65292;&#36866;&#29992;&#20110;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDP&#65289;&#12290;&#35768;&#22810;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#21644;&#29702;&#35770;&#26694;&#26550;&#34987;&#24320;&#21457;&#29992;&#20110;&#29702;&#35299;&#20160;&#20040;&#26500;&#25104;&#20102;&#26377;&#25928;&#30340;&#34920;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20043;&#38388;&#30340;&#20851;&#31995;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#20849;&#21516;&#23646;&#24615;&#20173;&#28982;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#35768;&#22810;&#30475;&#20284;&#19981;&#21516;&#30340;&#29366;&#24577;&#21644;&#21382;&#21490;&#25277;&#35937;&#26041;&#27861;&#21644;&#26694;&#26550;&#23454;&#38469;&#19978;&#22522;&#20110;&#33258;&#39044;&#27979;&#25277;&#35937;&#30340;&#20849;&#21516;&#24605;&#24819;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#24191;&#27867;&#37319;&#29992;&#30340;&#30446;&#26631;&#21644;&#20248;&#21270;&#65288;&#22914;&#20572;&#26799;&#24230;&#25216;&#26415;&#65289;&#22312;&#23398;&#20064;&#33258;&#39044;&#27979;&#34920;&#31034;&#20013;&#30340;&#29702;&#35770;&#27934;&#35265;&#12290;&#36825;&#20123;&#21457;&#29616;&#20849;&#21516;&#20135;&#29983;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#29366;&#24577;&#21644;&#21382;&#21490;&#30340;&#33258;&#39044;&#27979;&#34920;&#31034;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#24212;&#29992;&#20110;&#26631;&#20934;MDP&#12289;&#24102;&#26377;dist&#30340;MDP&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations are at the core of all deep reinforcement learning (RL) methods for both Markov decision processes (MDPs) and partially observable Markov decision processes (POMDPs). Many representation learning methods and theoretical frameworks have been developed to understand what constitutes an effective representation. However, the relationships between these methods and the shared properties among them remain unclear. In this paper, we show that many of these seemingly distinct methods and frameworks for state and history abstractions are, in fact, based on a common idea of self-predictive abstraction. Furthermore, we provide theoretical insights into the widely adopted objectives and optimization, such as the stop-gradient technique, in learning self-predictive representations. These findings together yield a minimalist algorithm to learn self-predictive representations for states and histories. We validate our theories by applying our algorithm to standard MDPs, MDPs with dist
&lt;/p&gt;</description></item><item><title>GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#20855;&#26377;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22914;&#26524;&#23558;&#25991;&#26412;&#35745;&#21010;&#36716;&#21270;&#20026;&#23454;&#38469;&#34892;&#21160;&#65292;GPT-4V&#21487;&#20197;&#22312;50%&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#36825;&#19968;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2401.01614</link><description>&lt;p&gt;
GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#22914;&#26524;&#26377;&#22522;&#30784;&#30340;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
GPT-4V(ision) is a Generalist Web Agent, if Grounded. (arXiv:2401.01614v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.01614
&lt;/p&gt;
&lt;p&gt;
GPT-4V(ision)&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#32593;&#32476;&#20195;&#29702;&#65292;&#20855;&#26377;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22914;&#26524;&#23558;&#25991;&#26412;&#35745;&#21010;&#36716;&#21270;&#20026;&#23454;&#38469;&#34892;&#21160;&#65292;GPT-4V&#21487;&#20197;&#22312;50%&#30340;&#20219;&#21153;&#19978;&#21462;&#24471;&#25104;&#21151;&#12290;&#36825;&#19968;&#32467;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#23545;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMM&#65289;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;GPT-4V(ision)&#21644;Gemini&#65292;&#24555;&#36895;&#25512;&#21160;&#20102;&#22810;&#27169;&#22411;&#30340;&#33021;&#21147;&#36793;&#30028;&#36229;&#36234;&#20256;&#32479;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#23383;&#24149;&#21644;&#35270;&#35273;&#38382;&#31572;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#20687;GPT-4V&#36825;&#26679;&#30340;LMM&#20316;&#20026;&#36890;&#29992;&#32593;&#32476;&#20195;&#29702;&#30340;&#28508;&#21147;&#65292;&#21487;&#20197;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#22312;&#20219;&#20309;&#32473;&#23450;&#30340;&#32593;&#31449;&#19978;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SEEACT&#65292;&#19968;&#31181;&#21033;&#29992;LMM&#30340;&#21147;&#37327;&#36827;&#34892;&#32508;&#21512;&#35270;&#35273;&#29702;&#35299;&#21644;&#32593;&#39029;&#25805;&#20316;&#30340;&#36890;&#29992;&#32593;&#32476;&#20195;&#29702;&#12290;&#25105;&#20204;&#22312;&#26368;&#26032;&#30340;MIND2WEB&#22522;&#20934;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#38500;&#20102;&#23545;&#32531;&#23384;&#32593;&#31449;&#30340;&#26631;&#20934;&#31163;&#32447;&#35780;&#20272;&#22806;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#24320;&#21457;&#19968;&#20010;&#20801;&#35768;&#22312;&#23454;&#26102;&#32593;&#31449;&#19978;&#36816;&#34892;&#32593;&#32476;&#20195;&#29702;&#30340;&#24037;&#20855;&#65292;&#23454;&#29616;&#20102;&#19968;&#31181;&#26032;&#30340;&#22312;&#32447;&#35780;&#20272;&#35774;&#32622;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;GPT-4V&#22312;&#32593;&#39029;&#20195;&#29702;&#26041;&#38754;&#34920;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;-&#22914;&#26524;&#25105;&#20204;&#23558;&#20854;&#25991;&#26412;&#35745;&#21010;&#25163;&#21160;&#22320;&#23454;&#26045;&#20026;&#32593;&#31449;&#19978;&#30340;&#34892;&#21160;&#65292;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#23436;&#25104;50%&#30340;&#20219;&#21153;&#12290;&#27492;&#32467;&#26524;&#26126;&#26174;&#36229;&#36807;&#20102;&#20256;&#32479;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent development on large multimodal models (LMMs), especially GPT-4V(ision) and Gemini, has been quickly expanding the capability boundaries of multimodal models beyond traditional tasks like image captioning and visual question answering. In this work, we explore the potential of LMMs like GPT-4V as a generalist web agent that can follow natural language instructions to complete tasks on any given website. We propose SEEACT, a generalist web agent that harnesses the power of LMMs for integrated visual understanding and acting on the web. We evaluate on the recent MIND2WEB benchmark. In addition to standard offline evaluation on cached websites, we enable a new online evaluation setting by developing a tool that allows running web agents on live websites. We show that GPT-4V presents a great potential for web agents - it can successfully complete 50% of the tasks on live websites if we manually ground its textual plans into actions on the websites. This substantially outperforms
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2312.05720</link><description>&lt;p&gt;
&#36229;&#36234;&#26799;&#24230;&#21644;&#20808;&#39564;&#30693;&#35782;&#22312;&#38544;&#31169;&#25915;&#20987;&#20013;&#65306;&#21033;&#29992;&#32852;&#37030;&#23398;&#20064;&#20013;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;
&lt;/p&gt;
&lt;p&gt;
Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning. (arXiv:2312.05720v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.05720
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#23454;&#29616;&#23545;&#38544;&#31169;&#25915;&#20987;&#30340;&#25913;&#36827;&#12290;&#36890;&#36807;&#24674;&#22797;&#27744;&#21270;&#23618;&#36755;&#20837;&#65292;&#36825;&#31181;&#26041;&#27861;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#19979;&#25552;&#20379;&#26356;&#39640;&#30340;&#25991;&#26412;&#24674;&#22797;&#29575;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#24378;&#35843;&#20998;&#25955;&#24335;&#35757;&#32451;&#65292;&#36890;&#36807;&#26412;&#22320;&#23384;&#20648;&#25968;&#25454;&#24182;&#20165;&#21457;&#36865;&#27169;&#22411;&#26356;&#26032;&#65292;&#24378;&#35843;&#29992;&#25143;&#38544;&#31169;&#12290;&#26368;&#36817;&#65292;&#19968;&#31995;&#21015;&#26377;&#20851;&#38544;&#31169;&#25915;&#20987;&#30340;&#24037;&#20316;&#36890;&#36807;&#20174;&#32852;&#37030;&#23398;&#20064;&#19978;&#19979;&#25991;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#25935;&#24863;&#30340;&#35757;&#32451;&#25991;&#26412;&#26469;&#25439;&#23475;&#29992;&#25143;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#25216;&#26415;&#38754;&#20020;&#30528;&#19981;&#21516;&#30340;&#38556;&#30861;&#65306;&#19968;&#20123;&#24037;&#20316;&#20027;&#35201;&#20351;&#29992;&#26377;&#38480;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#65288;&#20363;&#22914;&#65292;&#25209;&#22788;&#29702;&#22823;&#23567;&#20026;1&#65289;&#65292;&#32780;&#20854;&#20182;&#25216;&#26415;&#21017;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#20855;&#26377;&#38590;&#20197;&#26816;&#27979;&#30340;&#29305;&#28857;&#65292;&#22312;&#19981;&#21516;&#30340;&#25209;&#22788;&#29702;&#22823;&#23567;&#35774;&#32622;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#25991;&#26412;&#24674;&#22797;&#29575;&#12290;&#22522;&#20110;&#22522;&#26412;&#30340;&#26799;&#24230;&#21305;&#37197;&#21644;&#39046;&#22495;&#20808;&#39564;&#30693;&#35782;&#65292;&#25105;&#20204;&#36890;&#36807;&#24674;&#22797;&#35821;&#35328;&#27169;&#22411;&#30340;&#27744;&#21270;&#23618;&#36755;&#20837;&#26469;&#22686;&#24378;&#25915;&#20987;&#33021;&#21147;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#22312;&#29305;&#24449;&#32423;&#21035;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;&#19982;&#26799;&#24230;&#25968;&#25454;&#19981;&#21516;&#65292;&#36825;&#20123;&#20449;&#21495;&#19981;&#20250;&#22312;&#21477;&#23376;&#21644;&#26631;&#35760;&#20043;&#38388;&#36827;&#34892;&#24179;&#22343;&#65292;&#20174;&#32780;&#25552;&#20379;&#26356;&#32454;&#33268;&#21644;&#26377;&#25928;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) emphasizes decentralized training by storing data locally and sending only model updates, underlining user privacy. Recently, a line of works on privacy attacks impairs user privacy by extracting sensitive training text from language models in the context of FL. Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others are easily detectable. This paper introduces an innovative approach that is challenging to detect, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the attack by recovering the input of the Pooler layer of language models, which enables us to provide additional supervised signals at the feature level. Unlike gradient data, these signals do not average across sentences and tokens, thereby offering more nuanced and effective insights. We benchmark our method using t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#23548;&#33322;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#24102;&#26377;&#21487;&#36890;&#34892;&#38556;&#30861;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#20174;&#25991;&#26412;&#25351;&#20196;&#21040;&#21160;&#20316;&#24863;&#30693;&#36793;&#30028;&#26694;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#26080;&#38656;&#24494;&#35843;&#21644;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#21010;&#20998;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#65292;&#29983;&#25104;&#21160;&#20316;&#24863;&#30693;&#25104;&#26412;&#22320;&#22270;&#20197;&#29983;&#25104;&#21487;&#34892;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2310.08873</link><description>&lt;p&gt;
&#22312;&#24102;&#26377;&#21487;&#36890;&#34892;&#38556;&#30861;&#30340;&#29615;&#22659;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20132;&#20114;&#24335;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Interactive Navigation in Environments with Traversable Obstacles Using Large Language and Vision-Language Models. (arXiv:2310.08873v1 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.08873
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#23548;&#33322;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#24102;&#26377;&#21487;&#36890;&#34892;&#38556;&#30861;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#36890;&#36807;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#20174;&#25991;&#26412;&#25351;&#20196;&#21040;&#21160;&#20316;&#24863;&#30693;&#36793;&#30028;&#26694;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#26080;&#38656;&#24494;&#35843;&#21644;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#22823;&#22411;&#27169;&#22411;&#21010;&#20998;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#65292;&#29983;&#25104;&#21160;&#20316;&#24863;&#30693;&#25104;&#26412;&#22320;&#22270;&#20197;&#29983;&#25104;&#21487;&#34892;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#23548;&#33322;&#26694;&#26550;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#22312;&#24102;&#26377;&#21487;&#36890;&#34892;&#38556;&#30861;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(GPT-3.5)&#21644;&#24320;&#25918;&#24335;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;(&#22522;&#20110;Grounding DINO)&#21019;&#24314;&#20102;&#19968;&#20010;&#21160;&#20316;&#24863;&#30693;&#25104;&#26412;&#22320;&#22270;&#65292;&#29992;&#20110;&#36827;&#34892;&#26377;&#25928;&#30340;&#36335;&#24452;&#35268;&#21010;&#32780;&#26080;&#38656;&#24494;&#35843;&#12290;&#36890;&#36807;&#22823;&#22411;&#27169;&#22411;&#65292;&#25105;&#20204;&#21487;&#20197;&#23454;&#29616;&#20174;&#25991;&#26412;&#25351;&#20196;&#65288;&#20363;&#22914;&#8220;&#20320;&#33021;&#36890;&#36807;&#31383;&#24088;&#32473;&#25105;&#36865;&#33647;&#21527;&#65311;&#8221;&#65289;&#21040;&#20855;&#26377;&#21160;&#20316;&#24863;&#30693;&#23646;&#24615;&#30340;&#36793;&#30028;&#26694;&#65288;&#20363;&#22914;&#31383;&#24088;&#65289;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#12290;&#23427;&#20204;&#21487;&#20197;&#29992;&#20110;&#23558;&#28608;&#20809;&#38647;&#36798;&#28857;&#20113;&#20998;&#25104;&#20004;&#37096;&#20998;&#65306;&#21487;&#36890;&#34892;&#21644;&#19981;&#21487;&#36890;&#34892;&#37096;&#20998;&#65292;&#28982;&#21518;&#26500;&#24314;&#19968;&#20010;&#21160;&#20316;&#24863;&#30693;&#25104;&#26412;&#22320;&#22270;&#29992;&#20110;&#29983;&#25104;&#21487;&#34892;&#36335;&#24452;&#12290;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#27169;&#22411;&#20855;&#26377;&#24456;&#24378;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#20197;&#24555;&#36895;&#37096;&#32626;&#20110;&#20132;&#20114;&#24335;&#23548;&#33322;&#20219;&#21153;&#20013;&#12290;&#25105;&#20204;&#36873;&#25321;&#20351;&#29992;&#22810;&#20010;&#21487;&#36890;&#34892;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes an interactive navigation framework by using large language and vision-language models, allowing robots to navigate in environments with traversable obstacles. We utilize the large language model (GPT-3.5) and the open-set Vision-language Model (Grounding DINO) to create an action-aware costmap to perform effective path planning without fine-tuning. With the large models, we can achieve an end-to-end system from textual instructions like "Can you pass through the curtains to deliver medicines to me?", to bounding boxes (e.g., curtains) with action-aware attributes. They can be used to segment LiDAR point clouds into two parts: traversable and untraversable parts, and then an action-aware costmap is constructed for generating a feasible path. The pre-trained large models have great generalization ability and do not require additional annotated data for training, allowing fast deployment in the interactive navigation tasks. We choose to use multiple traversable object
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2310.07793</link><description>&lt;p&gt;
GenTKG: &#22522;&#20110;&#29983;&#25104;&#27169;&#22411;&#30340;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07793
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GenTKG&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#32467;&#21512;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#65292;&#20811;&#26381;&#20102;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;&#24222;&#22823;&#30340;&#25968;&#25454;&#37327;&#25152;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#24555;&#36895;&#21457;&#23637;&#24341;&#21457;&#20102;&#23545;&#26102;&#38388;&#30693;&#35782;&#22270;&#35889;(tKG)&#39046;&#22495;&#30340;&#20852;&#36259;&#65292;&#20854;&#20013;&#20256;&#32479;&#30340;&#22522;&#20110;&#23884;&#20837;&#21644;&#35268;&#21017;&#30340;&#27169;&#22411;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#30446;&#21069;&#20173;&#28982;&#23384;&#22312;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#39044;&#35757;&#32451;&#30340;LLM&#26159;&#21542;&#33021;&#22815;&#29702;&#35299;&#32467;&#26500;&#21270;&#30340;&#26102;&#38388;&#20851;&#31995;&#25968;&#25454;&#65292;&#24182;&#21462;&#20195;&#23427;&#20204;&#25104;&#20026;&#26102;&#38388;&#20851;&#31995;&#39044;&#27979;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;&#26102;&#38388;&#30693;&#35782;&#39044;&#27979;&#24341;&#20837;&#29983;&#25104;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#22312;&#22797;&#26434;&#30340;&#26102;&#38388;&#22270;&#25968;&#25454;&#32467;&#26500;&#21644;LLM&#21487;&#20197;&#22788;&#29702;&#30340;&#24207;&#21015;&#33258;&#28982;&#34920;&#36798;&#20043;&#38388;&#23384;&#22312;&#24040;&#22823;&#30340;&#40511;&#27807;&#65292;&#22312;tKG&#30340;&#24222;&#22823;&#25968;&#25454;&#37327;&#21644;&#24494;&#35843;LLM&#30340;&#24040;&#22823;&#35745;&#31639;&#25104;&#26412;&#20043;&#38388;&#20063;&#23384;&#22312;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#26694;&#26550;&#65292;&#31216;&#20026;GenTKG&#65292;&#23427;&#22312;tKG&#19978;&#25191;&#34892;&#29983;&#25104;&#24335;&#39044;&#27979;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#26102;&#38388;&#36923;&#36753;&#35268;&#21017;&#30340;&#26816;&#32034;&#31574;&#30053;&#21644;&#36731;&#37327;&#32423;&#30340;&#21442;&#25968;&#25928;&#29575;&#25351;&#23548;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;GenTKG&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapid advancements in large language models (LLMs) have ignited interest in the temporal knowledge graph (tKG) domain, where conventional carefully designed embedding-based and rule-based models dominate. The question remains open of whether pre-trained LLMs can understand structured temporal relational data and replace them as the foundation model for temporal relational forecasting. Therefore, we bring temporal knowledge forecasting into the generative setting. However, challenges occur in the huge chasms between complex temporal graph data structure and sequential natural expressions LLMs can handle, and between the enormous data sizes of tKGs and heavy computation costs of finetuning LLMs. To address these challenges, we propose a novel retrieval augmented generation framework that performs generative forecasting on tKGs named GenTKG, which combines a temporal logical rule-based retrieval strategy and lightweight parameter-efficient instruction tuning. Extensive experiments hav
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2310.02279</link><description>&lt;p&gt;
&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65306;&#23398;&#20064;&#25193;&#25955;&#30340;&#27010;&#29575;&#27969;ODE&#36712;&#36857;
&lt;/p&gt;
&lt;p&gt;
Consistency Trajectory Models: Learning Probability Flow ODE Trajectory of Diffusion. (arXiv:2310.02279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02279
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#21487;&#20197;&#21152;&#36895;&#25193;&#25955;&#27169;&#22411;&#30340;&#37319;&#26679;&#65292;&#21516;&#26102;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#37319;&#26679;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#33268;&#24615;&#27169;&#22411;&#65288;CM&#65289;&#21152;&#36895;&#22522;&#20110;&#24471;&#20998;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#65292;&#20294;&#20197;&#29306;&#29298;&#26679;&#26412;&#36136;&#37327;&#20026;&#20195;&#20215;&#65292;&#32570;&#20047;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26469;&#26435;&#34913;&#36895;&#24230;&#21644;&#36136;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#33268;&#24615;&#36712;&#36857;&#27169;&#22411;&#65288;CTM&#65289;&#65292;&#23427;&#26159;&#21253;&#25324;CM&#21644;&#22522;&#20110;&#24471;&#20998;&#27169;&#22411;&#22312;&#20869;&#30340;&#27867;&#21270;&#27169;&#22411;&#12290;CTM&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#36882;&#20013;&#36755;&#20986;&#24471;&#20998;&#65288;&#21363;&#23545;&#25968;&#23494;&#24230;&#30340;&#26799;&#24230;&#65289;&#65292;&#24182;&#20801;&#35768;&#22312;&#25193;&#25955;&#36807;&#31243;&#20013;&#20219;&#24847;&#21021;&#22987;&#21644;&#26368;&#32456;&#26102;&#38388;&#20043;&#38388;&#36827;&#34892;&#19981;&#21463;&#38480;&#21046;&#30340;&#36941;&#21382;&#27010;&#29575;&#27969;&#26222;&#36890;&#24494;&#20998;&#26041;&#31243;&#65288;ODE&#65289;&#12290;CTM&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#21644;&#21435;&#22122;&#24471;&#20998;&#21305;&#37197;&#25439;&#22833;&#30340;&#26377;&#25928;&#32452;&#21512;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#24182;&#22312;CIFAR-10&#65288;FID 1.73&#65289;&#21644;64X64&#20998;&#36776;&#29575;&#30340;ImageNet&#19978;&#23454;&#29616;&#26032;&#30340;&#26368;&#20808;&#36827;FID&#12290;CTM&#36824;&#23454;&#29616;&#20102;&#19968;&#31995;&#21015;&#26032;&#30340;&#37319;&#26679;&#26041;&#26696;&#65292;&#21253;&#25324;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426;&#30340;ODE&#35299;&#20013;&#30340;&#38271;&#36339;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Consistency Models (CM) (Song et al., 2023) accelerate score-based diffusion model sampling at the cost of sample quality but lack a natural way to trade-off quality for speed. To address this limitation, we propose Consistency Trajectory Model (CTM), a generalization encompassing CM and score-based models as special cases. CTM trains a single neural network that can -- in a single forward pass -- output scores (i.e., gradients of log-density) and enables unrestricted traversal between any initial and final time along the Probability Flow Ordinary Differential Equation (ODE) in a diffusion process. CTM enables the efficient combination of adversarial training and denoising score matching loss to enhance performance and achieves new state-of-the-art FIDs for single-step diffusion model sampling on CIFAR-10 (FID 1.73) and ImageNet at 64X64 resolution (FID 2.06). CTM also enables a new family of sampling schemes, both deterministic and stochastic, involving long jumps along the ODE soluti
&lt;/p&gt;</description></item><item><title>SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2310.01557</link><description>&lt;p&gt;
SmartPlay: &#19968;&#31181;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
SmartPlay : A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01557
&lt;/p&gt;
&lt;p&gt;
SmartPlay&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#20316;&#20026;&#26234;&#33021;Agent&#33021;&#21147;&#30340;&#22522;&#20934;&#65292;&#21253;&#25324;6&#20010;&#20855;&#26377;&#19981;&#21516;&#25361;&#25112;&#30340;&#28216;&#25103;&#65292;&#24182;&#27979;&#35797;&#20102;&#26234;&#33021;LLM Agent&#30340;&#22810;&#31181;&#20851;&#38190;&#33021;&#21147;&#12290;&#36825;&#19981;&#20165;&#26159;&#19968;&#20010;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#36824;&#21487;&#20197;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#26234;&#33021;Agent&#21644;&#19979;&#19968;&#20195;&#33258;&#21160;&#21270;&#26041;&#38754;&#23637;&#31034;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#31995;&#32479;&#21270;&#30340;&#22522;&#20934;&#26469;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SmartPlay&#65306;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22522;&#20934;&#21644;&#35780;&#20272;LLMs&#20316;&#20026;Agent&#30340;&#26041;&#27861;&#35770;&#12290;SmartPlay&#21253;&#25324;6&#20010;&#19981;&#21516;&#30340;&#28216;&#25103;&#65292;&#21253;&#25324;&#21098;&#20992;&#30707;&#22836;&#24067;&#12289;&#27721;&#35834;&#22612;&#12289;Minecraft&#31561;&#12290;&#27599;&#20010;&#28216;&#25103;&#37117;&#20855;&#26377;&#29420;&#29305;&#30340;&#35774;&#32622;&#65292;&#25552;&#20379;&#26368;&#22810;20&#20010;&#35780;&#20272;&#35774;&#32622;&#21644;&#26080;&#38480;&#30340;&#29615;&#22659;&#21464;&#21270;&#12290;SmartPlay&#20013;&#30340;&#27599;&#20010;&#28216;&#25103;&#37117;&#29420;&#29305;&#22320;&#25361;&#25112;&#20102;&#26234;&#33021;LLM Agent&#30340;9&#20010;&#37325;&#35201;&#33021;&#21147;&#30340;&#23376;&#38598;&#65292;&#21253;&#25324;&#23545;&#23545;&#35937;&#20381;&#36182;&#30340;&#25512;&#29702;&#12289;&#25552;&#21069;&#35268;&#21010;&#12289;&#31354;&#38388;&#25512;&#29702;&#12289;&#20174;&#21382;&#21490;&#20013;&#23398;&#20064;&#21644;&#29702;&#35299;&#38543;&#26426;&#24615;&#12290;&#27599;&#20010;&#28216;&#25103;&#27979;&#35797;&#30340;&#33021;&#21147;&#38598;&#30340;&#21306;&#21035;&#20351;&#25105;&#20204;&#33021;&#22815;&#21333;&#29420;&#20998;&#26512;&#27599;&#20010;&#33021;&#21147;&#12290;SmartPlay&#19981;&#20165;&#26159;&#35780;&#20272;LLM Agent&#25972;&#20307;&#24615;&#33021;&#30340;&#20005;&#26684;&#27979;&#35797;&#22330;&#22320;&#65292;&#32780;&#19988;&#20063;&#26159;&#35780;&#20272;Agent&#22312;&#19981;&#21516;&#33021;&#21147;&#26041;&#38754;&#30340;&#24615;&#33021;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent large language models (LLMs) have demonstrated great potential toward intelligent agents and next-gen automation, but there currently lacks a systematic benchmark for evaluating LLMs' abilities as agents. We introduce SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs as agents. SmartPlay consists of 6 different games, including Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique setting, providing up to 20 evaluation settings and infinite environment variations. Each game in SmartPlay uniquely challenges a subset of 9 important capabilities of an intelligent LLM agent, including reasoning with object dependencies, planning ahead, spatial reasoning, learning from history, and understanding randomness. The distinction between the set of capabilities each game test allows us to analyze each capability separately. SmartPlay serves not only as a rigorous testing ground for evaluating the overall performance of LLM agents but also as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21644;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#26469;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2310.00116</link><description>&lt;p&gt;
&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Certified Robustness via Dynamic Margin Maximization and Improved Lipschitz Regularization. (arXiv:2310.00116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.00116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21160;&#24577;&#36793;&#30028;&#26368;&#22823;&#21270;&#21644;&#25913;&#36827;&#30340;Lipschitz&#27491;&#21017;&#21270;&#30340;&#35748;&#35777;&#40065;&#26834;&#24615;&#35757;&#32451;&#31639;&#27861;&#65292;&#36890;&#36807;&#22686;&#21152;&#36755;&#20986;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21644;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#26469;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25552;&#39640;&#28145;&#24230;&#20998;&#31867;&#22120;&#23545;&#25239;&#24615;&#25200;&#21160;&#30340;&#40065;&#26834;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#65292;&#20363;&#22914;&#35774;&#35745;&#20855;&#26377;&#26356;&#22909;&#40065;&#26834;&#24615;&#24615;&#36136;&#30340;&#26032;&#26550;&#26500;&#65288;&#20363;&#22914;&#65292;Lipschitz-capped&#32593;&#32476;&#65289;&#25110;&#20462;&#25913;&#35757;&#32451;&#36807;&#31243;&#26412;&#36523;&#65288;&#20363;&#22914;&#65292;&#26368;&#23567;-&#26368;&#22823;&#20248;&#21270;&#65292;&#32422;&#26463;&#23398;&#20064;&#25110;&#27491;&#21017;&#21270;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23545;&#20110;&#22686;&#21152;&#36755;&#20837;&#65288;&#29305;&#24449;&#65289;&#31354;&#38388;&#20013;&#30340;&#36793;&#30028;&#21487;&#33021;&#24182;&#19981;&#26377;&#25928;&#12290;&#22240;&#27492;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#24320;&#22987;&#23545;&#24320;&#21457;&#33021;&#22815;&#30452;&#25509;&#25805;&#32437;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20915;&#31574;&#36793;&#30028;&#30340;&#35757;&#32451;&#36807;&#31243;&#24863;&#20852;&#36259;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#35813;&#31867;&#21035;&#30340;&#26368;&#26032;&#21457;&#23637;&#22522;&#30784;&#19978;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#40065;&#26834;&#35757;&#32451;&#31639;&#27861;&#65292;&#20854;&#30446;&#26631;&#26159;&#22312;&#36755;&#20986;&#65288;logit&#65289;&#31354;&#38388;&#20013;&#22686;&#21152;&#36793;&#30028;&#65292;&#24182;&#27839;&#30528;&#33030;&#24369;&#26041;&#21521;&#27491;&#21017;&#21270;&#27169;&#22411;&#30340;Lipschitz&#24120;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20004;&#20010;&#30446;&#26631;&#21487;&#20197;&#30452;&#25509;&#20419;&#36827;&#36755;&#20837;&#31354;&#38388;&#20013;&#26356;&#22823;&#30340;&#36793;&#30028;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;...
&lt;/p&gt;
&lt;p&gt;
To improve the robustness of deep classifiers against adversarial perturbations, many approaches have been proposed, such as designing new architectures with better robustness properties (e.g., Lipschitz-capped networks), or modifying the training process itself (e.g., min-max optimization, constrained learning, or regularization). These approaches, however, might not be effective at increasing the margin in the input (feature) space. As a result, there has been an increasing interest in developing training procedures that can directly manipulate the decision boundary in the input space. In this paper, we build upon recent developments in this category by developing a robust training algorithm whose objective is to increase the margin in the output (logit) space while regularizing the Lipschitz constant of the model along vulnerable directions. We show that these two objectives can directly promote larger margins in the input space. To this end, we develop a scalable method for calcula
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.15048</link><description>&lt;p&gt;
&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Class Incremental Learning via Likelihood Ratio Based Task Prediction. (arXiv:2309.15048v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.15048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20284;&#28982;&#27604;&#30340;&#20219;&#21153;&#39044;&#27979;&#30340;&#31867;&#22686;&#37327;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;&#26631;&#35782;&#39044;&#27979;&#65292;&#35299;&#20915;&#20102;&#26080;&#20219;&#21153;&#26631;&#35782;&#31526;&#30340;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#39044;&#27979;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#22686;&#37327;&#23398;&#20064;&#26159;&#19968;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#19981;&#26029;&#23398;&#20064;&#30340;&#35774;&#32622;&#65292;&#36890;&#36807;&#39034;&#24207;&#23398;&#20064;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#27599;&#20010;&#20219;&#21153;&#30001;&#19968;&#32452;&#21807;&#19968;&#30340;&#31867;&#32452;&#25104;&#12290;&#31867;&#22686;&#37327;&#23398;&#20064;&#30340;&#20851;&#38190;&#29305;&#28857;&#26159;&#65292;&#22312;&#27979;&#35797;&#26102;&#19981;&#25552;&#20379;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;&#26631;&#35782;&#31526;&#65288;&#25110;&#20219;&#21153;ID&#65289;&#12290;&#20026;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#39044;&#27979;&#20219;&#21153;ID&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#19968;&#31181;&#26032;&#20852;&#30340;&#29702;&#35770;&#19978;&#21512;&#29702;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#26681;&#25454;&#20219;&#21153;&#22686;&#37327;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#22312;&#20849;&#20139;&#32593;&#32476;&#20013;&#20026;&#25152;&#26377;&#20219;&#21153;&#35757;&#32451;&#27599;&#20010;&#20219;&#21153;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#20197;&#22788;&#29702;&#36951;&#24536;&#12290;&#35813;&#26041;&#27861;&#20013;&#27599;&#20010;&#20219;&#21153;&#30340;&#27169;&#22411;&#26159;&#19968;&#20010;&#38750;&#24120;&#35268;&#20998;&#31867;&#22120;&#32780;&#19981;&#26159;&#20256;&#32479;&#20998;&#31867;&#22120;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#12290;&#31163;&#32676;&#26816;&#27979;&#22120;&#21487;&#20197;&#23545;&#20219;&#21153;&#20869;&#65288;&#20998;&#24067;&#20869;&#65288;IND&#65289;&#65289;&#30340;&#31867;&#36827;&#34892;&#39044;&#27979;&#21644;&#35782;&#21035;&#31163;&#32676;&#25968;&#25454;&#12290;&#22312;&#25512;&#26029;&#26399;&#38388;&#65292;&#31163;&#32676;&#26816;&#27979;&#33021;&#21147;&#26159;&#27599;&#20010;&#27979;&#35797;&#26679;&#26412;&#30340;&#20219;&#21153;ID&#39044;&#27979;&#30340;&#20851;&#38190;&#12290;&#28982;&#32780;&#65292;&#26412;&#25991;&#35748;&#20026;&#20351;&#29992;&#20256;&#32479;&#30340;&#31163;&#32676;&#26816;&#27979;&#22120;&#36827;&#34892;&#20219;&#21153;ID&#39044;&#27979;&#26159;&#27425;&#20248;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Class incremental learning (CIL) is a challenging setting of continual learning, which learns a series of tasks sequentially. Each task consists of a set of unique classes. The key feature of CIL is that no task identifier (or task-id) is provided at test time for each test sample. Predicting the task-id for each test sample is a challenging problem. An emerging theoretically justified and effective approach is to train a task-specific model for each task in a shared network for all tasks based on a task-incremental learning (TIL) method to deal with forgetting. The model for each task in this approach is an out-of-distribution (OOD) detector rather than a conventional classifier. The OOD detector can perform both within-task (in-distribution (IND)) class prediction and OOD detection. The OOD detection capability is the key for task-id prediction during inference for each test sample. However, this paper argues that using a traditional OOD detector for task-id prediction is sub-optimal
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#27010;&#29575;&#36710;&#36947;&#22270;&#29983;&#25104;&#21644;&#35299;&#37322;&#35282;&#33853;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#29983;&#25104;&#26032;&#39062;&#32780;&#36924;&#30495;&#30340;&#35282;&#33853;&#24773;&#20917;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2308.13658</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;&#27010;&#29575;&#36710;&#36947;&#22270;&#26469;&#29983;&#25104;&#21644;&#35299;&#37322;&#35282;&#33853;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Generating and Explaining Corner Cases Using Learnt Probabilistic Lane Graphs. (arXiv:2308.13658v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20351;&#29992;&#27010;&#29575;&#36710;&#36947;&#22270;&#29983;&#25104;&#21644;&#35299;&#37322;&#35282;&#33853;&#24773;&#20917;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#29983;&#25104;&#26032;&#39062;&#32780;&#36924;&#30495;&#30340;&#35282;&#33853;&#24773;&#20917;&#65292;&#20197;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#21160;&#24577;&#29615;&#22659;&#20013;&#39564;&#35777;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;AV&#65289;&#30340;&#23433;&#20840;&#24615;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#36710;&#36742;&#26368;&#32456;&#20250;&#36935;&#21040;&#27809;&#26377;&#20195;&#34920;&#24615;&#35757;&#32451;&#25968;&#25454;&#30340;&#23433;&#20840;&#20851;&#38190;&#24773;&#20917;&#12290;&#36890;&#36807;&#22312;&#22522;&#20110;&#27169;&#25311;&#30340;&#22330;&#26223;&#27979;&#35797;&#20013;&#22686;&#21152;&#19981;&#21516;&#30340;&#36947;&#36335;&#21644;&#20132;&#36890;&#26465;&#20214;&#30340;&#35206;&#30422;&#33539;&#22260;&#65292;&#24182;&#21253;&#25324;&#35282;&#33853;&#24773;&#20917;&#65292;&#21487;&#20197;&#25552;&#39640;AV&#30340;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#21253;&#21547;&#22810;&#20010;&#20195;&#29702;&#30340;&#35282;&#33853;&#24773;&#20917;&#22330;&#26223;&#30340;&#21019;&#24314;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35753;&#24037;&#31243;&#24072;&#22522;&#20110;&#21382;&#21490;&#20132;&#36890;&#25968;&#25454;&#29983;&#25104;&#26032;&#39062;&#32780;&#36924;&#30495;&#30340;&#35282;&#33853;&#24773;&#20917;&#65292;&#24182;&#35299;&#37322;&#20026;&#20160;&#20040;&#36825;&#20123;&#24773;&#20917;&#26159;&#23433;&#20840;&#20851;&#38190;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#27010;&#29575;&#36710;&#36947;&#22270;&#65288;PLGs&#65289;&#26469;&#25551;&#36848;&#36710;&#36742;&#21487;&#33021;&#34892;&#39542;&#30340;&#26377;&#38480;&#19968;&#32452;&#36710;&#36947;&#20301;&#32622;&#21644;&#26041;&#21521;&#12290;PLGs&#30340;&#32467;&#26500;&#26159;&#30452;&#25509;&#20174;&#26102;&#31354;&#20132;&#36890;&#25968;&#25454;&#20013;&#23398;&#20064;&#24471;&#21040;&#30340;&#12290;&#22270;&#27169;&#22411;&#20197;&#27010;&#29575;&#31574;&#30053;&#30340;&#24418;&#24335;&#34920;&#31034;&#39550;&#39542;&#21592;&#23545;&#32473;&#23450;&#29366;&#24577;&#30340;&#21709;&#24212;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
Validating the safety of Autonomous Vehicles (AVs) operating in open-ended, dynamic environments is challenging as vehicles will eventually encounter safety-critical situations for which there is not representative training data. By increasing the coverage of different road and traffic conditions and by including corner cases in simulation-based scenario testing, the safety of AVs can be improved. However, the creation of corner case scenarios including multiple agents is non-trivial. Our approach allows engineers to generate novel, realistic corner cases based on historic traffic data and to explain why situations were safety-critical. In this paper, we introduce Probabilistic Lane Graphs (PLGs) to describe a finite set of lane positions and directions in which vehicles might travel. The structure of PLGs is learnt directly from spatio-temporal traffic data. The graph model represents the actions of the drivers in response to a given state in the form of a probabilistic policy. We use
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CPEM&#30340;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#29615;&#22659;&#24863;&#30693;&#35760;&#24518;&#26469;&#25913;&#36827;&#34892;&#20026;&#26234;&#33021;&#20307;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#35273;&#23548;&#33322;&#21644;&#29289;&#20307;&#20132;&#20114;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.07241</link><description>&lt;p&gt;
&#20855;&#26377;&#29615;&#22659;&#24863;&#30693;&#35760;&#24518;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#35268;&#21010;&#29992;&#20110;&#25351;&#23548;&#34892;&#20026;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents. (arXiv:2308.07241v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07241
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;CPEM&#30340;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#19978;&#19979;&#25991;&#20449;&#24687;&#21644;&#29615;&#22659;&#24863;&#30693;&#35760;&#24518;&#26469;&#25913;&#36827;&#34892;&#20026;&#26234;&#33021;&#20307;&#30340;&#24863;&#30693;&#33021;&#21147;&#65292;&#20174;&#32780;&#25552;&#39640;&#35270;&#35273;&#23548;&#33322;&#21644;&#29289;&#20307;&#20132;&#20114;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#25104;&#23478;&#21153;&#20219;&#21153;&#65288;&#20363;&#22914;&#8220;&#25343;&#19968;&#26479;&#27700;&#8221;&#65289;&#38656;&#35201;&#36890;&#36807;&#20445;&#25345;&#23545;&#31354;&#38388;&#23545;&#35937;&#30340;&#31354;&#38388;&#24067;&#23616;&#21644;&#20808;&#21069;&#34892;&#21160;&#30340;&#32467;&#26524;&#30340;&#30693;&#35782;&#26469;&#36827;&#34892;&#36880;&#27493;&#30340;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#34892;&#20026;&#26234;&#33021;&#20307;&#22312;&#24863;&#30693;&#27169;&#22411;&#26041;&#38754;&#32463;&#24120;&#20986;&#38169;&#65292;&#22240;&#20026;&#32570;&#20047;&#36825;&#31181;&#30693;&#35782;&#65292;&#32780;&#20381;&#36182;&#20110;&#19981;&#23436;&#32654;&#30340;&#23398;&#20064;&#30340;&#27169;&#20223;&#26234;&#33021;&#20307;&#25110;&#32773;&#27809;&#26377;&#20851;&#20110;&#20808;&#21069;&#34892;&#21160;&#23545;&#29615;&#22659;&#21464;&#21270;&#30340;&#30693;&#35782;&#30340;&#31639;&#27861;&#35268;&#21010;&#22120;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;CPEM&#65288;&#19978;&#19979;&#25991;&#24863;&#30693;&#35268;&#21010;&#22120;&#21644;&#29615;&#22659;&#24863;&#30693;&#35760;&#24518;&#65289;&#65292;&#23558;&#20808;&#21069;&#34892;&#21160;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#19982;&#29615;&#22659;&#20013;&#29289;&#20307;&#30340;&#31354;&#38388;&#24067;&#23616;&#21644;&#29366;&#24577;&#65288;&#20363;&#22914;&#29289;&#20307;&#26159;&#21542;&#34987;&#31227;&#21160;&#65289;&#32467;&#21512;&#21040;&#24863;&#30693;&#27169;&#22411;&#20013;&#65292;&#20197;&#25913;&#36827;&#35270;&#35273;&#23548;&#33322;&#21644;&#29289;&#20307;&#20132;&#20114;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;CPEM&#22312;&#21508;&#31181;&#24230;&#37327;&#25351;&#26631;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20219;&#21153;&#25104;&#21151;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accomplishing household tasks such as 'bringing a cup of water' requires planning step-by-step actions by maintaining knowledge about the spatial arrangement of objects and the consequences of previous actions. Perception models of the current embodied AI agents, however, often make mistakes due to a lack of such knowledge but rely on imperfect learning of imitating agents or an algorithmic planner without knowledge about the changed environment by the previous actions. To address the issue, we propose CPEM (Context-aware Planner and Environment-aware Memory) to incorporate the contextual information of previous actions for planning and maintaining spatial arrangement of objects with their states (e.g., if an object has been moved or not) in an environment to the perception model for improving both visual navigation and object interaction. We observe that CPEM achieves state-of-the-art task success performance in various metrics using a challenging interactive instruction following ben
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MAoE&#65292;&#19968;&#31181;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20248;&#21270;&#38382;&#39064;&#30340;&#23545;&#27604;&#35299;&#37322;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#31361;&#20986;&#26174;&#31034;&#19982;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#30340;&#24046;&#24322;&#65292;&#24110;&#21161;&#26234;&#33021;&#20307;&#29702;&#35299;&#20026;&#20160;&#20040;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#20182;&#20204;&#30340;&#26399;&#26395;&#12290;</title><link>http://arxiv.org/abs/2308.05984</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#20248;&#21270;&#35299;&#20915;&#26041;&#26696;&#30340;&#23545;&#27604;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Contrastive Explanations of Multi-agent Optimization Solutions. (arXiv:2308.05984v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;MAoE&#65292;&#19968;&#31181;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20248;&#21270;&#38382;&#39064;&#30340;&#23545;&#27604;&#35299;&#37322;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#29983;&#25104;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;&#24182;&#31361;&#20986;&#26174;&#31034;&#19982;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#30340;&#24046;&#24322;&#65292;&#24110;&#21161;&#26234;&#33021;&#20307;&#29702;&#35299;&#20026;&#20160;&#20040;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#20182;&#20204;&#30340;&#26399;&#26395;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#65292;&#26234;&#33021;&#20307;&#21442;&#19982;&#20248;&#21270;&#38382;&#39064;&#12290;&#30001;&#20110;&#22823;&#22810;&#25968;&#24773;&#20917;&#37117;&#26159;&#36229;&#32422;&#26463;&#30340;&#65292;&#26368;&#20248;&#35299;&#24182;&#19981;&#24635;&#33021;&#28385;&#36275;&#25152;&#26377;&#26234;&#33021;&#20307;&#30340;&#35201;&#27714;&#12290;&#26377;&#20123;&#26234;&#33021;&#20307;&#21487;&#33021;&#19981;&#28385;&#24847;&#65292;&#24182;&#25552;&#20986;&#31867;&#20284;&#8220;&#20026;&#20160;&#20040;&#35299;&#20915;&#26041;&#26696;S&#19981;&#28385;&#36275;&#23646;&#24615;P&#65311;&#8221;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MAoE&#65292;&#36825;&#26159;&#19968;&#31181;&#39046;&#22495;&#26080;&#20851;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#65288;i&#65289;&#29983;&#25104;&#19968;&#20010;&#26032;&#30340;&#35299;&#20915;&#26041;&#26696;S'&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#24378;&#21046;&#28385;&#36275;&#23646;&#24615;P&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;S&#21644;S'&#20043;&#38388;&#30340;&#24046;&#24322;&#65307;&#20197;&#21450;&#65288;ii&#65289;&#31361;&#20986;&#26174;&#31034;&#36825;&#20004;&#20010;&#35299;&#20915;&#26041;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36825;&#26679;&#30340;&#35299;&#37322;&#26088;&#22312;&#24110;&#21161;&#26234;&#33021;&#20307;&#29702;&#35299;&#20026;&#20160;&#20040;&#21021;&#22987;&#35299;&#20915;&#26041;&#26696;&#20248;&#20110;&#20182;&#20204;&#30340;&#26399;&#26395;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#35745;&#31639;&#35780;&#20272;&#65292;&#34920;&#26126;MAoE&#21487;&#20197;&#20026;&#22823;&#22411;&#22810;&#26234;&#33021;&#20307;&#20248;&#21270;&#38382;&#39064;&#29983;&#25104;&#23545;&#27604;&#35299;&#37322;&#12290;&#25105;&#20204;&#36824;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#39046;&#22495;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#25552;&#20379;&#36825;&#20123;&#35299;&#37322;&#21518;&#65292;&#29992;&#25143;&#23545;&#35299;&#20915;&#26041;&#26696;&#30340;&#29702;&#35299;&#26377;&#25152;&#25913;&#21892;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios, agents are involved in optimization problems. Since most of these scenarios are over-constrained, optimal solutions do not always satisfy all agents. Some agents might be unhappy and ask questions of the form ``Why does solution $S$ not satisfy property $P$?''. In this paper, we propose MAoE, a domain-independent approach to obtain contrastive explanations by (i) generating a new solution $S^\prime$ where the property $P$ is enforced, while also minimizing the differences between $S$ and $S^\prime$; and (ii) highlighting the differences between the two solutions. Such explanations aim to help agents understanding why the initial solution is better than what they expected. We have carried out a computational evaluation that shows that MAoE can generate contrastive explanations for large multi-agent optimization problems. We have also performed an extensive user study in four different domains that shows that, after being presented with these explanations, h
&lt;/p&gt;</description></item><item><title>&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#22312;&#19968;&#31687;&#31038;&#35770;&#20013;&#21628;&#21505;&#25105;&#20204;&#8220;&#20572;&#27490;&#35848;&#35770;&#26126;&#22825;&#30340;AI&#26411;&#26085;&#65292;&#32780;AI&#20170;&#22825;&#23601;&#23384;&#22312;&#39118;&#38505;&#12290;&#8221;&#36825;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#21028;&#26029;&#22833;&#35823;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26377;&#24433;&#21709;&#21147;&#30340;&#34892;&#21160;&#32773;&#26469;&#35828;&#65292;&#22240;&#20026;&#25105;&#20204;&#26399;&#26395;&#20182;&#20204;&#33021;&#22815;&#32771;&#34385;&#21040;&#38169;&#35823;&#30340;&#21518;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.04440</link><description>&lt;p&gt;
&#33258;&#28982;&#21644;&#26426;&#22120;
&lt;/p&gt;
&lt;p&gt;
Nature and the Machines. (arXiv:2308.04440v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.04440
&lt;/p&gt;
&lt;p&gt;
&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#22312;&#19968;&#31687;&#31038;&#35770;&#20013;&#21628;&#21505;&#25105;&#20204;&#8220;&#20572;&#27490;&#35848;&#35770;&#26126;&#22825;&#30340;AI&#26411;&#26085;&#65292;&#32780;AI&#20170;&#22825;&#23601;&#23384;&#22312;&#39118;&#38505;&#12290;&#8221;&#36825;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#21028;&#26029;&#22833;&#35823;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#26377;&#24433;&#21709;&#21147;&#30340;&#34892;&#21160;&#32773;&#26469;&#35828;&#65292;&#22240;&#20026;&#25105;&#20204;&#26399;&#26395;&#20182;&#20204;&#33021;&#22815;&#32771;&#34385;&#21040;&#38169;&#35823;&#30340;&#21518;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;(AI)&#26159;&#21542;&#23545;&#20154;&#31867;&#26500;&#25104;&#23384;&#22312;&#21361;&#38505;&#65311;&#19968;&#20123;&#25209;&#35780;&#23478;&#35748;&#20026;&#36825;&#20010;&#38382;&#39064;&#27491;&#22312;&#21463;&#21040;&#36807;&#22810;&#30340;&#20851;&#27880;&#65292;&#20182;&#20204;&#24076;&#26395;&#23558;&#20854;&#25512;&#21040;&#19968;&#36793;&#65292;&#36716;&#32780;&#35752;&#35770;AI&#30340;&#21363;&#26102;&#39118;&#38505;&#12290;&#36825;&#20123;&#25209;&#35780;&#23478;&#29616;&#22312;&#21253;&#25324;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#65292;&#22312;&#26368;&#36817;&#30340;&#19968;&#31687;&#31038;&#35770;&#20013;&#25958;&#20419;&#25105;&#20204;&#8220;&#20572;&#27490;&#35848;&#35770;&#26126;&#22825;&#30340;AI&#26411;&#26085;&#65292;&#32780;AI&#20170;&#22825;&#23601;&#23384;&#22312;&#39118;&#38505;&#12290;&#8221;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#19968;&#31181;&#20005;&#37325;&#30340;&#21028;&#26029;&#22833;&#35823;&#65292;&#23545;&#20110;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#26469;&#35828;&#12290;&#22312;&#31185;&#23398;&#39046;&#22495;&#65292;&#23601;&#20687;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#19968;&#26679;&#65292;&#25105;&#20204;&#24076;&#26395;&#26377;&#24433;&#21709;&#21147;&#30340;&#34892;&#20026;&#32773;&#33021;&#22815;&#32771;&#34385;&#38169;&#35823;&#30340;&#21518;&#26524;&#12290;&#20316;&#20026;&#19990;&#30028;&#39046;&#20808;&#30340;&#31185;&#23398;&#26399;&#21002;&#65292;&#12298;&#33258;&#28982;&#12299;&#26434;&#24535;&#26080;&#30097;&#26159;&#19968;&#20010;&#26377;&#24433;&#21709;&#21147;&#30340;&#34892;&#20026;&#32773;&#65292;&#29305;&#21035;&#26159;&#22312;&#32570;&#20047;&#20581;&#20840;&#20840;&#29699;AI&#30417;&#31649;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#23427;&#22312;&#36825;&#20010;&#26696;&#20363;&#20013;&#26126;&#26174;&#26410;&#33021;&#32771;&#34385;&#21040;&#38169;&#35823;&#30340;&#20195;&#20215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Does artificial intelligence (AI) pose existential risks to humanity? Some critics feel this question is getting too much attention, and want to push it aside in favour of conversations about the immediate risks of AI. These critics now include the journal Nature, where a recent editorial urges us to 'stop talking about tomorrow's AI doomsday when AI poses risks today.' We argue that this is a serious failure of judgement, on Nature's part. In science, as in everyday life, we expect influential actors to consider the consequences of error. As the world's leading scientific journal, Nature is certainly an influential actor, especially so in the absence of robust global regulation of AI. Yet it has manifestly failed to consider the cost of error in this case.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;</title><link>http://arxiv.org/abs/2307.12375</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22312;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#19978;&#20855;&#26377;&#21019;&#26032;&#65292;&#20294;&#24182;&#38750;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning in Large Language Models Learns Label Relationships but Is Not Conventional Learning. (arXiv:2307.12375v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12375
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21253;&#21547;&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#30340;&#23398;&#20064;&#33021;&#21147;&#20351;&#20854;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#34920;&#29616;&#26174;&#33879;&#25552;&#39640;&#65292;&#20294;&#19982;&#20256;&#32479;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#12289;&#39044;&#35757;&#32451;&#20013;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#19978;&#19979;&#25991;&#23398;&#20064;&#22914;&#20309;&#32858;&#21512;&#26631;&#31614;&#20449;&#24687;&#12290;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;LLMs&#30340;&#24037;&#20316;&#26426;&#21046;&#21450;&#20854;&#23545;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#22788;&#29702;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24615;&#33021;&#22312;&#21253;&#21547;&#36755;&#20837;-&#26631;&#31614;&#20851;&#31995;&#31034;&#20363;&#30340;&#19978;&#19979;&#25991;&#20013;&#36890;&#24120;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;LLMs&#30340;&#36825;&#31181;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#30340;&#24037;&#20316;&#26426;&#21046;&#23578;&#26080;&#20849;&#35782;&#65306;&#20363;&#22914;&#65292;&#34429;&#28982;Xie&#31561;&#20154;&#65288;2021&#24180;&#65289;&#23558;ICL&#27604;&#20316;&#19968;&#31181;&#36890;&#29992;&#23398;&#20064;&#31639;&#27861;&#65292;&#20294;Min&#31561;&#20154;&#65288;2022b&#24180;&#65289;&#35748;&#20026;ICL&#29978;&#33267;&#19981;&#33021;&#20174;&#19978;&#19979;&#25991;&#31034;&#20363;&#20013;&#23398;&#20064;&#26631;&#31614;&#20851;&#31995;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#19977;&#20010;&#38382;&#39064;&#65306;&#65288;1&#65289;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#32467;&#26524;&#65292;&#65288;2&#65289;&#39044;&#35757;&#32451;&#26399;&#38388;&#23398;&#20064;&#21040;&#30340;&#26631;&#31614;&#20851;&#31995;&#22914;&#20309;&#19982;&#19978;&#19979;&#25991;&#20013;&#25552;&#20379;&#30340;&#36755;&#20837;-&#26631;&#31614;&#31034;&#20363;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#21450;&#65288;3&#65289;ICL&#22914;&#20309;&#32858;&#21512;&#26469;&#33258;&#19978;&#19979;&#25991;&#31034;&#20363;&#30340;&#26631;&#31614;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;LLMs&#36890;&#24120;&#20250;&#25972;&#21512;&#19978;&#19979;&#25991;&#26631;&#31614;&#30340;&#20449;&#24687;&#65292;&#20294;&#39044;&#35757;&#32451;&#21644;&#19978;&#19979;&#25991;&#26631;&#31614;&#20851;&#31995;&#34987;&#21306;&#21035;&#23545;&#24453;&#65292;&#27169;&#22411;&#19981;&#20250;&#23558;&#25152;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#31561;&#21516;&#23545;&#24453;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#20102;&#23545;LLMs&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
The performance of Large Language Models (LLMs) on downstream tasks often improves significantly when including examples of the input-label relationship in the context. However, there is currently no consensus about how this in-context learning (ICL) ability of LLMs works: for example, while Xie et al. (2021) liken ICL to a general-purpose learning algorithm, Min et al. (2022b) argue ICL does not even learn label relationships from in-context examples. In this paper, we study (1) how labels of in-context examples affect predictions, (2) how label relationships learned during pre-training interact with input-label examples provided in-context, and (3) how ICL aggregates label information across in-context examples. Our findings suggests LLMs usually incorporate information from in-context labels, but that pre-training and in-context label relationships are treated differently, and that the model does not consider all in-context information equally. Our results give insights into underst
&lt;/p&gt;</description></item><item><title>UniTabE&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.09249</link><description>&lt;p&gt;
UniTabE: &#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
UniTabE: Pretraining a Unified Tabular Encoder for Heterogeneous Tabular Data. (arXiv:2307.09249v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09249
&lt;/p&gt;
&lt;p&gt;
UniTabE&#26159;&#19968;&#31181;&#38754;&#21521;&#24322;&#26500;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#34920;&#26684;&#32534;&#30721;&#22120;&#65292;&#33021;&#22815;&#22788;&#29702;&#19981;&#21516;&#34920;&#26684;&#32467;&#26500;&#30340;&#25361;&#25112;&#65292;&#24182;&#20855;&#26377;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#26126;&#35777;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#31361;&#30772;&#24615;&#24433;&#21709;&#65292;&#22312;&#21508;&#31181;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#23558;&#39044;&#35757;&#32451;&#26041;&#27861;&#30340;&#23041;&#21147;&#25193;&#23637;&#21040;&#20256;&#32479;&#34987;&#24573;&#35270;&#30340;&#34920;&#26684;&#25968;&#25454;&#39046;&#22495;&#65292;&#35813;&#39046;&#22495;&#30001;&#20110;&#19981;&#21516;&#20219;&#21153;&#22266;&#26377;&#30340;&#20247;&#22810;&#34920;&#26684;&#27169;&#24335;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#24037;&#20316;&#30340;&#20027;&#35201;&#30740;&#31350;&#38382;&#39064;&#22260;&#32469;&#24322;&#26500;&#34920;&#26684;&#32467;&#26500;&#30340;&#36866;&#24212;&#24615;&#12289;&#34920;&#26684;&#25968;&#25454;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#21327;&#35758;&#30340;&#24314;&#31435;&#12289;&#23398;&#21040;&#30340;&#30693;&#35782;&#22312;&#20219;&#21153;&#20043;&#38388;&#30340;&#27867;&#21270;&#21644;&#21487;&#20256;&#36882;&#24615;&#12289;&#23545;&#22810;&#26679;&#21270;&#19979;&#28216;&#24212;&#29992;&#30340;&#36866;&#24212;&#24615;&#20197;&#21450;&#38543;&#26102;&#38388;&#30340;&#22686;&#37327;&#21015;&#30340;&#32435;&#20837;&#36827;&#34892;&#20102;&#25506;&#35752;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;UniTabE&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#20197;&#19968;&#33268;&#30340;&#26041;&#24335;&#22788;&#29702;&#34920;&#26684;&#65292;&#25670;&#33073;&#20102;&#29305;&#23450;&#34920;&#26684;&#32467;&#26500;&#24378;&#21152;&#30340;&#32422;&#26463;&#12290;UniTabE&#30340;&#26680;&#24515;&#27010;&#24565;&#26159;&#23545;&#27599;&#20010;&#22522;&#26412;&#34920;&#26684;&#36827;&#34892;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Recent advancements in Natural Language Processing (NLP) have witnessed the groundbreaking impact of pretrained models, yielding impressive outcomes across various tasks. This study seeks to extend the power of pretraining methodologies to tabular data, a domain traditionally overlooked, yet inherently challenging due to the plethora of table schemas intrinsic to different tasks. The primary research questions underpinning this work revolve around the adaptation to heterogeneous table structures, the establishment of a universal pretraining protocol for tabular data, the generalizability and transferability of learned knowledge across tasks, the adaptation to diverse downstream applications, and the incorporation of incremental columns over time. In response to these challenges, we introduce UniTabE, a pioneering method designed to process tables in a uniform manner, devoid of constraints imposed by specific table structures. UniTabE's core concept relies on representing each basic tab
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;&#20004;&#20010;&#24433;&#21709;VFL&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24341;&#20837;&#30495;&#23454;&#30340;VFL&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#22270;&#20687;-&#22270;&#20687;VFL&#24773;&#26223;&#20013;&#30340;&#19981;&#36275;&#12290;&#30740;&#31350;&#23545;&#20110;&#26410;&#26469;&#30340;VFL&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2307.02040</link><description>&lt;p&gt;
VertiBench: &#22312;&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#22522;&#20934;&#20013;&#25512;&#36827;&#29305;&#24449;&#20998;&#24067;&#22810;&#26679;&#24615;
&lt;/p&gt;
&lt;p&gt;
VertiBench: Advancing Feature Distribution Diversity in Vertical Federated Learning Benchmarks. (arXiv:2307.02040v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02040
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#20004;&#20010;&#24433;&#21709;VFL&#24615;&#33021;&#30340;&#20851;&#38190;&#22240;&#32032;&#65306;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26041;&#27861;&#12290;&#21516;&#26102;&#65292;&#36890;&#36807;&#24341;&#20837;&#30495;&#23454;&#30340;VFL&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#22270;&#20687;-&#22270;&#20687;VFL&#24773;&#26223;&#20013;&#30340;&#19981;&#36275;&#12290;&#30740;&#31350;&#23545;&#20110;&#26410;&#26469;&#30340;VFL&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#22312;&#29305;&#24449;&#21010;&#20998;&#30340;&#20998;&#24067;&#24335;&#25968;&#25454;&#19978;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#38544;&#31169;&#38480;&#21046;&#65292;&#24456;&#23569;&#26377;&#20844;&#24320;&#30340;&#30495;&#23454;&#19990;&#30028;VFL&#25968;&#25454;&#38598;&#29992;&#20110;&#31639;&#27861;&#35780;&#20272;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#38598;&#21482;&#20195;&#34920;&#20102;&#26377;&#38480;&#30340;&#29305;&#24449;&#20998;&#24067;&#12290;&#29616;&#26377;&#30340;&#22522;&#20934;&#36890;&#24120;&#37319;&#29992;&#20174;&#20840;&#23616;&#38598;&#21512;&#20013;&#30340;&#20219;&#24847;&#29305;&#24449;&#21010;&#20998;&#23548;&#20986;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36825;&#21482;&#25429;&#25417;&#21040;&#20102;&#19968;&#37096;&#20998;&#29305;&#24449;&#20998;&#24067;&#65292;&#23548;&#33268;&#31639;&#27861;&#24615;&#33021;&#35780;&#20272;&#19981;&#36275;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#24433;&#21709;VFL&#24615;&#33021;&#30340;&#20004;&#20010;&#20851;&#38190;&#22240;&#32032;&#8212;&#8212;&#29305;&#24449;&#37325;&#35201;&#24615;&#21644;&#29305;&#24449;&#30456;&#20851;&#24615;&#65292;&#24182;&#25552;&#20986;&#30456;&#20851;&#30340;&#35780;&#20272;&#25351;&#26631;&#21644;&#25968;&#25454;&#38598;&#21010;&#20998;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#30495;&#23454;&#30340;VFL&#25968;&#25454;&#38598;&#26469;&#24357;&#34917;&#22270;&#20687;-&#22270;&#20687;VFL&#24773;&#26223;&#20013;&#30340;&#19981;&#36275;&#12290;&#25105;&#20204;&#23545;&#23574;&#31471;VFL&#31639;&#27861;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical Federated Learning (VFL) is a crucial paradigm for training machine learning models on feature-partitioned, distributed data. However, due to privacy restrictions, few public real-world VFL datasets exist for algorithm evaluation, and these represent a limited array of feature distributions. Existing benchmarks often resort to synthetic datasets, derived from arbitrary feature splits from a global set, which only capture a subset of feature distributions, leading to inadequate algorithm performance assessment. This paper addresses these shortcomings by introducing two key factors affecting VFL performance - feature importance and feature correlation - and proposing associated evaluation metrics and dataset splitting methods. Additionally, we introduce a real VFL dataset to address the deficit in image-image VFL scenarios. Our comprehensive evaluation of cutting-edge VFL algorithms provides valuable insights for future research in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#29575;&#32422;&#26463;&#19979;&#30340;&#23433;&#20840;&#20851;&#38190;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26126;&#30830;&#26799;&#24230;&#34920;&#36798;&#24335;&#30340;Safe Policy Gradient-REINFORCE&#65288;SPG-REINFORCE&#65289;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#30028;&#38480;&#35777;&#26126;&#20102;&#27010;&#29575;&#32422;&#26463;&#35774;&#32622;&#22312;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#20855;&#26377;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;</title><link>http://arxiv.org/abs/2306.17279</link><description>&lt;p&gt;
&#23433;&#20840;&#20851;&#38190;&#24378;&#21270;&#23398;&#20064;&#30340;&#27010;&#29575;&#32422;&#26463;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Constraint for Safety-Critical Reinforcement Learning. (arXiv:2306.17279v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27010;&#29575;&#32422;&#26463;&#19979;&#30340;&#23433;&#20840;&#20851;&#38190;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26126;&#30830;&#26799;&#24230;&#34920;&#36798;&#24335;&#30340;Safe Policy Gradient-REINFORCE&#65288;SPG-REINFORCE&#65289;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#29702;&#35770;&#30028;&#38480;&#35777;&#26126;&#20102;&#27010;&#29575;&#32422;&#26463;&#35774;&#32622;&#22312;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#20043;&#38388;&#20855;&#26377;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#27010;&#29575;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#20013;&#23398;&#20064;&#23433;&#20840;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#23433;&#20840;&#31574;&#30053;&#25110;&#25511;&#21046;&#22120;&#26159;&#25351;&#20197;&#39640;&#27010;&#29575;&#20445;&#25345;&#20195;&#29702;&#22312;&#32473;&#23450;&#23433;&#20840;&#38598;&#21512;&#20013;&#30340;&#36712;&#36857;&#12290;&#25105;&#20204;&#22312;&#29616;&#26377;&#25991;&#29486;&#20013;&#39057;&#32321;&#25506;&#32034;&#30340;&#32047;&#31215;&#32422;&#26463;&#38382;&#39064;&#21644;&#36825;&#31181;&#27010;&#29575;&#32422;&#26463;&#38382;&#39064;&#20043;&#38388;&#24314;&#31435;&#20102;&#32852;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29702;&#35770;&#30028;&#38480;&#65292;&#38416;&#26126;&#27010;&#29575;&#32422;&#26463;&#35774;&#32622;&#22312;&#26368;&#20248;&#24615;&#21644;&#23433;&#20840;&#24615;&#65288;&#32422;&#26463;&#28385;&#36275;&#65289;&#26041;&#38754;&#20855;&#26377;&#26356;&#22909;&#30340;&#26435;&#34913;&#12290;&#22312;&#22788;&#29702;&#27010;&#29575;&#32422;&#26463;&#26102;&#36935;&#21040;&#30340;&#25361;&#25112;&#65292;&#27491;&#22914;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25152;&#25506;&#32034;&#30340;&#37027;&#26679;&#65292;&#28304;&#20110;&#27809;&#26377;&#26126;&#30830;&#30340;&#26799;&#24230;&#34920;&#36798;&#24335;&#12290;&#25105;&#20204;&#20043;&#21069;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#36825;&#31181;&#26126;&#30830;&#30340;&#26799;&#24230;&#34920;&#36798;&#24335;&#65292;&#31216;&#20043;&#20026;Safe Policy Gradient-REINFORCE&#65288;SPG-REINFORCE&#65289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#25913;&#36827;&#30340;&#26799;&#24230;SPG-Actor-Critic
&lt;/p&gt;
&lt;p&gt;
In this paper, we consider the problem of learning safe policies for probabilistic-constrained reinforcement learning (RL). Specifically, a safe policy or controller is one that, with high probability, maintains the trajectory of the agent in a given safe set. We establish a connection between this probabilistic-constrained setting and the cumulative-constrained formulation that is frequently explored in the existing literature. We provide theoretical bounds elucidating that the probabilistic-constrained setting offers a better trade-off in terms of optimality and safety (constraint satisfaction). The challenge encountered when dealing with the probabilistic constraints, as explored in this work, arises from the absence of explicit expressions for their gradients. Our prior work provides such an explicit gradient expression for probabilistic constraints which we term Safe Policy Gradient-REINFORCE (SPG-REINFORCE). In this work, we provide an improved gradient SPG-Actor-Critic that lead
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#32544;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.16334</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#26631;&#24535;&#26816;&#27979;&#26469;&#35782;&#21035;&#31163;&#25955;&#21270;&#28508;&#22312;&#22352;&#26631;&#31995;&#32479;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability of Discretized Latent Coordinate Systems via Density Landmarks Detection. (arXiv:2306.16334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#32544;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#26088;&#22312;&#20165;&#20174;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#20013;&#24674;&#22797;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#30495;&#23454;&#22240;&#32032;&#12290; &#21487;&#35782;&#21035;&#24615;&#20026;&#35299;&#32544;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290; &#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#33258;&#36866;&#24212;&#29420;&#31435;&#28508;&#21464;&#37327;&#22240;&#23376;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#20809;&#28369;&#22240;&#23376;&#21040;&#35266;&#27979;&#30340;&#26144;&#23556;&#19979;&#65292;&#26080;&#30417;&#30563;&#30340;&#21487;&#35782;&#21035;&#24615;&#22312;i.i.d.&#35774;&#32622;&#19979;&#26159;&#29702;&#35770;&#19978;&#19981;&#21487;&#33021;&#30340;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#24120;&#24778;&#20154;&#30340;&#26159;&#65292;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#20809;&#28369;&#26144;&#23556;&#65288;&#19968;&#20010;&#24494;&#20998;&#21516;&#32986;&#65289;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#26144;&#23556;&#36827;&#34892;&#20219;&#20309;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290; &#36825;&#26159;&#22312;&#20551;&#35774;&#28508;&#22312;&#23494;&#24230;&#20855;&#26377;&#36724;&#23545;&#40784;&#30340;&#19981;&#36830;&#32493;&#26631;&#24535;&#30340;&#24773;&#20917;&#19979;&#65292;&#20294;&#19981;&#20570;&#22240;&#32032;&#30340;&#32479;&#35745;&#29420;&#31435;&#30340;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#12290; &#25105;&#20204;&#24341;&#20837;&#20102;&#36825;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#23545;&#24674;&#22797;&#31163;&#25955;&#22352;&#26631;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentanglement aims to recover meaningful latent ground-truth factors from only the observed distribution. Identifiability provides the theoretical grounding for disentanglement to be well-founded. Unfortunately, unsupervised identifiability of independent latent factors is a theoretically proven impossibility in the i.i.d. setting under a general nonlinear smooth map from factors to observations. In this work, we show that, remarkably, it is possible to recover discretized latent coordinates under a highly generic nonlinear smooth mapping (a diffeomorphism) without any additional inductive bias on the mapping. This is, assuming that latent density has axis-aligned discontinuity landmarks, but without making the unrealistic assumption of statistical independence of the factors. We introduce this novel form of identifiability, termed quantized coordinate identifiability, and provide a comprehensive proof of the recovery of discretized coordinates.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.00010</link><description>&lt;p&gt;
&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Explainability in Simplicial Map Neural Networks. (arXiv:2306.00010v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#30340;&#35757;&#32451;&#36807;&#31243;&#21644;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31616;&#21333;&#24418;&#24335;&#26144;&#23556;&#31070;&#32463;&#32593;&#32476;&#65288;SMNN&#65289;&#26159;&#22522;&#20110;&#25299;&#25169;&#23398;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20855;&#26377;&#26222;&#36866;&#36924;&#36817;&#33021;&#21147;&#21644;&#22312;&#36866;&#24403;&#26465;&#20214;&#19979;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#40065;&#26834;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#32500;&#20013;&#24212;&#29992; SMNN &#23384;&#22312;&#19968;&#20123;&#29942;&#39048;&#65292;&#39318;&#20808;&#27809;&#26377;&#23450;&#20041; SMNN &#30340;&#35757;&#32451;&#36807;&#31243;&#65292;&#20854;&#27425;&#23545;&#20110;&#36755;&#20837;&#25968;&#25454;&#38598;&#38656;&#35201;&#26500;&#24314;&#19968;&#20010;&#21253;&#22260;&#20984;&#22810;&#38754;&#20307;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#32473;&#23450;&#25968;&#25454;&#38598;&#30340;&#25903;&#25345;&#23376;&#38598;&#21644;&#25237;&#24433;&#21040;&#36229;&#29699;&#38754;&#30340;&#26041;&#27861;&#20316;&#20026;&#26367;&#20195;&#20984;&#22810;&#38754;&#20307;&#30340; SMNN &#35757;&#32451;&#36807;&#31243;&#65292;&#24182;&#39318;&#27425;&#24341;&#20837;&#20102; SMNN &#30340;&#21487;&#35299;&#37322;&#24615;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simplicial map neural networks (SMNNs) are topology-based neural networks with interesting properties such as universal approximation capability and robustness to adversarial examples under appropriate conditions. However, SMNNs present some bottlenecks for their possible application in high dimensions. First, no SMNN training process has been defined so far. Second, SMNNs require the construction of a convex polytope surrounding the input dataset. In this paper, we propose a SMNN training procedure based on a support subset of the given dataset and a method based on projection to a hypersphere as a replacement for the convex polytope construction. In addition, the explainability capacity of SMNNs is also introduced for the first time in this paper.
&lt;/p&gt;</description></item><item><title>GRACE++&#26159;&#19968;&#20010;&#25239;&#20002;&#21253;&#30340;&#23454;&#26102;&#35270;&#39057;&#31995;&#32479;&#65292;&#36890;&#36807;&#31070;&#32463;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#20002;&#21253;&#24773;&#20917;&#19979;&#20445;&#25345;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.12333</link><description>&lt;p&gt;
GRACE++&#65306;&#36890;&#36807;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#25239;&#20002;&#21253;&#30340;&#23454;&#26102;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
GRACE++: Loss-Resilient Real-Time Video through Neural Codecs. (arXiv:2305.12333v2 [cs.MM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12333
&lt;/p&gt;
&lt;p&gt;
GRACE++&#26159;&#19968;&#20010;&#25239;&#20002;&#21253;&#30340;&#23454;&#26102;&#35270;&#39057;&#31995;&#32479;&#65292;&#36890;&#36807;&#31070;&#32463;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#20002;&#21253;&#24773;&#20917;&#19979;&#20445;&#25345;&#29992;&#25143;&#20307;&#39564;&#36136;&#37327;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#26102;&#35270;&#39057;&#36890;&#20449;&#20013;&#65292;&#30001;&#20110;&#20005;&#26684;&#30340;&#24310;&#36831;&#35201;&#27714;&#65292;&#37325;&#26032;&#20256;&#36755;&#20002;&#22833;&#30340;&#25968;&#25454;&#21253;&#22312;&#39640;&#24310;&#36831;&#32593;&#32476;&#19979;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#20026;&#20102;&#24212;&#23545;&#27809;&#26377;&#37325;&#20256;&#30340;&#20002;&#21253;&#24773;&#20917;&#65292;&#20351;&#29992;&#20102;&#20004;&#31181;&#20027;&#35201;&#31574;&#30053;--&#22522;&#20110;&#32534;&#30721;&#22120;&#30340;&#21069;&#21521;&#24046;&#38169;&#32416;&#27491;&#65288;FEC&#65289;&#21644;&#22522;&#20110;&#35299;&#30721;&#22120;&#30340;&#38169;&#35823;&#38544;&#34255;&#12290;&#21069;&#32773;&#22312;&#20256;&#36755;&#20043;&#21069;&#29992;&#20887;&#20313;&#32534;&#30721;&#25968;&#25454;&#65292;&#20294;&#25552;&#21069;&#30830;&#23450;&#26368;&#20339;&#20887;&#20313;&#32423;&#21035;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21518;&#32773;&#20174;&#37096;&#20998;&#25910;&#21040;&#30340;&#24103;&#20013;&#37325;&#24314;&#35270;&#39057;&#65292;&#20294;&#23558;&#24103;&#21010;&#20998;&#20026;&#29420;&#31435;&#32534;&#30721;&#30340;&#20998;&#21306;&#20250;&#38477;&#20302;&#21387;&#32553;&#25928;&#29575;&#65292;&#24182;&#19988;&#20002;&#22833;&#30340;&#20449;&#24687;&#22312;&#27809;&#26377;&#36866;&#24212;&#32534;&#30721;&#22120;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#26377;&#25928;&#22320;&#34987;&#35299;&#30721;&#22120;&#24674;&#22797;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GRACE++&#30340;&#25239;&#20002;&#21253;&#23454;&#26102;&#35270;&#39057;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#35270;&#39057;&#32534;&#35299;&#30721;&#22120;&#65292;&#23427;&#33021;&#22815;&#22312;&#21508;&#31181;&#20002;&#21253;&#24773;&#20917;&#19979;&#20445;&#25345;&#29992;&#25143;&#30340;&#20307;&#39564;&#36136;&#37327;&#65288;QoE&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-time video communication, retransmitting lost packets over high-latency networks is not viable due to strict latency requirements. To counter packet losses without retransmission, two primary strategies are employed -- encoder-based forward error correction (FEC) and decoder-based error concealment. The former encodes data with redundancy before transmission, yet determining the optimal redundancy level in advance proves challenging. The latter reconstructs video from partially received frames, but dividing a frame into independently coded partitions inherently compromises compression efficiency, and the lost information cannot be effectively recovered by the decoder without adapting the encoder.  We present a loss-resilient real-time video system called GRACE++, which preserves the user's quality of experience (QoE) across a wide range of packet losses through a new neural video codec. Central to GRACE++'s enhanced loss resilience is its joint training of the neural encoder an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#32780;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#23545;&#20110;&#35760;&#24518;&#24418;&#25104;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.09144</link><description>&lt;p&gt;
&#35760;&#24518;&#36824;&#26159;&#24536;&#21364;&#65311;&#28145;&#20837;&#25506;&#35752;&#35821;&#35328;&#27169;&#22411;&#30340;&#30693;&#35782;&#35760;&#24518;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Retentive or Forgetful? Diving into the Knowledge Memorizing Mechanism of Language Models. (arXiv:2305.09144v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09144
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#21457;&#29616;&#39044;&#35757;&#32451;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#27169;&#22411;&#30340;&#35760;&#24518;&#33021;&#21147;&#65292;&#32780;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#23545;&#20110;&#35760;&#24518;&#24418;&#25104;&#20063;&#26377;&#26174;&#33879;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35760;&#24518;&#26159;&#26368;&#22522;&#26412;&#30340;&#35748;&#30693;&#21151;&#33021;&#20043;&#19968;&#65292;&#26159;&#23384;&#20648;&#19990;&#30028;&#30693;&#35782;&#21644;&#27963;&#21160;&#32463;&#21382;&#30340;&#20648;&#34255;&#24211;&#12290;&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#35760;&#24518;&#33021;&#21147;&#12290;&#30456;&#21453;&#65292;&#27809;&#26377;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#23384;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#31181;&#20445;&#25345;-&#36951;&#24536;&#30340;&#30683;&#30462;&#24182;&#20102;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#35760;&#24518;&#26426;&#21046;&#65292;&#25105;&#20204;&#36890;&#36807;&#25511;&#21046;&#30446;&#26631;&#30693;&#35782;&#31867;&#22411;&#12289;&#23398;&#20064;&#31574;&#30053;&#21644;&#23398;&#20064;&#26102;&#38388;&#34920;&#31561;&#65292;&#24320;&#23637;&#20102;&#28145;&#20837;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#32467;&#26524;&#21457;&#29616;&#65306;1&#65289;&#20256;&#32479;&#35821;&#35328;&#27169;&#22411;&#26159;&#23481;&#26131;&#36951;&#24536;&#30340;&#65307;2&#65289;&#39044;&#35757;&#32451;&#21487;&#20197;&#20351;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#35760;&#24518;&#33021;&#21147;&#65307;3&#65289;&#30693;&#35782;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#26174;&#33879;&#24433;&#21709;&#35760;&#24518;&#24418;&#25104;&#12290;&#36825;&#20123;&#32467;&#35770;&#26377;&#21161;&#20110;&#29702;&#35299;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#24182;&#20026;&#35774;&#35745;&#21644;&#35780;&#20272;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#23398;&#20064;&#26041;&#27861;&#21644;&#25512;&#29702;&#31639;&#27861;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Memory is one of the most essential cognitive functions serving as a repository of world knowledge and episodes of activities. In recent years, large-scale pre-trained language models have shown remarkable memorizing ability. On the contrary, vanilla neural networks without pre-training have been long observed suffering from the catastrophic forgetting problem. To investigate such a retentive-forgetful contradiction and understand the memory mechanism of language models, we conduct thorough experiments by controlling the target knowledge types, the learning strategies and the learning schedules. We find that: 1) Vanilla language models are forgetful; 2) Pre-training leads to retentive language models; 3) Knowledge relevance and diversification significantly influence the memory formation. These conclusions are useful for understanding the abilities of pre-trained language models and shed light on designing and evaluating new learning and inference algorithms of language models.
&lt;/p&gt;</description></item><item><title>AGI&#25216;&#26415;&#20855;&#26377;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#28508;&#21147;&#65292;&#21487;&#20197;&#24314;&#31435;e-learning&#24179;&#21488;&#12289;&#25945;&#32946;&#21327;&#20316;&#24037;&#20855;&#31561;&#65292;&#24357;&#34917;&#20256;&#32479;AI&#27169;&#22411;&#22240;&#21463;&#38480;&#20110;&#25968;&#25454;&#21644;&#20154;&#38469;&#20132;&#20114;&#38480;&#21046;&#32780;&#26080;&#27861;&#28385;&#36275;&#25945;&#32946;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2304.12479</link><description>&lt;p&gt;
&#29992;&#20110;&#25945;&#32946;&#30340;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;
&lt;/p&gt;
&lt;p&gt;
Artificial General Intelligence (AGI) for Education. (arXiv:2304.12479v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12479
&lt;/p&gt;
&lt;p&gt;
AGI&#25216;&#26415;&#20855;&#26377;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#28508;&#21147;&#65292;&#21487;&#20197;&#24314;&#31435;e-learning&#24179;&#21488;&#12289;&#25945;&#32946;&#21327;&#20316;&#24037;&#20855;&#31561;&#65292;&#24357;&#34917;&#20256;&#32479;AI&#27169;&#22411;&#22240;&#21463;&#38480;&#20110;&#25968;&#25454;&#21644;&#20154;&#38469;&#20132;&#20114;&#38480;&#21046;&#32780;&#26080;&#27861;&#28385;&#36275;&#25945;&#32946;&#38656;&#27714;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;&#22914;GPT-4&#21644;ChatGPT&#65289;&#30340;&#20986;&#29616;&#65292;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AGI&#65289;&#20316;&#20026;&#26410;&#26469;&#25216;&#26415;&#24050;&#32463;&#24471;&#21040;&#20840;&#29699;&#35748;&#21487;&#12290;AGI&#26088;&#22312;&#36890;&#36807;&#35745;&#31639;&#26426;&#31995;&#32479;&#22797;&#21046;&#20154;&#31867;&#26234;&#33021;&#65292;&#26159;&#20855;&#26377;&#38761;&#21629;&#25945;&#32946;&#39046;&#22495;&#28508;&#21147;&#30340;&#20851;&#38190;&#25216;&#26415;&#20043;&#19968;&#12290;&#19982;&#20256;&#32479;&#30340;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30456;&#27604;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#21482;&#38024;&#23545;&#26377;&#38480;&#33539;&#22260;&#30340;&#20219;&#21153;&#36827;&#34892;&#35774;&#35745;&#65292;&#38656;&#35201;&#22823;&#37327;&#29305;&#23450;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#33021;&#26080;&#27861;&#32771;&#34385;&#25945;&#32946;&#20013;&#22797;&#26434;&#30340;&#20154;&#38469;&#21160;&#24577;&#12290;&#21463;&#26368;&#36817;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#39537;&#21160;&#65292;AGI&#20195;&#34920;&#20102;&#26426;&#22120;&#22312;&#25191;&#34892;&#38656;&#35201;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#20219;&#21153;&#26041;&#38754;&#30340;&#37325;&#22823;&#39134;&#36291;&#65292;&#20363;&#22914;&#25512;&#29702;&#12289;&#35299;&#20915;&#38382;&#39064;&#12289;&#20570;&#20986;&#20915;&#31574;&#65292;&#29978;&#33267;&#29702;&#35299;&#20154;&#31867;&#24773;&#24863;&#21644;&#31038;&#20132;&#20114;&#21160;&#12290;&#26412;&#30740;&#31350;&#22238;&#39038;&#20102;AGI&#30340;&#20851;&#38190;&#27010;&#24565;&#12289;&#33021;&#21147;&#12289;&#33539;&#22260;&#21644;&#22312;&#26410;&#26469;&#25945;&#32946;&#20013;&#30340;&#28508;&#21147;&#65292;&#21253;&#25324;&#24314;&#31435;e-learning&#24179;&#21488;&#21644;&#25945;&#32946;&#21327;&#20316;&#24037;&#20855;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial general intelligence (AGI) has gained global recognition as a future technology due to the emergence of breakthrough large language models and chatbots such as GPT-4 and ChatGPT, respectively. AGI aims to replicate human intelligence through computer systems, which is one of the critical technologies having the potential to revolutionize the field of education. Compared to conventional AI models, typically designed for a limited range of tasks, demand significant amounts of domain-specific data for training and may not always consider intricate interpersonal dynamics in education. AGI, driven by the recent large pre-trained models, represents a significant leap in the capability of machines to perform tasks that require human-level intelligence, such as reasoning, problem-solving, decision-making, and even understanding human emotions and social interactions. This work reviews AGI's key concepts, capabilities, scope, and potential within future education, including setting e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#32954;&#30284;&#35786;&#26029;&#30340;&#22810;&#20219;&#21153;CT&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#65292;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#32954;&#37096;CT&#20998;&#21106;&#31561;&#22810;&#20010;&#21307;&#23398;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2304.02649</link><description>&lt;p&gt;
&#24102;&#26377;&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#30340;CT&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
CT Multi-Task Learning with a Large Image-Text (LIT) Model. (arXiv:2304.02649v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#29992;&#20110;&#32954;&#30284;&#35786;&#26029;&#30340;&#22810;&#20219;&#21153;CT&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#65292;&#33021;&#24456;&#22909;&#22320;&#25191;&#34892;&#32954;&#37096;CT&#20998;&#21106;&#31561;&#22810;&#20010;&#21307;&#23398;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#20165;&#33021;&#22815;&#25903;&#25345;&#22810;&#31181;&#35821;&#35328;&#20219;&#21153;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#20316;&#20026;&#19981;&#21516;&#39046;&#22495;&#30340;&#36890;&#29992;&#25509;&#21475;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#35777;&#26126;&#22914;&#20309;&#23558;LLM&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#30340;&#25104;&#21151;&#26377;&#25928;&#22320;&#36716;&#21270;&#20026;&#28041;&#21450;&#39640;&#32500;&#21644;&#22810;&#27169;&#24577;&#21307;&#23398;&#22270;&#20687;&#30340;&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25253;&#21578;&#20102;&#19968;&#39033;&#21487;&#34892;&#24615;&#30740;&#31350;&#65292;&#36890;&#36807;&#32452;&#21512;LLM&#21644;&#22823;&#22411;&#22270;&#20687;&#27169;&#22411;&#65288;LIM&#65289;&#65292;&#24314;&#31435;&#22810;&#20219;&#21153;CT&#22823;&#22411;&#22270;&#20687;&#25991;&#26412;&#65288;LIT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#32954;&#30284;&#35786;&#26029;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;LLM&#21644;LIM&#29992;&#20316;&#32534;&#30721;&#22120;&#65292;&#26681;&#25454;&#29305;&#23450;&#20219;&#21153;&#30340;&#25991;&#26412;&#25552;&#31034;&#26469;&#24863;&#30693;&#22810;&#27169;&#24577;&#20449;&#24687;&#65292;&#20174;&#32780;&#21327;&#21516;&#20316;&#29992;&#20110;&#22810;&#28304;&#20449;&#24687;&#21644;&#20219;&#21153;&#29305;&#23450;&#21644;&#24739;&#32773;&#29305;&#23450;&#30340;&#20808;&#39564;&#65292;&#20197;&#20248;&#21270;&#35786;&#26029;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;LIT&#27169;&#22411;&#21644;&#30456;&#20851;&#25216;&#26415;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#23558;&#37325;&#28857;&#35780;&#20272;3D&#32954;&#37096;CT&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#32467;&#26524;&#34920;&#26126;&#65292;LIT&#27169;&#22411;&#33021;&#22815;&#24456;&#22909;&#22320;&#25191;&#34892;&#22810;&#39033;&#21307;&#23398;&#20219;&#21153;&#65292;&#21253;&#25324;&#32954;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLM) not only empower multiple language tasks but also serve as a general interface across different spaces. Up to now, it has not been demonstrated yet how to effectively translate the successes of LLMs in the computer vision field to the medical imaging field which involves high-dimensional and multi-modal medical images. In this paper, we report a feasibility study of building a multi-task CT large image-text (LIT) model for lung cancer diagnosis by combining an LLM and a large image model (LIM). Specifically, the LLM and LIM are used as encoders to perceive multi-modal information under task-specific text prompts, which synergizes multi-source information and task-specific and patient-specific priors for optimized diagnostic performance. The key components of our LIT model and associated techniques are evaluated with an emphasis on 3D lung CT analysis. Our initial results show that the LIT model performs multiple medical tasks well, including lung segmentatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#24207;&#21644;&#38750;&#26102;&#24207;&#25968;&#25454;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#35782;&#21035;&#22240;&#26524;&#36793;&#32536;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#21487;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24120;&#35265;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2303.15027</link><description>&lt;p&gt;
&#26102;&#24207;&#21644;&#38750;&#26102;&#24207;&#25968;&#25454;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Causal Discovery Methods for Temporal and Non-Temporal Data. (arXiv:2303.15027v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#24207;&#21644;&#38750;&#26102;&#24207;&#25968;&#25454;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#35782;&#21035;&#22240;&#26524;&#36793;&#32536;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#21487;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24120;&#35265;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#65288;CD&#65289;&#26159;&#20174;&#25968;&#25454;&#20013;&#35782;&#21035;&#31995;&#32479;&#21464;&#37327;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#36807;&#31243;&#12290;&#22810;&#24180;&#26469;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20027;&#35201;&#22522;&#20110;&#25968;&#25454;&#30340;&#32479;&#35745;&#29305;&#24615;&#26469;&#25581;&#31034;&#28508;&#22312;&#30340;&#22240;&#26524;&#26426;&#21046;&#12290;&#26412;&#25991;&#23545;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#25968;&#25454;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#35752;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#24120;&#29992;&#26415;&#35821;&#65292;&#28982;&#21518;&#20840;&#38754;&#35752;&#35770;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#35782;&#21035;&#22240;&#26524;&#36793;&#32536;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#21487;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#25191;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#24037;&#20855;&#25110;&#36719;&#20214;&#21253;&#20197;&#21450;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#24120;&#35265;&#25351;&#26631;&#12290;&#25105;&#20204;&#36824;&#22312;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#19968;&#20123;&#24120;&#35265;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery (CD) is the process of identifying the cause-effect relationships among the variables of a system from data. Over the years, several methods have been developed primarily based on the statistical properties of data to uncover the underlying causal mechanism. In this study, we present an extensive discussion on the methods designed to perform causal discovery from both independent and identically distributed (i.i.d.) data and time series data. For this purpose, we first introduce the common terminologies in causal discovery, and then provide a comprehensive discussion of the algorithms designed to identify the causal edges in different settings. We further discuss some of the benchmark datasets available for evaluating the performance of the causal discovery methods, available tools or software packages to perform causal discovery readily, and the common metrics used to evaluate these methods. We also test some common causal discovery algorithms on different benchmark d
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23481;&#26131;&#23398;&#20064;&#30340;&#26679;&#26412;&#23545;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.09195</link><description>&lt;p&gt;
&#25968;&#25454;&#39640;&#25928;&#30340;&#23545;&#27604;&#33258;&#30417;&#30563;&#23398;&#20064;&#65306;&#26131;&#20110;&#23398;&#20064;&#30340;&#26679;&#26412;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-Efficient Contrastive Self-supervised Learning: Easy Examples Contribute the Most. (arXiv:2302.09195v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09195
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35777;&#26126;&#20102;&#22312;&#33258;&#30417;&#30563;&#23398;&#20064;&#20013;&#23481;&#26131;&#23398;&#20064;&#30340;&#26679;&#26412;&#23545;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#36215;&#21040;&#26368;&#22823;&#30340;&#20316;&#29992;&#65292;&#36825;&#26377;&#21161;&#20110;&#20943;&#23569;&#25152;&#38656;&#30340;&#35757;&#32451;&#25968;&#25454;&#37327;&#65292;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#20174;&#22823;&#37327;&#30340;&#26080;&#26631;&#31614;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#39640;&#36136;&#37327;&#30340;&#34920;&#31034;&#12290;&#38543;&#30528;&#25968;&#25454;&#38598;&#21464;&#24471;&#36234;&#26469;&#36234;&#22823;&#65292;&#35782;&#21035;&#23545;&#23398;&#20064;&#27492;&#31867;&#34920;&#31034;&#26368;&#26377;&#29992;&#30340;&#31034;&#20363;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#21487;&#20197;&#36890;&#36807;&#20943;&#23569;&#23398;&#20064;&#39640;&#36136;&#37327;&#34920;&#31034;&#25152;&#38656;&#30340;&#25968;&#25454;&#37327;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;SSL&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;SSL&#30340;&#20215;&#20540;&#22914;&#20309;&#37327;&#21270;&#19968;&#30452;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35777;&#26126;&#22312;&#26399;&#26395;&#24847;&#20041;&#19979;&#65292;&#23545;&#27604;SSL&#20013;&#23545;&#23398;&#20064;&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#31034;&#20363;&#26159;&#20855;&#26377;&#26368;&#30456;&#20284;&#25968;&#25454;&#22686;&#24378;&#30340;&#31034;&#20363;&#12290;&#25105;&#20204;&#23545;&#36825;&#20123;&#23376;&#38598;&#30340;SSL&#30340;&#24191;&#20041;&#24615;&#33021;&#25552;&#20379;&#20102;&#20005;&#26684;&#30340;&#20445;&#35777;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23545;SSL&#20570;&#20986;&#26368;&#22823;&#36129;&#29486;&#30340;&#23376;&#38598;&#26159;&#23545;&#30417;&#30563;&#23398;&#20064;&#20570;&#20986;&#26368;&#23567;&#36129;&#29486;&#30340;&#23376;&#38598;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#23376;&#38598;&#22312;CIFAR100&#12289;CIFAR&#20013;&#30340;&#34920;&#29616;&#20248;&#20110;&#38543;&#26426;&#23376;&#38598;3%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning (SSL) learns high-quality representations from large pools of unlabeled training data. As datasets grow larger, it becomes crucial to identify the examples that contribute the most to learning such representations. This enables efficient SSL by reducing the volume of data required for learning high-quality representations. Nevertheless, quantifying the value of examples for SSL has remained an open question. In this work, we address this for the first time, by proving that examples that contribute the most to contrastive SSL are those that have the most similar augmentations to other examples, in expectation. We provide rigorous guarantees for the generalization performance of SSL on such subsets. Empirically, we discover, perhaps surprisingly, the subsets that contribute the most to SSL are those that contribute the least to supervised learning. Through extensive experiments, we show that our subsets outperform random subsets by more than 3% on CIFAR100, CIFAR
&lt;/p&gt;</description></item><item><title>&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#24320;&#21457;&#20986;&#20849;&#20139;&#21327;&#35758;&#65292;&#20197;&#25351;&#20195;&#19968;&#32452;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#65292;&#24182;&#21487;&#29992;&#20110;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#26410;&#30693;&#23545;&#35937;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.08913</link><description>&lt;p&gt;
&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#25351;&#20195;&#24615;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Referential communication in heterogeneous communities of pre-trained visual deep networks. (arXiv:2302.08913v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08913
&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#35270;&#35273;&#28145;&#24230;&#32593;&#32476;&#31038;&#21306;&#20013;&#30340;&#39044;&#35757;&#32451;&#32593;&#32476;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#24320;&#21457;&#20986;&#20849;&#20139;&#21327;&#35758;&#65292;&#20197;&#25351;&#20195;&#19968;&#32452;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#65292;&#24182;&#21487;&#29992;&#20110;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#26410;&#30693;&#23545;&#35937;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#39044;&#35757;&#32451;&#22270;&#20687;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#34987;&#23884;&#20837;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25110;&#26426;&#22120;&#20154;&#31561;&#33258;&#20027;&#20195;&#29702;&#20013;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#22312;&#23427;&#20204;&#20855;&#26377;&#19981;&#21516;&#26550;&#26500;&#21644;&#35757;&#32451;&#26041;&#24335;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#30456;&#20114;&#20043;&#38388;&#36827;&#34892;&#27807;&#36890;&#20197;&#20102;&#35299;&#21608;&#22260;&#30340;&#19990;&#30028;&#12290;&#20316;&#20026;&#26397;&#30528;&#36825;&#20010;&#26041;&#21521;&#30340;&#31532;&#19968;&#27493;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#25506;&#32034;&#20102;&#22312;&#19968;&#32452;&#24322;&#26500;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#32593;&#32476;&#31038;&#21306;&#20013;&#36827;&#34892;"&#25351;&#20195;&#24615;&#27807;&#36890;"&#30340;&#20219;&#21153;&#65292;&#32467;&#26524;&#34920;&#26126;&#23427;&#20204;&#21487;&#20197;&#33258;&#25105;&#30417;&#30563;&#22320;&#21457;&#23637;&#19968;&#31181;&#20849;&#20139;&#21327;&#35758;&#26469;&#25351;&#20195;&#19968;&#32452;&#20505;&#36873;&#30446;&#26631;&#20013;&#30340;&#30446;&#26631;&#23545;&#35937;&#12290;&#22312;&#26576;&#31181;&#31243;&#24230;&#19978;&#65292;&#36825;&#31181;&#20849;&#20139;&#21327;&#35758;&#20063;&#21487;&#20197;&#29992;&#26469;&#27807;&#36890;&#19981;&#21516;&#31890;&#24230;&#30340;&#20808;&#21069;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#19968;&#20010;&#26368;&#21021;&#19981;&#23646;&#20110;&#29616;&#26377;&#31038;&#21306;&#30340;&#35270;&#35273;&#32593;&#32476;&#21487;&#20197;&#36731;&#26494;&#22320;&#23398;&#20064;&#21040;&#31038;&#21306;&#30340;&#21327;&#35758;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23450;&#24615;&#21644;&#23450;&#37327;&#22320;&#30740;&#31350;&#20102;&#36825;&#31181;&#26032;&#20135;&#29983;&#30340;&#21327;&#35758;&#30340;&#23646;&#24615;&#65292;&#25552;&#20379;&#20102;&#19968;&#20123;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large pre-trained image-processing neural networks are being embedded in autonomous agents such as self-driving cars or robots, the question arises of how such systems can communicate with each other about the surrounding world, despite their different architectures and training regimes. As a first step in this direction, we systematically explore the task of \textit{referential communication} in a community of heterogeneous state-of-the-art pre-trained visual networks, showing that they can develop, in a self-supervised way, a shared protocol to refer to a target object among a set of candidates. This shared protocol can also be used, to some extent, to communicate about previously unseen object categories of different granularity. Moreover, a visual network that was not initially part of an existing community can learn the community's protocol with remarkable ease. Finally, we study, both qualitatively and quantitatively, the properties of the emergent protocol, providing some evi
&lt;/p&gt;</description></item></channel></rss>