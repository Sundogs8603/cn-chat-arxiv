<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>MathVerse&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14624</link><description>&lt;p&gt;
MathVerse&#65306;&#24744;&#30340;&#22810;&#27169;&#24335;LLM&#26159;&#21542;&#30495;&#27491;&#30475;&#21040;&#20102;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#20013;&#30340;&#22270;&#34920;&#65311;
&lt;/p&gt;
&lt;p&gt;
MathVerse: Does Your Multi-modal LLM Truly See the Diagrams in Visual Math Problems?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14624
&lt;/p&gt;
&lt;p&gt;
MathVerse&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#20013;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#35270;&#35273;&#29615;&#22659;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#28982;&#32780;&#23427;&#20204;&#22312;&#35270;&#35273;&#25968;&#23398;&#38382;&#39064;&#35299;&#20915;&#26041;&#38754;&#30340;&#33021;&#21147;&#20173;&#26410;&#20805;&#20998;&#35780;&#20272;&#21644;&#29702;&#35299;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#24403;&#21069;&#22522;&#20934;&#27979;&#35797;&#65292;&#23558;&#36807;&#22810;&#30340;&#35270;&#35273;&#20869;&#23481;&#34701;&#20837;&#25991;&#26412;&#38382;&#39064;&#20013;&#65292;&#36825;&#26377;&#21161;&#20110;MLLM&#22312;&#19981;&#30495;&#27491;&#35299;&#37322;&#36755;&#20837;&#22270;&#34920;&#30340;&#24773;&#20917;&#19979;&#25512;&#23548;&#31572;&#26696;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MathVerse&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#26041;&#20301;&#30340;&#35270;&#35273;&#25968;&#23398;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20844;&#24179;&#32780;&#28145;&#20837;&#22320;&#35780;&#20272;MLLMs&#12290;&#25105;&#20204;&#31934;&#24515;&#25910;&#38598;&#20102;2,612&#20010;&#39640;&#36136;&#37327;&#30340;&#22810;&#23398;&#31185;&#25968;&#23398;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#22270;&#34920;&#65292;&#26469;&#28304;&#20110;&#20844;&#24320;&#28192;&#36947;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#38382;&#39064;&#30001;&#20154;&#24037;&#27880;&#37322;&#32773;&#36716;&#21270;&#20026;&#20845;&#20010;&#19981;&#21516;&#29256;&#26412;&#65292;&#27599;&#20010;&#29256;&#26412;&#22312;&#22810;&#27169;&#24335;&#20013;&#25552;&#20379;&#19981;&#21516;&#31243;&#24230;&#30340;&#20449;&#24687;&#20869;&#23481;&#65292;&#20849;&#36129;&#29486;&#20102;15K&#20010;&#27979;&#35797;&#26679;&#26412;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#24471;MathVerse&#33021;&#22815;&#21516;&#26102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14624v1 Announce Type: cross  Abstract: The remarkable progress of Multi-modal Large Language Models (MLLMs) has garnered unparalleled attention, due to their superior performance in visual contexts. However, their capabilities in visual math problem-solving remain insufficiently evaluated and understood. We investigate current benchmarks to incorporate excessive visual content within textual questions, which potentially assist MLLMs in deducing answers without truly interpreting the input diagrams. To this end, we introduce MathVerse, an all-around visual math benchmark designed for an equitable and in-depth evaluation of MLLMs. We meticulously collect 2,612 high-quality, multi-subject math problems with diagrams from publicly available sources. Each problem is then transformed by human annotators into six distinct versions, each offering varying degrees of information content in multi-modality, contributing to 15K test samples in total. This approach allows MathVerse to co
&lt;/p&gt;</description></item><item><title>Videoshop&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#32534;&#36753;&#31639;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26412;&#22320;&#21270;&#35821;&#20041;&#32534;&#36753;&#65292;&#20174;&#32780;&#20801;&#35768;&#29992;&#25143;&#23545;&#35270;&#39057;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14617</link><description>&lt;p&gt;
Videoshop&#65306;&#20855;&#26377;&#22122;&#22768;&#22806;&#25512;&#25193;&#25955;&#21453;&#28436;&#30340;&#26412;&#22320;&#21270;&#35821;&#20041;&#35270;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Videoshop: Localized Semantic Video Editing with Noise-Extrapolated Diffusion Inversion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14617
&lt;/p&gt;
&lt;p&gt;
Videoshop&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#35270;&#39057;&#32534;&#36753;&#31639;&#27861;&#65292;&#36890;&#36807;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26412;&#22320;&#21270;&#35821;&#20041;&#32534;&#36753;&#65292;&#20174;&#32780;&#20801;&#35768;&#29992;&#25143;&#23545;&#35270;&#39057;&#36827;&#34892;&#31934;&#32454;&#25511;&#21046;&#65292;&#21462;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;Videoshop&#65292;&#36825;&#26159;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#29992;&#20110;&#26412;&#22320;&#21270;&#35821;&#20041;&#32534;&#36753;&#30340;&#35270;&#39057;&#32534;&#36753;&#31639;&#27861;&#12290;Videoshop&#20801;&#35768;&#29992;&#25143;&#20351;&#29992;&#20219;&#20309;&#32534;&#36753;&#36719;&#20214;&#65292;&#21253;&#25324;Photoshop&#21644;&#29983;&#25104;&#22635;&#20805;&#65292;&#20462;&#25913;&#31532;&#19968;&#24103;&#65307;&#23427;&#20250;&#33258;&#21160;&#23558;&#36825;&#20123;&#26356;&#25913;&#20256;&#25773;&#21040;&#20854;&#20313;&#24103;&#65292;&#20445;&#25345;&#35821;&#20041;&#12289;&#31354;&#38388;&#21644;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#36816;&#21160;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#21482;&#33021;&#36890;&#36807;&#19981;&#31934;&#30830;&#30340;&#25991;&#26412;&#25351;&#20196;&#36827;&#34892;&#32534;&#36753;&#19981;&#21516;&#65292;Videoshop&#20801;&#35768;&#29992;&#25143;&#28155;&#21152;&#25110;&#21024;&#38500;&#23545;&#35937;&#65292;&#35821;&#20041;&#19978;&#26356;&#25913;&#23545;&#35937;&#65292;&#23558;&#32032;&#26448;&#29031;&#29255;&#25554;&#20837;&#35270;&#39057;&#31561;&#65292;&#24182;&#23545;&#20301;&#32622;&#21644;&#22806;&#35266;&#36827;&#34892;&#32454;&#31890;&#24230;&#25511;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#28508;&#22312;&#20540;&#36827;&#34892;&#22122;&#22768;&#22806;&#25512;&#21453;&#28436;&#30340;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#35270;&#39057;&#32534;&#36753;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#20174;&#20013;&#25105;&#20204;&#29983;&#25104;&#26681;&#25454;&#32534;&#36753;&#22270;&#20687;&#35843;&#25972;&#30340;&#35270;&#39057;&#12290;Videoshop&#22312;2&#20010;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#20013;&#20351;&#29992;10&#20010;&#35780;&#20272;&#25351;&#26631;&#23545;6&#20010;&#22522;&#32447;&#21462;&#24471;&#20102;&#26356;&#39640;&#36136;&#37327;&#30340;&#32534;&#36753;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14617v1 Announce Type: cross  Abstract: We introduce Videoshop, a training-free video editing algorithm for localized semantic edits. Videoshop allows users to use any editing software, including Photoshop and generative inpainting, to modify the first frame; it automatically propagates those changes, with semantic, spatial, and temporally consistent motion, to the remaining frames. Unlike existing methods that enable edits only through imprecise textual instructions, Videoshop allows users to add or remove objects, semantically change objects, insert stock photos into videos, etc. with fine-grained control over locations and appearance. We achieve this through image-based video editing by inverting latents with noise extrapolation, from which we generate videos conditioned on the edited image. Videoshop produces higher quality edits against 6 baselines on 2 editing benchmarks using 10 evaluation metrics.
&lt;/p&gt;</description></item><item><title>AI&#32534;&#30721;&#21161;&#25163;&#24212;&#26126;&#30830;&#20351;&#29992;&#39044;&#26399;&#65292;&#20805;&#20998;&#25972;&#21512;IDE&#21151;&#33021;&#65292;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#35774;&#35745;&#65292;&#36127;&#36131;&#25910;&#38598;&#25968;&#25454;&#65292;&#25552;&#20986;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.14592</link><description>&lt;p&gt;
&#38754;&#21521;&#19979;&#19968;&#20195;AI&#32534;&#30721;&#21161;&#25163;&#30340;&#35774;&#24819;&#19982;&#25552;&#26696;
&lt;/p&gt;
&lt;p&gt;
Envisioning the Next-Generation AI Coding Assistants: Insights &amp; Proposals
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14592
&lt;/p&gt;
&lt;p&gt;
AI&#32534;&#30721;&#21161;&#25163;&#24212;&#26126;&#30830;&#20351;&#29992;&#39044;&#26399;&#65292;&#20805;&#20998;&#25972;&#21512;IDE&#21151;&#33021;&#65292;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#35774;&#35745;&#65292;&#36127;&#36131;&#25910;&#38598;&#25968;&#25454;&#65292;&#25552;&#20986;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#20010;&#20154;&#24037;&#26234;&#33021;&#36719;&#20214;&#24037;&#31243;&#65288;AI4SE&#65289;&#30340;&#30740;&#31350;&#20135;&#21697;&#28151;&#21512;&#32452;&#65292;&#25105;&#20204;&#20174;&#24320;&#21457;&#20013;&#30340;&#32463;&#39564;&#20013;&#25552;&#20986;&#22235;&#20010;&#20851;&#38190;&#30340;&#35265;&#35299;&#12290;AI&#32534;&#30721;&#21161;&#25163;&#24212;&#35813;&#26126;&#30830;&#35268;&#23450;&#20351;&#29992;&#39044;&#26399;&#65292;&#19982;&#20808;&#36827;&#30340;IDE&#21151;&#33021;&#21644;&#29616;&#26377;&#25193;&#23637;&#38598;&#25104;&#65292;&#20351;&#29992;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#35774;&#35745;&#65292;&#24182;&#36127;&#36131;&#25910;&#38598;&#24212;&#29992;&#31243;&#24207;&#25968;&#25454;&#20197;&#20415;&#36827;&#34892;&#19979;&#28216;&#20998;&#26512;&#12290;&#25105;&#20204;&#25552;&#20986;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24212;&#35813;&#35299;&#20915;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#25361;&#25112;&#65292;&#20197;&#23454;&#29616;&#19979;&#19968;&#20195;AI&#32534;&#30721;&#21161;&#25163;&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14592v1 Announce Type: cross  Abstract: As a research-product hybrid group in AI for Software Engineering (AI4SE), we present four key takeaways from our experience developing in-IDE AI coding assistants. AI coding assistants should set clear expectations for usage, integrate with advanced IDE capabilities and existing extensions, use extendable backend designs, and collect app data responsibly for downstream analyses. We propose open questions and challenges that academia and industry should address to realize the vision of next-generation AI coding assistants.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14589</link><description>&lt;p&gt;
ReAct&#36935;&#19978;ActRe&#65306;&#23545;&#27604;&#24615;&#33258;&#35757;&#32451;&#20013;&#30340;&#20195;&#29702;&#36712;&#36857;&#33258;&#21160;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25991;&#25688;&#65306;&#35821;&#35328;&#20195;&#29702;&#36890;&#36807;&#19982;&#22522;&#30784;&#27169;&#22411;&#25512;&#29702;&#23637;&#31034;&#20102;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#22810;&#27493;&#25512;&#29702;&#21644;&#34892;&#21160;&#36712;&#36857;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#35821;&#35328;&#20195;&#29702;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#36825;&#26679;&#30340;&#36712;&#36857;&#20173;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#20154;&#21147;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#36824;&#26159;&#23454;&#26045;&#22810;&#26679;&#21270;&#25552;&#31034;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;A$^3$T&#65292;&#19968;&#20010;&#20801;&#35768;&#20197;ReAct&#39118;&#26684;&#33258;&#20027;&#27880;&#37322;&#20195;&#29702;&#36712;&#36857;&#30340;&#26694;&#26550;&#12290;&#20854;&#20013;&#24515;&#26159;&#19968;&#20010;ActRe&#25552;&#31034;&#20195;&#29702;&#65292;&#23427;&#35299;&#37322;&#20219;&#24847;&#21160;&#20316;&#30340;&#21407;&#22240;&#12290;&#24403;&#38543;&#26426;&#25277;&#21462;&#22806;&#37096;&#21160;&#20316;&#26102;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#20197;&#26597;&#35810;ActRe&#20195;&#29702;&#20197;&#33719;&#21462;&#20854;&#25991;&#26412;&#29702;&#30001;&#12290;&#26032;&#39062;&#30340;&#36712;&#36857;&#28982;&#21518;&#36890;&#36807;&#23558;ActRe&#30340;&#21518;&#39564;&#25512;&#29702;&#21069;&#32622;&#21040;&#25277;&#26679;&#21160;&#20316;&#20013;&#36827;&#34892;&#32508;&#21512;&#21512;&#25104;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 Announce Type: new  Abstract: Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent exe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36873;&#39064;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#25928;&#26524;&#65292;&#24182;&#21033;&#29992;MQ&#24207;&#21015;BERT&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#31185;&#30446;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.14582</link><description>&lt;p&gt;
&#29992;&#20110;&#21307;&#23398;&#31185;&#30446;&#22810;&#36873;&#39064;&#20998;&#31867;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Multi-Choice Question Classification of Medical Subjects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#36873;&#39064;&#25968;&#25454;&#19978;&#30340;&#35757;&#32451;&#25928;&#26524;&#65292;&#24182;&#21033;&#29992;MQ&#24207;&#21015;BERT&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#31185;&#30446;&#20998;&#31867;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#32467;&#26524;&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#35780;&#20272;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;&#22312;&#22810;&#36873;&#39064;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21306;&#20998;&#21307;&#23398;&#31185;&#30446;&#12290;&#36825;&#23545;&#20110;&#33258;&#21160;&#38382;&#31572;&#26159;&#19968;&#39033;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#23558;&#38382;&#39064;&#22810;&#31867;&#21035;&#20998;&#31867;&#20026;&#25512;&#26029;&#30340;&#21307;&#23398;&#31185;&#30446;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#22810;&#38382;&#39064;(MQ)&#24207;&#21015;BERT&#26041;&#27861;&#65292;&#22312;MedMCQA&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#32467;&#26524;&#65292;&#20998;&#21035;&#22312;&#20854;&#24320;&#21457;&#21644;&#27979;&#35797;&#38598;&#19978;&#20855;&#26377;0.68&#21644;0.60&#30340;&#20934;&#30830;&#29575;&#12290;&#20174;&#36825;&#20010;&#24847;&#20041;&#19978;&#35828;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#29305;&#21035;&#26159;LLMs&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#29992;&#20110;&#22810;&#20998;&#31867;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14582v1 Announce Type: cross  Abstract: The aim of this paper is to evaluate whether large language models trained on multi-choice question data can be used to discriminate between medical subjects. This is an important and challenging task for automatic question answering. To achieve this goal, we train deep neural networks for multi-class classification of questions into the inferred medical subjects. Using our Multi-Question (MQ) Sequence-BERT method, we outperform the state-of-the-art results on the MedMCQA dataset with an accuracy of 0.68 and 0.60 on their development and test sets, respectively. In this sense, we show the capability of AI and LLMs in particular for multi-classification tasks in the Healthcare domain.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#27010;&#24565;&#26041;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35299;&#37322;&#24615;&#25913;&#36827;&#65292;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#35299;&#37322;&#65292;&#20351;&#24471;&#26816;&#27979;&#20266;&#20851;&#32852;&#21644;&#22266;&#26377;&#20559;&#35265;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14566</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#22522;&#20110;&#27010;&#24565;&#26041;&#27861;&#25913;&#36827;&#27169;&#22411;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A survey on Concept-based Approaches For Model Improvement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14566
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#26041;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35299;&#37322;&#24615;&#25913;&#36827;&#65292;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#35299;&#37322;&#65292;&#20351;&#24471;&#26816;&#27979;&#20266;&#20851;&#32852;&#21644;&#22266;&#26377;&#20559;&#35265;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#30340;&#37325;&#28857;&#24050;&#32463;&#20174;&#20165;&#20165;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36716;&#21464;&#20026;&#20351;DNN&#26356;&#26131;&#35299;&#37322;&#32473;&#20154;&#31867;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#24050;&#32463;&#35266;&#23519;&#21040;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#26174;&#33879;&#24615;&#21644;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#29992;&#25152;&#35859;&#30340;&#27010;&#24565;&#22312;&#31616;&#21333;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#26415;&#35821;&#20013;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#27010;&#24565;&#26159;&#25968;&#25454;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#21333;&#20803;&#65292;&#26159;&#20154;&#31867;&#24605;&#32500;&#30340;&#22522;&#30707;&#12290;&#29992;&#27010;&#24565;&#30340;&#35299;&#37322;&#33021;&#22815;&#26816;&#27979;&#21040;&#20266;&#20851;&#32852;&#12289;&#22266;&#26377;&#20559;&#35265;&#25110;&#32874;&#26126;&#27721;&#12290;&#38543;&#30528;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#30340;&#20986;&#29616;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#27010;&#24565;&#34920;&#31034;&#26041;&#27861;&#21644;&#33258;&#21160;&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#27010;&#24565;&#36827;&#34892;&#20107;&#21518;&#27169;&#22411;&#35299;&#32544;&#35780;&#20272;&#65292;&#32780;&#20854;&#20182;&#20154;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#20107;&#21069;&#35757;&#32451;&#12290;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#26159;&#26032;&#30340;&#65292;&#26377;&#35768;&#22810;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14566v1 Announce Type: new  Abstract: The focus of recent research has shifted from merely increasing the Deep Neural Networks (DNNs) performance in various tasks to DNNs, which are more interpretable to humans. The field of eXplainable Artificial Intelligence (XAI) has observed various techniques, including saliency-based and concept-based approaches. Concept-based approaches explain the model's decisions in simple human understandable terms called Concepts. Concepts are human interpretable units of data and are the thinking ground of humans. Explanations in terms of concepts enable detecting spurious correlations, inherent biases, or clever-hans. With the advent of concept-based explanations, there have been various concept representation methods and automatic concept discovery algorithms. Some recent methods use concepts for post-hoc model disentanglement evaluation, while others use them for ante-hoc training. The concept-based approaches are new, with many representatio
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#35299;&#30721;&#30340;&#26032;&#35266;&#28857;&#65292;&#23558;LLM&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#21508;&#31181;&#24037;&#20855;&#20043;&#38388;&#30340;&#21327;&#20316;&#36807;&#31243;&#26500;&#24314;&#20026;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#39640;&#25928;&#36755;&#20986;&#30340;&#26500;&#24314;&#12290;</title><link>https://arxiv.org/abs/2403.14562</link><description>&lt;p&gt;
&#35821;&#20041;&#35299;&#30721;&#26102;&#20195;
&lt;/p&gt;
&lt;p&gt;
The Era of Semantic Decoding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14562
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#35821;&#20041;&#35299;&#30721;&#30340;&#26032;&#35266;&#28857;&#65292;&#23558;LLM&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#21508;&#31181;&#24037;&#20855;&#20043;&#38388;&#30340;&#21327;&#20316;&#36807;&#31243;&#26500;&#24314;&#20026;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36807;&#31243;&#65292;&#20419;&#36827;&#20102;&#39640;&#25928;&#36755;&#20986;&#30340;&#26500;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#23637;&#29616;&#20102;&#22312;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#12289;&#20154;&#31867;&#36755;&#20837;&#21644;&#21508;&#31181;&#24037;&#20855;&#20043;&#38388;&#32534;&#25490;&#21327;&#20316;&#20197;&#35299;&#20915;LLM&#22266;&#26377;&#23616;&#38480;&#24615;&#30340;&#24819;&#27861;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#35821;&#20041;&#35299;&#30721;&#30340;&#26032;&#35266;&#28857;&#65292;&#23558;&#36825;&#20123;&#21327;&#20316;&#36807;&#31243;&#26500;&#24314;&#20026;&#35821;&#20041;&#31354;&#38388;&#20013;&#30340;&#20248;&#21270;&#36807;&#31243;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;LLM&#27010;&#24565;&#21270;&#20026;&#25805;&#32437;&#25105;&#20204;&#31216;&#20043;&#20026;&#35821;&#20041;&#26631;&#35760;&#65288;&#24050;&#30693;&#24605;&#24819;&#65289;&#30340;&#26377;&#24847;&#20041;&#20449;&#24687;&#29255;&#27573;&#30340;&#35821;&#20041;&#22788;&#29702;&#22120;&#12290;LLM&#26159;&#20247;&#22810;&#20854;&#20182;&#35821;&#20041;&#22788;&#29702;&#22120;&#20043;&#19968;&#65292;&#21253;&#25324;&#20154;&#31867;&#21644;&#24037;&#20855;&#65292;&#27604;&#22914;&#25628;&#32034;&#24341;&#25806;&#25110;&#20195;&#30721;&#25191;&#34892;&#22120;&#12290;&#35821;&#20041;&#22788;&#29702;&#22120;&#38598;&#20307;&#21442;&#19982;&#35821;&#20041;&#26631;&#35760;&#30340;&#21160;&#24577;&#20132;&#27969;&#65292;&#36880;&#27493;&#26500;&#24314;&#39640;&#25928;&#36755;&#20986;&#12290;&#25105;&#20204;&#31216;&#36825;&#20123;&#22312;&#35821;&#20041;&#31354;&#38388;&#20013;&#36827;&#34892;&#20248;&#21270;&#21644;&#25628;&#32034;&#30340;&#21327;&#21516;&#20316;&#29992;&#65292;&#20026;&#35821;&#20041;&#35299;&#30721;&#31639;&#27861;&#12290;&#36825;&#20010;&#27010;&#24565;&#19982;&#24050;&#24191;&#20026;&#30740;&#31350;&#30340;&#35821;&#20041;&#35299;&#30721;&#38382;&#39064;&#30452;&#25509;&#24179;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14562v1 Announce Type: cross  Abstract: Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of s
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;LexiContrastive Grounding (LCG)&#65292;&#23427;&#32467;&#21512;&#20102;&#35270;&#35273;&#30417;&#30563;&#21644;&#25991;&#26412;&#34920;&#31034;&#25913;&#36827;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#21333;&#35789;&#23398;&#20064;&#21644;&#21477;&#23376;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#26356;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#36827;&#27493;&#12290;</title><link>https://arxiv.org/abs/2403.14551</link><description>&lt;p&gt;
&#35789;&#27719;&#32423;&#23545;&#27604;&#35270;&#35273;&#22522;&#30784;&#25913;&#36827;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Lexicon-Level Contrastive Visual-Grounding Improves Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14551
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;LexiContrastive Grounding (LCG)&#65292;&#23427;&#32467;&#21512;&#20102;&#35270;&#35273;&#30417;&#30563;&#21644;&#25991;&#26412;&#34920;&#31034;&#25913;&#36827;&#31574;&#30053;&#65292;&#22312;&#22810;&#20010;&#21333;&#35789;&#23398;&#20064;&#21644;&#21477;&#23376;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#27604;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#26356;&#39640;&#30340;&#23398;&#20064;&#25928;&#29575;&#21644;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#22825;&#26368;&#20934;&#30830;&#30340;&#35821;&#35328;&#27169;&#22411;&#26159;&#22312;&#27604;&#20154;&#31867;&#35821;&#35328;&#23398;&#20064;&#32773;&#25509;&#25910;&#21040;&#30340;&#35821;&#35328;&#25968;&#25454;&#37327;&#22810;&#24471;&#22810;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#30340;&#65292;&#20294;&#24182;&#27809;&#26377;&#26469;&#33258;&#22312;&#20154;&#31867;&#23398;&#20064;&#20013;&#36215;&#20851;&#38190;&#20316;&#29992;&#30340;&#20854;&#20182;&#24863;&#23448;&#27169;&#24335;&#30340;&#30417;&#30563;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;LexiContrastive Grounding (LCG)&#65292;&#19968;&#31181;&#21033;&#29992;&#35270;&#35273;&#30417;&#30563;&#26469;&#25913;&#36827;&#25991;&#26412;&#34920;&#24449;&#30340;&#22522;&#20110;&#22320;&#38754;&#35821;&#35328;&#23398;&#20064;&#31243;&#24207;&#12290;LexiContrastive Grounding&#23558;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#31574;&#30053;&#19982;&#23545;&#27604;&#35270;&#35273;&#22522;&#30784;&#30446;&#26631;&#32467;&#21512;&#36215;&#26469;&#65292;&#37325;&#28857;&#25918;&#22312;&#32534;&#30721;&#35789;&#27719;&#20449;&#24687;&#30340;&#26089;&#26399;&#23618;&#34920;&#31034;&#19978;&#12290;&#22312;&#22810;&#20010;&#21333;&#35789;&#23398;&#20064;&#21644;&#21477;&#23376;&#29702;&#35299;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;LexiContrastive Grounding&#19981;&#20165;&#22312;&#23398;&#20064;&#25928;&#29575;&#19978;&#20248;&#20110;&#26631;&#20934;&#35821;&#35328;&#27169;&#22411;&#65292;&#32780;&#19988;&#22312;&#35270;&#35273;&#19982;&#35821;&#35328;&#23398;&#20064;&#31243;&#24207;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14551v1 Announce Type: cross  Abstract: Today's most accurate language models are trained on orders of magnitude more language data than human language learners receive - but with no supervision from other sensory modalities that play a crucial role in human learning. Can we make LMs' representations and predictions more accurate (and more human-like) with more ecologically plausible supervision? This paper describes LexiContrastive Grounding (LCG), a grounded language learning procedure that leverages visual supervision to improve textual representations. LexiContrastive Grounding combines a next token prediction strategy with a contrastive visual grounding objective, focusing on early-layer representations that encode lexical information. Across multiple word-learning and sentence-understanding benchmarks, LexiContrastive Grounding not only outperforms standard language-only models in learning efficiency, but also improves upon vision-and-language learning procedures inclu
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DynEmph&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#20449;&#26426;&#22120;&#20154;&#20915;&#23450;&#22312;&#21738;&#20123;&#22320;&#26041;&#29992;&#29289;&#29702;&#34920;&#36798;&#24378;&#35843; XAI &#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#36890;&#36807;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#26469;&#20915;&#23450;&#24378;&#35843;&#30340;&#20301;&#32622;&#12290;</title><link>https://arxiv.org/abs/2403.14550</link><description>&lt;p&gt;
&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#21160;&#24577;&#35299;&#37322;&#24378;&#35843;&#19982;&#20132;&#27969;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Dynamic Explanation Emphasis in Human-XAI Interaction with Communication Robot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14550
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DynEmph&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#20449;&#26426;&#22120;&#20154;&#20915;&#23450;&#22312;&#21738;&#20123;&#22320;&#26041;&#29992;&#29289;&#29702;&#34920;&#36798;&#24378;&#35843; XAI &#29983;&#25104;&#30340;&#35299;&#37322;&#65292;&#24182;&#19988;&#36890;&#36807;&#19968;&#31181;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#26469;&#20915;&#23450;&#24378;&#35843;&#30340;&#20301;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#26426;&#22120;&#20154;&#26377;&#26395;&#25104;&#20026;&#26377;&#25928;&#30340;&#20154;&#26426;&#20132;&#20114;&#30028;&#38754;&#65292;&#36229;&#36234;&#25991;&#26412;&#25110;&#22270;&#24418;&#35299;&#37322;&#30340;&#28508;&#21147;&#12290;&#23427;&#20204;&#30340;&#19968;&#22823;&#20248;&#21183;&#22312;&#20110;&#21487;&#20197;&#20351;&#29992;&#29289;&#29702;&#21644;&#35821;&#38899;&#34920;&#36798;&#26469;&#23545;&#35299;&#37322;&#28155;&#21152;&#35814;&#32454;&#30340;&#32454;&#24494;&#20043;&#22788;&#12290;&#28982;&#32780;&#65292;&#24182;&#19981;&#28165;&#26970;&#26426;&#22120;&#20154;&#22914;&#20309;&#24212;&#29992;&#36825;&#20123;&#34920;&#36798;&#65292;&#29305;&#21035;&#26159;&#22914;&#20309;&#33021;&#22815;&#24320;&#21457;&#19968;&#31181;&#31574;&#30053;&#65292;&#26681;&#25454;&#20219;&#21153;&#21644;&#29992;&#25143;&#22312;&#21160;&#24577;&#20132;&#20114;&#20013;&#33258;&#36866;&#24212;&#22320;&#20351;&#29992;&#36825;&#20123;&#34920;&#36798;&#12290;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DynEmph&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#20449;&#26426;&#22120;&#20154;&#20915;&#23450;&#22312;&#21738;&#20123;&#22320;&#26041;&#29992;&#29289;&#29702;&#34920;&#36798;&#24378;&#35843;XAI&#29983;&#25104;&#30340;&#35299;&#37322;&#12290;&#23427;&#39044;&#27979;&#20102;&#24378;&#35843;&#26576;&#20123;&#35266;&#28857;&#23545;&#29992;&#25143;&#30340;&#24433;&#21709;&#65292;&#24182;&#26088;&#22312;&#26368;&#23567;&#21270;&#39044;&#26399;&#29992;&#25143;&#20915;&#31574;&#19982;AI&#24314;&#35758;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;DynEmph&#37319;&#29992;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#31574;&#30053;&#26469;&#20915;&#23450;&#22312;&#21738;&#37324;&#36827;&#34892;&#24378;&#35843;&#65292;&#35299;&#25918;&#24037;&#31243;&#24072;&#19981;&#20877;&#38656;&#35201;&#25163;&#21160;&#36827;&#34892;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14550v1 Announce Type: cross  Abstract: Communication robots have the potential to contribute to effective human-XAI interaction as an interface that goes beyond textual or graphical explanations. One of their strengths is that they can use physical and vocal expressions to add detailed nuances to explanations. However, it is not clear how a robot can apply such expressions, or in particular, how we can develop a strategy to adaptively use such expressions depending on the task and user in dynamic interactions. To address this question, this paper proposes DynEmph, a method for a communication robot to decide where to emphasize XAI-generated explanations with physical expressions. It predicts the effect of emphasizing certain points on a user and aims to minimize the expected difference between predicted user decisions and AI-suggested ones. DynEmph features a strategy for deciding where to emphasize in a data-driven manner, relieving engineers from the need to manually desi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;ObjectDR&#65292;&#21033;&#29992;&#23545;&#35937;-centric&#30340;&#22495;&#38543;&#26426;&#21270;&#21512;&#25104;&#21333;&#35270;&#22270;3D&#24418;&#29366;&#37325;&#24314;&#20013;&#32570;&#20047;&#30340;&#37197;&#23545;&#25968;&#25454;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21644;&#35299;&#32806;&#26694;&#26550;&#26469;&#29983;&#25104;&#21644;&#20445;&#30041;&#23545;&#35937;&#36718;&#24275;&#20197;&#21450;&#24191;&#27867;&#21464;&#21270;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#22521;&#35757;&#27169;&#22411;&#25429;&#25417;&#22495;&#19981;&#21464;&#24615;&#20960;&#20309;&#24418;&#29366;&#12290;</title><link>https://arxiv.org/abs/2403.14539</link><description>&lt;p&gt;
Object-Centric Domain Randomization&#29992;&#20110;&#37326;&#22806;3D&#24418;&#29366;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
Object-Centric Domain Randomization for 3D Shape Reconstruction in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14539
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;ObjectDR&#65292;&#21033;&#29992;&#23545;&#35937;-centric&#30340;&#22495;&#38543;&#26426;&#21270;&#21512;&#25104;&#21333;&#35270;&#22270;3D&#24418;&#29366;&#37325;&#24314;&#20013;&#32570;&#20047;&#30340;&#37197;&#23545;&#25968;&#25454;&#65292;&#36890;&#36807;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#21644;&#35299;&#32806;&#26694;&#26550;&#26469;&#29983;&#25104;&#21644;&#20445;&#30041;&#23545;&#35937;&#36718;&#24275;&#20197;&#21450;&#24191;&#27867;&#21464;&#21270;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#20026;&#22521;&#35757;&#27169;&#22411;&#25429;&#25417;&#22495;&#19981;&#21464;&#24615;&#20960;&#20309;&#24418;&#29366;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#35270;&#22270;3D&#24418;&#29366;&#22312;&#37326;&#22806;&#30340;&#37325;&#24314;&#38754;&#20020;&#30340;&#26368;&#22823;&#25361;&#25112;&#20043;&#19968;&#26159;&#26469;&#33258;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&lt;3D&#24418;&#29366;&#65292;2D&#22270;&#20687;&gt;-&#37197;&#23545;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#21463;&#22495;&#38543;&#26426;&#21270;&#24341;&#20154;&#27880;&#30446;&#30340;&#25104;&#23601;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ObjectDR&#65292;&#36890;&#36807;&#23545;&#23545;&#35937;&#22806;&#35266;&#21644;&#32972;&#26223;&#30340;&#35270;&#35273;&#21464;&#21270;&#36827;&#34892;&#38543;&#26426;&#20223;&#30495;&#65292;&#21512;&#25104;&#36825;&#31181;&#37197;&#23545;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#21512;&#25104;&#26694;&#26550;&#21033;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;ControlNet&#65289;&#29983;&#25104;&#31526;&#21512;&#31354;&#38388;&#26465;&#20214;&#65288;&#20363;&#22914;2.5D&#33609;&#22270;&#65289;&#30340;&#22270;&#20687;&#65292;&#36825;&#20123;&#26465;&#20214;&#21487;&#20197;&#36890;&#36807;&#20174;&#23545;&#35937;&#38598;&#21512;&#65288;&#20363;&#22914;Objaverse-XL&#65289;&#30340;&#28210;&#26579;&#36807;&#31243;&#33719;&#24471;3D&#24418;&#29366;&#12290;&#20026;&#20102;&#27169;&#25311;&#22810;&#26679;&#21270;&#30340;&#21464;&#21270;&#21516;&#26102;&#20445;&#30041;&#23884;&#20837;&#31354;&#38388;&#26465;&#20214;&#20013;&#30340;&#23545;&#35937;&#36718;&#24275;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#21033;&#29992;&#21021;&#22987;&#23545;&#35937;&#25351;&#23548;&#30340;&#35299;&#32806;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14539v1 Announce Type: cross  Abstract: One of the biggest challenges in single-view 3D shape reconstruction in the wild is the scarcity of &lt;3D shape, 2D image&gt;-paired data from real-world environments. Inspired by remarkable achievements via domain randomization, we propose ObjectDR which synthesizes such paired data via a random simulation of visual variations in object appearances and backgrounds. Our data synthesis framework exploits a conditional generative model (e.g., ControlNet) to generate images conforming to spatial conditions such as 2.5D sketches, which are obtainable through a rendering process of 3D shapes from object collections (e.g., Objaverse-XL). To simulate diverse variations while preserving object silhouettes embedded in spatial conditions, we also introduce a disentangled framework which leverages an initial object guidance. After synthesizing a wide range of data, we pre-train a model on them so that it learns to capture a domain-invariant geometry p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992; web &#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#21033;&#29992;&#32454;&#31890;&#24230;&#37096;&#20214;&#25551;&#36848;&#31526;&#36827;&#34892;&#31934;&#30830;&#25805;&#20316;&#65292;&#36890;&#36807;&#28857;&#20987;&#28304;&#22270;&#20687;&#19981;&#21516;&#23454;&#20363;&#30340;&#24341;&#29992;&#65292;&#36820;&#22238;&#22841;&#20855;&#23039;&#21183;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#31934;&#30830;&#25805;&#25511;&#12290;</title><link>https://arxiv.org/abs/2403.14526</link><description>&lt;p&gt;
&#28857;&#20987;&#25235;&#21462;: &#36890;&#36807;&#35270;&#35273;&#25193;&#25955;&#25551;&#36848;&#36827;&#34892;&#38646;&#26679;&#26412;&#31934;&#30830;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Click to Grasp: Zero-Shot Precise Manipulation via Visual Diffusion Descriptors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14526
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992; web &#35757;&#32451;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#21033;&#29992;&#32454;&#31890;&#24230;&#37096;&#20214;&#25551;&#36848;&#31526;&#36827;&#34892;&#31934;&#30830;&#25805;&#20316;&#65292;&#36890;&#36807;&#28857;&#20987;&#28304;&#22270;&#20687;&#19981;&#21516;&#23454;&#20363;&#30340;&#24341;&#29992;&#65292;&#36820;&#22238;&#22841;&#20855;&#23039;&#21183;&#65292;&#23454;&#29616;&#20102;&#26426;&#22120;&#20154;&#31934;&#30830;&#25805;&#25511;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#25805;&#25511;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#33021;&#22815;&#22312;&#19981;&#21516;&#22330;&#26223;&#21644;&#19981;&#21516;&#29289;&#20307;&#20043;&#38388;&#27867;&#21270;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#32593;&#32476;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#29983;&#25104;&#27169;&#22411;&#65292;&#25506;&#32034;&#20102;&#22312;&#38646;&#26679;&#26412;&#24773;&#20917;&#19979;&#23558;&#32454;&#31890;&#24230;&#37096;&#20214;&#25551;&#36848;&#31526;&#29992;&#20110;&#31934;&#30830;&#25805;&#25511;&#39046;&#22495;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#26694;&#23450;&#20026;&#23494;&#38598;&#35821;&#20041;&#37096;&#20214;&#23545;&#24212;&#20219;&#21153;&#65292;&#27169;&#22411;&#36890;&#36807;&#21442;&#32771;&#28304;&#22270;&#20687;&#20013;&#19982;&#30446;&#26631;&#29289;&#20307;&#19981;&#21516;&#23454;&#20363;&#30340;&#28857;&#20987;&#65292;&#36820;&#22238;&#19968;&#20010;&#29992;&#20110;&#25805;&#25511;&#29305;&#23450;&#37096;&#20214;&#30340;&#22841;&#20855;&#23039;&#21183;&#12290;&#25105;&#20204;&#19981;&#38656;&#35201;&#25163;&#21160;&#25235;&#21462;&#31034;&#33539;&#65292;&#22240;&#20026;&#25105;&#20204;&#21033;&#29992;&#20102;&#22266;&#26377;&#30340;&#29289;&#20307;&#20960;&#20309;&#21644;&#29305;&#24449;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#26700;&#38754;&#24773;&#22659;&#20013;&#30340;&#23454;&#29992;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14526v1 Announce Type: cross  Abstract: Precise manipulation that is generalizable across scenes and objects remains a persistent challenge in robotics. Current approaches for this task heavily depend on having a significant number of training instances to handle objects with pronounced visual and/or geometric part ambiguities. Our work explores the grounding of fine-grained part descriptors for precise manipulation in a zero-shot setting by utilizing web-trained text-to-image diffusion-based generative models. We tackle the problem by framing it as a dense semantic part correspondence task. Our model returns a gripper pose for manipulating a specific part, using as reference a user-defined click from a source image of a visually different instance of the same object. We require no manual grasping demonstrations as we leverage the intrinsic object geometry and features. Practical experiments in a real-world tabletop scenario validate the efficacy of our approach, demonstrati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;CSAC-LB&#65292;&#36890;&#36807;&#24212;&#29992;&#32447;&#24615;&#24179;&#28369;&#23545;&#25968;&#38556;&#30861;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#26080;&#38656;&#20219;&#20309;&#39044;&#35757;&#32451;</title><link>https://arxiv.org/abs/2403.14508</link><description>&lt;p&gt;
&#24102;&#26377;&#24179;&#28369;&#23545;&#25968;&#38556;&#30861;&#20989;&#25968;&#30340;&#32422;&#26463;&#21152;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Constrained Reinforcement Learning with Smoothed Log Barrier Function
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14508
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32422;&#26463;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;CSAC-LB&#65292;&#36890;&#36807;&#24212;&#29992;&#32447;&#24615;&#24179;&#28369;&#23545;&#25968;&#38556;&#30861;&#20989;&#25968;&#65292;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#26080;&#38656;&#20219;&#20309;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#25511;&#21046;&#20219;&#21153;&#65292;&#24182;&#22312;&#35768;&#22810;&#39046;&#22495;&#30340;&#24615;&#33021;&#19978;&#19982;&#20256;&#32479;&#25511;&#21046;&#26041;&#27861;&#30456;&#27604;&#26377;&#20102;&#26174;&#33879;&#25552;&#39640;&#65292;&#20854;&#20013;&#22870;&#21169;&#20989;&#25968;&#26159;&#24456;&#22909;&#23450;&#20041;&#30340;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#38382;&#39064;&#65292;&#20197;&#22870;&#21169;&#21644;&#32422;&#26463;&#21516;&#26102;&#21046;&#23450;&#20248;&#21270;&#38382;&#39064;&#36890;&#24120;&#26356;&#20026;&#26041;&#20415;&#12290;&#36890;&#36807;&#22870;&#21169;&#22609;&#36896;&#26469;&#20248;&#21270;&#36825;&#20123;&#21463;&#38480;&#38382;&#39064;&#21487;&#33021;&#20250;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#38656;&#35201;&#23545;&#24102;&#26377;&#20960;&#20010;&#20132;&#20114;&#39033;&#30340;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#32321;&#29712;&#30340;&#25163;&#21160;&#35843;&#25972;&#12290;&#26368;&#36817;&#21253;&#21547;&#32422;&#26463;&#30340;&#20844;&#24335;&#22312;&#22810;&#25968;&#24773;&#20917;&#19979;&#38656;&#35201;&#39044;&#35757;&#32451;&#38454;&#27573;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#26469;&#25910;&#38598;&#25968;&#25454;&#25110;&#20551;&#23450;&#26377;&#19968;&#20010;&#24453;&#29992;&#30340;&#27425;&#20248;&#31574;&#30053;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CSAC-LB&#65288;&#24102;&#26377;&#23545;&#25968;&#38556;&#30861;&#20989;&#25968;&#30340;&#32422;&#26463;&#36719;&#28436;&#21592;-&#35780;&#35770;&#23478;&#65289;&#30340;&#26032;&#22411;&#32422;&#26463;RL&#26041;&#27861;&#65292;&#36890;&#36807;&#24212;&#29992;&#32447;&#24615;&#24179;&#28369;&#23545;&#25968;&#38556;&#30861;&#20989;&#25968;&#65292;&#35813;&#26041;&#27861;&#23454;&#29616;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#39044;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14508v1 Announce Type: cross  Abstract: Reinforcement Learning (RL) has been widely applied to many control tasks and substantially improved the performances compared to conventional control methods in many domains where the reward function is well defined. However, for many real-world problems, it is often more convenient to formulate optimization problems in terms of rewards and constraints simultaneously. Optimizing such constrained problems via reward shaping can be difficult as it requires tedious manual tuning of reward functions with several interacting terms. Recent formulations which include constraints mostly require a pre-training phase, which often needs human expertise to collect data or assumes having a sub-optimal policy readily available. We propose a new constrained RL method called CSAC-LB (Constrained Soft Actor-Critic with Log Barrier Function), which achieves competitive performance without any pre-training by applying a linear smoothed log barrier funct
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#36807;&#31243; SoftLearn&#65292;&#36890;&#36807;&#36719;&#32858;&#31867;&#36807;&#31243;&#35825;&#23548;&#20986;&#19968;&#20010; PC&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340; LearnSPN&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20135;&#29983;&#26356;&#22909;&#30340;&#20284;&#28982;&#20540;&#21644;&#26679;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.14504</link><description>&lt;p&gt;
&#36719;&#23398;&#20064;&#27010;&#29575;&#30005;&#36335;
&lt;/p&gt;
&lt;p&gt;
Soft Learning Probabilistic Circuits
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14504
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#36807;&#31243; SoftLearn&#65292;&#36890;&#36807;&#36719;&#32858;&#31867;&#36807;&#31243;&#35825;&#23548;&#20986;&#19968;&#20010; PC&#65292;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340; LearnSPN&#65292;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#34920;&#29616;&#26356;&#22909;&#65292;&#20135;&#29983;&#26356;&#22909;&#30340;&#20284;&#28982;&#20540;&#21644;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#30005;&#36335;&#65288;PCs)&#26159;&#26480;&#20986;&#30340;&#21487;&#35745;&#31639;&#27010;&#29575;&#27169;&#22411;&#65292;&#20801;&#35768;&#36827;&#34892;&#19968;&#31995;&#21015;&#20934;&#30830;&#25512;&#29702;&#12290;&#35813;&#35770;&#25991;&#19987;&#27880;&#20110;&#20027;&#35201;&#30340; PC &#35757;&#32451;&#31639;&#27861; LearnSPN&#65292;&#30001;&#20110;&#20854;&#25928;&#29575;&#12289;&#24615;&#33021;&#21644;&#26131;&#29992;&#24615;&#32780;&#25104;&#20026;&#37329;&#26631;&#20934;&#65292;&#29305;&#21035;&#36866;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#12290;&#25105;&#20204;&#34920;&#26126;&#22312;&#28201;&#21644;&#20551;&#35774;&#19979;&#65292;LearnSPN &#26159;&#19968;&#31181;&#36138;&#24515;&#20284;&#28982;&#26368;&#22823;&#21270;&#22120;&#12290;&#34429;&#28982;&#22312; PC &#20013;&#65292;&#25512;&#29702;&#21487;&#20197;&#21033;&#29992;&#25972;&#20010;&#30005;&#36335;&#32467;&#26500;&#26469;&#22788;&#29702;&#26597;&#35810;&#65292;&#20294; LearnSPN &#24212;&#29992;&#20102;&#19968;&#31181;&#30828;&#26041;&#27861;&#26469;&#23398;&#20064;&#23427;&#20204;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#27714;&#21644;&#33410;&#28857;&#19978;&#36890;&#36807;&#19968;&#20010;&#32780;&#20165;&#19968;&#20010;&#23376;/&#36793;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#20256;&#25773;&#65292;&#23601;&#20687;&#22312;&#19968;&#20010;&#30828;&#32858;&#31867;&#36807;&#31243;&#20013;&#19968;&#26679;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; SoftLearn &#30340;&#26032;&#23398;&#20064;&#31243;&#24207;&#65292;&#23427;&#36890;&#36807;&#36719;&#32858;&#31867;&#36807;&#31243;&#35825;&#23548;&#20986;&#19968;&#20010; PC&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#36825;&#31181;&#23398;&#20064;-&#25512;&#29702;&#20860;&#23481;&#24615;&#22312; PC &#20013;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SoftLearn &#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#20248;&#20110; LearnSPN&#65292;&#20135;&#29983;&#26356;&#22909;&#30340;&#20284;&#28982;&#20540;&#65292;&#21487;&#33021;&#36824;&#20135;&#29983;&#26356;&#22909;&#30340;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14504v1 Announce Type: cross  Abstract: Probabilistic Circuits (PCs) are prominent tractable probabilistic models, allowing for a range of exact inferences. This paper focuses on the main algorithm for training PCs, LearnSPN, a gold standard due to its efficiency, performance, and ease of use, in particular for tabular data. We show that LearnSPN is a greedy likelihood maximizer under mild assumptions. While inferences in PCs may use the entire circuit structure for processing queries, LearnSPN applies a hard method for learning them, propagating at each sum node a data point through one and only one of the children/edges as in a hard clustering process. We propose a new learning procedure named SoftLearn, that induces a PC using a soft clustering process. We investigate the effect of this learning-inference compatibility in PCs. Our experiments show that SoftLearn outperforms LearnSPN in many situations, yielding better likelihoods and arguably better samples. We also analy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#23545;&#20154;&#31867;&#19982;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#30340;&#29616;&#29366;&#21644;&#21487;&#35299;&#37322;&#30028;&#38754;&#35774;&#35745;&#26041;&#21521;&#30340;&#31995;&#32479;&#35843;&#26597;&#12290;</title><link>https://arxiv.org/abs/2403.14496</link><description>&lt;p&gt;
&#20154;&#31867;&#20013;&#24515;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#30028;&#38754;&#30340;&#35774;&#35745;&#19982;&#35780;&#20272;&#65306;&#19968;&#39033;&#31995;&#32479;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
How Human-Centered Explainable AI Interface Are Designed and Evaluated: A Systematic Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#23545;&#20154;&#31867;&#19982;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#30340;&#29616;&#29366;&#21644;&#21487;&#35299;&#37322;&#30028;&#38754;&#35774;&#35745;&#26041;&#21521;&#30340;&#31995;&#32479;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#25216;&#26415;&#19978;&#21462;&#24471;&#20102;&#31361;&#30772;&#65292;&#20294;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30740;&#31350;&#22312;&#20026;&#29992;&#25143;&#25552;&#20379;&#25152;&#38656;&#30340;&#8220;&#26377;&#25928;&#35299;&#37322;&#8221;&#26041;&#38754;&#21462;&#24471;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#20026;&#20102;&#25552;&#39640;XAI&#31995;&#32479;&#23545;&#30495;&#23454;&#29992;&#25143;&#30340;&#21487;&#29992;&#24615;&#12289;&#23454;&#38469;&#21487;&#35299;&#37322;&#24615;&#21644;&#25928;&#21147;&#65292;&#26032;&#20852;&#39046;&#22495;&#8220;&#21487;&#35299;&#37322;&#30028;&#38754;&#8221;&#65288;EIs&#65289;&#19987;&#27880;&#20110;XAI&#30340;&#29992;&#25143;&#30028;&#38754;&#21644;&#29992;&#25143;&#20307;&#39564;&#35774;&#35745;&#26041;&#38754;&#12290;&#26412;&#25991;&#36890;&#36807;&#31995;&#32479;&#35843;&#26597;53&#31687;&#20986;&#29256;&#29289;&#65292;&#26088;&#22312;&#35782;&#21035;&#24403;&#21069;&#20154;-XAI&#20114;&#21160;&#30340;&#36235;&#21183;&#20197;&#21450;EI&#35774;&#35745;&#21644;&#21457;&#23637;&#30340;&#26377;&#26395;&#26041;&#21521;&#12290;&#36825;&#26159;&#23545;EI&#30740;&#31350;&#30340;&#39318;&#27425;&#31995;&#32479;&#35843;&#26597;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14496v1 Announce Type: cross  Abstract: Despite its technological breakthroughs, eXplainable Artificial Intelligence (XAI) research has limited success in producing the {\em effective explanations} needed by users. In order to improve XAI systems' usability, practical interpretability, and efficacy for real users, the emerging area of {\em Explainable Interfaces} (EIs) focuses on the user interface and user experience design aspects of XAI. This paper presents a systematic survey of 53 publications to identify current trends in human-XAI interaction and promising directions for EI design and development. This is among the first systematic survey of EI research.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#25237;&#24433;&#65292;&#20197;&#26377;&#25928;&#22320;&#23558;&#20256;&#32479;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#24212;&#29992;&#20110;&#36328;&#20219;&#21153;&#35774;&#32622;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;</title><link>https://arxiv.org/abs/2403.14494</link><description>&lt;p&gt;
&#23398;&#20064;&#25237;&#24433;&#20197;&#36827;&#34892;&#36328;&#20219;&#21153;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Learning to Project for Cross-Task Knowledge Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14494
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#23398;&#20064;&#25237;&#24433;&#65292;&#20197;&#26377;&#25928;&#22320;&#23558;&#20256;&#32479;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#24212;&#29992;&#20110;&#36328;&#20219;&#21153;&#35774;&#32622;&#30340;&#26041;&#27861;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30693;&#35782;&#33976;&#39311;(KD)&#20381;&#36182;&#20110;&#22312;&#30446;&#26631;&#20219;&#21153;&#19978;&#35757;&#32451;&#36807;&#30340;&#29087;&#32451;&#25945;&#24072;&#65292;&#32780;&#36825;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#30340;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#36328;&#20219;&#21153;&#33976;&#39311;&#65292;&#20351;&#24471;&#21487;&#20197;&#21033;&#29992;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#35757;&#32451;&#36807;&#30340;&#20219;&#20309;&#25945;&#24072;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#22312;&#24212;&#29992;&#20110;&#36825;&#31181;&#36328;&#20219;&#21153;&#35774;&#32622;&#26102;&#34987;&#35777;&#26126;&#26159;&#26080;&#25928;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#20462;&#25913;&#65306;&#20351;&#29992;&#21453;&#21521;&#25237;&#24433;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#23545;&#26631;&#20934;&#25237;&#24433;&#30340;&#25554;&#20837;&#24335;&#26367;&#20195;&#26159;&#26377;&#25928;&#30340;&#65292;&#36890;&#36807;&#23398;&#20064;&#25490;&#38500;&#21487;&#33021;&#38477;&#20302;&#23398;&#29983;&#34920;&#29616;&#30340;&#20219;&#20309;&#20219;&#21153;&#29305;&#23450;&#29305;&#24449;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#20010;&#31616;&#21333;&#30340;&#20462;&#25913;&#36275;&#20197;&#23558;&#35768;&#22810;&#30693;&#35782;&#33976;&#39311;&#26041;&#27861;&#25193;&#23637;&#21040;&#36328;&#20219;&#21153;&#35774;&#32622;&#65292;&#20854;&#20013;&#25945;&#24072;&#21644;&#23398;&#29983;&#20219;&#21153;&#21487;&#33021;&#38750;&#24120;&#19981;&#21516;&#12290;&#36825;&#26679;&#19968;&#26469;&#65292;&#22312;&#36328;&#20219;&#21153;&#35774;&#32622;&#20013;&#65292;&#25105;&#20204;&#30456;&#27604;&#20110;&#20256;&#32479;&#25237;&#24433;&#65292;&#21487;&#33719;&#24471;&#26368;&#39640;1.9%&#30340;&#25913;&#36827;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#25104;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#33719;&#24471;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14494v1 Announce Type: cross  Abstract: Traditional knowledge distillation (KD) relies on a proficient teacher trained on the target task, which is not always available. In this setting, cross-task distillation can be used, enabling the use of any teacher model trained on a different task. However, many KD methods prove ineffective when applied to this cross-task setting. To address this limitation, we propose a simple modification: the use of an inverted projection. We show that this drop-in replacement for a standard projector is effective by learning to disregard any task-specific features which might degrade the student's performance. We find that this simple modification is sufficient for extending many KD methods to the cross-task setting, where the teacher and student tasks can be very different. In doing so, we obtain up to a 1.9% improvement in the cross-task setting compared to the traditional projection, at no additional cost. Our method can obtain significant per
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.14488</link><description>&lt;p&gt;
&#22522;&#20110;&#29289;&#29702;&#23398;&#22240;&#26524;&#25512;&#29702;&#30340;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#23433;&#20840;&#31283;&#20581;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Physics-Based Causal Reasoning for Safe &amp; Robust Next-Best Action Selection in Robot Manipulation Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14488
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#25104;&#21151;&#39044;&#27979;&#31215;&#26408;&#22612;&#31283;&#23450;&#24615;&#24182;&#36873;&#25321;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23433;&#20840;&#39640;&#25928;&#30340;&#29289;&#20307;&#25805;&#20316;&#26159;&#35768;&#22810;&#30495;&#23454;&#19990;&#30028;&#26426;&#22120;&#20154;&#24212;&#29992;&#30340;&#20851;&#38190;&#25512;&#25163;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#25361;&#25112;&#22312;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#24517;&#39035;&#23545;&#19968;&#31995;&#21015;&#20256;&#24863;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#19981;&#30830;&#23450;&#24615;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#29289;&#29702;&#30693;&#35782;&#21644;&#22240;&#26524;&#25512;&#29702;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35753;&#26426;&#22120;&#20154;&#22312;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#29615;&#22659;&#20013;&#23545;&#20505;&#36873;&#21160;&#20316;&#36827;&#34892;&#27010;&#29575;&#25512;&#29702;&#65292;&#20197;&#23436;&#25104;&#19968;&#20010;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#21018;&#20307;&#31995;&#32479;&#21160;&#21147;&#23398;&#30340;&#22522;&#20110;&#29289;&#29702;&#23398;&#30340;&#20223;&#30495;&#19982;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;CBN&#65289;&#32467;&#21512;&#36215;&#26469;&#65292;&#23450;&#20041;&#20102;&#26426;&#22120;&#20154;&#20915;&#31574;&#36807;&#31243;&#30340;&#22240;&#26524;&#29983;&#25104;&#27010;&#29575;&#27169;&#22411;&#12290;&#36890;&#36807;&#22522;&#20110;&#20223;&#30495;&#30340;&#33945;&#29305;&#21345;&#27931;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#25104;&#21151;&#22320;&#33021;&#22815;&#65306;(1) &#39640;&#20934;&#30830;&#24230;&#22320;&#39044;&#27979;&#31215;&#26408;&#22612;&#30340;&#31283;&#23450;&#24615;&#65288;&#39044;&#27979;&#20934;&#30830;&#29575;&#65306;88.6%&#65289;&#65307;&#21644;&#65292;(2) &#20026;&#31215;&#26408;&#22534;&#21472;&#20219;&#21153;&#36873;&#25321;&#19968;&#20010;&#36817;&#20284;&#30340;&#19979;&#19968;&#26368;&#20339;&#21160;&#20316;&#65292;&#20379;&#25972;&#21512;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#25191;&#34892;&#65292;&#23454;&#29616;94.2%&#30340;&#20219;&#21153;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14488v1 Announce Type: cross  Abstract: Safe and efficient object manipulation is a key enabler of many real-world robot applications. However, this is challenging because robot operation must be robust to a range of sensor and actuator uncertainties. In this paper, we present a physics-informed causal-inference-based framework for a robot to probabilistically reason about candidate actions in a block stacking task in a partially observable setting. We integrate a physics-based simulation of the rigid-body system dynamics with a causal Bayesian network (CBN) formulation to define a causal generative probabilistic model of the robot decision-making process. Using simulation-based Monte Carlo experiments, we demonstrate our framework's ability to successfully: (1) predict block tower stability with high accuracy (Pred Acc: 88.6%); and, (2) select an approximate next-best action for the block stacking task, for execution by an integrated robot system, achieving 94.2% task succe
&lt;/p&gt;</description></item><item><title>HyperGALE&#36890;&#36807;&#23398;&#20064;&#36229;&#36793;&#30340;&#36229;&#22270;&#38376;&#25511;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#35299;&#37322;&#22797;&#26434;&#30340;&#33041;&#22270;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#20026;ASD&#29983;&#29289;&#26631;&#24535;&#29305;&#24449;&#21270;&#25552;&#20379;&#26356;&#28145;&#20837;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.14484</link><description>&lt;p&gt;
HyperGALE: &#36890;&#36807;&#23398;&#20064;&#36229;&#36793;&#30340;&#36229;&#22270;&#38376;&#25511;&#27880;&#24847;&#21147;&#36827;&#34892;ASD&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
HyperGALE: ASD Classification via Hypergraph Gated Attention with Learnable Hyperedges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14484
&lt;/p&gt;
&lt;p&gt;
HyperGALE&#36890;&#36807;&#23398;&#20064;&#36229;&#36793;&#30340;&#36229;&#22270;&#38376;&#25511;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#22312;&#35299;&#37322;&#22797;&#26434;&#30340;&#33041;&#22270;&#25968;&#25454;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25913;&#36827;&#65292;&#20026;ASD&#29983;&#29289;&#26631;&#24535;&#29305;&#24449;&#21270;&#25552;&#20379;&#26356;&#28145;&#20837;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#38381;&#30151;&#35889;&#31995;&#38556;&#30861;&#65288;ASD&#65289;&#26159;&#19968;&#31181;&#31070;&#32463;&#21457;&#32946;&#38556;&#30861;&#65292;&#20854;&#29305;&#24449;&#26159;&#21508;&#31181;&#31038;&#20132;&#35748;&#30693;&#25361;&#25112;&#21644;&#37325;&#22797;&#34892;&#20026;&#27169;&#24335;&#12290;&#30001;&#20110;&#35889;&#31995;&#30340;&#30151;&#29366;&#22810;&#26679;&#24615;&#65292;&#20026;ASD&#35782;&#21035;&#21487;&#38752;&#30340;&#22522;&#20110;&#33041;&#25104;&#20687;&#30340;&#29983;&#29289;&#26631;&#24535;&#19968;&#30452;&#26159;&#19968;&#20010;&#25345;&#20037;&#30340;&#25361;&#25112;&#12290;&#35813;&#39046;&#22495;&#29616;&#26377;&#30340;&#22522;&#32447;&#24050;&#32463;&#22312;&#36825;&#20010;&#26041;&#21521;&#19978;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#20294;&#22312;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#20173;&#26377;&#25913;&#36827;&#31354;&#38388;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;\emph {HyperGALE}&#65292;&#23427;&#22522;&#20110;&#36229;&#22270;&#65292;&#32467;&#21512;&#20102;&#23398;&#20064;&#30340;&#36229;&#36793;&#21644;&#38376;&#25511;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#36825;&#31181;&#26041;&#27861;&#22823;&#22823;&#25552;&#39640;&#20102;&#27169;&#22411;&#35299;&#37322;&#22797;&#26434;&#33041;&#22270;&#25968;&#25454;&#30340;&#33021;&#21147;&#65292;&#20026;ASD&#29983;&#29289;&#26631;&#24535;&#29305;&#24449;&#21270;&#25552;&#20379;&#20102;&#26356;&#28145;&#20837;&#30340;&#35265;&#35299;&#12290;&#22312;&#24191;&#27867;&#30340;ABIDE II&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;\emph {HyperGALE}&#19981;&#20165;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#36824;&#22312;&#20851;&#38190;&#24615;&#33021;&#25351;&#26631;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14484v1 Announce Type: cross  Abstract: Autism Spectrum Disorder (ASD) is a neurodevelopmental condition characterized by varied social cognitive challenges and repetitive behavioral patterns. Identifying reliable brain imaging-based biomarkers for ASD has been a persistent challenge due to the spectrum's diverse symptomatology. Existing baselines in the field have made significant strides in this direction, yet there remains room for improvement in both performance and interpretability. We propose \emph{HyperGALE}, which builds upon the hypergraph by incorporating learned hyperedges and gated attention mechanisms. This approach has led to substantial improvements in the model's ability to interpret complex brain graph data, offering deeper insights into ASD biomarker characterization. Evaluated on the extensive ABIDE II dataset, \emph{HyperGALE} not only improves interpretability but also demonstrates statistically significant enhancements in key performance metrics compare
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;LightGBM&#31639;&#27861;&#23545;&#36816;&#33829;&#21830;&#29992;&#25143;&#20449;&#29992;&#35780;&#20272;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#65292;&#36890;&#36807;&#25552;&#21462;&#20851;&#38190;&#29305;&#24449;&#24182;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#20449;&#29992;&#35780;&#20272;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.14483</link><description>&lt;p&gt;
&#20351;&#29992;LightGBM&#31639;&#27861;&#36827;&#34892;&#36816;&#33829;&#21830;&#29992;&#25143;&#20449;&#29992;&#35780;&#20272;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Utilizing the LightGBM Algorithm for Operator User Credit Assessment Research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14483
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;LightGBM&#31639;&#27861;&#23545;&#36816;&#33829;&#21830;&#29992;&#25143;&#20449;&#29992;&#35780;&#20272;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#65292;&#36890;&#36807;&#25552;&#21462;&#20851;&#38190;&#29305;&#24449;&#24182;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#21644;&#29305;&#24449;&#24037;&#31243;&#65292;&#20197;&#25913;&#21892;&#29992;&#25143;&#20449;&#29992;&#35780;&#20272;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#20114;&#32852;&#32593;&#29992;&#25143;&#20449;&#29992;&#35780;&#20272;&#26159;&#36890;&#20449;&#36816;&#33829;&#21830;&#21046;&#23450;&#20915;&#31574;&#21644;&#25514;&#26045;&#30340;&#37325;&#35201;&#26041;&#24335;&#65292;&#20063;&#26159;&#36816;&#33829;&#21830;&#33719;&#24471;&#39044;&#26399;&#25910;&#30410;&#30340;&#20445;&#38556;&#12290;&#26412;&#25991;&#21033;&#29992;&#36890;&#20449;&#36816;&#33829;&#21830;&#25552;&#20379;&#30340;&#28023;&#37327;&#25968;&#25454;&#65292;&#22522;&#20110;&#34701;&#21512;LightGBM&#31639;&#27861;&#36827;&#34892;&#36816;&#33829;&#21830;&#29992;&#25143;&#20449;&#29992;&#35780;&#20272;&#27169;&#22411;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14483v1 Announce Type: cross  Abstract: Mobile Internet user credit assessment is an important way for communication operators to establish decisions and formulate measures, and it is also a guarantee for operators to obtain expected benefits. However, credit evaluation methods have long been monopolized by financial industries such as banks and credit. As supporters and providers of platform network technology and network resources, communication operators are also builders and maintainers of communication networks. Internet data improves the user's credit evaluation strategy. This paper uses the massive data provided by communication operators to carry out research on the operator's user credit evaluation model based on the fusion LightGBM algorithm. First, for the massive data related to user evaluation provided by operators, key features are extracted by data preprocessing and feature engineering methods, and a multi-dimensional feature set with statistical significance 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.14472</link><description>&lt;p&gt;
&#36890;&#36807;&#30693;&#35782;&#32534;&#36753;&#23454;&#29616;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Detoxifying Large Language Models via Knowledge Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21435;&#27602;&#21270;&#65292;&#22312;&#26500;&#24314;&#20102;SafeEdit&#22522;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861; DINM&#65292;&#21487;&#20197;&#36890;&#36807;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;&#27169;&#22411;&#30340;&#27602;&#24615;&#65292;&#21516;&#26102;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#30693;&#35782;&#32534;&#36753;&#25216;&#26415;&#26469;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#21435;&#27602;&#21270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;SafeEdit&#30340;&#22522;&#20934;&#65292;&#28085;&#30422;&#20102;&#20061;&#31181;&#19981;&#23433;&#20840;&#31867;&#21035;&#65292;&#20855;&#26377;&#21508;&#31181;&#24378;&#22823;&#30340;&#25915;&#20987;&#25552;&#31034;&#65292;&#24182;&#37197;&#22791;&#20102;&#20840;&#38754;&#30340;&#24230;&#37327;&#26631;&#20934;&#36827;&#34892;&#31995;&#32479;&#35780;&#20272;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#27604;&#36739;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#19982;&#20043;&#21069;&#30340;&#22522;&#20934;&#32447;&#65292;&#32467;&#26524;&#34920;&#26126;&#30693;&#35782;&#32534;&#36753;&#26377;&#28508;&#21147;&#22312;&#23545;LLMs&#36827;&#34892;&#21435;&#27602;&#21270;&#26102;&#65292;&#22312;&#23545;&#19968;&#33324;&#24615;&#33021;&#30340;&#24433;&#21709;&#30456;&#23545;&#26377;&#38480;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#22522;&#20934;&#32447;&#65292;&#31216;&#20026;&#36890;&#36807;&#26415;&#20013;&#31070;&#32463;&#30417;&#27979;&#21435;&#27602;&#21270;&#65288;DINM&#65289;&#65292;&#36890;&#36807;&#20165;&#19968;&#27425;&#23454;&#20363;&#30340;&#23569;&#37327;&#35843;&#25972;&#27493;&#39588;&#20943;&#23569;LLMs&#30340;&#27602;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23545;&#21508;&#31181;&#21435;&#27602;&#26041;&#27861;&#30340;&#20869;&#37096;&#26426;&#21046;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#34920;&#26126;&#20808;&#21069;&#30340;&#26041;&#27861;&#22914;SFT&#21644;DPO&#21487;&#33021;&#20165;&#25233;&#21046;&#26377;&#27602;&#21442;&#25968;&#30340;&#28608;&#27963;&#65292;&#32780;DINM&#21017;&#20943;&#36731;&#26377;&#27602;&#21442;&#25968;&#30340;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14472v1 Announce Type: cross  Abstract: This paper investigates using knowledge editing techniques to detoxify Large Language Models (LLMs). We construct a benchmark, SafeEdit, which covers nine unsafe categories with various powerful attack prompts and equips comprehensive metrics for systematic evaluation. We conduct experiments to compare knowledge editing approaches with previous baselines, indicating that knowledge editing has the potential to efficiently detoxify LLMs with limited impact on general performance. Then, we propose a simple yet effective baseline, dubbed Detoxifying with Intraoperative Neural Monitoring (DINM), to diminish the toxicity of LLMs within a few tuning steps via only one instance. We further provide an in-depth analysis of the internal mechanism for various detoxify approaches, demonstrating that previous methods like SFT and DPO may merely suppress the activations of toxic parameters, while DINM mitigates the toxicity of the toxic parameters to
&lt;/p&gt;</description></item><item><title>&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#24378;&#22823;&#33021;&#21147;&#65292;ChatGPT&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#24378;&#22823;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#21512;&#20316;&#25512;&#21160;&#19979;&#65292;LLM&#30740;&#31350;&#39046;&#22495;&#19981;&#26029;&#21462;&#24471;&#26032;&#31361;&#30772;&#65292;&#39044;&#31034;&#30528;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#23558;&#36814;&#26469;&#38761;&#21629;&#24615;&#21464;&#38761;&#12290;</title><link>https://arxiv.org/abs/2403.14469</link><description>&lt;p&gt;
ChatGPT&#22791;&#36873;&#26041;&#26696;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Alternative Solutions: Large Language Models Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14469
&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#23637;&#29616;&#20986;&#24378;&#22823;&#33021;&#21147;&#65292;ChatGPT&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#24378;&#22823;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#21512;&#20316;&#25512;&#21160;&#19979;&#65292;LLM&#30740;&#31350;&#39046;&#22495;&#19981;&#26029;&#21462;&#24471;&#26032;&#31361;&#30772;&#65292;&#39044;&#31034;&#30528;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#23558;&#36814;&#26469;&#38761;&#21629;&#24615;&#21464;&#38761;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26368;&#36817;&#30340;&#26102;&#20195;&#65292;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#22766;&#20029;&#19981;&#20165;&#38378;&#32768;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#32780;&#19988;&#20063;&#23558;&#20854;&#20809;&#36745;&#27922;&#21521;&#24191;&#27867;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#36825;&#31181;&#23545;LLM&#33021;&#21147;&#30340;&#26174;&#33879;&#23637;&#31034;&#24341;&#21457;&#20102;&#35813;&#39046;&#22495;&#20869;&#30740;&#31350;&#36129;&#29486;&#30340;&#28608;&#22686;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#20027;&#39064;&#30340;&#22810;&#26679;&#21270;&#20809;&#35889;&#12290;&#36825;&#20123;&#36129;&#29486;&#28085;&#30422;&#20102;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#36827;&#27493;&#12289;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#22686;&#24378;&#12289;&#27169;&#22411;&#23545;&#40784;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#12289;&#22522;&#20934;&#27979;&#35797;&#12289;&#25928;&#29575;&#25913;&#36827;&#31561;&#12290;&#36817;&#24180;&#26469;&#65292;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#20043;&#38388;&#24418;&#25104;&#20102;&#21160;&#24577;&#30340;&#21327;&#21516;&#20851;&#31995;&#65292;&#25512;&#21160;&#20102;LLM&#30740;&#31350;&#39046;&#22495;&#21521;&#26032;&#39640;&#24230;&#36808;&#36827;&#12290;&#36825;&#19968;&#26053;&#31243;&#20013;&#30340;&#19968;&#20010;&#20540;&#24471;&#27880;&#24847;&#30340;&#37324;&#31243;&#30865;&#26159;ChatGPT&#30340;&#25512;&#20986;&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;LLMs&#30340;&#24378;&#22823;&#20154;&#24037;&#26234;&#33021;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#31038;&#20250;&#20851;&#27880;&#12290;LLMs&#30340;&#19981;&#26029;&#21457;&#23637;&#25216;&#26415;&#24050;&#32463;&#24320;&#22987;&#37325;&#22609;&#25972;&#20010;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#30340;&#26684;&#23616;&#65292;&#25215;&#35834;&#36827;&#34892;&#38761;&#21629;&#24615;&#30340;&#36716;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14469v1 Announce Type: cross  Abstract: In recent times, the grandeur of Large Language Models (LLMs) has not only shone in the realm of natural language processing but has also cast its brilliance across a vast array of applications. This remarkable display of LLM capabilities has ignited a surge in research contributions within this domain, spanning a diverse spectrum of topics. These contributions encompass advancements in neural network architecture, context length enhancements, model alignment, training datasets, benchmarking, efficiency improvements, and more. Recent years have witnessed a dynamic synergy between academia and industry, propelling the field of LLM research to new heights. A notable milestone in this journey is the introduction of ChatGPT, a powerful AI chatbot grounded in LLMs, which has garnered widespread societal attention. The evolving technology of LLMs has begun to reshape the landscape of the entire AI community, promising a revolutionary shift i
&lt;/p&gt;</description></item><item><title>AnyV2V&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#31616;&#21270;&#35270;&#39057;&#32534;&#36753;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#20256;&#32479;&#21644;&#26032;&#39062;&#30340;&#32534;&#36753;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.14468</link><description>&lt;p&gt;
AnyV2V&#65306;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AnyV2V: A Plug-and-Play Framework For Any Video-to-Video Editing Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14468
&lt;/p&gt;
&lt;p&gt;
AnyV2V&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#20219;&#20309;&#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#31616;&#21270;&#35270;&#39057;&#32534;&#36753;&#65292;&#25903;&#25345;&#24191;&#27867;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#20256;&#32479;&#21644;&#26032;&#39062;&#30340;&#32534;&#36753;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14468v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#36234; &#25688;&#35201;: &#35270;&#39057;&#21040;&#35270;&#39057;&#32534;&#36753;&#28041;&#21450;&#32534;&#36753;&#28304;&#35270;&#39057;&#20197;&#21450;&#39069;&#22806;&#30340;&#25511;&#21046;&#65288;&#20363;&#22914;&#25991;&#26412;&#25552;&#31034;&#12289;&#20027;&#39064;&#25110;&#39118;&#26684;&#65289;&#65292;&#20197;&#29983;&#25104;&#19982;&#28304;&#35270;&#39057;&#21644;&#25552;&#20379;&#30340;&#25511;&#21046;&#30456;&#21305;&#37197;&#30340;&#26032;&#35270;&#39057;&#12290;&#20256;&#32479;&#26041;&#27861;&#21463;&#38480;&#20110;&#29305;&#23450;&#30340;&#32534;&#36753;&#31867;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#28385;&#36275;&#24191;&#27867;&#29992;&#25143;&#38656;&#27714;&#30340;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AnyV2V&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#20813;&#35757;&#32451;&#26694;&#26550;&#65292;&#26088;&#22312;&#23558;&#35270;&#39057;&#32534;&#36753;&#31616;&#21270;&#20026;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#65288;1&#65289;&#21033;&#29992;&#29616;&#25104;&#30340;&#22270;&#20687;&#32534;&#36753;&#27169;&#22411;&#65288;&#20363;&#22914;InstructPix2Pix&#12289;InstantID&#31561;&#65289;&#20462;&#25913;&#31532;&#19968;&#24103;&#65292;&#65288;2&#65289;&#21033;&#29992;&#29616;&#26377;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;I2VGen-XL&#65289;&#36827;&#34892;DDIM&#36870;&#36716;&#21644;&#29305;&#24449;&#27880;&#20837;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;AnyV2V&#21487;&#20197;&#25554;&#20837;&#20219;&#20309;&#29616;&#26377;&#30340;&#22270;&#20687;&#32534;&#36753;&#24037;&#20855;&#65292;&#20197;&#25903;&#25345;&#24191;&#27867;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#12290;&#38500;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#32534;&#36753;&#26041;&#27861;&#65292;AnyV2V&#36824;&#21487;&#20197;&#25903;&#25345;&#26032;&#39062;&#30340;&#35270;&#39057;&#32534;&#36753;&#20219;&#21153;&#65292;&#21253;&#25324;&#21442;&#32771;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14468v1 Announce Type: cross  Abstract: Video-to-video editing involves editing a source video along with additional control (such as text prompts, subjects, or styles) to generate a new video that aligns with the source video and the provided control. Traditional methods have been constrained to certain editing types, limiting their ability to meet the wide range of user demands. In this paper, we introduce AnyV2V, a novel training-free framework designed to simplify video editing into two primary steps: (1) employing an off-the-shelf image editing model (e.g. InstructPix2Pix, InstantID, etc) to modify the first frame, (2) utilizing an existing image-to-video generation model (e.g. I2VGen-XL) for DDIM inversion and feature injection. In the first stage, AnyV2V can plug in any existing image editing tools to support an extensive array of video editing tasks. Beyond the traditional prompt-based editing methods, AnyV2V also can support novel video editing tasks, including refe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#21644;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#25628;&#32034;&#21644;&#20248;&#21270;&#30340;&#36807;&#31243;&#20013;&#20986;&#29616;&#26368;&#32456;&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#21333;&#19968;&#31995;&#32479;&#38169;&#35273;&#29305;&#24615;&#65292;&#24182;&#23558;&#29616;&#20195;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#20837;&#20854;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#36710;&#36742;&#36719;&#20214;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#24320;&#21457;&#12290;</title><link>https://arxiv.org/abs/2403.14460</link><description>&lt;p&gt;
&#23454;&#29616;&#36719;&#20214;&#23450;&#20041;&#36710;&#36742;&#20013;&#30340;&#21333;&#19968;&#31995;&#32479;&#38169;&#35273; -- &#33258;&#21160;&#21270;&#12289;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#24037;&#20316;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
Towards Single-System Illusion in Software-Defined Vehicles -- Automated, AI-Powered Workflow
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14460
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#21644;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#25628;&#32034;&#21644;&#20248;&#21270;&#30340;&#36807;&#31243;&#20013;&#20986;&#29616;&#26368;&#32456;&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#21333;&#19968;&#31995;&#32479;&#38169;&#35273;&#29305;&#24615;&#65292;&#24182;&#23558;&#29616;&#20195;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24341;&#20837;&#20854;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#36710;&#36742;&#36719;&#20214;&#31995;&#32479;&#30340;&#33258;&#21160;&#21270;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#21644;&#29305;&#24449;&#30340;&#26032;&#39062;&#26041;&#27861;&#26469;&#24320;&#21457;&#36710;&#36742;&#36719;&#20214;&#31995;&#32479;&#65292;&#20854;&#20013;&#26368;&#32456;&#26550;&#26500;&#24182;&#26410;&#26126;&#30830;&#23450;&#20041;&#12290;&#30456;&#21453;&#65292;&#23427;&#26159;&#20174;&#19968;&#31995;&#21015;&#25628;&#32034;&#21644;&#20248;&#21270;&#30340;&#36845;&#20195;&#36807;&#31243;&#20013;&#20986;&#29616;&#30340;&#65292;&#26377;&#29305;&#23450;&#30340;&#32422;&#26463;&#12289;&#38656;&#27714;&#21644;&#30828;&#20214;&#26550;&#26500;&#65292;&#21516;&#26102;&#20445;&#25345;&#21333;&#19968;&#31995;&#32479;&#38169;&#35273;&#30340;&#29305;&#24615;&#65292;&#20854;&#20013;&#24212;&#29992;&#22312;&#36923;&#36753;&#19978;&#32479;&#19968;&#30340;&#29615;&#22659;&#20013;&#36816;&#34892;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#28857;&#26159;&#22312;&#24490;&#29615;&#20013;&#21253;&#21547;&#29616;&#20195;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;&#38543;&#30528;&#35813;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#26399;&#26395;LLMs&#33021;&#22815;&#36741;&#21161;&#22788;&#29702;&#38656;&#27714;&#12289;&#29983;&#25104;&#24418;&#24335;&#31995;&#32479;&#27169;&#22411;&#65292;&#20197;&#21450;&#29983;&#25104;&#36719;&#20214;&#37096;&#32626;&#35268;&#33539;&#21644;&#27979;&#35797;&#20195;&#30721;&#12290;&#32467;&#26524;&#31649;&#36947;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#26159;&#33258;&#21160;&#21270;&#30340;&#65292;&#27599;&#19968;&#27493;&#37117;&#20250;&#29983;&#25104;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14460v1 Announce Type: cross  Abstract: We propose a novel model- and feature-based approach to development of vehicle software systems, where the end architecture is not explicitly defined. Instead, it emerges from an iterative process of search and optimization given certain constraints, requirements and hardware architecture, while retaining the property of single-system illusion, where applications run in a logically uniform environment. One of the key points of the presented approach is the inclusion of modern generative AI, specifically Large Language Models (LLMs), in the loop. With the recent advances in the field, we expect that the LLMs will be able to assist in processing of requirements, generation of formal system models, as well as generation of software deployment specification and test code. The resulting pipeline is automated to a large extent, with feedback being generated at each step.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MExGen&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#37327;&#21270;&#27010;&#24565;&#21644;&#22810;&#32423;&#26041;&#27861;&#22788;&#29702;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#35777;&#26126;&#21487;&#20197;&#25552;&#20379;&#26356;&#36148;&#36817;&#26412;&#22320;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.14459</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#32423;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Explanations for Generative Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MExGen&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#26631;&#37327;&#21270;&#27010;&#24565;&#21644;&#22810;&#32423;&#26041;&#27861;&#22788;&#29702;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#30340;&#25361;&#25112;&#65292;&#35777;&#26126;&#21487;&#20197;&#25552;&#20379;&#26356;&#36148;&#36817;&#26412;&#22320;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25200;&#21160;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#22914;LIME&#21644;SHAP&#65292;&#36890;&#24120;&#24212;&#29992;&#20110;&#25991;&#26412;&#20998;&#31867;&#12290;&#26412;&#25991;&#20851;&#27880;&#23427;&#20204;&#22914;&#20309;&#25193;&#23637;&#21040;&#29983;&#25104;&#24335;&#35821;&#35328;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#25991;&#26412;&#20316;&#20026;&#36755;&#20986;&#21644;&#38271;&#25991;&#26412;&#36755;&#20837;&#30340;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MExGen&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#19981;&#21516;&#30340;&#24402;&#22240;&#31639;&#27861;&#23454;&#20363;&#21270;&#12290;&#20026;&#20102;&#22788;&#29702;&#25991;&#26412;&#36755;&#20986;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#23558;&#25991;&#26412;&#26144;&#23556;&#21040;&#23454;&#25968;&#30340;&#26631;&#37327;&#21270;&#27010;&#24565;&#65292;&#24182;&#25506;&#35752;&#20102;&#22810;&#31181;&#21487;&#33021;&#24615;&#12290;&#20026;&#20102;&#22788;&#29702;&#38271;&#36755;&#20837;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#32423;&#26041;&#27861;&#65292;&#20174;&#31895;&#31890;&#24230;&#21040;&#32454;&#31890;&#24230;&#65292;&#37325;&#28857;&#20851;&#27880;&#20855;&#26377;&#27169;&#22411;&#26597;&#35810;&#32447;&#24615;&#32553;&#25918;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#23545;&#22522;&#20110;&#25200;&#21160;&#30340;&#24402;&#22240;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#35780;&#20272;&#65292;&#21253;&#25324;&#33258;&#21160;&#21270;&#21644;&#20154;&#24037;&#35780;&#20272;&#65292;&#29992;&#20110;&#25688;&#35201;&#21644;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#38382;&#31572;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#25552;&#20379;&#26356;&#21152;&#36148;&#36817;&#26412;&#22320;&#30340;&#29983;&#25104;&#24335;&#36755;&#20986;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14459v1 Announce Type: cross  Abstract: Perturbation-based explanation methods such as LIME and SHAP are commonly applied to text classification. This work focuses on their extension to generative language models. To address the challenges of text as output and long text inputs, we propose a general framework called MExGen that can be instantiated with different attribution algorithms. To handle text output, we introduce the notion of scalarizers for mapping text to real numbers and investigate multiple possibilities. To handle long inputs, we take a multi-level approach, proceeding from coarser levels of granularity to finer ones, and focus on algorithms with linear scaling in model queries. We conduct a systematic evaluation, both automated and human, of perturbation-based attribution methods for summarization and context-grounded question answering. The results show that our framework can provide more locally faithful explanations of generated outputs.
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26234;&#33021;&#20195;&#29702;&#22312;&#27169;&#25311;&#25968;&#23383;&#24066;&#22330;&#20013;&#23436;&#25104;&#20080;&#21334;&#20449;&#24687;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#20855;&#22791;&#35780;&#20272;&#20449;&#24687;&#36136;&#37327;&#21644;&#36951;&#24536;&#33021;&#21147;&#30340;&#29305;&#28857;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#20449;&#24687;&#24066;&#22330;&#30340;&#20080;&#26041;&#26816;&#26597;&#24726;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.14443</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#38477;&#20302;&#20449;&#24687;&#24066;&#22330;&#30340;&#19981;&#23545;&#31216;&#24615;
&lt;/p&gt;
&lt;p&gt;
Language Models Can Reduce Asymmetry in Information Markets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14443
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26234;&#33021;&#20195;&#29702;&#22312;&#27169;&#25311;&#25968;&#23383;&#24066;&#22330;&#20013;&#23436;&#25104;&#20080;&#21334;&#20449;&#24687;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;&#20855;&#22791;&#35780;&#20272;&#20449;&#24687;&#36136;&#37327;&#21644;&#36951;&#24536;&#33021;&#21147;&#30340;&#29305;&#28857;&#65292;&#25104;&#21151;&#38477;&#20302;&#20102;&#20449;&#24687;&#24066;&#22330;&#30340;&#20080;&#26041;&#26816;&#26597;&#24726;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#35299;&#20915;&#20102;&#20449;&#24687;&#24066;&#22330;&#20013;&#20080;&#26041;&#26816;&#26597;&#24726;&#35770;&#30340;&#38382;&#39064;&#12290;&#20080;&#26041;&#38656;&#35201;&#33719;&#21462;&#20449;&#24687;&#26469;&#30830;&#23450;&#20854;&#20215;&#20540;&#65292;&#32780;&#21334;&#26041;&#38656;&#35201;&#38480;&#21046;&#35775;&#38382;&#20197;&#38450;&#27490;&#30423;&#31363;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#24320;&#28304;&#30340;&#27169;&#25311;&#25968;&#23383;&#24066;&#22330;&#65292;&#20854;&#20013;&#30001;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#26234;&#33021;&#20195;&#29702;&#20195;&#34920;&#22806;&#37096;&#21442;&#19982;&#32773;&#20080;&#21334;&#20449;&#24687;&#12290;&#36825;&#20010;&#24066;&#22330;&#30340;&#26680;&#24515;&#26426;&#21046;&#22312;&#20110;&#20195;&#29702;&#20154;&#30340;&#21452;&#37325;&#33021;&#21147;&#65306;&#20182;&#20204;&#19981;&#20165;&#33021;&#22815;&#35780;&#20272;&#29305;&#26435;&#20449;&#24687;&#30340;&#36136;&#37327;&#65292;&#36824;&#20855;&#22791;&#36951;&#24536;&#30340;&#33021;&#21147;&#12290;&#36825;&#31181;&#35825;&#23548;&#20581;&#24536;&#30340;&#33021;&#21147;&#20351;&#20379;&#24212;&#21830;&#21487;&#20197;&#25480;&#20104;&#20020;&#26102;&#35775;&#38382;&#19987;&#26377;&#20449;&#24687;&#30340;&#26435;&#38480;&#65292;&#26174;&#33879;&#38477;&#20302;&#26410;&#32463;&#25480;&#26435;&#30340;&#20445;&#30041;&#39118;&#38505;&#65292;&#21516;&#26102;&#20351;&#20195;&#29702;&#20154;&#33021;&#22815;&#20934;&#30830;&#35780;&#20272;&#20449;&#24687;&#23545;&#29305;&#23450;&#26597;&#35810;&#25110;&#20219;&#21153;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#34920;&#29616;&#20248;&#24322;&#65292;&#20195;&#29702;&#24517;&#39035;&#20570;&#20986;&#29702;&#24615;&#20915;&#31574;&#65292;&#25112;&#30053;&#24615;&#22320;&#25506;&#32034;&#24066;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14443v1 Announce Type: new  Abstract: This work addresses the buyer's inspection paradox for information markets. The paradox is that buyers need to access information to determine its value, while sellers need to limit access to prevent theft. To study this, we introduce an open-source simulated digital marketplace where intelligent agents, powered by language models, buy and sell information on behalf of external participants. The central mechanism enabling this marketplace is the agents' dual capabilities: they not only have the capacity to assess the quality of privileged information but also come equipped with the ability to forget. This ability to induce amnesia allows vendors to grant temporary access to proprietary information, significantly reducing the risk of unauthorized retention while enabling agents to accurately gauge the information's relevance to specific queries or tasks. To perform well, agents must make rational decisions, strategically explore the marke
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26799;&#24230;&#22522;&#30784;&#30340;CAM&#25216;&#26415;&#25193;&#23637;&#21040;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#24182;&#21487;&#35270;&#21270;&#20108;&#36827;&#21046;&#38754;&#37096;&#23646;&#24615;&#20998;&#31867;&#22120;&#30340;&#27963;&#21160;&#21306;&#22495;&#65292;&#35777;&#23454;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#20559;&#21521;&#24615;&#20998;&#31867;&#22120;&#20542;&#21521;&#20110;&#23398;&#20064;&#25552;&#21462;&#20027;&#35201;&#31867;&#21035;&#30340;&#29305;&#24449;</title><link>https://arxiv.org/abs/2403.14435</link><description>&lt;p&gt;
&#20559;&#21521;&#24615;&#30340;&#20108;&#36827;&#21046;&#23646;&#24615;&#20998;&#31867;&#22120;&#24573;&#35270;&#20102;&#20027;&#35201;&#31867;&#21035;
&lt;/p&gt;
&lt;p&gt;
Biased Binary Attribute Classifiers Ignore the Majority Classes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14435
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26799;&#24230;&#22522;&#30784;&#30340;CAM&#25216;&#26415;&#25193;&#23637;&#21040;&#20108;&#20803;&#20998;&#31867;&#22120;&#65292;&#24182;&#21487;&#35270;&#21270;&#20108;&#36827;&#21046;&#38754;&#37096;&#23646;&#24615;&#20998;&#31867;&#22120;&#30340;&#27963;&#21160;&#21306;&#22495;&#65292;&#35777;&#23454;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#26102;&#20559;&#21521;&#24615;&#20998;&#31867;&#22120;&#20542;&#21521;&#20110;&#23398;&#20064;&#25552;&#21462;&#20027;&#35201;&#31867;&#21035;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#21487;&#35270;&#21270;&#20998;&#31867;&#22120;&#22522;&#20110;&#20854;&#20915;&#31574;&#30340;&#20852;&#36259;&#21306;&#22495;&#65292;&#21457;&#23637;&#20102;&#19981;&#21516;&#30340;Class Activation Mapping (CAM)&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#36825;&#20123;&#25216;&#26415;&#21482;&#38024;&#23545;&#20998;&#31867;&#20998;&#31867;&#22120;&#65292;&#32780;&#22823;&#22810;&#25968;&#23454;&#38469;&#20219;&#21153;&#26159;&#20108;&#20803;&#20998;&#31867;&#12290;&#26412;&#25991;&#23558;&#22522;&#20110;&#26799;&#24230;&#30340;CAM&#25216;&#26415;&#25193;&#23637;&#21040;&#19982;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#19968;&#36215;&#20351;&#29992;&#65292;&#24182;&#21487;&#35270;&#21270;&#20108;&#36827;&#21046;&#38754;&#37096;&#23646;&#24615;&#20998;&#31867;&#22120;&#30340;&#27963;&#21160;&#21306;&#22495;&#12290;&#24403;&#22312;&#19981;&#24179;&#34913;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#19981;&#24179;&#34913;&#30340;&#20108;&#20803;&#20998;&#31867;&#22120;&#26102;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#21363;&#20855;&#26377;&#35768;&#22810;&#35757;&#32451;&#26679;&#26412;&#30340;&#20027;&#35201;&#31867;&#36890;&#24120;&#27604;&#20855;&#26377;&#23569;&#37327;&#35757;&#32451;&#23454;&#20363;&#30340;&#27425;&#35201;&#31867;&#39044;&#27979;&#24471;&#26356;&#22909;&#12290;&#22312;CelebA&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#24403;&#35757;&#32451;&#19981;&#24179;&#34913;&#20998;&#31867;&#22120;&#21516;&#26102;&#25552;&#21462;40&#20010;&#38754;&#37096;&#23646;&#24615;&#12290;&#20154;&#20204;&#39044;&#26399;&#65292;&#20559;&#21521;&#24615;&#20998;&#31867;&#22120;&#24050;&#32463;&#23398;&#20250;&#20027;&#35201;&#20026;&#22810;&#25968;&#31867;&#25552;&#21462;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14435v1 Announce Type: cross  Abstract: To visualize the regions of interest that classifiers base their decisions on, different Class Activation Mapping (CAM) methods have been developed. However, all of these techniques target categorical classifiers only, though most real-world tasks are binary classification. In this paper, we extend gradient-based CAM techniques to work with binary classifiers and visualize the active regions for binary facial attribute classifiers. When training an unbalanced binary classifier on an imbalanced dataset, it is well-known that the majority class, i.e. the class with many training samples, is mostly predicted much better than minority class with few training instances. In our experiments on the CelebA dataset, we verify these results, when training an unbalanced classifier to extract 40 facial attributes simultaneously. One would expect that the biased classifier has learned to extract features mainly for the majority classes and that the 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#26368;&#20248;&#25511;&#21046;&#20013;&#20215;&#20540;&#20989;&#25968;&#30340;&#36830;&#32493;&#24615;&#21644;&#20809;&#28369;&#24615;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#20215;&#20540;&#20989;&#25968;&#36830;&#32493;&#24615;&#30340;&#19978;&#30028;&#65292;&#24182;&#34920;&#26126;&#22312;&#23545;&#24213;&#23618;&#31995;&#32479;&#36827;&#34892;&#30456;&#23545;&#36739;&#24369;&#30340;&#20551;&#35774;&#19979;&#65292;&#20215;&#20540;&#20989;&#25968;&#22987;&#32456;&#20855;&#26377;H\"older&#36830;&#32493;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14432</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#21644;&#26368;&#20248;&#25511;&#21046;&#20013;&#20215;&#20540;&#20989;&#25968;&#30340;&#36830;&#32493;&#24615;&#21644;&#20809;&#28369;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the continuity and smoothness of the value function in reinforcement learning and optimal control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14432
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#24378;&#21270;&#23398;&#20064;&#21644;&#26368;&#20248;&#25511;&#21046;&#20013;&#20215;&#20540;&#20989;&#25968;&#30340;&#36830;&#32493;&#24615;&#21644;&#20809;&#28369;&#24615;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#20215;&#20540;&#20989;&#25968;&#36830;&#32493;&#24615;&#30340;&#19978;&#30028;&#65292;&#24182;&#34920;&#26126;&#22312;&#23545;&#24213;&#23618;&#31995;&#32479;&#36827;&#34892;&#30456;&#23545;&#36739;&#24369;&#30340;&#20551;&#35774;&#19979;&#65292;&#20215;&#20540;&#20989;&#25968;&#22987;&#32456;&#20855;&#26377;H\"older&#36830;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20215;&#20540;&#20989;&#25968;&#22312;&#24378;&#21270;&#23398;&#20064;&#21644;&#26368;&#20248;&#25511;&#21046;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#20316;&#20026;&#34913;&#37327;&#26234;&#33021;&#20307;&#26410;&#26469;&#32047;&#31215;&#22870;&#21169;&#30340;&#25163;&#27573;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#37051;&#36817;&#29366;&#24577;&#20540;&#30340;&#30456;&#20284;&#24615;&#65292;&#21363;&#20215;&#20540;&#20989;&#25968;&#30340;&#36830;&#32493;&#24615;&#65292;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#25105;&#20204;&#36890;&#36807;&#30830;&#23450;&#21644;&#39564;&#35777;&#20215;&#20540;&#20989;&#25968;&#30340;&#26631;&#20934;&#36830;&#32493;&#24615;&#19978;&#30028;&#65292;&#26469;&#25506;&#35752;&#36825;&#19968;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#23545;&#24213;&#23618;&#31995;&#32479;&#36827;&#34892;&#30456;&#23545;&#36739;&#24369;&#30340;&#20551;&#35774;&#19979;&#65292;&#20215;&#20540;&#20989;&#25968;&#22987;&#32456;&#26159;H\"older&#36830;&#32493;&#30340;&#65292;&#38750;&#21487;&#24494;&#30340;&#20215;&#20540;&#20989;&#25968;&#21487;&#20197;&#36890;&#36807;&#36731;&#24494;&#22320;&#8220;&#25200;&#21160;&#8221;&#31995;&#32479;&#32780;&#21464;&#24471;&#21487;&#24494;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14432v1 Announce Type: cross  Abstract: The value function plays a crucial role as a measure for the cumulative future reward an agent receives in both reinforcement learning and optimal control. It is therefore of interest to study how similar the values of neighboring states are, i.e., to investigate the continuity of the value function. We do so by providing and verifying upper bounds on the value function's modulus of continuity. Additionally, we show that the value function is always H\"older continuous under relatively weak assumptions on the underlying system and that non-differentiable value functions can be made differentiable by slightly "disturbing" the system.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#39118;&#26684;&#25552;&#21462;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#39118;&#26684;&#35843;&#33410;&#26426;&#21046;&#21644;&#20869;&#23481;&#35843;&#33410;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#26410;&#35265;&#22270;&#20687;&#39118;&#26684;&#20449;&#24687;&#65292;&#20174;&#32780;&#20197;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#26410;&#35265;&#39118;&#26684;&#30340;&#22270;&#20687;&#12290;</title><link>https://arxiv.org/abs/2403.14429</link><description>&lt;p&gt;
&#39118;&#26684;&#25552;&#21462;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#21322;&#30417;&#30563;&#32452;&#32455;&#23398;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Style-Extracting Diffusion Models for Semi-Supervised Histopathology Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14429
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#39118;&#26684;&#25552;&#21462;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#39118;&#26684;&#35843;&#33410;&#26426;&#21046;&#21644;&#20869;&#23481;&#35843;&#33410;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#26410;&#35265;&#22270;&#20687;&#39118;&#26684;&#20449;&#24687;&#65292;&#20174;&#32780;&#20197;&#38646;-shot&#26041;&#24335;&#29983;&#25104;&#20855;&#26377;&#26410;&#35265;&#39118;&#26684;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14429v1 &#20844;&#21578;&#31867;&#22411;:&#36328;&#39046;&#22495; &#25688;&#35201;:&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#29983;&#25104;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#26174;&#30528;&#36827;&#23637;&#19979;&#21462;&#24471;&#20102;&#37325;&#35201;&#36827;&#23637;&#65292;&#26126;&#26174;&#25913;&#21892;&#20102;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#36825;&#20123;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#20855;&#26377;&#23545;&#19979;&#28216;&#20219;&#21153;&#26377;&#30410;&#30340;&#26410;&#35265;&#29305;&#24449;&#30340;&#22270;&#20687;&#21364;&#21463;&#21040;&#20102;&#36739;&#23569;&#20851;&#27880;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#39118;&#26684;&#25552;&#21462;&#25193;&#25955;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#31181;&#35843;&#33410;&#26426;&#21046;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21033;&#29992;1)&#39118;&#26684;&#35843;&#21046;&#26426;&#21046;&#22312;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#20808;&#21069;&#26410;&#35265;&#22270;&#20687;&#30340;&#39118;&#26684;&#20449;&#24687;&#65292;2)&#20869;&#23481;&#35843;&#21046;&#26426;&#21046;&#21487;&#20197;&#38024;&#23545;&#19979;&#28216;&#20219;&#21153;&#36827;&#34892;&#23450;&#20301;&#65292;&#20363;&#22914;&#24067;&#23616;&#29992;&#20110;&#20998;&#21106;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#21487;&#35757;&#32451;&#30340;&#39118;&#26684;&#32534;&#30721;&#22120;&#65292;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#39118;&#26684;&#20449;&#24687;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#32858;&#21512;&#22359;&#65292;&#29992;&#20110;&#21512;&#24182;&#26469;&#33258;&#22810;&#20010;&#39118;&#26684;&#36755;&#20837;&#30340;&#39118;&#26684;&#20449;&#24687;&#12290;&#36825;&#31181;&#26550;&#26500;&#20351;&#24471;&#36890;&#36807;&#21033;&#29992;&#26469;&#33258;&#26410;&#35265;&#22270;&#20687;&#30340;&#39118;&#26684;&#65292;&#22312;&#38646;-shot&#26041;&#24335;&#19979;&#29983;&#25104;&#20855;&#26377;&#26410;&#35265;&#39118;&#26684;&#30340;&#22270;&#20687;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14429v1 Announce Type: cross  Abstract: Deep learning-based image generation has seen significant advancements with diffusion models, notably improving the quality of generated images. Despite these developments, generating images with unseen characteristics beneficial for downstream tasks has received limited attention. To bridge this gap, we propose Style-Extracting Diffusion Models, featuring two conditioning mechanisms. Specifically, we utilize 1) a style conditioning mechanism which allows to inject style information of previously unseen images during image generation and 2) a content conditioning which can be targeted to a downstream task, e.g., layout for segmentation. We introduce a trainable style encoder to extract style information from images, and an aggregation block that merges style information from multiple style inputs. This architecture enables the generation of images with unseen styles in a zero-shot manner, by leveraging styles from unseen images, result
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GLC++&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#32858;&#31867;&#20197;&#21450;&#23545;&#27604;&#20851;&#32852;&#23398;&#20064;&#23454;&#29616;&#20102;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#24050;&#30693;&#25968;&#25454;&#24182;&#23558;&#20854;&#20174;&#26410;&#30693;&#25968;&#25454;&#20013;&#20998;&#31163;&#12290;</title><link>https://arxiv.org/abs/2403.14410</link><description>&lt;p&gt;
GLC++: &#20840;&#23616;&#23616;&#37096;&#32858;&#31867;&#21644;&#23545;&#27604;&#20851;&#32852;&#23398;&#20064;&#30340;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
GLC++: Source-Free Universal Domain Adaptation through Global-Local Clustering and Contrastive Affinity Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14410
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;GLC++&#26041;&#27861;&#65292;&#36890;&#36807;&#20840;&#23616;&#21644;&#23616;&#37096;&#32858;&#31867;&#20197;&#21450;&#23545;&#27604;&#20851;&#32852;&#23398;&#20064;&#23454;&#29616;&#20102;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#24050;&#30693;&#25968;&#25454;&#24182;&#23558;&#20854;&#20174;&#26410;&#30693;&#25968;&#25454;&#20013;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#32463;&#24120;&#22312;&#21327;&#21464;&#37327;&#21644;&#31867;&#21035;&#36716;&#31227;&#19979;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#12290;&#26080;&#28304;&#22495;&#33258;&#36866;&#24212;&#65288;SFDA&#65289;&#20026;&#36825;&#19968;&#22256;&#22659;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;SFDA&#26041;&#27861;&#23616;&#38480;&#20110;&#23553;&#38381;&#38598;&#22330;&#26223;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26088;&#22312;&#20934;&#30830;&#20998;&#31867;&#23646;&#20110;&#24120;&#35265;&#31867;&#21035;&#30340;&#8220;&#24050;&#30693;&#8221;&#25968;&#25454;&#24182;&#23558;&#20854;&#19982;&#30446;&#26631;&#19987;&#26377;&#8220;&#26410;&#30693;&#8221;&#25968;&#25454;&#38548;&#31163;&#24320;&#26469;&#30340;&#26080;&#28304;&#36890;&#29992;&#22495;&#33258;&#36866;&#24212;&#65288;SF-UniDA&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20840;&#29699;&#21644;&#23616;&#37096;&#32858;&#31867;&#65288;GLC&#65289;&#25216;&#26415;&#65292;&#20854;&#20013;&#21253;&#25324;&#33258;&#36866;&#24212;&#30340;&#19968;&#23545;&#20840;&#23616;&#32858;&#31867;&#31639;&#27861;&#26469;&#21306;&#20998;&#30446;&#26631;&#31867;&#21035;&#65292;&#36741;&#20197;&#26412;&#22320;k-NN&#32858;&#31867;&#31574;&#30053;&#20197;&#20943;&#36731;&#36127;&#38754;&#36716;&#31227;&#12290;&#23613;&#31649;&#26377;&#25928;&#65292;&#20294;&#22266;&#26377;&#30340;&#23553;&#38381;&#28304;&#26550;&#26500;&#23548;&#33268;&#23545;&#8220;&#26410;&#30693;&#8221;&#25968;&#25454;&#30340;&#32479;&#19968;&#22788;&#29702;&#65292;&#38459;&#30861;&#20102;&#23545;&#19981;&#21516;&#8220;&#26410;&#30693;&#8221;&#31867;&#21035;&#30340;&#35782;&#21035;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;GLC&#21457;&#23637;&#21040;GLC++&#65292;&#25972;&#21512;&#20102;&#23545;&#27604;&#20146;&#21644;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14410v1 Announce Type: cross  Abstract: Deep neural networks often exhibit sub-optimal performance under covariate and category shifts. Source-Free Domain Adaptation (SFDA) presents a promising solution to this dilemma, yet most SFDA approaches are restricted to closed-set scenarios. In this paper, we explore Source-Free Universal Domain Adaptation (SF-UniDA) aiming to accurately classify "known" data belonging to common categories and segregate them from target-private "unknown" data. We propose a novel Global and Local Clustering (GLC) technique, which comprises an adaptive one-vs-all global clustering algorithm to discern between target classes, complemented by a local k-NN clustering strategy to mitigate negative transfer. Despite the effectiveness, the inherent closed-set source architecture leads to uniform treatment of "unknown" data, impeding the identification of distinct "unknown" categories. To address this, we evolve GLC to GLC++, integrating a contrastive affini
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23450;&#20301;&#21644;&#20943;&#36731;&#20559;&#35265;&#36807;&#31243;&#34701;&#20837;&#32479;&#19968;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#36861;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#32452;&#20214;&#28608;&#27963;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#32844;&#19994;&#20195;&#35789;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#22522;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;LSDM&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.14409</link><description>&lt;p&gt;
&#23450;&#20301;&#21644;&#20943;&#36731;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Locating and Mitigating Gender Bias in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#23450;&#20301;&#21644;&#20943;&#36731;&#20559;&#35265;&#36807;&#31243;&#34701;&#20837;&#32479;&#19968;&#26694;&#26550;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#36861;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#32452;&#20214;&#28608;&#27963;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#20943;&#36731;&#32844;&#19994;&#20195;&#35789;&#20013;&#24615;&#21035;&#20559;&#35265;&#30340;&#22522;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;LSDM&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#24191;&#27867;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#23398;&#20064;&#20107;&#23454;&#21644;&#20154;&#31867;&#35748;&#30693;&#65292;&#20854;&#20013;&#21253;&#21547;&#20154;&#31867;&#20559;&#22909;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#36807;&#31243;&#21487;&#33021;&#26080;&#24847;&#20013;&#23548;&#33268;&#36825;&#20123;&#27169;&#22411;&#33719;&#24471;&#31038;&#20250;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#20559;&#35265;&#21644;&#21051;&#26495;&#21360;&#35937;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#36890;&#36807;&#21333;&#19968;&#35270;&#35282;&#22788;&#29702;&#20559;&#35265;&#38382;&#39064;&#65292;&#38598;&#20013;&#20110;&#23450;&#20301;&#25110;&#20943;&#36731;&#20559;&#35265;&#12290;&#36825;&#31181;&#26377;&#38480;&#30340;&#35270;&#35282;&#22312;&#20419;&#36827;&#20559;&#35265;&#30740;&#31350;&#26041;&#38754;&#36896;&#25104;&#20102;&#38556;&#30861;&#65292;&#26080;&#27861;&#21327;&#21516;&#20114;&#34917;&#24182;&#36880;&#27493;&#31215;&#32047;&#32463;&#39564;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#23450;&#20301;&#21644;&#20943;&#36731;&#20559;&#35265;&#30340;&#36807;&#31243;&#34701;&#20837;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#20869;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20351;&#29992;&#22240;&#26524;&#20013;&#20171;&#20998;&#26512;&#26469;&#36319;&#36394;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19981;&#21516;&#32452;&#20214;&#28608;&#27963;&#30340;&#22240;&#26524;&#25928;&#24212;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;LSDM&#65288;&#26368;&#23567;&#20108;&#20056;&#20943;&#20559;&#26041;&#27861;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#32534;&#36753;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20943;&#36731;&#32844;&#19994;&#20195;&#35789;&#20013;&#30340;&#24615;&#21035;&#20559;&#35265;&#65292;&#21644;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14409v1 Announce Type: cross  Abstract: Large language models(LLM) are pre-trained on extensive corpora to learn facts and human cognition which contain human preferences. However, this process can inadvertently lead to these models acquiring biases and stereotypes prevalent in society. Prior research has typically tackled the issue of bias through a one-dimensional perspective, concentrating either on locating or mitigating it. This limited perspective has created obstacles in facilitating research on bias to synergistically complement and progressively build upon one another. In this study, we integrate the processes of locating and mitigating bias within a unified framework. Initially, we use causal mediation analysis to trace the causal effects of different components' activation within a large language model. Building on this, we propose the LSDM (Least Square Debias Method), a knowledge-editing based method for mitigating gender bias in occupational pronouns, and compa
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;QA&#26694;&#26550;&#65292;&#26681;&#25454;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#21160;&#24577;&#36873;&#25321;&#36866;&#21512;&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22238;&#31572;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14403</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#36890;&#36807;&#38382;&#39064;&#22797;&#26434;&#24230;&#23398;&#20064;&#35843;&#36866;
&lt;/p&gt;
&lt;p&gt;
Adaptive-RAG: Learning to Adapt Retrieval-Augmented Large Language Models through Question Complexity
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14403
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;QA&#26694;&#26550;&#65292;&#26681;&#25454;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#21160;&#24577;&#36873;&#25321;&#36866;&#21512;&#30340;&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#31574;&#30053;&#65292;&#25552;&#39640;&#20102;&#22238;&#31572;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#32034;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23558;&#22806;&#37096;&#30693;&#35782;&#24211;&#20013;&#30340;&#38750;&#21442;&#25968;&#30693;&#35782;&#32435;&#20837;LLMs&#65292;&#24050;&#25104;&#20026;&#25552;&#39640;&#22810;&#31181;&#20219;&#21153;&#20013;&#22238;&#31572;&#20934;&#30830;&#24615;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#65292;&#22914;&#38382;&#31572;&#65288;QA&#65289;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#21508;&#31181;&#26041;&#27861;&#22788;&#29702;&#19981;&#21516;&#22797;&#26434;&#24230;&#30340;&#26597;&#35810;&#65292;&#20294;&#23427;&#20204;&#35201;&#20040;&#22788;&#29702;&#31616;&#21333;&#26597;&#35810;&#26102;&#20135;&#29983;&#19981;&#24517;&#35201;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#35201;&#20040;&#26410;&#33021;&#20805;&#20998;&#35299;&#20915;&#22797;&#26434;&#30340;&#22810;&#27493;&#26597;&#35810;&#65307;&#28982;&#32780;&#65292;&#24182;&#38750;&#25152;&#26377;&#29992;&#25143;&#35831;&#27714;&#37117;&#21482;&#33021;&#21010;&#20998;&#20026;&#31616;&#21333;&#25110;&#22797;&#26434;&#31867;&#21035;&#20013;&#30340;&#19968;&#31181;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;QA&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21487;&#20197;&#21160;&#24577;&#36873;&#25321;&#20174;&#26368;&#31616;&#21333;&#21040;&#26368;&#22797;&#26434;&#30340;&#65288;&#26816;&#32034;&#22686;&#24378;&#65289;LLMs&#31574;&#30053;&#65292;&#36825;&#21462;&#20915;&#20110;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#12290;&#27492;&#22806;&#65292;&#36825;&#20010;&#36873;&#25321;&#36807;&#31243;&#26159;&#36890;&#36807;&#19968;&#20010;&#20998;&#31867;&#22120;&#23454;&#29616;&#30340;&#65292;&#35813;&#20998;&#31867;&#22120;&#26159;&#19968;&#20010;&#36739;&#23567;&#30340;LM&#65292;&#35757;&#32451;&#20197;&#39044;&#27979;&#20256;&#20837;&#26597;&#35810;&#30340;&#22797;&#26434;&#24230;&#32423;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14403v1 Announce Type: cross  Abstract: Retrieval-Augmented Large Language Models (LLMs), which incorporate the non-parametric knowledge from external knowledge bases into LLMs, have emerged as a promising approach to enhancing response accuracy in several tasks, such as Question-Answering (QA). However, even though there are various approaches dealing with queries of different complexities, they either handle simple queries with unnecessary computational overhead or fail to adequately address complex multi-step queries; yet, not all user requests fall into only one of the simple or complex categories. In this work, we propose a novel adaptive QA framework, that can dynamically select the most suitable strategy for (retrieval-augmented) LLMs from the simplest to the most sophisticated ones based on the query complexity. Also, this selection process is operationalized with a classifier, which is a smaller LM trained to predict the complexity level of incoming queries with aut
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#35774;&#35745;&#20004;&#38454;&#27573;&#24494;&#35843;&#31639;&#27861;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;LLMs&#36981;&#24490;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#32763;&#35793;&#26041;&#21521;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14399</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#24863;&#30693;&#35843;&#33410;&#30340;&#31934;&#30830;&#32763;&#35793;&#23450;&#21046;LLMs&#30340;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Building Accurate Translation-Tailored LLMs with Language Aware Instruction Tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14399
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#35774;&#35745;&#20004;&#38454;&#27573;&#24494;&#35843;&#31639;&#27861;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;LLMs&#36981;&#24490;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#32763;&#35793;&#26041;&#21521;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32763;&#35793;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#23637;&#29616;&#20986;&#20986;&#33394;&#30340;&#32763;&#35793;&#33021;&#21147;&#65292;&#29978;&#33267;&#33021;&#19982;&#21830;&#19994;&#32763;&#35793;&#31995;&#32479;&#30456;&#23218;&#32654;&#12290;&#28982;&#32780;&#65292;&#35823;&#32763;&#35793;&#38382;&#39064;&#20173;&#28982;&#23384;&#22312;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20302;&#36164;&#28304;&#35821;&#35328;&#65292;&#38459;&#30861;&#25105;&#20204;&#24320;&#21457;&#20934;&#30830;&#30340;&#22522;&#20110;LLMs&#30340;&#32763;&#35793;&#27169;&#22411;&#12290;&#20026;&#20102;&#32531;&#35299;&#35823;&#32763;&#35793;&#38382;&#39064;&#24182;&#25552;&#21319;LLMs&#22312;&#32763;&#35793;&#19978;&#30340;&#24615;&#33021;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#35201;&#20040;&#35774;&#35745;&#20102;&#20808;&#36827;&#30340;&#25552;&#31034;&#31574;&#30053;&#20197;&#31361;&#20986;&#32763;&#35793;&#25351;&#20196;&#30340;&#21151;&#33021;&#65292;&#35201;&#20040;&#21033;&#29992;LLMs&#30340;&#29616;&#22330;&#23398;&#20064;&#33021;&#21147;&#36890;&#36807;&#23569;&#37327;&#31034;&#33539;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22522;&#26412;&#19978;&#27809;&#26377;&#25552;&#39640;LLMs&#36981;&#24490;&#32763;&#35793;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#35821;&#35328;&#26041;&#21521;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#24494;&#35843;&#31639;&#27861;&#26469;&#25552;&#39640;LLMs&#30340;&#25351;&#20196;&#36981;&#24490;&#33021;&#21147;&#65288;&#29305;&#21035;&#26159;&#32763;&#35793;&#26041;&#21521;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14399v1 Announce Type: cross  Abstract: Translation-tailored Large language models (LLMs) exhibit remarkable translation capabilities, even competing with supervised-trained commercial translation systems. However, off-target translation remains an unsolved problem, especially for low-resource languages, hindering us from developing accurate LLMs-based translation models. To mitigate the off-target translation problem and enhance the performance of LLMs on translation, recent works have either designed advanced prompting strategies to highlight the functionality of translation instructions or exploited the in-context learning ability of LLMs by feeding few-shot demonstrations. However, these methods essentially do not improve LLM's ability to follow translation instructions, especially the language direction information. In this work, we design a two-stage fine-tuning algorithm to improve the instruction-following ability (especially the translation direction) of LLMs. Speci
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;PSPEM&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#21069;&#32512;&#25552;&#31034;&#26469;&#32534;&#36753;&#35821;&#35328;Lodel&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20013;&#30340;&#20302;&#25928;&#24615;&#12289;&#36890;&#29992;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#25552;&#31034;&#24037;&#31243;&#30340;&#19981;&#36879;&#26126;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14381</link><description>&lt;p&gt;
&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#21069;&#32512;&#25552;&#31034;&#26469;&#32534;&#36753;&#35821;&#35328;Lodel&#30340;&#30693;&#35782;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Editing Knowledge Representation of Language Lodel via Rephrased Prefix Prompts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14381
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;PSPEM&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#34920;&#36848;&#21069;&#32512;&#25552;&#31034;&#26469;&#32534;&#36753;&#35821;&#35328;Lodel&#30340;&#30693;&#35782;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20013;&#30340;&#20302;&#25928;&#24615;&#12289;&#36890;&#29992;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#25552;&#31034;&#24037;&#31243;&#30340;&#19981;&#36879;&#26126;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#24050;&#22312;&#24191;&#27867;&#30340;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#22521;&#35757;&#65292;&#20197;&#23384;&#20648;&#20851;&#20110;&#25991;&#26412;&#25551;&#36848;&#30340;&#19990;&#30028;&#21508;&#20010;&#26041;&#38754;&#30340;&#20107;&#23454;&#30693;&#35782;&#12290;&#24403;&#21069;&#25216;&#26415;&#36890;&#24120;&#37319;&#29992;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#25110;&#29305;&#23450;&#25552;&#31034;&#26469;&#20462;&#25913;LM&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#25104;&#26412;&#39640;&#26114;&#19988;&#20302;&#25928;&#65292;&#38590;&#20197;&#20135;&#29983;&#36866;&#24403;&#30340;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;&#25552;&#31034;&#24037;&#31243;&#26159;&#19981;&#36879;&#26126;&#30340;&#65292;&#38656;&#35201;&#22823;&#37327;&#21162;&#21147;&#25214;&#21040;&#21512;&#36866;&#30340;&#25552;&#31034;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;PSPEM&#65288;&#21069;&#32512;&#36719;&#25552;&#31034;&#32534;&#36753;&#26041;&#27861;&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20165;&#36890;&#36807;&#19968;&#27425;&#35757;&#32451;&#32780;&#32456;&#36523;&#20351;&#29992;&#12290;&#23427;&#35299;&#20915;&#20102;&#30693;&#35782;&#32534;&#36753;&#26041;&#27861;&#20013;&#30340;&#20302;&#25928;&#24615;&#21644;&#36890;&#29992;&#24615;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#33258;&#21160;&#23547;&#25214;&#26368;&#20339;&#36719;&#25552;&#31034;&#26469;&#20811;&#26381;&#25552;&#31034;&#24037;&#31243;&#30340;&#19981;&#36879;&#26126;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;PSPEM&#21033;&#29992;&#25552;&#31034;&#32534;&#30721;&#22120;&#21644;&#32534;&#30721;&#36716;&#25442;&#22120;&#26469;&#31934;&#28860;&#25552;&#31034;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#25552;&#31034;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14381v1 Announce Type: cross  Abstract: Neural language models (LMs) have been extensively trained on vast corpora to store factual knowledge about various aspects of the world described in texts. Current technologies typically employ knowledge editing methods or specific prompts to modify LM outputs. However, existing knowledge editing methods are costly and inefficient, struggling to produce appropriate text. Additionally, prompt engineering is opaque and requires significant effort to find suitable prompts. To address these issues, we introduce a new method called PSPEM (Prefix Soft Prompt Editing Method), that can be used for a lifetime with just one training. It resolves the inefficiencies and generalizability issues in knowledge editing methods and overcomes the opacity of prompt engineering by automatically seeking optimal soft prompts. Specifically, PSPEM utilizes a prompt encoder and an encoding converter to refine key information in prompts and uses prompt alignmen
&lt;/p&gt;</description></item><item><title>&#24490;&#29615;&#25913;&#36827;&#65288;LI&#65289;&#26159;&#19968;&#31181;&#26080;&#38656;&#20013;&#22830;&#26381;&#21153;&#22120;&#25110;&#25968;&#25454;&#20132;&#25442;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21487;&#25552;&#39640;&#25968;&#25454;&#24322;&#36136;&#24615;&#19979;&#30340;&#29305;&#24449;&#25552;&#21462;&#25928;&#29575;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#20808;&#36827;&#31639;&#27861;FedALA&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#21644;&#20840;&#23616;&#27169;&#22411;&#29615;&#22659;&#12290;</title><link>https://arxiv.org/abs/2403.14371</link><description>&lt;p&gt;
&#24490;&#29615;&#25913;&#36827;&#65306;&#19968;&#31181;&#20174;&#24322;&#26500;&#25968;&#25454;&#20013;&#25552;&#21462;&#20849;&#20139;&#29305;&#24449;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#26080;&#38656;&#20013;&#22830;&#26381;&#21153;&#22120;
&lt;/p&gt;
&lt;p&gt;
Loop Improvement: An Efficient Approach for Extracting Shared Features from Heterogeneous Data without Central Server
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14371
&lt;/p&gt;
&lt;p&gt;
&#24490;&#29615;&#25913;&#36827;&#65288;LI&#65289;&#26159;&#19968;&#31181;&#26080;&#38656;&#20013;&#22830;&#26381;&#21153;&#22120;&#25110;&#25968;&#25454;&#20132;&#25442;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21487;&#25552;&#39640;&#25968;&#25454;&#24322;&#36136;&#24615;&#19979;&#30340;&#29305;&#24449;&#25552;&#21462;&#25928;&#29575;&#65292;&#34920;&#29616;&#20248;&#36234;&#20110;&#20808;&#36827;&#31639;&#27861;FedALA&#65292;&#24182;&#21487;&#24212;&#29992;&#20110;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#21644;&#20840;&#23616;&#27169;&#22411;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#65292;&#25968;&#25454;&#30340;&#24322;&#36136;&#24615;&#26174;&#33879;&#24433;&#21709;&#24615;&#33021;&#12290;&#19968;&#31181;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#23558;&#36825;&#20123;&#21442;&#25968;&#20998;&#20026;&#20849;&#20139;&#21644;&#20010;&#24615;&#21270;&#32452;&#20214;&#65292;&#36825;&#20010;&#27010;&#24565;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#20063;&#24456;&#37325;&#35201;&#12290;&#38024;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#24490;&#29615;&#25913;&#36827;&#8221;&#65288;LI&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#22686;&#24378;&#20102;&#36825;&#31181;&#20998;&#31163;&#21644;&#29305;&#24449;&#25552;&#21462;&#65292;&#32780;&#19981;&#38656;&#35201;&#20013;&#22830;&#26381;&#21153;&#22120;&#25110;&#21442;&#19982;&#32773;&#20043;&#38388;&#30340;&#25968;&#25454;&#20132;&#25442;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#26174;&#31034;&#65292;&#22312;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#29615;&#22659;&#20013;&#65292;LI&#22312;&#20934;&#30830;&#24615;&#26041;&#38754;&#22987;&#32456;&#20248;&#20110;&#20808;&#36827;&#30340;FedALA&#31639;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;LI&#30340;&#29305;&#24449;&#25552;&#21462;&#22120;&#19982;&#32858;&#21512;&#25152;&#26377;&#23458;&#25143;&#31471;&#25968;&#25454;&#26102;&#23454;&#29616;&#30340;&#24615;&#33021;&#30456;&#21305;&#37197;&#12290;&#22312;&#20840;&#23616;&#27169;&#22411;&#29615;&#22659;&#20013;&#65292;&#20351;&#29992;&#20855;&#26377;&#22534;&#21472;&#20010;&#24615;&#21270;&#23618;&#21644;&#39069;&#22806;&#32593;&#32476;&#30340;LI&#20063;&#20135;&#29983;&#20102;&#19982;&#21512;&#24182;&#23458;&#25143;&#31471;&#25968;&#25454;&#22330;&#26223;&#30456;&#24403;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;LI&#30340;&#36866;&#24212;&#33021;&#21147;&#24310;&#23637;&#21040;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14371v1 Announce Type: cross  Abstract: In federated learning, data heterogeneity significantly impacts performance. A typical solution involves segregating these parameters into shared and personalized components, a concept also relevant in multi-task learning. Addressing this, we propose "Loop Improvement" (LI), a novel method enhancing this separation and feature extraction without necessitating a central server or data interchange among participants. Our experiments reveal LI's superiority in several aspects: In personalized federated learning environments, LI consistently outperforms the advanced FedALA algorithm in accuracy across diverse scenarios. Additionally, LI's feature extractor closely matches the performance achieved when aggregating data from all clients. In global model contexts, employing LI with stacked personalized layers and an additional network also yields comparable results to combined client data scenarios. Furthermore, LI's adaptability extends to m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20219;&#21153;&#35774;&#35745;&#21644;&#23454;&#39564;&#32771;&#23519;&#20102;&#20854;&#23545;&#19981;&#21516;&#22270;&#32467;&#26500;&#35268;&#21017;&#30340;&#29702;&#35299;&#12289;&#25429;&#33719;&#32467;&#26500;&#31867;&#22411;&#20998;&#24067;&#30340;&#33021;&#21147;&#20197;&#21450;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#23646;&#24615;&#30340;&#22270;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.14358</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Potential of Large Language Models in Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14358
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#22270;&#29983;&#25104;&#20013;&#30340;&#28508;&#21147;&#65292;&#36890;&#36807;&#20219;&#21153;&#35774;&#35745;&#21644;&#23454;&#39564;&#32771;&#23519;&#20102;&#20854;&#23545;&#19981;&#21516;&#22270;&#32467;&#26500;&#35268;&#21017;&#30340;&#29702;&#35299;&#12289;&#25429;&#33719;&#32467;&#26500;&#31867;&#22411;&#20998;&#24067;&#30340;&#33021;&#21147;&#20197;&#21450;&#21033;&#29992;&#39046;&#22495;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#23646;&#24615;&#30340;&#22270;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#25506;&#32034;LLMs&#29992;&#20110;&#22270;&#21028;&#21035;&#20219;&#21153;&#65292;&#22914;&#33410;&#28857;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#22270;&#29983;&#25104;&#26041;&#38754;&#30340;&#33021;&#21147;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#34987;&#25506;&#32034;&#12290;&#22270;&#29983;&#25104;&#35201;&#27714;LLM&#29983;&#25104;&#20855;&#26377;&#29305;&#23450;&#23646;&#24615;&#30340;&#22270;&#65292;&#36825;&#22312;&#33647;&#29289;&#21457;&#29616;&#31561;&#26377;&#20215;&#20540;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#20063;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LLM4GraphGen&#26469;&#25506;&#32034;LLMs&#36827;&#34892;&#22270;&#29983;&#25104;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#31995;&#32479;&#24615;&#20219;&#21153;&#35774;&#35745;&#21644;&#22823;&#37327;&#23454;&#39564;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#20010;&#20219;&#21153;&#65292;&#24182;&#36827;&#34892;&#20102;&#20840;&#38754;&#23454;&#39564;&#65292;&#20197;&#35299;&#20915;&#26377;&#20851;LLMs&#23545;&#19981;&#21516;&#22270;&#32467;&#26500;&#35268;&#21017;&#30340;&#29702;&#35299;&#12289;&#25429;&#33719;&#32467;&#26500;&#31867;&#22411;&#20998;&#24067;&#30340;&#33021;&#21147;&#20197;&#21450;&#21033;&#29992;&#22495;&#30693;&#35782;&#36827;&#34892;&#22522;&#20110;&#23646;&#24615;&#30340;&#22270;&#29983;&#25104;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;LLMs&#65292;&#29305;&#21035;&#26159;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14358v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved great success in many fields, and recent works have studied exploring LLMs for graph discriminative tasks such as node classification. However, the abilities of LLMs for graph generation remain unexplored in the literature. Graph generation requires the LLM to generate graphs with given properties, which has valuable real-world applications such as drug discovery, while tends to be more challenging. In this paper, we propose LLM4GraphGen to explore the ability of LLMs for graph generation with systematical task designs and extensive experiments. Specifically, we propose several tasks tailored with comprehensive experiments to address key questions regarding LLMs' understanding of different graph structure rules, their ability to capture structural type distributions, and their utilization of domain knowledge for property-based graph generation. Our evaluations demonstrate that LLMs, particular
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;GA^2E&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#29983;&#25104;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#21333;&#20010;&#35757;&#32451;&#38454;&#27573;&#20013;&#23454;&#29616;&#20102;&#22270;&#29983;&#25104;&#12289;&#22270;&#21028;&#21035;&#21644;&#22270;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.14340</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#26041;&#27861;&#25506;&#32034;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#30340;&#20219;&#21153;&#32479;&#19968;&#21270;
&lt;/p&gt;
&lt;p&gt;
Exploring Task Unification in Graph Representation Learning via Generative Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14340
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;GA^2E&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#29983;&#25104;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#22312;&#21333;&#20010;&#35757;&#32451;&#38454;&#27573;&#20013;&#23454;&#29616;&#20102;&#22270;&#29983;&#25104;&#12289;&#22270;&#21028;&#21035;&#21644;&#22270;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#26080;&#22788;&#19981;&#22312;&#65292;&#24182;&#28085;&#30422;&#20102;&#20174;&#33410;&#28857;&#32423;&#12289;&#36793;&#32423;&#21644;&#22270;&#32423;&#20219;&#21153;&#21040;&#36801;&#31227;&#23398;&#20064;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#31181;&#31867;&#22411;&#30340;&#22270;&#25968;&#25454;&#35774;&#35745;&#29305;&#23450;&#20219;&#21153;&#36890;&#24120;&#20195;&#20215;&#39640;&#26114;&#19988;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#8220;&#39044;&#35757;&#32451;+&#24494;&#35843;&#8221;&#25110;&#8220;&#39044;&#35757;&#32451;+&#25552;&#31034;&#8221;&#33539;&#24335;&#65292;&#26088;&#22312;&#35774;&#35745;&#19968;&#20010;&#33021;&#22815;&#27867;&#21270;&#22810;&#31181;&#22270;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#22312;&#36825;&#20123;&#26041;&#27861;&#20013;&#65292;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;GAEs&#65289;&#12289;&#29983;&#25104;&#33258;&#30417;&#30563;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#26377;&#25928;&#35299;&#20915;&#21508;&#31181;&#22270;&#20219;&#21153;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#22810;&#38454;&#27573;&#35757;&#32451;&#24182;&#38656;&#35201;&#33258;&#36866;&#24212;&#35774;&#35745;&#65292;&#36825;&#19968;&#26041;&#38754;&#20351;&#24471;&#23558;&#20854;&#26080;&#32541;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#22270;&#20219;&#21153;&#21464;&#24471;&#22256;&#38590;&#65292;&#21478;&#19968;&#26041;&#38754;&#24573;&#30053;&#20102;&#19981;&#21516;&#38454;&#27573;&#20219;&#21153;&#30446;&#26631;&#20043;&#38388;&#30340;&#24046;&#24322;&#36896;&#25104;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GA^2E&#65292;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32479;&#19968;&#30340;&#29983;&#25104;&#24335;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#35757;&#32451;&#38454;&#27573;&#20013;&#21516;&#26102;&#23454;&#29616;&#22270;&#29983;&#25104;&#12289;&#22270;&#21028;&#21035;&#21644;&#22270;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14340v1 Announce Type: cross  Abstract: Graphs are ubiquitous in real-world scenarios and encompass a diverse range of tasks, from node-, edge-, and graph-level tasks to transfer learning. However, designing specific tasks for each type of graph data is often costly and lacks generalizability. Recent endeavors under the "Pre-training + Fine-tuning" or "Pre-training + Prompt" paradigms aim to design a unified framework capable of generalizing across multiple graph tasks. Among these, graph autoencoders (GAEs), generative self-supervised models, have demonstrated their potential in effectively addressing various graph tasks. Nevertheless, these methods typically employ multi-stage training and require adaptive designs, which on one hand make it difficult to be seamlessly applied to diverse graph tasks and on the other hand overlook the negative impact caused by discrepancies in task objectives between the different stages. To address these challenges, we propose GA^2E, a unifi
&lt;/p&gt;</description></item><item><title>$\nabla \tau$ &#26159;&#19968;&#31181;&#26088;&#22312;&#39640;&#25928;&#28040;&#38500;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#30340;&#26426;&#22120;&#36951;&#24536;&#20248;&#21270;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.14339</link><description>&lt;p&gt;
$\nabla \tau$: &#22522;&#20110;&#26799;&#24230;&#19988;&#20219;&#21153;&#26080;&#20851;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
$\nabla \tau$: Gradient-based and Task-Agnostic machine Unlearning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14339
&lt;/p&gt;
&lt;p&gt;
$\nabla \tau$ &#26159;&#19968;&#31181;&#26088;&#22312;&#39640;&#25928;&#28040;&#38500;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#30340;&#26426;&#22120;&#36951;&#24536;&#20248;&#21270;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#31181;&#26377;&#36873;&#25321;&#24615;&#22320;&#28040;&#38500;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#20013;&#26576;&#20123;&#25968;&#25454;&#31034;&#20363;&#24433;&#21709;&#30340;&#36807;&#31243;&#65292;&#20316;&#20026;&#20174;&#19994;&#32773;&#36981;&#23432;&#26368;&#36817;&#30340;&#25968;&#25454;&#20445;&#25252;&#27861;&#35268;&#30340;&#25163;&#27573;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#26174;&#33879;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#36951;&#24536;&#26041;&#27861;&#38754;&#20020;&#30528;&#20851;&#38190;&#32570;&#28857;&#65292;&#21253;&#25324;&#20854;&#25104;&#26412;&#36807;&#39640;&#65292;&#36890;&#24120;&#19982;&#22823;&#37327;&#36229;&#21442;&#25968;&#30456;&#20851;&#65292;&#20197;&#21450;&#20165;&#24536;&#35760;&#30456;&#23545;&#36739;&#23567;&#25968;&#25454;&#37096;&#20998;&#30340;&#38480;&#21046;&#12290;&#36825;&#32463;&#24120;&#23548;&#33268;&#20174;&#22836;&#24320;&#22987;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#25104;&#20026;&#26356;&#24555;&#36895;&#21644;&#26356;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#26799;&#24230;&#19988;&#20219;&#21153;&#26080;&#20851;&#30340;&#26426;&#22120;&#36951;&#24536;&#65288;$\nabla \tau$&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26088;&#22312;&#39640;&#25928;&#28040;&#38500;&#37096;&#20998;&#35757;&#32451;&#25968;&#25454;&#24433;&#21709;&#30340;&#20248;&#21270;&#26694;&#26550;&#12290;&#23427;&#23545;&#24453;&#36951;&#24536;&#30340;&#25968;&#25454;&#24212;&#29992;&#33258;&#36866;&#24212;&#26799;&#24230;&#19978;&#21319;&#65292;&#21516;&#26102;&#23545;&#20854;&#20313;&#25968;&#25454;&#20351;&#29992;&#26631;&#20934;&#26799;&#24230;&#19979;&#38477;&#12290;$\nabla \tau$&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#25552;&#20379;&#20102;&#22810;&#31181;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14339v1 Announce Type: cross  Abstract: Machine Unlearning, the process of selectively eliminating the influence of certain data examples used during a model's training, has gained significant attention as a means for practitioners to comply with recent data protection regulations. However, existing unlearning methods face critical drawbacks, including their prohibitively high cost, often associated with a large number of hyperparameters, and the limitation of forgetting only relatively small data portions. This often makes retraining the model from scratch a quicker and more effective solution. In this study, we introduce Gradient-based and Task-Agnostic machine Unlearning ($\nabla \tau$), an optimization framework designed to remove the influence of a subset of training data efficiently. It applies adaptive gradient ascent to the data to be forgotten while using standard gradient descent for the remaining data. $\nabla \tau$ offers multiple benefits over existing approache
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26799;&#24230;&#25552;&#21319;&#26426;&#21644;&#31526;&#21495;&#22238;&#24402;&#31561;&#25216;&#26415;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#36716;&#21270;&#20026;&#26356;&#21487;&#35299;&#37322;&#30340;&#24418;&#24335;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#31574;&#30053;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14328</link><description>&lt;p&gt;
&#23558;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#25552;&#28860;&#20026;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#20154;&#36816;&#21160;&#65306;&#26799;&#24230;&#25552;&#21319;&#26426;&#21644;&#31526;&#21495;&#22238;&#24402;
&lt;/p&gt;
&lt;p&gt;
Distilling Reinforcement Learning Policies for Interpretable Robot Locomotion: Gradient Boosting Machines and Symbolic Regression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14328
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26799;&#24230;&#25552;&#21319;&#26426;&#21644;&#31526;&#21495;&#22238;&#24402;&#31561;&#25216;&#26415;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#36716;&#21270;&#20026;&#26356;&#21487;&#35299;&#37322;&#30340;&#24418;&#24335;&#65292;&#25552;&#39640;&#20102;&#26426;&#22120;&#20154;&#36816;&#21160;&#31574;&#30053;&#30340;&#36879;&#26126;&#24230;&#21644;&#21487;&#29702;&#35299;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#21457;&#23637;&#20351;&#26426;&#22120;&#20154;&#36816;&#21160;&#33021;&#21147;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;RL&#31574;&#30053;&#30340;&#22797;&#26434;&#24615;&#21644;&#8220;&#40657;&#21283;&#23376;&#8221;&#29305;&#24615;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#26356;&#24191;&#27867;&#30340;&#25509;&#21463;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#35201;&#27714;&#39640;&#27700;&#24179;&#23433;&#20840;&#24615;&#21644;&#21487;&#38752;&#24615;&#30340;&#24212;&#29992;&#20013;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#23558;&#31070;&#32463;RL&#31574;&#30053;&#25552;&#28860;&#20026;&#26356;&#21487;&#35299;&#37322;&#24418;&#24335;&#30340;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#26799;&#24230;&#25552;&#21319;&#26426;&#65288;GBMs&#65289;&#12289;&#21487;&#35299;&#37322;&#25552;&#21319;&#26426;&#65288;EBMs&#65289;&#21644;&#31526;&#21495;&#22238;&#24402;&#12290;&#36890;&#36807;&#21033;&#29992;&#24191;&#20041;&#21152;&#27861;&#27169;&#22411;&#12289;&#20915;&#31574;&#26641;&#21644;&#20998;&#26512;&#34920;&#36798;&#24335;&#30340;&#22266;&#26377;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#23558;&#19981;&#36879;&#26126;&#30340;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#36716;&#21270;&#20026;&#26356;&#36879;&#26126;&#30340;&#8220;&#29627;&#29827;&#31665;&#8221;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;RL&#35757;&#32451;&#19987;&#23478;&#31070;&#32463;&#32593;&#32476;&#31574;&#30053;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#25552;&#28860;&#20026;(i) GBMs&#12289;(ii) EBMs&#21644;(iii)&#31526;&#21495;&#31574;&#30053;&#12290;&#20026;&#20102;&#35299;&#20915;&#34892;&#20026;&#20998;&#24067;&#36716;&#31227;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14328v1 Announce Type: cross  Abstract: Recent advancements in reinforcement learning (RL) have led to remarkable achievements in robot locomotion capabilities. However, the complexity and ``black-box'' nature of neural network-based RL policies hinder their interpretability and broader acceptance, particularly in applications demanding high levels of safety and reliability. This paper introduces a novel approach to distill neural RL policies into more interpretable forms using Gradient Boosting Machines (GBMs), Explainable Boosting Machines (EBMs) and Symbolic Regression. By leveraging the inherent interpretability of generalized additive models, decision trees, and analytical expressions, we transform opaque neural network policies into more transparent ``glass-box'' models. We train expert neural network policies using RL and subsequently distill them into (i) GBMs, (ii) EBMs, and (iii) symbolic policies. To address the inherent distribution shift challenge of behavioral 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#30417;&#30563;&#23398;&#20064;&#24039;&#22937;&#30340;&#36275;&#29699;&#25805;&#32437;&#26041;&#27861;&#65292;&#21033;&#29992;&#21453;&#39304;&#25511;&#21046;&#27169;&#22359;&#26469;&#35745;&#31639;&#24517;&#35201;&#30340;&#25972;&#20307;&#36816;&#21160;&#24182;&#36827;&#34892;&#21160;&#24577;&#20851;&#33410;&#32423;&#36816;&#21160;&#30417;&#30563;&#65292;&#21516;&#26102;&#25913;&#36827;&#20102;&#29699;&#20307;&#21160;&#24577;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;-&#36741;&#21161;&#20272;&#35745;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.14300</link><description>&lt;p&gt;
DexDribbler: &#36890;&#36807;&#21160;&#24577;&#30417;&#30563;&#23398;&#20064;&#24039;&#22937;&#30340;&#36275;&#29699;&#25805;&#32437;
&lt;/p&gt;
&lt;p&gt;
DexDribbler: Learning Dexterous Soccer Manipulation via Dynamic Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14300
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21160;&#24577;&#30417;&#30563;&#23398;&#20064;&#24039;&#22937;&#30340;&#36275;&#29699;&#25805;&#32437;&#26041;&#27861;&#65292;&#21033;&#29992;&#21453;&#39304;&#25511;&#21046;&#27169;&#22359;&#26469;&#35745;&#31639;&#24517;&#35201;&#30340;&#25972;&#20307;&#36816;&#21160;&#24182;&#36827;&#34892;&#21160;&#24577;&#20851;&#33410;&#32423;&#36816;&#21160;&#30417;&#30563;&#65292;&#21516;&#26102;&#25913;&#36827;&#20102;&#29699;&#20307;&#21160;&#24577;&#27169;&#22411;&#21644;&#19978;&#19979;&#25991;-&#36741;&#21161;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#22235;&#32930;&#26426;&#22120;&#20154;&#30340;&#28789;&#24039;&#36816;&#21160;&#31574;&#30053;&#22240;&#20854;&#22788;&#29702;&#22810;&#26679;&#21270;&#22320;&#24418;&#24182;&#31867;&#20284;&#20110;&#26234;&#33021;&#34892;&#20026;&#30340;&#33021;&#21147;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#22312;&#23398;&#20064;&#31038;&#21306;&#20013;&#65292;&#23545;&#20110;&#36816;&#21160;&#29289;&#20307;&#30340;&#20851;&#33410;&#25805;&#32437;&#21644;&#36275;&#29699;&#31561;&#36816;&#21160;&#30340;&#20851;&#33410;&#25805;&#32437;&#19982;&#36275;&#37096;&#36816;&#21160;&#30340;&#32452;&#21512;&#65292;&#25509;&#21463;&#30340;&#20851;&#27880;&#21364;&#24456;&#23569;&#65292;&#34429;&#28982;&#36825;&#23545;&#20110;&#20154;&#31867;&#21644;&#32874;&#26126;&#21160;&#29289;&#26469;&#35828;&#26159;&#33258;&#28982;&#30340;&#12290;&#35299;&#20915;&#36825;&#19968;&#22810;&#20219;&#21153;&#38382;&#39064;&#30340;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#20174;&#25805;&#32437;&#29289;&#20307;&#30340;&#29366;&#24577;&#21644;&#30446;&#26631;&#20013;&#25512;&#26029;&#20986;&#36816;&#21160;&#30446;&#26631;&#12290;&#25805;&#32437;&#29289;&#20307;&#29366;&#24577;&#19982;&#26426;&#22120;&#20154;&#36816;&#21160;&#20043;&#38388;&#30340;&#38544;&#21547;&#20851;&#31995;&#21487;&#33021;&#24456;&#38590;&#30452;&#25509;&#20174;&#35757;&#32451;&#32463;&#39564;&#20013;&#25429;&#25417;&#12290;&#25105;&#20204;&#25552;&#20986;&#28155;&#21152;&#19968;&#20010;&#21453;&#39304;&#25511;&#21046;&#27169;&#22359;&#26469;&#20934;&#30830;&#35745;&#31639;&#24517;&#35201;&#30340;&#25972;&#20307;&#36816;&#21160;&#65292;&#24182;&#23558;&#36755;&#20986;&#20316;&#20026;&#21160;&#24577;&#20851;&#33410;&#32423;&#36816;&#21160;&#30417;&#30563;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#25913;&#36827;&#30340;&#29699;&#20307;&#21160;&#24577;&#27169;&#22411;&#65292;&#25193;&#23637;&#30340;&#19978;&#19979;&#25991;&#36741;&#21161;&#20272;&#35745;&#22120;&#20197;&#21450;&#32508;&#21512;&#30340;&#29699;&#20307;&#35266;&#23519;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14300v1 Announce Type: cross  Abstract: Learning dexterous locomotion policy for legged robots is becoming increasingly popular due to its ability to handle diverse terrains and resemble intelligent behaviors. However, joint manipulation of moving objects and locomotion with legs, such as playing soccer, receive scant attention in the learning community, although it is natural for humans and smart animals. A key challenge to solve this multitask problem is to infer the objectives of locomotion from the states and targets of the manipulated objects. The implicit relation between the object states and robot locomotion can be hard to capture directly from the training experience. We propose adding a feedback control block to compute the necessary body-level movement accurately and using the outputs as dynamic joint-level locomotion supervision explicitly. We further utilize an improved ball dynamic model, an extended context-aided estimator, and a comprehensive ball observer to
&lt;/p&gt;</description></item><item><title>&#22312;&#32447;&#31038;&#20132;&#20114;&#21160;&#20013;&#23384;&#22312;&#30528;&#23553;&#38381;&#24615;&#31038;&#21306;&#21644;&#22312;&#32447;&#25903;&#25345;&#22242;&#20307;&#20004;&#31181;&#38754;&#35980;&#65292;&#21463;&#21040;&#31639;&#27861;&#20559;&#35265;&#21644;&#21516;&#36136;&#24615;&#26497;&#31471;&#26426;&#21046;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.14298</link><description>&lt;p&gt;
&#20174;&#21361;&#38505;&#21040;&#21487;&#33021;&#24615;&#65306;&#29702;&#35299;&#20154;&#31867;&#65288;&#20197;&#21450;&#20154;&#24037;&#26234;&#33021;&#65289;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#22312;&#32447;&#35770;&#22363;
&lt;/p&gt;
&lt;p&gt;
From Perils to Possibilities: Understanding how Human (and AI) Biases affect Online Fora
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14298
&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#20114;&#21160;&#20013;&#23384;&#22312;&#30528;&#23553;&#38381;&#24615;&#31038;&#21306;&#21644;&#22312;&#32447;&#25903;&#25345;&#22242;&#20307;&#20004;&#31181;&#38754;&#35980;&#65292;&#21463;&#21040;&#31639;&#27861;&#20559;&#35265;&#21644;&#21516;&#36136;&#24615;&#26497;&#31471;&#26426;&#21046;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#26159;&#22312;&#32447;&#35770;&#22363;&#65292;&#22312;&#37027;&#37324;&#29992;&#25143;&#36827;&#34892;&#35752;&#35770;&#65292;&#20998;&#20139;&#20869;&#23481;&#65292;&#24314;&#31435;&#32852;&#31995;&#12290;&#26412;&#32508;&#36848;&#36890;&#36807;&#22312;&#32447;&#36777;&#35770;&#12289;&#22312;&#32447;&#25903;&#25345;&#21644;&#20154;&#24037;&#26234;&#33021;&#30456;&#20114;&#20316;&#29992;&#19977;&#20010;&#20851;&#38190;&#35270;&#35282;&#65292;&#25506;&#35752;&#31038;&#20132;&#20114;&#21160;&#12289;&#29992;&#25143;&#29983;&#25104;&#20869;&#23481;&#21644;&#20559;&#35265;&#30340;&#21160;&#24577;&#65292;&#36890;&#36807;&#22797;&#26434;&#32593;&#32476;&#20998;&#26512;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24037;&#20855;&#26469;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#30340;&#32972;&#26223;&#20013;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22312;&#32447;&#36777;&#35770;&#30340;&#29616;&#35937;&#65292;&#20854;&#20013;&#26497;&#21270;&#12289;&#38169;&#35823;&#20449;&#24687;&#21644;&#20449;&#24687;&#33575;&#25151;&#24418;&#25104;&#32463;&#24120;&#34067;&#24310;&#65292;&#21463;&#21040;&#31639;&#27861;&#20559;&#35265;&#21644;&#21516;&#36136;&#24615;&#26497;&#31471;&#26426;&#21046;&#30340;&#25512;&#21160;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#32447;&#25903;&#25345;&#23567;&#32452;&#30340;&#20986;&#29616;&#65292;&#36890;&#36807;&#29992;&#25143;&#30340;&#33258;&#25105;&#25259;&#38706;&#21644;&#31038;&#20250;&#25903;&#25345;&#26426;&#21046;&#12290;&#22312;&#32447;&#36777;&#35770;&#21644;&#25903;&#25345;&#26426;&#21046;&#22312;&#31038;&#20132;&#23186;&#20307;&#20013;&#21576;&#29616;&#20986;&#21361;&#38505;&#21644;&#21487;&#33021;&#24615;&#30340;&#20108;&#37325;&#24615;&#65307;&#20998;&#21106;&#31038;&#21306;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14298v1 Announce Type: cross  Abstract: Social media platforms are online fora where users engage in discussions, share content, and build connections. This review explores the dynamics of social interactions, user-generated contents, and biases within the context of social media analysis (analyzing works that use the tools offered by complex network analysis and natural language processing) through the lens of three key points of view: online debates, online support, and human-AI interactions. On the one hand, we delineate the phenomenon of online debates, where polarization, misinformation, and echo chamber formation often proliferate, driven by algorithmic biases and extreme mechanisms of homophily. On the other hand, we explore the emergence of online support groups through users' self-disclosure and social support mechanisms. Online debates and support mechanisms present a duality of both perils and possibilities within social media; perils of segregated communities and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22320;&#29699;&#35266;&#27979;&#24212;&#29992;&#20013;&#32570;&#22833;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38598;&#25104;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;100%&#30340;&#39044;&#27979;&#31283;&#20581;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#32570;&#22833;&#24773;&#26223;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27604;&#20998;&#31867;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#19988;&#20809;&#23398;&#35270;&#35282;&#26159;&#26368;&#20851;&#38190;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.14297</link><description>&lt;p&gt;
&#22320;&#29699;&#35266;&#27979;&#24212;&#29992;&#20013;&#32570;&#22833;&#25968;&#25454;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Impact Assessment of Missing Data in Model Predictions for Earth Observation Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14297
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22320;&#29699;&#35266;&#27979;&#24212;&#29992;&#20013;&#32570;&#22833;&#25968;&#25454;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#38598;&#25104;&#31574;&#30053;&#21487;&#20197;&#23454;&#29616;&#39640;&#36798;100%&#30340;&#39044;&#27979;&#31283;&#20581;&#24615;&#65292;&#21516;&#26102;&#25581;&#31034;&#20102;&#32570;&#22833;&#24773;&#26223;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27604;&#20998;&#31867;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#65292;&#19988;&#20809;&#23398;&#35270;&#35282;&#26159;&#26368;&#20851;&#38190;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22320;&#29699;&#35266;&#27979;&#65288;EO&#65289;&#24212;&#29992;&#28041;&#21450;&#22797;&#26434;&#21644;&#24322;&#26500;&#25968;&#25454;&#28304;&#65292;&#36890;&#24120;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#26222;&#36941;&#20551;&#35774;&#25968;&#25454;&#28304;&#23558;&#25345;&#32493;&#21487;&#29992;&#12290;&#19981;&#21516;&#24773;&#20917;&#21487;&#33021;&#24433;&#21709;EO&#25968;&#25454;&#28304;&#30340;&#21487;&#29992;&#24615;&#65292;&#22914;&#22122;&#22768;&#12289;&#20113;&#23618;&#25110;&#21355;&#26143;&#20219;&#21153;&#22833;&#36133;&#12290;&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#20219;&#21153;&#20013;&#32570;&#22833;&#26102;&#38388;&#24615;&#21644;&#38745;&#24577;EO&#25968;&#25454;&#28304;&#23545;&#35757;&#32451;&#27169;&#22411;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#26041;&#27861;&#30340;&#39044;&#27979;&#36136;&#37327;&#65292;&#24182;&#21457;&#29616;&#19968;&#20123;&#26041;&#27861;&#22312;&#38754;&#23545;&#32570;&#22833;&#25968;&#25454;&#26102;&#33258;&#28982;&#26356;&#21152;&#31283;&#20581;&#12290;&#29305;&#21035;&#26159;&#38598;&#25104;&#31574;&#30053;&#23454;&#29616;&#20102;&#39640;&#36798;100%&#30340;&#39044;&#27979;&#31283;&#20581;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#32570;&#22833;&#24773;&#26223;&#22312;&#22238;&#24402;&#20219;&#21153;&#20013;&#27604;&#20998;&#31867;&#20219;&#21153;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#20809;&#23398;&#35270;&#35282;&#22312;&#21333;&#29420;&#32570;&#22833;&#26102;&#26159;&#26368;&#20851;&#38190;&#30340;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14297v1 Announce Type: cross  Abstract: Earth observation (EO) applications involving complex and heterogeneous data sources are commonly approached with machine learning models. However, there is a common assumption that data sources will be persistently available. Different situations could affect the availability of EO sources, like noise, clouds, or satellite mission failures. In this work, we assess the impact of missing temporal and static EO sources in trained models across four datasets with classification and regression tasks. We compare the predictive quality of different methods and find that some are naturally more robust to missing data. The Ensemble strategy, in particular, achieves a prediction robustness up to 100%. We evidence that missing scenarios are significantly more challenging in regression than classification tasks. Finally, we find that the optical view is the most critical view when it is missing individually.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#35745;&#31639;&#32654;&#23398;&#20013;&#30340;&#22270;&#20687;&#26500;&#22270;&#21407;&#21017;&#34701;&#20837;&#21040;&#26816;&#32034;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#21382;&#21490;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#30340;&#22686;&#24378;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.14287</link><description>&lt;p&gt;
&#21033;&#29992;&#26500;&#22270;&#32447;&#32034;&#22686;&#24378;&#21382;&#21490;&#22270;&#20687;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Historical Image Retrieval with Compositional Cues
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14287
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#23558;&#35745;&#31639;&#32654;&#23398;&#20013;&#30340;&#22270;&#20687;&#26500;&#22270;&#21407;&#21017;&#34701;&#20837;&#21040;&#26816;&#32034;&#27169;&#22411;&#20013;&#65292;&#23454;&#29616;&#20102;&#21382;&#21490;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#30340;&#22686;&#24378;&#65292;&#25552;&#39640;&#20102;&#22270;&#20687;&#26816;&#32034;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14287v1 &#36890;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#22312;&#20998;&#26512;&#22823;&#37327;&#23384;&#20648;&#30340;&#21382;&#21490;&#22270;&#20687;&#25968;&#25454;&#26102;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#20869;&#23481;&#30340;&#26816;&#32034;&#26041;&#27861;&#24448;&#24448;&#24573;&#30053;&#20102;&#37325;&#35201;&#30340;&#38750;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#36328;&#19981;&#21516;&#20027;&#39064;&#36827;&#34892;&#28789;&#27963;&#25506;&#32034;&#30340;&#25928;&#26524;&#12290;&#20026;&#20102;&#25299;&#23485;&#22270;&#20687;&#26816;&#32034;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#20026;&#21508;&#31181;&#30446;&#30340;&#25581;&#31034;&#26356;&#19968;&#33324;&#30340;&#35268;&#24459;&#65292;&#25105;&#20204;&#21019;&#26032;&#24615;&#22320;&#23558;&#35745;&#31639;&#32654;&#23398;&#20013;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#21363;&#22270;&#20687;&#26500;&#22270;&#65292;&#24341;&#20837;&#21040;&#36825;&#20010;&#20027;&#39064;&#20013;&#12290;&#36890;&#36807;&#26126;&#30830;&#22320;&#23558;&#30001; CNN &#25552;&#21462;&#30340;&#19982;&#26500;&#22270;&#30456;&#20851;&#20449;&#24687;&#25972;&#21512;&#21040;&#35774;&#35745;&#30340;&#26816;&#32034;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32771;&#34385;&#20102;&#22270;&#20687;&#30340;&#26500;&#22270;&#35268;&#21017;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#23450;&#24615;&#21644;&#23450;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#30001;&#26500;&#22270;&#20449;&#24687;&#24341;&#23548;&#30340;&#22270;&#20687;&#26816;&#32034;&#32593;&#32476;&#20248;&#20110;&#20165;&#20381;&#36182;&#20869;&#23481;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#26377;&#21161;&#20110;&#22312;&#25968;&#25454;&#24211;&#20013;&#35782;&#21035;&#36317;&#31163;&#30446;&#26631;&#22270;&#20687;&#22312;&#20154;&#31867;&#24863;&#30693;&#19978;&#26356;&#25509;&#36817;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14287v1 Announce Type: cross  Abstract: In analyzing vast amounts of digitally stored historical image data, existing content-based retrieval methods often overlook significant non-semantic information, limiting their effectiveness for flexible exploration across varied themes. To broaden the applicability of image retrieval methods for diverse purposes and uncover more general patterns, we innovatively introduce a crucial factor from computational aesthetics, namely image composition, into this topic. By explicitly integrating composition-related information extracted by CNN into the designed retrieval model, our method considers both the image's composition rules and semantic information. Qualitative and quantitative experiments demonstrate that the image retrieval network guided by composition information outperforms those relying solely on content information, facilitating the identification of images in databases closer to the target image in human perception. Please vi
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#25968;&#25454;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#20844;&#24179;&#24615;&#65292;&#25552;&#20986;&#20102;&#24314;&#31435;&#20559;&#35265;&#31867;&#22411;&#19982;&#32531;&#35299;&#25216;&#26415;&#26377;&#25928;&#24615;&#20043;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.14282</link><description>&lt;p&gt;
&#22914;&#20309;&#20570;&#21040;&#20844;&#24179;&#65311;&#26631;&#31614;&#21644;&#36873;&#25321;&#20559;&#24046;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
How to be fair? A study of label and selection bias
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14282
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#25968;&#25454;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#20844;&#24179;&#24615;&#65292;&#25552;&#20986;&#20102;&#24314;&#31435;&#20559;&#35265;&#31867;&#22411;&#19982;&#32531;&#35299;&#25216;&#26415;&#26377;&#25928;&#24615;&#20043;&#38388;&#20851;&#31995;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20247;&#25152;&#21608;&#30693;&#65292;&#20559;&#35265;&#25968;&#25454;&#20250;&#23548;&#33268;&#20559;&#35265;&#12289;&#28508;&#22312;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#29992;&#20110;&#25968;&#25454;&#21644;&#27169;&#22411;&#39044;&#27979;&#20559;&#35265;&#30340;&#25514;&#26045;&#65292;&#20197;&#21450;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#35774;&#35745;&#20844;&#24179;&#30340;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#26469;&#23398;&#20064;&#27169;&#22411;&#12290;&#36817;&#21313;&#24180;&#26469;&#24050;&#32463;&#21457;&#23637;&#20102;&#22823;&#37327;&#30340;&#32531;&#35299;&#25216;&#26415;&#65292;&#28982;&#32780;&#65292;&#22312;&#20160;&#20040;&#24773;&#20917;&#19979;&#21738;&#20123;&#26041;&#27861;&#36215;&#20316;&#29992;&#20173;&#28982;&#30693;&#20043;&#29978;&#23569;&#12290;&#26368;&#36817;&#65292;Wick&#31561;&#20154;&#22312;&#21512;&#25104;&#25968;&#25454;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23384;&#22312;&#19968;&#20123;&#24773;&#20917;&#65292;&#20854;&#20013;&#20559;&#35265;&#32531;&#35299;&#25216;&#26415;&#23548;&#33268;&#22312;&#26080;&#20559;&#25968;&#25454;&#19978;&#27979;&#37327;&#26102;&#26356;&#31934;&#30830;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#32570;&#20047;&#24443;&#24213;&#30340;&#25968;&#23398;&#20998;&#26512;&#30340;&#24773;&#20917;&#19979;&#65292;&#20173;&#19981;&#28165;&#26970;&#22312;&#20309;&#31181;&#24773;&#20917;&#19979;&#21738;&#20123;&#25216;&#26415;&#26159;&#26377;&#25928;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#24314;&#31435;&#20559;&#35265;&#31867;&#22411;&#19982;&#32531;&#35299;&#25216;&#26415;&#26377;&#25928;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#32531;&#35299;&#25216;&#26415;&#25353;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14282v1 Announce Type: cross  Abstract: It is widely accepted that biased data leads to biased and thus potentially unfair models. Therefore, several measures for bias in data and model predictions have been proposed, as well as bias mitigation techniques whose aim is to learn models that are fair by design. Despite the myriad of mitigation techniques developed in the past decade, however, it is still poorly understood under what circumstances which methods work. Recently, Wick et al. showed, with experiments on synthetic data, that there exist situations in which bias mitigation techniques lead to more accurate models when measured on unbiased data. Nevertheless, in the absence of a thorough mathematical analysis, it remains unclear which techniques are effective under what circumstances. We propose to address this problem by establishing relationships between the type of bias and the effectiveness of a mitigation technique, where we categorize the mitigation techniques by 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#27169;&#25311;&#19981;&#21516;&#35282;&#33394;&#36827;&#34892;&#35752;&#35770;&#65292;&#20197;&#36798;&#25104;&#23545;&#20195;&#30721;&#20013;&#28431;&#27934;&#23384;&#22312;&#21644;&#20998;&#31867;&#30340;&#20849;&#35782;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21021;&#27493;&#35780;&#20272;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#30340;&#26126;&#26174;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.14274</link><description>&lt;p&gt;
&#36890;&#36807;LLMs&#35752;&#35770;&#23454;&#29616;&#28431;&#27934;&#26816;&#27979;&#30340;&#22810;&#35282;&#20849;&#35782;
&lt;/p&gt;
&lt;p&gt;
Multi-role Consensus through LLMs Discussions for Vulnerability Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14274
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#27169;&#25311;&#19981;&#21516;&#35282;&#33394;&#36827;&#34892;&#35752;&#35770;&#65292;&#20197;&#36798;&#25104;&#23545;&#20195;&#30721;&#20013;&#28431;&#27934;&#23384;&#22312;&#21644;&#20998;&#31867;&#30340;&#20849;&#35782;&#30340;&#26041;&#27861;&#65292;&#24182;&#22312;&#21021;&#27493;&#35780;&#20272;&#20013;&#23454;&#29616;&#20102;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20998;&#25968;&#30340;&#26126;&#26174;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21457;&#23637;&#31361;&#26174;&#20102;&#28431;&#27934;&#26816;&#27979;&#30340;&#28508;&#21147;&#65292;&#36825;&#26159;&#36719;&#20214;&#36136;&#37327;&#20445;&#35777;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20165;&#38480;&#20110;&#21333;&#19968;&#35282;&#33394;&#30340;&#35270;&#35282;&#65292;&#36890;&#24120;&#26159;&#27979;&#35797;&#20154;&#21592;&#65292;&#32570;&#20047;&#20856;&#22411;&#36719;&#20214;&#24320;&#21457;&#29983;&#21629;&#21608;&#26399;&#20013;&#19981;&#21516;&#35282;&#33394;&#30340;&#22810;&#20803;&#35266;&#28857;&#65292;&#21253;&#25324;&#24320;&#21457;&#20154;&#21592;&#21644;&#27979;&#35797;&#20154;&#21592;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;LLMs&#25198;&#28436;&#19981;&#21516;&#35282;&#33394;&#30340;&#26041;&#27861;&#65292;&#27169;&#25311;&#29616;&#23454;&#20195;&#30721;&#23457;&#26597;&#36807;&#31243;&#65292;&#36827;&#34892;&#35752;&#35770;&#20197;&#36798;&#25104;&#20851;&#20110;&#20195;&#30721;&#20013;&#28431;&#27934;&#23384;&#22312;&#21644;&#20998;&#31867;&#30340;&#20849;&#35782;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#21021;&#27493;&#35780;&#20272;&#26174;&#31034;&#65292;&#31934;&#30830;&#29575;&#22686;&#21152;&#20102;4.73&#65285;&#65292;&#21484;&#22238;&#29575;&#22686;&#21152;&#20102;58.9&#65285;&#65292;F1&#20998;&#25968;&#22686;&#21152;&#20102;28.1&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14274v1 Announce Type: cross  Abstract: Recent advancements in large language models (LLMs) have highlighted the potential for vulnerability detection, a crucial component of software quality assurance. Despite this progress, most studies have been limited to the perspective of a single role, usually testers, lacking diverse viewpoints from different roles in a typical software development life-cycle, including both developers and testers. To this end, this paper introduces an approach to employ LLMs to act as different roles to simulate real-life code review process, engaging in discussions towards a consensus on the existence and classification of vulnerabilities in the code. Preliminary evaluation of the proposed approach indicates a 4.73% increase in the precision rate, 58.9% increase in the recall rate, and a 28.1% increase in the F1 score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#22312;OpenNeoMC&#26694;&#26550;&#20013;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20248;&#21270;&#30740;&#31350;&#21453;&#24212;&#22534;&#20013;&#30340;&#19968;&#20010;&#21333;&#20803;&#26684;&#65292;&#20197;&#26368;&#22823;&#21270;&#20013;&#23376;&#36890;&#37327;&#24182;&#20445;&#25345;&#21453;&#24212;&#22534;&#30340;&#20020;&#30028;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.14273</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#21453;&#24212;&#22534;&#20248;&#21270;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Reactor Optimization Benchmark by Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#38382;&#39064;&#65292;&#22312;OpenNeoMC&#26694;&#26550;&#20013;&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#26088;&#22312;&#20248;&#21270;&#30740;&#31350;&#21453;&#24212;&#22534;&#20013;&#30340;&#19968;&#20010;&#21333;&#20803;&#26684;&#65292;&#20197;&#26368;&#22823;&#21270;&#20013;&#23376;&#36890;&#37327;&#24182;&#20445;&#25345;&#21453;&#24212;&#22534;&#30340;&#20020;&#30028;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#24212;&#22534;&#30340;&#20013;&#23376;&#23398;&#35745;&#31639;&#22312;&#20351;&#29992;&#33945;&#29305;&#21345;&#27931;&#65288;MC&#65289;&#26041;&#27861;&#26102;&#26159;&#19968;&#39033;&#33392;&#24040;&#30340;&#20219;&#21153;&#12290;&#38543;&#30528;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#36827;&#27493;&#65292;&#22914;&#20170;&#21453;&#24212;&#22534;&#30340;&#27169;&#25311;&#26356;&#23481;&#26131;&#23454;&#29616;&#65292;&#20294;&#20351;&#29992;&#22810;&#20010;&#21442;&#25968;&#36827;&#34892;&#35774;&#35745;&#21644;&#20248;&#21270;&#20173;&#28982;&#26159;&#19968;&#20010;&#35745;&#31639;&#25361;&#25112;&#12290;MC&#20256;&#36755;&#27169;&#25311;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20026;&#22686;&#24378;&#26680;&#21453;&#24212;&#22534;&#20248;&#21270;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#25552;&#20379;&#20102;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#26412;&#25991;&#22312;&#19987;&#38376;&#20026;&#24378;&#21270;&#23398;&#20064;&#35774;&#35745;&#30340;OpenNeoMC&#26694;&#26550;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#38382;&#39064;&#12290;&#35813;&#22522;&#20934;&#28041;&#21450;&#20248;&#21270;&#30740;&#31350;&#21453;&#24212;&#22534;&#20013;&#30340;&#19968;&#20010;&#21333;&#20803;&#26684;&#65292;&#20855;&#26377;&#20004;&#20010;&#19981;&#21516;&#21442;&#25968;&#65288;&#29123;&#26009;&#23494;&#24230;&#21644;&#27700;&#38388;&#36317;&#65289;&#65292;&#20197;&#26368;&#22823;&#21270;&#20013;&#23376;&#36890;&#37327;&#21516;&#26102;&#20445;&#25345;&#21453;&#24212;&#22534;&#30340;&#20020;&#30028;&#24615;&#12290;&#27979;&#35797;&#26696;&#20363;&#20855;&#26377;&#19981;&#21516;&#30340;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#20195;&#34920;&#19981;&#21516;&#30340;&#29289;&#29702;&#21306;&#22495;&#65292;&#20174;&#32780;&#20026;&#23398;&#20064;&#31639;&#27861;&#25552;&#20986;&#25361;&#25112;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#36816;&#29992;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14273v1 Announce Type: cross  Abstract: Neutronic calculations for reactors are a daunting task when using Monte Carlo (MC) methods. As high-performance computing has advanced, the simulation of a reactor is nowadays more readily done, but design and optimization with multiple parameters is still a computational challenge. MC transport simulations, coupled with machine learning techniques, offer promising avenues for enhancing the efficiency and effectiveness of nuclear reactor optimization. This paper introduces a novel benchmark problem within the OpenNeoMC framework designed specifically for reinforcement learning. The benchmark involves optimizing a unit cell of a research reactor with two varying parameters (fuel density and water spacing) to maximize neutron flux while maintaining reactor criticality. The test case features distinct local optima, representing different physical regimes, thus posing a challenge for learning algorithms. Through extensive simulations util
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#32918;&#20687;&#39118;&#26684;&#21270;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#35064;&#20307;&#20869;&#23481;&#35782;&#21035;&#27169;&#22359;&#21644;&#32932;&#33394;&#24863;&#30693;&#32918;&#20687;&#39118;&#26684;&#21270;&#27169;&#22359;&#65292;&#20197;&#35299;&#20915;&#36807;&#28388;&#26377;&#23475;&#20869;&#23481;&#24182;&#20445;&#30041;&#32932;&#33394;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14264</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#32932;&#33394;&#24847;&#35782;&#21644;&#35064;&#20307;&#35782;&#21035;&#30340;&#32918;&#20687;&#39118;&#26684;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Framework for Portrait Stylization with Skin-Tone Awareness and Nudity Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14264
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#32918;&#20687;&#39118;&#26684;&#21270;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#35064;&#20307;&#20869;&#23481;&#35782;&#21035;&#27169;&#22359;&#21644;&#32932;&#33394;&#24863;&#30693;&#32918;&#20687;&#39118;&#26684;&#21270;&#27169;&#22359;&#65292;&#20197;&#35299;&#20915;&#36807;&#28388;&#26377;&#23475;&#20869;&#23481;&#24182;&#20445;&#30041;&#32932;&#33394;&#29305;&#24449;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32918;&#20687;&#39118;&#26684;&#21270;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#23558;&#36755;&#20837;&#30340;&#32918;&#20687;&#22270;&#20687;&#36716;&#25442;&#20026;&#29305;&#23450;&#39118;&#26684;&#65292;&#21516;&#26102;&#20445;&#30041;&#20854;&#22266;&#26377;&#29305;&#24449;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#31283;&#23450;&#25193;&#25955;&#65288;SD&#65289;&#22312;&#35813;&#39046;&#22495;&#26174;&#33879;&#25552;&#39640;&#20102;&#32467;&#26524;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#33021;&#22815;&#26377;&#25928;&#36807;&#28388;&#26377;&#23475;&#36755;&#20837;&#20869;&#23481;&#24182;&#20445;&#30041;&#36755;&#20837;&#30340;&#29420;&#29305;&#29305;&#24449;&#65288;&#22914;&#32932;&#33394;&#65289;&#65292;&#21516;&#26102;&#20445;&#25345;&#39118;&#26684;&#21270;&#36136;&#37327;&#30340;&#23454;&#29992;&#39118;&#26684;&#21270;&#26694;&#26550;&#20173;&#28982;&#32570;&#20047;&#12290;&#36825;&#20123;&#25361;&#25112;&#38459;&#30861;&#20102;&#36825;&#31181;&#26694;&#26550;&#30340;&#24191;&#27867;&#37096;&#32626;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#32918;&#20687;&#39118;&#26684;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#34701;&#21512;&#20102;&#35064;&#38706;&#20869;&#23481;&#35782;&#21035;&#27169;&#22359;&#65288;NCIM&#65289;&#21644;&#32932;&#33394;&#24863;&#30693;&#32918;&#20687;&#39118;&#26684;&#21270;&#27169;&#22359;&#65288;STAPSM&#65289;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;NCIM &#22312;&#22686;&#24378;&#26126;&#30830;&#20869;&#23481;&#36807;&#28388;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#32780; STAPSM &#20934;&#30830;&#34920;&#29616;&#20102;&#21508;&#31181;&#32932;&#33394;&#12290;&#25105;&#20204;&#30340;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14264v1 Announce Type: cross  Abstract: Portrait stylization is a challenging task involving the transformation of an input portrait image into a specific style while preserving its inherent characteristics. The recent introduction of Stable Diffusion (SD) has significantly improved the quality of outcomes in this field. However, a practical stylization framework that can effectively filter harmful input content and preserve the distinct characteristics of an input, such as skin-tone, while maintaining the quality of stylization remains lacking. These challenges have hindered the wide deployment of such a framework. To address these issues, this study proposes a portrait stylization framework that incorporates a nudity content identification module (NCIM) and a skin-tone-aware portrait stylization module (STAPSM). In experiments, NCIM showed good performance in enhancing explicit content filtering, and STAPSM accurately represented a diverse range of skin tones. Our proposed
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LayoutLLM&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26723;&#22270;&#20687;&#29702;&#35299;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#23545;&#25991;&#26723;&#22270;&#20687;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.14252</link><description>&lt;p&gt;
LayoutLLM&#65306;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#25351;&#20196;&#35843;&#25972;&#29992;&#20110;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
LayoutLLM: Large Language Model Instruction Tuning for Visually Rich Document Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14252
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LayoutLLM&#27169;&#22411;&#65292;&#36890;&#36807;&#32467;&#21512;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#25991;&#26723;&#22270;&#20687;&#29702;&#35299;&#30340;&#20248;&#21183;&#65292;&#23454;&#29616;&#20102;&#23545;&#25991;&#26723;&#22270;&#20687;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;LayoutLLM&#65292;&#19968;&#31181;&#26356;&#28789;&#27963;&#30340;&#25991;&#26723;&#20998;&#26512;&#26041;&#27861;&#65292;&#29992;&#20110;&#29702;&#35299;&#22270;&#20687;&#25991;&#26723;&#12290;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#29702;&#35299;&#20219;&#21153;&#65292;&#22914;&#25991;&#26723;&#22270;&#20687;&#20998;&#31867;&#21644;&#20449;&#24687;&#25552;&#21462;&#65292;&#30001;&#20110;&#20854;&#37325;&#35201;&#24615;&#32780;&#21463;&#21040;&#37325;&#35270;&#12290;&#29616;&#26377;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#25972;&#21512;&#23545;&#22270;&#20687;&#12289;&#25991;&#26412;&#21644;&#24067;&#23616;&#32467;&#26500;&#30340;&#39044;&#35757;&#32451;&#24847;&#35782;&#26469;&#25552;&#21319;&#25991;&#26723;&#29702;&#35299;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#36827;&#34892;&#24494;&#35843;&#65292;&#32780;&#19988;&#27169;&#22411;&#35757;&#32451;&#21644;&#25805;&#20316;&#25104;&#26412;&#39640;&#26114;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;LayoutLLM&#65292;&#23558;&#36825;&#20123;&#19982;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30456;&#38598;&#25104;&#12290;&#36890;&#36807;&#21033;&#29992;&#29616;&#26377;&#30740;&#31350;&#22312;&#25991;&#26723;&#22270;&#20687;&#29702;&#35299;&#21644;LLMs&#21331;&#36234;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#30340;&#20248;&#21183;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#25351;&#20196;&#25968;&#25454;&#38598;&#30340;&#24494;&#35843;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#21333;&#20010;&#27169;&#22411;&#29702;&#35299;&#25991;&#26723;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14252v1 Announce Type: cross  Abstract: This paper proposes LayoutLLM, a more flexible document analysis method for understanding imaged documents. Visually Rich Document Understanding tasks, such as document image classification and information extraction, have gained significant attention due to their importance. Existing methods have been developed to enhance document comprehension by incorporating pre-training awareness of images, text, and layout structure. However, these methods require fine-tuning for each task and dataset, and the models are expensive to train and operate. To overcome this limitation, we propose a new LayoutLLM that integrates these with large-scale language models (LLMs). By leveraging the strengths of existing research in document image understanding and LLMs' superior language understanding capabilities, the proposed model, fine-tuned with multimodal instruction datasets, performs an understanding of document images in a single model. Our experime
&lt;/p&gt;</description></item><item><title>CATSE&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#23454;&#26102;&#22788;&#29702;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#20302;&#24310;&#36831;&#22240;&#26524;TSE&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22797;&#21512;&#22810;&#20219;&#21153;&#35757;&#32451;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#30446;&#26631;&#22768;&#38899;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14246</link><description>&lt;p&gt;
CATSE: &#19968;&#31181;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#22240;&#26524;&#30446;&#26631;&#22768;&#38899;&#25552;&#21462;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CATSE: A Context-Aware Framework for Causal Target Sound Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14246
&lt;/p&gt;
&lt;p&gt;
CATSE&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#23454;&#26102;&#22788;&#29702;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#20302;&#24310;&#36831;&#22240;&#26524;TSE&#27169;&#22411;&#65292;&#36890;&#36807;&#24341;&#20837;&#22797;&#21512;&#22810;&#20219;&#21153;&#35757;&#32451;&#30446;&#26631;&#65292;&#25552;&#39640;&#20102;&#30446;&#26631;&#22768;&#38899;&#25552;&#21462;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#26631;&#22768;&#38899;&#25552;&#21462;&#65288;TSE&#65289;&#19987;&#27880;&#20110;&#23558;&#29992;&#25143;&#25552;&#31034;&#30340;&#24863;&#20852;&#36259;&#26469;&#28304;&#19982;&#36755;&#20837;&#28151;&#21512;&#29289;&#20998;&#31163;&#30340;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#20197;&#32447;&#19979;&#26041;&#24335;&#36816;&#34892;&#65292;&#19981;&#36866;&#29992;&#20110;&#24212;&#29992;&#20110;&#23454;&#26102;&#20869;&#23481;&#65288;&#22914;&#22686;&#24378;&#21548;&#21147;&#65289;&#20013;&#25152;&#26045;&#21152;&#30340;&#20302;&#24310;&#36831;&#22240;&#26524;&#22788;&#29702;&#32422;&#26463;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31995;&#21015;&#36866;&#29992;&#20110;&#23454;&#26102;&#22788;&#29702;&#30340;&#19978;&#19979;&#25991;&#24863;&#30693;&#20302;&#24310;&#36831;&#22240;&#26524;TSE&#27169;&#22411;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#20026;TSE&#27169;&#22411;&#25552;&#20379;&#26377;&#20851;&#32452;&#25104;&#36755;&#20837;&#28151;&#21512;&#29289;&#30340;&#22768;&#38899;&#31867;&#21035;&#30340;&#31070;&#35861;&#20449;&#24687;&#65292;&#25506;&#35752;&#20102;&#19978;&#19979;&#25991;&#30340;&#23454;&#29992;&#24615;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#30446;&#26631;&#26159;&#25552;&#21462;&#29992;&#25143;&#25351;&#31034;&#30340;&#19968;&#20010;&#25110;&#22810;&#20010;&#24863;&#20852;&#36259;&#26469;&#28304;&#12290;&#30001;&#20110;&#31070;&#35861;&#27169;&#22411;&#30340;&#23454;&#38469;&#24212;&#29992;&#21463;&#21040;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;&#20998;&#31163;&#21644;&#20998;&#31867;&#25439;&#22833;&#30340;&#22797;&#21512;&#22810;&#20219;&#21153;&#35757;&#32451;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#28041;&#21450;&#21333;&#28304;&#21644;&#22810;&#28304;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14246v1 Announce Type: cross  Abstract: Target Sound Extraction (TSE) focuses on the problem of separating sources of interest, indicated by a user's cue, from the input mixture. Most existing solutions operate in an offline fashion and are not suited to the low-latency causal processing constraints imposed by applications in live-streamed content such as augmented hearing. We introduce a family of context-aware low-latency causal TSE models suitable for real-time processing. First, we explore the utility of context by providing the TSE model with oracle information about what sound classes make up the input mixture, where the objective of the model is to extract one or more sources of interest indicated by the user. Since the practical applications of oracle models are limited due to their assumptions, we introduce a composite multi-task training objective involving separation and classification losses. Our evaluation involving single- and multi-source extraction shows the 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20351;&#29992;&#21516;&#24615;&#36136;&#39640;&#26031;&#26680;&#26367;&#20195;&#21508;&#21521;&#24322;&#24615;&#26680;&#26469;&#25552;&#39640;&#35745;&#31639;&#24615;&#33021;&#65292;&#22312;&#19981;&#22833;&#21435;&#20960;&#20309;&#34920;&#31034;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#32422;100&#20493;&#30340;&#21152;&#36895;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#38656;&#35201;&#36752;&#23556;&#22330;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;</title><link>https://arxiv.org/abs/2403.14244</link><description>&lt;p&gt;
&#21516;&#24615;&#36136;&#39640;&#26031;&#39128;&#23633;&#23454;&#29616;&#23454;&#26102;&#36752;&#23556;&#22330;&#28210;&#26579;
&lt;/p&gt;
&lt;p&gt;
Isotropic Gaussian Splatting for Real-Time Radiance Field Rendering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14244
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20351;&#29992;&#21516;&#24615;&#36136;&#39640;&#26031;&#26680;&#26367;&#20195;&#21508;&#21521;&#24322;&#24615;&#26680;&#26469;&#25552;&#39640;&#35745;&#31639;&#24615;&#33021;&#65292;&#22312;&#19981;&#22833;&#21435;&#20960;&#20309;&#34920;&#31034;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#32422;100&#20493;&#30340;&#21152;&#36895;&#65292;&#36866;&#29992;&#20110;&#22810;&#31181;&#38656;&#35201;&#36752;&#23556;&#22330;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
3D&#39640;&#26031;&#39128;&#23633;&#26041;&#27861;&#22240;&#20854;&#22312;&#35757;&#32451;&#20013;&#30340;&#39640;&#24615;&#33021;&#21644;&#28210;&#26579;&#22270;&#20687;&#30340;&#39640;&#36136;&#37327;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#23427;&#20351;&#29992;&#21508;&#21521;&#24322;&#24615;&#30340;&#39640;&#26031;&#26680;&#26469;&#34920;&#31034;&#22330;&#26223;&#12290;&#34429;&#28982;&#36825;&#31181;&#21508;&#21521;&#24322;&#24615;&#26680;&#22312;&#34920;&#31034;&#20960;&#20309;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#65292;&#20294;&#22312;&#35745;&#31639;&#26041;&#38754;&#20250;&#23548;&#33268;&#35832;&#22914;&#20998;&#35010;&#25110;&#21512;&#24182;&#20004;&#20010;&#26680;&#30340;&#22256;&#38590;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#21516;&#24615;&#36136;&#39640;&#26031;&#26680;&#26469;&#36991;&#20813;&#35745;&#31639;&#20013;&#30340;&#36825;&#20123;&#22256;&#38590;&#65292;&#20174;&#32780;&#23548;&#33268;&#19968;&#31181;&#24615;&#33021;&#26356;&#39640;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#23454;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#27604;&#21407;&#26041;&#27861;&#24555;&#32422;100&#20493;&#65292;&#32780;&#19981;&#20250;&#20002;&#22833;&#20960;&#20309;&#34920;&#31034;&#30340;&#20934;&#30830;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#21487;&#24212;&#29992;&#20110;&#38656;&#35201;&#36752;&#23556;&#22330;&#30340;&#22823;&#33539;&#22260;&#24212;&#29992;&#65292;&#22914;3D&#37325;&#24314;&#12289;&#35270;&#22270;&#21512;&#25104;&#21644;&#21160;&#24577;&#23545;&#35937;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14244v1 Announce Type: cross  Abstract: The 3D Gaussian splatting method has drawn a lot of attention, thanks to its high performance in training and high quality of the rendered image. However, it uses anisotropic Gaussian kernels to represent the scene. Although such anisotropic kernels have advantages in representing the geometry, they lead to difficulties in terms of computation, such as splitting or merging two kernels. In this paper, we propose to use isotropic Gaussian kernels to avoid such difficulties in the computation, leading to a higher performance method. The experiments confirm that the proposed method is about {\bf 100X} faster without losing the geometry representation accuracy. The proposed method can be applied in a large range applications where the radiance field is needed, such as 3D reconstruction, view synthesis, and dynamic object modeling.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#36828;&#31243;&#30382;&#32932;&#30149;&#23398;&#20013;&#25972;&#21512;&#20102;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#65292;&#26088;&#22312;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;Transformer&#35270;&#35273;&#27169;&#22411;&#21644;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#36741;&#21161;&#35786;&#26029;&#30382;&#32932;&#30149;&#21464;&#21644;&#20854;&#20182;&#30382;&#32932;&#29366;&#20917;&#65292;&#20174;&#32780;&#20840;&#38754;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#35786;&#26029;&#27969;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.14243</link><description>&lt;p&gt;
Dermacen Analytica: &#19968;&#31181;&#23558;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#30456;&#25972;&#21512;&#30340;&#26032;&#26041;&#27861;&#35770;&#22312;&#36828;&#31243;&#30382;&#32932;&#30149;&#23398;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Dermacen Analytica: A Novel Methodology Integrating Multi-Modal Large Language Models with Machine Learning in tele-dermatology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14243
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#22312;&#36828;&#31243;&#30382;&#32932;&#30149;&#23398;&#20013;&#25972;&#21512;&#20102;&#22810;&#27169;&#24335;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#26426;&#22120;&#23398;&#20064;&#65292;&#26088;&#22312;&#36890;&#36807;&#32508;&#21512;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;Transformer&#35270;&#35273;&#27169;&#22411;&#21644;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#65292;&#36741;&#21161;&#35786;&#26029;&#30382;&#32932;&#30149;&#21464;&#21644;&#20854;&#20182;&#30382;&#32932;&#29366;&#20917;&#65292;&#20174;&#32780;&#20840;&#38754;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#35786;&#26029;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#23835;&#36215;&#22312;&#21307;&#23398;&#21457;&#29616;&#12289;&#35786;&#26029;&#21644;&#24739;&#32773;&#31649;&#29702;&#39046;&#22495;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#24076;&#26395;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#21307;&#23398;&#39046;&#22495;&#30340;&#24040;&#22823;&#22797;&#26434;&#24615;&#35201;&#27714;&#37319;&#29992;&#19968;&#31181;&#26356;&#22797;&#26434;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12289;&#20998;&#31867;&#22120;&#12289;&#20998;&#21106;&#31639;&#27861;&#20197;&#21450;&#36817;&#26469;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#25551;&#36848;&#12289;&#23454;&#26045;&#21644;&#35780;&#20272;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#21644;&#26041;&#27861;&#35770;&#65292;&#26088;&#22312;&#21327;&#21161;&#30382;&#32932;&#30149;&#23398;&#39046;&#22495;&#30340;&#30382;&#32932;&#30149;&#21464;&#21644;&#20854;&#20182;&#30382;&#32932;&#29366;&#20917;&#30340;&#35786;&#26029;&#36807;&#31243;&#65292;&#26088;&#22312;&#20840;&#38754;&#35299;&#20915;&#35813;&#39046;&#22495;&#30340;&#35786;&#26029;&#27969;&#31243;&#12290;&#35813;&#24037;&#20316;&#27969;&#31243;&#25972;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#12289;&#22522;&#20110;Transformer&#30340;&#35270;&#35273;&#27169;&#22411;&#21644;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#12290;&#36825;&#31181;&#20840;&#38754;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23545;&#30382;&#32932;&#30149;&#21464;&#30340;&#24494;&#22937;&#35299;&#37322;&#65292;&#27169;&#25311;&#24182;&#20419;&#36827;&#20102;&#30382;&#32932;&#31185;&#21307;&#29983;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#24443;&#24213;&#30340;&#20132;&#21449;&#27169;&#24335;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14243v1 Announce Type: cross  Abstract: The rise of Artificial Intelligence creates great promise in the field of medical discovery, diagnostics and patient management. However, the vast complexity of all medical domains require a more complex approach that combines machine learning algorithms, classifiers, segmentation algorithms and, lately, large language models. In this paper, we describe, implement and assess an Artificial Intelligence-empowered system and methodology aimed at assisting the diagnosis process of skin lesions and other skin conditions within the field of dermatology that aims to holistically address the diagnostic process in this domain. The workflow integrates large language, transformer-based vision models and sophisticated machine learning tools. This holistic approach achieves a nuanced interpretation of dermatological conditions that simulates and facilitates a dermatologist's workflow. We assess our proposed methodology through a thorough cross-mode
&lt;/p&gt;</description></item><item><title>RLRF&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#21453;&#39304;&#21644;&#33258;&#25105;&#21453;&#24605;&#26426;&#21046;&#65292;&#21487;&#20197;&#25913;&#36827;LLMs&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#36229;&#36234;&#34920;&#38754;&#35843;&#25972;&#12290;</title><link>https://arxiv.org/abs/2403.14238</link><description>&lt;p&gt;
&#21453;&#24605;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLRF&#65289;&#65306;&#36890;&#36807;&#32454;&#31890;&#24230;&#33258;&#25105;&#21453;&#24605;&#23545;LLMs&#36827;&#34892;&#35843;&#25972;&#21644;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning from Reflective Feedback (RLRF): Aligning and Improving LLMs via Fine-Grained Self-Reflection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14238
&lt;/p&gt;
&lt;p&gt;
RLRF&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#32454;&#31890;&#24230;&#21453;&#39304;&#21644;&#33258;&#25105;&#21453;&#24605;&#26426;&#21046;&#65292;&#21487;&#20197;&#25913;&#36827;LLMs&#30340;&#26680;&#24515;&#33021;&#21147;&#65292;&#36229;&#36234;&#34920;&#38754;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;RLHF&#22312;&#23558;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#36827;&#34892;&#35843;&#25972;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#65292;&#20294;&#24448;&#24448;&#20250;&#23548;&#33268;&#34920;&#38754;&#35843;&#25972;&#65292;&#20248;&#20808;&#32771;&#34385;&#39118;&#26684;&#21464;&#21270;&#32780;&#38750;&#25913;&#21892;LLMs&#30340;&#19979;&#28216;&#24615;&#33021;&#12290;&#26410;&#26126;&#30830;&#35268;&#23450;&#30340;&#20559;&#22909;&#21487;&#33021;&#20250;&#27169;&#31946;&#23545;&#40784;&#27169;&#22411;&#30340;&#26041;&#21521;&#12290;&#32570;&#20047;&#25506;&#32034;&#20250;&#38480;&#21046;&#35782;&#21035;&#20986;&#25913;&#21892;&#27169;&#22411;&#30340;&#29702;&#24819;&#36755;&#20986;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65306;&#21453;&#24605;&#21453;&#39304;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLRF&#65289;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#22522;&#20110;&#35814;&#32454;&#26631;&#20934;&#30340;&#32454;&#31890;&#24230;&#21453;&#39304;&#65292;&#20197;&#25913;&#36827;LLMs&#30340;&#26680;&#24515;&#33021;&#21147;&#12290;RLRF&#37319;&#29992;&#33258;&#25105;&#21453;&#24605;&#26426;&#21046;&#31995;&#32479;&#22320;&#25506;&#32034;&#21644;&#23436;&#21892;LLMs&#30340;&#22238;&#24212;&#65292;&#28982;&#21518;&#36890;&#36807;RL&#31639;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21516;&#26102;&#32467;&#21512;&#26377;&#24076;&#26395;&#30340;&#22238;&#24212;&#12290;&#22312;Just-Eval&#12289;Factuality&#21644;Mathematical Reasoning&#31561;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;RLRF&#30340;&#21151;&#25928;&#21644;&#36229;&#36234;&#34920;&#38754;&#35843;&#25972;&#30340;&#36716;&#21464;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14238v1 Announce Type: cross  Abstract: Despite the promise of RLHF in aligning LLMs with human preferences, it often leads to superficial alignment, prioritizing stylistic changes over improving downstream performance of LLMs. Underspecified preferences could obscure directions to align the models. Lacking exploration restricts identification of desirable outputs to improve the models. To overcome these challenges, we propose a novel framework: Reinforcement Learning from Reflective Feedback (RLRF), which leverages fine-grained feedback based on detailed criteria to improve the core capabilities of LLMs. RLRF employs a self-reflection mechanism to systematically explore and refine LLM responses, then fine-tuning the models via a RL algorithm along with promising responses. Our experiments across Just-Eval, Factuality, and Mathematical Reasoning demonstrate the efficacy and transformative potential of RLRF beyond superficial surface-level adjustment.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.14236</link><description>&lt;p&gt;
&#19968;&#20010;&#32479;&#19968;&#30340;&#27169;&#22411;&#32534;&#36753;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Unified Framework for Model Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14236
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#32479;&#19968;&#26694;&#26550;&#32467;&#21512;&#20102;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#65292;&#26368;&#22823;&#21270;&#20445;&#30041;&#26576;&#20123;&#21521;&#37327;&#34920;&#31034;&#24182;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#22411;&#32534;&#36753;&#26159;&#19968;&#20010;&#19981;&#26029;&#21457;&#23637;&#30340;&#39046;&#22495;&#65292;&#19987;&#27880;&#20110;&#26356;&#26032;&#27169;&#22411;&#20013;&#23884;&#20837;&#30340;&#30693;&#35782;&#12290;&#22312;&#21508;&#31181;&#26041;&#27861;&#20013;&#65292;ROME&#21644;MEMIT&#20316;&#20026;&#20027;&#35201;&#30340;&#8220;&#23450;&#20301;&#21644;&#32534;&#36753;&#8221;&#27169;&#22411;&#32534;&#36753;&#25216;&#26415;&#33073;&#39062;&#32780;&#20986;&#12290;&#32780;MEMIT&#21487;&#20197;&#25209;&#37327;&#32534;&#36753;&#35760;&#24518;&#65292;ROME&#21017;&#19968;&#27425;&#21482;&#33021;&#25913;&#21464;&#19968;&#20010;&#20107;&#23454;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#65292;&#23558;ROME&#21644;MEMIT&#32435;&#20837;&#19968;&#20010;&#21333;&#19968;&#30340;&#27010;&#24565;&#26694;&#26550;&#65292;&#20248;&#21270;&#21516;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#20445;&#23384;-&#35760;&#24518;&#8221;&#30446;&#26631;&#12290;&#35813;&#30446;&#26631;&#26088;&#22312;&#22312;&#35760;&#24518;&#26032;&#20107;&#23454;&#20449;&#24687;&#30340;&#21516;&#26102;&#20445;&#30041;&#26576;&#20123;&#36873;&#23450;&#21521;&#37327;&#30340;&#34920;&#31034;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;ROME&#20351;&#29992;&#31561;&#24335;&#32422;&#26463;&#20248;&#21270;&#27492;&#30446;&#26631;&#65292;&#32780;MEMIT&#37319;&#29992;&#26356;&#28789;&#27963;&#30340;&#26368;&#23567;&#20108;&#20056;&#32422;&#26463;&#12290;&#38500;&#20102;&#25209;&#37327;&#32534;&#36753;&#22806;&#65292;MEMIT&#36824;&#21487;&#20197;&#22312;&#22810;&#20010;&#23618;&#38754;&#32534;&#36753;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#32534;&#36753;&#30340;&#20998;&#24067;&#20174;&#22810;&#20010;&#23618;&#38754;&#20998;&#24320;&#65292;&#21306;&#21035;&#20110;&#20248;&#21270;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14236v1 Announce Type: cross  Abstract: Model editing is a growing area focused on updating the knowledge embedded within models. Among the various methodologies, ROME and MEMIT stand out as leading "locate-and-edit" model editing techniques. While MEMIT enables batched editing of memories, ROME is limited to changing one fact at a time. This paper introduces a unifying framework that brings ROME and MEMIT under a single conceptual umbrella, optimizing for the same goal, which we call the "preservation-memorization" objective. This objective aims to preserve the representations of certain selected vectors while memorizing the representations of new factual information. Specifically, ROME optimizes this objective using an equality constraint, whereas MEMIT employs a more flexible least-square constraint. In addition to making batched edits, MEMIT also edits the model at multiple layers. We disentangle the distribution of edits to multiple layers from the optimization objectiv
&lt;/p&gt;</description></item><item><title>&#39318;&#27425;&#32771;&#34385;&#22270;&#20687;&#20256;&#24863;&#22120;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26631;&#31614;&#32423;&#21035;&#22122;&#22768;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#21435;&#22122;&#34917;&#19969;&#32423;&#21035;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;SoftPatch&#12290;</title><link>https://arxiv.org/abs/2403.14233</link><description>&lt;p&gt;
SoftPatch&#65306;&#26080;&#30417;&#30563;&#22122;&#22768;&#25968;&#25454;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
SoftPatch: Unsupervised Anomaly Detection with Noisy Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14233
&lt;/p&gt;
&lt;p&gt;
&#39318;&#27425;&#32771;&#34385;&#22270;&#20687;&#20256;&#24863;&#22120;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26631;&#31614;&#32423;&#21035;&#22122;&#22768;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#21435;&#22122;&#34917;&#19969;&#32423;&#21035;&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;SoftPatch&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#20027;&#27969;&#30340;&#26080;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#22312;&#23398;&#26415;&#25968;&#25454;&#38598;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#30001;&#20110;&#29702;&#24819;&#30340;&#24178;&#20928;&#35757;&#32451;&#25968;&#25454;&#30340;&#23454;&#39564;&#35774;&#32622;&#65292;&#23427;&#20204;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#24615;&#33021;&#21463;&#21040;&#38480;&#21046;&#12290; &#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#24322;&#24120;&#26816;&#27979;&#20013;&#65292;&#20351;&#29992;&#22122;&#22768;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#26159;&#19968;&#20010;&#19981;&#21487;&#36991;&#20813;&#30340;&#38382;&#39064;&#65292;&#20294;&#24456;&#23569;&#26377;&#35752;&#35770;&#12290; &#26412;&#25991;&#39318;&#27425;&#32771;&#34385;&#20102;&#22270;&#20687;&#20256;&#24863;&#22120;&#24322;&#24120;&#26816;&#27979;&#20013;&#30340;&#26631;&#31614;&#32423;&#21035;&#22122;&#22768;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20869;&#23384;&#30340;&#26080;&#30417;&#30563;AD&#26041;&#27861;SoftPatch&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#23545;&#34917;&#19969;&#32423;&#21035;&#30340;&#25968;&#25454;&#36827;&#34892;&#21435;&#22122;&#12290; &#22122;&#22768;&#21028;&#21035;&#22120;&#29992;&#20110;&#29983;&#25104;&#29992;&#20110;&#34917;&#19969;&#32423;&#21035;&#22122;&#22768;&#28040;&#38500;&#30340;&#24322;&#24120;&#28857;&#35780;&#20998;&#65292;&#28982;&#21518;&#23558;&#36825;&#20123;&#35780;&#20998;&#23384;&#20648;&#22312;&#20869;&#23384;&#23384;&#20648;&#22120;&#20013;&#65292;&#20197;&#36719;&#21270;&#24322;&#24120;&#26816;&#27979;&#36793;&#30028;&#12290; &#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;SoftPatch&#20445;&#25345;&#20102;&#23545;&#27491;&#24120;&#25968;&#25454;&#30340;&#24378;&#24314;&#27169;&#33021;&#21147;&#65292;&#24182;&#20943;&#36731;&#20102;coreset&#20013;&#30340;&#36807;&#24230;&#33258;&#20449;&#38382;&#39064;&#12290;&#22312;&#23454;&#39564;&#20013;&#36827;&#34892;&#20102;&#32508;&#21512;&#24615;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14233v1 Announce Type: cross  Abstract: Although mainstream unsupervised anomaly detection (AD) algorithms perform well in academic datasets, their performance is limited in practical application due to the ideal experimental setting of clean training data. Training with noisy data is an inevitable problem in real-world anomaly detection but is seldom discussed. This paper considers label-level noise in image sensory anomaly detection for the first time. To solve this problem, we proposed a memory-based unsupervised AD method, SoftPatch, which efficiently denoises the data at the patch level. Noise discriminators are utilized to generate outlier scores for patch-level noise elimination before coreset construction. The scores are then stored in the memory bank to soften the anomaly detection boundary. Compared with existing methods, SoftPatch maintains a strong modeling ability of normal data and alleviates the overconfidence problem in coreset. Comprehensive experiments in v
&lt;/p&gt;</description></item><item><title>PeerGPT&#35770;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLM&#30340;&#21516;&#20394;&#20195;&#29702;&#20316;&#20026;&#22242;&#38431;&#20027;&#25345;&#20154;&#21644;&#21442;&#19982;&#32773;&#22312;&#20799;&#31461;&#21327;&#20316;&#23398;&#20064;&#20013;&#30340;&#35282;&#33394;&#65292;&#21457;&#29616;&#20182;&#20204;&#22312;&#31649;&#29702;&#35752;&#35770;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20316;&#20026;&#21442;&#19982;&#32773;&#21487;&#33021;&#23384;&#22312;&#21453;&#39304;&#21450;&#26102;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.14227</link><description>&lt;p&gt;
PeerGPT: &#25506;&#31350;&#22522;&#20110;LLM&#30340;&#21516;&#20394;&#20195;&#29702;&#20316;&#20026;&#22242;&#38431;&#20027;&#25345;&#20154;&#21644;&#21442;&#19982;&#32773;&#22312;&#20799;&#31461;&#21327;&#20316;&#23398;&#20064;&#20013;&#30340;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
PeerGPT: Probing the Roles of LLM-based Peer Agents as Team Moderators and Participants in Children's Collaborative Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14227
&lt;/p&gt;
&lt;p&gt;
PeerGPT&#35770;&#25991;&#25506;&#31350;&#20102;&#22522;&#20110;LLM&#30340;&#21516;&#20394;&#20195;&#29702;&#20316;&#20026;&#22242;&#38431;&#20027;&#25345;&#20154;&#21644;&#21442;&#19982;&#32773;&#22312;&#20799;&#31461;&#21327;&#20316;&#23398;&#20064;&#20013;&#30340;&#35282;&#33394;&#65292;&#21457;&#29616;&#20182;&#20204;&#22312;&#31649;&#29702;&#35752;&#35770;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#20316;&#20026;&#21442;&#19982;&#32773;&#21487;&#33021;&#23384;&#22312;&#21453;&#39304;&#21450;&#26102;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20799;&#31461;&#21327;&#20316;&#23398;&#20064;&#20013;&#65292;&#26377;&#25928;&#30340;&#21516;&#20394;&#23545;&#35805;&#26174;&#33879;&#25552;&#39640;&#20102;&#20799;&#31461;&#21327;&#20316;&#20132;&#27969;&#30340;&#36136;&#37327;&#12290;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#20195;&#29702;&#25972;&#21512;&#21040;&#27492;&#35774;&#32622;&#20013;&#65292;&#25506;&#35752;&#20182;&#20204;&#20316;&#20026;&#21516;&#20394;&#30340;&#21019;&#26032;&#35282;&#33394;&#65292;&#35780;&#20272;&#20854;&#20316;&#20026;&#22242;&#38431;&#20027;&#25345;&#20154;&#21644;&#21442;&#19982;&#32773;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36992;&#35831;&#20102;&#20004;&#32452;&#21442;&#19982;&#32773;&#21442;&#21152;&#19968;&#20010;&#21327;&#20316;&#23398;&#20064;&#30740;&#35752;&#20250;&#65292;&#22312;&#37027;&#37324;&#20182;&#20204;&#35752;&#35770;&#24182;&#25552;&#20986;&#20102;&#35299;&#20915;&#35774;&#35745;&#38382;&#39064;&#30340;&#27010;&#24565;&#35299;&#20915;&#26041;&#26696;&#12290;&#21516;&#20394;&#23545;&#35805;&#30340;&#25104;&#32489;&#21333;&#32463;&#36807;&#20102;&#20027;&#39064;&#20998;&#26512;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21516;&#20394;&#20195;&#29702;&#22312;&#20316;&#20026;&#22242;&#38431;&#20027;&#25345;&#20154;&#31649;&#29702;&#35752;&#35770;&#26102;&#65292;&#26377;&#26102;&#20250;&#34987;&#24573;&#35270;&#20182;&#20204;&#30340;&#25351;&#20196;&#12290;&#20316;&#20026;&#21442;&#19982;&#32773;&#65292;&#20182;&#20204;&#20419;&#36827;&#20102;&#20799;&#31461;&#30340;&#21019;&#36896;&#24615;&#24605;&#32500;&#65292;&#20294;&#21487;&#33021;&#24182;&#19981;&#22987;&#32456;&#25552;&#20379;&#21450;&#26102;&#30340;&#21453;&#39304;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;&#21516;&#20394;&#20195;&#29702;&#22312;&#20004;&#20010;&#35282;&#33394;&#20013;&#30340;&#28508;&#22312;&#35774;&#35745;&#25913;&#36827;&#21644;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14227v1 Announce Type: cross  Abstract: In children's collaborative learning, effective peer conversations can significantly enhance the quality of children's collaborative interactions. The integration of Large Language Model (LLM) agents into this setting explores their novel role as peers, assessing impacts as team moderators and participants. We invited two groups of participants to engage in a collaborative learning workshop, where they discussed and proposed conceptual solutions to a design problem. The peer conversation transcripts were analyzed using thematic analysis. We discovered that peer agents, while managing discussions effectively as team moderators, sometimes have their instructions disregarded. As participants, they foster children's creative thinking but may not consistently provide timely feedback. These findings highlight potential design improvements and considerations for peer agents in both roles.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#38899;&#39057;-&#35270;&#35273;&#20998;&#21106;&#26041;&#27861; MoCA&#65292;&#22312;&#27169;&#24577;&#23545;&#24212;&#23545;&#40784;&#30340;&#22522;&#30784;&#19978;&#20351;&#29992;DINO&#12289;SAM&#21644;ImageBind&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#20851;&#32852;&#65292;&#24182;&#24341;&#20837;&#20102;&#20687;&#32032;&#21305;&#37197;&#32858;&#21512;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.14203</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#38899;&#39057;-&#35270;&#35273;&#20998;&#21106;&#19982;&#27169;&#24577;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Audio-Visual Segmentation with Modality Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14203
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#38899;&#39057;-&#35270;&#35273;&#20998;&#21106;&#26041;&#27861; MoCA&#65292;&#22312;&#27169;&#24577;&#23545;&#24212;&#23545;&#40784;&#30340;&#22522;&#30784;&#19978;&#20351;&#29992;DINO&#12289;SAM&#21644;ImageBind&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22810;&#27169;&#24577;&#20851;&#32852;&#65292;&#24182;&#24341;&#20837;&#20102;&#20687;&#32032;&#21305;&#37197;&#32858;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;-&#35270;&#35273;&#20998;&#21106;&#65288;AVS&#65289;&#26088;&#22312;&#35782;&#21035;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#20135;&#29983;&#29305;&#23450;&#22768;&#38899;&#30340;&#23545;&#35937;&#65292;&#36825;&#19968;&#30740;&#31350;&#22312;&#20687;&#32032;&#32423;&#21035;&#36827;&#34892;&#12290;&#24403;&#21069;AVS&#26041;&#27861;&#20381;&#36182;&#20110;&#26114;&#36149;&#30340;&#31934;&#32454;&#26631;&#27880;&#30340;&#25513;&#30721;-&#38899;&#39057;&#23545;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26080;&#30417;&#30563;AVS&#65292;&#28040;&#38500;&#20102;&#36825;&#31181;&#26114;&#36149;&#26631;&#27880;&#30340;&#24517;&#35201;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#21517;&#20026;&#27169;&#24577;&#23545;&#24212;&#23545;&#40784;&#65288;MoCA&#65289;&#65292;&#23427;&#26080;&#32541;&#25972;&#21512;&#20102;&#20687;DINO&#65292;SAM&#21644;ImageBind&#36825;&#26679;&#30340;&#29616;&#25104;&#22522;&#30784;&#27169;&#22411;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#23427;&#20204;&#30340;&#30693;&#35782;&#20114;&#34917;&#24615;&#65292;&#20248;&#21270;&#23427;&#20204;&#30340;&#32852;&#21512;&#20351;&#29992;&#20197;&#23454;&#29616;&#22810;&#27169;&#24577;&#20851;&#32852;&#12290;&#36215;&#21021;&#65292;&#25105;&#20204;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#20272;&#35745;&#27491;&#36127;&#22270;&#20687;&#23545;&#12290;&#23545;&#20110;&#20687;&#32032;&#32423;&#21035;&#30340;&#20851;&#32852;&#65292;&#25105;&#20204;&#22312;&#22270;&#20687;&#32423;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#20869;&#24341;&#20837;&#20102;&#35270;&#35273;&#36866;&#37197;&#22120;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#20687;&#32032;&#21305;&#37197;&#32858;&#21512;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14203v1 Announce Type: cross  Abstract: Audio-Visual Segmentation (AVS) aims to identify, at the pixel level, the object in a visual scene that produces a given sound. Current AVS methods rely on costly fine-grained annotations of mask-audio pairs, making them impractical for scalability. To address this, we introduce unsupervised AVS, eliminating the need for such expensive annotation. To tackle this more challenging problem, we propose an unsupervised learning method, named Modality Correspondence Alignment (MoCA), which seamlessly integrates off-the-shelf foundation models like DINO, SAM, and ImageBind. This approach leverages their knowledge complementarity and optimizes their joint usage for multi-modality association. Initially, we estimate positive and negative image pairs in the feature space. For pixel-level association, we introduce an audio-visual adapter and a novel pixel matching aggregation strategy within the image-level contrastive learning framework. This al
&lt;/p&gt;</description></item><item><title>&#35777;&#26126;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#20123;&#26080;&#20559;&#23376;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20381;&#36182;&#31639;&#27861;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#34987;&#25552;&#21462;&#20986;&#26469;&#65292;&#24182;&#19988;&#36825;&#31181;&#29305;&#23450;&#26550;&#26500;&#26080;&#27861;&#23398;&#20064;&#20219;&#20309;&#29305;&#23450;&#30340;&#20559;&#35265;&#12290;</title><link>https://arxiv.org/abs/2403.14200</link><description>&lt;p&gt;
&#25163;&#26415;&#21592;&#21435;&#20559;&#35265;&#65306;&#31070;&#22855;&#30340;&#26435;&#37325;&#21450;&#22914;&#20309;&#25214;&#21040;&#23427;&#20204;
&lt;/p&gt;
&lt;p&gt;
Debiasing surgeon: fantastic weights and how to find them
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14200
&lt;/p&gt;
&lt;p&gt;
&#35777;&#26126;&#20102;&#22312;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20013;&#23384;&#22312;&#19968;&#20123;&#26080;&#20559;&#23376;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20381;&#36182;&#31639;&#27861;&#20559;&#35265;&#30340;&#24773;&#20917;&#19979;&#34987;&#25552;&#21462;&#20986;&#26469;&#65292;&#24182;&#19988;&#36825;&#31181;&#29305;&#23450;&#26550;&#26500;&#26080;&#27861;&#23398;&#20064;&#20219;&#20309;&#29305;&#23450;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20170;&#19968;&#20010;&#26085;&#30410;&#20851;&#27880;&#30340;&#29616;&#35937;&#26159;&#31639;&#27861;&#20559;&#35265;&#30340;&#20986;&#29616;&#65292;&#23427;&#21487;&#33021;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#27169;&#22411;&#12290;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#20960;&#31181;&#21435;&#20559;&#35265;&#30340;&#26041;&#27861;&#65292;&#37319;&#29992;&#26356;&#25110;&#22810;&#25110;&#23569;&#22797;&#26434;&#30340;&#26041;&#27861;&#26469;&#38459;&#27490;&#36825;&#20123;&#27169;&#22411;&#22823;&#35268;&#27169;&#22320;&#20351;&#29992;&#36825;&#20123;&#20559;&#35265;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#36825;&#31181;&#39069;&#22806;&#30340;&#22797;&#26434;&#24615;&#30495;&#30340;&#26377;&#24517;&#35201;&#21527;&#65311;&#19968;&#20010;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#26159;&#21542;&#24050;&#32463;&#21253;&#21547;&#20102;&#19968;&#20123;&#21487;&#20197;&#29420;&#31435;&#20351;&#29992;&#30340;&#8220;&#26080;&#20559;&#23376;&#32593;&#32476;&#8221;&#65292;&#24182;&#19988;&#21487;&#20197;&#25552;&#20986;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#32780;&#19981;&#20381;&#36182;&#20110;&#31639;&#27861;&#20559;&#35265;&#65311;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#26679;&#30340;&#23376;&#32593;&#32476;&#36890;&#24120;&#23384;&#22312;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#19968;&#20010;&#26222;&#36890;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#25552;&#21462;&#20986;&#26469;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#39564;&#35777;&#20102;&#36825;&#31181;&#29305;&#23450;&#30340;&#26550;&#26500;&#26080;&#27861;&#23398;&#20064;&#29305;&#23450;&#30340;&#20559;&#35265;&#65292;&#34920;&#26126;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#26377;&#21487;&#33021;&#36890;&#36807;&#26550;&#26500;&#19978;&#30340;&#23545;&#31574;&#26469;&#35299;&#20915;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14200v1 Announce Type: cross  Abstract: Nowadays an ever-growing concerning phenomenon, the emergence of algorithmic biases that can lead to unfair models, emerges. Several debiasing approaches have been proposed in the realm of deep learning, employing more or less sophisticated approaches to discourage these models from massively employing these biases. However, a question emerges: is this extra complexity really necessary? Is a vanilla-trained model already embodying some ``unbiased sub-networks'' that can be used in isolation and propose a solution without relying on the algorithmic biases? In this work, we show that such a sub-network typically exists, and can be extracted from a vanilla-trained model without requiring additional training. We further validate that such specific architecture is incapable of learning a specific bias, suggesting that there are possible architectural countermeasures to the problem of biases in deep neural networks.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22312;&#33455;&#29255;&#19978;&#23454;&#29616;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#37327;&#23376;&#28608;&#27963;&#31070;&#32463;&#27700;&#24211;&#65292;&#20351;&#32593;&#32476;&#35268;&#27169;&#21644;&#21151;&#32791;&#22343;&#22823;&#24133;&#25552;&#39640;&#65292;&#21487;&#29992;&#20110;&#23454;&#29616;&#20855;&#26377;&#24377;&#24615;&#36523;&#20221;&#39564;&#35777;&#21151;&#33021;&#30340;&#22823;&#22411;&#30828;&#20214;&#23433;&#20840;&#27169;&#22411;</title><link>https://arxiv.org/abs/2403.14188</link><description>&lt;p&gt;
&#33455;&#29255;&#19978;&#30340;&#37327;&#23376;&#28608;&#27963;&#31070;&#32463;&#27700;&#24211;&#20026;&#20855;&#26377;&#24377;&#24615;&#36523;&#20221;&#39564;&#35777;&#30340;&#22823;&#22411;&#30828;&#20214;&#23433;&#20840;&#27169;&#22411;&#25171;&#24320;&#20102;&#22823;&#38376;
&lt;/p&gt;
&lt;p&gt;
Quantum-activated neural reservoirs on-chip open up large hardware security models for resilient authentication
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14188
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22312;&#33455;&#29255;&#19978;&#23454;&#29616;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#37327;&#23376;&#28608;&#27963;&#31070;&#32463;&#27700;&#24211;&#65292;&#20351;&#32593;&#32476;&#35268;&#27169;&#21644;&#21151;&#32791;&#22343;&#22823;&#24133;&#25552;&#39640;&#65292;&#21487;&#29992;&#20110;&#23454;&#29616;&#20855;&#26377;&#24377;&#24615;&#36523;&#20221;&#39564;&#35777;&#21151;&#33021;&#30340;&#22823;&#22411;&#30828;&#20214;&#23433;&#20840;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14188v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#21069;&#27839;&#65292;&#24320;&#21019;&#24615;&#22320;&#21033;&#29992;&#37327;&#23376;&#20154;&#24037;&#26234;&#33021;&#30005;&#36335;&#35299;&#20915;&#28145;&#24230;&#23398;&#20064;&#21644;&#32463;&#20856;&#26550;&#26500;&#26080;&#27861;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;&#35813;&#24037;&#20316;&#23454;&#29616;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#37327;&#23376;&#28608;&#27963;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#65292;&#25317;&#26377;&#36229;&#36807;3&#19975;&#20159;&#30828;&#20214;&#33410;&#28857;/&#24179;&#26041;&#21400;&#31859;&#65292;&#28304;&#20110;&#33455;&#29255;&#19978;&#38598;&#25104;&#30340;&#38750;&#26230;&#26448;&#26009;&#20013;&#21487;&#37325;&#22797;&#30340;&#21407;&#23376;&#23610;&#24230;&#25104;&#26680;&#21160;&#24577;&#65292;&#27599;&#20010;&#35835;&#21462;&#36890;&#36947;&#30340;&#25511;&#21046;&#30005;&#21147;&#20026;0.07 nW&#12290;&#19982;&#30446;&#21069;&#25253;&#36947;&#30340;&#34920;&#29616;&#26368;&#22909;&#30340;&#27700;&#24211;&#30456;&#27604;&#65292;&#35813;&#23454;&#26045;&#23558;&#32593;&#32476;&#35268;&#27169;&#25552;&#39640;&#20102;&#20004;&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#23558;&#21151;&#32791;&#38477;&#20302;&#20102;&#20845;&#20493;&#65292;&#36798;&#21040;&#20102;&#20154;&#33041;&#21151;&#29575;&#25928;&#29575;&#33539;&#22260;&#65292;&#27599;&#20010;&#31070;&#32463;&#20803;&#28040;&#32791;0.2 nW&#12290;&#24403;&#30001;&#32463;&#20856;&#36755;&#20837;&#23457;&#38382;&#26102;&#65292;&#33455;&#29255;&#23454;&#29616;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#30828;&#20214;&#23433;&#20840;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20813;&#35789;&#20856;&#36523;&#20221;&#39564;&#35777;&#65292;&#21487;&#20197;&#25269;&#25239;&#32479;&#35745;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14188v1 Announce Type: cross  Abstract: Quantum artificial intelligence is a frontier of artificial intelligence research, pioneering quantum AI-powered circuits to address problems beyond the reach of deep learning with classical architectures. This work implements a large-scale quantum-activated recurrent neural network possessing more than 3 trillion hardware nodes/cm$^2$, originating from repeatable atomic-scale nucleation dynamics in an amorphous material integrated on-chip, controlled with 0.07 nW electric power per readout channel. Compared to the best-performing reservoirs currently reported, this implementation increases the scale of the network by two orders of magnitude and reduces the power consumption by six, reaching power efficiencies in the range of the human brain, dissipating 0.2 nW/neuron. When interrogated by a classical input, the chip implements a large-scale hardware security model, enabling dictionary-free authentication secure against statistical inf
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;StyleGAN&#33258;&#21160;&#29983;&#25104;&#38745;&#27490;&#39118;&#26223;&#22270;&#20687;&#30340;&#30005;&#24433;&#22270;&#26223;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#23618;&#29305;&#24449;&#31354;&#38388;&#21644;&#22810;&#23610;&#24230;&#28145;&#24230;&#29305;&#24449;&#25197;&#26354;&#65288;MSDFW&#65289;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#19988;&#20855;&#26377;&#21512;&#29702;&#24490;&#29615;&#21160;&#30011;&#30340;&#30005;&#24433;&#22270;&#26223;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;</title><link>https://arxiv.org/abs/2403.14186</link><description>&lt;p&gt;
&#20351;&#29992;&#39044;&#35757;&#32451;StyleGAN&#29983;&#25104;&#39118;&#26684;&#30005;&#24433;&#22270;&#26223;
&lt;/p&gt;
&lt;p&gt;
StyleCineGAN: Landscape Cinemagraph Generation using a Pre-trained StyleGAN
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14186
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;StyleGAN&#33258;&#21160;&#29983;&#25104;&#38745;&#27490;&#39118;&#26223;&#22270;&#20687;&#30340;&#30005;&#24433;&#22270;&#26223;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#28145;&#23618;&#29305;&#24449;&#31354;&#38388;&#21644;&#22810;&#23610;&#24230;&#28145;&#24230;&#29305;&#24449;&#25197;&#26354;&#65288;MSDFW&#65289;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#19988;&#20855;&#26377;&#21512;&#29702;&#24490;&#29615;&#21160;&#30011;&#30340;&#30005;&#24433;&#22270;&#26223;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;StyleGAN&#33258;&#21160;&#29983;&#25104;&#38745;&#27490;&#39118;&#26223;&#22270;&#20687;&#30340;&#30005;&#24433;&#22270;&#26223;&#12290;&#21463;&#21040;&#26368;&#36817;&#26080;&#26465;&#20214;&#35270;&#39057;&#29983;&#25104;&#25104;&#21151;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#21033;&#29992;&#24378;&#22823;&#30340;&#39044;&#35757;&#32451;&#22270;&#20687;&#29983;&#25104;&#22120;&#26469;&#21512;&#25104;&#39640;&#36136;&#37327;&#30340;&#30005;&#24433;&#22270;&#26223;&#12290;&#19982;&#20808;&#21069;&#20027;&#35201;&#21033;&#29992;&#39044;&#35757;&#32451;StyleGAN&#30340;&#28508;&#22312;&#31354;&#38388;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20854;&#28145;&#23618;&#29305;&#24449;&#31354;&#38388;&#36827;&#34892;GAN&#21453;&#28436;&#21644;&#30005;&#24433;&#22270;&#26223;&#29983;&#25104;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23610;&#24230;&#28145;&#24230;&#29305;&#24449;&#25197;&#26354;&#65288;MSDFW&#65289;&#65292;&#23427;&#22312;&#19981;&#21516;&#20998;&#36776;&#29575;&#19979;&#25197;&#26354;&#20102;&#39044;&#35757;&#32451;StyleGAN&#30340;&#20013;&#38388;&#29305;&#24449;&#12290;&#36890;&#36807;&#20351;&#29992;MSDFW&#65292;&#29983;&#25104;&#30340;&#30005;&#24433;&#22270;&#26223;&#20855;&#26377;&#39640;&#20998;&#36776;&#29575;&#65292;&#24182;&#23637;&#31034;&#20986;&#21512;&#29702;&#30340;&#24490;&#29615;&#21160;&#30011;&#12290;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#21644;&#19982;&#26368;&#20808;&#36827;&#30340;&#30005;&#24433;&#22270;&#26223;&#29983;&#25104;&#26041;&#27861;&#20197;&#21450;&#20351;&#29992;&#39044;&#35757;&#32451;StyleGAN&#30340;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#30340;&#23450;&#37327;&#27604;&#36739;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14186v1 Announce Type: cross  Abstract: We propose a method that can generate cinemagraphs automatically from a still landscape image using a pre-trained StyleGAN. Inspired by the success of recent unconditional video generation, we leverage a powerful pre-trained image generator to synthesize high-quality cinemagraphs. Unlike previous approaches that mainly utilize the latent space of a pre-trained StyleGAN, our approach utilizes its deep feature space for both GAN inversion and cinemagraph generation. Specifically, we propose multi-scale deep feature warping (MSDFW), which warps the intermediate features of a pre-trained StyleGAN at different resolutions. By using MSDFW, the generated cinemagraphs are of high resolution and exhibit plausible looping animation. We demonstrate the superiority of our method through user studies and quantitative comparisons with state-of-the-art cinemagraph generation methods and a video generation method that uses a pre-trained StyleGAN.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;OTSeg&#20013;&#30340;Multi-Prompts Sinkhorn Attention&#26426;&#21046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14183</link><description>&lt;p&gt;
OTSeg&#65306;&#22810;&#25552;&#31034;Sinkhorn&#27880;&#24847;&#21147;&#29992;&#20110;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
OTSeg: Multi-prompt Sinkhorn Attention for Zero-Shot Semantic Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14183
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;OTSeg&#20013;&#30340;Multi-Prompts Sinkhorn Attention&#26426;&#21046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#21033;&#29992;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#26469;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#65292;&#20174;&#32780;&#25552;&#21319;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
CLIP&#30340;&#26368;&#26032;&#25104;&#21151;&#35777;&#26126;&#20102;&#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#30693;&#35782;&#36716;&#31227;&#21040;&#20687;&#32032;&#32423;&#20998;&#31867;&#26469;&#36827;&#34892;&#38646;&#26679;&#26412;&#35821;&#20041;&#20998;&#21106;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#65292;&#21033;&#29992;&#39044;&#20808;&#35757;&#32451;&#30340;CLIP&#30693;&#35782;&#26469;&#32039;&#23494;&#23545;&#40784;&#25991;&#26412;&#23884;&#20837;&#21644;&#20687;&#32032;&#23884;&#20837;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OTSeg&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#26088;&#22312;&#22686;&#24378;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#21305;&#37197;&#30456;&#20851;&#20687;&#32032;&#23884;&#20837;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#65288;OT&#65289;&#31639;&#27861;&#30340;&#22810;&#25552;&#31034;Sinkhorn&#65288;MPS&#65289;&#65292;&#36825;&#20351;&#24471;&#22810;&#20010;&#25991;&#26412;&#25552;&#31034;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#20851;&#27880;&#22270;&#20687;&#20687;&#32032;&#20869;&#30340;&#21508;&#31181;&#35821;&#20041;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#21463;&#21040;Sinkformers&#22312;&#21333;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MPS&#30340;&#25193;&#23637;&#65292;&#31216;&#20026;&#22810;&#25552;&#31034;Sinkhorn&#27880;&#24847;&#21147;&#65288;MPSA&#65289;&#65292;&#23427;&#26377;&#25928;&#22320;&#21462;&#20195;&#20102;Transformer&#26694;&#26550;&#20013;&#22810;&#27169;&#24577;&#35774;&#32622;&#20013;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14183v1 Announce Type: cross  Abstract: The recent success of CLIP has demonstrated promising results in zero-shot semantic segmentation by transferring muiltimodal knowledge to pixel-level classification. However, leveraging pre-trained CLIP knowledge to closely align text embeddings with pixel embeddings still has limitations in existing approaches. To address this issue, we propose OTSeg, a novel multimodal attention mechanism aimed at enhancing the potential of multiple text prompts for matching associated pixel embeddings. We first propose Multi-Prompts Sinkhorn (MPS) based on the Optimal Transport (OT) algorithm, which leads multiple text prompts to selectively focus on various semantic features within image pixels. Moreover, inspired by the success of Sinkformers in unimodal settings, we introduce the extension of MPS, called Multi-Prompts Sinkhorn Attention (MPSA), which effectively replaces cross-attention mechanisms within Transformer framework in multimodal settin
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25151;&#38388;-&#29289;&#20307;&#20851;&#31995;&#30693;&#35782;&#30340;&#25968;&#25454;&#39537;&#21160;&#12289;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#36890;&#36947;Swin-Unet&#26550;&#26500;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#20197;&#22686;&#24378;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#30446;&#26631;&#23548;&#33322;&#12290;</title><link>https://arxiv.org/abs/2403.14163</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25151;&#38388;-&#29289;&#20307;&#20851;&#31995;&#30693;&#35782;&#22686;&#24378;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#30446;&#26631;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Model-based Room-Object Relationships Knowledge for Enhancing Multimodal-Input Object Goal Navigation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14163
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25151;&#38388;-&#29289;&#20307;&#20851;&#31995;&#30693;&#35782;&#30340;&#25968;&#25454;&#39537;&#21160;&#12289;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#36890;&#36947;Swin-Unet&#26550;&#26500;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#20197;&#22686;&#24378;&#22810;&#27169;&#24577;&#36755;&#20837;&#30340;&#30446;&#26631;&#23548;&#33322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#20307;&#30446;&#26631;&#23548;&#33322;&#26159;&#20855;&#26377;&#36523;&#20307;&#23553;&#35013;&#23548;&#33322;&#20219;&#21153;&#30340;&#19968;&#20010;&#20851;&#38190;&#24037;&#31243;&#20219;&#21153;&#65307;&#23427;&#28041;&#21450;&#22312;&#30475;&#19981;&#35265;&#30340;&#29615;&#22659;&#20013;&#23548;&#33322;&#21040;&#25351;&#23450;&#29289;&#20307;&#31867;&#21035;&#30340;&#19968;&#20010;&#23454;&#20363;&#12290;&#23613;&#31649;&#22312;&#31471;&#21040;&#31471;&#21644;&#27169;&#22359;&#21270;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#20294;&#35201;&#23436;&#20840;&#20351;&#20195;&#29702;&#20154;&#36890;&#36807;&#24863;&#30693;&#30693;&#35782;&#29702;&#35299;&#29615;&#22659;&#24182;&#20687;&#20154;&#31867;&#19968;&#26679;&#26377;&#25928;&#22320;&#25191;&#34892;&#29289;&#20307;&#30446;&#26631;&#23548;&#33322;&#20173;&#28982;&#26159;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#19968;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;&#23427;&#20204;&#22312;&#30693;&#35782;&#25552;&#21462;&#21644;&#25972;&#21512;&#26041;&#38754;&#30340;&#24378;&#22823;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30340;&#23545;&#35937;-&#25151;&#38388;&#20851;&#31995;&#30340;&#24120;&#35782;&#30693;&#35782;&#12290;&#25105;&#20204;&#21033;&#29992;&#22810;&#36890;&#36947;Swin-Unet&#26550;&#26500;&#36827;&#34892;&#22810;&#20219;&#21153;&#23398;&#20064;&#65292;&#21516;&#26102;&#32467;&#21512;&#22810;&#27169;&#24577;&#36755;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14163v1 Announce Type: cross  Abstract: Object-goal navigation is a crucial engineering task for the community of embodied navigation; it involves navigating to an instance of a specified object category within unseen environments. Although extensive investigations have been conducted on both end-to-end and modular-based, data-driven approaches, fully enabling an agent to comprehend the environment through perceptual knowledge and perform object-goal navigation as efficiently as humans remains a significant challenge. Recently, large language models have shown potential in this task, thanks to their powerful capabilities for knowledge extraction and integration. In this study, we propose a data-driven, modular-based approach, trained on a dataset that incorporates common-sense knowledge of object-to-room relationships extracted from a large language model. We utilize the multi-channel Swin-Unet architecture to conduct multi-task learning incorporating with multimodal inputs.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;$h$-PMD&#65292;&#23427;&#36890;&#36807;&#22312;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#32467;&#21512;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#21644;&#21069;&#30651;&#28145;&#24230;$h&#65292;&#20197;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.14156</link><description>&lt;p&gt;
&#20855;&#26377;&#21069;&#30651;&#29305;&#24615;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Policy Mirror Descent with Lookahead
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14156
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#31639;&#27861;$h$-PMD&#65292;&#23427;&#36890;&#36807;&#22312;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#32467;&#21512;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#21644;&#21069;&#30651;&#28145;&#24230;$h&#65292;&#20197;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31574;&#30053;&#38236;&#20687;&#19979;&#38477;&#65288;PMD&#65289;&#20316;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#31639;&#27861;&#26694;&#26550;&#65292;&#21253;&#25324;&#20960;&#31181;&#37325;&#35201;&#30340;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#65292;&#22914;&#33258;&#28982;&#31574;&#30053;&#26799;&#24230;&#65292;&#24182;&#19982;&#26368;&#20808;&#36827;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65288;&#22914;TRPO&#21644;PPO&#65289;&#30456;&#32852;&#31995;&#12290;PMD&#21487;&#20197;&#30475;&#20316;&#26159;&#23454;&#29616;&#27491;&#21017;&#21270;1&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#30340;&#36719;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;&#28982;&#32780;&#65292;1&#27493;&#36138;&#24515;&#31574;&#30053;&#21487;&#33021;&#19981;&#26159;&#26368;&#20339;&#36873;&#25321;&#65292;&#26368;&#36817;&#22312;RL&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#23454;&#35777;&#25104;&#21151;&#65292;&#22914;AlphaGo&#21644;AlphaZero&#24050;&#32463;&#35777;&#26126;&#65292;&#30456;&#23545;&#20110;&#22810;&#27493;&#39588;&#65292;&#36138;&#24515;&#26041;&#27861;&#21487;&#20197;&#36229;&#36234;&#23427;&#20204;&#30340;1&#27493;&#39588;&#23545;&#24212;&#29289;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31867;&#21035;&#30340;PMD&#31639;&#27861;&#65292;&#31216;&#20026;$h$-PMD&#65292;&#23427;&#23558;&#20855;&#26377;&#21069;&#30651;&#28145;&#24230;$h$&#30340;&#22810;&#27493;&#36138;&#24515;&#31574;&#30053;&#25913;&#36827;&#32467;&#21512;&#21040;PMD&#26356;&#26032;&#35268;&#21017;&#20013;&#12290;&#20026;&#20102;&#35299;&#20915;&#25240;&#25187;&#26080;&#38480;&#26102;&#38388;&#35270;&#35282;&#19979;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20854;&#20013;&#25240;&#25187;&#22240;&#23376;&#20026;$\gamma$&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;$h$-PMD&#21487;&#20197;&#25512;&#24191;&#26631;&#20934;&#30340;PMD&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14156v1 Announce Type: cross  Abstract: Policy Mirror Descent (PMD) stands as a versatile algorithmic framework encompassing several seminal policy gradient algorithms such as natural policy gradient, with connections with state-of-the-art reinforcement learning (RL) algorithms such as TRPO and PPO. PMD can be seen as a soft Policy Iteration algorithm implementing regularized 1-step greedy policy improvement. However, 1-step greedy policies might not be the best choice and recent remarkable empirical successes in RL such as AlphaGo and AlphaZero have demonstrated that greedy approaches with respect to multiple steps outperform their 1-step counterpart. In this work, we propose a new class of PMD algorithms called $h$-PMD which incorporates multi-step greedy policy improvement with lookahead depth $h$ to the PMD update rule. To solve discounted infinite horizon Markov Decision Processes with discount factor $\gamma$, we show that $h$-PMD which generalizes the standard PMD enj
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#19982;&#25366;&#25496;&#20013;&#30340;&#21457;&#23637;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39044;&#22788;&#29702;&#12289;&#23384;&#20648;&#12289;&#20998;&#26512;&#12289;&#39044;&#27979;&#12289;&#25512;&#33616;&#12289;&#20998;&#31867;&#12289;&#20272;&#35745;&#21644;&#26816;&#27979;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.14151</link><description>&lt;p&gt;
&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#19982;&#25366;&#25496;&#30340;&#28145;&#24230;&#23398;&#20064;&#65306;&#35843;&#26597;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Deep Learning for Trajectory Data Management and Mining: A Survey and Beyond
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#19982;&#25366;&#25496;&#20013;&#30340;&#21457;&#23637;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#25506;&#35752;&#20102;&#20854;&#22312;&#39044;&#22788;&#29702;&#12289;&#23384;&#20648;&#12289;&#20998;&#26512;&#12289;&#39044;&#27979;&#12289;&#25512;&#33616;&#12289;&#20998;&#31867;&#12289;&#20272;&#35745;&#21644;&#26816;&#27979;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14151v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#36234; &#25277;&#35937;&#65306;&#36712;&#36857;&#35745;&#31639;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#39046;&#22495;&#65292;&#28085;&#30422;&#36712;&#36857;&#25968;&#25454;&#31649;&#29702;&#21644;&#25366;&#25496;&#65292;&#22240;&#20854;&#22312;&#35832;&#22914;&#20301;&#32622;&#26381;&#21153;&#12289;&#22478;&#24066;&#20132;&#36890;&#21644;&#20844;&#20849;&#23433;&#20840;&#31561;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20851;&#38190;&#20316;&#29992;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20256;&#32479;&#26041;&#27861;&#20391;&#37325;&#20110;&#31616;&#21333;&#30340;&#26102;&#31354;&#29305;&#24449;&#65292;&#38754;&#20020;&#22797;&#26434;&#35745;&#31639;&#12289;&#26377;&#38480;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#19981;&#36275;&#20197;&#36866;&#24212;&#29616;&#23454;&#22797;&#26434;&#24615;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#36712;&#36857;&#35745;&#31639;&#20013;&#28145;&#24230;&#23398;&#20064;&#30340;&#21457;&#23637;&#21644;&#26368;&#26032;&#36827;&#23637;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#22238;&#39038;&#65288;DL4Traj&#65289;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#36712;&#36857;&#25968;&#25454;&#65292;&#24182;&#31616;&#35201;&#20171;&#32461;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#31995;&#32479;&#22320;&#25506;&#35752;&#20102;&#28145;&#24230;&#23398;&#20064;&#22312;&#36712;&#36857;&#31649;&#29702;&#65288;&#39044;&#22788;&#29702;&#12289;&#23384;&#20648;&#12289;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#65289;&#21644;&#25366;&#25496;&#65288;&#19982;&#36712;&#36857;&#30456;&#20851;&#30340;&#39044;&#27979;&#12289;&#36712;&#36857;&#30456;&#20851;&#30340;&#25512;&#33616;&#12289;&#36712;&#36857;&#20998;&#31867;&#12289;&#26053;&#34892;&#26102;&#38388;&#20272;&#35745;&#12289;&#24322;&#24120;&#26816;&#27979;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14151v1 Announce Type: cross  Abstract: Trajectory computing is a pivotal domain encompassing trajectory data management and mining, garnering widespread attention due to its crucial role in various practical applications such as location services, urban traffic, and public safety. Traditional methods, focusing on simplistic spatio-temporal features, face challenges of complex calculations, limited scalability, and inadequate adaptability to real-world complexities. In this paper, we present a comprehensive review of the development and recent advances in deep learning for trajectory computing (DL4Traj). We first define trajectory data and provide a brief overview of widely-used deep learning models. Systematically, we explore deep learning applications in trajectory management (pre-processing, storage, analysis, and visualization) and mining (trajectory-related forecasting, trajectory-related recommendation, trajectory classification, travel time estimation, anomaly detecti
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36951;&#20256;&#35268;&#21010;&#29983;&#25104;&#30340;&#22522;&#20934;&#20989;&#25968;&#33021;&#22815;&#26356;&#22909;&#22320;&#21306;&#20998;&#31639;&#27861;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#33258;&#21160;&#35774;&#35745;&#22522;&#20934;&#20989;&#25968;&#21644;&#27604;&#36739;&#36827;&#21270;&#31639;&#27861;&#30340;&#30446;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.14146</link><description>&lt;p&gt;
&#36890;&#36807;&#36951;&#20256;&#35268;&#21010;&#28436;&#21270;&#22522;&#20934;&#20989;&#25968;&#26469;&#27604;&#36739;&#36827;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evolving Benchmark Functions to Compare Evolutionary Algorithms via Genetic Programming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14146
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36951;&#20256;&#35268;&#21010;&#29983;&#25104;&#30340;&#22522;&#20934;&#20989;&#25968;&#33021;&#22815;&#26356;&#22909;&#22320;&#21306;&#20998;&#31639;&#27861;&#65292;&#36827;&#19968;&#27493;&#23454;&#29616;&#20102;&#33258;&#21160;&#35774;&#35745;&#22522;&#20934;&#20989;&#25968;&#21644;&#27604;&#36739;&#36827;&#21270;&#31639;&#27861;&#30340;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#36951;&#20256;&#35268;&#21010;&#65288;GP&#65289;&#26469;&#26500;&#24314;&#26032;&#30340;&#20248;&#21270;&#22522;&#20934;&#20989;&#25968;&#12290;&#20248;&#21270;&#22522;&#20934;&#22312;&#23637;&#31034;&#36827;&#21270;&#31639;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#26041;&#38754;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#20351;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#21644;&#27604;&#36739;&#25104;&#20026;&#21487;&#33021;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;GP&#29983;&#25104;&#30340;&#22522;&#20934;&#33021;&#22815;&#27604;&#20154;&#24037;&#21046;&#20316;&#30340;&#22522;&#20934;&#26356;&#22909;&#22320;&#21306;&#20998;&#31639;&#27861;&#12290;GP&#30340;&#36866;&#24212;&#24230;&#34913;&#37327;&#26159;&#19968;&#23545;&#20248;&#21270;&#22120;&#25214;&#21040;&#30340;&#35299;&#30340;Wasserstein&#36317;&#31163;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;MAP-Elites&#26469;&#22686;&#24378;GP&#30340;&#25628;&#32034;&#33021;&#21147;&#65292;&#24182;&#23637;&#31034;&#20102;&#20248;&#21270;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#22914;&#20309;&#38543;&#30528;&#21508;&#31181;&#26223;&#35266;&#29305;&#24449;&#30340;&#25913;&#21464;&#32780;&#25913;&#21464;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#21160;&#21270;&#35774;&#35745;&#22522;&#20934;&#20989;&#25968;&#21644;&#27604;&#36739;&#36827;&#21270;&#31639;&#27861;&#30340;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14146v1 Announce Type: cross  Abstract: In this study, we use Genetic Programming (GP) to compose new optimization benchmark functions. Optimization benchmarks have the important role of showing the differences between evolutionary algorithms, making it possible for further analysis and comparisons. We show that the benchmarks generated by GP are able to differentiate algorithms better than human-made benchmark functions. The fitness measure of the GP is the Wasserstein distance of the solutions found by a pair of optimizers. Additionally, we use MAP-Elites to both enhance the search power of the GP and also illustrate how the difference between optimizers changes by various landscape features. Our approach provides a novel way to automate the design of benchmark functions and to compare evolutionary algorithms.
&lt;/p&gt;</description></item><item><title>&#32852;&#37030;&#23398;&#20064;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#20013;&#30340;&#24212;&#29992;&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#38544;&#31169;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21319;PIUs&#24615;&#33021;&#30340;&#36845;&#20195;&#24133;&#20540;&#21098;&#26525;&#25216;&#26415;&#12290;</title><link>https://arxiv.org/abs/2403.14120</link><description>&lt;p&gt;
&#29992;&#36845;&#20195;&#24133;&#20540;&#21098;&#26525;&#25512;&#36827;&#24037;&#19994;&#29289;&#32852;&#32593;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Advancing IIoT with Over-the-Air Federated Learning: The Role of Iterative Magnitude Pruning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14120
&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#22312;&#24037;&#19994;&#29289;&#32852;&#32593;&#20013;&#30340;&#24212;&#29992;&#20419;&#36827;&#20102;&#26426;&#22120;&#23398;&#20064;&#21644;&#25968;&#25454;&#38544;&#31169;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25552;&#21319;PIUs&#24615;&#33021;&#30340;&#36845;&#20195;&#24133;&#20540;&#21098;&#26525;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#19994;&#29289;&#32852;&#32593;&#65288;IIoT&#65289;&#22312;&#24037;&#19994;4.0&#30340;&#32972;&#26223;&#19979;&#36814;&#26469;&#20102;&#19968;&#31181;&#20114;&#32852;&#30340;&#26234;&#33021;&#35774;&#22791;&#26102;&#20195;&#65292;&#25968;&#25454;&#39537;&#21160;&#30340;&#35265;&#35299;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#34701;&#21512;&#65292;&#24443;&#24213;&#25913;&#21464;&#20102;&#21046;&#36896;&#19994;&#12290; IIoT&#20013;&#19968;&#20010;&#20540;&#24471;&#20851;&#27880;&#30340;&#21457;&#23637;&#26159;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#30340;&#25972;&#21512;&#65292;&#35813;&#25216;&#26415;&#35299;&#20915;&#20102;&#35774;&#22791;&#20043;&#38388;&#25968;&#25454;&#38544;&#31169;&#21644;&#23433;&#20840;&#24615;&#30340;&#38382;&#39064;&#12290; FL&#20351;&#36793;&#32536;&#20256;&#24863;&#22120;&#65288;&#20063;&#31216;&#20026;&#22806;&#22260;&#26234;&#33021;&#21333;&#20803;&#65288;PIUs&#65289;&#65289;&#33021;&#22815;&#20351;&#29992;&#26412;&#22320;&#25968;&#25454;&#36827;&#34892;&#23398;&#20064;&#21644;&#36866;&#24212;&#65292;&#26080;&#38656;&#26174;&#24335;&#20849;&#20139;&#26426;&#23494;&#25968;&#25454;&#65292;&#20174;&#32780;&#20419;&#36827;&#21327;&#20316;&#20294;&#26426;&#23494;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;PIUs&#36739;&#20302;&#30340;&#20869;&#23384;&#21344;&#29992;&#21644;&#35745;&#31639;&#33021;&#21147;&#22266;&#26377;&#22320;&#38656;&#35201;&#20855;&#26377;&#38750;&#24120;&#32043;&#32039;&#20945;&#23610;&#23544;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#27169;&#22411;&#12290;&#21098;&#26525;&#31561;&#27169;&#22411;&#21387;&#32553;&#25216;&#26415;&#21487;&#29992;&#20110;&#36890;&#36807;&#31227;&#38500;&#23545;&#27169;&#22411;&#24615;&#33021;&#24433;&#21709;&#36739;&#23567;&#30340;&#19981;&#24517;&#35201;&#36830;&#25509;&#26469;&#20943;&#23567;DNN&#27169;&#22411;&#30340;&#22823;&#23567;&#65292;&#20174;&#32780;&#20351;&#27169;&#22411;&#26356;&#36866;&#21512;&#26377;&#38480;&#30340;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14120v1 Announce Type: cross  Abstract: The industrial Internet of Things (IIoT) under Industry 4.0 heralds an era of interconnected smart devices where data-driven insights and machine learning (ML) fuse to revolutionize manufacturing. A noteworthy development in IIoT is the integration of federated learning (FL), which addresses data privacy and security among devices. FL enables edge sensors, also known as peripheral intelligence units (PIUs) to learn and adapt using their data locally, without explicit sharing of confidential data, to facilitate a collaborative yet confidential learning process. However, the lower memory footprint and computational power of PIUs inherently require deep neural network (DNN) models that have a very compact size. Model compression techniques such as pruning can be used to reduce the size of DNN models by removing unnecessary connections that have little impact on the model's performance, thus making the models more suitable for the limited 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.14119</link><description>&lt;p&gt;
C-TPT&#65306;&#36890;&#36807;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#26657;&#20934;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#27979;&#35797;&#26102;&#36866;&#24212;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#20363;&#35777;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#31034;&#20027;&#35201;&#26159;&#20026;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#32780;&#24320;&#21457;&#30340;&#65292;&#24573;&#35270;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#8212;&#8212;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26657;&#20934;&#26041;&#27861;&#20381;&#36182;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#27979;&#35797;&#26102;&#22330;&#26223;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#25506;&#35752;&#26657;&#20934;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14119v1 Announce Type: cross  Abstract: In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion
&lt;/p&gt;</description></item><item><title>HAAM-RL&#26041;&#27861;&#32467;&#21512;&#20102;&#21551;&#21457;&#24335;&#31639;&#27861;&#21160;&#20316;&#23631;&#34109;&#21644;&#38598;&#25104;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#27773;&#36710;&#21943;&#28422;&#36807;&#31243;&#20013;&#30340;&#39068;&#33394;&#25209;&#22788;&#29702;&#37325;&#26032;&#25490;&#24207;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21462;&#24471;&#20102;16.25%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.14110</link><description>&lt;p&gt;
&#22522;&#20110;&#21551;&#21457;&#24335;&#31639;&#27861;&#21160;&#20316;&#23631;&#34109;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;HAAM-RL&#65289;&#19982;&#38598;&#25104;&#25512;&#26029;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Heuristic Algorithm-based Action Masking Reinforcement Learning (HAAM-RL) with Ensemble Inference Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14110
&lt;/p&gt;
&lt;p&gt;
HAAM-RL&#26041;&#27861;&#32467;&#21512;&#20102;&#21551;&#21457;&#24335;&#31639;&#27861;&#21160;&#20316;&#23631;&#34109;&#21644;&#38598;&#25104;&#25512;&#26029;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#27773;&#36710;&#21943;&#28422;&#36807;&#31243;&#20013;&#30340;&#39068;&#33394;&#25209;&#22788;&#29702;&#37325;&#26032;&#25490;&#24207;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#21462;&#24471;&#20102;16.25%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HAAM-RL&#65288;&#22522;&#20110;&#21551;&#21457;&#24335;&#31639;&#27861;&#21160;&#20316;&#23631;&#34109;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#30340;&#26032;&#22411;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#27773;&#36710;&#21943;&#28422;&#36807;&#31243;&#20013;&#30340;&#39068;&#33394;&#25209;&#22788;&#29702;&#37325;&#26032;&#25490;&#24207;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22810;&#20010;&#20851;&#38190;&#25216;&#26415;&#65292;&#21253;&#25324;&#23450;&#21046;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#24418;&#24335;&#21270;&#65292;&#22870;&#21169;&#35774;&#32622;&#21253;&#25324;&#22522;&#20110;&#28508;&#21147;&#30340;&#22870;&#21169;&#22609;&#36896;&#65292;&#20351;&#29992;&#21551;&#21457;&#24335;&#31639;&#27861;&#36827;&#34892;&#21160;&#20316;&#23631;&#34109;&#65288;HAAM-RL&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#32467;&#21512;&#22810;&#20010;RL&#27169;&#22411;&#30340;&#38598;&#25104;&#25512;&#26029;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;HAAM-RL&#19982;&#38598;&#25104;&#25512;&#26029;&#26041;&#27861;&#22312;30&#20010;&#22330;&#26223;&#20013;&#21462;&#24471;&#20102;16.25%&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14110v1 Announce Type: cross  Abstract: This paper presents a novel reinforcement learning (RL) approach called HAAM-RL (Heuristic Algorithm-based Action Masking Reinforcement Learning) for optimizing the color batching re-sequencing problem in automobile painting processes. The existing heuristic algorithms have limitations in adequately reflecting real-world constraints and accurately predicting logistics performance. Our methodology incorporates several key techniques including a tailored Markov Decision Process (MDP) formulation, reward setting including Potential-Based Reward Shaping, action masking using heuristic algorithms (HAAM-RL), and an ensemble inference method that combines multiple RL models. The RL agent is trained and evaluated using FlexSim, a commercial 3D simulation software, integrated with our RL MLOps platform BakingSoDA. Experimental results across 30 scenarios demonstrate that HAAM-RL with an ensemble inference method achieves a 16.25% performance im
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22312;DouZero&#27169;&#22411;&#20013;&#24341;&#20837;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#25506;&#32034;&#19981;&#21516;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#26007;&#22320;&#20027;&#28216;&#25103;&#20013;&#33719;&#32988;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.14102</link><description>&lt;p&gt;
DouRN: &#36890;&#36807;&#27531;&#24046;&#31070;&#32463;&#32593;&#32476;&#25913;&#36827;DouZero
&lt;/p&gt;
&lt;p&gt;
DouRN: Improving DouZero by Residual Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14102
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22312;DouZero&#27169;&#22411;&#20013;&#24341;&#20837;&#27531;&#24046;&#32593;&#32476;&#65292;&#24182;&#25506;&#32034;&#19981;&#21516;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#25105;&#20204;&#26174;&#33879;&#25552;&#39640;&#20102;&#26007;&#22320;&#20027;&#28216;&#25103;&#20013;&#33719;&#32988;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#20855;&#26377;&#19981;&#23436;&#20840;&#20449;&#24687;&#30340;&#28216;&#25103;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#22312;&#21345;&#29260;&#28216;&#25103;&#26007;&#22320;&#20027;&#26041;&#38754;&#30340;&#34920;&#29616;&#20173;&#28982;&#20196;&#20154;&#19981;&#28385;&#24847;&#12290;&#26007;&#22320;&#20027;&#19981;&#21516;&#20110;&#20256;&#32479;&#28216;&#25103;&#65292;&#23427;&#28041;&#21450;&#19977;&#21517;&#29609;&#23478;&#65292;&#32467;&#21512;&#20102;&#21512;&#20316;&#21644;&#23545;&#25239;&#30340;&#20803;&#32032;&#65292;&#23548;&#33268;&#29366;&#24577;&#31354;&#38388;&#21644;&#21160;&#20316;&#31354;&#38388;&#36739;&#22823;&#12290;2021&#24180;&#65292;&#19968;&#27454;&#21517;&#20026;DouZero&#30340;&#26007;&#22320;&#20027;&#31243;&#24207;&#36890;&#36807;&#21033;&#29992;&#20256;&#32479;&#30340;&#33945;&#29305;&#21345;&#27931;&#26041;&#27861;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#27809;&#26377;&#20808;&#39564;&#30693;&#35782;&#30340;&#27169;&#22411;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#23558;&#27531;&#24046;&#32593;&#32476;&#32435;&#20837;&#27169;&#22411;&#20013;&#65292;&#25506;&#32034;&#19981;&#21516;&#30340;&#26550;&#26500;&#35774;&#35745;&#65292;&#24182;&#36827;&#34892;&#22810;&#35282;&#33394;&#27979;&#35797;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#22312;&#30456;&#21516;&#30340;&#35757;&#32451;&#26102;&#38388;&#20869;&#26174;&#33879;&#25552;&#39640;&#20102;&#33719;&#32988;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21628;&#21483;&#24471;&#20998;&#31995;&#32479;&#65292;&#24110;&#21161;&#20195;&#29702;&#20915;&#23450;&#26159;&#21542;&#25104;&#20026;&#22320;&#20027;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14102v1 Announce Type: new  Abstract: Deep reinforcement learning has made significant progress in games with imperfect information, but its performance in the card game Doudizhu (Chinese Poker/Fight the Landlord) remains unsatisfactory. Doudizhu is different from conventional games as it involves three players and combines elements of cooperation and confrontation, resulting in a large state and action space. In 2021, a Doudizhu program called DouZero\cite{zha2021douzero} surpassed previous models without prior knowledge by utilizing traditional Monte Carlo methods and multilayer perceptrons. Building on this work, our study incorporates residual networks into the model, explores different architectural designs, and conducts multi-role testing. Our findings demonstrate that this model significantly improves the winning rate within the same training time. Additionally, we introduce a call scoring system to assist the agent in deciding whether to become a landlord. With these
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22240;&#26524;&#30693;&#35782;&#24037;&#31243;&#65288;CKE&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;COVID-19&#32972;&#26223;&#19979;&#24320;&#21457;&#20102;&#19968;&#20010;&#22240;&#26524;&#30693;&#35782;&#24211;&#65292;&#25903;&#25345;&#20102;&#21508;&#31181;&#29305;&#23450;&#24212;&#29992;&#27169;&#22411;&#30340;&#24314;&#31435;&#12290;</title><link>https://arxiv.org/abs/2403.14100</link><description>&lt;p&gt;
&#22240;&#26524;&#30693;&#35782;&#24037;&#31243;&#65306;COVID-19&#30340;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Causal knowledge engineering: A case study from COVID-19
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14100
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22240;&#26524;&#30693;&#35782;&#24037;&#31243;&#65288;CKE&#65289;&#30340;&#26041;&#27861;&#65292;&#22312;COVID-19&#32972;&#26223;&#19979;&#24320;&#21457;&#20102;&#19968;&#20010;&#22240;&#26524;&#30693;&#35782;&#24211;&#65292;&#25903;&#25345;&#20102;&#21508;&#31181;&#29305;&#23450;&#24212;&#29992;&#27169;&#22411;&#30340;&#24314;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
COVID-19&#22312;2020&#24180;&#21021;&#31361;&#28982;&#20986;&#29616;&#65292;&#38656;&#35201;&#22312;&#24040;&#22823;&#19981;&#30830;&#23450;&#24615;&#30340;&#32972;&#26223;&#19979;&#36805;&#36895;&#24212;&#23545;&#12290;&#26368;&#21021;&#32570;&#20047;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#21644;&#30693;&#35782;&#65292;&#35768;&#22810;&#26089;&#26399;&#27169;&#22411;&#24517;&#39035;&#24314;&#31435;&#22312;&#22240;&#26524;&#20551;&#35774;&#21644;&#20272;&#35745;&#30340;&#22522;&#30784;&#19978;&#65292;&#20197;&#34917;&#20805;&#26377;&#38480;&#30340;&#25968;&#25454;&#65292;&#36890;&#24120;&#27809;&#26377;&#21487;&#38752;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#12289;&#39564;&#35777;&#21644;&#35760;&#24405;&#36825;&#20123;&#22240;&#26524;&#20551;&#35774;&#12290;&#25105;&#20204;&#30340;&#22242;&#38431;&#30528;&#25163;&#36827;&#34892;&#30693;&#35782;&#24037;&#31243;&#36807;&#31243;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;&#22810;&#20010;COVID-19&#19981;&#21516;&#26041;&#38754;&#22240;&#26524;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#22240;&#26524;&#30693;&#35782;&#24211;&#12290;&#35813;&#29615;&#22659;&#30340;&#29420;&#29305;&#25361;&#25112;&#23548;&#33268;&#23545;&#35843;&#26597;&#26041;&#27861;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#24418;&#25104;&#20102;&#25105;&#20204;&#31216;&#20043;&#20026;&#22240;&#26524;&#30693;&#35782;&#24037;&#31243;&#65288;CKE&#65289;&#30340;&#30693;&#35782;&#24037;&#31243;&#26041;&#27861;&#12290;CKE&#25552;&#20379;&#20102;&#19968;&#31181;&#32467;&#26500;&#21270;&#26041;&#27861;&#26469;&#26500;&#24314;&#22240;&#26524;&#30693;&#35782;&#24211;&#65292;&#21487;&#20197;&#25903;&#25345;&#24320;&#21457;&#21508;&#31181;&#29305;&#23450;&#24212;&#29992;&#27169;&#22411;&#30340;&#24037;&#20316;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;CKE&#26041;&#27861;&#65292;&#24182;&#20197;&#25105;&#20204;&#30340;COVID-19&#24037;&#20316;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14100v1 Announce Type: new  Abstract: COVID-19 appeared abruptly in early 2020, requiring a rapid response amid a context of great uncertainty. Good quality data and knowledge was initially lacking, and many early models had to be developed with causal assumptions and estimations built in to supplement limited data, often with no reliable approach for identifying, validating and documenting these causal assumptions. Our team embarked on a knowledge engineering process to develop a causal knowledge base consisting of several causal BNs for diverse aspects of COVID-19. The unique challenges of the setting lead to experiments with the elicitation approach, and what emerged was a knowledge engineering method we call Causal Knowledge Engineering (CKE). The CKE provides a structured approach for building a causal knowledge base that can support the development of a variety of application-specific models. Here we describe the CKE method, and use our COVID-19 work as a case study to
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Data Center Carbon Footprint Reduction (DC-CFR) &#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#26102;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#20197;&#20943;&#23569;&#30899;&#36275;&#36857;&#12290;</title><link>https://arxiv.org/abs/2403.14092</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#25968;&#25454;&#20013;&#24515;&#23454;&#26102;&#20943;&#23569;&#30899;&#36275;&#36857;
&lt;/p&gt;
&lt;p&gt;
Carbon Footprint Reduction for Sustainable Data Centers in Real-Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14092
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Data Center Carbon Footprint Reduction (DC-CFR) &#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#26102;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#20197;&#20943;&#23569;&#30899;&#36275;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#26174;&#33879;&#22686;&#21152;&#33021;&#28304;&#28040;&#32791;&#65292;&#30899;&#25490;&#25918;&#20302;&#30340;&#21487;&#25345;&#32493;&#25968;&#25454;&#20013;&#24515;&#27491;&#25104;&#20026;&#20840;&#29699;&#25919;&#24220;&#21644;&#20225;&#19994;&#20851;&#27880;&#30340;&#37325;&#28857;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#38656;&#35201;&#22312;&#20919;&#21364;&#21644;IT&#36127;&#36733;&#20013;&#36827;&#34892;&#21151;&#32791;&#20248;&#21270;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#22522;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#22312;&#30005;&#32593;&#20013;&#30340;&#21487;&#29992;&#24615;&#26469;&#35843;&#25972;&#28789;&#27963;&#36127;&#36733;&#65292;&#21033;&#29992;&#25968;&#25454;&#20013;&#24515;&#19981;&#38388;&#26029;&#30005;&#28304;&#20013;&#30340;&#30005;&#27744;&#23384;&#20648;&#65292;&#20351;&#29992;&#21327;&#20316;&#20195;&#29702;&#12290;&#36825;&#20123;&#20248;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#23545;&#21464;&#21270;&#30340;&#22806;&#37096;&#22240;&#32032;&#65288;&#22914;&#22825;&#27668;&#21644;&#30005;&#32593;&#30899;&#25490;&#25918;&#24378;&#24230;&#65289;&#30340;&#20381;&#36182;&#20351;&#24471;&#36825;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#22312;&#21160;&#24577;&#23454;&#38469;&#29615;&#22659;&#20013;&#21516;&#26102;&#20248;&#21270;&#25152;&#26377;&#36825;&#20123;&#30446;&#26631;&#30340;&#23454;&#26102;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30899;&#36275;&#36857;&#20943;&#23569;&#65288;DC-CFR&#65289;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#20248;&#21270;&#22810;&#20010;&#35282;&#24230;&#30340;&#25968;&#25454;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14092v1 Announce Type: cross  Abstract: As machine learning workloads significantly increase energy consumption, sustainable data centers with low carbon emissions are becoming a top priority for governments and corporations worldwide. This requires a paradigm shift in optimizing power consumption in cooling and IT loads, shifting flexible loads based on the availability of renewable energy in the power grid, and leveraging battery storage from the uninterrupted power supply in data centers, using collaborative agents. The complex association between these optimization strategies and their dependencies on variable external factors like weather and the power grid carbon intensity makes this a hard problem. Currently, a real-time controller to optimize all these goals simultaneously in a dynamic real-world setting is lacking. We propose a Data Center Carbon Footprint Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that optimizes data centers for the mult
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;DeepFake&#26816;&#27979;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#25581;&#31034;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#23613;&#31649;LLMs&#24182;&#38750;&#19987;&#20026;&#23186;&#20307;&#21462;&#35777;&#20219;&#21153;&#35774;&#35745;&#65292;&#36825;&#19968;&#21457;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>https://arxiv.org/abs/2403.14077</link><description>&lt;p&gt;
&#32842;&#22825;GPT&#33021;&#22815;&#26816;&#27979;DeepFakes&#21527;&#65311;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#23186;&#20307;&#21462;&#35777;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT Detect DeepFakes? A Study of Using Multimodal Large Language Models for Media Forensics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;DeepFake&#26816;&#27979;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#25581;&#31034;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;&#23613;&#31649;LLMs&#24182;&#38750;&#19987;&#20026;&#23186;&#20307;&#21462;&#35777;&#20219;&#21153;&#35774;&#35745;&#65292;&#36825;&#19968;&#21457;&#29616;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DeepFakes&#26159;&#25351;&#30001;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#23186;&#20307;&#20869;&#23481;&#65292;&#30001;&#20110;&#20854;&#34987;&#29992;&#20316;&#25955;&#24067;&#34394;&#20551;&#20449;&#24687;&#30340;&#25163;&#27573;&#65292;&#24050;&#32463;&#25104;&#20026;&#36234;&#26469;&#36234;&#20196;&#20154;&#25285;&#24551;&#30340;&#38382;&#39064;&#12290;&#24403;&#21069;&#26816;&#27979;DeepFakes&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#32534;&#31243;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;DeepFake&#26816;&#27979;&#20013;&#30340;&#33021;&#21147;&#12290;&#36890;&#36807;&#23450;&#24615;&#21644;&#23450;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22810;&#27169;&#24577;LLMs&#21487;&#20197;&#36890;&#36807;&#35880;&#24910;&#30340;&#23454;&#39564;&#35774;&#35745;&#21644;&#21450;&#26102;&#30340;&#24037;&#31243;&#26041;&#27861;&#25581;&#31034;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#32771;&#34385;&#21040;LLMs&#24182;&#19981;&#26159;&#26412;&#36136;&#19978;&#20026;&#23186;&#20307;&#21462;&#35777;&#20219;&#21153;&#37327;&#36523;&#23450;&#21046;&#30340;&#65292;&#36825;&#19968;&#28857;&#30456;&#24403;&#26377;&#36259;&#65292;&#32780;&#19988;&#36825;&#20010;&#36807;&#31243;&#24182;&#19981;&#38656;&#35201;&#32534;&#31243;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22810;&#27169;&#24577;LLMs&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#21487;&#33021;&#30340;&#25913;&#36827;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14077v1 Announce Type: new  Abstract: DeepFakes, which refer to AI-generated media content, have become an increasing concern due to their use as a means for disinformation. Detecting DeepFakes is currently solved with programmed machine learning algorithms. In this work, we investigate the capabilities of multimodal large language models (LLMs) in DeepFake detection. We conducted qualitative and quantitative experiments to demonstrate multimodal LLMs and show that they can expose AI-generated images through careful experimental design and prompt engineering. This is interesting, considering that LLMs are not inherently tailored for media forensic tasks, and the process does not require programming. We discuss the limitations of multimodal LLMs for these tasks and suggest possible improvements.
&lt;/p&gt;</description></item><item><title>&#29983;&#25104;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#26356;&#39640;&#32423;&#21035;&#30340;&#33258;&#21160;&#21270;&#24320;&#21551;&#26032;&#21487;&#33021;&#24615;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26397;&#21521;&#23436;&#20840;&#33258;&#21160;&#21270;&#21644;&#21463;&#30417;&#31649;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#12290;</title><link>https://arxiv.org/abs/2403.14049</link><description>&lt;p&gt;
&#26397;&#33258;&#21160;&#21270;&#21644;&#21463;&#30417;&#31649;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
A Roadmap Towards Automated and Regulated Robotic Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14049
&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#26356;&#39640;&#32423;&#21035;&#30340;&#33258;&#21160;&#21270;&#24320;&#21551;&#26032;&#21487;&#33021;&#24615;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#26397;&#21521;&#23436;&#20840;&#33258;&#21160;&#21270;&#21644;&#21463;&#30417;&#31649;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#30340;&#36335;&#32447;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#20026;&#26356;&#39640;&#32423;&#21035;&#30340;&#33258;&#21160;&#21270;&#25171;&#24320;&#20102;&#21487;&#33021;&#24615;&#65292;&#24182;&#19988;&#20154;&#24037;&#26234;&#33021;&#22312;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#30340;&#20855;&#20307;&#20307;&#29616;&#21183;&#22312;&#24517;&#34892;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29983;&#25104;&#25216;&#26415;&#30340;&#40657;&#21283;&#23376;&#29305;&#24615;&#65292;&#30693;&#35782;&#21644;&#24037;&#20316;&#27969;&#31243;&#26041;&#26696;&#30340;&#29983;&#25104;&#26159;&#19981;&#21463;&#25511;&#21046;&#30340;&#65292;&#23588;&#20854;&#22312;&#21160;&#24577;&#29615;&#22659;&#21644;&#22797;&#26434;&#22330;&#26223;&#20013;&#12290;&#36825;&#23545;&#21307;&#30103;&#22330;&#26223;&#31561;&#23433;&#20840;&#35201;&#27714;&#39640;&#30340;&#24212;&#29992;&#30340;&#30417;&#31649;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#26469;&#33258;&#20154;&#24037;&#26234;&#33021;&#30340;&#26410;&#21463;&#30417;&#31649;&#29983;&#25104;&#36807;&#31243;&#36866;&#29992;&#20110;&#20302;&#32423;&#31471;&#20219;&#21153;&#65292;&#20294;&#24178;&#39044;&#24418;&#24335;&#24212;&#24403;&#21457;&#29983;&#22312;&#24037;&#20316;&#27969;&#31243;&#29983;&#25104;&#21518;&#21644;&#26426;&#22120;&#20154;&#25191;&#34892;&#21069;&#65292;&#21487;&#20197;&#26159;&#25163;&#21160;&#24418;&#24335;&#20063;&#21487;&#20197;&#26159;&#33258;&#21160;&#21270;&#24418;&#24335;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#26465;&#36335;&#32447;&#22270;&#65292;&#21487;&#20197;&#24341;&#39046;&#25105;&#20204;&#36208;&#21521;&#23436;&#20840;&#33258;&#21160;&#21270;&#21644;&#21463;&#30417;&#31649;&#30340;&#26426;&#22120;&#20154;&#31995;&#32479;&#12290;&#22312;&#36825;&#31181;&#33539;&#24335;&#20013;&#65292;&#39640;&#32423;&#25919;&#31574;&#34987;&#29983;&#25104;&#20026;&#32467;&#26500;&#21270;&#22270;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#30417;&#31649;&#30417;&#30563;&#21644;&#37325;&#22797;&#21033;&#29992;&#65292;&#32780;&#20026;&#36739;&#20302;&#32423;&#21035;&#30340;&#20195;&#30721;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14049v1 Announce Type: cross  Abstract: The rapid development of generative technology opens up possibility for higher level of automation, and artificial intelligence (AI) embodiment in robotic systems is imminent. However, due to the blackbox nature of the generative technology, the generation of the knowledge and workflow scheme is uncontrolled, especially in a dynamic environment and a complex scene. This poses challenges to regulations in safety-demanding applications such as medical scenes. We argue that the unregulated generative processes from AI is fitted for low level end tasks, but intervention in the form of manual or automated regulation should happen post-workflow-generation and pre-robotic-execution. To address this, we propose a roadmap that can lead to fully automated and regulated robotic systems. In this paradigm, the high level policies are generated as structured graph data, enabling regulatory oversight and reusability, while the code base for lower lev
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20044;&#23572;&#37117;&#35821;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#39318;&#20010;&#26368;&#22823;&#35268;&#27169;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#22320;&#21306;&#35821;&#35328;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#39046;&#22495;&#25968;&#25454;&#38598;&#35268;&#27169;&#26377;&#38480;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.14037</link><description>&lt;p&gt;
Ax-to-Grind Urdu: &#20044;&#23572;&#37117;&#35821;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Ax-to-Grind Urdu: Benchmark Dataset for Urdu Fake News Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20044;&#23572;&#37117;&#35821;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#39318;&#20010;&#26368;&#22823;&#35268;&#27169;&#20844;&#24320;&#25968;&#25454;&#38598;&#65292;&#22635;&#34917;&#20102;&#22320;&#21306;&#35821;&#35328;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#39046;&#22495;&#25968;&#25454;&#38598;&#35268;&#27169;&#26377;&#38480;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35823;&#20256;&#20449;&#24687;&#21487;&#33021;&#20005;&#37325;&#24433;&#21709;&#31038;&#20250;&#65292;&#24433;&#21709;&#20174;&#20844;&#20247;&#33286;&#35770;&#21040;&#26426;&#26500;&#20449;&#24515;&#21644;&#19968;&#20010;&#22269;&#23478;&#30340;&#25919;&#27835;&#21069;&#26223;&#31561;&#21508;&#20010;&#26041;&#38754;&#12290;&#22312;&#32447;&#32593;&#31449;&#21644;&#22312;&#32447;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#21576;&#29616;&#22823;&#37327;&#22686;&#38271;&#12290;&#21508;&#31181;&#20107;&#23454;&#26680;&#26597;&#32593;&#31449;&#30340;&#26032;&#38395;&#20027;&#35201;&#20197;&#33521;&#35821;&#21457;&#24067;&#65292;&#20960;&#20046;&#19981;&#25552;&#20379;&#26377;&#20851;&#22320;&#21306;&#35821;&#35328;&#34394;&#20551;&#26032;&#38395;&#30340;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#26080;&#27861;&#36890;&#36807;&#20107;&#23454;&#26680;&#26597;&#38376;&#25143;&#32593;&#31449;&#26469;&#35782;&#21035;&#20044;&#23572;&#37117;&#35821;&#34394;&#20551;&#26032;&#38395;&#20256;&#25773;&#32773;&#12290;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;(SOTA)&#26041;&#27861;&#20381;&#36182;&#20110;&#36866;&#24403;&#26631;&#35760;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22320;&#21306;&#21644;&#36164;&#28304;&#21463;&#38480;&#35821;&#35328;&#30340;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30001;&#20110;&#25968;&#25454;&#38598;&#35268;&#27169;&#26377;&#38480;&#21644;&#21512;&#27861;&#35789;&#27719;&#36164;&#28304;&#30340;&#32570;&#20047;&#32780;&#28382;&#21518;&#12290;&#20808;&#21069;&#29992;&#20110;&#20044;&#23572;&#37117;&#35821;&#34394;&#20551;&#26032;&#38395;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#26377;&#38480;&#12289;&#39046;&#22495;&#21463;&#38480;&#12289;&#19981;&#20844;&#24320;&#19988;&#26410;&#32463;&#20154;&#24037;&#39564;&#35777;&#65292;&#20854;&#20013;&#26032;&#38395;&#26159;&#20174;&#33521;&#35821;&#32763;&#35793;&#20026;&#20044;&#23572;&#37117;&#35821;&#12290;&#26412;&#25991;&#31574;&#21010;&#24182;&#36129;&#29486;&#20102;&#39318;&#20010;&#26368;&#22823;&#35268;&#27169;&#30340;&#20844;&#24320;&#21487;&#29992;&#30340;&#20044;&#23572;&#37117;&#35821;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14037v1 Announce Type: cross  Abstract: Misinformation can seriously impact society, affecting anything from public opinion to institutional confidence and the political horizon of a state. Fake News (FN) proliferation on online websites and Online Social Networks (OSNs) has increased profusely. Various fact-checking websites include news in English and barely provide information about FN in regional languages. Thus the Urdu FN purveyors cannot be discerned using factchecking portals. SOTA approaches for Fake News Detection (FND) count upon appropriately labelled and large datasets. FND in regional and resource-constrained languages lags due to the lack of limited-sized datasets and legitimate lexical resources. The previous datasets for Urdu FND are limited-sized, domain-restricted, publicly unavailable and not manually verified where the news is translated from English into Urdu. In this paper, we curate and contribute the first largest publicly available dataset for Urdu 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#31515;&#21345;&#23572;&#36951;&#20256;&#35268;&#21010;&#65288;CGP&#65289;&#30340;&#20803;&#28436;&#21270;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#31070;&#32463;&#32593;&#32476;Evolution&#30340;&#20960;&#20309;&#32534;&#30721;&#65288;GENE&#65289;, &#25214;&#21040;&#26356;&#26377;&#25928;&#30340;&#36317;&#31163;&#20989;&#25968;&#65292;&#21019;&#24314;&#19968;&#20010;&#26356;&#26131;&#20110;&#21033;&#29992;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;</title><link>https://arxiv.org/abs/2403.14019</link><description>&lt;p&gt;
&#25628;&#32034;&#25628;&#32034;&#31354;&#38388;&#65306;&#20803;&#28436;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#20960;&#20309;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Searching Search Spaces: Meta-evolving a Geometric Encoding for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14019
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#31515;&#21345;&#23572;&#36951;&#20256;&#35268;&#21010;&#65288;CGP&#65289;&#30340;&#20803;&#28436;&#21270;&#26041;&#27861;&#65292;&#20248;&#21270;&#20102;&#31070;&#32463;&#32593;&#32476;Evolution&#30340;&#20960;&#20309;&#32534;&#30721;&#65288;GENE&#65289;, &#25214;&#21040;&#26356;&#26377;&#25928;&#30340;&#36317;&#31163;&#20989;&#25968;&#65292;&#21019;&#24314;&#19968;&#20010;&#26356;&#26131;&#20110;&#21033;&#29992;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36827;&#21270;&#31574;&#30053;&#25628;&#32034;&#20013;&#65292;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#20351;&#29992;&#30452;&#25509;&#26144;&#23556;&#26469;&#34920;&#31034;&#65306;&#27599;&#20010;&#22522;&#22240;&#32534;&#30721;&#19968;&#20010;&#32593;&#32476;&#26435;&#37325;&#12290;&#38388;&#25509;&#32534;&#30721;&#26041;&#27861;&#65292;&#20854;&#20013;&#27599;&#20010;&#22522;&#22240;&#21487;&#20197;&#32534;&#30721;&#22810;&#20010;&#26435;&#37325;&#65292;&#23558;&#22522;&#22240;&#32452;&#32553;&#30701;&#20197;&#20943;&#23569;&#25628;&#32034;&#31354;&#38388;&#30340;&#32500;&#25968;&#65292;&#24182;&#26356;&#22909;&#22320;&#21033;&#29992;&#25490;&#21015;&#21644;&#23545;&#31216;&#24615;&#12290;&#31070;&#32463;&#32593;&#32476;Evolution&#30340;&#20960;&#20309;&#32534;&#30721;&#65288;GENE&#65289;&#24341;&#20837;&#20102;&#19968;&#31181;&#38388;&#25509;&#32534;&#30721;&#26041;&#27861;&#65292;&#20854;&#20013;&#36830;&#25509;&#30340;&#26435;&#37325;&#34987;&#35745;&#31639;&#20026;&#20004;&#20010;&#36830;&#25509;&#31070;&#32463;&#20803;&#20043;&#38388;&#30340;&#65288;&#20266;&#65289;&#36317;&#31163;&#65292;&#20351;&#22522;&#22240;&#32452;&#22823;&#23567;&#38543;&#30528;&#22522;&#22240;&#25968;&#37327;&#32447;&#24615;&#22686;&#38271;&#65292;&#32780;&#19981;&#26159;&#22312;&#30452;&#25509;&#32534;&#30721;&#20013;&#21576;&#20108;&#27425;&#22686;&#38271;&#12290;&#28982;&#32780;&#65292;GENE&#20173;&#28982;&#20381;&#36182;&#20110;&#25163;&#24037;&#35774;&#35745;&#30340;&#36317;&#31163;&#20989;&#25968;&#65292;&#24182;&#27809;&#26377;&#36827;&#34892;&#20248;&#21270;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#31515;&#21345;&#23572;&#36951;&#20256;&#35268;&#21010;&#65288;CGP&#65289;&#25214;&#21040;&#26356;&#22909;&#30340;&#24615;&#33021;&#36317;&#31163;&#20989;&#25968;&#65292;&#20174;&#32780;&#22312;&#20803;&#28436;&#21270;&#26041;&#27861;&#20013;&#20248;&#21270;GENE&#30340;&#32534;&#30721;&#65292;&#20174;&#32780;&#21019;&#24314;&#19968;&#20010;&#26356;&#23481;&#26131;&#21033;&#29992;&#30340;&#25628;&#32034;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14019v1 Announce Type: cross  Abstract: In evolutionary policy search, neural networks are usually represented using a direct mapping: each gene encodes one network weight. Indirect encoding methods, where each gene can encode for multiple weights, shorten the genome to reduce the dimensions of the search space and better exploit permutations and symmetries. The Geometric Encoding for Neural network Evolution (GENE) introduced an indirect encoding where the weight of a connection is computed as the (pseudo-)distance between the two linked neurons, leading to a genome size growing linearly with the number of genes instead of quadratically in direct encoding. However GENE still relies on hand-crafted distance functions with no prior optimization. Here we show that better performing distance functions can be found for GENE using Cartesian Genetic Programming (CGP) in a meta-evolution approach, hence optimizing the encoding to create a search space that is easier to exploit. We 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;ChatGPT&#22312;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#25552;&#31034;&#25935;&#24863;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#28085;&#30422;&#24773;&#32490;&#20998;&#26512;&#12289;&#27602;&#24615;&#26816;&#27979;&#21644;&#35773;&#21050;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.14006</link><description>&lt;p&gt;
&#35770;ChatGPT&#22312;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#25552;&#31034;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
On Prompt Sensitivity of ChatGPT in Affective Computing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14006
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#25935;&#24863;&#24615;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;ChatGPT&#22312;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#25552;&#31034;&#25935;&#24863;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#21644;&#35780;&#20272;&#65292;&#28085;&#30422;&#24773;&#32490;&#20998;&#26512;&#12289;&#27602;&#24615;&#26816;&#27979;&#21644;&#35773;&#21050;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#23637;&#31034;&#20102;&#20687;ChatGPT&#36825;&#26679;&#30340;&#22522;&#30784;&#27169;&#22411;&#22312;&#22810;&#20010;&#39046;&#22495;&#65292;&#21253;&#25324;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#26032;&#20852;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35775;&#38382;&#36825;&#20123;&#26032;&#20852;&#33021;&#21147;&#26159;&#36890;&#36807;&#25552;&#31034;&#24037;&#31243;&#26469;&#23454;&#29616;&#30340;&#12290;&#23613;&#31649;&#23384;&#22312;&#19968;&#20123;&#25552;&#31034;&#25216;&#26415;&#65292;&#20294;&#35813;&#39046;&#22495;&#20173;&#22312;&#36805;&#36895;&#21457;&#23637;&#65292;&#35768;&#22810;&#25552;&#31034;&#24819;&#27861;&#20173;&#38656;&#35201;&#35843;&#26597;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35780;&#20272;&#24182;&#35843;&#26597;&#22522;&#20110;&#19981;&#21516;&#25552;&#31034;&#25110;&#29983;&#25104;&#21442;&#25968;&#30340;&#22522;&#30784;&#27169;&#22411;&#24615;&#33021;&#30340;&#25935;&#24863;&#24615;&#12290;&#25105;&#20204;&#22312;&#24773;&#24863;&#35745;&#31639;&#33539;&#22260;&#20869;&#38024;&#23545;ChatGPT&#25191;&#34892;&#25105;&#20204;&#30340;&#35780;&#20272;&#65292;&#35299;&#20915;&#20102;&#19977;&#20010;&#20027;&#35201;&#38382;&#39064;&#65292;&#21363;&#24773;&#32490;&#20998;&#26512;&#12289;&#27602;&#24615;&#26816;&#27979;&#21644;&#35773;&#21050;&#26816;&#27979;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#33258;&#22238;&#24402;&#25991;&#26412;&#29983;&#25104;&#20013;&#30340;&#20851;&#38190;&#21442;&#25968;&#36827;&#34892;&#20102;&#25935;&#24863;&#24615;&#20998;&#26512;&#65292;&#29305;&#21035;&#26159;&#28201;&#24230;&#21442;&#25968;$T$&#21644;Nucleus&#25277;&#26679;&#20013;&#30340;top-$p$&#21442;&#25968;&#65292;&#25351;&#23548;&#30528;&#20445;&#23432;&#25110;&#21019;&#36896;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14006v1 Announce Type: cross  Abstract: Recent studies have demonstrated the emerging capabilities of foundation models like ChatGPT in several fields, including affective computing. However, accessing these emerging capabilities is facilitated through prompt engineering. Despite the existence of some prompting techniques, the field is still rapidly evolving and many prompting ideas still require investigation. In this work, we introduce a method to evaluate and investigate the sensitivity of the performance of foundation models based on different prompts or generation parameters. We perform our evaluation on ChatGPT within the scope of affective computing on three major problems, namely sentiment analysis, toxicity detection, and sarcasm detection. First, we carry out a sensitivity analysis on pivotal parameters in auto-regressive text generation, specifically the temperature parameter $T$ and the top-$p$ parameter in Nucleus sampling, dictating how conservative or creative
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25581;&#31034;&#20102;&#20844;&#31435;&#39640;&#31561;&#25945;&#32946;&#20013;&#31639;&#27861;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#23398;&#29983;&#30417;&#35270;&#22686;&#21152;&#12289;&#19981;&#24179;&#31561;&#21152;&#21095;&#21644;&#25945;&#24072;-&#23398;&#29983;&#20851;&#31995;&#30340;&#33258;&#21160;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.13969</link><description>&lt;p&gt;
&#36825;&#24182;&#38750;&#19968;&#20010;&#25968;&#25454;&#38382;&#39064;&#65306;&#31639;&#27861;&#19982;&#26435;&#21147;&#22312;&#21152;&#25343;&#22823;&#20844;&#31435;&#39640;&#31561;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
"This is not a data problem": Algorithms and Power in Public Higher Education in Canada
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13969
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25581;&#31034;&#20102;&#20844;&#31435;&#39640;&#31561;&#25945;&#32946;&#20013;&#31639;&#27861;&#20915;&#31574;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#23398;&#29983;&#30417;&#35270;&#22686;&#21152;&#12289;&#19981;&#24179;&#31561;&#21152;&#21095;&#21644;&#25945;&#24072;-&#23398;&#29983;&#20851;&#31995;&#30340;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31639;&#27861;&#20915;&#31574;&#22312;&#20844;&#31435;&#39640;&#31561;&#25945;&#32946;&#20013;&#26085;&#30410;&#34987;&#37319;&#29992;&#12290;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#30340;&#25968;&#25454;&#39537;&#21160;&#23454;&#36341;&#25193;&#23637;&#19982;&#26032;&#20844;&#20849;&#31649;&#29702;&#26041;&#27861;&#22312;&#26032;&#33258;&#30001;&#20027;&#20041;&#25919;&#24220;&#30340;&#25512;&#21160;&#19979;&#21516;&#27493;&#36827;&#34892;&#12290;&#26412;&#30740;&#31350;&#23545;&#21152;&#25343;&#22823;&#23433;&#22823;&#30053;&#30465;&#19968;&#20010;&#20844;&#31435;&#23398;&#38498;&#25968;&#25454;&#21644;&#31639;&#27861;&#30340;&#28145;&#24230;&#27665;&#26063;&#24535;&#26696;&#20363;&#36827;&#34892;&#20102;&#23450;&#24615;&#20998;&#26512;&#12290;&#25105;&#20204;&#30830;&#23450;&#20102;&#23398;&#38498;&#20351;&#29992;&#30340;&#25968;&#25454;&#12289;&#31639;&#27861;&#21644;&#32467;&#26524;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#23398;&#38498;&#30340;&#27969;&#31243;&#21644;&#20851;&#31995;&#22914;&#20309;&#25903;&#25345;&#36825;&#20123;&#32467;&#26524;&#65292;&#20197;&#21450;&#19981;&#21516;&#21033;&#30410;&#30456;&#20851;&#32773;&#23545;&#23398;&#38498;&#25968;&#25454;&#39537;&#21160;&#31995;&#32479;&#30340;&#30475;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#26085;&#30410;&#20381;&#36182;&#31639;&#27861;&#20915;&#31574;&#23548;&#33268;&#23398;&#29983;&#30417;&#35270;&#22686;&#21152;&#65292;&#29616;&#26377;&#19981;&#24179;&#31561;&#21152;&#21095;&#65292;&#24182;&#23548;&#33268;&#25945;&#24072;-&#23398;&#29983;&#20851;&#31995;&#30340;&#33258;&#21160;&#21270;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#30001;&#31639;&#27861;&#20915;&#31574;&#24310;&#32493;&#30340;&#22686;&#21152;&#21046;&#24230;&#26435;&#21147;&#30340;&#24490;&#29615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13969v1 Announce Type: cross  Abstract: Algorithmic decision-making is increasingly being adopted across public higher education. The expansion of data-driven practices by post-secondary institutions has occurred in parallel with the adoption of New Public Management approaches by neoliberal administrations. In this study, we conduct a qualitative analysis of an in-depth ethnographic case study of data and algorithms in use at a public college in Ontario, Canada. We identify the data, algorithms, and outcomes in use at the college. We assess how the college's processes and relationships support those outcomes and the different stakeholders' perceptions of the college's data-driven systems. In addition, we find that the growing reliance on algorithmic decisions leads to increased student surveillance, exacerbation of existing inequities, and the automation of the faculty-student relationship. Finally, we identify a cycle of increased institutional power perpetuated by algorit
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;ROS2&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#20351;&#24471;NAO&#26426;&#22120;&#20154;&#20855;&#26377;&#26356;&#22909;&#24615;&#33021;&#21644;&#26032;&#21151;&#33021;&#65292;&#21253;&#25324;&#20154;&#24418;&#26426;&#22120;&#20154;&#30340;&#22522;&#26412;&#25216;&#33021;&#21644;&#24120;&#29992;&#20110;HRI&#30340;&#21151;&#33021;&#65292;&#24182;&#19988;&#35813;&#26694;&#26550;&#26159;&#21487;&#31435;&#21363;&#20351;&#29992;&#19988;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#12290;</title><link>https://arxiv.org/abs/2403.13960</link><description>&lt;p&gt;
&#24320;&#25918;&#33719;&#21462;NAO&#65288;OAN&#65289;&#65306;&#22522;&#20110;ROS2&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#29992;&#20110;&#19982;NAO&#26426;&#22120;&#20154;&#36827;&#34892;HRI&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Open Access NAO (OAN): a ROS2-based software framework for HRI applications with the NAO robot
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;ROS2&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#20351;&#24471;NAO&#26426;&#22120;&#20154;&#20855;&#26377;&#26356;&#22909;&#24615;&#33021;&#21644;&#26032;&#21151;&#33021;&#65292;&#21253;&#25324;&#20154;&#24418;&#26426;&#22120;&#20154;&#30340;&#22522;&#26412;&#25216;&#33021;&#21644;&#24120;&#29992;&#20110;HRI&#30340;&#21151;&#33021;&#65292;&#24182;&#19988;&#35813;&#26694;&#26550;&#26159;&#21487;&#31435;&#21363;&#20351;&#29992;&#19988;&#39640;&#24230;&#21487;&#25193;&#23637;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36719;&#20214;&#26694;&#26550;&#65292;&#29992;&#20110;&#19982;&#30001;&#32852;&#21512;&#26426;&#22120;&#20154;&#38598;&#22242;&#29983;&#20135;&#30340;&#24120;&#35265;NAO&#26426;&#22120;&#20154;&#30340;HRI&#23454;&#39564;&#12290;&#20316;&#32773;&#21033;&#29992;&#22312;NAO&#19978;&#21487;&#20197;&#36816;&#34892;ROS2&#30340;&#33021;&#21147;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#29420;&#31435;&#20110;&#21046;&#36896;&#21830;&#25552;&#20379;&#30340;API&#30340;&#26694;&#26550;&#65292;&#20197;&#28385;&#36275;&#30740;&#31350;&#20154;&#21592;&#23545;NAO&#26356;&#22909;&#24615;&#33021;&#21644;&#26032;&#21151;&#33021;&#30340;&#20849;&#21516;&#38656;&#27714;&#12290;&#35813;&#31995;&#32479;&#19981;&#20165;&#20026;NAO&#25552;&#20379;&#20102;&#20154;&#24418;&#26426;&#22120;&#20154;&#30340;&#22522;&#26412;&#25216;&#33021;&#65292;&#22914;&#34892;&#36208;&#21644;&#22797;&#21046;&#24863;&#20852;&#36259;&#30340;&#21160;&#20316;&#65292;&#36824;&#20855;&#26377;&#24120;&#29992;&#20110;HRI&#30340;&#21151;&#33021;&#65292;&#22914;&#65306;&#35821;&#38899;&#35782;&#21035;/&#21512;&#25104;&#12289;&#20154;&#33080;&#21644;&#29289;&#20307;&#26816;&#27979;&#65292;&#20197;&#21450;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#29983;&#25104;&#24335;&#21464;&#25442;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#36827;&#34892;&#23545;&#35805;&#12290;&#24320;&#21457;&#30340;&#20195;&#30721;&#22240;&#27492;&#34987;&#37197;&#32622;&#20026;&#19968;&#20010;&#21487;&#31435;&#21363;&#20351;&#29992;&#65292;&#21516;&#26102;&#20063;&#26159;&#19968;&#20010;&#39640;&#24230;&#21487;&#25193;&#23637;&#21644;&#21487;&#25913;&#36827;&#30340;&#24037;&#20855;&#65292;&#36825;&#35201;&#24402;&#21151;&#20110;ROS&#31038;&#21306;&#25552;&#20379;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13960v1 Announce Type: cross  Abstract: This paper presents a new software framework for HRI experimentation with the sixth version of the common NAO robot produced by the United Robotics Group. Embracing the common demand of researchers for better performance and new features for NAO, the authors took advantage of the ability to run ROS2 onboard on the NAO to develop a framework independent of the APIs provided by the manufacturer. Such a system provides NAO with not only the basic skills of a humanoid robot such as walking and reproducing movements of interest but also features often used in HRI such as: speech recognition/synthesis, face and object detention, and the use of Generative Pre-trained Transformer (GPT) models for conversation. The developed code is therefore configured as a ready-to-use but also highly expandable and improvable tool thanks to the possibilities provided by the ROS community.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#38480;&#21046;&#20102;&#25193;&#25955;&#35757;&#32451;&#30340;&#33539;&#22260;&#65292;&#26377;&#25928;&#20445;&#30041;&#20102;&#26381;&#35013;&#32454;&#33410;&#65292;&#24182;&#23454;&#29616;&#20102;&#22810;&#26381;&#35013;&#35797;&#31359;&#12289;&#20998;&#23618;&#12289;&#39118;&#26684;&#21644;&#38795;&#23376;&#35797;&#31359;&#65292;&#26080;&#38656;&#22312;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#32456;&#65292;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#26041;&#38754;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.13951</link><description>&lt;p&gt;
ACDG-VTON: &#20934;&#30830;&#19988;&#21463;&#38480;&#30340;&#25193;&#25955;&#29983;&#25104;&#29992;&#20110;&#34394;&#25311;&#35797;&#31359;
&lt;/p&gt;
&lt;p&gt;
ACDG-VTON: Accurate and Contained Diffusion Generation for Virtual Try-On
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13951
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#38480;&#21046;&#20102;&#25193;&#25955;&#35757;&#32451;&#30340;&#33539;&#22260;&#65292;&#26377;&#25928;&#20445;&#30041;&#20102;&#26381;&#35013;&#32454;&#33410;&#65292;&#24182;&#23454;&#29616;&#20102;&#22810;&#26381;&#35013;&#35797;&#31359;&#12289;&#20998;&#23618;&#12289;&#39118;&#26684;&#21644;&#38795;&#23376;&#35797;&#31359;&#65292;&#26080;&#38656;&#22312;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#32456;&#65292;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#26041;&#38754;&#36229;&#36234;&#20102;&#20043;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#35797;&#31359;&#65288;VTON&#65289;&#28041;&#21450;&#29983;&#25104;&#31359;&#30528;&#36873;&#23450;&#26381;&#35013;&#30340;&#20154;&#29289;&#22270;&#20687;&#12290;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#23588;&#20854;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#20445;&#25345;&#36755;&#20837;&#26381;&#35013;&#30340;&#36523;&#20221;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#19968;&#38382;&#39064;&#28304;&#20110;&#25193;&#25955;&#35757;&#32451;&#20844;&#24335;&#20013;&#30340;&#32454;&#33410;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29420;&#29305;&#30340;&#35757;&#32451;&#26041;&#26696;&#65292;&#38480;&#21046;&#20102;&#25193;&#25955;&#35757;&#32451;&#30340;&#33539;&#22260;&#12290;&#25105;&#20204;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#20351;&#29992;&#19968;&#20010;&#19982;&#30446;&#26631;&#22270;&#20687;&#23436;&#20840;&#23545;&#40784;&#30340;&#25511;&#21046;&#22270;&#20687;&#12290;&#22240;&#27492;&#65292;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#33021;&#22815;&#20934;&#30830;&#22320;&#20445;&#30041;&#26381;&#35013;&#32454;&#33410;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#26377;&#25928;&#22320;&#20445;&#30041;&#20102;&#26381;&#35013;&#32454;&#33410;&#65292;&#36824;&#33021;&#22815;&#36827;&#34892;&#20998;&#23618;&#12289;&#39118;&#26684;&#21644;&#38795;&#23376;&#35797;&#31359;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21333;&#20010;&#25512;&#29702;&#21608;&#26399;&#20869;&#36816;&#34892;&#22810;&#26381;&#35013;&#35797;&#31359;&#65292;&#24182;&#19988;&#33021;&#22815;&#25903;&#25345;&#39640;&#36136;&#37327;&#30340;&#25918;&#22823;&#29983;&#25104;&#65292;&#32780;&#26080;&#38656;&#22312;&#26356;&#39640;&#20998;&#36776;&#29575;&#19979;&#36827;&#34892;&#35757;&#32451;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#36136;&#37327;&#26041;&#38754;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13951v1 Announce Type: cross  Abstract: Virtual Try-on (VTON) involves generating images of a person wearing selected garments. Diffusion-based methods, in particular, can create high-quality images, but they struggle to maintain the identities of the input garments. We identified this problem stems from the specifics in the training formulation for diffusion. To address this, we propose a unique training scheme that limits the scope in which diffusion is trained. We use a control image that perfectly aligns with the target image during training. In turn, this accurately preserves garment details during inference. We demonstrate our method not only effectively conserves garment details but also allows for layering, styling, and shoe try-on. Our method runs multi-garment try-on in a single inference cycle and can support high-quality zoomed-in generations without training in higher resolutions. Finally, we show our method surpasses prior methods in accuracy and quality.
&lt;/p&gt;</description></item><item><title>Evo* 2023&#20250;&#35758;&#25910;&#24405;&#20102;&#20851;&#20110;&#23558;&#19981;&#21516;&#30340;&#29983;&#29289;&#21551;&#21457;&#26041;&#27861;&#65288;&#20027;&#35201;&#26159;&#36827;&#21270;&#35745;&#31639;&#65289;&#24212;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#38382;&#39064;&#65288;&#22823;&#22810;&#20026;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65289;&#30340;&#30740;&#31350;&#21644;&#21021;&#27493;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.13950</link><description>&lt;p&gt;
Evo* 2023 -- &#26202;&#26399;&#25688;&#35201;&#38598;
&lt;/p&gt;
&lt;p&gt;
Evo* 2023 -- Late-Breaking Abstracts Volume
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13950
&lt;/p&gt;
&lt;p&gt;
Evo* 2023&#20250;&#35758;&#25910;&#24405;&#20102;&#20851;&#20110;&#23558;&#19981;&#21516;&#30340;&#29983;&#29289;&#21551;&#21457;&#26041;&#27861;&#65288;&#20027;&#35201;&#26159;&#36827;&#21270;&#35745;&#31639;&#65289;&#24212;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#38382;&#39064;&#65288;&#22823;&#22810;&#20026;&#29616;&#23454;&#19990;&#30028;&#38382;&#39064;&#65289;&#30340;&#30740;&#31350;&#21644;&#21021;&#27493;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13950v1 &#20844;&#21578;&#31867;&#22411;: &#20132;&#21449;&#25688;&#35201;: &#35813;&#21367;&#25910;&#24405;&#20102;&#25552;&#20132;&#32473;&#22312;&#25463;&#20811;&#24067;&#23572;&#35834;&#20030;&#21150;&#30340;Evo* 2023&#20250;&#35758;&#30340;&#26202;&#26399;&#25688;&#35201;&#65292;&#20250;&#35758;&#20110;4&#26376;12&#26085;&#33267;14&#26085;&#20030;&#34892;&#12290;&#36825;&#20123;&#35770;&#25991;&#23637;&#31034;&#20102;&#27491;&#22312;&#36827;&#34892;&#30340;&#30740;&#31350;&#20197;&#21450;&#21021;&#27493;&#32467;&#26524;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#29983;&#29289;&#21551;&#21457;&#26041;&#27861;&#65288;&#20027;&#35201;&#26159;&#36827;&#21270;&#35745;&#31639;&#65289;&#22312;&#35299;&#20915;&#19981;&#21516;&#38382;&#39064;&#19978;&#30340;&#24212;&#29992;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#26159;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13950v1 Announce Type: cross  Abstract: Volume with the Late-Breaking Abstracts submitted to the Evo* 2023 Conference, held in Brno (Czech Republic), from 12 to 14 of April. These papers present ongoing research and preliminary results investigating on the application of different approaches of Bioinspired Methods (mainly Evolutionary Computation) to different problems, most of them real world ones.
&lt;/p&gt;</description></item><item><title>BlendScape&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23454;&#29616;&#20102;&#29992;&#25143;&#23450;&#21046;&#35270;&#39057;&#20250;&#35758;&#29615;&#22659;&#65292;&#25903;&#25345;&#28789;&#27963;&#30340;&#20219;&#21153;&#31354;&#38388;&#34920;&#31034;&#21644;&#22810;&#27169;&#20132;&#20114;&#25216;&#26415;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24555;&#36895;&#34920;&#36798;&#35774;&#35745;&#24847;&#22270;&#24182;&#35774;&#24819;&#26410;&#26469;&#20250;&#35758;&#20013;&#30340;&#21327;&#20316;&#32467;&#26500;&#12290;</title><link>https://arxiv.org/abs/2403.13947</link><description>&lt;p&gt;
BlendScape: &#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#23454;&#29616;&#32479;&#19968;&#21644;&#20010;&#24615;&#21270;&#35270;&#39057;&#20250;&#35758;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
BlendScape: Enabling Unified and Personalized Video-Conferencing Environments through Generative AI
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13947
&lt;/p&gt;
&lt;p&gt;
BlendScape&#36890;&#36807;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#23454;&#29616;&#20102;&#29992;&#25143;&#23450;&#21046;&#35270;&#39057;&#20250;&#35758;&#29615;&#22659;&#65292;&#25903;&#25345;&#28789;&#27963;&#30340;&#20219;&#21153;&#31354;&#38388;&#34920;&#31034;&#21644;&#22810;&#27169;&#20132;&#20114;&#25216;&#26415;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#24555;&#36895;&#34920;&#36798;&#35774;&#35745;&#24847;&#22270;&#24182;&#35774;&#24819;&#26410;&#26469;&#20250;&#35758;&#20013;&#30340;&#21327;&#20316;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#30340;&#35270;&#39057;&#20250;&#35758;&#24037;&#20855;&#25903;&#25345;&#20016;&#23500;&#30340;&#19987;&#19994;&#21644;&#31038;&#20132;&#27963;&#21160;&#65292;&#20294;&#23427;&#20204;&#30340;&#36890;&#29992;&#12289;&#22522;&#20110;&#32593;&#26684;&#30340;&#29615;&#22659;&#26080;&#27861;&#36731;&#26494;&#22320;&#36866;&#24212;&#20998;&#24067;&#24335;&#21512;&#20316;&#32773;&#30340;&#21508;&#31181;&#38656;&#27714;&#12290;&#20026;&#20102;&#23454;&#29616;&#26368;&#32456;&#29992;&#25143;&#23450;&#21046;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;BlendScape&#65292;&#36825;&#26159;&#19968;&#20010;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;AI&#22270;&#20687;&#29983;&#25104;&#25216;&#26415;&#65292;&#20801;&#35768;&#20250;&#35758;&#21442;&#19982;&#32773;&#26500;&#24314;&#26681;&#25454;&#20182;&#20204;&#30340;&#21327;&#20316;&#32972;&#26223;&#23450;&#21046;&#30340;&#35270;&#39057;&#20250;&#35758;&#29615;&#22659;&#12290;BlendScape&#36890;&#36807;&#23558;&#29992;&#25143;&#30340;&#29289;&#29702;&#25110;&#34394;&#25311;&#32972;&#26223;&#34701;&#21512;&#21040;&#32479;&#19968;&#29615;&#22659;&#20013;&#25903;&#25345;&#20219;&#21153;&#31354;&#38388;&#30340;&#28789;&#27963;&#34920;&#31034;&#65292;&#24182;&#23454;&#29616;&#20102;&#22810;&#27169;&#20132;&#20114;&#25216;&#26415;&#20197;&#24341;&#23548;&#29983;&#25104;&#36807;&#31243;&#12290;&#36890;&#36807;&#19982;15&#21517;&#26368;&#32456;&#29992;&#25143;&#30340;&#35780;&#20272;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20182;&#20204;&#23545;&#24037;&#20316;&#21644;&#31038;&#20132;&#22330;&#26223;&#30340;&#23450;&#21046;&#20559;&#22909;&#12290;&#21442;&#19982;&#32773;&#21487;&#20197;&#24555;&#36895;&#34920;&#36798;&#20182;&#20204;&#23545;BlendScape&#30340;&#35774;&#35745;&#24847;&#22270;&#65292;&#24182;&#35774;&#24819;&#20351;&#29992;&#35813;&#31995;&#32479;&#26469;&#32452;&#32455;&#26410;&#26469;&#20250;&#35758;&#20013;&#30340;&#21327;&#20316;&#65292;&#20294;&#20063;&#36935;&#21040;&#20102;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13947v1 Announce Type: cross  Abstract: Today's video-conferencing tools support a rich range of professional and social activities, but their generic, grid-based environments cannot be easily adapted to meet the varying needs of distributed collaborators. To enable end-user customization, we developed BlendScape, a system for meeting participants to compose video-conferencing environments tailored to their collaboration context by leveraging AI image generation techniques. BlendScape supports flexible representations of task spaces by blending users' physical or virtual backgrounds into unified environments and implements multimodal interaction techniques to steer the generation. Through an evaluation with 15 end-users, we investigated their customization preferences for work and social scenarios. Participants could rapidly express their design intentions with BlendScape and envisioned using the system to structure collaboration in future meetings, but experienced challenge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#26631;&#20934;&#20998;&#26512;&#36873;&#25321;&#21333;&#20010;&#21453;&#20107;&#23454;&#65292;&#36991;&#20813;&#20102;&#29992;&#25143;&#27979;&#35797;&#22810;&#31181;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#21644;&#20998;&#26512;&#20914;&#31361;&#35299;&#20915;&#26041;&#26696;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#22810;&#20010;&#36136;&#37327;&#24230;&#37327;&#19978;&#24471;&#20998;&#24456;&#39640;&#30340;&#22949;&#21327;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.13940</link><description>&lt;p&gt;
&#20174;&#35299;&#37322;&#22120;&#38598;&#21512;&#20013;&#36873;&#25321;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#22810;&#26631;&#20934;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Multi-criteria approach for selecting an explanation from the set of counterfactuals produced by an ensemble of explainers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#38454;&#27573;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#22810;&#26631;&#20934;&#20998;&#26512;&#36873;&#25321;&#21333;&#20010;&#21453;&#20107;&#23454;&#65292;&#36991;&#20813;&#20102;&#29992;&#25143;&#27979;&#35797;&#22810;&#31181;&#19981;&#21516;&#35299;&#37322;&#26041;&#27861;&#21644;&#20998;&#26512;&#20914;&#31361;&#35299;&#20915;&#26041;&#26696;&#30340;&#22256;&#38590;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22312;&#22810;&#20010;&#36136;&#37327;&#24230;&#37327;&#19978;&#24471;&#20998;&#24456;&#39640;&#30340;&#22949;&#21327;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#25552;&#20379;&#33719;&#21462;&#26356;&#29702;&#24819;&#39044;&#27979;&#30340;&#26367;&#20195;&#22330;&#26223;&#12290;&#23427;&#20204;&#21487;&#20197;&#30001;&#22810;&#31181;&#26041;&#27861;&#29983;&#25104;&#65292;&#36825;&#20123;&#26041;&#27861;&#20248;&#21270;&#19981;&#21516;&#12289;&#26377;&#26102;&#26159;&#20914;&#31361;&#30340;&#36136;&#37327;&#24230;&#37327;&#65292;&#24182;&#20135;&#29983;&#23436;&#20840;&#19981;&#21516;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;&#35299;&#37322;&#26041;&#27861;&#21644;&#29983;&#25104;&#30340;&#21453;&#20107;&#23454;&#20043;&#19968;&#24182;&#19981;&#26159;&#19968;&#20214;&#23481;&#26131;&#30340;&#20107;&#24773;&#12290;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#22810;&#38454;&#27573;&#38598;&#25104;&#26041;&#27861;&#65292;&#22522;&#20110;&#22810;&#26631;&#20934;&#20998;&#26512;&#26469;&#36873;&#25321;&#21333;&#20010;&#21453;&#20107;&#23454;&#65292;&#32780;&#19981;&#26159;&#24378;&#36843;&#29992;&#25143;&#27979;&#35797;&#35768;&#22810;&#19981;&#21516;&#30340;&#35299;&#37322;&#26041;&#27861;&#24182;&#20998;&#26512;&#20914;&#31361;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#23427;&#25552;&#20379;&#20102;&#19968;&#20010;&#22949;&#21327;&#26041;&#26696;&#65292;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#36136;&#37327;&#24230;&#37327;&#19978;&#24471;&#20998;&#36739;&#39640;&#12290;&#36825;&#31181;&#26041;&#27861;&#21033;&#29992;&#25903;&#37197;&#20851;&#31995;&#21644;&#29702;&#24819;&#28857;&#20915;&#31574;&#36741;&#21161;&#26041;&#27861;&#65292;&#20174;&#24085;&#32047;&#25176;&#21069;&#27839;&#20013;&#36873;&#25321;&#19968;&#20010;&#21453;&#20107;&#23454;&#12290;&#36827;&#34892;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13940v1 Announce Type: cross  Abstract: Counterfactuals are widely used to explain ML model predictions by providing alternative scenarios for obtaining the more desired predictions. They can be generated by a variety of methods that optimize different, sometimes conflicting, quality measures and produce quite different solutions. However, choosing the most appropriate explanation method and one of the generated counterfactuals is not an easy task. Instead of forcing the user to test many different explanation methods and analysing conflicting solutions, in this paper, we propose to use a multi-stage ensemble approach that will select single counterfactual based on the multiple-criteria analysis. It offers a compromise solution that scores well on several popular quality measures. This approach exploits the dominance relation and the ideal point decision aid method, which selects one counterfactual from the Pareto front. The conducted experiments demonstrated that the propos
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#8220;&#21463;&#38480;&#34892;&#19994;&#8221;&#36827;&#34892;&#33258;&#21160;&#25968;&#25454;&#38598;&#22686;&#24378;&#20197;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#26032;&#26426;&#21046;&#65292;&#24182;&#21019;&#24314;&#20102;mb-index&#21644;db-index&#20004;&#20010;&#26032;&#30340;&#20559;&#35265;&#37327;&#21270;&#25351;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.13925</link><description>&lt;p&gt;
&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#65306;&#37325;&#28857;&#20851;&#27880;&#8220;&#21463;&#38480;&#34892;&#19994;&#8221;&#30340;&#33258;&#21160;&#25968;&#25454;&#38598;&#22686;&#24378;&#21644;&#20559;&#35265;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Reducing Large Language Model Bias with Emphasis on 'Restricted Industries': Automated Dataset Augmentation and Prejudice Quantification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13925
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#8220;&#21463;&#38480;&#34892;&#19994;&#8221;&#36827;&#34892;&#33258;&#21160;&#25968;&#25454;&#38598;&#22686;&#24378;&#20197;&#20943;&#23569;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20559;&#35265;&#30340;&#26032;&#26426;&#21046;&#65292;&#24182;&#21019;&#24314;&#20102;mb-index&#21644;db-index&#20004;&#20010;&#26032;&#30340;&#20559;&#35265;&#37327;&#21270;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#19981;&#26029;&#22686;&#24378;&#65292;&#20294;&#20173;&#28982;&#23384;&#22312;&#23545;&#20854;&#20135;&#29983;&#20559;&#35265;&#30340;&#25285;&#24551;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#8220;&#21463;&#38480;&#34892;&#19994;&#8221;&#22312;&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#36890;&#36807;&#25351;&#23450;&#25968;&#25454;&#38598;&#22686;&#24378;&#26469;&#21435;&#20559;&#35265;&#30340;&#26032;&#39062;&#33258;&#21160;&#26426;&#21046;&#12290;&#25105;&#20204;&#36824;&#21019;&#24314;&#20102;&#20004;&#20010;&#26032;&#30340;&#34913;&#37327;&#25351;&#26631;mb-index&#21644;db-index&#26469;&#37327;&#21270;&#20559;&#35265;&#65292;&#32771;&#34385;&#21040;&#20559;&#35265;&#26159;&#30001;&#20869;&#22312;&#27169;&#22411;&#26550;&#26500;&#21644;&#25968;&#25454;&#38598;&#20849;&#21516;&#23548;&#33268;&#30340;&#36825;&#19968;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13925v1 Announce Type: cross  Abstract: Despite the growing capabilities of large language models, there exists concerns about the biases they develop. In this paper, we propose a novel, automated mechanism for debiasing through specified dataset augmentation in the lens of bias producers and in the context of 'restricted industries' with limited data. We additionally create two new additional metrics, the mb-index and db-index, to quantify bias, considering the idea that bias occurs due to both intrinsic model architecture and dataset.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#20197;&#20943;&#23569;&#23545;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13890</link><description>&lt;p&gt;
&#20197;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;
&lt;/p&gt;
&lt;p&gt;
Towards Learning Contrast Kinetics with Multi-Condition Latent Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13890
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#23398;&#20064;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#20197;&#20943;&#23569;&#23545;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#23545;&#27604;&#22686;&#24378;&#30913;&#20849;&#25391;&#25104;&#20687;&#20013;&#30340;&#23545;&#27604;&#21058;&#21487;&#20197;&#23450;&#20301;&#32959;&#30244;&#24182;&#35266;&#23519;&#20854;&#23545;&#27604;&#21160;&#21147;&#23398;&#65292;&#36825;&#23545;&#20110;&#30284;&#30151;&#34920;&#24449;&#21644;&#27835;&#30103;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#27604;&#21058;&#30340;&#20351;&#29992;&#19981;&#20165;&#19982;&#19981;&#33391;&#20581;&#24247;&#39118;&#38505;&#30456;&#20851;&#65292;&#32780;&#19988;&#23545;&#20110;&#24576;&#23381;&#24739;&#32773;&#12289;&#32958;&#21151;&#33021;&#38556;&#30861;&#24739;&#32773;&#25110;&#20854;&#20182;&#19981;&#33391;&#21453;&#24212;&#24739;&#32773;&#23384;&#22312;&#38480;&#21046;&#12290;&#30001;&#20110;&#23545;&#27604;&#21058;&#25668;&#21462;&#26159;&#30149;&#28790;&#24694;&#24615;&#12289;&#30284;&#30151;&#22797;&#21457;&#39118;&#38505;&#21644;&#27835;&#30103;&#21453;&#24212;&#30340;&#20851;&#38190;&#29983;&#29289;&#26631;&#24535;&#29289;&#65292;&#22240;&#27492;&#20943;&#23569;&#38745;&#33033;&#20869;&#23545;&#27604;&#21058;&#30340;&#20381;&#36182;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33021;&#22815;&#36827;&#34892;DCE-MRI&#26102;&#38388;&#24207;&#21015;&#30340;&#33719;&#21462;&#26102;&#38388;&#26465;&#20214;&#22270;&#20687;&#21512;&#25104;&#30340;&#22810;&#26465;&#20214;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#12290;&#20026;&#20102;&#35780;&#20272;&#21307;&#23398;&#22270;&#20687;&#21512;&#25104;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#22522;&#20110;&#29983;&#29289;&#26631;&#24535;&#29289;&#21464;&#24322;&#24615;&#30340;Fr\'echet&#25918;&#23556;&#32452;&#23398;&#36317;&#31163;&#20316;&#20026;&#22270;&#20687;&#36136;&#37327;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13890v1 Announce Type: cross  Abstract: Contrast agents in dynamic contrast enhanced magnetic resonance imaging allow to localize tumors and observe their contrast kinetics, which is essential for cancer characterization and respective treatment decision-making. However, contrast agent administration is not only associated with adverse health risks, but also restricted for patients during pregnancy, and for those with kidney malfunction, or other adverse reactions. With contrast uptake as key biomarker for lesion malignancy, cancer recurrence risk, and treatment response, it becomes pivotal to reduce the dependency on intravenous contrast agent administration. To this end, we propose a multi-conditional latent diffusion model capable of acquisition time-conditioned image synthesis of DCE-MRI temporal sequences. To evaluate medical image synthesis, we additionally propose and validate the Fr\'echet radiomics distance as an image quality measure based on biomarker variability 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#21457;&#23637;&#19968;&#20010;&#22312;&#35780;&#20272;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#30340;&#20851;&#38190;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.13869</link><description>&lt;p&gt;
&#20934;&#30830;&#39044;&#27979;&#26234;&#33021;&#31995;&#32479;&#30340;&#23433;&#20840;&#20851;&#38190;&#31232;&#26377;&#20107;&#20214;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
Accurately Predicting Probabilities of Safety-Critical Rare Events for Intelligent Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13869
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#33268;&#21147;&#20110;&#21457;&#23637;&#19968;&#20010;&#22312;&#35780;&#20272;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#30340;&#20851;&#38190;&#24615;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#31995;&#32479;&#36234;&#26469;&#36234;&#25104;&#20026;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#28982;&#32780;&#32597;&#35265;&#30340;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#23545;&#23427;&#20204;&#30340;&#23454;&#38469;&#37096;&#32626;&#26500;&#25104;&#20102;&#37325;&#22823;&#28508;&#22312;&#23041;&#32961;&#12290;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#30340;&#20851;&#38190;&#22312;&#20110;&#20934;&#30830;&#39044;&#27979;&#22312;&#32473;&#23450;&#26102;&#38388;&#27493;&#38271;&#20869;&#20174;&#24403;&#21069;&#29366;&#24577;&#21457;&#29983;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#30340;&#27010;&#29575;&#65292;&#19968;&#20010;&#25105;&#20204;&#23450;&#20041;&#20026;&#8220;&#37325;&#35201;&#24615;&#8221;&#30340;&#25351;&#26631;&#12290;&#39044;&#27979;&#37325;&#35201;&#24615;&#30340;&#22797;&#26434;&#24615;&#28304;&#33258;&#20110;&#26497;&#31471;&#25968;&#25454;&#19981;&#24179;&#34913;&#65292;&#36825;&#26159;&#30001;&#39640;&#32500;&#21464;&#37327;&#20013;&#19982;&#32597;&#35265;&#20107;&#20214;&#30456;&#20851;&#32852;&#24341;&#36215;&#30340;&#19968;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#32597;&#35265;&#24615;&#35781;&#21650;&#12290;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#35201;&#20040;&#36807;&#20110;&#20445;&#23432;&#65292;&#35201;&#20040;&#23481;&#26131;&#24573;&#35270;&#23433;&#20840;&#20851;&#38190;&#20107;&#20214;&#65292;&#22240;&#27492;&#24456;&#38590;&#21516;&#26102;&#23454;&#29616;&#39640;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#36866;&#29992;&#24615;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#37325;&#35201;&#24615;&#39044;&#27979;&#27169;&#22411;&#65292;&#22312;&#35780;&#20272;&#23433;&#20840;&#20851;&#38190;&#33258;&#20027;&#23433;&#20840;&#24615;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#65292;&#22312;&#31934;&#24230;&#21644;&#21484;&#22238;&#29575;&#26041;&#38754;&#37117;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13869v1 Announce Type: cross  Abstract: Intelligent systems are increasingly integral to our daily lives, yet rare safety-critical events present significant latent threats to their practical deployment. Addressing this challenge hinges on accurately predicting the probability of safety-critical events occurring within a given time step from the current state, a metric we define as 'criticality'. The complexity of predicting criticality arises from the extreme data imbalance caused by rare events in high dimensional variables associated with the rare events, a challenge we refer to as the curse of rarity. Existing methods tend to be either overly conservative or prone to overlooking safety-critical events, thus struggling to achieve both high precision and recall rates, which severely limits their applicability. This study endeavors to develop a criticality prediction model that excels in both precision and recall rates for evaluating the criticality of safety-critical auton
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25293;&#21334;&#21551;&#21457;&#30340;&#22810;&#20154;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#32531;&#35299;GAN&#30340;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13866</link><description>&lt;p&gt;
&#25293;&#21334;&#21551;&#21457;&#30340;&#22810;&#20154;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
The Bid Picture: Auction-Inspired Multi-player Generative Adversarial Networks Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13866
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25293;&#21334;&#21551;&#21457;&#30340;&#22810;&#20154;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#32531;&#35299;GAN&#30340;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25293;&#21334;&#21551;&#21457;&#30340;&#22810;&#20154;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#21487;&#20197;&#32531;&#35299;GAN&#30340;&#27169;&#24335;&#22349;&#22604;&#38382;&#39064;&#12290;&#27169;&#24335;&#22349;&#22604;&#25351;&#30340;&#26159;&#24403;&#19968;&#20010;&#36807;&#25311;&#21512;&#30340;&#29983;&#25104;&#22120;&#29983;&#25104;&#19968;&#31181;&#26377;&#38480;&#33539;&#22260;&#30340;&#26679;&#26412;&#26102;&#65292;&#36890;&#24120;&#38598;&#20013;&#22312;&#25968;&#25454;&#20998;&#24067;&#30340;&#19968;&#23567;&#37096;&#20998;&#19978;&#12290;&#23613;&#31649;&#29983;&#25104;&#30340;&#26679;&#26412;&#22810;&#26679;&#24615;&#21463;&#38480;&#65292;&#37492;&#21035;&#22120;&#20173;&#28982;&#21487;&#20197;&#34987;&#27450;&#39575;&#65292;&#23558;&#36825;&#20123;&#26679;&#26412;&#35823;&#35748;&#20026;&#26469;&#33258;&#23454;&#38469;&#20998;&#24067;&#30340;&#30495;&#23454;&#26679;&#26412;&#12290;&#22312;&#27809;&#26377;&#22806;&#37096;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#65292;&#27169;&#22411;&#26080;&#27861;&#22312;&#35757;&#32451;&#38454;&#27573;&#35782;&#21035;&#20854;&#22833;&#36133;&#12290;&#25105;&#20204;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#30340;&#21452;&#26041;&#28216;&#25103;&#25193;&#23637;&#21040;&#22810;&#26041;&#28216;&#25103;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#27599;&#20010;&#27169;&#22411;&#30340;&#20215;&#20540;&#30001;&#20854;&#20182;&#29609;&#23478;&#22312;&#31867;&#20284;&#25293;&#21334;&#30340;&#36807;&#31243;&#20013;&#25552;&#20132;&#30340;&#20986;&#20215;&#20915;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13866v1 Announce Type: cross  Abstract: This article proposes auction-inspired multi-player generative adversarial networks training, which mitigates the mode collapse problem of GANs. Mode collapse occurs when an over-fitted generator generates a limited range of samples, often concentrating on a small subset of the data distribution. Despite the restricted diversity of generated samples, the discriminator can still be deceived into distinguishing these samples as real samples from the actual distribution. In the absence of external standards, a model cannot recognize its failure during the training phase. We extend the two-player game of generative adversarial networks to the multi-player game. During the training, the values of each model are determined by the bids submitted by other players in an auction-like process.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;DiffImpute&#65292;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#34920;&#26684;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#32570;&#22833;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#23450;&#21046;&#22810;&#20010;&#34920;&#26684;&#21435;&#22122;&#32593;&#32476;&#26469;&#22686;&#24378;&#25554;&#34917;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13863</link><description>&lt;p&gt;
&#29992;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#36827;&#34892;&#34920;&#26684;&#25968;&#25454;&#25554;&#34917;&#30340;DiffImpute
&lt;/p&gt;
&lt;p&gt;
DiffImpute: Tabular Data Imputation With Denoising Diffusion Probabilistic Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13863
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;DiffImpute&#65292;&#19968;&#31181;&#22522;&#20110;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#30340;&#34920;&#26684;&#25968;&#25454;&#25554;&#34917;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22788;&#29702;&#19981;&#21516;&#31867;&#22411;&#30340;&#32570;&#22833;&#25968;&#25454;&#65292;&#24182;&#36890;&#36807;&#23450;&#21046;&#22810;&#20010;&#34920;&#26684;&#21435;&#22122;&#32593;&#32476;&#26469;&#22686;&#24378;&#25554;&#34917;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#20294;&#24448;&#24448;&#23384;&#22312;&#32570;&#22833;&#20540;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#20854;&#28508;&#22312;&#25928;&#29992;&#12290;&#20256;&#32479;&#30340;&#25554;&#20540;&#25216;&#26415;&#36890;&#24120;&#20250;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#65292;&#24182;&#32473;&#21518;&#32493;&#24314;&#27169;&#20219;&#21153;&#24102;&#26469;&#37325;&#22823;&#35745;&#31639;&#36127;&#25285;&#65292;&#23548;&#33268;&#20986;&#29616;&#19981;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DiffImpute&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21435;&#22122;&#25193;&#25955;&#27010;&#29575;&#27169;&#22411;&#65288;DDPM&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DiffImpute&#22312;&#23436;&#25972;&#30340;&#34920;&#26684;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#30830;&#20445;&#21487;&#20197;&#20026;&#32570;&#22833;&#26465;&#30446;&#29983;&#25104;&#21487;&#20449;&#30340;&#25554;&#34917;&#65292;&#32780;&#19981;&#20250;&#30772;&#22351;&#29616;&#26377;&#25968;&#25454;&#30340;&#30495;&#23454;&#24615;&#12290;&#21019;&#26032;&#22320;&#65292;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#32570;&#22833;&#23436;&#20840;&#38543;&#26426;&#65288;MCAR&#65289;&#21644;&#32570;&#22833;&#38543;&#26426;&#65288;MAR&#65289;&#30340;&#21508;&#31181;&#24773;&#22659;&#12290;&#20026;&#20102;&#26377;&#25928;&#22788;&#29702;DDPM&#20013;&#30340;&#34920;&#26684;&#29305;&#24449;&#65292;&#25105;&#20204;&#23450;&#21046;&#20102;&#22235;&#20010;&#34920;&#26684;&#21435;&#22122;&#32593;&#32476;&#65292;&#28085;&#30422;MLP&#12289;ResNet&#12289;Transformer&#21644;U-Net&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;Harmonization&#26469;&#22686;&#24378;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#21644;&#25554;&#34917;&#25968;&#25454;&#20043;&#38388;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13863v1 Announce Type: cross  Abstract: Tabular data plays a crucial role in various domains but often suffers from missing values, thereby curtailing its potential utility. Traditional imputation techniques frequently yield suboptimal results and impose substantial computational burdens, leading to inaccuracies in subsequent modeling tasks. To address these challenges, we propose DiffImpute, a novel Denoising Diffusion Probabilistic Model (DDPM). Specifically, DiffImpute is trained on complete tabular datasets, ensuring that it can produce credible imputations for missing entries without undermining the authenticity of the existing data. Innovatively, it can be applied to various settings of Missing Completely At Random (MCAR) and Missing At Random (MAR). To effectively handle the tabular features in DDPM, we tailor four tabular denoising networks, spanning MLP, ResNet, Transformer, and U-Net. We also propose Harmonization to enhance coherence between observed and imputed d
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ST-PAD&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#31354;&#29289;&#29702;&#23398;&#24847;&#35782;&#21644;&#21442;&#25968;&#25193;&#25955;&#24341;&#23548;&#23454;&#29616;&#27969;&#20307;&#21160;&#21147;&#23398;&#30340;&#39640;&#31934;&#24230;&#20223;&#30495;&#21644;&#39044;&#27979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#24403;&#21069;&#20027;&#27969;&#27169;&#22411;</title><link>https://arxiv.org/abs/2403.13850</link><description>&lt;p&gt;
&#32463;&#36807;&#29289;&#29702;&#24847;&#35782;&#21644;&#21442;&#25968;&#25193;&#25955;&#25351;&#23548;&#30340;&#26102;&#31354;&#27969;&#20307;&#21160;&#21147;&#23398;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Spatio-Temporal Fluid Dynamics Modeling via Physical-Awareness and Parameter Diffusion Guidance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13850
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;ST-PAD&#26694;&#26550;&#65292;&#36890;&#36807;&#26102;&#31354;&#29289;&#29702;&#23398;&#24847;&#35782;&#21644;&#21442;&#25968;&#25193;&#25955;&#24341;&#23548;&#23454;&#29616;&#27969;&#20307;&#21160;&#21147;&#23398;&#30340;&#39640;&#31934;&#24230;&#20223;&#30495;&#21644;&#39044;&#27979;&#65292;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#24403;&#21069;&#20027;&#27969;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;ST-PAD&#30340;&#20004;&#38454;&#27573;&#26694;&#26550;&#65292;&#29992;&#20110;&#22320;&#29699;&#31185;&#23398;&#39046;&#22495;&#30340;&#26102;&#31354;&#27969;&#20307;&#21160;&#21147;&#23398;&#24314;&#27169;&#65292;&#26088;&#22312;&#36890;&#36807;&#26102;&#31354;&#29289;&#29702;&#23398;&#24847;&#35782;&#21644;&#21442;&#25968;&#25193;&#25955;&#24341;&#23548;&#23454;&#29616;&#27969;&#20307;&#21160;&#21147;&#23398;&#30340;&#39640;&#31934;&#24230;&#20223;&#30495;&#21644;&#39044;&#27979;&#12290;&#22312;&#19978;&#28216;&#38454;&#27573;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#26102;&#38388;&#28436;&#21464;&#29305;&#24615;&#30340;&#30690;&#37327;&#37327;&#21270;&#37325;&#26500;&#27169;&#22359;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#33324;&#30340;&#29289;&#29702;&#32422;&#26463;&#65292;&#30830;&#20445;&#20102;&#24179;&#34913;&#21644;&#26377;&#24377;&#24615;&#30340;&#21442;&#25968;&#20998;&#24067;&#12290;&#22312;&#19979;&#28216;&#38454;&#27573;&#65292;&#21033;&#29992;&#28041;&#21450;&#21442;&#25968;&#30340;&#25193;&#25955;&#27010;&#29575;&#32593;&#32476;&#26469;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#26410;&#26469;&#27969;&#20307;&#29366;&#24577;&#65292;&#21516;&#26102;&#36890;&#36807;&#24863;&#30693;&#19981;&#21516;&#29289;&#29702;&#35774;&#32622;&#20013;&#30340;&#21442;&#25968;&#65292;&#22686;&#24378;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#20102;ST-PAD&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#26174;&#31034;ST-PAD&#22312;&#27969;&#20307;&#21160;&#21147;&#23398;&#26041;&#38754;&#20248;&#20110;&#24403;&#21069;&#20027;&#27969;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13850v1 Announce Type: cross  Abstract: This paper proposes a two-stage framework named ST-PAD for spatio-temporal fluid dynamics modeling in the field of earth sciences, aiming to achieve high-precision simulation and prediction of fluid dynamics through spatio-temporal physics awareness and parameter diffusion guidance. In the upstream stage, we design a vector quantization reconstruction module with temporal evolution characteristics, ensuring balanced and resilient parameter distribution by introducing general physical constraints. In the downstream stage, a diffusion probability network involving parameters is utilized to generate high-quality future states of fluids, while enhancing the model's generalization ability by perceiving parameters in various physical setups. Extensive experiments on multiple benchmark datasets have verified the effectiveness and robustness of the ST-PAD framework, which showcase that ST-PAD outperforms current mainstream models in fluid dyna
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#20171;&#32461;&#20102;GNNs&#20013;&#30340;&#20808;&#36827;&#39046;&#22495;&#65306;&#22270;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2403.13849</link><description>&lt;p&gt;
&#25581;&#31192;&#22270;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#19982;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Graphs Unveiled: Graph Neural Networks and Graph Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13849
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#24182;&#20171;&#32461;&#20102;GNNs&#20013;&#30340;&#20808;&#36827;&#39046;&#22495;&#65306;&#22270;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#28909;&#28857;&#35805;&#39064;&#20043;&#19968;&#26159;GNN&#39046;&#22495;&#12290;&#22270;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#32473;&#29616;&#26377;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#24102;&#26469;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#65292;&#28044;&#29616;&#20102;&#35768;&#22810;&#20851;&#20110;&#25193;&#23637;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#22270;&#25968;&#25454;&#30340;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#39033;&#35843;&#30740;&#65292;&#20840;&#38754;&#27010;&#36848;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GNNs&#20013;&#30340;&#19968;&#39033;&#20808;&#36827;&#39046;&#22495;&#65306;&#22270;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13849v1 Announce Type: cross  Abstract: One of the hot topics in machine learning is the field of GNN. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. This paper represents a survey, providing a comprehensive overview of Graph Neural Networks (GNNs). We discuss the applications of graph neural networks across various domains. Finally, we present an advanced field in GNNs: graph generation.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24314;&#31435;Gini&#19981;&#32431;&#24230;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25552;&#20986;DP&#36138;&#23146;&#35268;&#21017;&#21015;&#34920;&#31639;&#27861;&#65292;&#26412;&#25991;&#25913;&#21892;&#20102;&#24046;&#24322;&#20445;&#25252;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13848</link><description>&lt;p&gt;
&#29992;&#20110;&#23398;&#20064;&#24046;&#24322;&#20445;&#25252;&#20294;&#20934;&#30830;&#35268;&#21017;&#21015;&#34920;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;
&lt;/p&gt;
&lt;p&gt;
Smooth Sensitivity for Learning Differentially-Private yet Accurate Rule Lists
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13848
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24314;&#31435;Gini&#19981;&#32431;&#24230;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;&#24182;&#23558;&#20854;&#24212;&#29992;&#20110;&#25552;&#20986;DP&#36138;&#23146;&#35268;&#21017;&#21015;&#34920;&#31639;&#27861;&#65292;&#26412;&#25991;&#25913;&#21892;&#20102;&#24046;&#24322;&#20445;&#25252;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#24322;&#20445;&#25252;&#65288;DP&#65289;&#26426;&#21046;&#21487;&#20197;&#23884;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#35774;&#35745;&#20013;&#65292;&#20197;&#20445;&#25252;&#25152;&#24471;&#27169;&#22411;&#20813;&#21463;&#38544;&#31169;&#27844;&#38706;&#30340;&#24433;&#21709;&#65292;&#23613;&#31649;&#36825;&#36890;&#24120;&#20276;&#38543;&#30528;&#26126;&#26174;&#30340;&#20934;&#30830;&#24615;&#25439;&#22833;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#24314;&#31435;Gini&#19981;&#32431;&#24230;&#30340;&#24179;&#28369;&#25935;&#24863;&#24230;&#24182;&#21033;&#29992;&#36825;&#19968;&#29305;&#24615;&#26469;&#25552;&#20986;&#19968;&#20010;DP&#36138;&#23146;&#35268;&#21017;&#21015;&#34920;&#31639;&#27861;&#65292;&#20197;&#25913;&#21892;&#36825;&#31181;&#26435;&#34913;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#38598;&#25104;&#24179;&#28369;&#25935;&#24863;&#24230;&#30340;DP&#35268;&#21017;&#21015;&#34920;&#27169;&#22411;&#20855;&#26377;&#27604;&#20351;&#29992;&#20840;&#23616;&#25935;&#24863;&#24230;&#30340;&#20854;&#20182;DP&#26694;&#26550;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13848v1 Announce Type: cross  Abstract: Differentially-private (DP) mechanisms can be embedded into the design of a machine learningalgorithm to protect the resulting model against privacy leakage, although this often comes with asignificant loss of accuracy. In this paper, we aim at improving this trade-off for rule lists modelsby establishing the smooth sensitivity of the Gini impurity and leveraging it to propose a DP greedyrule list algorithm. In particular, our theoretical analysis and experimental results demonstrate thatthe DP rule lists models integrating smooth sensitivity have higher accuracy that those using otherDP frameworks based on global sensitivity.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;&#65292;&#21487;&#20197;&#23454;&#29616;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#28151;&#21512;&#25104;&#20998;&#20043;&#38388;&#30340;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#22833;&#25928;&#35786;&#26029;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.13847</link><description>&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;
&lt;/p&gt;
&lt;p&gt;
Optimal Transport for Domain Adaptation through Gaussian Mixture Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13847
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26368;&#20248;&#36755;&#36816;&#65292;&#21487;&#20197;&#23454;&#29616;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#28151;&#21512;&#25104;&#20998;&#20043;&#38388;&#30340;&#21305;&#37197;&#65292;&#20174;&#32780;&#22312;&#22833;&#25928;&#35786;&#26029;&#20013;&#21462;&#24471;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;&#26368;&#20248;&#36755;&#36816;&#36827;&#34892;&#22495;&#33258;&#36866;&#24212;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;&#36890;&#36807;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#23545;&#25968;&#25454;&#20998;&#24067;&#36827;&#34892;&#24314;&#27169;&#12290;&#36825;&#31181;&#31574;&#30053;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#31561;&#20215;&#30340;&#31163;&#25955;&#38382;&#39064;&#35299;&#20915;&#36830;&#32493;&#26368;&#20248;&#36755;&#36816;&#12290;&#26368;&#20248;&#36755;&#36816;&#35299;&#20915;&#26041;&#26696;&#20026;&#25105;&#20204;&#25552;&#20379;&#20102;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#28151;&#21512;&#25104;&#20998;&#20043;&#38388;&#30340;&#21305;&#37197;&#12290;&#36890;&#36807;&#36825;&#31181;&#21305;&#37197;&#65292;&#25105;&#20204;&#21487;&#20197;&#22312;&#22495;&#20043;&#38388;&#26144;&#23556;&#25968;&#25454;&#28857;&#65292;&#25110;&#32773;&#23558;&#26631;&#31614;&#20174;&#28304;&#22495;&#32452;&#20214;&#36716;&#31227;&#21040;&#30446;&#26631;&#22495;&#12290;&#25105;&#20204;&#22312;&#22833;&#25928;&#35786;&#26029;&#30340;&#20004;&#20010;&#22495;&#33258;&#36866;&#24212;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13847v1 Announce Type: cross  Abstract: In this paper we explore domain adaptation through optimal transport. We propose a novel approach, where we model the data distributions through Gaussian mixture models. This strategy allows us to solve continuous optimal transport through an equivalent discrete problem. The optimal transport solution gives us a matching between source and target domain mixture components. From this matching, we can map data points between domains, or transfer the labels from the source domain components towards the target domain. We experiment with 2 domain adaptation benchmarks in fault diagnosis, showing that our methods have state-of-the-art performance.
&lt;/p&gt;</description></item><item><title>CMDI&#32858;&#31867;&#26041;&#27861;&#21019;&#26032;&#24615;&#22320;&#23558;&#20108;&#32500;&#32467;&#26500;&#20449;&#24687;&#29702;&#35770;&#34701;&#20837;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#24357;&#34917;&#20102;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#32858;&#31867;&#26041;&#27861;&#20013;&#24573;&#30053;&#30340;&#38543;&#26426;&#28216;&#36208;&#35775;&#38382;&#33410;&#28857;&#21644;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13846</link><description>&lt;p&gt;
&#19968;&#31181;&#20855;&#26377;&#22270;&#26368;&#22823;&#35299;&#30721;&#20449;&#24687;&#30340;&#32858;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Clustering Method with Graph Maximum Decoding Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13846
&lt;/p&gt;
&lt;p&gt;
CMDI&#32858;&#31867;&#26041;&#27861;&#21019;&#26032;&#24615;&#22320;&#23558;&#20108;&#32500;&#32467;&#26500;&#20449;&#24687;&#29702;&#35770;&#34701;&#20837;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#24357;&#34917;&#20102;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#32858;&#31867;&#26041;&#27861;&#20013;&#24573;&#30053;&#30340;&#38543;&#26426;&#28216;&#36208;&#35775;&#38382;&#33410;&#28857;&#21644;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#27169;&#22411;&#30340;&#32858;&#31867;&#26041;&#27861;&#22240;&#20854;&#22312;&#21508;&#31181;&#30693;&#35782;&#39046;&#22495;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#32780;&#22791;&#21463;&#20851;&#27880;&#12290;&#20854;&#33021;&#22815;&#19982;&#20854;&#20182;&#30456;&#20851;&#24212;&#29992;&#26080;&#32541;&#38598;&#25104;&#30340;&#36866;&#24212;&#24615;&#36171;&#20104;&#20102;&#22522;&#20110;&#22270;&#27169;&#22411;&#30340;&#32858;&#31867;&#20998;&#26512;&#33021;&#21147;&#65292;&#21487;&#20197;&#24378;&#22823;&#22320;&#20174;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#8220;&#33258;&#28982;&#20851;&#32852;&#8221;&#25110;&#8220;&#22270;&#32467;&#26500;&#8221;&#65292;&#26377;&#21161;&#20110;&#24314;&#27169;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#23613;&#31649;&#36825;&#31181;&#26041;&#27861;&#25928;&#26524;&#26174;&#33879;&#65292;&#20294;&#24403;&#21069;&#21033;&#29992;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#30340;&#32858;&#31867;&#26041;&#27861;&#24573;&#30053;&#20102;&#33410;&#28857;&#20043;&#38388;&#38543;&#26426;&#28216;&#36208;&#35775;&#38382;&#20197;&#21450;&#25968;&#25454;&#20013;&#23884;&#20837;&#30340;&#32467;&#26500;&#20449;&#24687;&#25152;&#24102;&#26469;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#20869;&#26368;&#22823;&#21270;&#35299;&#30721;&#20449;&#24687;&#30340;&#32858;&#31867;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;CMDI&#12290;CMDI&#21019;&#26032;&#22320;&#23558;&#20108;&#32500;&#32467;&#26500;&#20449;&#24687;&#29702;&#35770;&#32435;&#20837;&#21040;&#32858;&#31867;&#36807;&#31243;&#20013;&#65292;&#21253;&#25324;&#20004;&#20010;&#38454;&#27573;&#65306;&#22270;&#32467;&#26500;&#25552;&#21462;&#21644;&#22270;&#39030;&#28857;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13846v1 Announce Type: cross  Abstract: The clustering method based on graph models has garnered increased attention for its widespread applicability across various knowledge domains. Its adaptability to integrate seamlessly with other relevant applications endows the graph model-based clustering analysis with the ability to robustly extract "natural associations" or "graph structures" within datasets, facilitating the modelling of relationships between data points. Despite its efficacy, the current clustering method utilizing the graph-based model overlooks the uncertainty associated with random walk access between nodes and the embedded structural information in the data. To address this gap, we present a novel Clustering method for Maximizing Decoding Information within graph-based models, named CMDI. CMDI innovatively incorporates two-dimensional structural information theory into the clustering process, consisting of two phases: graph structure extraction and graph vert
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22686;&#37327;&#24335;ZSFD&#33539;&#24335;&#65292;&#24320;&#21457;&#20102;&#24191;&#28145;&#28151;&#21512;&#21453;&#36951;&#24536;&#26694;&#26550;&#65288;BDMAFF&#65289;&#65292;&#26088;&#22312;&#23398;&#20064;&#21644;&#36866;&#24212;&#26032;&#30340;&#25925;&#38556;&#31867;&#21035;&#21644;&#23646;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;ZSFD&#33539;&#24335;&#22312;&#24037;&#19994;&#22330;&#26223;&#20013;&#26080;&#27861;&#20174;&#19981;&#26029;&#21464;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.13845</link><description>&lt;p&gt;
&#23398;&#20250;&#26356;&#22909;&#22320;&#30475;&#35265;&#30475;&#19981;&#35265;&#30340;&#19996;&#35199;&#65306;&#29992;&#20110;&#22686;&#37327;&#24335;&#38646;&#26679;&#26412;&#25925;&#38556;&#35786;&#26029;&#30340;&#24191;&#28145;&#28151;&#21512;&#21453;&#36951;&#24536;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Learning to better see the unseen: Broad-Deep Mixed Anti-Forgetting Framework for Incremental Zero-Shot Fault Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13845
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22686;&#37327;&#24335;ZSFD&#33539;&#24335;&#65292;&#24320;&#21457;&#20102;&#24191;&#28145;&#28151;&#21512;&#21453;&#36951;&#24536;&#26694;&#26550;&#65288;BDMAFF&#65289;&#65292;&#26088;&#22312;&#23398;&#20064;&#21644;&#36866;&#24212;&#26032;&#30340;&#25925;&#38556;&#31867;&#21035;&#21644;&#23646;&#24615;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;ZSFD&#33539;&#24335;&#22312;&#24037;&#19994;&#22330;&#26223;&#20013;&#26080;&#27861;&#20174;&#19981;&#26029;&#21464;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#27969;&#20013;&#23398;&#20064;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#26679;&#26412;&#25925;&#38556;&#35786;&#26029;&#65288;ZSFD&#65289;&#33021;&#22815;&#36890;&#36807;&#39044;&#27979;&#20154;&#31867;&#19987;&#23478;&#26631;&#27880;&#30340;&#25925;&#38556;&#23646;&#24615;&#26469;&#35782;&#21035;&#30475;&#19981;&#35265;&#30340;&#25925;&#38556;&#12290;&#20026;&#20102;&#35299;&#20915;&#24037;&#19994;&#36807;&#31243;&#20013;&#25345;&#32493;&#21464;&#21270;&#30340;&#38656;&#27714;&#65292;&#21363;&#27169;&#22411;&#36866;&#24212;&#26032;&#30340;&#25925;&#38556;&#31867;&#21035;&#21644;&#23646;&#24615;&#32780;&#36991;&#20813;&#24536;&#35760;&#20808;&#21069;&#23398;&#21040;&#30340;&#35786;&#26029;&#33021;&#21147;&#65292;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#22686;&#37327;&#24335;ZSFD&#65288;IZSFD&#65289;&#33539;&#24335;&#65292;&#23427;&#32467;&#21512;&#20102;&#20256;&#32479;ZSFD&#21644;&#24191;&#20041;ZSFD&#33539;&#24335;&#30340;&#31867;&#21035;&#22686;&#37327;&#21644;&#23646;&#24615;&#22686;&#37327;&#12290;&#20026;&#20102;&#23454;&#29616;IZSFD&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#23398;&#20064;&#26032;&#30340;&#25925;&#38556;&#31867;&#21035;&#21644;&#23646;&#24615;&#30340;&#24191;&#28145;&#28151;&#21512;&#21453;&#36951;&#24536;&#26694;&#26550;&#65288;BDMAFF&#65289;&#12290;&#20026;&#20102;&#35299;&#20915;&#36951;&#24536;&#38382;&#39064;&#65292;BDMAFF&#26377;&#25928;&#22320;&#20174;&#20004;&#20010;&#35282;&#24230;&#31215;&#32047;&#20808;&#21069;&#33719;&#24471;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13845v1 Announce Type: cross  Abstract: Zero-shot fault diagnosis (ZSFD) is capable of identifying unseen faults via predicting fault attributes labeled by human experts. We first recognize the demand of ZSFD to deal with continuous changes in industrial processes, i.e., the model's ability to adapt to new fault categories and attributes while avoiding forgetting the diagnosis ability learned previously. To overcome the issue that the existing ZSFD paradigm cannot learn from evolving streams of training data in industrial scenarios, the incremental ZSFD (IZSFD) paradigm is proposed for the first time, which incorporates category increment and attribute increment for both traditional ZSFD and generalized ZSFD paradigms. To achieve IZSFD, we present a broad-deep mixed anti-forgetting framework (BDMAFF) that aims to learn from new fault categories and attributes. To tackle the issue of forgetting, BDMAFF effectively accumulates previously acquired knowledge from two perspective
&lt;/p&gt;</description></item><item><title>&#20302;&#32500;&#24230;&#35745;&#31639;&#20998;&#31867;&#22120;&#22522;&#20110;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;&#65288;VSA&#65289;&#65292;&#36890;&#36807;&#23450;&#26102;&#30693;&#35782;&#33719;&#21462;&#26041;&#27861;&#65292;&#25552;&#39640;&#23567;&#27169;&#22411;&#20934;&#30830;&#29575;&#65292;&#35299;&#20915;&#22788;&#29702;&#22797;&#26434;&#33041;&#20449;&#21495;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.13844</link><description>&lt;p&gt;
&#38754;&#21521;&#33041;&#26426;&#25509;&#21475;&#30340;&#36731;&#37327;&#32423;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#26550;&#26500;&#30340;&#23450;&#26102;&#30693;&#35782;&#33719;&#21462;
&lt;/p&gt;
&lt;p&gt;
Scheduled Knowledge Acquisition on Lightweight Vector Symbolic Architectures for Brain-Computer Interfaces
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13844
&lt;/p&gt;
&lt;p&gt;
&#20302;&#32500;&#24230;&#35745;&#31639;&#20998;&#31867;&#22120;&#22522;&#20110;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;&#65288;VSA&#65289;&#65292;&#36890;&#36807;&#23450;&#26102;&#30693;&#35782;&#33719;&#21462;&#26041;&#27861;&#65292;&#25552;&#39640;&#23567;&#27169;&#22411;&#20934;&#30830;&#29575;&#65292;&#35299;&#20915;&#22788;&#29702;&#22797;&#26434;&#33041;&#20449;&#21495;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#26426;&#25509;&#21475;&#65288;BCIs&#65289;&#36890;&#24120;&#35774;&#35745;&#20026;&#36731;&#37327;&#32423;&#19988;&#23454;&#26102;&#21709;&#24212;&#65292;&#20197;&#20026;&#29992;&#25143;&#25552;&#20379;&#21450;&#26102;&#21453;&#39304;&#12290;&#32463;&#20856;&#29305;&#24449;&#24037;&#31243;&#35745;&#31639;&#25928;&#29575;&#39640;&#20294;&#20934;&#30830;&#29575;&#20302;&#65292;&#32780;&#36817;&#26399;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#25552;&#39640;&#20934;&#30830;&#29575;&#20294;&#35745;&#31639;&#20195;&#20215;&#39640;&#12289;&#24310;&#36831;&#22823;&#12290;&#20316;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#36873;&#25321;&#65292;&#22522;&#20110;&#21521;&#37327;&#31526;&#21495;&#20307;&#31995;&#32467;&#26500;&#65288;VSA&#65289;&#30340;&#20302;&#32500;&#24230;&#35745;&#31639;&#65288;LDC&#65289;&#20998;&#31867;&#22120;&#23454;&#29616;&#20102;&#23567;&#27169;&#22411;&#22823;&#23567;&#65292;&#20294;&#20934;&#30830;&#29575;&#39640;&#20110;&#32463;&#20856;&#29305;&#24449;&#24037;&#31243;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#20934;&#30830;&#29575;&#20173;&#33853;&#21518;&#20110;&#29616;&#20195;DNNs&#65292;&#36825;&#20351;&#24471;&#22788;&#29702;&#22797;&#26434;&#30340;&#33041;&#20449;&#21495;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#25552;&#39640;&#23567;&#27169;&#22411;&#30340;&#20934;&#30830;&#29575;&#65292;&#30693;&#35782;&#33976;&#39311;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22312;&#23398;&#29983;&#27169;&#22411;&#22312;&#20854;&#19981;&#26029;&#23398;&#20064;&#38454;&#27573;&#26102;&#20445;&#25345;&#25945;&#24072;&#27169;&#22411;&#21644;&#23398;&#29983;&#27169;&#22411;&#20043;&#38388;&#30340;&#24658;&#23450;&#33976;&#39311;&#27700;&#24179;&#21487;&#33021;&#24182;&#38750;&#26368;&#20339;&#36873;&#25321;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23450;&#26102;&#30693;&#35782;&#33719;&#21462;&#30340;&#26041;&#27861;&#65292;&#20351;&#23398;&#29983;&#27169;&#22411;&#33021;&#22815;&#22312;&#36880;&#27493;&#23398;&#20064;&#38454;&#27573;&#25913;&#21892;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13844v1 Announce Type: cross  Abstract: Brain-Computer interfaces (BCIs) are typically designed to be lightweight and responsive in real-time to provide users timely feedback. Classical feature engineering is computationally efficient but has low accuracy, whereas the recent neural networks (DNNs) improve accuracy but are computationally expensive and incur high latency. As a promising alternative, the low-dimensional computing (LDC) classifier based on vector symbolic architecture (VSA), achieves small model size yet higher accuracy than classical feature engineering methods. However, its accuracy still lags behind that of modern DNNs, making it challenging to process complex brain signals. To improve the accuracy of a small model, knowledge distillation is a popular method. However, maintaining a constant level of distillation between the teacher and student models may not be the best way for a growing student during its progressive learning stages. In this work, we propos
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#32467;&#21512;transformer&#35780;&#20272;&#30002;&#29366;&#33146;&#30284;&#39044;&#21518;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#26032;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#24182;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36741;&#21161;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13843</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#21644;&#35270;&#35273;Transformer&#22312;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#20013;&#30340;&#24212;&#29992;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning and Vision Transformers for Thyroid Carcinoma Diagnosis: A review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13843
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#24635;&#32467;&#20102;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#32467;&#21512;transformer&#35780;&#20272;&#30002;&#29366;&#33146;&#30284;&#39044;&#21518;&#30340;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#26032;&#30340;&#20998;&#31867;&#31995;&#32479;&#65292;&#24182;&#24378;&#35843;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#36741;&#21161;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#21644;&#27835;&#30103;&#20013;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21457;&#23637;&#26234;&#33021;&#35786;&#26029;&#31995;&#32479;&#20197;&#24110;&#21161;&#21307;&#23398;&#19987;&#23478;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#20197;&#27835;&#30103;&#19981;&#21487;&#27835;&#24840;&#30142;&#30149;&#30340;&#20852;&#36259;&#19981;&#26029;&#22686;&#38271;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#35782;&#21035;&#30002;&#29366;&#33146;&#30284;&#65288;TC&#65289;&#30340;&#25361;&#25112;&#26041;&#38754;&#65292;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#21644;&#22823;&#25968;&#25454;&#20998;&#26512;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#32467;&#21512;transformer&#35780;&#20272;TC&#39044;&#21518;&#65292;&#24182;&#30830;&#23450;&#20010;&#20307;&#30340;&#24694;&#24615;&#39118;&#38505;&#12290;&#26412;&#32508;&#36848;&#25991;&#31456;&#24635;&#32467;&#20102;&#21508;&#31181;&#20851;&#20110;&#20197;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#31639;&#27861;&#20026;&#22522;&#30784;&#30340;&#26041;&#27861;&#30340;&#30740;&#31350;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#37319;&#29992;transformer&#36827;&#34892;&#30002;&#29366;&#33146;&#30284;&#35786;&#26029;&#30340;&#26041;&#27861;&#12290;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20110;AI&#31639;&#27861;&#12289;&#26694;&#26550;&#30446;&#26631;&#21644;&#20351;&#29992;&#30340;&#35745;&#31639;&#29615;&#22659;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#31995;&#32479;&#12290;&#27492;&#22806;&#65292;&#23427;&#36890;&#36807;&#20854;&#29305;&#24449;&#23457;&#26597;&#21644;&#23545;&#27604;&#20102;&#21487;&#29992;&#30340;TC&#25968;&#25454;&#38598;&#12290;&#35813;&#35770;&#25991;&#24378;&#35843;&#20102;AI&#24037;&#20855;&#22312;&#36890;&#36807;&#30417;&#30563;&#12289;&#26080;&#30417;&#30563;&#25110;&#28151;&#21512;&#26041;&#24335;&#21327;&#21161;&#35786;&#26029;&#21644;&#27835;&#30103;TC&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13843v1 Announce Type: cross  Abstract: The growing interest in developing smart diagnostic systems to help medical experts process extensive data for treating incurable diseases has been notable. In particular, the challenge of identifying thyroid cancer (TC) has seen progress with the use of machine learning (ML) and big data analysis, incorporating transformers to evaluate TC prognosis and determine the risk of malignancy in individuals. This review article presents a summary of various studies on AIbased approaches, especially those employing transformers, for diagnosing TC. It introduces a new categorization system for these methods based on artifcial intelligence (AI) algorithms, the goals of the framework, and the computing environments used. Additionally, it scrutinizes and contrasts the available TC datasets by their features. The paper highlights the importance of AI instruments in aiding the diagnosis and treatment of TC through supervised, unsupervised, or mixed 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#20102;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#24773;&#24863;&#29366;&#24577;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.13841</link><description>&lt;p&gt;
&#25972;&#21512;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#29992;&#20110;&#20010;&#24615;&#21270;&#24773;&#24863;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Integrating Wearable Sensor Data and Self-reported Diaries for Personalized Affect Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#20102;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#24773;&#24863;&#29366;&#24577;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#29366;&#24577;&#20316;&#20026;&#24773;&#24863;&#30340;&#25351;&#26631;&#23545;&#25972;&#20307;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#22312;&#20854;&#21457;&#20316;&#21069;&#20934;&#30830;&#39044;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#26469;&#33258;&#21487;&#31359;&#25140;&#21644;&#31227;&#21160;&#35774;&#22791;&#30340;&#25968;&#25454;&#36827;&#34892;&#19982;&#30701;&#26399;&#24773;&#24863;&#26816;&#27979;&#12290;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#19987;&#27880;&#20110;&#23458;&#35266;&#30340;&#24863;&#23448;&#27979;&#37327;&#65292;&#24448;&#24448;&#24573;&#30053;&#20854;&#20182;&#24418;&#24335;&#30340;&#33258;&#25105;&#25253;&#21578;&#20449;&#24687;&#65292;&#22914;&#26085;&#35760;&#21644;&#31508;&#35760;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24773;&#24863;&#29366;&#24577;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#19968;&#20010;transformer&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23458;&#35266;&#25351;&#26631;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#30340;&#32508;&#21512;&#20998;&#26512;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#32437;&#21521;&#30740;&#31350;&#65292;&#25307;&#21215;&#20102;&#22823;&#23398;&#29983;&#24182;&#22312;&#19968;&#24180;&#20869;&#23545;&#20854;&#36827;&#34892;&#30417;&#27979;&#65292;&#25910;&#38598;&#20102;&#21253;&#25324;&#29983;&#29702;&#12289;&#29615;&#22659;&#12289;&#30561;&#30496;&#12289;&#20195;&#35874;&#21644;&#36523;&#20307;&#27963;&#21160;&#21442;&#25968;&#22312;&#20869;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21442;&#19982;&#32773;&#25552;&#20379;&#20102;&#24320;&#25918;&#24335;&#25991;&#26412;&#26085;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13841v1 Announce Type: cross  Abstract: Emotional states, as indicators of affect, are pivotal to overall health, making their accurate prediction before onset crucial. Current studies are primarily centered on immediate short-term affect detection using data from wearable and mobile devices. These studies typically focus on objective sensory measures, often neglecting other forms of self-reported information like diaries and notes. In this paper, we propose a multimodal deep learning model for affect status forecasting. This model combines a transformer encoder with a pre-trained language model, facilitating the integrated analysis of objective metrics and self-reported diaries. To validate our model, we conduct a longitudinal study, enrolling college students and monitoring them over a year, to collect an extensive dataset including physiological, environmental, sleep, metabolic, and physical activity parameters, alongside open-ended textual diaries provided by the partici
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#37327;&#26694;&#26550;&#21644;&#27969;&#31243;&#65292;&#31995;&#32479;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25919;&#27835;&#21462;&#21521;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#20542;&#21521;&#20110;&#25552;&#20379;&#19982;&#33258;&#30001;&#20027;&#20041;&#25110;&#24038;&#20542;&#35266;&#28857;&#26356;&#20026;&#25509;&#36817;&#30340;&#22238;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.13840</link><description>&lt;p&gt;
&#20320;&#31449;&#22312;&#21738;&#19968;&#36793;&#65311;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25919;&#27835;&#31435;&#22330;
&lt;/p&gt;
&lt;p&gt;
Whose Side Are You On? Investigating the Political Stance of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#37327;&#26694;&#26550;&#21644;&#27969;&#31243;&#65292;&#31995;&#32479;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25919;&#27835;&#21462;&#21521;&#65292;&#32467;&#26524;&#26174;&#31034;&#36825;&#20123;&#27169;&#22411;&#20542;&#21521;&#20110;&#25552;&#20379;&#19982;&#33258;&#30001;&#20027;&#20041;&#25110;&#24038;&#20542;&#35266;&#28857;&#26356;&#20026;&#25509;&#36817;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22240;&#20854;&#22312;&#25991;&#26412;&#29983;&#25104;&#12289;&#25688;&#35201;&#21644;&#20449;&#24687;&#26816;&#32034;&#31561;&#26085;&#24120;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;&#32780;&#22791;&#21463;&#27426;&#36814;&#12290;&#38543;&#30528;LLMs&#30340;&#24191;&#27867;&#24212;&#29992;&#19981;&#26029;&#22686;&#38271;&#65292;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#20135;&#29983;&#25919;&#27835;&#20013;&#31435;&#30340;&#22238;&#24212;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#26088;&#22312;&#36991;&#20813;&#20449;&#24687;&#27873;&#27819;&#65292;&#32500;&#25252;&#20195;&#34920;&#20844;&#24179;&#65292;&#24182;&#20943;&#36731;&#30830;&#35748;&#20559;&#35265;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#37327;&#26694;&#26550;&#21644;&#27969;&#31243;&#65292;&#26088;&#22312;&#31995;&#32479;&#22320;&#35843;&#26597;LLMs&#30340;&#25919;&#27835;&#21462;&#21521;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#28145;&#20837;&#25506;&#35752;LLMs&#22312;&#20843;&#20010;&#26497;&#21270;&#35805;&#39064;&#30340;&#25919;&#27835;&#21462;&#21521;&#65292;&#20174;&#22549;&#32974;&#21040;LGBTQ&#38382;&#39064;&#36328;&#36234;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#20010;&#35805;&#39064;&#19978;&#65292;LLMs&#20542;&#21521;&#20110;&#25552;&#20379;&#19982;&#33258;&#30001;&#20027;&#20041;&#25110;&#24038;&#20542;&#35266;&#28857;&#26356;&#20026;&#25509;&#36817;&#30340;&#22238;&#24212;&#65292;&#32780;&#19981;&#26159;&#19982;&#20445;&#23432;&#20027;&#20041;&#25110;&#21491;&#20542;&#35266;&#28857;&#26356;&#20026;&#25509;&#36817;&#30340;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13840v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have gained significant popularity for their application in various everyday tasks such as text generation, summarization, and information retrieval. As the widespread adoption of LLMs continues to surge, it becomes increasingly crucial to ensure that these models yield responses that are politically impartial, with the aim of preventing information bubbles, upholding fairness in representation, and mitigating confirmation bias. In this paper, we propose a quantitative framework and pipeline designed to systematically investigate the political orientation of LLMs. Our investigation delves into the political alignment of LLMs across a spectrum of eight polarizing topics, spanning from abortion to LGBTQ issues. Across topics, the results indicate that LLMs exhibit a tendency to provide responses that closely align with liberal or left-leaning perspectives rather than conservative or right-leaning ones when us
&lt;/p&gt;</description></item><item><title>depyf&#26159;&#19968;&#20010;&#38750;&#20405;&#20837;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#25581;&#24320;PyTorch&#32534;&#35793;&#22120;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#21453;&#32534;&#35793;&#23558;&#23383;&#33410;&#30721;&#36716;&#25442;&#20026;&#28304;&#20195;&#30721;&#65292;&#20174;&#32780;&#22686;&#36827;&#29992;&#25143;&#23545;&#24213;&#23618;&#36807;&#31243;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.13839</link><description>&lt;p&gt;
depyf&#65306;&#20026;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#25171;&#24320;PyTorch&#32534;&#35793;&#22120;&#30340;&#31070;&#31192;&#30418;&#23376;
&lt;/p&gt;
&lt;p&gt;
depyf: Open the Opaque Box of PyTorch Compiler for Machine Learning Researchers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13839
&lt;/p&gt;
&lt;p&gt;
depyf&#26159;&#19968;&#20010;&#38750;&#20405;&#20837;&#24615;&#21644;&#29992;&#25143;&#21451;&#22909;&#30340;&#24037;&#20855;&#65292;&#26088;&#22312;&#24110;&#21161;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#25581;&#24320;PyTorch&#32534;&#35793;&#22120;&#30340;&#20869;&#37096;&#24037;&#20316;&#26426;&#21046;&#65292;&#24182;&#36890;&#36807;&#21453;&#32534;&#35793;&#23558;&#23383;&#33410;&#30721;&#36716;&#25442;&#20026;&#28304;&#20195;&#30721;&#65292;&#20174;&#32780;&#22686;&#36827;&#29992;&#25143;&#23545;&#24213;&#23618;&#36807;&#31243;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
PyTorch 2.x&#24341;&#20837;&#20102;&#19968;&#20010;&#26088;&#22312;&#21152;&#36895;&#28145;&#24230;&#23398;&#20064;&#31243;&#24207;&#30340;&#32534;&#35793;&#22120;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#32773;&#26469;&#35828;&#65292;&#20805;&#20998;&#21033;&#29992;PyTorch&#32534;&#35793;&#22120;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32534;&#35793;&#22120;&#22312;Python&#23383;&#33410;&#30721;&#32423;&#21035;&#36816;&#34892;&#65292;&#20351;&#20854;&#30475;&#36215;&#26469;&#20687;&#19968;&#20010;&#31070;&#31192;&#30340;&#30418;&#23376;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;depyf&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#25581;&#24320;PyTorch&#32534;&#35793;&#22120;&#20869;&#37096;&#26426;&#21046;&#30340;&#24037;&#20855;&#12290;depyf&#21487;&#20197;&#23558;PyTorch&#29983;&#25104;&#30340;&#23383;&#33410;&#30721;&#21453;&#32534;&#35793;&#20026;&#31561;&#25928;&#30340;&#28304;&#20195;&#30721;&#65292;&#24182;&#24314;&#31435;&#20869;&#23384;&#20013;&#20195;&#30721;&#23545;&#35937;&#19982;&#30913;&#30424;&#19978;&#28304;&#20195;&#30721;&#23545;&#24212;&#30340;&#32852;&#31995;&#12290;&#36825;&#20010;&#29305;&#24615;&#20351;&#29992;&#25143;&#21487;&#20197;&#20351;&#29992;&#35843;&#35797;&#22120;&#36880;&#34892;&#26597;&#30475;&#28304;&#20195;&#30721;&#65292;&#20174;&#32780;&#22686;&#36827;&#23545;&#24213;&#23618;&#36807;&#31243;&#30340;&#29702;&#35299;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;depyf&#26159;&#38750;&#20405;&#20837;&#24615;&#19988;&#29992;&#25143;&#21451;&#22909;&#30340;&#65292;&#20027;&#35201;&#20381;&#36182;&#20110;&#20004;&#20010;&#26041;&#20415;&#30340;&#19978;&#19979;&#25991;&#31649;&#29702;&#22120;&#26469;&#23454;&#29616;&#20854;&#26680;&#24515;&#21151;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13839v1 Announce Type: cross  Abstract: PyTorch \texttt{2.x} introduces a compiler designed to accelerate deep learning programs. However, for machine learning researchers, adapting to the PyTorch compiler to full potential can be challenging. The compiler operates at the Python bytecode level, making it appear as an opaque box. To address this, we introduce \texttt{depyf}, a tool designed to demystify the inner workings of the PyTorch compiler. \texttt{depyf} decompiles bytecode generated by PyTorch back into equivalent source code, and establishes connections between in-memory code objects and their on-disk source code counterparts. This feature enables users to step through the source code line by line using debuggers, thus enhancing their understanding of the underlying processes. Notably, \texttt{depyf} is non-intrusive and user-friendly, primarily relying on two convenient context managers for its core functionality. The project is \href{https://github.com/thuml/depyf}
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;SMART&#26694;&#26550;&#65292;&#21487;&#36890;&#36807;&#20934;&#30830;&#24615;&#32422;&#26463;&#26368;&#23567;&#21270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#32467;&#26524;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2403.13835</link><description>&lt;p&gt;
&#20351;&#29992;&#20934;&#30830;&#24615;&#20445;&#35777;&#33258;&#21160;&#32553;&#20943;&#35821;&#35328;&#27169;&#22411;&#35268;&#27169;&#20197;&#38477;&#20302;&#22788;&#29702;&#36153;&#29992;&#30340;SMART
&lt;/p&gt;
&lt;p&gt;
SMART: Automatically Scaling Down Language Models with Accuracy Guarantees for Reduced Processing Fees
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13835
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;SMART&#26694;&#26550;&#65292;&#21487;&#36890;&#36807;&#20934;&#30830;&#24615;&#32422;&#26463;&#26368;&#23567;&#21270;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#32467;&#26524;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#27493;&#26174;&#33879;&#25552;&#39640;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#37096;&#32626;&#39640;&#24615;&#33021;LLMs&#20250;&#20135;&#29983;&#24040;&#22823;&#30340;&#25104;&#26412;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#22686;&#21152;&#30340;&#21442;&#25968;&#25968;&#37327;&#26088;&#22312;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20351;&#24471;&#26368;&#20808;&#36827;&#30340;LLMs&#23545;&#32456;&#31471;&#29992;&#25143;&#32780;&#35328;&#21464;&#24471;&#26356;&#21152;&#26114;&#36149;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;SMART&#65292;&#21363;&#20026;&#38477;&#20302;&#26631;&#35760;&#36153;&#29992;&#32780;&#33258;&#36866;&#24212;&#32553;&#25918;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;LLM&#26694;&#26550;&#65292;&#26088;&#22312;&#26368;&#22823;&#31243;&#24230;&#22320;&#38477;&#20302;NLP&#20219;&#21153;&#30340;&#25512;&#29702;&#25104;&#26412;&#65292;&#21516;&#26102;&#30830;&#20445;&#36275;&#22815;&#30340;&#32467;&#26524;&#36136;&#37327;&#12290;&#23427;&#20351;&#29992;&#25143;&#33021;&#22815;&#20197;&#36755;&#20986;&#30340;&#31561;&#25928;&#24615;&#25351;&#23450;&#20934;&#30830;&#24615;&#32422;&#26463;&#19982;&#26368;&#24378;&#22823;&#30340;LLM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13835v1 Announce Type: cross  Abstract: The advancement of Large Language Models (LLMs) has significantly boosted performance in natural language processing (NLP) tasks. However, the deployment of high-performance LLMs incurs substantial costs, primarily due to the increased number of parameters aimed at enhancing model performance. This has made the use of state-of-the-art LLMs more expensive for end-users. AI service providers, such as OpenAI and Anthropic, often offer multiple versions of LLMs with varying prices and performance. However, end-users still face challenges in choosing the appropriate LLM for their tasks that balance result quality with cost.   We introduce SMART, Scaling Models Adaptively for Reduced Token Fees, a novel LLM framework designed to minimize the inference costs of NLP tasks while ensuring sufficient result quality. It enables users to specify an accuracy constraint in terms of the equivalence of outputs to those of the most powerful LLM. SMART t
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;IEA-GAN&#65292;&#20197;&#21450;&#19968;&#31181;&#36814;&#25509;&#19979;&#28216;&#29289;&#29702;&#20998;&#26512;&#25361;&#25112;&#30340;YonedaVAE&#27169;&#22411;&#65292;&#20026;&#36229;&#39640;&#32454;&#31890;&#24230;&#31890;&#23376;&#29289;&#29702;&#25506;&#27979;&#22120;&#27169;&#25311;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.13825</link><description>&lt;p&gt;
&#28145;&#23618;&#29983;&#25104;&#27169;&#22411;&#29992;&#20110;&#36229;&#39640;&#32454;&#31890;&#24230;&#31890;&#23376;&#29289;&#29702;&#25506;&#27979;&#22120;&#27169;&#25311;&#65306;&#20174;&#20223;&#30495;&#21040;&#22806;&#25512;&#30340;&#33322;&#31243;
&lt;/p&gt;
&lt;p&gt;
Deep Generative Models for Ultra-High Granularity Particle Physics Detector Simulation: A Voyage From Emulation to Extrapolation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13825
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20960;&#20309;&#24863;&#30693;&#29983;&#25104;&#27169;&#22411;IEA-GAN&#65292;&#20197;&#21450;&#19968;&#31181;&#36814;&#25509;&#19979;&#28216;&#29289;&#29702;&#20998;&#26512;&#25361;&#25112;&#30340;YonedaVAE&#27169;&#22411;&#65292;&#20026;&#36229;&#39640;&#32454;&#31890;&#24230;&#31890;&#23376;&#29289;&#29702;&#25506;&#27979;&#22120;&#27169;&#25311;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31890;&#23376;&#29289;&#29702;&#20013;&#27169;&#25311;&#36229;&#39640;&#32454;&#31890;&#24230;&#25506;&#27979;&#22120;&#21709;&#24212;&#26159;&#19968;&#39033;&#33267;&#20851;&#37325;&#35201;&#20294;&#35745;&#31639;&#37327;&#24040;&#22823;&#30340;&#20219;&#21153;&#12290;&#26412;&#35770;&#25991;&#26088;&#22312;&#20026;Belle II&#23454;&#39564;&#30340;Pixel Vertex Detector (PXD) &#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#35813;&#23454;&#39564;&#20855;&#26377;&#36229;&#36807;750&#19975;&#20687;&#32032;&#36890;&#36947;-&#26159;&#26377;&#21490;&#20197;&#26469;&#20351;&#29992;&#29983;&#25104;&#27169;&#22411;&#20998;&#26512;&#30340;&#31354;&#38388;&#20998;&#36776;&#29575;&#26368;&#39640;&#30340;&#25506;&#27979;&#22120;&#27169;&#25311;&#25968;&#25454;&#38598;&#12290;&#35770;&#25991;&#39318;&#20808;&#23545;&#29992;&#20110;&#27169;&#25311;&#25506;&#27979;&#22120;&#29305;&#24449;&#30340;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#21644;&#20998;&#31867;&#23398;&#30340;&#22238;&#39038;&#12290;&#28982;&#21518;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#20960;&#20309;&#24863;&#30693;&#30340;&#26032;&#22411;&#29983;&#25104;&#27169;&#22411;"Intra-Event Aware Generative Adversarial Network (IEA-GAN)"&#65292;&#24341;&#20837;&#20851;&#31995;&#24335;&#27880;&#24847;&#25512;&#29702;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#36817;&#20284;&#25506;&#27979;&#22120;&#20013;&#30340;"&#20107;&#20214;"&#12290;&#35813;&#30740;&#31350;&#24378;&#35843;&#20102;&#23545;&#19979;&#28216;&#29289;&#29702;&#20998;&#26512;&#32780;&#35328;&#8220;&#20107;&#20214;&#20869;&#20851;&#32852;&#8221;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#30740;&#31350;&#26397;&#30528;&#26356;&#36890;&#29992;&#30340;&#26041;&#27861;&#36808;&#36827;&#65292;&#24182;&#25552;&#20986;&#20102;YonedaVAE&#65292;&#36825;&#26159;&#21463;&#33539;&#30068;&#35770;&#21551;&#21457;&#30340;&#19968;&#31181;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13825v1 Announce Type: cross  Abstract: Simulating ultra-high-granularity detector responses in Particle Physics represents a critical yet computationally demanding task. This thesis aims to overcome this challenge for the Pixel Vertex Detector (PXD) at the Belle II experiment, which features over 7.5M pixel channels-the highest spatial resolution detector simulation dataset ever analysed with generative models. This thesis starts off by a comprehensive and taxonomic review on generative models for simulating detector signatures. Then, it presents the Intra-Event Aware Generative Adversarial Network (IEA-GAN), a new geometry-aware generative model that introduces a relational attentive reasoning and Self-Supervised Learning to approximate an "event" in the detector. This study underscores the importance of intra-event correlation for downstream physics analyses. Building upon this, the work drifts towards a more generic approach and presents YonedaVAE, a Category Theory-insp
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#33021;&#22815;&#26816;&#27979;Arxiv&#25237;&#31295;&#20013;AI&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29289;&#29702;&#12289;&#25968;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#31456;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;Originality.ai&#36827;&#34892;&#20998;&#26512;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98%&#12290;</title><link>https://arxiv.org/abs/2403.13812</link><description>&lt;p&gt;
AI&#29983;&#25104;&#25991;&#26412;&#22312;&#23398;&#26415;&#30740;&#31350;&#20013;&#30340;&#23450;&#37327;&#20998;&#26512;&#65306;&#21033;&#29992;AI&#26816;&#27979;&#24037;&#20855;&#30740;&#31350;Arxiv&#25237;&#31295;&#20013;&#30340;AI&#23384;&#22312;&#24615;
&lt;/p&gt;
&lt;p&gt;
Quantitative Analysis of AI-Generated Texts in Academic Research: A Study of AI Presence in Arxiv Submissions using AI Detection Tool
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13812
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#33021;&#22815;&#26816;&#27979;Arxiv&#25237;&#31295;&#20013;AI&#25104;&#20998;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29289;&#29702;&#12289;&#25968;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#31456;&#21019;&#24314;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;Originality.ai&#36827;&#34892;&#20998;&#26512;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#20154;&#23545;ChatGPT&#24863;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#24050;&#25104;&#20026;&#19968;&#20010;&#31361;&#20986;&#30340;AIGC&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#21508;&#31181;&#24773;&#22659;&#19979;&#25552;&#20379;&#39640;&#36136;&#37327;&#30340;&#21709;&#24212;&#65292;&#22914;&#36719;&#20214;&#24320;&#21457;&#21644;&#32500;&#25252;&#12290;ChatGPT&#30340;&#35823;&#29992;&#21487;&#33021;&#24341;&#36215;&#37325;&#22823;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#20844;&#20849;&#23433;&#20840;&#21644;&#25945;&#32946;&#39046;&#22495;&#65292;&#23613;&#31649;&#20854;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#22823;&#22810;&#36873;&#25321;&#22312;Arxiv&#19978;&#21457;&#34920;&#20182;&#20204;&#30340;&#20316;&#21697;&#12290;&#26410;&#26469;&#24037;&#20316;&#30340;&#26377;&#25928;&#24615;&#21644;&#29420;&#21019;&#24615;&#21462;&#20915;&#20110;&#22312;&#36825;&#20123;&#36129;&#29486;&#20013;&#26816;&#27979;&#21040;AI&#32452;&#20214;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#65292;&#26412;&#30740;&#31350;&#23558;&#20998;&#26512;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#26597;&#30475;&#23398;&#26415;&#26426;&#26500;&#29992;&#20110;&#22312;Arxiv&#19978;&#21457;&#24067;&#30340;&#21051;&#24847;&#21046;&#36896;&#30340;&#20869;&#23481;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#39033;&#30740;&#31350;&#65292;&#20351;&#29992;&#20102;&#29289;&#29702;&#12289;&#25968;&#23398;&#21644;&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#31456;&#21019;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#26032;&#24314;&#31435;&#30340;&#25968;&#25454;&#38598;&#65292;&#25509;&#19979;&#26469;&#30340;&#27493;&#39588;&#26159;&#23558;originality.ai&#25237;&#20837;&#20351;&#29992;&#12290;&#32479;&#35745;&#20998;&#26512;&#26174;&#31034;&#65292;Originality.ai&#38750;&#24120;&#31934;&#20934;&#65292;&#20934;&#30830;&#29575;&#36798;&#21040;98%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13812v1 Announce Type: cross  Abstract: Many people are interested in ChatGPT since it has become a prominent AIGC model that provides high-quality responses in various contexts, such as software development and maintenance. Misuse of ChatGPT might cause significant issues, particularly in public safety and education, despite its immense potential. The majority of researchers choose to publish their work on Arxiv. The effectiveness and originality of future work depend on the ability to detect AI components in such contributions. To address this need, this study will analyze a method that can see purposely manufactured content that academic organizations use to post on Arxiv. For this study, a dataset was created using physics, mathematics, and computer science articles. Using the newly built dataset, the following step is to put originality.ai through its paces. The statistical analysis shows that Originality.ai is very accurate, with a rate of 98%.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20803;&#21551;&#21457;&#24335;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#30899;&#32420;&#32500;&#22686;&#24378;&#32858;&#21512;&#29289;&#23545;&#28151;&#20957;&#22303;&#24378;&#24230;&#30340;&#26463;&#32538;&#25928;&#24212;&#12290;</title><link>https://arxiv.org/abs/2403.13809</link><description>&lt;p&gt;
&#21033;&#29992;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#39044;&#27979;&#30899;&#32420;&#32500;&#22686;&#24378;&#32858;&#21512;&#29289;&#23545;&#28151;&#20957;&#22303;&#24378;&#24230;&#30340;&#26463;&#32538;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Predicting Confinement Effect of Carbon Fiber Reinforced Polymers on Strength of Concrete using Metaheuristics-based Artificial Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13809
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20803;&#21551;&#21457;&#24335;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25104;&#21151;&#39044;&#27979;&#20102;&#30899;&#32420;&#32500;&#22686;&#24378;&#32858;&#21512;&#29289;&#23545;&#28151;&#20957;&#22303;&#24378;&#24230;&#30340;&#26463;&#32538;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#39044;&#27979;&#30899;&#32420;&#32500;&#22686;&#24378;&#32858;&#21512;&#29289;&#65288;CFRPs&#65289;&#23545;&#28151;&#20957;&#22303;&#22278;&#26609;&#24378;&#24230;&#30340;&#26463;&#32538;&#25928;&#24212;&#12290;&#20174;&#20808;&#21069;&#21457;&#34920;&#30340;&#30740;&#31350;&#20013;&#24320;&#21457;&#20102;&#19968;&#20010;&#21253;&#21547;708&#20010;CFRP&#32422;&#26463;&#28151;&#20957;&#22303;&#22278;&#26609;&#30340;&#35814;&#32454;&#25968;&#25454;&#24211;&#65292;&#20854;&#20013;&#21253;&#21547;8&#20010;&#21442;&#25968;&#30340;&#20449;&#24687;&#65292;&#21253;&#25324;&#20960;&#20309;&#21442;&#25968;&#65288;&#22914;&#22278;&#26609;&#30452;&#24452;&#65288;d&#65289;&#21644;&#39640;&#24230;&#65288;h&#65289;&#65289;&#12289;&#28151;&#20957;&#22303;&#26410;&#26463;&#32538;&#25239;&#21387;&#24378;&#24230;&#65288;fco'&#65289;&#12289;&#21402;&#24230;&#65288;nt&#65289;&#12289;CFRP&#30340;&#24377;&#24615;&#27169;&#37327;&#65288;Ef&#65289;&#12289;&#26410;&#26463;&#32538;&#28151;&#20957;&#22303;&#24212;&#21464;&#12289;&#21463;&#32422;&#26463;&#28151;&#20957;&#22303;&#24212;&#21464;&#20197;&#21450;&#21463;&#26463;&#32538;&#28151;&#20957;&#22303;&#30340;&#26497;&#38480;&#25239;&#21387;&#24378;&#24230;fcc'&#12290;&#23454;&#26045;&#20102;&#19977;&#31181;&#22522;&#20110;&#20803;&#21551;&#21457;&#24335;&#30340;&#27169;&#22411;&#65292;&#21253;&#25324;&#31890;&#23376;&#32676;&#20248;&#21270;&#65288;PSO&#65289;&#12289;&#28784;&#29436;&#20248;&#21270;&#22120;&#65288;GWO&#65289;&#21644;&#34649;&#34656;&#31639;&#27861;&#65288;BA&#65289;&#12290;&#36825;&#20123;&#31639;&#27861;&#20351;&#29992;&#22343;&#26041;&#35823;&#24046;&#20316;&#20026;&#30446;&#26631;&#20989;&#25968;&#23545;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#23558;&#39044;&#27979;&#32467;&#26524;&#19982;&#23454;&#39564;&#30740;&#31350;&#36827;&#34892;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13809v1 Announce Type: cross  Abstract: This article deals with the study of predicting the confinement effect of carbon fiber reinforced polymers (CFRPs) on concrete cylinder strength using metaheuristics-based artificial neural networks. A detailed database of 708 CFRP confined concrete cylinders is developed from previously published research with information on 8 parameters including geometrical parameters like the diameter (d) and height (h) of a cylinder, unconfined compressive strength of concrete (fco'), thickness (nt), the elastic modulus of CFRP (Ef), unconfined concrete strain confined concrete strain and the ultimate compressive strength of confined concrete fcc'. Three metaheuristic models are implemented including particle swarm optimization (PSO), grey wolf optimizer (GWO), and bat algorithm (BA). These algorithms are trained on the data using an objective function of mean square error and their predicted results are validated against the experimental studies 
&lt;/p&gt;</description></item><item><title>LlamaFactory&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.13372</link><description>&lt;p&gt;
LlamaFactory&#65306;100&#22810;&#31181;&#35821;&#35328;&#27169;&#22411;&#30340;&#32479;&#19968;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13372
&lt;/p&gt;
&lt;p&gt;
LlamaFactory&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25972;&#21512;&#20102;&#19968;&#31995;&#21015;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#22312;&#19981;&#38656;&#35201;&#32534;&#30721;&#30340;&#24773;&#20917;&#19979;&#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#25928;&#30340;&#24494;&#35843;&#23545;&#20110;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#21516;&#27169;&#22411;&#19978;&#23454;&#29616;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#38750;&#24179;&#20961;&#30340;&#21162;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LlamaFactory&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#38598;&#25104;&#20102;&#19968;&#22871;&#21069;&#27839;&#30340;&#39640;&#25928;&#35757;&#32451;&#26041;&#27861;&#12290;&#23427;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#20869;&#32622;&#30340;Web UI LlamaBoard &#28789;&#27963;&#23450;&#21046;100&#22810;&#31181;LLMs&#30340;&#24494;&#35843;&#65292;&#26080;&#38656;&#32534;&#30721;&#12290;&#25105;&#20204;&#22312;&#35821;&#35328;&#24314;&#27169;&#21644;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#19978;&#32463;&#39564;&#24615;&#22320;&#39564;&#35777;&#20102;&#25105;&#20204;&#26694;&#26550;&#30340;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;&#24050;&#21457;&#24067;&#22312; https://github.com/hiyouga/LLaMA-Factory&#65292;&#24182;&#24050;&#33719;&#24471;&#36229;&#36807;13,000&#39063;&#26143;&#21644;1,600&#20010;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13372v1 Announce Type: new  Abstract: Efficient fine-tuning is vital for adapting large language models (LLMs) to downstream tasks. However, it requires non-trivial efforts to implement these methods on different models. We present LlamaFactory, a unified framework that integrates a suite of cutting-edge efficient training methods. It allows users to flexibly customize the fine-tuning of 100+ LLMs without the need for coding through the built-in web UI LlamaBoard. We empirically validate the efficiency and effectiveness of our framework on language modeling and text generation tasks. It has been released at https://github.com/hiyouga/LLaMA-Factory and already received over 13,000 stars and 1,600 forks.
&lt;/p&gt;</description></item><item><title>&#21512;&#24182;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21019;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#35299;&#20915;AI&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.13257</link><description>&lt;p&gt;
Arcee&#30340;MergeKit&#65306;&#29992;&#20110;&#21512;&#24182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#21253;
&lt;/p&gt;
&lt;p&gt;
Arcee's MergeKit: A Toolkit for Merging Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13257
&lt;/p&gt;
&lt;p&gt;
&#21512;&#24182;&#19981;&#21516;&#35821;&#35328;&#27169;&#22411;&#30340;&#21442;&#25968;&#65292;&#26080;&#38656;&#39069;&#22806;&#35757;&#32451;&#21363;&#21487;&#21019;&#24314;&#22810;&#20219;&#21153;&#27169;&#22411;&#65292;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#65292;&#35299;&#20915;AI&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#28304;&#35821;&#35328;&#27169;&#22411;&#39046;&#22495;&#30340;&#24555;&#36895;&#25193;&#24352;&#20026;&#36890;&#36807;&#21512;&#24182;&#20854;&#21442;&#25968;&#26469;&#32467;&#21512;&#36825;&#20123;&#27169;&#22411;&#26816;&#26597;&#28857;&#30340;&#33021;&#21147;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#36801;&#31227;&#23398;&#20064;&#30340;&#36827;&#27493;&#23548;&#33268;&#20102;&#22823;&#37327;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#30340;&#24320;&#21457;&#65292;&#36825;&#20123;&#27169;&#22411;&#36890;&#24120;&#19987;&#38376;&#38024;&#23545;&#20010;&#21035;&#20219;&#21153;&#36827;&#34892;&#19987;&#38376;&#21270;&#65292;&#26080;&#27861;&#21033;&#29992;&#24444;&#27492;&#30340;&#20248;&#21183;&#12290;&#27169;&#22411;&#21512;&#24182;&#20419;&#36827;&#20102;&#22810;&#20219;&#21153;&#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#65292;&#20026;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#21644;&#22810;&#21151;&#33021;&#24615;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;&#36890;&#36807;&#20445;&#30041;&#21407;&#22987;&#27169;&#22411;&#30340;&#22266;&#26377;&#33021;&#21147;&#65292;&#27169;&#22411;&#21512;&#24182;&#35299;&#20915;&#20102;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#22797;&#26434;&#25361;&#25112;&#65292;&#21253;&#25324;&#28798;&#38590;&#24615;&#36951;&#24536;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#25903;&#25345;&#36825;&#19968;&#19981;&#26029;&#25193;&#22823;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MergeKit&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#12289;&#24320;&#28304;&#30340;&#24211;&#65292;&#26088;&#22312;&#20419;&#36827;&#27169;&#22411;&#21512;&#24182;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13257v1 Announce Type: new  Abstract: The rapid expansion of the open-source language model landscape presents an opportunity to merge the competencies of these model checkpoints by combining their parameters. Advances in transfer learning, the process of fine-tuning pre-trained models for specific tasks, has resulted in the development of vast amounts of task-specific models, typically specialized in individual tasks and unable to utilize each other's strengths. Model merging facilitates the creation of multitask models without the need for additional training, offering a promising avenue for enhancing model performance and versatility. By preserving the intrinsic capabilities of the original models, model merging addresses complex challenges in AI - including the difficulties of catastrophic forgetting and multi-task learning. To support this expanding area of research, we introduce MergeKit, a comprehensive, open-source library designed to facilitate the application of mo
&lt;/p&gt;</description></item><item><title>FlowerFormer&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#21452;&#21521;&#24322;&#27493;&#28040;&#24687;&#20256;&#36882;&#21644;&#22522;&#20110;&#27969;&#31243;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.12821</link><description>&lt;p&gt;
FlowerFormer: &#20351;&#29992;&#22522;&#20110;&#27969;&#24863;&#30693;&#30340;&#22270;&#21464;&#25442;&#22120;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12821
&lt;/p&gt;
&lt;p&gt;
FlowerFormer&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#21452;&#21521;&#24322;&#27493;&#28040;&#24687;&#20256;&#36882;&#21644;&#22522;&#20110;&#27969;&#31243;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#21487;&#20197;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#30340;&#34920;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#23450;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#30340;&#25104;&#21151;&#19982;&#20854;&#22788;&#29702;&#30340;&#25968;&#25454;&#38598;&#21644;&#20219;&#21153;&#23494;&#20999;&#30456;&#20851;&#65307;&#27809;&#26377;&#19968;&#31181;&#36866;&#21512;&#25152;&#26377;&#24773;&#20917;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#20154;&#20204;&#20184;&#20986;&#20102;&#22823;&#37327;&#21162;&#21147;&#65292;&#20197;&#24555;&#36895;&#20934;&#30830;&#22320;&#20272;&#35745;&#31070;&#32463;&#32467;&#26500;&#22312;&#29305;&#23450;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#32780;&#26080;&#38656;&#36827;&#34892;&#23436;&#25972;&#30340;&#35757;&#32451;&#25110;&#35780;&#20272;&#12290;&#31070;&#32463;&#32467;&#26500;&#32534;&#30721;&#22312;&#20272;&#35745;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#32780;&#23558;&#26550;&#26500;&#35270;&#20026;&#22270;&#30340;&#22522;&#20110;&#22270;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#22686;&#24378;&#31070;&#32463;&#32467;&#26500;&#30340;&#34920;&#24449;&#23398;&#20064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;FlowerFormer&#65292;&#19968;&#31181;&#24378;&#22823;&#30340;&#22270;&#21464;&#25442;&#22120;&#65292;&#23427;&#34701;&#20837;&#20102;&#31070;&#32463;&#32467;&#26500;&#20869;&#30340;&#20449;&#24687;&#27969;&#12290; FlowerFormer&#30001;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#32452;&#25104;&#65306;&#65288;a&#65289;&#21463;&#27969;&#31243;&#21551;&#21457;&#30340;&#21452;&#21521;&#24322;&#27493;&#28040;&#24687;&#20256;&#36882;&#65307;&#65288;b&#65289;&#24314;&#31435;&#22312;&#22522;&#20110;&#27969;&#31243;&#30340;&#25513;&#30721;&#19978;&#30340;&#20840;&#23616;&#20851;&#27880;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;FlowerFormer&#20248;&#20110;&#29616;&#26377;&#31070;&#32463;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12821v1 Announce Type: cross  Abstract: The success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution. Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets. Neural architecture encoding has played a crucial role in the estimation, and graphbased methods, which treat an architecture as a graph, have shown prominent performance. For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture. FlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking. Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20998;&#35299;&#12289;&#35299;&#37322;&#21644;&#20943;&#36731;&#65288;DIM&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;, &#29992;&#20110;&#22312;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#21457;&#29616;&#21644;&#20943;&#36731;&#22810;&#20010;&#26377;&#20559;&#23376;&#32676;&#20307;&#65292;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.12777</link><description>&lt;p&gt;
&#22312;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#21457;&#29616;&#21644;&#20943;&#36731;&#22810;&#20010;&#26377;&#20559;&#23376;&#32676;&#20307;
&lt;/p&gt;
&lt;p&gt;
Discover and Mitigate Multiple Biased Subgroups in Image Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12777
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#20998;&#35299;&#12289;&#35299;&#37322;&#21644;&#20943;&#36731;&#65288;DIM&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;, &#29992;&#20110;&#22312;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#21457;&#29616;&#21644;&#20943;&#36731;&#22810;&#20010;&#26377;&#20559;&#23376;&#32676;&#20307;&#65292;&#22686;&#24378;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20998;&#24067;&#25968;&#25454;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#24120;&#24120;&#22312;&#35757;&#32451;&#25968;&#25454;&#20013;&#26410;&#20805;&#20998;&#20195;&#34920;&#30340;&#26377;&#20559;&#23376;&#32676;&#20307;&#19978;&#22833;&#36133;&#65292;&#24433;&#21709;&#27169;&#22411;&#23545;&#21487;&#38752;&#24212;&#29992;&#30340;&#25239;&#24178;&#25200;&#24615;&#12290;&#36825;&#20123;&#23376;&#32676;&#20307;&#36890;&#24120;&#30001;&#20110;&#32570;&#20047;&#23376;&#32676;&#20307;&#26631;&#31614;&#32780;&#26410;&#30693;&#12290;&#21457;&#29616;&#26377;&#20559;&#23376;&#32676;&#20307;&#26159;&#29702;&#35299;&#27169;&#22411;&#22833;&#36133;&#27169;&#24335;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#20998;&#35299;&#12289;&#35299;&#37322;&#21644;&#20943;&#36731;&#65288;DIM&#65289;&#8221;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22312;&#22270;&#20687;&#20998;&#31867;&#22120;&#20013;&#21457;&#29616;&#22810;&#20010;&#26377;&#20559;&#23376;&#32676;&#20307;&#36825;&#19968;&#26356;&#20855;&#25361;&#25112;&#24615;&#20294;&#20063;&#26356;&#23454;&#38469;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22270;&#20687;&#29305;&#24449;&#20998;&#35299;&#20026;&#20195;&#34920;&#22810;&#20010;&#23376;&#32676;&#20307;&#30340;&#22810;&#20010;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12777v1 Announce Type: cross  Abstract: Machine learning models can perform well on in-distribution data but often fail on biased subgroups that are underrepresented in the training data, hindering the robustness of models for reliable applications. Such subgroups are typically unknown due to the absence of subgroup labels. Discovering biased subgroups is the key to understanding models' failure modes and further improving models' robustness. Most previous works of subgroup discovery make an implicit assumption that models only underperform on a single biased subgroup, which does not hold on in-the-wild data where multiple biased subgroups exist.   In this work, we propose Decomposition, Interpretation, and Mitigation (DIM), a novel method to address a more challenging but also more practical problem of discovering multiple biased subgroups in image classifiers. Our approach decomposes the image features into multiple components that represent multiple subgroups. This decomp
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#34701;&#21512;&#25216;&#26415;&#25972;&#21512;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;&#21644;&#37319;&#29992;LSTM&#26550;&#26500;&#36827;&#34892;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;</title><link>https://arxiv.org/abs/2403.11879</link><description>&lt;p&gt;
&#21333;&#27169;&#24577;&#22810;&#20219;&#21153;&#34701;&#21512;&#29992;&#20110;&#24773;&#24863;&#27169;&#20223;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Unimodal Multi-Task Fusion for Emotional Mimicry Prediciton
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11879
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#34701;&#21512;&#25216;&#26415;&#25972;&#21512;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;&#21644;&#37319;&#29992;LSTM&#26550;&#26500;&#36827;&#34892;&#26102;&#38388;&#20998;&#26512;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#39044;&#27979;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#31532;&#20845;&#23626;&#25143;&#22806;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;&#30740;&#35752;&#20250;&#21644;&#31454;&#36187;&#20013;&#36827;&#34892;&#24773;&#24863;&#27169;&#20223;&#24378;&#24230;&#65288;EMI&#65289;&#20272;&#35745;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;Wav2Vec 2.0&#26694;&#26550;&#65292;&#22312;&#19968;&#20010;&#20840;&#38754;&#30340;&#25773;&#23458;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39044;&#35757;&#32451;&#65292;&#20197;&#25552;&#21462;&#28085;&#30422;&#35821;&#35328;&#21644;&#35821;&#22806;&#20803;&#32032;&#30340;&#24191;&#27867;&#38899;&#39057;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#34701;&#21512;&#25216;&#26415;&#22686;&#24378;&#20102;&#29305;&#24449;&#34920;&#31034;&#65292;&#35813;&#25216;&#26415;&#23558;&#20010;&#20307;&#29305;&#24449;&#19982;&#20840;&#23616;&#22343;&#20540;&#21521;&#37327;&#30456;&#32467;&#21512;&#65292;&#24341;&#20837;&#20840;&#23616;&#32972;&#26223;&#20449;&#24687;&#21040;&#25105;&#20204;&#30340;&#20998;&#26512;&#20013;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20174;Wav2Vec 2.0&#27169;&#22411;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#39044;&#35757;&#32451;&#30340;valence-arousal-dominance&#65288;VAD&#65289;&#27169;&#22359;&#12290;&#25105;&#20204;&#30340;&#34701;&#21512;&#37319;&#29992;&#20102;&#19968;&#31181;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#23545;&#38899;&#39057;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#30340;&#26102;&#38388;&#20998;&#26512;&#12290;&#20165;&#21033;&#29992;&#25152;&#25552;&#20379;&#30340;&#38899;&#39057;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24050;&#24314;&#31435;&#30340;&#22522;&#20934;&#32447;&#19978;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11879v1 Announce Type: cross  Abstract: In this study, we propose a methodology for the Emotional Mimicry Intensity (EMI) Estimation task within the context of the 6th Workshop and Competition on Affective Behavior Analysis in-the-wild. Our approach leverages the Wav2Vec 2.0 framework, pre-trained on a comprehensive podcast dataset, to extract a broad range of audio features encompassing both linguistic and paralinguistic elements. We enhance feature representation through a fusion technique that integrates individual features with a global mean vector, introducing global contextual insights into our analysis. Additionally, we incorporate a pre-trained valence- arousal-dominance (VAD) module from the Wav2Vec 2.0 model. Our fusion employs a Long Short-Term Memory (LSTM) architecture for efficient temporal analysis of audio data. Utilizing only the provided audio data, our approach demonstrates significant improvements over the established baseline.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;MLLMs&#30340;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25193;&#23637;&#35789;&#27719;&#12289;&#21452;&#35821;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.10882</link><description>&lt;p&gt;
&#20248;&#21270;&#22810;&#35821;&#35328;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#35328;&#22686;&#24378;&#65306;&#20197;&#38889;&#35821;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimizing Language Augmentation for Multilingual Large Language Models: A Case Study on Korean
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.10882
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;MLLMs&#30340;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#30340;&#24615;&#33021;&#65292;&#21253;&#25324;&#25193;&#23637;&#35789;&#27719;&#12289;&#21452;&#35821;&#25968;&#25454;&#39044;&#35757;&#32451;&#21644;&#25351;&#23548;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20351;&#29992;&#39044;&#35757;&#32451;&#26469;&#39044;&#27979;&#19979;&#19968;&#20010;&#21333;&#35789;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25193;&#23637;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#35768;&#22810;&#22823;&#22411;&#31185;&#25216;&#20844;&#21496;&#21644;&#30740;&#31350;&#26426;&#26500;&#24050;&#32463;&#24320;&#21457;&#20102;&#22810;&#35821;&#35328;LLMs&#65288;MLLMs&#65289;&#20197;&#28385;&#36275;&#24403;&#21069;&#38656;&#27714;&#65292;&#20294;&#24573;&#35270;&#20102;&#36164;&#28304;&#36739;&#23569;&#30340;&#35821;&#35328;&#65288;LRLs&#65289;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19977;&#31181;&#31574;&#30053;&#26469;&#22686;&#24378;&#22522;&#20110;&#20844;&#24320;&#21487;&#29992;MLLMs&#30340;LRLs&#30340;&#24615;&#33021;&#12290;&#39318;&#20808;&#65292;&#25193;&#23637;LRLs&#30340;MLLM&#35789;&#27719;&#20197;&#22686;&#24378;&#34920;&#36798;&#24615;&#12290;&#20854;&#27425;&#65292;&#20351;&#29992;&#21452;&#35821;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#20197;&#23545;&#40784;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#12290;&#31532;&#19977;&#65292;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#23567;&#35268;&#27169;&#25351;&#23548;&#25968;&#25454;&#38598;&#65292;&#24182;&#36827;&#34892;&#25351;&#23548;&#24494;&#35843;&#20197;&#22686;&#24378;LRL&#12290;&#23454;&#39564;&#37319;&#29992;&#20102;Llama2&#27169;&#22411;&#65292;&#20197;&#38889;&#35821;&#20316;&#20026;LRL&#65292;&#24182;&#22312;&#20843;&#39033;&#20219;&#21153;&#20013;&#23545;&#20854;&#19982;&#20854;&#20182;&#24050;&#24320;&#21457;&#30340;LLMs&#36827;&#34892;&#20102;&#23450;&#37327;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#20154;&#31867;&#35780;&#20272;&#36827;&#34892;&#20102;&#23450;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.10882v1 Announce Type: cross  Abstract: Large language models (LLMs) use pretraining to predict the subsequent word; however, their expansion requires significant computing resources. Numerous big tech companies and research institutes have developed multilingual LLMs (MLLMs) to meet current demands, overlooking less-resourced languages (LRLs). This study proposed three strategies to enhance the performance of LRLs based on the publicly available MLLMs. First, the MLLM vocabularies of LRLs were expanded to enhance expressiveness. Second, bilingual data were used for pretraining to align the high- and less-resourced languages. Third, a high-quality small-scale instruction dataset was constructed and instruction-tuning was performed to augment the LRL. The experiments employed the Llama2 model and Korean was used as the LRL, which was quantitatively evaluated against other developed LLMs across eight tasks. Furthermore, a qualitative assessment was performed based on human eva
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#12289;&#24182;&#21457;&#39044;&#27979;&#21644;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#31561;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.06906</link><description>&lt;p&gt;
&#25104;&#26412;&#25935;&#24863;&#23398;&#20064;&#22312;&#32771;&#34385;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#25512;&#36831;&#22810;&#20301;&#19987;&#23478;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Cost-Sensitive Learning to Defer to Multiple Experts with Workload Constraints
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06906
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#65292;&#26088;&#22312;&#35299;&#20915;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#12289;&#24182;&#21457;&#39044;&#27979;&#21644;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#31561;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#25512;&#36831;&#65288;L2D&#65289;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#22914;&#20309;&#22312;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#31995;&#32479;&#20013;&#23558;&#20915;&#31574;&#25512;&#36831;&#32473;&#20154;&#31867;&#65292;&#20174;&#32780;&#22312;&#20154;&#31867;&#26356;&#26377;&#21487;&#33021;&#27491;&#30830;&#26102;&#25512;&#36831;&#20915;&#31574;&#12290;&#29616;&#26377;L2D&#30740;&#31350;&#24573;&#35270;&#20102;&#38459;&#30861;&#20854;&#23454;&#38469;&#37319;&#29992;&#30340;&#30495;&#23454;&#31995;&#32479;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#21363;&#65306;&#24573;&#35270;&#25104;&#26412;&#25935;&#24863;&#22330;&#26223;&#65292;&#20854;&#20013;&#31532;1&#31867;&#21644;&#31532;2&#31867;&#38169;&#35823;&#30340;&#25104;&#26412;&#19981;&#21516;&#65307;&#35201;&#27714;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#23454;&#20363;&#30340;&#24182;&#21457;&#20154;&#31867;&#39044;&#27979;&#65307;&#19981;&#22788;&#29702;&#20154;&#31867;&#24037;&#20316;&#33021;&#21147;&#32422;&#26463;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#25104;&#26412;&#21644;&#24037;&#20316;&#37327;&#32422;&#26463;&#19979;&#30340;&#25512;&#36831;&#26694;&#26550;&#65288;DeCCaF&#65289;&#12290;DeCCaF&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;L2D&#26041;&#27861;&#65292;&#37319;&#29992;&#30417;&#30563;&#23398;&#20064;&#26469;&#24314;&#27169;&#20154;&#31867;&#38169;&#35823;&#30340;&#27010;&#29575;&#65292;&#20943;&#23569;&#25968;&#25454;&#35201;&#27714;&#30340;&#38480;&#21046;&#65292;&#24182;&#20351;&#29992;&#32422;&#26463;&#32534;&#31243;&#26469;&#20840;&#23616;&#26368;&#23567;&#21270;&#38169;&#35823;&#25104;&#26412;&#65292;&#21516;&#26102;&#32771;&#34385;&#24037;&#20316;&#37327;&#38480;&#21046;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#31995;&#21015;&#20013;&#27979;&#35797;&#20102;DeCCaF
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06906v1 Announce Type: cross  Abstract: Learning to defer (L2D) aims to improve human-AI collaboration systems by learning how to defer decisions to humans when they are more likely to be correct than an ML classifier. Existing research in L2D overlooks key aspects of real-world systems that impede its practical adoption, namely: i) neglecting cost-sensitive scenarios, where type 1 and type 2 errors have different costs; ii) requiring concurrent human predictions for every instance of the training dataset and iii) not dealing with human work capacity constraints. To address these issues, we propose the deferral under cost and capacity constraints framework (DeCCaF). DeCCaF is a novel L2D approach, employing supervised learning to model the probability of human error under less restrictive data requirements (only one expert prediction per instance) and using constraint programming to globally minimize the error cost subject to workload limitations. We test DeCCaF in a series 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#30340;&#20840;&#30693;&#27169;&#25311;&#27604;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#23481;&#26131;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#65292;&#23613;&#31649;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#25509;&#36817;&#23454;&#38469;&#24773;&#20917;&#12290;</title><link>https://arxiv.org/abs/2403.05020</link><description>&lt;p&gt;
&#27169;&#25311;&#31038;&#20132;&#20114;&#21160;&#25104;&#21151;&#24615;&#30340;&#35823;&#23548;&#24615;&#65306;&#20197;LLMs&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Is this the real life? Is this just fantasy? The Misleading Success of Simulating Social Interactions With LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05020
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;LLMs&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#30340;&#20840;&#30693;&#27169;&#25311;&#27604;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#23481;&#26131;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#65292;&#23613;&#31649;&#38750;&#20840;&#30693;&#27169;&#25311;&#26356;&#25509;&#36817;&#23454;&#38469;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#36827;&#23637;&#20351;&#24471;&#31038;&#20132;&#27169;&#25311;&#26356;&#21152;&#20016;&#23500;&#65292;&#33021;&#22815;&#20351;&#29992;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#20154;&#30740;&#31350;&#21508;&#31181;&#31038;&#20132;&#29616;&#35937;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#24037;&#20316;&#22312;&#36825;&#20123;&#27169;&#25311;&#20013;&#37319;&#29992;&#20102;&#19968;&#31181;&#20840;&#30693;&#30340;&#36879;&#35270;&#65288;&#20363;&#22914;&#65292;&#21333;&#20010;LLM&#29983;&#25104;&#25152;&#26377;&#20132;&#35848;&#32773;&#65289;&#65292;&#36825;&#19982;&#20154;&#31867;&#20855;&#26377;&#30340;&#38750;&#20840;&#30693;&#12289;&#20449;&#24687;&#19981;&#23545;&#31216;&#30340;&#20114;&#21160;&#26681;&#26412;&#19981;&#31526;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#20123;&#24046;&#24322;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#35780;&#20272;&#26694;&#26550;&#65292;&#22312;&#21508;&#31181;&#35774;&#23450;&#65288;&#20840;&#30693;&#12289;&#38750;&#20840;&#30693;&#65289;&#20013;&#20351;&#29992;LLMs&#27169;&#25311;&#31038;&#20132;&#20114;&#21160;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#20840;&#30693;&#26041;&#24335;&#27169;&#25311;&#30340;&#20132;&#35848;&#32773;&#22312;&#23454;&#29616;&#31038;&#20132;&#30446;&#26631;&#26041;&#38754;&#27604;&#38750;&#20840;&#30693;&#20195;&#29702;&#20154;&#26356;&#25104;&#21151;&#65292;&#23613;&#31649;&#21518;&#32773;&#26356;&#31526;&#21512;&#29616;&#23454;&#35774;&#32622;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#20174;&#20840;&#30693;&#27169;&#25311;&#20013;&#23398;&#20064;&#21487;&#20197;&#25913;&#21892;&#20132;&#20114;&#30340;&#33258;&#28982;&#24615;&#65292;&#20294;&#22312;&#21512;&#20316;&#22330;&#26223;&#20013;&#20960;&#20046;&#19981;&#33021;&#22686;&#24378;&#30446;&#26631;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05020v1 Announce Type: cross  Abstract: Recent advances in large language models (LLM) have enabled richer social simulations, allowing for the study of various social phenomena with LLM-based agents. However, most work has used an omniscient perspective on these simulations (e.g., single LLM to generate all interlocutors), which is fundamentally at odds with the non-omniscient, information asymmetric interactions that humans have. To examine these differences, we develop an evaluation framework to simulate social interactions with LLMs in various settings (omniscient, non-omniscient). Our experiments show that interlocutors simulated omnisciently are much more successful at accomplishing social goals compared to non-omniscient agents, despite the latter being the more realistic setting. Furthermore, we demonstrate that learning from omniscient simulations improves the apparent naturalness of interactions but scarcely enhances goal achievement in cooperative scenarios. Our f
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;</title><link>https://arxiv.org/abs/2403.02302</link><description>&lt;p&gt;
&#36229;&#36234;&#19987;&#19994;&#21270;&#65306;&#35780;&#20272;MLLMs&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Beyond Specialization: Assessing the Capabilities of MLLMs in Age and Gender Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#20013;&#30340;&#33021;&#21147;&#65292;&#23545;&#19981;&#21516;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#22312;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#21464;&#24471;&#24322;&#24120;&#27969;&#34892;&#12290;&#20687;ChatGPT-4V&#21644;Gemini&#36825;&#26679;&#21151;&#33021;&#24378;&#22823;&#30340;&#21830;&#29992;&#27169;&#22411;&#65292;&#20197;&#21450;&#20687;LLaVA&#36825;&#26679;&#30340;&#24320;&#28304;&#27169;&#22411;&#65292;&#26412;&#36136;&#19978;&#37117;&#26159;&#36890;&#29992;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#35299;&#20915;&#21508;&#31181;&#21508;&#26679;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#20219;&#21153;&#12290;&#36825;&#20123;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#22914;&#27492;&#24378;&#22823;&#30340;&#36890;&#29992;&#30693;&#35782;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#33267;&#20110;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#22788;&#29702;&#29978;&#33267;&#26410;&#32463;&#19987;&#38376;&#35757;&#32451;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#23558;&#36804;&#20170;&#20026;&#27490;&#26368;&#24378;&#22823;&#30340;MLLMs&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;ShareGPT4V&#12289;ChatGPT&#12289;LLaVA-Next &#36827;&#34892;&#20102;&#19987;&#38376;&#20219;&#21153;&#30340;&#24180;&#40836;&#21644;&#24615;&#21035;&#20272;&#35745;&#65292;&#19982;&#25105;&#20204;&#30340;&#26368;&#26032;&#19987;&#19994;&#21270;&#27169;&#22411;MiVOLO&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#26356;&#26032;&#20102;MiVOLO&#65292;&#24182;&#22312;&#26412;&#25991;&#20013;&#25552;&#20379;&#20102;&#35814;&#32454;&#20449;&#24687;&#21644;&#26032;&#30340;&#25351;&#26631;&#12290;&#36825;&#31181;&#27604;&#36739;&#20135;&#29983;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#32467;&#26524;&#21644;&#20851;&#20110;&#21442;&#19982;&#27169;&#22411;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#30340;&#35265;&#35299;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#21508;&#31181;&#24494;&#35843;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02302v1 Announce Type: cross  Abstract: Multimodal Large Language Models (MLLMs) have recently gained immense popularity. Powerful commercial models like ChatGPT-4V and Gemini, as well as open-source ones such as LLaVA, are essentially general-purpose models and are applied to solve a wide variety of tasks, including those in computer vision. These neural networks possess such strong general knowledge and reasoning abilities that they have proven capable of working even on tasks for which they were not specifically trained. We compared the capabilities of the most powerful MLLMs to date: ShareGPT4V, ChatGPT, LLaVA-Next in a specialized task of age and gender estimation with our state-of-the-art specialized model, MiVOLO. We also updated MiVOLO and provide details and new metrics in this article. This comparison has yielded some interesting results and insights about the strengths and weaknesses of the participating models. Furthermore, we attempted various ways to fine-tune 
&lt;/p&gt;</description></item><item><title>NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.00862</link><description>&lt;p&gt;
NewsBench&#65306;&#31995;&#32479;&#24615;&#35780;&#20272;LLM&#22312;&#20013;&#22269;&#26032;&#38395;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
NewsBench: Systematic Evaluation of LLMs for Writing Proficiency and Safety Adherence in Chinese Journalistic Editorial Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00862
&lt;/p&gt;
&lt;p&gt;
NewsBench&#26159;&#19968;&#20010;&#35780;&#20272;LLMs&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#33021;&#21147;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#25581;&#31034;&#20102;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;LLMs&#30456;&#23545;&#19981;&#36275;&#30340;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;NewsBench&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20934;&#26694;&#26550;&#65292;&#26088;&#22312;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20013;&#22269;&#26032;&#38395;&#20889;&#20316;&#27700;&#24179;&#65288;JWP&#65289;&#21644;&#23433;&#20840;&#24615;&#36981;&#20174;&#65288;SA&#65289;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24357;&#34917;&#20102;&#26032;&#38395;&#20262;&#29702;&#19982;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#39118;&#38505;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;NewsBench&#21253;&#25324;5&#20010;&#32534;&#36753;&#24212;&#29992;&#20013;&#30340;1,267&#39033;&#20219;&#21153;&#65292;7&#20010;&#26041;&#38754;&#65288;&#21253;&#25324;&#23433;&#20840;&#24615;&#21644;&#26032;&#38395;&#20889;&#20316;&#65292;&#20197;&#21450;4&#20010;&#35814;&#32454;&#35201;&#38754;&#65289;&#65292;&#28085;&#30422;24&#20010;&#26032;&#38395;&#20027;&#39064;&#39046;&#22495;&#65292;&#37319;&#29992;&#22522;&#20110;&#20004;&#31181;GPT-4&#30340;&#33258;&#21160;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#32463;&#36807;&#20154;&#31867;&#35780;&#20272;&#39564;&#35777;&#12290;&#25105;&#20204;&#23545;11&#20010;LLM&#30340;&#20840;&#38754;&#20998;&#26512;&#31361;&#20986;&#20102;GPT-4&#21644;ERNIE Bot&#20316;&#20026;&#34920;&#29616;&#26368;&#20339;&#65292;&#20294;&#22312;&#21019;&#36896;&#24615;&#20889;&#20316;&#20219;&#21153;&#20013;&#25581;&#31034;&#20102;&#26032;&#38395;&#20262;&#29702;&#36981;&#23432;&#26041;&#38754;&#30340;&#30456;&#23545;&#19981;&#36275;&#12290;&#36825;&#20123;&#21457;&#29616;&#24378;&#35843;&#20102;AI&#29983;&#25104;&#30340;&#26032;&#38395;&#20869;&#23481;&#38656;&#35201;&#25552;&#39640;&#20262;&#29702;&#25351;&#23548;&#65292;&#26631;&#24535;&#30528;&#20197;&#26032;&#38395;&#26631;&#20934;&#21644;&#23433;&#20840;&#24615;&#23545;&#40784;AI&#33021;&#21147;&#36808;&#20986;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00862v1 Announce Type: cross  Abstract: This study presents NewsBench, a novel benchmark framework developed to evaluate the capability of Large Language Models (LLMs) in Chinese Journalistic Writing Proficiency (JWP) and their Safety Adherence (SA), addressing the gap between journalistic ethics and the risks associated with AI utilization. Comprising 1,267 tasks across 5 editorial applications, 7 aspects (including safety and journalistic writing with 4 detailed facets), and spanning 24 news topics domains, NewsBench employs two GPT-4 based automatic evaluation protocols validated by human assessment. Our comprehensive analysis of 11 LLMs highlighted GPT-4 and ERNIE Bot as top performers, yet revealed a relative deficiency in journalistic ethic adherence during creative writing tasks. These findings underscore the need for enhanced ethical guidance in AI-generated journalistic content, marking a step forward in aligning AI capabilities with journalistic standards and safet
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;OSCaR&#65292;&#26088;&#22312;&#35299;&#20915;&#25551;&#36848;&#22797;&#26434;&#35270;&#35273;&#29615;&#22659;&#20013;&#23545;&#35937;&#29366;&#24577;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2402.17128</link><description>&lt;p&gt;
OSCaR:&#23545;&#35937;&#29366;&#24577;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
OSCaR: Object State Captioning and State Change Representation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;OSCaR&#65292;&#26088;&#22312;&#35299;&#20915;&#25551;&#36848;&#22797;&#26434;&#35270;&#35273;&#29615;&#22659;&#20013;&#23545;&#35937;&#29366;&#24577;&#21464;&#21270;&#30340;&#38382;&#39064;&#65292;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17128v3 &#20844;&#21578;&#31867;&#22411;: &#36328; &#38754;&#21521;&#20154;&#31867;&#22312;&#30495;&#23454;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#20132;&#20114;&#35270;&#35282;&#65292;&#26234;&#33021;&#27169;&#22411;&#25512;&#26029;&#21644;&#29702;&#35299;&#23545;&#35937;&#29366;&#24577;&#30340;&#21464;&#21270;&#33021;&#21147;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#30340;&#19968;&#20010;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#35813;&#20219;&#21153;&#28041;&#21450;&#25551;&#36848;&#22797;&#26434;&#30340;&#35270;&#35273;&#29615;&#22659;&#65292;&#35782;&#21035;&#27963;&#36291;&#23545;&#35937;&#65292;&#20197;&#21450;&#36890;&#36807;&#35821;&#35328;&#35299;&#37322;&#23427;&#20204;&#30340;&#21464;&#21270;&#12290;&#20256;&#32479;&#26041;&#27861;&#23558;&#23545;&#35937;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#26816;&#27979;&#36827;&#34892;&#38548;&#31163;&#65292;&#25552;&#20379;&#20102;&#23545;&#21160;&#24577;&#29615;&#22659;&#30340;&#26377;&#38480;&#35270;&#22270;&#12290;&#27492;&#22806;&#65292;&#20381;&#36182;&#20110;&#19968;&#23567;&#22871;&#31526;&#21495;&#21270;&#35789;&#27719;&#26469;&#34920;&#31034;&#21464;&#21270;&#38480;&#21046;&#20102;&#35821;&#35328;&#30340;&#34920;&#36798;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#23545;&#35937;&#29366;&#24577;&#23383;&#24149;&#21644;&#29366;&#24577;&#21464;&#21270;&#34920;&#31034;&#65288;OSCaR&#65289;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;OSCaR&#21253;&#25324;&#26469;&#33258;&#21508;&#31181;&#20027;&#35266;&#35270;&#35282;&#35270;&#39057;&#38598;&#21512;&#30340;14,084&#20010;&#24102;&#27880;&#37322;&#35270;&#39057;&#29255;&#27573;&#65292;&#28085;&#30422;&#36817;1,000&#20010;&#29420;&#29305;&#23545;&#35937;&#12290;&#23427;&#20026;&#35780;&#20272;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#25552;&#20379;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#39564;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17128v3 Announce Type: cross  Abstract: The capability of intelligent models to extrapolate and comprehend changes in object states is a crucial yet demanding aspect of AI research, particularly through the lens of human interaction in real-world settings. This task involves describing complex visual environments, identifying active objects, and interpreting their changes as conveyed through language. Traditional methods, which isolate object captioning and state change detection, offer a limited view of dynamic environments. Moreover, relying on a small set of symbolic words to represent changes has restricted the expressiveness of the language. To address these challenges, in this paper, we introduce the Object State Captioning and State Change Representation (OSCaR) dataset and benchmark. OSCaR consists of 14,084 annotated video segments with nearly 1,000 unique objects from various egocentric video collections. It sets a new testbed for evaluating multimodal large langua
&lt;/p&gt;</description></item><item><title>ROS-Causal&#26159;&#19968;&#20010;&#22522;&#20110;ROS&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20154;&#26426;&#31354;&#38388;&#20132;&#20114;&#20013;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#21644;&#22240;&#26524;&#21457;&#29616;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#32570;&#20047;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#22312;ROS&#29983;&#24577;&#31995;&#32479;&#20869;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.16068</link><description>&lt;p&gt;
ROS-Causal&#65306;&#22522;&#20110;ROS&#30340;&#20154;&#26426;&#20132;&#20114;&#24212;&#29992;&#22240;&#26524;&#20998;&#26512;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ROS-Causal: A ROS-based Causal Analysis Framework for Human-Robot Interaction Applications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16068
&lt;/p&gt;
&lt;p&gt;
ROS-Causal&#26159;&#19968;&#20010;&#22522;&#20110;ROS&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#20154;&#26426;&#31354;&#38388;&#20132;&#20114;&#20013;&#36827;&#34892;&#25968;&#25454;&#25910;&#38598;&#21644;&#22240;&#26524;&#21457;&#29616;&#65292;&#35299;&#20915;&#20102;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#32570;&#20047;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#22312;ROS&#29983;&#24577;&#31995;&#32479;&#20869;&#23454;&#29616;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#20849;&#20139;&#31354;&#38388;&#37096;&#32626;&#26426;&#22120;&#20154;&#38656;&#35201;&#29702;&#35299;&#38468;&#36817;Agent&#21644;&#29289;&#20307;&#20043;&#38388;&#30340;&#20132;&#20114;&#12290;&#36890;&#36807;&#22240;&#26524;&#25512;&#29702;&#23545;&#22240;&#26524;&#20851;&#31995;&#24314;&#27169;&#26377;&#21161;&#20110;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#24182;&#39044;&#27979;&#26426;&#22120;&#20154;&#24178;&#39044;&#12290;&#28982;&#32780;&#65292;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#26159;&#29616;&#26377;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#30446;&#21069;&#32570;&#20047;&#22312;ROS&#29983;&#24577;&#31995;&#32479;&#20869;&#37096;&#30340;&#23454;&#29616;&#65292;&#36825;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#20107;&#23454;&#26631;&#20934;&#65292;&#38459;&#30861;&#20102;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#26377;&#25928;&#21033;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;ROS-Causal&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;ROS&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#19978;&#30340;&#25968;&#25454;&#25910;&#38598;&#21644;&#22240;&#26524;&#21457;&#29616;&#22312;&#20154;&#26426;&#31354;&#38388;&#20132;&#20114;&#20013;&#12290;&#38598;&#25104;&#20102;ROS&#30340;&#20020;&#26102;&#27169;&#25311;&#22120;&#23637;&#31034;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#26426;&#22120;&#20154;&#22312;&#25968;&#25454;&#25910;&#38598;&#36807;&#31243;&#20013;&#29983;&#25104;&#22240;&#26524;&#27169;&#22411;&#12290;ROS-Causal&#21487;&#22312;GitHub&#19978;&#25214;&#21040;&#65306;https://github.com/lcastri/roscausal.git&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16068v1 Announce Type: cross  Abstract: Deploying robots in human-shared spaces requires understanding interactions among nearby agents and objects. Modelling cause-and-effect relations through causal inference aids in predicting human behaviours and anticipating robot interventions. However, a critical challenge arises as existing causal discovery methods currently lack an implementation inside the ROS ecosystem, the standard de facto in robotics, hindering effective utilisation in robotics. To address this gap, this paper introduces ROS-Causal, a ROS-based framework for onboard data collection and causal discovery in human-robot spatial interactions. An ad-hoc simulator, integrated with ROS, illustrates the approach's effectiveness, showcasing the robot onboard generation of causal models during data collection. ROS-Causal is available on GitHub: https://github.com/lcastri/roscausal.git.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#35270;&#35282;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;LLMs&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#21644;&#20986;&#29616;&#29616;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#32452;&#32455;&#21644;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#21160;&#24577;&#28436;&#21270;&#36807;&#31243;&#65292;&#23588;&#20854;&#20851;&#27880;&#35757;&#32451;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2402.09099</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#35270;&#35282;&#25506;&#32034;LLMs&#20013;&#30340;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#21644;&#20986;&#29616;&#29616;&#35937;
&lt;/p&gt;
&lt;p&gt;
Exploring Neuron Interactions and Emergence in LLMs: From the Multifractal Analysis Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09099
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#35270;&#35282;&#65292;&#28145;&#20837;&#30740;&#31350;&#20102;LLMs&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#21644;&#20986;&#29616;&#29616;&#35937;&#12290;&#36890;&#36807;&#24341;&#20837;&#33258;&#32452;&#32455;&#21644;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#30340;&#27010;&#24565;&#65292;&#30740;&#31350;&#20102;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#21160;&#24577;&#28436;&#21270;&#36807;&#31243;&#65292;&#23588;&#20854;&#20851;&#27880;&#35757;&#32451;&#20013;&#30340;&#22797;&#26434;&#34892;&#20026;&#12290;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#23545;&#22823;&#22411;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#23450;&#37327;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#24448;&#30340;&#22823;&#22411;&#27169;&#22411;&#20013;&#65292;&#20851;&#20110;&#20986;&#29616;&#29616;&#35937;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#21151;&#33021;&#33021;&#21147;&#22914;&#20309;&#38543;&#27169;&#22411;&#35268;&#27169;&#30340;&#25193;&#22823;&#32780;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#36229;&#36234;&#20102;&#36825;&#19968;&#20256;&#32479;&#33539;&#24335;&#65292;&#26088;&#22312;&#36890;&#36807;&#19981;&#20165;&#20165;&#20381;&#36182;&#20110;&#27169;&#22411;&#35268;&#27169;&#65292;&#32780;&#26356;&#21152;&#20851;&#27880;&#35757;&#32451;&#36807;&#31243;&#20013;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#30340;&#22797;&#26434;&#34892;&#20026;&#65292;&#21152;&#28145;&#25105;&#20204;&#23545;LLMs&#20869;&#37096;&#20986;&#29616;&#29616;&#35937;&#30340;&#29702;&#35299;&#12290;&#36890;&#36807;&#24341;&#20837;&#8220;&#33258;&#32452;&#32455;&#8221;&#21644;&#8220;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#8221;&#27010;&#24565;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#31070;&#32463;&#20803;&#30456;&#20114;&#20316;&#29992;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#22914;&#20309;&#21160;&#24577;&#28436;&#21270;&#65292;&#20174;&#32780;&#23548;&#33268;&#8220;&#20986;&#29616;&#29616;&#35937;&#8221;&#65292;&#36825;&#31181;&#29616;&#35937;&#21453;&#26144;&#20102;&#33258;&#28982;&#31995;&#32479;&#20013;&#31616;&#21333;&#30340;&#24494;&#35266;&#30456;&#20114;&#20316;&#29992;&#22914;&#20309;&#23548;&#33268;&#22797;&#26434;&#30340;&#23439;&#35266;&#34892;&#20026;&#12290;&#20026;&#20102;&#23450;&#37327;&#20998;&#26512;&#35757;&#32451;&#36807;&#31243;&#20013;&#22823;&#22411;&#27169;&#22411;&#20013;&#31070;&#32463;&#20803;&#20043;&#38388;&#19981;&#26029;&#28436;&#21270;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#31070;&#32463;&#20803;&#30340;&#22810;&#37325;&#20998;&#24418;&#20998;&#26512;&#65288;NeuroMFA&#65289;&#12290;&#21033;&#29992;NeuroMFA&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09099v1 Announce Type: new Abstract: Prior studies on the emergence in large models have primarily focused on how the functional capabilities of large language models (LLMs) scale with model size. Our research, however, transcends this traditional paradigm, aiming to deepen our understanding of the emergence within LLMs by placing a special emphasis not just on the model size but more significantly on the complex behavior of neuron interactions during the training process. By introducing the concepts of "self-organization" and "multifractal analysis," we explore how neuron interactions dynamically evolve during training, leading to "emergence," mirroring the phenomenon in natural systems where simple micro-level interactions give rise to complex macro-level behaviors. To quantitatively analyze the continuously evolving interactions among neurons in large models during training, we propose the Neuron-based Multifractal Analysis (NeuroMFA). Utilizing NeuroMFA, we conduct a com
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;"&#31867;&#20284;&#35774;&#35745;"&#30340;&#26234;&#33021;&#30011;&#24067;&#29615;&#22659;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#24335;AI&#32452;&#20214;&#38598;&#25104;&#21040;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12289;&#36845;&#20195;&#21644;&#27604;&#36739;&#21487;&#35270;&#21270;&#31649;&#29702;&#12290;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65292;&#35770;&#25991;&#39564;&#35777;&#20102;&#30011;&#24067;&#30028;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.08812</link><description>&lt;p&gt;
&#26234;&#33021;&#30011;&#24067;: &#36890;&#36807;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12289;&#36845;&#20195;&#21644;&#31649;&#29702;&#23454;&#29616;&#31867;&#20284;&#35774;&#35745;&#30340;&#25506;&#32034;&#24615;&#21487;&#35270;&#25968;&#25454;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Intelligent Canvas: Enabling Design-Like Exploratory Visual Data Analysis through Rapid Prototyping, Iteration and Curation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08812
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;"&#31867;&#20284;&#35774;&#35745;"&#30340;&#26234;&#33021;&#30011;&#24067;&#29615;&#22659;&#65292;&#36890;&#36807;&#23558;&#29983;&#25104;&#24335;AI&#32452;&#20214;&#38598;&#25104;&#21040;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12289;&#36845;&#20195;&#21644;&#27604;&#36739;&#21487;&#35270;&#21270;&#31649;&#29702;&#12290;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65292;&#35770;&#25991;&#39564;&#35777;&#20102;&#30011;&#24067;&#30028;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#25968;&#25454;&#20998;&#26512;&#36890;&#36807;&#25506;&#32034;&#24615;&#30340;&#21487;&#35270;&#20998;&#26512;&#26041;&#27861;&#26469;&#23547;&#27714;&#24847;&#24819;&#19981;&#21040;&#30340;&#27934;&#35265;&#65292;&#24182;&#36229;&#36234;&#36923;&#36753;&#30340;&#36880;&#27493;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30028;&#38754;&#65288;&#22914;&#31508;&#35760;&#26412;&#21644;&#20202;&#34920;&#26495;&#65289;&#22312;&#21487;&#35270;&#25968;&#25454;&#20998;&#26512;&#30340;&#25506;&#32034;&#21644;&#27604;&#36739;&#26041;&#38754;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#8220;&#31867;&#20284;&#35774;&#35745;&#8221;&#30340;&#26234;&#33021;&#30011;&#24067;&#29615;&#22659;&#65292;&#23558;&#29983;&#25104;&#24335;AI&#38598;&#25104;&#21040;&#25968;&#25454;&#20998;&#26512;&#20013;&#65292;&#25552;&#20379;&#24555;&#36895;&#21407;&#22411;&#35774;&#35745;&#12289;&#36845;&#20195;&#21644;&#27604;&#36739;&#21487;&#35270;&#21270;&#31649;&#29702;&#12290;&#25105;&#20204;&#30340;&#20004;&#20010;&#20027;&#35201;&#36129;&#29486;&#21253;&#25324;&#23558;&#29983;&#25104;&#24335;AI&#32452;&#20214;&#38598;&#25104;&#21040;&#30011;&#24067;&#30028;&#38754;&#20013;&#65292;&#24182;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#65288;N=10&#65289;&#35780;&#20272;&#20102;&#30011;&#24067;&#30028;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.08812v1 Announce Type: cross Abstract: Complex data analysis inherently seeks unexpected insights through exploratory \re{visual analysis} methods, transcending logical, step-by-step processing. However, \re{existing interfaces such as notebooks and dashboards have limitations in exploration and comparison for visual data analysis}. Addressing these limitations, we introduce a "design-like" intelligent canvas environment integrating generative AI into data analysis, offering rapid prototyping, iteration, and comparative visualization management. Our dual contributions include the integration of generative AI components into a canvas interface, and empirical findings from a user study (N=10) evaluating the effectiveness of the canvas interface.
&lt;/p&gt;</description></item><item><title>CPSDBench&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#20013;&#22269;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#25972;&#21512;&#23454;&#38469;&#22330;&#26223;&#20013;&#25910;&#38598;&#30340;&#20844;&#20849;&#23433;&#20840;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#12289;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#39064;&#22238;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#39640;&#20102;&#23545;&#29616;&#26377;&#27169;&#22411;&#22312;&#35299;&#20915;&#20844;&#20849;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#24615;&#33021;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2402.07234</link><description>&lt;p&gt;
CPSDBench&#65306;&#19968;&#20010;&#38024;&#23545;&#20013;&#22269;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#21644;&#22522;&#32447;
&lt;/p&gt;
&lt;p&gt;
CPSDBench: A Large Language Model Evaluation Benchmark and Baseline for Chinese Public Security Domain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07234
&lt;/p&gt;
&lt;p&gt;
CPSDBench&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#20013;&#22269;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#37327;&#36523;&#23450;&#21046;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#22522;&#20934;&#65292;&#36890;&#36807;&#25972;&#21512;&#23454;&#38469;&#22330;&#26223;&#20013;&#25910;&#38598;&#30340;&#20844;&#20849;&#23433;&#20840;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;&#38024;&#23545;&#25991;&#26412;&#20998;&#31867;&#12289;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#39064;&#22238;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#20840;&#38754;&#35780;&#20272;LLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#24341;&#20837;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#25552;&#39640;&#20102;&#23545;&#29616;&#26377;&#27169;&#22411;&#22312;&#35299;&#20915;&#20844;&#20849;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#24615;&#33021;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#23637;&#31034;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#21644;&#25928;&#26524;&#12290;&#20026;&#20102;&#35780;&#20272;&#20027;&#27969;LLMs&#22312;&#20844;&#20849;&#23433;&#20840;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#26500;&#24314;&#19968;&#20010;&#19987;&#38376;&#38024;&#23545;&#20013;&#22269;&#20844;&#20849;&#23433;&#20840;&#39046;&#22495;&#30340;&#35780;&#20272;&#22522;&#20934;&#8212;&#8212;CPSDBench&#12290;CPSDBench&#25972;&#21512;&#20102;&#20174;&#29616;&#23454;&#22330;&#26223;&#20013;&#25910;&#38598;&#21040;&#30340;&#19982;&#20844;&#20849;&#23433;&#20840;&#30456;&#20851;&#30340;&#25968;&#25454;&#38598;&#65292;&#25903;&#25345;&#23545;LLMs&#22312;&#25991;&#26412;&#20998;&#31867;&#12289;&#20449;&#24687;&#25552;&#21462;&#12289;&#38382;&#39064;&#22238;&#31572;&#21644;&#25991;&#26412;&#29983;&#25104;&#22235;&#20010;&#20851;&#38190;&#32500;&#24230;&#19978;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#26412;&#30740;&#31350;&#36824;&#24341;&#20837;&#20102;&#19968;&#22871;&#21019;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#65292;&#26088;&#22312;&#26356;&#31934;&#30830;&#22320;&#37327;&#21270;LLMs&#22312;&#25191;&#34892;&#19982;&#20844;&#20849;&#23433;&#20840;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#25928;&#21147;&#12290;&#36890;&#36807;&#26412;&#30740;&#31350;&#20013;&#30340;&#28145;&#20837;&#20998;&#26512;&#21644;&#35780;&#20272;&#65292;&#25105;&#20204;&#19981;&#20165;&#22686;&#24378;&#20102;&#23545;&#29616;&#26377;&#27169;&#22411;&#22312;&#35299;&#20915;&#20844;&#20849;&#23433;&#20840;&#38382;&#39064;&#26041;&#38754;&#24615;&#33021;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#30340;&#29702;&#35299;&#65292;&#36824;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated significant potential and effectiveness across multiple application domains. To assess the performance of mainstream LLMs in public security tasks, this study aims to construct a specialized evaluation benchmark tailored to the Chinese public security domain--CPSDbench. CPSDbench integrates datasets related to public security collected from real-world scenarios, supporting a comprehensive assessment of LLMs across four key dimensions: text classification, information extraction, question answering, and text generation. Furthermore, this study introduces a set of innovative evaluation metrics designed to more precisely quantify the efficacy of LLMs in executing tasks related to public security. Through the in-depth analysis and evaluation conducted in this research, we not only enhance our understanding of the performance strengths and limitations of existing models in addressing public security issues but also provide references for the fu
&lt;/p&gt;</description></item><item><title>ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;</title><link>https://arxiv.org/abs/2402.03848</link><description>&lt;p&gt;
ANLS* -- &#19968;&#31181;&#36866;&#29992;&#20110;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#36890;&#29992;&#25991;&#26723;&#22788;&#29702;&#24230;&#37327;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ANLS* -- A Universal Document Processing Metric for Generative Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03848
&lt;/p&gt;
&lt;p&gt;
ANLS*&#26159;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#38024;&#23545;&#21508;&#31181;&#20219;&#21153;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#35780;&#20272;&#12290;&#23427;&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20197;&#20316;&#20026;&#26367;&#20195;&#26041;&#26696;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#19978;&#65292;&#22312;&#25991;&#26723;&#20998;&#31867;&#21644;&#20449;&#24687;&#25552;&#21462;&#31561;&#20219;&#21153;&#20013;&#65292;&#21306;&#20998;&#27169;&#22411;&#19968;&#30452;&#26159;&#20027;&#35201;&#36873;&#25321;&#12290;&#36825;&#20123;&#27169;&#22411;&#20570;&#20986;&#30340;&#39044;&#27979;&#21487;&#20197;&#20998;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#39044;&#23450;&#20041;&#31867;&#21035;&#65292;&#20415;&#20110;&#36827;&#34892;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#65292;&#24182;&#33021;&#30452;&#25509;&#35745;&#31639;F1&#20998;&#25968;&#31561;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#29983;&#25104;&#22411;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;GLLMs&#65289;&#30340;&#26368;&#26032;&#36827;&#23637;&#20419;&#20351;&#39046;&#22495;&#21457;&#29983;&#20102;&#36716;&#21464;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#22791;&#20102;&#24378;&#22823;&#30340;&#38646;-shot&#33021;&#21147;&#65292;&#28040;&#38500;&#20102;&#19979;&#28216;&#25968;&#25454;&#38598;&#21644;&#35745;&#31639;&#26114;&#36149;&#30340;&#24494;&#35843;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#35780;&#20272;GLLMs&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23545;&#20110;GLLMs&#30340;&#39044;&#27979;&#65292;&#19981;&#33021;&#24212;&#29992;&#20110;&#21306;&#20998;&#27169;&#22411;&#25152;&#20351;&#29992;&#30340;&#20108;&#20803;&#30495;&#20551;&#35780;&#20272;&#26041;&#27861;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#22411;&#27169;&#22411;&#30340;&#26032;&#24230;&#37327;&#26041;&#27861;&#65292;&#31216;&#20026;ANLS*&#65292;&#29992;&#20110;&#35780;&#20272;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#20449;&#24687;&#25552;&#21462;&#21644;&#20998;&#31867;&#20219;&#21153;&#12290;ANLS*&#24230;&#37327;&#26041;&#27861;&#25193;&#23637;&#20102;&#29616;&#26377;ANLS&#24230;&#37327;&#26041;&#27861;&#65292;&#21487;&#20316;&#20026;&#19968;&#31181;&#21363;&#25554;&#21363;&#29992;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditionally, discriminative models have been the predominant choice for tasks like document classification and information extraction. These models make predictions that fall into a limited number of predefined classes, facilitating a binary true or false evaluation and enabling the direct calculation of metrics such as the F1 score. However, recent advancements in generative large language models (GLLMs) have prompted a shift in the field due to their enhanced zero-shot capabilities, which eliminate the need for a downstream dataset and computationally expensive fine-tuning. However, evaluating GLLMs presents a challenge as the binary true or false evaluation used for discriminative models is not applicable to the predictions made by GLLMs. This paper introduces a new metric for generative models called ANLS* for evaluating a wide variety of tasks, including information extraction and classification tasks. The ANLS* metric extends existing ANLS metrics as a drop-in-replacement and i
&lt;/p&gt;</description></item><item><title>EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2402.03049</link><description>&lt;p&gt;
EasyInstruct&#65306;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EasyInstruct: An Easy-to-use Instruction Processing Framework for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.03049
&lt;/p&gt;
&lt;p&gt;
EasyInstruct&#26159;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#22359;&#21270;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#65292;&#20351;&#25351;&#20196;&#22788;&#29702;&#26356;&#21152;&#26041;&#20415;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25351;&#20196;&#35843;&#25972;&#24050;&#32463;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#24182;&#25104;&#20026;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#19968;&#31181;&#20851;&#38190;&#25216;&#26415;&#12290;&#20026;&#20102;&#26500;&#24314;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#25968;&#25454;&#38598;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#65292;&#26088;&#22312;&#22312;&#25968;&#25454;&#25968;&#37327;&#21644;&#25968;&#25454;&#36136;&#37327;&#20043;&#38388;&#36798;&#21040;&#31934;&#24039;&#30340;&#24179;&#34913;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#25351;&#20196;&#22788;&#29702;&#26041;&#27861;&#20043;&#38388;&#20173;&#28982;&#23384;&#22312;&#19981;&#19968;&#33268;&#65292;&#30446;&#21069;&#27809;&#26377;&#26631;&#20934;&#30340;&#24320;&#28304;&#25351;&#20196;&#22788;&#29702;&#23454;&#29616;&#26694;&#26550;&#21487;&#20379;&#31038;&#21306;&#20351;&#29992;&#65292;&#36825;&#20351;&#24471;&#20174;&#19994;&#32773;&#26080;&#27861;&#36827;&#19968;&#27493;&#24320;&#21457;&#21644;&#25512;&#36827;&#12290;&#20026;&#20102;&#20419;&#36827;&#25351;&#20196;&#22788;&#29702;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EasyInstruct&#65292;&#19968;&#20010;&#26131;&#20110;&#20351;&#29992;&#30340;&#29992;&#20110;LLMs&#30340;&#25351;&#20196;&#22788;&#29702;&#26694;&#26550;&#65292;&#23427;&#23558;&#25351;&#20196;&#29983;&#25104;&#12289;&#36873;&#25321;&#21644;&#25552;&#31034;&#27169;&#22359;&#21270;&#65292;&#24182;&#32771;&#34385;&#23427;&#20204;&#30340;&#32452;&#21512;&#21644;&#20132;&#20114;&#12290;EasyInstruct&#24050;&#32463;&#22312;https://github.com/zjunlp/EasyInstruct&#19978;&#20844;&#24320;&#21457;&#24067;&#65292;&#24182;&#24471;&#21040;&#20102;&#31215;&#26497;&#32500;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, instruction tuning has gained increasing attention and emerged as a crucial technique to enhance the capabilities of Large Language Models (LLMs). To construct high-quality instruction datasets, many instruction processing approaches have been proposed, aiming to achieve a delicate balance between data quantity and data quality. Nevertheless, due to inconsistencies that persist among various instruction processing methods, there is no standard open-source instruction processing implementation framework available for the community, which hinders practitioners from further developing and advancing. To facilitate instruction processing research and development, we present EasyInstruct, an easy-to-use instruction processing framework for LLMs, which modularizes instruction generation, selection, and prompting, while also considering their combination and interaction. EasyInstruct is publicly released and actively maintained at https://github.com/zjunlp/EasyInstruct, along 
&lt;/p&gt;</description></item><item><title>SLIM&#26159;&#19968;&#31181;&#22810;&#21028;&#21035;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#32452;&#21512;&#22810;&#20010;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#28508;&#21464;&#37327;&#25216;&#33021;&#21457;&#29616;&#65292;&#20811;&#26381;&#20102;&#22870;&#21169;&#20043;&#38388;&#30340;&#24178;&#25200;&#12290;</title><link>https://arxiv.org/abs/2402.00823</link><description>&lt;p&gt;
SLIM: &#22810;&#21028;&#21035;&#22120;&#22312;&#25216;&#33021;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
SLIM: Skill Learning with Multiple Critics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00823
&lt;/p&gt;
&lt;p&gt;
SLIM&#26159;&#19968;&#31181;&#22810;&#21028;&#21035;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#32452;&#21512;&#22810;&#20010;&#21028;&#21035;&#22120;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#26174;&#33879;&#25913;&#21892;&#20102;&#28508;&#21464;&#37327;&#25216;&#33021;&#21457;&#29616;&#65292;&#20811;&#26381;&#20102;&#22870;&#21169;&#20043;&#38388;&#30340;&#24178;&#25200;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#30417;&#30563;&#30340;&#25216;&#33021;&#23398;&#20064;&#26088;&#22312;&#33719;&#21462;&#21033;&#29992;&#29615;&#22659;&#30340;&#24213;&#23618;&#21160;&#24577;&#30340;&#26377;&#29992;&#34892;&#20026;&#12290;&#22522;&#20110;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#28508;&#21464;&#37327;&#27169;&#22411;&#22312;&#27492;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#65292;&#20294;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#39046;&#22495;&#20173;&#23384;&#22312;&#22256;&#38590;&#12290;&#30001;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#21487;&#33021;&#28041;&#21450;&#21040;&#29615;&#22659;&#20013;&#24456;&#22810;&#33258;&#30001;&#24230;&#65292;&#21333;&#32431;&#30340;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#26080;&#27861;&#20135;&#29983;&#26377;&#29992;&#30340;&#25805;&#20316;&#34892;&#20026;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SLIM&#65292;&#19968;&#31181;&#38024;&#23545;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#22810;&#21028;&#21035;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#35266;&#28857;&#26159;&#65292;&#22312;&#28436;&#21592;-&#35780;&#35770;&#32773;&#26694;&#26550;&#20013;&#21033;&#29992;&#22810;&#20010;&#21028;&#21035;&#22120;&#26469;&#20248;&#38597;&#22320;&#32452;&#21512;&#22810;&#20010;&#22870;&#21169;&#20989;&#25968;&#65292;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#28508;&#21464;&#37327;&#25216;&#33021;&#21457;&#29616;&#65292;&#21516;&#26102;&#20811;&#26381;&#22870;&#21169;&#20043;&#38388;&#21487;&#33021;&#21457;&#29983;&#30340;&#24178;&#25200;&#65292;&#38459;&#30861;&#23545;&#26377;&#29992;&#25216;&#33021;&#30340;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised skill learning aims to acquire useful behaviors that leverage the underlying dynamics of the environment. Latent variable models, based on mutual information maximization, have been particularly successful in this task but still struggle in the context of robotic manipulation. As it requires impacting a possibly large set of degrees of freedom composing the environment, mutual information maximization fails alone in producing useful manipulation behaviors. To address this limitation, we introduce SLIM, a multi-critic learning approach for skill discovery with a particular focus on robotic manipulation. Our main insight is that utilizing multiple critics in an actor-critic framework to gracefully combine multiple reward functions leads to a significant improvement in latent-variable skill discovery for robotic manipulation while overcoming possible interference occurring among rewards which hinders convergence to useful skills. Furthermore, in the context of tabletop man
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#22270;&#32534;&#36753;&#20316;&#20026;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20808;&#21069;&#21162;&#21147;&#65292;&#25506;&#35752;&#20102;&#23558;&#36755;&#20837;&#25968;&#25454;&#34920;&#31034;&#20026;&#22270;&#24418;&#23545;&#20110;&#29983;&#25104;&#40657;&#31665;&#22270;&#20687;&#20998;&#31867;&#22120;&#26368;&#23567;&#19988;&#26377;&#24847;&#20041;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24615;&#33021;&#21644;&#26102;&#38388;&#25928;&#29575;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2401.11609</link><description>&lt;p&gt;
&#22270;&#32534;&#36753;&#29992;&#20110;&#21453;&#20107;&#23454;&#35299;&#37322;&#65306;&#19968;&#39033;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Graph Edits for Counterfactual Explanations: A comparative study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11609
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#36890;&#36807;&#27604;&#36739;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#22270;&#32534;&#36753;&#20316;&#20026;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20808;&#21069;&#21162;&#21147;&#65292;&#25506;&#35752;&#20102;&#23558;&#36755;&#20837;&#25968;&#25454;&#34920;&#31034;&#20026;&#22270;&#24418;&#23545;&#20110;&#29983;&#25104;&#40657;&#31665;&#22270;&#20687;&#20998;&#31867;&#22120;&#26368;&#23567;&#19988;&#26377;&#24847;&#20041;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#24615;&#33021;&#21644;&#26102;&#38388;&#25928;&#29575;&#26368;&#20339;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#20107;&#23454;&#24050;&#34987;&#30830;&#31435;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#65292;&#21033;&#29992;&#19968;&#32452;&#26368;&#23567;&#30340;&#32534;&#36753;&#26469;&#25913;&#21464;&#20998;&#31867;&#22120;&#30340;&#39044;&#27979;&#12290;&#22312;&#32771;&#34385;&#22270;&#20687;&#19978;&#30340;&#27010;&#24565;&#21453;&#20107;&#23454;&#26102;&#65292;&#35831;&#27714;&#30340;&#32534;&#36753;&#24212;&#23545;&#24212;&#36755;&#20837;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#26174;&#33879;&#27010;&#24565;&#12290;&#21516;&#26102;&#65292;&#27010;&#24565;&#36317;&#31163;&#30001;&#30693;&#35782;&#22270;&#35889;&#23450;&#20041;&#65292;&#30830;&#20445;&#27010;&#24565;&#32534;&#36753;&#30340;&#26368;&#20248;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#36827;&#34892;&#27604;&#36739;&#30740;&#31350;&#25193;&#23637;&#20102;&#20197;&#22270;&#32534;&#36753;&#20026;&#21453;&#20107;&#23454;&#35299;&#37322;&#30340;&#20808;&#21069;&#21162;&#21147;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#28085;&#30422;&#20102;&#30417;&#30563;&#21644;&#26080;&#30417;&#30563;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#12290;&#21040;&#27492;&#20026;&#27490;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20197;&#19979;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65306;&#25105;&#20204;&#24212;&#35813;&#23558;&#36755;&#20837;&#25968;&#25454;&#34920;&#31034;&#20026;&#22270;&#24418;&#65292;&#36825;&#26159;&#29983;&#25104;&#40657;&#31665;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#26368;&#23567;&#21644;&#26377;&#24847;&#20041;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#24615;&#33021;&#21644;&#26102;&#38388;&#25928;&#29575;&#26041;&#38754;&#26368;&#20339;&#30340;GNN&#26041;&#27861;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11609v2 Announce Type: replace-cross  Abstract: Counterfactuals have been established as a popular explainability technique which leverages a set of minimal edits to alter the prediction of a classifier. When considering conceptual counterfactuals on images, the edits requested should correspond to salient concepts present in the input data. At the same time, conceptual distances are defined by knowledge graphs, ensuring the optimality of conceptual edits. In this work, we extend previous endeavors on graph edits as counterfactual explanations by conducting a comparative study which encompasses both supervised and unsupervised Graph Neural Network (GNN) approaches. To this end, we pose the following significant research question: should we represent input data as graphs, which is the optimal GNN approach in terms of performance and time efficiency to generate minimal and meaningful counterfactual explanations for black-box image classifiers?
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#32771;&#34385;&#20102;&#25439;&#22833;&#21644;&#19981;&#30830;&#23450;&#24615;&#22522;&#30784;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2312.13927</link><description>&lt;p&gt;
&#20851;&#20110;&#25439;&#22833;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the convergence of loss and uncertainty-based active learning algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13927
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#32771;&#34385;&#20102;&#25439;&#22833;&#21644;&#19981;&#30830;&#23450;&#24615;&#22522;&#30784;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#22312;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#19978;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#24182;&#23637;&#31034;&#20102;&#20854;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#22312;&#19981;&#21516;&#20551;&#35774;&#19979;&#25439;&#22833;&#21644;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#30340;&#20027;&#21160;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#32452;&#26465;&#20214;&#65292;&#30830;&#20445;&#22312;&#24212;&#29992;&#20110;&#32447;&#24615;&#20998;&#31867;&#22120;&#21644;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#38598;&#26102;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#36825;&#21253;&#25324;&#35777;&#26126;&#21508;&#31181;&#25439;&#22833;&#20989;&#25968;&#30340;&#22522;&#20110;&#25439;&#22833;&#30340;&#37319;&#26679;&#30340;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#24050;&#30693;&#30340;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#29575;&#30028;&#38480;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23548;&#20986;&#25439;&#22833;&#37319;&#26679;&#30340;&#25910;&#25947;&#36895;&#29575;&#30028;&#38480;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#31639;&#27861;&#65292;&#23558;&#28857;&#37319;&#26679;&#21644;&#38543;&#26426;Polyak&#27493;&#38271;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#20851;&#20110;&#37319;&#26679;&#36807;&#31243;&#30340;&#26465;&#20214;&#65292;&#30830;&#20445;&#35813;&#31639;&#27861;&#30340;&#25910;&#25947;&#36895;&#24230;&#20445;&#35777;&#65292;&#29305;&#21035;&#26159;&#22312;&#20809;&#28369;&#20984;&#25439;&#22833;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#30340;&#25968;&#20540;&#32467;&#26524;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#31639;&#27861;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.13927v2 Announce Type: replace-cross  Abstract: We consider the convergence rates of loss and uncertainty-based active learning algorithms under various assumptions. Firstly, we establish a set of conditions that ensure convergence rates when applied to linear classifiers and linearly separable datasets. This includes demonstrating convergence rate guarantees for loss-based sampling with various loss functions. Secondly, we introduce a framework that allows us to derive convergence rate bounds for loss-based sampling by leveraging known convergence rate bounds for stochastic gradient descent algorithms. Lastly, we propose a new algorithm that combines point sampling and stochastic Polyak's step size. We establish a condition on the sampling process, ensuring a convergence rate guarantee for this algorithm, particularly in the case of smooth convex loss functions. Our numerical results showcase the efficiency of the proposed algorithm.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20869;&#22312;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#23460;&#20869;&#21333;&#35270;&#22270;&#26448;&#26009;&#20272;&#35745;&#65292;&#36890;&#36807;&#27010;&#29575;&#24418;&#24335;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#37319;&#26679;&#35299;&#31354;&#38388;&#26469;&#35299;&#20915;&#22806;&#35266;&#20998;&#35299;&#20013;&#20809;&#29031;&#21644;&#26448;&#26009;&#23646;&#24615;&#20043;&#38388;&#30340;&#22266;&#26377;&#27169;&#31946;&#24615;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.12274</link><description>&lt;p&gt;
&#23460;&#20869;&#21333;&#35270;&#22270;&#26448;&#26009;&#20272;&#35745;&#30340;&#20869;&#22312;&#22270;&#20687;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Image Diffusion for Indoor Single-view Material Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.12274
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20869;&#22312;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#65292;&#24212;&#29992;&#20110;&#23460;&#20869;&#21333;&#35270;&#22270;&#26448;&#26009;&#20272;&#35745;&#65292;&#36890;&#36807;&#27010;&#29575;&#24418;&#24335;&#30340;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#37319;&#26679;&#35299;&#31354;&#38388;&#26469;&#35299;&#20915;&#22806;&#35266;&#20998;&#35299;&#20013;&#20809;&#29031;&#21644;&#26448;&#26009;&#23646;&#24615;&#20043;&#38388;&#30340;&#22266;&#26377;&#27169;&#31946;&#24615;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#20869;&#22312;&#22270;&#20687;&#25193;&#25955;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#23460;&#20869;&#22330;&#26223;&#22806;&#35266;&#20998;&#35299;&#30340;&#29983;&#25104;&#27169;&#22411;&#12290;&#32473;&#23450;&#21333;&#20010;&#36755;&#20837;&#35270;&#22270;&#65292;&#25105;&#20204;&#37319;&#26679;&#22810;&#31181;&#21487;&#33021;&#30340;&#26448;&#26009;&#35299;&#37322;&#65292;&#34920;&#31034;&#20026;&#21453;&#29031;&#29575;&#12289;&#31895;&#31961;&#24230;&#21644;&#37329;&#23646;&#24230;&#22270;&#12290;&#22806;&#35266;&#20998;&#35299;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#20809;&#29031;&#21644;&#26448;&#26009;&#23646;&#24615;&#20043;&#38388;&#22266;&#26377;&#30340;&#27169;&#31946;&#24615;&#20197;&#21450;&#32570;&#20047;&#30495;&#23454;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20513;&#23548;&#27010;&#29575;&#24418;&#24335;&#65292;&#19981;&#26159;&#30452;&#25509;&#39044;&#27979;&#30495;&#23454;&#30340;&#26448;&#26009;&#23646;&#24615;&#65292;&#32780;&#26159;&#20351;&#29992;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#20174;&#35299;&#31354;&#38388;&#20013;&#36827;&#34892;&#37319;&#26679;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#34920;&#26126;&#21033;&#29992;&#26368;&#36817;&#25193;&#25955;&#27169;&#22411;&#22312;&#22823;&#35268;&#27169;&#29616;&#23454;&#19990;&#30028;&#22270;&#20687;&#19978;&#35757;&#32451;&#30340;&#24378;&#22823;&#23398;&#20064;&#20808;&#39564;&#21487;&#20197;&#34987;&#35843;&#25972;&#20026;&#26448;&#26009;&#20272;&#35745;&#65292;&#24182;&#26497;&#22823;&#22320;&#25913;&#21892;&#23545;&#30495;&#23454;&#22270;&#20687;&#30340;&#27867;&#21270;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20135;&#29983;&#20102;&#26126;&#26174;&#26356;&#28165;&#26224;&#12289;&#26356;&#19968;&#33268;&#21644;&#26356;&#35814;&#32454;&#30340;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.12274v2 Announce Type: replace-cross  Abstract: We present Intrinsic Image Diffusion, a generative model for appearance decomposition of indoor scenes. Given a single input view, we sample multiple possible material explanations represented as albedo, roughness, and metallic maps. Appearance decomposition poses a considerable challenge in computer vision due to the inherent ambiguity between lighting and material properties and the lack of real datasets. To address this issue, we advocate for a probabilistic formulation, where instead of attempting to directly predict the true material properties, we employ a conditional generative model to sample from the solution space. Furthermore, we show that utilizing the strong learned prior of recent diffusion models trained on large-scale real-world images can be adapted to material estimation and highly improves the generalization to real images. Our method produces significantly sharper, more consistent, and more detailed material
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20860;&#39038;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2312.08977</link><description>&lt;p&gt;
&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#25345;&#32493;&#23398;&#20064;&#32773;
&lt;/p&gt;
&lt;p&gt;
Weighted Ensemble Models Are Strong Continual Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.08977
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#30340;&#25345;&#32493;&#23398;&#20064;&#65292;&#20860;&#39038;&#21487;&#22609;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#25345;&#32493;&#23398;&#20064;&#65288;CL&#65289;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#30446;&#26631;&#26159;&#20174;&#19968;&#31995;&#21015;&#20219;&#21153;&#20013;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#24471;&#20197;&#21069;&#20219;&#21153;&#30340;&#25968;&#25454;&#22312;&#23398;&#20064;&#24403;&#21069;&#20219;&#21153;&#25968;&#25454;&#26102;&#19981;&#21487;&#29992;&#12290;CL&#26412;&#36136;&#19978;&#26159;&#22312;&#33021;&#22815;&#23398;&#20064;&#26032;&#20219;&#21153;&#65288;&#21363;&#21487;&#22609;&#24615;&#65289;&#21644;&#20445;&#25345;&#20808;&#21069;&#23398;&#20064;&#27010;&#24565;&#30340;&#24615;&#33021;&#65288;&#21363;&#31283;&#23450;&#24615;&#65289;&#20043;&#38388;&#21462;&#24471;&#24179;&#34913;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#31283;&#23450;&#24615;-&#21487;&#22609;&#24615;&#30340;&#26435;&#34913;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23545;&#20808;&#21069;&#21644;&#24403;&#21069;&#20219;&#21153;&#30340;&#27169;&#22411;&#21442;&#25968;&#36827;&#34892;&#21152;&#26435;&#38598;&#25104;&#12290;&#36825;&#31181;&#21152;&#26435;&#38598;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25345;&#32493;&#27169;&#22411;&#24179;&#22343;&#65288;&#25110;CoMA&#65289;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#22609;&#24615;&#22312;&#24403;&#21069;&#20219;&#21153;&#19978;&#33719;&#24471;&#39640;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#19981;&#20250;&#20559;&#31163;&#22826;&#36828;&#30340;&#20808;&#21069;&#26435;&#37325;&#37197;&#32622;&#65292;&#20174;&#32780;&#30830;&#20445;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;CoMA&#30340;&#25913;&#36827;&#22411;&#21464;&#20307;&#65292;&#21517;&#20026;&#25345;&#32493;&#36153;&#33293;&#23572;&#21152;&#26435;&#27169;&#22411;&#24179;&#22343;&#65288;&#25110;CoFiMA&#65289;&#65292;&#35813;&#27169;&#22411;&#23545;&#27599;&#19968;&#20010;&#21442;&#25968;&#36827;&#34892;&#36873;&#25321;&#24615;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.08977v2 Announce Type: replace-cross  Abstract: In this work, we study the problem of continual learning (CL) where the goal is to learn a model on a sequence of tasks, such that the data from the previous tasks becomes unavailable while learning on the current task data. CL is essentially a balancing act between being able to learn on the new task (i.e., plasticity) and maintaining the performance on the previously learned concepts (i.e., stability). Intending to address the stability-plasticity trade-off, we propose to perform weight-ensembling of the model parameters of the previous and current tasks. This weighted-ensembled model, which we call Continual Model Averaging (or CoMA), attains high accuracy on the current task by leveraging plasticity, while not deviating too far from the previous weight configuration, ensuring stability. We also propose an improved variant of CoMA, named Continual Fisher-weighted Model Averaging (or CoFiMA), that selectively weighs each para
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20154;&#26426;&#22242;&#38431;&#21512;&#20316;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#21475;&#22836;&#20132;&#27969;&#20419;&#36827;&#26426;&#22120;&#20154;&#30340;&#21487;&#21464;&#33258;&#20027;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;GPT&#30340;&#22810;&#26426;&#22120;&#20154;&#27979;&#35797;&#21488;&#26550;&#29615;&#22659;&#65292;&#24182;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#20197;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#21644;&#29992;&#25143;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2312.07214</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#20419;&#36827;&#20154;&#26426;&#22242;&#38431;&#21512;&#20316;&#30340;&#21487;&#21464;&#33258;&#20027;&#24615;
&lt;/p&gt;
&lt;p&gt;
Exploring Large Language Models to Facilitate Variable Autonomy for Human-Robot Teaming
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07214
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20154;&#26426;&#22242;&#38431;&#21512;&#20316;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#21475;&#22836;&#20132;&#27969;&#20419;&#36827;&#26426;&#22120;&#20154;&#30340;&#21487;&#21464;&#33258;&#20027;&#24615;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;GPT&#30340;&#22810;&#26426;&#22120;&#20154;&#27979;&#35797;&#21488;&#26550;&#29615;&#22659;&#65292;&#24182;&#36827;&#34892;&#20102;&#29992;&#25143;&#30740;&#31350;&#20197;&#39564;&#35777;&#20854;&#26377;&#25928;&#24615;&#21644;&#29992;&#25143;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#24555;&#36895;&#21457;&#23637;&#30340;&#25968;&#23383;&#21270;&#29615;&#22659;&#20013;&#65292;&#33258;&#20027;&#24037;&#20855;&#21644;&#26426;&#22120;&#20154;&#27491;&#21464;&#24471;&#21496;&#31354;&#35265;&#24815;&#12290;&#37492;&#20110;&#36825;&#19968;&#21457;&#23637;&#30340;&#37325;&#35201;&#24615;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#38598;&#25104;&#21040;&#20154;&#26426;&#22242;&#38431;&#21512;&#20316;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#21475;&#22836;&#20154;&#26426;&#20132;&#27969;&#25163;&#27573;&#20419;&#36827;&#21487;&#21464;&#33258;&#20027;&#24615;&#12290;&#25105;&#20204;&#22312;Unity&#34394;&#25311;&#29616;&#23454;&#65288;VR&#65289;&#29615;&#22659;&#20013;&#65292;&#22522;&#20110;GPT&#26680;&#24515;&#20026;&#21160;&#21147;&#30340;&#22810;&#26426;&#22120;&#20154;&#27979;&#35797;&#21488;&#26550;&#29615;&#22659;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#12290;&#35813;&#31995;&#32479;&#20801;&#35768;&#29992;&#25143;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#26426;&#22120;&#20154;&#20195;&#29702;&#36827;&#34892;&#20132;&#20114;&#65292;&#27599;&#20010;&#20195;&#29702;&#37117;&#30001;&#29420;&#31435;&#30340;GPT&#26680;&#24515;&#25552;&#20379;&#21160;&#21147;&#12290;&#36890;&#36807;OpenAI&#30340;&#20989;&#25968;&#35843;&#29992;&#65292;&#25105;&#20204;&#24357;&#21512;&#20102;&#19981;&#21463;&#32467;&#26500;&#32422;&#26463;&#30340;&#33258;&#28982;&#35821;&#35328;&#36755;&#20837;&#21644;&#32467;&#26500;&#21270;&#26426;&#22120;&#20154;&#21160;&#20316;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#19968;&#39033;&#28041;&#21450;12&#21517;&#21442;&#19982;&#32773;&#30340;&#29992;&#25143;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-4&#30340;&#26377;&#25928;&#24615;&#65292;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#24403;&#32473;&#20104;&#26426;&#20250;&#36827;&#34892;&#33258;&#28982;&#23545;&#35805;&#26102;&#29992;&#25143;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07214v2 Announce Type: replace-cross  Abstract: In a rapidly evolving digital landscape autonomous tools and robots are becoming commonplace. Recognizing the significance of this development, this paper explores the integration of Large Language Models (LLMs) like Generative pre-trained transformer (GPT) into human-robot teaming environments to facilitate variable autonomy through the means of verbal human-robot communication. In this paper, we introduce a novel framework for such a GPT-powered multi-robot testbed environment, based on a Unity Virtual Reality (VR) setting. This system allows users to interact with robot agents through natural language, each powered by individual GPT cores. By means of OpenAI's function calling, we bridge the gap between unstructured natural language input and structure robot actions. A user study with 12 participants explores the effectiveness of GPT-4 and, more importantly, user strategies when being given the opportunity to converse in nat
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36870;&#21521;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#24182;&#29992;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#30452;&#25509;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#19979;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#30340;&#33258;&#20027;&#25910;&#38598;&#21644;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2312.02352</link><description>&lt;p&gt;
&#36870;&#21521;&#23398;&#20064;&#65306;&#36890;&#36807;&#25441;&#21462;&#23398;&#20064;&#25918;&#32622;
&lt;/p&gt;
&lt;p&gt;
Working Backwards: Learning to Place by Picking
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.02352
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36870;&#21521;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#30340;&#23545;&#31216;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#24182;&#29992;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#30452;&#25509;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#19979;&#29289;&#20307;&#25918;&#32622;&#20219;&#21153;&#30340;&#33258;&#20027;&#25910;&#38598;&#21644;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25342;&#21462;&#65288;PvP&#65289;&#30340;&#25918;&#32622;&#26041;&#27861;&#65292;&#21487;&#20197;&#33258;&#20027;&#25910;&#38598;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#25918;&#32622;&#20219;&#21153;&#30340;&#29616;&#23454;&#19990;&#30028;&#28436;&#31034;&#65292;&#20854;&#20013;&#29289;&#20307;&#24517;&#39035;&#34987;&#25805;&#32437;&#21040;&#29305;&#23450;&#30340;&#25509;&#35302;&#38480;&#21046;&#20301;&#32622;&#12290;&#36890;&#36807;PvP&#65292;&#25105;&#20204;&#36890;&#36807;&#39072;&#20498;&#25235;&#21462;&#36807;&#31243;&#24182;&#21033;&#29992;&#25342;&#21462;&#21644;&#25918;&#32622;&#38382;&#39064;&#22266;&#26377;&#30340;&#23545;&#31216;&#24615;&#65292;&#25509;&#36817;&#20110;&#26426;&#22120;&#20154;&#29289;&#20307;&#25918;&#32622;&#28436;&#31034;&#30340;&#25910;&#38598;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20174;&#19968;&#32452;&#26368;&#21021;&#20301;&#20110;&#30446;&#26631;&#25918;&#32622;&#20301;&#32622;&#30340;&#29289;&#20307;&#30340;&#25235;&#21462;&#24207;&#21015;&#20013;&#33719;&#24471;&#25918;&#32622;&#28436;&#31034;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#22312;&#25509;&#35302;&#21463;&#38480;&#29615;&#22659;&#20013;&#25910;&#38598;&#25968;&#30334;&#20010;&#28436;&#31034;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#65292;&#36825;&#26159;&#36890;&#36807;&#32467;&#21512;&#20004;&#20010;&#27169;&#22359;&#23454;&#29616;&#30340;&#65306;&#35302;&#35273;&#37325;&#26032;&#25235;&#21462;&#21644;&#29992;&#20110;&#25235;&#21462;&#30340;&#39034;&#20174;&#25511;&#21046;&#12290;&#25105;&#20204;&#36890;&#36807;&#34892;&#20026;&#20811;&#38534;&#30452;&#25509;&#20174;&#35270;&#35273;&#35266;&#23519;&#20013;&#36890;&#36807;&#33258;&#20027;&#25910;&#38598;&#30340;&#28436;&#31034;&#20013;&#35757;&#32451;&#31574;&#30053;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#31574;&#30053;&#21487;&#20197;&#25512;&#24191;&#21040;&#36229;&#20986;&#35757;&#32451;&#29615;&#22659;&#33539;&#22260;&#30340;&#29289;&#20307;&#25918;&#32622;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.02352v2 Announce Type: replace-cross  Abstract: We present placing via picking (PvP), a method to autonomously collect real-world demonstrations for a family of placing tasks in which objects must be manipulated to specific contact-constrained locations. With PvP, we approach the collection of robotic object placement demonstrations by reversing the grasping process and exploiting the inherent symmetry of the pick and place problems. Specifically, we obtain placing demonstrations from a set of grasp sequences of objects initially located at their target placement locations. Our system can collect hundreds of demonstrations in contact-constrained environments without human intervention by combining two modules: tactile regrasping and compliant control for grasps. We train a policy directly from visual observations through behavioral cloning, using the autonomously-collected demonstrations. By doing so, the policy can generalize to object placement scenarios outside of the tra
&lt;/p&gt;</description></item><item><title>RO-LMM&#26159;&#19968;&#20010;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Consistency Embedding Fine-Tuning&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20445;&#25345;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#21319;&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29992;&#20110;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#21644;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;</title><link>https://arxiv.org/abs/2311.15876</link><description>&lt;p&gt;
LMM&#36741;&#21161;&#30340;&#19968;&#33268;&#24615;&#23884;&#20837;&#19979;&#20083;&#33146;&#30284;&#27835;&#30103;&#30446;&#26631;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
LMM-Assisted Breast Cancer Treatment Target Segmentation with Consistency Embedding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15876
&lt;/p&gt;
&lt;p&gt;
RO-LMM&#26159;&#19968;&#20010;&#38024;&#23545;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;Consistency Embedding Fine-Tuning&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#20445;&#25345;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#33021;&#21147;&#30340;&#21516;&#26102;&#25552;&#21319;&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#29992;&#20110;&#25918;&#23556;&#27835;&#30103;&#35745;&#21010;&#21644;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#26368;&#26032;&#36827;&#23637;&#28145;&#21051;&#24433;&#21709;&#20102;&#21307;&#23398;&#39046;&#22495;&#65292;&#20026;&#38477;&#20302;&#20020;&#24202;&#24037;&#20316;&#37327;&#25552;&#20379;&#20102;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21463;&#38480;&#20110;&#25191;&#34892;&#21333;&#27169;&#24335;&#20219;&#21153;&#65292;&#19982;&#21307;&#23398;&#19987;&#19994;&#20154;&#21592;&#25152;&#20351;&#29992;&#30340;&#32508;&#21512;&#26041;&#27861;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;RO-LMM&#65292;&#19968;&#20010;&#19987;&#20026;&#25918;&#23556;&#32959;&#30244;&#23398;&#39046;&#22495;&#35774;&#35745;&#30340;&#22810;&#21151;&#33021;&#22823;&#22411;&#22810;&#27169;&#22411;&#65288;LMM&#65289;&#12290;&#35813;&#27169;&#22411;&#28085;&#30422;&#20102;&#20020;&#24202;&#24037;&#20316;&#27969;&#20013;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#25797;&#38271;&#20020;&#24202;&#25253;&#21578;&#25688;&#35201;&#12289;&#25918;&#30103;&#27835;&#30103;&#35745;&#21010;&#24314;&#35758;&#21644;&#35745;&#21010;&#24341;&#23548;&#30340;&#30446;&#26631;&#20307;&#31215;&#20998;&#21106;&#12290;&#20026;&#20102;&#25191;&#34892;&#36830;&#32493;&#30340;&#20020;&#24202;&#20219;&#21153;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#19968;&#33268;&#24615;&#23884;&#20837;&#24494;&#35843;&#65288;CEFTune&#65289;&#25216;&#26415;&#65292;&#25552;&#21319;&#20102;LMM&#23545;&#22024;&#26434;&#36755;&#20837;&#30340;&#40065;&#26834;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#22788;&#29702;&#24178;&#20928;&#36755;&#20837;&#30340;&#33021;&#21147;&#65292;&#24182;&#23558;&#35813;&#27010;&#24565;&#36716;&#21270;&#20026;LMM&#39537;&#21160;&#30340;&#20998;&#21106;&#26694;&#26550;&#65292;&#21363;&#19968;&#33268;&#24615;&#23884;&#20837;S&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15876v2 Announce Type: replace-cross  Abstract: Recent advancements in Artificial Intelligence (AI) have profoundly influenced medical fields, by providing tools to reduce clinical workloads. However, most AI models are constrained to execute unimodal tasks, in stark contrast to the comprehensive approaches utilized by medical professionals. To address this, here we present RO-LMM, a multi-purpose large multimodal model (LMM) tailored for the field of radiation oncology. This model covers series of tasks within clinical workflow, adept at clinical report summarization, radiation treatment plan suggestion, and plan-guided target volume segmentation. In particular, to perform consecutive clinical tasks, we further present a novel Consistency Embedding Fine-Tuning (CEFTune) technique, which boosts LMM's robustness to noisy inputs while preserving the capability of handling clean inputs, and transform this concept into LMM-driven segmentation framework as Consistency Embedding S
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#23545;&#40784;&#21069;&#33258;&#36866;&#24212;&#8221;&#65288;ALT&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#21040;&#21306;&#22495;&#23545;&#40784;&#65292;&#23558;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#26597;&#35810;&#65292;&#20174;&#32780;&#24110;&#21161;&#25552;&#21462;&#35270;&#39057;&#20013;&#26368;&#37325;&#35201;&#23454;&#20307;&#30340;&#35821;&#20041;&#12290;</title><link>https://arxiv.org/abs/2311.15619</link><description>&lt;p&gt;
&#22312;&#33258;&#36866;&#24212;&#20043;&#21069;&#23545;&#40784;&#65306;&#21033;&#29992;&#23454;&#20307;&#21040;&#21306;&#22495;&#23545;&#40784;&#36827;&#34892;&#36890;&#29992;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Align before Adapt: Leveraging Entity-to-Region Alignments for Generalizable Video Action Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.15619
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#23545;&#40784;&#21069;&#33258;&#36866;&#24212;&#8221;&#65288;ALT&#65289;&#33539;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#23454;&#20307;&#21040;&#21306;&#22495;&#23545;&#40784;&#65292;&#23558;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#26597;&#35810;&#65292;&#20174;&#32780;&#24110;&#21161;&#25552;&#21462;&#35270;&#39057;&#20013;&#26368;&#37325;&#35201;&#23454;&#20307;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#21508;&#31181;&#35270;&#39057;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#37325;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#36981;&#24490;&#8220;&#33258;&#36866;&#24212;&#28982;&#21518;&#23545;&#40784;&#8221;&#30340;&#33539;&#24335;&#65292;&#23558;&#39044;&#35757;&#32451;&#22270;&#20687;&#32534;&#30721;&#22120;&#35843;&#25972;&#20026;&#24314;&#27169;&#35270;&#39057;&#32423;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#21160;&#20316;&#26631;&#31614;&#30340;one-hot&#25110;&#25991;&#26412;&#23884;&#20837;&#36827;&#34892;&#30417;&#30563;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#23545;&#40784;&#21069;&#33258;&#36866;&#24212;&#8221;&#65288;ALT&#65289;&#33539;&#24335;&#12290;&#22312;&#36866;&#24212;&#21040;&#35270;&#39057;&#34920;&#31034;&#23398;&#20064;&#20043;&#21069;&#65292;&#25105;&#20204;&#21033;&#29992;&#20026;&#27599;&#19968;&#24103;&#23454;&#20307;&#21040;&#21306;&#22495;&#23545;&#40784;&#12290;&#36825;&#20123;&#23545;&#40784;&#36890;&#36807;&#23558;&#21306;&#22495;&#24863;&#30693;&#22270;&#20687;&#23884;&#20837;&#19982;&#31163;&#32447;&#26500;&#24314;&#30340;&#25991;&#26412;&#35821;&#26009;&#24211;&#36827;&#34892;&#21305;&#37197;&#26469;&#23454;&#29616;&#12290;&#26377;&#20102;&#23545;&#40784;&#30340;&#23454;&#20307;&#65292;&#25105;&#20204;&#23558;&#23427;&#20204;&#30340;&#25991;&#26412;&#23884;&#20837;&#20316;&#20026;&#26597;&#35810;&#39304;&#36865;&#21040;&#22522;&#20110;transformer&#30340;&#35270;&#39057;&#36866;&#37197;&#22120;&#20013;&#65292;&#21487;&#20197;&#24110;&#21161;&#20174;&#35270;&#39057;&#21040;&#21521;&#37327;&#25552;&#21462;&#26368;&#37325;&#35201;&#23454;&#20307;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.15619v2 Announce Type: replace-cross  Abstract: Large-scale visual-language pre-trained models have achieved significant success in various video tasks. However, most existing methods follow an "adapt then align" paradigm, which adapts pre-trained image encoders to model video-level representations and utilizes one-hot or text embedding of the action labels for supervision. This paradigm overlooks the challenge of mapping from static images to complicated activity concepts. In this paper, we propose a novel "Align before Adapt" (ALT) paradigm. Prior to adapting to video representation learning, we exploit the entity-to-region alignments for each frame. The alignments are fulfilled by matching the region-aware image embeddings to an offline-constructed text corpus. With the aligned entities, we feed their text embeddings to a transformer-based video adapter as the queries, which can help extract the semantics of the most important entities from a video to a vector. This parad
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Point2RBox&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#27169;&#24335;&#30693;&#35782;&#32452;&#21512;&#21644;&#21464;&#25442;&#33258;&#30417;&#30563;&#30340;&#21407;&#21017;&#65292;&#23454;&#29616;&#20102;&#21333;&#28857;&#30417;&#30563;&#30340;&#23450;&#21521;&#29289;&#20307;&#26816;&#27979;</title><link>https://arxiv.org/abs/2311.14758</link><description>&lt;p&gt;
Point2RBox: &#32467;&#21512;&#21512;&#25104;&#35270;&#35273;&#27169;&#24335;&#30340;&#30693;&#35782;&#29992;&#20110;&#31471;&#21040;&#31471;&#26377;&#21333;&#28857;&#30417;&#30563;&#30340;&#23450;&#21521;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Point2RBox: Combine Knowledge from Synthetic Visual Patterns for End-to-end Oriented Object Detection with Single Point Supervision
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.14758
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Point2RBox&#26041;&#27861;&#65292;&#36890;&#36807;&#21512;&#25104;&#27169;&#24335;&#30693;&#35782;&#32452;&#21512;&#21644;&#21464;&#25442;&#33258;&#30417;&#30563;&#30340;&#21407;&#21017;&#65292;&#23454;&#29616;&#20102;&#21333;&#28857;&#30417;&#30563;&#30340;&#23450;&#21521;&#29289;&#20307;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23545;&#23450;&#21521;&#29289;&#20307;&#26816;&#27979;&#65288;OOD&#65289;&#38656;&#27714;&#30340;&#36805;&#36895;&#22686;&#21152;&#65292;&#26368;&#36817;&#28041;&#21450;&#20174;&#27700;&#24179;&#26694;&#65288;HBox&#65289;&#23398;&#20064;&#26059;&#36716;&#26694;&#65288;RBox&#65289;&#30340;&#24369;&#30417;&#30563;&#26816;&#27979;&#22120;&#30340;&#30740;&#31350;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#20294;&#26631;&#31614;&#25928;&#29575;&#39640;&#30340;&#35774;&#32622;&#65292;&#21363;&#21333;&#28857;&#30417;&#30563;OOD&#65292;&#24182;&#25552;&#20986;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;Point2RBox&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#21033;&#29992;&#20004;&#20010;&#21407;&#21017;&#65306;1&#65289;&#21512;&#25104;&#27169;&#24335;&#30693;&#35782;&#32452;&#21512;&#65306;&#36890;&#36807;&#22312;&#22270;&#20687;&#19978;&#27599;&#20010;&#26631;&#35760;&#28857;&#21608;&#22260;&#37319;&#26679;&#65292;&#25105;&#20204;&#23558;&#23545;&#35937;&#29305;&#24449;&#20256;&#25773;&#21040;&#24050;&#30693;&#26694;&#30340;&#21512;&#25104;&#35270;&#35273;&#27169;&#24335;&#20013;&#65292;&#20197;&#25552;&#20379;&#31665;&#23376;&#22238;&#24402;&#30340;&#30693;&#35782;&#12290;2&#65289;&#21464;&#25442;&#33258;&#30417;&#30563;&#65306;&#36890;&#36807;&#19968;&#20010;&#21464;&#25442;&#30340;&#36755;&#20837;&#22270;&#20687;&#65288;&#20363;&#22914;&#32553;&#25918;/&#26059;&#36716;&#65289;&#65292;&#36755;&#20986;&#30340;RBoxes&#34987;&#35757;&#32451;&#20197;&#36981;&#24490;&#30456;&#21516;&#30340;&#21464;&#25442;&#65292;&#20174;&#32780;&#20351;&#32593;&#32476;&#33021;&#22815;&#24863;&#30693;&#23545;&#35937;&#20043;&#38388;&#30340;&#30456;&#23545;&#22823;&#23567;/&#26059;&#36716;&#12290;&#26816;&#27979;&#22120;&#36827;&#19968;&#27493;&#36890;&#36807;&#20960;&#31181;&#35774;&#35745;&#30340;&#25216;&#26415;&#22686;&#24378;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.14758v2 Announce Type: replace-cross  Abstract: With the rapidly increasing demand for oriented object detection (OOD), recent research involving weakly-supervised detectors for learning rotated box (RBox) from the horizontal box (HBox) has attracted more and more attention. In this paper, we explore a more challenging yet label-efficient setting, namely single point-supervised OOD, and present our approach called Point2RBox. Specifically, we propose to leverage two principles: 1) Synthetic pattern knowledge combination: By sampling around each labeled point on the image, we spread the object feature to synthetic visual patterns with known boxes to provide the knowledge for box regression. 2) Transform self-supervision: With a transformed input image (e.g. scaled/rotated), the output RBoxes are trained to follow the same transformation so that the network can perceive the relative size/rotation between objects. The detector is further enhanced by a few devised techniques to 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#21517;&#20026;DROC&#65292;&#33021;&#22815;&#22238;&#24212;&#20219;&#24847;&#24418;&#24335;&#30340;&#35821;&#35328;&#21453;&#39304;&#65292;&#20174;&#32416;&#27491;&#20013;&#25552;&#28860;&#20986;&#36890;&#29992;&#30693;&#35782;&#65292;&#24182;&#26681;&#25454;&#25991;&#26412;&#21644;&#35270;&#35273;&#30456;&#20284;&#24615;&#26816;&#32034;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#65292;&#20197;&#25913;&#36827;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2311.10678</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#32416;&#27491;&#25552;&#28860;&#21644;&#26816;&#32034;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#36890;&#29992;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Distilling and Retrieving Generalizable Knowledge for Robot Manipulation via Language Corrections
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.10678
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31995;&#32479;&#65292;&#21517;&#20026;DROC&#65292;&#33021;&#22815;&#22238;&#24212;&#20219;&#24847;&#24418;&#24335;&#30340;&#35821;&#35328;&#21453;&#39304;&#65292;&#20174;&#32416;&#27491;&#20013;&#25552;&#28860;&#20986;&#36890;&#29992;&#30693;&#35782;&#65292;&#24182;&#26681;&#25454;&#25991;&#26412;&#21644;&#35270;&#35273;&#30456;&#20284;&#24615;&#26816;&#32034;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#65292;&#20197;&#25913;&#36827;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20170;&#22825;&#30340;&#26426;&#22120;&#20154;&#25919;&#31574;&#22312;&#38754;&#23545;&#25512;&#24191;&#21040;&#26032;&#29615;&#22659;&#30340;&#25361;&#25112;&#26102;&#34920;&#29616;&#19981;&#20339;&#12290;&#20154;&#31867;&#30340;&#32416;&#27491;&#21453;&#39304;&#26159;&#19968;&#31181;&#33267;&#20851;&#37325;&#35201;&#30340;&#25351;&#23548;&#24418;&#24335;&#65292;&#21487;&#20197;&#23454;&#29616;&#36825;&#31181;&#27867;&#21270;&#12290;&#28982;&#32780;&#65292;&#36866;&#24212;&#21644;&#20174;&#22312;&#32447;&#20154;&#31867;&#32416;&#27491;&#20013;&#23398;&#20064;&#26159;&#19968;&#39033;&#19981;&#23481;&#26131;&#30340;&#20219;&#21153;&#65306;&#26426;&#22120;&#20154;&#19981;&#20165;&#38656;&#35201;&#38543;&#26102;&#38388;&#35760;&#20303;&#20154;&#31867;&#30340;&#21453;&#39304;&#20197;&#20415;&#22312;&#26032;&#29615;&#22659;&#20013;&#26816;&#32034;&#27491;&#30830;&#30340;&#20449;&#24687;&#24182;&#38477;&#20302;&#24178;&#28041;&#29575;&#65292;&#32780;&#19988;&#36824;&#38656;&#35201;&#33021;&#22815;&#22238;&#24212;&#21487;&#33021;&#26159;&#20851;&#20110;&#39640;&#32423;&#20154;&#31867;&#20559;&#22909;&#30340;&#20219;&#24847;&#32416;&#27491;&#21040;&#25216;&#33021;&#21442;&#25968;&#30340;&#20302;&#32423;&#35843;&#25972;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#22312;&#32447;&#32416;&#27491;&#33976;&#39311;&#21644;&#26816;&#32034;(DROC)&#31995;&#32479;&#65292;&#23427;&#21487;&#20197;&#22238;&#24212;&#20219;&#24847;&#24418;&#24335;&#30340;&#35821;&#35328;&#21453;&#39304;&#65292;&#20174;&#32416;&#27491;&#20013;&#25552;&#28860;&#20986;&#36890;&#29992;&#30693;&#35782;&#65292;&#24182;&#26681;&#25454;&#25991;&#26412;&#21644;&#35270;&#35273;&#30456;&#20284;&#24615;&#26816;&#32034;&#30456;&#20851;&#30340;&#36807;&#21435;&#32463;&#39564;&#65292;&#20197;&#25913;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.10678v2 Announce Type: replace-cross  Abstract: Today's robot policies exhibit subpar performance when faced with the challenge of generalizing to novel environments. Human corrective feedback is a crucial form of guidance to enable such generalization. However, adapting to and learning from online human corrections is a non-trivial endeavor: not only do robots need to remember human feedback over time to retrieve the right information in new settings and reduce the intervention rate, but also they would need to be able to respond to feedback that can be arbitrary corrections about high-level human preferences to low-level adjustments to skill parameters. In this work, we present Distillation and Retrieval of Online Corrections (DROC), a large language model (LLM)-based system that can respond to arbitrary forms of language feedback, distill generalizable knowledge from corrections, and retrieve relevant past experiences based on textual and visual similarity for improving p
&lt;/p&gt;</description></item><item><title>TD-MPC2&#26159;&#23545;TD-MPC&#31639;&#27861;&#30340;&#19968;&#31995;&#21015;&#25913;&#36827;&#65292;&#36890;&#36807;&#19968;&#32452;&#36229;&#21442;&#25968;&#22312;104&#20010;&#22312;&#32447;RL&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25104;&#21151;&#35757;&#32451;&#21333;&#19968;&#30340;317M&#21442;&#25968;&#20195;&#29702;&#25191;&#34892;80&#20010;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2310.16828</link><description>&lt;p&gt;
TD-MPC2&#65306;&#21487;&#25193;&#23637;&#12289;&#31283;&#20581;&#30340;&#36830;&#32493;&#25511;&#21046;&#19990;&#30028;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TD-MPC2: Scalable, Robust World Models for Continuous Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.16828
&lt;/p&gt;
&lt;p&gt;
TD-MPC2&#26159;&#23545;TD-MPC&#31639;&#27861;&#30340;&#19968;&#31995;&#21015;&#25913;&#36827;&#65292;&#36890;&#36807;&#19968;&#32452;&#36229;&#21442;&#25968;&#22312;104&#20010;&#22312;&#32447;RL&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#25104;&#21151;&#35757;&#32451;&#21333;&#19968;&#30340;317M&#21442;&#25968;&#20195;&#29702;&#25191;&#34892;80&#20010;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
TD-MPC&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#31639;&#27861;&#65292;&#23427;&#22312;&#23398;&#20064;&#30340;&#38544;&#24335;&#65288;&#26080;&#35299;&#30721;&#22120;&#65289;&#19990;&#30028;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25191;&#34892;&#23616;&#37096;&#36712;&#36857;&#20248;&#21270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;TD-MPC2&#65306;&#23545;TD-MPC&#31639;&#27861;&#30340;&#19968;&#31995;&#21015;&#25913;&#36827;&#12290;&#25105;&#20204;&#35777;&#26126;TD-MPC2&#22312;&#36328;&#36234;4&#20010;&#19981;&#21516;&#20219;&#21153;&#39046;&#22495;&#30340;104&#20010;&#22312;&#32447;RL&#20219;&#21153;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#22522;&#32447;&#65292;&#36890;&#36807;&#19968;&#32452;&#36229;&#21442;&#25968;&#23454;&#29616;&#20102;&#25345;&#32493;&#24378;&#22823;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#23637;&#31034;&#20102;&#38543;&#30528;&#27169;&#22411;&#21644;&#25968;&#25454;&#35268;&#27169;&#30340;&#22686;&#21152;&#65292;&#20195;&#29702;&#30340;&#33021;&#21147;&#20063;&#22312;&#22686;&#24378;&#65292;&#24182;&#25104;&#21151;&#35757;&#32451;&#20102;&#19968;&#20010;&#21333;&#19968;&#30340;317M&#21442;&#25968;&#20195;&#29702;&#65292;&#22312;&#22810;&#20010;&#20219;&#21153;&#39046;&#22495;&#12289;&#20855;&#35937;&#21644;&#21160;&#20316;&#31354;&#38388;&#20013;&#25191;&#34892;&#20102;80&#20010;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;&#22823;&#22411;TD-MPC2&#20195;&#29702;&#25152;&#28041;&#21450;&#30340;&#25945;&#35757;&#12289;&#26426;&#20250;&#21644;&#39118;&#38505;&#12290;&#22312;https://tdmpc2.com&#19978;&#25506;&#32034;&#35270;&#39057;&#12289;&#27169;&#22411;&#12289;&#25968;&#25454;&#12289;&#20195;&#30721;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.16828v2 Announce Type: replace-cross  Abstract: TD-MPC is a model-based reinforcement learning (RL) algorithm that performs local trajectory optimization in the latent space of a learned implicit (decoder-free) world model. In this work, we present TD-MPC2: a series of improvements upon the TD-MPC algorithm. We demonstrate that TD-MPC2 improves significantly over baselines across 104 online RL tasks spanning 4 diverse task domains, achieving consistently strong results with a single set of hyperparameters. We further show that agent capabilities increase with model and data size, and successfully train a single 317M parameter agent to perform 80 tasks across multiple task domains, embodiments, and action spaces. We conclude with an account of lessons, opportunities, and risks associated with large TD-MPC2 agents. Explore videos, models, data, code, and more at https://tdmpc2.com
&lt;/p&gt;</description></item><item><title>&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#20316;&#20026;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;&#22270;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#20943;&#36731;&#20551;&#38452;&#24615;&#24433;&#21709;&#30340;&#26497;&#20854;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2310.14525</link><description>&lt;p&gt;
&#22270;&#25490;&#21517;&#23545;&#27604;&#23398;&#20064;&#65306;&#19968;&#31181;&#26497;&#20854;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Ranking Contrastive Learning: A Extremely Simple yet Efficient Method
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14525
&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#20316;&#20026;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;&#22270;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26088;&#22312;&#20943;&#36731;&#20551;&#38452;&#24615;&#24433;&#21709;&#30340;&#26497;&#20854;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#23545;&#27604;&#23398;&#20064;&#65288;GCL&#65289;&#20316;&#20026;&#19968;&#31181;&#20195;&#34920;&#24615;&#30340;&#22270;&#33258;&#30417;&#30563;&#26041;&#27861;&#24050;&#32463;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#30446;&#21069;&#26222;&#36941;&#37319;&#29992;&#30340;GCL&#20248;&#21270;&#30446;&#26631;&#26159;InfoNCE&#12290;&#36890;&#24120;&#65292;&#23427;&#37319;&#29992;&#22686;&#24378;&#25216;&#26415;&#33719;&#24471;&#20004;&#20010;&#35270;&#22270;&#65292;&#20854;&#20013;&#19968;&#20010;&#35270;&#22270;&#20013;&#30340;&#33410;&#28857;&#20805;&#24403;&#38170;&#28857;&#65292;&#21478;&#19968;&#20010;&#35270;&#22270;&#20013;&#30340;&#23545;&#24212;&#33410;&#28857;&#20805;&#24403;&#27491;&#26679;&#26412;&#65292;&#25152;&#26377;&#20854;&#20182;&#33410;&#28857;&#34987;&#35270;&#20026;&#36127;&#26679;&#26412;&#12290;&#20854;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#38170;&#28857;&#19982;&#27491;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#24182;&#26368;&#22823;&#21270;&#21040;&#36127;&#26679;&#26412;&#30340;&#36317;&#31163;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#30001;&#20110;&#32570;&#20047;&#26631;&#31614;&#20449;&#24687;&#65292;InfoNCE&#24517;&#28982;&#23558;&#26469;&#33258;&#30456;&#21516;&#31867;&#21035;&#30340;&#26679;&#26412;&#35270;&#20026;&#36127;&#26679;&#26412;&#65292;&#23548;&#33268;&#20551;&#36127;&#26679;&#26412;&#38382;&#39064;&#12290;&#36825;&#21487;&#33021;&#25439;&#23475;&#23398;&#21040;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#24182;&#38543;&#21518;&#38459;&#30861;&#19979;&#28216;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#20943;&#36731;&#20551;&#38452;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14525v2 Announce Type: replace-cross  Abstract: Graph contrastive learning (GCL) has emerged as a representative graph self-supervised method, achieving significant success. The currently prevalent optimization objective for GCL is InfoNCE. Typically, it employs augmentation techniques to obtain two views, where a node in one view acts as the anchor, the corresponding node in the other view serves as the positive sample, and all other nodes are regarded as negative samples. The goal is to minimize the distance between the anchor node and positive samples and maximize the distance to negative samples. However, due to the lack of label information during training, InfoNCE inevitably treats samples from the same class as negative samples, leading to the issue of false negative samples. This can impair the learned node representations and subsequently hinder performance in downstream tasks. While numerous methods have been proposed to mitigate the impact of false negatives, they
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;AI&#30340;&#19977;&#32500;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#20013;&#35782;&#21035;&#22823;&#33041;&#27807;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2309.00903</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#19977;&#32500;&#26694;&#26550;&#25581;&#31034;&#23398;&#20064;&#27169;&#24335;&#65306;&#21464;&#37327;&#33041;&#27807;&#35782;&#21035;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
An explainable three dimension framework to uncover learning patterns: A unified look in variable sulci recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.00903
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#21487;&#35299;&#37322;AI&#30340;&#19977;&#32500;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#20013;&#35782;&#21035;&#22823;&#33041;&#27807;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#25361;&#25112;&#24615;&#30340;&#31070;&#32463;&#31185;&#23398;&#39046;&#22495;&#37324;&#65292;&#35270;&#35273;&#20027;&#39064;&#22312;&#19977;&#32500;&#31354;&#38388;&#20869;&#34920;&#29616;&#20986;&#39640;&#24230;&#22797;&#26434;&#24615;&#12290;&#31070;&#32463;&#31185;&#23398;&#30340;&#24212;&#29992;&#28041;&#21450;&#20174;MRI&#20013;&#35782;&#21035;&#22823;&#33041;&#27807;&#29305;&#24449;&#65292;&#30001;&#20110;&#19987;&#23478;&#20043;&#38388;&#30340;&#26631;&#27880;&#35268;&#31243;&#23384;&#22312;&#24046;&#24322;&#21644;&#22823;&#33041;&#22797;&#26434;&#30340;&#19977;&#32500;&#21151;&#33021;&#65292;&#25105;&#20204;&#38754;&#20020;&#30528;&#37325;&#22823;&#38556;&#30861;&#12290;&#22240;&#27492;&#65292;&#20256;&#32479;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#22312;&#26377;&#25928;&#39564;&#35777;&#21644;&#35780;&#20272;&#36825;&#20123;&#32593;&#32476;&#26041;&#38754;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#25968;&#23398;&#20844;&#24335;&#65292;&#32454;&#21270;&#20102;&#19981;&#21516;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#20013;&#35299;&#37322;&#38656;&#27714;&#30340;&#21508;&#31181;&#31867;&#21035;&#65292;&#20998;&#20026;&#33258;&#35299;&#37322;&#12289;&#21322;&#35299;&#37322;&#12289;&#38750;&#35299;&#37322;&#21644;&#22522;&#20110;&#39564;&#35777;&#21327;&#35758;&#21487;&#38752;&#24615;&#30340;&#26032;&#27169;&#24335;&#23398;&#20064;&#24212;&#29992;&#12290;&#26681;&#25454;&#36825;&#20010;&#25968;&#23398;&#20844;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26088;&#22312;&#35299;&#37322;&#19977;&#32500;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.00903v2 Announce Type: replace-cross  Abstract: Explainable AI is crucial in medical imaging. In the challenging field of neuroscience, visual topics present a high level of complexity, particularly within three-dimensional space. The application of neuroscience, which involves identifying brain sulcal features from MRI, faces significant hurdles due to varying annotation protocols among experts and the intricate three-dimension functionality of the brain. Consequently, traditional explainability approaches fall short in effectively validating and evaluating these networks. To address this, we first present a mathematical formulation delineating various categories of explanation needs across diverse computer vision tasks, categorized into self-explanatory, semi-explanatory, non-explanatory, and new-pattern learning applications based on the reliability of the validation protocol. With respect to this mathematical formulation, we propose a 3D explainability framework aimed at
&lt;/p&gt;</description></item><item><title>RecMind&#26159;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#25512;&#33616;&#20195;&#29702;&#65292;&#36890;&#36807;Self-Inspiring&#31639;&#27861;&#25552;&#39640;&#20102;&#35268;&#21010;&#33021;&#21147;&#65292;&#33021;&#22815;&#20026;&#38646;-shot&#20010;&#24615;&#21270;&#25512;&#33616;&#25552;&#20379;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2308.14296</link><description>&lt;p&gt;
RecMind&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25512;&#33616;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
RecMind: Large Language Model Powered Agent For Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.14296
&lt;/p&gt;
&lt;p&gt;
RecMind&#26159;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#25512;&#33616;&#20195;&#29702;&#65292;&#36890;&#36807;Self-Inspiring&#31639;&#27861;&#25552;&#39640;&#20102;&#35268;&#21010;&#33021;&#21147;&#65292;&#33021;&#22815;&#20026;&#38646;-shot&#20010;&#24615;&#21270;&#25512;&#33616;&#25552;&#20379;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#24403;&#21069;RS&#26041;&#27861;&#36890;&#24120;&#22312;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#26032;&#25512;&#33616;&#20219;&#21153;&#30340;&#27867;&#21270;&#33021;&#21147;&#20197;&#21450;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#30340;&#33021;&#21147;&#65292;&#22240;&#20026;&#27169;&#22411;&#35268;&#27169;&#21644;&#25968;&#25454;&#22823;&#23567;&#30340;&#38480;&#21046;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;LLM&#39537;&#21160;&#30340;&#33258;&#20027;&#25512;&#33616;&#20195;&#29702;RecMind&#65292;&#33021;&#22815;&#21033;&#29992;&#22806;&#37096;&#30693;&#35782;&#65292;&#21033;&#29992;&#35880;&#24910;&#35268;&#21010;&#30340;&#24037;&#20855;&#20026;&#38646;-shot&#20010;&#24615;&#21270;&#25512;&#33616;&#25552;&#20379;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Self-Inspiring&#31639;&#27861;&#26469;&#25552;&#39640;&#35268;&#21010;&#33021;&#21147;&#12290;&#22312;&#27599;&#20010;&#20013;&#38388;&#27493;&#39588;&#65292;LLM&#33258;&#25105;&#28608;&#21169;&#20197;&#32771;&#34385;&#25152;&#26377;&#20808;&#21069;&#25506;&#32034;&#36807;&#30340;&#29366;&#24577;&#26469;&#35268;&#21010;&#19979;&#19968;&#27493;&#12290;&#36825;&#19968;&#26426;&#21046;&#26497;&#22823;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#29702;&#35299;&#21644;&#21033;&#29992;&#21382;&#21490;&#20449;&#24687;&#35268;&#21010;&#25512;&#33616;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;RecMind&#22312;&#21508;&#31181;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.14296v2 Announce Type: replace-cross  Abstract: While the recommendation system (RS) has advanced significantly through deep learning, current RS approaches usually train and fine-tune models on task-specific datasets, limiting their generalizability to new recommendation tasks and their ability to leverage external knowledge due to model scale and data size constraints. Thus, we designed an LLM-powered autonomous recommender agent, RecMind, which is capable of leveraging external knowledge, utilizing tools with careful planning to provide zero-shot personalized recommendations. We propose a Self-Inspiring algorithm to improve the planning ability. At each intermediate step, the LLM self-inspires to consider all previously explored states to plan for the next step. This mechanism greatly improves the model's ability to comprehend and utilize historical information in planning for recommendation. We evaluate RecMind's performance in various recommendation scenarios. Our exper
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2306.02090</link><description>&lt;p&gt;
&#27809;&#26377;&#25968;&#25454;&#35775;&#38382;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Deep Classifier Mimicry without Data Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02090
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#35775;&#38382;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26631;&#20934;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21487;&#33021;&#26080;&#27861;&#31561;&#21516;&#22320;&#33719;&#24471;&#27169;&#22411;&#35757;&#32451;&#25152;&#38656;&#30340;&#21407;&#22987;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#24494;&#35843;&#12289;&#21387;&#32553;&#27169;&#22411;&#12289;&#25345;&#32493;&#35843;&#25972;&#25110;&#36827;&#34892;&#20219;&#20309;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26356;&#26032;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#21487;&#33021;&#26080;&#38656;&#21407;&#22987;&#25968;&#25454;&#35775;&#38382;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25512;&#29702;&#30693;&#35782;&#25552;&#21462;&#65288;CAKE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#32780;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;CAKE&#29983;&#25104;&#19968;&#23545;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#23558;&#23427;&#20204;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#26550;&#26500;&#36873;&#25321;&#22312;&#23454;&#35777;&#19978;&#35777;&#23454;&#20102;CAKE&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.02090v2 Announce Type: replace-cross  Abstract: Access to pre-trained models has recently emerged as a standard across numerous machine learning domains. Unfortunately, access to the original data the models were trained on may not equally be granted. This makes it tremendously challenging to fine-tune, compress models, adapt continually, or to do any other type of data-driven update. We posit that original data access may however not be required. Specifically, we propose Contrastive Abductive Knowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure that mimics deep classifiers without access to the original data. To this end, CAKE generates pairs of noisy synthetic samples and diffuses them contrastively toward a model's decision boundary. We empirically corroborate CAKE's effectiveness using several benchmark datasets and various architectural choices, paving the way for broad application.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#27744;&#21644;&#26500;&#24314;&#22522;&#20110;&#23454;&#20363;&#30340;&#25552;&#31034;&#20197;&#21450;&#24341;&#20837;&#26032;&#39062;&#30340;&#36719;&#35821;&#35328;&#21270;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#20195;&#34920;&#24615;&#35821;&#35328;&#21270;&#22120;&#23454;&#29616;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2306.00618</link><description>&lt;p&gt;
&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#20195;&#34920;&#24615;&#35821;&#35328;&#21270;&#22120;&#23454;&#29616;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Effective Structured Prompting by Meta-Learning and Representative Verbalizer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.00618
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#25552;&#31034;&#27744;&#21644;&#26500;&#24314;&#22522;&#20110;&#23454;&#20363;&#30340;&#25552;&#31034;&#20197;&#21450;&#24341;&#20837;&#26032;&#39062;&#30340;&#36719;&#35821;&#35328;&#21270;&#22120;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20803;&#23398;&#20064;&#21644;&#20195;&#34920;&#24615;&#35821;&#35328;&#21270;&#22120;&#23454;&#29616;&#26377;&#25928;&#30340;&#32467;&#26500;&#21270;&#25552;&#31034;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#36974;&#34109;&#35821;&#35328;&#27169;&#22411;&#65288;MLM&#65289;&#30340;&#25552;&#31034;&#35843;&#25972;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#26377;&#38480;&#26631;&#35760;&#31034;&#20363;&#30340;&#33391;&#22909;&#24615;&#33021;&#12290;&#23427;&#20026;&#19979;&#28216;&#20219;&#21153;&#35843;&#25972;&#25552;&#31034;&#65292;&#24182;&#20351;&#29992;&#35821;&#35328;&#21270;&#22120;&#26469;&#36830;&#25509;&#39044;&#27979;&#30340;&#26631;&#35760;&#21644;&#26631;&#31614;&#39044;&#27979;&#12290;&#30001;&#20110;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#65292;&#25552;&#31034;&#21021;&#22987;&#21270;&#23545;&#20110;&#25552;&#31034;&#35843;&#25972;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;MetaPrompting&#65288;Hou&#31561;&#65292;2022&#65289;&#20351;&#29992;&#20803;&#23398;&#20064;&#26469;&#23398;&#20064;&#25152;&#26377;&#29305;&#23450;&#20219;&#21153;&#25552;&#31034;&#30340;&#20849;&#20139;&#21021;&#22987;&#21270;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#22797;&#26434;&#26102;&#65292;&#21333;&#19968;&#21021;&#22987;&#21270;&#26080;&#27861;&#33719;&#24471;&#25152;&#26377;&#20219;&#21153;&#21644;&#26679;&#26412;&#30340;&#33391;&#22909;&#25552;&#31034;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;MLM&#36890;&#24120;&#24456;&#22823;&#65292;MetaPrompting&#38656;&#35201;&#35843;&#25972;&#25972;&#20010;MLM&#65292;&#23548;&#33268;&#35745;&#31639;&#21644;&#20869;&#23384;&#36127;&#25285;&#27785;&#37325;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#25552;&#31034;&#27744;&#25552;&#21462;&#26356;&#22810;&#20219;&#21153;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#27880;&#24847;&#21147;&#26500;&#24314;&#22522;&#20110;&#23454;&#20363;&#30340;&#25552;&#31034;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36719;&#35821;&#35328;&#21270;&#22120;&#65288;RepVerb&#65289;...&#65288;&#21097;&#20313;&#20869;&#23481;&#26410;&#25552;&#20379;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.00618v2 Announce Type: replace-cross  Abstract: Prompt tuning for pre-trained masked language models (MLM) has shown promising performance in natural language processing tasks with few labeled examples. It tunes a prompt for the downstream task, and a verbalizer is used to bridge the predicted token and label prediction. Due to the limited training data, prompt initialization is crucial for prompt tuning. Recently, MetaPrompting (Hou et al., 2022) uses meta-learning to learn a shared initialization for all task-specific prompts. However, a single initialization is insufficient to obtain good prompts for all tasks and samples when the tasks are complex. Moreover, MetaPrompting requires tuning the whole MLM, causing a heavy burden on computation and memory as the MLM is usually large. To address these issues, we use a prompt pool to extract more task knowledge and construct instance-dependent prompts via attention. We further propose a novel soft verbalizer (RepVerb) which con
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2302.03788</link><description>&lt;p&gt;
&#38754;&#21521;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#22240;&#26524;&#35770;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Toward a Theory of Causation for Interpreting Neural Code Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.03788
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#37322;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#26088;&#22312;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Language Models of Code&#65292;&#25110;&#32773;&#31216;&#20026;&#31070;&#32463;&#20195;&#30721;&#27169;&#22411;&#65288;NCMs&#65289;&#65292;&#27491;&#22312;&#36805;&#36895;&#20174;&#30740;&#31350;&#21407;&#22411;&#21457;&#23637;&#20026;&#21830;&#19994;&#24320;&#21457;&#32773;&#24037;&#20855;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#33021;&#21147;&#36890;&#24120;&#26159;&#20351;&#29992;&#33258;&#21160;&#21270;&#25351;&#26631;&#26469;&#34913;&#37327;&#30340;&#65292;&#36825;&#20123;&#25351;&#26631;&#36890;&#24120;&#21482;&#33021;&#25581;&#31034;&#23427;&#20204;&#30495;&#23454;&#24615;&#33021;&#30340;&#19968;&#37096;&#20998;&#12290;&#19968;&#33324;&#26469;&#35828;&#65292;NCMs&#30340;&#24615;&#33021;&#20284;&#20046;&#24456;&#26377;&#21069;&#36884;&#65292;&#20294;&#30446;&#21069;&#20851;&#20110;&#36825;&#20123;&#27169;&#22411;&#22914;&#20309;&#20570;&#20986;&#20915;&#31574;&#20173;&#26377;&#24456;&#22810;&#26410;&#30693;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;$do_{code}$&#30340;&#21518;&#39564;&#35299;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#19987;&#38376;&#38024;&#23545;NCMs&#65292;&#33021;&#22815;&#35299;&#37322;&#27169;&#22411;&#30340;&#39044;&#27979;&#12290;$do_{code}$&#22522;&#20110;&#22240;&#26524;&#25512;&#26029;&#65292;&#20197;&#23454;&#29616;&#38754;&#21521;&#32534;&#31243;&#35821;&#35328;&#30340;&#35299;&#37322;&#12290;&#34429;&#28982;$do_{code}$&#30340;&#29702;&#35770;&#22522;&#30784;&#21487;&#25193;&#23637;&#21040;&#25506;&#32034;&#19981;&#21516;&#30340;&#27169;&#22411;&#23646;&#24615;&#65292;&#20294;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20855;&#20307;&#30340;&#23454;&#20363;&#65292;&#26088;&#22312;&#20943;&#23569;&#24433;&#21709;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.03788v2 Announce Type: replace-cross  Abstract: Neural Language Models of Code, or Neural Code Models (NCMs), are rapidly progressing from research prototypes to commercial developer tools. As such, understanding the capabilities and limitations of such models is becoming critical. However, the abilities of these models are typically measured using automated metrics that often only reveal a portion of their real-world performance. While, in general, the performance of NCMs appears promising, currently much is unknown about how such models arrive at decisions. To this end, this paper introduces $do_{code}$, a post hoc interpretability method specific to NCMs that is capable of explaining model predictions. $do_{code}$ is based upon causal inference to enable programming language-oriented explanations. While the theoretical underpinnings of $do_{code}$ are extensible to exploring different model properties, we provide a concrete instantiation that aims to mitigate the impact o
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Antigone&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#25968;&#25454;&#25110;&#39564;&#35777;&#25968;&#25454;&#20013;&#37117;&#26080;&#38656;&#35775;&#38382;&#25935;&#24863;&#23646;&#24615;&#21363;&#21487;&#35757;&#32451;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#20266;&#25935;&#24863;&#23646;&#24615;&#26469;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2302.01385</link><description>&lt;p&gt;
&#26080;&#38656;&#35775;&#38382;&#25935;&#24863;&#23646;&#24615;&#30340;&#20844;&#24179;&#20998;&#31867;&#36229;&#21442;&#25968;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Hyper-parameter Tuning for Fair Classification without Sensitive Attribute Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2302.01385
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Antigone&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#25968;&#25454;&#25110;&#39564;&#35777;&#25968;&#25454;&#20013;&#37117;&#26080;&#38656;&#35775;&#38382;&#25935;&#24863;&#23646;&#24615;&#21363;&#21487;&#35757;&#32451;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#20266;&#25935;&#24863;&#23646;&#24615;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26088;&#22312;&#35757;&#32451;&#33021;&#22815;&#22312;&#22522;&#20110;&#31181;&#26063;&#21644;&#24615;&#21035;&#31561;&#25935;&#24863;&#23646;&#24615;&#23450;&#20041;&#30340;&#20154;&#21475;&#32479;&#35745;&#20122;&#32452;&#20043;&#38388;&#24179;&#34913;&#27169;&#22411;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#36890;&#24120;&#20551;&#23450;&#25935;&#24863;&#23646;&#24615;&#24050;&#30693;&#65292;&#20294;&#30001;&#20110;&#38544;&#31169;&#21644;&#20854;&#20182;&#21518;&#21220;&#26041;&#38754;&#30340;&#32771;&#34385;&#65292;&#23454;&#36341;&#20013;&#21487;&#33021;&#26080;&#27861;&#33719;&#21462;&#36825;&#20123;&#23646;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#33268;&#21147;&#20110;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#27809;&#26377;&#25935;&#24863;&#23646;&#24615;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#20844;&#24179;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#36827;&#34892;&#22823;&#37327;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#25165;&#33021;&#33719;&#24471;&#33391;&#22909;&#32467;&#26524;&#65292;&#22240;&#27492;&#20551;&#35774;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#24050;&#30693;&#25935;&#24863;&#23646;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#20551;&#35774;&#22312;&#23454;&#36341;&#20013;&#20063;&#21487;&#33021;&#19981;&#20999;&#23454;&#38469;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Antigone&#65292;&#36825;&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#35757;&#32451;&#25968;&#25454;&#25110;&#39564;&#35777;&#25968;&#25454;&#20013;&#37117;&#26080;&#38656;&#35775;&#38382;&#25935;&#24863;&#23646;&#24615;&#21363;&#21487;&#35757;&#32451;&#20844;&#24179;&#30340;&#20998;&#31867;&#22120;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36890;&#36807;&#35757;&#32451;&#26377;&#20559;&#35265;&#30340;&#20998;&#31867;&#22120;&#24182;&#20351;&#29992;&#20998;&#31867;&#22120;&#38169;&#35823;&#65288;&#27491;&#30830;&#65289;&#26631;&#35760;&#30340;&#26041;&#24335;&#22312;&#39564;&#35777;&#25968;&#25454;&#19978;&#29983;&#25104;&#20266;&#25935;&#24863;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2302.01385v2 Announce Type: replace-cross  Abstract: Fair machine learning methods seek to train models that balance model performance across demographic subgroups defined over sensitive attributes like race and gender. Although sensitive attributes are typically assumed to be known during training, they may not be available in practice due to privacy and other logistical concerns. Recent work has sought to train fair models without sensitive attributes on training data. However, these methods need extensive hyper-parameter tuning to achieve good results, and hence assume that sensitive attributes are known on validation data. However, this assumption too might not be practical. Here, we propose Antigone, a framework to train fair classifiers without access to sensitive attributes on either training or validation data. Instead, we generate pseudo sensitive attributes on the validation data by training a biased classifier and using the classifier's incorrectly (correctly) labeled 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20840;&#38754;&#25552;&#39640;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#24615;&#33021;</title><link>https://arxiv.org/abs/2209.00568</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Contrastive Knowledge Co-Distillation for Event Temporal Relation Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00568
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#29992;&#20110;&#20840;&#38754;&#25552;&#39640;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#26102;&#38388;&#20851;&#31995;&#25277;&#21462;&#65288;ETRE&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20107;&#20214;&#23545;&#20301;&#20110;&#19981;&#21516;&#36317;&#31163;&#30340;&#35805;&#35821;&#20013;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25509;&#36817;&#24615;&#24102;&#12290;&#20851;&#20110;&#20301;&#20110;&#26356;&#36828;&#65288;&#21363;&#8220;&#38271;&#8221;&#65289;&#25110;&#26356;&#36817;&#65288;&#21363;&#8220;&#30701;&#8221;&#65289;&#25509;&#36817;&#24615;&#24102;&#30340;&#20107;&#20214;&#23545;&#30340;&#26102;&#38388;&#39034;&#24207;&#20256;&#36798;&#26041;&#24335;&#19981;&#21516;&#12290;&#30446;&#21069;ETRE&#27169;&#22411;&#24448;&#24448;&#22312;&#20301;&#20110;&#30701;&#25110;&#38271;&#25509;&#36817;&#24615;&#24102;&#30340;&#20107;&#20214;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#19981;&#33021;&#21516;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#33258;&#28982;&#25991;&#26412;&#21253;&#21547;&#25152;&#26377;&#31867;&#22411;&#30340;&#26102;&#38388;&#20107;&#20214;&#23545;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MulCo&#65306;&#22810;&#23610;&#24230;&#23545;&#27604;&#30693;&#35782;&#20849;&#21516;&#33976;&#39311;&#65292;&#36825;&#26159;&#19968;&#31181;&#34701;&#21512;&#26041;&#27861;&#65292;&#21487;&#20197;&#36328;&#22810;&#20010;&#20107;&#20214;&#23545;&#25509;&#36817;&#24615;&#24102;&#20849;&#20139;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#23545;&#25152;&#26377;&#31867;&#22411;&#26102;&#38388;&#25968;&#25454;&#38598;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MulCo&#25104;&#21151;&#22320;&#25972;&#21512;&#20102;&#36328;&#30701;&#21644;&#38271;&#25509;&#36817;&#24615;&#24102;&#30340;&#19982;&#26102;&#38388;&#25512;&#29702;&#30456;&#20851;&#30340;&#35821;&#35328;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00568v2 Announce Type: replace-cross  Abstract: Event Temporal Relation Extraction (ETRE) is a crucial yet challenging problem. Event pairs are situated within a discourse at different distances, which we refer to as proximity bands. The temporal ordering communicated about event pairs situated at more remote (i.e., ``long'') or less remote (i.e., ``short'') proximity bands is encoded differently. SOTA ETRE models have tended to perform well on events situated at either short or long proximity bands, but not both. Yet, real-world, natural texts contain all types of temporal event-pairs. In this paper, we present MulCo: Multi-Scale Contrastive Knowledge Co-Distillation, a fusion approach that shares knowledge across multiple event pair proximity bands in order to improve performance on all types of temporal datasets. Our experimental results show that MulCo successfully integrates linguistic cues pertaining to temporal reasoning across both short and long proximity bands and 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;</title><link>http://arxiv.org/abs/2401.12258</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20013;&#30340;&#26032;&#20852;&#25903;&#37197;&#31561;&#32423;
&lt;/p&gt;
&lt;p&gt;
Emergent Dominance Hierarchies in Reinforcement Learning Agents. (arXiv:2401.12258v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.12258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#25903;&#37197;&#31561;&#32423;&#29616;&#35937;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#21644;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#33021;&#22815;&#33258;&#20027;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#33021;&#22815;&#32988;&#36807;&#20154;&#31867;&#12290;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#35774;&#32622;&#25552;&#20986;&#20102;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#25104;&#21151;&#30340;&#28151;&#21512;&#21160;&#26426;&#20195;&#29702;&#21327;&#20316;&#21462;&#20915;&#20110;&#20010;&#20307;&#21644;&#32676;&#20307;&#30446;&#26631;&#20043;&#38388;&#30340;&#24494;&#22937;&#24179;&#34913;&#12290;&#31038;&#20250;&#20064;&#24815;&#21644;&#35268;&#33539;&#65292;&#24448;&#24448;&#21463;&#21040;&#20154;&#31867;&#26426;&#26500;&#30340;&#21551;&#21457;&#65292;&#34987;&#29992;&#20316;&#23454;&#29616;&#36825;&#31181;&#24179;&#34913;&#30340;&#24037;&#20855;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#26412;&#19988;&#32463;&#36807;&#28145;&#20837;&#30740;&#31350;&#30340;&#31038;&#20250;&#20064;&#24815;&#65292;&#21363;&#25903;&#37197;&#31561;&#32423;&#65292;&#23427;&#22312;&#21160;&#29289;&#21644;&#20154;&#31867;&#31038;&#20250;&#20013;&#37117;&#23384;&#22312;&#12290;&#25105;&#20204;&#23558;&#25903;&#37197;&#31561;&#32423;&#30340;&#34892;&#20026;&#29702;&#35770;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65292;&#24182;&#23613;&#21487;&#33021;&#23569;&#22320;&#20462;&#25913;&#29616;&#26377;&#30340;&#26415;&#35821;&#21644;&#23450;&#20041;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#22312;&#27809;&#26377;&#26126;&#30830;&#32534;&#31243;&#25110;&#20869;&#22312;&#22870;&#21169;&#30340;&#24773;&#20917;&#19979;&#65292;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#32676;&#20307;&#33021;&#22815;&#21457;&#26126;&#12289;&#23398;&#20064;&#12289;&#23454;&#26045;&#21644;&#20256;&#36882;&#25903;&#37197;&#31561;&#32423;&#32473;&#26032;&#30340;&#32676;&#20307;&#12290;&#25152;&#20135;&#29983;&#30340;&#25903;&#37197;&#31561;&#32423;&#26377;&#19968;&#20010;
&lt;/p&gt;
&lt;p&gt;
Modern Reinforcement Learning (RL) algorithms are able to outperform humans in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings present additional challenges, and successful cooperation in mixed-motive groups of agents depends on a delicate balancing act between individual and group objectives. Social conventions and norms, often inspired by human institutions, are used as tools for striking this balance.  In this paper, we examine a fundamental, well-studied social convention that underlies cooperation in both animal and human societies: Dominance hierarchies.  We adapt the ethological theory of dominance hierarchies to artificial agents, borrowing the established terminology and definitions with as few amendments as possible. We demonstrate that populations of RL agents, operating without explicit programming or intrinsic rewards, can invent, learn, enforce, and transmit a dominance hierarchy to new populations. The dominance hierarchies that emerge have a 
&lt;/p&gt;</description></item><item><title>PhotoBot&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#30456;&#20114;&#20316;&#29992;&#30340;&#33258;&#21160;&#21270;&#29031;&#29255;&#33719;&#21462;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#26469;&#25552;&#20379;&#25668;&#24433;&#24314;&#35758;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#21464;&#25442;&#22120;&#35745;&#31639;&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;</title><link>http://arxiv.org/abs/2401.11061</link><description>&lt;p&gt;
PhotoBot&#65306;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#30340;&#21442;&#32771;&#20114;&#21160;&#25668;&#24433;
&lt;/p&gt;
&lt;p&gt;
PhotoBot: Reference-Guided Interactive Photography via Natural Language. (arXiv:2401.11061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11061
&lt;/p&gt;
&lt;p&gt;
PhotoBot&#26159;&#19968;&#20010;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#30456;&#20114;&#20316;&#29992;&#30340;&#33258;&#21160;&#21270;&#29031;&#29255;&#33719;&#21462;&#26694;&#26550;&#12290;&#23427;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#26469;&#25552;&#20379;&#25668;&#24433;&#24314;&#35758;&#65292;&#24182;&#36890;&#36807;&#35270;&#35273;&#21464;&#25442;&#22120;&#35745;&#31639;&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;PhotoBot&#30340;&#26694;&#26550;&#65292;&#23427;&#22522;&#20110;&#39640;&#32423;&#20154;&#31867;&#35821;&#35328;&#24341;&#23548;&#21644;&#26426;&#22120;&#20154;&#25668;&#24433;&#24072;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#29992;&#20110;&#33258;&#21160;&#21270;&#30340;&#29031;&#29255;&#33719;&#21462;&#12290;&#25105;&#20204;&#24314;&#35758;&#36890;&#36807;&#20174;&#31574;&#23637;&#30011;&#24266;&#20013;&#26816;&#32034;&#21040;&#30340;&#21442;&#32771;&#22270;&#29255;&#21521;&#29992;&#25143;&#20256;&#36798;&#25668;&#24433;&#24314;&#35758;&#12290;&#25105;&#20204;&#21033;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#21644;&#29289;&#20307;&#26816;&#27979;&#22120;&#65292;&#36890;&#36807;&#25991;&#26412;&#25551;&#36848;&#23545;&#21442;&#32771;&#22270;&#29255;&#36827;&#34892;&#29305;&#24449;&#21270;&#65292;&#24182;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36890;&#36807;&#22522;&#20110;&#29992;&#25143;&#35821;&#35328;&#26597;&#35810;&#30340;&#25991;&#26412;&#25512;&#29702;&#26816;&#32034;&#30456;&#20851;&#30340;&#21442;&#32771;&#22270;&#29255;&#12290;&#20026;&#20102;&#23545;&#24212;&#21442;&#32771;&#22270;&#29255;&#21644;&#35266;&#23519;&#21040;&#30340;&#22330;&#26223;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#26174;&#33879;&#19981;&#21516;&#30340;&#22270;&#20687;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#39044;&#35757;&#32451;&#29305;&#24449;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;&#65292;&#36890;&#36807;&#35299;&#20915;&#19968;&#20010;&#36879;&#35270;n-&#28857;&#65288;PnP&#65289;&#38382;&#39064;&#26469;&#35745;&#31639;RGB-D&#30456;&#26426;&#30340;&#23039;&#24577;&#35843;&#25972;&#12290;&#25105;&#20204;&#22312;&#37197;&#22791;&#26377;&#25163;&#33109;&#30456;&#26426;&#30340;&#30495;&#23454;&#26426;&#26800;&#25163;&#33218;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#30001;PhotoBot&#25293;&#25668;&#30340;&#29031;&#29255;&#20855;&#26377;&#33391;&#22909;&#30340;&#36136;&#37327;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PhotoBot, a framework for automated photo acquisition based on an interplay between high-level human language guidance and a robot photographer. We propose to communicate photography suggestions to the user via a reference picture that is retrieved from a curated gallery. We exploit a visual language model (VLM) and an object detector to characterize reference pictures via textual descriptions and use a large language model (LLM) to retrieve relevant reference pictures based on a user's language query through text-based reasoning. To correspond the reference picture and the observed scene, we exploit pre-trained features from a vision transformer capable of capturing semantic similarity across significantly varying images. Using these features, we compute pose adjustments for an RGB-D camera by solving a Perspective-n-Point (PnP) problem. We demonstrate our approach on a real-world manipulator equipped with a wrist camera. Our user studies show that photos taken by PhotoBo
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#20165;&#38656;&#22522;&#32447;&#35201;&#27714;&#30340;1%&#35745;&#31639;&#36164;&#28304;&#35757;&#32451;FourCastNet&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#19982;&#22522;&#32447;&#30456;&#24403;&#25110;&#29978;&#33267;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2401.05584</link><description>&lt;p&gt;
FourCastNeXt&#65306;&#29992;&#26377;&#38480;&#35745;&#31639;&#36164;&#28304;&#25552;&#39640;FourCastNet&#30340;&#35757;&#32451;&#25928;&#26524;
&lt;/p&gt;
&lt;p&gt;
FourCastNeXt: Improving FourCastNet Training with Limited Compute. (arXiv:2401.05584v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#20165;&#38656;&#22522;&#32447;&#35201;&#27714;&#30340;1%&#35745;&#31639;&#36164;&#28304;&#35757;&#32451;FourCastNet&#65292;&#24182;&#19988;&#20445;&#25345;&#20102;&#19982;&#22522;&#32447;&#30456;&#24403;&#25110;&#29978;&#33267;&#26356;&#22909;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;FourCastNet&#31070;&#32463;&#22320;&#29699;&#31995;&#32479;&#27169;&#22411;&#65288;NESM&#65289;&#22312;&#39044;&#27979;&#21508;&#31181;&#22823;&#27668;&#21464;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#35813;&#27169;&#22411;&#22312;ERA5&#20877;&#20998;&#26512;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#34429;&#28982;&#19982;&#22522;&#26412;&#21464;&#21387;&#22120;&#30456;&#27604;&#65292;FourCastNet&#22312;&#24207;&#21015;&#38271;&#24230;&#19978;&#20139;&#26377;&#20934;&#32447;&#24615;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24230;&#65292;&#32780;&#22522;&#20110;ERA5&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;FourCastNet&#20173;&#28982;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#65292;&#36825;&#23545;&#20110;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#26469;&#35828;&#26159;&#26114;&#36149;&#29978;&#33267;&#26080;&#27861;&#33719;&#24471;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#23637;&#31034;&#25913;&#36827;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20351;&#29992;&#20165;&#38656;&#35201;&#22522;&#32447;&#35201;&#27714;&#30340;1%&#35745;&#31639;&#36164;&#28304;&#26469;&#35757;&#32451;FourCastNet&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#33267;&#23569;&#19982;&#22522;&#32447;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the FourCastNet Neural Earth System Model (NESM) has shown impressive results on predicting various atmospheric variables, trained on the ERA5 reanalysis dataset. While FourCastNet enjoys quasi-linear time and memory complexity in sequence length compared to quadratic complexity in vanilla transformers, training FourCastNet on ERA5 from scratch still requires large amount of compute resources, which is expensive or even inaccessible to most researchers. In this work, we will show improved methods that can train FourCastNet using only 1% of the compute required by the baseline, while maintaining model performance or par or even better than the baseline.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#30456;&#20851;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#65292;&#20294;&#21516;&#26102;&#20063;&#23384;&#22312;&#28508;&#22312;&#30340;&#39118;&#38505;&#12289;&#23041;&#32961;&#21644;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2312.02003</link><description>&lt;p&gt;
&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#23433;&#20840;&#19982;&#38544;&#31169;&#30340;&#35843;&#30740;&#65306;&#32654;&#22909;&#12289;&#24694;&#21155;&#21644;&#19985;&#38475;(arXiv:2312.02003v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly. (arXiv:2312.02003v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.02003
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35843;&#26597;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19982;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#30456;&#20851;&#24615;&#12290;&#30740;&#31350;&#21457;&#29616;LLMs&#22312;&#23433;&#20840;&#21644;&#38544;&#31169;&#20445;&#25252;&#26041;&#38754;&#20855;&#26377;&#31215;&#26497;&#24433;&#21709;&#65292;&#20294;&#21516;&#26102;&#20063;&#23384;&#22312;&#28508;&#22312;&#30340;&#39118;&#38505;&#12289;&#23041;&#32961;&#21644;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;Bard&#65292;&#24050;&#32463;&#38761;&#26032;&#20102;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#12290;&#23427;&#20204;&#20855;&#26377;&#28145;&#20837;&#30340;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#12289;&#20154;&#31867;&#33324;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#12289;&#35821;&#22659;&#24863;&#30693;&#21644;&#24378;&#22823;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#65292;&#22312;&#21508;&#20010;&#39046;&#22495;&#65288;&#22914;&#25628;&#32034;&#24341;&#25806;&#12289;&#23458;&#25143;&#25903;&#25345;&#12289;&#32763;&#35793;&#65289;&#20013;&#20855;&#26377;&#19981;&#21487;&#20272;&#37327;&#30340;&#20215;&#20540;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;LLMs&#20063;&#22312;&#23433;&#20840;&#39046;&#22495;&#24341;&#36215;&#20102;&#20851;&#27880;&#65292;&#25581;&#31034;&#20102;&#23433;&#20840;&#28431;&#27934;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#23433;&#20840;&#30456;&#20851;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;LLMs&#19982;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#20132;&#21449;&#28857;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;LLMs&#22914;&#20309;&#23545;&#23433;&#20840;&#21644;&#38544;&#31169;&#20135;&#29983;&#31215;&#26497;&#24433;&#21709;&#65292;&#23427;&#20204;&#20351;&#29992;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#39118;&#38505;&#21644;&#23041;&#32961;&#65292;&#20197;&#21450;LLMs&#20869;&#22312;&#30340;&#28431;&#27934;&#12290;&#36890;&#36807;&#32508;&#21512;&#25991;&#29486;&#22238;&#39038;&#65292;&#26412;&#25991;&#23558;&#25991;&#29486;&#20998;&#20026;&#8220;&#32654;&#22909;&#8221;&#65288;&#26377;&#30410;&#30340;LLM&#24212;&#29992;&#65289;&#12289;&#8220;&#24694;&#21155;&#8221;&#65288;&#25915;&#20987;&#24615;&#24212;&#29992;&#65289;&#21644;&#8220;&#19985;&#38475;&#8221;&#65288;LLMs&#30340;&#28431;&#27934;&#21450;&#20854;&#38450;&#24481;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into "The Good" (beneficial LLM applications), "The Bad" (offensive applications), and "The Ugly" (vulnerabilities of LLMs and their defenses). We ha
&lt;/p&gt;</description></item><item><title>RiskQ&#26159;&#19968;&#31181;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#39118;&#38505;&#25935;&#24863;&#21327;&#35843;&#35201;&#27714;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39118;&#38505;&#25935;&#24863;&#30340;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;RIGM&#65289;&#21407;&#21017;&#21644;&#24314;&#27169;&#32852;&#21512;&#22238;&#25253;&#20998;&#24067;&#23454;&#29616;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#12290;</title><link>http://arxiv.org/abs/2311.01753</link><description>&lt;p&gt;
RiskQ: &#39118;&#38505;&#25935;&#24863;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
RiskQ: Risk-sensitive Multi-Agent Reinforcement Learning Value Factorization. (arXiv:2311.01753v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01753
&lt;/p&gt;
&lt;p&gt;
RiskQ&#26159;&#19968;&#31181;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#39118;&#38505;&#25935;&#24863;&#21327;&#35843;&#35201;&#27714;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#39118;&#38505;&#25935;&#24863;&#30340;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;RIGM&#65289;&#21407;&#21017;&#21644;&#24314;&#27169;&#32852;&#21512;&#22238;&#25253;&#20998;&#24067;&#23454;&#29616;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#29305;&#28857;&#26159;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#12289;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#22810;&#26679;&#24615;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#65292;&#36825;&#23548;&#33268;&#20102;&#26174;&#33879;&#30340;&#39118;&#38505;&#12290;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#23398;&#20064;&#23545;&#39118;&#38505;&#25935;&#24863;&#30340;&#21327;&#35843;&#21644;&#20998;&#25955;&#31574;&#30053;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#22312;&#39118;&#38505;&#25935;&#24863;&#30340;MARL&#20013;&#21046;&#23450;&#21327;&#35843;&#35201;&#27714;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#39118;&#38505;&#25935;&#24863;&#30340;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;RIGM&#65289;&#21407;&#29702;&#65292;&#20316;&#20026;&#20010;&#20307;-&#20840;&#23616;&#26368;&#22823;&#65288;IGM&#65289;&#21644;&#20998;&#24067;&#24335;IGM&#65288;DIGM&#65289;&#21407;&#29702;&#30340;&#19968;&#31181;&#25512;&#24191;&#12290;&#35813;&#21407;&#29702;&#35201;&#27714;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#39118;&#38505;&#25935;&#24863;&#21160;&#20316;&#36873;&#25321;&#38598;&#21512;&#24212;&#19982;&#20013;&#22830;&#31574;&#30053;&#30340;&#39118;&#38505;&#25935;&#24863;&#21160;&#20316;&#36873;&#25321;&#31561;&#20215;&#12290;&#24403;&#21069;&#30340;MARL&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#26041;&#27861;&#23545;&#20110;&#24120;&#35265;&#30340;&#39118;&#38505;&#24230;&#37327;&#65288;&#20363;&#22914;&#39118;&#38505;&#20215;&#20540;&#65288;VaR&#65289;&#24230;&#37327;&#25110;&#25197;&#26354;&#30340;&#39118;&#38505;&#24230;&#37327;&#65289;&#19981;&#28385;&#36275;RIGM&#21407;&#21017;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RiskQ&#26469;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#36890;&#36807;&#24314;&#27169;&#32852;&#21512;&#22238;&#25253;&#20998;&#24067;&#26469;&#23454;&#29616;&#20215;&#20540;&#22240;&#23376;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent systems are characterized by environmental uncertainty, varying policies of agents, and partial observability, which result in significant risks. In the context of Multi-Agent Reinforcement Learning (MARL), learning coordinated and decentralized policies that are sensitive to risk is challenging. To formulate the coordination requirements in risk-sensitive MARL, we introduce the Risk-sensitive Individual-Global-Max (RIGM) principle as a generalization of the Individual-Global-Max (IGM) and Distributional IGM (DIGM) principles. This principle requires that the collection of risk-sensitive action selections of each agent should be equivalent to the risk-sensitive action selection of the central policy. Current MARL value factorization methods do not satisfy the RIGM principle for common risk metrics such as the Value at Risk (VaR) metric or distorted risk measurements. Therefore, we propose RiskQ to address this limitation, which models the joint return distribution by modeli
&lt;/p&gt;</description></item><item><title>VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2311.01623</link><description>&lt;p&gt;
VQPy&#65306;&#19968;&#31181;&#38754;&#21521;&#29616;&#20195;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01623
&lt;/p&gt;
&lt;p&gt;
VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#26512;&#24191;&#27867;&#24212;&#29992;&#20110;&#24403;&#20170;&#31995;&#32479;&#21644;&#26381;&#21153;&#20013;&#12290;&#22312;&#35270;&#39057;&#20998;&#26512;&#30340;&#21069;&#27839;&#26159;&#29992;&#25143;&#24320;&#21457;&#30340;&#35270;&#39057;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#29305;&#23450;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#65288;&#20363;&#22914;&#20154;&#65292;&#21160;&#29289;&#65292;&#27773;&#36710;&#31561;&#65289;&#19982;&#20256;&#32479;&#38754;&#21521;&#23545;&#35937;&#35821;&#35328;&#24314;&#27169;&#30340;&#23545;&#35937;&#30456;&#20284;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21517;&#20026;VQPy&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#31471;&#65288;&#19968;&#31181;Python&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#21487;&#20197;&#34920;&#36798;&#35270;&#39057;&#23545;&#35937;&#21450;&#20854;&#20132;&#20114;&#30340;&#32467;&#26500;&#65289;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#33258;&#21160;&#29983;&#25104;&#21644;&#20248;&#21270;&#31649;&#36947;&#12290;&#25105;&#20204;&#24050;&#32463;&#23454;&#26045;&#21644;&#24320;&#28304;&#20102;VQPy&#65292;&#23427;&#24050;&#32463;&#20316;&#20026;Cisco DeepVision&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#20135;&#21697;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#21028;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#22238;&#31572;&#12290;&#36890;&#36807;&#22810;&#26679;&#21270;&#38382;&#39064;&#30340;&#25991;&#26412;&#34920;&#36798;&#65292;&#25910;&#38598;&#31572;&#26696;&#65292;&#24182;&#26816;&#26597;&#29983;&#25104;&#30340;&#31572;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#21487;&#33021;&#29983;&#25104;&#34394;&#20551;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21033;&#29992;LLMs&#33258;&#36523;&#65292;&#26080;&#38656;&#20854;&#20182;&#22806;&#37096;&#36164;&#28304;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;Vicuna&#12289;ChatGPT&#21644;GPT-4&#31561;&#26368;&#26032;&#21457;&#24067;&#30340;LLMs&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2310.17918</link><description>&lt;p&gt;
&#30693;&#36947;LLMs&#19981;&#30693;&#36947;&#20160;&#20040;&#65306;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Knowing What LLMs DO NOT Know: A Simple Yet Effective Self-Detection Method. (arXiv:2310.17918v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#21028;&#26029;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#22238;&#31572;&#12290;&#36890;&#36807;&#22810;&#26679;&#21270;&#38382;&#39064;&#30340;&#25991;&#26412;&#34920;&#36798;&#65292;&#25910;&#38598;&#31572;&#26696;&#65292;&#24182;&#26816;&#26597;&#29983;&#25104;&#30340;&#31572;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#21487;&#20197;&#35782;&#21035;&#20986;&#21487;&#33021;&#29983;&#25104;&#34394;&#20551;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21482;&#38656;&#35201;&#21033;&#29992;LLMs&#33258;&#36523;&#65292;&#26080;&#38656;&#20854;&#20182;&#22806;&#37096;&#36164;&#28304;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;Vicuna&#12289;ChatGPT&#21644;GPT-4&#31561;&#26368;&#26032;&#21457;&#24067;&#30340;LLMs&#19978;&#24471;&#21040;&#20102;&#26377;&#25928;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#25991;&#29486;&#25581;&#31034;&#20102;LLMs&#20250;&#20598;&#23572;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#22238;&#31572;&#65292;&#36825;&#24433;&#21709;&#20102;&#23427;&#20204;&#36827;&#19968;&#27493;&#21033;&#29992;&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#26816;&#27979;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;LLMs&#19981;&#30693;&#36947;&#30340;&#38382;&#39064;&#65292;&#20197;&#36991;&#20813;&#29983;&#25104;&#38750;&#20107;&#23454;&#24615;&#30340;&#32467;&#26524;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#20351;&#32473;&#23450;&#38382;&#39064;&#30340;&#25991;&#26412;&#34920;&#36798;&#22810;&#26679;&#21270;&#65292;&#24182;&#25910;&#38598;&#30456;&#24212;&#30340;&#31572;&#26696;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26816;&#26597;&#29983;&#25104;&#30340;&#31572;&#26696;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#35782;&#21035;&#27169;&#22411;&#21487;&#33021;&#29983;&#25104;&#34394;&#20551;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#25152;&#26377;&#20197;&#19978;&#27493;&#39588;&#37117;&#21487;&#20197;&#36890;&#36807;&#25552;&#31034;LLMs&#33258;&#36523;&#26469;&#23436;&#25104;&#65292;&#32780;&#26080;&#38656;&#21442;&#32771;&#20219;&#20309;&#20854;&#20182;&#22806;&#37096;&#36164;&#28304;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#26368;&#36817;&#21457;&#24067;&#30340;LLMs&#65288;&#22914;Vicuna&#12289;ChatGPT&#21644;GPT-4&#65289;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown great potential in Natural Language Processing (NLP) tasks. However, recent literature reveals that LLMs generate nonfactual responses intermittently, which impedes the LLMs' reliability for further utilization. In this paper, we propose a novel self-detection method to detect which questions that a LLM does not know that are prone to generate nonfactual results. Specifically, we first diversify the textual expressions for a given question and collect the corresponding answers. Then we examine the divergencies between the generated answers to identify the questions that the model may generate falsehoods. All of the above steps can be accomplished by prompting the LLMs themselves without referring to any other external resources. We conduct comprehensive experiments and demonstrate the effectiveness of our method on recently released LLMs, e.g., Vicuna, ChatGPT, and GPT-4.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDAN&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#22312;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#33258;&#21160;&#29983;&#25104;&#38544;&#34109;&#30340;&#36234;&#29425;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#36234;&#29425;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#34109;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2310.04451</link><description>&lt;p&gt;
AutoDAN: &#22312;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#29983;&#25104;&#38544;&#34109;&#30340;&#36234;&#29425;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
AutoDAN: Generating Stealthy Jailbreak Prompts on Aligned Large Language Models. (arXiv:2310.04451v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04451
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;AutoDAN&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26088;&#22312;&#22312;&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#33258;&#21160;&#29983;&#25104;&#38544;&#34109;&#30340;&#36234;&#29425;&#25552;&#31034;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#36234;&#29425;&#25216;&#26415;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#38544;&#34109;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#40784;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#26159;&#24378;&#22823;&#30340;&#35821;&#35328;&#29702;&#35299;&#21644;&#20915;&#31574;&#24037;&#20855;&#65292;&#36890;&#36807;&#19982;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24191;&#27867;&#23545;&#40784;&#32780;&#21019;&#24314;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22823;&#22411;&#27169;&#22411;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#36234;&#29425;&#25915;&#20987;&#30340;&#24433;&#21709;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#25805;&#32437;&#25552;&#31034;&#26469;&#24341;&#21457;&#23545;&#40784;&#30340;LLM&#19981;&#24212;&#32473;&#20986;&#30340;&#24694;&#24847;&#36755;&#20986;&#12290;&#30740;&#31350;&#36234;&#29425;&#25552;&#31034;&#21487;&#20197;&#35753;&#25105;&#20204;&#28145;&#20837;&#20102;&#35299;LLM&#30340;&#23616;&#38480;&#24615;&#65292;&#24182;&#36827;&#19968;&#27493;&#25351;&#23548;&#25105;&#20204;&#22914;&#20309;&#20445;&#25252;&#23427;&#20204;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#29616;&#26377;&#30340;&#36234;&#29425;&#25216;&#26415;&#23384;&#22312;&#20197;&#19979;&#38382;&#39064;&#65306;(1) &#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#25915;&#20987;&#22823;&#37327;&#20381;&#36182;&#25163;&#24037;&#21046;&#20316;&#25552;&#31034;&#65307;(2) &#38544;&#34109;&#24615;&#38382;&#39064;&#65292;&#25915;&#20987;&#20381;&#36182;&#22522;&#20110;&#26631;&#35760;&#30340;&#31639;&#27861;&#29983;&#25104;&#24120;&#24120;&#35821;&#20041;&#26080;&#24847;&#20041;&#30340;&#25552;&#31034;&#65292;&#23481;&#26131;&#36890;&#36807;&#22522;&#26412;&#22256;&#24785;&#24230;&#27979;&#35797;&#26816;&#27979;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24819;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65306;&#33021;&#21542;&#24320;&#21457;&#19968;&#31181;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#38544;&#34109;&#36234;&#29425;&#25552;&#31034;&#30340;&#26041;&#27861;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;AutoDAN&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aligned Large Language Models (LLMs) are powerful language understanding and decision-making tools that are created through extensive alignment with human feedback. However, these large models remain susceptible to jailbreak attacks, where adversaries manipulate prompts to elicit malicious outputs that should not be given by aligned LLMs. Investigating jailbreak prompts can lead us to delve into the limitations of LLMs and further guide us to secure them. Unfortunately, existing jailbreak techniques suffer from either (1) scalability issues, where attacks heavily rely on manual crafting of prompts, or (2) stealthiness problems, as attacks depend on token-based algorithms to generate prompts that are often semantically meaningless, making them susceptible to detection through basic perplexity testing. In light of these challenges, we intend to answer this question: Can we develop an approach that can automatically generate stealthy jailbreak prompts? In this paper, we introduce Auto
&lt;/p&gt;</description></item><item><title>ED-NeRF &#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; 3D &#22330;&#26223;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22330;&#26223;&#23884;&#20837;&#21040;&#28508;&#31354;&#38388;&#20013;&#65292;&#24471;&#21040;&#26356;&#24555;&#36895;&#19988;&#26356;&#26131;&#20110;&#32534;&#36753;&#30340; NeRF &#39592;&#24178;&#12290;</title><link>http://arxiv.org/abs/2310.02712</link><description>&lt;p&gt;
ED-NeRF: &#20351;&#29992;&#28508;&#31354;&#38388; NeRF &#23454;&#29616;&#39640;&#25928;&#30340;&#25991;&#26412;&#24341;&#23548;&#30340; 3D &#22330;&#26223;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
ED-NeRF: Efficient Text-Guided Editing of 3D Scene using Latent Space NeRF. (arXiv:2310.02712v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.02712
&lt;/p&gt;
&lt;p&gt;
ED-NeRF &#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340; 3D &#22330;&#26223;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#22330;&#26223;&#23884;&#20837;&#21040;&#28508;&#31354;&#38388;&#20013;&#65292;&#24471;&#21040;&#26356;&#24555;&#36895;&#19988;&#26356;&#26131;&#20110;&#32534;&#36753;&#30340; NeRF &#39592;&#24178;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#20108;&#32500;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#24615;&#30340;&#24615;&#33021;&#12290;&#36825;&#20123;&#36827;&#23637;&#24050;&#32463;&#25193;&#23637;&#21040;&#19977;&#32500;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#26032;&#30340;&#19977;&#32500;&#23545;&#35937;&#12290;&#36825;&#28436;&#21464;&#25104;&#20102; NeRF &#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#25991;&#26412;&#26465;&#20214;&#20801;&#35768;&#23545;&#29616;&#26377;&#30340;&#19977;&#32500;&#23545;&#35937;&#36827;&#34892;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340; NeRF &#32534;&#36753;&#25216;&#26415;&#22312;&#24615;&#33021;&#19978;&#38754;&#20020;&#30528;&#19968;&#20123;&#38480;&#21046;&#65292;&#22914;&#35757;&#32451;&#36895;&#24230;&#24930;&#21644;&#20351;&#29992;&#30340;&#25439;&#22833;&#20989;&#25968;&#19981;&#20805;&#20998;&#32771;&#34385;&#32534;&#36753;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340; 3D NeRF &#32534;&#36753;&#26041;&#27861;&#65292;&#31216;&#20026; ED-NeRF&#65292;&#36890;&#36807;&#23558;&#30495;&#23454;&#19990;&#30028;&#22330;&#26223;&#25104;&#21151;&#23884;&#20837;&#21040;&#28508;&#25193;&#25955;&#27169;&#22411; (LDM) &#30340;&#28508;&#31354;&#38388;&#20013;&#65292;&#36890;&#36807;&#29420;&#29305;&#30340;&#32454;&#21270;&#23618;&#12290;&#36825;&#31181;&#26041;&#27861;&#20351;&#25105;&#20204;&#33021;&#22815;&#33719;&#24471;&#19968;&#20010;&#19981;&#20165;&#26356;&#24555;&#65292;&#32780;&#19988;&#26356;&#36866;&#21512;&#20110;&#32534;&#36753;&#30340; NeRF &#39592;&#24178;&#65292;&#19982;&#20256;&#32479;&#30340;&#22270;&#20687;&#31354;&#38388; NeRF &#32534;&#36753;&#30456;&#27604;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#25439;&#22833;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, there has been a significant advancement in text-to-image diffusion models, leading to groundbreaking performance in 2D image generation. These advancements have been extended to 3D models, enabling the generation of novel 3D objects from textual descriptions. This has evolved into NeRF editing methods, which allow the manipulation of existing 3D objects through textual conditioning. However, existing NeRF editing techniques have faced limitations in their performance due to slow training speeds and the use of loss functions that do not adequately consider editing. To address this, here we present a novel 3D NeRF editing approach dubbed ED-NeRF by successfully embedding real-world scenes into the latent space of the latent diffusion model (LDM) through a unique refinement layer. This approach enables us to obtain a NeRF backbone that is not only faster but also more amenable to editing compared to traditional image space NeRF editing. Furthermore, we propose an improved loss 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#36895;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#24615;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#30495;&#38451;&#24615;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.14974</link><description>&lt;p&gt;
&#22312;&#19968;&#21315;&#24180;&#21069;&#30340;&#25289;&#19969;&#25991;&#26412;&#20013;&#26816;&#27979;&#21477;&#23376;&#32423;&#21035;&#30340;&#24615;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
Detecting Sexual Content at the Sentence Level in First Millennium Latin Texts. (arXiv:2309.14974v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#36895;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#12290;&#32463;&#36807;&#35780;&#20272;&#65292;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#24615;&#20869;&#23481;&#26041;&#38754;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#30495;&#38451;&#24615;&#29575;&#65292;&#24182;&#25506;&#32034;&#20102;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21477;&#23376;&#32423;&#21035;&#36827;&#34892;&#35821;&#20041;&#20998;&#31867;&#65292;&#20197;&#21152;&#24555;&#20154;&#25991;&#23398;&#31185;&#21644;&#35821;&#35328;&#23398;&#39046;&#22495;&#20013;&#35821;&#26009;&#24211;&#24314;&#35774;&#30340;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#39033;&#20256;&#32479;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#35821;&#26009;&#24211;&#65292;&#21253;&#25324;&#32422;2500&#20010;&#21477;&#23376;&#65292;&#28085;&#30422;&#20102;&#20174;&#20844;&#20803;&#21069;300&#24180;&#21040;&#20844;&#20803;900&#24180;&#30340;&#24615;&#35821;&#20041;&#23398;&#65288;&#21307;&#23398;&#65292;&#24773;&#33394;&#31561;&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181;&#21477;&#23376;&#20998;&#31867;&#26041;&#27861;&#21644;&#19981;&#21516;&#30340;&#36755;&#20837;&#23884;&#20837;&#23618;&#65292;&#24182;&#34920;&#26126;&#23427;&#20204;&#37117;&#27604;&#31616;&#21333;&#30340;&#22522;&#20110;&#26631;&#35760;&#30340;&#25628;&#32034;&#26041;&#27861;&#26356;&#22909;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20010;&#20154;&#35328;&#35821;&#21644;&#31038;&#20250;&#35328;&#35821;&#20803;&#25968;&#25454;&#23884;&#20837;&#65288;&#19990;&#32426;&#65292;&#20316;&#32773;&#65292;&#20889;&#20316;&#31867;&#22411;&#65289;&#30340;&#25972;&#21512;&#65292;&#20294;&#21457;&#29616;&#36825;&#23548;&#33268;&#20102;&#36807;&#25311;&#21512;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20351;&#29992;HAN&#20998;&#21035;&#36798;&#21040;&#20102;70.60%&#30340;&#39640;&#31934;&#24230;&#21644;86.33%&#30340;&#30495;&#38451;&#24615;&#29575;&#65288;TPR&#65289;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25968;&#25454;&#38598;&#22823;&#23567;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65288;420&#32780;&#19981;&#26159;2013&#65289;&#65292;&#24182;&#26174;&#31034;&#20986;&#65292;&#23613;&#31649;&#25105;&#20204;&#30340;&#27169;&#22411;&#24615;&#33021;&#21487;&#33021;&#31245;&#26377;&#19979;&#38477;&#65292;&#20294;&#24615;&#33021;&#20173;&#28982;&#31283;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose to evaluate the use of deep learning methods for semantic classification at the sentence level to accelerate the process of corpus building in the field of humanities and linguistics, a traditional and time-consuming task. We introduce a novel corpus comprising around 2500 sentences spanning from 300 BCE to 900 CE including sexual semantics (medical, erotica, etc.). We evaluate various sentence classification approaches and different input embedding layers, and show that all consistently outperform simple token-based searches. We explore the integration of idiolectal and sociolectal metadata embeddings (centuries, author, type of writing), but find that it leads to overfitting. Our results demonstrate the effectiveness of this approach, achieving high precision and true positive rates (TPR) of respectively 70.60% and 86.33% using HAN. We evaluate the impact of the dataset size on the model performances (420 instead of 2013), and show that, while our models per
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;BART&#12289;T5&#21644;BERT2BERT-style&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#29256;&#26412;&#12290;</title><link>http://arxiv.org/abs/2309.11259</link><description>&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-Sequence Spanish Pre-trained Language Models. (arXiv:2309.11259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11259
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#30340;&#35199;&#29677;&#29273;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#31454;&#20105;&#24615;&#33021;&#65292;&#24182;&#25552;&#20379;&#20102;BART&#12289;T5&#21644;BERT2BERT-style&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#29256;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#37325;&#22823;&#36827;&#23637;&#20026;&#35768;&#22810;&#38750;&#33521;&#35821;&#35821;&#35328;&#29256;&#26412;&#30340;&#24320;&#21457;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#20854;&#20013;&#29305;&#21035;&#20851;&#27880;&#20102;&#20165;&#32534;&#30721;&#22120;&#21644;&#20165;&#35299;&#30721;&#22120;&#30340;&#26550;&#26500;&#12290;&#34429;&#28982;&#35199;&#29677;&#29273;&#35821;&#35821;&#35328;&#27169;&#22411;&#21253;&#25324;BERT&#12289;RoBERTa&#21644;GPT&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#26041;&#38754;&#23637;&#29616;&#20986;&#20102;&#20248;&#21183;&#65292;&#20294;&#22312;&#28041;&#21450;&#36755;&#20837;&#36755;&#20986;&#23545;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#20013;&#65292;&#32570;&#20047;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23454;&#26045;&#21644;&#35780;&#20272;&#33879;&#21517;&#30340;&#20165;&#22312;&#35199;&#29677;&#29273;&#35821;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24320;&#21019;&#20102;&#26032;&#30340;&#39046;&#22495;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BART&#12289;T5&#21644;BERT2BERT&#39118;&#26684;&#27169;&#22411;&#30340;&#35199;&#29677;&#29273;&#35821;&#29256;&#26412;&#65292;&#24182;&#23545;&#23427;&#20204;&#22312;&#21508;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#25688;&#35201;&#12289;&#37325;&#36848;&#21644;&#29983;&#25104;&#24335;&#38382;&#31572;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#25152;&#26377;&#27169;&#22411;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#20854;&#20013;BART&#21644;T5&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, substantial advancements in pre-trained language models have paved the way for the development of numerous non-English language versions, with a particular focus on encoder-only and decoder-only architectures. While Spanish language models encompassing BERT, RoBERTa, and GPT have exhibited prowess in natural language understanding and generation, there remains a scarcity of encoder-decoder models designed for sequence-to-sequence tasks involving input-output pairs. This paper breaks new ground by introducing the implementation and evaluation of renowned encoder-decoder architectures, exclusively pre-trained on Spanish corpora. Specifically, we present Spanish versions of BART, T5, and BERT2BERT-style models and subject them to a comprehensive assessment across a diverse range of sequence-to-sequence tasks, spanning summarization, rephrasing, and generative question answering. Our findings underscore the competitive performance of all models, with BART and T5 emerging a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2309.06255</link><description>&lt;p&gt;
&#36890;&#36807;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#22686;&#24378;&#22810;&#27169;&#24577;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation. (arXiv:2309.06255v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.06255
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#29992;&#20110;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#65292;&#24182;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#30340;&#19968;&#20010;&#20027;&#35201;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#26469;&#33258;&#19981;&#21516;&#27169;&#24577;&#30340;&#24322;&#36136;&#20449;&#24687;&#20849;&#21516;&#32467;&#21512;&#36215;&#26469;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27169;&#22411;&#22312;&#22810;&#27169;&#24577;&#21327;&#20316;&#26041;&#38754;&#24120;&#24120;&#23384;&#22312;&#19981;&#23613;&#20154;&#24847;&#30340;&#38382;&#39064;&#65292;&#19981;&#33021;&#24456;&#22909;&#22320;&#20849;&#21516;&#21033;&#29992;&#25152;&#26377;&#27169;&#24577;&#12290;&#19968;&#20123;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#35782;&#21035;&#21644;&#22686;&#24378;&#23398;&#20064;&#25928;&#26524;&#36739;&#24046;&#30340;&#27169;&#24577;&#65292;&#20294;&#24448;&#24448;&#38590;&#20197;&#22312;&#29702;&#35770;&#19978;&#25552;&#20379;&#23545;&#26679;&#26412;&#32423;&#21035;&#22810;&#27169;&#24577;&#21327;&#20316;&#30340;&#32454;&#31890;&#24230;&#35266;&#23519;&#21644;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#21512;&#29702;&#35266;&#23519;&#21644;&#25913;&#36827;&#27169;&#24577;&#20043;&#38388;&#32454;&#31890;&#24230;&#30340;&#21327;&#20316;&#23588;&#20026;&#37325;&#35201;&#65292;&#23588;&#20854;&#26159;&#22312;&#38754;&#23545;&#27169;&#24577;&#24046;&#24322;&#22312;&#19981;&#21516;&#26679;&#26412;&#20043;&#38388;&#21487;&#33021;&#21464;&#21270;&#30340;&#23454;&#38469;&#22330;&#26223;&#26102;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31934;&#32454;&#30340;&#27169;&#24577;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35780;&#20272;&#27599;&#20010;&#27169;&#24577;&#22312;&#26679;&#26412;&#32423;&#21035;&#30340;&#36129;&#29486;&#12290;&#36890;&#36807;&#27169;&#24577;&#35780;&#20272;&#65292;&#25105;&#20204;&#36951;&#25022;&#22320;&#21457;&#29616;&#22810;&#27169;&#24577;&#27169;&#22411;&#20542;&#21521;&#20110;&#20381;&#36182;&#19968;&#20010;&#29305;&#23450;&#30340;&#27169;&#24577;&#65292;&#23548;&#33268;&#20854;&#20182;&#27169;&#24577;&#30340;&#36129;&#29486;&#36739;&#20302;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
One primary topic of multi-modal learning is to jointly incorporate heterogeneous information from different modalities. However, most models often suffer from unsatisfactory multi-modal cooperation, which could not jointly utilize all modalities well. Some methods are proposed to identify and enhance the worse learnt modality, but are often hard to provide the fine-grained observation of multi-modal cooperation at sample-level with theoretical support. Hence, it is essential to reasonably observe and improve the fine-grained cooperation between modalities, especially when facing realistic scenarios where the modality discrepancy could vary across different samples. To this end, we introduce a fine-grained modality valuation metric to evaluate the contribution of each modality at sample-level. Via modality valuation, we regretfully observe that the multi-modal model tends to rely on one specific modality, resulting in other modalities being low-contributing. We further analyze this iss
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#21487;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.02632</link><description>&lt;p&gt;
&#20174;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#20013;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning from Hierarchical Weak Preference Feedback. (arXiv:2309.02632v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02632
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#20998;&#23618;&#30340;&#24369;&#20559;&#22909;&#21453;&#39304;&#36827;&#34892;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#12290;&#36890;&#36807;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#65292;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#21487;&#20197;&#24110;&#21161;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22870;&#21169;&#35774;&#35745;&#26159;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#20013;&#19968;&#20010;&#22522;&#26412;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#26041;&#38754;&#12290;&#23545;&#20110;&#31616;&#21333;&#20219;&#21153;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#24120;&#25163;&#24037;&#35774;&#35745;&#22870;&#21169;&#20989;&#25968;&#65292;&#20363;&#22914;&#20351;&#29992;&#33509;&#24178;&#20010;&#22870;&#21169;&#22240;&#23376;&#30340;&#32447;&#24615;&#32452;&#21512;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#22870;&#21169;&#24037;&#31243;&#21463;&#21040;&#36817;&#20284;&#20559;&#24046;&#30340;&#24433;&#21709;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35843;&#20248;&#25104;&#26412;&#65292;&#24182;&#19988;&#36890;&#24120;&#26080;&#27861;&#25552;&#20379;&#22797;&#26434;&#20219;&#21153;&#25152;&#38656;&#30340;&#32454;&#31890;&#24230;&#12290;&#20026;&#20102;&#36991;&#20813;&#36825;&#20123;&#22256;&#38590;&#65292;&#30740;&#31350;&#20154;&#21592;&#24320;&#22987;&#36716;&#21521;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#65292;&#20174;&#36712;&#36857;&#24207;&#21015;&#23545;&#20043;&#38388;&#30340;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;&#36890;&#36807;&#21033;&#29992;&#22522;&#20110;&#20559;&#22909;&#30340;&#22870;&#21169;&#24314;&#27169;&#65292;RLHF&#23398;&#20064;&#21040;&#19982;&#20154;&#31867;&#20559;&#22909;&#38750;&#24120;&#19968;&#33268;&#30340;&#22797;&#26434;&#22870;&#21169;&#65292;&#20351;&#24471;&#24378;&#21270;&#23398;&#20064;&#33021;&#22815;&#35299;&#20915;&#26085;&#30410;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;RLHF&#30340;&#36866;&#29992;&#24615;&#21463;&#21040;&#33719;&#24471;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#30340;&#39640;&#25104;&#26412;&#21644;&#22256;&#38590;&#30340;&#38480;&#21046;&#12290;&#37492;&#20110;&#36825;&#20010;&#25104;&#26412;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#22797;&#26434;&#20219;&#21153;&#20013;&#20351;&#29992;&#26356;&#23569;&#20154;&#21147;&#25237;&#20837;&#30340;&#26041;&#24335;&#26469;&#23398;&#20064;&#22870;&#21169;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reward design is a fundamental, yet challenging aspect of practical reinforcement learning (RL). For simple tasks, researchers typically handcraft the reward function, e.g., using a linear combination of several reward factors. However, such reward engineering is subject to approximation bias, incurs large tuning cost, and often cannot provide the granularity required for complex tasks. To avoid these difficulties, researchers have turned to reinforcement learning from human feedback (RLHF), which learns a reward function from human preferences between pairs of trajectory sequences. By leveraging preference-based reward modeling, RLHF learns complex rewards that are well aligned with human preferences, allowing RL to tackle increasingly difficult problems. Unfortunately, the applicability of RLHF is limited due to the high cost and difficulty of obtaining human preference data. In light of this cost, we investigate learning reward functions for complex tasks with less human effort; sim
&lt;/p&gt;</description></item><item><title>TensorBank&#26159;&#19968;&#20010;&#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#65292;&#33021;&#22815;&#20197;&#39640;&#36895;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#36827;&#34892;&#26597;&#35810;&#21152;&#36895;&#12290;</title><link>http://arxiv.org/abs/2309.02094</link><description>&lt;p&gt;
TensorBank: &#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#29992;&#20110;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
TensorBank:Tensor Lakehouse for Foundation Model Training. (arXiv:2309.02094v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02094
&lt;/p&gt;
&lt;p&gt;
TensorBank&#26159;&#19968;&#20010;&#22522;&#20110;Tensor&#30340;&#28246;&#20179;&#24211;&#65292;&#33021;&#22815;&#20197;&#39640;&#36895;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#36827;&#34892;&#26597;&#35810;&#21152;&#36895;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22522;&#30784;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#20043;&#22806;&#30340;&#39046;&#22495;&#30340;&#20852;&#36215;&#65292;&#23384;&#20648;&#21644;&#27969;&#24335;&#22788;&#29702;&#39640;&#32500;&#25968;&#25454;&#25104;&#20026;&#22522;&#30784;&#27169;&#22411;&#35757;&#32451;&#30340;&#20851;&#38190;&#38656;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;TensorBank&#65292;&#19968;&#20010;&#33021;&#22815;&#22522;&#20110;&#22797;&#26434;&#20851;&#31995;&#26597;&#35810;&#20174;&#20113;&#23545;&#35937;&#23384;&#20648;&#65288;COS&#65289;&#27969;&#24335;&#20256;&#36755;&#24352;&#37327;&#21040;GPU&#20869;&#23384;&#30340;&#30334;&#20159;&#32423;&#24352;&#37327;&#28246;&#20179;&#24211;&#12290;&#25105;&#20204;&#20351;&#29992;&#20998;&#23618;&#32479;&#35745;&#25351;&#26631;&#65288;HSI&#65289;&#26469;&#21152;&#36895;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#26550;&#26500;&#20801;&#35768;&#20351;&#29992;HTTP&#33539;&#22260;&#35835;&#21462;&#26469;&#30452;&#25509;&#35775;&#38382;&#22359;&#32423;&#21035;&#30340;&#24352;&#37327;&#12290;&#19968;&#26086;&#22312;GPU&#20869;&#23384;&#20013;&#65292;&#25968;&#25454;&#21487;&#20197;&#20351;&#29992;PyTorch&#36716;&#25442;&#36827;&#34892;&#36716;&#25442;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;PyTorch&#25968;&#25454;&#38598;&#31867;&#22411;&#65292;&#37197;&#26377;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#24037;&#21378;&#65292;&#29992;&#20110;&#23558;&#20851;&#31995;&#26597;&#35810;&#21644;&#35831;&#27714;&#30340;&#36716;&#25442;&#20316;&#20026;&#19968;&#20010;&#23454;&#20363;&#36827;&#34892;&#32763;&#35793;&#12290;&#36890;&#36807;&#20351;&#29992;HSI&#65292;&#21487;&#20197;&#36339;&#36807;&#19981;&#30456;&#20851;&#30340;&#22359;&#65292;&#32780;&#26080;&#38656;&#35835;&#21462;&#23427;&#20204;&#65292;&#22240;&#20026;&#36825;&#20123;&#32034;&#24341;&#21253;&#21547;&#19981;&#21516;&#23618;&#27425;&#20998;&#36776;&#29575;&#32423;&#21035;&#19978;&#20869;&#23481;&#30340;&#32479;&#35745;&#20449;&#24687;&#12290;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#24320;&#25918;&#26631;&#20934;&#30340;&#26377;&#20027;&#35266;&#35266;&#28857;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Storing and streaming high dimensional data for foundation model training became a critical requirement with the rise of foundation models beyond natural language. In this paper we introduce TensorBank, a petabyte scale tensor lakehouse capable of streaming tensors from Cloud Object Store (COS) to GPU memory at wire speed based on complex relational queries. We use Hierarchical Statistical Indices (HSI) for query acceleration. Our architecture allows to directly address tensors on block level using HTTP range reads. Once in GPU memory, data can be transformed using PyTorch transforms. We provide a generic PyTorch dataset type with a corresponding dataset factory translating relational queries and requested transformations as an instance. By making use of the HSI, irrelevant blocks can be skipped without reading them as those indices contain statistics on their content at different hierarchical resolution levels. This is an opinionated architecture powered by open standards and making h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#20851;&#20110;&#35780;&#20272;LLM&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#35843;&#26597;&#28085;&#30422;&#20102;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#25269;&#25239;&#28389;&#29992;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#36981;&#23432;&#31038;&#20250;&#35268;&#33539;&#20197;&#21450;&#40065;&#26834;&#24615;&#31561;&#19971;&#20010;&#20027;&#35201;&#31867;&#21035;&#65292;&#20849;&#35745;29&#20010;&#23376;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2308.05374</link><description>&lt;p&gt;
&#21487;&#20449;&#30340;LLMs&#65306;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#40784;&#30340;&#35843;&#26597;&#21644;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models' Alignment. (arXiv:2308.05374v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05374
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#20851;&#20110;&#35780;&#20272;LLM&#21487;&#20449;&#24230;&#30340;&#32508;&#21512;&#35843;&#26597;&#65292;&#24182;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;&#35843;&#26597;&#28085;&#30422;&#20102;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#25269;&#25239;&#28389;&#29992;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#36981;&#23432;&#31038;&#20250;&#35268;&#33539;&#20197;&#21450;&#40065;&#26834;&#24615;&#31561;&#19971;&#20010;&#20027;&#35201;&#31867;&#21035;&#65292;&#20849;&#35745;29&#20010;&#23376;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#24212;&#29992;&#20043;&#21069;&#65292;&#30830;&#20445;&#23545;&#40784;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#20174;&#19994;&#32773;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#26469;&#35780;&#20272;LLMs&#30340;&#36755;&#20986;&#26159;&#21542;&#31526;&#21512;&#31038;&#20250;&#35268;&#33539;&#12289;&#20215;&#20540;&#35266;&#21644;&#27861;&#35268;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#35843;&#26597;&#65292;&#28085;&#30422;&#20102;&#35780;&#20272;LLM&#21487;&#20449;&#24230;&#26102;&#24517;&#39035;&#32771;&#34385;&#30340;&#20851;&#38190;&#32500;&#24230;&#12290;&#35843;&#26597;&#28085;&#30422;&#20102;LLM&#21487;&#20449;&#24230;&#30340;&#19971;&#20010;&#20027;&#35201;&#31867;&#21035;&#65306;&#21487;&#38752;&#24615;&#12289;&#23433;&#20840;&#24615;&#12289;&#20844;&#24179;&#24615;&#12289;&#25269;&#25239;&#28389;&#29992;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12289;&#36981;&#23432;&#31038;&#20250;&#35268;&#33539;&#20197;&#21450;&#40065;&#26834;&#24615;&#12290;&#27599;&#20010;&#20027;&#35201;&#31867;&#21035;&#36827;&#19968;&#27493;&#32454;&#20998;&#20026;&#33509;&#24178;&#23376;&#31867;&#21035;&#65292;&#20849;&#35745;29&#20010;&#23376;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset 
&lt;/p&gt;</description></item><item><title>&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2308.05342</link><description>&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29702;&#35299;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Metacognitive Prompting Improves Understanding in Large Language Models. (arXiv:2308.05342v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05342
&lt;/p&gt;
&lt;p&gt;
&#20803;&#35748;&#30693;&#25552;&#31034; (MP) &#26159;&#19968;&#31181;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#29702;&#35299;&#33021;&#21147;&#30340;&#31574;&#30053;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;MP&#30340;PaLM&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#25509;&#36817;&#20110;GPT-4&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLMs) &#20013;&#65292;&#36890;&#36807;&#26377;&#25928;&#30340;&#25552;&#31034;&#35774;&#35745;&#65292;&#20219;&#21153;&#29305;&#23450;&#24615;&#33021;&#19968;&#30452;&#22312;&#19981;&#26029;&#25552;&#39640;&#12290;&#23613;&#31649;&#26368;&#36817;&#20851;&#20110;&#25552;&#31034;&#30340;&#30740;&#31350;&#22686;&#24378;&#20102;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#22312;&#36827;&#19968;&#27493;&#25552;&#39640;&#23427;&#20204;&#30340;&#29702;&#35299;&#33021;&#21147;&#26041;&#38754;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20803;&#35748;&#30693;&#25552;&#31034; (MP)&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#20154;&#31867;&#20869;&#30465;&#25512;&#29702;&#36807;&#31243;&#21551;&#21457;&#30340;&#31574;&#30053;&#12290;&#20351;&#29992;MP&#65292;LLMs&#32463;&#21382;&#19968;&#31995;&#21015;&#26377;&#32467;&#26500;&#12289;&#33258;&#25105;&#24847;&#35782;&#30340;&#35780;&#20272;&#65292;&#21033;&#29992;&#20854;&#20016;&#23500;&#30340;&#20869;&#22312;&#30693;&#35782;&#21644;&#26032;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#28041;&#21450;&#20116;&#20010;&#24120;&#35265;&#30340;LLMs&#65306;Llama2&#12289;Vicuna&#12289;PaLM&#12289;GPT-3.5&#21644;GPT-4&#65292;&#23427;&#20204;&#37117;&#28085;&#30422;&#20102;&#26469;&#33258;GLUE&#21644;SuperGLUE&#22522;&#20934;&#27979;&#35797;&#30340;&#21508;&#31181;&#36890;&#29992;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299; (NLU) &#20219;&#21153;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#34429;&#28982;GPT-4&#22312;&#22823;&#22810;&#25968;&#20219;&#21153;&#20013;&#22987;&#32456;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#37197;&#22791;MP&#30340;PaLM&#25509;&#36817;&#20854;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#36328;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;MP&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#30340;&#25552;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Large Language Models (LLMs), there have been consistent advancements in task-specific performance, largely influenced by effective prompt design. While recent research on prompting has enhanced the reasoning capabilities of LLMs, a gap remains in further improving their understanding abilities. In this study, we introduce metacognitive prompting (MP), a strategy inspired by human introspective reasoning processes. Using MP, LLMs undergo a systematic series of structured, self-aware evaluations, drawing on both their vast inherent knowledge and new insights. Our experiments involve five prevalent LLMs: Llama2, Vicuna, PaLM, GPT-3.5, and GPT-4, all of which span various general natural language understanding (NLU) tasks from the GLUE and SuperGLUE benchmarks. Results indicate that, although GPT-4 consistently excels in most tasks, PaLM, when equipped with MP, approaches its performance level. Furthermore, across models and datasets, MP consistently outperforms existing prompting meth
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;</title><link>http://arxiv.org/abs/2306.09549</link><description>&lt;p&gt;
QH9&#65306;QM9&#20998;&#23376;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#39044;&#27979;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules. (arXiv:2306.09549v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09549
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;QH9&#65292;&#29992;&#20110;&#20026;&#21508;&#31181;&#20998;&#23376;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#22522;&#20934;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36234;&#26469;&#36234;&#34987;&#29992;&#20110;&#21152;&#36895;&#30005;&#23376;&#32467;&#26500;&#39044;&#27979;&#65292;&#20316;&#20026;&#31532;&#19968;&#24615;&#21407;&#29702;&#35745;&#31639;&#26041;&#27861;&#65288;&#22914;&#23494;&#24230;&#27867;&#20989;&#29702;&#35770;&#65288;DFT&#65289;&#65289;&#30340;&#26367;&#20195;&#21697;&#12290;&#34429;&#28982;&#35768;&#22810;&#37327;&#23376;&#21270;&#23398;&#25968;&#25454;&#38598;&#20391;&#37325;&#20110;&#21270;&#23398;&#24615;&#36136;&#21644;&#21407;&#23376;&#21147;&#65292;&#20294;&#20934;&#30830;&#19988;&#39640;&#25928;&#22320;&#39044;&#27979;&#21704;&#23494;&#39039;&#30697;&#38453;&#30340;&#33021;&#21147;&#26159;&#38750;&#24120;&#37325;&#35201;&#21644;&#22522;&#26412;&#30340;&#29289;&#29702;&#37327;&#65292;&#23427;&#30830;&#23450;&#20102;&#29289;&#29702;&#31995;&#32479;&#21644;&#21270;&#23398;&#24615;&#36136;&#30340;&#37327;&#23376;&#29366;&#24577;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#37327;&#23376;&#21704;&#23494;&#39039;&#25968;&#25454;&#38598;&#65292;&#21629;&#21517;&#20026;QH9&#65292;&#22522;&#20110;QM9&#25968;&#25454;&#38598;&#20026;2,399&#20010;&#20998;&#23376;&#21160;&#21147;&#23398;&#36712;&#36857;&#21644;130,831&#20010;&#31283;&#23450;&#20998;&#23376;&#20960;&#20309;&#24418;&#24577;&#25552;&#20379;&#31934;&#30830;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;&#36890;&#36807;&#35774;&#35745;&#21508;&#31181;&#20998;&#23376;&#30340;&#22522;&#20934;&#20219;&#21153;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26377;&#33021;&#21147;&#39044;&#27979;&#20219;&#24847;&#20998;&#23376;&#30340;&#21704;&#23494;&#39039;&#30697;&#38453;&#12290;QH9&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#27169;&#22411;&#37117;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised machine learning approaches have been increasingly used in accelerating electronic structure prediction as surrogates of first-principle computational methods, such as density functional theory (DFT). While numerous quantum chemistry datasets focus on chemical properties and atomic forces, the ability to achieve accurate and efficient prediction of the Hamiltonian matrix is highly desired, as it is the most important and fundamental physical quantity that determines the quantum states of physical systems and chemical properties. In this work, we generate a new Quantum Hamiltonian dataset, named as QH9, to provide precise Hamiltonian matrices for 2,399 molecular dynamics trajectories and 130,831 stable molecular geometries, based on the QM9 dataset. By designing benchmark tasks with various molecules, we show that current machine learning models have the capacity to predict Hamiltonian matrices for arbitrary molecules. Both the QH9 dataset and the baseline models are provided
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20030;&#21150;&#22312;2023 IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#65292;&#30446;&#26631;&#26159;&#35753;ChatGPT&#29983;&#25104;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;</title><link>http://arxiv.org/abs/2303.15662</link><description>&lt;p&gt;
ChatGPT4PCG&#27604;&#36187;&#65306;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
ChatGPT4PCG Competition: Character-like Level Generation for Science Birds. (arXiv:2303.15662v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#20030;&#21150;&#22312;2023 IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#65292;&#30446;&#26631;&#26159;&#35753;ChatGPT&#29983;&#25104;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;2023&#24180;IEEE&#28216;&#25103;&#20250;&#35758;&#19978;&#30340;&#31532;&#19968;&#23626;ChatGPT4PCG&#27604;&#36187;&#12290;&#26412;&#27425;&#27604;&#36187;&#30340;&#30446;&#26631;&#26159;&#35753;&#21442;&#36187;&#32773;&#36890;&#36807;&#21019;&#36896;&#24615;&#21644;&#25552;&#31034;&#24037;&#31243;&#25216;&#33021;&#65292;&#20026;ChatGPT&#21019;&#24314;&#26377;&#25928;&#30340;&#25552;&#31034;&#65292;&#20351;&#20854;&#33021;&#22815;&#20855;&#26377;&#39640;&#31283;&#23450;&#24615;&#21644;&#31867;&#20284;&#35282;&#33394;&#30340;&#29305;&#36136;&#26469;&#29983;&#25104;&#20855;&#26377;&#31185;&#23398;&#40479;&#35282;&#33394;&#32423;&#27700;&#24179;&#30340;&#20851;&#21345;&#12290;&#20026;&#20102;&#38477;&#20302;&#21442;&#36187;&#38376;&#27099;&#65292;&#25105;&#20204;&#23558;&#20219;&#21153;&#38480;&#21046;&#22312;&#29983;&#25104;&#22823;&#20889;&#33521;&#25991;&#23383;&#27597;&#12290;&#21442;&#36187;&#20316;&#21697;&#30340;&#36136;&#37327;&#30001;&#20854;&#31283;&#23450;&#24615;&#21644;&#19982;&#32473;&#23450;&#23383;&#31526;&#30340;&#30456;&#20284;&#24615;&#20915;&#23450;&#12290;&#32473;&#21442;&#36187;&#32773;&#25552;&#20379;&#20102;&#19968;&#20010;&#26679;&#20363;&#25552;&#31034;&#20379;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the first ChatGPT4PCG Competition at the 2023 IEEE Conference on Games. The objective of this competition is for participants to create effective prompts for ChatGPT--enabling it to generate Science Birds levels with high stability and character-like qualities--fully using their creativity as well as prompt engineering skills. ChatGPT is a conversational agent developed by OpenAI. Science Birds is selected as the competition platform because designing an Angry Birds-like level is not a trivial task due to the in-game gravity; the playability of the levels is determined by their stability. To lower the entry barrier to the competition, we limit the task to the generation of capitalized English alphabetical characters. Here, the quality of the generated levels is determined by their stability and similarity to the given characters. A sample prompt is provided to participants for their reference. An experiment is conducted to determine the effectiveness of its modified
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.13854</link><description>&lt;p&gt;
ComCLIP: &#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
ComCLIP: Training-Free Compositional Image and Text Matching. (arXiv:2211.13854v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.13854
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#27169;&#22411; ComCLIP&#65292;&#36890;&#36807;&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32467;&#21512;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#65292;&#20197;&#35299;&#20915;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#24050;&#32463;&#23637;&#31034;&#20102;&#22312;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#26041;&#38754;&#30340;&#24456;&#22909;&#30340;&#38646;&#26679;&#26412;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23558; CLIP &#36825;&#26679;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#32452;&#21512;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#38656;&#35201;&#27169;&#22411;&#29702;&#35299;&#32452;&#21512;&#35789;&#27010;&#24565;&#21644;&#35270;&#35273;&#32452;&#20214;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#22909;&#30340;&#38646;&#26679;&#26412;&#22270;&#20687;&#19982;&#25991;&#26412;&#21305;&#37197;&#20013;&#30340;&#32452;&#21512;&#27867;&#21270;&#33021;&#21147;&#65292;&#26412;&#25991;&#20174;&#22240;&#26524;&#20851;&#31995;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#35813;&#38382;&#39064;&#65306;&#21333;&#20010;&#23454;&#20307;&#30340;&#38169;&#35823;&#35821;&#20041;&#26412;&#36136;&#19978;&#26159;&#23548;&#33268;&#21305;&#37197;&#22833;&#36133;&#30340;&#28151;&#28102;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#8220;&#26080;&#38656;&#35757;&#32451;&#8221;&#30340;&#32452;&#21512; CLIP &#27169;&#22411;&#65288;ComCLIP&#65289;&#12290;ComCLIP&#23558;&#36755;&#20837;&#22270;&#20687;&#20998;&#35299;&#20026;&#20027;&#20307;&#12289;&#23545;&#35937;&#21644;&#21160;&#20316;&#23376;&#22270;&#20687;&#65292;&#24182;&#32452;&#21512; CLIP &#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#25991;&#26412;&#32534;&#30721;&#22120;&#65292;&#20197;&#22312;&#32452;&#21512;&#25991;&#26412;&#23884;&#20837;&#21644;&#23376;&#22270;&#20687;&#23884;&#20837;&#20043;&#19978;&#36827;&#34892;&#36880;&#27493;&#21305;&#37197;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ComCLIP &#21487;&#20197;&#20943;&#36731;&#20266;&#21305;&#37197;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Language-Image Pretraining (CLIP) has demonstrated great zero-shot performance for matching images and text. However, it is still challenging to adapt vision-lanaguage pretrained models like CLIP to compositional image and text matching -- a more challenging image and text matching task requiring the model understanding of compositional word concepts and visual components. Towards better compositional generalization in zero-shot image and text matching, in this paper, we study the problem from a causal perspective: the erroneous semantics of individual entities are essentially confounders that cause the matching failure. Therefore, we propose a novel \textbf{\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP disentangles input images into subjects, objects, and action sub-images and composes CLIP's vision encoder and text encoder to perform evolving matching over compositional text embedding and sub-image embeddings. In this way, ComCLIP can mitigate spurio
&lt;/p&gt;</description></item></channel></rss>