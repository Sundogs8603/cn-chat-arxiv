<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>BioImage.IO Chatbot &#26159;&#19968;&#20010;&#26681;&#25454;&#29992;&#25143;&#20010;&#24615;&#21270;&#38656;&#27714;&#25552;&#20379;&#31572;&#26696;&#30340;AI&#32842;&#22825;&#21161;&#25163;&#65292;&#36890;&#36807;&#27719;&#38598;&#21644;&#35299;&#37322;&#22810;&#20010;&#25968;&#25454;&#24211;&#12289;&#24037;&#20855;&#25991;&#26723;&#21644;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#31038;&#21306;&#36129;&#29486;&#30340;&#30693;&#35782;&#24211;&#21644;&#20248;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#20026;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#30340;&#20351;&#29992;&#32773;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20307;&#39564;&#65292;&#20026;&#21487;&#35775;&#38382;&#30340;&#31185;&#23398;&#30740;&#31350;&#35774;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;</title><link>http://arxiv.org/abs/2310.18351</link><description>&lt;p&gt;
BioImage.IO Chatbot: &#19968;&#20010;&#20197;&#31038;&#21306;&#30693;&#35782;&#24211;&#22686;&#24378;&#30340;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#20010;&#20154;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
BioImage.IO Chatbot: A Personalized Assistant for BioImage Analysis Augmented by Community Knowledge Base. (arXiv:2310.18351v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.18351
&lt;/p&gt;
&lt;p&gt;
BioImage.IO Chatbot &#26159;&#19968;&#20010;&#26681;&#25454;&#29992;&#25143;&#20010;&#24615;&#21270;&#38656;&#27714;&#25552;&#20379;&#31572;&#26696;&#30340;AI&#32842;&#22825;&#21161;&#25163;&#65292;&#36890;&#36807;&#27719;&#38598;&#21644;&#35299;&#37322;&#22810;&#20010;&#25968;&#25454;&#24211;&#12289;&#24037;&#20855;&#25991;&#26723;&#21644;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#20197;&#21450;&#31038;&#21306;&#36129;&#29486;&#30340;&#30693;&#35782;&#24211;&#21644;&#20248;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#20026;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#30340;&#20351;&#29992;&#32773;&#25552;&#20379;&#20102;&#20010;&#24615;&#21270;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20307;&#39564;&#65292;&#20026;&#21487;&#35775;&#38382;&#30340;&#31185;&#23398;&#30740;&#31350;&#35774;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#25193;&#23637;&#30340;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#26223;&#35266;&#32473;&#19987;&#23478;&#21644;&#26032;&#26469;&#32773;&#37117;&#24102;&#26469;&#20102;&#23548;&#33322;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#25628;&#32034;&#26041;&#27861;&#22312;&#36825;&#20010;&#22797;&#26434;&#29615;&#22659;&#20013;&#24120;&#24120;&#26080;&#27861;&#25552;&#20379;&#24110;&#21161;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;BioImage.IO Chatbot&#65292;&#19968;&#20010;&#20026;&#29983;&#29289;&#22270;&#20687;&#31038;&#21306;&#37327;&#36523;&#23450;&#21046;&#30340;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#23545;&#35805;&#21161;&#25163;&#12290;&#36825;&#20010;&#32842;&#22825;&#26426;&#22120;&#20154;&#24314;&#31435;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#65292;&#36890;&#36807;&#32858;&#21512;&#21644;&#35299;&#37322;&#26469;&#33258;&#22810;&#20010;&#25968;&#25454;&#24211;&#12289;&#29305;&#23450;&#24037;&#20855;&#25991;&#26723;&#21644;&#32467;&#26500;&#21270;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#31572;&#26696;&#12290;&#36890;&#36807;&#31038;&#21306;&#36129;&#29486;&#30340;&#30693;&#35782;&#24211;&#21644;&#32463;&#36807;&#20248;&#21270;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;BioImage.IO Chatbot &#19981;&#20165;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#20114;&#21160;&#65292;&#36824;&#25552;&#20379;&#20016;&#23500;&#30340;&#30693;&#35782;&#12289;&#19978;&#19979;&#25991;&#24863;&#30693;&#30340;&#20307;&#39564;&#12290;&#23427;&#20174;&#26681;&#26412;&#19978;&#25913;&#21464;&#20102;&#29983;&#29289;&#23398;&#23478;&#12289;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24072;&#21644;&#24320;&#21457;&#32773;&#23548;&#33322;&#21644;&#21033;&#29992;&#20808;&#36827;&#30340;&#29983;&#29289;&#22270;&#20687;&#20998;&#26512;&#24037;&#20855;&#30340;&#26041;&#24335;&#65292;&#20026;&#31038;&#21306;&#39537;&#21160;&#30340;&#21487;&#35775;&#38382;&#31185;&#23398;&#30740;&#31350;&#26641;&#31435;&#20102;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rapidly expanding landscape of bioimage analysis tools presents a navigational challenge for both experts and newcomers. Traditional search methods often fall short in assisting users in this complex environment. To address this, we introduce the BioImage$.$IO Chatbot, an AI-driven conversational assistant tailored for the bioimage community. Built upon large language models, this chatbot provides personalized, context-aware answers by aggregating and interpreting information from diverse databases, tool-specific documentation, and structured data sources. Enhanced by a community-contributed knowledge base and fine-tuned retrieval methods, the BioImage$.$IO Chatbot offers not just a personalized interaction but also a knowledge-enriched, context-aware experience. It fundamentally transforms the way biologists, bioimage analysts, and developers navigate and utilize advanced bioimage analysis tools, setting a new standard for community-driven, accessible scientific research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26497;&#20540;&#29702;&#35770;&#20998;&#26512;&#21442;&#25968;&#25935;&#24863;&#24615;&#25490;&#21517;&#65292;&#26088;&#22312;&#22635;&#34917;&#23545;&#21442;&#25968;&#25935;&#24863;&#24615;&#25490;&#21517;&#22914;&#20309;&#25214;&#21040;&#23548;&#33268;&#38169;&#35823;&#35782;&#21035;&#30340;&#28388;&#27874;&#22120;&#30340;&#29702;&#35299;&#30340;&#30693;&#35782;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2310.17951</link><description>&lt;p&gt;
&#36890;&#36807;&#26497;&#20540;&#29702;&#35770;&#29702;&#35299;&#21442;&#25968;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding Parameter Saliency via Extreme Value Theory. (arXiv:2310.17951v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.17951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26497;&#20540;&#29702;&#35770;&#20998;&#26512;&#21442;&#25968;&#25935;&#24863;&#24615;&#25490;&#21517;&#65292;&#26088;&#22312;&#22635;&#34917;&#23545;&#21442;&#25968;&#25935;&#24863;&#24615;&#25490;&#21517;&#22914;&#20309;&#25214;&#21040;&#23548;&#33268;&#38169;&#35823;&#35782;&#21035;&#30340;&#28388;&#27874;&#22120;&#30340;&#29702;&#35299;&#30340;&#30693;&#35782;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#31038;&#20250;&#20013;&#36234;&#26469;&#36234;&#24120;&#35265;&#12290;&#22312;&#35786;&#26029;&#19981;&#33391;&#27169;&#22411;&#34892;&#20026;&#26102;&#65292;&#35782;&#21035;&#21738;&#20123;&#21442;&#25968;&#20250;&#35302;&#21457;&#38169;&#35823;&#20998;&#31867;&#26159;&#24456;&#26377;&#29992;&#30340;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21442;&#25968;&#25935;&#24863;&#24615;&#30340;&#27010;&#24565;&#65292;&#24182;&#36890;&#36807;&#25490;&#21517;&#21487;&#33021;&#23548;&#33268;&#38169;&#35823;&#20998;&#31867;&#30340;&#21367;&#31215;&#28388;&#27874;&#22120;&#26469;&#35786;&#26029;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#12290;&#30740;&#31350;&#36824;&#34920;&#26126;&#65292;&#23545;&#25490;&#21517;&#38752;&#21069;&#30340;&#25935;&#24863;&#28388;&#27874;&#22120;&#36827;&#34892;&#24494;&#35843;&#21487;&#20197;&#26377;&#25928;&#32416;&#27491;&#22312;ImageNet&#19978;&#30340;&#38169;&#35823;&#35782;&#21035;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#20110;&#29702;&#35299;&#21442;&#25968;&#25935;&#24863;&#24615;&#25490;&#21517;&#22914;&#20309;&#25214;&#21040;&#23548;&#33268;&#38169;&#35823;&#35782;&#21035;&#30340;&#28388;&#27874;&#22120;&#20173;&#23384;&#22312;&#30693;&#35782;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#35797;&#22270;&#36890;&#36807;&#20174;&#32479;&#35745;&#35282;&#24230;(&#26497;&#20540;&#29702;&#35770;)&#20998;&#26512;&#21442;&#25968;&#25935;&#24863;&#24615;&#25490;&#21517;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#29616;&#26377;&#30740;&#31350;&#38544;&#21547;&#22320;&#20551;&#35774;&#27599;&#20010;&#28388;&#27874;&#22120;&#30340;&#26799;&#24230;&#33539;&#25968;&#26381;&#20174;&#27491;&#24577;&#20998;&#24067;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#38416;&#26126;&#20102;&#26799;&#24230;&#33539;&#25968;&#19982;&#26497;&#20540;&#29702;&#35770;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are being increasingly implemented throughout society in recent years. It is useful to identify which parameters trigger misclassification in diagnosing undesirable model behaviors. The concept of parameter saliency is proposed and used to diagnose convolutional neural networks (CNNs) by ranking convolution filters that may have caused misclassification on the basis of parameter saliency. It is also shown that fine-tuning the top ranking salient filters has efficiently corrected misidentification on ImageNet. However, there is still a knowledge gap in terms of understanding why parameter saliency ranking can find the filters inducing misidentification. In this work, we attempt to bridge the gap by analyzing parameter saliency ranking from a statistical viewpoint, namely, extreme value theory. We first show that the existing work implicitly assumes that the gradient norm computed for each filter follows a normal distribution. Then, we clarify the relationship betwee
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#20559;&#21521;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2310.16546</link><description>&lt;p&gt;
&#20048;&#35266;&#20027;&#20041;&#30340;&#38519;&#38449;&#65306;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pitfall of Optimism: Distributional Reinforcement Learning by Randomizing Risk Criterion. (arXiv:2310.16546v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.16546
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20197;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#20559;&#21521;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#20998;&#24067;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#35797;&#22270;&#21033;&#29992;&#20272;&#35745;&#30340;&#19981;&#30830;&#23450;&#24615;&#36827;&#34892;&#25506;&#32034;&#65292;&#22914;&#22312;&#38754;&#23545;&#19981;&#30830;&#23450;&#24615;&#26102;&#30340;&#20048;&#35266;&#20027;&#20041;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20272;&#35745;&#30340;&#26041;&#24046;&#36827;&#34892;&#20048;&#35266;&#25506;&#32034;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#25910;&#38598;&#30340;&#20559;&#24046;&#65292;&#38459;&#30861;&#25910;&#25947;&#25110;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#38543;&#26426;&#21270;&#39118;&#38505;&#26631;&#20934;&#26469;&#36873;&#25321;&#21160;&#20316;&#65292;&#36991;&#20813;&#22312;&#39118;&#38505;&#19978;&#30340;&#21333;&#21521;&#20542;&#21521;&#12290;&#25105;&#20204;&#36890;&#36807;&#25197;&#26354;&#39118;&#38505;&#24230;&#37327;&#25552;&#20379;&#20102;&#19968;&#20010;&#25200;&#21160;&#30340;&#20998;&#24067;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#31639;&#23376;&#65292;&#24182;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#20855;&#26377;&#36739;&#24369;&#30340;&#25910;&#32553;&#24615;&#36136;&#30340;&#25910;&#25947;&#24615;&#21644;&#26368;&#20248;&#24615;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#25903;&#25345;&#65292;&#25152;&#25552;&#26041;&#27861;&#19981;&#20250;&#38519;&#20837;&#20559;&#21521;&#24615;&#30340;&#25506;&#32034;&#65292;&#24182;&#30830;&#20445;&#25910;&#25947;&#21040;&#26368;&#20248;&#22238;&#25253;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21253;&#25324;Atari 55&#28216;&#25103;&#22312;&#20869;&#30340;&#21508;&#31181;&#29615;&#22659;&#20013;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#20854;&#20182;&#29616;&#26377;&#30340;&#22522;&#20110;&#20998;&#24067;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distributional reinforcement learning algorithms have attempted to utilize estimated uncertainty for exploration, such as optimism in the face of uncertainty. However, using the estimated variance for optimistic exploration may cause biased data collection and hinder convergence or performance. In this paper, we present a novel distributional reinforcement learning algorithm that selects actions by randomizing risk criterion to avoid one-sided tendency on risk. We provide a perturbed distributional Bellman optimality operator by distorting the risk measure and prove the convergence and optimality of the proposed method with the weaker contraction property. Our theoretical results support that the proposed method does not fall into biased exploration and is guaranteed to converge to an optimal return. Finally, we empirically show that our method outperforms other existing distribution-based algorithms in various environments including Atari 55 games.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28431;&#27934;&#65292;&#24182;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#27700;&#21360;&#26426;&#21046;&#23481;&#26131;&#34987;&#23545;&#25163;&#30772;&#35299;&#12290;</title><link>http://arxiv.org/abs/2310.07726</link><description>&lt;p&gt;
&#23545;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28431;&#27934;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards the Vulnerability of Watermarking Artificial Intelligence Generated Content. (arXiv:2310.07726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07726
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#27700;&#21360;&#25216;&#26415;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#30340;&#28431;&#27934;&#65292;&#24182;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#27700;&#21360;&#26426;&#21046;&#23481;&#26131;&#34987;&#23545;&#25163;&#30772;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#35768;&#22810;&#21830;&#19994;&#26381;&#21153;&#24050;&#32463;&#25512;&#20986;&#12290;&#36825;&#20123;&#26381;&#21153;&#21033;&#29992;&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20026;&#29992;&#25143;&#29983;&#25104;&#21019;&#24847;&#20869;&#23481;&#65288;&#20363;&#22914;&#36924;&#30495;&#30340;&#22270;&#20687;&#12289;&#27969;&#30021;&#30340;&#21477;&#23376;&#65289;&#12290;&#23545;&#20110;&#27492;&#31867;&#29983;&#25104;&#20869;&#23481;&#30340;&#20351;&#29992;&#38656;&#35201;&#39640;&#24230;&#30417;&#31649;&#65292;&#22240;&#20026;&#26381;&#21153;&#25552;&#20379;&#21830;&#38656;&#35201;&#30830;&#20445;&#29992;&#25143;&#19981;&#36829;&#21453;&#20351;&#29992;&#25919;&#31574;&#65288;&#20363;&#22914;&#28389;&#29992;&#21830;&#19994;&#21270;&#12289;&#29983;&#25104;&#21644;&#20998;&#21457;&#19981;&#23433;&#20840;&#30340;&#20869;&#23481;&#65289;&#12290;&#26368;&#36817;&#25552;&#20986;&#20102;&#35768;&#22810;&#27700;&#21360;&#25216;&#26415;&#65292;&#20294;&#26159;&#26412;&#25991;&#34920;&#26126;&#23545;&#25163;&#21487;&#20197;&#36731;&#26131;&#30772;&#35299;&#36825;&#20123;&#27700;&#21360;&#26426;&#21046;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#21487;&#33021;&#30340;&#25915;&#20987;&#26041;&#24335;&#65306;&#65288;1&#65289;&#27700;&#21360;&#21435;&#38500;&#65306;&#23545;&#25163;&#21487;&#20197;&#36731;&#26494;&#22320;&#20174;&#29983;&#25104;&#20869;&#23481;&#20013;&#21024;&#38500;&#23884;&#20837;&#30340;&#27700;&#21360;&#65292;&#28982;&#21518;&#33258;&#30001;&#20351;&#29992;&#32780;&#19981;&#21463;&#26381;&#21153;&#25552;&#20379;&#21830;&#30340;&#38480;&#21046;&#65307;&#65288;2&#65289;&#27700;&#21360;&#20266;&#36896;&#65306;&#23545;&#25163;&#21487;&#20197;&#21019;&#24314;&#38750;&#27861;&#30340;&#27700;&#21360;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence Generated Content (AIGC) is gaining great popularity in social media, with many commercial services available. These services leverage advanced generative models, such as latent diffusion models and large language models, to generate creative content (e.g., realistic images, fluent sentences) for users. The usage of such generated content needs to be highly regulated, as the service providers need to ensure the users do not violate the usage policies (e.g., abuse for commercialization, generating and distributing unsafe content).  Numerous watermarking approaches have been proposed recently. However, in this paper, we show that an adversary can easily break these watermarking mechanisms. Specifically, we consider two possible attacks. (1) Watermark removal: the adversary can easily erase the embedded watermark from the generated content and then use it freely without the regulation of the service provider. (2) Watermark forge: the adversary can create illegal co
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;</title><link>http://arxiv.org/abs/2310.04406</link><description>&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#32479;&#19968;&#20102;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25512;&#29702;&#12289;&#34892;&#21160;&#21644;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Language Agent Tree Search Unifies Reasoning Acting and Planning in Language Models. (arXiv:2310.04406v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04406
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65288;LATS&#65289;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30456;&#20114;&#21327;&#21516;&#65292;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#23454;&#29616;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#12290;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;LATS&#22312;&#22810;&#20010;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#24615;&#65292;&#29305;&#21035;&#22312;&#32534;&#31243;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#19968;&#31995;&#21015;&#20915;&#31574;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#20294;&#23427;&#20204;&#20381;&#36182;&#20110;&#31616;&#21333;&#30340;&#34892;&#21160;&#36807;&#31243;&#65292;&#24182;&#26410;&#33021;&#24191;&#27867;&#37096;&#32626;&#20316;&#20026;&#33258;&#20027;&#20195;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;LATS&#65288;&#35821;&#35328;&#20195;&#29702;&#26641;&#25628;&#32034;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#23558;LLMs&#22312;&#35268;&#21010;&#12289;&#34892;&#21160;&#21644;&#25512;&#29702;&#26041;&#38754;&#30340;&#33021;&#21147;&#30456;&#20114;&#21327;&#21516;&#12290;LATS&#20511;&#37492;&#20102;&#27169;&#22411;&#23548;&#21521;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#30340;&#24605;&#24819;&#65292;&#23558;LLMs&#29992;&#20316;&#20195;&#29702;&#12289;&#20215;&#20540;&#20989;&#25968;&#21644;&#20248;&#21270;&#22120;&#65292;&#37325;&#26032;&#21033;&#29992;&#20854;&#28508;&#22312;&#30340;&#20248;&#21183;&#20197;&#25552;&#21319;&#20915;&#31574;&#33021;&#21147;&#12290;&#20851;&#38190;&#30340;&#19968;&#28857;&#26159;LATS&#20351;&#29992;&#19968;&#20010;&#20855;&#26377;&#22806;&#37096;&#21453;&#39304;&#30340;&#29615;&#22659;&#65292;&#36825;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#21152;&#28145;&#24605;&#29087;&#34385;&#21644;&#36866;&#24212;&#24615;&#30340;&#38382;&#39064;&#35299;&#20915;&#26426;&#21046;&#65292;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#32534;&#31243;&#12289;HotPotQA&#21644;WebShop&#31561;&#22810;&#20010;&#39046;&#22495;&#36827;&#34892;&#20102;&#23454;&#39564;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;LATS&#22312;&#25512;&#29702;&#21644;&#34892;&#21160;&#26041;&#38754;&#30340;&#36866;&#29992;&#24615;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#32534;&#31243;&#26041;&#38754;&#65292;LATS&#23454;&#29616;&#20102;94.4%&#30340;&#20934;&#30830;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) have demonstrated impressive performance on a range of decision-making tasks, they rely on simple acting processes and fall short of broad deployment as autonomous agents. We introduce LATS (Language Agent Tree Search), a general framework that synergizes the capabilities of LLMs in planning, acting, and reasoning. Drawing inspiration from Monte Carlo tree search in model-based reinforcement learning, LATS employs LLMs as agents, value functions, and optimizers, repurposing their latent strengths for enhanced decision-making. What is crucial in this method is the use of an environment for external feedback, which offers a more deliberate and adaptive problem-solving mechanism that moves beyond the limitations of existing techniques. Our experimental evaluation across diverse domains, such as programming, HotPotQA, and WebShop, illustrates the applicability of LATS for both reasoning and acting. In particular, LATS achieves 94.4\% for programming on Hu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;OpenAI GPT-3.5&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39550;&#39542;&#36712;&#36857;&#65292;&#25552;&#39640;&#20102;&#36816;&#21160;&#35268;&#21010;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.01415</link><description>&lt;p&gt;
GPT-Driver: &#20351;&#29992;GPT&#23398;&#20064;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
GPT-Driver: Learning to Drive with GPT. (arXiv:2310.01415v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01415
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;OpenAI GPT-3.5&#27169;&#22411;&#24212;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#36816;&#21160;&#35268;&#21010;&#36716;&#21270;&#20026;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#39550;&#39542;&#36712;&#36857;&#65292;&#25552;&#39640;&#20102;&#36816;&#21160;&#35268;&#21010;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;OpenAI GPT-3.5&#27169;&#22411;&#36716;&#21270;&#20026;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#30340;&#21487;&#38752;&#36816;&#21160;&#35268;&#21010;&#22120;&#12290;&#36816;&#21160;&#35268;&#21010;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#26680;&#24515;&#25361;&#25112;&#65292;&#26088;&#22312;&#35268;&#21010;&#19968;&#20010;&#23433;&#20840;&#33298;&#36866;&#30340;&#39550;&#39542;&#36712;&#36857;&#12290;&#29616;&#26377;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#20027;&#35201;&#21033;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#39044;&#27979;&#39550;&#39542;&#36712;&#36857;&#65292;&#28982;&#32780;&#36825;&#20123;&#26041;&#27861;&#22312;&#38754;&#23545;&#26032;&#39062;&#21644;&#26410;&#30693;&#30340;&#39550;&#39542;&#22330;&#26223;&#26102;&#23637;&#29616;&#20986;&#19981;&#36275;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22266;&#26377;&#30340;&#24378;&#22823;&#25512;&#29702;&#33021;&#21147;&#21644;&#27867;&#21270;&#28508;&#21147;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#22522;&#26412;&#35265;&#35299;&#26159;&#23558;&#36816;&#21160;&#35268;&#21010;&#37325;&#26032;&#26500;&#24314;&#20026;&#19968;&#20010;&#35821;&#35328;&#24314;&#27169;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20043;&#21069;&#26410;&#34987;&#25506;&#32034;&#30340;&#35270;&#35282;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#35268;&#21010;&#22120;&#30340;&#36755;&#20837;&#21644;&#36755;&#20986;&#34920;&#31034;&#20026;&#35821;&#35328;&#35760;&#21495;&#65292;&#24182;&#21033;&#29992;LLM&#36890;&#36807;&#23545;&#22352;&#26631;&#30340;&#35821;&#35328;&#25551;&#36848;&#29983;&#25104;&#39550;&#39542;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a simple yet effective approach that can transform the OpenAI GPT-3.5 model into a reliable motion planner for autonomous vehicles. Motion planning is a core challenge in autonomous driving, aiming to plan a driving trajectory that is safe and comfortable. Existing motion planners predominantly leverage heuristic methods to forecast driving trajectories, yet these approaches demonstrate insufficient generalization capabilities in the face of novel and unseen driving scenarios. In this paper, we propose a novel approach to motion planning that capitalizes on the strong reasoning capabilities and generalization potential inherent to Large Language Models (LLMs). The fundamental insight of our approach is the reformulation of motion planning as a language modeling problem, a perspective not previously explored. Specifically, we represent the planner inputs and outputs as language tokens, and leverage the LLM to generate driving trajectories through a language description of coo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#25193;&#25955;&#26725;&#27169;&#22411;&#65288;DDBMs&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#26725;&#30340;&#20998;&#25968;&#65292;&#24182;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#20998;&#25968;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#26469;&#23454;&#29616;&#20174;&#19968;&#20010;&#20998;&#24067;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#23558;&#22810;&#31867;&#29983;&#25104;&#27169;&#22411;&#32479;&#19968;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2309.16948</link><description>&lt;p&gt;
&#21435;&#22122;&#25193;&#25955;&#26725;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Denoising Diffusion Bridge Models. (arXiv:2309.16948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.16948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21435;&#22122;&#25193;&#25955;&#26725;&#27169;&#22411;&#65288;DDBMs&#65289;&#65292;&#35813;&#27169;&#22411;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#26725;&#30340;&#20998;&#25968;&#65292;&#24182;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#20998;&#25968;&#27714;&#35299;&#24494;&#20998;&#26041;&#31243;&#26469;&#23454;&#29616;&#20174;&#19968;&#20010;&#20998;&#24067;&#21040;&#21478;&#19968;&#20010;&#20998;&#24067;&#30340;&#26144;&#23556;&#65292;&#20174;&#32780;&#23558;&#22810;&#31867;&#29983;&#25104;&#27169;&#22411;&#32479;&#19968;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#26159;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#38543;&#26426;&#36807;&#31243;&#23558;&#22122;&#22768;&#26144;&#23556;&#21040;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35768;&#22810;&#24212;&#29992;&#65292;&#22914;&#22270;&#20687;&#32534;&#36753;&#65292;&#27169;&#22411;&#30340;&#36755;&#20837;&#19981;&#26159;&#38543;&#26426;&#22122;&#22768;&#30340;&#20998;&#24067;&#12290;&#22240;&#27492;&#65292;&#25193;&#25955;&#27169;&#22411;&#24517;&#39035;&#20381;&#36182;&#20110;&#32321;&#29712;&#30340;&#26041;&#27861;&#65292;&#22914;&#24341;&#23548;&#25110;&#25237;&#24433;&#37319;&#26679;&#65292;&#20197;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#21152;&#20837;&#36825;&#20123;&#20449;&#24687;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21435;&#22122;&#25193;&#25955;&#26725;&#27169;&#22411;&#65288;DDBMs&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#26725;&#30340;&#33539;&#20363;&#65292;&#25193;&#25955;&#26725;&#26159;&#19968;&#26063;&#36807;&#31243;&#65292;&#20854;&#22312;&#32473;&#23450;&#30340;&#31471;&#28857;&#19979;&#25554;&#20540;&#20004;&#20010;&#37197;&#23545;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23398;&#20064;&#25193;&#25955;&#26725;&#30340;&#20998;&#25968;&#65292;&#22522;&#20110;&#23398;&#20064;&#21040;&#30340;&#20998;&#25968;&#27714;&#35299;&#19968;&#20010;&#65288;&#38543;&#26426;&#30340;&#65289;&#24494;&#20998;&#26041;&#31243;&#65292;&#20174;&#19968;&#20010;&#31471;&#28857;&#20998;&#24067;&#26144;&#23556;&#21040;&#21478;&#19968;&#20010;&#31471;&#28857;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33258;&#28982;&#22320;&#32479;&#19968;&#20102;&#20960;&#31867;&#29983;&#25104;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#21644;OT-Flow-Matching&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#29616;&#26377;&#30340;&#35774;&#35745;&#21644;&#26550;&#26500;&#36873;&#25321;&#36866;&#24212;&#21040;&#26356;&#36890;&#29992;&#30340;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models are powerful generative models that map noise to data using stochastic processes. However, for many applications such as image editing, the model input comes from a distribution that is not random noise. As such, diffusion models must rely on cumbersome methods like guidance or projected sampling to incorporate this information in the generative process. In our work, we propose Denoising Diffusion Bridge Models (DDBMs), a natural alternative to this paradigm based on diffusion bridges, a family of processes that interpolate between two paired distributions given as endpoints. Our method learns the score of the diffusion bridge from data and maps from one endpoint distribution to the other by solving a (stochastic) differential equation based on the learned score. Our method naturally unifies several classes of generative models, such as score-based diffusion models and OT-Flow-Matching, allowing us to adapt existing design and architectural choices to our more general 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#30340;&#25345;&#32493;&#25919;&#31574;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38381;&#29615;&#33258;&#21160;&#39550;&#39542;&#20013;&#24341;&#20837;&#32972;&#26223;&#36710;&#36742;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20043;&#38388;&#30340;&#21338;&#24328;&#24335;&#20132;&#20114;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#38271;&#23614;&#20998;&#24067;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.14235</link><description>&lt;p&gt;
Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#22330;&#26223;&#30340;&#38381;&#29615;&#33258;&#21160;&#39550;&#39542;&#20013;&#30340;&#25345;&#32493;&#25919;&#31574;&#25913;&#36827;
&lt;/p&gt;
&lt;p&gt;
Stackelberg Driver Model for Continual Policy Improvement in Scenario-Based Closed-Loop Autonomous Driving. (arXiv:2309.14235v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14235
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#30340;&#25345;&#32493;&#25919;&#31574;&#25913;&#36827;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#38381;&#29615;&#33258;&#21160;&#39550;&#39542;&#20013;&#24341;&#20837;&#32972;&#26223;&#36710;&#36742;&#21644;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20043;&#38388;&#30340;&#21338;&#24328;&#24335;&#20132;&#20114;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#35299;&#20915;&#38271;&#23614;&#20998;&#24067;&#39550;&#39542;&#22330;&#26223;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#65288;AVs&#65289;&#30340;&#37096;&#32626;&#38754;&#20020;&#30528;&#22256;&#38590;&#65292;&#22240;&#20026;&#38271;&#23614;&#20998;&#24067;&#30340;&#39550;&#39542;&#22330;&#26223;&#20013;&#23384;&#22312;&#32597;&#35265;&#20294;&#20851;&#38190;&#30340;&#36793;&#38469;&#24773;&#20917;&#65292;&#36825;&#20250;&#23545;&#23427;&#20204;&#30340;&#25972;&#20307;&#24615;&#33021;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#65292;&#23545;&#25239;&#24615;&#29983;&#25104;&#26041;&#27861;&#24050;&#32463;&#25104;&#20026;&#19968;&#31867;&#26377;&#25928;&#30340;&#36884;&#24452;&#65292;&#29992;&#20110;&#21512;&#25104;AV&#27979;&#35797;&#30340;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#29983;&#25104;&#30340;&#22330;&#26223;&#36890;&#24120;&#34987;&#29992;&#20110;AV&#35757;&#32451;&#30340;&#26426;&#20250;&#26377;&#38480;&#65292;&#36896;&#25104;&#20102;&#25345;&#32493;AV&#25919;&#31574;&#25913;&#36827;&#30340;&#28508;&#21147;&#26410;&#34987;&#20805;&#20998;&#21033;&#29992;&#65292;&#21516;&#26102;&#20063;&#32570;&#20047;&#38381;&#29615;&#35774;&#35745;&#26469;&#23454;&#29616;&#36825;&#19968;&#25913;&#36827;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#23558;Stackelberg&#39550;&#39542;&#21592;&#27169;&#22411;&#65288;SDM&#65289;&#36827;&#34892;&#35843;&#25972;&#65292;&#20197;&#20934;&#30830;&#25551;&#36848;&#36710;&#36742;&#20132;&#20114;&#21160;&#21147;&#23398;&#30340;&#23618;&#27425;&#24615;&#36136;&#65292;&#36890;&#36807;&#23558;&#32972;&#26223;&#36710;&#36742;&#65288;BVs&#65289;&#21644;AV&#22312;&#19968;&#31181;&#39034;&#24207;&#21338;&#24328;&#24335;&#30340;&#20132;&#20114;&#33539; Paradigm&#20869;&#36827;&#34892;&#36845;&#20195;&#25913;&#36827;&#12290;&#36890;&#36807;AV&#20805;&#24403;&#39046;&#23548;&#32773;&#65292;BVs&#20316;&#20026;&#36861;&#38543;&#32773;&#65292;&#36825;&#31181;&#39046;&#23548;&#32773;-&#36861;&#38543;&#32773;&#27169;&#22411;&#30830;&#20445;&#20102;AV&#22987;&#32456;&#20445;&#25345;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deployment of autonomous vehicles (AVs) has faced hurdles due to the dominance of rare but critical corner cases within the long-tail distribution of driving scenarios, which negatively affects their overall performance. To address this challenge, adversarial generation methods have emerged as a class of efficient approaches to synthesize safety-critical scenarios for AV testing. However, these generated scenarios are often underutilized for AV training, resulting in the potential for continual AV policy improvement remaining untapped, along with a deficiency in the closed-loop design needed to achieve it. Therefore, we tailor the Stackelberg Driver Model (SDM) to accurately characterize the hierarchical nature of vehicle interaction dynamics, facilitating iterative improvement by engaging background vehicles (BVs) and AV in a sequential game-like interaction paradigm. With AV acting as the leader and BVs as followers, this leader-follower modeling ensures that AV would consistentl
&lt;/p&gt;</description></item><item><title>LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.12307</link><description>&lt;p&gt;
LongLoRA: &#39640;&#25928;&#30340;&#38271;&#19978;&#19979;&#25991;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. (arXiv:2309.12307v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12307
&lt;/p&gt;
&lt;p&gt;
LongLoRA&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#23427;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#23454;&#29616;&#31934;&#32454;&#35843;&#25972;&#65292;&#24182;&#20351;&#29992;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#23454;&#29616;&#19978;&#19979;&#25991;&#25193;&#23637;&#65292;&#19982;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31934;&#32454;&#35843;&#25972;&#26041;&#27861;&#8212;&#8212;LongLoRA&#65292;&#21487;&#20197;&#22312;&#26377;&#38480;&#30340;&#35745;&#31639;&#25104;&#26412;&#19979;&#25193;&#23637;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#30340;&#19978;&#19979;&#25991;&#22823;&#23567;&#12290;&#36890;&#24120;&#65292;&#20351;&#29992;&#38271;&#19978;&#19979;&#25991;&#22823;&#23567;&#35757;&#32451;LLM&#30340;&#35745;&#31639;&#25104;&#26412;&#24456;&#39640;&#65292;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#26102;&#38388;&#21644;GPU&#36164;&#28304;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#20004;&#20010;&#26041;&#38754;&#21152;&#24555;&#20102;LLM&#30340;&#19978;&#19979;&#25991;&#25193;&#23637;&#12290;&#19968;&#26041;&#38754;&#65292;&#23613;&#31649;&#25512;&#29702;&#36807;&#31243;&#20013;&#38656;&#35201;&#31264;&#23494;&#30340;&#20840;&#23616;&#27880;&#24847;&#21147;&#65292;&#20294;&#27169;&#22411;&#30340;&#31934;&#32454;&#35843;&#25972;&#21487;&#20197;&#36890;&#36807;&#31232;&#30095;&#30340;&#23616;&#37096;&#27880;&#24847;&#21147;&#26377;&#25928;&#19988;&#39640;&#25928;&#22320;&#23436;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;&#31227;&#21160;&#30701;&#27880;&#24847;&#21147;&#26377;&#25928;&#22320;&#23454;&#29616;&#20102;&#19978;&#19979;&#25991;&#30340;&#25193;&#23637;&#65292;&#22312;&#19982;&#20351;&#29992;&#20256;&#32479;&#27880;&#24847;&#21147;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#26102;&#20855;&#26377;&#30456;&#20284;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#21482;&#29992;&#20004;&#34892;&#20195;&#30721;&#23454;&#29616;&#65292;&#22312;&#25512;&#29702;&#20013;&#26159;&#21487;&#36873;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#21442;&#25968;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shift short attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-effici
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2309.10313</link><description>&lt;p&gt;
&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#36827;&#34892;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating the Catastrophic Forgetting in Multimodal Large Language Models. (arXiv:2309.10313v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10313
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#38024;&#23545;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#36827;&#34892;&#30740;&#31350;&#65292;&#24341;&#20837;&#20102;EMT&#26041;&#27861;&#26469;&#35780;&#20272;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#21457;&#29616;&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#65292;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;&#27169;&#22411;&#37117;&#26080;&#27861;&#20445;&#25345;&#19982;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#24494;&#35843;&#38454;&#27573;&#23545;&#24615;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;GPT4&#30340;&#25104;&#21151;&#20043;&#21518;&#65292;&#22810;&#27169;&#24577;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;MLLM&#65289;&#30740;&#31350;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#19968;&#30740;&#31350;&#26041;&#21521;&#20391;&#37325;&#20110;&#36890;&#36807;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;LLM&#21644;&#35270;&#35273;&#27169;&#22411;&#26469;&#24320;&#21457;&#36890;&#29992;&#30340;LLM&#12290;&#28982;&#32780;&#65292;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#21363;&#24494;&#35843;&#27169;&#22411;&#26080;&#27861;&#20445;&#25345;&#19982;&#39044;&#35757;&#32451;&#27169;&#22411;&#30456;&#20284;&#30340;&#24615;&#33021;&#27700;&#24179;&#65292;&#20173;&#28982;&#26159;&#22810;&#27169;&#24577;LLM&#65288;MLLM&#65289;&#20013;&#30340;&#19968;&#20010;&#22266;&#26377;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;EMT&#65306;&#29992;&#20110;&#35780;&#20272;MLLM&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#35780;&#20272;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;MLLM&#20316;&#20026;&#19968;&#20010;&#22270;&#20687;&#20998;&#31867;&#22120;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#24212;&#29992;EMT&#26469;&#35780;&#20272;&#20960;&#20010;&#24320;&#28304;&#30340;&#24494;&#35843;MLLM&#65292;&#24182;&#21457;&#29616;&#20960;&#20046;&#25152;&#26377;&#35780;&#20272;&#30340;MLLM&#22312;&#26631;&#20934;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#26080;&#27861;&#20445;&#25345;&#19982;&#20182;&#20204;&#30340;&#35270;&#35273;&#32534;&#30721;&#22120;&#30456;&#21516;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#32487;&#32493;&#24494;&#35843;LLaVA&#65292;&#19968;&#31181;MLLM&#65292;&#24182;&#21033;&#29992;EMT&#26469;&#35780;&#20272;&#25972;&#20010;&#24494;&#35843;&#36807;&#31243;&#20013;&#30340;&#24615;&#33021;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#30340;&#24494;&#35843;&#38454;&#27573;&#26159;&#20851;&#38190;&#30340;&#65292;&#36807;&#26089;&#20572;&#27490;&#24494;&#35843;&#21487;&#33021;&#23548;&#33268;&#20302;&#24615;&#33021;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Following the success of GPT4, there has been a surge in interest in multimodal large language model (MLLM) research. This line of research focuses on developing general-purpose LLMs through fine-tuning pre-trained LLMs and vision models. However, catastrophic forgetting, a notorious phenomenon where the fine-tuned model fails to retain similar performance compared to the pre-trained model, still remains an inherent problem in multimodal LLMs (MLLM). In this paper, we introduce EMT: Evaluating MulTimodality for evaluating the catastrophic forgetting in MLLMs, by treating each MLLM as an image classifier. We first apply EMT to evaluate several open-source fine-tuned MLLMs and we discover that almost all evaluated MLLMs fail to retain the same performance levels as their vision encoders on standard image classification tasks. Moreover, we continue fine-tuning LLaVA, an MLLM and utilize EMT to assess performance throughout the fine-tuning. Interestingly, our results suggest that early-sta
&lt;/p&gt;</description></item><item><title>FIMO&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;IMO&#27700;&#24179;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#65292;&#21253;&#21547;149&#20010;&#24418;&#24335;&#21270;&#25968;&#23398;&#38382;&#39064;&#38472;&#36848;&#65292;&#30446;&#21069;&#20173;&#23384;&#22312;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.04295</link><description>&lt;p&gt;
FIMO: &#19968;&#20010;&#29992;&#20110;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#30340;&#25361;&#25112;&#24418;&#24335;&#21270;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
FIMO: A Challenge Formal Dataset for Automated Theorem Proving. (arXiv:2309.04295v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.04295
&lt;/p&gt;
&lt;p&gt;
FIMO&#26159;&#19968;&#20010;&#21019;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;IMO&#27700;&#24179;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#65292;&#21253;&#21547;149&#20010;&#24418;&#24335;&#21270;&#25968;&#23398;&#38382;&#39064;&#38472;&#36848;&#65292;&#30446;&#21069;&#20173;&#23384;&#22312;&#25361;&#25112;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;FIMO&#65292;&#19968;&#20010;&#21019;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;&#22269;&#38469;&#25968;&#23398;&#22885;&#26519;&#21305;&#20811;&#31454;&#36187;&#65288;IMO&#65289;&#30340;&#20837;&#22260;&#38382;&#39064;&#20013;&#33719;&#24471;&#30340;&#24418;&#24335;&#21270;&#25968;&#23398;&#38382;&#39064;&#38472;&#36848;&#12290;FIMO&#26088;&#22312;&#20419;&#36827;IMO&#32423;&#21035;&#30340;&#39640;&#32423;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#65292;&#30446;&#21069;&#19987;&#20026;Lean&#24418;&#24335;&#35821;&#35328;&#35774;&#35745;&#12290;&#23427;&#21253;&#25324;149&#20010;&#24418;&#24335;&#21270;&#38382;&#39064;&#38472;&#36848;&#65292;&#21516;&#26102;&#38468;&#24102;&#38750;&#27491;&#24335;&#30340;&#38382;&#39064;&#25551;&#36848;&#21644;&#30456;&#24212;&#30340;&#22522;&#20110;LaTeX&#30340;&#38750;&#27491;&#24335;&#35777;&#26126;&#12290;&#36890;&#36807;&#20351;&#29992;GPT-4&#36827;&#34892;&#21021;&#27493;&#23454;&#39564;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#34920;&#26126;&#22312;&#23454;&#29616;&#20196;&#20154;&#28385;&#24847;&#30340;IMO&#32423;&#21035;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#32467;&#26524;&#20043;&#21069;&#36824;&#26377;&#24456;&#38271;&#30340;&#36335;&#35201;&#36208;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present FIMO, an innovative dataset comprising formal mathematical problem statements sourced from the International Mathematical Olympiad (IMO) Shortlisted Problems. Designed to facilitate advanced automated theorem proving at the IMO level, FIMO is currently tailored for the Lean formal language. It comprises 149 formal problem statements, accompanied by both informal problem descriptions and their corresponding LaTeX-based informal proofs. Through initial experiments involving GPT-4, our findings underscore the existing limitations in current methodologies, indicating a substantial journey ahead before achieving satisfactory IMO-level automated theorem proving outcomes.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#32467;&#26500;&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#39640;&#38454;&#20132;&#20114;&#12289;&#32858;&#31867;&#21644;&#28040;&#24687;&#20256;&#36882;&#31561;&#27493;&#39588;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#22312;&#21387;&#32553;&#20449;&#21495;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#35823;&#24046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2308.11068</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#32467;&#26500;&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;
&lt;/p&gt;
&lt;p&gt;
Topological Graph Signal Compression. (arXiv:2308.11068v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11068
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25299;&#25169;&#32467;&#26500;&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;&#26041;&#27861;&#65292;&#36890;&#36807;&#22788;&#29702;&#39640;&#38454;&#20132;&#20114;&#12289;&#32858;&#31867;&#21644;&#28040;&#24687;&#20256;&#36882;&#31561;&#27493;&#39588;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#22312;&#21387;&#32553;&#20449;&#21495;&#26102;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#35823;&#24046;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20986;&#29616;&#30340;&#25299;&#25169;&#28145;&#24230;&#23398;&#20064;&#65288;TDL&#65289;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#33258;&#28982;&#22320;&#22788;&#29702;&#39640;&#38454;&#20132;&#20114;&#65292;&#36229;&#36234;&#30001;&#22270;&#34920;&#31034;&#23450;&#20041;&#30340;&#25104;&#23545;&#20851;&#31995;&#21644;&#23616;&#37096;&#37051;&#22495;&#65292;&#20174;&#32780;&#25193;&#23637;&#24403;&#21069;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;TDL&#30340;&#22270;&#20449;&#21495;&#21387;&#32553;&#26041;&#27861;&#65292;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#27493;&#39588;&#65306;&#39318;&#20808;&#65292;&#22522;&#20110;&#21407;&#22987;&#20449;&#21495;&#25512;&#26029;&#20986;&#19981;&#30456;&#20132;&#30340;&#39640;&#38454;&#32467;&#26500;&#65292;&#36890;&#36807;&#23558;N&#20010;&#25968;&#25454;&#28857;&#32858;&#31867;&#25104;K&#20010;&#38598;&#21512;&#65307;&#28982;&#21518;&#65292;&#22522;&#20110;&#25299;&#25169;&#21551;&#31034;&#30340;&#28040;&#24687;&#20256;&#36882;&#22312;&#36825;&#20123;&#22810;&#20803;&#32032;&#38598;&#21512;&#20013;&#33719;&#24471;&#20449;&#21495;&#30340;&#21387;&#32553;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#21387;&#32553;&#26469;&#33258;&#20004;&#20010;&#30495;&#23454;&#30340;&#20114;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#21830;&#32593;&#32476;&#25968;&#25454;&#38598;&#30340;&#26102;&#38388;&#38142;&#36335;&#20449;&#21495;&#26102;&#65292;&#27604;&#26631;&#20934;&#30340;GNN&#21644;&#21069;&#39304;&#26550;&#26500;&#20855;&#26377;&#26356;&#22909;&#30340;&#37325;&#24314;&#35823;&#24046;&#8212;&#8212;&#22312;&#25152;&#26377;&#35780;&#20272;&#22330;&#26223;&#20013;&#65292;&#37325;&#24314;&#35823;&#24046;&#25552;&#39640;&#20102;&#20174;30%&#21040;90%&#12290;&#36825;&#34920;&#26126;&#23427;&#26356;&#22909;&#22320;&#25429;&#25417;&#21644;&#21033;&#29992;&#20102;&#31354;&#38388;&#21644;&#26102;&#38388;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently emerged Topological Deep Learning (TDL) methods aim to extend current Graph Neural Networks (GNN) by naturally processing higher-order interactions, going beyond the pairwise relations and local neighborhoods defined by graph representations. In this paper we propose a novel TDL-based method for compressing signals over graphs, consisting in two main steps: first, disjoint sets of higher-order structures are inferred based on the original signal --by clustering $N$ datapoints into $K\ll N$ collections; then, a topological-inspired message passing gets a compressed representation of the signal within those multi-element sets. Our results show that our framework improves both standard GNN and feed-forward architectures in compressing temporal link-based signals from two real-word Internet Service Provider Networks' datasets --from $30\%$ up to $90\%$ better reconstruction errors across all evaluation scenarios--, suggesting that it better captures and exploits spatial and tempor
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2308.06053</link><description>&lt;p&gt;
&#22312;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#19978;&#20855;&#26377;MiRo&#30340;&#25104;&#26412;&#25928;&#30410;&#30340;&#35774;&#22791;&#19978;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cost-effective On-device Continual Learning over Memory Hierarchy with Miro. (arXiv:2308.06053v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06053
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#26088;&#22312;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#25104;&#26412;&#25928;&#30410;&#12290;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#36890;&#36807;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#30340;&#26032;&#39062;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26159;&#20174;&#25345;&#32493;&#30340;&#20219;&#21153;&#27969;&#20013;&#36880;&#27493;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20026;&#20102;&#35760;&#20303;&#20808;&#21069;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20043;&#21069;&#30340;&#30740;&#31350;&#23558;&#26087;&#26679;&#26412;&#23384;&#20648;&#22312;&#19968;&#20010;&#20869;&#23384;&#23618;&#27425;&#32467;&#26500;&#20013;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#21040;&#26469;&#26102;&#36827;&#34892;&#22238;&#25918;&#12290;&#37319;&#29992;&#25345;&#32493;&#23398;&#20064;&#20197;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#36793;&#32536;&#35774;&#22791;&#36890;&#24120;&#23545;&#33021;&#28304;&#25935;&#24863;&#65292;&#22240;&#27492;&#38656;&#35201;&#22312;&#19981;&#25439;&#23475;&#33021;&#28304;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#20445;&#25345;&#39640;&#27169;&#22411;&#20934;&#30830;&#24230;&#65292;&#21363;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#26159;&#39318;&#27425;&#25506;&#32034;&#22522;&#20110;&#23618;&#27425;&#20869;&#23384;&#22238;&#25918;&#30340;&#25345;&#32493;&#23398;&#20064;&#30340;&#35774;&#35745;&#31354;&#38388;&#65292;&#20197;&#33719;&#24471;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Miro&#65292;&#19968;&#20010;&#26032;&#39062;&#30340;&#31995;&#32479;&#36816;&#34892;&#26102;&#65292;&#36890;&#36807;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#36164;&#28304;&#29366;&#24577;&#21160;&#24577;&#37197;&#32622;&#25345;&#32493;&#23398;&#20064;&#31995;&#32479;&#65292;&#20174;&#32780;&#23558;&#25105;&#20204;&#30340;&#35265;&#35299;&#31934;&#30830;&#22320;&#25972;&#21512;&#21040;&#25345;&#32493;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#25104;&#26412;&#25928;&#30410;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;Miro&#36824;&#23545;&#24102;&#26377;&#26126;&#30830;&#20934;&#30830;&#24230;-&#33021;&#37327;&#24179;&#34913;&#30340;&#21442;&#25968;&#36827;&#34892;&#22312;&#32447;&#20998;&#26512;&#65292;&#24182;&#20197;&#20302;&#24320;&#38144;&#22320;&#36866;&#24212;&#26368;&#20339;&#20540;&#12290;&#24191;&#27867;&#30340;&#35780;&#20272;&#26174;&#31034;Miro&#26126;&#26174;&#20248;&#20110;&#20854;&#20182;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning (CL) trains NN models incrementally from a continuous stream of tasks. To remember previously learned knowledge, prior studies store old samples over a memory hierarchy and replay them when new tasks arrive. Edge devices that adopt CL to preserve data privacy are typically energy-sensitive and thus require high model accuracy while not compromising energy efficiency, i.e., cost-effectiveness. Our work is the first to explore the design space of hierarchical memory replay-based CL to gain insights into achieving cost-effectiveness on edge devices. We present Miro, a novel system runtime that carefully integrates our insights into the CL framework by enabling it to dynamically configure the CL system based on resource states for the best cost-effectiveness. To reach this goal, Miro also performs online profiling on parameters with clear accuracy-energy trade-offs and adapts to optimal values with low overhead. Extensive evaluations show that Miro significantly outperfo
&lt;/p&gt;</description></item><item><title>&#26790;&#22659;&#21644;&#24819;&#35937;&#19981;&#20165;&#20165;&#26159;&#24863;&#35273;&#36755;&#20837;&#30340;&#20877;&#29616;&#65292;&#23427;&#20204;&#23545;&#20110;&#22609;&#36896;&#22823;&#33041;&#30382;&#23618;&#30340;&#35821;&#20041;&#34920;&#31034;&#21516;&#26679;&#37325;&#35201;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#34394;&#25311;&#32463;&#39564;&#26469;&#24433;&#21709;&#21644;&#32452;&#32455;&#36825;&#20123;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2308.01830</link><description>&lt;p&gt;
&#36229;&#36234;&#24863;&#35273;&#30340;&#23398;&#20064;: &#26790;&#22914;&#20309;&#32452;&#32455;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning beyond sensations: how dreams organize neuronal representations. (arXiv:2308.01830v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.01830
&lt;/p&gt;
&lt;p&gt;
&#26790;&#22659;&#21644;&#24819;&#35937;&#19981;&#20165;&#20165;&#26159;&#24863;&#35273;&#36755;&#20837;&#30340;&#20877;&#29616;&#65292;&#23427;&#20204;&#23545;&#20110;&#22609;&#36896;&#22823;&#33041;&#30382;&#23618;&#30340;&#35821;&#20041;&#34920;&#31034;&#21516;&#26679;&#37325;&#35201;&#65292;&#21487;&#20197;&#36890;&#36807;&#29983;&#25104;&#34394;&#25311;&#32463;&#39564;&#26469;&#24433;&#21709;&#21644;&#32452;&#32455;&#36825;&#20123;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32423;&#24863;&#35273;&#30382;&#23618;&#20013;&#30340;&#35821;&#20041;&#34920;&#31034;&#26159;&#31283;&#20581;&#32780;&#28789;&#27963;&#34892;&#20026;&#30340;&#22522;&#30784;&#12290;&#36825;&#20123;&#34920;&#31034;&#26159;&#22312;&#21457;&#32946;&#36807;&#31243;&#20013;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#33719;&#24471;&#30340;&#65292;&#24182;&#22312;&#29983;&#29289;&#30340;&#25972;&#20010;&#29983;&#21629;&#21608;&#26399;&#20013;&#25345;&#32493;&#32500;&#25345;&#12290;&#39044;&#27979;&#24615;&#23398;&#20064;&#29702;&#35770;&#35748;&#20026;&#65292;&#36825;&#20123;&#34920;&#31034;&#26159;&#36890;&#36807;&#39044;&#27979;&#25110;&#37325;&#24314;&#24863;&#35273;&#36755;&#20837;&#32780;&#20135;&#29983;&#30340;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22823;&#33041;&#20250;&#20135;&#29983;&#34394;&#25311;&#32463;&#39564;&#65292;&#20363;&#22914;&#22312;&#24819;&#35937;&#21644;&#26790;&#22659;&#20013;&#65292;&#36229;&#36234;&#20808;&#21069;&#32463;&#21382;&#30340;&#36755;&#20837;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#34394;&#25311;&#32463;&#39564;&#21487;&#33021;&#19982;&#23454;&#38469;&#24863;&#35273;&#36755;&#20837;&#21516;&#26679;&#37325;&#35201;&#65292;&#33021;&#22815;&#22609;&#36896;&#30382;&#23618;&#34920;&#31034;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36890;&#36807;&#29983;&#25104;&#34394;&#25311;&#32463;&#39564;&#26469;&#32452;&#32455;&#34920;&#31034;&#30340;&#20004;&#20010;&#20114;&#34917;&#23398;&#20064;&#21407;&#21017;&#12290;&#39318;&#20808;&#65292;&#8220;&#23545;&#25239;&#24615;&#26790;&#24819;&#8221;&#25552;&#20986;&#21019;&#36896;&#24615;&#26790;&#24819;&#25903;&#25345;&#20102;&#22312;&#30382;&#23618;&#20013;&#23454;&#29616;&#23545;&#25239;&#24615;&#23398;&#20064;&#65292;&#20854;&#20013;&#21453;&#39304;&#21644;&#21069;&#39304;&#36335;&#24452;&#21442;&#19982;&#21040;&#30456;&#20114;&#27450;&#39575;&#30340;&#26377;&#30410;&#28216;&#25103;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic representations in higher sensory cortices form the basis for robust, yet flexible behavior. These representations are acquired over the course of development in an unsupervised fashion and continuously maintained over an organism's lifespan. Predictive learning theories propose that these representations emerge from predicting or reconstructing sensory inputs. However, brains are known to generate virtual experiences, such as during imagination and dreaming, that go beyond previously experienced inputs. Here, we suggest that virtual experiences may be just as relevant as actual sensory inputs in shaping cortical representations.In particular, we discuss two complementary learning principles that organize representations through the generation of virtual experiences. First, "adversarial dreaming" proposes that creative dreams support a cortical implementation of adversarial learning in which feedback and feedforward pathways engage in a productive game of trying to fool each o
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;OOD&#27979;&#35797;&#22810;&#22270;&#20013;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#21487;&#20132;&#25442;&#24615;&#27010;&#24565;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#39044;&#27979;&#27169;&#24335;&#19988;&#21487;&#33021;&#20914;&#31361;&#30340;&#20851;&#31995;&#31867;&#22411;&#38598;&#21512;&#30340;&#23646;&#24615;&#22810;&#22270;&#12290;</title><link>http://arxiv.org/abs/2307.06046</link><description>&lt;p&gt;
&#20511;&#21161;&#26032;&#30340;&#20851;&#31995;&#31867;&#22411;&#21644;&#33410;&#28857;&#65292;&#20197;OOD&#22810;&#20219;&#21153;&#35270;&#35282;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
An OOD Multi-Task Perspective for Link Prediction with New Relation Types and Nodes. (arXiv:2307.06046v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;OOD&#27979;&#35797;&#22810;&#22270;&#20013;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#21452;&#21487;&#20132;&#25442;&#24615;&#27010;&#24565;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#39044;&#27979;&#27169;&#24335;&#19988;&#21487;&#33021;&#20914;&#31361;&#30340;&#20851;&#31995;&#31867;&#22411;&#38598;&#21512;&#30340;&#23646;&#24615;&#22810;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#25512;&#26029;&#20855;&#26377;&#23646;&#24615;&#30340;&#22810;&#22270;&#20013;&#26032;&#27979;&#35797;&#22810;&#22270;&#20013;&#33410;&#28857;&#20043;&#38388;&#30340;&#32570;&#22833;&#23646;&#24615;&#38142;&#25509;&#65288;&#20851;&#31995;&#65289;&#12290;&#20256;&#32479;&#30340;&#20851;&#31995;&#23398;&#20064;&#26041;&#27861;&#38754;&#20020;&#30528;&#23545;OOD&#27979;&#35797;&#22810;&#22270;&#30340;&#26377;&#38480;&#27867;&#21270;&#33021;&#21147;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#22810;&#22270;&#21253;&#21547;&#20102;&#35757;&#32451;&#20013;&#26410;&#35265;&#36807;&#30340;&#26032;&#33410;&#28857;&#21644;&#26032;&#20851;&#31995;&#31867;&#22411;&#12290;&#26368;&#36817;&#65292;&#39640;&#31561;&#20154;&#65288;2023&#65289;&#22312;&#25152;&#26377;&#20851;&#31995;&#31867;&#22411;&#20849;&#20139;&#30456;&#21516;&#32467;&#26500;&#39044;&#27979;&#27169;&#24335;&#65288;&#21333;&#20010;&#20219;&#21153;&#65289;&#30340;&#21807;&#19968;&#20551;&#35774;&#19979;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#21452;&#21487;&#20132;&#25442;&#24615;&#29702;&#35770;&#27010;&#24565;&#65288;&#29992;&#20110;&#33410;&#28857;&#21644;&#20851;&#31995;&#31867;&#22411;&#65289;&#26469;&#36827;&#34892;OOD&#38142;&#25509;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#19982;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#35774;&#35745;&#30340;&#65288;&#21333;&#20010;&#65289;&#21487;&#20132;&#25442;&#24615;&#65288;&#20165;&#29992;&#20110;&#33410;&#28857;&#65289;&#30456;&#21453;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#23558;&#21452;&#21487;&#20132;&#25442;&#24615;&#27010;&#24565;&#25193;&#23637;&#21040;&#22810;&#20219;&#21153;&#21452;&#21487;&#20132;&#25442;&#24615;&#65292;&#20854;&#20013;&#25105;&#20204;&#23450;&#20041;&#20102;&#23646;&#24615;&#22810;&#22270;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#65292;&#36825;&#20123;&#22270;&#21487;&#33021;&#23545;&#19981;&#21516;&#30340;&#20851;&#31995;&#31867;&#22411;&#38598;&#21512;&#20855;&#26377;&#19981;&#21516;&#19988;&#21487;&#33021;&#20914;&#31361;&#30340;&#39044;&#27979;&#27169;&#24335;&#65288;&#22810;&#20010;&#20219;&#21153;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of inductive link prediction in (discrete) attributed multigraphs infers missing attributed links (relations) between nodes in new test multigraphs. Traditional relational learning methods face the challenge of limited generalization to OOD test multigraphs containing both novel nodes and novel relation types not seen in training. Recently, under the only assumption that all relation types share the same structural predictive patterns (single task), Gao et al. (2023) proposed an OOD link prediction method using the theoretical concept of double exchangeability (for nodes &amp; relation types), in contrast to the (single) exchangeability (only for nodes) used to design Graph Neural Networks (GNNs). In this work we further extend the double exchangeability concept to multi-task double exchangeability, where we define link prediction in attributed multigraphs that can have distinct and potentially conflicting predictive patterns for different sets of relation types (multiple tasks). 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#32544;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2306.16334</link><description>&lt;p&gt;
&#36890;&#36807;&#23494;&#24230;&#26631;&#24535;&#26816;&#27979;&#26469;&#35782;&#21035;&#31163;&#25955;&#21270;&#28508;&#22312;&#22352;&#26631;&#31995;&#32479;&#30340;&#21487;&#35782;&#21035;&#24615;
&lt;/p&gt;
&lt;p&gt;
Identifiability of Discretized Latent Coordinate Systems via Density Landmarks Detection. (arXiv:2306.16334v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#12290;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#26144;&#23556;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#36825;&#19968;&#21457;&#29616;&#23545;&#35299;&#32544;&#30740;&#31350;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#32544;&#26088;&#22312;&#20165;&#20174;&#35266;&#23519;&#21040;&#30340;&#20998;&#24067;&#20013;&#24674;&#22797;&#26377;&#24847;&#20041;&#30340;&#28508;&#22312;&#30495;&#23454;&#22240;&#32032;&#12290; &#21487;&#35782;&#21035;&#24615;&#20026;&#35299;&#32544;&#25552;&#20379;&#20102;&#29702;&#35770;&#22522;&#30784;&#12290; &#19981;&#24184;&#30340;&#26159;&#65292;&#22312;&#33258;&#36866;&#24212;&#29420;&#31435;&#28508;&#21464;&#37327;&#22240;&#23376;&#30340;&#24773;&#20917;&#19979;&#65292;&#22312;&#19968;&#33324;&#30340;&#38750;&#32447;&#24615;&#20809;&#28369;&#22240;&#23376;&#21040;&#35266;&#27979;&#30340;&#26144;&#23556;&#19979;&#65292;&#26080;&#30417;&#30563;&#30340;&#21487;&#35782;&#21035;&#24615;&#22312;i.i.d.&#35774;&#32622;&#19979;&#26159;&#29702;&#35770;&#19978;&#19981;&#21487;&#33021;&#30340;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#38750;&#24120;&#24778;&#20154;&#30340;&#26159;&#65292;&#22312;&#39640;&#24230;&#36890;&#29992;&#30340;&#38750;&#32447;&#24615;&#20809;&#28369;&#26144;&#23556;&#65288;&#19968;&#20010;&#24494;&#20998;&#21516;&#32986;&#65289;&#19979;&#65292;&#21487;&#20197;&#24674;&#22797;&#31163;&#25955;&#21270;&#30340;&#28508;&#22312;&#22352;&#26631;&#65292;&#32780;&#19981;&#38656;&#35201;&#23545;&#26144;&#23556;&#36827;&#34892;&#20219;&#20309;&#39069;&#22806;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290; &#36825;&#26159;&#22312;&#20551;&#35774;&#28508;&#22312;&#23494;&#24230;&#20855;&#26377;&#36724;&#23545;&#40784;&#30340;&#19981;&#36830;&#32493;&#26631;&#24535;&#30340;&#24773;&#20917;&#19979;&#65292;&#20294;&#19981;&#20570;&#22240;&#32032;&#30340;&#32479;&#35745;&#29420;&#31435;&#30340;&#19981;&#29616;&#23454;&#30340;&#20551;&#35774;&#12290; &#25105;&#20204;&#24341;&#20837;&#20102;&#36825;&#31181;&#26032;&#39062;&#30340;&#21487;&#35782;&#21035;&#24615;&#24418;&#24335;&#65292;&#31216;&#20026;&#37327;&#21270;&#22352;&#26631;&#21487;&#35782;&#21035;&#24615;&#65292;&#24182;&#23545;&#24674;&#22797;&#31163;&#25955;&#22352;&#26631;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
Disentanglement aims to recover meaningful latent ground-truth factors from only the observed distribution. Identifiability provides the theoretical grounding for disentanglement to be well-founded. Unfortunately, unsupervised identifiability of independent latent factors is a theoretically proven impossibility in the i.i.d. setting under a general nonlinear smooth map from factors to observations. In this work, we show that, remarkably, it is possible to recover discretized latent coordinates under a highly generic nonlinear smooth mapping (a diffeomorphism) without any additional inductive bias on the mapping. This is, assuming that latent density has axis-aligned discontinuity landmarks, but without making the unrealistic assumption of statistical independence of the factors. We introduce this novel form of identifiability, termed quantized coordinate identifiability, and provide a comprehensive proof of the recovery of discretized coordinates.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22635;&#20805;&#22240;&#26524;&#27169;&#26495;&#26469;&#29983;&#25104;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20043;&#21069;&#35780;&#20272;&#32467;&#26524;&#19981;&#19968;&#33268;&#21644;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23384;&#22312;&#30097;&#34385;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#20182;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31038;&#20132;&#25512;&#29702;&#22522;&#20934;&#65288;BigToM&#65289;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#21442;&#19982;&#32773;&#35780;&#20215;&#36825;&#20010;&#22522;&#20934;&#30340;&#36136;&#37327;&#26356;&#39640;&#12290;</title><link>http://arxiv.org/abs/2306.15448</link><description>&lt;p&gt;
&#36890;&#36807;&#35821;&#35328;&#27169;&#22411;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20132;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Understanding Social Reasoning in Language Models with Language Models. (arXiv:2306.15448v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15448
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22635;&#20805;&#22240;&#26524;&#27169;&#26495;&#26469;&#29983;&#25104;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#35780;&#20272;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#20043;&#21069;&#35780;&#20272;&#32467;&#26524;&#19981;&#19968;&#33268;&#21644;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23384;&#22312;&#30097;&#34385;&#30340;&#25361;&#25112;&#12290;&#20351;&#29992;&#36825;&#20010;&#26694;&#26550;&#65292;&#20182;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31038;&#20132;&#25512;&#29702;&#22522;&#20934;&#65288;BigToM&#65289;&#65292;&#24182;&#21457;&#29616;&#20154;&#31867;&#21442;&#19982;&#32773;&#35780;&#20215;&#36825;&#20010;&#22522;&#20934;&#30340;&#36136;&#37327;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#36234;&#26469;&#36234;&#22810;&#22320;&#34701;&#20837;&#21040;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20102;&#35299;&#23427;&#20204;&#29702;&#35299;&#20154;&#31867;&#24515;&#29702;&#29366;&#24577;&#30340;&#33021;&#21147;&#23545;&#20110;&#30830;&#20445;&#26377;&#25928;&#30340;&#20132;&#20114;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26368;&#36817;&#26377;&#20154;&#23581;&#35797;&#35780;&#20272;LLM&#30340;&#29702;&#35770;&#24515;&#26234;&#65288;ToM&#65289;&#25512;&#29702;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#19982;&#20154;&#31867;ToM&#30340;&#19968;&#33268;&#31243;&#24230;&#20173;&#28982;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25506;&#32034;&#20027;&#39064;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#23384;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25361;&#25112;&#65306;&#65288;1&#65289;&#20043;&#21069;&#35780;&#20272;&#32467;&#26524;&#19981;&#19968;&#33268;&#65292;&#65288;2&#65289;&#29616;&#26377;&#35780;&#20272;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#23384;&#22312;&#30097;&#34385;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#22635;&#20805;&#22240;&#26524;&#27169;&#26495;&#26469;&#29983;&#25104;&#19982;LLM&#30340;&#35780;&#20272;&#12290;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#20026;LLM&#21019;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#31038;&#20132;&#25512;&#29702;&#22522;&#20934;&#65288;BigToM&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;25&#20010;&#25511;&#21046;&#21644;5000&#20010;&#27169;&#22411;&#20889;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#20043;&#21069;&#20247;&#21253;&#35780;&#20272;&#30456;&#27604;&#65292;&#20154;&#31867;&#21442;&#19982;&#32773;&#23545;&#25105;&#20204;&#30340;&#22522;&#20934;&#30340;&#36136;&#37327;&#35780;&#20215;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evalua
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.11300</link><description>&lt;p&gt;
RS5M&#65306;&#29992;&#20110;&#36965;&#24863;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#30340;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
RS5M: A Large Scale Vision-Language Dataset for Remote Sensing Vision-Language Foundation Model. (arXiv:2306.11300v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;RS5M&#65292;&#35813;&#26694;&#26550;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#29992;&#20110;&#23454;&#29616;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#36716;&#25442;&#12290;&#21478;&#22806;&#65292;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#26631;&#31614;&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22823;&#37327;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#22270;&#20687;-&#25991;&#26412;&#20851;&#32852;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#12290;&#20851;&#38190;&#25361;&#25112;&#26159;&#22914;&#20309;&#21033;&#29992;&#24050;&#26377;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#22495;&#30456;&#20851;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#36827;&#34892;&#39046;&#22495;&#29305;&#23450;&#30340;&#36801;&#31227;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#21253;&#25324;&#39046;&#22495;&#22522;&#30784;&#27169;&#22411;&#65288;DFM&#65289;&#65292;&#24357;&#21512;&#20102;&#36890;&#29992;&#22522;&#30784;&#27169;&#22411;&#65288;GFM&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20171;&#32461;&#20102;&#19968;&#20010;&#36965;&#24863;&#39046;&#22495;&#65288;RS&#65289;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;RS5M&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;500&#19975;&#24352;&#24102;&#26377;&#33521;&#25991;&#25551;&#36848;&#30340;RS&#22270;&#20687;&#12290;&#35813;&#25968;&#25454;&#38598;&#26159;&#36890;&#36807;&#36807;&#28388;&#20844;&#24320;&#21487;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#65292;&#24182;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#22522;&#30784;&#27169;&#22411;&#20026;&#20165;&#24102;&#26631;&#31614;&#30340;RS&#25968;&#25454;&#38598;&#29983;&#25104;&#26631;&#39064;&#12290;&#36825;&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;RS&#22270;&#20687;-&#25991;&#26412;&#37197;&#23545;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Vision-Language Foundation Models utilizing extensive image-text paired data have demonstrated unprecedented image-text association capabilities, achieving remarkable results across various downstream tasks. A critical challenge is how to make use of existing large-scale pre-trained VLMs, which are trained on common objects, to perform the domain-specific transfer for accomplishing domain-related downstream tasks. In this paper, we propose a new framework that includes the Domain Foundation Model (DFM), bridging the gap between the General Foundation Model (GFM) and domain-specific downstream tasks. Moreover, we present an image-text paired dataset in the field of remote sensing (RS), RS5M, which has 5 million RS images with English descriptions. The dataset is obtained from filtering publicly available image-text paired datasets and captioning label-only RS datasets with pre-trained VLM. These constitute the first large-scale RS image-text paired dataset. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#36807;&#22810;&#26631;&#35760;&#26679;&#26412;&#12290;&#36890;&#36807;&#22238;&#24518;&#25968;&#25454;&#36827;&#34892;&#36731;&#37327;&#26356;&#26032;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10191</link><description>&lt;p&gt;
&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#29992;&#20110;&#23567;&#26679;&#26412;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Neural Priming for Sample-Efficient Adaptation. (arXiv:2306.10191v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10191
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#20351;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#19979;&#28216;&#20219;&#21153;&#65292;&#26080;&#38656;&#36807;&#22810;&#26631;&#35760;&#26679;&#26412;&#12290;&#36890;&#36807;&#22238;&#24518;&#25968;&#25454;&#36827;&#34892;&#36731;&#37327;&#26356;&#26032;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#31070;&#32463;&#21551;&#21160;&#25216;&#26415;&#65292;&#29992;&#20110;&#22312;&#26410;&#32463;&#36807;&#22823;&#37327;&#26631;&#35760;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36866;&#24212;&#20110;&#20998;&#24067;&#21464;&#21270;&#21644;&#19979;&#28216;&#20219;&#21153;&#12290;&#22312;&#32473;&#23450;&#31867;&#21517;&#25110;&#26080;&#26631;&#31614;&#27979;&#35797;&#26679;&#26412;&#26102;&#65292;&#31070;&#32463;&#21551;&#21160;&#21487;&#20197;&#20351;&#27169;&#22411;&#22238;&#24518;&#36215;&#39044;&#35757;&#32451;&#26399;&#38388;&#30475;&#21040;&#30340;&#30456;&#20851;&#25968;&#25454;&#24182;&#20197;&#27492;&#20026;&#22522;&#30784;&#26465;&#20214;&#21270;&#20854;&#21442;&#25968;&#65292;&#20174;&#32780;&#20351;&#20854;&#38024;&#23545;&#27979;&#35797;&#20998;&#24067;&#20570;&#22909;&#20934;&#22791;&#12290;&#31070;&#32463;&#21551;&#21160;&#36824;&#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#36827;&#34892;&#65292;&#21363;&#20351;&#26159;&#38024;&#23545;&#22914;LAION-2B&#36825;&#26679;&#22823;&#22411;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#12290;&#22312;&#21508;&#31181;&#20998;&#24067;&#21464;&#21270;&#21644;&#36801;&#31227;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#23545;&#22238;&#24518;&#25968;&#25454;&#36827;&#34892;&#36731;&#37327;&#26356;&#26032;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#38646;&#26679;&#26412;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#30475;&#21040;ImageNet&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;2.45&#65285;&#65292;&#22312;&#26631;&#20934;&#30340;&#36801;&#31227;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#24179;&#22343;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;3.81&#65285;&#12290;&#27492;&#22806;&#65292;&#22312;&#25512;&#29702;&#26102;&#20351;&#29992;&#31070;&#32463;&#21551;&#21160;&#26469;&#36866;&#24212;&#20998;&#24067;&#21464;&#21270;&#65292;&#25105;&#20204;&#30475;&#21040;ImageNetV2&#30340;&#20934;&#30830;&#24615;&#25552;&#39640;&#20102;1.41&#65285;&#12290;&#36825;&#20123;&#32467;&#26524;&#35777;&#26126;&#20102;&#31070;&#32463;&#21551;&#21160;&#22312;&#22788;&#29702;&#23567;&#26679;&#26412;&#33258;&#36866;&#24212;&#25361;&#25112;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Neural Priming, a technique for adapting large pretrained models to distribution shifts and downstream tasks given few or no labeled examples. Presented with class names or unlabeled test samples, Neural Priming enables the model to recall and conditions its parameters on relevant data seen throughout pretraining, thereby priming it for the test distribution. Neural Priming can be performed at test time, even for pretraining datasets as large as LAION-2B. Performing lightweight updates on the recalled data significantly improves accuracy across a variety of distribution shift and transfer learning benchmarks. Concretely, in the zero-shot setting, we see a 2.45% improvement in accuracy on ImageNet and 3.81% accuracy improvement on average across standard transfer learning benchmarks. Further, using Neural Priming at inference to adapt to distribution shift, we see a 1.41% accuracy improvement on ImageNetV2. These results demonstrate the effectiveness of Neural Priming in addr
&lt;/p&gt;</description></item><item><title>Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.09750</link><description>&lt;p&gt;
Fedstellar&#65306;&#19968;&#20010;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
Fedstellar: A Platform for Decentralized Federated Learning. (arXiv:2306.09750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09750
&lt;/p&gt;
&lt;p&gt;
Fedstellar&#26159;&#19968;&#20010;&#32852;&#37030;&#23398;&#20064;&#24179;&#21488;&#65292;&#25903;&#25345;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#38382;&#39064;&#26102;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2016&#24180;&#65292;&#35895;&#27468;&#25552;&#20986;&#20102;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#21487;&#20197;&#22312;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#30340;&#21516;&#26102;&#36328;&#32852;&#30431;&#21442;&#19982;&#32773;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#34429;&#28982;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;CFL&#65289;&#26159;&#26368;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#20294;&#23427;&#23384;&#22312;&#36890;&#20449;&#29942;&#39048;&#12289;&#21333;&#28857;&#25925;&#38556;&#21644;&#23545;&#20013;&#22830;&#26381;&#21153;&#22120;&#30340;&#20381;&#36182;&#31561;&#23616;&#38480;&#12290;&#21435;&#20013;&#24515;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;DFL&#65289;&#36890;&#36807;&#23454;&#29616;&#21435;&#20013;&#24515;&#21270;&#27169;&#22411;&#32858;&#21512;&#21644;&#26368;&#23567;&#21270;&#23545;&#20013;&#22830;&#23454;&#20307;&#30340;&#20381;&#36182;&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#35757;&#32451;DFL&#27169;&#22411;&#30340;&#24179;&#21488;&#22312;&#22788;&#29702;&#24322;&#26500;&#32852;&#30431;&#32593;&#32476;&#25299;&#25169;&#31561;&#20851;&#38190;&#38382;&#39064;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Fedstellar&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#30340;&#24179;&#21488;&#65292;&#26088;&#22312;&#22312;&#29289;&#29702;&#25110;&#34394;&#25311;&#35774;&#22791;&#30340;&#19981;&#21516;&#32852;&#30431;&#20013;&#20197;&#21435;&#20013;&#24515;&#21270;&#12289;&#21322;&#21435;&#20013;&#24515;&#21270;&#21644;&#20013;&#24515;&#21270;&#30340;&#26041;&#24335;&#35757;&#32451;FL&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
In 2016, Google proposed Federated Learning (FL) as a novel paradigm to train Machine Learning (ML) models across the participants of a federation while preserving data privacy. Since its birth, Centralized FL (CFL) has been the most used approach, where a central entity aggregates participants' models to create a global one. However, CFL presents limitations such as communication bottlenecks, single point of failure, and reliance on a central server. Decentralized Federated Learning (DFL) addresses these issues by enabling decentralized model aggregation and minimizing dependency on a central entity. Despite these advances, current platforms training DFL models struggle with key issues such as managing heterogeneous federation network topologies. To overcome these challenges, this paper presents Fedstellar, a novel platform designed to train FL models in a decentralized, semi-decentralized, and centralized fashion across diverse federations of physical or virtualized devices. The Feds
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#23450;&#37327;&#20998;&#26512;&#20102;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#31350;&#20102;&#19981;&#21516;&#23646;&#24615;&#30340;XAI&#26041;&#27861;&#65292;&#25552;&#20379;&#36873;&#21462;&#21512;&#36866;&#26041;&#27861;&#20197;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#20915;&#31574;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2306.04037</link><description>&lt;p&gt;
&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#30340;&#20027;&#35201;&#36129;&#29486;&#30340;&#23450;&#37327;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Quantitative Analysis of Primary Attribution Explainable Artificial Intelligence Methods for Remote Sensing Image Classification. (arXiv:2306.04037v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04037
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#23450;&#37327;&#20998;&#26512;&#20102;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#65292;&#25506;&#31350;&#20102;&#19981;&#21516;&#23646;&#24615;&#30340;XAI&#26041;&#27861;&#65292;&#25552;&#20379;&#36873;&#21462;&#21512;&#36866;&#26041;&#27861;&#20197;&#28145;&#20837;&#20102;&#35299;&#27169;&#22411;&#20915;&#31574;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32508;&#21512;&#20998;&#26512;&#23450;&#37327;&#35780;&#20272;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#29992;&#20110;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#22810;&#31181;&#27169;&#24577;&#19979;&#25191;&#34892;&#36965;&#24863;&#22270;&#20687;&#20998;&#31867;&#12290;&#25105;&#20204;&#36890;&#36807;XAI&#26041;&#27861;&#23450;&#24615;&#22320;&#30740;&#31350;&#20102;&#27169;&#22411;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#25152;&#38656;&#23646;&#24615;&#30340;&#21508;&#31181;&#31867;&#21035;&#26469;&#23450;&#37327;&#27604;&#36739;XAI&#26041;&#27861;&#12290;&#36890;&#36807;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#36873;&#25321;&#26368;&#21512;&#36866;&#30340;XAI&#26041;&#27861;&#20197;&#21152;&#28145;&#23545;&#27169;&#22411;&#20915;&#31574;&#36807;&#31243;&#29702;&#35299;&#30340;&#35265;&#35299;&#21644;&#24314;&#35758;&#12290;&#27492;&#24037;&#20316;&#30340;&#20195;&#30721;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a comprehensive analysis of quantitatively evaluating explainable artificial intelligence (XAI) techniques for remote sensing image classification. Our approach leverages state-of-the-art machine learning approaches to perform remote sensing image classification across multiple modalities. We investigate the results of the models qualitatively through XAI methods. Additionally, we compare the XAI methods quantitatively through various categories of desired properties. Through our analysis, we offer insights and recommendations for selecting the most appropriate XAI method(s) to gain a deeper understanding of the models' decision-making processes. The code for this work is publicly available.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;</title><link>http://arxiv.org/abs/2306.04026</link><description>&lt;p&gt;
&#20215;&#20540;&#20989;&#25968;&#21363;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#65306;&#20351;&#29992;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#23398;&#20064;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Your Value Function is a Control Barrier Function: Verification of Learned Policies using Control Theory. (arXiv:2306.04026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#39640;&#24230;&#30340;&#36890;&#29992;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#65292;&#20294;&#39564;&#35777;&#31574;&#30053;&#34892;&#20026;&#30340;&#38590;&#24230;&#23545;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#25511;&#21046;&#29702;&#35770;&#20013;&#20351;&#29992;&#30340;&#39564;&#35777;&#26041;&#27861;&#24212;&#29992;&#20110;&#23398;&#20064;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#36890;&#36807;&#20998;&#26512;&#23433;&#20840;&#32500;&#25252;&#30340;&#31616;&#21333;&#20219;&#21153;&#32467;&#26500;&#65292;&#25105;&#20204;&#25512;&#23548;&#20986;&#23558;&#20540;&#20989;&#25968;&#19982;&#25511;&#21046;&#38556;&#30861;&#20989;&#25968;&#30456;&#32852;&#31995;&#30340;&#21407;&#22987;&#23450;&#29702;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#24230;&#37327;&#26041;&#27861;&#65292;&#20197;&#39564;&#35777;&#23433;&#20840;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#24182;&#25552;&#20986;&#20102;&#25913;&#21892;&#23398;&#20064;&#30340;&#23454;&#38469;&#23454;&#26045;&#32454;&#33410;&#12290;&#38500;&#20102;&#25552;&#20986;&#35777;&#20070;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#22806;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#20026;RL&#31574;&#30053;&#35299;&#38145;&#20102;&#20016;&#23500;&#30340;&#25511;&#21046;&#29702;&#35770;&#39564;&#35777;&#26041;&#27861;&#65292;&#24182;&#20195;&#34920;&#20102;&#36890;&#29992;&#12289;&#21487;&#20280;&#32553;&#21644;&#21487;&#39564;&#35777;&#30340;&#25511;&#21046;&#31995;&#32479;&#35774;&#35745;&#26694;&#26550;&#30340;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although RL is highly general and scalable, the difficulty of verifying policy behaviours poses challenges for safety-critical applications. To remedy this, we propose to apply verification methods used in control theory to learned value functions. By analyzing a simple task structure for safety preservation, we derive original theorems linking value functions to control barrier functions. Inspired by this, we propose novel metrics for verification of value functions in safe control tasks, and practical implementation details that improve learning. Besides proposing a novel method for certificate learning, our work unlocks a wealth of verification methods in control theory for RL policies, and represents a first step towards a framework for general, scalable, and verifiable design of control systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21363;Primal-Attention&#65292;&#26469;&#20248;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#25237;&#24433;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2305.19798</link><description>&lt;p&gt;
&#22522;&#20110;&#21407;&#22987;&#34920;&#36798;&#30340;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Primal-Attention: Self-attention through Asymmetric Kernel SVD in Primal Representation. (arXiv:2305.19798v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#30340;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#21363;Primal-Attention&#65292;&#26469;&#20248;&#21270;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#25552;&#39640;&#27880;&#24847;&#21147;&#36755;&#20986;&#30340;&#25237;&#24433;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#19968;&#31995;&#21015;&#24037;&#20316;&#23558;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#35270;&#20026;&#26680;&#26426;&#22120;&#65292;&#20197;&#27492;&#26469;&#29702;&#35299;&#21644;&#25913;&#36827;Transformers&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#23545;&#31216;&#26680;&#32780;&#19981;&#36866;&#29992;&#20110;&#19981;&#23545;&#31216;&#30340;&#33258;&#27880;&#24847;&#21147;&#65292;&#23548;&#33268;&#20102;&#29702;&#35770;&#21644;&#23454;&#38469;&#30340;&#24046;&#36317;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#19981;&#23545;&#31216;&#26680;&#22855;&#24322;&#20540;&#20998;&#35299;&#65288;KSVD&#65289;&#26469;&#34920;&#36798;&#21644;&#20248;&#21270;&#33258;&#27880;&#24847;&#21147;&#30340;&#26032;&#35270;&#35282;&#12290;&#36890;&#36807;&#19981;&#23545;&#31216;KSVD&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#65306;i&#65289;&#33258;&#27880;&#24847;&#21147;&#30340;&#19968;&#31181;&#21407;&#22987;-&#23545;&#20598;&#34920;&#36798;&#65292;&#20854;&#20013;&#20248;&#21270;&#30446;&#26631;&#34987;&#36716;&#21270;&#20026;&#26368;&#22823;&#21270;&#27880;&#24847;&#21147;&#36755;&#20986;&#20013;&#30340;&#25237;&#24433;&#26041;&#24046;&#65307;ii&#65289;&#19968;&#31181;&#26032;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;-Primal-Attention&#65292;&#36890;&#36807;KSVD&#30340;&#21407;&#22987;&#34920;&#36798;&#24335;&#36991;&#20813;&#20102;&#22312;&#23545;&#20598;&#20013;&#26174;&#24335;&#35745;&#31639;&#26680;&#30697;&#38453;&#30340;&#38382;&#39064;&#65307;iii&#65289;&#36890;&#36807;KKT&#26465;&#20214;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;Primal-Attention&#30340;&#29366;&#24577;&#26368;&#23567;&#21270;&#38382;&#39064;&#30340;&#35299;&#19982;&#20043;&#21069;&#30340;&#23545;&#20598;&#31639;&#27861;&#20855;&#26377;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, a new line of works has emerged to understand and improve self-attention in Transformers by treating it as a kernel machine. However, existing works apply the methods for symmetric kernels to the asymmetric self-attention, resulting in a nontrivial gap between the analytical understanding and numerical implementation. In this paper, we provide a new perspective to represent and optimize self-attention through asymmetric Kernel Singular Value Decomposition (KSVD), which is also motivated by the low-rank property of self-attention normally observed in deep layers. Through asymmetric KSVD, $i$) a primal-dual representation of self-attention is formulated, where the optimization objective is cast to maximize the projection variances in the attention outputs; $ii$) a novel attention mechanism, i.e., Primal-Attention, is proposed via the primal representation of KSVD, avoiding explicit computation of the kernel matrix in the dual; $iii$) with KKT conditions, we prove that the stati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;</title><link>http://arxiv.org/abs/2305.11863</link><description>&lt;p&gt;
&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#30340;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Scaling laws for language encoding models in fMRI. (arXiv:2305.11863v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22522;&#20110;fMRI&#30340;&#35821;&#35328;&#32534;&#30721;&#27169;&#22411;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#22312;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21464;&#21387;&#22120;&#30340;&#21333;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#34920;&#31034;&#24050;&#34987;&#35777;&#26126;&#33021;&#22815;&#26377;&#25928;&#22320;&#39044;&#27979;&#22823;&#33041;&#23545;&#33258;&#28982;&#35821;&#35328;&#30340;&#21453;&#24212;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#27604;&#36739;&#35821;&#35328;&#27169;&#22411;&#19982;&#22823;&#33041;&#30340;&#30740;&#31350;&#37117;&#20351;&#29992;&#20102;&#31867;&#20284;GPT-2&#22823;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#30740;&#31350;&#27979;&#35797;&#20102;&#26159;&#21542;&#26356;&#22823;&#30340;&#24320;&#28304;&#27169;&#22411;&#65288;&#22914;OPT&#21644;LLaMA&#31995;&#21015;&#65289;&#26356;&#36866;&#29992;&#20110;&#39044;&#27979;&#20351;&#29992;fMRI&#35760;&#24405;&#30340;&#22823;&#33041;&#21453;&#24212;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#20174;125M&#21040;30B&#21442;&#25968;&#27169;&#22411;&#36827;&#34892;&#35268;&#27169;&#25193;&#23637;&#26102;&#65292;&#22823;&#33041;&#39044;&#27979;&#24615;&#33021;&#19982;&#27169;&#22411;&#22823;&#23567;&#21576;&#23545;&#25968;&#32447;&#24615;&#20851;&#31995;&#65292;&#36328;3&#20010;&#21463;&#35797;&#32773;&#30340;&#20445;&#30041;&#27979;&#35797;&#38598;&#30456;&#20851;&#24615;&#34920;&#29616;&#25552;&#39640;&#20102;&#32422;15&#65285;&#12290;&#24403;&#25193;&#23637;fMRI&#35757;&#32451;&#38598;&#30340;&#22823;&#23567;&#26102;&#65292;&#25105;&#20204;&#20063;&#35266;&#23519;&#21040;&#20102;&#31867;&#20284;&#30340;&#23545;&#25968;&#32447;&#24615;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#23545;&#20351;&#29992;HuBERT&#65292;WavLM&#21644;Whisper&#30340;&#22768;&#23398;&#32534;&#30721;&#27169;&#22411;&#36827;&#34892;&#20102;&#35268;&#27169;&#23450;&#24459;&#30740;&#31350;&#65292;&#21457;&#29616;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#24102;&#26469;&#20102;&#31867;&#20284;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#22122;&#38899;&#22825;&#33457;&#26495;&#20998;&#26512;&#20102;&#36825;&#20123;&#22823;&#35268;&#27169;&#19988;&#39640;&#24615;&#33021;&#30340;&#32534;&#30721;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations from transformer-based unidirectional language models are known to be effective at predicting brain responses to natural language. However, most studies comparing language models to brains have used GPT-2 or similarly sized language models. Here we tested whether larger open-source models such as those from the OPT and LLaMA families are better at predicting brain responses recorded using fMRI. Mirroring scaling results from other contexts, we found that brain prediction performance scales log-linearly with model size from 125M to 30B parameter models, with ~15% increased encoding performance as measured by correlation with a held-out test set across 3 subjects. Similar log-linear behavior was observed when scaling the size of the fMRI training set. We also characterized scaling for acoustic encoding models that use HuBERT, WavLM, and Whisper, and we found comparable improvements with model size. A noise ceiling analysis of these large, high-performance encoding models 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#22833;&#35823;&#21450;&#20854;&#24212;&#29992;&#20110;&#26816;&#27979;Deepfakes&#65292;&#35782;&#21035;&#20102;&#20116;&#31181;&#23450;&#24615;&#32570;&#38519;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#24182;&#21046;&#23450;&#26816;&#27979;Deepfakes&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.06470</link><description>&lt;p&gt;
&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#23450;&#24615;&#22833;&#36133;&#21450;&#20854;&#22312;&#26816;&#27979;Deepfakes&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Qualitative Failures of Image Generation Models and Their Application in Detecting Deepfakes. (arXiv:2304.06470v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06470
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#36136;&#37327;&#22833;&#35823;&#21450;&#20854;&#24212;&#29992;&#20110;&#26816;&#27979;Deepfakes&#65292;&#35782;&#21035;&#20102;&#20116;&#31181;&#23450;&#24615;&#32570;&#38519;&#12290;&#36825;&#20123;&#21457;&#29616;&#26377;&#21161;&#20110;&#25913;&#36827;&#27169;&#22411;&#24182;&#21046;&#23450;&#26816;&#27979;Deepfakes&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#21644;&#35270;&#39057;&#29983;&#25104;&#27169;&#22411;&#21019;&#36896;&#20986;&#36924;&#30495;&#30340;&#24433;&#20687;&#30340;&#33021;&#21147;&#24050;&#32463;&#36798;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#39640;&#24230;&#65292;&#36825;&#20351;&#24471;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#24456;&#38590;&#21306;&#20998;&#30495;&#23454;&#21644;&#20266;&#36896;&#30340;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#29983;&#25104;&#22270;&#20687;&#30340;&#36136;&#37327;&#21644;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#22270;&#20687;&#20043;&#38388;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#22823;&#37327;&#23398;&#26415;&#35770;&#25991;&#21644;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#65292;&#20197;&#30830;&#23450;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#30340;&#23450;&#24615;&#32570;&#38519;&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20116;&#31867;&#12290;&#36890;&#36807;&#20102;&#35299;&#36825;&#20123;&#22833;&#36133;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#25913;&#36827;&#30340;&#39046;&#22495;&#65292;&#24182;&#21046;&#23450;&#26816;&#27979;Deepfakes&#30340;&#31574;&#30053;&#12290;&#20170;&#22825;&#31038;&#20250;&#20013;Deepfakes&#30340;&#26222;&#36941;&#23384;&#22312;&#26159;&#19968;&#20010;&#20005;&#37325;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#21487;&#20197;&#24110;&#21161;&#20943;&#36731;&#23427;&#20204;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of image and video generation models to create photorealistic images has reached unprecedented heights, making it difficult to distinguish between real and fake images in many cases. However, despite this progress, a gap remains between the quality of generated images and those found in the real world. To address this, we have reviewed a vast body of literature from both academic publications and social media to identify qualitative shortcomings in image generation models, which we have classified into five categories. By understanding these failures, we can identify areas where these models need improvement, as well as develop strategies for detecting deep fakes. The prevalence of deep fakes in today's society is a serious concern, and our findings can help mitigate their negative impact.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#24182;&#22686;&#24378;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.16894</link><description>&lt;p&gt;
ViewRefer: &#22522;&#20110;GPT&#21644;&#26679;&#20363;&#24341;&#23548;&#30340;&#22810;&#35270;&#35282;&#30693;&#35782;&#22788;&#29702;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;
&lt;/p&gt;
&lt;p&gt;
ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance. (arXiv:2303.16894v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.16894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21644;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#24182;&#22686;&#24378;&#26694;&#26550;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22810;&#35270;&#35282;&#36755;&#20837;&#30340;3D&#22330;&#26223;&#65292;&#21487;&#20197;&#32531;&#35299;3D&#35270;&#35273;&#23450;&#20301;&#20013;&#30340;&#35270;&#35282;&#24046;&#24322;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#20102;&#23884;&#20837;&#22312;&#25991;&#26412;&#27169;&#24577;&#20013;&#30340;&#35270;&#35282;&#32447;&#32034;&#65292;&#24182;&#19988;&#26410;&#33021;&#26435;&#34913;&#19981;&#21516;&#35270;&#22270;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ViewRefer&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#35270;&#35282;&#30340;&#19977;&#32500;&#35270;&#35273;&#23450;&#20301;&#26694;&#26550;&#65292;&#25506;&#32034;&#22914;&#20309;&#20174;&#25991;&#26412;&#21644;3D&#27169;&#24577;&#20013;&#33719;&#21462;&#35270;&#35282;&#30693;&#35782;&#12290;&#20854;&#20013;&#65292;ViewRefer&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;&#20363;&#22914;GPT&#65289;&#30340;&#22810;&#26679;&#21270;&#35821;&#35328;&#30693;&#35782;&#65292;&#23558;&#21333;&#19968;&#30340;&#23450;&#20301;&#25991;&#26412;&#25193;&#23637;&#20026;&#22810;&#20010;&#20960;&#20309;&#19968;&#33268;&#30340;&#25551;&#36848;&#65307;&#21516;&#26102;&#65292;&#22312;3D&#27169;&#24577;&#20013;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;Transformer&#30340;&#34701;&#21512;&#27169;&#22359;&#21644;&#35270;&#22270;&#38388;&#27880;&#24847;&#21147;&#65292;&#20197;&#22686;&#24378;&#35270;&#22270;&#20043;&#38388;&#29289;&#20307;&#30340;&#20132;&#20114;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#32452;&#21487;&#23398;&#20064;&#30340;&#22810;&#35270;&#35282;&#21407;&#22411;&#65292;&#29992;&#20110;&#35760;&#24518;&#19981;&#21516;&#35270;&#35282;&#19979;&#30340;&#22330;&#26223;&#26080;&#20851;&#30693;&#35782;&#65292;&#20174;&#20004;&#20010;&#26041;&#38754;&#22686;&#24378;&#20102;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding 3D scenes from multi-view inputs has been proven to alleviate the view discrepancy issue in 3D visual grounding. However, existing methods normally neglect the view cues embedded in the text modality and fail to weigh the relative importance of different views. In this paper, we propose ViewRefer, a multi-view framework for 3D visual grounding exploring how to grasp the view knowledge from both text and 3D modalities. For the text branch, ViewRefer leverages the diverse linguistic knowledge of large-scale language models, e.g., GPT, to expand a single grounding text to multiple geometry-consistent descriptions. Meanwhile, in the 3D modality, a transformer fusion module with inter-view attention is introduced to boost the interaction of objects across views. On top of that, we further present a set of learnable multi-view prototypes, which memorize scene-agnostic knowledge for different views, and enhance the framework from two perspectives: a view-guided attention module 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26367;&#20195;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#33258;&#22238;&#24402;&#20302;&#24310;&#36831;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#22330;&#26223;&#19979;&#22343;&#33021;&#24102;&#26469;&#31283;&#23450;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2211.01751</link><description>&lt;p&gt;
&#36845;&#20195;&#33258;&#22238;&#24402;&#65306;&#25552;&#39640;&#20302;&#24310;&#36831;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#30340;&#26032;&#25216;&#24039;
&lt;/p&gt;
&lt;p&gt;
Iterative autoregression: a novel trick to improve your low-latency speech enhancement model. (arXiv:2211.01751v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01751
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26367;&#20195;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#33258;&#22238;&#24402;&#20302;&#24310;&#36831;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#22330;&#26223;&#19979;&#22343;&#33021;&#24102;&#26469;&#31283;&#23450;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#24335;&#27169;&#22411;&#26159;&#23454;&#26102;&#35821;&#38899;&#22686;&#24378;&#24037;&#20855;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#27969;&#24335;&#27169;&#24335;&#38480;&#21046;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#20165;&#33021;&#20351;&#29992;&#26497;&#23569;&#37327;&#26410;&#26469;&#20449;&#24687;&#20316;&#20026;&#19978;&#19979;&#25991;&#12290;&#22240;&#27492;&#65292;&#20302;&#24310;&#36831;&#27969;&#24335;&#35774;&#32622;&#36890;&#24120;&#34987;&#35270;&#20026;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#36825;&#23545;&#27169;&#22411;&#30340;&#36136;&#37327;&#26377;&#30528;&#26174;&#33879;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#27969;&#24335;&#29983;&#25104;&#30340;&#39034;&#24207;&#24615;&#25552;&#20379;&#20102;&#33258;&#22238;&#24402;&#30340;&#33258;&#28982;&#21487;&#33021;&#24615;&#65292;&#21363;&#22312;&#36827;&#34892;&#24403;&#21069;&#39044;&#27979;&#26102;&#21033;&#29992;&#20197;&#21069;&#30340;&#39044;&#27979;&#12290;&#24120;&#35268;&#35757;&#32451;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#26041;&#27861;&#26159;&#25945;&#24072;&#24378;&#21046;&#65292;&#20294;&#20854;&#20027;&#35201;&#32570;&#28857;&#22312;&#20110;&#35757;&#32451;-&#25512;&#29702;&#19981;&#21305;&#37197;&#21487;&#33021;&#20250;&#23548;&#33268;&#22823;&#24133;&#24230;&#30340;&#36136;&#37327;&#38477;&#32423;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26367;&#20195;&#25216;&#26415;&#65292;&#29992;&#20110;&#35757;&#32451;&#33258;&#22238;&#24402;&#20302;&#24310;&#36831;&#35821;&#38899;&#22686;&#24378;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#30340;&#26550;&#26500;&#21644;&#35757;&#32451;&#22330;&#26223;&#19979;&#37117;&#33021;&#24102;&#26469;&#31283;&#23450;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Streaming models are an essential component of real-time speech enhancement tools. The streaming regime constrains speech enhancement models to use only a tiny context of future information. As a result, the low-latency streaming setup is generally considered a challenging task and has a significant negative impact on the model's quality. However, the sequential nature of streaming generation offers a natural possibility for autoregression, that is, utilizing previous predictions while making current ones. The conventional method for training autoregressive models is teacher forcing, but its primary drawback lies in the training-inference mismatch that can lead to a substantial degradation in quality. In this study, we propose a straightforward yet effective alternative technique for training autoregressive low-latency speech enhancement models. We demonstrate that the proposed approach leads to stable improvement across diverse architectures and training scenarios.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;USR&#65292;&#36890;&#36807;&#26500;&#24314;&#36716;&#25442;&#20989;&#25968;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26469;&#25552;&#39640;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#20540;&#20989;&#25968;&#36827;&#34892;&#23545;&#25239;&#29983;&#25104;&#26410;&#30693;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;USR&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.02016</link><description>&lt;p&gt;
&#22312;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#27491;&#21017;&#21270;&#30340;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Reinforcement Learning in Continuous Control Tasks with Uncertainty Set Regularization. (arXiv:2207.02016v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.02016
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;USR&#65292;&#36890;&#36807;&#26500;&#24314;&#36716;&#25442;&#20989;&#25968;&#21442;&#25968;&#31354;&#38388;&#19978;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26469;&#25552;&#39640;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#24615;&#33021;&#12290;&#36890;&#36807;&#23545;&#20540;&#20989;&#25968;&#36827;&#34892;&#23545;&#25239;&#29983;&#25104;&#26410;&#30693;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#36827;&#19968;&#27493;&#22686;&#24378;&#20102;USR&#30340;&#28789;&#27963;&#24615;&#12290;&#22312;&#30495;&#23454;&#19990;&#30028;&#24378;&#21270;&#23398;&#20064;&#22522;&#20934;&#27979;&#35797;&#20013;&#24471;&#21040;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#34987;&#35748;&#20026;&#22312;&#29615;&#22659;&#25200;&#21160;&#19979;&#32570;&#20047;&#27867;&#21270;&#24615;&#21644;&#40065;&#26834;&#24615;&#65292;&#36825;&#20005;&#37325;&#38480;&#21046;&#20102;&#20854;&#22312;&#23454;&#38469;&#26426;&#22120;&#20154;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#22768;&#31216;&#65292;&#22312;&#20540;&#20989;&#25968;&#20013;&#28155;&#21152;&#27491;&#21017;&#21270;&#31561;&#20215;&#20110;&#23398;&#20064;&#20855;&#26377;&#19981;&#30830;&#23450;&#36716;&#25442;&#30340;&#40065;&#26834;&#31574;&#30053;&#12290;&#23613;&#31649;&#27491;&#21017;&#21270;-&#40065;&#26834;&#24615;&#36716;&#25442;&#22240;&#20854;&#31616;&#21333;&#21644;&#39640;&#25928;&#32780;&#20855;&#26377;&#21560;&#24341;&#21147;&#65292;&#20294;&#22312;&#36830;&#32493;&#25511;&#21046;&#20219;&#21153;&#20013;&#20173;&#28982;&#23384;&#22312;&#19981;&#36275;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#21017;&#21270;&#22120;&#65292;&#21517;&#20026;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#27491;&#21017;&#21270;&#22120;&#65288;USR&#65289;&#65292;&#36890;&#36807;&#22312;&#36716;&#25442;&#20989;&#25968;&#30340;&#21442;&#25968;&#31354;&#38388;&#19978;&#26500;&#24314;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#26469;&#23454;&#29616;&#12290;&#29305;&#21035;&#26159;&#65292;USR&#36275;&#22815;&#28789;&#27963;&#65292;&#21487;&#20197;&#25554;&#20837;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;RL&#26694;&#26550;&#20013;&#12290;&#20026;&#20102;&#22788;&#29702;&#26410;&#30693;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#21512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20540;&#20989;&#25968;&#29983;&#25104;&#30340;&#26032;&#39062;&#23545;&#25239;&#26041;&#27861;&#26469;&#29983;&#25104;&#23427;&#20204;&#12290;&#25105;&#20204;&#22312;&#30495;&#23454;&#19990;&#30028;&#24378;&#21270;&#23398;&#20064;&#65288;RWRL&#65289;&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;USR&#65292;&#23637;&#31034;&#20102;&#25913;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is recognized as lacking generalization and robustness under environmental perturbations, which excessively restricts its application for real-world robotics. Prior work claimed that adding regularization to the value function is equivalent to learning a robust policy with uncertain transitions. Although the regularization-robustness transformation is appealing for its simplicity and efficiency, it is still lacking in continuous control tasks. In this paper, we propose a new regularizer named $\textbf{U}$ncertainty $\textbf{S}$et $\textbf{R}$egularizer (USR), by formulating the uncertainty set on the parameter space of the transition function. In particular, USR is flexible enough to be plugged into any existing RL framework. To deal with unknown uncertainty sets, we further propose a novel adversarial approach to generate them based on the value function. We evaluate USR on the Real-world Reinforcement Learning (RWRL) benchmark, demonstrating improvements i
&lt;/p&gt;</description></item></channel></rss>