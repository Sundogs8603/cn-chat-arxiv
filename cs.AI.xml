<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#25928;&#30340;&#20998;&#21106;&#33021;&#21147;&#21644;&#20219;&#21153;&#36801;&#31227;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#26032;&#30340;&#22270;&#20687;&#20998;&#24067;&#21644;&#20219;&#21153;&#20013;&#36827;&#34892;&#38646;-shot &#20219;&#21153;&#36801;&#31227;&#65292;&#20351;&#20854;&#38646;-shot&#34920;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#24182;&#21457;&#24067;&#20102;Segment Anything Model&#65288;SAM&#65289;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65288;SA-1B&#65289;&#65292;&#20197;&#20419;&#36827;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;</title><link>http://arxiv.org/abs/2304.02643</link><description>&lt;p&gt;
&#23436;&#25104;&#20998;&#21106;&#20219;&#21153;&#30340;&#19975;&#33021;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Segment Anything. (arXiv:2304.02643v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02643
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#39640;&#25928;&#30340;&#20998;&#21106;&#33021;&#21147;&#21644;&#20219;&#21153;&#36801;&#31227;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#26032;&#30340;&#22270;&#20687;&#20998;&#24067;&#21644;&#20219;&#21153;&#20013;&#36827;&#34892;&#38646;-shot &#20219;&#21153;&#36801;&#31227;&#65292;&#20351;&#20854;&#38646;-shot&#34920;&#29616;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#65292;&#24182;&#21457;&#24067;&#20102;Segment Anything Model&#65288;SAM&#65289;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65288;SA-1B&#65289;&#65292;&#20197;&#20419;&#36827;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;Segment Anything&#65288;SA&#65289;&#8221;&#39033;&#30446;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#12289;&#27169;&#22411;&#21644;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#25928;&#30340;&#27169;&#22411;&#22312;&#25968;&#25454;&#25910;&#38598;&#24490;&#29615;&#20013;&#26500;&#24314;&#20102;&#21040;&#30446;&#21069;&#20026;&#27490;&#26368;&#22823;&#30340;&#20998;&#21106;&#25968;&#25454;&#38598;&#65292;&#28085;&#30422;1100&#19975;&#20010;&#35768;&#21487;&#21644;&#38544;&#31169;&#23562;&#37325;&#30340;&#22270;&#20687;&#65292;&#20854;&#20013;&#21253;&#21547;10&#20159;&#20010;&#25513;&#33180;&#12290;&#35813;&#27169;&#22411;&#32463;&#36807;&#35774;&#35745;&#21644;&#35757;&#32451;&#65292;&#21487;&#20197;&#24555;&#36895;&#21709;&#24212;&#65292;&#22240;&#27492;&#21487;&#20197;&#22312;&#26032;&#30340;&#22270;&#20687;&#20998;&#24067;&#21644;&#20219;&#21153;&#20013;&#36827;&#34892;&#38646;-shot &#20219;&#21153;&#36801;&#31227;&#12290;&#25105;&#20204;&#23545;&#20854;&#22312;&#20247;&#22810;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#20854;&#38646;-shot &#24615;&#33021;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051; -- &#36890;&#24120;&#26159;&#31454;&#20105;&#23545;&#25163;&#25110;&#29978;&#33267;&#36229;&#36807;&#20043;&#21069;&#30340;&#23436;&#20840;&#30417;&#30563;&#32467;&#26524;&#12290;&#25105;&#20204;&#27491;&#22312; https://segment-anything.com &#19978;&#21457;&#24067; Segment Anything Model&#65288;SAM&#65289;&#21644;&#30456;&#24212;&#30340;&#25968;&#25454;&#38598;&#65288;SA-1B&#65289;&#65292;&#20197;&#20419;&#36827;&#35745;&#31639;&#26426;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30740;&#31350;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce the Segment Anything (SA) project: a new task, model, and dataset for image segmentation. Using our efficient model in a data collection loop, we built the largest segmentation dataset to date (by far), with over 1 billion masks on 11M licensed and privacy respecting images. The model is designed and trained to be promptable, so it can transfer zero-shot to new image distributions and tasks. We evaluate its capabilities on numerous tasks and find that its zero-shot performance is impressive -- often competitive with or even superior to prior fully supervised results. We are releasing the Segment Anything Model (SAM) and corresponding dataset (SA-1B) of 1B masks and 11M images at https://segment-anything.com to foster research into foundation models for computer vision.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#65306;GenPhys&#65292;&#23427;&#23558;&#20174;&#29289;&#29702;&#36807;&#31243;&#20013;&#25551;&#36848;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#32763;&#35793;&#20026;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#26356;&#24191;&#27867;&#30340;&#29983;&#25104;&#27169;&#22411;&#35774;&#35745;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.02637</link><description>&lt;p&gt;
GenPhys&#65306;&#20174;&#29289;&#29702;&#36807;&#31243;&#21040;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
GenPhys: From Physical Processes to Generative Models. (arXiv:2304.02637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#65306;GenPhys&#65292;&#23427;&#23558;&#20174;&#29289;&#29702;&#36807;&#31243;&#20013;&#25551;&#36848;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;&#32763;&#35793;&#20026;&#29983;&#25104;&#27169;&#22411;&#65292;&#24182;&#25506;&#32034;&#20102;&#26356;&#24191;&#27867;&#30340;&#29983;&#25104;&#27169;&#22411;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25193;&#25955;&#27169;&#22411;(DM)&#21644;&#26368;&#36817;&#30340;&#27850;&#26494;&#27969;&#29983;&#25104;&#27169;&#22411;(PFGM)&#37117;&#21463;&#21040;&#29289;&#29702;&#36807;&#31243;&#30340;&#21551;&#21457;&#65292;&#22240;&#27492;&#21512;&#29702;&#22320;&#38382;&#19968;&#19979;&#65306;&#29289;&#29702;&#36807;&#31243;&#33021;&#21542;&#25552;&#20379;&#26356;&#22810;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#65311;&#25105;&#20204;&#23637;&#31034;&#20102;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23478;&#26063;&#65292;&#21363;&#20174;&#29289;&#29702;&#36807;&#31243;&#21040;&#29983;&#25104;&#27169;&#22411;(GenPhys)&#65292;&#20854;&#20013;&#25105;&#20204;&#23558;&#25551;&#36848;&#29289;&#29702;&#36807;&#31243;&#30340;&#20559;&#24494;&#20998;&#26041;&#31243;(PDEs)&#32763;&#35793;&#20026;&#29983;&#25104;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20174;s-&#29983;&#25104;&#30340;PDEs (s&#20195;&#34920;&#24179;&#28369;)&#26500;&#24314;&#29983;&#25104;&#27169;&#22411;&#12290;GenPhys&#21253;&#21547;&#20102;&#20004;&#20010;&#29616;&#26377;&#29983;&#25104;&#27169;&#22411;(DM&#21644;PFGM)&#65292;&#24182;&#29978;&#33267;&#24341;&#20986;&#20102;&#26032;&#30340;&#29983;&#25104;&#27169;&#22411;&#23478;&#26063;&#65292;&#20363;&#22914;&#65292;&#21463;&#24369;&#30456;&#20114;&#20316;&#29992;&#21551;&#21457;&#30340;&#8220;Yukawa&#29983;&#25104;&#27169;&#22411;&#8221;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#26576;&#20123;&#29289;&#29702;&#36807;&#31243;&#40664;&#35748;&#19981;&#23646;&#20110;GenPhys&#23478;&#26063;&#65292;&#20363;&#22914;&#27874;&#21160;&#26041;&#31243;&#21644;&#34203;&#23450;&#35860;&#26041;&#31243;&#65292;&#20294;&#21487;&#20197;&#36890;&#36807;&#19968;&#20123;&#20462;&#25913;&#21152;&#20837;&#21040;GenPhys&#23478;&#26063;&#20013;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;GenPhys&#26469;&#25506;&#32034;&#21644;&#25193;&#23637;&#29983;&#25104;&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since diffusion models (DM) and the more recent Poisson flow generative models (PFGM) are inspired by physical processes, it is reasonable to ask: Can physical processes offer additional new generative models? We show that the answer is yes. We introduce a general family, Generative Models from Physical Processes (GenPhys), where we translate partial differential equations (PDEs) describing physical processes to generative models. We show that generative models can be constructed from s-generative PDEs (s for smooth). GenPhys subsume the two existing generative models (DM and PFGM) and even give rise to new families of generative models, e.g., "Yukawa Generative Models" inspired from weak interactions. On the other hand, some physical processes by default do not belong to the GenPhys family, e.g., the wave equation and the Schr\"{o}dinger equation, but could be made into the GenPhys family with some modifications. Our goal with GenPhys is to explore and expand the design space of gener
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#31561;&#21464;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#31561;&#21464;&#24615;&#27979;&#37327;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#12289;&#20943;&#23569;&#27169;&#22411;&#23481;&#37327;&#21644;&#21367;&#31215;&#25805;&#20316;&#21487;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#31561;&#21464;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02628</link><description>&lt;p&gt;
&#24433;&#21709;&#28145;&#24230;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#20013;&#23398;&#20064;&#31561;&#21464;&#24615;&#30340;&#22240;&#32032;&#26159;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
What Affects Learned Equivariance in Deep Image Recognition Models?. (arXiv:2304.02628v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02628
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#31561;&#21464;&#24615;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#31561;&#21464;&#24615;&#27979;&#37327;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#12289;&#20943;&#23569;&#27169;&#22411;&#23481;&#37327;&#21644;&#21367;&#31215;&#25805;&#20316;&#21487;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#19982;&#20960;&#20309;&#21464;&#25442;&#30456;&#20851;&#30340;&#31561;&#21464;&#24615;&#21487;&#20197;&#25552;&#39640;&#25968;&#25454;&#25928;&#29575;&#12289;&#21442;&#25968;&#25928;&#29575;&#21644;&#23545;&#22495;&#22806;&#36879;&#35270;&#21464;&#25442;&#30340;&#40065;&#26834;&#24615;&#12290;&#24403;&#31561;&#21464;&#24615;&#27809;&#26377;&#34987;&#35774;&#35745;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#26102;&#65292;&#32593;&#32476;&#20173;&#28982;&#21487;&#20197;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#31561;&#21464;&#20989;&#25968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#31561;&#21464;&#24615;&#27979;&#37327;&#26041;&#27861;&#26469;&#37327;&#21270;&#36825;&#31181;&#23398;&#20064;&#21040;&#30340;&#31561;&#21464;&#24615;&#65292;&#24182;&#21457;&#29616;&#23398;&#20064;&#21040;&#30340;&#32763;&#35793;&#31561;&#21464;&#24615;&#19982;&#22312;ImageNet&#19978;&#30340;&#39564;&#35777;&#20934;&#30830;&#24615;&#20043;&#38388;&#23384;&#22312;&#30456;&#20851;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#20013;&#23398;&#20064;&#21040;&#30340;&#31561;&#21464;&#24615;&#65292;&#24182;&#21457;&#29616;&#25968;&#25454;&#22686;&#24378;&#12289;&#20943;&#23569;&#27169;&#22411;&#23481;&#37327;&#20197;&#21450;&#21367;&#31215;&#24418;&#24335;&#30340;&#24402;&#32435;&#20559;&#24046;&#21487;&#20197;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#26356;&#39640;&#30340;&#23398;&#20064;&#21040;&#30340;&#31561;&#21464;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equivariance w.r.t. geometric transformations in neural networks improves data efficiency, parameter efficiency and robustness to out-of-domain perspective shifts. When equivariance is not designed into a neural network, the network can still learn equivariant functions from the data. We quantify this learned equivariance, by proposing an improved measure for equivariance. We find evidence for a correlation between learned translation equivariance and validation accuracy on ImageNet. We therefore investigate what can increase the learned equivariance in neural networks, and find that data augmentation, reduced model capacity and inductive bias in the form of convolutions induce higher learned equivariance in neural networks.
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#20165;&#26377;&#30340;&#19968;&#20010;&#36755;&#20837;&#22270;&#20687;&#29983;&#25104;3D&#24863;&#30693;&#30340;&#26032;&#35270;&#35282;&#25554;&#22270;&#65292;&#36890;&#36807;&#25972;&#21512;3D&#20960;&#20309;&#20808;&#39564;&#39033;&#65292;&#20351;&#29983;&#25104;&#32467;&#26524;&#26356;&#21152;&#36924;&#30495;&#65292;&#33021;&#22815;&#29983;&#25104;&#35270;&#35282;&#19968;&#33268;&#30340;&#26032;&#28210;&#26579;&#65292;&#19988;&#20855;&#26377;&#29983;&#25104;3D&#19968;&#33268;&#30340;&#24207;&#21015;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.02602</link><description>&lt;p&gt;
&#22522;&#20110;3D&#24863;&#30693;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#26032;&#35270;&#35282;&#25554;&#22270;
&lt;/p&gt;
&lt;p&gt;
Generative Novel View Synthesis with 3D-Aware Diffusion Models. (arXiv:2304.02602v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02602
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#20165;&#26377;&#30340;&#19968;&#20010;&#36755;&#20837;&#22270;&#20687;&#29983;&#25104;3D&#24863;&#30693;&#30340;&#26032;&#35270;&#35282;&#25554;&#22270;&#65292;&#36890;&#36807;&#25972;&#21512;3D&#20960;&#20309;&#20808;&#39564;&#39033;&#65292;&#20351;&#29983;&#25104;&#32467;&#26524;&#26356;&#21152;&#36924;&#30495;&#65292;&#33021;&#22815;&#29983;&#25104;&#35270;&#35282;&#19968;&#33268;&#30340;&#26032;&#28210;&#26579;&#65292;&#19988;&#20855;&#26377;&#29983;&#25104;3D&#19968;&#33268;&#30340;&#24207;&#21015;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#20165;&#26377;&#30340;&#19968;&#20010;&#36755;&#20837;&#22270;&#20687;&#29983;&#25104;3D&#24863;&#30693;&#30340;&#26032;&#35270;&#35282;&#25554;&#22270;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20174;&#19982;&#36755;&#20837;&#19968;&#33268;&#30340;&#21487;&#33021;&#28210;&#26579;&#20998;&#24067;&#20013;&#36827;&#34892;&#37319;&#26679;&#65292;&#21363;&#20351;&#23384;&#22312;&#27495;&#20041;&#65292;&#20063;&#33021;&#22815;&#29983;&#25104;&#22810;&#26679;&#21270;&#19988;&#36924;&#30495;&#30340;&#26032;&#35270;&#35282;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#29616;&#26377;&#30340;&#20108;&#32500;&#25193;&#25955;&#20027;&#24178;&#65292;&#20294;&#20851;&#38190;&#22320;&#65292;&#23558;3D&#29305;&#24449;&#20307;&#31215;&#20316;&#20026;&#20960;&#20309;&#20808;&#39564;&#39033;&#21152;&#20837;&#20854;&#20013;&#12290;&#36825;&#20010;&#28508;&#22312;&#30340;&#29305;&#24449;&#22330;&#25429;&#25417;&#20102;&#21487;&#33021;&#30340;&#22330;&#26223;&#34920;&#31034;&#20998;&#24067;&#65292;&#24182;&#25552;&#39640;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#29983;&#25104;&#35270;&#35282;&#19968;&#33268;&#30340;&#26032;&#28210;&#26579;&#30340;&#33021;&#21147;&#12290;&#38500;&#20102;&#29983;&#25104;&#26032;&#35270;&#35282;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#33021;&#33258;&#22238;&#24402;&#22320;&#21512;&#25104;3D&#19968;&#33268;&#30340;&#24207;&#21015;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#20110;&#21512;&#25104;&#28210;&#26579;&#21644;&#25151;&#38388;&#35268;&#27169;&#30340;&#22330;&#26223;&#30340;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#20063;&#23637;&#31034;&#20102;&#23545;&#20110;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30495;&#23454;&#23545;&#35937;&#30340;&#20196;&#20154;&#20449;&#26381;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a diffusion-based model for 3D-aware generative novel view synthesis from as few as a single input image. Our model samples from the distribution of possible renderings consistent with the input and, even in the presence of ambiguity, is capable of rendering diverse and plausible novel views. To achieve this, our method makes use of existing 2D diffusion backbones but, crucially, incorporates geometry priors in the form of a 3D feature volume. This latent feature field captures the distribution over possible scene representations and improves our method's ability to generate view-consistent novel renderings. In addition to generating novel views, our method has the ability to autoregressively synthesize 3D-consistent sequences. We demonstrate state-of-the-art results on synthetic renderings and room-scale scenes; we also show compelling results for challenging, real-world objects.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;MCMC&#26041;&#27861;&#24212;&#29992;&#65292;&#36890;&#36807;&#25945;&#31243;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02595</link><description>&lt;p&gt;
&#22522;&#20110;MCMC&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65306;&#22522;&#20110;Python&#30340;&#25945;&#31243;
&lt;/p&gt;
&lt;p&gt;
Bayesian neural networks via MCMC: a Python-based tutorial. (arXiv:2304.02595v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02595
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;Python&#30340;&#25945;&#31243;&#65292;&#20171;&#32461;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;MCMC&#26041;&#27861;&#24212;&#29992;&#65292;&#36890;&#36807;&#25945;&#31243;&#20351;&#24471;&#28145;&#24230;&#23398;&#20064;&#24320;&#21457;&#32773;&#33021;&#22815;&#26356;&#22909;&#22320;&#24212;&#29992;&#36125;&#21494;&#26031;&#25512;&#26029;&#36827;&#34892;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#20026;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#25552;&#20379;&#20102;&#21442;&#25968;&#20272;&#35745;&#21644;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#26041;&#27861;&#12290;&#21464;&#20998;&#25512;&#26029;&#21644;&#39532;&#23572;&#31185;&#22827;&#38142;&#33945;&#29305;&#21345;&#32599;&#65288;MCMC&#65289;&#37319;&#26679;&#25216;&#26415;&#29992;&#20110;&#23454;&#29616;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#22312;&#36807;&#21435;&#19977;&#21313;&#24180;&#20013;&#65292;MCMC&#26041;&#27861;&#22312;&#36866;&#24212;&#26356;&#22823;&#30340;&#27169;&#22411;&#65288;&#22914;&#28145;&#24230;&#23398;&#20064;&#65289;&#21644;&#22823;&#25968;&#25454;&#38382;&#39064;&#26041;&#38754;&#38754;&#20020;&#20102;&#35768;&#22810;&#25361;&#25112;&#12290;&#21253;&#25324;&#26799;&#24230;&#30340;&#39640;&#32423;&#25552;&#35758;&#65288;&#20363;&#22914;Langevin&#25552;&#35758;&#20998;&#24067;&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#35299;&#20915;MCMC&#37319;&#26679;&#20013;&#30340;&#19968;&#20123;&#38480;&#21046;&#30340;&#26041;&#27861;&#65292;&#27492;&#22806;&#65292;MCMC&#26041;&#27861;&#36890;&#24120;&#34987;&#38480;&#21046;&#22312;&#32479;&#35745;&#23398;&#23478;&#30340;&#20351;&#29992;&#33539;&#22260;&#20869;&#65292;&#24182;&#19988;&#20173;&#19981;&#26159;&#28145;&#24230;&#23398;&#20064;&#30740;&#31350;&#20154;&#21592;&#30340;&#20027;&#27969;&#26041;&#27861;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;MCMC&#26041;&#27861;&#30340;&#25945;&#31243;&#65292;&#28085;&#30422;&#20102;&#31616;&#21333;&#30340;&#36125;&#21494;&#26031;&#32447;&#24615;&#21644;&#36923;&#36753;&#27169;&#22411;&#65292;&#20197;&#21450;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20010;&#25945;&#31243;&#30340;&#30446;&#30340;&#26159;&#36890;&#36807;&#32534;&#30721;&#26469;&#24357;&#21512;&#29702;&#35770;&#21644;&#23454;&#29616;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#37492;&#20110;&#24403;&#21069;MCMC&#26041;&#27861;&#30340;&#26222;&#21450;&#31243;&#24230;&#20173;&#28982;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian inference provides a methodology for parameter estimation and uncertainty quantification in machine learning and deep learning methods. Variational inference and Markov Chain Monte-Carlo (MCMC) sampling techniques are used to implement Bayesian inference. In the past three decades, MCMC methods have faced a number of challenges in being adapted to larger models (such as in deep learning) and big data problems. Advanced proposals that incorporate gradients, such as a Langevin proposal distribution, provide a means to address some of the limitations of MCMC sampling for Bayesian neural networks. Furthermore, MCMC methods have typically been constrained to use by statisticians and are still not prominent among deep learning researchers. We present a tutorial for MCMC methods that covers simple Bayesian linear and logistic models, and Bayesian neural networks. The aim of this tutorial is to bridge the gap between theory and implementation via coding, given a general sparsity of li
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#33041;&#22914;&#20309;&#36890;&#36807;&#31454;&#20105;&#24615;&#21487;&#22609;&#24615;&#38477;&#20302;&#23398;&#20064;&#30340;&#33021;&#37327;&#25104;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#30465;&#33021;&#30340;&#21487;&#22609;&#24615;&#38480;&#21046;&#31639;&#27861;&#65292;&#32467;&#21512;&#36215;&#26469;&#21487;&#20197;&#22823;&#24133;&#38477;&#20302;&#33021;&#37327;&#28040;&#32791;&#12290;</title><link>http://arxiv.org/abs/2304.02594</link><description>&lt;p&gt;
&#38477;&#20302;&#23398;&#20064;&#30340;&#33021;&#37327;&#25104;&#26412;&#30340;&#31454;&#20105;&#24615;&#21487;&#22609;&#24615;
&lt;/p&gt;
&lt;p&gt;
Competitive plasticity to reduce the energetic costs of learning. (arXiv:2304.02594v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02594
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22823;&#33041;&#22914;&#20309;&#36890;&#36807;&#31454;&#20105;&#24615;&#21487;&#22609;&#24615;&#38477;&#20302;&#23398;&#20064;&#30340;&#33021;&#37327;&#25104;&#26412;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#20010;&#30465;&#33021;&#30340;&#21487;&#22609;&#24615;&#38480;&#21046;&#31639;&#27861;&#65292;&#32467;&#21512;&#36215;&#26469;&#21487;&#20197;&#22823;&#24133;&#38477;&#20302;&#33021;&#37327;&#28040;&#32791;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#33041;&#19981;&#20165;&#22312;&#25903;&#25345;&#35745;&#31639;&#26041;&#38754;&#21463;&#33021;&#37327;&#38480;&#21046;&#65292;&#32780;&#19988;&#22312;&#24418;&#25104;&#35760;&#24518;&#30340;&#33021;&#37327;&#26041;&#38754;&#20063;&#21463;&#38480;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#23398;&#20064;&#31616;&#21333;&#30340;&#26465;&#20214;&#21453;&#23556;&#20219;&#21153;&#24050;&#32463;&#20135;&#29983;&#20102;&#24456;&#22823;&#30340;&#20195;&#35874;&#25104;&#26412;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#20687;MNIST&#36825;&#26679;&#30340;&#20219;&#21153;&#21040;95&#65285;&#30340;&#20934;&#30830;&#24230;&#20284;&#20046;&#33267;&#23569;&#38656;&#35201;10^8&#20010;&#31361;&#35302;&#26356;&#26032;&#12290;&#22240;&#27492;&#65292;&#22823;&#33041;&#21487;&#33021;&#24050;&#32463;&#36827;&#21270;&#20986;&#33021;&#22815;&#20197;&#23613;&#21487;&#33021;&#23569;&#30340;&#33021;&#37327;&#23398;&#20064;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#37327;&#38656;&#27714;&#12290;&#22522;&#20110;&#19968;&#20010;&#31616;&#32422;&#30340;&#33021;&#37327;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#35758;&#20102;&#20004;&#20010;&#33410;&#33021;&#30340;&#21487;&#22609;&#24615;&#38480;&#21046;&#31639;&#27861;&#65306;1&#65289;&#20165;&#20462;&#25913;&#22823;&#26356;&#26032;&#30340;&#31361;&#35302;&#65292;2&#65289;&#23558;&#21487;&#22609;&#24615;&#38480;&#21046;&#22312;&#24418;&#25104;&#32593;&#32476;&#36335;&#24452;&#30340;&#31361;&#35302;&#23376;&#38598;&#19978;&#12290;&#23558;&#36825;&#20004;&#31181;&#26041;&#27861;&#32467;&#21512;&#36215;&#26469;&#21487;&#20197;&#22823;&#22823;&#33410;&#30465;&#33021;&#37327;&#65292;&#21516;&#26102;&#21482;&#22686;&#21152;&#24456;&#23569;&#30340;&#23398;&#20064;&#26102;&#38388;&#12290;&#22312;&#29983;&#29289;&#32593;&#32476;&#20013;&#65292;&#32593;&#32476;&#36890;&#24120;&#27604;&#20219;&#21153;&#35201;&#27714;&#30340;&#22823;&#24471;&#22810;&#12290;&#29305;&#21035;&#26159;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#23454;&#29616;&#22823;&#24133;&#24230;&#30340;&#33410;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The brain is not only constrained by energy needed to fuel computation, but it is also constrained by energy needed to form memories. Experiments have shown that learning simple conditioning tasks already carries a significant metabolic cost. Yet, learning a task like MNIST to 95% accuracy appears to require at least 10^{8} synaptic updates. Therefore the brain has likely evolved to be able to learn using as little energy as possible. We explored the energy required for learning in feedforward neural networks. Based on a parsimonious energy model, we propose two plasticity restricting algorithms that save energy: 1) only modify synapses with large updates, and 2) restrict plasticity to subsets of synapses that form a path through the network. Combining these two methods leads to substantial energy savings while only incurring a small increase in learning time. In biology networks are often much larger than the task requires. In particular in that case, large savings can be achieved. Th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#35268;&#39044;&#27979;&#30340;&#24322;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#19968;&#23450;&#30340;&#30830;&#23450;&#24615;&#27700;&#24179;&#36755;&#20986;&#21253;&#21547;&#30446;&#26631;&#31574;&#30053;&#30340;&#30495;&#23454;&#22870;&#21169;&#30340;&#21306;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#22312;&#20445;&#35777;&#21512;&#35268;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02574</link><description>&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#21512;&#35268;&#24322;&#31574;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Conformal Off-Policy Evaluation in Markov Decision Processes. (arXiv:2304.02574v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02574
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#35268;&#39044;&#27979;&#30340;&#24322;&#31574;&#35780;&#20272;&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#19968;&#23450;&#30340;&#30830;&#23450;&#24615;&#27700;&#24179;&#36755;&#20986;&#21253;&#21547;&#30446;&#26631;&#31574;&#30053;&#30340;&#30495;&#23454;&#22870;&#21169;&#30340;&#21306;&#38388;&#65292;&#24182;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#22788;&#29702;&#20998;&#24067;&#20559;&#31227;&#26041;&#27861;&#65292;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#22312;&#20445;&#35777;&#21512;&#35268;&#24615;&#30340;&#21069;&#25552;&#19979;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#20174;&#25968;&#25454;&#20013;&#35782;&#21035;&#21644;&#35780;&#20272;&#26377;&#25928;&#30340;&#25511;&#21046;&#31574;&#30053;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#23398;&#20064;&#32773;&#19981;&#33021;&#36827;&#34892;&#23454;&#39564;&#65292;&#20063;&#19981;&#33021;&#20197;&#22312;&#32447;&#26041;&#24335;&#33719;&#21462;&#25968;&#25454;&#65288;&#22312;&#23454;&#39564;&#36153;&#29992;&#39640;&#26114;&#12289;&#39118;&#38505;&#39640;&#25110;&#19981;&#36947;&#24503;&#30340;&#24773;&#20917;&#19979;&#65292;&#23601;&#20250;&#20986;&#29616;&#36825;&#31181;&#24773;&#20917;&#65289;&#12290;&#38024;&#23545;&#36825;&#31181;&#24212;&#29992;&#65292;&#24517;&#39035;&#20351;&#29992;&#22312;&#19981;&#21516;&#31574;&#30053;&#19979;&#25910;&#38598;&#30340;&#21382;&#21490;&#25968;&#25454;&#65288;&#34892;&#20026;&#31574;&#30053;&#65289;&#26469;&#20272;&#35745;&#32473;&#23450;&#31574;&#30053;&#65288;&#30446;&#26631;&#31574;&#30053;&#65289;&#30340;&#22870;&#21169;&#12290;&#22823;&#22810;&#25968;&#38024;&#23545;&#36825;&#31181;&#23398;&#20064;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#21363;&#24322;&#31574;&#35780;&#20272;&#65288;OPE&#65289;&#65292;&#37117;&#27809;&#26377;&#20934;&#30830;&#24615;&#21644;&#30830;&#23450;&#24615;&#20445;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#35268;&#39044;&#27979;&#30340;&#26032;&#22411;OPE&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36755;&#20986;&#19968;&#20010;&#21253;&#21547;&#30446;&#26631;&#31574;&#30053;&#30340;&#30495;&#23454;&#22870;&#21169;&#30340;&#21306;&#38388;&#65292;&#21516;&#26102;&#20855;&#26377;&#19968;&#23450;&#30340;&#30830;&#23450;&#24615;&#27700;&#24179;&#12290;OPE&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#26469;&#33258;&#20110;&#30446;&#26631;&#31574;&#30053;&#21644;&#34892;&#20026;&#31574;&#30053;&#20043;&#38388;&#30340;&#24046;&#24322;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#25105;&#20204;&#25552;&#20986;&#24182;&#32463;&#39564;&#24615;&#22320;&#35780;&#20272;&#20102;&#19981;&#21516;&#22788;&#29702;&#36825;&#31181;&#20559;&#31227;&#30340;&#26041;&#27861;&#12290;&#20854;&#20013;&#19968;&#20123;&#26041;&#27861;&#22312;&#20445;&#35777;&#20272;&#35745;&#30340;&#22870;&#21169;&#21306;&#38388;&#30340;&#21512;&#35268;&#24615;&#30340;&#21516;&#26102;&#65292;&#22312;&#22522;&#20934;&#29615;&#22659;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning aims at identifying and evaluating efficient control policies from data. In many real-world applications, the learner is not allowed to experiment and cannot gather data in an online manner (this is the case when experimenting is expensive, risky or unethical). For such applications, the reward of a given policy (the target policy) must be estimated using historical data gathered under a different policy (the behavior policy). Most methods for this learning task, referred to as Off-Policy Evaluation (OPE), do not come with accuracy and certainty guarantees. We present a novel OPE method based on Conformal Prediction that outputs an interval containing the true reward of the target policy with a prescribed level of certainty. The main challenge in OPE stems from the distribution shift due to the discrepancies between the target and the behavior policies. We propose and empirically evaluate different ways to deal with this shift. Some of these methods yield conform
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;ChatGPT&#23478;&#26063;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#34429;&#28982;Fine-tuned&#30340;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#24182;&#26410;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#22686;&#30410;&#65292;&#20294;&#26174;&#31034;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36229;&#36234;&#38382;&#31572;&#30340;&#28508;&#21147;&#65292;&#38656;&#35201;&#26356;&#23450;&#21046;&#21270;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;Fine-tuning&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2304.02496</link><description>&lt;p&gt;
ChatGPT&#23478;&#26063;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#25512;&#29702;&#21644;&#20998;&#31867;&#20013;&#30340;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Evaluation of ChatGPT Family of Models for Biomedical Reasoning and Classification. (arXiv:2304.02496v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02496
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;ChatGPT&#23478;&#26063;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#34429;&#28982;Fine-tuned&#30340;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#24182;&#26410;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#22686;&#30410;&#65292;&#20294;&#26174;&#31034;&#20986;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36229;&#36234;&#38382;&#31572;&#30340;&#28508;&#21147;&#65292;&#38656;&#35201;&#26356;&#23450;&#21046;&#21270;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;Fine-tuning&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#25552;&#21319;&#23637;&#29616;&#20102;&#20854;&#22312;&#29983;&#29289;&#21307;&#23398;&#38382;&#31572;&#26041;&#38754;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#20294;&#23578;&#26410;&#20805;&#20998;&#30740;&#31350;&#20854;&#22312;&#26356;&#20855;&#20307;&#30340;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#30340;&#34920;&#29616;&#12290;&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#23478;&#26063;&#27169;&#22411;&#65288;GPT-3.5s&#65292;GPT-4&#65289;&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22312;OpenAI API&#20844;&#20849;&#25509;&#21475;&#20013;&#19981;&#33021;&#20256;&#36882;&#24739;&#32773;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#36229;&#36807;10000&#20010;&#26679;&#26412;&#20316;&#20026;&#20004;&#20010;&#22522;&#26412;&#20020;&#24202;&#20219;&#21153;&#20998;&#31867;&#21644;&#25512;&#29702;&#30340;&#20195;&#29702;&#36827;&#34892;&#27169;&#22411;&#24615;&#33021;&#35780;&#20272;&#12290;&#31532;&#19968;&#20010;&#20219;&#21153;&#26159;&#23558;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#20020;&#24202;&#21644;&#25919;&#31574;&#24314;&#35758;&#38472;&#36848;&#24402;&#31867;&#20026;&#20581;&#24247;&#24314;&#35758;&#12290;&#31532;&#20108;&#20010;&#20219;&#21153;&#26159;&#20174;&#29983;&#29289;&#21307;&#23398;&#25991;&#29486;&#20013;&#26816;&#27979;&#22240;&#26524;&#20851;&#31995;&#12290;&#25105;&#20204;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#31616;&#21333;&#27169;&#22411;&#65288;&#22914;&#36923;&#36753;&#22238;&#24402;&#30340;&#35789;&#34955;&#27169;&#22411;&#65289;&#21644;Fine-tuned&#30340;BioBERT&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#23613;&#31649;ChatGPT&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#65292;Fine-tuned&#30340;&#29983;&#29289;&#21307;&#23398;NLP&#20219;&#21153;&#24182;&#26410;&#34920;&#29616;&#20986;&#19968;&#33268;&#30340;&#24615;&#33021;&#22686;&#30410;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#24212;&#29992;&#20013;&#36229;&#36234;&#38382;&#31572;&#30340;&#28508;&#21147;&#65292;&#24378;&#35843;&#20102;&#38656;&#35201;&#26356;&#23450;&#21046;&#21270;&#30340;&#27169;&#22411;&#35774;&#35745;&#21644;Fine-tuning&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in large language models (LLMs) have shown impressive ability in biomedical question-answering, but have not been adequately investigated for more specific biomedical applications. This study investigates the performance of LLMs such as the ChatGPT family of models (GPT-3.5s, GPT-4) in biomedical tasks beyond question-answering. Because no patient data can be passed to the OpenAI API public interface, we evaluated model performance with over 10000 samples as proxies for two fundamental tasks in the clinical domain classification and reasoning. The first task is classifying whether statements of clinical and policy recommendations in scientific literature constitute health advice. The second task is causal relation detection from the biomedical literature. We compared LLMs with simpler models, such as bag-of-words (BoW) with logistic regression, and fine-tuned BioBERT models. Despite the excitement around viral ChatGPT, we found that fine-tuning for two fundamental NLP
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26089;&#26399;&#21160;&#35789;&#23398;&#20064;&#19981;&#23545;&#31216;&#24615;&#30340;&#21407;&#22240;&#65292;&#24182;&#37327;&#21270;&#22320;&#35777;&#26126;&#65292;&#19982;&#21517;&#35789;&#30456;&#27604;&#65292;&#21160;&#35789;&#26356;&#38590;&#20064;&#24471;&#65292;&#38656;&#35201;&#25972;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#12290;&#21478;&#22806;&#65292;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24335;&#36755;&#20837;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2304.02492</link><description>&lt;p&gt;
&#37327;&#21270;&#35270;&#35273;&#12289;&#35821;&#35328;&#21644;&#35270;&#35273;-&#35821;&#35328;&#22797;&#26434;&#24230;&#22312;&#21160;&#35789;&#20064;&#24471;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Quantifying the Roles of Visual, Linguistic, and Visual-Linguistic Complexity in Verb Acquisition. (arXiv:2304.02492v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26089;&#26399;&#21160;&#35789;&#23398;&#20064;&#19981;&#23545;&#31216;&#24615;&#30340;&#21407;&#22240;&#65292;&#24182;&#37327;&#21270;&#22320;&#35777;&#26126;&#65292;&#19982;&#21517;&#35789;&#30456;&#27604;&#65292;&#21160;&#35789;&#26356;&#38590;&#20064;&#24471;&#65292;&#38656;&#35201;&#25972;&#21512;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#12290;&#21478;&#22806;&#65292;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24335;&#36755;&#20837;&#21487;&#20197;&#25913;&#21892;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24188;&#20799;&#23398;&#20064;&#21517;&#35789;&#30340;&#24847;&#20041;&#36890;&#24120;&#27604;&#23398;&#20064;&#21160;&#35789;&#30340;&#24847;&#20041;&#26089;&#12290;&#20294;&#26159;&#65292;&#19981;&#28165;&#26970;&#36825;&#31181;&#19981;&#23545;&#31216;&#24615;&#26159;&#30001;&#20110;&#35821;&#35328;&#25152;&#25351;&#30340;&#19990;&#30028;&#20013;&#31867;&#21035;&#30340;&#35270;&#35273;&#32467;&#26500;&#30340;&#22797;&#26434;&#24615;&#65292;&#35821;&#35328;&#26412;&#36523;&#30340;&#32467;&#26500;&#65292;&#36824;&#26159;&#20004;&#31181;&#20449;&#24687;&#26469;&#28304;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#25152;&#33268;&#12290;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#30340;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#20013;&#26469;&#28304;&#20110;&#35270;&#35273;&#21644;&#35821;&#35328;&#30340;&#21333;&#35789;&#34920;&#31034;&#65292;&#23450;&#37327;&#22320;&#27979;&#35797;&#20102;&#20851;&#20110;&#26089;&#26399;&#21160;&#35789;&#23398;&#20064;&#30340;&#36825;&#19977;&#20010;&#20551;&#35828;&#12290;&#36890;&#36807;&#26816;&#26597;&#35270;&#35273;&#21644;&#35821;&#35328;&#23884;&#20837;&#31354;&#38388;&#30340;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#39318;&#20808;&#65292;&#19982;&#21517;&#35789;&#30340;&#34920;&#31034;&#30456;&#27604;&#65292;&#21160;&#35789;&#30340;&#34920;&#31034;&#22312;&#22495;&#20869;&#36890;&#24120;&#26356;&#21152;&#21464;&#21270;&#21644;&#19981;&#21487;&#36776;&#35782;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22914;&#26524;&#27599;&#20010;&#31867;&#21035;&#21482;&#26377;&#19968;&#20010;&#23398;&#20064;&#23454;&#20363;&#65292;&#37027;&#20040;&#21160;&#35789;&#31995;&#32479;&#20013;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#34920;&#31034;&#27604;&#21517;&#35789;&#31995;&#32479;&#20013;&#30340;&#19981;&#22826;&#21563;&#21512;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#35821;&#35328;&#21457;&#23637;&#30340;&#36807;&#31243;&#31867;&#20284;&#65292;&#22914;&#26524;&#22312;&#23398;&#20064;&#26399;&#38388;&#26377;&#22810;&#35821;&#35328;&#21644;&#22810;&#27169;&#24335;&#36755;&#20837;&#65292;&#37027;&#20040;&#21517;&#35789;&#21644;&#21160;&#35789;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;&#20449;&#24687;&#37117;&#21464;&#24471;&#26356;&#21152;&#21563;&#21512;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26089;&#26399;&#21160;&#35789;&#23398;&#20064;&#30340;&#19981;&#23545;&#31216;&#24615;&#33267;&#23569;&#37096;&#20998;&#24402;&#22240;&#20110;&#26356;&#22797;&#26434;&#30340;&#21160;&#35789;&#21547;&#20041;&#65292;&#36825;&#38656;&#35201;&#38598;&#25104;&#35270;&#35273;&#21644;&#35821;&#35328;&#20449;&#24687;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#35270;&#35273;&#22797;&#26434;&#24230;&#25110;&#35821;&#35328;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
Children typically learn the meanings of nouns earlier than the meanings of verbs. However, it is unclear whether this asymmetry is a result of complexity in the visual structure of categories in the world to which language refers, the structure of language itself, or the interplay between the two sources of information. We quantitatively test these three hypotheses regarding early verb learning by employing visual and linguistic representations of words sourced from large-scale pre-trained artificial neural networks. Examining the structure of both visual and linguistic embedding spaces, we find, first, that the representation of verbs is generally more variable and less discriminable within domain than the representation of nouns. Second, we find that if only one learning instance per category is available, visual and linguistic representations are less well aligned in the verb system than in the noun system. However, in parallel with the course of human language development, if mult
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#35266;&#23519;&#21021;&#23398;&#32534;&#31243;&#32773;&#20351;&#29992;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#24037;&#20855; "Github Copilot" &#23436;&#25104;&#20837;&#38376;&#32423;&#32534;&#31243;&#20316;&#19994;&#12290;&#36890;&#36807;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#21021;&#23398;&#32773;&#23545;&#20110;&#36825;&#31181;&#25216;&#26415;&#30340;&#21033;&#24330;&#30340;&#24863;&#30693;&#65292;&#20171;&#32461;&#20102;&#26032;&#30340;&#20132;&#20114;&#27169;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23398;&#29983;&#38754;&#20020;&#30340;&#35748;&#30693;&#21644;&#20803;&#35748;&#30693;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2304.02491</link><description>&lt;p&gt;
&#8220;&#23427;&#30693;&#36947;&#25105;&#24819;&#35201;&#20160;&#20040;&#8221;&#65306;&#21021;&#23398;&#32773;&#19982; Copilot &#30340;&#26131;&#29992;&#24615;&#21644;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
"It's Weird That it Knows What I Want": Usability and Interactions with Copilot for Novice Programmers. (arXiv:2304.02491v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02491
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#39318;&#27425;&#35266;&#23519;&#21021;&#23398;&#32534;&#31243;&#32773;&#20351;&#29992;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#24037;&#20855; "Github Copilot" &#23436;&#25104;&#20837;&#38376;&#32423;&#32534;&#31243;&#20316;&#19994;&#12290;&#36890;&#36807;&#30740;&#31350;&#65292;&#25506;&#35752;&#20102;&#21021;&#23398;&#32773;&#23545;&#20110;&#36825;&#31181;&#25216;&#26415;&#30340;&#21033;&#24330;&#30340;&#24863;&#30693;&#65292;&#20171;&#32461;&#20102;&#26032;&#30340;&#20132;&#20114;&#27169;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23398;&#29983;&#38754;&#20020;&#30340;&#35748;&#30693;&#21644;&#20803;&#35748;&#30693;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#21457;&#23637;&#24050;&#32463;&#20135;&#29983;&#20102;&#33021;&#22815;&#39640;&#31934;&#24230;&#22320;&#23558;&#33258;&#28982;&#35821;&#35328;&#21644;&#22522;&#20110;&#20195;&#30721;&#25552;&#31034;&#29983;&#25104;&#28304;&#20195;&#30721;&#30340;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#12290;&#36825;&#21487;&#33021;&#20250;&#22312;&#35838;&#22530;&#19978;&#20135;&#29983;&#28145;&#21051;&#30340;&#24433;&#21709;&#65292;&#22312;&#37027;&#37324;&#21021;&#23398;&#32773;&#23398;&#20064;&#32534;&#31243;&#21487;&#20197;&#20351;&#29992;&#20813;&#36153;&#24037;&#20855;&#33258;&#21160;&#24314;&#35758;&#32534;&#31243;&#32451;&#20064;&#21644;&#20316;&#19994;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23545;&#21021;&#23398;&#32773;&#22914;&#20309;&#23454;&#38469;&#25805;&#20316;&#36825;&#20123;&#24037;&#20855;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#39033;&#30740;&#31350;&#65292;&#35266;&#23519;&#20102;&#21021;&#23398;&#32773;&#20351;&#29992;&#19968;&#31181;&#20195;&#30721;&#33258;&#21160;&#29983;&#25104;&#24037;&#20855; Github Copilot &#22312;&#20856;&#22411;&#30340;&#20837;&#38376;&#32423;&#32534;&#31243;&#65288;CS1&#65289;&#20316;&#19994;&#19978;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#35266;&#23519;&#21644;&#37319;&#35775;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23398;&#29983;&#23545;&#20110;&#36825;&#31181;&#23398;&#20064;&#25216;&#26415;&#30340;&#21033;&#24330;&#30340;&#24863;&#30693;&#65292;&#20171;&#32461;&#20102;&#26032;&#30340;&#35266;&#23519;&#21040;&#30340;&#20132;&#20114;&#27169;&#24335;&#65292;&#24182;&#35752;&#35770;&#20102;&#23398;&#29983;&#38754;&#20020;&#30340;&#35748;&#30693;&#21644;&#20803;&#35748;&#30693;&#22256;&#38590;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#36825;&#20123;&#21457;&#29616;&#30340;&#35774;&#35745;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#22914;&#20309;&#26356;&#22909;&#22320;&#25903;&#25345; Copilot &#31561;&#24037;&#20855;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent developments in deep learning have resulted in code-generation models that produce source code from natural language and code-based prompts with high accuracy. This is likely to have profound effects in the classroom, where novices learning to code can now use free tools to automatically suggest solutions to programming exercises and assignments. However, little is currently known about how novices interact with these tools in practice. We present the first study that observes students at the introductory level using one such code auto-generating tool, Github Copilot, on a typical introductory programming (CS1) assignment. Through observations and interviews we explore student perceptions of the benefits and pitfalls of this technology for learning, present new observed interaction patterns, and discuss cognitive and metacognitive difficulties faced by students. We consider design implications of these findings, specifically in terms of how tools like Copilot can better support 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#23398;&#29983;&#20889;&#20316;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26377;&#19968;&#23450;&#22909;&#22788;&#65292;&#20294;&#36807;&#24230;&#20381;&#36182;&#27492;&#31867;&#24037;&#20855;&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2304.02478</link><description>&lt;p&gt;
&#25506;&#32034;&#23398;&#29983;&#20889;&#20316;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#65306;AI&#33021;&#36215;&#21040;&#20160;&#20040;&#20316;&#29992;&#65311;
&lt;/p&gt;
&lt;p&gt;
Exploring AI-Generated Text in Student Writing: How Does AI Help?. (arXiv:2304.02478v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02478
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#22312;&#23398;&#29983;&#20889;&#20316;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26377;&#19968;&#23450;&#22909;&#22788;&#65292;&#20294;&#36807;&#24230;&#20381;&#36182;&#27492;&#31867;&#24037;&#20855;&#20063;&#23384;&#22312;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#33521;&#35821;&#20316;&#20026;&#22806;&#35821;&#30340;&#23398;&#29983;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#24037;&#20855;&#29983;&#25104;&#30340;&#25991;&#26412;&#21487;&#33021;&#20250;&#25552;&#39640;&#20182;&#20204;&#30340;&#20889;&#20316;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#36825;&#20123;&#23398;&#29983;&#30340;&#20889;&#20316;&#20013;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#20250;&#23548;&#33268;&#26356;&#39640;&#36136;&#37327;&#30340;&#20889;&#20316;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;23&#21517;&#39321;&#28207;&#20013;&#23398;&#29983;&#25776;&#20889;&#25925;&#20107;&#65288;&#21253;&#21547;&#33258;&#24049;&#30340;&#25991;&#23383;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#25991;&#26412;&#65289;&#30340;&#23581;&#35797;&#12290;&#20154;&#31867;&#19987;&#23478;&#23545;&#36825;&#20123;&#25925;&#20107;&#36827;&#34892;&#20102;&#20869;&#23481;&#12289;&#35821;&#35328;&#21644;&#32452;&#32455;&#26041;&#38754;&#30340;&#35780;&#20998;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#25925;&#20107;&#20013;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#22522;&#26412;&#32452;&#32455;&#32467;&#26500;&#21644;&#21477;&#27861;&#22797;&#26434;&#24230;&#65292;&#24182;&#25191;&#34892;&#20102;&#22810;&#20803;&#32447;&#24615;&#22238;&#24402;&#21644;&#32858;&#31867;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#31867;&#35789;&#35821;&#30340;&#25968;&#37327;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#35789;&#35821;&#30340;&#25968;&#37327;&#23545;&#20998;&#25968;&#26377;&#37325;&#35201;&#36129;&#29486;&#12290;&#27492;&#22806;&#65292;&#19982;&#21516;&#40836;&#20154;&#30456;&#27604;&#65292;&#23398;&#29983;&#30340;&#20889;&#20316;&#21487;&#20197;&#20998;&#20026;&#25797;&#38271;&#21644;&#19981;&#25797;&#38271;&#20351;&#29992;&#26356;&#22810;&#25110;&#26356;&#23569;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#30340;&#20004;&#32452;&#12290;&#32858;&#31867;&#27604;&#36739;&#26174;&#31034;&#65292;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#22312;&#23398;&#29983;&#20889;&#20316;&#20013;&#26377;&#19968;&#23450;&#22909;&#22788;&#65292;&#20294;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#36807;&#24230;&#20381;&#36182;&#36825;&#31181;&#24037;&#20855;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;
English as foreign language_EFL_students' use of text generated from artificial intelligence_AI_natural language generation_NLG_tools may improve their writing quality. However, it remains unclear to what extent AI-generated text in these students' writing might lead to higher-quality writing. We explored 23 Hong Kong secondary school students' attempts to write stories comprising their own words and AI-generated text. Human experts scored the stories for dimensions of content, language and organization. We analyzed the basic organization and structure and syntactic complexity of the stories' AI-generated text and performed multiple linear regression and cluster analyses. The results show the number of human words and the number of AI-generated words contribute significantly to scores. Besides, students can be grouped into competent and less competent writers who use more AI-generated text or less AI-generated text compared to their peers. Comparisons of clusters reveal some benefit of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#39564;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#65288;QKT&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#23398;&#29983;&#22522;&#20110;&#27979;&#39564;&#30340;&#23398;&#20064;&#20132;&#20114;&#26469;&#30417;&#27979;&#20854;&#30693;&#35782;&#29366;&#24577;&#12290;</title><link>http://arxiv.org/abs/2304.02413</link><description>&lt;p&gt;
&#22522;&#20110;&#27979;&#39564;&#30340;&#30693;&#35782;&#36861;&#36394;
&lt;/p&gt;
&lt;p&gt;
Quiz-based Knowledge Tracing. (arXiv:2304.02413v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27979;&#39564;&#30340;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#65288;QKT&#65289;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#26681;&#25454;&#23398;&#29983;&#22522;&#20110;&#27979;&#39564;&#30340;&#23398;&#20064;&#20132;&#20114;&#26469;&#30417;&#27979;&#20854;&#30693;&#35782;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#26088;&#22312;&#26681;&#25454;&#23398;&#20064;&#32773;&#19982;&#22312;&#32447;&#23398;&#20064;&#31995;&#32479;&#65288;OIS&#65289;&#20013;&#19981;&#21516;&#32451;&#20064;&#30340;&#23398;&#20064;&#20132;&#20114;&#26469;&#35780;&#20272;&#20854;&#19981;&#26029;&#21457;&#23637;&#30340;&#30693;&#35782;&#29366;&#24577;&#65292;&#36825;&#23545;&#20110;&#25903;&#25345;&#38543;&#21518;&#30340;&#26234;&#33021;&#26381;&#21153;&#65292;&#22914;&#20010;&#24615;&#21270;&#23398;&#20064;&#36164;&#28304;&#25512;&#33616;&#65292;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#20915;&#31574;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#20154;&#21592;&#24191;&#27867;&#30740;&#31350;&#20102;KT&#24182;&#24320;&#21457;&#20102;&#35768;&#22810;&#26377;&#25928;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#26041;&#27861;&#20551;&#35774;&#23398;&#29983;&#30340;&#21382;&#21490;&#20132;&#20114;&#22312;&#36830;&#32493;&#24207;&#21015;&#20013;&#22343;&#21248;&#20998;&#24067;&#65292;&#24573;&#30053;&#20102;&#23454;&#38469;&#20132;&#20114;&#24207;&#21015;&#26159;&#22522;&#20110;&#19968;&#31995;&#21015;&#20855;&#26377;&#28165;&#26224;&#36793;&#30028;&#30340;&#27979;&#39564;&#32452;&#32455;&#30340;&#20107;&#23454;&#65292;&#22312;&#19968;&#20010;&#27979;&#39564;&#20869;&#30340;&#20132;&#20114;&#36830;&#32493;&#23436;&#25104;&#65292;&#20294;&#26159;&#36328;&#19981;&#21516;&#27979;&#39564;&#30340;&#20132;&#20114;&#26159;&#31163;&#25955;&#30340;&#65292;&#21487;&#33021;&#20250;&#38388;&#38548;&#25968;&#22825;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#27979;&#39564;&#30340;&#30693;&#35782;&#36861;&#36394;&#65288;QKT&#65289;&#27169;&#22411;&#65292;&#26681;&#25454;&#23398;&#29983;&#22522;&#20110;&#27979;&#39564;&#30340;&#23398;&#20064;&#20132;&#20114;&#26469;&#30417;&#27979;&#20854;&#30693;&#35782;&#29366;&#24577;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge tracing (KT) aims to assess individuals' evolving knowledge states according to their learning interactions with different exercises in online learning systems (OIS), which is critical in supporting decision-making for subsequent intelligent services, such as personalized learning source recommendation. Existing researchers have broadly studied KT and developed many effective methods. However, most of them assume that students' historical interactions are uniformly distributed in a continuous sequence, ignoring the fact that actual interaction sequences are organized based on a series of quizzes with clear boundaries, where interactions within a quiz are consecutively completed, but interactions across different quizzes are discrete and may be spaced over days. In this paper, we present the Quiz-based Knowledge Tracing (QKT) model to monitor students' knowledge states according to their quiz-based learning interactions. Specifically, as students' interactions within a quiz ar
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#30740;&#31350;&#27169;&#24577;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#37326;&#21306;&#22320;&#22270;&#21046;&#20316;&#20219;&#21153;&#20013;&#65292;&#21033;&#29992;&#22303;&#22320;&#35206;&#30422;&#21644;&#22812;&#38388;&#20809;&#25968;&#25454;&#33021;&#22815;&#22823;&#22823;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02407</link><description>&lt;p&gt;
&#35299;&#37322;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#65306;&#37326;&#21306;&#22320;&#22270;&#21046;&#20316;&#20013;&#30340;&#36974;&#25377;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Explaining Multimodal Data Fusion: Occlusion Analysis for Wilderness Mapping. (arXiv:2304.02407v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#37319;&#29992;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26469;&#30740;&#31350;&#27169;&#24577;&#30340;&#24433;&#21709;&#65292;&#32467;&#26524;&#26174;&#31034;&#22312;&#37326;&#21306;&#22320;&#22270;&#21046;&#20316;&#20219;&#21153;&#20013;&#65292;&#21033;&#29992;&#22303;&#22320;&#35206;&#30422;&#21644;&#22812;&#38388;&#20809;&#25968;&#25454;&#33021;&#22815;&#22823;&#22823;&#25552;&#39640;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#22312;&#24456;&#20037;&#20197;&#21069;&#20154;&#20204;&#23601;&#21457;&#29616;&#65292;&#22312;&#19968;&#20010;&#20849;&#21516;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#20849;&#21516;&#21033;&#29992;&#22810;&#27169;&#24577;&#36755;&#20837;&#25968;&#25454;&#30340;&#20114;&#34917;&#29305;&#24449;&#26159;&#26377;&#30410;&#30340;&#12290;&#28982;&#32780;&#65292;&#27599;&#31181;&#27169;&#24577;&#23545;&#27169;&#22411;&#20915;&#31574;&#30340;&#24433;&#21709;&#20173;&#28982;&#20196;&#20154;&#22256;&#24785;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22810;&#27169;&#24577;&#22320;&#29699;&#35266;&#27979;&#25968;&#25454;&#30340;&#27169;&#24577;&#32423;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#21487;&#35299;&#37322;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#21363;&#36974;&#25377;&#25935;&#24863;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#26089;&#26399;&#34701;&#21512;&#24773;&#20917;&#19979;&#27169;&#24577;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#37326;&#21306;&#22320;&#22270;&#21046;&#20316;&#36825;&#19968;&#20219;&#21153;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#30410;&#20110;&#35832;&#22914;&#22303;&#22320;&#35206;&#30422;&#21644;&#22812;&#38388;&#20809;&#25968;&#25454;&#31561;&#36741;&#21161;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Jointly harnessing complementary features of multi-modal input data in a common latent space has been found to be beneficial long ago. However, the influence of each modality on the models decision remains a puzzle. This study proposes a deep learning framework for the modality-level interpretation of multimodal earth observation data in an end-to-end fashion. While leveraging an explainable machine learning method, namely Occlusion Sensitivity, the proposed framework investigates the influence of modalities under an early-fusion scenario in which the modalities are fused before the learning process. We show that the task of wilderness mapping largely benefits from auxiliary data such as land cover and night time light data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02396</link><description>&lt;p&gt;
AutoRL&#36229;&#21442;&#25968;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
AutoRL Hyperparameter Landscapes. (arXiv:2304.02396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21462;&#24471;&#20196;&#20154;&#30633;&#30446;&#25104;&#26524;&#30340;&#21516;&#26102;&#65292;&#20854;&#36229;&#21442;&#25968;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#33539;&#22260;&#12290;&#36825;&#32463;&#24120;&#20351;&#24471;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#33258;&#21160;&#21270;RL&#65288;AutoRL&#65289;&#35299;&#20915;&#20102;&#36825;&#20010;&#38590;&#39064;&#65292;&#20294;&#26377;&#20851;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#26041;&#27861;&#22312;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#26102;&#25152;&#36941;&#21382;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#21160;&#24577;&#21464;&#21270;&#30340;&#20449;&#24687;&#24456;&#23569;&#12290;&#37492;&#20110;&#29616;&#26377;AutoRL&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#36229;&#21442;&#25968;&#37197;&#32622;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#20165;&#22312;&#19968;&#20010;&#26102;&#38388;&#28857;&#65292;&#32780;&#19988;&#22312;&#22810;&#20010;&#26102;&#38388;&#28857;&#19978;&#24314;&#31435;&#21644;&#20998;&#26512;&#36825;&#20123;&#36229;&#21442;&#25968;&#26223;&#35266;&#12290;&#38024;&#23545;&#20851;&#20110;&#36825;&#31181;&#21160;&#24577;AutoRL&#26041;&#27861;&#21512;&#27861;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#24320;&#25918;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20805;&#20998;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#22312;&#19981;&#21516;&#31181;&#31867;&#30340;&#29615;&#22659;&#65288;Cartpole&#21644;Pendulum&#65289;&#20013;&#65292;&#26469;&#33258;RL&#25991;&#29486;&#30340;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#24378;&#28872;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Reinforcement Learning (RL) has shown to be capable of producing impressive results, its use is limited by the impact of its hyperparameters on performance. This often makes it difficult to achieve good results in practice. Automated RL (AutoRL) addresses this difficulty, yet little is known about the dynamics of the hyperparameter landscapes that hyperparameter optimization (HPO) methods traverse in search of optimal configurations. In view of existing AutoRL approaches dynamically adjusting hyperparameter configurations, we propose an approach to build and analyze these hyperparameter landscapes not just for one point in time but at multiple points in time throughout training. Addressing an important open question on the legitimacy of such dynamic AutoRL approaches, we provide thorough empirical evidence that the hyperparameter landscapes strongly vary over time across representative algorithms from RL literature (DQN and SAC) in different kinds of environments (Cartpole and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#33021;&#37327;&#26223;&#35266;&#26041;&#27861;&#20026;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20915;&#31574;&#30340;&#39537;&#21160;&#22240;&#32032;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#24230;&#25935;&#24863;&#39046;&#22495;&#24212;&#29992;&#30340;&#38590;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02381</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Physics-Inspired Interpretability Of Machine Learning Models. (arXiv:2304.02381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02381
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;&#33021;&#37327;&#26223;&#35266;&#26041;&#27861;&#20026;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30740;&#31350;&#26041;&#27861;&#65292;&#21487;&#20197;&#35782;&#21035;&#20915;&#31574;&#30340;&#39537;&#21160;&#22240;&#32032;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39640;&#24230;&#25935;&#24863;&#39046;&#22495;&#24212;&#29992;&#30340;&#38590;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20915;&#31574;&#35299;&#37322;&#33021;&#21147;&#19968;&#30452;&#26159;&#38480;&#21046;&#20854;&#22312;&#39640;&#24230;&#25935;&#24863;&#39046;&#22495;&#22914;&#21307;&#23398;&#12289;&#32593;&#32476;&#23433;&#20840;&#25110;&#33258;&#21160;&#39550;&#39542;&#20013;&#24191;&#27867;&#24212;&#29992;&#30340;&#20027;&#35201;&#38556;&#30861;&#20043;&#19968;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21463;&#29289;&#29702;&#23398;&#39046;&#22495;&#30340;&#33021;&#37327;&#26223;&#35266;&#30740;&#31350;&#26041;&#27861;&#21551;&#21457;&#65292;&#20197;&#35782;&#21035;&#36755;&#20837;&#25968;&#25454;&#30340;&#30456;&#20851;&#29305;&#24449;&#20197;&#20419;&#36827;&#27169;&#22411;&#20915;&#31574;&#12290;&#36890;&#36807;&#35782;&#21035;&#25439;&#22833;&#26223;&#35266;&#23616;&#37096;&#26497;&#23567;&#20540;&#32452;&#20013;&#30340;&#23432;&#24658;&#26435;&#37325;&#65292;&#25105;&#20204;&#21487;&#20197;&#30830;&#23450;&#27169;&#22411;&#20915;&#31574;&#30340;&#39537;&#21160;&#22240;&#32032;&#12290;&#22312;&#20998;&#23376;&#31185;&#23398;&#20013;&#23384;&#22312;&#31867;&#20284;&#30340;&#24605;&#24819;&#65292;&#20351;&#29992;&#22352;&#26631;&#19981;&#21464;&#37327;&#25110;&#26377;&#24207;&#21442;&#25968;&#26469;&#30830;&#23450;&#20998;&#23376;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#25439;&#22833;&#26223;&#35266;&#20013;&#27809;&#26377;&#36825;&#26679;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#23558;&#23637;&#31034;&#33021;&#37327;&#26223;&#35266;&#26041;&#27861;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#20030;&#20363;&#35828;&#26126;&#20854;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to explain decisions made by machine learning models remains one of the most significant hurdles towards widespread adoption of AI in highly sensitive areas such as medicine, cybersecurity or autonomous driving. Great interest exists in understanding which features of the input data prompt model decision making. In this contribution, we propose a novel approach to identify relevant features of the input data, inspired by methods from the energy landscapes field, developed in the physical sciences. By identifying conserved weights within groups of minima of the loss landscapes, we can identify the drivers of model decision making. Analogues to this idea exist in the molecular sciences, where coordinate invariants or order parameters are employed to identify critical features of a molecule. However, no such approach exists for machine learning loss landscapes. We will demonstrate the applicability of energy landscape methods to machine learning models and give examples, both 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#21160;&#28388;&#27874;&#21098;&#26525;&#26041;&#27861;&#26469;&#36798;&#21040;&#39640;&#25928;CNN&#30340;&#30446;&#30340;&#65292;&#35299;&#20915;&#20102;&#39640;&#21098;&#26525;&#29575;&#19979;&#36880;&#20803;&#32032;&#33539;&#25968;&#26041;&#27861;&#23384;&#22312;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02319</link><description>&lt;p&gt;
&#36890;&#36807;&#34987;&#21160;&#28388;&#27874;&#21098;&#26525;&#23454;&#29616;&#39640;&#25928;CNN
&lt;/p&gt;
&lt;p&gt;
Efficient CNNs via Passive Filter Pruning. (arXiv:2304.02319v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#21160;&#28388;&#27874;&#21098;&#26525;&#26041;&#27861;&#26469;&#36798;&#21040;&#39640;&#25928;CNN&#30340;&#30446;&#30340;&#65292;&#35299;&#20915;&#20102;&#39640;&#21098;&#26525;&#29575;&#19979;&#36880;&#20803;&#32032;&#33539;&#25968;&#26041;&#27861;&#23384;&#22312;&#30340;&#24615;&#33021;&#19979;&#38477;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24050;&#32463;&#23637;&#31034;&#20986;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#20294;&#26159;&#30001;&#20110;&#23545;&#39640;&#35745;&#31639;&#22797;&#26434;&#24230;&#21644;&#23384;&#20648;&#22120;&#30340;&#35201;&#27714;&#65292;CNN&#26159;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#12290;&#26368;&#36817;&#23454;&#29616;CNN&#35745;&#31639;&#25928;&#29575;&#30340;&#21162;&#21147;&#21253;&#25324;&#28388;&#27874;&#21098;&#26525;&#26041;&#27861;&#65292;&#20854;&#26681;&#25454;&#28388;&#27874;&#22120;&#30340;&#8220;&#37325;&#35201;&#24615;&#8221;&#28040;&#38500;CNN&#20013;&#30340;&#26576;&#20123;&#28388;&#27874;&#22120;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#28388;&#27874;&#21098;&#26525;&#26041;&#27861;&#35201;&#20040;&#26159;&#8220;&#20027;&#21160;&#22411;&#8221;&#65292;&#21363;&#20351;&#29992;&#25968;&#25454;&#38598;&#21644;&#29983;&#25104;&#29305;&#24449;&#26144;&#23556;&#26469;&#37327;&#21270;&#28388;&#27874;&#22120;&#30340;&#37325;&#35201;&#24615;&#65307;&#35201;&#20040;&#26159;&#8220;&#34987;&#21160;&#22411;&#8221;&#65292;&#21363;&#20351;&#29992;&#28388;&#27874;&#22120;&#30340;&#36880;&#20803;&#32032;&#33539;&#25968;&#35745;&#31639;&#28388;&#27874;&#22120;&#30340;&#37325;&#35201;&#24615;&#32780;&#19981;&#28041;&#21450;&#25968;&#25454;&#12290;&#22312;&#39640;&#21098;&#26525;&#29575;&#19979;&#65292;&#21363;&#38656;&#35201;&#20174;&#32593;&#32476;&#20013;&#21098;&#26525;&#22823;&#37327;&#30340;&#28388;&#27874;&#22120;&#26102;&#65292;&#36880;&#20803;&#32032;&#33539;&#25968;&#26041;&#27861;&#28040;&#38500;&#30456;&#23545;&#36739;&#23567;&#30340;&#33539;&#25968;&#28388;&#27874;&#22120;&#32780;&#19981;&#32771;&#34385;&#28388;&#27874;&#22120;&#22312;&#20135;&#29983;&#33410;&#28857;&#36755;&#20986;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#65292;&#23548;&#33268;&#24615;&#33021;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#34987;&#21160;&#28388;&#27874;&#21098;&#26525;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional neural networks (CNNs) have shown state-of-the-art performance in various applications. However, CNNs are resource-hungry due to their requirement of high computational complexity and memory storage. Recent efforts toward achieving computational efficiency in CNNs involve filter pruning methods that eliminate some of the filters in CNNs based on the \enquote{importance} of the filters. The majority of existing filter pruning methods are either "active", which use a dataset and generate feature maps to quantify filter importance, or "passive", which compute filter importance using entry-wise norm of the filters without involving data. Under a high pruning ratio where large number of filters are to be pruned from the network, the entry-wise norm methods eliminate relatively smaller norm filters without considering the significance of the filters in producing the node output, resulting in degradation in the performance. To address this, we present a passive filter pruning me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#23558;&#30072;&#21464;&#25918;&#32622;&#20110;&#20013;&#24515;&#20301;&#32622;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#26426;&#21046;FiT&#65292;&#35813;&#26426;&#21046;&#26088;&#22312;&#36890;&#36807;&#21482;&#36827;&#34892;&#20960;&#20010;&#21021;&#27493;&#26597;&#35810;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#30340;&#28304;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.02312</link><description>&lt;p&gt;
&#22914;&#20309;&#36873;&#25321;&#26368;&#20339;&#30340;&#30431;&#21451;&#36827;&#34892;&#21487;&#36716;&#31227;&#25915;&#20987;&#65311;
&lt;/p&gt;
&lt;p&gt;
How to choose your best allies for a transferable attack?. (arXiv:2304.02312v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#26469;&#35780;&#20272;&#21487;&#36716;&#31227;&#24615;&#65292;&#36890;&#36807;&#23558;&#30072;&#21464;&#25918;&#32622;&#20110;&#20013;&#24515;&#20301;&#32622;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#26426;&#21046;FiT&#65292;&#35813;&#26426;&#21046;&#26088;&#22312;&#36890;&#36807;&#21482;&#36827;&#34892;&#20960;&#20010;&#21021;&#27493;&#26597;&#35810;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#30340;&#28304;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#26679;&#26412;&#30340;&#21487;&#36716;&#31227;&#24615;&#26159;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#12290;&#19968;&#20010;&#20026;&#28304;&#27169;&#22411;&#32780;&#21046;&#36896;&#30340;&#23545;&#25239;&#26679;&#26412;&#21487;&#20197;&#27450;&#39575;&#21478;&#19968;&#20010;&#30446;&#26631;&#27169;&#22411;&#65292;&#20351;&#23545;&#25239;&#25915;&#20987;&#30340;&#23041;&#32961;&#26356;&#21152;&#30495;&#23454;&#12290;&#34913;&#37327;&#21487;&#36716;&#31227;&#24615;&#26159;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#20294;&#25915;&#20987;&#25104;&#21151;&#29575;&#26412;&#36523;&#24182;&#19981;&#33021;&#25552;&#20379;&#22362;&#23454;&#30340;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21487;&#36716;&#31227;&#24615;&#30340;&#26032;&#26041;&#27861;&#65292;&#23558;&#30072;&#21464;&#25918;&#32622;&#20110;&#20013;&#24515;&#20301;&#32622;&#12290;&#36825;&#20010;&#26032;&#24037;&#20855;&#26174;&#31034;&#65292;&#22914;&#26524;&#25915;&#20987;&#32773;&#38543;&#26426;&#36873;&#25321;&#28304;&#27169;&#22411;&#65292;&#37027;&#20040;&#21487;&#36716;&#31227;&#25915;&#20987;&#30340;&#34920;&#29616;&#21487;&#33021;&#36828;&#36828;&#19981;&#21450;&#40657;&#30418;&#25915;&#20987;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36873;&#25321;&#26426;&#21046;&#65292;&#31216;&#20026;FiT&#65292;&#35813;&#26426;&#21046;&#26088;&#22312;&#36890;&#36807;&#21482;&#36827;&#34892;&#20960;&#20010;&#21021;&#27493;&#26597;&#35810;&#21363;&#21487;&#36873;&#25321;&#26368;&#20339;&#30340;&#28304;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;FiT&#22312;&#36873;&#25321;&#22810;&#20010;&#25915;&#20987;&#24773;&#22659;&#19979;&#30340;&#26368;&#20339;&#28304;&#27169;&#22411;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#65292;&#20363;&#22914;&#21333;&#19968;&#27169;&#22411;&#25915;&#20987;&#12289;&#38598;&#25104;&#27169;&#22411;&#25915;&#20987;&#21644;&#22810;&#25915;&#20987;&#65288;&#20195;&#30721;&#21487;&#22312;https://github.com/weny1choi/FiT&#20013;&#25214;&#21040;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transferability of adversarial examples is a key issue in the security of deep neural networks. The possibility of an adversarial example crafted for a source model fooling another targeted model makes the threat of adversarial attacks more realistic. Measuring transferability is a crucial problem, but the Attack Success Rate alone does not provide a sound evaluation. This paper proposes a new methodology for evaluating transferability by putting distortion in a central position. This new tool shows that transferable attacks may perform far worse than a black box attack if the attacker randomly picks the source model. To address this issue, we propose a new selection mechanism, called FiT, which aims at choosing the best source model with only a few preliminary queries to the target. Our experimental results show that FiT is highly effective at selecting the best source model for multiple scenarios such as single-model attacks, ensemble-model attacks and multiple attacks (Code avai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23475;&#34411;&#35745;&#25968;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#23610;&#24230;&#21644;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#20013;&#24515;&#28857;&#32593;&#32476;&#36827;&#34892;&#20869;&#37096;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#32852;&#21512;&#29305;&#24449;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#23475;&#34411;&#22270;&#20687;&#20013;&#20005;&#37325;&#36974;&#25377;&#12289;&#24191;&#27867;&#23039;&#24577;&#21464;&#21270;&#21644;&#23610;&#24230;&#21464;&#21270;&#31561;&#38382;&#39064;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#20004;&#27493;&#24335;&#30340;&#22810;&#23610;&#24230;&#28909;&#22270;&#29983;&#25104;&#26041;&#27861;&#21644;&#21487;&#21464;&#24418;&#21367;&#31215;&#12290;&#22312;&#19977;&#20010;&#23475;&#34411;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#23475;&#34411;&#35745;&#25968;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02291</link><description>&lt;p&gt;
&#22522;&#20110;&#25429;&#33719;&#38519;&#38449;&#30340;&#23475;&#34411;&#35745;&#25968;&#65306;&#20869;&#37096;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#32852;&#21512;&#29305;&#24449;&#23398;&#20064;&#30340;&#22810;&#23610;&#24230;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#20013;&#24515;&#28857;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Trap-Based Pest Counting: Multiscale and Deformable Attention CenterNet Integrating Internal LR and HR Joint Feature Learning. (arXiv:2304.02291v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#23475;&#34411;&#35745;&#25968;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#23610;&#24230;&#21644;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#20013;&#24515;&#28857;&#32593;&#32476;&#36827;&#34892;&#20869;&#37096;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#32852;&#21512;&#29305;&#24449;&#23398;&#20064;&#12290;&#20026;&#20102;&#35299;&#20915;&#23475;&#34411;&#22270;&#20687;&#20013;&#20005;&#37325;&#36974;&#25377;&#12289;&#24191;&#27867;&#23039;&#24577;&#21464;&#21270;&#21644;&#23610;&#24230;&#21464;&#21270;&#31561;&#38382;&#39064;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#37319;&#29992;&#20102;&#20004;&#27493;&#24335;&#30340;&#22810;&#23610;&#24230;&#28909;&#22270;&#29983;&#25104;&#26041;&#27861;&#21644;&#21487;&#21464;&#24418;&#21367;&#31215;&#12290;&#22312;&#19977;&#20010;&#23475;&#34411;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#23475;&#34411;&#35745;&#25968;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23475;&#34411;&#35745;&#25968;&#26159;&#38750;&#24120;&#37325;&#35201;&#30340;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#39044;&#27979;&#26089;&#26399;&#23475;&#34411;&#25968;&#37327;&#65292;&#20174;&#32780;&#23454;&#29616;&#24555;&#36895;&#23475;&#34411;&#25511;&#21046;&#65292;&#20943;&#23569;&#23545;&#20316;&#29289;&#30340;&#25439;&#23475;&#65292;&#25552;&#39640;&#29983;&#20135;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#20809;&#38519;&#38449;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#24341;&#35825;&#21644;&#25293;&#25668;&#23475;&#34411;&#36827;&#34892;&#23475;&#34411;&#35745;&#25968;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20005;&#37325;&#30340;&#36974;&#25377;&#12289;&#24191;&#27867;&#30340;&#23039;&#24577;&#21464;&#21270;&#29978;&#33267;&#23610;&#24230;&#21464;&#21270;&#65292;&#23475;&#34411;&#22270;&#20687;&#30340;&#22806;&#35266;&#24046;&#24322;&#24456;&#22823;&#65292;&#36825;&#20351;&#24471;&#23475;&#34411;&#35745;&#25968;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23475;&#34411;&#35745;&#25968;&#27169;&#22411;&#65292;&#34987;&#31216;&#20026;&#22810;&#23610;&#24230;&#21487;&#21464;&#24418;&#27880;&#24847;&#21147;&#20013;&#24515;&#28857;&#32593;&#32476; (Mada-CenterNet)&#65292;&#29992;&#20110;&#20869;&#37096;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#32852;&#21512;&#29305;&#24449;&#23398;&#20064;&#12290;&#19982;&#20256;&#32479;&#30340;CenterNet&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;Mada-CenterNet&#37319;&#29992;&#20102;&#20004;&#27493;&#24335;&#30340;&#22810;&#23610;&#24230;&#28909;&#22270;&#29983;&#25104;&#26041;&#27861;&#26469;&#33258;&#36866;&#24212;&#22320;&#23398;&#20064;&#36866;&#24212;&#20110;&#23610;&#24230;&#21464;&#21270;&#65288;&#21363;&#23475;&#34411;&#25968;&#21464;&#21270;&#65289;&#30340;&#20302;&#20998;&#36776;&#29575;&#21644;&#39640;&#20998;&#36776;&#29575;&#28909;&#22270;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#20811;&#26381;&#23039;&#24577;&#21644;&#36974;&#25377;&#30340;&#25361;&#25112;&#65292;&#37319;&#29992;&#21487;&#21464;&#24418;&#21367;&#31215;&#26469;&#22686;&#24378;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;&#22312;&#19977;&#20010;&#23475;&#34411;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;Mada-CenterNet&#22312;&#23475;&#34411;&#35745;&#25968;&#20934;&#30830;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pest counting, which predicts the number of pests in the early stage, is very important because it enables rapid pest control, reduces damage to crops, and improves productivity. In recent years, light traps have been increasingly used to lure and photograph pests for pest counting. However, pest images have a wide range of variability in pest appearance owing to severe occlusion, wide pose variation, and even scale variation. This makes pest counting more challenging. To address these issues, this study proposes a new pest counting model referred to as multiscale and deformable attention CenterNet (Mada-CenterNet) for internal low-resolution (LR) and high-resolution (HR) joint feature learning. Compared with the conventional CenterNet, the proposed Mada-CenterNet adopts a multiscale heatmap generation approach in a two-step fashion to predict LR and HR heatmaps adaptively learned to scale variations, that is, changes in the number of pests. In addition, to overcome the pose and occlus
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#29992;&#20110;&#22312;&#23562;&#37325;&#22240;&#26524;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#29289;&#29702;&#23398;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#35843;&#25972;&#32553;&#25918;&#31995;&#25968;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02282</link><description>&lt;p&gt;
&#20851;&#20110;&#22312;&#23562;&#37325;&#22240;&#26524;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#29289;&#29702;&#23398;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#26368;&#20248;&#25439;&#22833;&#20989;&#25968;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
About optimal loss function for training physics-informed neural networks under respecting causality. (arXiv:2304.02282v1 [math.NA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02282
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#29992;&#20110;&#22312;&#23562;&#37325;&#22240;&#26524;&#20851;&#31995;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#29289;&#29702;&#23398;&#30693;&#35782;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#28040;&#38500;&#35843;&#25972;&#32553;&#25918;&#31995;&#25968;&#30340;&#38656;&#35201;&#65292;&#24182;&#22312;&#25968;&#20540;&#23454;&#39564;&#20013;&#35777;&#26126;&#20102;&#20854;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#23558;&#20855;&#26377;&#21021;&#22987;&#21644;&#36793;&#30028;&#26465;&#20214;&#30340;&#24494;&#20998;&#26041;&#31243;&#38382;&#39064;&#31616;&#21270;&#20026;&#20165;&#30001;&#24494;&#20998;&#26041;&#31243;&#25551;&#36848;&#30340;&#38382;&#39064;&#12290;&#20462;&#25913;&#21518;&#30340;&#29289;&#29702;&#21551;&#21457;&#24335;&#31070;&#32463;&#32593;&#32476;&#65288;PINNs&#65289;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#65292;&#21487;&#20197;&#23558;&#25439;&#22833;&#20989;&#25968;&#34920;&#31034;&#20026;&#19982;&#24494;&#20998;&#26041;&#31243;&#30456;&#20851;&#30340;&#21333;&#20010;&#39033;&#30340;&#24418;&#24335;&#65292;&#20174;&#32780;&#28040;&#38500;&#20102;&#35843;&#25972;&#19982;&#36793;&#30028;&#21644;&#21021;&#22987;&#26465;&#20214;&#30456;&#20851;&#30340;&#39033;&#30340;&#32553;&#25918;&#31995;&#25968;&#30340;&#38656;&#35201;&#12290;&#23562;&#37325;&#22240;&#26524;&#20851;&#31995;&#30340;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#34987;&#20462;&#25913;&#65292;&#22522;&#20110;&#24191;&#20041;&#20989;&#25968;&#25512;&#23548;&#20986;&#20102;&#26032;&#30340;&#21152;&#26435;&#25439;&#22833;&#20989;&#25968;&#12290;&#23545;&#20110;&#22810;&#20010;&#38382;&#39064;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
A method is presented that allows to reduce a problem described by differential equations with initial and boundary conditions to the problem described only by differential equations. The advantage of using the modified problem for physics-informed neural networks (PINNs) methodology is that it becomes possible to represent the loss function in the form of a single term associated with differential equations, thus eliminating the need to tune the scaling coefficients for the terms related to boundary and initial conditions. The weighted loss functions respecting causality were modified and new weighted loss functions based on generalized functions are derived. Numerical experiments have been carried out for a number of problems, demonstrating the accuracy of the proposed methods.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#34987;&#21149;&#35828;&#32773;&#30340;&#26465;&#20214;&#21644;&#29305;&#24449;&#36873;&#25321;&#26356;&#26377;&#25928;&#30340;&#21149;&#35828;&#20449;&#24687;&#21644;&#39044;&#27979;&#34892;&#20026;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#39033;&#38271;&#26399;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20010;&#26041;&#27861;&#33021;&#25104;&#21151;&#22320;&#20419;&#20351;&#26085;&#24120;&#21560;&#28895;&#32773;&#36827;&#34892;&#25106;&#28895;&#20934;&#22791;&#27963;&#21160;&#12290;</title><link>http://arxiv.org/abs/2304.02264</link><description>&lt;p&gt;
&#20511;&#21161;&#34394;&#25311;&#25945;&#32451;&#21149;&#23548;&#25106;&#28895;&#20934;&#22791;&#65306;&#22522;&#20110;&#29366;&#24577;&#21644;&#29992;&#25143;&#29305;&#24449;&#39044;&#27979;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Persuading to Prepare for Quitting Smoking with a Virtual Coach: Using States and User Characteristics to Predict Behavior. (arXiv:2304.02264v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02264
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25506;&#35752;&#20102;&#22914;&#20309;&#21033;&#29992;&#34987;&#21149;&#35828;&#32773;&#30340;&#26465;&#20214;&#21644;&#29305;&#24449;&#36873;&#25321;&#26356;&#26377;&#25928;&#30340;&#21149;&#35828;&#20449;&#24687;&#21644;&#39044;&#27979;&#34892;&#20026;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#39033;&#38271;&#26399;&#30740;&#31350;&#65292;&#32467;&#26524;&#34920;&#26126;&#36825;&#20010;&#26041;&#27861;&#33021;&#25104;&#21151;&#22320;&#20419;&#20351;&#26085;&#24120;&#21560;&#28895;&#32773;&#36827;&#34892;&#25106;&#28895;&#20934;&#22791;&#27963;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22312;&#34892;&#20026;&#21464;&#38761;&#30340;eHealth&#24212;&#29992;&#20013;&#38750;&#24120;&#26222;&#36941;&#65292;&#20294;&#21149;&#35828;&#20449;&#24687;&#23545;&#34892;&#20026;&#30340;&#24433;&#21709;&#24448;&#24448;&#24456;&#23567;&#12290;&#34987;&#21149;&#35828;&#32773;&#30340;&#26465;&#20214;&#25110;&#29366;&#24577;&#65288;&#20363;&#22914;&#20449;&#24515;&#65292;&#30693;&#35782;&#21644;&#21160;&#26426;&#65289;&#21644;&#29305;&#24449;&#65288;&#20363;&#22914;&#24615;&#21035;&#65292;&#24180;&#40836;&#21644;&#20010;&#24615;&#65289;&#26159;&#36873;&#25321;&#21149;&#35828;&#20449;&#24687;&#26356;&#26377;&#25928;&#31639;&#27861;&#30340;&#20004;&#20010;&#26377;&#21069;&#36884;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#33267;&#20170;&#36824;&#19981;&#22826;&#28165;&#26970;&#22312;&#36827;&#34892;&#21149;&#35828;&#23581;&#35797;&#21518;&#65292;&#29305;&#21035;&#26159;&#22312;&#38271;&#26399;&#20869;&#65292;&#32771;&#34385;&#36825;&#20123;&#32452;&#20214;&#33021;&#21542;&#24456;&#22909;&#22320;&#39044;&#27979;&#34892;&#20026;&#12290;&#30001;&#20110;&#25910;&#38598;&#35768;&#22810;&#31639;&#27861;&#32452;&#20214;&#30340;&#25968;&#25454;&#26159;&#26114;&#36149;&#30340;&#24182;&#19988;&#20250;&#32473;&#29992;&#25143;&#24102;&#26469;&#36127;&#25285;&#65292;&#22240;&#27492;&#27426;&#36814;&#26356;&#22909;&#22320;&#20102;&#35299;&#20010;&#21035;&#32452;&#20214;&#22312;&#23454;&#36341;&#20013;&#30340;&#24433;&#21709;&#65292;&#36825;&#26377;&#21161;&#20110;&#22312;&#20351;&#29992;&#21738;&#20123;&#32452;&#20214;&#19978;&#20570;&#20986;&#26126;&#26234;&#30340;&#20915;&#31574;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#32437;&#21521;&#30740;&#31350;&#65292;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#34394;&#25311;&#25945;&#32451;&#21149;&#35828;671&#21517;&#26085;&#24120;&#21560;&#28895;&#32773;&#20570;&#20934;&#22791;&#27963;&#21160;&#65292;&#22914;&#35774;&#24819;&#33258;&#24049;&#25152;&#24076;&#26395;&#30340;&#26410;&#26469;&#33258;&#25105;&#65292;&#20197;&#20934;&#22791;&#25106;&#28895;&#21644;&#21464;&#24471;&#26356;&#21152;&#36523;&#20307;&#27963;&#36291;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite their prevalence in eHealth applications for behavior change, persuasive messages tend to have small effects on behavior. Conditions or states (e.g., confidence, knowledge, motivation) and characteristics (e.g., gender, age, personality) of persuadees are two promising components for more effective algorithms for choosing persuasive messages. However, it is not yet sufficiently clear how well considering these components allows one to predict behavior after persuasive attempts, especially in the long run. Since collecting data for many algorithm components is costly and places a burden on users, a better understanding of the impact of individual components in practice is welcome. This can help to make an informed decision on which components to use. We thus conducted a longitudinal study in which a virtual coach persuaded 671 daily smokers to do preparatory activities for quitting smoking and becoming more physically active, such as envisioning one's desired future self. Based 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Ericson&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#24320;&#25918;&#39046;&#22495;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#20154;&#65292;&#20854;&#20013;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;&#38382;&#31572;&#21644;&#20449;&#24687;&#26816;&#32034;&#32452;&#20214;&#65292;&#20197;&#21450;&#29992;&#20110;&#20027;&#21160;&#38382;&#39064;&#32454;&#21270;&#21644;&#25512;&#33616;&#30340;&#24847;&#22270;&#25512;&#26029;&#21644;&#23545;&#35805;&#31649;&#29702;&#27169;&#22411;&#65292;&#22312;Alexa Prize&#20013;&#32463;&#36807;&#20102;&#21387;&#21147;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#25628;&#32034;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.02233</link><description>&lt;p&gt;
Ericson: &#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#24320;&#25918;&#39046;&#22495;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#20154;
&lt;/p&gt;
&lt;p&gt;
Ericson: An Interactive Open-Domain Conversational Search Agent. (arXiv:2304.02233v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Ericson&#65292;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#24320;&#25918;&#39046;&#22495;&#20250;&#35805;&#25628;&#32034;&#20195;&#29702;&#20154;&#65292;&#20854;&#20013;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;&#38382;&#31572;&#21644;&#20449;&#24687;&#26816;&#32034;&#32452;&#20214;&#65292;&#20197;&#21450;&#29992;&#20110;&#20027;&#21160;&#38382;&#39064;&#32454;&#21270;&#21644;&#25512;&#33616;&#30340;&#24847;&#22270;&#25512;&#26029;&#21644;&#23545;&#35805;&#31649;&#29702;&#27169;&#22411;&#65292;&#22312;Alexa Prize&#20013;&#32463;&#36807;&#20102;&#21387;&#21147;&#27979;&#35797;&#65292;&#24182;&#21457;&#29616;&#20102;&#29616;&#26377;&#25628;&#32034;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#39046;&#22495;&#20250;&#35805;&#25628;&#32034;&#65288;ODCS&#65289;&#26088;&#22312;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#12289;&#26368;&#26032;&#30340;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#25345;&#33258;&#28982;&#30340;&#23545;&#35805;&#65292;&#20197;&#24110;&#21161;&#29992;&#25143;&#32454;&#21270;&#24182;&#26368;&#32456;&#22238;&#31572;&#20449;&#24687;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#21019;&#24314;&#19968;&#20010;&#26377;&#25928;&#21644;&#24378;&#22823;&#30340;ODCS&#20195;&#29702;&#20154;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#23436;&#20840;&#21151;&#33021;&#30340;ODCS&#31995;&#32479;Ericson&#65292;&#20854;&#20013;&#21253;&#25324;&#26368;&#20808;&#36827;&#30340;&#38382;&#31572;&#21644;&#20449;&#24687;&#26816;&#32034;&#32452;&#20214;&#65292;&#20197;&#21450;&#29992;&#20110;&#20027;&#21160;&#38382;&#39064;&#32454;&#21270;&#21644;&#25512;&#33616;&#30340;&#24847;&#22270;&#25512;&#26029;&#21644;&#23545;&#35805;&#31649;&#29702;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;&#20122;&#39532;&#36874;Alexa Prize&#20013;&#32463;&#36807;&#20102;&#21387;&#21147;&#27979;&#35797;&#65292;&#19982;&#25968;&#21315;&#21517;Alexa&#29992;&#25143;&#36827;&#34892;&#20102;&#23454;&#26102;&#23545;&#35805;&#65292;&#20174;&#32780;&#20026;&#20998;&#26512;ODCS&#31995;&#32479;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#30340;&#22522;&#30784;&#25552;&#20379;&#20102;&#32463;&#39564;&#20381;&#25454;&#12290;&#25105;&#20204;&#30340;&#20132;&#20114;&#25968;&#25454;&#20998;&#26512;&#34920;&#26126;&#65292;&#31934;&#30830;&#30340;&#24847;&#22270;&#20998;&#31867;&#12289;&#40723;&#21169;&#29992;&#25143;&#21442;&#19982;&#21644;&#20180;&#32454;&#30340;&#20027;&#21160;&#25512;&#33616;&#23545;&#29992;&#25143;&#28385;&#24847;&#24230;&#20570;&#20986;&#20102;&#26368;&#22823;&#30340;&#36129;&#29486;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36827;&#19968;&#27493;&#30830;&#23450;&#20102;&#29616;&#26377;&#25628;&#32034;&#25216;&#26415;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open-domain conversational search (ODCS) aims to provide valuable, up-to-date information, while maintaining natural conversations to help users refine and ultimately answer information needs. However, creating an effective and robust ODCS agent is challenging. In this paper, we present a fully functional ODCS system, Ericson, which includes state-of-the-art question answering and information retrieval components, as well as intent inference and dialogue management models for proactive question refinement and recommendations. Our system was stress-tested in the Amazon Alexa Prize, by engaging in live conversations with thousands of Alexa users, thus providing empirical basis for the analysis of the ODCS system in real settings. Our interaction data analysis revealed that accurate intent classification, encouraging user engagement, and careful proactive recommendations contribute most to the users satisfaction. Our study further identifies limitations of the existing search techniques, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.02213</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20316;&#20026;&#38053;&#21273;&#65306;&#29992;GPT&#35299;&#23494;&#26448;&#26009;&#31185;&#23398;&#30340;&#31192;&#23494;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models as Master Key: Unlocking the Secrets of Materials Science with GPT. (arXiv:2304.02213v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02213
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#21033;&#29992;GPT-3&#27169;&#22411;&#33021;&#22815;&#20934;&#30830;&#25552;&#21462;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#30340;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#23637;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26448;&#26009;&#23398;&#20013;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#8212;&#8212;&#32467;&#26500;&#21270;&#20449;&#24687;&#25512;&#29702;&#65288;SIS&#65289;&#65292;&#20197;&#35299;&#20915;&#26448;&#26009;&#31185;&#23398;&#35774;&#22791;&#23618;&#38754;&#20449;&#24687;&#25552;&#21462;&#30340;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#29616;&#26377;&#30340;&#38041;&#38043;&#30719;&#22826;&#38451;&#33021;&#30005;&#27744;FAIR&#25968;&#25454;&#38598;&#23545;GPT-3&#36827;&#34892;&#24494;&#35843;&#65292;&#33719;&#24471;&#20102;91.8 F1&#24471;&#20998;&#65292;&#24182;&#26356;&#26032;&#20102;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#36804;&#20170;&#20026;&#27490;&#25152;&#26377;&#30456;&#20851;&#31185;&#23398;&#35770;&#25991;&#12290;&#25152;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#24050;&#34987;&#26684;&#24335;&#21270;&#21644;&#26631;&#20934;&#21270;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#30452;&#25509;&#20316;&#20026;&#21518;&#32493;&#25968;&#25454;&#20998;&#26512;&#30340;&#36755;&#20837;&#12290;&#36825;&#20010;&#29305;&#24615;&#23558;&#20351;&#26448;&#26009;&#31185;&#23398;&#23478;&#36890;&#36807;&#36873;&#25321;&#39640;&#36136;&#37327;&#30340;&#39046;&#22495;&#35780;&#35770;&#25991;&#31456;&#26469;&#24320;&#21457;&#20854;&#33258;&#24049;&#30340;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#23454;&#39564;&#26469;&#39044;&#27979;PCE&#21644;&#21453;&#21521;&#39044;&#27979;&#21442;&#25968;&#65292;&#24182;&#33719;&#24471;&#20102;&#19982;DFT&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#35777;&#26126;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#20687;&#26448;&#26009;&#23398;&#23478;&#19968;&#26679;&#35780;&#21028;&#26448;&#26009;&#21644;&#35774;&#35745;&#26032;&#26448;&#26009;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a new NLP task called structured information inference (SIS) to address the complexities of information extraction at the device level in materials science. We accomplished this task by finetuning GPT-3 on a exsiting perovskite solar cell FAIR dataset with 91.8 F1-score and we updated the dataset with all related scientific papers up to now. The produced dataset is formatted and normalized, enabling its direct utilization as input in subsequent data analysis. This feature will enable materials scientists to develop their own models by selecting high-quality review papers within their domain. Furthermore, we designed experiments to predict PCE and reverse-predict parameters and obtained comparable performance with DFT, which demonstrates the potential of large language models to judge materials and design new materials like a materials scientist.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20197;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20026;&#35797;&#39564;&#22330;&#65292;&#28145;&#20837;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#31687;&#24314;&#27169;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#21033;&#29992;LLMs&#24378;&#22823;&#30340;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#25552;&#31034;&#26041;&#38754;&#36827;&#34892;&#25913;&#36827;&#20063;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#24182;&#19988;LLMs&#26377;&#28508;&#21147;&#32534;&#30721;&#20016;&#23500;&#30340;&#35821;&#31687;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.02210</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Document-Level Machine Translation with Large Language Models. (arXiv:2304.02210v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20197;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20026;&#35797;&#39564;&#22330;&#65292;&#28145;&#20837;&#35780;&#20272;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35821;&#31687;&#24314;&#27169;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#30740;&#31350;&#21457;&#29616;&#21033;&#29992;LLMs&#24378;&#22823;&#30340;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#21487;&#20197;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#22312;&#25552;&#31034;&#26041;&#38754;&#36827;&#34892;&#25913;&#36827;&#20063;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65292;&#24182;&#19988;LLMs&#26377;&#28508;&#21147;&#32534;&#30721;&#20016;&#23500;&#30340;&#35821;&#31687;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;Chat-GPT&#21487;&#20197;&#20026;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#29983;&#25104;&#36830;&#36143;&#65292;&#36830;&#36143;&#65292;&#30456;&#20851;&#21644;&#27969;&#30021;&#30340;&#31572;&#26696;&#12290;&#26412;&#25991;&#20197;&#25991;&#26723;&#32423;&#26426;&#22120;&#32763;&#35793;&#20026;&#35797;&#39564;&#22330;&#65292;&#25552;&#20379;&#20102;LLMs&#22312;&#35821;&#31687;&#24314;&#27169;&#26041;&#38754;&#30340;&#28145;&#20837;&#35780;&#20272;&#12290;&#26412;&#30740;&#31350;&#30528;&#37325;&#20851;&#27880;&#19977;&#20010;&#26041;&#38754;&#65306;1&#65289;&#35821;&#31687;&#24863;&#30693;&#25552;&#31034;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#35843;&#26597;&#19981;&#21516;&#25552;&#31034;&#23545;&#25991;&#26723;&#32423;&#32763;&#35793;&#36136;&#37327;&#21644;&#35821;&#31687;&#29616;&#35937;&#30340;&#24433;&#21709;&#65307;2&#65289;&#32763;&#35793;&#27169;&#22411;&#30340;&#27604;&#36739;&#65292;&#25105;&#20204;&#27604;&#36739;Chat-GPT&#19982;&#21830;&#19994;MT&#31995;&#32479;&#21644;&#39640;&#32423;&#25991;&#26723;&#32423;MT&#26041;&#27861;&#30340;&#32763;&#35793;&#24615;&#33021;&#65307;3&#65289;&#35821;&#31687;&#24314;&#27169;&#33021;&#21147;&#20998;&#26512;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#31350;LLMs&#20013;&#32534;&#30721;&#30340;&#35821;&#31687;&#30693;&#35782;&#65292;&#24182;&#30740;&#31350;&#22521;&#35757;&#25216;&#26415;&#23545;&#35821;&#31687;&#24314;&#27169;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#35780;&#20272;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#65292;&#25105;&#20204;&#24778;&#35766;&#22320;&#21457;&#29616;&#65292;1&#65289;&#21033;&#29992;&#24378;&#22823;&#30340;&#38271;&#25991;&#26412;&#24314;&#27169;&#33021;&#21147;&#65292;ChatGPT&#22312;&#25991;&#26723;&#32423;&#32763;&#35793;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#21830;&#19994;MT&#31995;&#32479;&#21644;&#39640;&#32423;&#25991;&#26723;&#32423;MT&#26041;&#27861;&#65307;2&#65289;&#20462;&#25913;&#26126;&#30830;&#38024;&#23545;&#35821;&#31687;&#29616;&#35937;&#30340;&#25552;&#31034;&#21487;&#20197;&#26174;&#30528;&#25552;&#39640;&#32763;&#35793;&#36136;&#37327;&#65307;3&#65289;LLMs&#26377;&#28508;&#21147;&#32534;&#30721;&#20016;&#23500;&#30340;&#35821;&#31687;&#30693;&#35782;&#65292;&#22521;&#35757;&#25216;&#26415;&#21487;&#20197;&#36827;&#19968;&#27493;&#22686;&#24378;&#36825;&#31181;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) such as Chat-GPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs' ability on discourse modeling. The study fo-cuses on three aspects: 1) Effects of Discourse-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of Chat-GPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and examine the impact of training techniques on discourse modeling. By evaluating a number of benchmarks, we surprisingly find that 1) leveraging their powerful long-text mod-eling capabilities, ChatGPT outperforms commercial MT systems 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; MoocRadar&#65292;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#12289;&#32454;&#31890;&#24230;&#30340;&#30693;&#35782;&#24211;&#65292;&#29992;&#20110;&#25552;&#39640; MOOC &#20013;&#35748;&#30693;&#23398;&#29983;&#24314;&#27169;&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.02205</link><description>&lt;p&gt;
MoocRadar: &#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#12289;&#32454;&#31890;&#24230;&#30340;&#30693;&#35782;&#24211;&#65292;&#29992;&#20110;&#25552;&#39640; MOOC &#20013;&#35748;&#30693;&#23398;&#29983;&#24314;&#27169;&#30340;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;
MoocRadar: A Fine-grained and Multi-aspect Knowledge Repository for Improving Cognitive Student Modeling in MOOCs. (arXiv:2304.02205v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02205
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; MoocRadar&#65292;&#19968;&#31181;&#22810;&#26041;&#38754;&#30340;&#12289;&#32454;&#31890;&#24230;&#30340;&#30693;&#35782;&#24211;&#65292;&#29992;&#20110;&#25552;&#39640; MOOC &#20013;&#35748;&#30693;&#23398;&#29983;&#24314;&#27169;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#29983;&#24314;&#27169;&#26159;&#26234;&#33021;&#25945;&#32946;&#20013;&#25512;&#26029;&#23398;&#29983;&#23398;&#20064;&#29305;&#24449;&#30340;&#19968;&#39033;&#22522;&#26412;&#20219;&#21153;&#12290;&#23613;&#31649;&#20174;&#30693;&#35782;&#36319;&#36394;&#21644;&#35748;&#30693;&#35786;&#26029;&#30340;&#26368;&#36817;&#23581;&#35797;&#25552;&#20986;&#20102;&#20960;&#20010;&#26377;&#24076;&#26395;&#25913;&#36827;&#24403;&#21069;&#27169;&#22411;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#30340;&#26041;&#21521;&#65292;&#20294;&#29616;&#26377;&#20844;&#20849;&#25968;&#25454;&#38598;&#20173;&#28982;&#19981;&#36275;&#20197;&#28385;&#36275;&#36825;&#20123;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#30340;&#38656;&#27714;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#30053;&#20102;&#23436;&#25972;&#30340;&#32451;&#20064;&#24773;&#22659;&#12289;&#32454;&#31890;&#24230;&#30340;&#27010;&#24565;&#21644;&#35748;&#30693;&#26631;&#31614;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102; MoocRadar&#65292;&#23427;&#26159;&#19968;&#20010;&#30001; 2,513 &#20010;&#32451;&#20064;&#38382;&#39064;&#12289;5,600 &#20010;&#30693;&#35782;&#27010;&#24565;&#21644;&#36229;&#36807; 1200 &#19975;&#34892;&#20026;&#35760;&#24405;&#32452;&#25104;&#30340;&#32454;&#31890;&#24230;&#12289;&#22810;&#26041;&#38754;&#30340;&#30693;&#35782;&#24211;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20197;&#20445;&#35777;&#32454;&#31890;&#24230;&#27010;&#24565;&#21644;&#35748;&#30693;&#26631;&#31614;&#30340;&#39640;&#36136;&#37327;&#21644;&#20840;&#38754;&#24615;&#27880;&#37322;&#12290;&#32479;&#35745;&#21644;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#20026;&#26410;&#26469;&#25913;&#36827;&#26234;&#33021;&#25945;&#32946;&#27169;&#22411;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Student modeling, the task of inferring a student's learning characteristics through their interactions with coursework, is a fundamental issue in intelligent education. Although the recent attempts from knowledge tracing and cognitive diagnosis propose several promising directions for improving the usability and effectiveness of current models, the existing public datasets are still insufficient to meet the need for these potential solutions due to their ignorance of complete exercising contexts, fine-grained concepts, and cognitive labels. In this paper, we present MoocRadar, a fine-grained, multi-aspect knowledge repository consisting of 2,513 exercise questions, 5,600 knowledge concepts, and over 12 million behavioral records. Specifically, we propose a framework to guarantee a high-quality and comprehensive annotation of fine-grained concepts and cognitive labels. The statistical and experimental results indicate that our dataset provides the basis for the future improvements of e
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20197;&#38750;&#27954;&#21355;&#29983;&#39046;&#22495;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#20840;&#29699;&#21270;&#30340;&#20844;&#27491;&#23646;&#24615;&#30340;&#20307;&#31995;&#65292;&#24182;&#24212;&#29992;&#20110;&#21307;&#30103;&#27169;&#24335;&#65292;&#20026;&#20419;&#36827;&#20840;&#29699;&#21355;&#29983;&#20013;&#30340;&#20844;&#24179;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#21644;&#34892;&#21160;&#30340;&#21628;&#21505;&#12290;</title><link>http://arxiv.org/abs/2304.02190</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#20840;&#29699;&#21270;&#30340;&#20844;&#27491;&#23646;&#24615;&#65306;&#20197;&#38750;&#27954;&#20581;&#24247;&#20026;&#20363;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Globalizing Fairness Attributes in Machine Learning: A Case Study on Health in Africa. (arXiv:2304.02190v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02190
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20197;&#38750;&#27954;&#21355;&#29983;&#39046;&#22495;&#20026;&#26696;&#20363;&#30740;&#31350;&#65292;&#25552;&#20986;&#20102;&#26426;&#22120;&#23398;&#20064;&#20013;&#20840;&#29699;&#21270;&#30340;&#20844;&#27491;&#23646;&#24615;&#30340;&#20307;&#31995;&#65292;&#24182;&#24212;&#29992;&#20110;&#21307;&#30103;&#27169;&#24335;&#65292;&#20026;&#20419;&#36827;&#20840;&#29699;&#21355;&#29983;&#20013;&#30340;&#20844;&#24179;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#21644;&#34892;&#21160;&#30340;&#21628;&#21505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#24212;&#29992;&#26085;&#30410;&#22686;&#21152;&#65292;&#26377;&#20154;&#21628;&#21505;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#23454;&#29616;&#20844;&#24179;&#65292;&#20197;&#20102;&#35299;&#21644;&#32531;&#35299;&#36825;&#20123;&#31995;&#32479;&#21487;&#33021;&#24102;&#26469;&#30340;&#20262;&#29702;&#38382;&#39064;&#12290;&#20844;&#24179;&#23545;&#38750;&#27954;&#30340;&#20840;&#29699;&#21355;&#29983;&#20855;&#26377;&#24433;&#21709;&#65292;&#22240;&#20026;&#20840;&#29699;&#21335;&#21271;&#20043;&#38388;&#24050;&#32463;&#23384;&#22312;&#19981;&#20844;&#24179;&#30340;&#26435;&#21147;&#22833;&#34913;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#38750;&#27954;&#20581;&#24247;&#30340;&#20844;&#24179;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#38750;&#27954;&#35821;&#22659;&#19979;&#32771;&#34385;&#20844;&#24179;&#24615;&#30340;&#23646;&#24615;&#65292;&#24182;&#21246;&#30011;&#20102;&#36825;&#20123;&#23646;&#24615;&#21487;&#33021;&#22312;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#21307;&#23398;&#27169;&#24335;&#20013;&#21457;&#25381;&#20316;&#29992;&#30340;&#33539;&#22260;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#20419;&#36827;&#20840;&#29699;&#21355;&#29983;&#20013;&#30340;&#20844;&#24179;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#21644;&#34892;&#21160;&#30340;&#21628;&#21505;&#12290;
&lt;/p&gt;
&lt;p&gt;
With growing machine learning (ML) applications in healthcare, there have been calls for fairness in ML to understand and mitigate ethical concerns these systems may pose. Fairness has implications for global health in Africa, which already has inequitable power imbalances between the Global North and South. This paper seeks to explore fairness for global health, with Africa as a case study. We propose fairness attributes for consideration in the African context and delineate where they may come into play in different ML-enabled medical modalities. This work serves as a basis and call for action for furthering research into fairness in global health.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#25289;&#29677;&#21160;&#20316;&#20998;&#26512;&#36816;&#21160;&#32534;&#30721;&#31995;&#32479;&#30340;&#39640;&#36136;&#37327;&#20154;&#31867;&#36816;&#21160;&#35201;&#32032;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#27492;&#25968;&#25454;&#38598;&#20849;&#21516;&#23398;&#20064;&#36816;&#21160;&#35201;&#32032;&#21644;&#24773;&#32490;&#65292;&#25552;&#39640;&#26426;&#22120;&#29702;&#35299;&#36523;&#20307;&#35821;&#35328;&#34920;&#36798;&#30340;&#24773;&#32490;&#33021;&#21147;&#65292;&#20026;&#31934;&#31070;&#31185;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#23450;&#37327;&#35786;&#26029;&#21644;&#39044;&#21518;&#36741;&#21161;&#65292;&#20197;&#21450;&#24110;&#21161;&#25191;&#27861;&#26426;&#26500;&#35782;&#21035;&#27450;&#39575;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2304.02187</link><description>&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#25289;&#29677;&#21160;&#20316;&#20998;&#26512;&#23454;&#29616;&#36523;&#20307;&#34920;&#36798;&#24773;&#32490;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Bodily expressed emotion understanding through integrating Laban movement analysis. (arXiv:2304.02187v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02187
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;&#25289;&#29677;&#21160;&#20316;&#20998;&#26512;&#36816;&#21160;&#32534;&#30721;&#31995;&#32479;&#30340;&#39640;&#36136;&#37327;&#20154;&#31867;&#36816;&#21160;&#35201;&#32032;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#27492;&#25968;&#25454;&#38598;&#20849;&#21516;&#23398;&#20064;&#36816;&#21160;&#35201;&#32032;&#21644;&#24773;&#32490;&#65292;&#25552;&#39640;&#26426;&#22120;&#29702;&#35299;&#36523;&#20307;&#35821;&#35328;&#34920;&#36798;&#30340;&#24773;&#32490;&#33021;&#21147;&#65292;&#20026;&#31934;&#31070;&#31185;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#23450;&#37327;&#35786;&#26029;&#21644;&#39044;&#21518;&#36741;&#21161;&#65292;&#20197;&#21450;&#24110;&#21161;&#25191;&#27861;&#26426;&#26500;&#35782;&#21035;&#27450;&#39575;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36523;&#20307;&#21160;&#20316;&#20256;&#36882;&#30528;&#20154;&#20204;&#24773;&#32490;&#21644;&#31934;&#31070;&#29366;&#24577;&#30340;&#37325;&#35201;&#20449;&#24687;&#65292;&#26159;&#26085;&#24120;&#20132;&#27969;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#12290;&#22686;&#24378;&#26426;&#22120;&#29702;&#35299;&#36523;&#20307;&#35821;&#35328;&#34920;&#36798;&#30340;&#24773;&#32490;&#33021;&#21147;&#21487;&#20197;&#25552;&#39640;&#36741;&#21161;&#26426;&#22120;&#20154;&#19982;&#20799;&#31461;&#21644;&#32769;&#24180;&#29992;&#25143;&#30340;&#20132;&#27969;&#65292;&#20026;&#31934;&#31070;&#31185;&#19987;&#19994;&#20154;&#21592;&#25552;&#20379;&#23450;&#37327;&#35786;&#26029;&#21644;&#39044;&#21518;&#36741;&#21161;&#65292;&#20197;&#21450;&#24110;&#21161;&#25191;&#27861;&#26426;&#26500;&#35782;&#21035;&#27450;&#39575;&#34892;&#20026;&#12290;&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#22522;&#20110;&#25289;&#29677;&#21160;&#20316;&#20998;&#26512;&#36816;&#21160;&#32534;&#30721;&#31995;&#32479;&#30340;&#39640;&#36136;&#37327;&#20154;&#31867;&#36816;&#21160;&#35201;&#32032;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#27492;&#25968;&#25454;&#38598;&#20849;&#21516;&#23398;&#20064;&#36816;&#21160;&#35201;&#32032;&#21644;&#24773;&#32490;&#12290;&#25105;&#20204;&#30340;&#38271;&#26399;&#30446;&#26631;&#26159;&#25972;&#21512;&#35745;&#31639;&#26426;&#12289;&#24515;&#29702;&#23398;&#21644;&#34920;&#28436;&#33402;&#26415;&#30340;&#30693;&#35782;&#65292;&#36890;&#36807;&#36523;&#20307;&#35821;&#35328;&#23454;&#29616;&#33258;&#21160;&#29702;&#35299;&#21644;&#20998;&#26512;&#24773;&#32490;&#21644;&#31934;&#31070;&#29366;&#24577;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#20154;&#31867;&#36816;&#21160;&#20998;&#26512;&#20013;&#30340;&#24773;&#32490;&#35782;&#21035;&#25552;&#20379;&#20102;&#33391;&#22909;&#30340;&#36215;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Body movements carry important information about a person's emotions or mental state and are essential in daily communication. Enhancing the ability of machines to understand emotions expressed through body language can improve the communication of assistive robots with children and elderly users, provide psychiatric professionals with quantitative diagnostic and prognostic assistance, and aid law enforcement in identifying deception. This study develops a high-quality human motor element dataset based on the Laban Movement Analysis movement coding system and utilizes that to jointly learn about motor elements and emotions. Our long-term ambition is to integrate knowledge from computing, psychology, and performing arts to enable automated understanding and analysis of emotion and mental state through body language. This work serves as a launchpad for further research into recognizing emotions through analysis of human movement.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25506;&#31350;&#20102;&#20154;&#20204;&#23545;&#31639;&#27861;&#20260;&#23475;&#30340;&#21453;&#24212;&#65292;&#21457;&#29616;&#20154;&#20204;&#23545;&#26426;&#22120;&#12289;&#35774;&#35745;&#24072;&#21644;&#29992;&#25143;&#30340;&#36131;&#22791;&#20998;&#37197;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#19988;&#20154;&#20204;&#23545;&#26426;&#22120;&#21457;&#36215;&#36131;&#22791;&#30340;&#20915;&#31574;&#21462;&#20915;&#20110;&#20182;&#20204;&#26159;&#21542;&#35748;&#20026;&#36825;&#26159;&#19968;&#20010;&#21512;&#36866;&#30340;&#21453;&#24212;&#12290;</title><link>http://arxiv.org/abs/2304.02176</link><description>&lt;p&gt;
&#24402;&#21646;&#20110;&#20154;&#21644;&#26426;&#22120;&#65306;&#20160;&#20040;&#20915;&#23450;&#20154;&#20204;&#23545;&#31639;&#27861;&#20260;&#23475;&#30340;&#21453;&#24212;
&lt;/p&gt;
&lt;p&gt;
Blaming Humans and Machines: What Shapes People's Reactions to Algorithmic Harm. (arXiv:2304.02176v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25506;&#31350;&#20102;&#20154;&#20204;&#23545;&#31639;&#27861;&#20260;&#23475;&#30340;&#21453;&#24212;&#65292;&#21457;&#29616;&#20154;&#20204;&#23545;&#26426;&#22120;&#12289;&#35774;&#35745;&#24072;&#21644;&#29992;&#25143;&#30340;&#36131;&#22791;&#20998;&#37197;&#26377;&#25152;&#19981;&#21516;&#65292;&#24182;&#19988;&#20154;&#20204;&#23545;&#26426;&#22120;&#21457;&#36215;&#36131;&#22791;&#30340;&#20915;&#31574;&#21462;&#20915;&#20110;&#20182;&#20204;&#26159;&#21542;&#35748;&#20026;&#36825;&#26159;&#19968;&#20010;&#21512;&#36866;&#30340;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#21487;&#33021;&#20250;&#23545;&#20154;&#20204;&#36896;&#25104;&#20260;&#23475;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#36131;&#22791;&#30340;&#35270;&#35282;&#30740;&#31350;&#20102;&#20010;&#20307;&#23545;&#27492;&#31867;&#20260;&#23475;&#30340;&#21453;&#24212;&#12290;&#22312;&#21069;&#20154;&#26377;&#20154;&#20204;&#24402;&#21646;&#20110;AI&#31995;&#32479;&#30340;&#30740;&#31350;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#20960;&#20010;&#22240;&#32032;&#23545;&#20010;&#20307;&#21453;&#24212;&#26426;&#22120;&#12289;&#35774;&#35745;&#24072;&#21644;&#29992;&#25143;&#30340;&#36131;&#22791;&#39118;&#38505;&#30340;&#24433;&#21709;&#12290;&#19977;&#39033;&#30740;&#31350;&#32467;&#26524;&#65288;N = 1,153&#65289;&#34920;&#26126;&#65292;&#22312;&#24402;&#21646;&#20110;&#36825;&#20123;&#34892;&#20026;&#32773;&#30340;&#36807;&#31243;&#20013;&#23384;&#22312;&#24046;&#24322;&#12290;AI&#31995;&#32479;&#26159;&#21542;&#21487;&#35299;&#37322;&#24182;&#19981;&#24433;&#21709;&#20154;&#20204;&#23558;&#36131;&#22791;&#25918;&#22312;&#20182;&#20204;&#12289;&#20182;&#20204;&#30340;&#24320;&#21457;&#32773;&#21644;&#29992;&#25143;&#30340;&#36523;&#19978;&#12290;&#20844;&#24179;&#21644;&#26377;&#23475;&#30340;&#32771;&#34385;&#22686;&#21152;&#20102;&#23545;&#35774;&#35745;&#24072;&#21644;&#29992;&#25143;&#30340;&#36131;&#22791;&#65292;&#20294;&#23545;AI&#31995;&#32479;&#30340;&#21028;&#26029;&#20960;&#20046;&#27809;&#26377;&#20219;&#20309;&#24433;&#21709;&#12290;&#30456;&#21453;&#65292;&#20915;&#23450;&#20154;&#20204;&#23545;&#26426;&#22120;&#30340;&#21453;&#24212;&#24577;&#24230;&#30340;&#26159;&#20154;&#20204;&#35748;&#20026;&#23558;&#36131;&#22791;&#20182;&#20204;&#20316;&#20026;&#23545;&#31639;&#27861;&#20260;&#23475;&#30340;&#21512;&#36866;&#21453;&#24212;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#19968;&#20123;&#21547;&#20041;&#65292;&#20363;&#22914;&#26410;&#26469;&#20851;&#20110;&#23558;AI&#31995;&#32479;&#32435;&#20837;&#31038;&#20250;&#21644;&#36947;&#24503;&#33539;&#30068;&#30340;&#20915;&#31574;&#23558;&#22914;&#20309;&#22609;&#36896;&#26222;&#36890;&#20154;&#23545;AI&#30340;&#21453;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence (AI) systems can cause harm to people. This research examines how individuals react to such harm through the lens of blame. Building upon research suggesting that people blame AI systems, we investigated how several factors influence people's reactive attitudes towards machines, designers, and users. The results of three studies (N = 1,153) indicate differences in how blame is attributed to these actors. Whether AI systems were explainable did not impact blame directed at them, their developers, and their users. Considerations about fairness and harmfulness increased blame towards designers and users but had little to no effect on judgments of AI systems. Instead, what determined people's reactive attitudes towards machines was whether people thought blaming them would be a suitable response to algorithmic harm. We discuss implications, such as how future decisions about including AI systems in the social and moral spheres will shape laypeople's reactions to AI-
&lt;/p&gt;</description></item><item><title>ChartReader &#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#21487;&#20197;&#26080;&#21551;&#21457;&#24335;&#35268;&#21017;&#36827;&#34892;&#22270;&#34920;&#21435;&#28210;&#26579;&#21644;&#29702;&#35299;&#65292;&#36890;&#36807;&#20174;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#23398;&#20064;&#22270;&#34920;&#35268;&#21017;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#35268;&#21017;&#21046;&#23450;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.02173</link><description>&lt;p&gt;
ChartReader&#65306;&#19968;&#20010;&#26080;&#21551;&#21457;&#24335;&#35268;&#21017;&#30340;&#22270;&#34920;&#21435;&#28210;&#26579;&#19982;&#29702;&#35299;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules. (arXiv:2304.02173v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02173
&lt;/p&gt;
&lt;p&gt;
ChartReader &#26159;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#21487;&#20197;&#26080;&#21551;&#21457;&#24335;&#35268;&#21017;&#36827;&#34892;&#22270;&#34920;&#21435;&#28210;&#26579;&#21644;&#29702;&#35299;&#65292;&#36890;&#36807;&#20174;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#23398;&#20064;&#22270;&#34920;&#35268;&#21017;&#65292;&#28040;&#38500;&#20102;&#25163;&#21160;&#35268;&#21017;&#21046;&#23450;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#65292;&#22312;&#22810;&#39033;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#34920;&#26159;&#19968;&#31181;&#20256;&#36798;&#22797;&#26434;&#25968;&#25454;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#20294;&#30001;&#20110;&#22270;&#34920;&#31867;&#22411;&#30340;&#22810;&#26679;&#24615;&#21644;&#22797;&#26434;&#30340;&#32452;&#20214;&#65292;&#20854;&#29702;&#35299;&#23545;&#20110;&#25105;&#20204;&#26469;&#35828;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#22270;&#34920;&#29702;&#35299;&#26041;&#27861;&#35201;&#20040;&#20381;&#36182;&#21551;&#21457;&#24335;&#35268;&#21017;&#65292;&#35201;&#20040;&#36807;&#24230;&#20381;&#36182;OCR&#31995;&#32479;&#65292;&#32467;&#26524;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChartReader&#65292;&#19968;&#20010;&#26080;&#32541;&#38598;&#25104;&#22270;&#34920;&#21435;&#28210;&#26579;&#21644;&#29702;&#35299;&#20219;&#21153;&#30340;&#32479;&#19968;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#22522;&#20110;Transformer&#30340;&#22270;&#34920;&#32452;&#20214;&#26816;&#27979;&#27169;&#22359;&#21644;&#19968;&#20010;&#25193;&#23637;&#30340;&#39044;&#35757;&#32451;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#22270;&#34920;&#21040;X&#20219;&#21153;&#12290;&#36890;&#36807;&#20174;&#27880;&#37322;&#25968;&#25454;&#38598;&#20013;&#33258;&#21160;&#23398;&#20064;&#22270;&#34920;&#35268;&#21017;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#28040;&#38500;&#20102;&#25163;&#21160;&#35268;&#21017;&#21046;&#23450;&#30340;&#38656;&#27714;&#65292;&#20943;&#23569;&#20102;&#24037;&#20316;&#37327;&#24182;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#25968;&#25454;&#21464;&#37327;&#26367;&#25442;&#25216;&#26415;&#65292;&#24182;&#25193;&#23637;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36755;&#20837;&#21644;&#20301;&#32622;&#23884;&#20837;&#65292;&#23454;&#29616;&#36328;&#20219;&#21153;&#35757;&#32451;&#12290;&#25105;&#20204;&#23545;ChartReader&#36827;&#34892;&#20102;Chart-to-Table&#12289;ChartQA&#21644;Chart-to-Text&#20219;&#21153;&#30340;&#35780;&#20272;&#65292;&#34920;&#26126;&#20854;&#24615;&#33021;&#20248;&#20110;&#29616;&#26377;&#30340;&#22270;&#34920;&#29702;&#35299;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Charts are a powerful tool for visually conveying complex data, but their comprehension poses a challenge due to the diverse chart types and intricate components. Existing chart comprehension methods suffer from either heuristic rules or an over-reliance on OCR systems, resulting in suboptimal performance. To address these issues, we present ChartReader, a unified framework that seamlessly integrates chart derendering and comprehension tasks. Our approach includes a transformer-based chart component detection module and an extended pre-trained vision-language model for chart-to-X tasks. By learning the rules of charts automatically from annotated datasets, our approach eliminates the need for manual rule-making, reducing effort and enhancing accuracy.~We also introduce a data variable replacement technique and extend the input and position embeddings of the pre-trained model for cross-task training. We evaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks, demonstrat
&lt;/p&gt;</description></item><item><title>&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#65292;&#19988;&#26080;&#38656;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02169</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#32423;&#33258;&#22238;&#24402;&#35821;&#35328;&#27169;&#22411;&#21512;&#25104;&#26497;&#39640;&#32500;&#38271;&#26399;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;
&lt;/p&gt;
&lt;p&gt;
Synthesize Extremely High-dimensional Longitudinal Electronic Health Records via Hierarchical Autoregressive Language Model. (arXiv:2304.02169v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02169
&lt;/p&gt;
&lt;p&gt;
&#27492;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HALO&#30340;&#26041;&#27861;&#65292;&#23427;&#26159;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#21487;&#20197;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#25968;&#25454;&#65292;&#32780;&#36825;&#20123;&#25968;&#25454;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#65292;&#19988;&#26080;&#38656;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHRs)&#33021;&#22815;&#22312;&#26426;&#22120;&#23398;&#20064;(ML)&#21644;&#32479;&#35745;&#20998;&#26512;&#20013;&#20316;&#20026;&#30495;&#23454;EHRs&#30340;&#26367;&#20195;&#21697;&#65292;&#26082;&#30495;&#23454;&#21448;&#20445;&#25252;&#38544;&#31169;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39640;&#32500;&#25968;&#25454;&#30340;&#20869;&#22312;&#22797;&#26434;&#24615;&#65292;&#20197;&#20854;&#21407;&#22987;&#39640;&#24230;&#32500;&#24418;&#24335;&#29983;&#25104;&#39640;&#20445;&#30495;&#12289;&#32454;&#31890;&#24230;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHR)&#25968;&#25454;&#23545;&#29616;&#26377;&#26041;&#27861;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;Hierarchical Autoregressive Language mOdel (HALO)&#8221;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#29983;&#25104;&#32437;&#21521;&#39640;&#32500;EHR&#25968;&#25454;&#65292;&#35813;&#26041;&#27861;&#20445;&#30041;&#20102;&#30495;&#23454;EHR&#30340;&#32479;&#35745;&#29305;&#24615;&#65292;&#21487;&#20197;&#29992;&#20110;&#35757;&#32451;&#20934;&#30830;&#30340;ML&#27169;&#22411;&#32780;&#19981;&#28041;&#21450;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;HALO&#26041;&#27861;&#34987;&#35774;&#35745;&#20026;&#19968;&#20010;&#23618;&#32423;&#33258;&#22238;&#24402;&#27169;&#22411;&#65292;&#29983;&#25104;&#19968;&#32452;&#38024;&#23545;&#21307;&#23398;&#20195;&#30721;&#12289;&#20020;&#24202;&#23601;&#35786;&#21644;&#30149;&#20154;&#35760;&#24405;&#30340;&#27010;&#29575;&#23494;&#24230;&#20989;&#25968;&#65292;&#21487;&#20197;&#22312;&#20854;&#21407;&#22987;&#26410;&#32858;&#21512;&#24418;&#24335;&#19979;&#29983;&#25104;&#30495;&#23454;&#30340;EHR&#25968;&#25454;&#65292;&#26080;&#38656;&#36827;&#34892;&#21464;&#37327;&#36873;&#25321;&#25110;&#32858;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#36824;&#20135;&#29983;&#22823;&#37327;&#30340;&#38543;&#26426;&#26679;&#26412;&#65292;&#20197;&#25552;&#20379;&#22797;&#26434;&#24230;&#36739;&#20302;&#20294;&#20173;&#26377;&#24847;&#20041;&#30340;EHR&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic electronic health records (EHRs) that are both realistic and preserve privacy can serve as an alternative to real EHRs for machine learning (ML) modeling and statistical analysis. However, generating high-fidelity and granular electronic health record (EHR) data in its original, highly-dimensional form poses challenges for existing methods due to the complexities inherent in high-dimensional data. In this paper, we propose Hierarchical Autoregressive Language mOdel (HALO) for generating longitudinal high-dimensional EHR, which preserve the statistical properties of real EHR and can be used to train accurate ML models without privacy concerns. Our HALO method, designed as a hierarchical autoregressive model, generates a probability density function of medical codes, clinical visits, and patient records, allowing for the generation of realistic EHR data in its original, unaggregated form without the need for variable selection or aggregation. Additionally, our model also produc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ImprovisetoInitialize(I2I)&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#26469;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#36825;&#20351;&#24471;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2304.02168</link><description>&lt;p&gt;
I2I: &#29992;&#25913;&#36827;&#30340;&#30693;&#35782;&#21021;&#22987;&#21270;&#36716;&#25509;&#22120;
&lt;/p&gt;
&lt;p&gt;
I2I: Initializing Adapters with Improvised Knowledge. (arXiv:2304.02168v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;ImprovisetoInitialize(I2I)&#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#26469;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#36825;&#20351;&#24471;&#20174;&#19968;&#20010;&#20219;&#21153;&#21040;&#21478;&#19968;&#20010;&#20219;&#21153;&#30340;&#30693;&#35782;&#20256;&#36882;&#26356;&#21152;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36716;&#25509;&#22120;&#26159;&#24310;&#32493;&#23398;&#20064;&#20013;&#35299;&#20915;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#35757;&#32451;&#29420;&#31435;&#30340;&#36866;&#37197;&#22120;&#27169;&#22359;&#38169;&#22833;&#20102;&#36328;&#20219;&#21153;&#30693;&#35782;&#36716;&#31227;&#30340;&#26426;&#20250;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Improvise to Initialize (I2I) &#30340;&#36830;&#32493;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#25552;&#21462;&#20808;&#21069;&#23398;&#20064;&#30340;&#20219;&#21153;&#36866;&#37197;&#22120;&#30340;&#30693;&#35782;&#65292;&#20026;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#21021;&#22987;&#21270;&#36866;&#37197;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;&#35270;&#35273;&#38382;&#31572;&#20219;&#21153;&#24207;&#21015;&#36827;&#34892;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102; I2I &#22312; CLiMB&#65292;&#19968;&#20010;&#22810;&#27169;&#24577;&#30340;&#36830;&#32493;&#23398;&#20064;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;&#20351;&#29992; I2I &#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#22987;&#32456;&#27604;&#29420;&#31435;&#35757;&#32451;&#30340;&#36866;&#37197;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#20219;&#21153;&#31934;&#24230;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#20419;&#36827;&#20102;&#20219;&#21153;&#36866;&#37197;&#22120;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#65292;&#24182;&#19988;&#30456;&#23545;&#20110;&#20808;&#36827;&#30340; AdapterFusion&#65292;I2I &#20063;&#33021;&#23454;&#29616;&#26356;&#22909;&#30340;&#36328;&#20219;&#21153;&#30693;&#35782;&#36716;&#31227;&#32780;&#19981;&#20135;&#29983;&#30456;&#20851;&#30340;&#21442;&#25968;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adapters present a promising solution to the catastrophic forgetting problem in continual learning. However, training independent Adapter modules for every new task misses an opportunity for cross-task knowledge transfer. We propose Improvise to Initialize (I2I), a continual learning algorithm that initializes Adapters for incoming tasks by distilling knowledge from previously-learned tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning benchmark, by conducting experiments on sequences of visual question answering tasks. Adapters trained with I2I consistently achieve better task accuracy than independently-trained Adapters, demonstrating that our algorithm facilitates knowledge transfer between task Adapters. I2I also results in better cross-task knowledge transfer than the state-of-the-art AdapterFusion without incurring the associated parametric cost.
&lt;/p&gt;</description></item><item><title>GINA-3D&#26159;&#19968;&#31181;&#23398;&#20064;&#20174;&#30495;&#23454;&#22330;&#26223;&#20013;&#29983;&#25104;3D&#38544;&#24335;&#31070;&#32463;&#36164;&#20135;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#33258;&#20027;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#23398;&#20064;&#38382;&#39064;&#30340;&#27979;&#35797;&#21644;&#39564;&#35777;&#29615;&#22659;&#26159;&#37325;&#35201;&#30340;&#21019;&#26032;&#12290;</title><link>http://arxiv.org/abs/2304.02163</link><description>&lt;p&gt;
GINA-3D&#65306;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#23398;&#20064;&#29983;&#25104;&#38544;&#24335;&#31070;&#32463;&#36164;&#20135;
&lt;/p&gt;
&lt;p&gt;
GINA-3D: Learning to Generate Implicit Neural Assets in the Wild. (arXiv:2304.02163v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02163
&lt;/p&gt;
&lt;p&gt;
GINA-3D&#26159;&#19968;&#31181;&#23398;&#20064;&#20174;&#30495;&#23454;&#22330;&#26223;&#20013;&#29983;&#25104;3D&#38544;&#24335;&#31070;&#32463;&#36164;&#20135;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36825;&#23545;&#20110;&#33258;&#20027;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#23398;&#20064;&#38382;&#39064;&#30340;&#27979;&#35797;&#21644;&#39564;&#35777;&#29615;&#22659;&#26159;&#37325;&#35201;&#30340;&#21019;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#24314;&#27169;3D&#19990;&#30028;&#20197;&#36827;&#34892;&#20223;&#30495;&#26159;&#24320;&#21457;&#33258;&#21160;&#39550;&#39542;&#31561;&#26426;&#22120;&#20154;&#23398;&#20064;&#38382;&#39064;&#30340;&#27979;&#35797;&#21644;&#39564;&#35777;&#29615;&#22659;&#30340;&#21487;&#25193;&#23637;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#21019;&#24314;&#25110;&#37325;&#26032;&#21019;&#24314;&#31867;&#20284;&#30495;&#23454;&#19990;&#30028;&#30340;&#29615;&#22659;&#26159;&#22256;&#38590;&#65292;&#26114;&#36149;&#19988;&#19981;&#21487;&#25193;&#23637;&#30340;&#12290;&#26368;&#36817;&#30340;&#29983;&#25104;&#27169;&#22411;&#25216;&#26415;&#65292;&#36890;&#36807;&#20165;&#20351;&#29992;&#20016;&#23500;&#30340;2D&#22270;&#20687;&#26469;&#23398;&#20064;3D&#36164;&#20135;&#65292;&#24050;&#32463;&#26174;&#31034;&#20986;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#26377;&#24076;&#26395;&#30340;&#36827;&#23637; -- &#20294;&#20173;&#28982;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#22240;&#20026;&#23427;&#20204;&#21033;&#29992;&#20154;&#31867;&#31574;&#21010;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#25110;&#25163;&#21160;&#21019;&#24314;&#30340;&#21512;&#25104;3D&#29615;&#22659;&#30340;&#28210;&#26579;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;GINA-3D&#65292;&#36825;&#26159;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#26469;&#33258;&#30456;&#26426;&#21644;LiDAR&#20256;&#24863;&#22120;&#30340;&#30495;&#23454;&#39550;&#39542;&#25968;&#25454;&#21019;&#24314;&#30495;&#23454;3D&#38544;&#24335;&#31070;&#32463;&#36164;&#20135;&#65292;&#21253;&#25324;&#21508;&#31181;&#36710;&#36742;&#21644;&#34892;&#20154;&#12290;&#19982;&#29616;&#26377;&#30340;&#22270;&#20687;&#25968;&#25454;&#38598;&#30456;&#27604;&#65292;&#30495;&#23454;&#39550;&#39542;&#29615;&#22659;&#30001;&#20110;&#36974;&#25377;&#65292;&#20809;&#29031;&#21464;&#21270;&#21644;&#38271;&#23614;&#20998;&#24067;&#32780;&#38754;&#20020;&#26032;&#30340;&#25361;&#25112;&#12290;GINA-3D&#36890;&#36807;&#35299;&#32806;&#20174;&#21333;&#20010;&#35270;&#35282;&#29983;&#25104;3D&#36798;&#21040;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#30446;&#30340;&#65292;&#36827;&#32780;&#20351;3D&#22330;&#26223;&#30340;&#33258;&#21160;&#21270;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modeling the 3D world from sensor data for simulation is a scalable way of developing testing and validation environments for robotic learning problems such as autonomous driving. However, manually creating or re-creating real-world-like environments is difficult, expensive, and not scalable. Recent generative model techniques have shown promising progress to address such challenges by learning 3D assets using only plentiful 2D images -- but still suffer limitations as they leverage either human-curated image datasets or renderings from manually-created synthetic 3D environments. In this paper, we introduce GINA-3D, a generative model that uses real-world driving data from camera and LiDAR sensors to create realistic 3D implicit neural assets of diverse vehicles and pedestrians. Compared to the existing image datasets, the real-world driving setting poses new challenges due to occlusions, lighting-variations and long-tail distributions. GINA-3D tackles these challenges by decoupling re
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; GUTS &#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#24322;&#26500;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#37096;&#32626;&#65292;&#35299;&#20915;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#19979;&#30340;&#20027;&#21160;&#25628;&#32034;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#20256;&#24863;&#22120;&#19981;&#30830;&#23450;&#24615;&#12289;&#36974;&#25377;&#38382;&#39064;&#12289;&#24322;&#26500;&#25628;&#32034;&#22242;&#38431;&#21644;&#30828;&#20214;&#12289;&#36890;&#20449;&#25925;&#38556;&#30340;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#65292;&#24182;&#22312;&#20223;&#30495;&#20013;&#36229;&#36807;&#29616;&#26377;&#31639;&#27861;&#39640;&#36798;80%&#12290;</title><link>http://arxiv.org/abs/2304.02075</link><description>&lt;p&gt;
GUTS&#65306;&#22810;&#26234;&#33021;&#20307;&#20027;&#21160;&#25628;&#32034;&#30340;&#24191;&#20041;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693; Thompson Sampling&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
GUTS: Generalized Uncertainty-Aware Thompson Sampling for Multi-Agent Active Search. (arXiv:2304.02075v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; GUTS &#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#24322;&#26500;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#20013;&#37096;&#32626;&#65292;&#35299;&#20915;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#19979;&#30340;&#20027;&#21160;&#25628;&#32034;&#38382;&#39064;&#65292;&#32771;&#34385;&#20102;&#20256;&#24863;&#22120;&#19981;&#30830;&#23450;&#24615;&#12289;&#36974;&#25377;&#38382;&#39064;&#12289;&#24322;&#26500;&#25628;&#32034;&#22242;&#38431;&#21644;&#30828;&#20214;&#12289;&#36890;&#20449;&#25925;&#38556;&#30340;&#40065;&#26834;&#24615;&#31561;&#38382;&#39064;&#65292;&#24182;&#22312;&#20223;&#30495;&#20013;&#36229;&#36807;&#29616;&#26377;&#31639;&#27861;&#39640;&#36798;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#28798;&#38590;&#21709;&#24212;&#30340;&#26426;&#22120;&#20154;&#35299;&#20915;&#26041;&#26696;&#23545;&#30830;&#20445;&#26368;&#23567;&#29983;&#21629;&#25439;&#22833;&#33267;&#20851;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#24403;&#25628;&#32034;&#21306;&#22495;&#23545;&#20110;&#20154;&#31867;&#25937;&#25588;&#32773;&#32780;&#35328;&#36807;&#20110;&#21361;&#38505;&#25110;&#36807;&#20110;&#24191;&#38420;&#26102;&#12290;&#26412;&#25991;&#23558;&#36825;&#20010;&#38382;&#39064;&#24314;&#27169;&#20026;&#19968;&#20010;&#24322;&#27493;&#22810;&#26234;&#33021;&#20307;&#20027;&#21160;&#25628;&#32034;&#20219;&#21153;&#65292;&#22312;&#27492;&#20219;&#21153;&#20013;&#65292;&#27599;&#20010;&#26426;&#22120;&#20154;&#26088;&#22312;&#39640;&#25928;&#22320;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#23547;&#25214;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#65288;OOI&#65289;&#12290;&#36825;&#31181;&#34920;&#36848;&#35299;&#20915;&#20102;&#25628;&#32034;&#20219;&#21153;&#24212;&#35813;&#19987;&#27880;&#20110;&#24555;&#36895;&#24674;&#22797;OOI&#32780;&#19981;&#26159;&#23545;&#25628;&#32034;&#21306;&#22495;&#36827;&#34892;&#20840;&#35206;&#30422;&#30340;&#35201;&#27714;&#12290;&#20808;&#21069;&#30340;&#26041;&#27861;&#26410;&#33021;&#20934;&#30830;&#24314;&#27169;&#20256;&#24863;&#22120;&#19981;&#30830;&#23450;&#24615;&#65292;&#32771;&#34385;&#21040;&#30001;&#20110;&#26893;&#34987;&#25110;&#22320;&#24418;&#30340;&#36974;&#25377;&#32780;&#23548;&#33268;&#30340;&#36974;&#25377;&#38382;&#39064;&#65292;&#25110;&#32773;&#32771;&#34385;&#21040;&#24322;&#26500;&#25628;&#32034;&#22242;&#38431;&#21644;&#30828;&#20214;&#12289;&#36890;&#20449;&#25925;&#38556;&#30340;&#40065;&#26834;&#24615;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#24191;&#20041;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;Thompson&#25277;&#26679;&#65288;GUTS&#65289;&#31639;&#27861;&#65292;&#23427;&#35299;&#20915;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#24182;&#36866;&#29992;&#20110;&#22312;&#22823;&#22411;&#38750;&#32467;&#26500;&#21270;&#29615;&#22659;&#20013;&#37096;&#32626;&#24322;&#26500;&#22810;&#26426;&#22120;&#20154;&#31995;&#32479;&#36827;&#34892;&#20027;&#21160;&#25628;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#20223;&#30495;&#23454;&#39564;&#34920;&#26126;&#65292;GUTS&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#29616;&#26377;&#31639;&#27861;&#39640;&#36798;80%&#12290;
&lt;/p&gt;
&lt;p&gt;
Robotic solutions for quick disaster response are essential to ensure minimal loss of life, especially when the search area is too dangerous or too vast for human rescuers. We model this problem as an asynchronous multi-agent active-search task where each robot aims to efficiently seek objects of interest (OOIs) in an unknown environment. This formulation addresses the requirement that search missions should focus on quick recovery of OOIs rather than full coverage of the search region. Previous approaches fail to accurately model sensing uncertainty, account for occlusions due to foliage or terrain, or consider the requirement for heterogeneous search teams and robustness to hardware and communication failures. We present the Generalized Uncertainty-aware Thompson Sampling (GUTS) algorithm, which addresses these issues and is suitable for deployment on heterogeneous multi-robot systems for active search in large unstructured environments. We show through simulation experiments that GU
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#20307;&#30340;&#22810;&#27169;&#24577;&#38754;&#37096;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26381;&#35013;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#26102;&#23578;&#25554;&#22270;&#30340;&#33258;&#21160;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02051</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#26381;&#35013;&#35774;&#35745;&#24072;&#65306;&#38754;&#21521;&#20154;&#30340;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#29992;&#20110;&#26102;&#23578;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing. (arXiv:2304.02051v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#20307;&#30340;&#22810;&#27169;&#24577;&#38754;&#37096;&#22270;&#20687;&#32534;&#36753;&#26041;&#27861;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#26469;&#29983;&#25104;&#26381;&#35013;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#26102;&#23578;&#25554;&#22270;&#30340;&#33258;&#21160;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#23578;&#25554;&#22270;&#26159;&#35774;&#35745;&#24072;&#29992;&#26469;&#20256;&#36798;&#20182;&#20204;&#30340;&#35270;&#35273;&#21644;&#23558;&#35774;&#35745;&#29702;&#24565;&#20174;&#26500;&#24605;&#21040;&#23454;&#29616;&#65292;&#23637;&#31034;&#26381;&#35013;&#22914;&#20309;&#19982;&#20154;&#20307;&#20132;&#20114;&#30340;&#26041;&#24335;&#12290;&#22312;&#36825;&#20010;&#32972;&#26223;&#19979;&#65292;&#35745;&#31639;&#26426;&#35270;&#35273;&#21487;&#20197;&#29992;&#20110;&#25913;&#36827;&#26102;&#23578;&#35774;&#35745;&#36807;&#31243;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26550;&#26500;&#65292;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#22810;&#27169;&#24577;&#26465;&#20214;&#30340;&#26102;&#23578;&#22270;&#20687;&#32534;&#36753;&#20219;&#21153;&#65292;&#36890;&#36807;&#36319;&#38543;&#22810;&#27169;&#24577;&#25552;&#31034;&#65292;&#22914;&#25991;&#26412;&#12289;&#20154;&#20307;&#23039;&#21183;&#21644;&#26381;&#35013;&#33609;&#22270;&#65292;&#25351;&#23548;&#29983;&#25104;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26102;&#23578;&#22270;&#20687;&#12290;&#30001;&#20110;&#32570;&#20047;&#36866;&#21512;&#36825;&#39033;&#20219;&#21153;&#30340;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#23637;&#24320;&#20102;&#20004;&#20010;&#29616;&#26377;&#26102;&#23578;&#25968;&#25454;&#38598; Dress Code &#21644; VITON-HD&#65292;&#29992;&#21322;&#33258;&#21160;&#30340;&#26041;&#27861;&#34917;&#20805;&#20102;&#22810;&#27169;&#24577;&#27880;&#37322;&#12290;&#26032;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fashion illustration is used by designers to communicate their vision and to bring the design idea from conceptualization to realization, showing how clothes interact with the human body. In this context, computer vision can thus be used to improve the fashion design process. Differently from previous works that mainly focused on the virtual try-on of garments, we propose the task of multimodal-conditioned fashion image editing, guiding the generation of human-centric fashion images by following multimodal prompts, such as text, human body poses, and garment sketches. We tackle this problem by proposing a new architecture based on latent diffusion models, an approach that has not been used before in the fashion domain. Given the lack of existing datasets suitable for the task, we also extend two existing fashion datasets, namely Dress Code and VITON-HD, with multimodal annotations collected in a semi-automatic manner. Experimental results on these new datasets demonstrate the effective
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#28388;&#27874;&#30340;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#26410;&#35757;&#32451;&#36718;&#20013;&#21462;&#28040;&#23398;&#20064;&#32593;&#32476;&#30340;&#25152;&#26377;&#31867;&#21035;&#65292;&#24182;&#19988;&#24674;&#22797;&#21487;&#35299;&#37322;&#30340;&#31867;&#21035;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2304.02049</link><description>&lt;p&gt;
&#22522;&#20110;&#26435;&#37325;&#28388;&#27874;&#30340;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Multi-Class Explainable Unlearning for Image Classification via Weight Filtering. (arXiv:2304.02049v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02049
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#26435;&#37325;&#28388;&#27874;&#30340;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#22270;&#20687;&#20998;&#31867;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#26410;&#35757;&#32451;&#36718;&#20013;&#21462;&#28040;&#23398;&#20064;&#32593;&#32476;&#30340;&#25152;&#26377;&#31867;&#21035;&#65292;&#24182;&#19988;&#24674;&#22797;&#21487;&#35299;&#37322;&#30340;&#31867;&#21035;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#21368;&#36733;&#26159;&#26368;&#36817;&#28014;&#29616;&#30340;&#19968;&#31181;&#36873;&#25321;&#24615;&#22320;&#23558;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#24433;&#21709;&#20174;&#32593;&#32476;&#20013;&#21024;&#38500;&#30340;&#33539;&#24335;&#12290;&#23613;&#31649;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#38598;&#20013;&#22312;&#21368;&#36733;&#35757;&#32451;&#25968;&#25454;&#30340;&#23567;&#23376;&#38598;&#25110;&#21333;&#20010;&#31867;&#21035;&#65292;&#20294;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#21462;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#26410;&#35757;&#32451;&#36718;&#20013;&#21462;&#28040;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#30340;&#25152;&#26377;&#31867;&#21035;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36890;&#36807;&#20869;&#37096;&#32452;&#20214;&#30340;&#35760;&#24518;&#30697;&#38453;&#26469;&#35843;&#33410;&#22270;&#20687;&#20998;&#31867;&#32593;&#32476;&#65292;&#20197;&#20415;&#22312;&#35757;&#32451;&#21518;&#65292;&#21516;&#19968;&#32593;&#32476;&#21487;&#20197;&#26377;&#36873;&#25321;&#22320;&#23637;&#31034;&#20219;&#20309;&#31867;&#21035;&#30340;&#26410;&#23398;&#20064;&#34892;&#20026;&#12290;&#36890;&#36807;&#21457;&#29616;&#27599;&#20010;&#31867;&#21035;&#29305;&#23450;&#30340;&#26435;&#37325;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36824;&#36890;&#36807;&#35774;&#35745;&#21487;&#35299;&#37322;&#24615;&#26426;&#21046;&#26469;&#24674;&#22797;&#31867;&#21035;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;&#23567;&#35268;&#27169;&#21644;&#20013;&#35268;&#27169;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;CNN&#21644;Transformer-based&#39592;&#26550;&#27979;&#35797;&#20102;&#25552;&#20986;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25552;&#20379;&#20102;&#19968;&#31181;&#22810;&#31867;&#21487;&#35299;&#37322;&#24615;&#21368;&#36733;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine Unlearning has recently been emerging as a paradigm for selectively removing the impact of training datapoints from a network. While existing approaches have focused on unlearning either a small subset of the training data or a single class, in this paper we take a different path and devise a framework that can unlearn all classes of an image classification network in a single untraining round. Our proposed technique learns to modulate the inner components of an image classification network through memory matrices so that, after training, the same network can selectively exhibit an unlearning behavior over any of the classes. By discovering weights which are specific to each of the classes, our approach also recovers a representation of the classes which is explainable by-design. We test the proposed framework, which we name Weight Filtering network (WF-Net), on small-scale and medium-scale image classification datasets, with both CNN and Transformer-based backbones. Our work p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#26415;&#25968;&#25454;&#38598; MATH 401 &#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#24182;&#20855;&#20307;&#27979;&#35797;&#20102; GPT-4&#12289;ChatGPT&#12289;InstrctGPT&#12289;Galactica &#21644; LLaMA &#31561;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.02015</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31639;&#26415;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
How well do Large Language Models perform in Arithmetic tasks?. (arXiv:2304.02015v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02015
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#26415;&#25968;&#25454;&#38598; MATH 401 &#20197;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#65292;&#24182;&#20855;&#20307;&#27979;&#35797;&#20102; GPT-4&#12289;ChatGPT&#12289;InstrctGPT&#12289;Galactica &#21644; LLaMA &#31561;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#20855;&#22791;&#20102;&#36830;&#36143;&#24605;&#36335;&#30340;&#33021;&#21147;&#65292;&#33021;&#22815;&#36880;&#27493;&#35299;&#31572;&#25968;&#23398;&#38382;&#39064;&#12290;&#35299;&#20915;&#25968;&#23398;&#38382;&#39064;&#19981;&#20165;&#38656;&#35201;&#36890;&#36807;&#24605;&#32500;&#36830;&#36143;&#30340;&#33021;&#21147;&#20998;&#35299;&#38382;&#39064;&#65292;&#36824;&#38656;&#35201;&#38024;&#23545;&#27599;&#20010;&#27493;&#39588;&#27491;&#30830;&#35745;&#31639;&#31639;&#26415;&#34920;&#36798;&#24335;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36824;&#27809;&#26377;&#20219;&#20309;&#30740;&#31350;&#19987;&#38376;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35745;&#31639;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31639;&#26415;&#25968;&#25454;&#38598; MATH 401&#65292;&#29992;&#20110;&#27979;&#35797;&#26368;&#26032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#21253;&#25324; GPT-4&#12289;ChatGPT&#12289;InstrctGPT&#12289;Galactica &#21644; LLaMA&#65292;&#20854;&#20013;&#28041;&#21450;&#21508;&#31181;&#31639;&#26415;&#34920;&#36798;&#24335;&#65292;&#24182;&#25552;&#20379;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#30340;&#35814;&#32454;&#20998;&#26512;&#12290;MATH 401 &#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#20195;&#30721;&#24050;&#22312; \url{https://github.com/GanjinZero/math401-llm} &#19978;&#21457;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have emerged abilities including chain-of-thought to answer math word problems step by step. Solving math word problems not only requires abilities to disassemble problems via chain-of-thought but also needs to calculate arithmetic expressions correctly for each step. To the best of our knowledge, there is no work to focus on evaluating the arithmetic ability of large language models. In this work, we propose an arithmetic dataset MATH 401 to test the latest large language models including GPT-4, ChatGPT, InstrctGPT, Galactica, and LLaMA with various arithmetic expressions and provide a detailed analysis of the ability of large language models. MATH 401 and evaluation codes are released at \url{https://github.com/GanjinZero/math401-llm}.
&lt;/p&gt;</description></item><item><title>HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01890</link><description>&lt;p&gt;
&#31038;&#20250;&#25991;&#21270;&#30693;&#35782;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#23545;&#36873;&#39033;&#30340;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01890
&lt;/p&gt;
&lt;p&gt;
HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;HATELEXICON&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#30340;&#34065;&#31216;&#21644;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#30340;&#35789;&#27719;&#34920;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#22914;&#20309;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#65292;&#34920;&#26126;&#21457;&#23637;&#29992;&#20110;&#20998;&#31867;&#26497;&#31471;&#35328;&#35770;&#30340;&#27169;&#22411;&#65292;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#20005;&#37325;&#20381;&#36182;&#30446;&#26631;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;HATELEXICON&#26469;&#36741;&#21161;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#36873;&#39033;&#30340;&#26041;&#27861;&#65292;&#36873;&#39033;&#36873;&#25321;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;HASOC&#25968;&#25454;&#23545;&#24503;&#35821;&#21644;&#21360;&#22320;&#35821;&#36827;&#34892;&#20102;&#20960;&#20010;&#31034;&#33539;&#23398;&#20064;&#65292;&#24182;&#23558;Multilingual HateCheck&#65288;MHC&#65289;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#36873;&#25321;&#26679;&#26412;&#65292;&#30456;&#23545;&#20110;&#38543;&#26426;&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;MHC&#19978;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#24403;&#20165;&#26377;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#26102;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#26469;&#36873;&#25321;&#21253;&#21547;&#26356;&#22810;&#31038;&#20250;&#25991;&#21270;&#20449;&#24687;&#30340;&#26679;&#26412;&#33021;&#22815;&#26356;&#22909;&#22320;&#25552;&#39640;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#25490;&#24207;&#26041;&#27861;&#26469;&#35299;&#20915;&#21333;&#27169;&#31946;&#22270;&#20687;&#21040;&#35270;&#39057;&#24207;&#21015;&#30340;&#37325;&#24314;&#38382;&#39064;&#65292;&#26174;&#24335;&#22320;&#20998;&#37197;&#27599;&#20010;&#24207;&#21015;&#30340;&#39034;&#24207;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#21435;&#27169;&#31946;&#27169;&#22411;&#30340;&#35757;&#32451;&#36136;&#37327;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.01686</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#25490;&#24207;&#30340;&#21333;&#27169;&#31946;&#22270;&#20687;&#21040;&#35270;&#39057;&#24207;&#21015;&#26041;&#27861;&#8212;&#8212;HyperCUT
&lt;/p&gt;
&lt;p&gt;
HyperCUT: Video Sequence from a Single Blurry Image using Unsupervised Ordering. (arXiv:2304.01686v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#25490;&#24207;&#26041;&#27861;&#26469;&#35299;&#20915;&#21333;&#27169;&#31946;&#22270;&#20687;&#21040;&#35270;&#39057;&#24207;&#21015;&#30340;&#37325;&#24314;&#38382;&#39064;&#65292;&#26174;&#24335;&#22320;&#20998;&#37197;&#27599;&#20010;&#24207;&#21015;&#30340;&#39034;&#24207;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#21435;&#27169;&#31946;&#27169;&#22411;&#30340;&#35757;&#32451;&#36136;&#37327;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#21040;&#35270;&#39057;&#21435;&#27169;&#31946;&#30340;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#30001;&#20110;&#21069;&#21521;&#21644;&#21518;&#21521;&#24207;&#21015;&#37117;&#21487;&#20197;&#20316;&#20026;&#27169;&#31946;&#22270;&#20687;&#23545;&#24212;&#30340;&#28165;&#26224;&#22270;&#20687;&#24207;&#21015;&#65292;&#22240;&#27492;&#24207;&#21015;&#39034;&#24207;&#30340;&#27169;&#31946;&#24615;&#26497;&#22823;&#22320;&#24178;&#25200;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#25490;&#24207;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#35270;&#39057;&#24207;&#21015;&#26144;&#23556;&#21040;&#19968;&#20010;&#39640;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#36229;&#24179;&#38754;&#23558;&#20854;&#19982;&#20854;&#21453;&#21521;&#24207;&#21015;&#21306;&#20998;&#24320;&#26469;&#65292;&#26174;&#24335;&#22320;&#20998;&#37197;&#20102;&#27599;&#20010;&#24207;&#21015;&#30340;&#39034;&#24207;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#21435;&#27169;&#31946;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26368;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the challenging task of training models for image-to-video deblurring, which aims to recover a sequence of sharp images corresponding to a given blurry image input. A critical issue disturbing the training of an image-to-video model is the ambiguity of the frame ordering since both the forward and backward sequences are plausible solutions. This paper proposes an effective self-supervised ordering scheme that allows training high-quality image-to-video deblurring models. Unlike previous methods that rely on order-invariant losses, we assign an explicit order for each video sequence, thus avoiding the order-ambiguity issue. Specifically, we map each video sequence to a vector in a latent high-dimensional space so that there exists a hyperplane such that for every video sequence, the vectors extracted from it and its reversed sequence are on different sides of the hyperplane. The side of the vectors will be used to define the order of the corresponding sequence. Last but not 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#20854;&#20182;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01487</link><description>&lt;p&gt;
&#32842;&#22825;GPT&#65292;&#36824;&#26159;&#19981;&#32842;&#22825;GPT&#65306;&#36825;&#26159;&#19968;&#20010;&#38382;&#39064;&#65281;
&lt;/p&gt;
&lt;p&gt;
To ChatGPT, or not to ChatGPT: That is the question!. (arXiv:2304.01487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01487
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#20854;&#20182;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;GPT&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20840;&#29699;&#24863;&#30693;&#12290;&#38543;&#30528;&#32842;&#22825;GPT&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#20182;&#20204;&#30340;&#35823;&#29992;&#30340;&#25285;&#24551;&#20063;&#22686;&#21152;&#20102;&#65292;&#20363;&#22914;&#20256;&#25773;&#34394;&#20551;&#28040;&#24687;&#65292;&#25220;&#34989;&#65292;&#25805;&#32437;&#20844;&#20247;&#33286;&#35770;&#65292;&#27450;&#39575;&#21644;&#27450;&#35784;&#12290;&#22240;&#27492;&#65292;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#65292;&#20174;&#22522;&#26412;&#30340;&#20108;&#20803;&#20998;&#31867;&#22120;&#21040;&#26356;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#19968;&#20123;&#26816;&#27979;&#25216;&#26415;&#20381;&#36182;&#20110;&#32479;&#35745;&#29305;&#24449;&#25110;&#21477;&#27861;&#27169;&#24335;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#21253;&#21547;&#35821;&#20041;&#25110;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#26368;&#26032;&#25216;&#26415;&#36827;&#34892;&#20840;&#38754;&#21644;&#29616;&#20195;&#21270;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#20854;&#20182;&#26410;&#19987;&#38376;&#22768;&#31216;&#26816;&#27979;&#32842;&#22825;GPT&#29983;&#25104;&#20869;&#23481;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#26816;&#27979;&#32842;&#22825;GPT&#29983;&#25104;&#20869;&#23481;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#24037;&#32534;&#20889;&#21644;&#32842;&#22825;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has become a global sensation. As ChatGPT and other Large Language Models (LLMs) emerge, concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud. Hence, distinguishing AI-generated from human-generated becomes increasingly essential. Researchers have proposed various detection methodologies, ranging from basic binary classifiers to more complex deep-learning models. Some detection techniques rely on statistical characteristics or syntactic patterns, while others incorporate semantic or contextual information to improve accuracy. The primary objective of this study is to provide a comprehensive and contemporary assessment of the most recent techniques in ChatGPT detection. Additionally, we evaluated other AI-generated text detection tools that do not specifically claim to detect ChatGPT-generated content to assess their performance in detecting ChatGPT-generated content. For our evaluation,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#32463;&#36807;&#25913;&#36827;&#21644;&#24494;&#35843;&#30340;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01238</link><description>&lt;p&gt;
Spam-T5&#65306;&#22522;&#20110;&#23567;&#26679;&#26412;&#30340;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection. (arXiv:2304.01238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#32463;&#36807;&#25913;&#36827;&#21644;&#24494;&#35843;&#30340;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;BERT-like&#12289;Sentence Transformers&#21644;Seq2Seq&#65289;&#20197;&#21450;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;LightGBM&#65289;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65288;&#23436;&#25972;&#35757;&#32451;&#38598;&#21644;&#23567;&#26679;&#26412;&#65289;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290; &#21457;&#29616;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;LLMs&#20248;&#20110;&#22522;&#32447;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#23567;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#36825;&#31181;&#36866;&#24212;&#24615;&#20351;LLMs&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#65292;&#22240;&#20026;&#26631;&#35760;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;&#24182;&#19988;&#27169;&#22411;&#38656;&#35201;&#32463;&#24120;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#19987;&#38376;&#20026;&#26816;&#27979;&#30005;&#23376;&#37038;&#20214;&#22403;&#22334;&#32780;&#36827;&#34892;&#20102;&#25913;&#36827;&#21644;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Spam-T5&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the effectiveness of large language models (LLMs) in email spam detection by comparing prominent models from three distinct families: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we examine well-established machine learning techniques for spam detection, such as Na\"ive Bayes and LightGBM, as baseline methods. We assess the performance of these models across four public datasets, utilizing different numbers of training samples (full training set and few-shot settings). Our findings reveal that, in the majority of cases, LLMs surpass the performance of the popular baseline techniques, particularly in few-shot scenarios. This adaptability renders LLMs uniquely suited to spam detection tasks, where labeled samples are limited in number and models require frequent updates. Additionally, we introduce Spam-T5, a Flan-T5 model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results demonstrate that Spam-T5 
&lt;/p&gt;</description></item><item><title>POLAR-Express &#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23427;&#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#21644;&#36880;&#23618;&#20256;&#25773;&#25216;&#26415;&#65292;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01218</link><description>&lt;p&gt;
POLAR-Express: &#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#39640;&#25928;&#20934;&#30830;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
POLAR-Express: Efficient and Precise Formal Reachability Analysis of Neural-Network Controlled Systems. (arXiv:2304.01218v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01218
&lt;/p&gt;
&lt;p&gt;
POLAR-Express &#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23427;&#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#21644;&#36880;&#23618;&#20256;&#25773;&#25216;&#26415;&#65292;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#38382;&#39064;&#19978;&#65292;&#25198;&#28436;&#25511;&#21046;&#22120;&#35282;&#33394;&#30340;&#31070;&#32463;&#32593;&#32476; (NN) &#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23454;&#39564;&#24615;&#33021;&#12290;&#20294;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479; (NNCS) &#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#37319;&#29992;&#20063;&#24341;&#36215;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#23545;&#36825;&#20123; NNCS &#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; POLAR-Express&#65292;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777; NNCS &#30340;&#23433;&#20840;&#24615;&#12290;POLAR-Express &#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#65292;&#36880;&#23618;&#27178;&#36328;&#31070;&#32463;&#32593;&#32476;&#26469;&#20256;&#25773; Taylor &#27169;&#22411; (TM) &#20197;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#30340;&#36817;&#20284;&#20540;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#20219;&#20309;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#26356;&#26377;&#25928;&#22320;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;POLAR-Express &#20026;&#36880;&#23618;&#20256;&#25773;&#25552;&#20379;&#20102;&#24182;&#34892;&#35745;&#31639;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) playing the role of controllers have demonstrated impressive empirical performances on challenging control problems. However, the potential adoption of NN controllers in real-life applications also gives rise to a growing concern over the safety of these neural-network controlled systems (NNCSs), especially when used in safety-critical applications. In this work, we present POLAR-Express, an efficient and precise formal reachability analysis tool for verifying the safety of NNCSs. POLAR-Express uses Taylor model arithmetic to propagate Taylor models (TMs) across a neural network layer-by-layer to compute an overapproximation of the neural-network function. It can be applied to analyze any feed-forward neural network with continuous activation functions. We also present a novel approach to propagate TMs more efficiently and precisely across ReLU activation functions. In addition, POLAR-Express provides parallel computation support for the layer-by-layer propagation
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#20196;&#24341;&#23548;&#38899;&#39057;&#32534;&#36753;&#27169;&#22411;AUDIT&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#38899;&#39057;&#32534;&#36753;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#26041;&#26696;&#21644;&#25991;&#26412;&#21040;&#29255;&#27573;&#21305;&#37197;&#27169;&#22359;&#35299;&#20915;&#20102;&#20808;&#21069;&#25193;&#25955;-based&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.00830</link><description>&lt;p&gt;
AUDIT&#65306;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#20196;&#24341;&#23548;&#38899;&#39057;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models. (arXiv:2304.00830v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00830
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#20196;&#24341;&#23548;&#38899;&#39057;&#32534;&#36753;&#27169;&#22411;AUDIT&#65292;&#23454;&#29616;&#20102;&#22312;&#21508;&#31181;&#38899;&#39057;&#32534;&#36753;&#20219;&#21153;&#19978;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#65292;&#24182;&#36890;&#36807;&#33258;&#36866;&#24212;&#26041;&#26696;&#21644;&#25991;&#26412;&#21040;&#29255;&#27573;&#21305;&#37197;&#27169;&#22359;&#35299;&#20915;&#20102;&#20808;&#21069;&#25193;&#25955;-based&#26041;&#27861;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#39057;&#32534;&#36753;&#21487;&#29992;&#20110;&#21508;&#31181;&#30446;&#30340;&#65292;&#20363;&#22914;&#28155;&#21152;&#32972;&#26223;&#38899;&#25928;&#12289;&#26367;&#25442;&#20048;&#22120;&#21644;&#20462;&#22797;&#25439;&#22351;&#30340;&#38899;&#39057;&#12290;&#26368;&#36817;&#65292;&#19968;&#20123;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#20197;&#36755;&#20986;&#38899;&#39057;&#30340;&#25991;&#26412;&#35828;&#26126;&#20026;&#26465;&#20214;&#30340;&#25193;&#25955;&#21644;&#21435;&#22122;&#36807;&#31243;&#26469;&#23454;&#29616;&#38646;-shot&#38899;&#39057;&#32534;&#36753;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20173;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65306;1&#65289;&#23427;&#20204;&#24182;&#27809;&#26377;&#34987;&#35757;&#32451;&#29992;&#20110;&#32534;&#36753;&#20219;&#21153;&#65292;&#19981;&#33021;&#20445;&#35777;&#33391;&#22909;&#30340;&#32534;&#36753;&#25928;&#26524;&#65307;2&#65289;&#23427;&#20204;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#20462;&#25913;&#19981;&#38656;&#35201;&#32534;&#36753;&#30340;&#38899;&#39057;&#29255;&#27573;&#65307;3&#65289;&#20182;&#20204;&#38656;&#35201;&#36755;&#20986;&#38899;&#39057;&#30340;&#23436;&#25972;&#25551;&#36848;&#65292;&#36825;&#22312;&#23454;&#38469;&#24773;&#20917;&#19979;&#24182;&#19981;&#24635;&#26159;&#21487;&#29992;&#25110;&#24517;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AUDIT&#65292;&#19968;&#31181;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;&#25351;&#20196;&#24341;&#23548;&#38899;&#39057;&#32534;&#36753;&#27169;&#22411;&#12290;&#29305;&#21035;&#26159;&#65292;AUDIT&#20855;&#26377;&#19977;&#20010;&#20027;&#35201;&#35774;&#35745;&#29305;&#28857;&#65306;1&#65289;&#25105;&#20204;&#20026;&#19981;&#21516;&#30340;&#38899;&#39057;&#32534;&#36753;&#20219;&#21153;&#26500;&#24314;&#19977;&#20803;&#35757;&#32451;&#25968;&#25454;&#65288;&#25351;&#20196;&#65292;&#36755;&#20837;&#38899;&#39057;&#65292;&#36755;&#20986;&#38899;&#39057;&#65289;&#24182;&#20351;&#29992;&#25351;&#20196;&#21644;&#36755;&#20837;&#65288;&#35201;&#32534;&#36753;&#30340;&#38899;&#39057;&#65289;&#38899;&#39057;&#23545;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#65307;2&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#33258;&#36866;&#24212;&#26041;&#26696;&#65292;&#20351;&#29992;&#24456;&#23569;&#37327;&#30340;&#32534;&#36753;&#20219;&#21153;&#38899;&#39057;&#25968;&#25454;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65307;3&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25991;&#26412;&#21040;&#29255;&#27573;&#21305;&#37197;&#27169;&#22359;&#65292;&#33258;&#21160;&#23558;&#36755;&#20837;&#25351;&#20196;&#19982;&#35201;&#32534;&#36753;&#30340;&#30456;&#24212;&#38899;&#39057;&#29255;&#27573;&#23545;&#40784;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AUDIT&#22312;&#21508;&#31181;&#38899;&#39057;&#32534;&#36753;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#27604;&#20197;&#21069;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#34920;&#29616;&#26356;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;
Audio editing is applicable for various purposes, such as adding background sound effects, replacing a musical instrument, and repairing damaged audio. Recently, some diffusion-based methods achieved zero-shot audio editing by using a diffusion and denoising process conditioned on the text description of the output audio. However, these methods still have some problems: 1) they have not been trained on editing tasks and cannot ensure good editing effects; 2) they can erroneously modify audio segments that do not require editing; 3) they need a complete description of the output audio, which is not always available or necessary in practical scenarios. In this work, we propose AUDIT, an instruction-guided audio editing model based on latent diffusion models. Specifically, AUDIT has three main design features: 1) we construct triplet training data (instruction, input audio, output audio) for different audio editing tasks and train a diffusion model using instruction and input (to be edite
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#21307;&#30103;&#20803;&#23431;&#23449;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#20171;&#32461;&#20102;&#32852;&#37030;&#23398;&#20064;&#12289;&#29289;&#32852;&#32593;&#20013;&#30340;&#21307;&#30103;&#31995;&#32479;&#21644;&#20803;&#23431;&#23449;&#21307;&#30103;&#30340;&#22522;&#30784;&#19978;&#65292;&#35752;&#35770;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#20803;&#23431;&#23449;&#21307;&#30103;&#20013;&#30340;&#20248;&#28857;&#21644;&#25361;&#25112;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#38382;&#39064;&#20063;&#34987;&#30830;&#23450;&#12290;</title><link>http://arxiv.org/abs/2304.00524</link><description>&lt;p&gt;
&#22522;&#20110;&#32852;&#37030;&#23398;&#20064;&#30340;&#21307;&#30103;&#20803;&#23431;&#23449;&#30740;&#31350;&#32508;&#36848;&#65306;&#27010;&#24565;&#12289;&#24212;&#29992;&#12289;&#25361;&#25112;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
A Survey on Federated Learning for the Healthcare Metaverse: Concepts, Applications, Challenges, and Future Directions. (arXiv:2304.00524v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#21307;&#30103;&#20803;&#23431;&#23449;&#20013;&#30340;&#24212;&#29992;&#12290;&#22312;&#20171;&#32461;&#20102;&#32852;&#37030;&#23398;&#20064;&#12289;&#29289;&#32852;&#32593;&#20013;&#30340;&#21307;&#30103;&#31995;&#32479;&#21644;&#20803;&#23431;&#23449;&#21307;&#30103;&#30340;&#22522;&#30784;&#19978;&#65292;&#35752;&#35770;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#20803;&#23431;&#23449;&#21307;&#30103;&#20013;&#30340;&#20248;&#28857;&#21644;&#25361;&#25112;&#12290;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#21644;&#38382;&#39064;&#20063;&#34987;&#30830;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#25216;&#26415;&#30340;&#36827;&#27493;&#22823;&#22823;&#25913;&#21892;&#20102;&#21307;&#30103;&#31995;&#32479;&#65292;&#25552;&#20379;&#20102;&#21508;&#31181;&#26234;&#33021;&#21307;&#30103;&#26381;&#21153;&#65292;&#25552;&#39640;&#20102;&#29983;&#27963;&#36136;&#37327;&#12290;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#20316;&#20026;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#26032;&#20998;&#25903;&#65292;&#20026;&#22788;&#29702;&#21307;&#30103;&#31995;&#32479;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#24182;&#21033;&#29992;&#20998;&#24067;&#24335;&#35774;&#22791;&#19978;&#30340;&#25968;&#25454;&#21644;&#35745;&#31639;&#36164;&#28304;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;&#27492;&#22806;&#65292;&#20803;&#23431;&#23449;&#36890;&#36807;&#25972;&#21512;&#20154;&#24037;&#26234;&#33021;&#12289;&#20113;&#36793;&#32536;&#35745;&#31639;&#12289;&#29289;&#32852;&#32593;&#65288;IoT&#65289;&#12289;&#21306;&#22359;&#38142;&#21644;&#35821;&#20041;&#36890;&#20449;&#31561;&#26032;&#20852;&#25216;&#26415;&#65292;&#24050;&#32463;&#22312;&#19968;&#33324;&#21644;&#29305;&#21035;&#26159;&#22312;&#21307;&#30103;&#34892;&#19994;&#20013;&#65292;&#25913;&#21464;&#20102;&#35768;&#22810;&#22402;&#30452;&#39046;&#22495;&#12290;&#26174;&#28982;&#65292;FL&#23637;&#31034;&#20102;&#35768;&#22810;&#20248;&#28857;&#65292;&#24182;&#20026;&#20256;&#32479;&#30340;&#21644;&#20803;&#23431;&#23449;&#30340;&#21307;&#30103;&#31995;&#32479;&#25552;&#20379;&#20102;&#26032;&#30340;&#26426;&#20250;&#65292;&#36825;&#20419;&#20351;&#25105;&#20204;&#23545;FL&#22312;&#20803;&#23431;&#23449;&#21307;&#30103;&#31995;&#32479;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#35843;&#26597;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22522;&#20110;&#29289;&#32852;&#32593;&#30340;&#21307;&#30103;&#31995;&#32479;&#12289;&#20256;&#32479;&#21307;&#30103;&#39046;&#22495;&#20013;&#30340;FL&#20197;&#21450;&#20803;&#23431;&#23449;&#21307;&#30103;&#12290;&#28982;&#21518;&#65292;&#35752;&#35770;&#20102;FL&#22312;&#20803;&#23431;&#23449;&#21307;&#30103;&#20013;&#30340;&#24212;&#29992;&#21450;&#20854;&#20248;&#28857;&#12290;&#38543;&#21518;&#65292;&#31361;&#20986;&#20102;&#20803;&#23431;&#23449;&#21307;&#30103;&#20013;FL&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#21644;&#38382;&#39064;&#12290;&#26368;&#21518;&#65292;&#30830;&#23450;&#20102;&#26410;&#26469;&#30340;&#26041;&#21521;&#21644;&#30740;&#31350;&#38382;&#39064;&#65292;&#20197;&#25351;&#23548;&#21644;&#21551;&#21457;&#23545;FL&#22312;&#21307;&#30103;&#20803;&#23431;&#23449;&#20013;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent technological advancements have considerately improved healthcare systems to provide various intelligent healthcare services and improve the quality of life. Federated learning (FL), a new branch of artificial intelligence (AI), opens opportunities to deal with privacy issues in healthcare systems and exploit data and computing resources available at distributed devices. Additionally, the Metaverse, through integrating emerging technologies, such as AI, cloud edge computing, Internet of Things (IoT), blockchain, and semantic communications, has transformed many vertical domains in general and the healthcare sector in particular. Obviously, FL shows many benefits and provides new opportunities for conventional and Metaverse healthcare, motivating us to provide a survey on the usage of FL for Metaverse healthcare systems. First, we present preliminaries to IoT-based healthcare systems, FL in conventional healthcare, and Metaverse healthcare. The benefits of FL in Metaverse healthc
&lt;/p&gt;</description></item><item><title>DAMO-StreamNet&#26159;&#19968;&#20010;&#20248;&#21270;&#33258;&#21160;&#39550;&#39542;&#20013;&#27969;&#24335;&#24863;&#30693;&#30340;&#26694;&#26550;&#65292;&#23427;&#34701;&#21512;&#20102;YOLO&#31995;&#21015;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#36890;&#36807;&#39048;&#37096;&#32467;&#26500;&#12289;&#21452;&#20998;&#25903;&#32467;&#26500;&#12289;&#33976;&#39311;&#26426;&#21046;&#21644;&#23454;&#26102;&#39044;&#27979;&#26426;&#21046;&#31561;&#20851;&#38190;&#21019;&#26032;&#28857;&#65292;&#25552;&#20379;&#20102;&#23574;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2303.17144</link><description>&lt;p&gt;
DAMO-StreamNet&#65306;&#33258;&#21160;&#39550;&#39542;&#20013;&#27969;&#24335;&#24863;&#30693;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
DAMO-StreamNet: Optimizing Streaming Perception in Autonomous Driving. (arXiv:2303.17144v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17144
&lt;/p&gt;
&lt;p&gt;
DAMO-StreamNet&#26159;&#19968;&#20010;&#20248;&#21270;&#33258;&#21160;&#39550;&#39542;&#20013;&#27969;&#24335;&#24863;&#30693;&#30340;&#26694;&#26550;&#65292;&#23427;&#34701;&#21512;&#20102;YOLO&#31995;&#21015;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#36890;&#36807;&#39048;&#37096;&#32467;&#26500;&#12289;&#21452;&#20998;&#25903;&#32467;&#26500;&#12289;&#33976;&#39311;&#26426;&#21046;&#21644;&#23454;&#26102;&#39044;&#27979;&#26426;&#21046;&#31561;&#20851;&#38190;&#21019;&#26032;&#28857;&#65292;&#25552;&#20379;&#20102;&#23574;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#26102;&#24863;&#30693;&#65292;&#25110;&#32773;&#35828;&#27969;&#24335;&#24863;&#30693;&#65292;&#26159;&#33258;&#21160;&#39550;&#39542;&#20013;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#20294;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#30740;&#31350;&#30340;&#26041;&#38754;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DAMO-StreamNet&#65292;&#23427;&#23558;YOLO&#31995;&#21015;&#30340;&#26368;&#26032;&#36827;&#23637;&#19982;&#31354;&#38388;&#21644;&#26102;&#38388;&#24863;&#30693;&#26426;&#21046;&#30340;&#20840;&#38754;&#20998;&#26512;&#30456;&#32467;&#21512;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#23574;&#31471;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;DAMO-StreamNet&#30340;&#20851;&#38190;&#21019;&#26032;&#28857;&#21253;&#25324;&#65306;(1)&#19968;&#20010;&#40065;&#26834;&#30340;&#39048;&#37096;&#32467;&#26500;&#65292;&#34701;&#21512;&#20102;&#21487;&#21464;&#24418;&#21367;&#31215;&#65292;&#22686;&#24378;&#20102;&#24863;&#21463;&#37326;&#21644;&#29305;&#24449;&#23545;&#40784;&#33021;&#21147;&#12290;(2)&#19968;&#20010;&#21452;&#20998;&#25903;&#32467;&#26500;&#65292;&#25972;&#21512;&#20102;&#30701;&#36890;&#36947;&#35821;&#20041;&#29305;&#24449;&#21644;&#38271;&#36890;&#36947;&#26102;&#24207;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#36816;&#21160;&#29366;&#24577;&#39044;&#27979;&#30340;&#31934;&#24230;&#12290;(3)&#19968;&#20010;&#22312;logits&#32423;&#21035;&#19978;&#36827;&#34892;&#30340;&#33976;&#39311;&#26426;&#21046;&#65292;&#23545;&#40784;&#25945;&#24072;&#32593;&#32476;&#21644;&#23398;&#29983;&#32593;&#32476;&#30340;&#35821;&#20041;&#31354;&#38388;&#12290;(4)&#19968;&#20010;&#23454;&#26102;&#39044;&#27979;&#26426;&#21046;&#65292;&#26356;&#26032;&#25903;&#25345;&#24103;&#30340;&#29305;&#24449;&#19982;&#24403;&#21069;&#24103;&#65292;&#30830;&#20445;&#25512;&#29702;&#36807;&#31243;&#20013;&#30340;&#26080;&#32541;&#27969;&#24335;&#24863;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time perception, or streaming perception, is a crucial aspect of autonomous driving that has yet to be thoroughly explored in existing research. To address this gap, we present DAMO-StreamNet, an optimized framework that combines recent advances from the YOLO series with a comprehensive analysis of spatial and temporal perception mechanisms, delivering a cutting-edge solution. The key innovations of DAMO-StreamNet are: (1) A robust neck structure incorporating deformable convolution, enhancing the receptive field and feature alignment capabilities. (2) A dual-branch structure that integrates short-path semantic features and long-path temporal features, improving motion state prediction accuracy. (3) Logits-level distillation for efficient optimization, aligning the logits of teacher and student networks in semantic space. (4) A real-time forecasting mechanism that updates support frame features with the current frame, ensuring seamless streaming perception during inference. Our ex
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.15747</link><description>&lt;p&gt;
TabRet: &#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65292;&#25903;&#25345;&#26410;&#30693;&#21015;
&lt;/p&gt;
&lt;p&gt;
TabRet: Pre-training Transformer-based Tabular Models for Unseen Columns. (arXiv:2303.15747v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15747
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#39044;&#35757;&#32451;&#30340;Transformer-based&#34920;&#26684;&#27169;&#22411;&#65306;TabRet&#65292;&#33021;&#22815;&#25903;&#25345;&#26410;&#30693;&#21015;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#34920;&#29616;&#20248;&#31168;&#12290;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TabRet&#30340;&#21487;&#39044;&#35757;&#32451;Transformer-based&#34920;&#26684;&#27169;&#22411;&#12290;TabRet&#26088;&#22312;&#20026;&#21253;&#21547;&#26410;&#22312;&#39044;&#35757;&#32451;&#20013;&#35265;&#36807;&#30340;&#21015;&#30340;&#19979;&#28216;&#20219;&#21153;&#25552;&#20379;&#25903;&#25345;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;TabRet&#22312;&#24494;&#35843;&#20043;&#21069;&#26377;&#19968;&#20010;&#39069;&#22806;&#30340;&#23398;&#20064;&#27493;&#39588;&#65292;&#31216;&#20026;&#37325;&#26032;&#26631;&#35760;&#21270;&#65292;&#23427;&#22522;&#20110;&#36974;&#34109;&#33258;&#21160;&#32534;&#30721;&#25439;&#22833;&#26469;&#26657;&#20934;&#29305;&#24449;&#23884;&#20837;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#22823;&#37327;&#30340;&#20844;&#20849;&#20581;&#24247;&#35843;&#26597;&#25968;&#25454;&#23545;TabRet&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#22312;&#21307;&#30103;&#20445;&#20581;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#65292;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;AUC&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#28040;&#34701;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#26399;&#38388;&#36827;&#34892;&#37325;&#26032;&#26631;&#35760;&#21270;&#21644;&#38543;&#26426;&#27927;&#29260;&#22686;&#24378;&#23545;&#24615;&#33021;&#25552;&#21319;&#26377;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present \emph{TabRet}, a pre-trainable Transformer-based model for tabular data. TabRet is designed to work on a downstream task that contains columns not seen in pre-training. Unlike other methods, TabRet has an extra learning step before fine-tuning called \emph{retokenizing}, which calibrates feature embeddings based on the masked autoencoding loss. In experiments, we pre-trained TabRet with a large collection of public health surveys and fine-tuned it on classification tasks in healthcare, and TabRet achieved the best AUC performance on four datasets. In addition, an ablation study shows retokenizing and random shuffle augmentation of columns during pre-training contributed to performance gains.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#24207;&#21644;&#38750;&#26102;&#24207;&#25968;&#25454;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#35782;&#21035;&#22240;&#26524;&#36793;&#32536;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#21487;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24120;&#35265;&#25351;&#26631;&#12290;</title><link>http://arxiv.org/abs/2303.15027</link><description>&lt;p&gt;
&#26102;&#24207;&#21644;&#38750;&#26102;&#24207;&#25968;&#25454;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Causal Discovery Methods for Temporal and Non-Temporal Data. (arXiv:2303.15027v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26102;&#24207;&#21644;&#38750;&#26102;&#24207;&#25968;&#25454;&#22240;&#26524;&#21457;&#29616;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#19981;&#21516;&#31639;&#27861;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#35782;&#21035;&#22240;&#26524;&#36793;&#32536;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#36824;&#20171;&#32461;&#20102;&#21487;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#24120;&#35265;&#25351;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#65288;CD&#65289;&#26159;&#20174;&#25968;&#25454;&#20013;&#35782;&#21035;&#31995;&#32479;&#21464;&#37327;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#36807;&#31243;&#12290;&#22810;&#24180;&#26469;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#20960;&#31181;&#26041;&#27861;&#65292;&#20027;&#35201;&#22522;&#20110;&#25968;&#25454;&#30340;&#32479;&#35745;&#29305;&#24615;&#26469;&#25581;&#31034;&#28508;&#22312;&#30340;&#22240;&#26524;&#26426;&#21046;&#12290;&#26412;&#25991;&#23545;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#25968;&#25454;&#21644;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#30340;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#36827;&#34892;&#20102;&#24191;&#27867;&#25506;&#35752;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#20102;&#22240;&#26524;&#21457;&#29616;&#20013;&#30340;&#24120;&#29992;&#26415;&#35821;&#65292;&#28982;&#21518;&#20840;&#38754;&#35752;&#35770;&#20102;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#35782;&#21035;&#22240;&#26524;&#36793;&#32536;&#30340;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#21487;&#29992;&#20110;&#35780;&#20272;&#22240;&#26524;&#21457;&#29616;&#26041;&#27861;&#24615;&#33021;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21487;&#29992;&#20110;&#25191;&#34892;&#22240;&#26524;&#21457;&#29616;&#30340;&#24037;&#20855;&#25110;&#36719;&#20214;&#21253;&#20197;&#21450;&#35780;&#20272;&#36825;&#20123;&#26041;&#27861;&#30340;&#24120;&#35265;&#25351;&#26631;&#12290;&#25105;&#20204;&#36824;&#22312;&#19981;&#21516;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#19968;&#20123;&#24120;&#35265;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery (CD) is the process of identifying the cause-effect relationships among the variables of a system from data. Over the years, several methods have been developed primarily based on the statistical properties of data to uncover the underlying causal mechanism. In this study, we present an extensive discussion on the methods designed to perform causal discovery from both independent and identically distributed (i.i.d.) data and time series data. For this purpose, we first introduce the common terminologies in causal discovery, and then provide a comprehensive discussion of the algorithms designed to identify the causal edges in different settings. We further discuss some of the benchmark datasets available for evaluating the performance of the causal discovery methods, available tools or software packages to perform causal discovery readily, and the common metrics used to evaluate these methods. We also test some common causal discovery algorithms on different benchmark d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;</title><link>http://arxiv.org/abs/2303.11413</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#25391;&#21160;&#20449;&#21495;&#21435;&#22122;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Vibration Signal Denoising Using Deep Learning. (arXiv:2303.11413v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11413
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#33050;&#27493;&#24341;&#36215;&#30340;&#32467;&#26500;&#25391;&#21160;&#20449;&#21495;&#34987;&#24191;&#27867;&#29992;&#20110;&#20154;&#21592;&#35782;&#21035;&#12289;&#23450;&#20301;&#12289;&#20154;&#31867;&#27963;&#21160;&#25512;&#26029;&#12289;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#31561;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#29615;&#22659;&#22122;&#22768;&#12289;&#30005;&#30913;&#24178;&#25200;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#23454;&#38469;&#37319;&#38598;&#30340;&#20449;&#21495;&#36890;&#24120;&#20250;&#24102;&#26377;&#22122;&#22768;&#12290;&#22122;&#22768;&#30340;&#23384;&#22312;&#24433;&#21709;&#20102;&#20449;&#21495;&#22788;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#24433;&#21709;&#20102;&#26368;&#32456;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#21644;&#35823;&#24046;&#12290;&#26412;&#25991;&#20027;&#35201;&#25506;&#35752;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21435;&#38500;&#33050;&#27493;&#24341;&#36215;&#30340;&#25391;&#21160;&#20449;&#21495;&#30340;&#22122;&#22768;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#22122;&#22768;&#65292;&#21253;&#25324;&#39640;&#26031;&#22122;&#22768;&#21644;&#38750;&#24179;&#31283;&#22122;&#22768;&#31561;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure vibration signals induced by footsteps are widely used for tasks like occupant identification, localization, human activity inference, structure health monitoring and so on. The vibration signals are collected as time series with amplitude values. However, the collected signals are always noisy in practice due to the influence of environmental noise, electromagnetic interference and other factors. The presence of noise affects the process of signal analysis, thus affecting the accuracy and error of the final tasks. In this paper, we mainly explore the denoising methods for footstep-induced vibration signals. We have considered different kinds of noise including stationary noises such as gaussian noises and non-stationary noises such as item-dropping vibration noise and music noises.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#39640;&#24230;&#20010;&#24615;&#21270;&#25991;&#26412;&#23884;&#20837;&#26469;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;&#65292;&#21487;&#20197;&#36816;&#29992;&#20110;&#22270;&#20687;&#30340;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#21160;&#20316;&#30340;&#32534;&#36753;&#65292;&#19981;&#38656;&#35201;&#22810;&#20010;&#21442;&#32771;&#22270;&#20687;&#25110;&#22797;&#26434;&#35757;&#32451;&#65292;&#33021;&#23454;&#29616;&#22797;&#26434;&#35821;&#20041;&#22270;&#20687;&#32534;&#36753;&#12290;</title><link>http://arxiv.org/abs/2303.08767</link><description>&lt;p&gt;
&#22522;&#20110;&#31283;&#23450;&#25193;&#25955;&#30340;&#39640;&#24230;&#20010;&#24615;&#21270;&#25991;&#26412;&#23884;&#20837;&#22270;&#20687;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Highly Personalized Text Embedding for Image Manipulation by Stable Diffusion. (arXiv:2303.08767v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#20294;&#39640;&#25928;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#8212;&#8212;&#20351;&#29992;&#39640;&#24230;&#20010;&#24615;&#21270;&#25991;&#26412;&#23884;&#20837;&#26469;&#36827;&#34892;&#22270;&#20687;&#25805;&#20316;&#65292;&#21487;&#20197;&#36816;&#29992;&#20110;&#22270;&#20687;&#30340;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#21160;&#20316;&#30340;&#32534;&#36753;&#65292;&#19981;&#38656;&#35201;&#22810;&#20010;&#21442;&#32771;&#22270;&#20687;&#25110;&#22797;&#26434;&#35757;&#32451;&#65292;&#33021;&#23454;&#29616;&#22797;&#26434;&#35821;&#20041;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31283;&#23450;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#23637;&#29616;&#20986;&#22312;&#22270;&#20687;&#29983;&#25104;&#21644;&#25805;&#20316;&#26041;&#38754;&#30340;&#21331;&#36234;&#24615;&#33021;&#65292;&#20294;&#20869;&#37096;&#38543;&#26426;&#24615;&#24102;&#26469;&#30340;&#25361;&#25112;&#22312;&#20110;&#22914;&#20309;&#20445;&#30041;&#21644;&#25805;&#20316;&#22270;&#20687;&#20869;&#23481;&#21644;&#36523;&#20221;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#26041;&#27861;&#22914;&#8220;&#26790;&#22659;&#30456;&#26426;&#8221;&#21644;&#8220;&#25991;&#26412;&#21453;&#36716;&#8221;&#25552;&#20986;&#20102;&#20351;&#29992;&#27169;&#22411;&#25110;&#28508;&#22312;&#34920;&#31034;&#20010;&#24615;&#21270;&#26469;&#20445;&#25345;&#20869;&#23481;&#65292;&#20294;&#23427;&#20204;&#23545;&#22810;&#20010;&#21442;&#32771;&#22270;&#20687;&#21644;&#22797;&#26434;&#35757;&#32451;&#30340;&#20381;&#36182;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#21448;&#38750;&#24120;&#26377;&#25928;&#30340;&#20010;&#24615;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#39640;&#24230;&#20010;&#24615;&#21270;&#65288;HiPer&#65289;&#25991;&#26412;&#23884;&#20837;&#36890;&#36807;&#20998;&#35299;CLIP&#23884;&#20837;&#31354;&#38388;&#23454;&#29616;&#20010;&#24615;&#21270;&#21644;&#20869;&#23481;&#25805;&#20316;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#27169;&#22411;&#24494;&#35843;&#25110;&#35782;&#21035;&#31526;&#65292;&#20294;&#20173;&#21487;&#20197;&#20165;&#36890;&#36807;&#21333;&#20010;&#22270;&#20687;&#21644;&#30446;&#26631;&#25991;&#26412;&#26469;&#23454;&#29616;&#32972;&#26223;&#12289;&#32441;&#29702;&#21644;&#21160;&#20316;&#30340;&#25805;&#20316;&#12290;&#36890;&#36807;&#23545;&#22810;&#26679;&#21270;&#30340;&#30446;&#26631;&#25991;&#26412;&#30340;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#37117;&#33021;&#20135;&#29983;&#39640;&#24230;&#20010;&#24615;&#21270;&#21644;&#22797;&#26434;&#30340;&#35821;&#20041;&#22270;&#20687;&#32534;&#36753;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models have shown superior performance in image generation and manipulation, but the inherent stochasticity presents challenges in preserving and manipulating image content and identity. While previous approaches like DreamBooth and Textual Inversion have proposed model or latent representation personalization to maintain the content, their reliance on multiple reference images and complex training limits their practicality. In this paper, we present a simple yet highly effective approach to personalization using highly personalized (HiPer) text embedding by decomposing the CLIP embedding space for personalization and content manipulation. Our method does not require model fine-tuning or identifiers, yet still enables manipulation of background, texture, and motion with just a single image and target text. Through experiments on diverse target texts, we demonstrate that our approach produces highly personalized and complex semantic image edits across a wide range of tasks. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#31579;&#36873;&#65292;&#20165;&#20351;&#29992;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2303.06241</link><description>&lt;p&gt;
&#23545;&#25239;&#35757;&#32451;&#26159;&#21542;&#38656;&#35201;&#20351;&#29992;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do we need entire training data for adversarial training?. (arXiv:2303.06241v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06241
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#35757;&#32451;&#25968;&#25454;&#38598;&#36827;&#34892;&#31579;&#36873;&#65292;&#20165;&#20351;&#29992;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#20943;&#23569;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new adversarial training method that reduces training time by selecting only the adversarially-prone samples from the training dataset.
&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#34987;&#29992;&#20110;&#35299;&#20915;&#35768;&#22810;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21644;&#21307;&#23398;&#22270;&#20687;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#12290;DNN&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#24050;&#32463;&#34987;&#24191;&#27867;&#20851;&#27880;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#36890;&#36807;&#23545;&#25239;&#35757;&#32451;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#20960;&#20046;&#25152;&#26377;&#30340;&#26041;&#27861;&#37117;&#20250;&#20026;&#25972;&#20010;&#35757;&#32451;&#25968;&#25454;&#38598;&#29983;&#25104;&#23545;&#25239;&#24615;&#31034;&#20363;&#65292;&#20174;&#32780;&#22823;&#22823;&#22686;&#21152;&#20102;&#35757;&#32451;&#26102;&#38388;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#20165;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#30340;&#23376;&#38598;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#21487;&#20197;&#20943;&#23569;&#20219;&#20309;&#23545;&#25239;&#35757;&#32451;&#31639;&#27861;&#30340;&#35757;&#32451;&#26102;&#38388;&#12290;&#20026;&#20102;&#36873;&#25321;&#23376;&#38598;&#65292;&#25105;&#20204;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#36807;&#28388;&#20986;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#26679;&#26412;&#12290;&#25105;&#20204;&#23545;&#25152;&#26377;&#35757;&#32451;&#26679;&#26412;&#25191;&#34892;&#31616;&#21333;&#30340;&#23545;&#25239;&#25915;&#20987;&#65292;&#20197;&#36807;&#28388;&#20986;&#36825;&#20010;&#23376;&#38598;&#12290;&#22312;&#36825;&#20010;&#25915;&#20987;&#20013;&#65292;&#25105;&#20204;&#21521;&#27599;&#20010;&#20687;&#32032;&#28155;&#21152;&#19968;&#20010;&#23567;&#25200;&#21160;&#21644;&#20960;&#26465;&#32593;&#26684;&#32447;&#21040;&#36755;&#20837;&#22270;&#20687;&#20013;&#12290;&#25105;&#20204;&#23545;&#26131;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#23376;&#38598;&#36827;&#34892;&#23545;&#25239;&#35757;&#32451;&#65292;&#24182;&#19988;...
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks (DNNs) are being used to solve a wide range of problems in many domains including safety-critical domains like self-driving cars and medical imagery. DNNs suffer from vulnerability against adversarial attacks. In the past few years, numerous approaches have been proposed to tackle this problem by training networks using adversarial training. Almost all the approaches generate adversarial examples for the entire training dataset, thus increasing the training time drastically. We show that we can decrease the training time for any adversarial training algorithm by using only a subset of training data for adversarial training. To select the subset, we filter the adversarially-prone samples from the training data. We perform a simple adversarial attack on all training examples to filter this subset. In this attack, we add a small perturbation to each pixel and a few grid lines to the input image.  We perform adversarial training on the adversarially-prone subset and mi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;XAI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#25351;&#26631;&#37492;&#21035;in-distribution&#21644;out of-distribution&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#38750;&#21442;&#25968;&#32479;&#35745;&#27169;&#22411;&#30340;OoD&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22797;&#26434;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#26816;&#27979;&#20934;&#30830;&#24230;&#21644;&#35780;&#20272;&#35757;&#32451;&#21644;&#25805;&#20316;&#26465;&#20214;&#30340;&#25509;&#36817;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.01860</link><description>&lt;p&gt;
&#22522;&#20110;&#35268;&#21017;&#30340;OoD&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Rule-based Out-Of-Distribution Detection. (arXiv:2303.01860v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;XAI&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#19981;&#21516;&#25351;&#26631;&#37492;&#21035;in-distribution&#21644;out of-distribution&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#38750;&#21442;&#25968;&#32479;&#35745;&#27169;&#22411;&#30340;OoD&#26816;&#27979;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#22797;&#26434;&#22330;&#26223;&#19979;&#34920;&#29616;&#20986;&#33391;&#22909;&#30340;&#26816;&#27979;&#20934;&#30830;&#24230;&#21644;&#35780;&#20272;&#35757;&#32451;&#21644;&#25805;&#20316;&#26465;&#20214;&#30340;&#25509;&#36817;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#25968;&#25454;&#38598;&#20013;&#26159;&#21542;&#23384;&#22312;OoD&#26159;&#37096;&#32626;&#26426;&#22120;&#23398;&#20064;&#30340;&#26368;&#20851;&#38190;&#38382;&#39064;&#20043;&#19968;&#12290;&#26412;&#25991;&#37319;&#29992;XAI&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#36890;&#36807;&#32771;&#34385;&#19981;&#21516;&#30340;&#25351;&#26631;&#37492;&#21035;&#20986;in-distribution&#21644;out of-distribution&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#35813;&#26041;&#27861;&#26159;&#38750;&#21442;&#25968;&#32479;&#35745;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20998;&#24067;&#20551;&#35774;&#12290;&#22312;&#22797;&#26434;&#24212;&#29992;&#22330;&#26223;&#19979;&#65288;&#22914;&#39044;&#27979;&#24615;&#32500;&#25252;&#12289;&#36710;&#38431;&#31649;&#21046;&#12289;&#32593;&#32476;&#23433;&#20840;&#36861;&#36394;&#31561;&#65289;&#30340;&#39564;&#35777;&#26174;&#31034;&#20102;&#35813;&#26041;&#27861;&#22312;&#26816;&#27979;&#20934;&#30830;&#24230;&#21644;&#35780;&#20272;&#35757;&#32451;&#21644;&#25805;&#20316;&#26465;&#20214;&#30340;&#25509;&#36817;&#31243;&#24230;&#26041;&#38754;&#30340;&#20248;&#33391;&#24615;&#33021;&#12290;&#32467;&#26524;&#21487;&#22312;&#20197;&#19979;&#38142;&#25509;&#20013;&#30340;github&#19978;&#33719;&#24471;: https://github.com/giacomo97cnr/Rule-based-ODD&#12290;
&lt;/p&gt;
&lt;p&gt;
Out-of-distribution detection is one of the most critical issue in the deployment of machine learning. The data analyst must assure that data in operation should be compliant with the training phase as well as understand if the environment has changed in a way that autonomous decisions would not be safe anymore. The method of the paper is based on eXplainable Artificial Intelligence (XAI); it takes into account different metrics to identify any resemblance between in-distribution and out of, as seen by the XAI model. The approach is non-parametric and distributional assumption free. The validation over complex scenarios (predictive maintenance, vehicle platooning, covert channels in cybersecurity) corroborates both precision in detection and evaluation of training-operation conditions proximity. Results are available via open source and open data at the following link: https://github.com/giacomo97cnr/Rule-based-ODD.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#24863;&#30693;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;&#23618;&#27425;&#21270;&#27880;&#24847;&#21147;&#32593;&#32476;&#12289;&#22806;&#37096;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.12126</link><description>&lt;p&gt;
KHAN&#65306;&#22522;&#20110;&#30693;&#35782;&#30340;&#23618;&#27425;&#21270;&#27880;&#24847;&#21147;&#32593;&#32476;&#29992;&#20110;&#20934;&#30830;&#30340;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
KHAN: Knowledge-Aware Hierarchical Attention Networks for Accurate Political Stance Prediction. (arXiv:2302.12126v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#24863;&#30693;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#26041;&#27861;&#65292;&#37319;&#29992;&#23618;&#27425;&#21270;&#27880;&#24847;&#21147;&#32593;&#32476;&#12289;&#22806;&#37096;&#30693;&#35782;&#24211;&#21644;&#30693;&#35782;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26032;&#38395;&#25991;&#31456;&#30340;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20197;&#20943;&#36731;&#22238;&#22768;&#23460;&#25928;&#24212;&#65292;&#21363;&#20154;&#20204;&#33853;&#20837;&#20854;&#24605;&#24819;&#65292;&#24378;&#21270;&#20854;&#29616;&#26377;&#20449;&#24565;&#12290;&#20197;&#24448;&#20851;&#20110;&#25919;&#27835;&#31435;&#22330;&#38382;&#39064;&#30340;&#30740;&#31350;&#37325;&#28857;&#22312;&#20110;&#65288;1&#65289;&#35782;&#21035;&#21487;&#20197;&#21453;&#26144;&#26032;&#38395;&#25991;&#31456;&#25919;&#27835;&#31435;&#22330;&#30340;&#25919;&#27835;&#22240;&#32032;&#21644;&#65288;2&#65289;&#26377;&#25928;&#22320;&#25429;&#25417;&#36825;&#20123;&#22240;&#32032;&#12290;&#23613;&#31649;&#23427;&#20204;&#22312;&#32463;&#39564;&#19978;&#25104;&#21151;&#20102;&#65292;&#20294;&#22312;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#20013;&#20854;&#35782;&#21035;&#30340;&#22240;&#32032;&#30340;&#26377;&#25928;&#24615;&#27809;&#26377;&#24471;&#21040;&#20805;&#20998;&#35777;&#26126;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#36890;&#36807;&#29992;&#25143;&#30740;&#31350;&#35843;&#26597;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#20013;&#30340;&#37325;&#35201;&#22240;&#32032;&#65292;&#24182;&#35266;&#23519;&#21040;&#26032;&#38395;&#25991;&#31456;&#30340;&#29615;&#22659;&#21644;&#35821;&#35843;&#65288;&#38544;&#21547;&#65289;&#20197;&#21450;&#25991;&#31456;&#20013;&#28041;&#21450;&#30340;&#29616;&#23454;&#23454;&#20307;&#30340;&#22806;&#37096;&#30693;&#35782;&#65288;&#26174;&#24335;&#65289;&#22312;&#30830;&#23450;&#20854;&#25919;&#27835;&#31435;&#22330;&#26041;&#38754;&#26159;&#37325;&#35201;&#30340;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#30693;&#35782;&#24863;&#30693;&#25919;&#27835;&#31435;&#22330;&#39044;&#27979;&#26041;&#27861;&#65288;KHAN&#65289;&#65292;&#37319;&#29992;&#65288;1&#65289;&#23618;&#27425;&#21270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26377;&#25928;&#22320;&#25429;&#25417;&#26032;&#38395;&#25991;&#31456;&#20013;&#30340;&#20851;&#38190;&#20449;&#24687;&#65292;&#65288;2&#65289;&#22806;&#37096;&#30693;&#35782;&#24211;&#25552;&#20379;&#20851;&#20110;&#25991;&#31456;&#20013;&#25552;&#21040;&#30340;&#23454;&#20307;&#30340;&#39069;&#22806;&#20449;&#24687;&#65292;&#20197;&#21450;&#65288;3&#65289;&#30693;&#35782;&#24863;&#30693;&#25439;&#22833;&#20989;&#25968;&#23398;&#20064;&#20248;&#20808;&#32771;&#34385;&#37325;&#35201;&#20449;&#24687;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The political stance prediction for news articles has been widely studied to mitigate the echo chamber effect -- people fall into their thoughts and reinforce their pre-existing beliefs. The previous works for the political stance problem focus on (1) identifying political factors that could reflect the political stance of a news article and (2) capturing those factors effectively. Despite their empirical successes, they are not sufficiently justified in terms of how effective their identified factors are in the political stance prediction. Motivated by this, in this work, we conduct a user study to investigate important factors in political stance prediction, and observe that the context and tone of a news article (implicit) and external knowledge for real-world entities appearing in the article (explicit) are important in determining its political stance. Based on this observation, we propose a novel knowledge-aware approach to political stance prediction (KHAN), employing (1) hierar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35770;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21033;&#29992;&#25299;&#25169;&#23398;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#22312;16&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#20248;&#20110;&#25110;&#21305;&#37197;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.09543</link><description>&lt;p&gt;
&#22522;&#20110;&#25299;&#25169;&#23398;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65306;&#19968;&#31181;&#22522;&#20110;&#22270;&#35770;&#30340;&#36807;&#28388;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Topological Feature Selection: A Graph-Based Filter Feature Selection Approach. (arXiv:2302.09543v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09543
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#35770;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;&#65292;&#21033;&#29992;&#25299;&#25169;&#23398;&#21644;&#20381;&#36182;&#20851;&#31995;&#65292;&#20855;&#26377;&#39640;&#24230;&#28789;&#27963;&#24615;&#21644;&#35299;&#37322;&#24615;&#65292;&#22312;16&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#31034;&#20986;&#20248;&#20110;&#25110;&#21305;&#37197;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#26080;&#30417;&#30563;&#30340;&#22522;&#20110;&#22270;&#35770;&#36807;&#28388;&#26041;&#27861;&#36827;&#34892;&#29305;&#24449;&#36873;&#25321;&#30340;&#25216;&#26415;&#65292;&#21033;&#29992;&#20102;&#25299;&#25169;&#32422;&#26463;&#32593;&#32476;&#34920;&#31034;&#30340;&#23041;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#24358;&#22270;&#65288;&#19977;&#35282;&#26368;&#22823;&#36807;&#28388;&#22270;&#65289;&#27169;&#25311;&#29305;&#24449;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#30740;&#31350;&#29305;&#24449;&#22312;&#32593;&#32476;&#20869;&#30340;&#30456;&#23545;&#20301;&#32622;&#26469;&#26368;&#22823;&#21270;&#29305;&#24449;&#30456;&#20851;&#24615;&#30340;&#21487;&#33021;&#24615;&#12290;&#36825;&#31181;&#26041;&#27861;&#30456;&#23545;&#20110;&#20854;&#20182;&#26041;&#27861;&#26377;&#19977;&#20010;&#29305;&#28857;&#65306;&#65288;i&#65289;&#39640;&#24230;&#21487;&#35843;&#65292;&#26131;&#20110;&#36866;&#24212;&#36755;&#20837;&#25968;&#25454;&#30340;&#24615;&#36136;&#65307;&#65288;ii&#65289;&#23436;&#20840;&#21487;&#35299;&#37322;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#26174;&#33879;&#30340;&#31616;&#21333;&#24615;&#65307;&#65288;iii&#65289;&#35745;&#31639;&#25104;&#26412;&#27604;&#20854;&#26367;&#20195;&#26041;&#26696;&#26356;&#21152;&#20415;&#23452;&#12290;&#25105;&#20204;&#22312;&#26469;&#33258;&#19981;&#21516;&#24212;&#29992;&#39046;&#22495;&#30340;16&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#24322;&#26500;&#35780;&#20272;&#26465;&#20214;&#19979;&#65292;&#23427;&#20248;&#20110;&#25110;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#25216;&#26415;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce a novel unsupervised, graph-based filter feature selection technique which exploits the power of topologically constrained network representations. We model dependency structures among features using a family of chordal graphs (the Triangulated Maximally Filtered Graph), and we maximise the likelihood of features' relevance by studying their relative position inside the network. Such an approach presents three aspects that are particularly satisfactory compared to its alternatives: (i) it is highly tunable and easily adaptable to the nature of input data; (ii) it is fully explainable, maintaining, at the same time, a remarkable level of simplicity; (iii) it is computationally cheaper compared to its alternatives. We test our algorithm on 16 benchmark datasets from different applicative domains showing that it outperforms or matches the current state-of-the-art under heterogeneous evaluation conditions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#25968;&#25454;&#30340;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;MUlti-modal Generator (MUG)&#12290;&#22312;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#26159;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;3.4%&#21644;2.2%&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2301.07088</link><description>&lt;p&gt;
&#35270;&#35273;&#23398;&#20064;&#32773;&#36935;&#35265;Web&#22270;&#20687;-&#25991;&#26412;&#23545;
&lt;/p&gt;
&lt;p&gt;
Vision Learners Meet Web Image-Text Pairs. (arXiv:2301.07088v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07088
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32593;&#32476;&#25968;&#25454;&#30340;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#26041;&#27861;MUlti-modal Generator (MUG)&#12290;&#22312;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#34920;&#29616;&#65292;&#26159;&#20043;&#21069;&#26368;&#20339;&#32467;&#26524;&#30340;3.4%&#21644;2.2%&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26368;&#26032;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#37117;&#26159;&#22312;&#32500;&#25252;&#33391;&#22909;&#30340;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#32771;&#34385;&#21040;&#32593;&#32476;&#25968;&#25454;&#30340;&#20986;&#33394;&#21487;&#20280;&#32553;&#24615;&#65292;&#25105;&#20204;&#35748;&#20026;&#33258;&#25105;&#30417;&#30563;&#39044;&#35757;&#32451;&#24212;&#35813;&#22522;&#20110;&#22024;&#26434;&#30340;&#32593;&#32476;&#28304;&#22270;&#25991;&#37197;&#23545;&#25968;&#25454;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#22914;&#27492;&#35774;&#32622;&#19979;&#65292;&#23545;&#22823;&#35268;&#27169;&#32593;&#32476;&#25968;&#25454;&#19978;&#30340;&#20195;&#34920;&#24615;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#30740;&#31350;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19968;&#31995;&#21015;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#34987;&#23631;&#34109;&#30340;&#35757;&#32451;&#30446;&#26631;&#30340;&#21333;&#27169;&#24335;&#26041;&#27861;&#21644;&#20351;&#29992;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#35757;&#32451;&#30340;&#22810;&#27169;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#22312;&#35270;&#35273;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#24182;&#19981;&#27604;&#21333;&#27169;&#24577;&#26041;&#27861;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#35270;&#35282;&#26469;&#35299;&#37322;&#36825;&#20123;&#22522;&#20934;&#32467;&#26524;&#65292;&#36825;&#25552;&#20379;&#20102;&#22914;&#20309;&#35774;&#35745;&#26032;&#22411;&#35270;&#35273;&#23398;&#20064;&#32773;&#30340;&#35265;&#35299;&#12290;&#21463;&#21040;&#36825;&#20123;&#35265;&#35299;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35270;&#35273;&#34920;&#31034;&#39044;&#35757;&#32451;&#26041;&#27861;&#8212;&#8212;&#22810;&#27169;&#24335;&#29983;&#25104;&#22120;&#65288;MUG&#65289;&#65292;&#23427;&#20174;&#21487;&#20280;&#32553;&#30340;&#32593;&#32476;&#28304;&#22270;&#25991;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;MUG&#22312;&#20960;&#20010;&#35270;&#35273;&#25968;&#25454;&#38598;&#30340;&#36716;&#31227;&#23398;&#20064;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#22312;CIFAR-10&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#30340;&#32467;&#26524;3.4&#65285;&#65292;&#22312;STL-10&#19978;&#20248;&#20110;&#20043;&#21069;&#26368;&#20339;&#30340;&#32467;&#26524;2.2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most recent self-supervised learning methods are pre-trained on the well-curated ImageNet-1K dataset. In this work, given the excellent scalability of web data, we consider self-supervised pre-training on noisy web sourced image-text paired data. First, we conduct a benchmark study of representative self-supervised pre-training methods on large-scale web data in a like-for-like setting. We compare a range of methods, including single-modal ones that use masked training objectives and multi-modal ones that use image-text constrastive training. We observe that existing multi-modal methods do not outperform their single-modal counterparts on vision transfer learning tasks. We derive an information-theoretical view to explain these benchmark results, which provides insight into how to design a novel vision learner. Inspired by this insight, we present a new visual representation pre-training method, MUlti-modal Generator~(MUG), that learns from scalable web sourced image-text data. MUG ach
&lt;/p&gt;</description></item><item><title>PIVOT &#26159;&#19968;&#31181;&#38024;&#23545;&#35270;&#39057;&#25968;&#25454;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#20197;&#21450;&#30456;&#24212;&#30340;&#36951;&#24536;&#65292;&#24182;&#19988;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#26080;&#22495;&#20869;&#39044;&#35757;&#32451;&#24773;&#20917;&#19979;&#26377;&#25928;&#20351;&#29992;&#25552;&#31034;&#26426;&#21046;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.04842</link><description>&lt;p&gt;
PIVOT&#65306;&#22522;&#20110;&#25552;&#31034;&#30340;&#35270;&#39057;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PIVOT: Prompting for Video Continual Learning. (arXiv:2212.04842v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04842
&lt;/p&gt;
&lt;p&gt;
PIVOT &#26159;&#19968;&#31181;&#38024;&#23545;&#35270;&#39057;&#25968;&#25454;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#20197;&#21450;&#30456;&#24212;&#30340;&#36951;&#24536;&#65292;&#24182;&#19988;&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#22312;&#26080;&#22495;&#20869;&#39044;&#35757;&#32451;&#24773;&#20917;&#19979;&#26377;&#25928;&#20351;&#29992;&#25552;&#31034;&#26426;&#21046;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#30001;&#20110;&#25968;&#25454;&#21487;&#29992;&#24615;&#12289;&#23384;&#20648;&#37197;&#39069;&#12289;&#38544;&#31169;&#35268;&#23450;&#21644;&#26114;&#36149;&#30340;&#27880;&#37322;&#36807;&#31243;&#32780;&#21463;&#38480;&#12290;&#36825;&#20123;&#32422;&#26463;&#20351;&#24471;&#22312;&#27492;&#31867;&#21160;&#24577;&#27880;&#37322;&#38598;&#19978;&#35757;&#32451;&#21644;&#26356;&#26032;&#22823;&#35268;&#27169;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#25110;&#19981;&#21487;&#33021;&#12290;&#19981;&#26029;&#23398;&#20064;&#30452;&#25509;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20854;&#26368;&#32456;&#30446;&#26631;&#26159;&#35774;&#35745;&#20986;&#26041;&#27861;&#65292;&#22312;&#26032;&#65288;&#26410;&#35265;&#36807;&#30340;&#65289;&#31867;&#21035;&#20013;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#30456;&#20851;&#27169;&#24335;&#65292;&#32780;&#19981;&#20250;&#23545;&#20808;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#30340;&#34920;&#29616;&#20135;&#29983;&#26174;&#33879;&#24433;&#21709;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#38024;&#23545;&#35270;&#39057;&#25968;&#25454;&#30340;&#25345;&#32493;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;PIVOT&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#26469;&#33258;&#22270;&#20687;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#24191;&#27867;&#30693;&#35782;&#65292;&#20174;&#32780;&#20943;&#23569;&#20102;&#21487;&#35757;&#32451;&#21442;&#25968;&#30340;&#25968;&#37327;&#20197;&#21450;&#30456;&#24212;&#30340;&#36951;&#24536;&#12290;&#19982;&#20197;&#21069;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#31532;&#19968;&#31181;&#22312;&#27809;&#26377;&#22495;&#20869;&#39044;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#26377;&#25928;&#20351;&#29992;&#25552;&#31034;&#26426;&#21046;&#36827;&#34892;&#25345;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;PIVOT &#25913;&#21892;&#20102;&#29616;&#26377;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern machine learning pipelines are limited due to data availability, storage quotas, privacy regulations, and expensive annotation processes. These constraints make it difficult or impossible to train and update large-scale models on such dynamic annotated sets. Continual learning directly approaches this problem, with the ultimate goal of devising methods where a deep neural network effectively learns relevant patterns for new (unseen) classes, without significantly altering its performance on previously learned ones. In this paper, we address the problem of continual learning for video data. We introduce PIVOT, a novel method that leverages extensive knowledge in pre-trained models from the image domain, thereby reducing the number of trainable parameters and the associated forgetting. Unlike previous methods, ours is the first approach that effectively uses prompting mechanisms for continual learning without any in-domain pre-training. Our experiments show that PIVOT improves sta
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#20027;&#21160;&#25628;&#32034;&#26694;&#26550;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21327;&#21161;&#22320;&#29702;&#31354;&#38388;&#25506;&#32034;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20197;&#24191;&#38420;&#21306;&#22495;&#30340;&#22270;&#20687;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#26597;&#35810;&#26469;&#39564;&#35777;&#30446;&#26631;&#23545;&#35937;&#31034;&#20363;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#20851;&#20110;&#30446;&#26631;&#23545;&#35937;&#31354;&#38388;&#20998;&#24067;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2211.15788</link><description>&lt;p&gt;
&#29992;&#20110;&#22320;&#29702;&#31354;&#38388;&#25506;&#32034;&#30340;&#35270;&#35273;&#20027;&#21160;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Visual Active Search Framework for Geospatial Exploration. (arXiv:2211.15788v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#20027;&#21160;&#25628;&#32034;&#26694;&#26550;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21327;&#21161;&#22320;&#29702;&#31354;&#38388;&#25506;&#32034;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20197;&#24191;&#38420;&#21306;&#22495;&#30340;&#22270;&#20687;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#26597;&#35810;&#26469;&#39564;&#35777;&#30446;&#26631;&#23545;&#35937;&#31034;&#20363;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#20851;&#20110;&#30446;&#26631;&#23545;&#35937;&#31354;&#38388;&#20998;&#24067;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#38382;&#39064;&#21487;&#20197;&#34987;&#35270;&#20026;&#21033;&#29992;&#33322;&#31354;&#24433;&#20687;&#21327;&#21161;&#30340;&#22320;&#29702;&#31354;&#38388;&#25628;&#32034;&#30340;&#24418;&#24335;&#65292;&#20363;&#22914;&#26816;&#27979;&#30423;&#29454;&#27963;&#21160;&#21644;&#20154;&#21475;&#36137;&#21334;&#31561;&#12290;&#26412;&#35770;&#25991;&#22312;&#35270;&#35273;&#20027;&#21160;&#25628;&#32034;&#65288;VAS&#65289;&#26694;&#26550;&#20013;&#24314;&#31435;&#20102;&#36825;&#31867;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#35813;&#26694;&#26550;&#20197;&#24191;&#38420;&#21306;&#22495;&#30340;&#22270;&#20687;&#20026;&#36755;&#20837;&#65292;&#24182;&#26088;&#22312;&#23613;&#21487;&#33021;&#22810;&#22320;&#35782;&#21035;&#30446;&#26631;&#23545;&#35937;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#26377;&#38480;&#30340;&#26597;&#35810;&#65292;VAS&#20250;&#39564;&#35777;&#22312;&#32473;&#23450;&#21306;&#22495;&#20869;&#26159;&#21542;&#23384;&#22312;&#31034;&#20363;&#12290;VAS&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#26159;&#65292;&#27599;&#20010;&#36825;&#26679;&#30340;&#26597;&#35810;&#37117;&#20250;&#25552;&#20379;&#26377;&#20851;&#30446;&#26631;&#23545;&#35937;&#31354;&#38388;&#20998;&#24067;&#30340;&#20449;&#24687;&#65292;&#36229;&#20986;&#20102;&#21487;&#35270;&#21270;&#25152;&#25429;&#25417;&#30340;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#30001;&#20110;&#31354;&#38388;&#30456;&#20851;&#24615;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;VAS&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23436;&#20840;&#27880;&#37322;&#30340;&#25628;&#32034;&#20219;&#21153;&#38598;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#26469;&#23398;&#20064;&#25628;&#32034;&#31574;&#30053;&#65292;&#24182;&#23558;&#36755;&#20837;&#22270;&#20687;&#30340;&#29305;&#24449;&#19982;&#20027;&#21160;&#25628;&#32034;&#29366;&#24577;&#30340;&#33258;&#28982;&#34920;&#31034;&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#20197;&#25913;&#21892;&#20915;&#31574;&#26102;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many problems can be viewed as forms of geospatial search aided by aerial imagery, with examples ranging from detecting poaching activity to human trafficking. We model this class of problems in a visual active search (VAS) framework, which takes as input an image of a broad area, and aims to identify as many examples of a target object as possible. It does this through a limited sequence of queries, each of which verifies whether an example is present in a given region. A crucial feature of VAS is that each such query is informative about the spatial distribution of target objects beyond what is captured visually (for example, due to spatial correlation). We propose a reinforcement learning approach for VAS that leverages a collection of fully annotated search tasks as training data to learn a search policy, and combines features of the input image with a natural representation of active search state. Additionally, we propose domain adaptation techniques to improve the policy at decis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29615;&#22659;&#33258;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.15136</link><description>&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;2D&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Collective Intelligence for 2D Push Manipulation with Mobile Robots. (arXiv:2211.15136v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29615;&#22659;&#33258;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#31995;&#32479;&#36890;&#24120;&#34920;&#29616;&#20986;&#33021;&#22815;&#33258;&#25105;&#32452;&#32455;&#21644;&#36866;&#24212;&#21464;&#21270;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#20294;&#22823;&#22810;&#25968;&#20154;&#24037;&#31995;&#32479;&#32570;&#20047;&#36825;&#31181;&#31561;&#25928;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#31227;&#21160;&#26426;&#22120;&#20154;&#36827;&#34892;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#30340;&#38598;&#20307;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20174;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#27966;&#29983;&#30340;&#35268;&#21010;&#22120;&#25552;&#28860;&#20026;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#21518;&#65292;&#25105;&#20204;&#30340;&#22810;&#26426;&#22120;&#20154;&#25512;&#21160;&#25805;&#20316;&#31995;&#32479;&#30456;&#23545;&#20110;&#22522;&#32447;&#31995;&#32479;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#36866;&#24212;&#22806;&#37096;&#25200;&#21160;&#21644;&#29615;&#22659;&#21464;&#21270;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While natural systems often present collective intelligence that allows them to self-organize and adapt to changes, the equivalent is missing in most artificial systems. We explore the possibility of such a system in the context of cooperative 2D push manipulations using mobile robots. Although conventional works demonstrate potential solutions for the problem in restricted settings, they have computational and learning difficulties. More importantly, these systems do not possess the ability to adapt when facing environmental changes. In this work, we show that by distilling a planner derived from a differentiable soft-body physics simulator into an attention-based neural network, our multi-robot push manipulation system achieves better performance than baselines. In addition, our system also generalizes to configurations not seen during training and is able to adapt toward task completions when external turbulence and environmental changes are applied. Supplementary videos can be foun
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#25351;&#23548;&#27169;&#20223;&#26041;&#27861;&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#22870;&#21169;&#26368;&#22823;&#21270;&#31574;&#30053;&#20998;&#35299;&#25104;&#24341;&#23548;&#31574;&#30053;&#21644;&#25191;&#34892;&#31574;&#30053;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#24341;&#23548;&#31574;&#30053;&#21644;&#25191;&#34892;&#31574;&#30053;&#22312;&#20165;&#20351;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#35780;&#20272;&#26399;&#38388;&#65292;&#24341;&#23548;&#31574;&#30053;&#36890;&#36807;&#25351;&#24341;&#25191;&#34892;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#36215;&#21040;&#8220;&#20808;&#30693;&#8221;&#30340;&#20316;&#29992;&#65292;&#20801;&#35768;&#21512;&#29702;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2210.08323</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#31574;&#30053;&#25351;&#23548;&#27169;&#20223;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Policy-Guided Imitation Approach for Offline Reinforcement Learning. (arXiv:2210.08323v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08323
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#25351;&#23548;&#27169;&#20223;&#26041;&#27861;&#65292;&#23427;&#23558;&#20256;&#32479;&#30340;&#22870;&#21169;&#26368;&#22823;&#21270;&#31574;&#30053;&#20998;&#35299;&#25104;&#24341;&#23548;&#31574;&#30053;&#21644;&#25191;&#34892;&#31574;&#30053;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#24341;&#23548;&#31574;&#30053;&#21644;&#25191;&#34892;&#31574;&#30053;&#22312;&#20165;&#20351;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#35780;&#20272;&#26399;&#38388;&#65292;&#24341;&#23548;&#31574;&#30053;&#36890;&#36807;&#25351;&#24341;&#25191;&#34892;&#31574;&#30053;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#36215;&#21040;&#8220;&#20808;&#30693;&#8221;&#30340;&#20316;&#29992;&#65292;&#20801;&#35768;&#21512;&#29702;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#36890;&#24120;&#21487;&#20197;&#23558;&#26041;&#27861;&#20998;&#20026;&#20004;&#31181;&#65306;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#12290;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#21407;&#21017;&#19978;&#21487;&#20197;&#20139;&#21463;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#65292;&#20294;&#20250;&#20986;&#29616;&#38169;&#35823;&#30340;&#31163;&#31574;&#30053;&#35780;&#20272;&#12290;&#22522;&#20110;&#27169;&#20223;&#23398;&#20064;&#30340;&#26041;&#27861;&#36991;&#20813;&#20102;&#31163;&#31574;&#30053;&#35780;&#20272;&#65292;&#20294;&#36807;&#20110;&#20445;&#23432;&#65292;&#38590;&#20197;&#36229;&#36234;&#25968;&#25454;&#38598;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#32487;&#25215;&#20102;&#27169;&#20223;&#24335;&#26041;&#27861;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#65292;&#21516;&#26102;&#20173;&#20801;&#35768;&#36923;&#36753;&#19978;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;&#25105;&#20204;&#23558;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#20256;&#32479;&#30340;&#22870;&#21169;&#26368;&#22823;&#21270;&#31574;&#30053;&#20998;&#35299;&#25104;&#24341;&#23548;&#31574;&#30053;&#21644;&#25191;&#34892;&#31574;&#30053;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#24341;&#23548;&#31574;&#30053;&#21644;&#25191;&#34892;&#31574;&#30053;&#20165;&#20351;&#29992;&#25968;&#25454;&#38598;&#20013;&#30340;&#25968;&#25454;&#65292;&#20197;&#30417;&#30563;&#21644;&#35299;&#32806;&#30340;&#26041;&#24335;&#36827;&#34892;&#23398;&#20064;&#12290;&#22312;&#35780;&#20272;&#26399;&#38388;&#65292;&#24341;&#23548;&#31574;&#30053;&#36890;&#36807;&#21578;&#35785;&#25191;&#34892;&#31574;&#30053;&#24212;&#35813;&#21435;&#21738;&#37324;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#65292;&#26469;&#25351;&#24341;&#25191;&#34892;&#31574;&#30053;&#30340;&#36208;&#21521;&#65292;&#20316;&#20026;&#8220;&#20808;&#30693;&#8221;&#12290;&#22914;&#27492;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#20801;&#35768;&#8220;&#29366;&#24577;&#32452;&#21512;&#24615;&#8221;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#21512;&#29702;&#30340;&#36229;&#20986;&#20998;&#24067;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) methods can generally be categorized into two types: RL-based and Imitation-based. RL-based methods could in principle enjoy out-of-distribution generalization but suffer from erroneous off-policy evaluation. Imitation-based methods avoid off-policy evaluation but are too conservative to surpass the dataset. In this study, we propose an alternative approach, inheriting the training stability of imitation-style methods while still allowing logical out-of-distribution generalization. We decompose the conventional reward-maximizing policy in offline RL into a guide-policy and an execute-policy. During training, the guide-poicy and execute-policy are learned using only data from the dataset, in a supervised and decoupled manner. During evaluation, the guide-policy guides the execute-policy by telling where it should go so that the reward can be maximized, serving as the \textit{Prophet}. By doing so, our algorithm allows \textit{state-compositionality} f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29256;&#26435;&#20445;&#25252;&#30340;&#26080;&#23475;&#21644;&#38544;&#34109;&#30340;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#26041;&#26696;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#26041;&#26696;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27700;&#21360;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#23475;&#19988;&#38544;&#34109;&#12290;</title><link>http://arxiv.org/abs/2210.00875</link><description>&lt;p&gt;
&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#65306;&#26397;&#30528;&#26080;&#23475;&#21644;&#38544;&#34109;&#30340;&#25968;&#25454;&#38598;&#29256;&#26435;&#20445;&#25252;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Untargeted Backdoor Watermark: Towards Harmless and Stealthy Dataset Copyright Protection. (arXiv:2210.00875v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.00875
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25968;&#25454;&#38598;&#29256;&#26435;&#20445;&#25252;&#30340;&#26080;&#23475;&#21644;&#38544;&#34109;&#30340;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#26041;&#26696;&#65292;&#21487;&#20197;&#36798;&#21040;&#19982;&#26368;&#20808;&#36827;&#26041;&#26696;&#30456;&#24403;&#25110;&#26356;&#22909;&#30340;&#27700;&#21360;&#25928;&#26524;&#65292;&#24182;&#35777;&#26126;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#23475;&#19988;&#38544;&#34109;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#24050;&#22312;&#23454;&#36341;&#20013;&#23637;&#29616;&#20986;&#20102;&#20854;&#20248;&#36234;&#24615;&#12290;&#21487;&#20197;&#35828;&#65292;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24471;&#30410;&#20110;&#39640;&#36136;&#37327;&#65288;&#24320;&#28304;&#65289;&#25968;&#25454;&#38598;&#65292;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#22312;&#27492;&#22522;&#30784;&#19978;&#36731;&#26494;&#22320;&#35780;&#20272;&#21644;&#25913;&#36827;&#20182;&#20204;&#30340;&#23398;&#20064;&#26041;&#27861;&#12290;&#30001;&#20110;&#25968;&#25454;&#25910;&#38598;&#36890;&#24120;&#26159;&#32791;&#26102;&#29978;&#33267;&#26114;&#36149;&#30340;&#65292;&#22914;&#20309;&#20445;&#25252;&#20854;&#29256;&#26435;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#20540;&#24471;&#36827;&#19968;&#27493;&#25506;&#32034;&#12290;&#26412;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#25968;&#25454;&#38598;&#30340;&#25152;&#26377;&#26435;&#39564;&#35777;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#39564;&#35777;&#26041;&#27861;&#30001;&#20110;&#26377;&#30446;&#26631;&#30340;&#21518;&#38376;&#27700;&#21360;&#30340;&#29305;&#24615;&#65292;&#20250;&#22312;&#21463;&#20445;&#25252;&#30340;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#24341;&#20837;&#26032;&#30340;&#23433;&#20840;&#39118;&#38505;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#26041;&#26696;&#65292;&#20854;&#20013;&#24322;&#24120;&#30340;&#27169;&#22411;&#34892;&#20026;&#19981;&#26159;&#30830;&#23450;&#24615;&#30340;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20004;&#20010;&#20998;&#25955;&#24230;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#30340;&#30456;&#20851;&#24615;&#65292;&#22522;&#20110;&#27492;&#25105;&#20204;&#22312;&#21463;&#27745;&#26579;&#26631;&#31614;&#21644;&#24178;&#20928;&#26631;&#31614;&#25915;&#20987;&#35774;&#32622;&#19979;&#35774;&#35745;&#20102;&#26080;&#30446;&#26631;&#21518;&#38376;&#27700;&#21360;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#27700;&#21360;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#27169;&#22411;&#24615;&#33021;&#19978;&#37117;&#33021;&#22815;&#36798;&#21040;&#29978;&#33267;&#36229;&#36807;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#36824;&#34987;&#35777;&#26126;&#23545;&#27169;&#22411;&#24615;&#33021;&#26080;&#23475;&#19988;&#38544;&#34109;&#65292;&#19981;&#20250;&#24341;&#20837;&#20219;&#20309;&#21487;&#26816;&#27979;&#30340;&#25197;&#26354;&#25110;&#25925;&#38556;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have demonstrated their superiority in practice. Arguably, the rapid development of DNNs is largely benefited from high-quality (open-sourced) datasets, based on which researchers and developers can easily evaluate and improve their learning methods. Since the data collection is usually time-consuming or even expensive, how to protect their copyrights is of great significance and worth further exploration. In this paper, we revisit dataset ownership verification. We find that existing verification methods introduced new security risks in DNNs trained on the protected dataset, due to the targeted nature of poison-only backdoor watermarks. To alleviate this problem, in this work, we explore the untargeted backdoor watermarking scheme, where the abnormal model behaviors are not deterministic. Specifically, we introduce two dispersibilities and prove their correlation, based on which we design the untargeted backdoor watermark under both poisoned-label and clean
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25163;&#25915;&#20987;VAEs&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#24341;&#36215;&#30340;&#38543;&#26426;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#30340;&#26041;&#21521;&#20559;&#24046;&#12290;</title><link>http://arxiv.org/abs/2208.03923</link><description>&lt;p&gt;
&#36890;&#36807;&#26412;&#22320;&#20960;&#20309;&#35282;&#24230;&#29702;&#35299;VAEs&#30340;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Adversarial robustness of VAEs through the lens of local geometry. (arXiv:2208.03923v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.03923
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25163;&#25915;&#20987;VAEs&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#24341;&#36215;&#30340;&#38543;&#26426;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#30340;&#26041;&#21521;&#20559;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23545;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAEs&#65289;&#36827;&#34892;&#26080;&#30417;&#30563;&#25915;&#20987;&#26102;&#65292;&#23545;&#25163;&#20250;&#25214;&#21040;&#19968;&#20010;&#36755;&#20837;&#26679;&#26412;&#20013;&#30340;&#23567;&#25200;&#21160;&#65292;&#20174;&#32780;&#26174;&#30528;&#25913;&#21464;&#20854;&#28508;&#22312;&#31354;&#38388;&#32534;&#30721;&#65292;&#20174;&#32780;&#25439;&#23475;&#20102;&#19968;&#20010;&#22266;&#23450;&#32534;&#30721;&#22120;&#30340;&#37325;&#26500;&#12290;&#36825;&#31181;&#33030;&#24369;&#24615;&#24050;&#30693;&#30340;&#21407;&#22240;&#26159;&#28508;&#22312;&#21518;&#39564;&#20998;&#24067;&#30340;&#36817;&#20284;&#19982;&#20808;&#39564;&#20998;&#24067;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#23548;&#33268;&#30340;&#28508;&#22312;&#31354;&#38388;&#25197;&#26354;&#12290;&#22240;&#27492;&#65292;&#36755;&#20837;&#26679;&#26412;&#20013;&#30340;&#24494;&#23567;&#21464;&#21270;&#21487;&#33021;&#20250;&#23558;&#20854;&#32534;&#30721;&#31227;&#21160;&#21040;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#20302;/&#38646;&#23494;&#24230;&#21306;&#22495;&#65292;&#20174;&#32780;&#20135;&#29983;&#26080;&#38480;&#21046;&#30340;&#29983;&#25104;&#12290;&#26412;&#25991;&#35777;&#26126;&#20102;&#23545;&#25163;&#25915;&#20987;VAEs&#30340;&#26368;&#20339;&#26041;&#27861;&#26159;&#21033;&#29992;&#30001;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#32593;&#32476;&#24341;&#36215;&#30340;&#38543;&#26426;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#30340;&#26041;&#21521;&#20559;&#24046;&#12290;&#32534;&#30721;&#22120;&#30340;&#22238;&#28335;&#24230;&#35268;&#24352;&#37327;&#27979;&#37327;&#23427;&#20174;&#36755;&#20837;&#21040;&#28508;&#22312;&#31354;&#38388;&#30340;&#24494;&#23567;&#28508;&#22312;&#20307;&#31215;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#23427;&#21487;&#20197;&#34987;&#35270;&#20026;&#20998;&#26512;&#36755;&#20837;&#25200;&#21160;&#23548;&#33268;&#28508;&#22312;&#31354;&#38388;&#25197;&#26354;&#25928;&#26524;&#30340;&#38236;&#22836;&#12290;
&lt;/p&gt;
&lt;p&gt;
In an unsupervised attack on variational autoencoders (VAEs), an adversary finds a small perturbation in an input sample that significantly changes its latent space encoding, thereby compromising the reconstruction for a fixed decoder. A known reason for such vulnerability is the distortions in the latent space resulting from a mismatch between approximated latent posterior and a prior distribution. Consequently, a slight change in an input sample can move its encoding to a low/zero density region in the latent space resulting in an unconstrained generation. This paper demonstrates that an optimal way for an adversary to attack VAEs is to exploit a directional bias of a stochastic pullback metric tensor induced by the encoder and decoder networks. The pullback metric tensor of an encoder measures the change in infinitesimal latent volume from an input to a latent space. Thus, it can be viewed as a lens to analyse the effect of input perturbations leading to latent space distortions. We
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;DeepIPC&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#24863;&#30693;&#21644;&#25511;&#21046;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DeepIPC&#20855;&#26377;&#26368;&#20339;&#30340;&#21487;&#39550;&#24615;&#21644;&#22810;&#20219;&#21153;&#24615;&#33021;&#65292;&#29978;&#33267;&#27604;&#20854;&#20182;&#27169;&#22411;&#25152;&#38656;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;</title><link>http://arxiv.org/abs/2207.09934</link><description>&lt;p&gt;
DeepIPC&#65306;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#30340;&#28145;&#24230;&#24863;&#30693;&#21644;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
DeepIPC: Deeply Integrated Perception and Control for an Autonomous Vehicle in Real Environments. (arXiv:2207.09934v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.09934
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;DeepIPC&#65292;&#19968;&#20010;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#22788;&#29702;&#24863;&#30693;&#21644;&#25511;&#21046;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DeepIPC&#20855;&#26377;&#26368;&#20339;&#30340;&#21487;&#39550;&#24615;&#21644;&#22810;&#20219;&#21153;&#24615;&#33021;&#65292;&#29978;&#33267;&#27604;&#20854;&#20182;&#27169;&#22411;&#25152;&#38656;&#30340;&#21442;&#25968;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;DeepIPC&#65292;&#19968;&#20010;&#22788;&#29702;&#33258;&#21160;&#39550;&#39542;&#20013;&#24863;&#30693;&#21644;&#25511;&#21046;&#30340;&#31471;&#21040;&#31471;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#30001;&#24863;&#30693;&#21644;&#25511;&#21046;&#22120;&#20004;&#20010;&#20027;&#35201;&#37096;&#20998;&#32452;&#25104;&#12290;&#24863;&#30693;&#27169;&#22359;&#20351;&#29992;RGBD&#22270;&#20687;&#25191;&#34892;&#35821;&#20041;&#20998;&#21106;&#21644;&#40479;&#30640;&#22270;&#35821;&#20041;&#26144;&#23556;&#65292;&#24182;&#25552;&#20379;&#23427;&#20204;&#30340;&#32534;&#30721;&#29305;&#24449;&#12290;&#21516;&#26102;&#65292;&#25511;&#21046;&#22120;&#27169;&#22359;&#20351;&#29992;GNSS&#23450;&#20301;&#21644;&#35282;&#36895;&#24230;&#30340;&#27979;&#37327;&#26469;&#22788;&#29702;&#36825;&#20123;&#29305;&#24449;&#65292;&#20272;&#35745;&#19968;&#31995;&#21015;&#36335;&#24452;&#28857;&#21644;&#28508;&#22312;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#20004;&#20010;&#19981;&#21516;&#30340;&#20195;&#29702;&#23558;&#36335;&#24452;&#28857;&#21644;&#28508;&#22312;&#29305;&#24449;&#36716;&#25442;&#20026;&#19968;&#32452;&#23548;&#33322;&#25511;&#21046;&#65292;&#20197;&#39537;&#21160;&#36710;&#36742;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#39044;&#27979;&#34892;&#36710;&#35760;&#24405;&#21644;&#22312;&#19981;&#21516;&#30495;&#23454;&#22330;&#26223;&#19979;&#36827;&#34892;&#33258;&#21160;&#39550;&#39542;&#26469;&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DeepIPC&#21363;&#20351;&#20351;&#29992;&#36739;&#23569;&#30340;&#21442;&#25968;&#20063;&#33021;&#23454;&#29616;&#26368;&#20339;&#21487;&#39550;&#24615;&#21644;&#22810;&#20219;&#21153;&#24615;&#33021;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#30456;&#20851;&#20195;&#30721;&#21487;&#22312; https://github.com &#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose DeepIPC, an end-to-end autonomous driving model that handles both perception and control tasks in driving a vehicle. The model consists of two main parts, perception and controller modules. The perception module takes an RGBD image to perform semantic segmentation and bird's eye view (BEV) semantic mapping along with providing their encoded features. Meanwhile, the controller module processes these features with the measurement of GNSS locations and angular speed to estimate waypoints that come with latent features. Then, two different agents are used to translate waypoints and latent features into a set of navigational controls to drive the vehicle. The model is evaluated by predicting driving records and performing automated driving under various conditions in real environments. The experimental results show that DeepIPC achieves the best drivability and multi-task performance even with fewer parameters compared to the other models. Codes are available at https://github.co
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;&#29275;&#39039;&#26041;&#27861;&#21644;&#24182;&#34892;&#22788;&#29702;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#26469;&#22788;&#29702;Hessian&#30697;&#38453;&#65292;&#24182;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2112.01401</link><description>&lt;p&gt;
&#22522;&#20110;&#29275;&#39039;&#26041;&#27861;&#21644;&#24182;&#34892;&#22788;&#29702;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Newton methods based convolution neural networks using parallel processing. (arXiv:2112.01401v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.01401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;&#29275;&#39039;&#26041;&#27861;&#21644;&#24182;&#34892;&#22788;&#29702;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#26041;&#27861;&#65292;&#20351;&#29992;&#23436;&#25972;&#30340;&#25968;&#25454;&#38598;&#26469;&#22788;&#29702;Hessian&#30697;&#38453;&#65292;&#24182;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#26159;&#19968;&#20010;&#39640;&#32500;&#30340;&#38750;&#20984;&#20248;&#21270;&#38382;&#39064;&#12290;&#30446;&#21069;&#65292;&#22312;&#26080;&#27861;&#33258;&#20449;&#22320;&#35774;&#32622;&#21442;&#25968;&#23398;&#20064;&#29575;&#30340;&#24773;&#20917;&#19979;&#65292;&#25928;&#29575;&#24456;&#20302;&#12290;&#36807;&#21435;&#30340;&#19968;&#20123;&#30740;&#31350;&#24341;&#20837;&#20102;&#29275;&#39039;&#26041;&#27861;&#26469;&#35757;&#32451;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#12290;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65292;&#29275;&#39039;&#26041;&#27861;&#28041;&#21450;&#22797;&#26434;&#30340;&#25805;&#20316;&#12290;&#36890;&#36807;&#20351;&#29992;&#23376;&#37319;&#26679;&#30340;Hessian&#29275;&#39039;&#26041;&#27861;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#29275;&#39039;&#26041;&#27861;&#22788;&#29702;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#23436;&#25972;&#30340;&#25968;&#25454;&#32780;&#19981;&#26159;&#19968;&#27425;&#21482;&#22788;&#29702;&#37096;&#20998;&#25968;&#25454;&#30340;&#23376;&#37319;&#26679;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#24182;&#34892;&#22788;&#29702;&#32780;&#19981;&#26159;&#20018;&#34892;&#22788;&#29702;&#26469;&#36827;&#34892;&#23567;&#25209;&#37327;&#35745;&#31639;&#12290;&#26412;&#30740;&#31350;&#20013;&#20351;&#29992;&#24182;&#34892;&#22788;&#29702;&#24471;&#21040;&#30340;&#32467;&#26524;&#20248;&#20110;&#20043;&#21069;&#25152;&#29992;&#26041;&#27861;&#25152;&#38656;&#30340;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training of convolutional neural networks is a high dimensional and a non-convex optimization problem. At present, it is inefficient in situations where parametric learning rates can not be confidently set. Some past works have introduced Newton methods for training deep neural networks. Newton methods for convolutional neural networks involve complicated operations. Finding the Hessian matrix in second-order methods becomes very complex as we mainly use the finite differences method with the image data. Newton methods for convolutional neural networks deals with this by using the sub-sampled Hessian Newton methods. In this paper, we have used the complete data instead of the sub-sampled methods that only handle partial data at a time. Further, we have used parallel processing instead of serial processing in mini-batch computations. The results obtained using parallel processing in this study, outperform the time taken by the previous approach.
&lt;/p&gt;</description></item><item><title>PatchCensor&#26159;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;Transformer&#30340;&#34917;&#19969;&#40065;&#26834;&#24615;&#35748;&#35777;&#26041;&#27861;&#65292;&#22522;&#20110;&#20840;&#38754;&#27979;&#35797;&#24182;&#32771;&#34385;&#26368;&#22351;&#30340;&#34917;&#19969;&#25915;&#20987;&#24773;&#22659;&#65292;&#33021;&#22815;&#25552;&#20379;&#21463;&#20445;&#35777;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2111.10481</link><description>&lt;p&gt;
PatchCensor&#65306;&#36890;&#36807;&#31351;&#23613;&#27979;&#35797;&#25552;&#39640;&#35270;&#35273;Transformers&#30340;&#34917;&#19969;&#40065;&#26834;&#24615;&#35748;&#35777;
&lt;/p&gt;
&lt;p&gt;
PatchCensor: Patch Robustness Certification for Transformers via Exhaustive Testing. (arXiv:2111.10481v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.10481
&lt;/p&gt;
&lt;p&gt;
PatchCensor&#26159;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;Transformer&#30340;&#34917;&#19969;&#40065;&#26834;&#24615;&#35748;&#35777;&#26041;&#27861;&#65292;&#22522;&#20110;&#20840;&#38754;&#27979;&#35797;&#24182;&#32771;&#34385;&#26368;&#22351;&#30340;&#34917;&#19969;&#25915;&#20987;&#24773;&#22659;&#65292;&#33021;&#22815;&#25552;&#20379;&#21463;&#20445;&#35777;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20854;&#20182;&#32463;&#20856;&#31070;&#32463;&#32593;&#32476;&#19968;&#26679;&#65292;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#20063;&#34987;&#35748;&#20026;&#26159;&#39640;&#24230;&#38750;&#32447;&#24615;&#30340;&#65292;&#24182;&#19988;&#23481;&#26131;&#21463;&#21040;&#33258;&#28982;&#21644;&#23545;&#25239;&#24615;&#34917;&#19969;&#25200;&#21160;&#30340;&#24178;&#25200;&#12290;&#22312;&#30495;&#23454;&#30340;&#24037;&#19994;&#29615;&#22659;&#20013;&#65292;&#36825;&#31181;&#38480;&#21046;&#21487;&#33021;&#23545;ViT&#30340;&#37096;&#32626;&#20135;&#29983;&#23041;&#32961;&#65292;&#23588;&#20854;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#22330;&#26223;&#20013;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;PatchCensor&#65292;&#26088;&#22312;&#36890;&#36807;&#20840;&#38754;&#27979;&#35797;&#26469;&#35777;&#26126;ViT&#30340;&#34917;&#19969;&#40065;&#26834;&#24615;&#12290;&#25105;&#20204;&#35797;&#22270;&#36890;&#36807;&#32771;&#34385;&#26368;&#22351;&#30340;&#34917;&#19969;&#25915;&#20987;&#24773;&#22659;&#26469;&#25552;&#20379;&#21487;&#35777;&#26126;&#30340;&#20445;&#35777;&#12290;&#19982;&#37027;&#20123;&#21487;&#33021;&#34987;&#33258;&#36866;&#24212;&#25915;&#20987;&#30772;&#22351;&#30340;&#23545;&#25239;&#24615;&#34917;&#19969;&#30340;&#32463;&#39564;&#24615;&#38450;&#24481;&#25514;&#26045;&#19981;&#21516;&#65292;&#35748;&#35777;&#21487;&#38752;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#26576;&#20123;&#26465;&#20214;&#19979;&#25552;&#20379;&#23545;&#20110;&#20219;&#24847;&#25915;&#20987;&#30340;&#21463;&#20445;&#35777;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#30340;&#40065;&#26834;&#24615;&#35748;&#35777;&#20027;&#35201;&#22522;&#20110;&#40065;&#26834;&#24615;&#35757;&#32451;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#24037;&#20316;&#65292;&#24182;&#19988;&#20250;&#22312;&#27491;&#24120;&#26679;&#26412;&#19978;&#29306;&#29298;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;PatchCensor&#26088;&#22312;&#36890;&#36807;&#26816;&#27979;&#21644;&#21076;&#38500;&#19981;&#21512;&#35268;&#21017;&#30340;&#21306;&#22495;&#26469;&#25552;&#39640;&#25972;&#20010;&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) is known to be highly nonlinear like other classical neural networks and could be easily fooled by both natural and adversarial patch perturbations. This limitation could pose a threat to the deployment of ViT in the real industrial environment, especially in safety-critical scenarios. In this work, we propose PatchCensor, aiming to certify the patch robustness of ViT by applying exhaustive testing. We try to provide a provable guarantee by considering the worst patch attack scenarios. Unlike empirical defenses against adversarial patches that may be adaptively breached, certified robust approaches can provide a certified accuracy against arbitrary attacks under certain conditions. However, existing robustness certifications are mostly based on robust training, which often requires substantial training efforts and the sacrifice of model performance on normal samples. To bridge the gap, PatchCensor seeks to improve the robustness of the whole system by detecting
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#26088;&#22312;&#36830;&#25509;&#24191;&#27867;&#30340;&#22810;&#23398;&#31185;&#29616;&#26377;&#30693;&#35782;&#65292;&#20026;&#31227;&#21160;&#26426;&#22120;&#20154;&#26500;&#24314;&#23436;&#25972;&#30340;&#24773;&#22659;&#24863;&#30693;&#65288;SA&#65289;&#31995;&#32479;&#65292;&#20197;&#25552;&#21319;&#20854;&#33258;&#20027;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2110.00273</link><description>&lt;p&gt;
&#20174;SLAM&#21040;&#24773;&#22659;&#24863;&#30693;&#65306;&#25361;&#25112;&#19982;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
From SLAM to Situational Awareness: Challenges and Survey. (arXiv:2110.00273v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.00273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#26088;&#22312;&#36830;&#25509;&#24191;&#27867;&#30340;&#22810;&#23398;&#31185;&#29616;&#26377;&#30693;&#35782;&#65292;&#20026;&#31227;&#21160;&#26426;&#22120;&#20154;&#26500;&#24314;&#23436;&#25972;&#30340;&#24773;&#22659;&#24863;&#30693;&#65288;SA&#65289;&#31995;&#32479;&#65292;&#20197;&#25552;&#21319;&#20854;&#33258;&#20027;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#25928;&#12289;&#23433;&#20840;&#22320;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#21463;&#38480;&#20110;&#20854;&#23545;&#29615;&#22659;&#65288;&#21363;&#24773;&#20917;&#65289;&#30340;&#20102;&#35299;&#12290;&#20808;&#36827;&#30340;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#25191;&#34892;&#25216;&#33021;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#33258;&#20027;&#34892;&#21160;&#12290;&#24773;&#22659;&#24863;&#30693;&#65288;SA&#65289;&#26159;&#20154;&#31867;&#30340;&#19968;&#31181;&#22522;&#26412;&#33021;&#21147;&#65292;&#24050;&#22312;&#24515;&#29702;&#23398;&#12289;&#20891;&#20107;&#12289;&#33322;&#31354;&#33322;&#22825;&#21644;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#28145;&#20837;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#23578;&#26410;&#32771;&#34385;&#24773;&#22659;&#24863;&#30693;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#21333;&#19968;&#30340;&#27010;&#24565;&#65292;&#22914;&#24863;&#30693;&#12289;&#31354;&#38388;&#24863;&#30693;&#12289;&#20256;&#24863;&#22120;&#34701;&#21512;&#12289;&#29366;&#24577;&#20272;&#35745;&#21644;&#21516;&#26102;&#23450;&#20301;&#19982;&#26144;&#23556;&#65288;SLAM&#65289;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36830;&#25509;&#24191;&#27867;&#30340;&#22810;&#23398;&#31185;&#29616;&#26377;&#30693;&#35782;&#65292;&#20026;&#25105;&#20204;&#35748;&#20026;&#23545;&#20110;&#33258;&#20027;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#23436;&#25972;&#30340;SA&#31995;&#32479;&#38138;&#24179;&#36947;&#36335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#26500;&#24314;&#26426;&#22120;&#20154;SA&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#21450;&#20854;&#25152;&#28041;&#21450;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;
&lt;/p&gt;
&lt;p&gt;
The capability of a mobile robot to efficiently and safely perform complex missions is limited by its knowledge of the environment, namely the situation. Advanced reasoning, decision-making, and execution skills enable an intelligent agent to act autonomously in unknown environments. Situational Awareness (SA) is a fundamental capability of humans that has been deeply studied in various fields, such as psychology, military, aerospace, and education. Nevertheless, it has yet to be considered in robotics, which has focused on single compartmentalized concepts such as sensing, spatial perception, sensor fusion, state estimation, and Simultaneous Localization and Mapping (SLAM). Hence, the present research aims to connect the broad multidisciplinary existing knowledge to pave the way for a complete SA system for mobile robotics that we deem paramount for autonomy. To this aim, we define the principal components to structure a robotic SA and their area of competence. Accordingly, this paper
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#31232;&#30095;&#25968;&#25454;&#29305;&#24449;&#30340;&#32852;&#37030;&#23376;&#27169;&#22411;&#24179;&#22343;&#31639;&#27861;(FedSubAvg)&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#36991;&#20813;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#23548;&#33268;&#30340;&#35745;&#31639;&#19979;&#38477;&#65292;&#24182;&#20445;&#35777;&#27599;&#20010;&#27169;&#22411;&#21442;&#25968;&#30340;&#20840;&#23616;&#26356;&#26032;&#26399;&#26395;&#31561;&#20110;&#28041;&#21450;&#23427;&#30340;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#24179;&#22343;&#20540;&#12290;&#35813;&#31639;&#27861;&#30340;&#26032;&#24230;&#37327;&#20803;&#32032;&#26799;&#24230;&#33539;&#25968;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#24449;&#22312;&#31232;&#30095;&#25968;&#25454;&#19978;&#30340;&#32852;&#37030;&#20248;&#21270;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2109.07704</link><description>&lt;p&gt;
Federated Submodel Optimization for Hot and Cold Data Features&#65288;&#28909;&#28857;&#21644;&#20919;&#38376;&#25968;&#25454;&#29305;&#24449;&#30340;&#32852;&#37030;&#23376;&#27169;&#22411;&#20248;&#21270;&#65289;
&lt;/p&gt;
&lt;p&gt;
Federated Submodel Optimization for Hot and Cold Data Features. (arXiv:2109.07704v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.07704
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#32852;&#37030;&#23398;&#20064;&#31232;&#30095;&#25968;&#25454;&#29305;&#24449;&#30340;&#32852;&#37030;&#23376;&#27169;&#22411;&#24179;&#22343;&#31639;&#27861;(FedSubAvg)&#65292;&#35813;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#36991;&#20813;&#25968;&#25454;&#31232;&#30095;&#38382;&#39064;&#23548;&#33268;&#30340;&#35745;&#31639;&#19979;&#38477;&#65292;&#24182;&#20445;&#35777;&#27599;&#20010;&#27169;&#22411;&#21442;&#25968;&#30340;&#20840;&#23616;&#26356;&#26032;&#26399;&#26395;&#31561;&#20110;&#28041;&#21450;&#23427;&#30340;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#24179;&#22343;&#20540;&#12290;&#35813;&#31639;&#27861;&#30340;&#26032;&#24230;&#37327;&#20803;&#32032;&#26799;&#24230;&#33539;&#25968;&#21487;&#20197;&#26356;&#22909;&#22320;&#34920;&#24449;&#22312;&#31232;&#30095;&#25968;&#25454;&#19978;&#30340;&#32852;&#37030;&#20248;&#21270;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#23454;&#38469;&#25968;&#25454;&#29305;&#24449;&#65292;&#20854;&#20013;&#23458;&#25143;&#31471;&#30340;&#38750;&#29420;&#31435;&#21516;&#20998;&#24067;&#25968;&#25454;&#20855;&#26377;&#31232;&#30095;&#29305;&#24449;&#65292;&#24182;&#19988;&#26576;&#20010;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#25968;&#25454;&#36890;&#24120;&#20165;&#28041;&#21450;&#23436;&#25972;&#27169;&#22411;&#30340;&#19968;&#23567;&#37096;&#20998;&#65292;&#31216;&#20026;&#23376;&#27169;&#22411;&#12290;&#30001;&#20110;&#25968;&#25454;&#31232;&#30095;&#24615;&#65292;&#20256;&#32479;&#30340;&#32852;&#37030;&#24179;&#22343;&#65288;FedAvg&#65289;&#31639;&#27861;&#25110;&#20854;&#21464;&#20307;&#23558;&#20005;&#37325;&#20943;&#24930;&#65292;&#22240;&#20026;&#22312;&#26356;&#26032;&#20840;&#23616;&#27169;&#22411;&#26102;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#38500;&#20854;&#23376;&#27169;&#22411;&#22806;&#30340;&#38646;&#26356;&#26032;&#34987;&#19981;&#20934;&#30830;&#22320;&#32858;&#21512;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#23376;&#27169;&#22411;&#24179;&#22343;&#65288;FedSubAvg&#65289;&#65292;&#30830;&#20445;&#27599;&#20010;&#27169;&#22411;&#21442;&#25968;&#30340;&#20840;&#23616;&#26356;&#26032;&#30340;&#26399;&#26395;&#31561;&#20110;&#28041;&#21450;&#23427;&#30340;&#23458;&#25143;&#31471;&#30340;&#26412;&#22320;&#26356;&#26032;&#30340;&#24179;&#22343;&#20540;&#12290;&#25105;&#20204;&#36890;&#36807;&#25512;&#23548;&#19968;&#20010;&#31216;&#20026;&#20803;&#32032;&#26799;&#24230;&#33539;&#25968;&#30340;&#26032;&#24230;&#37327;&#19978;&#30028;&#26469;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;FedSubAvg&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#20010;&#26032;&#24230;&#37327;&#21487;&#20197;&#34920;&#24449;&#22312;&#31232;&#30095;&#25968;&#25454;&#19978;&#30340;&#32852;&#37030;&#20248;&#21270;&#25910;&#25947;&#65292;&#32780;&#20256;&#32479;&#30340;&#24179;&#26041;&#26799;&#24230;&#24230;&#37327;&#21017;&#26080;&#27861;&#23436;&#25104;&#36825;&#19968;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study practical data characteristics underlying federated learning, where non-i.i.d. data from clients have sparse features, and a certain client's local data normally involves only a small part of the full model, called a submodel. Due to data sparsity, the classical federated averaging (FedAvg) algorithm or its variants will be severely slowed down, because when updating the global model, each client's zero update of the full model excluding its submodel is inaccurately aggregated. Therefore, we propose federated submodel averaging (FedSubAvg), ensuring that the expectation of the global update of each model parameter is equal to the average of the local updates of the clients who involve it. We theoretically proved the convergence rate of FedSubAvg by deriving an upper bound under a new metric called the element-wise gradient norm. In particular, this new metric can characterize the convergence of federated optimization over sparse data, while the conventional metric of squared g
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25913;&#36827;&#21322;&#23548;&#20307;&#22120;&#20214;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#22686;&#24378;&#31574;&#30053;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#25216;&#26415;&#65292;&#21482;&#38656;&#23569;&#37327;&#23454;&#39564;&#25968;&#25454;&#28857;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#39044;&#27979;&#65292;&#26377;&#25928;&#38477;&#20302;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65292;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2105.11453</link><description>&lt;p&gt;
&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25913;&#36827;&#21322;&#23548;&#20307;&#22120;&#20214;&#24314;&#27169;&#30340;&#30005;&#23376;&#35774;&#35745;&#33258;&#21160;&#21270;
&lt;/p&gt;
&lt;p&gt;
Improving Semiconductor Device Modeling for Electronic Design Automation by Machine Learning Techniques. (arXiv:2105.11453v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.11453
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#25913;&#36827;&#21322;&#23548;&#20307;&#22120;&#20214;&#24314;&#27169;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#25105;&#22686;&#24378;&#31574;&#30053;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#25216;&#26415;&#65292;&#21482;&#38656;&#23569;&#37327;&#23454;&#39564;&#25968;&#25454;&#28857;&#21363;&#21487;&#23454;&#29616;&#39640;&#31934;&#24230;&#39044;&#27979;&#65292;&#26377;&#25928;&#38477;&#20302;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#65292;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25216;&#26415;&#22312;&#25216;&#26415;&#35745;&#31639;&#26426;&#36741;&#21161;&#35774;&#35745;&#65288;TCAD&#65289;&#26041;&#27861;&#20013;&#30340;&#24212;&#29992;&#23545;&#21322;&#23548;&#20307;&#34892;&#19994;&#26377;&#24456;&#22823;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;ML&#27169;&#22411;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#25968;&#37327;&#12290;&#30001;&#20110;&#22120;&#20214;&#21046;&#36896;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#22312;&#21322;&#23548;&#20307;&#34892;&#19994;&#20013;&#24456;&#38590;&#33719;&#24471;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#33258;&#25105;&#22686;&#24378;&#31574;&#30053;&#65292;&#21033;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#25216;&#26415;&#25913;&#36827;&#22522;&#20110;ML&#30340;&#22120;&#20214;&#24314;&#27169;&#12290;&#36825;&#20123;&#25216;&#26415;&#21482;&#38656;&#35201;&#23569;&#37327;&#23454;&#39564;&#25968;&#25454;&#28857;&#65292;&#24182;&#19988;&#19981;&#20381;&#36182;&#20110;TCAD&#24037;&#20855;&#12290;&#20026;&#20102;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#38227;&#27694;&#22120;&#20214;&#20013;&#27431;&#22982;&#30005;&#38459;&#20540;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#20219;&#21153;&#12290;&#22312;&#39044;&#27979;&#23454;&#39564;&#32467;&#26524;&#26102;&#65292;&#24179;&#22343;&#32477;&#23545;&#35823;&#24046;&#38477;&#20302;&#20102;70%&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#22266;&#26377;&#30340;&#28789;&#27963;&#24615;&#65292;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#21508;&#31181;&#20219;&#21153;&#65292;&#22240;&#27492;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The semiconductors industry benefits greatly from the integration of Machine Learning (ML)-based techniques in Technology Computer-Aided Design (TCAD) methods. The performance of ML models however relies heavily on the quality and quantity of training datasets. They can be particularly difficult to obtain in the semiconductor industry due to the complexity and expense of the device fabrication. In this paper, we propose a self-augmentation strategy for improving ML-based device modeling using variational autoencoder-based techniques. These techniques require a small number of experimental data points and does not rely on TCAD tools. To demonstrate the effectiveness of our approach, we apply it to a deep neural network-based prediction task for the Ohmic resistance value in Gallium Nitride devices. A 70% reduction in mean absolute error when predicting experimental results is achieved. The inherent flexibility of our approach allows easy adaptation to various tasks, thus making it highl
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#20013;&#38388;&#23618;&#23545;&#20110;BERT&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#30340;&#20316;&#29992;&#65292;&#34920;&#26126;&#21024;&#38500;&#20013;&#38388;&#23618;&#25968;&#37327;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#19981;&#21463;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2012.11881</link><description>&lt;p&gt;
&#26080;&#20998;&#21106;&#20851;&#27880;&#65306;BERT&#26159;&#21542;&#38656;&#35201;&#20013;&#38388;&#23618;&#65311;
&lt;/p&gt;
&lt;p&gt;
Undivided Attention: Are Intermediate Layers Necessary for BERT?. (arXiv:2012.11881v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2012.11881
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35843;&#26597;&#20102;&#20013;&#38388;&#23618;&#23545;&#20110;BERT&#22312;&#19979;&#28216;&#20219;&#21153;&#34920;&#29616;&#30340;&#20316;&#29992;&#65292;&#34920;&#26126;&#21024;&#38500;&#20013;&#38388;&#23618;&#25968;&#37327;&#21487;&#20197;&#20943;&#23569;&#27169;&#22411;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#65292;&#21516;&#26102;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#24433;&#21709;&#36739;&#23567;&#65292;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#19981;&#21463;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;BERT&#30340;&#27169;&#22411;&#22312;&#35299;&#20915;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#65292;&#20363;&#22914;&#38405;&#35835;&#29702;&#35299;&#12289;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#24773;&#24863;&#20998;&#26512;&#31561;&#12290;&#25152;&#26377;&#22522;&#20110;BERT&#30340;&#26550;&#26500;&#37117;&#20855;&#26377;&#19968;&#20010;&#33258;&#27880;&#24847;&#21147;&#22359;&#65292;&#21518;&#36319;&#19968;&#20010;&#20013;&#38388;&#23618;&#22359;&#20316;&#20026;&#22522;&#26412;&#26500;&#24314;&#32452;&#20214;&#12290;&#28982;&#32780;&#65292;&#25991;&#29486;&#20013;&#32570;&#20047;&#23545;&#21253;&#21547;&#36825;&#20123;&#20013;&#38388;&#23618;&#30340;&#24378;&#26377;&#21147;&#30340;&#29702;&#30001;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#20013;&#38388;&#23618;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#25972;&#20307;&#32593;&#32476;&#24615;&#33021;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20943;&#23569;&#20013;&#38388;&#23618;&#25968;&#37327;&#24182;&#20462;&#25913;BERT-BASE&#30340;&#26550;&#26500;&#20250;&#23548;&#33268;&#19979;&#28216;&#20219;&#21153;&#30340;&#24494;&#23567;&#25439;&#22833;&#65292;&#21516;&#26102;&#20943;&#23569;&#27169;&#22411;&#30340;&#21442;&#25968;&#21644;&#35757;&#32451;&#26102;&#38388;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#20013;&#24515;&#21270;&#20869;&#26680;&#23545;&#40784;&#21644;&#25506;&#27979;&#32447;&#24615;&#20998;&#31867;&#22120;&#26469;&#33719;&#24471;&#23545;&#25105;&#20204;&#30340;&#26550;&#26500;&#20462;&#25913;&#30340;&#27934;&#35265;&#65292;&#24182;&#35777;&#26126;&#21024;&#38500;&#20013;&#38388;&#23618;&#23545;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#27809;&#26377;&#26174;&#30528;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent times, BERT-based models have been extremely successful in solving a variety of natural language processing (NLP) tasks such as reading comprehension, natural language inference, sentiment analysis, etc. All BERT-based architectures have a self-attention block followed by a block of intermediate layers as the basic building component. However, a strong justification for the inclusion of these intermediate layers remains missing in the literature. In this work we investigate the importance of intermediate layers on the overall network performance of downstream tasks. We show that reducing the number of intermediate layers and modifying the architecture for BERT-BASE results in minimal loss in fine-tuning accuracy for downstream tasks while decreasing the number of parameters and training time of the model. Additionally, we use centered kernel alignment and probing linear classifiers to gain insight into our architectural modifications and justify that removal of intermediate l
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Coke&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#19978;&#19979;&#25991;&#30693;&#35782;&#21160;&#24577;&#36873;&#25321;&#21644;&#23884;&#20837;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#36991;&#20813;&#23545;&#36755;&#20837;&#25991;&#26412;&#21305;&#37197;&#25928;&#26524;&#24046;&#30340;&#20887;&#20313;&#21644;&#27169;&#31946;&#30693;&#35782;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#30693;&#35782;&#39537;&#21160;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2009.13964</link><description>&lt;p&gt;
CokeBERT: &#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#36873;&#25321;&#19982;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
CokeBERT: Contextual Knowledge Selection and Embedding towards Enhanced Pre-Trained Language Models. (arXiv:2009.13964v5 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.13964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Coke&#30340;&#26032;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;&#19978;&#19979;&#25991;&#30693;&#35782;&#21160;&#24577;&#36873;&#25321;&#21644;&#23884;&#20837;&#21040;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20197;&#36991;&#20813;&#23545;&#36755;&#20837;&#25991;&#26412;&#21305;&#37197;&#25928;&#26524;&#24046;&#30340;&#20887;&#20313;&#21644;&#27169;&#31946;&#30693;&#35782;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#30693;&#35782;&#39537;&#21160;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#26377;&#35768;&#22810;&#24037;&#20316;&#33268;&#21147;&#20110;&#21033;&#29992;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#22806;&#37096;&#24322;&#26500;&#30693;&#35782;&#26469;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#65292;&#24182;&#22312;&#21508;&#31181;&#30693;&#35782;&#39537;&#21160;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#33719;&#24471;&#20102;&#19968;&#33268;&#30340;&#25913;&#36827;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#30693;&#35782;&#22686;&#24378;&#30340;PLMs&#20165;&#23884;&#20837;KG&#20013;&#30340;&#38745;&#24577;&#23376;&#22270;&#65288;&#8220;&#30693;&#35782;&#19978;&#19979;&#25991;&#8221;&#65289;&#65292;&#32780;&#19981;&#32771;&#34385;PLMs&#21487;&#33021;&#26681;&#25454;&#29305;&#23450;&#25991;&#26412;&#65288;&#8220;&#25991;&#26412;&#19978;&#19979;&#25991;&#8221;&#65289;&#21160;&#24577;&#21464;&#21270;&#25152;&#38656;&#30340;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;Coke&#65292;&#29992;&#20110;&#20026;PLMs&#21160;&#24577;&#36873;&#25321;&#19978;&#19979;&#25991;&#30693;&#35782;&#24182;&#26681;&#25454;&#25991;&#26412;&#19978;&#19979;&#25991;&#23884;&#20837;&#30693;&#35782;&#19978;&#19979;&#25991;&#65292;&#20174;&#32780;&#21487;&#20197;&#36991;&#20813;KG&#20013;&#30340;&#20887;&#20313;&#21644;&#27169;&#31946;&#30693;&#35782;&#23545;&#36755;&#20837;&#25991;&#26412;&#30340;&#21305;&#37197;&#25928;&#26524;&#19981;&#20339;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;Coke&#22312;&#20856;&#22411;&#30340;&#30693;&#35782;&#39537;&#21160;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#20248;&#20110;&#21508;&#31181;&#22522;&#20934;&#27169;&#22411;&#65292;&#34920;&#26126;&#20102;&#21033;&#29992;&#21160;&#24577;&#30693;&#35782;&#19978;&#19979;&#25991;&#36827;&#34892;&#35821;&#35328;&#29702;&#35299;&#30340;&#26377;&#25928;&#24615;&#12290;&#38500;&#20102;&#24615;&#33021;&#26041;&#38754;&#30340;&#25913;&#36827;&#65292;&#25152;&#36873;&#25321;&#30340;&#21160;&#24577;&#30693;&#35782;&#33021;&#22815;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#25351;&#23548;&#28145;&#20837;&#29702;&#35299;PLMs&#25152;&#23398;&#20064;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;
Several recent efforts have been devoted to enhancing pre-trained language models (PLMs) by utilizing extra heterogeneous knowledge in knowledge graphs (KGs) and achieved consistent improvements on various knowledge-driven NLP tasks. However, most of these knowledge-enhanced PLMs embed static sub-graphs of KGs ("knowledge context"), regardless of that the knowledge required by PLMs may change dynamically according to specific text ("textual context"). In this paper, we propose a novel framework named Coke to dynamically select contextual knowledge and embed knowledge context according to textual context for PLMs, which can avoid the effect of redundant and ambiguous knowledge in KGs that cannot match the input text. Our experimental results show that Coke outperforms various baselines on typical knowledge-driven NLP tasks, indicating the effectiveness of utilizing dynamic knowledge context for language understanding. Besides the performance improvements, the dynamically selected knowle
&lt;/p&gt;</description></item></channel></rss>