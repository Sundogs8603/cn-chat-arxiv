<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#22810;&#26679;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#24212;&#29992;&#36827;&#23637;&#32531;&#24930;&#65292;LLMs&#36824;&#27809;&#26377;&#30495;&#27491;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.10070</link><description>&lt;p&gt;
ChatGPT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Opportunities and Challenges for ChatGPT and Large Language Models in Biomedicine and Health. (arXiv:2306.10070v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10070
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;ChatGPT&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#22810;&#26679;&#24212;&#29992;&#65292;&#21457;&#29616;&#22312;&#25991;&#26412;&#29983;&#25104;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23545;&#20110;&#20854;&#20182;&#24212;&#29992;&#36827;&#23637;&#32531;&#24930;&#65292;LLMs&#36824;&#27809;&#26377;&#30495;&#27491;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#30001;&#20110;&#20854;&#21331;&#36234;&#30340;&#25991;&#26412;&#29983;&#25104;&#33021;&#21147;&#65292;&#24050;&#32463;&#24341;&#36215;&#20102;&#20844;&#20247;&#21644;&#39046;&#22495;&#19987;&#23478;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#24182;&#20135;&#29983;&#20102;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#21508;&#31181;&#24212;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#20581;&#24247;&#39046;&#22495;&#30340;&#22810;&#26679;&#24212;&#29992;&#65292;&#20855;&#20307;&#25506;&#35752;&#29983;&#29289;&#21307;&#23398;&#20449;&#24687;&#26816;&#32034;&#12289;&#38382;&#31572;&#12289;&#21307;&#30103;&#25991;&#26412;&#25688;&#35201;&#12289;&#20449;&#24687;&#25277;&#21462;&#21644;&#21307;&#23398;&#25945;&#32946;&#31561;&#39046;&#22495;&#65292;&#24182;&#30740;&#31350;LLMs&#26159;&#21542;&#20855;&#26377;&#30495;&#27491;&#30340;&#36716;&#22411;&#21147;&#37327;&#20197;&#24443;&#24213;&#25913;&#21464;&#36825;&#20123;&#20219;&#21153;&#25110;&#32773;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29420;&#29305;&#22797;&#26434;&#24615;&#26159;&#21542;&#25552;&#20986;&#20102;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#25991;&#29486;&#35843;&#30740;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#36229;&#36234;&#20102;&#20197;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#23545;&#20110;&#20854;&#20182;&#24212;&#29992;&#65292;&#36827;&#23637;&#36824;&#27604;&#36739;&#32531;&#24930;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;LLMs&#36824;&#27809;&#26377;&#24443;&#24213;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has drawn considerable attention from both the general public and domain experts with its remarkable text generation capabilities. This has subsequently led to the emergence of diverse applications in the field of biomedicine and health. In this work, we examine the diverse applications of large language models (LLMs), such as ChatGPT, in biomedicine and health. Specifically we explore the areas of biomedical information retrieval, question answering, medical text summarization, information extraction, and medical education, and investigate whether LLMs possess the transformative power to revolutionize these tasks or whether the distinct complexities of biomedical domain presents unique challenges. Following an extensive literature survey, we find that significant advances have been made in the field of text generation tasks, surpassing the previous state-of-the-art methods. For other applications, the advances have been modest. Overall, LLMs have not yet revolutionized the bio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22312;&#32447;AI&#21019;&#24847;&#35838;&#31243;&#65292;&#22522;&#20110;&#19977;&#20010;AI&#27169;&#22411;&#24314;&#31435;&#20102;&#27969;&#34892;&#27468;&#26354;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#35838;&#31243;&#37319;&#29992;Piaget&#26500;&#36896;&#20027;&#20041;&#29702;&#24565;&#30340;"&#23454;&#36341;&#23398;&#20064;"&#26041;&#27861;&#65292;&#36890;&#36807;&#35814;&#32454;&#30340;&#20116;&#21608;&#35838;&#31243;&#35774;&#35745;&#25552;&#21319;&#23398;&#29983;&#30340;&#25216;&#26415;&#21644;&#21019;&#20316;&#33021;&#21147;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#65292;&#21487;&#20197;&#29983;&#25104;&#19968;&#39318;&#27969;&#34892;&#27468;&#26354;&#12290;</title><link>http://arxiv.org/abs/2306.10069</link><description>&lt;p&gt;
&#27969;&#34892;&#27468;&#26354;&#29983;&#25104;&#22120;&#65306;&#35774;&#35745;&#19968;&#20010;&#22312;&#32447;&#35838;&#31243;&#26469;&#25945;&#25480;&#21327;&#20316;&#21019;&#24847; AI
&lt;/p&gt;
&lt;p&gt;
The pop song generator: designing an online course to teach collaborative, creative AI. (arXiv:2306.10069v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#22312;&#32447;AI&#21019;&#24847;&#35838;&#31243;&#65292;&#22522;&#20110;&#19977;&#20010;AI&#27169;&#22411;&#24314;&#31435;&#20102;&#27969;&#34892;&#27468;&#26354;&#29983;&#25104;&#31995;&#32479;&#12290;&#35813;&#35838;&#31243;&#37319;&#29992;Piaget&#26500;&#36896;&#20027;&#20041;&#29702;&#24565;&#30340;"&#23454;&#36341;&#23398;&#20064;"&#26041;&#27861;&#65292;&#36890;&#36807;&#35814;&#32454;&#30340;&#20116;&#21608;&#35838;&#31243;&#35774;&#35745;&#25552;&#21319;&#23398;&#29983;&#30340;&#25216;&#26415;&#21644;&#21019;&#20316;&#33021;&#21147;&#12290;&#36890;&#36807;&#23450;&#37327;&#21644;&#23450;&#24615;&#20998;&#26512;&#65292;&#21487;&#20197;&#29983;&#25104;&#19968;&#39318;&#27969;&#34892;&#27468;&#26354;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#24182;&#35780;&#20272;&#20102;&#19968;&#38376;&#26032;&#30340;&#22312;&#32447; AI &#21019;&#24847;&#35838;&#31243;&#12290;&#35813;&#35838;&#31243;&#22522;&#20110;&#19977;&#20010;&#36817;&#20046;&#26368;&#20808;&#36827;&#30340; AI &#27169;&#22411;&#65292;&#32467;&#21512;&#25104;&#20102;&#19968;&#20010;&#27969;&#34892;&#27468;&#26354;&#29983;&#25104;&#31995;&#32479;&#12290;&#32463;&#36807;&#24494;&#35843;&#30340; GPT-2 &#27169;&#22411;&#32534;&#20889;&#27468;&#35789;&#65292;Music-VAE &#32452;&#21512;&#38899;&#20048;&#20048;&#35889;&#21644;&#22120;&#20048;&#65292;&#32780; Diffsinger &#21017;&#21512;&#25104;&#21809;&#27468;&#30340;&#22768;&#38899;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#35774;&#35745;&#35838;&#31243;&#25152;&#20570;&#30340;&#20915;&#23450;&#65292;&#22522;&#20110; Piaget &#30340;&#26500;&#36896;&#20027;&#20041;&#8220;&#23398;&#20197;&#33268;&#29992;&#8221;&#30340;&#29702;&#24565;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#20116;&#21608;&#35838;&#31243;&#35774;&#35745;&#30340;&#32454;&#33410;&#65292;&#21253;&#25324;&#23398;&#20064;&#30446;&#26631;&#12289;&#25216;&#26415;&#27010;&#24565;&#20197;&#21450;&#21019;&#24847;&#21644;&#25216;&#26415;&#27963;&#21160;&#12290;&#25105;&#20204;&#35299;&#37322;&#20102;&#22914;&#20309;&#20811;&#26381;&#25216;&#26415;&#25361;&#25112;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#23436;&#25972;&#30340;&#27969;&#34892;&#27468;&#26354;&#29983;&#25104;&#22120;&#31995;&#32479;&#65292;&#30001; Python &#33050;&#26412;&#12289;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#36890;&#36807;&#22522;&#20110; Web &#30340; IDE &#22312; docker &#21270; Linux &#23481;&#22120;&#20013;&#36816;&#34892;&#30340; Javascript &#20195;&#30721;&#32452;&#25104;&#12290;&#36890;&#36807;&#23545;&#23398;&#29983;&#27963;&#21160;&#30340;&#23450;&#37327;&#20998;&#26512;&#25552;&#20379;&#20102;&#21442;&#32771;&#65292;&#24182;&#20026;&#26410;&#26469;&#25913;&#36827;&#25552;&#20379;&#20102;&#22522;&#20934;&#12290;&#19987;&#23478;&#24037;&#20316;&#22346;&#30340;&#23450;&#24615;&#20998;&#26512;&#39564;&#35777;&#20102;&#25972;&#20010;&#35838;&#31243;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article describes and evaluates a new online AI-creativity course. The course is based around three near-state-of-the-art AI models combined into a pop song generating system. A fine-tuned GPT-2 model writes lyrics, Music-VAE composes musical scores and instrumentation and Diffsinger synthesises a singing voice. We explain the decisions made in designing the course which is based on Piagetian, constructivist 'learning-by-doing'. We present details of the five-week course design with learning objectives, technical concepts, and creative and technical activities. We explain how we overcame technical challenges to build a complete pop song generator system, consisting of Python scripts, pre-trained models, and Javascript code that runs in a dockerised Linux container via a web-based IDE. A quantitative analysis of student activity provides evidence on engagement and a benchmark for future improvements. A qualitative analysis of a workshop with experts validated the overall course des
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#22312;&#24212;&#24613;&#21709;&#24212;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20869;&#22312;&#30340;&#22235;&#20010;&#23376;&#38382;&#39064;&#65306;&#20107;&#20214;&#39044;&#27979;&#65292;&#20107;&#20214;&#26816;&#27979;&#65292;&#36164;&#28304;&#20998;&#37197;&#21644;&#36164;&#28304;&#35843;&#24230;&#12290;</title><link>http://arxiv.org/abs/2306.10068</link><description>&lt;p&gt;
&#24212;&#24613;&#21709;&#24212;&#20154;&#24037;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Emergency Response. (arXiv:2306.10068v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#22312;&#24212;&#24613;&#21709;&#24212;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#20869;&#22312;&#30340;&#22235;&#20010;&#23376;&#38382;&#39064;&#65306;&#20107;&#20214;&#39044;&#27979;&#65292;&#20107;&#20214;&#26816;&#27979;&#65292;&#36164;&#28304;&#20998;&#37197;&#21644;&#36164;&#28304;&#35843;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#24613;&#21709;&#24212;&#31649;&#29702;&#26159;&#20840;&#29699;&#31038;&#21306;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;&#24212;&#24613;&#21709;&#24212;&#20154;&#21592;&#24517;&#39035;&#36805;&#36895;&#21709;&#24212;&#21508;&#31181;&#20107;&#20214;&#65292;&#20363;&#22914;&#28779;&#28798;&#12289;&#20132;&#36890;&#20107;&#25925;&#21644;&#21307;&#30103;&#32039;&#24613;&#24773;&#20917;&#12290;&#36943;&#21046;&#20107;&#20214;&#20197;&#26368;&#23567;&#21270;&#20154;&#21592;&#20260;&#20129;&#39118;&#38505;&#12290;&#22240;&#27492;&#65292;&#22312;&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#20154;&#20204;&#23545;&#24212;&#24613;&#20107;&#20214;&#21644;&#21709;&#24212;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#12290;&#29305;&#21035;&#26159;&#65292;&#25968;&#25454;&#39537;&#21160;&#27169;&#22411;&#26377;&#21161;&#20110;&#20943;&#23569;&#20154;&#21592;&#21644;&#36130;&#21153;&#25439;&#22833;&#65292;&#24182;&#25913;&#21892;&#35774;&#35745;&#35268;&#33539;&#12289;&#20132;&#36890;&#27861;&#35268;&#21644;&#23433;&#20840;&#25514;&#26045;&#12290;&#26412;&#25945;&#31243;&#35770;&#25991;&#25506;&#35752;&#20102;&#24212;&#24613;&#21709;&#24212;&#20013;&#30340;&#22235;&#20010;&#23376;&#38382;&#39064;&#65306;&#20107;&#20214;&#39044;&#27979;&#12289;&#20107;&#20214;&#26816;&#27979;&#12289;&#36164;&#28304;&#20998;&#37197;&#21644;&#36164;&#28304;&#35843;&#24230;&#12290;&#25105;&#20204;&#26088;&#22312;&#25552;&#20986;&#36825;&#20123;&#38382;&#39064;&#30340;&#25968;&#23398;&#20844;&#24335;&#21644;&#24191;&#27867;&#26694;&#26550;&#12290;&#25105;&#20204;&#36824;&#20998;&#20139;&#20102;&#19968;&#20221;&#26469;&#33258;&#32654;&#22269;&#19968;&#20010;&#22823;&#37117;&#24066;&#21306;&#30340;&#24320;&#28304;&#65288;&#21512;&#25104;&#65289;&#25968;&#25454;&#65292;&#20197;&#20415;&#20110;&#26410;&#26469;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#30340;&#24212;&#24613;&#21709;&#24212;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
Emergency response management (ERM) is a challenge faced by communities across the globe. First responders must respond to various incidents, such as fires, traffic accidents, and medical emergencies. They must respond quickly to incidents to minimize the risk to human life. Consequently, considerable attention has been devoted to studying emergency incidents and response in the last several decades. In particular, data-driven models help reduce human and financial loss and improve design codes, traffic regulations, and safety measures. This tutorial paper explores four sub-problems within emergency response: incident prediction, incident detection, resource allocation, and resource dispatch. We aim to present mathematical formulations for these problems and broad frameworks for each problem. We also share open-source (synthetic) data from a large metropolitan area in the USA for future work on data-driven emergency response.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#28436;&#31034;&#22914;&#20309;&#21033;&#29992;&#29616;&#26377;&#26041;&#27861;&#21644;&#36719;&#20214;&#24037;&#20855;&#32467;&#21512;&#23884;&#20837;&#25216;&#26415;&#35774;&#35745;&#38754;&#21521;&#31185;&#23398;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#35813;&#26426;&#22120;&#20154;&#33021;&#22815;&#22788;&#29702;&#31185;&#23398;&#25991;&#29486;&#65292;&#25552;&#20379;&#29305;&#23450;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#21021;&#27493;&#30740;&#31350;&#36741;&#21161;&#30693;&#35782;&#26041;&#38754;&#20026;&#29289;&#29702;&#31185;&#23398;&#23478;&#25552;&#20379;&#24110;&#21161;&#12290;</title><link>http://arxiv.org/abs/2306.10067</link><description>&lt;p&gt;
&#21033;&#29992;&#23884;&#20837;&#25216;&#26415;&#35774;&#35745;&#38754;&#21521;&#31185;&#23398;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;
&lt;/p&gt;
&lt;p&gt;
Domain-specific ChatBots for Science using Embeddings. (arXiv:2306.10067v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#28436;&#31034;&#22914;&#20309;&#21033;&#29992;&#29616;&#26377;&#26041;&#27861;&#21644;&#36719;&#20214;&#24037;&#20855;&#32467;&#21512;&#23884;&#20837;&#25216;&#26415;&#35774;&#35745;&#38754;&#21521;&#31185;&#23398;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#35813;&#26426;&#22120;&#20154;&#33021;&#22815;&#22788;&#29702;&#31185;&#23398;&#25991;&#29486;&#65292;&#25552;&#20379;&#29305;&#23450;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#24182;&#22312;&#21021;&#27493;&#30740;&#31350;&#36741;&#21161;&#30693;&#35782;&#26041;&#38754;&#20026;&#29289;&#29702;&#31185;&#23398;&#23478;&#25552;&#20379;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#24050;&#25104;&#20026;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#33021;&#22788;&#29702;&#22810;&#31181;&#20219;&#21153;&#12290;&#32463;&#35843;&#25972;&#30340;&#36825;&#20123;&#31995;&#32479;&#24050;&#34987;&#36716;&#21270;&#20026;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#33021;&#22238;&#31572;&#29992;&#25143;&#23545;&#24191;&#27867;&#35805;&#39064;&#30340;&#26597;&#35810;&#65292;&#25552;&#20379;&#20016;&#23500;&#30340;&#20449;&#24687;&#21644;&#21019;&#24847;&#22238;&#31572;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23427;&#20204;&#22312;&#33258;&#28982;&#31185;&#23398;&#39046;&#22495;&#30340;&#30693;&#35782;&#20173;&#19981;&#23436;&#25972;&#65292;&#24182;&#19988;&#38754;&#20020;&#20005;&#26684;&#38656;&#27714;&#21644;&#26469;&#28304;&#26631;&#20934;&#65292;&#22240;&#27492;&#20854;&#22312;&#29289;&#29702;&#31185;&#23398;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;&#20173;&#21463;&#21040;&#38480;&#21046;&#12290;&#26412;&#25991;&#28436;&#31034;&#20102;&#22914;&#20309;&#36731;&#26494;&#22320;&#23558;&#29616;&#26377;&#26041;&#27861;&#21644;&#36719;&#20214;&#24037;&#20855;&#32467;&#21512;&#36215;&#26469;&#65292;&#23454;&#29616;&#38754;&#21521;&#29305;&#23450;&#39046;&#22495;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#35813;&#31995;&#32479;&#33021;&#25509;&#21463;&#29616;&#26377;&#26684;&#24335;&#30340;&#31185;&#23398;&#25991;&#29486;&#65292;&#24182;&#20351;&#29992;&#25991;&#26412;&#23884;&#20837;&#26597;&#25214;&#26469;&#20026;LLM&#25552;&#20379;&#29305;&#23450;&#39046;&#22495;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#20415;&#22312;&#25776;&#20889;&#22238;&#31572;&#26102;&#20351;&#29992;&#12290;&#25105;&#20204;&#21516;&#26679;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#22270;&#20687;&#23884;&#20837;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#36328;&#20986;&#29256;&#29289;&#22270;&#29255;&#30340;&#25628;&#32034;&#21644;&#26816;&#32034;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#25552;&#20379;&#21021;&#27493;&#30740;&#31350;&#36741;&#21161;&#30693;&#35782;&#26041;&#38754;&#65292;LLM&#24050;&#32463;&#36866;&#29992;&#20110;&#29289;&#29702;&#31185;&#23398;&#23478;&#30340;&#20351;&#29992;&#65292;&#24182;&#19988;&#36827;&#19968;&#27493;&#30340;&#24320;&#21457;&#21487;&#20197;&#25193;&#23637;&#36825;&#20123;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have emerged as powerful machine-learning systems capable of handling a myriad of tasks. Tuned versions of these systems have been turned into chatbots that can respond to user queries on a vast diversity of topics, providing informative and creative replies. However, their application to physical science research remains limited owing to their incomplete knowledge in these areas, contrasted with the needs of rigor and sourcing in science domains. Here, we demonstrate how existing methods and software tools can be easily combined to yield a domain-specific chatbot. The system ingests scientific documents in existing formats, and uses text embedding lookup to provide the LLM with domain-specific contextual information when composing its reply. We similarly demonstrate that existing image embedding methods can be used for search and retrieval across publication figures. These results confirm that LLMs are already suitable for use by physical scientists in acc
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;QM9&#25968;&#25454;&#38598;&#20013;&#65292;&#37319;&#29992;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#25968;&#25454;&#37319;&#26679;&#26041;&#27861;&#26469;&#36873;&#25321;&#26377;&#25928;&#30340;&#35757;&#32451;&#38598;&#65292;&#20877;&#19982;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#26368;&#22823;&#21270;&#20998;&#23376;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10066</link><description>&lt;p&gt;
&#12300;&#23376;&#38598;&#36873;&#25321;&#19982;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#30456;&#20114;&#20316;&#29992;&#12301;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Interplay of Subset Selection and Informed Graph Neural Networks. (arXiv:2306.10066v1 [physics.chem-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10066
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;QM9&#25968;&#25454;&#38598;&#20013;&#65292;&#37319;&#29992;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#25968;&#25454;&#37319;&#26679;&#26041;&#27861;&#26469;&#36873;&#25321;&#26377;&#25928;&#30340;&#35757;&#32451;&#38598;&#65292;&#20877;&#19982;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20197;&#26368;&#22823;&#21270;&#20998;&#23376;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#32467;&#21512;&#28023;&#37327;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#26174;&#33879;&#25552;&#39640;&#20102;&#25105;&#20204;&#25506;&#31350;&#21270;&#21512;&#29289;&#31354;&#38388;&#30340;&#33021;&#21147;&#65292;&#36890;&#36807;&#24555;&#36895;&#20934;&#30830;&#22320;&#39044;&#27979;&#20998;&#23376;&#24615;&#36136;&#12290;&#28982;&#32780;&#65292;&#22823;&#25968;&#25454;&#38598;&#30340;&#23398;&#20064;&#21463;&#21040;&#35745;&#31639;&#36164;&#28304;&#30340;&#38480;&#21046;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#21487;&#33021;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#27492;&#22806;&#65292;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#21487;&#33021;&#36824;&#27809;&#26377;&#34987;&#26631;&#35760;&#65292;&#29983;&#25104;&#26631;&#35760;&#21487;&#33021;&#25104;&#26412;&#39640;&#26114;&#65292;&#20363;&#22914;&#37327;&#23376;&#21270;&#23398;&#35745;&#31639;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#20174;&#22823;&#22411;&#26410;&#26631;&#35760;&#25968;&#25454;&#28857;&#27744;&#20013;&#36873;&#25321;&#23567;&#35757;&#32451;&#23376;&#38598;&#65292;&#24182;&#24320;&#21457;&#21487;&#38752;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#20174;&#23567;&#35757;&#32451;&#38598;&#20013;&#23398;&#20064;&#12290;&#26412;&#25991;&#38598;&#20013;&#20110;&#39044;&#27979; QM9 &#25968;&#25454;&#38598;&#20013;&#20998;&#23376;&#30340;&#21407;&#23376;&#21270;&#33021;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#37319;&#29992;&#22522;&#20110;&#39046;&#22495;&#30693;&#35782;&#30340;&#25968;&#25454;&#37319;&#26679;&#26041;&#27861;&#26469;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#38598;&#36873;&#25321;&#19982;&#36890;&#30693;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30456;&#32467;&#21512;&#30340;&#20248;&#21183;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#26368;&#22823;&#21270;&#20998;&#23376;&#30340;&#22810;&#26679;&#24615;&#21644;&#30456;&#20851;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20449;&#24687;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#38376;&#20026;&#20998;&#23376;&#25968;&#25454;&#35774;&#35745;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning techniques paired with the availability of massive datasets dramatically enhance our ability to explore the chemical compound space by providing fast and accurate predictions of molecular properties. However, learning on large datasets is strongly limited by the availability of computational resources and can be infeasible in some scenarios. Moreover, the instances in the datasets may not yet be labelled and generating the labels can be costly, as in the case of quantum chemistry computations. Thus, there is a need to select small training subsets from large pools of unlabelled data points and to develop reliable ML methods that can effectively learn from small training sets. This work focuses on predicting the molecules atomization energy in the QM9 dataset. We investigate the advantages of employing domain knowledge-based data sampling methods for an efficient training set selection combined with informed ML techniques. In particular, we show how maximizing molecular
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-Conductor&#65292;&#19968;&#31181;&#22522;&#20110;DDIM&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#39537;&#21160;&#30340;&#25351;&#25381;&#36816;&#21160;&#29983;&#25104;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#24182;&#24212;&#29992;&#38543;&#26426;&#23631;&#34109;&#31574;&#30053;&#21644;&#19968;&#23545;&#20960;&#20309;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#36816;&#21160;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10065</link><description>&lt;p&gt;
&#39535;&#26381;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#38899;&#20048;&#39537;&#21160;&#30340;&#25351;&#25381;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Taming Diffusion Models for Music-driven Conducting Motion Generation. (arXiv:2306.10065v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-Conductor&#65292;&#19968;&#31181;&#22522;&#20110;DDIM&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#39537;&#21160;&#30340;&#25351;&#25381;&#36816;&#21160;&#29983;&#25104;&#65292;&#21033;&#29992;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#24182;&#24212;&#29992;&#38543;&#26426;&#23631;&#34109;&#31574;&#30053;&#21644;&#19968;&#23545;&#20960;&#20309;&#25439;&#22833;&#20989;&#25968;&#26469;&#25552;&#39640;&#36816;&#21160;&#22810;&#26679;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20132;&#21709;&#20048;&#20013;&#29983;&#25104;&#25351;&#25381;&#23478;&#30340;&#21160;&#20316;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#38656;&#35201;&#23398;&#20064;&#35821;&#20041;&#38899;&#20048;&#29305;&#24449;&#24182;&#25429;&#25417;&#30495;&#23454;&#25351;&#25381;&#21160;&#20316;&#30340;&#28508;&#22312;&#20998;&#24067;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24050;&#32463;&#23558;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#24212;&#29992;&#20110;&#27492;&#20219;&#21153;&#65292;&#20294;&#21069;&#26223;&#20809;&#36861;&#36857;&#30340;&#27169;&#22411;&#22312;&#35757;&#32451;&#31283;&#23450;&#24615;&#21644;&#36755;&#20986;&#36136;&#37327;&#26041;&#38754;&#26174;&#31034;&#20986;&#20248;&#21183;&#65292;&#20294;&#22312;&#27492;&#19978;&#19979;&#25991;&#20013;&#36824;&#26410;&#34987;&#21033;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Diffusion-Conductor&#65292;&#19968;&#31181;&#22522;&#20110;DDIM&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#38899;&#20048;&#39537;&#21160;&#30340;&#25351;&#25381;&#36816;&#21160;&#29983;&#25104;&#65292;&#23558;&#25193;&#25955;&#27169;&#22411;&#25972;&#21512;&#21040;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#38543;&#26426;&#23631;&#34109;&#31574;&#30053;&#20197;&#25552;&#39640;&#29305;&#24449;&#30340;&#40065;&#26834;&#24615;&#65292;&#24182;&#20351;&#29992;&#19968;&#23545;&#20960;&#20309;&#25439;&#22833;&#20989;&#25968;&#26045;&#21152;&#38468;&#21152;&#35268;&#21017;&#21270;&#21644;&#22686;&#21152;&#36816;&#21160;&#22810;&#26679;&#24615;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;&#20960;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#21253;&#25324;Frechet Gesture Distance&#65288;FGD&#65289;&#21644;Beat Consistency Score&#65288;BC&#65289;&#65292;&#20197;&#36827;&#34892;&#26356;&#20840;&#38754;&#30340;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generating the motion of orchestral conductors from a given piece of symphony music is a challenging task since it requires a model to learn semantic music features and capture the underlying distribution of real conducting motion. Prior works have applied Generative Adversarial Networks (GAN) to this task, but the promising diffusion model, which recently showed its advantages in terms of both training stability and output quality, has not been exploited in this context. This paper presents Diffusion-Conductor, a novel DDIM-based approach for music-driven conducting motion generation, which integrates the diffusion model to a two-stage learning framework. We further propose a random masking strategy to improve the feature robustness, and use a pair of geometric loss functions to impose additional regularizations and increase motion diversity. We also design several novel metrics, including Frechet Gesture Distance (FGD) and Beat Consistency Score (BC) for a more comprehensive evaluati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#26500;&#24314;&#38754;&#21521;&#25945;&#32946;&#30340;&#31038;&#20132;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#38656;&#35201;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#24182;&#20855;&#22791;&#23545;&#20114;&#32852;&#32593;&#36164;&#28304;&#30340;&#35775;&#38382;&#21644;&#36129;&#29486;&#33021;&#21147;&#65292;&#20294;&#21516;&#26102;&#36824;&#38656;&#35201;&#35748;&#35782;&#21040;&#33258;&#24049;&#30340;&#23616;&#38480;&#24615;&#65292;&#23545;&#23398;&#29983;&#36127;&#36131;&#24182;&#23562;&#37325;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;</title><link>http://arxiv.org/abs/2306.10063</link><description>&lt;p&gt;
&#38754;&#21521;&#25945;&#32946;&#30340;&#31038;&#20132;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65306;&#29702;&#35770;&#65292;&#23454;&#36341;&#21644;&#20262;&#29702;&#23398;
&lt;/p&gt;
&lt;p&gt;
Towards social generative AI for education: theory, practices and ethics. (arXiv:2306.10063v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10063
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#22914;&#20309;&#26500;&#24314;&#38754;&#21521;&#25945;&#32946;&#30340;&#31038;&#20132;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#38656;&#35201;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#65292;&#24182;&#20855;&#22791;&#23545;&#20114;&#32852;&#32593;&#36164;&#28304;&#30340;&#35775;&#38382;&#21644;&#36129;&#29486;&#33021;&#21147;&#65292;&#20294;&#21516;&#26102;&#36824;&#38656;&#35201;&#35748;&#35782;&#21040;&#33258;&#24049;&#30340;&#23616;&#38480;&#24615;&#65292;&#23545;&#23398;&#29983;&#36127;&#36131;&#24182;&#23562;&#37325;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20154;&#19982;&#20154;&#24037;&#26234;&#33021;&#20043;&#38388;&#30340;&#25945;&#32946;&#20132;&#20114;&#65292;&#19981;&#26159;&#20316;&#20026;&#25552;&#31034;&#21644;&#21709;&#24212;&#30340;&#24207;&#21015;&#65292;&#32780;&#26159;&#20316;&#20026;&#23545;&#35805;&#21644;&#25506;&#32034;&#30340;&#31038;&#20132;&#36807;&#31243;&#12290;&#22312;&#36825;&#31181;&#26500;&#24605;&#19979;&#65292;&#23398;&#20064;&#32773;&#22312;&#20114;&#32852;&#32593;&#24037;&#20855;&#21644;&#36164;&#28304;&#30340;&#21160;&#24577;&#35745;&#31639;&#29615;&#22659;&#20013;&#19981;&#26029;&#19982;AI&#35821;&#35328;&#27169;&#22411;&#20132;&#27969;&#12290;&#24403;&#36825;&#20010;&#20998;&#24067;&#24335;&#31995;&#32479;&#35774;&#23450;&#30446;&#26631;&#12289;&#20174;&#25968;&#25454;&#20013;&#24314;&#31435;&#24847;&#20041;&#12289;&#24041;&#22266;&#29702;&#35299;&#12289;&#35843;&#21644;&#24046;&#24322;&#24182;&#23558;&#30693;&#35782;&#20256;&#36882;&#21040;&#26032;&#30340;&#39046;&#22495;&#26102;&#65292;&#23398;&#20064;&#23601;&#20250;&#21457;&#29983;&#12290;&#26500;&#24314;&#38754;&#21521;&#25945;&#32946;&#30340;&#31038;&#20132;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#38656;&#35201;&#24320;&#21457;&#24378;&#22823;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#36825;&#20123;&#31995;&#32479;&#21487;&#20197;&#24444;&#27492;&#20132;&#35848;&#65292;&#26500;&#24314;&#22806;&#37096;&#34920;&#31034;&#65288;&#22914;&#30693;&#35782;&#22320;&#22270;&#65289;&#65292;&#35775;&#38382;&#21644;&#36129;&#29486;&#20114;&#32852;&#32593;&#36164;&#28304;&#65292;&#24182;&#25285;&#20219;&#25945;&#24072;&#12289;&#23398;&#20064;&#32773;&#12289;&#23548;&#24072;&#21644;&#25351;&#23548;&#12290;&#36825;&#24341;&#20986;&#20102;&#22522;&#26412;&#30340;&#20262;&#29702;&#38382;&#39064;&#12290;&#36825;&#20123;&#31995;&#32479;&#24212;&#35813;&#35748;&#35782;&#21040;&#20182;&#20204;&#30340;&#23616;&#38480;&#24615;&#65292;&#23545;&#23398;&#20064;&#32773;&#21644;&#20114;&#32852;&#32593;&#30340;&#23436;&#25972;&#24615;&#36127;&#36131;&#65292;&#24182;&#23562;&#37325;&#20154;&#31867;&#20215;&#20540;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores educational interactions involving humans and artificial intelligences not as sequences of prompts and responses, but as a social process of conversation and exploration. In this conception, learners continually converse with AI language models within a dynamic computational medium of internet tools and resources. Learning happens when this distributed system sets goals, builds meaning from data, consolidates understanding, reconciles differences, and transfers knowledge to new domains. Building social generative AI for education will require development of powerful AI systems that can converse with each other as well as humans, construct external representations such as knowledge maps, access and contribute to internet resources, and act as teachers, learners, guides and mentors. This raises fundamental problems of ethics. Such systems should be aware of their limitations, their responsibility to learners and the integrity of the internet, and their respect for hum
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#32467;&#26500;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#19981;&#26159;&#21333;&#19968;&#33021;&#21147;&#65292;&#32780;&#26159;&#30001;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#26680;&#24515;&#35821;&#35328;&#24314;&#27169;&#31561;&#19977;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#32032;&#32452;&#25104;&#65292;&#24182;&#19988;&#36825;&#19977;&#20010;&#33021;&#21147;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#20013;&#30340;&#22823;&#37096;&#20998;&#26041;&#24046;&#12290;</title><link>http://arxiv.org/abs/2306.10062</link><description>&lt;p&gt;
&#25581;&#31034;&#35821;&#35328;&#27169;&#22411;&#33021;&#21147;&#30340;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Revealing the structure of language model capabilities. (arXiv:2306.10062v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#32467;&#26500;&#65292;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#19981;&#26159;&#21333;&#19968;&#33021;&#21147;&#65292;&#32780;&#26159;&#30001;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#26680;&#24515;&#35821;&#35328;&#24314;&#27169;&#31561;&#19977;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#32032;&#32452;&#25104;&#65292;&#24182;&#19988;&#36825;&#19977;&#20010;&#33021;&#21147;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#20013;&#30340;&#22823;&#37096;&#20998;&#26041;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24314;&#31435;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#29702;&#35770;&#29702;&#35299;&#23545;&#20110;&#25105;&#20204;&#39044;&#27979;&#21644;&#35299;&#37322;&#36825;&#20123;&#31995;&#32479;&#30340;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#36890;&#36807;&#20174;&#21508;&#31181;LLMs&#30340;&#20010;&#20307;&#24046;&#24322;&#27169;&#24335;&#20013;&#25552;&#21462;&#28508;&#22312;&#33021;&#21147;&#26469;&#35843;&#26597;LLMs&#33021;&#21147;&#30340;&#32467;&#26500;&#12290;&#20351;&#29992;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#22240;&#23376;&#20998;&#26512;&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#26469;&#33258;29&#20010;&#19981;&#21516;LLMs&#30340;27&#31181;&#35748;&#30693;&#20219;&#21153;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;LLMs&#33021;&#21147;&#24182;&#38750;&#21333;&#19968;&#30340;&#65292;&#30456;&#21453;&#65292;&#23427;&#20204;&#26356;&#22909;&#22320;&#30001;&#19977;&#20010;&#26126;&#30830;&#23450;&#20041;&#30340;&#22240;&#32032;&#35299;&#37322;&#65292;&#20998;&#21035;&#20195;&#34920;&#25512;&#29702;&#12289;&#29702;&#35299;&#21644;&#26680;&#24515;&#35821;&#35328;&#24314;&#27169;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#19977;&#20010;&#22240;&#32032;&#21487;&#20197;&#35299;&#37322;&#27169;&#22411;&#24615;&#33021;&#20013;&#30340;&#39640;&#27604;&#20363;&#26041;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#25581;&#31034;&#20102;&#19981;&#21516;LLMs&#33021;&#21147;&#30340;&#19968;&#33268;&#32467;&#26500;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#33021;&#21147;&#30340;&#22810;&#26041;&#38754;&#24615;&#36136;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#36825;&#19977;&#20010;&#21151;&#33021;&#19982;&#27169;&#22411;&#23646;&#24615;&#20855;&#26377;&#19981;&#21516;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building a theoretical understanding of the capabilities of large language models (LLMs) is vital for our ability to predict and explain the behavior of these systems. Here, we investigate the structure of LLM capabilities by extracting latent capabilities from patterns of individual differences across a varied population of LLMs. Using a combination of Bayesian and frequentist factor analysis, we analyzed data from 29 different LLMs across 27 cognitive tasks. We found evidence that LLM capabilities are not monolithic. Instead, they are better explained by three well-delineated factors that represent reasoning, comprehension and core language modeling. Moreover, we found that these three factors can explain a high proportion of the variance in model performance. These results reveal a consistent structure in the capabilities of different LLMs and demonstrate the multifaceted nature of these capabilities. We also found that the three abilities show different relationships to model prope
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102; OASIS 2 &#26412;&#20307;&#35770;&#65292;&#19968;&#31181;&#20026;&#20195;&#29702;&#25552;&#20379;&#35821;&#20041;&#34920;&#31034;&#21644;&#36890;&#20449;&#30340;&#34892;&#20026;&#20027;&#20041;&#26041;&#27861;&#12290;&#35813;&#26412;&#20307;&#35770;&#24050;&#24212;&#29992;&#20110;&#21306;&#22359;&#38142;&#21450;&#20854;&#20182;&#39046;&#22495;&#12290;</title><link>http://arxiv.org/abs/2306.10061</link><description>&lt;p&gt;
&#20195;&#29702;&#12289;&#31995;&#32479;&#21644;&#26381;&#21153;&#38598;&#25104;&#30340;&#26412;&#20307;&#35770;&#65306;OASIS 2 &#29256;&#26412;
&lt;/p&gt;
&lt;p&gt;
The Ontology for Agents, Systems and Integration of Services: OASIS version 2. (arXiv:2306.10061v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102; OASIS 2 &#26412;&#20307;&#35770;&#65292;&#19968;&#31181;&#20026;&#20195;&#29702;&#25552;&#20379;&#35821;&#20041;&#34920;&#31034;&#21644;&#36890;&#20449;&#30340;&#34892;&#20026;&#20027;&#20041;&#26041;&#27861;&#12290;&#35813;&#26412;&#20307;&#35770;&#24050;&#24212;&#29992;&#20110;&#21306;&#22359;&#38142;&#21450;&#20854;&#20182;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#34920;&#31034;&#26159;&#22810;&#20010;&#24212;&#29992;&#39046;&#22495;&#30340;&#20851;&#38190;&#26041;&#27861;&#65292;&#20854;&#20013;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#39046;&#22495;&#20063;&#19981;&#20363;&#22806;&#12290;&#22312;&#20026;&#20195;&#29702;&#36827;&#34892;&#35821;&#20041;&#34920;&#31034;&#30340;&#26041;&#27861;&#20013;&#65292;&#19968;&#20010;&#22522;&#26412;&#23454;&#29616;&#25163;&#27573;&#26159;&#37319;&#21462;&#34892;&#20026;&#20027;&#20041;&#35270;&#35282;&#65292;&#36890;&#36807;&#25551;&#36848;&#26234;&#33021;&#20307;&#22914;&#20309;&#25805;&#20316;&#24182;&#19982;&#20854;&#21516;&#34892;&#20114;&#21160;&#12290;&#36825;&#31181;&#26041;&#27861;&#20027;&#35201;&#26088;&#22312;&#36890;&#36807;&#19982;&#20219;&#21153;&#23436;&#25104;&#30456;&#20851;&#30340;&#24515;&#29702;&#29366;&#24577;&#26469;&#23450;&#20041;&#20195;&#29702;&#30340;&#25805;&#20316;&#33021;&#21147;&#12290;OASIS &#26412;&#20307;&#35770;&#65288;&#19968;&#31181;&#20195;&#29702;&#12289;&#31995;&#32479;&#21644;&#26381;&#21153;&#38598;&#25104;&#30340;&#26412;&#20307;&#35770;&#65289;&#65292;&#20110;2019&#24180;&#25552;&#20986;&#20102;&#36825;&#31181;&#34892;&#20026;&#20027;&#20041;&#26041;&#27861;&#65292;&#20197;&#25552;&#20379;&#26234;&#33021;&#20195;&#29702;&#21450;&#20854;&#25215;&#35834;&#30340;&#35821;&#20041;&#34920;&#31034;&#31995;&#32479;&#21644;&#36890;&#20449;&#21327;&#35758;&#12290;&#26412;&#25991;&#25253;&#21578;&#20102; OASIS 2 &#20013;&#26377;&#20851;&#20195;&#29702;&#34920;&#31034;&#30340;&#20027;&#35201;&#24314;&#27169;&#36873;&#25321;&#65292;&#36825;&#26159; OASIS &#30340;&#26368;&#26032;&#37325;&#22823;&#21319;&#32423;&#29256;&#26412;&#65292;&#24182;&#20171;&#32461;&#20102;&#33258;&#20854;&#39318;&#27425;&#24341;&#20837;&#20197;&#26469;&#26412;&#20307;&#35770;&#22312;&#21306;&#22359;&#38142;&#26412;&#20307;&#35770;&#31561;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#21450;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic representation is a key enabler for several application domains, and the multi-agent systems realm makes no exception. Among the methods for semantically representing agents, one has been essentially achieved by taking a behaviouristic vision, through which one can describe how they operate and engage with their peers. The approach essentially aims at defining the operational capabilities of agents through the mental states related with the achievement of tasks. The OASIS ontology -- An Ontology for Agent, Systems, and Integration of Services, presented in 2019 -- pursues the behaviouristic approach to deliver a semantic representation system and a communication protocol for agents and their commitments. This paper reports on the main modeling choices concerning the representation of agents in OASIS 2, the latest major upgrade of OASIS, and the achievement reached by the ontology since it was first introduced, in particular in the context of ontologies for blockchains.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25551;&#36848;&#21644;&#35780;&#20272;&#20102;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#33258;&#21160;&#21019;&#24314;&#36807;&#28193;&#26230;&#26684;&#21333;&#20803;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26230;&#26684;&#32467;&#26500;&#20013;&#19981;&#21516;&#25299;&#25169;&#30340;&#21333;&#20803;&#26230;&#26684;&#20043;&#38388;&#24179;&#28369;&#36716;&#25442;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.10055</link><description>&lt;p&gt;
&#24179;&#28369;&#31895;&#31961;&#30340;&#36793;&#32536;&#65306;&#35780;&#20272;&#33258;&#21160;&#29983;&#25104;&#30340;&#22797;&#21512;&#26230;&#26684;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Smoothing the Rough Edges: Evaluating Automatically Generated Multi-Lattice Transitions. (arXiv:2306.10055v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25551;&#36848;&#21644;&#35780;&#20272;&#20102;&#20351;&#29992;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#33258;&#21160;&#21019;&#24314;&#36807;&#28193;&#26230;&#26684;&#21333;&#20803;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#22810;&#26230;&#26684;&#32467;&#26500;&#20013;&#19981;&#21516;&#25299;&#25169;&#30340;&#21333;&#20803;&#26230;&#26684;&#20043;&#38388;&#24179;&#28369;&#36716;&#25442;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28155;&#21152;&#21046;&#36896;&#25216;&#26415;&#22312;&#28385;&#36275;&#22797;&#26434;&#35774;&#35745;&#35201;&#27714;&#30340;&#21516;&#26102;&#65292;&#29983;&#20135;&#36731;&#37327;&#32423;&#30340;&#37096;&#20214;&#19978;&#20855;&#26377;&#38750;&#24120;&#22823;&#30340;&#20248;&#21183;&#12290;&#24341;&#20837;&#21333;&#20803;&#26230;&#26684;&#21333;&#20803;&#21644;&#36825;&#20123;&#21333;&#20803;&#30340;&#28176;&#21464;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#36825;&#31181;&#33021;&#21147;&#12290;&#22312;&#37096;&#20214;&#21152;&#36733;&#22312;&#19981;&#21516;&#24773;&#20917;&#19979;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#20351;&#29992;&#22810;&#20010;&#19981;&#21516;&#30340;&#21333;&#20803;&#26230;&#26684;&#31867;&#22411;&#21487;&#33021;&#26159;&#26377;&#30410;&#30340;&#65292;&#36825;&#23558;&#23548;&#33268;&#22810;&#26230;&#26684;&#32467;&#26500;&#12290;&#22312;&#36825;&#31181;&#32467;&#26500;&#20013;&#65292;&#21333;&#20803;&#26230;&#26684;&#25299;&#25169;&#20043;&#38388;&#30340;&#31361;&#28982;&#36716;&#25442;&#21487;&#33021;&#20250;&#23548;&#33268;&#24212;&#21147;&#38598;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Additive manufacturing is advantageous for producing lightweight components while addressing complex design requirements. This capability has been bolstered by the introduction of unit lattice cells and the gradation of those cells. In cases where loading varies throughout a part, it may be beneficial to use multiple, distinct lattice cell types, resulting in multi-lattice structures. In such structures, abrupt transitions between unit cell topologies may cause stress concentrations, making the boundary between unit cell types a primary failure point. Thus, these regions require careful design in order to ensure the overall functionality of the part. Although computational design approaches have been proposed, smooth transition regions are still difficult to achieve, especially between lattices of drastically different topologies. This work demonstrates and assesses a method for using variational autoencoders to automate the creation of transitional lattice cells, examining the factors
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#31361;&#30772;&#20102;&#33402;&#26415;&#12289;&#38899;&#20048;&#21644;&#23186;&#20307;&#39046;&#22495;&#65292;&#24341;&#21457;&#20102;&#25991;&#21270;&#36716;&#21464;&#12290;&#23427;&#36890;&#36807;&#25913;&#21464;&#20154;&#20204;&#30340;&#35282;&#33394;&#12289;&#36716;&#21464;&#20215;&#20540;&#35266;&#20197;&#21450;&#25361;&#25112;&#20256;&#32479;&#23454;&#36341;&#26041;&#24335;&#65292;&#20026;&#33402;&#26415;&#30340;&#26410;&#26469;&#25171;&#24320;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10054</link><description>&lt;p&gt;
&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#24341;&#36215;&#30340;&#33402;&#26415;&#23454;&#36341;&#30340;&#36716;&#21464;
&lt;/p&gt;
&lt;p&gt;
A Shift In Artistic Practices through Artificial Intelligence. (arXiv:2306.10054v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10054
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#20869;&#23481;&#31361;&#30772;&#20102;&#33402;&#26415;&#12289;&#38899;&#20048;&#21644;&#23186;&#20307;&#39046;&#22495;&#65292;&#24341;&#21457;&#20102;&#25991;&#21270;&#36716;&#21464;&#12290;&#23427;&#36890;&#36807;&#25913;&#21464;&#20154;&#20204;&#30340;&#35282;&#33394;&#12289;&#36716;&#21464;&#20215;&#20540;&#35266;&#20197;&#21450;&#25361;&#25112;&#20256;&#32479;&#23454;&#36341;&#26041;&#24335;&#65292;&#20026;&#33402;&#26415;&#30340;&#26410;&#26469;&#25171;&#24320;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29983;&#25104;&#30340;&#22823;&#37327;&#20869;&#23481;&#30340;&#29190;&#28856;&#24341;&#21457;&#20102;&#33402;&#26415;&#12289;&#38899;&#20048;&#21644;&#23186;&#20307;&#39046;&#22495;&#30340;&#25991;&#21270;&#36716;&#21464;&#65292;&#35282;&#33394;&#21464;&#21270;&#12289;&#20215;&#20540;&#35266;&#36716;&#21464;&#21644;&#20256;&#32479;&#21463;&#21040;&#25361;&#25112;&#12290;&#20114;&#32852;&#32593;&#19978;&#21487;&#33719;&#24471;&#30340;&#24191;&#38420;&#25968;&#25454;&#38598;&#20026;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#35757;&#32451;&#21019;&#36896;&#20102;&#19968;&#20010;&#29615;&#22659;&#12290;AI&#27169;&#22411;&#30340;&#20844;&#24320;&#20849;&#20139;&#21644;&#20840;&#29699;&#20351;&#29992;&#65292;&#22914;&#20309;&#25361;&#25112;&#33402;&#26415;&#23454;&#36341;&#20013;&#30340;&#29616;&#29366;&#65311;AI&#25216;&#26415;&#23558;&#32473;&#38899;&#20048;&#12289;&#33402;&#26415;&#21644;&#26032;&#23186;&#20307;&#24102;&#26469;&#20160;&#20040;&#26679;&#30340;&#21464;&#38761;&#65311;
&lt;/p&gt;
&lt;p&gt;
The explosion of content generated by Artificial Intelligence models has initiated a cultural shift in arts, music, and media, where roles are changing, values are shifting, and conventions are challenged. The readily available, vast dataset of the internet has created an environment for AI models to be trained on any content on the web. With AI models shared openly, and used by many, globally, how does this new paradigm shift challenge the status quo in artistic practices? What kind of changes will AI technology bring into music, arts, and new media?
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#19977;&#22823;&#20851;&#38190;&#29305;&#24449;&#8212;&#8212;&#22270;&#24418;&#27880;&#24847;&#21147;&#12289;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#38754;&#21521;NFT&#30340;&#22810;&#27880;&#24847;&#21147;&#25512;&#33616;&#31995;&#32479;(NFT-MARS)&#65292;&#20197;&#35299;&#20915;NFT&#24066;&#22330;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2306.10053</link><description>&lt;p&gt;
NFT&#21040;MARS&#65306;&#38754;&#21521;NFT&#30340;&#22810;&#27880;&#24847;&#21147;&#25512;&#33616;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
NFTs to MARS: Multi-Attention Recommender System for NFTs. (arXiv:2306.10053v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10053
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#20855;&#26377;&#19977;&#22823;&#20851;&#38190;&#29305;&#24449;&#8212;&#8212;&#22270;&#24418;&#27880;&#24847;&#21147;&#12289;&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#21644;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#38754;&#21521;NFT&#30340;&#22810;&#27880;&#24847;&#21147;&#25512;&#33616;&#31995;&#32479;(NFT-MARS)&#65292;&#20197;&#35299;&#20915;NFT&#24066;&#22330;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#24050;&#25104;&#20026;&#22686;&#24378;&#21508;&#20010;&#39046;&#22495;&#29992;&#25143;&#20307;&#39564;&#30340;&#24517;&#22791;&#24037;&#20855;&#12290;&#23613;&#31649;&#38024;&#23545;&#30005;&#24433;&#12289;&#38899;&#20048;&#21644;&#30005;&#23376;&#21830;&#21153;&#30340;&#25512;&#33616;&#31995;&#32479;&#24050;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26085;&#30410;&#22686;&#38271;&#21644;&#32463;&#27982;&#24847;&#20041;&#37325;&#22823;&#30340;&#38750;&#21516;&#36136;&#21270;&#20195;&#24065;&#65288;NFT&#65289;&#24066;&#22330;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;NFT&#24066;&#22330;&#30340;&#29420;&#29305;&#29305;&#24615;&#21644;&#26085;&#30410;&#31361;&#20986;&#30340;&#22320;&#20301;&#20984;&#26174;&#20102;&#24320;&#21457;&#19987;&#38376;&#38024;&#23545;&#20854;&#38656;&#27714;&#30340;&#23450;&#21046;&#25512;&#33616;&#31995;&#32479;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25581;&#31034;&#20854;&#20805;&#20998;&#28508;&#21147;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;NFT&#30340;&#29420;&#29305;&#29305;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#39318;&#20010;&#19987;&#38376;&#35774;&#35745;&#20197;&#24212;&#23545;NFT&#24066;&#22330;&#25361;&#25112;&#30340;&#25512;&#33616;&#31995;&#32479;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;NFT&#30340;&#22810;&#27880;&#24847;&#21147;&#25512;&#33616;&#31995;&#32479;(NFT-MARS)&#65292;&#20855;&#26377;&#19977;&#20010;&#20851;&#38190;&#29305;&#24449;&#65306;(1)&#22270;&#24418;&#27880;&#24847;&#21147;&#20197;&#22788;&#29702;&#31232;&#30095;&#30340;&#29992;&#25143;-&#39033;&#30446;&#20132;&#20114;;(2)&#22810;&#27169;&#24577;&#27880;&#24847;&#21147;&#20197;&#34701;&#20837;&#29992;&#25143;&#30340;&#29305;&#24449;&#20559;&#22909;;(3)&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#32771;&#34385;NFT&#20316;&#20026;&#33402;&#26415;&#20316;&#21697;&#21644;&#25968;&#23383;&#36164;&#20135;&#30340;&#21452;&#37325;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems have become essential tools for enhancing user experiences across various domains. While extensive research has been conducted on recommender systems for movies, music, and e-commerce, the rapidly growing and economically significant Non-Fungible Token (NFT) market remains underexplored. The unique characteristics and increasing prominence of the NFT market highlight the importance of developing tailored recommender systems to cater to its specific needs and unlock its full potential. In this paper, we examine the distinctive characteristics of NFTs and propose the first recommender system specifically designed to address NFT market challenges. In specific, we develop a Multi-Attention Recommender System for NFTs (NFT-MARS) with three key characteristics: (1) graph attention to handle sparse user-item interactions, (2) multi-modal attention to incorporate feature preference of users, and (3) multi-task learning to consider the dual nature of NFTs as both artwork and
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19971;&#31181;&#21033;&#29992; AI &#22312;&#35838;&#22530;&#19978;&#30340;&#26041;&#27861;&#65306;AI-&#23548;&#24072;&#12289;AI-&#36741;&#23548;&#21592;&#12289;AI-&#25351;&#23548;&#21592;&#12289;AI-&#38431;&#21451;&#12289;AI-&#24037;&#20855;&#12289;AI-&#27169;&#25311;&#22120;&#21644;AI-&#23398;&#29983;&#12290;&#36825;&#20123;&#31574;&#30053;&#20419;&#36827;&#23545; AI &#36755;&#20986;&#30340;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#20197;&#21450;&#21033;&#29992; AI &#30340;&#33021;&#21147;&#21644;&#23398;&#29983;&#29420;&#29305;&#35265;&#35299;&#30340;&#20114;&#34917;&#24615;&#65292;&#22686;&#24378;&#23398;&#20064;&#32467;&#26524;&#12290;&#27492;&#26694;&#26550;&#20026;&#25972;&#21512; AI &#36741;&#21161;&#23398;&#20064;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2306.10052</link><description>&lt;p&gt;
&#20998;&#37197; AI&#65306;&#20026;&#23398;&#29983;&#25552;&#20379;&#30340;&#19971;&#31181;&#26041;&#27861;&#21644;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assigning AI: Seven Approaches for Students, with Prompts. (arXiv:2306.10052v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10052
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19971;&#31181;&#21033;&#29992; AI &#22312;&#35838;&#22530;&#19978;&#30340;&#26041;&#27861;&#65306;AI-&#23548;&#24072;&#12289;AI-&#36741;&#23548;&#21592;&#12289;AI-&#25351;&#23548;&#21592;&#12289;AI-&#38431;&#21451;&#12289;AI-&#24037;&#20855;&#12289;AI-&#27169;&#25311;&#22120;&#21644;AI-&#23398;&#29983;&#12290;&#36825;&#20123;&#31574;&#30053;&#20419;&#36827;&#23545; AI &#36755;&#20986;&#30340;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#20197;&#21450;&#21033;&#29992; AI &#30340;&#33021;&#21147;&#21644;&#23398;&#29983;&#29420;&#29305;&#35265;&#35299;&#30340;&#20114;&#34917;&#24615;&#65292;&#22686;&#24378;&#23398;&#20064;&#32467;&#26524;&#12290;&#27492;&#26694;&#26550;&#20026;&#25972;&#21512; AI &#36741;&#21161;&#23398;&#20064;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25945;&#32946;&#20013;&#30340;&#21464;&#38761;&#20316;&#29992;&#21450;&#20854;&#20316;&#20026;&#23398;&#20064;&#24037;&#20855;&#30340;&#28508;&#21147;&#65292;&#23613;&#31649;&#23384;&#22312;&#22266;&#26377;&#30340;&#39118;&#38505;&#21644;&#38480;&#21046;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19971;&#31181;&#21033;&#29992; AI &#22312;&#35838;&#22530;&#19978;&#30340;&#26041;&#27861;&#65306;AI-&#23548;&#24072;&#12289;AI-&#36741;&#23548;&#21592;&#12289;AI-&#25351;&#23548;&#21592;&#12289;AI-&#38431;&#21451;&#12289;AI-&#24037;&#20855;&#12289;AI-&#27169;&#25311;&#22120;&#21644;AI-&#23398;&#29983;&#65292;&#27599;&#31181;&#26041;&#27861;&#37117;&#26377;&#29420;&#29305;&#30340;&#25945;&#23398;&#30410;&#22788;&#21644;&#39118;&#38505;&#12290;&#26088;&#22312;&#24110;&#21161;&#23398;&#29983;&#23398;&#20064;&#19982;&#20102;&#35299; AI&#65292;&#25552;&#20379;&#23454;&#29992;&#30340;&#31574;&#30053;&#26469;&#32531;&#35299;&#39118;&#38505;&#65292;&#22914;&#23545; AI &#36755;&#20986;&#30340;&#28385;&#36275;&#24863;&#12289;&#38169;&#35823;&#21644;&#20559;&#35265;&#12290;&#36825;&#20123;&#31574;&#30053;&#20419;&#36827;&#20027;&#21160;&#30417;&#30563;&#12289;&#23545; AI &#36755;&#20986;&#30340;&#25209;&#21028;&#24615;&#35780;&#20272;&#65292;&#20197;&#21450;&#21033;&#29992; AI &#30340;&#33021;&#21147;&#19982;&#23398;&#29983;&#29420;&#29305;&#35265;&#35299;&#30340;&#20114;&#34917;&#24615;&#12290;&#36890;&#36807;&#25361;&#25112;&#23398;&#29983;&#20445;&#25345;&#8220;&#20154;&#22312;&#29615;&#33410;&#8221;&#65292;&#20316;&#32773;&#26088;&#22312;&#25552;&#39640;&#23398;&#20064;&#32467;&#26524;&#65292;&#21516;&#26102;&#30830;&#20445; AI &#20316;&#20026;&#25903;&#25345;&#24615;&#24037;&#20855;&#65292;&#32780;&#38750;&#26367;&#20195;&#21697;&#12290;&#25552;&#20986;&#30340;&#26694;&#26550;&#20026;&#25945;&#32946;&#24037;&#20316;&#32773;&#25552;&#20379;&#20102;&#25351;&#23548;&#65292;&#24341;&#23548;&#20854;&#22312;&#35838;&#22530;&#20013;&#25972;&#21512; AI &#36741;&#21161;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper examines the transformative role of Large Language Models (LLMs) in education and their potential as learning tools, despite their inherent risks and limitations. The authors propose seven approaches for utilizing AI in classrooms: AI-tutor, AI-coach, AI-mentor, AI-teammate, AI-tool, AI-simulator, and AI-student, each with distinct pedagogical benefits and risks. The aim is to help students learn with and about AI, with practical strategies designed to mitigate risks such as complacency about the AI's output, errors, and biases. These strategies promote active oversight, critical assessment of AI outputs, and complementarity of AI's capabilities with the students' unique insights. By challenging students to remain the "human in the loop," the authors aim to enhance learning outcomes while ensuring that AI serves as a supportive tool rather than a replacement. The proposed framework offers a guide for educators navigating the integration of AI-assisted learning in classrooms
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#22495;&#37325;&#21472;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#65288;GNNO&#65289;&#65292;&#21033;&#29992;&#38544;&#34255;&#22312;&#29992;&#25143;&#34892;&#20026;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#36127;&#37319;&#26679;&#65292;&#29992;&#20110;&#22686;&#36827;&#24207;&#21015;&#25512;&#33616;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10047</link><description>&lt;p&gt;
&#22522;&#20110;&#37051;&#22495;&#30340;&#38590;&#20363;&#25366;&#25496;&#29992;&#20110;&#24207;&#21015;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Neighborhood-based Hard Negative Mining for Sequential Recommendation. (arXiv:2306.10047v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10047
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#22495;&#37325;&#21472;&#30340;&#22522;&#20110;&#22270;&#24418;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#65288;GNNO&#65289;&#65292;&#21033;&#29992;&#38544;&#34255;&#22312;&#29992;&#25143;&#34892;&#20026;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#36127;&#37319;&#26679;&#65292;&#29992;&#20110;&#22686;&#36827;&#24207;&#21015;&#25512;&#33616;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35757;&#32451;&#24207;&#21015;&#25512;&#33616;&#27169;&#22411;&#26102;&#65292;&#36127;&#37319;&#26679;&#22312;&#20854;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#30446;&#21069;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#31574;&#30053;&#26469;&#25366;&#25496;&#20449;&#24687;&#20016;&#23500;&#30340;&#36127;&#26679;&#26412;&#65292;&#20197;&#24378;&#21270;&#35757;&#32451;&#21644;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#24456;&#23569;&#26377;&#36825;&#20123;&#26041;&#27861;&#21033;&#29992;&#32467;&#26500;&#20449;&#24687;&#12290;&#26412;&#25991;&#35266;&#23519;&#21040;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#19981;&#21516;&#31243;&#24230;&#37051;&#22495;&#37325;&#21472;&#30340;&#19981;&#21516;&#32452;&#33410;&#28857;&#23545;&#30456;&#20284;&#24230;&#30340;&#20998;&#24067;&#21457;&#29983;&#26174;&#30528;&#21464;&#21270;&#65292;&#36825;&#34920;&#26126;&#19981;&#21516;&#32452;&#20013;&#30340;&#39033;&#30446;&#23545;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#36127;&#20851;&#31995;&#12290;&#21463;&#27492;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#37051;&#22495;&#37325;&#21472;&#30340;&#22522;&#20110;&#22270;&#30340;&#36127;&#37319;&#26679;&#26041;&#27861;&#65288;GNNO&#65289;&#26469;&#21033;&#29992;&#38544;&#34255;&#22312;&#29992;&#25143;&#34892;&#20026;&#20013;&#30340;&#32467;&#26500;&#20449;&#24687;&#36827;&#34892;&#36127;&#37319;&#26679;&#12290;GNNO&#39318;&#20808;&#20351;&#29992;&#35757;&#32451;&#24207;&#21015;&#26500;&#24314;&#20840;&#23616;&#21152;&#26435;&#39033;&#30446;&#36716;&#25442;&#22270;&#12290;&#38543;&#21518;&#65292;&#23427;&#26681;&#25454;&#19982;&#30446;&#26631;&#39033;&#30340;&#37325;&#21472;&#31243;&#24230;&#26469;&#25366;&#25496;&#38590;&#20363;&#36127;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Negative sampling plays a crucial role in training successful sequential recommendation models. Instead of merely employing random negative sample selection, numerous strategies have been proposed to mine informative negative samples to enhance training and performance. However, few of these approaches utilize structural information. In this work, we observe that as training progresses, the distributions of node-pair similarities in different groups with varying degrees of neighborhood overlap change significantly, suggesting that item pairs in distinct groups may possess different negative relationships. Motivated by this observation, we propose a Graph-based Negative sampling approach based on Neighborhood Overlap (GNNO) to exploit structural information hidden in user behaviors for negative mining. GNNO first constructs a global weighted item transition graph using training sequences. Subsequently, it mines hard negative samples based on the degree of overlap with the target item on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23558;&#23454;&#20307;&#38142;&#25509;&#21040;&#30693;&#35782;&#24211;&#20013;&#30340;&#36890;&#29992;&#31995;&#32479;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#38142;&#25509;&#29305;&#23450;&#39046;&#22495;&#30340;&#23454;&#20307;&#65292;&#29305;&#21035;&#26159; COVID-19 &#30456;&#20851;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#23884;&#20837;&#24335;&#23454;&#20307;&#12290;&#36890;&#36807;&#21033;&#29992;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#25972;&#20307;&#23454;&#20307;&#38142;&#25509;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10044</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#31185;&#23398;&#25991;&#29486;&#20013;&#34920;&#26684;&#23454;&#20307;&#38142;&#25509;&#30340;&#23454;&#29992;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Practical Entity Linking System for Tables in Scientific Literature. (arXiv:2306.10044v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10044
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#29992;&#20110;&#23558;&#23454;&#20307;&#38142;&#25509;&#21040;&#30693;&#35782;&#24211;&#20013;&#30340;&#36890;&#29992;&#31995;&#32479;&#65292;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#38142;&#25509;&#29305;&#23450;&#39046;&#22495;&#30340;&#23454;&#20307;&#65292;&#29305;&#21035;&#26159; COVID-19 &#30456;&#20851;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#23884;&#20837;&#24335;&#23454;&#20307;&#12290;&#36890;&#36807;&#21033;&#29992;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#65292;&#20197;&#25552;&#39640;&#25972;&#20307;&#23454;&#20307;&#38142;&#25509;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#38142;&#25509;&#26159;&#26500;&#24314;&#30693;&#35782;&#22270;&#35889;&#30340;&#37325;&#35201;&#27493;&#39588;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#22238;&#31572;&#21253;&#25324;&#20174;&#36825;&#20123;&#25991;&#26723;&#20013;&#26816;&#32034;&#30456;&#20851;&#20449;&#24687;&#22312;&#20869;&#30340;&#39640;&#32423;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#31995;&#32479;&#65292;&#29992;&#20110;&#23558;&#23454;&#20307;&#38142;&#25509;&#21040;&#32500;&#22522;&#25968;&#25454;&#30693;&#35782;&#24211;&#20013;&#30340;&#39033;&#12290;&#23427;&#25551;&#36848;&#20102;&#22914;&#20309;&#36866;&#24212;&#35813;&#31995;&#32479;&#20197;&#38142;&#25509;&#39046;&#22495;&#29305;&#23450;&#30340;&#23454;&#20307;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#26469;&#33258;COVID-19&#30456;&#20851;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#23884;&#20837;&#24335;&#23454;&#20307;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#31995;&#32479;&#30340;&#31163;&#32447;&#23454;&#20363;&#30340;&#35774;&#32622;&#65292;&#20351;&#25105;&#20204;&#30340;&#23454;&#20307;&#38142;&#25509;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#26356;&#21152;&#21487;&#34892;&#12290;&#20316;&#20026;&#25512;&#26029;&#31185;&#23398;&#34920;&#26684;&#30340;&#35821;&#20041;&#21547;&#20041;&#30340;&#26356;&#24191;&#27867;&#26041;&#27861;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#21033;&#29992;&#34920;&#26684;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#29305;&#24449;&#26469;&#25552;&#39640;&#25972;&#20307;&#23454;&#20307;&#38142;&#25509;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity linking is an important step towards constructing knowledge graphs that facilitate advanced question answering over scientific documents, including the retrieval of relevant information included in tables within these documents. This paper introduces a general-purpose system for linking entities to items in the Wikidata knowledge base. It describes how we adapt this system for linking domain-specific entities, especially for those entities embedded within tables drawn from COVID-19-related scientific literature. We describe the setup of an efficient offline instance of the system that enables our entity-linking approach to be more feasible in practice. As part of a broader approach to infer the semantic meaning of scientific tables, we leverage the structural and semantic characteristics of the tables to improve overall entity linking performance.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#20174;&#20215;&#20540;&#35266;&#12289;&#25968;&#25454;&#32452;&#25104;&#21644;&#36164;&#28304;&#22522;&#30784;&#35774;&#26045;&#19977;&#20010;&#35282;&#24230;&#20837;&#25163;&#24182;&#25351;&#20986;&#23427;&#20204;&#30456;&#20114;&#20381;&#23384;&#65292;&#38656;&#35201;&#19968;&#21516;&#32771;&#34385;&#35299;&#20915;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25581;&#31034;&#20102;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22914;&#26435;&#21147;&#38598;&#20013;&#21644;&#20381;&#36182;&#24615;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2306.10043</link><description>&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#20013;&#24322;&#36136;&#24615;&#30340;&#20132;&#32455;&#36724;&#35299;&#26512;&#20197;&#20419;&#36827;&#27665;&#20027;&#21644;&#21253;&#23481;&#24615;&#21457;&#23637;
&lt;/p&gt;
&lt;p&gt;
Unraveling the Interconnected Axes of Heterogeneity in Machine Learning for Democratic and Inclusive Advancements. (arXiv:2306.10043v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10043
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#35752;&#20102;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#30340;&#24322;&#36136;&#24615;&#38382;&#39064;&#65292;&#20174;&#20215;&#20540;&#35266;&#12289;&#25968;&#25454;&#32452;&#25104;&#21644;&#36164;&#28304;&#22522;&#30784;&#35774;&#26045;&#19977;&#20010;&#35282;&#24230;&#20837;&#25163;&#24182;&#25351;&#20986;&#23427;&#20204;&#30456;&#20114;&#20381;&#23384;&#65292;&#38656;&#35201;&#19968;&#21516;&#32771;&#34385;&#35299;&#20915;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#25581;&#31034;&#20102;&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#23384;&#22312;&#30340;&#38382;&#39064;&#65292;&#22914;&#26435;&#21147;&#38598;&#20013;&#21644;&#20381;&#36182;&#24615;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#24341;&#21457;&#20102;&#26377;&#20851;&#20854;&#23545;&#31038;&#20250;&#30340;&#21033;&#30410;&#30340;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25351;&#20986;&#24182;&#20998;&#26512;&#20102;&#19977;&#20010;&#26126;&#26174;&#24433;&#21709;&#26426;&#22120;&#23398;&#20064;&#20135;&#21697;&#21457;&#23637;&#36712;&#36857;&#30340;&#24322;&#36136;&#24615;&#36724;&#65292;&#23427;&#20204;&#26159;&#65306;&#20215;&#20540;&#35266;&#12289;&#25991;&#21270;&#21644;&#27861;&#35268;&#12289;&#25968;&#25454;&#32452;&#25104;&#20197;&#21450;&#36164;&#28304;&#21644;&#22522;&#30784;&#35774;&#26045;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20123;&#36724;&#22914;&#20309;&#30456;&#20114;&#20381;&#23384;&#24182;&#30456;&#20114;&#24433;&#21709;&#65292;&#24378;&#35843;&#38656;&#35201;&#20849;&#21516;&#32771;&#34385;&#21644;&#35299;&#20915;&#23427;&#20204;&#30340;&#24517;&#35201;&#24615;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#30340;&#30740;&#31350;&#26223;&#35266;&#22312;&#36825;&#26041;&#38754;&#36824;&#26377;&#25152;&#19981;&#36275;&#65292;&#24448;&#24448;&#26410;&#33021;&#37319;&#29992;&#25972;&#20307;&#24615;&#26041;&#27861;&#12290;&#25105;&#20204;&#26816;&#26597;&#20102;&#20559;&#21521;&#23569;&#25968;&#20154;&#30340;&#25903;&#37197;&#24615;&#20559;&#35265;&#21644;&#26041;&#27861;&#23398;&#65292;&#20197;&#21450;&#30001;&#27492;&#23548;&#33268;&#30340;&#26435;&#21147;&#38598;&#20013;&#12289;&#21516;&#36136;&#21270;&#25511;&#21046;&#21644;&#22686;&#21152;&#30340;&#20381;&#36182;&#24615;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#31181;&#19977;&#20010;&#36724;&#30340;&#29255;&#38754;&#30740;&#31350;&#26500;&#25104;&#30340;&#26174;&#33879;&#25361;&#25112;&#65292;&#23548;&#33268;&#19968;&#20010;&#32570;&#20047;&#21453;&#26144;&#29616;&#23454;&#24773;&#20917;&#30340;&#23454;&#38469;&#35299;&#33616;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing utilization of machine learning (ML) in decision-making processes raises questions about its benefits to society. In this study, we identify and analyze three axes of heterogeneity that significantly influence the trajectory of ML products. These axes are i) values, culture and regulations, ii) data composition, and iii) resource and infrastructure capacity. We demonstrate how these axes are interdependent and mutually influence one another, emphasizing the need to consider and address them jointly. Unfortunately, the current research landscape falls short in this regard, often failing to adopt a holistic approach. We examine the prevalent practices and methodologies that skew these axes in favor of a selected few, resulting in power concentration, homogenized control, and increased dependency. We discuss how this fragmented study of the three axes poses a significant challenge, leading to an impractical solution space that lacks reflection of real-world scenarios. Addressi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37197;&#23545;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23558;&#26041;&#38754;-&#24847;&#35265;&#37197;&#23545;&#30693;&#35782;&#27880;&#20837;&#21040;Aspect Sentiment Triplet Extraction&#65288;ASTE&#65289;&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#20102;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.10042</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;Aspect Sentiment Triplet Extraction&#30340;&#37197;&#23545;&#22686;&#24378;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Pairing Enhancement Approach for Aspect Sentiment Triplet Extraction. (arXiv:2306.10042v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10042
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37197;&#23545;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#23558;&#26041;&#38754;-&#24847;&#35265;&#37197;&#23545;&#30693;&#35782;&#27880;&#20837;&#21040;Aspect Sentiment Triplet Extraction&#65288;ASTE&#65289;&#27169;&#22411;&#20013;&#65292;&#25552;&#39640;&#20102;&#19977;&#20803;&#32452;&#25552;&#21462;&#30340;&#20934;&#30830;&#24615;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Aspect Sentiment Triplet Extraction&#65288;ASTE&#65289;&#26088;&#22312;&#20174;&#35780;&#35770;&#25991;&#26412;&#20013;&#25552;&#21462;&#19968;&#20010;&#26041;&#38754;&#26415;&#35821;&#12289;&#19968;&#20010;&#24847;&#35265;&#26415;&#35821;&#21644;&#23427;&#20204;&#30456;&#24212;&#30340;&#24773;&#24863;&#26497;&#24615;&#30340;&#19977;&#20803;&#32452;&#12290;&#30001;&#20110;&#35821;&#35328;&#30340;&#22797;&#26434;&#24615;&#21644;&#21333;&#20010;&#21477;&#23376;&#20013;&#23384;&#22312;&#22810;&#20010;&#26041;&#38754;&#26415;&#35821;&#21644;&#24847;&#35265;&#26415;&#35821;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#32463;&#24120;&#20250;&#28151;&#28102;&#25551;&#36848;&#23427;&#30340;&#26041;&#38754;&#26415;&#35821;&#21644;&#24847;&#35265;&#26415;&#35821;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#37197;&#23545;&#22686;&#24378;&#26041;&#27861;&#65292;&#23427;&#22312;&#35757;&#32451;&#38454;&#27573;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#65292;&#23558;&#26041;&#38754;-&#24847;&#35265;&#37197;&#23545;&#30693;&#35782;&#27880;&#20837;&#21040;&#19977;&#20803;&#32452;&#25552;&#21462;&#27169;&#22411;&#20013;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20960;&#31181;&#30456;&#20851;&#32463;&#20856;&#21644;&#26368;&#20808;&#36827;&#30340;&#19977;&#20803;&#32452;&#25552;&#21462;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22235;&#20010;ASTE&#25968;&#25454;&#38598;&#65288;&#21363;14lap&#65292;14res&#65292;15res&#21644;16res&#65289;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#27492;&#22806;&#65292;&#28040;&#34701;&#30740;&#31350;&#36827;&#34892;&#20998;&#26512;&#24182;&#39564;&#35777;&#20102;&#23545;&#27604;&#23398;&#20064;&#30456;&#27604;&#20854;&#20182;&#37197;&#23545;&#22686;&#24378;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aspect Sentiment Triplet Extraction (ASTE) aims to extract the triplet of an aspect term, an opinion term, and their corresponding sentiment polarity from the review texts. Due to the complexity of language and the existence of multiple aspect terms and opinion terms in a single sentence, current models often confuse the connections between an aspect term and the opinion term describing it. To address this issue, we propose a pairing enhancement approach for ASTE, which incorporates contrastive learning during the training stage to inject aspect-opinion pairing knowledge into the triplet extraction model. Experimental results demonstrate that our approach performs well on four ASTE datasets (i.e., 14lap, 14res, 15res and 16res) compared to several related classical and state-of-the-art triplet extraction methods. Moreover, ablation studies conduct an analysis and verify the advantage of contrastive learning over other pairing enhancement approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2306.09983</link><description>&lt;p&gt;
&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Evaluating Superhuman Models with Consistency Checks. (arXiv:2306.09983v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09983
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#65292;&#21363;&#20351;&#23545;&#20110;&#36229;&#20154;&#27169;&#22411;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#35780;&#20272;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#21508;&#31181;&#25512;&#29702;&#25110;&#20915;&#31574;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#36229;&#20154;&#33021;&#21147;&#65292;&#37027;&#20040;&#25105;&#20204;&#35813;&#22914;&#20309;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#65292;&#32771;&#34385;&#21040;&#20154;&#31867;&#20195;&#29702;&#20250;&#20135;&#29983;&#20559;&#24046;? &#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#19968;&#33268;&#24615;&#26816;&#26597;&#35780;&#20272;&#36229;&#20154;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#25105;&#20204;&#30340;&#21069;&#25552;&#26159;&#65292;&#34429;&#28982;&#35780;&#20272;&#36229;&#20154;&#20915;&#31574;&#30340;&#27491;&#30830;&#24615;&#21487;&#33021;&#26159;&#19981;&#21487;&#33021;&#30340;&#65292;&#20294;&#26159;&#22914;&#26524;&#27169;&#22411;&#30340;&#20915;&#31574;&#26410;&#33021;&#28385;&#36275;&#26576;&#20123;&#36923;&#36753;&#19978;&#12289;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#65292;&#25105;&#20204;&#20173;&#28982;&#21487;&#20197;&#21457;&#29616;&#38169;&#35823;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26694;&#26550;&#23454;&#29616;&#22312;&#19977;&#20010;&#20219;&#21153;&#19978;&#65292;&#36825;&#20123;&#20219;&#21153;&#30340;&#20915;&#31574;&#27491;&#30830;&#24615;&#30001;&#20110;&#36229;&#20154;&#27169;&#22411;&#33021;&#21147;&#25110;&#20854;&#20182;&#32570;&#20047;&#22522;&#26412;&#20107;&#23454;&#32780;&#38590;&#20197;&#35780;&#20272;&#65306;&#35780;&#20272;&#22269;&#38469;&#35937;&#26827;&#23616;&#38754;&#12289;&#39044;&#27979;&#26410;&#26469;&#20107;&#20214;&#21644;&#20316;&#20986;&#27861;&#24459;&#21028;&#26029;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#26080;&#35770;&#27169;&#22411;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#22914;&#20309;(&#21487;&#33021;&#26159;&#36229;&#20154;&#30340;)&#65292;&#25105;&#20204;&#37117;&#33021;&#21457;&#29616;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#12290;&#20363;&#22914;&#65306;&#22269;&#38469;&#35937;&#26827;&#24341;&#25806;&#32473;&#20986;&#23545;&#23616;&#20013;&#26827;&#23376;&#30456;&#23545;&#20272;&#20540;&#30340;&#19981;&#21516;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
If machine learning models were to achieve superhuman abilities at various reasoning or decision-making tasks, how would we go about evaluating such models, given that humans would necessarily be poor proxies for ground truth? In this paper, we propose a framework for evaluating superhuman models via consistency checks. Our premise is that while the correctness of superhuman decisions may be impossible to evaluate, we can still surface mistakes if the model's decisions fail to satisfy certain logical, human-interpretable rules. We instantiate our framework on three tasks where correctness of decisions is hard to evaluate due to either superhuman model abilities, or to otherwise missing ground truth: evaluating chess positions, forecasting future events, and making legal judgments. We show that regardless of a model's (possibly superhuman) performance on these tasks, we can discover logical inconsistencies in decision making. For example: a chess engine assigning opposing valuations to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2306.09896</link><description>&lt;p&gt;
&#25581;&#31192; GPT &#33258;&#25105;&#20462;&#22797;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Demystifying GPT Self-Repair for Code Generation. (arXiv:2306.09896v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09896
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#33258;&#25105;&#20462;&#22797;&#22312;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#25928;&#26524;&#26356;&#22909;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LLM) &#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#25361;&#25112;&#24615;&#32534;&#31243;&#20219;&#21153;&#19978;&#20173;&#38754;&#20020;&#22256;&#38590;&#12290;&#33258;&#25105;&#20462;&#22797;&#8212;&#8212;&#21363;&#27169;&#22411;&#35843;&#35797;&#24182;&#20462;&#22797;&#33258;&#24049;&#30340;&#20195;&#30721;&#8212;&#8212;&#26368;&#36817;&#25104;&#20026;&#25552;&#39640;&#24615;&#33021;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#20851;&#20110;&#33258;&#25105;&#20462;&#22797;&#22914;&#20309;&#26377;&#25928;&#22320;&#21457;&#25381;&#20316;&#29992;&#30340;&#30740;&#31350;&#36824;&#38750;&#24120;&#26377;&#38480;&#12290;&#26377;&#20154;&#20250;&#24819;&#30693;&#36947;&#65292;&#24403;&#21516;&#19968;&#27169;&#22411;&#29983;&#25104;&#20195;&#30721;&#26102;&#65292;&#27169;&#22411;&#31350;&#31455;&#33021;&#21542;&#25552;&#20379;&#20934;&#30830;&#30340;&#21453;&#39304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102; GPT-3.5 &#21644; GPT-4 &#22312; APPS &#25968;&#25454;&#38598;&#19978;&#25191;&#34892;&#33258;&#25105;&#20462;&#22797;&#30340;&#33021;&#21147;&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;&#22810;&#31181;&#32534;&#30721;&#25361;&#25112;&#32452;&#25104;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#39318;&#20808;&#24314;&#31435;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#31574;&#30053; pass@t&#65292;&#35813;&#31574;&#30053;&#34913;&#37327;&#20102;&#20219;&#21153;&#36890;&#36807;&#29575;&#19982;&#20174;&#27169;&#22411;&#20013;&#25277;&#26679;&#30340;&#24635;&#26631;&#35760;&#25968;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20165;&#22522;&#20110;&#25277;&#26679;&#30340;&#26041;&#27861;&#30340;&#20844;&#24179;&#27604;&#36739;&#12290;&#36890;&#36807;&#36825;&#31181;&#35780;&#20272;&#31574;&#30053;&#65292;&#25105;&#20204;&#21457;&#29616;&#33258;&#25105;&#20462;&#22797;&#22312; GPT &#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#20005;&#37325;&#21462;&#20915;&#20110;&#20219;&#21153;&#30340;&#36136;&#37327;&#21644;&#22797;&#26434;&#24615;&#65292;&#24182;&#30830;&#23450;&#20102;&#24433;&#21709;&#33258;&#25105;&#20462;&#22797;&#34920;&#29616;&#30340;&#20960;&#20010;&#22240;&#32032;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#36755;&#20837;&#22122;&#22768;&#36739;&#23569;&#19988;&#27169;&#22411;&#23545;&#21021;&#22987;&#36755;&#20986;&#19981;&#22826;&#33258;&#20449;&#30340;&#36739;&#30701;&#21644;&#36739;&#31616;&#21333;&#30340;&#20219;&#21153;&#20013;&#65292;&#33258;&#25105;&#20462;&#22797;&#25928;&#26524;&#26356;&#22909;&#12290;&#25105;&#20204;&#36824;&#34920;&#26126;&#65292;&#20165;&#22312;&#26576;&#20123;&#20195;&#30721;&#37096;&#20998;&#19978;&#24212;&#29992;&#33258;&#25105;&#20462;&#22797;&#21487;&#20197;&#38750;&#24120;&#26377;&#25928;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24341;&#23548;&#20462;&#22797;&#26041;&#27861;&#65292;&#21033;&#29992;&#22806;&#37096;&#21453;&#39304;&#26469;&#22686;&#24378; GPT &#27169;&#22411;&#30340;&#33258;&#25105;&#20462;&#22797;&#33021;&#21147;&#65292;&#22312; APPS &#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown remarkable aptitude in code generation but still struggle on challenging programming tasks. Self-repair -in which the model debugs and fixes mistakes in its own code -- has recently become a popular way to boost performance in these settings. However, only very limited studies on how and when self-repair works effectively exist in the literature, and one might wonder to what extent a model is really capable of providing accurate feedback on why the code is wrong when that code was generated by the same model. In this paper, we analyze GPT-3.5 and GPT-4's ability to perform self-repair on APPS, a challenging dataset consisting of diverse coding challenges. To do so, we first establish a new evaluation strategy dubbed pass@t that measures the pass rate of the tasks against the total number of tokens sampled from the model, enabling a fair comparison to purely sampling-based approaches. With this evaluation strategy, we find that the effectiveness
&lt;/p&gt;</description></item><item><title>GenORM&#36890;&#36807;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#21644;&#20351;&#29992;&#21508;&#31181;&#21487;&#21464;&#24418;&#32499;&#32034;&#30340;&#27169;&#25311;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#21033;&#29992;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#32499;&#32034;&#65292;&#20174;&#32780;&#33410;&#30465;&#28436;&#31034;&#26102;&#38388;&#21644;&#25552;&#39640;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.09872</link><description>&lt;p&gt;
&#21487;&#27867;&#21270;&#30340;&#19968;&#27425;&#24615;&#32499;&#32034;&#25805;&#20316;&#31574;&#30053;&#21450;&#20854;&#21442;&#25968;&#24863;&#30693;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generalizable One-shot Rope Manipulation with Parameter-Aware Policy. (arXiv:2306.09872v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09872
&lt;/p&gt;
&lt;p&gt;
GenORM&#36890;&#36807;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#21644;&#20351;&#29992;&#21508;&#31181;&#21487;&#21464;&#24418;&#32499;&#32034;&#30340;&#27169;&#25311;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#23454;&#29616;&#21033;&#29992;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#32499;&#32034;&#65292;&#20174;&#32780;&#33410;&#30465;&#28436;&#31034;&#26102;&#38388;&#21644;&#25552;&#39640;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#32499;&#32034;&#22312;&#36816;&#21160;&#36807;&#31243;&#20013;&#30340;&#22266;&#26377;&#19981;&#30830;&#23450;&#24615;&#20026;&#22240;&#32032;&#65292;&#20197;&#24448;&#32499;&#32034;&#25805;&#20316;&#26041;&#27861;&#24448;&#24448;&#38656;&#35201;&#25968;&#30334;&#27425;&#30495;&#23454;&#28436;&#31034;&#26469;&#20026;&#27599;&#20010;&#32499;&#32034;&#35757;&#32451;&#25805;&#20316;&#31574;&#30053;&#65292;&#21363;&#20351;&#26159;&#31616;&#21333;&#30340;&#8220;&#21040;&#36798;&#30446;&#26631;&#8221;&#20219;&#21153;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25105;&#20204;&#19981;&#26029;&#21464;&#21270;&#30340;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;GenORM&#65292;&#19968;&#20010;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#35753;&#25805;&#20316;&#31574;&#30053;&#36890;&#36807;&#19968;&#27425;&#30495;&#23454;&#28436;&#31034;&#23601;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#21487;&#24418;&#21464;&#30340;&#32499;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#22312;&#31574;&#30053;&#19978;&#22686;&#21152;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#24182;&#20351;&#29992;&#21508;&#31181;&#27169;&#25311;&#21487;&#21464;&#24418;&#32499;&#32034;&#26469;&#35757;&#32451;&#23427;&#65292;&#20351;&#31574;&#30053;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#32499;&#32034;&#21442;&#25968;&#35843;&#25972;&#34892;&#21160;&#12290;&#22312;&#25512;&#26029;&#26102;&#65292;GenORM&#36890;&#36807;&#26368;&#23567;&#21270;&#30495;&#23454;&#28436;&#31034;&#21644;&#27169;&#25311;&#28857;&#20113;&#30340;&#32593;&#26684;&#23494;&#24230;&#24046;&#24322;&#26469;&#20272;&#35745;&#21487;&#21464;&#24418;&#32499;&#32034;&#21442;&#25968;&#12290;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#24110;&#21161;&#65292;&#25105;&#20204;&#20165;&#38656;&#35201;&#19968;&#27425;&#28436;&#31034;&#25968;&#25454;&#23601;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#30340;&#32499;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the inherent uncertainty in their deformability during motion, previous methods in rope manipulation often require hundreds of real-world demonstrations to train a manipulation policy for each rope, even for simple tasks such as rope goal reaching, which hinder their applications in our ever-changing world. To address this issue, we introduce GenORM, a framework that allows the manipulation policy to handle different deformable ropes with a single real-world demonstration. To achieve this, we augment the policy by conditioning it on deformable rope parameters and training it with a diverse range of simulated deformable ropes so that the policy can adjust actions based on different rope parameters. At the time of inference, given a new rope, GenORM estimates the deformable rope parameters by minimizing the disparity between the grid density of point clouds of real-world demonstrations and simulations. With the help of a differentiable physics simulator, we require only a single r
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#26465;&#20214;&#19979;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38590;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26041;&#26696;FedC2SL&#65292;&#26080;&#38656;&#25910;&#38598;&#21407;&#22987;&#25968;&#25454;&#19988;&#23545;&#25968;&#25454;&#21464;&#24322;&#20855;&#26377;&#26356;&#24378;&#30340;&#25269;&#25239;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.09433</link><description>&lt;p&gt;
&#23454;&#29992;&#32852;&#37030;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#20043;&#36335;
&lt;/p&gt;
&lt;p&gt;
Towards Practical Federated Causal Structure Learning. (arXiv:2306.09433v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09433
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#26465;&#20214;&#19979;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38590;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26041;&#26696;FedC2SL&#65292;&#26080;&#38656;&#25910;&#38598;&#21407;&#22987;&#25968;&#25454;&#19988;&#23545;&#25968;&#25454;&#21464;&#24322;&#20855;&#26377;&#26356;&#24378;&#30340;&#25269;&#25239;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22240;&#26524;&#20851;&#31995;&#23545;&#20110;&#31185;&#23398;&#21457;&#29616;&#33267;&#20851;&#37325;&#35201;&#12290;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#30340;&#36807;&#31243;&#28041;&#21450;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#35782;&#21035;&#22240;&#26524;&#22270;&#20197;&#29702;&#35299;&#36825;&#31181;&#20851;&#31995;&#12290;&#36890;&#24120;&#65292;&#19968;&#20010;&#20013;&#22830;&#26381;&#21153;&#22120;&#25191;&#34892;&#27492;&#20219;&#21153;&#65292;&#20294;&#19982;&#26381;&#21153;&#22120;&#20849;&#20139;&#25968;&#25454;&#20250;&#24102;&#26469;&#38544;&#31169;&#39118;&#38505;&#12290;&#32852;&#37030;&#23398;&#20064;&#21487;&#20197;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#30340;&#32852;&#37030;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#35299;&#20915;&#26041;&#26696;&#23545;&#25968;&#25454;&#20570;&#20986;&#20102;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#24182;&#32570;&#20047;&#25910;&#25947;&#20445;&#35777;&#12290;FedC2SL&#26159;&#19968;&#31181;&#32852;&#37030;&#22522;&#20110;&#32422;&#26463;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#26041;&#26696;&#65292;&#23427;&#20351;&#29992;&#32852;&#37030;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#26469;&#23398;&#20064;&#22240;&#26524;&#22270;&#65292;&#35813;&#26816;&#39564;&#22312;&#19981;&#25910;&#38598;&#23458;&#25143;&#31471;&#21407;&#22987;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#26816;&#26597;&#20004;&#20010;&#21464;&#37327;&#22312;&#19968;&#32452;&#26465;&#20214;&#19979;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#12290;FedC2SL&#23545;&#25968;&#25454;&#20570;&#20986;&#20102;&#26356;&#24369;&#21644;&#26356;&#29616;&#23454;&#30340;&#20551;&#35774;&#65292;&#24182;&#26356;&#24378;&#22320;&#25269;&#24481;&#20102;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#25968;&#25454;&#21464;&#24322;&#12290;FedPC&#21644;FedFCI&#26159;FedC2SL&#30340;&#20004;&#20010;&#21464;&#20307;&#65292;&#29992;&#20110;&#22240;&#26524;&#20805;&#20998;&#24615;&#21644;&#22240;&#26524;&#19981;&#20805;&#20998;&#24615;&#24773;&#20917;&#19979;&#30340;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding causal relations is vital in scientific discovery. The process of causal structure learning involves identifying causal graphs from observational data to understand such relations. Usually, a central server performs this task, but sharing data with the server poses privacy risks. Federated learning can solve this problem, but existing solutions for federated causal structure learning make unrealistic assumptions about data and lack convergence guarantees. FedC2SL is a federated constraint-based causal structure learning scheme that learns causal graphs using a federated conditional independence test, which examines conditional independence between two variables under a condition set without collecting raw data from clients. FedC2SL requires weaker and more realistic assumptions about data and offers stronger resistance to data variability among clients. FedPC and FedFCI are the two variants of FedC2SL for causal structure learning in causal sufficiency and causal insuffic
&lt;/p&gt;</description></item><item><title>STAR&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#26102;&#31354;&#22270;&#26469;&#25429;&#25417;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#65292;&#20026;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.09381</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#25193;&#23637;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal-Augmented Graph Neural Networks for Human Mobility Simulation. (arXiv:2306.09381v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09381
&lt;/p&gt;
&lt;p&gt;
STAR&#26694;&#26550;&#36890;&#36807;&#35774;&#35745;&#22810;&#31181;&#26102;&#31354;&#22270;&#26469;&#25429;&#25417;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#65292;&#20026;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#25552;&#20379;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#31227;&#21160;&#27169;&#24335;&#22312;&#25919;&#31574;&#20915;&#31574;&#21644;&#32463;&#27982;&#34892;&#20026;&#30740;&#31350;&#20013;&#26377;&#30528;&#37325;&#35201;&#30340;&#24212;&#29992;&#12290;&#20154;&#31867;&#31227;&#21160;&#27169;&#25311;&#20219;&#21153;&#26088;&#22312;&#32473;&#23450;&#19968;&#23567;&#32452;&#36712;&#36857;&#25968;&#25454;&#29983;&#25104;&#20154;&#31867;&#31227;&#21160;&#36712;&#36857;&#65292;&#20294;&#30001;&#20110;&#20154;&#31867;&#31227;&#21160;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#21644;&#31232;&#30095;&#24615;&#65292;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#29616;&#26377;&#26041;&#27861;&#22823;&#22810;&#20381;&#36182;&#20110;&#22320;&#28857;&#20043;&#38388;&#30340;&#38745;&#24577;&#20851;&#31995;&#65292;&#32780;&#24456;&#22823;&#31243;&#24230;&#19978;&#24573;&#30053;&#20102;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;SpatioTemporal-Augmented gRaph&#31070;&#32463;&#32593;&#32476;&#65288;STAR&#65289;&#65292;&#26469;&#27169;&#25311;&#20301;&#32622;&#30340;&#21160;&#24577;&#26102;&#31354;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human mobility patterns have shown significant applications in policy-decision scenarios and economic behavior researches. The human mobility simulation task aims to generate human mobility trajectories given a small set of trajectory data, which have aroused much concern due to the scarcity and sparsity of human mobility data. Existing methods mostly rely on the static relationships of locations, while largely neglect the dynamic spatiotemporal effects of locations. On the one hand, spatiotemporal correspondences of visit distributions reveal the spatial proximity and the functionality similarity of locations. On the other hand, the varying durations in different locations hinder the iterative generation process of the mobility trajectory. Therefore, we propose a novel framework to model the dynamic spatiotemporal effects of locations, namely SpatioTemporal-Augmented gRaph neural networks (STAR). The STAR framework designs various spatiotemporal graphs to capture the spatiotemporal co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMTL&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#35268;&#33539;&#21270;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#65292;&#21487;&#20197;&#25552;&#39640;MTL&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#26041;&#24046;&#27491;&#21017;&#21270;&#21644;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#20445;&#35777;&#25910;&#25947;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.09373</link><description>&lt;p&gt;
&#20844;&#24179;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Equitable Multi-task Learning. (arXiv:2306.09373v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09373
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMTL&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#36890;&#36807;&#35268;&#33539;&#21270;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#65292;&#21487;&#20197;&#25552;&#39640;MTL&#30340;&#27867;&#21270;&#24615;&#33021;&#65292;&#24182;&#21033;&#29992;&#26041;&#24046;&#27491;&#21017;&#21270;&#21644;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#20445;&#35777;&#25910;&#25947;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#22343;&#34920;&#29616;&#20986;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#22312;&#21508;&#20010;&#30740;&#31350;&#39046;&#22495;&#65288;&#22914;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20449;&#24687;&#26816;&#32034;&#31561;&#65289;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#19988;&#30456;&#20114;&#31454;&#20105;&#30340;&#30456;&#20851;&#24615;&#65292;&#21333;&#32431;&#22320;&#35757;&#32451;&#25152;&#26377;&#20219;&#21153;&#21487;&#33021;&#20250;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#23398;&#20064;&#65292;&#21363;&#19968;&#20123;&#20219;&#21153;&#34987;&#24456;&#22909;&#22320;&#23398;&#20064;&#65292;&#32780;&#20854;&#20182;&#20219;&#21153;&#21017;&#34987;&#24573;&#35270;&#12290;&#22810;&#20219;&#21153;&#20248;&#21270;&#65288;MTO&#65289;&#26088;&#22312;&#21516;&#26102;&#25552;&#39640;&#25152;&#26377;&#20219;&#21153;&#30340;&#34920;&#29616;&#65292;&#20294;&#20256;&#32479;&#26041;&#27861;&#24448;&#24448;&#22312;&#20219;&#21153;&#25439;&#22833;&#35268;&#27169;&#25110;&#26799;&#24230;&#33539;&#25968;&#24046;&#24322;&#36739;&#22823;&#30340;&#24773;&#20917;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;MTL&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#22312;&#26356;&#26032;&#20849;&#20139;&#21442;&#25968;&#26102;&#65292;&#35268;&#33539;&#21270;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#65288;&#21363;&#20219;&#21153;&#29305;&#23450;&#25439;&#22833;&#20540;&#38500;&#20197;&#20854;&#21407;&#22987;&#26799;&#24230;&#33539;&#25968;&#30340;&#20540;&#65289;&#21487;&#20197;&#25552;&#39640;MTL&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#20219;&#21153;&#20248;&#21270;&#26041;&#27861;&#65292;&#21517;&#20026;EMTL&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#30340;MTL&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#26377;&#25928;&#22320;&#28155;&#21152;&#20102;&#26041;&#24046;&#27491;&#21017;&#21270;&#65292;&#20351;&#19981;&#21516;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#26356;&#20855;&#21487;&#27604;&#24615;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20248;&#21270;&#31639;&#27861;&#26469;&#20445;&#35777;&#25910;&#25947;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-task learning (MTL) has achieved great success in various research domains, such as CV, NLP and IR etc. Due to the complex and competing task correlation, na\"ive training all tasks may lead to inequitable learning, \textit{i.e.} some tasks are learned well while others are overlooked. Multi-task optimization (MTO) aims to improve all tasks at same time, but conventional methods often perform poor when tasks with large loss scale or gradient norm magnitude difference. To solve the issue, we in-depth investigate the equity problem for MTL and find that regularizing relative contribution of different tasks (\textit{i.e.} value of task-specific loss divides its raw gradient norm) in updating shared parameter can improve generalization performance of MTL. Based on our theoretical analysis, we propose a novel multi-task optimization method, named \textit{EMTL}, to achieve equitable MTL. Specifically, we efficiently add variance regularization to make different tasks' relative contribu
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23450;&#20041;&#22870;&#21169;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25351;&#23450;&#30340;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.08647</link><description>&lt;p&gt;
&#35821;&#35328;&#36716;&#22870;&#21169;&#65306;&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#33021;&#32508;&#21512;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Language to Rewards for Robotic Skill Synthesis. (arXiv:2306.08647v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08647
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#23450;&#20041;&#22870;&#21169;&#21442;&#25968;&#65292;&#24182;&#36890;&#36807;&#22870;&#21169;&#20989;&#25968;&#36827;&#34892;&#20248;&#21270;&#65292;&#20174;&#32780;&#20351;&#26426;&#22120;&#20154;&#21487;&#20197;&#25191;&#34892;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25351;&#23450;&#30340;&#20219;&#21153;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#21462;&#24471;&#20102;&#35768;&#22810;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#65292;&#20174;&#36923;&#36753;&#25512;&#29702;&#21040;&#20195;&#30721;&#32534;&#20889;&#31561;&#65292;&#23637;&#29616;&#20102;&#22312;&#24773;&#22659;&#23398;&#20064;&#20013;&#33719;&#24471;&#22810;&#31181;&#26032;&#33021;&#21147;&#30340;&#28508;&#21147;&#12290;&#26426;&#22120;&#20154;&#23398;&#30740;&#31350;&#20154;&#21592;&#20063;&#25506;&#32034;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25552;&#39640;&#26426;&#22120;&#20154;&#25511;&#21046;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20302;&#32423;&#26426;&#22120;&#20154;&#21160;&#20316;&#21462;&#20915;&#20110;&#30828;&#20214;&#24182;&#19988;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#35821;&#26009;&#24211;&#20013;&#25152;&#21344;&#30340;&#27604;&#37325;&#36739;&#23567;&#65292;&#22240;&#27492;&#22312;&#26426;&#22120;&#20154;&#23398;&#20013;&#24212;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#29616;&#26377;&#21162;&#21147;&#20027;&#35201;&#23558;&#20854;&#35270;&#20026;&#35821;&#20041;&#35268;&#21010;&#22120;&#65292;&#25110;&#20381;&#36182;&#20110;&#20154;&#24037;&#25511;&#21046;&#21407;&#35821;&#19982;&#26426;&#22120;&#20154;&#36827;&#34892;&#20132;&#20114;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22870;&#21169;&#20989;&#25968;&#34987;&#35777;&#26126;&#26159;&#21487;&#20197;&#28789;&#27963;&#34920;&#31034;&#24182;&#19988;&#21487;&#20197;&#34987;&#20248;&#21270;&#20197;&#23454;&#29616;&#22810;&#31181;&#20219;&#21153;&#30340;&#25511;&#21046;&#31574;&#30053;&#65292;&#20854;&#35821;&#20041;&#20016;&#23500;&#24615;&#20351;&#20854;&#36866;&#21512;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#25351;&#23450;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#23450;&#20041;&#21487;&#20197;&#34987;&#20248;&#21270;&#30340;&#22870;&#21169;&#21442;&#25968;&#24182;&#23436;&#25104;&#21508;&#31181;&#26426;&#22120;&#20154;&#20219;&#21153;&#12290;&#20351;&#29992;&#22870;&#21169;&#20316;&#20026;&#20013;&#38388;&#20171;&#36136;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#25191;&#34892;&#21508;&#31181;&#30001;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#25351;&#23450;&#30340;&#20219;&#21153;&#65292;&#32780;&#26080;&#38656;&#20154;&#31867;&#22312;&#35774;&#35745;&#34892;&#20026;&#21407;&#35821;&#26041;&#38754;&#20184;&#20986;&#21162;&#21147;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25342;&#21462;&#29289;&#21697;&#21644;&#25645;&#24314;&#22612;&#20004;&#20010;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated exciting progress in acquiring diverse new capabilities through in-context learning, ranging from logical reasoning to code-writing. Robotics researchers have also explored using LLMs to advance the capabilities of robotic control. However, since low-level robot actions are hardware-dependent and underrepresented in LLM training corpora, existing efforts in applying LLMs to robotics have largely treated LLMs as semantic planners or relied on human-engineered control primitives to interface with the robot. On the other hand, reward functions are shown to be flexible representations that can be optimized for control policies to achieve diverse tasks, while their semantic richness makes them suitable to be specified by LLMs. In this work, we introduce a new paradigm that harnesses this realization by utilizing LLMs to define reward parameters that can be optimized and accomplish variety of robotic tasks. Using reward as the intermediate inter
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#36335;&#32447;&#22270;&#65292;&#36890;&#36807;&#19977;&#20010;&#26694;&#26550;&#65306;&#22686;&#24378;KGs&#30340;LLMs&#65292;&#30693;&#35782;&#22686;&#24378;KGs&#21644;LLMs&#19982;KGs&#30340;&#32852;&#21512;&#25512;&#29702;&#65292;&#32508;&#21512;&#21033;&#29992;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.08302</link><description>&lt;p&gt;
&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;: &#19968;&#26465;&#36335;&#32447;&#22270;
&lt;/p&gt;
&lt;p&gt;
Unifying Large Language Models and Knowledge Graphs: A Roadmap. (arXiv:2306.08302v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08302
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#32479;&#19968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#22270;&#35889;&#30340;&#36335;&#32447;&#22270;&#65292;&#36890;&#36807;&#19977;&#20010;&#26694;&#26550;&#65306;&#22686;&#24378;KGs&#30340;LLMs&#65292;&#30693;&#35782;&#22686;&#24378;KGs&#21644;LLMs&#19982;KGs&#30340;&#32852;&#21512;&#25512;&#29702;&#65292;&#32508;&#21512;&#21033;&#29992;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22914;ChatGPT&#21644;GPT4&#27491;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#25472;&#36215;&#26032;&#30340;&#28909;&#28526;&#65292;&#30001;&#20110;&#23427;&#20204;&#30340;&#31361;&#29616;&#33021;&#21147;&#21644;&#19968;&#33324;&#21270;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLM&#26159;&#40657;&#30418;&#27169;&#22411;&#65292;&#24448;&#24448;&#19981;&#33021;&#25429;&#25417;&#21644;&#33719;&#21462;&#23454;&#38469;&#30693;&#35782;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#30693;&#35782;&#22270;&#35889;&#65288;KGs&#65289;&#22914;&#32500;&#22522;&#30334;&#31185;&#21644;&#21326;&#26222;&#21017;&#26159;&#26126;&#30830;&#23384;&#20648;&#20016;&#23500;&#23454;&#38469;&#30693;&#35782;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#27169;&#22411;&#12290;KGs&#21487;&#20197;&#36890;&#36807;&#20026;&#25512;&#29702;&#21644;&#21487;&#35299;&#37322;&#24615;&#25552;&#20379;&#22806;&#37096;&#30693;&#35782;&#26469;&#22686;&#24378;LLMs&#12290;&#21516;&#26102;&#65292;KGs&#30340;&#26500;&#24314;&#22256;&#38590;&#65292;&#33258;&#28982;&#32780;&#28982;&#22320;&#28436;&#21270;&#65292;&#36825;&#25361;&#25112;&#20102;&#29616;&#26377;&#30340;KGs&#26041;&#27861;&#26469;&#29983;&#25104;&#26032;&#20107;&#23454;&#24182;&#34920;&#31034;&#26410;&#35265;&#36807;&#30340;&#30693;&#35782;&#12290;&#22240;&#27492;&#65292;&#32479;&#19968;LLMs&#21644;KGs&#24182;&#21516;&#26102;&#21033;&#29992;&#23427;&#20204;&#30340;&#20248;&#28857;&#26159;&#26377;&#30410;&#30340;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21069;&#30651;&#24615;&#30340;&#32479;&#19968;LLMs&#21644;KGs&#30340;&#36335;&#32447;&#22270;&#12290;&#25105;&#20204;&#30340;&#36335;&#32447;&#22270;&#21253;&#25324;&#19977;&#20010;&#19968;&#33324;&#26694;&#26550;&#65292;&#21363;1&#65289;&#22686;&#24378;KGs&#30340;LLMs&#65292;&#23427;&#20204;&#23558;&#30693;&#35782;&#34920;&#31034;&#20026;LM&#30340;&#19968;&#37096;&#20998;&#65292;&#20174;&#32780;&#33021;&#22815;&#25429;&#25417;&#20016;&#23500;&#30340;&#23454;&#20307;&#20851;&#31995;&#65292;2&#65289;&#30693;&#35782;&#22686;&#24378;KGs&#65292;&#23427;&#20204;&#23558;LLMs&#29992;&#20316;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#20248;&#31168;&#24037;&#20855;&#65292;3&#65289;LLMs&#19982;KGs&#30340;&#32852;&#21512;&#25512;&#29702;&#65292;&#20854;&#20013;LLMs&#21644;KGs&#30456;&#20114;&#22686;&#24378;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#20934;&#30830;&#30340;&#25512;&#29702;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs), such as ChatGPT and GPT4, are making new waves in the field of natural language processing and artificial intelligence, due to their emergent ability and generalizability. However, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge. In contrast, Knowledge Graphs (KGs), Wikipedia and Huapu for example, are structured knowledge models that explicitly store rich factual knowledge. KGs can enhance LLMs by providing external knowledge for inference and interpretability. Meanwhile, KGs are difficult to construct and evolving by nature, which challenges the existing methods in KGs to generate new facts and represent unseen knowledge. Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages. In this article, we present a forward-looking roadmap for the unification of LLMs and KGs. Our roadmap consists of three general frameworks, namely, 1) KG-enhanced LLMs, which incorpo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36827;&#34892;&#20102;&#25968;&#23383;&#30149;&#29702;&#22270;&#20687;&#20013;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#25152;&#26377;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#35786;&#26029;&#20934;&#30830;&#24230;&#30340;&#31995;&#32479;&#32508;&#36848;&#21644;Meta&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#21462;&#24471;&#20102;&#39640;&#24230;&#30340;&#20934;&#30830;&#24230;&#65292;&#26159;&#21487;&#34892;&#30340;&#36741;&#21161;&#35786;&#26029;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2306.07999</link><description>&lt;p&gt;
&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#35786;&#26029;&#27979;&#35797;&#20934;&#30830;&#24230;&#65306;&#31995;&#32479;&#32508;&#36848;&#12289;Meta&#20998;&#26512;&#21644;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Diagnostic test accuracy (DTA) of artificial intelligence in digital pathology: a systematic review, meta-analysis and quality assessment. (arXiv:2306.07999v1 [physics.med-ph])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36827;&#34892;&#20102;&#25968;&#23383;&#30149;&#29702;&#22270;&#20687;&#20013;&#24212;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#25152;&#26377;&#30149;&#29702;&#23398;&#39046;&#22495;&#30340;&#35786;&#26029;&#20934;&#30830;&#24230;&#30340;&#31995;&#32479;&#32508;&#36848;&#21644;Meta&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20154;&#24037;&#26234;&#33021;&#22312;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;&#21462;&#24471;&#20102;&#39640;&#24230;&#30340;&#20934;&#30830;&#24230;&#65292;&#26159;&#21487;&#34892;&#30340;&#36741;&#21161;&#35786;&#26029;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30830;&#20445;&#20020;&#24202;&#20351;&#29992;&#20043;&#21069;AI&#27169;&#22411;&#30340;&#35786;&#26029;&#34920;&#29616;&#26159;&#20851;&#38190;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#25216;&#26415;&#30340;&#23433;&#20840;&#21644;&#25104;&#21151;&#30340;&#37319;&#29992;&#12290;&#36817;&#24180;&#26469;&#65292;&#25253;&#36947;&#24212;&#29992;&#20110;&#25968;&#23383;&#30149;&#29702;&#23398;&#22270;&#20687;&#36827;&#34892;&#35786;&#26029;&#30446;&#30340;&#30340;AI&#30740;&#31350;&#25968;&#37327;&#36805;&#36895;&#22686;&#21152;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#20379;&#25968;&#23383;&#30149;&#29702;&#23398;&#20013;AI&#30340;&#35786;&#26029;&#20934;&#30830;&#24230;&#30340;&#27010;&#36848;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#30149;&#29702;&#23398;&#39046;&#22495;&#12290;&#36825;&#39033;&#31995;&#32479;&#24615;&#32508;&#36848;&#21644;Meta&#20998;&#26512;&#21253;&#25324;&#20351;&#29992;&#20219;&#20309;&#31867;&#22411;&#30340;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#20110;&#20219;&#20309;&#30142;&#30149;&#31867;&#22411;&#30340;WSI&#22270;&#20687;&#30340;&#35786;&#26029;&#20934;&#30830;&#24615;&#30740;&#31350;&#12290;&#21442;&#32771;&#26631;&#20934;&#26159;&#36890;&#36807;&#32452;&#32455;&#30149;&#29702;&#23398;&#35780;&#20272;&#21644;/&#25110;&#20813;&#30123;&#32452;&#21270;&#35786;&#26029;&#12290;&#25628;&#32034;&#22312;2022&#24180;6&#26376;&#22312;PubMed&#12289;EMBASE&#21644;CENTRAL&#20013;&#36827;&#34892;&#12290;&#22312;2976&#39033;&#30740;&#31350;&#20013;&#65292;&#26377;100&#39033;&#32435;&#20837;&#32508;&#36848;&#65292;48&#39033;&#32435;&#20837;&#23436;&#25972;&#30340;Meta&#20998;&#26512;&#12290;&#20351;&#29992;QUADAS-2&#24037;&#20855;&#35780;&#20272;&#20102;&#20559;&#20506;&#39118;&#38505;&#21644;&#36866;&#29992;&#24615;&#30340;&#20851;&#27880;&#28857;&#12290;&#25968;&#25454;&#25552;&#21462;&#30001;&#20004;&#20010;&#35843;&#26597;&#21592;&#36827;&#34892;&#65292;&#24182;&#36827;&#34892;&#20102;Meta&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Ensuring diagnostic performance of AI models before clinical use is key to the safe and successful adoption of these technologies. Studies reporting AI applied to digital pathology images for diagnostic purposes have rapidly increased in number in recent years. The aim of this work is to provide an overview of the diagnostic accuracy of AI in digital pathology images from all areas of pathology. This systematic review and meta-analysis included diagnostic accuracy studies using any type of artificial intelligence applied to whole slide images (WSIs) in any disease type. The reference standard was diagnosis through histopathological assessment and / or immunohistochemistry. Searches were conducted in PubMed, EMBASE and CENTRAL in June 2022. We identified 2976 studies, of which 100 were included in the review and 48 in the full meta-analysis. Risk of bias and concerns of applicability were assessed using the QUADAS-2 tool. Data extraction was conducted by two investigators and meta-analy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#32452;&#21512;&#27169;&#22411;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#12289;&#21521;&#37327;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#24050;&#32463;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#39640;&#25928;&#20449;&#24687;&#25552;&#21462;&#12289;&#25552;&#21462;&#20449;&#24687;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#35201;&#27714;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#31995;&#32479;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#23383;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07786</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#26426;&#22120;&#23398;&#20064;&#31649;&#36947;&#65292;&#26377;&#25928;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Cloud-based Machine Learning Pipeline for the Efficient Extraction of Insights from Customer Reviews. (arXiv:2306.07786v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07786
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#31995;&#32479;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#32452;&#21512;&#27169;&#22411;&#20351;&#29992;&#20102;&#36716;&#25442;&#22120;&#31070;&#32463;&#32593;&#32476;&#12289;&#21521;&#37327;&#23884;&#20837;&#21644;&#32858;&#31867;&#65292;&#24050;&#32463;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#39640;&#25928;&#20449;&#24687;&#25552;&#21462;&#12289;&#25552;&#21462;&#20449;&#24687;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#35201;&#27714;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#26412;&#31995;&#32479;&#21487;&#20197;&#27604;&#29616;&#26377;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#23383;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#33719;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#29305;&#21035;&#26159;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#25928;&#29575;&#26377;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#28982;&#32780;&#65292;&#19968;&#20123;&#20219;&#21153;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#29305;&#21035;&#26159;&#32771;&#34385;&#21040;&#29305;&#23450;&#30340;&#24212;&#29992;&#39046;&#22495;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20113;&#30340;&#31995;&#32479;&#65292;&#21487;&#20197;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38598;&#25104;&#21040;&#31649;&#36947;&#20013;&#65292;&#20174;&#23458;&#25143;&#35780;&#35770;&#20013;&#25552;&#21462;&#35265;&#35299;&#12290;&#23545;&#20110;&#20027;&#39064;&#24314;&#27169;&#65292;&#25105;&#20204;&#30340;&#32452;&#21512;&#27169;&#22411;&#20351;&#29992;&#20102;&#22522;&#20110;&#36716;&#25442;&#22120;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31070;&#32463;&#32593;&#32476;&#12289;&#22522;&#20110;&#21521;&#37327;&#23884;&#20837;&#30340;&#20851;&#38190;&#23383;&#25552;&#21462;&#21644;&#32858;&#31867;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20803;&#32032;&#24050;&#32463;&#38598;&#25104;&#24182;&#36827;&#19968;&#27493;&#21457;&#23637;&#65292;&#20197;&#26356;&#22909;&#22320;&#28385;&#36275;&#39640;&#25928;&#20449;&#24687;&#25552;&#21462;&#12289;&#25552;&#21462;&#20449;&#24687;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#29992;&#25143;&#38656;&#27714;&#30340;&#35201;&#27714;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#27604;&#36825;&#20010;&#20219;&#21153;&#29616;&#26377;&#30340;&#20027;&#39064;&#24314;&#27169;&#21644;&#20851;&#38190;&#23383;&#25552;&#21462;&#35299;&#20915;&#26041;&#26696;&#33719;&#24471;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20844;&#24320;&#21487;&#29992;&#30340;&#23458;&#25143;&#35780;&#35770;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#24182;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The efficiency of natural language processing has improved dramatically with the advent of machine learning models, particularly neural network-based solutions. However, some tasks are still challenging, especially when considering specific domains. In this paper, we present a cloud-based system that can extract insights from customer reviews using machine learning methods integrated into a pipeline. For topic modeling, our composite model uses transformer-based neural networks designed for natural language processing, vector embedding-based keyword extraction, and clustering. The elements of our model have been integrated and further developed to meet better the requirements of efficient information extraction, topic modeling of the extracted information, and user needs. Furthermore, our system can achieve better results than this task's existing topic modeling and keyword extraction solutions. Our approach is validated and compared with other state-of-the-art methods using publicly a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;SHAP&#20540;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#25351;&#20986;&#38543;&#26426;&#24490;&#29615;&#27169;&#22411;&#26159;&#26356;&#26377;&#25928;&#30340;&#22791;&#36873;&#24490;&#29615;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.07218</link><description>&lt;p&gt;
SHAP&#35299;&#37322;&#30340;&#25345;&#32493;&#35299;&#37322;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
A Protocol for Continual Explanation of SHAP. (arXiv:2306.07218v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;SHAP&#20540;&#35299;&#37322;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21327;&#35758;&#65292;&#24182;&#25351;&#20986;&#38543;&#26426;&#24490;&#29615;&#27169;&#22411;&#26159;&#26356;&#26377;&#25928;&#30340;&#22791;&#36873;&#24490;&#29615;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25345;&#32493;&#23398;&#20064;&#26088;&#22312;&#22312;&#25968;&#25454;&#27969;&#20013;&#35757;&#32451;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#26032;&#20449;&#24687;&#32780;&#19981;&#20250;&#24536;&#35760;&#20808;&#21069;&#30340;&#30693;&#35782;&#12290;&#37492;&#20110;&#36825;&#31181;&#29615;&#22659;&#30340;&#21160;&#24577;&#24615;&#36136;&#65292;&#35299;&#37322;&#36825;&#20123;&#27169;&#22411;&#30340;&#39044;&#27979;&#21487;&#33021;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102; SHAP &#20540;&#35299;&#37322;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#34892;&#20026;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#35780;&#20272;&#21327;&#35758;&#65292;&#20197;&#21487;&#38752;&#22320;&#35780;&#20272;&#36880;&#31867;&#22686;&#37327;&#22330;&#26223;&#20013;&#35299;&#37322;&#30340;&#21464;&#21270;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#34429;&#28982;&#37325;&#25918;&#31574;&#30053;&#21487;&#20197;&#24378;&#21046;&#21069;&#39304;/&#21367;&#31215;&#27169;&#22411;&#20013;&#30340; SHAP &#20540;&#30340;&#31283;&#23450;&#24615;&#65292;&#20294;&#23427;&#20204;&#26080;&#27861;&#22312;&#23436;&#20840;&#35757;&#32451;&#30340;&#24490;&#29615;&#27169;&#22411;&#20013;&#20570;&#21040;&#36825;&#19968;&#28857;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#20687;&#38543;&#26426;&#24490;&#29615;&#27169;&#22411;&#36825;&#26679;&#30340;&#22791;&#36873;&#24490;&#29615;&#26041;&#27861;&#22312;&#38543;&#26102;&#38388;&#20445;&#25345;&#35299;&#37322;&#31283;&#23450;&#26041;&#38754;&#26356;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Learning trains models on a stream of data, with the aim of learning new information without forgetting previous knowledge. Given the dynamic nature of such environments, explaining the predictions of these models can be challenging. We study the behavior of SHAP values explanations in Continual Learning and propose an evaluation protocol to robustly assess the change of explanations in Class-Incremental scenarios. We observed that, while Replay strategies enforce the stability of SHAP values in feedforward/convolutional models, they are not able to do the same with fully-trained recurrent models. We show that alternative recurrent approaches, like randomized recurrent models, are more effective in keeping the explanations stable over time.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#33258;&#20030;&#30340;&#26102;&#38388;&#27493;&#32423;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#20026;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25552;&#20379;&#39640;&#25928;&#38477;&#32500;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22312;PeMS-BAY&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.06994</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#31354;&#33258;&#20030;&#30340;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Correlated Time Series Self-Supervised Representation Learning via Spatiotemporal Bootstrapping. (arXiv:2306.06994v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06994
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#33258;&#20030;&#30340;&#26102;&#38388;&#27493;&#32423;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#33021;&#20026;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25552;&#20379;&#39640;&#25928;&#38477;&#32500;&#34920;&#31034;&#65292;&#24182;&#36890;&#36807;&#22312;PeMS-BAY&#25968;&#25454;&#38598;&#19978;&#30340;&#27979;&#35797;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#22312;&#35768;&#22810;&#23454;&#38469;&#24037;&#19994;&#20013;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#12290;&#23545;&#22823;&#35268;&#27169;&#25968;&#25454;&#36827;&#34892;&#39640;&#25928;&#38477;&#32500;&#34920;&#31034;&#20197;&#20415;&#36827;&#34892;&#19979;&#28216;&#20219;&#21153;&#26159;&#24517;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#31354;&#33258;&#20030;&#34920;&#31034;&#39044;&#27979;&#30340;&#26102;&#38388;&#27493;&#32423;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#20415;&#20026;&#20010;&#20307;&#23454;&#20363;&#36827;&#34892;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#30456;&#20851;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#21644;&#23558;&#39044;&#27979;&#27169;&#22411;&#20919;&#21551;&#21160;&#36716;&#31227;&#21040;&#25968;&#25454;&#21463;&#38480;&#30340;&#26032;&#23454;&#20363;&#26041;&#38754;&#35780;&#20272;&#20102;&#35813;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#21644;&#28789;&#27963;&#24615;&#12290;&#32463;&#36807;&#35757;&#32451;&#22312;&#23398;&#20064;&#34920;&#31034;&#19978;&#30340;&#32447;&#24615;&#22238;&#24402;&#27169;&#22411;&#35777;&#26126;&#20102;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#25105;&#20204;&#30340;&#27169;&#22411;&#34920;&#29616;&#26368;&#20339;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#19982;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#30340;&#27604;&#36739;&#20013;&#65292;&#25105;&#20204;&#22312;PeMS-BAY&#25968;&#25454;&#38598;&#19978;&#23558;RMSE&#12289;MAE&#21644;MAPE&#20998;&#21035;&#20943;&#23569;&#20102;37&#65285;&#12289;49&#65285;&#21644;48&#65285;&#12290;&#22312;&#23454;&#38469;&#30340;&#22320;&#38081;&#23458;&#27969;&#25968;&#25454;&#20013;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#23637;&#31034;&#20102;&#23558;&#39044;&#27979;&#33021;&#21147;&#36716;&#31227;&#21040;&#26032;&#30340;&#20919;&#21551;&#21160;&#24773;&#20917;&#19979;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Correlated time series analysis plays an important role in many real-world industries. Learning an efficient representation of this large-scale data for further downstream tasks is necessary but challenging. In this paper, we propose a time-step-level representation learning framework for individual instances via bootstrapped spatiotemporal representation prediction. We evaluated the effectiveness and flexibility of our representation learning framework on correlated time series forecasting and cold-start transferring the forecasting model to new instances with limited data. A linear regression model trained on top of the learned representations demonstrates our model performs best in most cases. Especially compared to representation learning models, we reduce the RMSE, MAE, and MAPE by 37%, 49%, and 48% on the PeMS-BAY dataset, respectively. Furthermore, in real-world metro passenger flow data, our framework demonstrates the ability to transfer to infer future information of new cold-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#24191;&#27867;&#35270;&#35282;&#26469;&#30028;&#23450;&#26234;&#33021;&#24182;&#24314;&#31435;&#20102;&#19977;&#32423;&#23884;&#22871;&#32467;&#26500;&#21450;&#20854;&#22522;&#30784;&#30340;&#24191;&#27867;&#31354;&#38388;&#12290;&#21033;&#29992;&#36825;&#20123;&#23450;&#20041;&#21021;&#27493;&#25506;&#32034;&#20102;&#22855;&#28857;&#12289;&#29983;&#25104;AI&#12289;&#20262;&#29702;&#21644;&#30693;&#35782;&#20135;&#26435;&#31561;&#35805;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.06499</link><description>&lt;p&gt;
&#30830;&#23450;&#21644;&#25506;&#32034;&#26234;&#33021;&#31354;&#38388;
&lt;/p&gt;
&lt;p&gt;
Defining and Explorting the Intelligence Space. (arXiv:2306.06499v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06499
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#24191;&#27867;&#35270;&#35282;&#26469;&#30028;&#23450;&#26234;&#33021;&#24182;&#24314;&#31435;&#20102;&#19977;&#32423;&#23884;&#22871;&#32467;&#26500;&#21450;&#20854;&#22522;&#30784;&#30340;&#24191;&#27867;&#31354;&#38388;&#12290;&#21033;&#29992;&#36825;&#20123;&#23450;&#20041;&#21021;&#27493;&#25506;&#32034;&#20102;&#22855;&#28857;&#12289;&#29983;&#25104;AI&#12289;&#20262;&#29702;&#21644;&#30693;&#35782;&#20135;&#26435;&#31561;&#35805;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23581;&#35797;&#20102;&#35768;&#22810;&#27425;&#65292;&#26234;&#33021;&#26159;&#19968;&#20010;&#38590;&#20197;&#23450;&#20041;&#30340;&#27010;&#24565;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#35270;&#35282;&#26469;&#23450;&#20041;&#26234;&#33021;, &#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#23450;&#20041;&#65292;&#26500;&#24314;&#20102;&#19977;&#20010;&#23618;&#27425;&#30340;&#26234;&#33021;&#23884;&#22871;&#23618;&#27425;&#21644;&#20197;&#23427;&#20204;&#21450;&#20854;&#36817;&#20284;&#20540;&#20026;&#22522;&#30784;&#30340;&#24191;&#27867;&#31354;&#38388;&#12290;&#22312;&#36825;&#20010;&#26234;&#33021;&#31354;&#38388;&#20013;&#65292;&#37492;&#21035;&#20986;&#23545;&#24212;&#20110;&#33258;&#28982;&#8212;&#8212;&#23588;&#20854;&#26159;&#20154;&#31867;&#8212;&#8212;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20197;&#21450;&#31867;&#20154;&#26234;&#33021;&#30340;&#21306;&#22495;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#23450;&#20041;&#21021;&#27493;&#25506;&#32034;&#20102;&#22235;&#20010;&#26356;&#20808;&#36827;&#12289;&#21487;&#33021;&#26356;&#20855;&#20105;&#35758;&#24615;&#30340;&#35805;&#39064;&#65306;&#22855;&#28857;&#12289;&#29983;&#25104;AI&#12289;&#20262;&#29702;&#21644;&#30693;&#35782;&#20135;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligence is a difficult concept to define, despite many attempts at doing so. Rather than trying to settle on a single definition, this article introduces a broad perspective on what intelligence is, by laying out a cascade of definitions that induces both a nested hierarchy of three levels of intelligence and a wider-ranging space that is built around them and approximations to them. Within this intelligence space, regions are identified that correspond to both natural -- most particularly, human -- intelligence and artificial intelligence (AI), along with the crossover notion of humanlike intelligence. These definitions are then exploited in early explorations of four more advanced, and likely more controversial, topics: the singularity, generative AI, ethics, and intellectual property.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20013;&#22914;&#20309;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#30340;&#35821;&#20041;&#30456;&#21516;&#65292;&#22312;&#38271;&#23614;&#29616;&#35937;&#20013;&#25506;&#35752;&#20102;&#21387;&#32553;&#25552;&#39640;&#25512;&#24191;&#24615;&#33021;&#30340;&#35760;&#24518;&#35201;&#32032;&#12290;</title><link>http://arxiv.org/abs/2306.06238</link><description>&lt;p&gt;
&#29702;&#35299;&#38271;&#23614;&#25928;&#24212;&#23545;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Understanding the Effect of the Long Tail on Neural Network Compression. (arXiv:2306.06238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#21387;&#32553;&#20013;&#22914;&#20309;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#30340;&#35821;&#20041;&#30456;&#21516;&#65292;&#22312;&#38271;&#23614;&#29616;&#35937;&#20013;&#25506;&#35752;&#20102;&#21387;&#32553;&#25552;&#39640;&#25512;&#24191;&#24615;&#33021;&#30340;&#35760;&#24518;&#35201;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32593;&#32476;&#21387;&#32553;&#29616;&#22312;&#26159;&#31070;&#32463;&#32593;&#32476;&#30740;&#31350;&#30340;&#19968;&#20010;&#25104;&#29087;&#30340;&#23376;&#39046;&#22495;&#65292;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20197;&#20943;&#23567;&#27169;&#22411;&#23610;&#23544;&#21644;&#21152;&#36895;&#25512;&#26029;&#20026;&#30446;&#26631;&#65292;&#21516;&#26102;&#20445;&#25345;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#35266;&#23519;&#21040;&#65292;&#20165;&#20851;&#27880;&#24635;&#20307;&#20934;&#30830;&#24615;&#21487;&#33021;&#26159;&#35823;&#23548;&#30340;&#12290;&#20363;&#22914;&#65292;&#24050;&#32463;&#35777;&#26126;&#20840;&#27169;&#22411;&#21644;&#21387;&#32553;&#27169;&#22411;&#20043;&#38388;&#30340;&#24046;&#24322;&#21487;&#33021;&#20250;&#20559;&#21521;&#20110;&#22312;&#25968;&#25454;&#38598;&#20013;&#20302;&#39057;&#30340;&#31867;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#37325;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#21363;&#8220;&#25105;&#20204;&#33021;&#21542;&#22312;&#20445;&#25345;&#19982;&#21407;&#22987;&#32593;&#32476;&#35821;&#20041;&#31561;&#21516;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#32593;&#32476;&#21387;&#32553;&#65311;&#8221;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#37325;&#28857;&#20851;&#27880;&#35745;&#31639;&#26426;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;Feldman&#31561;&#20154;&#35266;&#23519;&#21040;&#30340;&#8220;&#38271;&#23614;&#8221;&#29616;&#35937;&#12290;&#20182;&#20204;&#35748;&#20026;&#65292;&#26576;&#20123;&#36755;&#20837;&#65288;&#36866;&#24403;&#23450;&#20041;&#65289;&#30340;&#35760;&#24518;&#23545;&#20110;&#23454;&#29616;&#33391;&#22909;&#30340;&#27867;&#21270;&#26159;&#24517;&#35201;&#30340;&#12290;&#30001;&#20110;&#21387;&#32553;&#38480;&#21046;&#20102;&#32593;&#32476;&#30340;&#23481;&#37327;&#65288;&#22240;&#27492;&#20063;&#38480;&#21046;&#20102;&#20854;&#35760;&#24518;&#33021;&#21147;&#65289;&#65292;&#25152;&#20197;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;
&lt;/p&gt;
&lt;p&gt;
Network compression is now a mature sub-field of neural network research: over the last decade, significant progress has been made towards reducing the size of models and speeding up inference, while maintaining the classification accuracy. However, many works have observed that focusing on just the overall accuracy can be misguided. E.g., it has been shown that mismatches between the full and compressed models can be biased towards under-represented classes. This raises the important research question, \emph{can we achieve network compression while maintaining ``semantic equivalence'' with the original network?} In this work, we study this question in the context of the ``long tail'' phenomenon in computer vision datasets observed by Feldman, et al. They argue that \emph{memorization} of certain inputs (appropriately defined) is essential to achieving good generalization. As compression limits the capacity of a network (and hence also its ability to memorize), we study the question: a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SequenceMatch&#30340;&#24102;&#26377;&#22238;&#28335;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#23567;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#24207;&#21015;&#21644;&#25968;&#25454;&#38598;&#24207;&#21015;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#27495;&#26469;&#20943;&#23569;&#22312;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22797;&#21512;&#35823;&#24046;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#22238;&#28335;&#21160;&#20316;&#12290;</title><link>http://arxiv.org/abs/2306.05426</link><description>&lt;p&gt;
SequenceMatch&#65306;&#24102;&#22238;&#28335;&#30340;&#33258;&#22238;&#24402;&#24207;&#21015;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking. (arXiv:2306.05426v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SequenceMatch&#30340;&#24102;&#26377;&#22238;&#28335;&#30340;&#33258;&#22238;&#24402;&#27169;&#22411;&#30340;&#27169;&#20223;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#26368;&#23567;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#24207;&#21015;&#21644;&#25968;&#25454;&#38598;&#24207;&#21015;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#27495;&#26469;&#20943;&#23569;&#22312;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#20013;&#30340;&#22797;&#21512;&#35823;&#24046;&#65292;&#24182;&#20801;&#35768;&#24341;&#20837;&#22238;&#28335;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#39046;&#22495;&#65292;&#33258;&#22238;&#24402;&#27169;&#22411;&#21487;&#20197;&#22312;&#39044;&#27979;&#19979;&#19968;&#20010;&#35266;&#27979;&#20540;&#30340;&#20219;&#21153;&#19978;&#33719;&#24471;&#39640;&#20284;&#28982;&#24230;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26368;&#22823;&#20284;&#28982;&#65288;MLE&#65289;&#30446;&#26631;&#19981;&#19968;&#23450;&#19982;&#33258;&#22238;&#24402;&#29983;&#25104;&#39640;&#36136;&#37327;&#24207;&#21015;&#30340;&#19979;&#28216;&#29992;&#20363;&#30456;&#21305;&#37197;&#12290;MLE&#30446;&#26631;&#25353;&#29031;&#25968;&#25454;&#20998;&#24067;&#19979;&#24207;&#21015;&#30340;&#39057;&#29575;&#21152;&#26435;&#65292;&#19981;&#25552;&#20379;&#27169;&#22411;&#22312;&#20998;&#24067;&#20043;&#22806;&#34892;&#20026;&#30340;&#25351;&#23548;&#65292;&#36825;&#20250;&#23548;&#33268;&#22312;&#33258;&#22238;&#24402;&#29983;&#25104;&#36807;&#31243;&#20013;&#22797;&#21512;&#35823;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#22797;&#21512;&#35823;&#24046;&#38382;&#39064;&#65292;&#25105;&#20204;&#23558;&#24207;&#21015;&#29983;&#25104;&#23450;&#20026;&#27169;&#20223;&#23398;&#20064;&#65288;IL&#65289;&#38382;&#39064;&#12290;&#36825;&#20351;&#25105;&#20204;&#21487;&#20197;&#26368;&#23567;&#21270;&#33258;&#22238;&#24402;&#27169;&#22411;&#29983;&#25104;&#30340;&#24207;&#21015;&#20998;&#24067;&#21644;&#25968;&#25454;&#38598;&#24207;&#21015;&#20043;&#38388;&#30340;&#21508;&#31181;&#20998;&#27495;&#65292;&#21253;&#25324;&#32771;&#34385;&#20986;&#20998;&#24067;&#24207;&#21015;&#30340;&#20998;&#27495;&#12290;IL&#26694;&#26550;&#36824;&#20801;&#35768;&#25105;&#20204;&#36890;&#36807;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#24341;&#20837;&#22238;&#26684;&#21160;&#20316;&#26469;&#24341;&#20837;&#22238;&#28335;&#12290;&#36825;&#36827;&#19968;&#27493;&#20943;&#36731;&#20102;&#22797;&#21512;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many domains, autoregressive models can attain high likelihood on the task of predicting the next observation. However, this maximum-likelihood (MLE) objective does not necessarily match a downstream use-case of autoregressively generating high-quality sequences. The MLE objective weights sequences proportionally to their frequency under the data distribution, with no guidance for the model's behaviour out of distribution (OOD): leading to compounding error during autoregressive generation. In order to address this compounding error problem, we formulate sequence generation as an imitation learning (IL) problem. This allows us to minimize a variety of divergences between the distribution of sequences generated by an autoregressive model and sequences from a dataset, including divergences with weight on OOD generated sequences. The IL framework also allows us to incorporate backtracking by introducing a backspace action into the generation process. This further mitigates the compound
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#31354;&#38388;&#25968;&#25454;&#20998;&#21306;&#30340;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#19979;&#24320;&#21457;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#36895;&#36317;&#31163;&#36830;&#25509;&#26597;&#35810;&#65292;&#32553;&#30701;&#24037;&#20316;&#36127;&#36733;&#36816;&#34892;&#26102;&#38388;&#39640;&#36798;59.4&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.04846</link><description>&lt;p&gt;
&#23398;&#20064;&#31354;&#38388;&#25968;&#25454;&#20998;&#21306;
&lt;/p&gt;
&lt;p&gt;
Learned spatial data partitioning. (arXiv:2306.04846v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04846
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23398;&#20064;&#31354;&#38388;&#25968;&#25454;&#20998;&#21306;&#30340;&#26041;&#27861;&#65292;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#26694;&#26550;&#19979;&#24320;&#21457;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#31639;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#21152;&#36895;&#36317;&#31163;&#36830;&#25509;&#26597;&#35810;&#65292;&#32553;&#30701;&#24037;&#20316;&#36127;&#36733;&#36816;&#34892;&#26102;&#38388;&#39640;&#36798;59.4&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#31354;&#38388;&#25968;&#25454;&#30340;&#22823;&#23567;&#26174;&#33879;&#22686;&#21152;&#65292;&#20351;&#29992;&#20998;&#24067;&#24335;&#24182;&#34892;&#22788;&#29702;&#31995;&#32479;&#26377;&#25928;&#22320;&#20998;&#26512;&#31354;&#38388;&#25968;&#25454;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#23398;&#20064;&#31354;&#38388;&#25968;&#25454;&#20998;&#21306;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#36890;&#36807;&#22522;&#20110;&#25968;&#25454;&#20301;&#32622;&#30340;&#20998;&#32452;&#23558;&#22823;&#35268;&#27169;&#31354;&#38388;&#25968;&#25454;&#20998;&#37197;&#21040;&#35745;&#31639;&#26426;&#19978;&#12290;&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064;&#30340;&#19978;&#19979;&#25991;&#20013;&#24418;&#24335;&#21270;&#20102;&#31354;&#38388;&#25968;&#25454;&#20998;&#21306;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#23398;&#20064;&#31639;&#27861;&#21033;&#29992;&#31354;&#38388;&#25968;&#25454;&#20998;&#21306;&#30340;&#29305;&#24449;&#65292;&#24182;&#21098;&#26525;&#26080;&#25928;&#30340;&#23398;&#20064;&#36807;&#31243;&#65292;&#20197;&#26377;&#25928;&#22320;&#25214;&#21040;&#26368;&#20248;&#20998;&#21306;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#30740;&#31350;&#20351;&#29992;Apache Sedona&#21644;&#30495;&#23454;&#30340;&#31354;&#38388;&#25968;&#25454;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25214;&#21040;&#20998;&#21306;&#65292;&#21152;&#36895;&#36317;&#31163;&#36830;&#25509;&#26597;&#35810;&#65292;&#24182;&#23558;&#24037;&#20316;&#36127;&#36733;&#36816;&#34892;&#26102;&#38388;&#32553;&#30701;&#20102;59.4&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the significant increase in the size of spatial data, it is essential to use distributed parallel processing systems to efficiently analyze spatial data. In this paper, we first study learned spatial data partitioning, which effectively assigns groups of big spatial data to computers based on locations of data by using machine learning techniques. We formalize spatial data partitioning in the context of reinforcement learning and develop a novel deep reinforcement learning algorithm. Our learning algorithm leverages features of spatial data partitioning and prunes ineffective learning processes to find optimal partitions efficiently. Our experimental study, which uses Apache Sedona and real-world spatial data, demonstrates that our method efficiently finds partitions for accelerating distance join queries and reduces the workload run time by up to 59.4%.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21307;&#23398;&#25104;&#20687;&#20013;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#12289;&#31574;&#30053;&#21644;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#31616;&#21270;&#20102;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#32467;&#21512;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;&#35745;&#31639;&#26426;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#31934;&#24230;&#21644;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.04750</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#30340;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
AutoML Systems For Medical Imaging. (arXiv:2306.04750v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04750
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#21307;&#23398;&#25104;&#20687;&#20013;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#30340;&#24212;&#29992;&#12289;&#31574;&#30053;&#21644;&#25216;&#26415;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#31616;&#21270;&#20102;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#21019;&#24314;&#65292;&#32467;&#21512;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;&#35745;&#31639;&#26426;&#31995;&#32479;&#21487;&#20197;&#25552;&#39640;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#30340;&#31934;&#24230;&#21644;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#24341;&#20837;&#26426;&#22120;&#23398;&#20064;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#21307;&#29983;&#25552;&#20379;&#30340;&#21307;&#30103;&#20445;&#20581;&#26381;&#21153;&#30340;&#36136;&#37327;&#12290;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;&#35745;&#31639;&#26426;&#31995;&#32479;&#30340;&#32467;&#21512;&#21487;&#20197;&#25552;&#39640;&#35786;&#26029;&#31934;&#24230;&#12290;&#33258;&#21160;&#21270;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#31070;&#32463;&#32467;&#26500;&#25628;&#32034;&#21644;&#36801;&#31227;&#23398;&#20064;&#25216;&#26415;&#31616;&#21270;&#20102;&#23450;&#21046;&#21270;&#22270;&#20687;&#35782;&#21035;&#27169;&#22411;&#30340;&#21019;&#24314;&#12290;&#21307;&#23398;&#25104;&#20687;&#25216;&#26415;&#29992;&#20110;&#26080;&#21019;&#22320;&#21019;&#24314;&#20869;&#37096;&#22120;&#23448;&#21644;&#36523;&#20307;&#37096;&#20301;&#22270;&#20687;&#65292;&#20197;&#36827;&#34892;&#35786;&#26029;&#21644;&#25805;&#20316;&#30446;&#30340;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#35777;&#25454;&#65292;&#31361;&#20986;&#21307;&#23398;&#25104;&#20687;&#20013;AutoML&#30340;&#28508;&#22312;&#24212;&#29992;&#12289;&#31574;&#30053;&#21644;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
The integration of machine learning in medical image analysis can greatly enhance the quality of healthcare provided by physicians. The combination of human expertise and computerized systems can result in improved diagnostic accuracy. An automated machine learning approach simplifies the creation of custom image recognition models by utilizing neural architecture search and transfer learning techniques. Medical imaging techniques are used to non-invasively create images of internal organs and body parts for diagnostic and procedural purposes. This article aims to highlight the potential applications, strategies, and techniques of AutoML in medical imaging through theoretical and empirical evidence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.04723</link><description>&lt;p&gt;
&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#20869;&#37096;&#32500;&#24230;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Intrinsic Dimension Estimation for Robust Detection of AI-Generated Texts. (arXiv:2306.04723v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#34913;&#37327;&#25991;&#26412;&#20869;&#37096;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#40065;&#26834;&#24615;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#26816;&#27979;&#65292;&#23637;&#31034;&#20102;&#20154;&#31867;&#25991;&#26412;&#19982;AI&#29983;&#25104;&#25991;&#26412;&#22312;&#20869;&#37096;&#32500;&#24230;&#19978;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24555;&#36895;&#25552;&#39640;&#30340;AI&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#20351;&#24471;&#24456;&#38590;&#21306;&#20998;&#20154;&#31867;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#65292;&#36825;&#21487;&#33021;&#20250;&#23545;&#31038;&#20250;&#20135;&#29983;&#19981;&#33391;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#30740;&#31350;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#23646;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20154;&#31867;&#25991;&#26412;&#30340;&#19981;&#21464;&#29305;&#24449;&#65292;&#21363;&#32473;&#23450;&#25991;&#26412;&#26679;&#26412;&#23884;&#20837;&#38598;&#21512;&#19979;&#30340;&#27969;&#24418;&#30340;&#20869;&#37096;&#32500;&#24230;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#27969;&#30021;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#22312;&#20960;&#20010;&#22522;&#20110;&#23383;&#27597;&#30340;&#35821;&#35328;&#20013;&#32422;&#20026; $9$&#65292;&#32780;&#20013;&#25991;&#32422;&#20026; $7$&#65292;&#32780;&#27599;&#31181;&#35821;&#35328;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#24179;&#22343;&#20869;&#37096;&#32500;&#24230;&#36739;&#20302;&#65292;&#24046;&#32422; $1.5$&#65292;&#24182;&#19988;&#26377;&#26126;&#26174;&#30340;&#32479;&#35745;&#20998;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;
Rapidly increasing quality of AI-generated content makes it difficult to distinguish between human and AI-generated texts, which may lead to undesirable consequences for society. Therefore, it becomes increasingly important to study the properties of human texts that are invariant over text domains and various proficiency of human writers, can be easily calculated for any language, and can robustly separate natural and AI-generated texts regardless of the generation model and sampling method. In this work, we propose such an invariant of human texts, namely the intrinsic dimensionality of the manifold underlying the set of embeddings of a given text sample. We show that the average intrinsic dimensionality of fluent texts in natural language is hovering around the value $9$ for several alphabet-based languages and around $7$ for Chinese, while the average intrinsic dimensionality of AI-generated texts for each language is $\approx 1.5$ lower, with a clear statistical separation between
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#33410;&#65292;&#35299;&#38145;&#20102;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30452;&#25509;&#36866;&#24212;&#36830;&#32493;&#35821;&#38899;&#21040;&#31163;&#25955;&#26631;&#35760;&#30340;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#38899;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.02207</link><description>&lt;p&gt;
SpeechGen: &#21033;&#29992;&#25552;&#31034;&#35299;&#38145;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
SpeechGen: Unlocking the Generative Power of Speech Language Models with Prompts. (arXiv:2306.02207v2 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02207
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#20010;&#21517;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#33410;&#65292;&#35299;&#38145;&#20102;&#35821;&#38899;&#35821;&#35328;&#27169;&#22411;&#30340;&#29983;&#25104;&#33021;&#21147;&#65292;&#25104;&#21151;&#22320;&#23454;&#29616;&#20102;&#30452;&#25509;&#36866;&#24212;&#36830;&#32493;&#35821;&#38899;&#21040;&#31163;&#25955;&#26631;&#35760;&#30340;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#38899;&#29983;&#25104;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#20013;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#65292;&#29305;&#21035;&#26159;&#38543;&#30528;ChatGPT&#30340;&#20986;&#29616;&#12290;&#28982;&#32780;&#65292;&#23558;&#36830;&#32493;&#35821;&#38899;&#30452;&#25509;&#36866;&#24212;&#20110;&#22788;&#29702;&#31163;&#25955;&#26631;&#35760;&#30340;LLM&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#36825;&#22952;&#30861;&#20102;LLM&#22312;&#35821;&#38899;&#29983;&#25104;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#39640;&#32423;&#35821;&#38899;LM&#20204;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#35821;&#38899;&#20449;&#21495;&#25152;&#21253;&#21547;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#21253;&#25324;&#35828;&#35805;&#32773;&#21644;&#24773;&#24863;&#31561;&#65292;&#36825;&#20123;&#20449;&#24687;&#20165;&#36890;&#36807;&#25991;&#26412;&#25968;&#25454;&#26080;&#27861;&#33719;&#21462;&#12290;&#22312;&#19968;&#20123;&#35821;&#38899;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#31616;&#21333;&#30340;&#25552;&#31034;&#35843;&#25972;&#24050;&#32463;&#34920;&#29616;&#20986;&#26126;&#26174;&#30340;&#21442;&#25968;&#25928;&#29575;&#21644;&#31454;&#20105;&#24615;&#33021;&#30340;&#25552;&#39640;&#12290;&#20294;&#22312;&#22810;&#22823;&#31243;&#24230;&#19978;&#25552;&#31034;&#33021;&#22815;&#26377;&#25928;&#22320;&#28608;&#21457;&#35821;&#38899;LM&#30340;&#29983;&#25104;&#20219;&#21153;&#20173;&#28982;&#26159;&#19968;&#20010;&#26410;&#30693;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#39033;&#20808;&#39537;&#24615;&#30740;&#31350;&#65292;&#35813;&#30740;&#31350;&#22312;&#31216;&#20026;SpeechGen&#30340;&#32479;&#19968;&#26694;&#26550;&#20013;&#20351;&#29992;&#25552;&#31034;&#35843;&#33410;&#26469;&#21050;&#28608;&#35821;&#38899;LM&#36827;&#34892;&#21508;&#31181;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#20855;&#26377;&#32422;10M&#21487;&#35757;&#32451;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have gained considerable attention for Artificial Intelligence Generated Content (AIGC), particularly with the emergence of ChatGPT. However, the direct adaptation of continuous speech to LLMs that process discrete tokens remains an unsolved challenge, hindering the application of LLMs for speech generation. The advanced speech LMs are in the corner, as that speech signals encapsulate a wealth of information, including speaker and emotion, beyond textual data alone. Prompt tuning has demonstrated notable gains in parameter efficiency and competitive performance on some speech classification tasks. However, the extent to which prompts can effectively elicit generation tasks from speech LMs remains an open question. In this paper, we present pioneering research that explores the application of prompt tuning to stimulate speech LMs for various generation tasks, within a unified framework called SpeechGen, with around 10M trainable parameters. The proposed unif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;X&#23556;&#32447;&#25104;&#20687;&#12289;MRI&#21644;&#26680;&#21307;&#23398;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;&#25104;&#20687;&#27169;&#24577;&#20013;&#36827;&#34892;&#31995;&#32479;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#65292;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;</title><link>http://arxiv.org/abs/2306.02055</link><description>&lt;p&gt;
X-Ray&#25104;&#20687;&#12289;MRI&#21644;&#26680;&#21307;&#23398;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Case Studies on X-Ray Imaging, MRI and Nuclear Imaging. (arXiv:2306.02055v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;X&#23556;&#32447;&#25104;&#20687;&#12289;MRI&#21644;&#26680;&#21307;&#23398;&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#21487;&#20197;&#23454;&#29616;&#20174;&#25104;&#20687;&#27169;&#24577;&#20013;&#36827;&#34892;&#31995;&#32479;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#65292;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25104;&#20687;&#39046;&#22495;&#26159;&#21307;&#23398;&#31185;&#23398;&#30340;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#28041;&#21450;&#21508;&#31181;&#24418;&#24335;&#30340;&#36752;&#23556;&#26469;&#25429;&#33719;&#36523;&#20307;&#20869;&#37096;&#32452;&#32455;&#21644;&#22120;&#23448;&#30340;&#22270;&#20687;&#12290;&#36825;&#20123;&#22270;&#20687;&#20026;&#20020;&#24202;&#35786;&#26029;&#25552;&#20379;&#20102;&#37325;&#35201;&#20449;&#24687;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;X&#23556;&#32447;&#12289;MRI&#21644;&#26680;&#21307;&#23398;&#22312;&#21457;&#29616;&#20005;&#37325;&#30142;&#30149;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35780;&#20272;&#21644;&#23384;&#20648;&#36825;&#20123;&#22270;&#20687;&#21487;&#33021;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#32791;&#26102;&#30340;&#36807;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#65292;&#24050;&#32463;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#29992;&#20110;&#20174;&#25104;&#20687;&#27169;&#24577;&#20013;&#36827;&#34892;&#31995;&#32479;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#65292;&#20174;&#32780;&#24110;&#21161;&#21307;&#29983;&#36827;&#34892;&#24555;&#36895;&#20934;&#30830;&#30340;&#35786;&#26029;&#12290;&#22312;&#26412;&#32508;&#36848;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23558;&#37325;&#28857;&#25506;&#35752;&#22522;&#20110;AI&#30340;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22914;&#20309;&#36890;&#36807;&#21307;&#23398;&#25104;&#20687;&#25216;&#26415;&#24110;&#21161;&#30142;&#30149;&#26816;&#27979;&#12290;CNN&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#22270;&#20687;&#20998;&#26512;&#26041;&#27861;&#65292;&#22240;&#20854;&#33021;&#22815;&#23398;&#20064;&#22797;&#26434;&#30340;&#22270;&#20687;&#29305;&#24449;&#24182;&#35782;&#21035;&#20256;&#32479;&#26041;&#27861;&#38590;&#20197;&#26816;&#27979;&#30340;&#27169;&#24335;&#32780;&#33719;&#24471;&#24191;&#27867;&#24212;&#29992;&#12290;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#65292;&#25105;&#20204;&#23558;&#23637;&#31034;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21307;&#23398;&#25104;&#20687;&#22312;&#20020;&#24202;&#23454;&#36341;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of medical imaging is an essential aspect of the medical sciences, involving various forms of radiation to capture images of the internal tissues and organs of the body. These images provide vital information for clinical diagnosis, and in this chapter, we will explore the use of X-ray, MRI, and nuclear imaging in detecting severe illnesses. However, manual evaluation and storage of these images can be a challenging and time-consuming process. To address this issue, artificial intelligence (AI)-based techniques, particularly deep learning (DL), have become increasingly popular for systematic feature extraction and classification from imaging modalities, thereby aiding doctors in making rapid and accurate diagnoses. In this review study, we will focus on how AI-based approaches, particularly the use of Convolutional Neural Networks (CNN), can assist in disease detection through medical imaging technology. CNN is a commonly used approach for image analysis due to its ability to
&lt;/p&gt;</description></item><item><title>&#20174;&#31070;&#32463;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#20855;&#22791;&#21754;&#20083;&#21160;&#29289;&#24847;&#35782;&#24863;&#30693;&#30456;&#20851;&#30340;&#19992;&#33041;&#30382;&#23618;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#32570;&#20047;&#21608;&#22260;&#19990;&#30028;&#30340;&#20855;&#20307;&#23884;&#20837;&#24335;&#20449;&#24687;&#65292;&#19988;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#26080;&#27861;&#20570;&#21040;&#23384;&#22312;&#30340;&#20381;&#36182;&#20110;&#20854;&#34892;&#20026;&#65292;&#36825;&#24847;&#21619;&#30528;&#20154;&#24037;&#24847;&#35782;&#30340;&#21487;&#34892;&#24615;&#23384;&#22312;&#29942;&#39048;&#12290;</title><link>http://arxiv.org/abs/2306.00915</link><description>&lt;p&gt;
&#36890;&#36807;&#31070;&#32463;&#31185;&#23398;&#30340;&#35270;&#35282;&#25506;&#31350;&#20154;&#24037;&#24847;&#35782;&#30340;&#21487;&#34892;&#24615;
&lt;/p&gt;
&lt;p&gt;
The feasibility of artificial consciousness through the lens of neuroscience. (arXiv:2306.00915v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00915
&lt;/p&gt;
&lt;p&gt;
&#20174;&#31070;&#32463;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#30446;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38590;&#20197;&#20855;&#22791;&#21754;&#20083;&#21160;&#29289;&#24847;&#35782;&#24863;&#30693;&#30456;&#20851;&#30340;&#19992;&#33041;&#30382;&#23618;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#65292;&#32570;&#20047;&#21608;&#22260;&#19990;&#30028;&#30340;&#20855;&#20307;&#23884;&#20837;&#24335;&#20449;&#24687;&#65292;&#19988;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#26080;&#27861;&#20570;&#21040;&#23384;&#22312;&#30340;&#20381;&#36182;&#20110;&#20854;&#34892;&#20026;&#65292;&#36825;&#24847;&#21619;&#30528;&#20154;&#24037;&#24847;&#35782;&#30340;&#21487;&#34892;&#24615;&#23384;&#22312;&#29942;&#39048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24341;&#21457;&#20102;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#24847;&#35782;&#30340;&#29468;&#27979;&#12290;&#20174;&#31070;&#32463;&#31185;&#23398;&#30340;&#35282;&#24230;&#26469;&#30475;&#65292;&#36825;&#31181;&#35266;&#28857;&#24456;&#38590;&#34987;&#35777;&#23454;&#12290;&#39318;&#20808;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26550;&#26500;&#32570;&#23569;&#21754;&#20083;&#21160;&#29289;&#24847;&#35782;&#24863;&#30693;&#30456;&#20851;&#30340;&#19992;&#33041;&#30382;&#23618;&#31995;&#32479;&#30340;&#20851;&#38190;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#36755;&#20837;&#32570;&#20047;&#25105;&#20204;&#19982;&#21608;&#22260;&#19990;&#30028;&#30340;&#24863;&#23448;&#25509;&#35302;&#30340;&#20855;&#26377;&#20307;&#39564;&#12289;&#23884;&#20837;&#24335;&#20449;&#24687;&#30340;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#34429;&#28982;&#21069;&#20004;&#20010;&#35770;&#28857;&#22312;&#26410;&#26469;&#30340;AI&#31995;&#32479;&#20013;&#21487;&#20197;&#34987;&#20811;&#26381;&#65292;&#20294;&#31532;&#19977;&#20010;&#21487;&#33021;&#26356;&#38590;&#22312;&#19981;&#20037;&#30340;&#23558;&#26469;&#36328;&#36234;&#12290;&#25442;&#35328;&#20043;&#65292;&#25105;&#20204;&#35748;&#20026;&#24847;&#35782;&#21487;&#33021;&#21462;&#20915;&#20110;&#26159;&#21542;&#22312;&#8220;&#28216;&#25103;&#20013;&#26377;&#30382;&#32932;&#8221;&#65292;&#21363;&#31995;&#32479;&#30340;&#23384;&#22312;&#26159;&#21542;&#21462;&#20915;&#20110;&#20854;&#34892;&#20026;&#65292;&#32780;&#36825;&#22312;&#24403;&#21069;&#30340;&#20154;&#24037;&#26234;&#33021;&#20013;&#24182;&#19981;&#25104;&#31435;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interactions with large language models have led to the suggestion that these models may be conscious. From the perspective of neuroscience, this position is difficult to defend. For one, the architecture of large language models is missing key features of the thalamocortical system that have been linked to conscious awareness in mammals. Secondly, the inputs to large language models lack the embodied, embedded information content characteristic of our sensory contact with the world around us. Finally, while the previous two arguments can be overcome in future AI systems, the third one might be harder to bridge in the near future. Namely, we argue that consciousness might depend on having 'skin in the game', in that the existence of the system depends on its actions, which is not true for present-day artificial intelligence.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#21644;&#23436;&#20840;&#20934;&#30830;&#26102;&#20998;&#21035;&#37319;&#21462;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;ImageNet&#21644;nuScenes&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#24635;&#32467;&#26356;&#22909;&#12290;</title><link>http://arxiv.org/abs/2306.00265</link><description>&lt;p&gt;
&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Doubly Robust Self-Training. (arXiv:2306.00265v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#20266;&#26631;&#31614;&#19981;&#20934;&#30830;&#21644;&#23436;&#20840;&#20934;&#30830;&#26102;&#20998;&#21035;&#37319;&#21462;&#19981;&#21516;&#30340;&#35757;&#32451;&#31574;&#30053;&#65292;&#23454;&#29616;&#26377;&#25928;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;ImageNet&#21644;nuScenes&#25968;&#25454;&#38598;&#19978;&#22343;&#27604;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#24635;&#32467;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#25105;&#35757;&#32451;&#26159;&#35299;&#20915;&#21322;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#19968;&#31181;&#37325;&#35201;&#25216;&#26415;&#12290;&#23427;&#36890;&#36807;&#29983;&#25104;&#20266;&#26631;&#31614;&#24182;&#23558;&#20854;&#19982;&#26377;&#38480;&#30340;&#26631;&#35760;&#25968;&#25454;&#38598;&#32467;&#21512;&#20351;&#29992;&#36827;&#34892;&#35757;&#32451;&#65292;&#20174;&#32780;&#21033;&#29992;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#33258;&#25105;&#35757;&#32451;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#36825;&#20123;&#20266;&#26631;&#31614;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#21452;&#37325;&#31283;&#20581;&#33258;&#25105;&#35757;&#32451;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#21322;&#30417;&#30563;&#31639;&#27861;&#65292;&#21487;&#20197;&#20445;&#35777;&#22312;&#20004;&#20010;&#26497;&#31471;&#20043;&#38388;&#24179;&#34913;&#12290;&#24403;&#20266;&#26631;&#31614;&#23436;&#20840;&#19981;&#27491;&#30830;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#34987;&#20943;&#23569;&#21040;&#20165;&#20351;&#29992;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#12290;&#30456;&#21453;&#65292;&#24403;&#20266;&#26631;&#31614;&#23436;&#20840;&#20934;&#30830;&#26102;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#21464;&#25104;&#21033;&#29992;&#25152;&#26377;&#20266;&#26631;&#31614;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#36807;&#31243;&#65292;&#20174;&#32780;&#22686;&#21152;&#26377;&#25928;&#30340;&#26679;&#26412;&#37327;&#12290;&#36890;&#36807;&#22312;ImageNet&#22270;&#20687;&#20998;&#31867;&#21644;nuScenes&#33258;&#20027;&#39550;&#39542;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21452;&#37325;&#31283;&#20581;&#25439;&#22833;&#20248;&#20110;&#26631;&#20934;&#33258;&#25105;&#35757;&#32451;&#22522;&#32447;&#30340;&#20248;&#36234;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-training is an important technique for solving semi-supervised learning problems. It leverages unlabeled data by generating pseudo-labels and combining them with a limited labeled dataset for training. The effectiveness of self-training heavily relies on the accuracy of these pseudo-labels. In this paper, we introduce doubly robust self-training, a novel semi-supervised algorithm that provably balances between two extremes. When the pseudo-labels are entirely incorrect, our method reduces to a training process solely using labeled data. Conversely, when the pseudo-labels are completely accurate, our method transforms into a training process utilizing all pseudo-labeled data and labeled data, thus increasing the effective sample size. Through empirical evaluations on both the ImageNet dataset for image classification and the nuScenes autonomous driving dataset for 3D object detection, we demonstrate the superiority of the doubly robust loss over the standard self-training baseline.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#8212;&#8212;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#65292;&#29992;&#20110;&#25193;&#23637;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#23545;&#28508;&#22312;&#32467;&#26500;&#30340;&#24863;&#30693;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#26463;&#25628;&#32034;&#20013;&#30828;&#21069;k&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26799;&#24230;&#20449;&#21495;&#20256;&#36882;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;BT-Cell&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#22810;&#20010;&#20855;&#26377;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.19999</link><description>&lt;p&gt;
&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65306;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Beam Tree Recursive Cells. (arXiv:2305.19999v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25903;&#25345;&#21453;&#21521;&#20256;&#25773;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#26694;&#26550;&#8212;&#8212;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#65292;&#29992;&#20110;&#25193;&#23637;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#23545;&#28508;&#22312;&#32467;&#26500;&#30340;&#24863;&#30693;&#65307;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25918;&#26494;&#26463;&#25628;&#32034;&#20013;&#30828;&#21069;k&#31639;&#23376;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#26799;&#24230;&#20449;&#21495;&#20256;&#36882;&#12290;&#22312;&#35780;&#20272;&#20013;&#21457;&#29616;&#65292;BT-Cell&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#22810;&#20010;&#20855;&#26377;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#26463;&#25628;&#32034;&#36882;&#24402;&#21333;&#20803;&#65288;BT-Cell&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#25193;&#23637;&#25903;&#25345;&#20351;&#29992;&#26463;&#25628;&#32034;&#36827;&#34892;&#28508;&#22312;&#32467;&#26500;&#24863;&#30693;&#30340;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RvNN&#65289;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#25552;&#20986;&#22312;&#26463;&#25628;&#32034;&#20013;&#23545;&#30828;&#24615;&#21069;k&#31639;&#23376;&#30340;&#25918;&#26494;&#26469;&#25193;&#23637;&#27492;&#26694;&#26550;&#65292;&#20197;&#26356;&#22909;&#22320;&#20256;&#36882;&#26799;&#24230;&#20449;&#21495;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#30340;&#19981;&#21516;&#20195;&#34920;&#24615;&#20998;&#24067;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;BT-Cell&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20307;&#29616;&#32467;&#26500;&#25935;&#24863;&#24615;&#30340;&#20219;&#21153;&#65288;&#22914;ListOps&#21644;&#36923;&#36753;&#25512;&#29702;&#65289;&#19978;&#36798;&#21040;&#20102;&#20960;&#20046;&#23436;&#32654;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#22312;&#23454;&#38469;&#25968;&#25454;&#19978;&#19982;&#20854;&#20182;&#22522;&#20110;RvNN&#30340;&#27169;&#22411;&#20855;&#26377;&#21487;&#27604;&#24615;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;ListOps&#20013;&#30830;&#23450;&#20102;&#31070;&#32463;&#27169;&#22411;&#22312;&#25512;&#24191;&#21040;&#26410;&#35265;&#36807;&#30340;&#21442;&#25968;&#25968;&#37327;&#19978;&#30340;&#26410;&#30693;&#22833;&#25928;&#26696;&#20363;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/JRC1995/BeamTreeRecursiveCells&#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Beam Tree Recursive Cell (BT-Cell) - a backpropagation-friendly framework to extend Recursive Neural Networks (RvNNs) with beam search for latent structure induction. We further extend this framework by proposing a relaxation of the hard top-k operators in beam search for better propagation of gradient signals. We evaluate our proposed models in different out-of-distribution splits in both synthetic and realistic data. Our experiments show that BTCell achieves near-perfect performance on several challenging structure-sensitive synthetic tasks like ListOps and logical inference while maintaining comparable performance in realistic data against other RvNN-based models. Additionally, we identify a previously unknown failure case for neural models in generalization to unseen number of arguments in ListOps. The code is available at: https://github.com/JRC1995/BeamTreeRecursiveCells.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#22411;&#36229;&#22768;&#22270;&#20687;&#21069;&#21015;&#33146;&#20998;&#21106;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#27880;&#37322;&#24341;&#23548;&#30340;Transformer UNet&#27169;&#22411;&#21644;&#27880;&#37322;&#24341;&#23548;&#30340;&#20108;&#20998;&#31867;&#20132;&#21449;&#29109;&#25439;&#22833;&#35299;&#20915;&#20302;&#20998;&#36776;&#29575;&#21644;&#30028;&#38480;&#19981;&#28165;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#20998;&#21106;&#30340;&#21306;&#22495;&#12290;</title><link>http://arxiv.org/abs/2305.19956</link><description>&lt;p&gt;
MicroSegNet&#65306;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#22411;&#36229;&#22768;&#22270;&#20687;&#21069;&#21015;&#33146;&#20998;&#21106;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
MicroSegNet: A Deep Learning Approach for Prostate Segmentation on Micro-Ultrasound Images. (arXiv:2305.19956v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24494;&#22411;&#36229;&#22768;&#22270;&#20687;&#21069;&#21015;&#33146;&#20998;&#21106;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#23610;&#24230;&#27880;&#37322;&#24341;&#23548;&#30340;Transformer UNet&#27169;&#22411;&#21644;&#27880;&#37322;&#24341;&#23548;&#30340;&#20108;&#20998;&#31867;&#20132;&#21449;&#29109;&#25439;&#22833;&#35299;&#20915;&#20302;&#20998;&#36776;&#29575;&#21644;&#30028;&#38480;&#19981;&#28165;&#30340;&#25361;&#25112;&#65292;&#35813;&#26041;&#27861;&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#20998;&#21106;&#30340;&#21306;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#22411;&#36229;&#22768;&#26159;&#19968;&#31181;&#26032;&#22411;&#30340;29MHz&#36229;&#22768;&#25216;&#26415;&#65292;&#25552;&#20379;&#27604;&#20256;&#32479;&#36229;&#22768;&#39640;3-4&#20493;&#30340;&#20998;&#36776;&#29575;&#65292;&#22312;&#35786;&#26029;&#21069;&#21015;&#33146;&#30284;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#19982;MRI&#30456;&#24403;&#65292;&#20294;&#25104;&#26412;&#26356;&#20302;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20302;&#20998;&#36776;&#29575;&#21644;&#21069;&#21015;&#33146;&#12289;&#33152;&#33009;&#21644;&#23615;&#36947;&#20013;&#32447;&#20043;&#38388;&#30340;&#30028;&#38480;&#19981;&#28165;&#65292;&#22522;&#20110;&#24494;&#22411;&#36229;&#22768;&#30340;&#21069;&#21015;&#33146;&#20998;&#21106;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MicroSegNet&#65292;&#36825;&#26159;&#19968;&#20010;&#29305;&#21035;&#35774;&#35745;&#29992;&#20110;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#30340;&#22810;&#23610;&#24230;&#27880;&#37322;&#24341;&#23548;&#30340;Transformer UNet&#27169;&#22411;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;MicroSegNet&#26356;&#21152;&#20851;&#27880;&#38590;&#20197;&#20998;&#21106;&#65288;&#38590;&#21306;&#22495;&#65289;&#30340;&#21306;&#22495;&#65292;&#36825;&#20123;&#21306;&#22495;&#20855;&#26377;&#19987;&#23478;&#21644;&#38750;&#19987;&#23478;&#27880;&#37322;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27880;&#37322;&#24341;&#23548;&#30340;&#20108;&#20998;&#31867;&#20132;&#21449;&#29109;&#65288;AG-BCE&#65289;&#25439;&#22833;&#65292;&#23427;&#22312;&#38590;&#21306;&#22495;&#20013;&#32473;&#39044;&#27979;&#35823;&#24046;&#20998;&#37197;&#26356;&#22823;&#30340;&#26435;&#37325;&#21644;&#36739;&#20302;&#30340;&#26435;&#37325;&#12290;
&lt;/p&gt;
&lt;p&gt;
Micro-ultrasound (micro-US) is a novel 29-MHz ultrasound technique that provides 3-4 times higher resolution than traditional ultrasound, delivering comparable accuracy for diagnosing prostate cancer to MRI but at a lower cost. Accurate prostate segmentation is crucial for prostate volume measurement, cancer diagnosis, prostate biopsy, and treatment planning. However, prostate segmentation on microUS is challenging due to artifacts and indistinct borders between the prostate, bladder, and urethra in the midline. This paper presents MicroSegNet, a multi-scale annotation-guided transformer UNet model designed specifically to tackle these challenges. During the training process, MicroSegNet focuses more on regions that are hard to segment (hard regions), characterized by discrepancies between expert and non-expert annotations. We achieve this by proposing an annotation-guided binary cross entropy (AG-BCE) loss that assigns a larger weight to prediction errors in hard regions and a lower w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#23558;&#20307;&#20869;&#24494;-US&#22270;&#20687;&#19982;&#31163;&#20307;&#20840;&#20999;&#29255;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#37197;&#20934;&#65292;&#20197;&#24110;&#21161;&#27852;&#23615;&#22806;&#31185;&#21307;&#29983;&#25552;&#39640;&#23567;&#21069;&#21015;&#33146;&#30284;&#30340;&#26816;&#27979;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.19939</link><description>&lt;p&gt;
&#21069;&#21015;&#33146;&#20307;&#20869;&#24494;&#22411;&#36229;&#22768;&#19982;&#31163;&#20307;&#20266;&#20840;&#20999;&#29255;&#32452;&#32455;&#26631;&#26412;&#22270;&#20687;&#30340;&#22270;&#20687;&#37197;&#20934;: &#19968;&#20010;&#27010;&#24565;&#39564;&#35777;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Image Registration of In Vivo Micro-Ultrasound and Ex Vivo Pseudo-Whole Mount Histopathology Images of the Prostate: A Proof-of-Concept Study. (arXiv:2305.19939v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#21322;&#33258;&#21160;&#21270;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#23558;&#20307;&#20869;&#24494;-US&#22270;&#20687;&#19982;&#31163;&#20307;&#20840;&#20999;&#29255;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#37197;&#20934;&#65292;&#20197;&#24110;&#21161;&#27852;&#23615;&#22806;&#31185;&#21307;&#29983;&#25552;&#39640;&#23567;&#21069;&#21015;&#33146;&#30284;&#30340;&#26816;&#27979;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21069;&#21015;&#33146;&#30284;&#30340;&#26089;&#26399;&#35786;&#26029;&#26174;&#33879;&#25552;&#39640;&#20102;&#24739;&#32773;5&#24180;&#29983;&#23384;&#29575;&#12290;&#22270;&#20687;&#24341;&#23548;&#19979;&#30340;&#27963;&#26816;&#21487;&#20197;&#25913;&#21892;&#23545;&#23567;&#22411;&#21069;&#21015;&#33146;&#30284;&#30340;&#26816;&#27979;&#12290;MRI-&#36229;&#22768;&#34701;&#21512;&#24341;&#23548;&#19979;&#30340;&#27963;&#26816;&#23545;&#26356;&#23567;&#30340;&#32959;&#30244;&#25935;&#24863;&#65292;&#20294;&#30001;&#20110;MRI&#21644;&#34701;&#21512;&#35774;&#22791;&#30340;&#39640;&#25104;&#26412;&#32780;&#34987;&#23569;&#20351;&#29992;&#12290;&#24494;&#22411;&#36229;&#22768;&#65288;&#24494;-US&#65289;&#26159;&#19968;&#31181;&#26032;&#22411;&#39640;&#20998;&#36776;&#29575;&#36229;&#22768;&#25216;&#26415;&#65292;&#21487;&#25552;&#20379;MRI&#25104;&#20687;&#31867;&#20284;&#30340;&#35786;&#26029;&#31934;&#24230;&#65292;&#21516;&#26102;&#25104;&#26412;&#26356;&#20302;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30284;&#32454;&#32990;&#21644;&#27491;&#24120;&#32452;&#32455;&#20043;&#38388;&#30340;&#28784;&#24230;&#21464;&#21270;&#24494;&#24369;&#65292;&#22240;&#27492;&#35299;&#37322;&#24494;-US&#22270;&#20687;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#21487;&#20197;&#36890;&#36807;&#21521;&#27852;&#23615;&#22806;&#31185;&#21307;&#29983;&#25552;&#20379;&#19968;&#20010;&#21253;&#21547;&#22320;&#38754;&#30495;&#23454;&#30284;&#21464;&#21306;&#22495;&#30340;&#24494;-US&#22270;&#20687;&#22823;&#25968;&#25454;&#38598;&#26469;&#35299;&#20915;&#36825;&#20010;&#25361;&#25112;&#12290;&#36825;&#26679;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#37197;&#20934;&#23558;&#25163;&#26415;&#26631;&#26412;&#65288;&#32452;&#32455;&#30149;&#29702;&#23398;&#65289;&#26144;&#23556;&#21040;&#24494;-US&#22270;&#20687;&#19978;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21322;&#33258;&#21160;&#21270;&#30340;&#27969;&#31243;&#65292;&#29992;&#20110;&#23558;&#20307;&#20869;&#24494;-US&#22270;&#20687;&#19982;&#31163;&#20307;&#20840;&#20999;&#29255;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#37197;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Early diagnosis of prostate cancer significantly improves a patient's 5-year survival rate. Biopsy of small prostate cancers is improved with image-guided biopsy. MRI-ultrasound fusion-guided biopsy is sensitive to smaller tumors but is underutilized due to the high cost of MRI and fusion equipment. Micro-ultrasound (micro-US), a novel high-resolution ultrasound technology, provides a cost-effective alternative to MRI while delivering comparable diagnostic accuracy. However, the interpretation of micro-US is challenging due to subtle gray scale changes indicating cancer vs normal tissue. This challenge can be addressed by training urologists with a large dataset of micro-US images containing the ground truth cancer outlines. Such a dataset can be mapped from surgical specimens (histopathology) onto micro-US images via image registration. In this paper, we present a semi-automated pipeline for registering in vivo micro-US images with ex vivo whole-mount histopathology images. Our pipeli
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#31574;&#30053;&#32593;&#32476;&#65292;&#37325;&#26032;&#26694;&#23450;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#20026;&#34920;&#31034;-&#21033;&#29992;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25506;&#32034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24212;&#29992;&#36827;&#21270;&#21644;&#31574;&#30053;&#26799;&#24230;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.19922</link><description>&lt;p&gt;
&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Representation-Driven Reinforcement Learning. (arXiv:2305.19922v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19922
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#20013;&#23884;&#20837;&#31574;&#30053;&#32593;&#32476;&#65292;&#37325;&#26032;&#26694;&#23450;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#20026;&#34920;&#31034;-&#21033;&#29992;&#38382;&#39064;&#65292;&#20197;&#23454;&#29616;&#26368;&#20339;&#30340;&#25506;&#32034;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#24212;&#29992;&#36827;&#21270;&#21644;&#31574;&#30053;&#26799;&#24230;&#27861;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#34920;&#31034;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#12290;&#36890;&#36807;&#23558;&#31574;&#30053;&#34920;&#31034;&#20026;&#20854;&#26399;&#26395;&#20540;&#30340;&#20272;&#35745;&#65292;&#25105;&#20204;&#21033;&#29992;&#26469;&#33258;&#24773;&#22659;&#25512;&#26029;&#30340;&#26041;&#27861;&#26469;&#25351;&#23548;&#25506;&#32034;&#21644;&#21033;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#23558;&#31574;&#30053;&#32593;&#32476;&#23884;&#20837;&#21040;&#32447;&#24615;&#29305;&#24449;&#31354;&#38388;&#20013;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#23558;&#25506;&#32034;-&#21033;&#29992;&#38382;&#39064;&#37325;&#26032;&#26694;&#23450;&#20026;&#34920;&#31034;-&#21033;&#29992;&#38382;&#39064;&#65292;&#20854;&#20013;&#33391;&#22909;&#30340;&#31574;&#30053;&#34920;&#31034;&#33021;&#22815;&#23454;&#29616;&#26368;&#20339;&#30340;&#25506;&#32034;&#12290;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#36827;&#21270;&#21644;&#31574;&#30053;&#26799;&#24230;&#27861;&#26469;&#23637;&#31034;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#65292;&#30456;&#27604;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102;&#31574;&#30053;&#34920;&#31034;&#22312;&#20915;&#23450;&#26368;&#20339;&#25506;&#32034;-&#21033;&#29992;&#31574;&#30053;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a representation-driven framework for reinforcement learning. By representing policies as estimates of their expected values, we leverage techniques from contextual bandits to guide exploration and exploitation. Particularly, embedding a policy network into a linear feature space allows us to reframe the exploration-exploitation problem as a representation-exploitation problem, where good policy representations enable optimal exploration. We demonstrate the effectiveness of this framework through its application to evolutionary and policy gradient-based approaches, leading to significantly improved performance compared to traditional methods. Our framework provides a new perspective on reinforcement learning, highlighting the importance of policy representation in determining optimal exploration-exploitation strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31961;&#38598;&#29702;&#35770;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36716;&#21270;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#20026;&#20449;&#24687;&#34920;&#65292;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#35299;&#20915;&#39046;&#22495;&#30693;&#35782;&#33719;&#21462;&#21644;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.19718</link><description>&lt;p&gt;
&#31895;&#31961;&#38598;&#19979;&#19968;&#31181;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A rule-general abductive learning by rough sets. (arXiv:2305.19718v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19718
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31895;&#31961;&#38598;&#29702;&#35770;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#36716;&#21270;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#20026;&#20449;&#24687;&#34920;&#65292;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#35299;&#20915;&#39046;&#22495;&#30693;&#35782;&#33719;&#21462;&#21644;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;&#36890;&#24120;&#23384;&#22312;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#21644;&#26631;&#35760;&#25968;&#25454;&#12290;&#23558;&#20004;&#32773;&#32452;&#21512;&#36215;&#26469;&#36827;&#34892;&#23398;&#20064;&#30340;&#20219;&#21153;&#34987;&#31216;&#20026;&#21322;&#30417;&#30563;&#23398;&#20064;&#12290;&#19987;&#23478;&#21487;&#20197;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#26469;&#26631;&#35760;&#26410;&#26631;&#35760;&#25968;&#25454;&#65292;&#20294;&#36825;&#20010;&#25805;&#20316;&#24456;&#26114;&#36149;&#12290;&#24863;&#30693;&#21644;&#25512;&#29702;&#30340;&#32467;&#21512;&#22312;&#22788;&#29702;&#20855;&#26377;&#39046;&#22495;&#30693;&#35782;&#30340;&#21322;&#30417;&#30563;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;&#28982;&#32780;&#65292;&#33719;&#21462;&#39046;&#22495;&#30693;&#35782;&#20197;&#21450;&#35268;&#21017;&#30340;&#20462;&#27491;&#12289;&#20943;&#23569;&#21644;&#29983;&#25104;&#20173;&#28982;&#26159;&#38656;&#35201;&#35299;&#20915;&#30340;&#22797;&#26434;&#38382;&#39064;&#12290;&#31895;&#31961;&#38598;&#29702;&#35770;&#26159;&#35299;&#20915;&#20449;&#24687;&#31995;&#32479;&#20013;&#30693;&#35782;&#22788;&#29702;&#30340;&#37325;&#35201;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31895;&#31961;&#38598;&#19979;&#30340;&#35268;&#21017;&#36890;&#29992;&#36870;&#25512;&#23398;&#20064;&#26041;&#27861;&#65288;RS-ABL&#65289;&#12290;&#36890;&#36807;&#23558;&#35268;&#21017;&#30340;&#30446;&#26631;&#27010;&#24565;&#21644;&#23376;&#27010;&#24565;&#36716;&#21270;&#20026;&#20449;&#24687;&#34920;&#65292;&#21033;&#29992;&#31895;&#31961;&#38598;&#29702;&#35770;&#26469;&#35299;&#20915;&#20197;&#26356;&#20302;&#30340;&#25104;&#26412;&#33719;&#21462;&#39046;&#22495;&#30693;&#35782;&#21644;&#20462;&#27491;&#12289;&#20943;&#23569;&#12289;&#29983;&#25104;&#35268;&#21017;&#30340;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#36824;&#21487;&#20197;&#29983;&#25104;&#26356;&#24191;&#27867;&#30340;&#36127;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#35268;&#21017;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
In real-world tasks, there is usually a large amount of unlabeled data and labeled data. The task of combining the two to learn is known as semi-supervised learning. Experts can use logical rules to label unlabeled data, but this operation is costly. The combination of perception and reasoning has a good effect in processing such semi-supervised tasks with domain knowledge. However, acquiring domain knowledge and the correction, reduction and generation of rules remain complex problems to be solved. Rough set theory is an important method for solving knowledge processing in information systems. In this paper, we propose a rule general abductive learning by rough set (RS-ABL). By transforming the target concept and sub-concepts of rules into information tables, rough set theory is used to solve the acquisition of domain knowledge and the correction, reduction and generation of rules at a lower cost. This framework can also generate more extensive negative rules to enhance the breadth of
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;meta-learning&#26694;&#26550;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22823;&#23567;&#21644;&#20998;&#24067;&#21464;&#21270;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#12290;</title><link>http://arxiv.org/abs/2305.19587</link><description>&lt;p&gt;
&#38754;&#21521;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#30340;&#20840;&#36890;&#29992;&#31070;&#32463;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Omni-generalizable Neural Methods for Vehicle Routing Problems. (arXiv:2305.19587v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19587
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;meta-learning&#26694;&#26550;&#65292;&#20351;&#24471;&#27169;&#22411;&#21487;&#20197;&#22312;&#25512;&#29702;&#36807;&#31243;&#20013;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#12290;&#36890;&#36807;&#22312;&#22810;&#20010;&#21512;&#25104;&#21644;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#22823;&#23567;&#21644;&#20998;&#24067;&#21464;&#21270;&#30340;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#36991;&#20813;&#20102;&#23545;&#25163;&#24037;&#35268;&#21017;&#30340;&#20381;&#36182;&#65292;&#23398;&#20064;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRP&#65289;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#22312;&#22266;&#23450;&#22823;&#23567;&#21644;&#33410;&#28857;&#20998;&#24067;&#30340;&#21516;&#19968;&#20219;&#21153;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#65292;&#22240;&#27492;&#20855;&#26377;&#26377;&#38480;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#20294;&#21448;&#29616;&#23454;&#30340;&#22330;&#26223;&#65292;&#35813;&#22330;&#26223;&#32771;&#34385;&#20102;VRP&#22312;&#22823;&#23567;&#21644;&#20998;&#24067;&#26041;&#38754;&#30340;&#19968;&#33324;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#25512;&#29702;&#26399;&#38388;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26032;&#20219;&#21153;&#30340;&#33021;&#21147;&#19979;&#23545;&#21021;&#22987;&#21270;&#27169;&#22411;&#36827;&#34892;&#26377;&#25928;&#35757;&#32451;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#24320;&#21457;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36817;&#20284;&#26041;&#27861;&#26469;&#20943;&#23569;&#35757;&#32451;&#24320;&#38144;&#12290;&#23545;&#26053;&#34892;&#21830;&#38382;&#39064;&#65288;TSP&#65289;&#21644;&#23481;&#37327;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;CVRP&#65289;&#30340;&#21512;&#25104;&#21644;&#22522;&#20934;&#23454;&#20363;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/RoyalSkye/Omni-VRP&#24471;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning heuristics for vehicle routing problems (VRPs) has gained much attention due to the less reliance on hand-crafted rules. However, existing methods are typically trained and tested on the same task with a fixed size and distribution (of nodes), and hence suffer from limited generalization performance. This paper studies a challenging yet realistic setting, which considers generalization across both size and distribution in VRPs. We propose a generic meta-learning framework, which enables effective training of an initialized model with the capability of fast adaptation to new tasks during inference. We further develop a simple yet efficient approximation method to reduce the training overhead. Extensive experiments on both synthetic and benchmark instances of the traveling salesman problem (TSP) and capacitated vehicle routing problem (CVRP) demonstrate the effectiveness of our method. The code is available at: https://github.com/RoyalSkye/Omni-VRP.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;BiDf-MKD&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;API&#24211;&#20013;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#30452;&#25509;&#36827;&#34892;&#20803;&#23398;&#20064;&#65307;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#40657;&#30418;API&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20803;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;</title><link>http://arxiv.org/abs/2305.18413</link><description>&lt;p&gt;
&#20174;API&#23398;&#20064;&#23398;&#20064;&#65306;&#40657;&#30418;&#25968;&#25454;&#26080;&#20851;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning to Learn from APIs: Black-Box Data-Free Meta-Learning. (arXiv:2305.18413v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18413
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;BiDf-MKD&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#19968;&#32452;API&#24211;&#20013;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#30452;&#25509;&#36827;&#34892;&#20803;&#23398;&#20064;&#65307;&#33021;&#22815;&#22312;&#26356;&#24191;&#27867;&#30340;&#40657;&#30418;API&#19978;&#36827;&#34892;&#20803;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20803;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#65288;DFML&#65289;&#26088;&#22312;&#36890;&#36807;&#20174;&#19968;&#32452;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;&#20803;&#23398;&#20064;&#32780;&#26080;&#38656;&#35775;&#38382;&#35757;&#32451;&#25968;&#25454;&#65292;&#20174;&#32780;&#23454;&#29616;&#39640;&#25928;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;DFML&#24037;&#20316;&#20165;&#33021;&#20174;&#65288;i&#65289;&#30333;&#30418;&#21644;&#65288;ii&#65289;&#23567;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;iii&#65289;&#30456;&#21516;&#30340;&#26550;&#26500;&#20013;&#20803;&#23398;&#20064;&#65292;&#24573;&#30053;&#20102;&#26356;&#23454;&#38469;&#30340;&#35774;&#32622;&#65292;&#21363;&#29992;&#25143;&#20165;&#33021;&#36890;&#36807;&#20219;&#24847;&#27169;&#22411;&#26550;&#26500;&#21644;&#35268;&#27169;&#30340;API&#36827;&#34892;&#25512;&#26029;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21452;&#23618;&#25968;&#25454;&#26080;&#20851;&#20803;&#30693;&#35782;&#33976;&#39311;&#65288;BiDf-MKD&#65289;&#26694;&#26550;&#65292;&#23558;&#26356;&#36890;&#29992;&#30340;&#20803;&#30693;&#35782;&#20174;&#19968;&#32452;&#40657;&#30418;API&#36716;&#31227;&#21040;&#19968;&#20010;&#21333;&#19968;&#30340;&#20803;&#27169;&#22411;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data-free meta-learning (DFML) aims to enable efficient learning of new tasks by meta-learning from a collection of pre-trained models without access to the training data. Existing DFML work can only meta-learn from (i) white-box and (ii) small-scale pre-trained models (iii) with the same architecture, neglecting the more practical setting where the users only have inference access to the APIs with arbitrary model architectures and model scale inside. To solve this issue, we propose a Bi-level Data-free Meta Knowledge Distillation (BiDf-MKD) framework to transfer more general meta knowledge from a collection of black-box APIs to one single meta model. Specifically, by just querying APIs, we inverse each API to recover its training data via a zero-order gradient estimator and then perform meta-learning via a novel bi-level meta knowledge distillation structure, in which we design a boundary query set recovery technique to recover a more informative query set near the decision boundary. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-3&#29983;&#25104;&#30340;&#38024;&#23545;&#20167;&#24680;&#20869;&#23481;&#30340;&#35299;&#37322;&#26159;&#21542;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3&#29983;&#25104;&#30340;&#35299;&#37322;&#26222;&#36941;&#23384;&#22312;&#36807;&#20110;&#27169;&#31946;&#12289;&#32858;&#28966;&#19981;&#24403;&#31561;&#32570;&#28857;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#20167;&#24680;&#35328;&#35770;&#29983;&#25104;&#30340;&#35299;&#37322;&#36136;&#37327;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17680</link><description>&lt;p&gt;
&#35780;&#20272;GPT-3&#29983;&#25104;&#30340;&#20167;&#24680;&#20869;&#23481;&#23457;&#26680;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-3 Generated Explanations for Hateful Content Moderation. (arXiv:2305.17680v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#35843;&#26597;&#21644;&#20998;&#26512;&#65292;&#35780;&#20272;&#20102;&#20351;&#29992;GPT-3&#29983;&#25104;&#30340;&#38024;&#23545;&#20167;&#24680;&#20869;&#23481;&#30340;&#35299;&#37322;&#26159;&#21542;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;GPT-3&#29983;&#25104;&#30340;&#35299;&#37322;&#26222;&#36941;&#23384;&#22312;&#36807;&#20110;&#27169;&#31946;&#12289;&#32858;&#28966;&#19981;&#24403;&#31561;&#32570;&#28857;&#65292;&#21516;&#26102;&#20063;&#23384;&#22312;&#19981;&#21516;&#31867;&#22411;&#20167;&#24680;&#35328;&#35770;&#29983;&#25104;&#30340;&#35299;&#37322;&#36136;&#37327;&#24046;&#24322;&#22823;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#20351;&#29992;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;Fine-tune&#25110;&#25552;&#31034;&#29983;&#25104;&#20167;&#24680;&#35328;&#35770;&#30340;&#35299;&#37322;&#12290;&#23613;&#31649;&#36825;&#20010;&#39046;&#22495;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#65292;&#20294;&#36825;&#20123;&#29983;&#25104;&#35299;&#37322;&#30340;&#26377;&#25928;&#24615;&#21644;&#28508;&#22312;&#38480;&#21046;&#20173;&#28982;&#19981;&#20026;&#20154;&#20204;&#25152;&#20102;&#35299;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#30001;LLMs&#29983;&#25104;&#30340;&#36825;&#20123;&#35299;&#37322;&#21487;&#33021;&#20250;&#23548;&#33268;&#29992;&#25143;&#21644;&#20869;&#23481;&#23457;&#26680;&#21592;&#23545;&#26631;&#35760;&#20869;&#23481;&#26412;&#36136;&#20570;&#20986;&#38169;&#35823;&#21028;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#20998;&#26512;&#26694;&#26550;&#26469;&#26816;&#26597;&#20167;&#24680;&#35328;&#35770;&#35299;&#37322;&#65292;&#24182;&#36827;&#34892;&#20102;&#19968;&#20010;&#24191;&#27867;&#30340;&#35843;&#26597;&#26469;&#35780;&#20272;&#36825;&#20123;&#35299;&#37322;&#12290;&#25105;&#20204;&#22312;GPT-3&#19978;&#36755;&#20837;&#20167;&#24680;&#21644;&#38750;&#20167;&#24680;&#20869;&#23481;&#65292;&#21457;&#29616;&#21463;&#35843;&#26597;&#32773;&#22312;&#20154;&#24037;&#23457;&#26680;GPT&#29983;&#25104;&#30340;&#35299;&#37322;&#26102;&#65292;&#23558;&#20167;&#24680;&#35328;&#35770;&#35299;&#37322;&#35780;&#20215;&#20026;&#19981;&#22815;&#20934;&#30830;&#21644;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent research has focused on using large language models (LLMs) to generate explanations for hate speech through fine-tuning or prompting. Despite the growing interest in this area, these generated explanations' effectiveness and potential limitations remain poorly understood. A key concern is that these explanations, generated by LLMs, may lead to erroneous judgments about the nature of flagged content by both users and content moderators. For instance, an LLM-generated explanation might inaccurately convince a content moderator that a benign piece of content is hateful. In light of this, we propose an analytical framework for examining hate speech explanations and conducted an extensive survey on evaluating such explanations. Specifically, we prompted GPT-3 to generate explanations for both hateful and non-hateful content, and a survey was conducted with 2,400 unique respondents to evaluate the generated explanations. Our findings reveal that (1) human evaluators rated the GPT-gene
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31867;&#27604;&#25512;&#29702;&#33021;&#21542;&#23454;&#29616;&#23545;&#21487;&#32452;&#21512;&#35270;&#35273;&#21050;&#28608;&#25104;&#20998;&#30340;&#19978;&#19979;&#25991;&#20869;&#32452;&#21512;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#35774;&#35745;&#31867;&#27604;&#25512;&#29702;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550; Im-Promptu&#12290;&#20351;&#29992; Im-Promptu &#21487;&#20197;&#35757;&#32451;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#32452;&#21512;&#27700;&#24179;&#30340;&#20195;&#29702;&#65292;&#21253;&#25324;&#30690;&#37327;&#34920;&#31034;&#12289;&#34917;&#19969;&#34920;&#31034;&#21644;&#29289;&#20307;&#27133;&#12290;</title><link>http://arxiv.org/abs/2305.17262</link><description>&lt;p&gt;
Im-Promptu: &#20174;&#22270;&#20687;&#25552;&#31034;&#36827;&#34892;&#19978;&#19979;&#25991;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
Im-Promptu: In-Context Composition from Image Prompts. (arXiv:2305.17262v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17262
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31867;&#27604;&#25512;&#29702;&#33021;&#21542;&#23454;&#29616;&#23545;&#21487;&#32452;&#21512;&#35270;&#35273;&#21050;&#28608;&#25104;&#20998;&#30340;&#19978;&#19979;&#25991;&#20869;&#32452;&#21512;&#65292;&#36890;&#36807;&#24341;&#20837;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#25552;&#20379;&#20102;&#35774;&#35745;&#31867;&#27604;&#25512;&#29702;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550; Im-Promptu&#12290;&#20351;&#29992; Im-Promptu &#21487;&#20197;&#35757;&#32451;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#32452;&#21512;&#27700;&#24179;&#30340;&#20195;&#29702;&#65292;&#21253;&#25324;&#30690;&#37327;&#34920;&#31034;&#12289;&#34917;&#19969;&#34920;&#31034;&#21644;&#29289;&#20307;&#27133;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#26159;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#65292;&#21487;&#20197;&#20174;&#23569;&#37327;&#28436;&#31034;&#20013;&#35299;&#20915;&#21508;&#31181;&#20219;&#21153;&#12290;&#36825;&#31181;&#38544;&#21547;&#30340;&#20219;&#21153;&#29702;&#35299;&#34920;&#26126;&#65292;&#21333;&#35789;&#20196;&#29260;&#19978;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#33021;&#22312;&#31867;&#27604;&#25512;&#29702;&#20013;&#21457;&#25381;&#20316;&#29992;&#12290;&#26412;&#25991;&#30740;&#31350;&#31867;&#27604;&#25512;&#29702;&#26159;&#21542;&#33021;&#23454;&#29616;&#23545;&#21487;&#32452;&#21512;&#35270;&#35273;&#21050;&#28608;&#25104;&#20998;&#30340;&#19978;&#19979;&#25991;&#20869;&#32452;&#21512;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#22522;&#20934;&#27979;&#35797;&#22871;&#20214;&#65292;&#20197;&#27979;&#35797;&#35270;&#35273;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#30340;&#27867;&#21270;&#23646;&#24615;&#12290;&#25105;&#20204;&#35268;&#33539;&#21270;&#20102;&#22522;&#20110;&#31867;&#27604;&#30340;&#19978;&#19979;&#25991;&#23398;&#20064;&#22120;&#30340;&#27010;&#24565;&#65292;&#24182;&#29992;&#23427;&#26469;&#35774;&#35745;&#19968;&#20010;&#20803;&#23398;&#20064;&#26694;&#26550;&#31216;&#20026; Im-Promptu&#12290;&#34429;&#28982;&#35821;&#35328;&#30340;&#25152;&#38656;&#20196;&#29260;&#31890;&#24230;&#24050;&#32463;&#24471;&#21040;&#20102;&#20805;&#20998;&#35777;&#23454;&#65292;&#20294;&#29992;&#20110;&#23454;&#29616;&#35270;&#35273;&#21050;&#28608;&#20869;&#19978;&#19979;&#25991;&#27867;&#21270;&#30340;&#36866;&#24403;&#32452;&#21512;&#31890;&#24230;&#36890;&#24120;&#26410;&#32463;&#25351;&#23450;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992; Im-Promptu &#35757;&#32451;&#22810;&#20010;&#20855;&#26377;&#19981;&#21516;&#32452;&#21512;&#27700;&#24179;&#30340;&#20195;&#29702;&#65292;&#21253;&#25324;&#30690;&#37327;&#34920;&#31034;&#12289;&#34917;&#19969;&#34920;&#31034;&#21644;&#29289;&#20307;&#27133;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are few-shot learners that can solve diverse tasks from a handful of demonstrations. This implicit understanding of tasks suggests that the attention mechanisms over word tokens may play a role in analogical reasoning. In this work, we investigate whether analogical reasoning can enable in-context composition over composable elements of visual stimuli. First, we introduce a suite of three benchmarks to test the generalization properties of a visual in-context learner. We formalize the notion of an analogy-based in-context learner and use it to design a meta-learning framework called Im-Promptu. Whereas the requisite token granularity for language is well established, the appropriate compositional granularity for enabling in-context generalization in visual stimuli is usually unspecified. To this end, we use Im-Promptu to train multiple agents with different levels of compositionality, including vector representations, patch representations, and object slots. Our e
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#30697;&#38453;&#21644;&#26684;&#25289;&#22982;&#36845;&#20195;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20272;&#35745;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Lipschitz&#24120;&#25968;&#19978;&#30028;&#12290;&#35813;&#26041;&#27861;&#31934;&#30830;&#12289;&#24555;&#36895;&#12289;&#21487;&#24494;&#20998;&#65292;&#24182;&#23637;&#29616;&#20102;&#36229;&#32447;&#24615;&#25910;&#25947;&#12290;&#22312;&#23454;&#39564;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#31934;&#24230;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#26041;&#38754;&#20063;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.16173</link><description>&lt;p&gt;
&#36890;&#36807;&#26684;&#25289;&#22982;&#36845;&#20195;&#23454;&#29616;&#21367;&#31215;&#23618;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#30340;&#39640;&#25928;&#36793;&#30028;
&lt;/p&gt;
&lt;p&gt;
Efficient Bound of Lipschitz Constant for Convolutional Layers by Gram Iteration. (arXiv:2305.16173v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16173
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24490;&#29615;&#30697;&#38453;&#21644;&#26684;&#25289;&#22982;&#36845;&#20195;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#25928;&#20272;&#35745;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;Lipschitz&#24120;&#25968;&#19978;&#30028;&#12290;&#35813;&#26041;&#27861;&#31934;&#30830;&#12289;&#24555;&#36895;&#12289;&#21487;&#24494;&#20998;&#65292;&#24182;&#23637;&#29616;&#20102;&#36229;&#32447;&#24615;&#25910;&#25947;&#12290;&#22312;&#23454;&#39564;&#19978;&#34920;&#29616;&#20986;&#36739;&#39640;&#30340;&#31934;&#24230;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#22312;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#26041;&#38754;&#20063;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#21033;&#26222;&#24076;&#33576;&#24120;&#25968;&#30340;&#25511;&#21046;&#23545;&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#31283;&#23450;&#24615;&#12289;&#27867;&#21270;&#21644;&#40065;&#26834;&#24615;&#26377;&#24456;&#22823;&#24433;&#21709;&#65292;&#22240;&#27492;&#20272;&#35745;&#36825;&#20010;&#20540;&#26159;&#30446;&#21069;&#30340;&#19968;&#20010;&#31185;&#23398;&#38590;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#24490;&#29615;&#30697;&#38453;&#29702;&#35770;&#21644;&#19968;&#31181;&#26032;&#30340;&#21151;&#29575;&#36845;&#20195;&#26367;&#20195;&#26041;&#27861;&#65292;&#20171;&#32461;&#20102;&#19968;&#20010;&#31934;&#30830;&#12289;&#24555;&#36895;&#21644;&#21487;&#24494;&#20998;&#30340;&#19978;&#30028;&#65292;&#29992;&#20110;&#21367;&#31215;&#23618;&#30340;&#35889;&#33539;&#25968;&#12290;&#31216;&#20026;&#26684;&#25289;&#22982;&#36845;&#20195;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23637;&#29616;&#20102;&#19968;&#20010;&#36229;&#32447;&#24615;&#30340;&#25910;&#25947;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#31934;&#24230;&#12289;&#35745;&#31639;&#25104;&#26412;&#21644;&#21487;&#20280;&#32553;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#23427;&#23545;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#21033;&#26222;&#24076;&#33576;&#27491;&#21017;&#21270;&#38750;&#24120;&#26377;&#25928;&#65292;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#32467;&#26524;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/blaisedelattre/lip4conv &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the control of the Lipschitz constant has a great impact on the training stability, generalization, and robustness of neural networks, the estimation of this value is nowadays a real scientific challenge. In this paper we introduce a precise, fast, and differentiable upper bound for the spectral norm of convolutional layers using circulant matrix theory and a new alternative to the Power iteration. Called the Gram iteration, our approach exhibits a superlinear convergence. First, we show through a comprehensive set of experiments that our approach outperforms other state-of-the-art methods in terms of precision, computational cost, and scalability. Then, it proves highly effective for the Lipschitz regularization of convolutional neural networks, with competitive results against concurrent approaches. Code is available at https://github.com/blaisedelattre/lip4conv.
&lt;/p&gt;</description></item><item><title>PromptNER&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31639;&#27861;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#21644;&#36328;&#39046;&#22495;NER&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15444</link><description>&lt;p&gt;
PromptNER: &#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
PromptNER: Prompting For Named Entity Recognition. (arXiv:2305.15444v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15444
&lt;/p&gt;
&lt;p&gt;
PromptNER&#26159;&#19968;&#31181;&#22522;&#20110;&#25552;&#31034;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31639;&#27861;&#65292;&#21033;&#29992;LLM&#29983;&#25104;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#24182;&#25552;&#20379;&#35299;&#37322;&#65292;&#22312;&#23569;&#26679;&#26412;NER&#21644;&#36328;&#39046;&#22495;NER&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21644;&#36234;&#26469;&#36234;&#22810;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#29616;&#22312;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29616;&#25104;&#26041;&#27861;&#65292;&#20026;&#21508;&#31181;&#32463;&#20856;&#30340;NLP&#38382;&#39064;&#25552;&#20379;&#20102;&#23569;&#37327;&#26679;&#26412;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#30528;&#20196;&#20154;&#26399;&#24453;&#30340;&#21021;&#27493;&#32467;&#26524;&#65292;&#20294;&#36825;&#20123;&#22522;&#20110;LLM&#30340;&#23569;&#26679;&#26412;&#26041;&#27861;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#26041;&#38754;&#20173;&#36828;&#26410;&#36798;&#21040;&#26368;&#20808;&#36827;&#27700;&#24179;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#31471;&#21040;&#31471;&#32467;&#26500;&#29702;&#35299;&#23398;&#20064;&#34920;&#31034;&#65292;&#24182;&#22312;&#26631;&#20934;&#26631;&#35760;&#35821;&#26009;&#24211;&#19978;&#36827;&#34892;&#24494;&#35843;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;PromptNER&#65292;&#19968;&#31181;&#26032;&#30340;&#29992;&#20110;&#23569;&#26679;&#26412;&#21644;&#36328;&#39046;&#22495;NER&#30340;&#26368;&#20808;&#36827;&#31639;&#27861;&#12290;&#20026;&#20102;&#36866;&#24212;&#20219;&#20309;&#26032;&#30340;NER&#20219;&#21153;&#65292;PromptNER&#38656;&#35201;&#25552;&#20379;&#19968;&#32452;&#23454;&#20307;&#23450;&#20041;&#65292;&#38500;&#22522;&#26412;&#30340;&#23569;&#26679;&#26412;&#26679;&#20363;&#20197;&#22806;&#12290;&#32473;&#23450;&#36755;&#20837;&#21477;&#23376;&#65292;PromptNER&#25552;&#31034;LLM&#29983;&#25104;&#19968;&#20010;&#28508;&#22312;&#23454;&#20307;&#21015;&#34920;&#65292;&#24182;&#25552;&#20379;&#30456;&#24212;&#30340;&#35299;&#37322;&#65292;&#35777;&#26126;&#23427;&#20204;&#19982;&#25552;&#20379;&#30340;&#23454;&#20307;&#31867;&#22411;&#23450;&#20041;&#30340;&#20860;&#23481;&#24615;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;PromptNER&#22312;&#23569;&#26679;&#26412;NER&#20219;&#21153;&#26041;&#38754;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;WikiAnn&#25968;&#25454;&#38598;&#19978;&#20026;&#36328;&#39046;&#22495;NER&#35774;&#23450;&#20102;&#26032;&#30340;SOTA&#12290;
&lt;/p&gt;
&lt;p&gt;
In a surprising turn, Large Language Models (LLMs) together with a growing arsenal of prompt-based heuristics now offer powerful off-the-shelf approaches providing few-shot solutions to myriad classic NLP problems. However, despite promising early results, these LLM-based few-shot methods remain far from the state of the art in Named Entity Recognition (NER), where prevailing methods include learning representations via end-to-end structural understanding and fine-tuning on standard labeled corpora. In this paper, we introduce PromptNER, a new state-of-the-art algorithm for few-Shot and cross-domain NER. To adapt to any new NER task PromptNER requires a set of entity definitions in addition to the standard few-shot examples. Given a sentence, PromptNER prompts an LLM to produce a list of potential entities along with corresponding explanations justifying their compatibility with the provided entity type definitions. Remarkably, PromptNER achieves state-of-the-art performance on few-sho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#29356;&#30340;ECG&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#65292;&#20998;&#31867;&#31934;&#24230;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.15424</link><description>&lt;p&gt;
PulseNet: &#20351;&#29992;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#36827;&#34892;&#29356;ECG&#20449;&#21495;&#20998;&#31867;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
PulseNet: Deep Learning ECG-signal classification using random augmentation policy and continous wavelet transform for canines. (arXiv:2305.15424v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#23545;&#29356;&#30340;ECG&#20449;&#21495;&#36827;&#34892;&#33258;&#21160;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#21644;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#65292;&#20998;&#31867;&#31934;&#24230;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#29356;&#30340;&#24515;&#30005;&#22270;(ECG)&#38656;&#35201;&#29087;&#32451;&#30340;&#20861;&#21307;&#65292;&#20294;&#30446;&#21069;&#21487;&#29992;&#30340;&#20861;&#21307;&#24515;&#33039;&#30149;&#19987;&#23478;&#29992;&#20110;ECG&#35299;&#35835;&#21644;&#35786;&#26029;&#25903;&#25345;&#30340;&#25968;&#37327;&#26377;&#38480;&#12290;&#24320;&#21457;&#33258;&#21160;&#35780;&#20272;ECG&#24207;&#21015;&#30340;&#24037;&#20855;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#20020;&#24202;&#21307;&#29983;&#23454;&#26102;&#32467;&#26524;&#21644;&#20915;&#31574;&#25903;&#25345;&#24037;&#20855;&#26469;&#25913;&#21892;&#20861;&#21307;&#25252;&#29702;&#12290;&#25105;&#20204;&#23454;&#29616;&#20102;&#19968;&#20010;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#23558;&#29356;&#30340;&#24515;&#30005;&#22270;&#24207;&#21015;&#20998;&#31867;&#20026;&#27491;&#24120;&#25110;&#24322;&#24120;&#12290;&#23558;ECG&#35760;&#24405;&#36716;&#25442;&#20026;8&#31186;&#30340;&#31532;&#20108;&#23548;&#32852;&#24207;&#21015;&#65292;&#26681;&#25454;&#26159;&#21542;&#23384;&#22312;&#19968;&#31181;&#25110;&#22810;&#31181;&#24515;&#33039;&#24322;&#24120;&#23558;&#20854;&#20998;&#31867;&#20026;&#27491;&#24120;&#25110;&#24322;&#24120;&#12290;&#35757;&#32451;ECG&#24207;&#21015;&#20351;&#29992;RandomAugmentECG&#36827;&#34892;&#38543;&#26426;&#25968;&#25454;&#22686;&#24378;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#38376;&#20026;&#35813;&#39033;&#30446;&#23454;&#29616;&#30340;&#26032;&#22686;&#24378;&#24211;&#12290;&#28982;&#21518;&#65292;&#27599;&#20010;&#22359;&#20351;&#29992;&#36830;&#32493;&#23567;&#27874;&#21464;&#25442;&#36716;&#25442;&#25104;2D scalogram&#12290;2D scalogram&#20351;&#29992;&#20108;&#20803;CNN&#20998;&#31867;&#22120;&#20998;&#31867;&#25104;&#27491;&#24120;&#25110;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating canine electrocardiograms (ECG) require skilled veterinarians, but current availability of veterinary cardiologists for ECG interpretation and diagnostic support is limited. Developing tools for automated assessment of ECG sequences can improve veterinary care by providing clinicians real-time results and decision support tools. We implement a deep convolutional neural network (CNN) approach for classifying canine electrocardiogram sequences as either normal or abnormal. ECG records are converted into 8 second Lead II sequences and classified as either normal (no evidence of cardiac abnormalities) or abnormal (presence of one or more cardiac abnormalities). For training ECG sequences are randomly augmented using RandomAugmentECG, a new augmentation library implemented specifically for this project. Each chunk is then is converted using a continuous wavelet transform into a 2D scalogram. The 2D scalogram are then classified as either normal or abnormal by a binary CNN classif
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#23384;&#22312;&#30340;&#31639;&#27861;&#20559;&#35265;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#25968;&#25454;&#38544;&#31169;&#20405;&#29359;&#12289;&#20891;&#20107;&#21270;&#12289;&#27450;&#35784;&#21644;&#29615;&#22659;&#38382;&#39064;&#65292;&#26088;&#22312;&#20419;&#36827;&#21746;&#23398;&#12289;&#25919;&#27835;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#20851;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;&#25506;&#35752;&#12290;</title><link>http://arxiv.org/abs/2305.15239</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#19982;&#20262;&#29702;&#23398;
&lt;/p&gt;
&lt;p&gt;
Deep Learning and Ethics. (arXiv:2305.15239v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15239
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#23384;&#22312;&#30340;&#31639;&#27861;&#20559;&#35265;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#25968;&#25454;&#38544;&#31169;&#20405;&#29359;&#12289;&#20891;&#20107;&#21270;&#12289;&#27450;&#35784;&#21644;&#29615;&#22659;&#38382;&#39064;&#65292;&#26088;&#22312;&#20419;&#36827;&#21746;&#23398;&#12289;&#25919;&#27835;&#31185;&#23398;&#21644;&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#20851;&#20110;&#36825;&#20123;&#38382;&#39064;&#30340;&#25506;&#35752;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;Prince&#65288;2023&#24180;&#65289;&#12298;&#29702;&#35299;&#28145;&#24230;&#23398;&#20064;&#12299;&#30340;&#31532;21&#31456;&#65292;&#25945;&#26448;&#30340;&#23436;&#25972;&#33609;&#31295;&#21487;&#22312;&#27492;http URL&#33719;&#24471;&#12290;&#26412;&#31456;&#32771;&#34385;&#20102;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#35774;&#35745;&#21644;&#20351;&#29992;&#21487;&#33021;&#20135;&#29983;&#30340;&#28508;&#22312;&#21361;&#23475;&#65292;&#21253;&#25324;&#31639;&#27861;&#20559;&#35265;&#12289;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12289;&#25968;&#25454;&#38544;&#31169;&#20405;&#29359;&#12289;&#20891;&#20107;&#21270;&#12289;&#27450;&#35784;&#21644;&#29615;&#22659;&#38382;&#39064;&#12290;&#30446;&#30340;&#19981;&#26159;&#20026;&#20102;&#25552;&#20379;&#26356;&#21152;&#36947;&#24503;&#30340;&#24314;&#35758;&#12290;&#30456;&#21453;&#65292;&#30446;&#26631;&#26159;&#22312;&#21746;&#23398;&#12289;&#25919;&#27835;&#31185;&#23398;&#21644;&#26356;&#24191;&#27867;&#30340;&#31038;&#20250;&#31185;&#23398;&#39046;&#22495;&#24341;&#21457;&#20851;&#38190;&#39046;&#22495;&#30340;&#24605;&#24819;&#21644;&#23545;&#35805;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article appears as chapter 21 of Prince (2023, Understanding Deep Learning); a complete draft of the textbook is available here: this http URL This chapter considers potential harms arising from the design and use of AI systems. These include algorithmic bias, lack of explainability, data privacy violations, militarization, fraud, and environmental concerns. The aim is not to provide advice on being more ethical. Instead, the goal is to express ideas and start conversations in key areas that have received attention in philosophy, political science, and the broader social sciences.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#65292;&#22312;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#20869;&#23384;BFGS&#31639;&#27861;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.14449</link><description>&lt;p&gt;
&#22270;&#35889;&#36935;&#35265;LLM&#65306;&#19968;&#31181;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#30340;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Graph Meets LLM: A Novel Approach to Collaborative Filtering for Robust Conversational Understanding. (arXiv:2305.14449v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14449
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#21327;&#21516;&#36807;&#28388;&#26032;&#26041;&#27861;&#29992;&#20110;&#31283;&#20581;&#23545;&#35805;&#29702;&#35299;&#65292;&#22312;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#30340;&#22522;&#30784;&#19978;&#65292;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65292;&#24182;&#20351;&#29992;&#26377;&#38480;&#20869;&#23384;BFGS&#31639;&#27861;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20250;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65288;&#20363;&#22914;Alexa&#65292;Siri&#65292;Google Assistant&#31561;&#65289;&#38656;&#35201;&#29702;&#35299;&#23384;&#22312;&#32570;&#38519;&#30340;&#26597;&#35810;&#20197;&#30830;&#20445;&#31283;&#20581;&#30340;&#20250;&#35805;&#29702;&#35299;&#24182;&#20943;&#23569;&#29992;&#25143;&#25705;&#25830;&#12290;&#36825;&#20123;&#26377;&#32570;&#38519;&#30340;&#26597;&#35810;&#36890;&#24120;&#26159;&#30001;&#29992;&#25143;&#30340;&#27495;&#20041;&#21644;&#38169;&#35823;&#65292;&#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65288;NLU&#65289;&#20013;&#30340;&#38169;&#35823;&#24341;&#36215;&#30340;&#12290;&#20010;&#24615;&#21270;&#26597;&#35810;&#37325;&#20889;&#65288;&#20010;&#24615;&#21270;QR&#65289;&#26088;&#22312;&#20943;&#23569;&#36523;&#20307;&#21644;&#23614;&#37096;&#29992;&#25143;&#26597;&#35810;&#27969;&#37327;&#20013;&#30340;&#32570;&#38519;&#65292;&#36890;&#24120;&#20381;&#36182;&#20110;&#19982;&#23545;&#35805;&#24335;&#20154;&#24037;&#26234;&#33021;&#30340;&#36807;&#21435;&#25104;&#21151;&#30340;&#29992;&#25143;&#20132;&#20114;&#30340;&#32034;&#24341;&#12290;&#26412;&#25991;&#25552;&#20986;&#25105;&#20204;&#30340;&#8220;&#21327;&#21516;&#26597;&#35810;&#37325;&#20889;&#8221;&#26041;&#27861;&#65292;&#19987;&#27880;&#20110;&#37325;&#20889;&#29992;&#25143;&#21382;&#21490;&#20013;&#27809;&#26377;&#20986;&#29616;&#36807;&#30340;&#26032;&#22411;&#29992;&#25143;&#20132;&#20114;&#12290;&#35813;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#8220;&#29992;&#25143;&#21453;&#39304;&#20132;&#20114;&#22270;&#8221;&#65288;FIG&#65289;&#65292;&#30001;&#21382;&#21490;&#29992;&#25143;-&#23454;&#20307;&#20132;&#20114;&#32452;&#25104;&#65292;&#24182;&#21033;&#29992;&#22810;&#36339;&#23458;&#25143;&#20146;&#21644;&#21147;&#26469;&#20016;&#23500;&#27599;&#20010;&#29992;&#25143;&#30340;&#32034;&#24341;&#65288;&#21363;&#21327;&#21516;&#29992;&#25143;&#32034;&#24341;&#65289;&#65292;&#20174;&#32780;&#24110;&#21161;&#35206;&#30422;&#26410;&#26469;&#26410;&#26366;&#35265;&#36807;&#30340;&#23384;&#22312;&#32570;&#38519;&#30340;&#26597;&#35810;&#12290;&#20026;&#20102;&#38450;&#27490;&#36825;&#20123;&#26032;&#30340;&#20016;&#23500;&#32034;&#24341;&#34987;&#22122;&#22768;&#21453;&#39304;&#20132;&#20114;&#25152;&#25903;&#37197;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#26377;&#38480;&#20869;&#23384;BFGS&#65288;LLM&#65289;&#31639;&#27861;&#21644;&#22238;&#36864;&#26041;&#26696;&#26469;&#35843;&#25972;&#27599;&#20010;&#32034;&#24341;&#30340;&#26435;&#37325;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26126;&#26174;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#20010;&#24615;&#21270;QR&#26041;&#27861;&#65292;&#24182;&#22312;&#26410;&#30475;&#21040;&#30340;&#29992;&#25143;&#20132;&#20114;&#19978;&#21462;&#24471;&#20102;&#36817;&#20046;&#23436;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conversational AI systems (e.g. Alexa, Siri, Google Assistant, etc.) need to understand queries with defects to ensure robust conversational understanding and reduce user frictions. The defective queries are often induced by user ambiguities and mistakes, or errors in the automatic speech recognition (ASR) and natural language understanding (NLU).  Personalized query rewriting (personalized QR) targets reducing defects in the torso and tail user query traffic, and it typically relies on an index of past successful user interactions with the conversational AI. This paper presents our "Collaborative Query Rewriting" approach that focuses on rewriting novel user interactions unseen in the user history. This approach builds a "user Feedback Interaction Graph" (FIG) consisting of historical user-entity interactions, and leverages multi-hop customer affinity to enrich each user's index (i.e. the Collaborative User Index) that would help cover future unseen defective queries. To counteract th
&lt;/p&gt;</description></item><item><title>DUBLIN&#26159;&#19968;&#20010;&#38024;&#23545;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#25513;&#27169;&#25991;&#26723;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#12289;&#36793;&#30028;&#26694;&#20219;&#21153;&#21644;&#28210;&#26579;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#31454;&#20105;&#24615;&#25110;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.14218</link><description>&lt;p&gt;
DUBLIN&#8212;&#8212;&#36890;&#36807;&#35821;&#35328;-&#22270;&#20687;&#32593;&#32476;&#36827;&#34892;&#25991;&#26723;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
DUBLIN -- Document Understanding By Language-Image Network. (arXiv:2305.14218v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14218
&lt;/p&gt;
&lt;p&gt;
DUBLIN&#26159;&#19968;&#20010;&#38024;&#23545;&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#25513;&#27169;&#25991;&#26723;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#12289;&#36793;&#30028;&#26694;&#20219;&#21153;&#21644;&#28210;&#26579;&#38382;&#31572;&#20219;&#21153;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#21033;&#29992;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#35813;&#27169;&#22411;&#22312;&#22810;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#36798;&#21040;&#20102;&#31454;&#20105;&#24615;&#25110;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#25991;&#26723;&#29702;&#35299;&#26159;&#19968;&#39033;&#22797;&#26434;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#20998;&#26512;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#20803;&#32032;&#12290;&#29616;&#26377;&#27169;&#22411;&#36890;&#24120;&#20381;&#36182;&#20110;&#25163;&#21160;&#29305;&#24449;&#24037;&#31243;&#25110;&#29305;&#23450;&#39046;&#22495;&#30340;&#27969;&#27700;&#32447;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#19981;&#21516;&#25991;&#26723;&#31867;&#22411;&#21644;&#35821;&#35328;&#20043;&#38388;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39044;&#20808;&#20351;&#29992;&#19977;&#31181;&#26032;&#39062;&#30446;&#26631;&#22312;Web&#39029;&#38754;&#19978;&#36827;&#34892;&#35757;&#32451;&#30340;DUBLIN&#27169;&#22411;&#65306;&#25513;&#27169;&#25991;&#26723;&#20869;&#23481;&#29983;&#25104;&#20219;&#21153;&#12289;&#36793;&#30028;&#26694;&#20219;&#21153;&#21644;&#28210;&#26579;&#38382;&#31572;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#31354;&#38388;&#21644;&#35821;&#20041;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#22810;&#39033;&#22522;&#20934;&#27979;&#35797;&#20013;&#23454;&#29616;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#25110;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#20363;&#22914;&#22522;&#20110;Web&#30340;&#32467;&#26500;&#21270;&#38405;&#35835;&#29702;&#35299;&#12289;&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#12289;&#20851;&#38190;&#20449;&#24687;&#25552;&#21462;&#12289;&#22270;&#35299;&#29702;&#35299;&#21644;&#34920;&#26684;&#38382;&#31572;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DUBLIN&#26159;&#39318;&#20010;&#22312;WebSRC&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;EM 77.75&#21644;F1 84.25&#30340;&#22522;&#20110;&#20687;&#32032;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual document understanding is a complex task that involves analyzing both the text and the visual elements in document images. Existing models often rely on manual feature engineering or domain-specific pipelines, which limit their generalization ability across different document types and languages. In this paper, we propose DUBLIN, which is pretrained on web pages using three novel objectives: Masked Document Content Generation Task, Bounding Box Task, and Rendered Question Answering Task, that leverage both the spatial and semantic information in the document images. Our model achieves competitive or state-of-the-art results on several benchmarks, such as Web-Based Structural Reading Comprehension, Document Visual Question Answering, Key Information Extraction, Diagram Understanding, and Table Question Answering. In particular, we show that DUBLIN is the first pixel-based model to achieve an EM of 77.75 and F1 of 84.25 on the WebSRC dataset. We also show that our model outperform
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;ChipGPT&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#35774;&#35745;&#29615;&#22659;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#29983;&#25104;&#30828;&#20214;&#36923;&#36753;&#35774;&#35745;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#20154;&#24037;&#35774;&#35745;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#19988;&#21487;&#33410;&#30465;&#36229;&#36807;75&#65285;&#30340;&#32534;&#30721;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2305.14019</link><description>&lt;p&gt;
ChipGPT: &#36828;&#31163;&#33258;&#28982;&#35821;&#35328;&#30828;&#20214;&#35774;&#35745;&#36824;&#26377;&#22810;&#36828;
&lt;/p&gt;
&lt;p&gt;
ChipGPT: How far are we from natural language hardware design. (arXiv:2305.14019v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;ChipGPT&#65292;&#19968;&#20010;&#33258;&#21160;&#21270;&#35774;&#35745;&#29615;&#22659;&#65292;&#23427;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#29983;&#25104;&#30828;&#20214;&#36923;&#36753;&#35774;&#35745;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#20154;&#24037;&#35774;&#35745;&#24615;&#33021;&#30456;&#23218;&#32654;&#30340;&#32467;&#26524;&#65292;&#19988;&#21487;&#33410;&#30465;&#36229;&#36807;75&#65285;&#30340;&#32534;&#30721;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#23637;&#31034;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#26426;&#22120;&#26234;&#33021;&#65292;&#23427;&#22312;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#20132;&#20114;&#26469;&#21327;&#21161;&#30828;&#20214;&#24037;&#31243;&#24072;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#36923;&#36753;&#35774;&#35745;&#26041;&#38754;&#20063;&#34920;&#29616;&#20986;&#26497;&#20339;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#35780;&#20272;LLMs&#21327;&#21161;&#30828;&#20214;&#35774;&#35745;&#36807;&#31243;&#30340;&#28508;&#21147;&#65292;&#26412;&#25991;&#23581;&#35797;&#28436;&#31034;&#19968;&#20010;&#33258;&#21160;&#21270;&#35774;&#35745;&#29615;&#22659;&#65292;&#35813;&#29615;&#22659;&#21033;&#29992;LLMs&#20174;&#33258;&#28982;&#35821;&#35328;&#35268;&#33539;&#29983;&#25104;&#30828;&#20214;&#36923;&#36753;&#35774;&#35745;&#12290;&#20026;&#20102;&#23454;&#29616;&#26356;&#26131;&#29992;&#19988;&#26356;&#39640;&#25928;&#30340;&#33455;&#29255;&#24320;&#21457;&#27969;&#31243;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;LLMs&#30340;&#21487;&#25193;&#23637;&#30340;&#22235;&#38454;&#27573;&#38646;&#20195;&#30721;&#36923;&#36753;&#35774;&#35745;&#26694;&#26550;&#65292;&#26080;&#38656;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#12290;&#39318;&#20808;&#65292;&#28436;&#31034;&#29256;&#26412;ChipGPT&#36890;&#36807;&#20026;LLM&#29983;&#25104;&#25552;&#31034;&#24320;&#22987;&#65292;&#28982;&#21518;&#20135;&#29983;&#21021;&#22987;Verilog&#31243;&#24207;&#12290; &#20854;&#27425;&#65292;&#36755;&#20986;&#31649;&#29702;&#22120;&#32416;&#27491;&#21644;&#20248;&#21270;&#36825;&#20123;&#31243;&#24207;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#25910;&#38598;&#21040;&#26368;&#32456;&#30340;&#35774;&#35745;&#31354;&#38388;&#20013;&#12290;&#26368;&#21518;&#65292;ChipGPT&#23558;&#22312;&#27492;&#31354;&#38388;&#20013;&#25628;&#32034;&#20197;&#36873;&#25321;&#31526;&#21512;&#30446;&#26631;&#25351;&#26631;&#30340;&#26368;&#20248;&#35774;&#35745;&#12290;&#35780;&#20272;&#34920;&#26126;&#65292;&#30001;ChipGPT&#35774;&#35745;&#30340;&#36923;&#36753;&#30005;&#36335;&#30340;&#24615;&#33021;&#19982;&#20154;&#24037;&#35774;&#35745;&#30340;&#24615;&#33021;&#30456;&#24403;&#65292;&#24182;&#19988;&#25972;&#20010;&#36807;&#31243;&#33410;&#30465;&#20102;&#36229;&#36807;75&#65285;&#30340;&#32534;&#30721;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
As large language models (LLMs) like ChatGPT exhibited unprecedented machine intelligence, it also shows great performance in assisting hardware engineers to realize higher-efficiency logic design via natural language interaction. To estimate the potential of the hardware design process assisted by LLMs, this work attempts to demonstrate an automated design environment that explores LLMs to generate hardware logic designs from natural language specifications. To realize a more accessible and efficient chip development flow, we present a scalable four-stage zero-code logic design framework based on LLMs without retraining or finetuning. At first, the demo, ChipGPT, begins by generating prompts for the LLM, which then produces initial Verilog programs. Second, an output manager corrects and optimizes these programs before collecting them into the final design space. Eventually, ChipGPT will search through this space to select the optimal design under the target metrics. The evaluation sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;</title><link>http://arxiv.org/abs/2305.11283</link><description>&lt;p&gt;
&#20851;&#20110;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;
On the Statistical Efficiency of Mean Field Reinforcement Learning with General Function Approximation. (arXiv:2305.11283v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;(MFC)&#21644;&#22343;&#22330;&#21338;&#24328;(MFG)&#20013;&#30340;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#65292;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#24182;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#26368;&#21518;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#33324;&#20989;&#25968;&#36924;&#36817;&#19979;&#30340;&#22343;&#22330;&#25511;&#21046;&#65288;MFC&#65289;&#21644;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#32479;&#35745;&#25928;&#29575;&#12290;&#24341;&#20837;&#20102;&#19968;&#31181;&#31216;&#20026;Mean-Field Model-Based Eluder Dimension (MBED)&#30340;&#26032;&#27010;&#24565;&#65292;&#21253;&#21547;&#20102;&#19968;&#31995;&#21015;&#20016;&#23500;&#30340;&#22343;&#22330;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20048;&#35266;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#36820;&#22238;&#19968;&#20010;$\epsilon$&#20248;&#30340;&#31574;&#30053;&#65292;&#36866;&#29992;&#20110;MFC&#25110;$\epsilon$&#32435;&#20160;&#22343;&#34913;&#31574;&#30053;&#36866;&#29992;&#20110;MFG&#65292;&#26679;&#26412;&#22797;&#26434;&#24230;&#22810;&#39033;&#24335;&#19982;&#30456;&#20851;&#21442;&#25968;&#26080;&#20851;&#65292;&#19982;&#29366;&#24577;&#12289;&#21160;&#20316;&#21644;&#20195;&#29702;&#25968;&#37327;&#26080;&#20851;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#20165;&#23545;&#36716;&#31227;&#21160;&#21147;&#23398;&#20855;&#26377;Lipschitz&#36830;&#32493;&#24615;&#30340;&#20551;&#35774;&#65292;&#36991;&#20813;&#20102;&#20197;&#21069;&#30340;&#24378;&#32467;&#26500;&#20551;&#35774;&#12290;&#26368;&#21518;&#65292;&#22312;tabular&#35774;&#32622;&#19979;&#65292;&#20551;&#35774;&#26377;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25351;&#25968;&#32423;&#30340;&#19979;&#30028;&#25903;&#25345;MFC&#35774;&#32622;&#65292;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26679;&#26412;&#39640;&#25928;&#30340;&#27169;&#22411;&#28040;&#38500;&#31639;&#27861;&#20197;&#36924;&#36817;&#26368;&#20248;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we study the statistical efficiency of Reinforcement Learning in Mean-Field Control (MFC) and Mean-Field Game (MFG) with general function approximation. We introduce a new concept called Mean-Field Model-Based Eluder Dimension (MBED), which subsumes a rich family of Mean-Field RL problems. Additionally, we propose algorithms based on Optimistic Maximal Likelihood Estimation, which can return an $\epsilon$-optimal policy for MFC or an $\epsilon$-Nash Equilibrium policy for MFG, with sample complexity polynomial w.r.t. relevant parameters and independent of the number of states, actions and the number of agents. Notably, our results only require a mild assumption of Lipschitz continuity on transition dynamics and avoid strong structural assumptions in previous work. Finally, in the tabular setting, given the access to a generative model, we establish an exponential lower bound for MFC setting, while providing a novel sample-efficient model elimination algorithm to approxim
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10847</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#34987;&#24341;&#23548;&#26469;&#35268;&#36991;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#26469;&#26377;&#25928;&#35268;&#36991;&#29616;&#26377;&#30340;&#25991;&#26412;&#26816;&#27979;&#31995;&#32479;&#65292;&#35777;&#26126;&#20102;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21253;&#25324;&#35770;&#25991;&#20889;&#20316;&#21644;&#38382;&#31572;&#31561;&#22810;&#20010;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#24517;&#39035;&#35299;&#20915;&#36825;&#20123;&#27169;&#22411;&#28508;&#22312;&#30340;&#35823;&#29992;&#38382;&#39064;&#65292;&#21542;&#21017;&#21487;&#33021;&#23548;&#33268;&#25220;&#34989;&#21644;&#22403;&#22334;&#20449;&#24687;&#31561;&#19981;&#33391;&#21518;&#26524;&#12290;&#26412;&#30740;&#31350;&#25581;&#31034;&#65292;&#36890;&#36807;&#31934;&#24515;&#35774;&#35745;&#30340;&#25552;&#31034;&#35821;&#65292;LLMs&#21487;&#20197;&#26377;&#25928;&#22320;&#35268;&#36991;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#26367;&#25442;&#30340;&#19978;&#19979;&#25991;&#31034;&#20363;&#20248;&#21270;&#26041;&#27861;&#65288;SICO&#65289;&#65292;&#29992;&#20110;&#33258;&#21160;&#29983;&#25104;&#36825;&#31181;&#25552;&#31034;&#35821;&#12290;&#22312;&#19977;&#20010;&#29616;&#23454;&#20219;&#21153;&#20013;&#65292;LLMs&#21487;&#33021;&#34987;&#35823;&#29992;&#65292;&#22312;SICO&#30340;&#24110;&#21161;&#19979;&#65292;ChatGPT&#25104;&#21151;&#22320;&#35268;&#36991;&#20102;&#20845;&#39033;&#29616;&#26377;&#30340;&#26816;&#27979;&#22120;&#65292;&#24179;&#22343;&#23548;&#33268;0.54&#30340;AUC&#19979;&#38477;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#36825;&#20123;&#26816;&#27979;&#22120;&#30340;&#34920;&#29616;&#29978;&#33267;&#27604;&#38543;&#26426;&#20998;&#31867;&#22120;&#36824;&#35201;&#24046;&#12290;&#36825;&#20123;&#32467;&#26524;&#22362;&#23450;&#22320;&#25581;&#31034;&#20102;&#29616;&#26377;&#26816;&#27979;&#22120;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have demonstrated exceptional performance in a variety of tasks, including essay writing and question answering. However, it is crucial to address the potential misuse of these models, which can lead to detrimental outcomes such as plagiarism and spamming. Recently, several detectors have been proposed, including fine-tuned classifiers and various statistical methods. In this study, we reveal that with the aid of carefully crafted prompts, LLMs can effectively evade these detection systems. We propose a novel Substitution-based In-Context example Optimization method (SICO) to automatically generate such prompts. On three real-world tasks where LLMs can be misused, SICO successfully enables ChatGPT to evade six existing detectors, causing a significant 0.54 AUC drop on average. Surprisingly, in most cases these detectors perform even worse than random classifiers. These results firmly reveal the vulnerability of existing detectors. Finally, the strong perfor
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#20219;&#21153;&#20013;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#21464;&#24322;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.08014</link><description>&lt;p&gt;
&#22522;&#20110;&#34920;&#38754;&#32908;&#30005;&#22270;&#20687;&#30340;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;&#30340;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Surface EMG-Based Inter-Session/Inter-Subject Gesture Recognition by Leveraging Lightweight All-ConvNet and Transfer Learning. (arXiv:2305.08014v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08014
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#20219;&#21153;&#20013;&#26377;&#25928;&#35299;&#20915;&#25968;&#25454;&#21464;&#24322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#20302;&#20998;&#36776;&#29575;&#30636;&#26102;&#39640;&#28165;&#32908;&#30005;&#22270;&#20687;&#36827;&#34892;&#25163;&#21183;&#35782;&#21035;&#21487;&#20197;&#24320;&#36767;&#21457;&#23637;&#26356;&#27969;&#30021;&#12289;&#26356;&#33258;&#28982;&#30340;&#32908;&#32905;-&#35745;&#31639;&#26426;&#30028;&#38754;&#30340;&#26032;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#36328;&#22330;&#26223;&#25968;&#25454;&#30340;&#21464;&#24322;&#24615;&#23384;&#22312;&#26497;&#22823;&#30340;&#25361;&#25112;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#37319;&#29992;&#38750;&#24120;&#22823;&#19988;&#22797;&#26434;&#30340;&#28145;&#24230;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25110;&#22522;&#20110;2SRNN&#30340;&#39046;&#22495;&#36866;&#24212;&#26041;&#27861;&#65292;&#26469;&#36924;&#36817;&#30001;&#36825;&#20123;&#36328;&#22330;&#26223;&#25968;&#25454;&#21464;&#24322;&#24615;&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#22240;&#27492;&#65292;&#36825;&#20123;&#26041;&#27861;&#20063;&#38656;&#35201;&#22312;&#39044;&#35757;&#32451;&#21644;&#36866;&#24212;&#38454;&#27573;&#20013;&#22312;&#25968;&#30334;&#19975;&#20010;&#35757;&#32451;&#21442;&#25968;&#21644;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#23398;&#20064;&#12290;&#32467;&#26524;&#65292;&#36825;&#20351;&#24471;&#22312;&#23454;&#26102;&#24212;&#29992;&#20013;&#36827;&#34892;&#39640;&#31471;&#36164;&#28304;&#32422;&#26463;&#21644;&#35745;&#31639;&#38750;&#24120;&#26114;&#36149;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36731;&#37327;&#32423;&#30340;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;+&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#65292;&#21033;&#29992;&#36731;&#37327;&#32423;&#20840;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#36801;&#31227;&#23398;&#20064;(TL)&#26469;&#22686;&#24378;&#36328;&#22330;&#26223;&#25163;&#21183;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Gesture recognition using low-resolution instantaneous HD-sEMG images opens up new avenues for the development of more fluid and natural muscle-computer interfaces. However, the data variability between inter-session and inter-subject scenarios presents a great challenge. The existing approaches employed very large and complex deep ConvNet or 2SRNN-based domain adaptation methods to approximate the distribution shift caused by these inter-session and inter-subject data variability. Hence, these methods also require learning over millions of training parameters and a large pre-trained and target domain dataset in both the pre-training and adaptation stages. As a result, it makes high-end resource-bounded and computationally very expensive for deployment in real-time applications. To overcome this problem, we propose a lightweight All-ConvNet+TL model that leverages lightweight All-ConvNet and transfer learning (TL) for the enhancement of inter-session and inter-subject gesture recogniti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#20934;&#30830;&#32780;&#31616;&#27905;&#22320;&#34920;&#31034;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#20197;&#21327;&#21516;&#22320;&#30830;&#23450;FJSP&#30340;&#20248;&#20808;&#32423;&#20998;&#37197;&#35268;&#21017;&#12290;</title><link>http://arxiv.org/abs/2305.05119</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#37325;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#24377;&#24615;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Flexible Job Shop Scheduling via Dual Attention Network Based Reinforcement Learning. (arXiv:2305.05119v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#33258;&#27880;&#24847;&#27169;&#22411;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21452;&#37325;&#27880;&#24847;&#21147;&#32593;&#32476;&#26469;&#20934;&#30830;&#32780;&#31616;&#27905;&#22320;&#34920;&#31034;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#20197;&#21327;&#21516;&#22320;&#30830;&#23450;FJSP&#30340;&#20248;&#20808;&#32423;&#20998;&#37197;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24377;&#24615;&#21046;&#36896;&#20652;&#29983;&#20102;&#22797;&#26434;&#30340;&#35843;&#24230;&#38382;&#39064;&#65292;&#22914;&#24377;&#24615;&#36710;&#38388;&#35843;&#24230;&#38382;&#39064;&#65288;FJSP&#65289;&#12290;&#22312;FJSP&#20013;&#65292;&#25805;&#20316;&#21487;&#20197;&#22312;&#22810;&#21488;&#26426;&#22120;&#19978;&#36827;&#34892;&#22788;&#29702;&#65292;&#23548;&#33268;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#23384;&#22312;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#26469;&#23398;&#20064;&#20248;&#20808;&#32423;&#20998;&#37197;&#35268;&#21017;&#65288;PDRs&#65289;&#20197;&#35299;&#20915;FJSP&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#20110;&#35832;&#22914;OR-Tools&#31561;&#31934;&#30830;&#26041;&#27861;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#35299;&#20915;&#26041;&#26696;&#30340;&#36136;&#37327;&#20173;&#26377;&#25552;&#39640;&#30340;&#31354;&#38388;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31471;&#21040;&#31471;&#23398;&#20064;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#33258;&#27880;&#24847;&#27169;&#22411;&#36827;&#34892;&#28145;&#24230;&#29305;&#24449;&#25552;&#21462;&#21644;DRL&#36827;&#34892;&#21487;&#25193;&#23637;&#20915;&#31574;&#21046;&#23450;&#30340;&#20248;&#28857;&#12290;&#25805;&#20316;&#21644;&#26426;&#22120;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#34987;&#20934;&#30830;&#32780;&#31616;&#27905;&#22320;&#34920;&#31034;&#20986;&#26469;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#30001;&#22810;&#20010;&#30456;&#20114;&#36830;&#25509;&#30340;&#25805;&#20316;&#20449;&#24687;&#27880;&#24847;&#22359;&#21644;&#26426;&#22120;&#20449;&#24687;&#27880;&#24847;&#22359;&#32452;&#25104;&#30340;&#21452;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;DAN&#65289;&#12290;DAN&#21033;&#29992;&#36825;&#20123;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#20197;&#21327;&#21516;&#22320;&#30830;&#23450;FJSP&#30340;PDRs&#12290;
&lt;/p&gt;
&lt;p&gt;
Flexible manufacturing has given rise to complex scheduling problems such as the flexible job shop scheduling problem (FJSP). In FJSP, operations can be processed on multiple machines, leading to intricate relationships between operations and machines. Recent works have employed deep reinforcement learning (DRL) to learn priority dispatching rules (PDRs) for solving FJSP. However, the quality of solutions still has room for improvement relative to that by the exact methods such as OR-Tools. To address this issue, this paper presents a novel end-to-end learning framework that weds the merits of self-attention models for deep feature extraction and DRL for scalable decision-making. The complex relationships between operations and machines are represented precisely and concisely, for which a dual-attention network (DAN) comprising several interconnected operation message attention blocks and machine message attention blocks is proposed. The DAN exploits the complicated relationships to co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;&#22270;&#28789;&#27979;&#35797;&#65292;&#22238;&#36991;&#20102;&#26426;&#22120;&#26159;&#21542;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#25506;&#35752;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#22914;&#20309;&#30830;&#23450;&#19968;&#20010;&#20132;&#20114;&#23545;&#35937;&#26159;&#20154;&#36824;&#26159;&#26426;&#22120;&#30340;&#25361;&#25112;&#65292;&#24182;&#24605;&#32771;&#20102;&#20854;&#24212;&#29992;&#21644;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04312</link><description>&lt;p&gt;
&#20154;&#36824;&#26159;&#26426;&#22120;&#65306;&#22522;&#20110;&#22270;&#28789;&#27979;&#35797;&#30340;&#26085;&#24120;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Human or Machine: Reflections on Turing-Inspired Testing for the Everyday. (arXiv:2305.04312v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;&#22270;&#28789;&#27979;&#35797;&#65292;&#22238;&#36991;&#20102;&#26426;&#22120;&#26159;&#21542;&#26234;&#33021;&#30340;&#38382;&#39064;&#65292;&#25506;&#35752;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#22914;&#20309;&#30830;&#23450;&#19968;&#20010;&#20132;&#20114;&#23545;&#35937;&#26159;&#20154;&#36824;&#26159;&#26426;&#22120;&#30340;&#25361;&#25112;&#65292;&#24182;&#24605;&#32771;&#20102;&#20854;&#24212;&#29992;&#21644;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20182;&#30340;&#24320;&#21019;&#24615;&#35770;&#25991;&#12298;&#35745;&#31639;&#26426;&#22120;&#26800;&#19982;&#26234;&#33021;&#12299;&#20013;&#65292;&#33406;&#20262;&#183;&#22270;&#28789;&#24341;&#20837;&#20102;&#8220;&#27169;&#20223;&#28216;&#25103;&#8221;&#65292;&#25506;&#35752;&#20102;&#26426;&#22120;&#26234;&#33021;&#30340;&#27010;&#24565;&#12290;&#22270;&#28789;&#27979;&#35797;&#33258;&#37027;&#26102;&#20197;&#26469;&#19968;&#30452;&#26159;&#24191;&#27867;&#35752;&#35770;&#12289;&#23436;&#21892;&#21644;&#25193;&#23637;&#30340;&#20027;&#39064;&#12290;&#26412;&#25991;&#22238;&#36991;&#20102;&#20851;&#20110;&#26576;&#20010;&#29305;&#23450;&#26426;&#22120;&#26159;&#21542;&#33021;&#34987;&#26631;&#35760;&#20026;&#26234;&#33021;&#25110;&#33021;&#21542;&#22312;&#32473;&#23450;&#29615;&#22659;&#20013;&#21305;&#37197;&#20154;&#31867;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#20294;&#21463;&#22270;&#28789;&#21551;&#21457;&#65292;&#25105;&#20204;&#20851;&#27880;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#30830;&#23450;&#26159;&#21542;&#27491;&#22312;&#19982;&#19968;&#20010;&#20154;&#25110;&#19968;&#20010;&#26426;&#22120;&#36827;&#34892;&#20132;&#20114;&#36825;&#20010;&#30475;&#20284;&#31616;&#21333;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#23545;&#36825;&#20010;&#20154;&#36824;&#26159;&#26426;&#22120;&#38382;&#39064;&#21450;&#20854;&#21487;&#38752;&#31572;&#26696;&#30340;&#24212;&#29992;&#24863;&#21040;&#24863;&#20852;&#36259;&#65292;&#24182;&#24076;&#26395;&#21453;&#24605;&#20854;&#37325;&#35201;&#24615;&#12290;&#34429;&#28982;&#22270;&#28789;&#30340;&#21407;&#22987;&#27979;&#35797;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19968;&#31181;&#24605;&#32500;&#23454;&#39564;&#65292;&#20294;&#26412;&#25991;&#35752;&#35770;&#30340;&#20154;&#36824;&#26159;&#26426;&#22120;&#38382;&#39064;&#20855;&#26377;&#26126;&#26174;&#30340;&#23454;&#38469;&#24847;&#20041;&#12290;&#34429;&#28982;&#20154;&#31867;&#26159;&#21542;&#33021;&#22815;&#21019;&#36896;&#20986;&#33021;&#22815;&#32988;&#20219;&#25152;&#26377;&#30340;&#20154;&#31867;&#24037;&#20316;&#30340;&#26426;&#22120;&#20063;&#26410;&#21487;&#30693;&#12290;
&lt;/p&gt;
&lt;p&gt;
In his seminal paper "Computing Machinery and Intelligence", Alan Turing introduced the "imitation game" as part of exploring the concept of machine intelligence. The Turing Test has since been the subject of much analysis, debate, refinement and extension. Here we sidestep the question of whether a particular machine can be labeled intelligent, or can be said to match human capabilities in a given context. Instead, but inspired by Turing, we draw attention to the seemingly simpler challenge of determining whether one is interacting with a human or with a machine, in the context of everyday life. We are interested in reflecting upon the importance of this Human-or-Machine question and the use one may make of a reliable answer thereto. Whereas Turing's original test is widely considered to be more of a thought experiment, the Human-or-Machine question as discussed here has obvious practical significance. And while the jury is still not in regarding the possibility of machines that can m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#30456;&#20284;&#24615;&#65288;LocalSim&#65289;&#23398;&#20064;&#33410;&#28857;&#32423;&#21152;&#26435;&#34701;&#21512;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#25552;&#21462;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#22810;&#36339;&#20449;&#24687;&#65292;&#24182;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#22312;&#30495;&#23454;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;LSGNN&#26041;&#27861;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#19978;&#22343;&#33021;&#25552;&#20379;&#21487;&#27604;&#25110;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.04225</link><description>&lt;p&gt;
LSGNN&#65306;&#36890;&#36807;&#23616;&#37096;&#30456;&#20284;&#24615;&#23454;&#29616;&#33410;&#28857;&#20998;&#31867;&#20013;&#30340;&#26222;&#36866;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LSGNN: Towards General Graph Neural Network in Node Classification by Local Similarity. (arXiv:2305.04225v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#30456;&#20284;&#24615;&#65288;LocalSim&#65289;&#23398;&#20064;&#33410;&#28857;&#32423;&#21152;&#26435;&#34701;&#21512;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#65292;&#25552;&#21462;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#22810;&#36339;&#20449;&#24687;&#65292;&#24182;&#23545;&#20854;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#22312;&#30495;&#23454;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;LSGNN&#26041;&#27861;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#19978;&#22343;&#33021;&#25552;&#20379;&#21487;&#27604;&#25110;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#36136;&#24615;&#34987;&#35748;&#20026;&#26159;&#20260;&#23475;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#29616;&#26377;&#30340;&#24037;&#20316;&#20351;&#29992;&#22810;&#36339;&#37051;&#23621;&#20449;&#24687;&#30340;&#22270;&#32423;&#21152;&#26435;&#34701;&#21512;&#26469;&#21253;&#21547;&#26356;&#22810;&#20855;&#26377;&#21516;&#36136;&#24615;&#30340;&#33410;&#28857;&#12290;&#28982;&#32780;&#65292;&#24322;&#36136;&#24615;&#21487;&#33021;&#22312;&#33410;&#28857;&#20043;&#38388;&#19981;&#21516;&#65292;&#38656;&#35201;&#32771;&#34385;&#23616;&#37096;&#25299;&#25169;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#23616;&#37096;&#30456;&#20284;&#24615;&#65288;LocalSim&#65289;&#23398;&#20064;&#33410;&#28857;&#32423;&#21152;&#26435;&#34701;&#21512;&#65292;&#24182;&#21487;&#20316;&#20026;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#34701;&#21512;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#12289;&#39640;&#25928;&#30340;&#21021;&#22987;&#27531;&#24046;&#24046;&#36830;&#25509;&#65288;IRDC&#65289;&#26469;&#25552;&#21462;&#26356;&#20855;&#20449;&#24687;&#24615;&#30340;&#22810;&#36339;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#21512;&#25104;&#22270;&#19978;LocalSim&#20195;&#34920;&#33410;&#28857;&#21516;&#36136;&#24615;&#30340;&#26377;&#25928;&#24615;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;&#22312;&#30495;&#23454;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#21363;&#23616;&#37096;&#30456;&#20284;&#24615;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;LSGNN&#65289;&#65292;&#22312;&#21516;&#36136;&#24615;&#21644;&#24322;&#36136;&#24615;&#22270;&#19978;&#22343;&#33021;&#25552;&#20379;&#21487;&#27604;&#25110;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterophily has been considered as an issue that hurts the performance of Graph Neural Networks (GNNs). To address this issue, some existing work uses a graph-level weighted fusion of the information of multi-hop neighbors to include more nodes with homophily. However, the heterophily might differ among nodes, which requires to consider the local topology. Motivated by it, we propose to use the local similarity (LocalSim) to learn node-level weighted fusion, which can also serve as a plug-and-play module. For better fusion, we propose a novel and efficient Initial Residual Difference Connection (IRDC) to extract more informative multi-hop information. Moreover, we provide theoretical analysis on the effectiveness of LocalSim representing node homophily on synthetic graphs. Extensive evaluations over real benchmark datasets show that our proposed method, namely Local Similarity Graph Neural Network (LSGNN), can offer comparable or superior state-of-the-art performance on both homophilic
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.03942</link><description>&lt;p&gt;
&#23398;&#20064;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#28151;&#21512;&#28436;&#21592;-&#35780;&#35770;&#21592;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning Hybrid Actor-Critic Maps for 6D Non-Prehensile Manipulation. (arXiv:2305.03942v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03942
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;HACMan&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#36827;&#34892;6D&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#30340;&#29289;&#20307;&#25805;&#32437;&#12290;HACMan&#37325;&#28857;&#20851;&#27880;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#23427;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#22312;&#23454;&#38469;&#27979;&#35797;&#20013;&#65292;HACMan&#30340;&#34920;&#29616;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#31867;&#30340;&#28789;&#24039;&#24615;&#20013;&#65292;&#38750;&#25235;&#21462;&#24335;&#25805;&#20316;&#26159;&#25805;&#20316;&#29289;&#20307;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#38750;&#25235;&#21462;&#24335;&#25805;&#32437;&#21487;&#20197;&#20351;&#19982;&#29289;&#20307;&#30340;&#20132;&#20114;&#26356;&#21152;&#22797;&#26434;&#65292;&#20294;&#20063;&#22312;&#25512;&#29702;&#20132;&#20114;&#26041;&#38754;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;HACMan&#30340;&#28151;&#21512;&#28436;&#21592;&#35780;&#35770;&#21592;&#22320;&#22270;&#65292;&#36825;&#26159;&#19968;&#31181;&#20351;&#29992;&#28857;&#20113;&#35266;&#23519;&#30340;6D&#38750;&#25235;&#21462;&#24335;&#29289;&#20307;&#25805;&#20316;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;HACMan&#25552;&#20986;&#20102;&#19968;&#31181;&#26102;&#38388;&#25277;&#35937;&#21644;&#31354;&#38388;&#22522;&#30784;&#30340;&#29289;&#20307;&#20013;&#24515;&#21160;&#20316;&#34920;&#31034;&#65292;&#35813;&#34920;&#31034;&#21253;&#25324;&#20174;&#29289;&#20307;&#28857;&#20113;&#20013;&#36873;&#25321;&#25509;&#35302;&#20301;&#32622;&#21644;&#19968;&#32452;&#25551;&#36848;&#26426;&#22120;&#20154;&#22312;&#25509;&#35302;&#21518;&#22914;&#20309;&#31227;&#21160;&#30340;&#36816;&#21160;&#21442;&#25968;&#12290;&#25105;&#20204;&#20462;&#25913;&#20102;&#19968;&#20010;&#29616;&#26377;&#30340;&#31163;&#32447;&#31574;&#30053;RL&#31639;&#27861;&#65292;&#20197;&#22312;&#36825;&#31181;&#28151;&#21512;&#30340;&#31163;&#25955;-&#36830;&#32493;&#21160;&#20316;&#34920;&#31034;&#23398;&#20064;&#12290;&#25105;&#20204;&#22312;&#20223;&#30495;&#21644;&#29616;&#23454;&#19990;&#30028;&#20013;&#23545;HACMan&#36827;&#34892;&#20102;6D&#29289;&#20307;&#23039;&#24577;&#23545;&#40784;&#20219;&#21153;&#30340;&#35780;&#20272;&#12290;&#22312;&#26368;&#38590;&#30340;&#20219;&#21153;&#29256;&#26412;&#20013;&#65292;&#36890;&#36807;&#38543;&#26426;&#21021;&#22987;&#21270;&#29289;&#20307;&#21644;&#26426;&#22120;&#20154;&#37197;&#32622;&#65292;HACMan&#30340;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#30340;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Manipulating objects without grasping them is an essential component of human dexterity, referred to as non-prehensile manipulation. Non-prehensile manipulation may enable more complex interactions with the objects, but also presents challenges in reasoning about the interactions. In this work, we introduce Hybrid Actor-Critic Maps for Manipulation (HACMan), a reinforcement learning approach for 6D non-prehensile manipulation of objects using point cloud observations. HACMan proposes a temporally-abstracted and spatially-grounded object-centric action representation that consists of selecting a contact location from the object point cloud and a set of motion parameters describing how the robot will move after making contact. We modify an existing off-policy RL algorithm to learn in this hybrid discrete-continuous action representation. We evaluate HACMan on a 6D object pose alignment task in both simulation and in the real world. On the hardest version of our task, with randomized init
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;</title><link>http://arxiv.org/abs/2305.03731</link><description>&lt;p&gt;
&#35780;&#20272;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;
&lt;/p&gt;
&lt;p&gt;
Assessing Working Memory Capacity of ChatGPT. (arXiv:2305.03731v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;ChatGPT&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#65292;&#32467;&#26524;&#26174;&#31034;&#20854;&#22312;N-back&#20219;&#21153;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#20154;&#31867;&#21442;&#19982;&#32773;&#30456;&#20284;&#65292;&#36825;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24037;&#20316;&#35760;&#24518;&#26159;&#20154;&#31867;&#26234;&#33021;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#65292;&#23427;&#20316;&#20026;&#20449;&#24687;&#20020;&#26102;&#23384;&#20648;&#21644;&#25805;&#20316;&#30340;&#24037;&#20316;&#31354;&#38388;&#12290;&#26412;&#25991;&#36890;&#36807;&#26816;&#26597;ChatGPT&#22312;N-back&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#35843;&#26597;&#20102;&#36825;&#19968;&#26368;&#20808;&#36827;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#24037;&#20316;&#35760;&#24518;&#23545;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#37325;&#35201;&#24615;&#65292;&#25509;&#30528;&#20171;&#32461;&#20102;&#35780;&#20272;ChatGPT&#24037;&#20316;&#35760;&#24518;&#23481;&#37327;&#30340;&#26041;&#27861;&#12290;&#30740;&#31350;&#27604;&#36739;&#20102;ChatGPT&#22312;&#35328;&#35821;&#21644;&#31354;&#38388;N- back&#20219;&#21153;&#19978;&#30340;&#34892;&#20026;&#34920;&#29616;&#19982;&#25991;&#29486;&#25253;&#36947;&#30340;&#20154;&#31867;&#21442;&#19982;&#32773;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#20102;&#26174;&#33879;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#20026;&#35774;&#35745;&#20855;&#26377;&#20154;&#31867;&#32423;&#35748;&#30693;&#33021;&#21147;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24403;&#21069;&#36827;&#23637;&#25552;&#20379;&#20102;&#20851;&#38190;&#27934;&#23519;&#65292;&#24182;&#20026;&#36890;&#36807;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#29702;&#35299;&#20154;&#31867;&#24037;&#20316;&#35760;&#24518;&#30340;&#26410;&#26469;&#21162;&#21147;&#25552;&#20379;&#20102;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Working memory is a critical aspect of both human intelligence and artificial intelligence (AI), serving as a workspace for the temporary storage and manipulation of information. This paper investigates working memory capacity of ChatGPT, a state-of-the-art language model, by examining its performance on N-back tasks. We begin by discussing the importance of working memory to humans and AI, followed by the methods employed to assess working memory capacity of ChatGPT. Our study compares behavioral performance of ChatGPT on verbal and spatial N-back tasks to that of human participants reported in the literature, revealing notable similarities. Our findings offer crucial insights into the current progress in designing AI systems with human-level cognitive abilities and hold promise for informing future endeavors aimed at enhancing AI working memory and understanding human working memory through AI models.
&lt;/p&gt;</description></item><item><title>ChatGPT&#21644;Bard&#31561;AI&#24037;&#20855;&#38656;&#35201;&#25345;&#32493;&#22823;&#37327;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#20294;&#29616;&#34892;&#30340;&#29256;&#26435;&#27861;&#21017;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#25968;&#25454;&#30340;&#33719;&#21462;&#12290;&#19982;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#23558;&#26377;&#21161;&#20110;&#23558;AI&#24037;&#20855;&#19982;&#22823;&#22810;&#25968;&#29256;&#26435;&#25968;&#25454;&#25317;&#26377;&#32773;&#20043;&#38388;&#30340;&#25932;&#23545;&#20851;&#31995;&#36716;&#21464;&#20026;&#21512;&#20316;&#20851;&#31995;&#65292;&#20351;AI&#29983;&#24577;&#31995;&#32479;&#26356;&#20581;&#24247;&#12290;</title><link>http://arxiv.org/abs/2305.02555</link><description>&lt;p&gt;
ChatGPT&#21644;Bard&#26159;&#21542;&#24212;&#35813;&#19982;&#20854;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#65311;AI&#26102;&#20195;&#30340;&#26032;&#21830;&#19994;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Should ChatGPT and Bard Share Revenue with Their Data Providers? A New Business Model for the AI Era. (arXiv:2305.02555v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02555
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#21644;Bard&#31561;AI&#24037;&#20855;&#38656;&#35201;&#25345;&#32493;&#22823;&#37327;&#19988;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#65292;&#20294;&#29616;&#34892;&#30340;&#29256;&#26435;&#27861;&#21017;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#25968;&#25454;&#30340;&#33719;&#21462;&#12290;&#19982;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#23558;&#26377;&#21161;&#20110;&#23558;AI&#24037;&#20855;&#19982;&#22823;&#22810;&#25968;&#29256;&#26435;&#25968;&#25454;&#25317;&#26377;&#32773;&#20043;&#38388;&#30340;&#25932;&#23545;&#20851;&#31995;&#36716;&#21464;&#20026;&#21512;&#20316;&#20851;&#31995;&#65292;&#20351;AI&#29983;&#24577;&#31995;&#32479;&#26356;&#20581;&#24247;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#31561;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#25105;&#20204;&#27491;&#36827;&#20837;&#30495;&#27491;&#30340;AI&#26102;&#20195;&#12290;&#25105;&#20204;&#21487;&#20197;&#39044;&#35265;&#65292;&#21331;&#36234;&#30340;AI&#24037;&#20855;&#24456;&#24555;&#23558;&#33719;&#24471;&#21487;&#35266;&#30340;&#21033;&#28070;&#12290;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#20986;&#29616;&#20102;&#65306;&#38500;&#20102;&#20256;&#32479;&#30340;&#21033;&#30410;&#30456;&#20851;&#32773;&#21644;&#32929;&#19996;&#65292;AI&#24037;&#20855;&#26159;&#21542;&#24212;&#35813;&#19982;&#23427;&#20204;&#30340;&#35757;&#32451;&#25968;&#25454;&#25552;&#20379;&#32773;&#20998;&#20139;&#25910;&#30410;&#65311;&#31572;&#26696;&#26159;&#32943;&#23450;&#30340;&#12290;&#22823;&#22411;AI&#24037;&#20855;&#65292;&#20363;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#22987;&#32456;&#38656;&#35201;&#26356;&#22810;&#12289;&#26356;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#19981;&#26029;&#25913;&#36827;&#65292;&#20294;&#24403;&#21069;&#30340;&#29256;&#26435;&#27861;&#38480;&#21046;&#20102;&#23427;&#20204;&#23545;&#21508;&#31181;&#31867;&#22411;&#25968;&#25454;&#30340;&#33719;&#21462;&#12290;&#22312;AI&#24037;&#20855;&#21644;&#25968;&#25454;&#25552;&#20379;&#32773;&#20043;&#38388;&#20998;&#20139;&#25910;&#30410;&#21487;&#20197;&#23558;&#24403;&#21069;&#25932;&#23545;&#30340;&#38646;&#21644;&#28216;&#25103;&#20851;&#31995;&#36716;&#21464;&#20026;&#19968;&#31181;&#21512;&#20316;&#21644;&#20114;&#21033;&#30340;&#20851;&#31995;&#65292;&#32780;&#36825;&#31181;&#20851;&#31995;&#23545;&#20110;&#20419;&#36827;AI&#24037;&#20855;&#12289;&#29992;&#25143;&#21644;&#25968;&#25454;&#25552;&#20379;&#32773;&#20043;&#38388;&#30340;&#33391;&#24615;&#24490;&#29615;&#21457;&#23637;&#12289;&#25512;&#21160;AI&#25216;&#26415;&#24182;&#24314;&#31435;&#20581;&#24247;&#30340;AI&#29983;&#24577;&#31995;&#32479;&#26159;&#24517;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25910;&#30410;&#20998;&#20139;&#21830;&#19994;&#27169;&#24335;&#8230;&#8230;
&lt;/p&gt;
&lt;p&gt;
With various AI tools such as ChatGPT becoming increasingly popular, we are entering a true AI era. We can foresee that exceptional AI tools will soon reap considerable profits. A crucial question arise: should AI tools share revenue with their training data providers in additional to traditional stakeholders and shareholders? The answer is Yes. Large AI tools, such as large language models, always require more and better quality data to continuously improve, but current copyright laws limit their access to various types of data. Sharing revenue between AI tools and their data providers could transform the current hostile zero-sum game relationship between AI tools and a majority of copyrighted data owners into a collaborative and mutually beneficial one, which is necessary to facilitate the development of a virtuous cycle among AI tools, their users and data providers that drives forward AI technology and builds a healthy AI ecosystem. However, current revenue-sharing business models 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29420;&#31435;&#30340;&#23616;&#37096;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#22312;&#22823;&#22411;&#24322;&#26500;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#22312;&#25628;&#32034;&#12289;&#25913;&#36827;&#21644;&#36824;&#21407;&#27169;&#24335;&#19979;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#21487;&#33258;&#36866;&#24212;&#20462;&#25913;&#21464;&#37327;&#20540;&#30340;&#31639;&#23376;&#21644;&#39640;&#25928;&#30340;&#20030;&#21319;&#31639;&#23376;&#65292;&#20174;&#32780;&#25552;&#39640;&#24403;&#21069;&#35299;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;MIPLIB2017&#30340;&#24322;&#26500;&#38382;&#39064;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.00188</link><description>&lt;p&gt;
&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#30340;&#23616;&#37096;&#25628;&#32034;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Local Search for Integer Linear Programming. (arXiv:2305.00188v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.00188
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#29420;&#31435;&#30340;&#23616;&#37096;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#22312;&#22823;&#22411;&#24322;&#26500;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#22312;&#25628;&#32034;&#12289;&#25913;&#36827;&#21644;&#36824;&#21407;&#27169;&#24335;&#19979;&#65292;&#20998;&#21035;&#25552;&#20986;&#20102;&#21487;&#33258;&#36866;&#24212;&#20462;&#25913;&#21464;&#37327;&#20540;&#30340;&#31639;&#23376;&#21644;&#39640;&#25928;&#30340;&#20030;&#21319;&#31639;&#23376;&#65292;&#20174;&#32780;&#25552;&#39640;&#24403;&#21069;&#35299;&#30340;&#36136;&#37327;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;MIPLIB2017&#30340;&#24322;&#26500;&#38382;&#39064;&#38598;&#19978;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#36866;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#30340;&#32452;&#21512;&#20248;&#21270;&#38382;&#39064;&#65292;&#23545;&#20110;&#20135;&#19994;&#21644;&#31649;&#29702;&#37096;&#38376;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;&#26412;&#35770;&#25991;&#24320;&#21457;&#20102;&#31532;&#19968;&#20010;&#29420;&#31435;&#30340;&#23616;&#37096;&#25628;&#32034;&#27714;&#35299;&#22120;&#65292;&#21487;&#29992;&#20110;&#35299;&#20915;&#19968;&#33324;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65292;&#24182;&#22312;&#22823;&#22411;&#24322;&#26500;&#38382;&#39064;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#25105;&#20204;&#25552;&#20986;&#19968;&#20010;&#23616;&#37096;&#25628;&#32034;&#26694;&#26550;&#65292;&#20999;&#25442;&#19977;&#31181;&#27169;&#24335;&#65292;&#20998;&#21035;&#20026;&#25628;&#32034;&#65292;&#25913;&#36827;&#21644;&#36824;&#21407;&#27169;&#24335;&#65292;&#24182;&#35774;&#35745;&#36866;&#24212;&#19981;&#21516;&#27169;&#24335;&#30340;&#23450;&#21046;&#31639;&#23376;&#65292;&#20174;&#32780;&#26681;&#25454;&#19981;&#21516;&#24773;&#20917;&#25552;&#39640;&#24403;&#21069;&#35299;&#30340;&#36136;&#37327;&#12290;&#23545;&#20110;&#25628;&#32034;&#21644;&#36824;&#21407;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#32039;&#36523;&#21160;&#20316;&#8221;&#30340;&#31639;&#23376;&#65292;&#23427;&#21487;&#20197;&#33258;&#36866;&#24212;&#22320;&#20462;&#25913;&#21464;&#37327;&#30340;&#20540;&#65292;&#35797;&#22270;&#20351;&#26576;&#20123;&#32422;&#26463;&#21464;&#24471;&#26356;&#32039;&#12290;&#23545;&#20110;&#25913;&#36827;&#27169;&#24335;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31639;&#23376;&#8220;&#20030;&#21319;&#21160;&#20316;&#8221;&#65292;&#21487;&#20197;&#22312;&#20445;&#25345;&#21487;&#34892;&#24615;&#30340;&#21516;&#26102;&#25552;&#39640;&#30446;&#26631;&#20989;&#25968;&#30340;&#36136;&#37327;&#12290;&#32467;&#21512;&#36825;&#20123;&#20869;&#23481;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#23616;&#37096;&#25628;&#32034;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#65292;&#31216;&#20026;Local-ILP&#12290;&#23545;MIPLIB2017&#30340;&#24322;&#26500;&#38382;&#39064;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;Local-ILP&#34920;&#29616;&#20248;&#24322;&#65292;&#21487;&#20197;&#19982;&#26368;&#20808;&#36827;&#30340;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27714;&#35299;&#22120;&#30456;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Integer linear programming models a wide range of practical combinatorial optimization problems and has significant impacts in industry and management sectors. This work develops the first standalone local search solver for general integer linear programming validated on a large heterogeneous problem dataset. We propose a local search framework that switches in three modes, namely Search, Improve, and Restore modes, and design tailored operators adapted to different modes, thus improve the quality of the current solution according to different situations. For the Search and Restore modes, we propose an operator named tight move, which adaptively modifies variables' values trying to make some constraint tight. For the Improve mode, an efficient operator lift move is proposed to improve the quality of the objective function while maintaining feasibility. Putting these together, we develop a local search solver for integer linear programming called Local-ILP. Experiments conducted on the 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#38382;&#39064;&#65292;&#20197;&#21450;&#36817;&#24180;&#26469;&#38450;&#27490;&#21644;&#21457;&#29616;&#27169;&#22411;&#31363;&#21462;&#21644;&#26410;&#32463;&#25480;&#26435;&#37325;&#26032;&#20998;&#21457;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.14613</link><description>&lt;p&gt;
&#28145;&#24230;&#30693;&#35782;&#20135;&#26435;: &#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Deep Intellectual Property: A Survey. (arXiv:2304.14613v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14613
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#20171;&#32461;&#20102;&#21033;&#29992;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26102;&#25152;&#38754;&#20020;&#30340;&#30693;&#35782;&#20135;&#26435;&#20445;&#25252;&#38382;&#39064;&#65292;&#20197;&#21450;&#36817;&#24180;&#26469;&#38450;&#27490;&#21644;&#21457;&#29616;&#27169;&#22411;&#31363;&#21462;&#21644;&#26410;&#32463;&#25480;&#26435;&#37325;&#26032;&#20998;&#21457;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#24037;&#19994;&#21046;&#36896;&#21644;&#21830;&#19994;&#26381;&#21153;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#32463;&#36807;&#20805;&#20998;&#35757;&#32451;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#30001;&#20110;&#24222;&#22823;&#30340;&#35757;&#32451;&#25104;&#26412;&#21644;&#20248;&#31168;&#30340;&#27867;&#21270;&#24615;&#33021;&#21464;&#24471;&#36234;&#26469;&#36234;&#26377;&#20215;&#20540;&#21644;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#20123;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#21487;&#20197;&#34987;&#29992;&#25143;&#21033;&#29992;&#65292;&#32780;&#26080;&#38656;&#20102;&#35299;&#22826;&#22810;&#19987;&#19994;&#30693;&#35782;&#65292;&#36825;&#24471;&#30410;&#20110;&#26032;&#20852;&#30340;&#8220;&#26426;&#22120;&#23398;&#20064;&#21363;&#26381;&#21153;&#8221;(MLaaS)&#33539;&#24335;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#33539;&#24335;&#20063;&#20351;&#24471;&#26114;&#36149;&#30340;&#27169;&#22411;&#38754;&#20020;&#35768;&#22810;&#28508;&#22312;&#23041;&#32961;&#65292;&#20363;&#22914;&#27169;&#22411;&#31363;&#21462;&#21644;&#28389;&#29992;&#12290;&#20026;&#20102;&#25269;&#24481;&#36825;&#20123;&#23041;&#32961;&#30340;&#36843;&#20999;&#38656;&#27714;&#65292;&#28145;&#24230;&#30693;&#35782;&#20135;&#26435;&#65288;Deep Intellectual Property&#65292;DeepIP&#65289;&#25104;&#20026;&#20102;&#19994;&#30028;&#21644;&#23398;&#26415;&#30028;&#30340;&#20849;&#35782;&#65292;&#20197;&#20445;&#25252;&#31169;&#26377;&#35757;&#32451;&#25968;&#25454;&#12289;&#36153;&#23613;&#24515;&#24605;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#25110;&#26114;&#36149;&#23398;&#20064;&#30340;&#27169;&#22411;&#26435;&#37325;&#12290;&#20026;&#27492;&#65292;&#36817;&#24180;&#26469;&#25552;&#20986;&#20102;&#35768;&#22810;&#26041;&#27861;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#29305;&#21035;&#26159;&#38450;&#27490;&#25110;&#21457;&#29616;&#27169;&#22411;&#31363;&#21462;&#21644;&#26410;&#32463;&#25480;&#26435;&#30340;&#37325;&#26032;&#20998;&#21457;&#12290;&#37492;&#20110;&#36825;&#19968;&#24555;&#36895;&#28436;&#21464;&#30340;&#26102;&#26399;&#65292;
&lt;/p&gt;
&lt;p&gt;
With the widespread application in industrial manufacturing and commercial services, well-trained deep neural networks (DNNs) are becoming increasingly valuable and crucial assets due to the tremendous training cost and excellent generalization performance. These trained models can be utilized by users without much expert knowledge benefiting from the emerging ''Machine Learning as a Service'' (MLaaS) paradigm. However, this paradigm also exposes the expensive models to various potential threats like model stealing and abuse. As an urgent requirement to defend against these threats, Deep Intellectual Property (DeepIP), to protect private training data, painstakingly-tuned hyperparameters, or costly learned model weights, has been the consensus of both industry and academia. To this end, numerous approaches have been proposed to achieve this goal in recent years, especially to prevent or discover model stealing and unauthorized redistribution. Given this period of rapid evolution, the g
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21704;&#23494;&#39039;&#22238;&#36335;&#30340;&#39640;&#32500;&#32858;&#31867;&#26694;&#26550;&#65292;&#23558;&#20840;&#23616;&#32467;&#26500;&#21644;&#23616;&#37096;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21704;&#23494;&#39039;&#22238;&#36335;&#23558;&#19981;&#21516;&#31751;&#30340;&#38170;&#28857;&#25490;&#24207;&#24182;&#26144;&#23556;&#21040;&#22278;&#30340;&#21608;&#38271;&#19978;&#65292;&#25913;&#36827;&#20102;&#32858;&#31867;&#30340;&#26631;&#31614;&#65292;&#36798;&#21040;&#26356;&#22909;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.14531</link><description>&lt;p&gt;
&#22522;&#20110;&#21704;&#23494;&#39039;&#22238;&#36335;&#30340;&#39640;&#32500;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
High-dimensional Clustering onto Hamiltonian Cycle. (arXiv:2304.14531v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14531
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21704;&#23494;&#39039;&#22238;&#36335;&#30340;&#39640;&#32500;&#32858;&#31867;&#26694;&#26550;&#65292;&#23558;&#20840;&#23616;&#32467;&#26500;&#21644;&#23616;&#37096;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#21704;&#23494;&#39039;&#22238;&#36335;&#23558;&#19981;&#21516;&#31751;&#30340;&#38170;&#28857;&#25490;&#24207;&#24182;&#26144;&#23556;&#21040;&#22278;&#30340;&#21608;&#38271;&#19978;&#65292;&#25913;&#36827;&#20102;&#32858;&#31867;&#30340;&#26631;&#31614;&#65292;&#36798;&#21040;&#26356;&#22909;&#30340;&#20998;&#31867;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32858;&#31867;&#26088;&#22312;&#26681;&#25454;&#26679;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#23545;&#26410;&#26631;&#35760;&#30340;&#26679;&#26412;&#36827;&#34892;&#20998;&#32452;&#12290;&#23427;&#24050;&#25104;&#20026;&#20998;&#26512;&#39640;&#32500;&#25968;&#25454;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#32858;&#31867;&#26041;&#27861;&#20165;&#29983;&#25104;&#20266;&#26631;&#31614;&#65292;&#22240;&#27492;&#26080;&#27861;&#21516;&#26102;&#21576;&#29616;&#19981;&#21516;&#31751;&#21644;&#24322;&#24120;&#20540;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#22522;&#20110;&#21704;&#23494;&#39039;&#22238;&#36335;&#30340;&#39640;&#32500;&#32858;&#31867;&#65288;HCHC&#65289;&#65292;&#20197;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;HCHC&#22312;&#19968;&#20010;&#30446;&#26631;&#20989;&#25968;&#20013;&#23558;&#20840;&#23616;&#32467;&#26500;&#19982;&#23616;&#37096;&#32467;&#26500;&#30456;&#32467;&#21512;&#65292;&#36827;&#34892;&#28145;&#24230;&#32858;&#31867;&#65292;&#23558;&#26631;&#31614;&#25913;&#36827;&#20026;&#30456;&#23545;&#27010;&#29575;&#65292;&#20197;&#25366;&#25496;&#19981;&#21516;&#31751;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#27599;&#20010;&#31751;&#20869;&#30340;&#23616;&#37096;&#32467;&#26500;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#31751;&#30456;&#20284;&#24615;&#29983;&#25104;&#26368;&#20248;&#21704;&#23494;&#39039;&#22238;&#36335;&#19978;&#23545;&#19981;&#21516;&#31751;&#30340;&#38170;&#36827;&#34892;&#25490;&#24207;&#65292;&#24182;&#26144;&#23556;&#21040;&#22278;&#30340;&#21608;&#38271;&#19978;&#12290;&#26368;&#21518;&#65292;&#23545;&#20110;&#19968;&#20010;&#26356;&#39640;&#27010;&#29575;&#23646;&#20110;&#26576;&#20010;&#31751;&#30340;&#26679;&#26412;&#65292;&#23427;&#20250;&#26144;&#23556;&#21040;&#19982;&#20043;&#30456;&#24212;&#30340;&#38170;&#28857;&#26356;&#36817;&#30340;&#20301;&#32622;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#20998;&#31867;&#25928;&#26524;&#24471;&#21040;&#20102;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clustering aims to group unlabelled samples based on their similarities. It has become a significant tool for the analysis of high-dimensional data. However, most of the clustering methods merely generate pseudo labels and thus are unable to simultaneously present the similarities between different clusters and outliers. This paper proposes a new framework called High-dimensional Clustering onto Hamiltonian Cycle (HCHC) to solve the above problems. First, HCHC combines global structure with local structure in one objective function for deep clustering, improving the labels as relative probabilities, to mine the similarities between different clusters while keeping the local structure in each cluster. Then, the anchors of different clusters are sorted on the optimal Hamiltonian cycle generated by the cluster similarities and mapped on the circumference of a circle. Finally, a sample with a higher probability of a cluster will be mapped closer to the corresponding anchor. In this way, ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#21160;&#24577;&#31995;&#32479;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#12289;&#22312;&#32447;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22312;&#32447;&#36801;&#31227;&#23398;&#20064;&#21644;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.12583</link><description>&lt;p&gt;
&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#21160;&#24577;&#31995;&#32479;&#30340;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#65306;&#26041;&#27861;&#21644;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Real-time Safety Assessment of Dynamic Systems in Non-stationary Environments: A Review of Methods and Techniques. (arXiv:2304.12583v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12583
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#21160;&#24577;&#31995;&#32479;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#65292;&#21253;&#25324;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#12289;&#22312;&#32447;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22312;&#32447;&#36801;&#31227;&#23398;&#20064;&#21644;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#65292;&#24182;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#24577;&#31995;&#32479;&#30340;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#26159;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#23545;&#20110;&#24037;&#19994;&#21644;&#20132;&#36890;&#24212;&#29992;&#31561;&#21508;&#20010;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#26041;&#27861;&#30340;&#20840;&#38754;&#32508;&#36848;&#38459;&#30861;&#20102;&#30456;&#20851;&#26041;&#27861;&#30340;&#36827;&#23637;&#21644;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#38024;&#23545;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#20219;&#21153;&#30340;&#26041;&#27861;&#21644;&#25216;&#26415;&#32508;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#39318;&#20808;&#31361;&#20986;&#20102;&#38750;&#24179;&#31283;&#29615;&#22659;&#20013;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#26041;&#27861;&#30340;&#32972;&#26223;&#21644;&#37325;&#35201;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#38382;&#39064;&#25551;&#36848;&#65292;&#21253;&#25324;&#23450;&#20041;&#12289;&#20998;&#31867;&#21644;&#20027;&#35201;&#25361;&#25112;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#30456;&#20851;&#25216;&#26415;&#30340;&#26368;&#26032;&#21457;&#23637;&#65292;&#22914;&#22312;&#32447;&#20027;&#21160;&#23398;&#20064;&#12289;&#22312;&#32447;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#22312;&#32447;&#36801;&#31227;&#23398;&#20064;&#21644;&#22312;&#32447;&#24322;&#24120;&#26816;&#27979;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26410;&#26469;&#30340;&#23637;&#26395;&#21644;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;&#26412;&#25991;&#30340;&#32508;&#36848;&#26088;&#22312;&#20026;&#38750;&#24179;&#31283;&#29615;&#22659;&#19979;&#23454;&#26102;&#23433;&#20840;&#35780;&#20272;&#25552;&#20379;&#21442;&#32771;&#20381;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-time safety assessment (RTSA) of dynamic systems is a critical task that has significant implications for various fields such as industrial and transportation applications, especially in non-stationary environments. However, the absence of a comprehensive review of real-time safety assessment methods in non-stationary environments impedes the progress and refinement of related methods. In this paper, a review of methods and techniques for RTSA tasks in non-stationary environments is provided. Specifically, the background and significance of RTSA approaches in non-stationary environments are firstly highlighted. We then present a problem description that covers the definition, classification, and main challenges. We review recent developments in related technologies such as online active learning, online semi-supervised learning, online transfer learning, and online anomaly detection. Finally, we discuss future outlooks and potential directions for further research. Our review aims
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#25143;&#27169;&#25311;&#22120;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#20027;&#21160;&#23545;&#35805;&#31574;&#30053;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#25506;&#32034;&#21644;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#34920;&#29616;&#30340;&#36866;&#24403;&#20027;&#21160;&#31574;&#30053;&#30340;&#36884;&#24452;&#12290;</title><link>http://arxiv.org/abs/2304.11913</link><description>&lt;p&gt;
&#24320;&#21457;&#19968;&#31181;&#20449;&#20219;&#24863;&#24863;&#30693;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#65292;&#29992;&#20110;&#32479;&#35745;&#23398;&#30340;&#20027;&#21160;&#24335;&#23545;&#35805;&#24314;&#27169;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;
&lt;/p&gt;
&lt;p&gt;
Development of a Trust-Aware User Simulator for Statistical Proactive Dialog Modeling in Human-AI Teams. (arXiv:2304.11913v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.11913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#31181;&#29992;&#25143;&#27169;&#25311;&#22120;&#26469;&#35757;&#32451;&#21644;&#27979;&#35797;&#20027;&#21160;&#23545;&#35805;&#31574;&#30053;&#65292;&#25552;&#20379;&#20102;&#19968;&#31181;&#25506;&#32034;&#21644;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#34920;&#29616;&#30340;&#36866;&#24403;&#20027;&#21160;&#31574;&#30053;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#30340;&#27010;&#24565;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#23454;&#29616;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#38431;&#21451;&#20043;&#38388;&#30340;&#26377;&#25928;&#21327;&#20316;&#65292;&#20027;&#21160;&#24615;&#23545;&#20110;&#32039;&#23494;&#21327;&#35843;&#21644;&#26377;&#25928;&#27807;&#36890;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22914;&#20309;&#20026;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#31995;&#32479;&#35774;&#35745;&#36866;&#24403;&#30340;&#20027;&#21160;&#24615;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20027;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22522;&#20110;&#35821;&#26009;&#24211;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#24320;&#21457;&#65292;&#29992;&#20110;&#35757;&#32451;&#21644;&#27979;&#35797;&#20027;&#21160;&#24335;&#23545;&#35805;&#31574;&#30053;&#12290;&#35813;&#27169;&#25311;&#22120;&#20197;&#26377;&#20851;&#20027;&#21160;&#24335;&#23545;&#35805;&#21450;&#20854;&#23545;&#29992;&#25143;&#20449;&#20219;&#24230;&#30340;&#30693;&#35782;&#20026;&#22522;&#30784;&#65292;&#27169;&#25311;&#29992;&#25143;&#34892;&#20026;&#21644;&#20010;&#20154;&#20449;&#24687;&#65292;&#21253;&#25324;&#31038;&#20250;&#20154;&#21475;&#29305;&#24449;&#21644;&#20154;&#26684;&#29305;&#24449;&#12290;&#23545;&#27604;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#27169;&#25311;&#26041;&#27861;&#65292;&#22522;&#20110;&#20219;&#21153;&#27493;&#39588;&#30340;&#26041;&#27861;&#30001;&#20110;&#21152;&#24378;&#20102;&#39034;&#24207;&#20381;&#36182;&#27169;&#22411;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24635;&#20307;&#32467;&#26524;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#25506;&#32034;&#21644;&#35780;&#20272;&#22312;&#23545;&#35805;&#28216;&#25103;&#29615;&#22659;&#20013;&#25913;&#21892;&#20154;&#24037;&#26234;&#33021;&#22242;&#38431;&#34920;&#29616;&#30340;&#36866;&#24403;&#20027;&#21160;&#31574;&#30053;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
The concept of a Human-AI team has gained increasing attention in recent years. For effective collaboration between humans and AI teammates, proactivity is crucial for close coordination and effective communication. However, the design of adequate proactivity for AI-based systems to support humans is still an open question and a challenging topic. In this paper, we present the development of a corpus-based user simulator for training and testing proactive dialog policies. The simulator incorporates informed knowledge about proactive dialog and its effect on user trust and simulates user behavior and personal information, including socio-demographic features and personality traits. Two different simulation approaches were compared, and a task-step-based approach yielded better overall results due to enhanced modeling of sequential dependencies. This research presents a promising avenue for exploring and evaluating appropriate proactive strategies in a dialog game setting for improving H
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;k-NN&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#33021;&#22815;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.09058</link><description>&lt;p&gt;
&#37325;&#35775;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;k-NN
&lt;/p&gt;
&lt;p&gt;
Revisiting k-NN for Pre-trained Language Models. (arXiv:2304.09058v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;k-NN&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#33021;&#22815;&#25552;&#39640;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#20316;&#20026;&#21442;&#25968;&#21270;&#30340;&#24613;&#20999;&#23398;&#20064;&#22120;&#65292;&#24050;&#25104;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24403;&#21069;&#33539;&#24335;&#30340;&#23454;&#38469;&#36873;&#25321;&#12290;&#19982;&#27492;&#24418;&#25104;&#23545;&#27604;&#30340;&#26159;&#65292;k-&#26368;&#36817;&#37051;&#65288;k-NN&#65289;&#20998;&#31867;&#22120;&#20316;&#20026;&#24310;&#36831;&#23398;&#20064;&#27169;&#22411;&#65292;&#20542;&#21521;&#20110;&#20943;&#36731;&#36807;&#25311;&#21512;&#21644;&#23396;&#31435;&#22122;&#22768;&#12290;&#26412;&#25991;&#20013;&#25105;&#20204;&#37325;&#35775;&#20102;k-NN&#20998;&#31867;&#22120;&#65292;&#20197;&#22686;&#24378;&#22522;&#20110;PLMs&#30340;&#20998;&#31867;&#22120;&#12290;&#20174;&#26041;&#27861;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#25991;&#26412;&#34920;&#31034;&#30340;PLMs&#22312;&#20004;&#20010;&#27493;&#39588;&#20013;&#37319;&#29992;k-NN&#65306;&#65288;1&#65289;&#21033;&#29992;k-NN&#20316;&#20026;&#20808;&#39564;&#30693;&#35782;&#26469;&#26657;&#20934;&#35757;&#32451;&#36807;&#31243;&#65288;2&#65289;&#32447;&#24615;&#25554;&#20540;k-NN&#39044;&#27979;&#30340;&#27010;&#29575;&#20998;&#24067;&#21644;PLMs&#20998;&#31867;&#22120;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26680;&#24515;&#26159;&#23454;&#29616;&#20102;k-NN&#26657;&#20934;&#35757;&#32451;&#65292;&#23558;&#39044;&#27979;&#32467;&#26524;&#20316;&#20026;&#35757;&#32451;&#36807;&#31243;&#20013;&#26131;&#20110;&#21644;&#38590;&#20197;&#23398;&#20064;&#30340;&#31034;&#20363;&#30340;&#25351;&#26631;&#12290;&#20174;&#24212;&#29992;&#22330;&#26223;&#22810;&#26679;&#24615;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#25105;&#20204;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#24494;&#35843;&#12289;&#25552;&#31034;&#24494;&#35843;&#33539;&#24335;&#21644;&#38646;&#26679;&#26412;&#20219;&#21153;&#35774;&#32622;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#32467;&#21512;k-NN&#21487;&#20197;&#22312;&#25152;&#26377;&#21463;&#21040;&#26816;&#26597;&#30340;&#35774;&#32622;&#20013;&#25345;&#32493;&#25552;&#39640;PLMs&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#25152;&#26377;&#21463;&#21040;&#32771;&#34385;&#30340;&#35774;&#32622;&#20013;&#36305;&#36194;&#20102;&#22522;&#20110;&#26222;&#36890;PLMs&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained Language Models (PLMs), as parametric-based eager learners, have become the de-facto choice for current paradigms of Natural Language Processing (NLP). In contrast, k-Nearest-Neighbor (k-NN) classifiers, as the lazy learning paradigm, tend to mitigate over-fitting and isolated noise. In this paper, we revisit k-NN classifiers for augmenting the PLMs-based classifiers. From the methodological level, we propose to adopt k-NN with textual representations of PLMs in two steps: (1) Utilize k-NN as prior knowledge to calibrate the training process. (2) Linearly interpolate the probability distribution predicted by k-NN with that of the PLMs' classifier. At the heart of our approach is the implementation of k-NN-calibrated training, which treats predicted results as indicators for easy versus hard examples during the training process. From the perspective of the diversity of application scenarios, we conduct extensive experiments on fine-tuning, prompt-tuning paradigms and zero-sh
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#29031;&#26126;&#31995;&#32479;DeePLT&#65292;&#20854;&#36890;&#36807;&#36712;&#36857;&#39044;&#27979;&#23454;&#29616;&#26234;&#33021;&#23478;&#23621;&#20013;&#30340;&#20010;&#24615;&#21270;&#29031;&#26126;&#35843;&#25972;&#65292;&#32473;&#27599;&#20010;&#20154;&#23450;&#21046;&#29420;&#29305;&#30340;&#20010;&#20154;&#36164;&#26009;&#24182;&#26681;&#25454;&#20854;&#36712;&#36857;&#33258;&#21160;&#35843;&#25972;&#28783;&#20809;&#12290;</title><link>http://arxiv.org/abs/2304.08027</link><description>&lt;p&gt;
DeePLT&#65306;&#22522;&#20110;&#36712;&#36857;&#39044;&#27979;&#23454;&#29616;&#26234;&#33021;&#23478;&#23621;&#20013;&#30340;&#20010;&#24615;&#21270;&#29031;&#26126;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
DeePLT: Personalized Lighting Facilitates by Trajectory Prediction of Recognized Residents in the Smart Home. (arXiv:2304.08027v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#29031;&#26126;&#31995;&#32479;DeePLT&#65292;&#20854;&#36890;&#36807;&#36712;&#36857;&#39044;&#27979;&#23454;&#29616;&#26234;&#33021;&#23478;&#23621;&#20013;&#30340;&#20010;&#24615;&#21270;&#29031;&#26126;&#35843;&#25972;&#65292;&#32473;&#27599;&#20010;&#20154;&#23450;&#21046;&#29420;&#29305;&#30340;&#20010;&#20154;&#36164;&#26009;&#24182;&#26681;&#25454;&#20854;&#36712;&#36857;&#33258;&#21160;&#35843;&#25972;&#28783;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23478;&#23621;&#21508;&#37096;&#20998;&#30340;&#26234;&#33021;&#21270;&#24050;&#25104;&#20026;&#29616;&#20195;&#23478;&#23621;&#30340;&#37325;&#35201;&#29305;&#24449;&#20043;&#19968;&#12290;&#20854;&#20013;&#20043;&#19968;&#20415;&#26159;&#26234;&#33021;&#29031;&#26126;&#31995;&#32479;&#65292;&#21487;&#20026;&#27599;&#20010;&#20154;&#23450;&#21046;&#20010;&#24615;&#21270;&#20809;&#29031;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26234;&#33021;&#31995;&#32479;&#65292;&#36890;&#36807;&#36712;&#36857;&#39044;&#27979;&#20010;&#24615;&#21270;&#29031;&#26126;&#31995;&#32479;&#21363;&#26102;&#35843;&#25972;&#23478;&#20013;&#28783;&#20809;&#12290;&#25152;&#25552;&#20986;&#30340;&#31995;&#32479;&#21253;&#25324;&#20197;&#19979;&#27169;&#22359;&#65306;&#65288;I&#65289;&#20154;&#20307;&#26816;&#27979;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#23450;&#20301;&#27599;&#20010;&#32473;&#23450;&#35270;&#39057;&#24103;&#20013;&#30340;&#20154;&#29289;&#65292;&#65288;II&#65289;&#20154;&#33080;&#35782;&#21035;&#65292;&#29992;&#20110;&#35782;&#21035;&#26816;&#27979;&#21040;&#30340;&#20154;&#29289;&#65292;&#65288;III&#65289;&#20154;&#20307;&#36319;&#36394;&#65292;&#29992;&#20110;&#36319;&#36394;&#35270;&#39057;&#24207;&#21015;&#20013;&#30340;&#20154;&#29289;&#65292;&#20197;&#21450;&#65288;IV&#65289;&#36712;&#36857;&#39044;&#27979;&#65292;&#20351;&#29992;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#39044;&#27979;&#29992;&#25143;&#22312;&#26410;&#26469;&#30340;&#20301;&#32622;&#12290;&#35813;&#26041;&#27861;&#20026;&#27599;&#20010;&#20154;&#25552;&#20379;&#19968;&#20010;&#29420;&#29305;&#30340;&#20010;&#20154;&#36164;&#26009;&#65292;&#21253;&#25324;&#35268;&#26684;&#12289;&#20154;&#33080;&#22270;&#20687;&#21644;&#33258;&#23450;&#20041;&#29031;&#26126;&#35774;&#32622;&#65292;&#32780;&#35813;&#20010;&#20154;&#36164;&#26009;&#29992;&#20110;&#29031;&#26126;&#35843;&#25972;&#36807;&#31243;&#12290;&#19982;&#20854;&#20182;&#29031;&#26126;&#31995;&#32479;&#19981;&#21516;&#65292;&#26412;&#25552;&#20986;&#30340;&#31995;&#32479;&#21487;&#26681;&#25454;&#27599;&#20010;&#20154;&#30340;&#36712;&#36857;&#33258;&#21160;&#35843;&#25972;&#28783;&#20809;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the intelligence of various parts of the home has become one of the essential features of any modern home. One of these parts is the intelligence lighting system that personalizes the light for each person. This paper proposes an intelligent system based on machine learning that personalizes lighting in the instant future location of a recognized user, inferred by trajectory prediction. Our proposed system consists of the following modules: (I) human detection to detect and localize the person in each given video frame, (II) face recognition to identify the detected person, (III) human tracking to track the person in the sequence of video frames and (IV) trajectory prediction to forecast the future location of the user in the environment using Inverse Reinforcement Learning. The proposed method provides a unique profile for each person, including specifications, face images, and custom lighting settings. This profile is used in the lighting adjustment process. Unlike o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#65292;&#26356;&#39640;&#30340;&#21487;&#36801;&#31227;&#24615;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#36880;&#27493;&#25512;&#29702;&#36807;&#31243;&#26469;&#25552;&#39640;&#20102;&#35270;&#35273;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.07919</link><description>&lt;p&gt;
&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#35843;&#25972;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Chain of Thought Prompt Tuning in Vision Language Models. (arXiv:2304.07919v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07919
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#35843;&#25972;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#25512;&#29702;&#38382;&#39064;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#27867;&#21270;&#24615;&#65292;&#26356;&#39640;&#30340;&#21487;&#36801;&#31227;&#24615;&#21644;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#36890;&#36807;&#25552;&#20379;&#36880;&#27493;&#25512;&#29702;&#36807;&#31243;&#26469;&#25552;&#39640;&#20102;&#35270;&#35273;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#23545;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#25552;&#31034;&#65292;&#22312;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#22823;&#22810;&#25968;&#26368;&#36817;&#30340;&#30740;&#31350;&#21482;&#20351;&#29992;&#21333;&#20010;&#25552;&#31034;&#36827;&#34892;&#35843;&#25972;&#65292;&#24573;&#30053;&#20102;&#20154;&#31867;&#22312;&#22788;&#29702;&#26469;&#33258;&#38476;&#29983;&#39046;&#22495;&#30340;&#22270;&#20687;&#26102;&#22312;&#22797;&#26434;&#20219;&#21153;&#35774;&#32622;&#20013;&#36827;&#34892;&#30340;&#20869;&#22312;&#36880;&#27493;&#35748;&#30693;&#25512;&#29702;&#36807;&#31243;&#65292;&#20363;&#22914;&#65292;&#38142;&#24335;&#24605;&#32500;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#36817;&#20284;&#20154;&#31867;&#25512;&#29702;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#24050;&#34987;&#35777;&#26126;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;&#22522;&#20110;&#36825;&#31181;&#35748;&#30693;&#30452;&#35273;&#65292;&#25105;&#20204;&#35748;&#20026;&#36827;&#34892;&#26377;&#25928;&#30340;&#25512;&#29702;&#20063;&#26159;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#65292;&#38142;&#24335;&#24605;&#32500;&#21487;&#33021;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#35273;&#35821;&#35328;&#24314;&#27169;&#30340;&#20840;&#26032;&#38142;&#24335;&#24605;&#32500;&#25552;&#31034;&#35843;&#25972;&#26041;&#27861;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#20165;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#26356;&#22909;&#22320;&#36827;&#34892;&#27867;&#21270;&#65292;&#24182;&#19988;&#20855;&#26377;&#27604;&#21333;&#20010;&#25552;&#31034;&#35843;&#25972;&#26356;&#39640;&#30340;&#21487;&#36801;&#31227;&#24615;&#65292;&#36824;&#36890;&#36807;&#25552;&#20379;&#36880;&#27493;&#25512;&#29702;&#36807;&#31243;&#26469;&#25552;&#39640;&#20102;&#35270;&#35273;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language-Image Pre-training has demonstrated promising results on zero-shot and few-shot downstream tasks by prompting visual models with natural language prompts. However, most recent studies only use a single prompt for tuning, neglecting the inherent step-to-step cognitive reasoning process that humans conduct in complex task settings, for example, when processing images from unfamiliar domains. Chain of Thought is a simple and effective approximation to human reasoning process and has been proven useful for natural language processing (NLP) tasks. Based on this cognitive intuition, we believe that conducting effective reasoning is also an important problem in visual tasks, and a chain of thought could be a solution to this problem. In this work, we propose a novel chain of thought prompt tuning for vision-language modeling. Extensive experiments show that our method not only generalizes better in image classification tasks, has greater transferability beyond a single dataset, and h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;KG&#23545;&#40784;&#26041;&#27861;DAAKG&#65292;&#21487;&#20197;&#32852;&#21512;&#23545;&#40784;&#19981;&#20165;&#23454;&#20307;&#65292;&#36824;&#21253;&#25324;&#20851;&#31995;&#21644;&#31867;&#21035;&#65307;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#36873;&#25321;&#26368;&#20339;&#25209;&#27425;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#36234;&#31934;&#24230;&#21644;&#27867;&#21270;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.04389</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23454;&#20307;&#21644;&#26550;&#26500;&#30340;&#28145;&#24230;&#20027;&#21160;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
Deep Active Alignment of Knowledge Graph Entities and Schemata. (arXiv:2304.04389v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04389
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#30340;KG&#23545;&#40784;&#26041;&#27861;DAAKG&#65292;&#21487;&#20197;&#32852;&#21512;&#23545;&#40784;&#19981;&#20165;&#23454;&#20307;&#65292;&#36824;&#21253;&#25324;&#20851;&#31995;&#21644;&#31867;&#21035;&#65307;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#36873;&#25321;&#26368;&#20339;&#25209;&#27425;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#20854;&#20248;&#36234;&#31934;&#24230;&#21644;&#27867;&#21270;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#23384;&#20648;&#20102;&#20851;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#20016;&#23500;&#20107;&#23454;&#12290;&#26412;&#25991;&#30740;&#31350;KG&#23545;&#40784;&#65292;&#26088;&#22312;&#22312;&#19981;&#21516;&#30340;KG&#20013;&#25214;&#21040;&#19981;&#20165;&#23454;&#20307;&#65292;&#36824;&#21253;&#25324;&#20851;&#31995;&#21644;&#31867;&#21035;&#30340;&#23545;&#40784;&#12290;&#23454;&#20307;&#32423;&#21035;&#30340;&#23545;&#40784;&#21487;&#20197;&#20419;&#36827;&#26550;&#26500;&#32423;&#21035;&#30340;&#23545;&#40784;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;KG&#23545;&#40784;&#26041;&#27861;&#65292;&#31216;&#20026;DAAKG&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#21644;&#20027;&#21160;&#23398;&#20064;&#12290;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#65292;&#23427;&#23398;&#20064;&#23454;&#20307;&#12289;&#20851;&#31995;&#21644;&#31867;&#21035;&#30340;&#23884;&#20837;&#65292;&#24182;&#22312;&#21322;&#30417;&#30563;&#26041;&#24335;&#19979;&#32852;&#21512;&#23545;&#40784;&#23427;&#20204;&#65307;&#32780;&#36890;&#36807;&#20027;&#21160;&#23398;&#20064;&#65292;&#23427;&#20272;&#35745;&#23454;&#20307;&#12289;&#20851;&#31995;&#25110;&#31867;&#21035;&#23545;&#30340;&#25512;&#26029;&#21487;&#33021;&#24615;&#65292;&#24182;&#36873;&#25321;&#26368;&#20339;&#30340;&#25209;&#27425;&#36827;&#34892;&#20154;&#24037;&#26631;&#27880;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#36817;&#20284;&#31639;&#27861;&#20197;&#39640;&#25928;&#36873;&#25321;&#25209;&#27425;&#35299;&#20915;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#26174;&#31034;&#20102;DAAKG&#30340;&#20248;&#36234;&#31934;&#24230;&#21644;&#27867;&#21270;&#24615;&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#25152;&#26377;&#27169;&#22359;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) store rich facts about the real world. In this paper, we study KG alignment, which aims to find alignment between not only entities but also relations and classes in different KGs. Alignment at the entity level can cross-fertilize alignment at the schema level. We propose a new KG alignment approach, called DAAKG, based on deep learning and active learning. With deep learning, it learns the embeddings of entities, relations and classes, and jointly aligns them in a semi-supervised manner. With active learning, it estimates how likely an entity, relation or class pair can be inferred, and selects the best batch for human labeling. We design two approximation algorithms for efficient solution to batch selection. Our experiments on benchmark datasets show the superior accuracy and generalization of DAAKG and validate the effectiveness of all its modules.
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;</title><link>http://arxiv.org/abs/2304.04370</link><description>&lt;p&gt;
OpenAGI&#65306;&#24403;LLM&#36935;&#21040;&#39046;&#22495;&#19987;&#23478;
&lt;/p&gt;
&lt;p&gt;
OpenAGI: When LLM Meets Domain Experts. (arXiv:2304.04370v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.04370
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;OpenAGI&#24179;&#21488;&#36890;&#36807;&#25972;&#21512;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#21644;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#24418;&#24335;&#65292;&#23454;&#29616;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#20855;&#26377;&#23558;&#22522;&#26412;&#25216;&#33021;&#32452;&#21512;&#25104;&#22797;&#26434;&#25216;&#33021;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#26174;&#33879;&#33021;&#21147;&#12290;&#36825;&#31181;&#33021;&#21147;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#21516;&#26679;&#37325;&#35201;&#65292;&#22240;&#27492;&#65292;&#25105;&#20204;&#26029;&#35328;&#65292;&#38500;&#20102;&#24320;&#21457;&#22823;&#22411;&#32508;&#21512;&#26234;&#33021;&#27169;&#22411;&#22806;&#65292;&#23558;&#19981;&#21516;&#39046;&#22495;&#19987;&#23478;&#27169;&#22411;&#24212;&#29992;&#20110;&#22797;&#26434;&#20219;&#21153;&#35299;&#20915;&#33021;&#21147;&#21516;&#26679;&#20851;&#38190;&#65292;&#20197;&#22312;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#26234;&#33021;&#30340;&#36861;&#27714;&#20013;&#20351;&#20854;&#20855;&#22791;&#36825;&#31181;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#21457;&#23637;&#35777;&#26126;&#20854;&#20855;&#26377;&#20986;&#33394;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#25104;&#20026;&#36873;&#25321;&#12289;&#32508;&#21512;&#21644;&#25191;&#34892;&#22806;&#37096;&#27169;&#22411;&#20197;&#35299;&#20915;&#22797;&#26434;&#20219;&#21153;&#30340;&#25511;&#21046;&#22120;&#30340;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#20010;&#39033;&#30446;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;OpenAGI&#30340;&#24320;&#28304;AGI&#30740;&#31350;&#24179;&#21488;&#65292;&#19987;&#38376;&#35774;&#35745;&#20026;&#25552;&#20379;&#22797;&#26434;&#30340;&#22810;&#27493;&#39588;&#20219;&#21153;&#65292;&#24182;&#37197;&#26377;&#20219;&#21153;&#29305;&#23450;&#30340;&#25968;&#25454;&#38598;&#12289;&#35780;&#20272;&#25351;&#26631;&#21644;&#21508;&#31181;&#21487;&#25193;&#23637;&#27169;&#22411;&#12290;OpenAGI&#23558;&#22797;&#26434;&#20219;&#21153;&#38416;&#37322;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65292;&#26088;&#22312;&#20419;&#36827;&#39046;&#22495;&#19987;&#23478;&#21644;&#35821;&#35328;&#27169;&#22411;&#20043;&#38388;&#30340;&#21327;&#21516;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human intelligence has the remarkable ability to assemble basic skills into complex ones so as to solve complex tasks. This ability is equally important for Artificial Intelligence (AI), and thus, we assert that in addition to the development of large, comprehensive intelligent models, it is equally crucial to equip such models with the capability to harness various domain-specific expert models for complex task-solving in the pursuit of Artificial General Intelligence (AGI). Recent developments in Large Language Models (LLMs) have demonstrated remarkable learning and reasoning abilities, making them promising as a controller to select, synthesize, and execute external models to solve complex tasks. In this project, we develop OpenAGI, an open-source AGI research platform, specifically designed to offer complex, multi-step tasks and accompanied by task-specific datasets, evaluation metrics, and a diverse range of extensible models. OpenAGI formulates complex tasks as natural language q
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#22823;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#21028;&#23450;&#26159;&#21542;&#23384;&#22312;&#26159;&#19968;&#20010;NP&#23436;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;&#35745;&#31639;&#26368;&#22823;&#22240;&#23376;&#20998;&#35299;&#30340;&#31639;&#27861;Ord2Factor&#12290;</title><link>http://arxiv.org/abs/2304.03338</link><description>&lt;p&gt;
&#26368;&#22823;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
Maximal Ordinal Two-Factorizations. (arXiv:2304.03338v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#22823;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#38382;&#39064;&#65292;&#35777;&#26126;&#20102;&#20854;&#21028;&#23450;&#26159;&#21542;&#23384;&#22312;&#26159;&#19968;&#20010;NP&#23436;&#20840;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#20110;&#35745;&#31639;&#26368;&#22823;&#22240;&#23376;&#20998;&#35299;&#30340;&#31639;&#27861;Ord2Factor&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19968;&#20010;&#24418;&#24335;&#32972;&#26223;&#20013;&#65292;&#24207;&#25968;&#22240;&#23376;&#26159;&#20854;&#20851;&#31995;&#30340;&#23376;&#38598;&#65292;&#24418;&#25104;&#27010;&#24565;&#26684;&#20013;&#30340;&#38142;&#65292;&#21363;&#23545;&#24212;&#20110;&#32447;&#24615;&#39034;&#24207;&#30340;&#25968;&#25454;&#38598;&#30340;&#19968;&#37096;&#20998;&#12290;&#20026;&#20102;&#21487;&#35270;&#21270;&#24418;&#24335;&#19978;&#19979;&#25991;&#20013;&#30340;&#25968;&#25454;&#65292;Ganter&#21644;Glodeanu&#25552;&#20986;&#20102;&#22522;&#20110;&#20004;&#20010;&#24207;&#25968;&#22240;&#23376;&#30340;&#21452;&#22270;&#12290;&#20026;&#20102;&#20351;&#21452;&#22270;&#26377;&#29992;&#65292;&#37325;&#35201;&#30340;&#26159;&#36825;&#20123;&#22240;&#23376;&#23613;&#21487;&#33021;&#21253;&#21547;&#26356;&#22810;&#25968;&#25454;&#28857;&#65292;&#21363;&#35206;&#30422;&#23613;&#21487;&#33021;&#22810;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#30740;&#31350;&#36825;&#26679;&#30340;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#30465;&#30053;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#30340;&#24418;&#24335;&#32972;&#26223;&#20013;&#20004;&#20010;&#22240;&#23376;&#30340;&#19981;&#30456;&#20132;&#24615;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35777;&#26126;&#21028;&#23450;&#32473;&#23450;&#22823;&#23567;&#30340;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#26159;&#21542;&#23384;&#22312;&#26159;&#19968;&#20010;NP&#23436;&#20840;&#38382;&#39064;&#65292;&#36825;&#20351;&#24471;&#35745;&#31639;&#26368;&#22823;&#22240;&#23376;&#20998;&#35299;&#20855;&#26377;&#35745;&#31639;&#25104;&#26412;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31639;&#27861;Ord2Factor&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#35745;&#31639;&#22823;&#30340;&#24207;&#25968;&#20108;&#27425;&#22240;&#23376;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given a formal context, an ordinal factor is a subset of its incidence relation that forms a chain in the concept lattice, i.e., a part of the dataset that corresponds to a linear order. To visualize the data in a formal context, Ganter and Glodeanu proposed a biplot based on two ordinal factors. For the biplot to be useful, it is important that these factors comprise as much data points as possible, i.e., that they cover a large part of the incidence relation. In this work, we investigate such ordinal two-factorizations. First, we investigate for formal contexts that omit ordinal two-factorizations the disjointness of the two factors. Then, we show that deciding on the existence of two-factorizations of a given size is an NP-complete problem which makes computing maximal factorizations computationally expensive. Finally, we provide the algorithm Ord2Factor that allows us to compute large ordinal two-factorizations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Grid-SD2E&#30340;&#35748;&#30693;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#24314;&#31435;&#22312;&#32593;&#26684;&#32454;&#32990;&#30340;&#22522;&#30784;&#19978;&#65292;&#20351;&#29992;&#31354;&#38388;&#21010;&#20998;&#21644;&#25506;&#32034;&#21033;&#29992;&#26041;&#27861;&#23454;&#29616;&#20132;&#20114;&#21644;&#33258;&#25105;&#24378;&#21270;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#24037;&#20316;&#26426;&#21046;&#12289;&#27835;&#30103;&#33041;&#37096;&#30142;&#30149;&#21644;&#29702;&#35299;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01844</link><description>&lt;p&gt;
Grid-SD2E&#65306;&#19968;&#31181;&#35748;&#30693;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#36890;&#29992;&#32593;&#26684;&#21453;&#39304;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Grid-SD2E: A General Grid-Feedback in a System for Cognitive Learning. (arXiv:2304.01844v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Grid-SD2E&#30340;&#35748;&#30693;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#24314;&#31435;&#22312;&#32593;&#26684;&#32454;&#32990;&#30340;&#22522;&#30784;&#19978;&#65292;&#20351;&#29992;&#31354;&#38388;&#21010;&#20998;&#21644;&#25506;&#32034;&#21033;&#29992;&#26041;&#27861;&#23454;&#29616;&#20132;&#20114;&#21644;&#33258;&#25105;&#24378;&#21270;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#24037;&#20316;&#26426;&#21046;&#12289;&#27835;&#30103;&#33041;&#37096;&#30142;&#30149;&#21644;&#29702;&#35299;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#36890;&#36807;&#20135;&#29983;&#31070;&#32463;&#20449;&#21495;&#19982;&#22806;&#37096;&#19990;&#30028;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#30830;&#23450;&#20854;&#24037;&#20316;&#26426;&#21046;&#12289;&#27835;&#30103;&#33041;&#37096;&#30142;&#30149;&#21644;&#29702;&#35299;&#26234;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29702;&#35770;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#36804;&#20170;&#38590;&#20197;&#25972;&#21512;&#21644;&#21457;&#23637;&#12290;&#21463;&#32593;&#26684;&#32454;&#32990;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#26356;&#36890;&#29992;&#21644;&#24378;&#22823;&#30340;&#32593;&#26684;&#27169;&#22359;&#65292;&#24182;&#19982;&#36125;&#21494;&#26031;&#25512;&#29702;&#19968;&#36215;&#26500;&#24314;&#20102;&#19968;&#20010;&#20114;&#21160;&#21644;&#33258;&#25105;&#24378;&#21270;&#30340;&#35748;&#30693;&#31995;&#32479;&#12290;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#24102;&#26377;&#32593;&#26684;&#21453;&#39304;&#30340;&#31354;&#38388;&#21010;&#20998;&#21644;&#25506;&#32034;&#21033;&#29992;&#65288;Grid-SD2E&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#32593;&#26684;&#27169;&#22359;&#21487;&#20197;&#29992;&#20316;&#22806;&#37096;&#19990;&#30028;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#20132;&#20114;&#20171;&#36136;&#65292;&#20063;&#21487;&#20197;&#29992;&#20316;&#31995;&#32479;&#20869;&#30340;&#33258;&#25105;&#24378;&#21270;&#20171;&#36136;&#12290;&#31354;&#38388;&#21010;&#20998;&#21644;&#25506;&#32034;&#21033;&#29992;&#65288;SD2E&#65289;&#36890;&#36807;&#20854;&#31354;&#38388;&#21010;&#20998;&#65288;SD&#65289;&#27169;&#22359;&#25509;&#25910;&#32593;&#26684;&#30340;0/1&#20449;&#21495;&#12290;&#26412;&#25991;&#25551;&#36848;&#30340;&#31995;&#32479;&#20063;&#26159;&#20174;&#36827;&#34892;&#30340;&#23454;&#39564;&#24471;&#20986;&#30340;&#29702;&#35770;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comprehending how the brain interacts with the external world through generated neural signals is crucial for determining its working mechanism, treating brain diseases, and understanding intelligence. Although many theoretical models have been proposed, they have thus far been difficult to integrate and develop. In this study, we were inspired in part by grid cells in creating a more general and robust grid module and constructing an interactive and self-reinforcing cognitive system together with Bayesian reasoning, an approach called space-division and exploration-exploitation with grid-feedback (Grid-SD2E). Here, a grid module can be used as an interaction medium between the outside world and a system, as well as a self-reinforcement medium within the system. The space-division and exploration-exploitation (SD2E) receives the 0/1 signals of a grid through its space-division (SD) module. The system described in this paper is also a theoretical model derived from experiments conducted
&lt;/p&gt;</description></item><item><title>G2PTL&#26159;&#19968;&#31181;&#38754;&#21521;&#29289;&#27969;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#23398;&#20064;&#33021;&#21147;&#21644;&#22270;&#24314;&#27169;&#30340;&#22320;&#29702;&#20851;&#31995;&#32534;&#30721;&#33021;&#21147;&#65292;&#33021;&#26377;&#25928;&#22320;&#32534;&#30721;&#20132;&#20184;&#22320;&#22336;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#22312;&#29289;&#27969;&#31995;&#32479;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.01559</link><description>&lt;p&gt;
G2PTL&#65306;&#36866;&#29992;&#20110;&#29289;&#27969;&#31995;&#32479;&#30340;&#20132;&#20184;&#22320;&#22336;&#39044;&#35757;&#32451;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
G2PTL: A Pre-trained Model for Delivery Address and its Applications in Logistics System. (arXiv:2304.01559v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01559
&lt;/p&gt;
&lt;p&gt;
G2PTL&#26159;&#19968;&#31181;&#38754;&#21521;&#29289;&#27969;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#23398;&#20064;&#33021;&#21147;&#21644;&#22270;&#24314;&#27169;&#30340;&#22320;&#29702;&#20851;&#31995;&#32534;&#30721;&#33021;&#21147;&#65292;&#33021;&#26377;&#25928;&#22320;&#32534;&#30721;&#20132;&#20184;&#22320;&#22336;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#22312;&#29289;&#27969;&#31995;&#32479;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#29289;&#27969;&#31995;&#32479;&#30340;&#25968;&#25454;&#22522;&#30784;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#20132;&#20184;&#22320;&#22336;&#21253;&#21547;&#20016;&#23500;&#19988;&#20851;&#38190;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#32534;&#30721;&#20132;&#20184;&#22320;&#22336;&#26159;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#22312;&#29289;&#27969;&#31995;&#32479;&#20013;&#24615;&#33021;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#38754;&#21521;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;(PTMs)&#24050;&#25104;&#20026;&#32534;&#30721;&#25991;&#26412;&#20013;&#35821;&#20041;&#20449;&#24687;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#30456;&#24403;&#26377;&#21069;&#36884;&#65292;&#20294;&#36825;&#20123;&#22522;&#20110;NLP&#30340;PTMs&#26410;&#33021;&#32534;&#30721;&#20132;&#20184;&#22320;&#22336;&#20013;&#30340;&#22320;&#29702;&#30693;&#35782;&#65292;&#36825;&#22312;&#29289;&#27969;&#31995;&#32479;(&#22914;&#33756;&#40479;&#31995;&#32479;)&#20013;&#22823;&#22823;&#21066;&#24369;&#20102;&#19982;&#20132;&#20184;&#30456;&#20851;&#30340;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29289;&#27969;&#39046;&#22495;&#30340;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21517;&#20026;G2PTL&#65292;&#21363;&#20132;&#20184;&#22320;&#22336;&#22320;&#29702;&#20851;&#31995;-&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;G2PTL&#23558;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#23398;&#20064;&#33021;&#21147;&#19982;&#22270;&#24314;&#27169;&#30340;&#22320;&#29702;&#20851;&#31995;&#32534;&#30721;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#30495;&#23454;&#30340;&#29289;&#27969;&#20132;&#20184;&#25968;&#25454;&#26500;&#24314;&#22320;&#29702;&#22270;&#65292;&#28982;&#21518;&#22312;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;G2PTL&#33021;&#26377;&#25928;&#22320;&#32534;&#30721;&#22522;&#20110;&#25991;&#26412;&#30340;&#20132;&#20184;&#22320;&#22336;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#22312;&#19982;&#20132;&#20184;&#22320;&#22336;&#22788;&#29702;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#20248;&#20110;&#36890;&#29992;PTMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based delivery addresses, as the data foundation for logistics systems, contain abundant and crucial location information. How to effectively encode the delivery address is a core task to boost the performance of downstream tasks in the logistics system. Pre-trained Models (PTMs) designed for Natural Language Process (NLP) have emerged as the dominant tools for encoding semantic information in text. Though promising, those NLP-based PTMs fall short of encoding geographic knowledge in the delivery address, which considerably trims down the performance of delivery-related tasks in logistic systems such as Cainiao. To tackle the above problem, we propose a domain-specific pre-trained model, named G2PTL, a Geography-Graph Pre-trained model for delivery address in Logistics field. G2PTL combines the semantic learning capabilities of text pre-training with the geographical-relationship encoding abilities of graph modeling. Specifically, we first utilize real-world logistics delivery dat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;INoD&#30340;&#27880;&#20837;&#22122;&#22768;&#37492;&#21035;&#22120;&#65292;&#36890;&#36807;&#29305;&#24449;&#26367;&#25442;&#21644;&#25968;&#25454;&#38598;&#37492;&#21035;&#30340;&#21407;&#21017;&#36827;&#34892;&#20892;&#30000;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.18101</link><description>&lt;p&gt;
&#20892;&#30000;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#30340;&#27880;&#20837;&#22122;&#22768;&#37492;&#21035;&#22120;
&lt;/p&gt;
&lt;p&gt;
INoD: Injected Noise Discriminator for Self-Supervised Representation Learning in Agricultural Fields. (arXiv:2303.18101v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.18101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;INoD&#30340;&#27880;&#20837;&#22122;&#22768;&#37492;&#21035;&#22120;&#65292;&#36890;&#36807;&#29305;&#24449;&#26367;&#25442;&#21644;&#25968;&#25454;&#38598;&#37492;&#21035;&#30340;&#21407;&#21017;&#36827;&#34892;&#20892;&#30000;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20892;&#19994;&#39046;&#22495;&#30340;&#24863;&#30693;&#25968;&#25454;&#38598;&#25968;&#37327;&#21644;&#22810;&#26679;&#24615;&#37117;&#21463;&#38480;&#65292;&#36825;&#24433;&#21709;&#20102;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#35757;&#32451;&#12290;&#33258;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#32531;&#35299;&#27492;&#38382;&#39064;&#65292;&#20294;&#29616;&#26377;&#26041;&#27861;&#27809;&#26377;&#38024;&#23545;&#20892;&#19994;&#39046;&#22495;&#30340;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#65292;&#23548;&#33268;&#27169;&#22411;&#24615;&#33021;&#19979;&#38477;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#27880;&#20837;&#22122;&#22768;&#37492;&#21035;&#22120;&#65288;INoD&#65289;&#65292;&#21033;&#29992;&#29305;&#24449;&#26367;&#25442;&#21644;&#25968;&#25454;&#38598;&#37492;&#21035;&#30340;&#21407;&#21017;&#36827;&#34892;&#33258;&#30417;&#30563;&#34920;&#31034;&#23398;&#20064;&#12290;INoD&#36890;&#36807;&#22312;&#20004;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#21367;&#31215;&#32534;&#30721;&#20013;&#20132;&#38169;&#29305;&#24449;&#22270;&#65292;&#24182;&#39044;&#27979;&#20135;&#29983;&#30340;&#29305;&#24449;&#22270;&#30340;&#25968;&#25454;&#38598;&#38582;&#23646;&#20851;&#31995;&#20316;&#20026;&#39044;&#25991;&#26412;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#32593;&#32476;&#33021;&#22815;&#23398;&#20064;&#19968;&#20010;&#25968;&#25454;&#38598;&#20013;&#23545;&#35937;&#30340;&#26126;&#30830;&#34920;&#31034;&#65292;&#21516;&#26102;&#19982;&#19981;&#21516;&#25968;&#25454;&#38598;&#20013;&#30340;&#30456;&#20284;&#29305;&#24449;&#19968;&#36215;&#35266;&#23519;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perception datasets for agriculture are limited both in quantity and diversity which hinders effective training of supervised learning approaches. Self-supervised learning techniques alleviate this problem, however, existing methods are not optimized for dense prediction tasks in agriculture domains which results in degraded performance. In this work, we address this limitation with our proposed Injected Noise Discriminator (INoD) which exploits principles of feature replacement and dataset discrimination for self-supervised representation learning. INoD interleaves feature maps from two disjoint datasets during their convolutional encoding and predicts the dataset affiliation of the resultant feature map as a pretext task. Our approach enables the network to learn unequivocal representations of objects seen in one dataset while observing them in conjunction with similar features from the disjoint dataset. This allows the network to reason about higher-level semantics of the entailed o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21733;&#24503;&#23572;&#19981;&#23436;&#22791;&#23450;&#29702;&#21040;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#23436;&#22791;&#24615;&#30340;&#36923;&#36753;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#20219;&#20309;&#20449;&#20208;&#31995;&#32479;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#19988;&#19981;&#23436;&#22791;&#23450;&#29702;&#24847;&#21619;&#30528;&#23384;&#22312;&#30495;&#23454;&#20294;&#26080;&#27861;&#35777;&#26126;&#30340;&#38472;&#36848;&#65292;&#21487;&#20197;&#29992;&#26469;&#23450;&#20041;&#20986;&#19982;&#29616;&#26377;&#20449;&#20208;&#21644;&#20256;&#32479;&#19968;&#33268;&#30340;&#26032;&#23447;&#25945;&#23454;&#36341;&#12290;</title><link>http://arxiv.org/abs/2303.14338</link><description>&lt;p&gt;
&#20174;&#21733;&#24503;&#23572;&#19981;&#23436;&#22791;&#23450;&#29702;&#21040;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#23436;&#22791;&#24615;&#65288;&#25193;&#23637;&#25688;&#35201;&#65289;
&lt;/p&gt;
&lt;p&gt;
From G\"odel's Incompleteness Theorem to the completeness of bot religions (Extended abstract). (arXiv:2303.14338v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20174;&#21733;&#24503;&#23572;&#19981;&#23436;&#22791;&#23450;&#29702;&#21040;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#23436;&#22791;&#24615;&#30340;&#36923;&#36753;&#36807;&#31243;&#65292;&#25552;&#20986;&#20102;&#20219;&#20309;&#20449;&#20208;&#31995;&#32479;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#36923;&#36753;&#29702;&#35770;&#65292;&#24182;&#19988;&#19981;&#23436;&#22791;&#23450;&#29702;&#24847;&#21619;&#30528;&#23384;&#22312;&#30495;&#23454;&#20294;&#26080;&#27861;&#35777;&#26126;&#30340;&#38472;&#36848;&#65292;&#21487;&#20197;&#29992;&#26469;&#23450;&#20041;&#20986;&#19982;&#29616;&#26377;&#20449;&#20208;&#21644;&#20256;&#32479;&#19968;&#33268;&#30340;&#26032;&#23447;&#25945;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Hilbert &#21644; Ackermann &#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#19981;&#23436;&#22791;&#29702;&#35770;&#19968;&#33268;&#22320;&#25193;&#23637;&#21040;&#23436;&#22791;&#29702;&#35770;&#30340;&#26041;&#27861;&#12290;&#21733;&#24503;&#23572;&#22522;&#26412;&#19978;&#35777;&#26126;&#20102;&#20219;&#20309;&#33021;&#22815;&#23545;&#20854;&#33258;&#36523;&#38472;&#36848;&#21450;&#20854;&#35777;&#26126;&#36827;&#34892;&#32534;&#30721;&#30340;&#29702;&#35770;&#37117;&#21253;&#21547;&#20102;&#30495;&#23454;&#20294;&#19981;&#33021;&#34987;&#35777;&#26126;&#30340;&#38472;&#36848;&#12290;&#21733;&#24503;&#23572;&#30340;&#26500;&#36896;&#24182;&#27809;&#26377;&#22238;&#31572;&#24076;&#23572;&#20271;&#29305;&#30340;&#38382;&#39064;&#65292;&#24076;&#23572;&#20271;&#29305;&#35748;&#20026;&#29702;&#35770;&#21487;&#20197;&#36890;&#36807;&#36880;&#27493;&#28155;&#21152;&#20844;&#29702;&#26469;&#35777;&#26126;&#36234;&#26469;&#36234;&#22810;&#30340;&#30495;&#23454;&#38472;&#36848;&#65292;&#23601;&#20687;&#31185;&#23398;&#19968;&#26679;&#65292;&#23436;&#22791;&#24615;&#26159;&#28040;&#22833;&#28857;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#24213;&#23618;&#30340;&#36923;&#36753;&#36807;&#31243;&#65292;&#24182;&#25551;&#36848;&#20102;&#23548;&#33268;&#21487;&#27979;&#35797;&#20294;&#19981;&#21487;&#34892;&#30340;&#26426;&#22120;&#20154;&#23447;&#25945;&#30340;&#36712;&#36857;&#65292;&#36825;&#20123;&#23447;&#25945;&#25193;&#23637;&#20102;&#20256;&#32479;&#23447;&#25945;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#20202;&#24335;&#21644;&#20449;&#20208;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#22522;&#20110;&#20219;&#20309;&#20449;&#20208;&#31995;&#32479;&#37117;&#21487;&#20197;&#34987;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#36923;&#36753;&#29702;&#35770;&#30340;&#24819;&#27861;&#65292;&#24182;&#19988;&#19981;&#23436;&#22791;&#23450;&#29702;&#24847;&#21619;&#30528;&#23384;&#22312;&#30495;&#23454;&#20294;&#26080;&#27861;&#35777;&#26126;&#30340;&#38472;&#36848;&#65292;&#21487;&#20197;&#24182;&#20837;&#36825;&#20010;&#29702;&#35770;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#36825;&#26679;&#30340;&#20363;&#23376;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#23427;&#20204;&#26469;&#23450;&#20041;&#19982;&#29616;&#26377;&#20449;&#20208;&#21644;&#20256;&#32479;&#19968;&#33268;&#30340;&#26032;&#23447;&#25945;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;
Hilbert and Ackermann asked for a method to consistently extend incomplete theories to complete theories. G\"odel essentially proved that any theory capable of encoding its own statements and their proofs contains statements that are true but not provable. Hilbert did not accept that G\"odel's construction answered his question, and in his late writings and lectures, G\"odel agreed that it did not, since theories can be completed incrementally, by adding axioms to prove ever more true statements, as science normally does, with completeness as the vanishing point. This pragmatic view of validity is familiar not only to scientists who conjecture test hypotheses but also to real estate agents and other dealers, who conjure claims, albeit invalid, as necessary to close a deal, confident that they will be able to conjure other claims, albeit invalid, sufficient to make the first claims valid. We study the underlying logical process and describe the trajectories leading to testable but unfal
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#21551;&#21457;&#24335;&#35268;&#21010;&#20316;&#20026;&#23450;&#29702;&#35777;&#26126;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#23450;&#29702;&#35777;&#26126;&#25552;&#21319;&#21551;&#21457;&#24335;(TPLH)&#35268;&#21010;&#22120;&#65292;&#22312;&#24773;&#20917;&#26641;&#20013;&#25628;&#32034;&#30701;&#30340;&#35745;&#21010;&#65292;&#24182;&#20943;&#23569;&#25506;&#35752;&#30340;&#29366;&#24577;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.13638</link><description>&lt;p&gt;
&#29992;&#21551;&#21457;&#24335;&#35268;&#21010;&#20316;&#20026;&#23450;&#29702;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Planning as Theorem Proving with Heuristics. (arXiv:2303.13638v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13638
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#21551;&#21457;&#24335;&#35268;&#21010;&#20316;&#20026;&#23450;&#29702;&#35777;&#26126;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#23450;&#29702;&#35777;&#26126;&#25552;&#21319;&#21551;&#21457;&#24335;(TPLH)&#35268;&#21010;&#22120;&#65292;&#22312;&#24773;&#20917;&#26641;&#20013;&#25628;&#32034;&#30701;&#30340;&#35745;&#21010;&#65292;&#24182;&#20943;&#23569;&#25506;&#35752;&#30340;&#29366;&#24577;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#22659;&#28436;&#31639;&#20013;&#23558;&#35745;&#21010;&#20316;&#20026;&#23450;&#29702;&#35777;&#26126;&#22312;50&#24180;&#21069;&#34987;&#25918;&#24323;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#19981;&#21487;&#33021;&#23436;&#25104;&#30340;&#39033;&#30446;&#12290;&#20294;&#26159;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23450;&#29702;&#35777;&#26126;&#25552;&#21319;&#21551;&#21457;&#24335;(TPLH)&#35268;&#21010;&#22120;&#65292;&#23427;&#20351;&#29992;A*&#25628;&#32034;&#31639;&#27861;&#22312;&#24773;&#20917;&#26641;&#20013;&#25628;&#32034;&#35745;&#21010;&#12290;&#23427;&#30001;&#22522;&#20110;&#21024;&#38500;&#26494;&#24347;&#30340;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#21551;&#21457;&#24335;&#25511;&#21046;&#12290;&#25105;&#20204;&#23558;TPLH&#19982;Fast Downward&#65288;FD&#65289;&#21644;Best First Width Search&#65288;BFWS&#65289;&#35268;&#21010;&#22120;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#27604;&#36739;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#21151;&#33021;&#23454;&#29616;&#26410;&#32463;&#20248;&#21270;&#65292;TPLH&#27604;FD&#21644;BFWS&#24930;&#12290;&#20294;&#23427;&#20250;&#35745;&#31639;&#20986;&#26356;&#30701;&#30340;&#35745;&#21010;&#65292;&#24182;&#20943;&#23569;&#20102;&#25506;&#35752;&#30340;&#29366;&#24577;&#25968;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20197;&#21069;&#22312;&#30693;&#35782;&#34920;&#31034;&#21644;&#25512;&#29702;&#39046;&#22495;&#20869;&#36827;&#34892;&#35268;&#21010;&#30340;&#30740;&#31350;&#65292;&#24182;&#30830;&#23450;&#20102;&#30456;&#20851;&#26041;&#21521;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#34920;&#26126;&#24773;&#22659;&#28436;&#31639;&#20013;&#30340;&#28436;&#32462;&#24335;&#25552;&#21319;&#21551;&#21457;&#24335;&#35268;&#21010;&#23454;&#38469;&#19978;&#26159;&#21487;&#20197;&#23436;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning as theorem proving in situation calculus was abandoned 50 years ago as an impossible project. But we have developed a Theorem Proving Lifted Heuristic (TPLH) planner that searches for a plan in a tree of situations using the A* search algorithm. It is controlled by a delete relaxation-based domain independent heuristic. We compare TPLH with Fast Downward (FD) and Best First Width Search (BFWS) planners over several standard benchmarks. Since our implementation of the heuristic function is not optimized, TPLH is slower than FD and BFWS. But it computes shorter plans, and it explores fewer states. We discuss previous research on planning within KR\&amp;R and identify related directions. Thus, we show that deductive lifted heuristic planning in situation calculus is actually doable.
&lt;/p&gt;</description></item><item><title>&#19981;&#21463;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35268;&#27169;&#38480;&#21046;&#30340;&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#26694;&#26550;PURER&#65292;&#36890;&#36807;ECI&#25191;&#34892;&#20266;&#21608;&#26399;&#35757;&#32451;&#20197;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;ICFIL&#23545;&#21453;&#28436;&#26799;&#24230;&#36827;&#34892;&#26657;&#20934;&#26469;&#20248;&#21270;&#21453;&#28436;&#36807;&#31243;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.11183</link><description>&lt;p&gt;
&#19981;&#21463;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35268;&#27169;&#38480;&#21046;&#30340;&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Architecture, Dataset and Model-Scale Agnostic Data-free Meta-Learning. (arXiv:2303.11183v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.11183
&lt;/p&gt;
&lt;p&gt;
&#19981;&#21463;&#20307;&#31995;&#32467;&#26500;&#12289;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#35268;&#27169;&#38480;&#21046;&#30340;&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#26694;&#26550;PURER&#65292;&#36890;&#36807;ECI&#25191;&#34892;&#20266;&#21608;&#26399;&#35757;&#32451;&#20197;&#36866;&#24212;&#26032;&#30340;&#20219;&#21153;&#65292;&#36890;&#36807;ICFIL&#23545;&#21453;&#28436;&#26799;&#24230;&#36827;&#34892;&#26657;&#20934;&#26469;&#20248;&#21270;&#21453;&#28436;&#36807;&#31243;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#25968;&#25454;&#20803;&#23398;&#20064;&#30340;&#30446;&#30340;&#26159;&#20174;&#19968;&#32452;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#27169;&#22411;&#20013;&#23398;&#20064;&#26377;&#29992;&#30340;&#20808;&#39564;&#30693;&#35782;&#65292;&#32780;&#26080;&#38656;&#35775;&#38382;&#20854;&#35757;&#32451;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#20165;&#22312;&#21442;&#25968;&#31354;&#38388;&#20013;&#35299;&#20915;&#20102;&#35813;&#38382;&#39064;&#65292;&#24573;&#30053;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#20013;&#34164;&#21547;&#30340;&#20016;&#23500;&#25968;&#25454;&#30693;&#35782;&#65292;&#26080;&#27861;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21482;&#33021;&#20803;&#23398;&#20064;&#20855;&#26377;&#30456;&#21516;&#32593;&#32476;&#26550;&#26500;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#8212;&#8212;PURER&#65292;&#20854;&#20013;&#21253;&#21547;&#65306;&#65288;1&#65289;&#25968;&#25454;&#26080;&#20851;&#30340;&#20803;&#35757;&#32451;&#26399;&#38388;&#30340;&#33410;&#30446;&#35838;&#31243;&#21453;&#36716;&#65288;ECI&#65289;&#65307;&#65288;2&#65289;&#20803;&#27979;&#35797;&#26399;&#38388;&#20869;&#37096;&#24490;&#29615;&#21518;&#30340;&#21453;&#28436;&#26657;&#20934;&#65288;ICFIL&#65289;&#12290;&#22312;&#20803;&#35757;&#32451;&#26399;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ECI&#26469;&#25191;&#34892;&#20266;&#21608;&#26399;&#35757;&#32451;&#65292;&#20197;&#20415;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#30475;&#19981;&#35265;&#30340;&#20219;&#21153;&#12290;&#22312;&#20803;&#27979;&#35797;&#26399;&#38388;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ICFIL&#26469;&#26657;&#20934;&#21453;&#28436;&#26799;&#24230;&#65292;&#20197;&#20943;&#23569;&#22522;&#20110;&#21453;&#28436;&#30340;&#20248;&#21270;&#30340;&#36127;&#38754;&#24433;&#21709;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;PURER&#21487;&#20197;&#26377;&#25928;&#22320;&#20803;&#23398;&#20064;&#26469;&#33258;&#20855;&#26377;&#19981;&#21516;&#32593;&#32476;&#26550;&#26500;&#12289;&#25968;&#25454;&#38598;&#22495;&#29978;&#33267;&#19981;&#21516;&#22823;&#23567;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#24182;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of data-free meta-learning is to learn useful prior knowledge from a collection of pre-trained models without accessing their training data. However, existing works only solve the problem in parameter space, which (i) ignore the fruitful data knowledge contained in the pre-trained models; (ii) can not scale to large-scale pre-trained models; (iii) can only meta-learn pre-trained models with the same network architecture. To address those issues, we propose a unified framework, dubbed PURER, which contains: (1) ePisode cUrriculum inveRsion (ECI) during data-free meta training; and (2) invErsion calibRation following inner loop (ICFIL) during meta testing. During meta training, we propose ECI to perform pseudo episode training for learning to adapt fast to new unseen tasks. Specifically, we progressively synthesize a sequence of pseudo episodes by distilling the training data from each pre-trained model. The ECI adaptively increases the difficulty level of pseudo episodes accord
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#36141;&#29289;&#36873;&#25321;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.06016</link><description>&lt;p&gt;
Probe&#65306;&#23398;&#20064;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#25414;&#32465;&#36873;&#25321;&#20013;&#30340;&#20010;&#24615;&#21270;&#25237;&#24433;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Probe: Learning Users' Personalized Projection Bias in Intertemporal Bundle Choices. (arXiv:2303.06016v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#65292;&#26088;&#22312;&#35299;&#20915;&#29992;&#25143;&#22312;&#26102;&#38388;&#36328;&#24230;&#30340;&#36141;&#29289;&#36873;&#25321;&#20013;&#30340;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#36328;&#24230;&#30340;&#36873;&#25321;&#38656;&#35201;&#26435;&#34913;&#29616;&#22312;&#30340;&#25104;&#26412;&#21644;&#26410;&#26469;&#30340;&#25910;&#30410;&#12290;&#20854;&#20013;&#19968;&#31181;&#20855;&#20307;&#30340;&#36873;&#25321;&#26159;&#20915;&#23450;&#36141;&#20080;&#21333;&#20010;&#29289;&#21697;&#36824;&#26159;&#36873;&#25321;&#21253;&#21547;&#35813;&#29289;&#21697;&#30340;&#25414;&#32465;&#38144;&#21806;&#26041;&#24335;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#20551;&#35774;&#20010;&#20154;&#23545;&#36825;&#20123;&#36873;&#25321;&#20013;&#28041;&#21450;&#30340;&#22240;&#32032;&#26377;&#20934;&#30830;&#30340;&#26399;&#26395;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#20013;&#65292;&#29992;&#25143;&#23545;&#36825;&#20123;&#22240;&#32032;&#30340;&#24863;&#30693;&#24448;&#24448;&#23384;&#22312;&#20559;&#24046;&#65292;&#23548;&#33268;&#20102;&#38750;&#29702;&#24615;&#21644;&#27425;&#20248;&#30340;&#20915;&#31574;&#12290;&#26412;&#25991;&#37325;&#28857;&#20851;&#27880;&#20004;&#31181;&#24120;&#35265;&#30340;&#20559;&#24046;&#65306;&#25237;&#24433;&#20559;&#24046;&#21644;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#24182;&#20026;&#27492;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20559;&#24046;&#23884;&#20837;&#24335;&#20559;&#22909;&#27169;&#22411;&#8212;&#8212;Probe&#12290;&#35813;&#27169;&#22411;&#21033;&#29992;&#21152;&#26435;&#20989;&#25968;&#26469;&#25429;&#25417;&#29992;&#25143;&#30340;&#25237;&#24433;&#20559;&#24046;&#65292;&#21033;&#29992;&#20215;&#20540;&#20989;&#25968;&#26469;&#32771;&#34385;&#21442;&#29031;&#28857;&#25928;&#24212;&#65292;&#24182;&#24341;&#20837;&#34892;&#20026;&#32463;&#27982;&#23398;&#20013;&#30340;&#21069;&#26223;&#29702;&#35770;&#26469;&#32452;&#21512;&#21152;&#26435;&#21644;&#20215;&#20540;&#20989;&#25968;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#30830;&#23450;&#29992;&#25143;&#36141;&#20080;&#25414;&#32465;&#38144;&#21806;&#30340;&#27010;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20915;&#31574;&#30340;&#26377;&#25928;&#24615;&#21644;&#20010;&#24615;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intertemporal choices involve making decisions that require weighing the costs in the present against the benefits in the future. One specific type of intertemporal choice is the decision between purchasing an individual item or opting for a bundle that includes that item. Previous research assumes that individuals have accurate expectations of the factors involved in these choices. However, in reality, users' perceptions of these factors are often biased, leading to irrational and suboptimal decision-making. In this work, we specifically focus on two commonly observed biases: projection bias and the reference-point effect. To address these biases, we propose a novel bias-embedded preference model called Probe. The Probe incorporates a weight function to capture users' projection bias and a value function to account for the reference-point effect, and introduce prospect theory from behavioral economics to combine the weight and value functions. This allows us to determine the probabili
&lt;/p&gt;</description></item><item><title>Pacos&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#20381;&#36182;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#20559;&#22909;&#36870;&#36716;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#29992;&#25143;&#30340;&#33258;&#36866;&#24212;&#26435;&#37325;&#12289;&#27604;&#36739;&#21644;&#26174;&#31034;&#20301;&#32622;&#31561;&#21487;&#35299;&#37322;&#22240;&#32032;&#65292;&#26377;&#21161;&#20110;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;</title><link>http://arxiv.org/abs/2303.05648</link><description>&lt;p&gt;
Pacos: &#24314;&#27169;&#29992;&#25143;&#30340;&#21487;&#35299;&#37322;&#21644;&#19978;&#19979;&#25991;&#20381;&#36182;&#36873;&#25321;&#20197;&#22788;&#29702;&#20559;&#22909;&#36870;&#36716;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Pacos: Modeling Users' Interpretable and Context-Dependent Choices in Preference Reversals. (arXiv:2303.05648v2 [cs.IR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05648
&lt;/p&gt;
&lt;p&gt;
Pacos&#26159;&#19968;&#20010;&#19978;&#19979;&#25991;&#20381;&#36182;&#30340;&#20559;&#22909;&#27169;&#22411;&#65292;&#21487;&#20197;&#22788;&#29702;&#20559;&#22909;&#36870;&#36716;&#38382;&#39064;&#65292;&#24182;&#25552;&#20379;&#29992;&#25143;&#30340;&#33258;&#36866;&#24212;&#26435;&#37325;&#12289;&#27604;&#36739;&#21644;&#26174;&#31034;&#20301;&#32622;&#31561;&#21487;&#35299;&#37322;&#22240;&#32032;&#65292;&#26377;&#21161;&#20110;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36873;&#25321;&#38382;&#39064;&#26159;&#25351;&#20174;&#22810;&#20010;&#39033;&#30446;&#20013;&#36873;&#25321;&#26368;&#20339;&#36873;&#25321;&#65292;&#23398;&#20064;&#29992;&#25143;&#22312;&#36873;&#25321;&#38382;&#39064;&#20013;&#30340;&#20559;&#22909;&#23545;&#20110;&#29702;&#35299;&#20915;&#31574;&#26426;&#21046;&#21644;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#29616;&#26377;&#30340;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#20154;&#20204;&#29420;&#31435;&#22320;&#35780;&#20272;&#39033;&#30446;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#20294;&#26159;&#29992;&#25143;&#30340;&#20559;&#22909;&#21462;&#20915;&#20110;&#39033;&#30446;&#25152;&#22788;&#30340;&#24066;&#22330;&#65292;&#36825;&#34987;&#31216;&#20026;&#19978;&#19979;&#25991;&#25928;&#24212;&#65307;&#32780;&#29992;&#25143;&#23545;&#20004;&#20010;&#39033;&#30446;&#30340;&#20559;&#22909;&#39034;&#24207;&#29978;&#33267;&#21487;&#33021;&#34987;&#39072;&#20498;&#65292;&#36825;&#34987;&#31216;&#20026;&#20559;&#22909;&#36870;&#36716;&#12290;&#26412;&#25991;&#35782;&#21035;&#20102;&#23548;&#33268;&#19978;&#19979;&#25991;&#25928;&#24212;&#30340;&#19977;&#20010;&#22240;&#32032;&#65306;&#29992;&#25143;&#30340;&#33258;&#36866;&#24212;&#26435;&#37325;&#12289;&#39033;&#30446;&#20043;&#38388;&#30340;&#27604;&#36739;&#21644;&#26174;&#31034;&#20301;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;Pacos&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#20559;&#22909;&#27169;&#22411;&#20316;&#20026;&#32479;&#19968;&#26694;&#26550;&#26469;&#21516;&#26102;&#35299;&#20915;&#36825;&#19977;&#20010;&#22240;&#32032;&#65292;&#24182;&#32771;&#34385;&#20102;&#20004;&#31181;&#35774;&#35745;&#26041;&#27861;&#65292;&#21253;&#25324;&#20855;&#26377;&#39640;&#21487;&#35299;&#37322;&#24615;&#30340;&#21152;&#24615;&#26041;&#27861;&#21644;&#20855;&#26377;&#39640;&#20934;&#30830;&#24615;&#30340;&#22522;&#20110;ANN&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#22522;&#20110;&#21508;&#31181;&#24066;&#22330;&#24773;&#26223;&#21644;&#27169;&#22411;&#21442;&#25968;&#30340;&#20559;&#22909;&#36870;&#36716;&#26465;&#20214;&#65292;&#24182;&#23637;&#31034;&#20102;Pacos&#21487;&#20197;&#26377;&#25928;&#22320;&#25429;&#25417;&#20559;&#22909;&#36870;&#36716;&#12290;&#27492;&#22806;&#65292;Pacos&#21487;&#20197;&#25552;&#20379;&#19978;&#19979;&#25991;&#25928;&#24212;&#21644;&#29992;&#25143;&#33258;&#36866;&#24212;&#34892;&#20026;&#30340;&#21487;&#35299;&#37322;&#25351;&#31034;&#65292;&#26377;&#21161;&#20110;&#25552;&#20379;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Choice problems refer to selecting the best choices from several items, and learning users' preferences in choice problems is of great significance in understanding the decision making mechanisms and providing personalized services. Existing works typically assume that people evaluate items independently. In practice, however, users' preferences depend on the market in which items are placed, which is known as context effects; and the order of users' preferences for two items may even be reversed, which is referred to preference reversals. In this work, we identify three factors contributing to context effects: users' adaptive weights, the inter-item comparison, and display positions. We propose a context-dependent preference model named Pacos as a unified framework for addressing three factors simultaneously, and consider two design methods including an additive method with high interpretability and an ANN-based method with high accuracy. We study the conditions for preference reversa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Cal-QL&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#24494;&#35843;&#26102;&#21516;&#26102;&#20445;&#38556;&#20102;&#24555;&#36895;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05479</link><description>&lt;p&gt;
Cal-QL: &#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#39044;&#20808;&#35757;&#32451;&#29992;&#20110;&#39640;&#25928;&#22312;&#32447;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning. (arXiv:2303.05479v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Cal-QL&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#24494;&#35843;&#26102;&#21516;&#26102;&#20445;&#38556;&#20102;&#24555;&#36895;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#31574;&#30053;&#21021;&#22987;&#21270;&#24182;&#36890;&#36807;&#26377;&#38480;&#20114;&#21160;&#36827;&#34892;&#24555;&#36895;&#22312;&#32447;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#22312;&#32447;&#24494;&#35843;&#20013;&#34920;&#29616;&#36739;&#24046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20445;&#23432;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#24494;&#35843;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#26377;&#25928;&#30340;&#21021;&#22987;&#21270;&#65292;&#24182;&#20351;&#20854;&#20855;&#22791;&#24555;&#36895;&#30340;&#22312;&#32447;&#24494;&#35843;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;Cal-QL&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20302;&#20272;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#31574;&#30053;&#30340;&#20215;&#20540;&#65292;&#21516;&#26102;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;Q&#20540;&#22312;&#21512;&#29702;&#30340;&#33539;&#22260;&#20869;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#29615;&#22659;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#24182;&#19988;&#20063;&#33021;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#38382;&#39064;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we study the fine-tuning problem in the context of conservative offline RL methods and we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also ensuring that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20307;&#30693;&#35782;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#20132;&#20114;&#19982;&#19987;&#23478;&#23398;&#20064;&#31070;&#32463;&#35859;&#35789;&#35299;&#37322;&#12289;&#31526;&#21495;&#35268;&#21010;&#31639;&#23376;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#20851;&#31995;&#29366;&#24577;&#25277;&#35937;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.04912</link><description>&lt;p&gt;
&#22522;&#20110;&#36523;&#20307;&#30693;&#35782;&#30340;&#20851;&#31995;&#29366;&#24577;&#25277;&#35937;&#30340;&#21452;&#23618;&#35268;&#21010;&#20013;&#30340;&#20027;&#21160;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Embodied Active Learning of Relational State Abstractions for Bilevel Planning. (arXiv:2303.04912v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04912
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20307;&#30693;&#35782;&#30340;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#32447;&#20132;&#20114;&#19982;&#19987;&#23478;&#23398;&#20064;&#31070;&#32463;&#35859;&#35789;&#35299;&#37322;&#12289;&#31526;&#21495;&#35268;&#21010;&#31639;&#23376;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#20851;&#31995;&#29366;&#24577;&#25277;&#35937;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29366;&#24577;&#25277;&#35937;&#26159;&#22312;&#26426;&#22120;&#20154;&#29615;&#22659;&#20013;&#36827;&#34892;&#35268;&#21010;&#30340;&#26377;&#25928;&#25216;&#26415;&#65292;&#35813;&#29615;&#22659;&#20855;&#26377;&#36830;&#32493;&#29366;&#24577;&#21644;&#21160;&#20316;&#12289;&#38271;&#20219;&#21153;&#26102;&#38388;&#21644;&#31232;&#30095;&#21453;&#39304;&#12290;&#22312;&#38754;&#21521;&#23545;&#35937;&#30340;&#29615;&#22659;&#20013;&#65292;&#35859;&#35789;&#26159;&#19968;&#31181;&#29305;&#21035;&#26377;&#29992;&#30340;&#29366;&#24577;&#25277;&#35937;&#24418;&#24335;&#65292;&#22240;&#20026;&#20854;&#19982;&#31526;&#21495;&#35268;&#21010;&#22120;&#30340;&#20860;&#23481;&#24615;&#20197;&#21450;&#20854;&#20851;&#31995;&#27867;&#21270;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#35201;&#20351;&#29992;&#35859;&#35789;&#36827;&#34892;&#35268;&#21010;&#65292;&#20195;&#29702;&#24517;&#39035;&#33021;&#22815;&#22312;&#36830;&#32493;&#29615;&#22659;&#29366;&#24577;&#19979;&#35299;&#37322;&#23427;&#20204;&#65288;&#21363;&#25509;&#22320;&#31526;&#21495;&#65289;&#12290;&#25163;&#21160;&#32534;&#31243;&#35859;&#35789;&#35299;&#37322;&#21487;&#33021;&#24456;&#22256;&#38590;&#65292;&#22240;&#27492;&#25105;&#20204;&#24076;&#26395;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#23427;&#20204;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36523;&#20307;&#30693;&#35782;&#30340;&#20027;&#21160;&#23398;&#20064;&#33539;&#24335;&#65292;&#20854;&#20013;&#20195;&#29702;&#36890;&#36807;&#19982;&#19987;&#23478;&#36827;&#34892;&#22312;&#32447;&#20132;&#20114;&#26469;&#23398;&#20064;&#35859;&#35789;&#35299;&#37322;&#12290;&#20363;&#22914;&#65292;&#22312;&#22534;&#21472;&#31215;&#26408;&#29615;&#22659;&#20013;&#37319;&#21462;&#34892;&#21160;&#21518;&#65292;&#20195;&#29702;&#21487;&#33021;&#20250;&#38382;&#19987;&#23478;&#65306;&#8220;On(block1&#65292;block2)&#26159;&#21542;&#20026;&#30495;&#65311;&#8221;&#20174;&#36825;&#20010;&#32463;&#39564;&#20013;&#65292;&#20195;&#29702;&#23398;&#20064;&#35268;&#21010;&#65306;&#23398;&#20064;&#31070;&#32463;&#35859;&#35789;&#35299;&#37322;&#12289;&#31526;&#21495;&#35268;&#21010;&#31639;&#23376;&#21644;&#20219;&#21153;&#29305;&#23450;&#30340;&#20851;&#31995;&#29366;&#24577;&#25277;&#35937;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#23398;&#20064;&#26377;&#25928;&#30340;&#25277;&#35937;&#65292;&#24182;&#22312;&#19968;&#31995;&#21015;&#26426;&#22120;&#20154;&#25805;&#20316;&#20219;&#21153;&#20013;&#25913;&#36827;&#22522;&#32447;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
State abstraction is an effective technique for planning in robotics environments with continuous states and actions, long task horizons, and sparse feedback. In object-oriented environments, predicates are a particularly useful form of state abstraction because of their compatibility with symbolic planners and their capacity for relational generalization. However, to plan with predicates, the agent must be able to interpret them in continuous environment states (i.e., ground the symbols). Manually programming predicate interpretations can be difficult, so we would instead like to learn them from data. We propose an embodied active learning paradigm where the agent learns predicate interpretations through online interaction with an expert. For example, after taking actions in a block stacking environment, the agent may ask the expert: "Is On(block1, block2) true?" From this experience, the agent learns to plan: it learns neural predicate interpretations, symbolic planning operators, an
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#22810;&#23545;&#31216;&#38598;&#21512;&#65288;MSE&#65289;&#65292;&#23427;&#36890;&#36807;&#23545;&#31216;&#36724;&#19978;&#20551;&#35774;&#30340;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#22810;&#26679;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#36229;&#36234;&#20256;&#32479;&#38543;&#26426;&#25200;&#21160;&#30340;&#26041;&#27861;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.02484</link><description>&lt;p&gt;
&#22810;&#23545;&#31216;&#38598;&#21512;&#65306;&#36890;&#36807;&#21453;&#21521;&#23545;&#31216;&#24615;&#25552;&#39640;&#22810;&#26679;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Multi-Symmetry Ensembles: Improving Diversity and Generalization via Opposing Symmetries. (arXiv:2303.02484v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02484
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#8212;&#8212;&#22810;&#23545;&#31216;&#38598;&#21512;&#65288;MSE&#65289;&#65292;&#23427;&#36890;&#36807;&#23545;&#31216;&#36724;&#19978;&#20551;&#35774;&#30340;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#22810;&#26679;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#36229;&#36234;&#20256;&#32479;&#38543;&#26426;&#25200;&#21160;&#30340;&#26041;&#27861;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#65292;&#24182;&#21462;&#24471;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#38598;&#21512;&#65288;DE&#65289;&#36890;&#36807;&#23398;&#20064;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#22810;&#26679;&#21270;&#25104;&#21592;&#25104;&#21151;&#22320;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#19968;&#20123;&#30740;&#31350;&#23581;&#35797;&#36890;&#36807;&#36229;&#21442;&#25968;&#25110;&#27491;&#21017;&#21270;&#25439;&#22833;&#20989;&#25968;&#26469;&#20419;&#36827;&#36827;&#19968;&#27493;&#30340;&#22810;&#26679;&#24615;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#20173;&#28982;&#20381;&#36182;&#20110;&#38543;&#26426;&#26041;&#27861;&#26469;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#23545;&#31216;&#38598;&#21512;&#65288;MSE&#65289;&#65292;&#36890;&#36807;&#25429;&#25417;&#23545;&#31216;&#36724;&#19978;&#20551;&#35774;&#30340;&#22810;&#26679;&#24615;&#65292;&#26500;&#24314;&#20102;&#19968;&#20010;&#26500;&#24314;&#22810;&#26679;&#24615;&#38598;&#21512;&#30340;&#26694;&#26550;&#65292;&#23427;&#21487;&#20197;&#36229;&#36234;&#27169;&#22411;&#26435;&#37325;&#21644;&#36229;&#21442;&#25968;&#30340;&#38543;&#26426;&#25200;&#21160;&#25506;&#32034;&#20551;&#35774;&#31354;&#38388;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#21019;&#24314;&#20102;&#20998;&#21035;&#25429;&#25417;&#19981;&#21464;&#21644;&#31561;&#21464;&#20989;&#25968;&#31867;&#30340;&#23545;&#31435;&#20551;&#35774;&#30340;&#27169;&#22411;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#20197;&#39640;&#25928;&#22320;&#32452;&#21512;&#36866;&#24403;&#30340;&#20551;&#35774;&#26469;&#23436;&#25104;&#32473;&#23450;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;MSE&#26377;&#25928;&#22320;&#25429;&#25417;&#20102;&#30456;&#20114;&#30683;&#30462;&#30340;&#20551;&#35774;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep ensembles (DE) have been successful in improving model performance by learning diverse members via the stochasticity of random initialization. While recent works have attempted to promote further diversity in DE via hyperparameters or regularizing loss functions, these methods primarily still rely on a stochastic approach to explore the hypothesis space. In this work, we present Multi-Symmetry Ensembles (MSE), a framework for constructing diverse ensembles by capturing the multiplicity of hypotheses along symmetry axes, which explore the hypothesis space beyond stochastic perturbations of model weights and hyperparameters. We leverage recent advances in contrastive representation learning to create models that separately capture opposing hypotheses of invariant and equivariant functional classes and present a simple ensembling approach to efficiently combine appropriate hypotheses for a given task. We show that MSE effectively captures the multiplicity of conflicting hypotheses th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22270;&#20132;&#38598;-&#35825;&#23548;&#22270;&#20256;&#36882;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#31232;&#30095;&#22270;&#22312;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#38382;&#39064;&#12290;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20132;&#38598;&#23376;&#22270;&#65292;&#22312;&#28304;&#22270;&#19978;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#30693;&#35782;&#20256;&#36882;&#21040;&#30446;&#26631;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2302.14189</link><description>&lt;p&gt;
&#20320;&#21482;&#36716;&#31227;&#20320;&#20998;&#20139;&#30340;&#20869;&#23481;&#65306;&#22522;&#20110;&#20132;&#38598;&#30340;&#22270;&#20256;&#36882;&#23398;&#20064;&#22312;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
You Only Transfer What You Share: Intersection-Induced Graph Transfer Learning for Link Prediction. (arXiv:2302.14189v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.14189
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#22270;&#20132;&#38598;-&#35825;&#23548;&#22270;&#20256;&#36882;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#31232;&#30095;&#22270;&#22312;&#38142;&#25509;&#39044;&#27979;&#20013;&#30340;&#38382;&#39064;&#12290;&#22312;&#27492;&#26041;&#27861;&#20013;&#65292;&#36890;&#36807;&#21019;&#24314;&#19968;&#20010;&#20132;&#38598;&#23376;&#22270;&#65292;&#22312;&#28304;&#22270;&#19978;&#35757;&#32451;&#27169;&#22411;&#24182;&#23558;&#30693;&#35782;&#20256;&#36882;&#21040;&#30446;&#26631;&#22270;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#26159;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#30340;&#26680;&#24515;&#65292;&#20294;&#24403;&#30446;&#26631;&#22270;&#38750;&#24120;&#31232;&#30095;&#26102;&#65292;&#39044;&#27979;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#21463;&#21040;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#31232;&#30095;&#22270;&#36896;&#25104;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20010;&#20043;&#21069;&#34987;&#24573;&#35270;&#30340;&#29616;&#35937;&#65306;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#65292;&#21407;&#22987;&#22270;&#24418;&#21487;&#33021;&#20250;&#26377;&#19968;&#20010;&#23494;&#38598;&#30456;&#36830;&#12289;&#20114;&#34917;&#30340;&#22270;&#24418;&#12290;&#36825;&#20010;&#23494;&#38598;&#30340;&#22270;&#24418;&#21487;&#33021;&#20250;&#19982;&#21407;&#22987;&#22270;&#24418;&#20849;&#20139;&#33410;&#28857;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#20010;&#23558;&#26377;&#24847;&#20041;&#30340;&#36873;&#25321;&#24615;&#30693;&#35782;&#36716;&#31227;&#30340;&#33258;&#28982;&#26725;&#26753;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#24773;&#20917;&#31216;&#20026;&#22522;&#20110;&#20132;&#38598;&#30340;&#22270;&#20256;&#36882;&#23398;&#20064;(GITL)&#65292;&#23427;&#21463;&#21040;&#20102;&#30005;&#23376;&#21830;&#21153;&#25110;&#23398;&#26415;&#21512;&#33879;&#39044;&#27979;&#31561;&#23454;&#38469;&#24212;&#29992;&#30340;&#21551;&#21457;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26377;&#25928;&#22320;&#21033;&#29992;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#32467;&#26500;&#24615;&#20808;&#39564;&#30693;&#35782;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#20004;&#20010;&#22270;&#20043;&#38388;&#20849;&#20139;&#30340;&#33410;&#28857;&#21019;&#24314;&#19968;&#20010;&#20132;&#38598;&#23376;&#22270;&#65292;&#28982;&#21518;&#23558;&#26469;&#33258;&#28304;&#20016;&#23500;&#30340;&#20132;&#38598;&#23376;&#22270;&#30340;&#30693;&#35782;&#20256;&#36882;&#21040;&#23436;&#25972;&#30340;&#30446;&#26631;&#22270;&#19978;&#12290;&#22312;&#31532;&#20108;&#27493;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#26041;&#27861;&#65306;&#19968;&#31181;&#26159;&#25913;&#36827;&#30340;&#26631;&#31614;&#20256;&#25773;&#26041;&#27861;&#65292;&#21478;&#19968;&#31181;&#26159;&#22810;&#23618;&#27425;&#30340;&#20256;&#36755;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction is central to many real-world applications, but its performance may be hampered when the graph of interest is sparse. To alleviate issues caused by sparsity, we investigate a previously overlooked phenomenon: in many cases, a densely connected, complementary graph can be found for the original graph. The denser graph may share nodes with the original graph, which offers a natural bridge for transferring selective, meaningful knowledge. We identify this setting as Graph Intersection-induced Transfer Learning (GITL), which is motivated by practical applications in e-commerce or academic co-authorship predictions. We develop a framework to effectively leverage the structural prior in this setting. We first create an intersection subgraph using the shared nodes between the two graphs, then transfer knowledge from the source-enriched intersection subgraph to the full target graph. In the second step, we consider two approaches: a modified label propagation, and a multi-layer
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DPLL&#31639;&#27861;&#30340;&#26032;&#30340;AND-OR&#22270;&#25628;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#26377;&#38480;&#36319;&#36394;&#65288;\LTLf&#65289;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#21512;&#25104;&#65292;&#20174;&#32780;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.13825</link><description>&lt;p&gt;
&#22522;&#20110;DPLL&#31639;&#27861;&#30340;LTLf&#21512;&#25104;&#36807;&#31243;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Forward LTLf Synthesis: DPLL At Work. (arXiv:2302.13825v2 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13825
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;DPLL&#31639;&#27861;&#30340;&#26032;&#30340;AND-OR&#22270;&#25628;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#26377;&#38480;&#36319;&#36394;&#65288;\LTLf&#65289;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#21512;&#25104;&#65292;&#20174;&#32780;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;AND-OR&#22270;&#25628;&#32034;&#26694;&#26550;&#65292;&#29992;&#20110;&#26377;&#38480;&#36319;&#36394;&#65288;\LTLf&#65289;&#30340;&#32447;&#24615;&#26102;&#24577;&#36923;&#36753;&#21512;&#25104;&#65292;&#20811;&#26381;&#20102;&#20043;&#21069;&#26041;&#27861;&#30340;&#19968;&#20123;&#38480;&#21046;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21463;Davis-Putnam-Logemann-Loveland&#65288;DPLL&#65289;&#31639;&#27861;&#21551;&#21457;&#30340;&#31243;&#24207;&#65292;&#22312;&#30495;&#27491;&#30340;&#28145;&#24230;&#20248;&#20808;&#26041;&#24335;&#19979;&#29983;&#25104;&#19979;&#19968;&#20010;&#21487;&#29992;&#30340;&#20195;&#29702; - &#29615;&#22659;&#31227;&#21160;&#65292;&#21487;&#33021;&#36991;&#20813;&#20102;&#32791;&#36153;&#26114;&#36149;&#30340;&#26522;&#20030;&#25110;&#32534;&#35793;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29366;&#24577;&#20844;&#24335;&#30340;&#35821;&#27861;&#31561;&#20215;&#24615;&#30340;&#25628;&#32034;&#33410;&#28857;&#30340;&#31561;&#20215;&#26816;&#26597;&#12290;&#30001;&#20110;&#25152;&#24471;&#21040;&#30340;&#31243;&#24207;&#19981;&#33021;&#20445;&#35777;&#32456;&#27490;&#65292;&#22240;&#27492;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#31181;&#20572;&#27490;&#26465;&#20214;&#26469;&#20013;&#27490;&#25191;&#34892;&#24182;&#22522;&#20110;&#20108;&#36827;&#21046;&#20915;&#31574;&#22270;&#65288;BDD&#65289;&#26816;&#26597;&#29366;&#24577;&#31561;&#20215;&#24615;&#37325;&#21551;&#25628;&#32034;&#65292;&#25105;&#20204;&#23637;&#31034;&#20854;&#26159;&#27491;&#30830;&#30340;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#25216;&#26415;&#22312;&#35768;&#22810;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#29616;Nike&#21442;&#21152;&#20102;SYNTCOM 2023&#24180;&#30340;LTLf Realizability Track&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a new AND-OR graph search framework for synthesis of Linear Temporal Logic on finite traces (\LTLf), that overcomes some limitations of previous approaches. Within such framework, we devise a procedure inspired by the Davis-Putnam-Logemann-Loveland (DPLL) algorithm to generate the next available agent-environment moves in a truly depth-first fashion, possibly avoiding exhaustive enumeration or costly compilations. We also propose a novel equivalence check for search nodes based on syntactic equivalence of state formulas. Since the resulting procedure is not guaranteed to terminate, we identify a stopping condition to abort execution and restart the search with state-equivalence checking based on Binary Decision Diagrams (BDD), which we show to be correct. The experimental results show that in many cases the proposed techniques outperform other state-of-the-art approaches. Our implementation Nike competed in the LTLf Realizability Track in the 2023 edition of SYNTCOM
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;3DIEBench&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#31561;&#21464;&#34920;&#36798;&#24335;&#39044;&#27979;&#22120;SIE&#65292;&#32467;&#21512;&#20998;&#35010;&#30340;&#19981;&#21464;&#19982;&#31561;&#21464;&#34920;&#36798;&#24335;&#20197;&#33719;&#24471;&#26356;&#21152;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2302.10283</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#20998;&#21106;&#19981;&#21464;&#31561;&#21464;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Self-supervised learning of Split Invariant Equivariant representations. (arXiv:2302.10283v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;3DIEBench&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#31561;&#21464;&#34920;&#36798;&#24335;&#39044;&#27979;&#22120;SIE&#65292;&#32467;&#21512;&#20998;&#35010;&#30340;&#19981;&#21464;&#19982;&#31561;&#21464;&#34920;&#36798;&#24335;&#20197;&#33719;&#24471;&#26356;&#21152;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#21516;&#26102;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#23398;&#20064;&#19981;&#21464;&#25110;&#31561;&#21464;&#34920;&#31034;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#12290;&#23613;&#31649;&#19981;&#21464;&#26041;&#27861;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35780;&#20272;&#65292;&#20294;&#31561;&#21464;&#26041;&#27861;&#22312;&#26356;&#23567;&#12289;&#26356;&#21487;&#25511;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#26088;&#22312;&#24357;&#21512;&#20004;&#32773;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20197;&#20415;&#23398;&#20064;&#26356;&#21152;&#22810;&#26679;&#21270;&#12289;&#36866;&#29992;&#20110;&#24191;&#27867;&#20219;&#21153;&#30340;&#34920;&#31034;&#26041;&#27861;&#12290;&#25105;&#20204;&#39318;&#20808;&#20171;&#32461;&#19968;&#20010;&#31216;&#20026;3DIEBench&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;55&#20010;&#31867;&#21035;&#30340;3D&#27169;&#22411;&#28210;&#26579;&#22270;&#20687;&#36229;&#36807;250&#19975;&#24352;&#65292;&#25105;&#20204;&#21487;&#20197;&#23436;&#20840;&#25511;&#21046;&#24212;&#29992;&#20110;&#23545;&#35937;&#30340;&#21464;&#25442;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#39044;&#27979;&#22120;&#20307;&#31995;&#32467;&#26500;&#65292;&#20197;&#23398;&#20064;&#31561;&#21464;&#34920;&#31034;&#65292;&#20174;&#32780;&#19981;&#21487;&#33021;&#20986;&#29616;&#19981;&#21464;&#24615;&#23849;&#22604;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SIE&#65288;&#20998;&#35010;&#19981;&#21464;-&#31561;&#21464;&#65289;&#30340;&#27010;&#24565;&#65292;&#23427;&#23558;&#22522;&#20110;&#36229;&#32593;&#32476;&#30340;&#39044;&#27979;&#22120;&#19982;&#20998;&#35010;&#20026;&#20004;&#37096;&#20998;&#65288;&#19968;&#37096;&#20998;&#20026;&#19981;&#21464;&#24418;&#65292;&#21478;&#19968;&#37096;&#20998;&#20026;&#31561;&#21464;&#24418;&#65289;&#30340;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#23398;&#20064;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent progress has been made towards learning invariant or equivariant representations with self-supervised learning. While invariant methods are evaluated on large scale datasets, equivariant ones are evaluated in smaller, more controlled, settings. We aim at bridging the gap between the two in order to learn more diverse representations that are suitable for a wide range of tasks. We start by introducing a dataset called 3DIEBench, consisting of renderings from 3D models over 55 classes and more than 2.5 million images where we have full control on the transformations applied to the objects. We further introduce a predictor architecture based on hypernetworks to learn equivariant representations with no possible collapse to invariance. We introduce SIE (Split Invariant-Equivariant) which combines the hypernetwork-based predictor with representations split in two parts, one invariant, the other equivariant, to learn richer representations. We demonstrate significant performance gains
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;x86&#25104;&#26412;&#27169;&#22411;&#35299;&#37322;&#30340;&#26694;&#26550;COMET&#65292;&#36890;&#36807;&#25552;&#20379;&#35299;&#37322;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#25104;&#26412;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#12290;&#30740;&#31350;&#26174;&#31034;&#35813;&#26694;&#26550;&#25152;&#25552;&#20379;&#30340;&#35821;&#20041;&#20449;&#24687;&#19982;&#25104;&#26412;&#39044;&#27979;&#35823;&#24046;&#21576;&#36127;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2302.06836</link><description>&lt;p&gt;
COMET: X86&#25104;&#26412;&#27169;&#22411;&#35299;&#37322;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
COMET: X86 Cost Model Explanation Framework. (arXiv:2302.06836v2 [cs.PF] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06836
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#29983;&#25104;x86&#25104;&#26412;&#27169;&#22411;&#35299;&#37322;&#30340;&#26694;&#26550;COMET&#65292;&#36890;&#36807;&#25552;&#20379;&#35299;&#37322;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#25104;&#26412;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25512;&#24191;&#24615;&#12290;&#30740;&#31350;&#26174;&#31034;&#35813;&#26694;&#26550;&#25152;&#25552;&#20379;&#30340;&#35821;&#20041;&#20449;&#24687;&#19982;&#25104;&#26412;&#39044;&#27979;&#35823;&#24046;&#21576;&#36127;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#31243;&#24207;&#25104;&#26412;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#21487;&#20197;&#25552;&#20379;&#30456;&#24403;&#20934;&#30830;&#30340;&#31243;&#24207;&#25104;&#26412;&#39044;&#27979;&#12290;&#23427;&#20204;&#21487;&#20197;&#21462;&#20195;&#20027;&#27969;&#32534;&#35793;&#22120;&#20013;&#32463;&#36807;&#22823;&#37327;&#24037;&#31243;&#35774;&#35745;&#30340;&#20998;&#26512;&#24615;&#31243;&#24207;&#25104;&#26412;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#30340;&#40657;&#21283;&#23376;&#26412;&#36136;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#37319;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#26694;&#26550;&#65292;COMET&#65292;&#29992;&#20110;&#20026;x86&#25104;&#26412;&#27169;&#22411;&#29983;&#25104;&#24544;&#23454;&#12289;&#21487;&#25512;&#24191;&#21644;&#30452;&#35266;&#30340;&#35299;&#37322;&#12290;COMET&#23558;&#21487;&#35299;&#37322;&#24615;&#29305;&#21035;&#24341;&#20837;ML-based&#25104;&#26412;&#27169;&#22411;&#65292;&#20363;&#22914;Ithemal&#12290;&#25105;&#20204;&#29983;&#25104;&#24182;&#27604;&#36739;Ithemal&#30340;COMET&#35299;&#37322;&#19982;&#25163;&#24037;&#31934;&#32454;&#30340;&#20998;&#26512;&#27169;&#22411;uiCA&#30340;COMET&#35299;&#37322;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#21457;&#29616;&#26174;&#31034;&#22312;&#32473;&#23450;&#30340;x86&#22522;&#26412;&#22359;&#30340;&#25104;&#26412;&#27169;&#22411;&#30340;COMET&#35299;&#37322;&#20013;&#65292;&#35821;&#20041;&#26356;&#20016;&#23500;&#30340;&#29305;&#24449;&#30340;&#31361;&#20986;&#31243;&#24230;&#19982;&#25104;&#26412;&#39044;&#27979;&#35823;&#24046;&#21576;&#36127;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
ML-based program cost models have been shown to yield fairly accurate program cost predictions. They can replace heavily-engineered analytical program cost models in mainstream compilers, but their black-box nature discourages their adoption. In this work, we propose the first framework, COMET, for generating faithful, generalizable, and intuitive explanations for x86 cost models. COMET brings interpretability specifically to ML-based cost models, such as Ithemal. We generate and compare COMET's explanations for Ithemal against COMET's explanations for a hand-crafted, accurate analytical model, uiCA. Our empirical findings show an inverse correlation between the error in the cost prediction of a cost model and the prominence of semantically-richer features in COMET's explanations for the cost model for a given x86 basic block.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21435;&#20559;&#24046;&#26041;&#27861;&#8212;&#8212;DAM&#65292;&#23427;&#37319;&#29992;AdapterFusion&#27010;&#24565;&#65292;&#23558;&#20559;&#24046;&#20462;&#27491;&#21151;&#33021;&#23553;&#35013;&#21040;&#29420;&#31435;&#30340;&#36866;&#37197;&#22120;&#20013;&#65292;&#22312;&#19981;&#24433;&#21709;&#26680;&#24515;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#25353;&#38656;&#30340;&#21435;&#20559;&#24046;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#27169;&#22411;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.06321</link><description>&lt;p&gt;
&#36890;&#36807;AdapterFusion&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#27169;&#22359;&#21270;&#20559;&#24046;&#20462;&#27491;
&lt;/p&gt;
&lt;p&gt;
Parameter-efficient Modularised Bias Mitigation via AdapterFusion. (arXiv:2302.06321v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#21435;&#20559;&#24046;&#26041;&#27861;&#8212;&#8212;DAM&#65292;&#23427;&#37319;&#29992;AdapterFusion&#27010;&#24565;&#65292;&#23558;&#20559;&#24046;&#20462;&#27491;&#21151;&#33021;&#23553;&#35013;&#21040;&#29420;&#31435;&#30340;&#36866;&#37197;&#22120;&#20013;&#65292;&#22312;&#19981;&#24433;&#21709;&#26680;&#24515;&#27169;&#22411;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20102;&#25353;&#38656;&#30340;&#21435;&#20559;&#24046;&#65292;&#21487;&#20197;&#26377;&#25928;&#38477;&#20302;&#27169;&#22411;&#30340;&#20559;&#35265;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#23384;&#22312;&#31038;&#20250;&#20559;&#35265;&#65292;&#24182;&#23558;&#36825;&#20123;&#20559;&#35265;&#24102;&#32473;&#19979;&#28216;&#20219;&#21153;&#12290;&#24403;&#21069;&#30340;&#20869;&#37096;&#22788;&#29702;&#20559;&#24046;&#20462;&#27491;&#26041;&#27861;&#65288;&#22914;&#23545;&#25239;&#35757;&#32451;&#65289;&#36890;&#36807;&#26356;&#26032;&#27169;&#22411;&#21442;&#25968;&#26469;&#26045;&#21152;&#21435;&#20559;&#24046;&#65292;&#20174;&#32780;&#23558;&#27169;&#22411;&#36716;&#31227;&#21040;&#26032;&#30340;&#12289;&#19981;&#21487;&#36870;&#30340;&#21435;&#20559;&#24046;&#29366;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20986;&#20102;&#29420;&#31435;&#30340;&#21435;&#20559;&#24046;&#21151;&#33021;&#65292;&#19982;&#27169;&#22411;&#20998;&#31163;&#65292;&#21487;&#20197;&#25353;&#38656;&#38598;&#25104;&#21040;&#27169;&#22411;&#20013;&#65292;&#21516;&#26102;&#20445;&#25345;&#26680;&#24515;&#27169;&#22411;&#19981;&#21464;&#12290;&#20511;&#37492;&#22810;&#20219;&#21153;&#23398;&#20064;&#20013;&#30340;AdapterFusion&#27010;&#24565;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;DAM&#65288;&#20351;&#29992;&#36866;&#37197;&#22120;&#27169;&#22359;&#36827;&#34892;&#21435;&#20559;&#24046;&#65289;&#8212;&#8212;&#19968;&#31181;&#21435;&#20559;&#24046;&#26041;&#27861;&#65292;&#39318;&#20808;&#23558;&#20219;&#24847;&#20559;&#24046;&#20462;&#27491;&#21151;&#33021;&#23553;&#35013;&#21040;&#29420;&#31435;&#30340;&#36866;&#37197;&#22120;&#20013;&#65292;&#28982;&#21518;&#25353;&#38656;&#23558;&#23427;&#20204;&#28155;&#21152;&#21040;&#27169;&#22411;&#20013;&#65292;&#20197;&#23454;&#29616;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#31181;&#20998;&#31867;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20445;&#25252;&#23646;&#24615;&#20026;&#24615;&#21035;&#12289;&#31181;&#26063;&#21644;&#24180;&#40836;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;DAM&#25913;&#36827;&#25110;&#20445;&#25345;&#20102;&#27169;&#22411;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large pre-trained language models contain societal biases and carry along these biases to downstream tasks. Current in-processing bias mitigation approaches (like adversarial training) impose debiasing by updating a model's parameters, effectively transferring the model to a new, irreversible debiased state. In this work, we propose a novel approach to develop stand-alone debiasing functionalities separate from the model, which can be integrated into the model on-demand, while keeping the core model untouched. Drawing from the concept of AdapterFusion in multi-task learning, we introduce DAM (Debiasing with Adapter Modules) - a debiasing approach to first encapsulate arbitrary bias mitigation functionalities into separate adapters, and then add them to the model on-demand in order to deliver fairness qualities. We conduct a large set of experiments on three classification tasks with gender, race, and age as protected attributes. Our results show that DAM improves or maintains the effec
&lt;/p&gt;</description></item><item><title>MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2302.05981</link><description>&lt;p&gt;
MarioGPT: &#36890;&#36807;&#22823;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24320;&#25918;&#24335;&#25991;&#26412;&#20851;&#21345;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
MarioGPT: Open-Ended Text2Level Generation through Large Language Models. (arXiv:2302.05981v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05981
&lt;/p&gt;
&lt;p&gt;
MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#28216;&#25103;&#20851;&#21345;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#24320;&#25918;&#24335;&#30340;&#12289;&#21487;&#25511;&#21046;&#30340;&#20851;&#21345;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#22797;&#26434;&#25968;&#19968;&#33268;&#30340;&#29615;&#22659;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#26041;&#27861;&#29983;&#25104;&#21453;&#26144;&#29305;&#23450;&#24847;&#22270;&#21644;&#38480;&#21046;&#30340;&#26377;&#24847;&#20041;&#20869;&#23481;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#27969;&#31243;&#20869;&#23481;&#29983;&#25104;&#31639;&#27861;&#32570;&#20047;&#20197;&#24320;&#25918;&#24335;&#26041;&#24335;&#29983;&#25104;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#39046;&#22495;&#37117;&#34920;&#29616;&#20986;&#20102;&#38750;&#24120;&#39640;&#30340;&#25928;&#29575;&#12290;&#36825;&#20123;&#35757;&#32451;&#26377;&#32032;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36827;&#34892;&#24494;&#35843;&#65292;&#37325;&#22797;&#20351;&#29992;&#20449;&#24687;&#24182;&#21152;&#36895;&#26032;&#20219;&#21153;&#30340;&#22521;&#35757;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;MarioGPT&#65292;&#36825;&#26159;&#19968;&#20010;&#32463;&#36807;&#20248;&#21270;&#30340;GPT2&#27169;&#22411;&#65292;&#29992;&#20110;&#29983;&#25104;&#22522;&#20110;&#29943;&#30742;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#25105;&#20204;&#20197;&#36229;&#32423;&#39532;&#37324;&#22885;&#20804;&#24351;&#30340;&#20851;&#21345;&#20026;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MarioGPT&#19981;&#20165;&#21487;&#20197;&#29983;&#25104;&#19981;&#21516;&#30340;&#28216;&#25103;&#20851;&#21345;&#65292;&#32780;&#19988;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#25552;&#31034;&#25511;&#21046;&#20851;&#21345;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#24403;&#21069;PCG&#25216;&#26415;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;MarioGPT&#26159;&#31532;&#19968;&#20010;&#25991;&#26412;&#21040;&#20851;&#21345;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Procedural Content Generation (PCG) algorithms provide a technique to generate complex and diverse environments in an automated way. However, while generating content with PCG methods is often straightforward, generating meaningful content that reflects specific intentions and constraints remains challenging. Furthermore, many PCG algorithms lack the ability to generate content in an open-ended manner. Recently, Large Language Models (LLMs) have shown to be incredibly effective in many diverse domains. These trained LLMs can be fine-tuned, re-using information and accelerating training for new tasks. In this work, we introduce MarioGPT, a fine-tuned GPT2 model trained to generate tile-based game levels, in our case Super Mario Bros levels. We show that MarioGPT can not only generate diverse levels, but can be text-prompted for controllable level generation, addressing one of the key challenges of current PCG techniques. As far as we know, MarioGPT is the first text-to-level model. We a
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#25628;&#32034;&#20877;&#25509;&#36817;&#30446;&#26631;&#20154;&#21592;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#26426;&#22120;&#20154;&#25509;&#36817;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#28151;&#21512;&#23398;&#20064;&#30340;&#26694;&#26550;&#29983;&#25104;&#26426;&#22120;&#20154;&#30340;&#21451;&#22909;&#21160;&#20316;&#65292;&#25104;&#21151;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2302.05324</link><description>&lt;p&gt;
SOCRATES&#65306;&#22522;&#20110;&#25991;&#26412;&#30340;&#26426;&#22120;&#20154;&#29399;&#20154;&#20307;&#25628;&#32034;&#21644;&#25509;&#36817;
&lt;/p&gt;
&lt;p&gt;
SOCRATES: Text-based Human Search and Approach using a Robot Dog. (arXiv:2302.05324v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05324
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20808;&#25628;&#32034;&#20877;&#25509;&#36817;&#30446;&#26631;&#20154;&#21592;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#26426;&#22120;&#20154;&#25509;&#36817;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#22522;&#20110;&#28151;&#21512;&#23398;&#20064;&#30340;&#26694;&#26550;&#29983;&#25104;&#26426;&#22120;&#20154;&#30340;&#21451;&#22909;&#21160;&#20316;&#65292;&#25104;&#21151;&#39564;&#35777;&#20102;&#35813;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#30001;&#25991;&#26412;&#34920;&#36848;&#30340;SOcratic&#27169;&#22411;&#65292;&#29992;&#20110;&#26426;&#22120;&#20154;&#25509;&#36817;&#20154;&#31867;&#65292;&#31216;&#20026;SOCRATES&#12290;&#39318;&#20808;&#65292;&#26426;&#22120;&#20154;&#25628;&#32034;&#30446;&#26631;&#29992;&#25143;&#65292;&#28982;&#21518;&#20197;&#20154;&#31867;&#21451;&#22909;&#30340;&#26041;&#24335;&#25509;&#36817;&#12290;&#25991;&#26412;&#25551;&#36848;&#30001;&#22806;&#35266;&#21644;&#20301;&#32622;&#32447;&#32034;&#32452;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20154;&#31867;&#25628;&#32034;SOCratic&#27169;&#22411;&#20197;&#35299;&#20915;&#30446;&#26631;&#20154;&#21592;&#25628;&#32034;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#35813;&#27169;&#22411;&#23558;&#35821;&#35328;&#22495;&#20013;&#30340;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#36830;&#25509;&#36215;&#26469;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#28151;&#21512;&#23398;&#20064;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#29983;&#25104;&#21040;&#36798;&#20154;&#30340;&#30446;&#26631;&#21451;&#22909;&#26426;&#22120;&#20154;&#21160;&#20316;&#65292;&#21253;&#25324;&#19968;&#20010;&#26469;&#33258;&#28436;&#31034;&#30340;&#23398;&#20064;&#27169;&#22359;&#21644;&#19968;&#20010;&#30693;&#35782;&#33976;&#39311;&#27169;&#22359;&#12290;&#25105;&#20204;&#36890;&#36807;&#34394;&#25311;&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;&#20223;&#30495;&#20197;&#21450;&#22312;&#22823;&#23398;&#26657;&#22253;&#29615;&#22659;&#20013;&#20351;&#29992;&#26426;&#22120;&#20154;&#29399;&#36827;&#34892;&#30340;&#30495;&#23454;&#23454;&#39564;&#65292;&#39564;&#35777;&#20102;&#25152;&#25552;&#20986;&#30340;&#25628;&#32034;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a SOCratic model for Robots Approaching humans based on TExt System (SOCRATES) focusing on the human search and approach based on free-form textual description; the robot first searches for the target user, then the robot proceeds to approach in a human-friendly manner. In particular, textual descriptions are composed of appearance (e.g., wearing white shirts with black hair) and location clues (e.g., is a student who works with robots). We initially present a Human Search Socratic Model that connects large pre-trained models in the language domain to solve the downstream task, which is searching for the target person based on textual descriptions. Then, we propose a hybrid learning-based framework for generating target-cordial robotic motion to approach a person, consisting of a learning-from-demonstration module and a knowledge distillation module. We validate the proposed searching module via simulation using a virtual mobile robot as well as through real-w
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;HyperSound&#8221;&#65292;&#21487;&#23558;&#36229;&#32593;&#32476;&#32467;&#26500;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#65292;&#20174;&#32780;&#29983;&#25104;&#38899;&#39057;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#30340;&#22788;&#29702;&#65292;&#24182;&#19988;&#37325;&#26500;&#36136;&#37327;&#21487;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#26159;&#24403;&#20195;&#38899;&#39057;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#26377;&#28508;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2302.04959</link><description>&lt;p&gt;
&#36229;&#32593;&#32476;&#26500;&#24314;&#38899;&#39057;&#30340;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Hypernetworks build Implicit Neural Representations of Sounds. (arXiv:2302.04959v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04959
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21517;&#20026;&#8220;HyperSound&#8221;&#65292;&#21487;&#23558;&#36229;&#32593;&#32476;&#32467;&#26500;&#24212;&#29992;&#20110;&#20803;&#23398;&#20064;&#65292;&#20174;&#32780;&#29983;&#25104;&#38899;&#39057;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#65292;&#35813;&#26041;&#27861;&#21487;&#29992;&#20110;&#38899;&#39057;&#20449;&#21495;&#30340;&#22788;&#29702;&#65292;&#24182;&#19988;&#37325;&#26500;&#36136;&#37327;&#21487;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#30456;&#23218;&#32654;&#65292;&#26159;&#24403;&#20195;&#38899;&#39057;&#22788;&#29702;&#20013;&#30340;&#19968;&#20010;&#26377;&#28508;&#21147;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#29616;&#22312;&#34987;&#24191;&#27867;&#22320;&#24212;&#29992;&#20110;&#21508;&#31181;&#23454;&#38469;&#24212;&#29992;&#31243;&#24207;&#20013;&#26469;&#20195;&#34920;&#22810;&#23186;&#20307;&#20449;&#21495;&#65292;&#21253;&#25324;&#22270;&#20687;&#36229;&#20998;&#36776;&#29575;&#12289;&#22270;&#20687;&#21387;&#32553;&#25110;3D&#28210;&#26579;&#12290;&#29616;&#26377;&#30340;&#21033;&#29992;INR&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#35270;&#35273;&#25968;&#25454;&#19978;&#65292;&#22240;&#20026;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;INR&#27169;&#22411;&#30340;&#26550;&#26500;&#23646;&#24615;&#20013;&#23384;&#22312;&#24402;&#32435;&#20559;&#24046;&#65292;&#25152;&#20197;&#23558;&#20854;&#24212;&#29992;&#20110;&#20854;&#20182;&#27169;&#24577;&#65292;&#22914;&#38899;&#39057;&#65292;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#36229;&#22768;&#65288;HyperSound&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#36229;&#32593;&#32476;&#36827;&#34892;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20026;&#38899;&#39057;&#26679;&#26412;&#29983;&#25104;INR&#65292;&#20197;&#20415;&#33021;&#22815;&#22312;&#35757;&#32451;&#20013;&#35266;&#23519;&#21040;&#30340;&#26679;&#26412;&#19978;&#36827;&#34892;&#25512;&#24191;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20197;&#21487;&#27604;&#36739;&#20854;&#20182;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#36136;&#37327;&#37325;&#26500;&#38899;&#39057;&#26679;&#26412;&#65292;&#24182;&#20026;&#29992;&#20110;&#38899;&#39057;&#22788;&#29702;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#24403;&#20195;&#22768;&#38899;&#34920;&#31034;&#25552;&#20379;&#20102;&#21487;&#34892;&#30340;&#26367;&#20195;&#26041;&#26696;&#65292;&#22914;&#35889;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Neural Representations (INRs) are nowadays used to represent multimedia signals across various real-life applications, including image super-resolution, image compression, or 3D rendering. Existing methods that leverage INRs are predominantly focused on visual data, as their application to other modalities, such as audio, is nontrivial due to the inductive biases present in architectural attributes of image-based INR models. To address this limitation, we introduce HyperSound, the first meta-learning approach to produce INRs for audio samples that leverages hypernetworks to generalize beyond samples observed in training. Our approach reconstructs audio samples with quality comparable to other state-of-the-art models and provides a viable alternative to contemporary sound representations used in deep neural networks for audio processing, such as spectrograms.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25552;&#21319;&#22270;&#24418;&#27169;&#22411;&#20013;&#32467;&#26500;&#27169;&#20307;&#25366;&#25496;&#30340;&#21407;&#21017;&#24615;&#21644;&#39640;&#25928;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#27493;&#39588;&#21644;&#25511;&#21046;&#36229;&#21442;&#25968;&#20248;&#21270;&#31639;&#27861;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2302.04599</link><description>&lt;p&gt;
&#38754;&#21521;&#25552;&#21319;&#22270;&#24418;&#27169;&#22411;&#32467;&#26500;&#23398;&#20064;&#30340;&#21407;&#21017;&#24615;&#21644;&#39640;&#25928;&#24615;&#27169;&#20307;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Principled and Efficient Motif Finding for Structure Learning of Lifted Graphical Models. (arXiv:2302.04599v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.04599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#25552;&#21319;&#22270;&#24418;&#27169;&#22411;&#20013;&#32467;&#26500;&#27169;&#20307;&#25366;&#25496;&#30340;&#21407;&#21017;&#24615;&#21644;&#39640;&#25928;&#24615;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#22788;&#29702;&#27493;&#39588;&#21644;&#25511;&#21046;&#36229;&#21442;&#25968;&#20248;&#21270;&#31639;&#27861;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#23398;&#20064;&#26159;&#20154;&#24037;&#26234;&#33021;&#20013;&#30340;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65292;&#23545;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#21644;&#32479;&#35745;&#20851;&#31995;&#23398;&#20064;&#39046;&#22495;&#33267;&#20851;&#37325;&#35201;&#12290;&#23427;&#21253;&#25324;&#20174;&#25968;&#25454;&#20013;&#33258;&#21160;&#23398;&#20064;&#36923;&#36753;&#29702;&#35770;&#30340;&#36807;&#31243;&#12290;&#32467;&#26500;&#23398;&#20064;&#30340;&#22522;&#30784;&#26159;&#25366;&#25496;&#25968;&#25454;&#20013;&#30340;&#37325;&#22797;&#27169;&#24335;&#65292;&#21363;&#32467;&#26500;&#27169;&#20307;&#12290;&#21457;&#29616;&#36825;&#20123;&#27169;&#24335;&#21487;&#38477;&#20302;&#25628;&#32034;&#31354;&#38388;&#65292;&#24182;&#24341;&#23548;&#20844;&#24335;&#30340;&#23398;&#20064;&#12290;&#23613;&#31649;&#27169;&#20307;&#23398;&#20064;&#38750;&#24120;&#37325;&#35201;&#65292;&#20294;&#23427;&#20173;&#19981;&#20026;&#20154;&#20204;&#25152;&#29702;&#35299;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#36807;&#31243;&#30340;&#65292;&#38024;&#23545;&#25552;&#21319;&#22270;&#24418;&#27169;&#22411;&#20013;&#30719;&#32467;&#26500;&#27169;&#20307;&#30340;&#21407;&#21017;&#24615;&#26041;&#27861;&#65292;&#36825;&#20123;&#27169;&#22411;&#23558;&#19968;&#38454;&#36923;&#36753;&#19982;&#27010;&#29575;&#27169;&#22411;&#32467;&#21512;&#22312;&#19968;&#36215;&#12290;&#25105;&#20204;&#30340;&#31532;&#19968;&#20010;&#21019;&#26032;&#26159;&#19968;&#31181;&#31639;&#27861;&#65292;&#23427;&#20381;&#36182;&#20110;&#20004;&#20010;&#30452;&#35266;&#30340;&#36229;&#21442;&#25968;&#65306;&#19968;&#20010;&#25511;&#21046;&#23454;&#20307;&#30456;&#20284;&#24615;&#24230;&#37327;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#21450;&#19968;&#20010;&#25511;&#21046;&#29983;&#25104;&#35268;&#21017;&#30340;&#26580;&#36719;&#24230;&#12290;&#31532;&#20108;&#20010;&#36129;&#29486;&#26159;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#36890;&#36807;&#20998;&#23618;&#32858;&#31867;&#23558;&#30456;&#20284;&#30340;&#23454;&#20307;&#20998;&#32452;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structure learning is a core problem in AI central to the fields of neuro-symbolic AI and statistical relational learning. It consists in automatically learning a logical theory from data. The basis for structure learning is mining repeating patterns in the data, known as structural motifs. Finding these patterns reduces the exponential search space and therefore guides the learning of formulas. Despite the importance of motif learning, it is still not well understood. We present the first principled approach for mining structural motifs in lifted graphical models, languages that blend first-order logic with probabilistic models, which uses a stochastic process to measure the similarity of entities in the data. Our first contribution is an algorithm, which depends on two intuitive hyperparameters: one controlling the uncertainty in the entity similarity measure, and one controlling the softness of the resulting rules. Our second contribution is a preprocessing step where we perform hie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Raincoat&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#30340;&#23553;&#38381;&#38598;&#21644;&#36890;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;Raincoat&#36890;&#36807;&#36328;&#22495;&#23545;&#40784;&#26102;&#38388;&#21644;&#39057;&#29575;&#29305;&#24449;&#65292;&#20462;&#27491;&#20559;&#31227;&#20197;&#20415;&#20110;&#26816;&#27979;&#31169;&#26377;&#26631;&#31614;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#20849;&#20139;&#32467;&#26500;&#26469;&#25552;&#39640;&#21487;&#20256;&#36882;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.03133</link><description>&lt;p&gt;
&#38024;&#23545;&#29305;&#24449;&#19982;&#26631;&#31614;&#20559;&#31227;&#30340;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
Domain Adaptation for Time Series Under Feature and Label Shifts. (arXiv:2302.03133v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Raincoat&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#30340;&#23553;&#38381;&#38598;&#21644;&#36890;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;Raincoat&#36890;&#36807;&#36328;&#22495;&#23545;&#40784;&#26102;&#38388;&#21644;&#39057;&#29575;&#29305;&#24449;&#65292;&#20462;&#27491;&#20559;&#31227;&#20197;&#20415;&#20110;&#26816;&#27979;&#31169;&#26377;&#26631;&#31614;&#65292;&#24182;&#36890;&#36807;&#35782;&#21035;&#20849;&#20139;&#32467;&#26500;&#26469;&#25552;&#39640;&#21487;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#21487;&#20197;&#23558;&#28304;&#22495;&#35757;&#32451;&#30340;&#27169;&#22411;&#36716;&#31227;&#21040;&#26410;&#26631;&#35760;&#30340;&#30446;&#26631;&#22495;&#12290;&#28982;&#32780;&#65292;&#36716;&#31227;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#38754;&#20020;&#30528;&#25361;&#25112;&#65292;&#22240;&#20026;&#19981;&#21516;&#22495;&#20013;&#23384;&#22312;&#21160;&#24577;&#30340;&#26102;&#38388;&#32467;&#26500;&#21464;&#21270;&#65292;&#23548;&#33268;&#26102;&#38388;&#21644;&#39057;&#29575;&#34920;&#31034;&#20013;&#30340;&#29305;&#24449;&#20559;&#31227;&#12290;&#27492;&#22806;&#65292;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20013;&#30340;&#20219;&#21153;&#26631;&#31614;&#20998;&#24067;&#21487;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#38590;&#20197;&#35299;&#20915;&#26631;&#31614;&#20559;&#31227;&#21644;&#35782;&#21035;&#21807;&#19968;&#20110;&#30446;&#26631;&#22495;&#30340;&#26631;&#31614;&#12290;&#26377;&#25928;&#22320;&#36716;&#31227;&#22797;&#26434;&#30340;&#26102;&#38388;&#24207;&#21015;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#21313;&#20998;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Raincoat&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#38024;&#23545;&#22797;&#26434;&#26102;&#38388;&#24207;&#21015;&#30340;&#23553;&#38381;&#38598;&#21644;&#36890;&#29992;&#39046;&#22495;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;Raincoat&#36890;&#36807;&#32771;&#34385;&#26102;&#38388;&#21644;&#39057;&#29575;&#29305;&#24449;&#65292;&#36328;&#22495;&#23545;&#40784;&#23427;&#20204;&#65292;&#20462;&#27491;&#19981;&#21305;&#37197;&#20197;&#20415;&#20110;&#26816;&#27979;&#31169;&#26377;&#26631;&#31614;&#26469;&#35299;&#20915;&#29305;&#24449;&#21644;&#26631;&#31614;&#20559;&#31227;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;Raincoat&#36890;&#36807;&#35782;&#21035;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#20849;&#20139;&#32467;&#26500;&#26469;&#25552;&#39640;&#21487;&#20256;&#36882;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) enables the transfer of models trained on source domains to unlabeled target domains. However, transferring complex time series models presents challenges due to the dynamic temporal structure variations across domains. This leads to feature shifts in the time and frequency representations. Additionally, the label distributions of tasks in the source and target domains can differ significantly, posing difficulties in addressing label shifts and recognizing labels unique to the target domain. Effectively transferring complex time series models remains a formidable problem. We present Raincoat, the first model for both closed-set and universal domain adaptation on complex time series. Raincoat addresses feature and label shifts by considering both temporal and frequency features, aligning them across domains, and correcting for misalignments to facilitate the detection of private labels. Additionally, Raincoat improves transferability by identifying l
&lt;/p&gt;</description></item><item><title>Mnemosyne&#20248;&#21270;&#22120;&#20351;&#29992;Performers&#26041;&#27861;&#26469;&#23398;&#20064;&#35757;&#32451;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#20854;&#20182;Transformers&#65292;&#32780;&#26080;&#38656;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#22120;&#35843;&#33410;&#65292;&#24182;&#25104;&#21151;&#35757;&#32451;ViTs&#21644;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#19982;&#24555;&#36895;&#25910;&#25947;&#12290;</title><link>http://arxiv.org/abs/2302.01128</link><description>&lt;p&gt;
Mnemosyne: &#20351;&#29992;Transformers&#26469;&#35757;&#32451;Transformers
&lt;/p&gt;
&lt;p&gt;
Mnemosyne: Learning to Train Transformers with Transformers. (arXiv:2302.01128v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01128
&lt;/p&gt;
&lt;p&gt;
Mnemosyne&#20248;&#21270;&#22120;&#20351;&#29992;Performers&#26041;&#27861;&#26469;&#23398;&#20064;&#35757;&#32451;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#20854;&#20182;Transformers&#65292;&#32780;&#26080;&#38656;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#22120;&#35843;&#33410;&#65292;&#24182;&#25104;&#21151;&#35757;&#32451;ViTs&#21644;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#19982;&#24555;&#36895;&#25910;&#25947;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;(ML)&#26550;&#26500;&#38656;&#35201;&#32791;&#36153;&#22823;&#37327;&#35745;&#31639;&#21644;&#26102;&#38388;&#26469;&#36873;&#25321;&#21512;&#36866;&#30340;&#20248;&#21270;&#22120;&#24182;&#35843;&#33410;&#20854;&#36229;&#21442;&#25968;&#12290;&#20174;&#25968;&#25454;&#20013;&#23398;&#20064;&#20248;&#21270;&#22120;&#30340;&#26032;&#23398;&#20064;&#33539;&#24335;&#24050;&#32463;&#25104;&#20026;&#25163;&#21160;&#35774;&#35745;ML&#20248;&#21270;&#22120;&#30340;&#26356;&#22909;&#36873;&#25321;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Mnemosyne&#20248;&#21270;&#22120;&#65292;&#23427;&#20351;&#29992;Performers: &#38544;&#24335;&#20302;&#31209;attention Transformers&#12290;&#23427;&#21487;&#20197;&#23398;&#20064;&#35757;&#32451;&#25972;&#20010;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#21253;&#25324;&#20854;&#20182;Transformers&#65292;&#32780;&#26080;&#38656;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#36827;&#34892;&#20248;&#21270;&#22120;&#35843;&#33410;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;Mnemosyne&#65306;(a)&#27604;&#27969;&#34892;&#30340;LSTM&#20248;&#21270;&#22120;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#65307;(b)&#29305;&#21035;&#22320;&#65292;&#21487;&#20197;&#22312;&#26631;&#20934;MLPs&#19978;&#36827;&#34892;&#20803;&#35757;&#32451;&#21518;&#25104;&#21151;&#22320;&#35757;&#32451;Vision Transformers(ViTs) (c)&#21487;&#20197;&#21021;&#22987;&#21270;&#20248;&#21270;&#22120;&#20197;&#23454;&#29616;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#26356;&#24555;&#30340;&#25910;&#25947;&#12290;&#25105;&#20204;&#30456;&#20449;&#36825;&#20123;&#32467;&#26524;&#24320;&#21551;&#20102;&#20351;&#29992;Transformers&#26500;&#24314;&#22522;&#30784;&#20248;&#21270;&#27169;&#22411;&#30340;&#21487;&#33021;&#24615;&#65292;&#21487;&#20197;&#24212;&#23545;&#24120;&#35268;&#30340;Transformer&#35757;&#32451;&#25361;&#25112;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#29702;&#35770;&#20998;&#26512;&#26469;&#34917;&#20805;&#25105;&#20204;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training complex machine learning (ML) architectures requires a compute and time consuming process of selecting the right optimizer and tuning its hyper-parameters. A new paradigm of learning optimizers from data has emerged as a better alternative to hand-designed ML optimizers. We propose Mnemosyne optimizer, that uses Performers: implicit low-rank attention Transformers. It can learn to train entire neural network architectures including other Transformers without any task-specific optimizer tuning. We show that Mnemosyne: (a) generalizes better than popular LSTM optimizer, (b) in particular can successfully train Vision Transformers (ViTs) while meta--trained on standard MLPs and (c) can initialize optimizers for faster convergence in Robotics applications. We believe that these results open the possibility of using Transformers to build foundational optimization models that can address the challenges of regular Transformer training. We complement our results with an extensive theo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;MILO&#65292;&#23558;&#23376;&#38598;&#36873;&#25321;&#19982;&#27169;&#22411;&#35757;&#32451;&#20998;&#31163;&#65292;&#36890;&#36807;&#26131;&#21040;&#38590;&#30340;&#35838;&#31243;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13287</link><description>&lt;p&gt;
MILO: &#27169;&#22411;&#26080;&#20851;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;&#65292;&#29992;&#20110;&#39640;&#25928;&#27169;&#22411;&#35757;&#32451;&#21644;&#35843;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;
MILO: Model-Agnostic Subset Selection Framework for Efficient Model Training and Tuning. (arXiv:2301.13287v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13287
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;MILO&#65292;&#23558;&#23376;&#38598;&#36873;&#25321;&#19982;&#27169;&#22411;&#35757;&#32451;&#20998;&#31163;&#65292;&#36890;&#36807;&#26131;&#21040;&#38590;&#30340;&#35838;&#31243;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#28145;&#24230;&#32593;&#32476;&#21644;&#35843;&#20248;&#22823;&#22411;&#25968;&#25454;&#38598;&#30340;&#36229;&#21442;&#25968;&#26159;&#35745;&#31639;&#23494;&#38598;&#22411;&#30340;&#12290;&#20943;&#23569;&#35757;&#32451;&#25104;&#26412;&#30340;&#20027;&#35201;&#30740;&#31350;&#26041;&#21521;&#20043;&#19968;&#26159;&#36890;&#36807;&#36873;&#25321;&#24456;&#22909;&#30340;&#35757;&#32451;&#25968;&#25454;&#23376;&#38598;&#26469;&#23454;&#29616;&#12290;&#19982;&#31616;&#21333;&#30340;&#33258;&#36866;&#24212;&#38543;&#26426;&#23376;&#38598;&#36873;&#25321;&#22522;&#20934;&#30456;&#27604;&#65292;&#29616;&#26377;&#30340;&#26234;&#33021;&#23376;&#38598;&#36873;&#25321;&#26041;&#27861;&#30001;&#20110;&#32791;&#26102;&#30340;&#23376;&#38598;&#36873;&#25321;&#27493;&#39588;&#32780;&#19981;&#20855;&#31454;&#20105;&#21147;&#65292;&#35813;&#27493;&#39588;&#28041;&#21450;&#35745;&#31639;&#20381;&#36182;&#20110;&#27169;&#22411;&#30340;&#26799;&#24230;&#21644;&#29305;&#24449;&#23884;&#20837;&#65292;&#24182;&#24212;&#29992;&#23376;&#27169;&#22359;&#30446;&#26631;&#30340;&#36138;&#24515;&#26368;&#22823;&#21270;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#27934;&#23519;&#26159;&#28040;&#38500;&#23545;&#19979;&#28216;&#27169;&#22411;&#21442;&#25968;&#30340;&#20381;&#36182;&#65292;&#23558;&#23376;&#38598;&#36873;&#25321;&#20316;&#20026;&#39044;&#22788;&#29702;&#27493;&#39588;&#65292;&#24182;&#20351;&#20854;&#33021;&#22815;&#22312;&#19981;&#22686;&#21152;&#25104;&#26412;&#30340;&#24773;&#20917;&#19979;&#35757;&#32451;&#22810;&#20010;&#27169;&#22411;&#12290;&#22312;&#36825;&#20010;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; MILO&#65292;&#19968;&#20010;&#27169;&#22411;&#26080;&#20851;&#30340;&#23376;&#38598;&#36873;&#25321;&#26694;&#26550;&#65292;&#23427;&#23558;&#23376;&#38598;&#36873;&#25321;&#19982;&#27169;&#22411;&#35757;&#32451;&#20998;&#31163;&#65292;&#21516;&#26102;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#26131;&#21040;&#38590;&#30340;&#35838;&#31243;&#23454;&#29616;&#20102;&#21331;&#36234;&#30340;&#27169;&#22411;&#25910;&#25947;&#21644;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training deep networks and tuning hyperparameters on large datasets is computationally intensive. One of the primary research directions for efficient training is to reduce training costs by selecting well-generalizable subsets of training data. Compared to simple adaptive random subset selection baselines, existing intelligent subset selection approaches are not competitive due to the time-consuming subset selection step, which involves computing model-dependent gradients and feature embeddings and applies greedy maximization of submodular objectives. Our key insight is that removing the reliance on downstream model parameters enables subset selection as a pre-processing step and enables one to train multiple models at no additional cost. In this work, we propose MILO, a model-agnostic subset selection framework that decouples the subset selection from model training while enabling superior model convergence and performance by using an easy-to-hard curriculum. Our empirical results in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;Wasserstein&#36317;&#31163;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;Wasserstein&#36317;&#31163;&#27979;&#37327;&#26469;&#22686;&#24378;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#20960;&#31181;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.12197</link><description>&lt;p&gt;
&#24207;&#21015;&#25512;&#33616;&#30340;&#20114;Wasserstein&#36317;&#31163;&#26368;&#23567;&#21270;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Mutual Wasserstein Discrepancy Minimization for Sequential Recommendation. (arXiv:2301.12197v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.12197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;Wasserstein&#36317;&#31163;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#29992;&#20110;&#25552;&#39640;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;Wasserstein&#36317;&#31163;&#27979;&#37327;&#26469;&#22686;&#24378;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#20960;&#31181;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#30417;&#30563;&#30340;&#39034;&#24207;&#25512;&#33616;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#21644;&#35774;&#35745;&#33391;&#22909;&#30340;&#25968;&#25454;&#22686;&#24191;&#26174;&#33879;&#25552;&#39640;&#20102;&#25512;&#33616;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;, &#30446;&#21069;&#30340;&#20114;&#20449;&#24687;&#20272;&#35745;&#22522;&#20110;&#35745;&#31639;Kullback Leibler&#24046;&#24322;&#65292;&#24182;&#19988;&#23384;&#22312;&#35768;&#22810;&#23616;&#38480;&#24615;&#65292;&#21253;&#25324;&#38750;&#23545;&#31216;&#20272;&#35745;&#12289;&#25351;&#25968;&#26679;&#26412;&#22823;&#23567;&#38656;&#27714;&#21644;&#35757;&#32451;&#19981;&#31283;&#23450;&#12290;&#32780;&#20351;&#29992;&#30340;&#29616;&#26377;&#25968;&#25454;&#22686;&#24191;&#22823;&#22810;&#26159;&#38543;&#26426;&#30340;&#65292;&#21487;&#33021;&#20250;&#22240;&#20026;&#38543;&#26426;&#20462;&#25913;&#32780;&#30772;&#22351;&#39034;&#24207;&#30456;&#20851;&#24615;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20114;Wasserstein&#36317;&#31163;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#33258;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;Wasserstein&#36317;&#31163;&#27979;&#37327;&#26469;&#35780;&#20272;&#22686;&#24191;&#24207;&#21015;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#65292;&#36890;&#36807;&#20943;&#23569;&#21407;&#22987;&#21644;&#22686;&#24191;&#24207;&#21015;&#30340;&#27010;&#29575;&#20998;&#24067;&#20043;&#38388;&#30340;&#36317;&#31163;&#65292;&#22686;&#24378;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#12290;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#20960;&#31181;&#39034;&#24207;&#25512;&#33616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Self-supervised sequential recommendation significantly improves recommendation performance by maximizing mutual information with well-designed data augmentations. However, the mutual information estimation is based on the calculation of Kullback Leibler divergence with several limitations, including asymmetrical estimation, the exponential need of the sample size, and training instability. Also, existing data augmentations are mostly stochastic and can potentially break sequential correlations with random modifications. These two issues motivate us to investigate an alternative robust mutual information measurement capable of modeling uncertainty and alleviating KL divergence limitations. To this end, we propose a novel self-supervised learning framework based on Mutual WasserStein discrepancy minimization MStein for the sequential recommendation. We propose the Wasserstein Discrepancy Measurement to measure the mutual information between augmented sequences. Wasserstein Discrepancy M
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22810;&#32500;&#27010;&#24565;&#21457;&#29616;(MCD)&#26041;&#27861;&#65292;&#23427;&#28385;&#36275;&#27010;&#24565;&#23618;&#38754;&#19978;&#30340;&#23436;&#25972;&#24615;&#20851;&#31995;&#65292;&#19981;&#38656;&#35201;&#21152;&#24378;&#27010;&#24565;&#21487;&#35299;&#37322;&#24615;&#25110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#37096;&#20998;&#65292;&#24182;&#25552;&#20379;&#27010;&#24565;&#28608;&#27963;&#22270;&#20998;&#26512;&#24037;&#20855;</title><link>http://arxiv.org/abs/2301.11911</link><description>&lt;p&gt;
&#22810;&#32500;&#27010;&#24565;&#21457;&#29616;(MCD): &#19968;&#20010;&#20855;&#26377;&#23436;&#25972;&#24615;&#20445;&#35777;&#30340;&#32479;&#19968;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Multi-dimensional concept discovery (MCD): A unifying framework with completeness guarantees. (arXiv:2301.11911v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11911
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22810;&#32500;&#27010;&#24565;&#21457;&#29616;(MCD)&#26041;&#27861;&#65292;&#23427;&#28385;&#36275;&#27010;&#24565;&#23618;&#38754;&#19978;&#30340;&#23436;&#25972;&#24615;&#20851;&#31995;&#65292;&#19981;&#38656;&#35201;&#21152;&#24378;&#27010;&#24565;&#21487;&#35299;&#37322;&#24615;&#25110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#37096;&#20998;&#65292;&#24182;&#25552;&#20379;&#27010;&#24565;&#28608;&#27963;&#22270;&#20998;&#26512;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23436;&#25972;&#24615;&#20844;&#29702;&#20351;&#24471;&#21518;&#32493;XAI&#26041;&#27861;&#30340;&#35299;&#37322;&#20165;&#23545;&#27169;&#22411;&#22312;&#21333;&#20010;&#20915;&#31574;&#19978;&#26377;&#25928;&#12290;&#20026;&#20102;&#21487;&#20449;&#22320;&#24212;&#29992;XAI&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#39640;&#39118;&#38505;&#30340;&#20915;&#31574;&#65292;&#38656;&#35201;&#26356;&#20840;&#29699;&#30340;&#27169;&#22411;&#29702;&#35299;&#12290;&#26368;&#36817;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#20445;&#35777;&#19982;&#23454;&#38469;&#30340;&#27169;&#22411;&#25512;&#29702;&#30456;&#32467;&#21512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#32500;&#27010;&#24565;&#21457;&#29616;(MCD)&#65292;&#20316;&#20026;&#20043;&#21069;&#26041;&#27861;&#30340;&#25193;&#23637;&#65292;&#28385;&#36275;&#27010;&#24565;&#23618;&#38754;&#19978;&#30340;&#23436;&#25972;&#24615;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20174;&#36890;&#29992;&#30340;&#32447;&#24615;&#23376;&#31354;&#38388;&#20316;&#20026;&#27010;&#24565;&#24320;&#22987;&#65292;&#24182;&#19981;&#38656;&#35201;&#21152;&#24378;&#27010;&#24565;&#21487;&#35299;&#37322;&#24615;&#25110;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#37096;&#20998;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#31232;&#30095;&#23376;&#31354;&#38388;&#32858;&#31867;&#26469;&#21457;&#29616;&#25913;&#36827;&#30340;&#27010;&#24565;&#65292;&#20805;&#20998;&#21033;&#29992;&#20102;&#22810;&#32500;&#23376;&#31354;&#38388;&#30340;&#28508;&#33021;&#12290;MCD&#25552;&#20379;&#20102;&#20004;&#31181;&#27010;&#24565;&#22312;&#36755;&#20837;&#31354;&#38388;&#20013;&#30340;&#20114;&#34917;&#20998;&#26512;&#24037;&#20855;&#65306;(1)&#27010;&#24565;&#28608;&#27963;&#22270;&#65292;&#26174;&#31034;&#27010;&#24565;&#34920;&#36798;&#30340;&#20301;&#32622;
&lt;/p&gt;
&lt;p&gt;
The completeness axiom renders the explanation of a post-hoc XAI method only locally faithful to the model, i.e. for a single decision. For the trustworthy application of XAI, in particular for high-stake decisions, a more global model understanding is required. Recently, concept-based methods have been proposed, which are however not guaranteed to be bound to the actual model reasoning. To circumvent this problem, we propose Multi-dimensional Concept Discovery (MCD) as an extension of previous approaches that fulfills a completeness relation on the level of concepts. Our method starts from general linear subspaces as concepts and does neither require reinforcing concept interpretability nor re-training of model parts. We propose sparse subspace clustering to discover improved concepts and fully leverage the potential of multi-dimensional subspaces. MCD offers two complementary analysis tools for concepts in input space: (1) concept activation maps, that show where a concept is express
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#36830;&#32493;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#20854;&#37319;&#29992;&#36741;&#21161;&#21464;&#37327;&#26469;&#21306;&#20998;&#35782;&#21035;&#21644;&#21160;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11308</link><description>&lt;p&gt;
&#38024;&#23545;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#36830;&#32493;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Neural Continuous-Discrete State Space Models for Irregularly-Sampled Time Series. (arXiv:2301.11308v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#19981;&#35268;&#21017;&#37319;&#26679;&#26102;&#38388;&#24207;&#21015;&#30340;&#31070;&#32463;&#36830;&#32493;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65292;&#20854;&#37319;&#29992;&#36741;&#21161;&#21464;&#37327;&#26469;&#21306;&#20998;&#35782;&#21035;&#21644;&#21160;&#24577;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#21644;&#25913;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#30495;&#23454;&#19990;&#30028;&#21160;&#24577;&#29616;&#35937;&#65288;&#22914;&#27668;&#20505;&#12289;&#29983;&#29289;&#23398;&#31561;&#65289;&#30340;&#20934;&#30830;&#39044;&#27979;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#19968;&#39033;&#20851;&#38190;&#38382;&#39064;&#26159;&#65292;&#33258;&#28982;&#21644;&#20154;&#24037;&#36807;&#31243;&#29983;&#25104;&#30340;&#25968;&#25454;&#24448;&#24448;&#21253;&#21547;&#19981;&#35268;&#21017;&#37319;&#26679;&#21644;/&#25110;&#32570;&#22833;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#31070;&#32463;&#36830;&#32493;&#31163;&#25955;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;NCDSSM&#65289;&#65292;&#29992;&#20110;&#36890;&#36807;&#31163;&#25955;&#26102;&#38388;&#35266;&#27979;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#36830;&#32493;&#26102;&#38388;&#24314;&#27169;&#12290;NCDSSM&#37319;&#29992;&#36741;&#21161;&#21464;&#37327;&#26469;&#21306;&#20998;&#35782;&#21035;&#21644;&#21160;&#24577;&#65292;&#22240;&#27492;&#20165;&#38656;&#35201;&#23545;&#36741;&#21161;&#21464;&#37327;&#36827;&#34892;&#25674;&#38144;&#25512;&#29702;&#12290;&#21033;&#29992;&#36830;&#32493;-&#31163;&#25955;&#28388;&#27874;&#29702;&#35770;&#30340;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23545;&#21160;&#24577;&#29366;&#24577;&#36827;&#34892;&#20934;&#30830;&#30340;&#36125;&#21494;&#26031;&#25512;&#26029;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#28789;&#27963;&#30340;&#28508;&#22312;&#21160;&#24577;&#21442;&#25968;&#21270;&#26041;&#27861;&#21644;&#19968;&#31181;&#22312;&#25512;&#26029;&#26399;&#38388;&#23545;&#21160;&#24577;&#29366;&#24577;&#36827;&#34892;&#36793;&#32536;&#21270;&#30340;&#39640;&#25928;&#22521;&#35757;&#30446;&#26631;&#12290;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#20102;&#25913;&#36827;&#20102;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning accurate predictive models of real-world dynamic phenomena (e.g., climate, biological) remains a challenging task. One key issue is that the data generated by both natural and artificial processes often comprise time series that are irregularly sampled and/or contain missing observations. In this work, we propose the Neural Continuous-Discrete State Space Model (NCDSSM) for continuous-time modeling of time series through discrete-time observations. NCDSSM employs auxiliary variables to disentangle recognition from dynamics, thus requiring amortized inference only for the auxiliary variables. Leveraging techniques from continuous-discrete filtering theory, we demonstrate how to perform accurate Bayesian inference for the dynamic states. We propose three flexible parameterizations of the latent dynamics and an efficient training objective that marginalizes the dynamic states during inference. Empirical results on multiple benchmark datasets across various domains show improved i
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPANet&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#21435;&#38500;&#21508;&#31181;&#22823;&#23567;&#30340;&#33707;&#23572;&#32441;&#22270;&#26696;&#26469;&#25913;&#21892;&#24674;&#22797;&#36136;&#37327;&#65292;&#37319;&#29992;&#22810;&#20010;&#36830;&#32493;&#24103;&#25552;&#21462;&#24103;&#19981;&#21464;&#20869;&#23481;&#29305;&#24449;&#65292;&#36755;&#20986;&#26102;&#38388;&#19968;&#33268;&#22270;&#20687;&#12290;</title><link>http://arxiv.org/abs/2301.07330</link><description>&lt;p&gt;
FPANet: &#22522;&#20110;&#39057;&#29575;&#30340;&#35270;&#39057;&#21435;&#33707;&#23572;&#32441;&#25216;&#26415;&#65292;&#20351;&#29992;&#24103;&#32423;&#21518;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
FPANet: Frequency-based Video Demoireing using Frame-level Post Alignment. (arXiv:2301.07330v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07330
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPANet&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#21435;&#38500;&#21508;&#31181;&#22823;&#23567;&#30340;&#33707;&#23572;&#32441;&#22270;&#26696;&#26469;&#25913;&#21892;&#24674;&#22797;&#36136;&#37327;&#65292;&#37319;&#29992;&#22810;&#20010;&#36830;&#32493;&#24103;&#25552;&#21462;&#24103;&#19981;&#21464;&#20869;&#23481;&#29305;&#24449;&#65292;&#36755;&#20986;&#26102;&#38388;&#19968;&#33268;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37325;&#21472;&#32593;&#26684;&#27169;&#24335;&#20043;&#38388;&#30340;&#24178;&#25200;&#20250;&#23548;&#33268;&#33707;&#23572;&#32441;&#65292;&#20174;&#32780;&#38477;&#20302;&#26222;&#36890;&#25968;&#30721;&#30456;&#26426;&#25429;&#25417;&#25968;&#23383;&#26174;&#31034;&#23631;&#30340;&#22270;&#20687;&#30340;&#35270;&#35273;&#36136;&#37327;&#12290;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FPANet&#30340;&#26032;&#27169;&#22411;&#65292;&#23427;&#23398;&#20064;&#39057;&#29575;&#21644;&#31354;&#38388;&#22495;&#20013;&#30340;&#28388;&#27874;&#22120;&#65292;&#36890;&#36807;&#21435;&#38500;&#21508;&#31181;&#22823;&#23567;&#30340;&#33707;&#23572;&#32441;&#22270;&#26696;&#26469;&#25913;&#21892;&#24674;&#22797;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#27169;&#22411;&#20351;&#29992;&#22810;&#20010;&#36830;&#32493;&#24103;&#65292;&#23398;&#20064;&#25552;&#21462;&#24103;&#19981;&#21464;&#20869;&#23481;&#29305;&#24449;&#65292;&#24182;&#36755;&#20986;&#26356;&#22909;&#36136;&#37327;&#30340;&#26102;&#38388;&#19968;&#33268;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interference between overlapping gird patterns creates moire patterns, degrading the visual quality of an image that captures a screen of a digital display device by an ordinary digital camera. Removing such moire patterns is challenging due to their complex patterns of diverse sizes and color distortions. Existing approaches mainly focus on filtering out in the spatial domain, failing to remove a large-scale moire pattern. In this paper, we propose a novel model called FPANet that learns filters in both frequency and spatial domains, improving the restoration quality by removing various sizes of moire patterns. To further enhance, our model takes multiple consecutive frames, learning to extract frame-invariant content features and outputting better quality temporally consistent images. We demonstrate the effectiveness of our proposed method with a publicly available large-scale dataset, observing that ours outperforms the state-of-the-art approaches, including ESDNet, VDmoire, MBCNN, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#21363;&#35745;&#31639;&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;DNN&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20998;&#21035;&#32473;&#20986;&#20102;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#21644;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#24182;&#22312;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;</title><link>http://arxiv.org/abs/2301.07068</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#23433;&#20840;&#36755;&#20837;&#35745;&#25968;&#30340;#DNN-Verification&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The #DNN-Verification problem: Counting Unsafe Inputs for Deep Neural Networks. (arXiv:2301.07068v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07068
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#21363;&#35745;&#31639;&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;DNN&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#30340;&#38382;&#39064;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#21644;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#20998;&#21035;&#32473;&#20986;&#20102;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#21644;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#24182;&#22312;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#38656;&#35201;&#39640;&#24230;&#23433;&#20840;&#24615;&#30340;&#20851;&#38190;&#20219;&#21153;&#20013;&#65292;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#20013;&#36234;&#26469;&#36234;&#34987;&#37319;&#29992;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#22120;&#21487;&#20197;&#29992;&#26469;&#26816;&#26597;DNN&#26159;&#21542;&#19981;&#23433;&#20840;&#65292;&#21363;&#26159;&#21542;&#23384;&#22312;&#33267;&#23569;&#19968;&#31181;&#19981;&#23433;&#20840;&#30340;&#36755;&#20837;&#37197;&#32622;&#65292;&#20294;&#23427;&#20204;&#30340;&#26159;/&#21542;&#36755;&#20986;&#23545;&#20110;&#20854;&#20182;&#30446;&#30340;&#65288;&#22914;&#23631;&#34109;&#12289;&#27169;&#22411;&#36873;&#25321;&#25110;&#22521;&#35757;&#25913;&#36827;&#65289;&#30340;&#20449;&#24687;&#19981;&#36275;&#22815;&#35814;&#32454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;#DNN-Verification&#38382;&#39064;&#65292;&#23427;&#28041;&#21450;&#35745;&#31639;&#23548;&#33268;DNN&#36829;&#21453;&#29305;&#23450;&#23433;&#20840;&#24615;&#36136;&#30340;&#36755;&#20837;&#37197;&#32622;&#25968;&#37327;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#23427;&#36820;&#22238;&#30830;&#20999;&#30340;&#36829;&#35268;&#35745;&#25968;&#12290;&#30001;&#20110;&#35813;&#38382;&#39064;&#30340;#P&#23436;&#22791;&#24615;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#30340;&#36817;&#20284;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25552;&#20379;&#20102;&#27491;&#30830;&#35745;&#25968;&#30340;&#21487;&#35777;&#26126;&#27010;&#29575;&#30028;&#65292;&#21516;&#26102;&#26174;&#33879;&#38477;&#20302;&#20102;&#35745;&#31639;&#35201;&#27714;&#12290;&#25105;&#20204;&#22312;&#19968;&#32452;&#23433;&#20840;&#20851;&#38190;&#22522;&#20934;&#27979;&#35797;&#19978;&#21576;&#29616;&#20102;&#23454;&#39564;&#32467;&#26524;&#65292;&#27604;&#36739;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#26368;&#20808;&#36827;&#30340;&#39564;&#35777;&#22120;&#21644;&#22522;&#20110;&#35745;&#25968;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep Neural Networks are increasingly adopted in critical tasks that require a high level of safety, e.g., autonomous driving. While state-of-the-art verifiers can be employed to check whether a DNN is unsafe w.r.t. some given property (i.e., whether there is at least one unsafe input configuration), their yes/no output is not informative enough for other purposes, such as shielding, model selection, or training improvements. In this paper, we introduce the #DNN-Verification problem, which involves counting the number of input configurations of a DNN that result in a violation of a particular safety property. We analyze the complexity of this problem and propose a novel approach that returns the exact count of violations. Due to the #P-completeness of the problem, we also propose a randomized, approximate method that provides a provable probabilistic bound of the correct count while significantly reducing computational requirements. We present experimental results on a set of safety-cr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.05599</link><description>&lt;p&gt;
&#22522;&#20110;&#23545;&#25239;&#29983;&#25104;&#32593;&#32476;&#30340;&#30701;SSVEP&#25968;&#25454;&#25193;&#23637;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Short-length SSVEP data extension by a novel generative adversarial networks based framework. (arXiv:2301.05599v3 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05599
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GAN&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;SSVEP&#30340;&#33041;&#26426;&#25509;&#21475;&#22240;&#20854;&#39640;&#20449;&#24687;&#20256;&#36755;&#36895;&#29575;&#21644;&#30446;&#26631;&#25968;&#37327;&#21487;&#29992;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#39057;&#29575;&#35782;&#21035;&#26041;&#27861;&#30340;&#24615;&#33021;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#29992;&#25143;&#26657;&#20934;&#25968;&#25454;&#30340;&#25968;&#37327;&#21644;&#25968;&#25454;&#38271;&#24230;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GANs&#65289;&#30340;&#25968;&#25454;&#29983;&#25104;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#37319;&#29992;&#26469;&#21019;&#24314;&#21512;&#25104;&#30340;&#33041;&#30005;&#25968;&#25454;&#65292;&#26377;&#26395;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;GANs&#30340;&#31471;&#21040;&#31471;&#20449;&#21495;&#36716;&#21270;&#32593;&#32476;TEGAN&#65292;&#29992;&#20110;&#25968;&#25454;&#38271;&#24230;&#25193;&#23637;&#12290;TEGAN&#21487;&#20197;&#23558;&#30701;SSVEP&#20449;&#21495;&#36716;&#25442;&#25104;&#38271;&#30340;&#20154;&#24037;SSVEP&#20449;&#21495;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;&#26032;&#39062;&#30340;U&#22411;&#29983;&#25104;&#22120;&#26550;&#26500;&#21644;&#19968;&#20010;&#36741;&#21161;&#20998;&#31867;&#22120;&#21152;&#20837;&#21040;&#32593;&#32476;&#32467;&#26500;&#20013;&#65292;TEGAN&#21487;&#20197;&#22312;&#21512;&#25104;&#25968;&#25454;&#20013;&#20135;&#29983;&#26377;&#26465;&#20214;&#30340;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23454;&#29616;&#24182;&#27604;&#36739;&#20102;&#20004;&#31181;&#26368;&#20808;&#36827;&#30340;&#39057;&#29575;&#35782;&#21035;&#26041;&#27861;&#65292;&#20197;&#35780;&#20272;TEGAN&#29983;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;TEGAN&#26041;&#27861;&#20248;&#20110;&#20256;&#32479;&#30340;&#32447;&#24615;&#25554;&#20540;&#26041;&#27861;&#21644;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;TEGAN&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;BCI&#31995;&#32479;&#30340;&#25928;&#29575;&#65292;&#20943;&#23569;&#25152;&#38656;&#30340;&#26657;&#20934;&#26102;&#38388;&#24182;&#25913;&#21892;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steady-state visual evoked potentials (SSVEPs) based brain-computer interface (BCI) has received considerable attention due to its high information transfer rate (ITR) and available quantity of targets. However, the performance of frequency identification methods heavily hinges on the amount of user calibration data and data length, which hinders the deployment in real-world applications. Recently, generative adversarial networks (GANs)-based data generation methods have been widely adopted to create synthetic electroencephalography (EEG) data, holds promise to address these issues. In this paper, we proposed a GAN-based end-to-end signal transformation network for data length extension, termed as TEGAN. TEGAN transforms short-length SSVEP signals into long-length artificial SSVEP signals. By incorporating a novel U-Net generator architecture and an auxiliary classifier into the network architecture, the TEGAN could produce conditioned features in the synthetic data. Additionally, we i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25345;&#20037;&#24615;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;Wasserstein&#36317;&#31163;&#26469;&#32416;&#27491;&#25345;&#20037;&#24615;&#20998;&#25968;&#20013;&#36807;&#24230;&#30340;&#20559;&#21521;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.05041</link><description>&lt;p&gt;
&#22522;&#20110;&#25345;&#20037;&#24615;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#29992;&#20110;&#20174;&#26102;&#38388;&#24207;&#21015;&#23398;&#20064;&#31163;&#25955;&#20107;&#20214;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Persistence-Based Discretization for Learning Discrete Event Systems from Time Series. (arXiv:2301.05041v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05041
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25345;&#20037;&#24615;&#30340;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;Wasserstein&#36317;&#31163;&#26469;&#32416;&#27491;&#25345;&#20037;&#24615;&#20998;&#25968;&#20013;&#36807;&#24230;&#30340;&#20559;&#21521;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#28145;&#20837;&#20102;&#35299;&#19968;&#20010;&#21160;&#24577;&#31995;&#32479;&#65292;&#25317;&#26377;&#19968;&#20010;&#21487;&#35299;&#37322;&#21644;&#22810;&#21151;&#33021;&#30340;&#27169;&#22411;&#26159;&#24456;&#26041;&#20415;&#30340;&#12290;&#23450;&#26102;&#31163;&#25955;&#20107;&#20214;&#31995;&#32479;&#23601;&#26159;&#36825;&#26679;&#19968;&#31181;&#27169;&#22411;&#65292;&#23427;&#20204;&#28385;&#36275;&#36825;&#20123;&#35201;&#27714;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#27169;&#22411;&#21482;&#33021;&#20174;&#26102;&#38388;&#25139;&#20107;&#20214;&#24207;&#21015;&#25512;&#26029;&#20986;&#26469;&#65292;&#32780;&#19981;&#33021;&#30452;&#25509;&#20174;&#25968;&#23383;&#25968;&#25454;&#20013;&#25512;&#26029;&#20986;&#26469;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24517;&#39035;&#36827;&#34892;&#31163;&#25955;&#21270;&#27493;&#39588;&#65292;&#20197;&#35782;&#21035;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20107;&#20214;&#25110;&#31526;&#21495;&#12290;Persist&#26159;&#19968;&#31181;&#31163;&#25955;&#21270;&#26041;&#27861;&#65292;&#26088;&#22312;&#20351;&#29992;&#31216;&#20026;&#25345;&#20037;&#24615;&#20998;&#25968;&#30340;&#24471;&#20998;&#26469;&#21019;&#24314;&#25345;&#20037;&#24615;&#31526;&#21495;&#12290;&#36825;&#26679;&#21487;&#20197;&#20943;&#36731;&#38750;&#26399;&#26395;&#31526;&#21495;&#21464;&#21270;&#30340;&#39118;&#38505;&#65292;&#20174;&#32780;&#23548;&#33268;&#27169;&#22411;&#36807;&#20110;&#22797;&#26434;&#12290;&#32463;&#36807;&#23545;&#25345;&#20037;&#24615;&#20998;&#25968;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#21457;&#29616;&#23427;&#24448;&#24448;&#20559;&#21521;&#20110;&#36807;&#24230;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#38169;&#36807;&#20102;&#26377;&#36259;&#30340;&#25345;&#20037;&#24615;&#31526;&#21495;&#12290;&#20026;&#20102;&#32416;&#27491;&#36825;&#31181;&#34892;&#20026;&#65292;&#25105;&#20204;&#29992;Wasserstein&#36317;&#31163;&#26367;&#25442;&#20102;&#25345;&#20037;&#24615;&#20998;&#25968;&#20013;&#20351;&#29992;&#30340;&#25351;&#26631;&#65292;&#21363;Kullback-Leibler&#20998;&#27495;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#25913;&#36827;&#21518;&#30340;&#25345;&#20037;&#24615;&#20998;&#25968;&#22686;&#24378;&#20102;Persist&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
To get a good understanding of a dynamical system, it is convenient to have an interpretable and versatile model of it. Timed discrete event systems are a kind of model that respond to these requirements. However, such models can be inferred from timestamped event sequences but not directly from numerical data. To solve this problem, a discretization step must be done to identify events or symbols in the time series. Persist is a discretization method that intends to create persisting symbols by using a score called persistence score. This allows to mitigate the risk of undesirable symbol changes that would lead to a too complex model. After the study of the persistence score, we point out that it tends to favor excessive cases making it miss interesting persisting symbols. To correct this behavior, we replace the metric used in the persistence score, the Kullback-Leibler divergence, with the Wasserstein distance. Experiments show that the improved persistence score enhances Persist's 
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#22823;&#22411;&#24179;&#21488;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#26576;&#20123;&#24230;&#37327;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#34920;&#29616;&#19981;&#20339;&#19988;&#20854;&#26377;&#29992;&#24615;&#19982;&#19979;&#28216;&#20219;&#21153;&#26377;&#20851;&#12290;</title><link>http://arxiv.org/abs/2212.10297</link><description>&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#30340;&#22806;&#22312;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Extrinsic Evaluation of Machine Translation Metrics. (arXiv:2212.10297v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10297
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#30740;&#31350;&#20102;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#22312;&#22823;&#22411;&#24179;&#21488;&#21644;&#19979;&#28216;&#20219;&#21153;&#20013;&#30340;&#21487;&#38752;&#24615;&#65292;&#21457;&#29616;&#26576;&#20123;&#24230;&#37327;&#22312;&#21477;&#23376;&#32423;&#21035;&#19978;&#34920;&#29616;&#19981;&#20339;&#19988;&#20854;&#26377;&#29992;&#24615;&#19982;&#19979;&#28216;&#20219;&#21153;&#26377;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#26426;&#22120;&#32763;&#35793;&#24230;&#37327;&#36890;&#24120;&#29992;&#20110;&#22312;&#36739;&#22823;&#30340;&#27979;&#35797;&#38598;&#19978;&#27604;&#36739;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#30340;&#32763;&#35793;&#36136;&#37327;&#65288;&#31995;&#32479;&#32423;&#35780;&#20272;&#65289;&#65292;&#20294;&#26159;&#65292;&#21477;&#23376;&#32423;&#21035;&#19978;&#33258;&#21160;&#24230;&#37327;&#26159;&#21542;&#33021;&#21487;&#38752;&#22320;&#21306;&#20998;&#22909;&#32763;&#35793;&#21644;&#24046;&#32763;&#35793;&#20173;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#22312;&#20855;&#26377;&#19979;&#28216;&#20219;&#21153;&#30340;&#22823;&#22411;&#24179;&#21488;&#19978;&#25918;&#32622;&#26426;&#22120;&#32763;&#35793;&#32452;&#20214;&#20197;&#26816;&#27979;&#20854;&#25104;&#21151;&#30340;MT&#24230;&#37327;&#30340;&#26377;&#29992;&#24615;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#36328;&#35821;&#35328;&#19979;&#28216;&#20219;&#21153;&#65288;&#23545;&#35805;&#29366;&#24577;&#36319;&#36394;&#65292;&#38382;&#39064;&#22238;&#31572;&#21644;&#35821;&#20041;&#35299;&#26512;&#65289;&#19978;&#35780;&#20272;&#20102;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;MT&#37327;&#24230;&#65288;chrF&#65292;COMET&#65292;BERTScore&#31561;&#65289;&#30340;&#20998;&#27573;&#24615;&#33021;&#12290;&#23545;&#20110;&#27599;&#20010;&#20219;&#21153;&#65292;&#25105;&#20204;&#20165;&#33021;&#35775;&#38382;&#21333;&#35821;&#31181;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#25105;&#20204;&#35745;&#31639;&#22312;Translate-Test&#35774;&#32622;&#19979;&#65292;&#24230;&#37327;&#39044;&#27979;&#22909;/&#22351;&#32763;&#35793;&#33021;&#21147;&#19982;&#26368;&#32456;&#20219;&#21153;&#25104;&#21151;/&#22833;&#36133;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#23613;&#31649;&#26576;&#20123;&#24230;&#37327;&#22312;&#31995;&#32479;&#32423;&#35780;&#20272;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#20998;&#27573;&#35780;&#20272;&#20013;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;&#27492;&#22806;&#65292;&#26576;&#20123;&#24230;&#37327;&#30340;&#26377;&#29992;&#24615;&#21462;&#20915;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic machine translation (MT) metrics are widely used to distinguish the translation qualities of machine translation systems across relatively large test sets (system-level evaluation). However, it is unclear if automatic metrics are reliable at distinguishing good translations from bad translations at the sentence level (segment-level evaluation). In this paper, we investigate how useful MT metrics are at detecting the success of a machine translation component when placed in a larger platform with a downstream task. We evaluate the segment-level performance of the most widely used MT metrics (chrF, COMET, BERTScore, etc.) on three downstream cross-lingual tasks (dialogue state tracking, question answering, and semantic parsing). For each task, we only have access to a monolingual task-specific model. We calculate the correlation between the metric's ability to predict a good/bad translation with the success/failure on the final task for the Translate-Test setup. Our experiments
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26080;&#32447;&#36793;&#32536;&#22788;&#32531;&#23384;&#20869;&#23481;&#20197;&#20943;&#36731;&#31227;&#21160;&#32593;&#32476;&#23545;&#26680;&#24515;&#32593;&#32476;&#21644;&#22238;&#31243;&#38142;&#36335;&#24102;&#26469;&#30340;&#36127;&#25285;&#12290;&#36890;&#36807;&#26080;&#20241;&#27490;&#36172;&#21338;&#31639;&#27861;&#23558;&#38382;&#39064;&#26368;&#23567;&#21270;&#12290;</title><link>http://arxiv.org/abs/2212.03291</link><description>&lt;p&gt;
&#26080;&#20241;&#27490;&#36172;&#21338;&#31639;&#27861;&#22312;&#32531;&#23384;&#19981;&#26029;&#21464;&#21270;&#30340;&#27969;&#34892;&#24230;&#20869;&#23481;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Caching Contents with Varying Popularity using Restless Bandits. (arXiv:2212.03291v3 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.03291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#26080;&#32447;&#36793;&#32536;&#22788;&#32531;&#23384;&#20869;&#23481;&#20197;&#20943;&#36731;&#31227;&#21160;&#32593;&#32476;&#23545;&#26680;&#24515;&#32593;&#32476;&#21644;&#22238;&#31243;&#38142;&#36335;&#24102;&#26469;&#30340;&#36127;&#25285;&#12290;&#36890;&#36807;&#26080;&#20241;&#27490;&#36172;&#21338;&#31639;&#27861;&#23558;&#38382;&#39064;&#26368;&#23567;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#32593;&#32476;&#23545;&#25968;&#25454;&#37327;&#21644;&#29992;&#25143;&#23494;&#24230;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#38271;&#65292;&#36825;&#32473;&#31227;&#21160;&#26680;&#24515;&#32593;&#32476;&#21644;&#22238;&#31243;&#38142;&#36335;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#36127;&#25285;&#12290;&#19968;&#31181;&#26377;&#25928;&#30340;&#25216;&#26415;&#26159;&#20351;&#29992;&#32531;&#23384;&#65292;&#21363;&#21033;&#29992;&#36793;&#32536;&#32593;&#32476;&#33410;&#28857;&#30340;&#32531;&#23384;&#65288;&#22914;&#22266;&#23450;&#25110;&#31227;&#21160;&#25509;&#20837;&#28857;&#29978;&#33267;&#29992;&#25143;&#35774;&#22791;&#65289;&#23558;&#25968;&#25454;&#38752;&#36817;&#29992;&#25143;&#12290;&#32531;&#23384;&#30340;&#24615;&#33021;&#21462;&#20915;&#20110;&#32531;&#23384;&#20869;&#23481;&#12290;&#26412;&#25991;&#30740;&#31350;&#22312;&#26080;&#32447;&#36793;&#32536;&#65288;&#21363;&#22522;&#31449;&#65289;&#22788;&#32531;&#23384;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#20197;&#26368;&#23567;&#21270;&#26080;&#38480;&#35270;&#37326;&#19979;&#20135;&#29983;&#30340;&#36148;&#29616;&#25104;&#26412;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#38382;&#39064;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#26080;&#20241;&#27490;&#30340;&#36172;&#21338;&#38382;&#39064;&#65292;&#35813;&#38382;&#39064;&#24456;&#38590;&#27714;&#35299;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#20248;&#21270;&#31574;&#30053;&#26159;&#38408;&#20540;&#22411;&#30340;&#12290;&#21033;&#29992;&#36825;&#20123;&#32467;&#26500;&#24615;&#32467;&#26524;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#38382;&#39064;&#30340;&#21487;&#25351;&#26631;&#24615;&#65292;&#24182;&#20351;&#29992;Whittle&#25351;&#25968;&#31574;&#30053;&#20197;&#26368;&#23567;&#21270;&#36148;&#29616;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mobile networks are experiencing prodigious increase in data volume and user density , which exerts a great burden on mobile core networks and backhaul links. An efficient technique to lessen this problem is to use caching i.e. to bring the data closer to the users by making use of the caches of edge network nodes, such as fixed or mobile access points and even user devices. The performance of a caching depends on contents that are cached. In this paper, we examine the problem of content caching at the wireless edge(i.e. base stations) to minimize the discounted cost incurred over infinite horizon. We formulate this problem as a restless bandit problem, which is hard to solve. We begin by showing an optimal policy is of threshold type. Using these structural results, we prove the indexability of the problem, and use Whittle index policy to minimize the discounted cost.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#33539;&#30068;&#35770;&#25506;&#31350;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26368;&#23567;&#25152;&#38656;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#35299;&#20915;&#21069;&#32622;&#20219;&#21153;&#25152;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#65292;&#21482;&#35201;&#20801;&#35768;&#24494;&#35843;&#19988;&#19979;&#28216;&#20219;&#21153;&#21487;&#22312;&#21069;&#32622;&#20219;&#21153;&#23450;&#20041;&#30340;&#33539;&#30068;&#20013;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2211.16327</link><description>&lt;p&gt;
&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#25506;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Power of Foundation Models. (arXiv:2211.16327v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.16327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#33539;&#30068;&#35770;&#25506;&#31350;&#20102;&#22522;&#30784;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#26368;&#23567;&#25152;&#38656;&#33021;&#21147;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#35299;&#20915;&#21069;&#32622;&#20219;&#21153;&#25152;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#65292;&#24182;&#19988;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#25193;&#23637;&#21040;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#65292;&#21482;&#35201;&#20801;&#35768;&#24494;&#35843;&#19988;&#19979;&#28216;&#20219;&#21153;&#21487;&#22312;&#21069;&#32622;&#20219;&#21153;&#23450;&#20041;&#30340;&#33539;&#30068;&#20013;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#26524;&#22522;&#30784;&#27169;&#22411;&#20855;&#26377;&#26080;&#38480;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#28857;&#12289;&#26080;&#38480;&#35745;&#31639;&#33021;&#21147;&#12289;&#19968;&#20010;&#26080;&#38480;&#22823;&#30340;&#23436;&#32654;&#35757;&#32451;&#31639;&#27861;&#12289;&#20197;&#21450;&#22312;&#39044;&#35774;&#20219;&#21153;&#19978;&#20445;&#35777;&#38646;&#27867;&#21270;&#35823;&#24046;&#65292;&#37027;&#20040;&#23427;&#21487;&#20197;&#29992;&#20110;&#19968;&#20999;&#21527;&#65311;&#20256;&#32479;&#30340;&#34920;&#31034;&#12289;&#20248;&#21270;&#25110;&#27867;&#21270;&#29702;&#35770;&#26080;&#27861;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20027;&#35201;&#25506;&#35752;&#30340;&#38382;&#39064;&#22312;&#36825;&#37324;&#37117;&#26159;&#19981;&#23384;&#22312;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#33539;&#30068;&#35770;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#29702;&#35770;&#24037;&#20855;&#65292;&#20197;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#19977;&#20010;&#32467;&#26524;&#65292;&#31532;&#19968;&#20010;&#38480;&#21046;&#20102;&#22522;&#20110;&#25552;&#31034;&#30340;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#21363;&#20165;&#24403;&#20219;&#21153;&#21487;&#34920;&#31034;&#26102;&#65292;&#27169;&#22411;&#25165;&#33021;&#29992;&#25552;&#31034;&#35299;&#20915;&#19979;&#28216;&#20219;&#21153;&#65307;&#31532;&#20108;&#20010;&#32467;&#26524;&#34920;&#26126;&#65292;&#24494;&#35843;&#19981;&#21463;&#36825;&#20010;&#38480;&#21046;&#65292;&#22240;&#20026;&#19968;&#20010;&#20855;&#26377;&#26368;&#23567;&#25152;&#38656;&#33021;&#21147;&#65288;&#23545;&#31216;&#24615;&#65289;&#30340;&#22522;&#30784;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#24494;&#35843;&#21644;&#36275;&#22815;&#30340;&#36164;&#28304;&#26469;&#29702;&#35770;&#19978;&#35299;&#20915;&#21069;&#32622;&#20219;&#21153;&#25152;&#23450;&#20041;&#30340;&#31867;&#21035;&#20013;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26368;&#32456;&#32467;&#26524;&#21487;&#20197;&#30475;&#20316;&#26159;&#31532;&#20108;&#20010;&#32467;&#26524;&#30340;&#19968;&#33324;&#21270;&#65292;&#34920;&#26126;&#22914;&#26524;&#20801;&#35768;&#24494;&#35843;&#24182;&#19988;&#19979;&#28216;&#20219;&#21153;&#21487;&#22312;&#21069;&#32622;&#20219;&#21153;&#23450;&#20041;&#30340;&#33539;&#30068;&#20013;&#34920;&#31034;&#65292;&#21017;&#22522;&#30784;&#27169;&#22411;&#30340;&#26368;&#23567;&#33021;&#21147;&#20063;&#36275;&#20197;&#35299;&#20915;&#20219;&#20309;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
With infinitely many high-quality data points, infinite computational power, an infinitely large foundation model with a perfect training algorithm and guaranteed zero generalization error on the pretext task, can the model be used for everything? This question cannot be answered by the existing theory of representation, optimization or generalization, because the issues they mainly investigate are assumed to be nonexistent here. In this paper, we show that category theory provides powerful machinery to answer this question. We have proved three results. The first one limits the power of prompt-based learning, saying that the model can solve a downstream task with prompts if and only if the task is representable. The second one says fine tuning does not have this limit, as a foundation model with the minimum required power (up to symmetry) can theoretically solve downstream tasks for the category defined by pretext task, with fine tuning and enough resources. Our final result can be se
&lt;/p&gt;</description></item><item><title>SegCLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#32858;&#38598;&#34917;&#19969;&#21040;&#35821;&#20041;&#21306;&#22495;&#36827;&#34892;&#20998;&#21106;&#65292;&#20855;&#26377;&#21160;&#24577;&#25429;&#25417;&#35821;&#20041;&#32452;&#30340;&#29305;&#28857;&#12290;</title><link>http://arxiv.org/abs/2211.14813</link><description>&lt;p&gt;
SegCLIP&#65306;&#22522;&#20110;&#21487;&#23398;&#20064;&#20013;&#24515;&#30340;&#34917;&#19969;&#32858;&#21512;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation. (arXiv:2211.14813v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14813
&lt;/p&gt;
&lt;p&gt;
SegCLIP&#26159;&#19968;&#31181;&#29992;&#20110;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#32858;&#38598;&#34917;&#19969;&#21040;&#35821;&#20041;&#21306;&#22495;&#36827;&#34892;&#20998;&#21106;&#65292;&#20855;&#26377;&#21160;&#24577;&#25429;&#25417;&#35821;&#20041;&#32452;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451; (&#22914;CLIP) &#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#24456;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23398;&#20064;&#22823;&#37327;&#30340;&#25991;&#26412;-&#22270;&#20687;&#25968;&#25454;&#26469;&#25429;&#25417;&#22270;&#20687;&#20013;&#20016;&#23500;&#30340;&#35270;&#35273;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#23558;&#25152;&#23398;&#30340;&#35270;&#35273;&#30693;&#35782;&#36716;&#31227;&#21040;&#24320;&#25918;&#35789;&#27719;&#35821;&#20041;&#20998;&#21106;&#20173;&#28982;&#19981;&#22815;&#20805;&#20998;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;CLIP&#30340;&#27169;&#22411;SegCLIP&#65292;&#29992;&#20110;&#26080;&#27880;&#37322;&#30340;&#24320;&#25918;&#35789;&#27719;&#20998;&#21106;&#12290;SegCLIP&#21033;&#29992;&#21487;&#23398;&#20064;&#20013;&#24515;&#32858;&#38598;&#34917;&#19969;&#21040;&#35821;&#20041;&#21306;&#22495;&#36890;&#36807;&#25991;&#26412;-&#22270;&#20687;&#23545;&#30340;&#35757;&#32451;&#12290;&#32858;&#38598;&#25805;&#20316;&#21487;&#20197;&#21160;&#24577;&#22320;&#25429;&#25417;&#35821;&#20041;&#32452;&#65292;&#29992;&#20110;&#20135;&#29983;&#26368;&#32456;&#30340;&#20998;&#21106;&#32467;&#26524;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#36974;&#32617;&#34917;&#19969;&#30340;&#37325;&#26500;&#25439;&#22833;&#21644;&#19968;&#20010;&#22522;&#20110;&#36229;&#20687;&#32032;&#30340;KL&#25439;&#22833;&#65292;&#20197;&#22686;&#24378;&#35270;&#35273;&#34920;&#31034;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;
&lt;/p&gt;
&lt;p&gt;
Recently, the contrastive language-image pre-training, e.g., CLIP, has demonstrated promising results on various downstream tasks. The pre-trained model can capture enriched visual concepts for images by learning from a large scale of text-image data. However, transferring the learned visual knowledge to open-vocabulary semantic segmentation is still under-explored. In this paper, we propose a CLIP-based model named SegCLIP for the topic of open-vocabulary segmentation in an annotation-free manner. The SegCLIP achieves segmentation based on ViT and the main idea is to gather patches with learnable centers to semantic regions through training on text-image pairs. The gathering operation can dynamically capture the semantic groups, which can be used to generate the final segmentation results. We further propose a reconstruction loss on masked patches and a superpixel-based KL loss with pseudo-labels to enhance the visual representation. Experimental results show that our model achieves c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32972;&#26223;&#28151;&#21512;&#22686;&#24378;&#21644;&#24369;&#30417;&#30563;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21464;&#21270;&#26816;&#27979;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2211.11478</link><description>&lt;p&gt;
&#32972;&#26223;&#28151;&#21512;&#22686;&#24378;&#29992;&#20110;&#24369;&#30417;&#30563;&#21464;&#21270;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Background-Mixed Augmentation for Weakly Supervised Change Detection. (arXiv:2211.11478v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.11478
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32972;&#26223;&#28151;&#21512;&#22686;&#24378;&#21644;&#24369;&#30417;&#30563;&#31639;&#27861;&#65292;&#25104;&#21151;&#22320;&#35299;&#20915;&#20102;&#21464;&#21270;&#26816;&#27979;&#30340;&#27867;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#21270;&#26816;&#27979;&#26159;&#20174;&#20004;&#20010;&#22312;&#21516;&#19968;&#22330;&#26223;&#20013;&#25293;&#25668;&#30340;&#22270;&#20687;&#20013;&#20998;&#31163;&#20986;&#29289;&#20307;&#21464;&#21270;&#65288;&#22914;&#29289;&#20307;&#30340;&#32570;&#22833;&#25110;&#20986;&#29616;&#65289;&#19982;&#32972;&#26223;&#21464;&#21270;&#65288;&#22914;&#20809;&#32447;&#21644;&#23395;&#33410;&#21464;&#21270;&#65289;&#30340;&#36807;&#31243;&#65292;&#22312;&#28798;&#38590;&#31649;&#29702;&#12289;&#22478;&#24066;&#21457;&#23637;&#31561;&#26041;&#38754;&#20855;&#26377;&#37325;&#35201;&#24212;&#29992;&#12290;&#26412;&#25991;&#20174;&#25968;&#25454;&#22686;&#24378;&#30340;&#35282;&#24230;&#30740;&#31350;&#20102;&#21464;&#21270;&#26816;&#27979;&#30340;&#27867;&#21270;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#24369;&#30417;&#30563;&#35757;&#32451;&#31639;&#27861;&#65292;&#21482;&#38656;&#35201;&#22270;&#20687;&#32423;&#21035;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
Change detection (CD) is to decouple object changes (i.e., object missing or appearing) from background changes (i.e., environment variations) like light and season variations in two images captured in the same scene over a long time span, presenting critical applications in disaster management, urban development, etc. In particular, the endless patterns of background changes require detectors to have a high generalization against unseen environment variations, making this task significantly challenging. Recent deep learning-based methods develop novel network architectures or optimization strategies with paired-training examples, which do not handle the generalization issue explicitly and require huge manual pixel-level annotation efforts. In this work, for the first attempt in the CD community, we study the generalization issue of CD from the perspective of data augmentation and develop a novel weakly supervised training algorithm that only needs image-level labels. Different from ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;LIBO&#65292;&#21487;&#20197;&#26080;&#38656;&#30452;&#25509;&#35775;&#38382;&#25968;&#25454;&#65292;&#23545;&#19968;&#31995;&#21015;&#36172;&#21338;&#20248;&#21270;&#20219;&#21153;&#36827;&#34892;&#23398;&#20064;&#21644;&#36866;&#24212;&#65292;&#24182;&#20445;&#35777;&#26368;&#20248;&#24615;&#33021;&#21644;&#20122;&#32447;&#24615;&#32456;&#36523;&#21518;&#24724;&#29575;&#12290;</title><link>http://arxiv.org/abs/2210.15513</link><description>&lt;p&gt;
&#32456;&#36523;&#36172;&#21338;&#20248;&#21270;&#65306;&#26080;&#20808;&#39564;&#30693;&#35782;&#21644;&#26080;&#21518;&#24724;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Lifelong Bandit Optimization: No Prior and No Regret. (arXiv:2210.15513v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.15513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31639;&#27861;LIBO&#65292;&#21487;&#20197;&#26080;&#38656;&#30452;&#25509;&#35775;&#38382;&#25968;&#25454;&#65292;&#23545;&#19968;&#31995;&#21015;&#36172;&#21338;&#20248;&#21270;&#20219;&#21153;&#36827;&#34892;&#23398;&#20064;&#21644;&#36866;&#24212;&#65292;&#24182;&#20445;&#35777;&#26368;&#20248;&#24615;&#33021;&#21644;&#20122;&#32447;&#24615;&#32456;&#36523;&#21518;&#24724;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#32463;&#24120;&#37325;&#22797;&#24212;&#29992;&#20110;&#30456;&#20284;&#32467;&#26500;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#20851;&#27880;&#35299;&#20915;&#19968;&#31995;&#21015;&#36172;&#21338;&#20248;&#21270;&#20219;&#21153;&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#31181;&#36866;&#24212;&#29615;&#22659;&#30340;&#31639;&#27861;LIBO&#12290;&#25105;&#20204;&#20551;&#35774;&#20869;&#26680;&#21270;&#32467;&#26500;&#65292;&#20854;&#20013;&#30340;&#20869;&#26680;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#26159;&#26410;&#30693;&#30340;&#20294;&#20849;&#20139;&#30340;&#12290;LIBO&#20381;&#27425;&#20803;&#23398;&#20064;&#19968;&#20010;&#36924;&#36817;&#30495;&#23454;&#20869;&#26680;&#30340;&#20869;&#26680;&#65292;&#28982;&#21518;&#29992;&#26368;&#26032;&#30340;&#20869;&#26680;&#20272;&#35745;&#26469;&#35299;&#20915;&#21363;&#23558;&#21040;&#26469;&#30340;&#20219;&#21153;&#12290;&#26412;&#31639;&#27861;&#21487;&#20197;&#19982;&#20219;&#20309;&#20869;&#26680;&#21270;&#25110;&#32447;&#24615;&#36172;&#21338;&#31639;&#27861;&#37197;&#23545;&#65292;&#24182;&#20445;&#35777;&#26368;&#20248;&#30340;&#39044;&#26399;&#24615;&#33021;&#12290;&#22914;&#26524;&#19982;&#20122;&#32447;&#24615;&#36172;&#21338;&#31639;&#27861;&#37197;&#23545;&#65292;LIBO&#23558;&#20135;&#29983;&#19968;&#20010;&#20122;&#32447;&#24615;&#32456;&#36523;&#21518;&#24724;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning algorithms are often repeatedly applied to problems with similar structure over and over again. We focus on solving a sequence of bandit optimization tasks and develop LIBO, an algorithm which adapts to the environment by learning from past experience and becomes more sample-efficient in the process. We assume a kernelized structure where the kernel is unknown but shared across all tasks. LIBO sequentially meta-learns a kernel that approximates the true kernel and solves the incoming tasks with the latest kernel estimate. Our algorithm can be paired with any kernelized or linear bandit algorithm and guarantees oracle optimal performance, meaning that as more tasks are solved, the regret of LIBO on each task converges to the regret of the bandit algorithm with oracle knowledge of the true kernel. Naturally, if paired with a sublinear bandit algorithm, LIBO yields a sublinear lifelong regret. We also show that direct access to the data from each task is not necessary for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23454;&#39564;&#35777;&#25454;&#30340;&#25991;&#26412;&#34920;&#24449;&#21644;&#35821;&#20041;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#32032;&#26364;&#21704;&#39039;&#36317;&#31163;&#21521;&#37327;&#29305;&#24449;&#35782;&#21035;&#25991;&#26412;-&#20551;&#35774;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2210.09723</link><description>&lt;p&gt;
&#24102;&#26377;&#23454;&#35777;&#25991;&#26412;&#34920;&#24449;&#30340;&#35821;&#20041;&#29305;&#24449;&#30340;&#25991;&#26412;&#34164;&#28085;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Textual Entailment Recognition with Semantic Features from Empirical Text Representation. (arXiv:2210.09723v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09723
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#23454;&#39564;&#35777;&#25454;&#30340;&#25991;&#26412;&#34920;&#24449;&#21644;&#35821;&#20041;&#29305;&#24449;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#32032;&#26364;&#21704;&#39039;&#36317;&#31163;&#21521;&#37327;&#29305;&#24449;&#35782;&#21035;&#25991;&#26412;-&#20551;&#35774;&#20043;&#38388;&#30340;&#34164;&#28085;&#20851;&#31995;&#65292;&#24182;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;F1&#20998;&#25968;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#34164;&#28085;&#35782;&#21035;&#26159;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#22522;&#26412;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#22312;&#33258;&#21160;&#35782;&#21035;&#25991;&#26412;&#34164;&#21547;&#20043;&#21069;&#65292;&#29702;&#35299;&#21477;&#23376;&#30340;&#21547;&#20041;&#26159;&#24517;&#35201;&#30340;&#21069;&#25552;&#12290;&#22914;&#26524;&#21069;&#25552;&#20026;&#30495;&#65292;&#21017;&#25991;&#26412;&#34164;&#28085;&#20551;&#35774;&#20063;&#20026;&#30495;&#12290;&#32463;&#20856;&#30340;&#26041;&#27861;&#36890;&#24120;&#21033;&#29992;&#26469;&#33258;&#35789;&#23884;&#20837;&#30340;&#27599;&#20010;&#21333;&#35789;&#30340;&#29305;&#24449;&#20540;&#26469;&#34920;&#31034;&#21477;&#23376;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20197;&#35782;&#21035;&#25991;&#26412;&#21644;&#20551;&#35774;&#20043;&#38388;&#30340;&#34164;&#21547;&#20851;&#31995;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#35777;&#22522;&#20110;&#38408;&#20540;&#30340;&#35821;&#20041;&#25991;&#26412;&#34920;&#24449;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#20010;&#22522;&#20110;&#20803;&#32032;&#30340;&#26364;&#21704;&#39039;&#36317;&#31163;&#21521;&#37327;&#29305;&#24449;&#65292;&#21487;&#20197;&#35782;&#21035;&#25991;&#26412;-&#20551;&#35774;&#23545;&#20043;&#38388;&#30340;&#35821;&#20041;&#34164;&#28085;&#20851;&#31995;&#12290;&#25105;&#20204;&#23545;&#22522;&#20934;&#34164;&#28085;&#20998;&#31867;(SICK-RTE)&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20960;&#39033;&#23454;&#39564;&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#35757;&#32451;&#20102;&#20960;&#20010;&#26426;&#22120;&#23398;&#20064;(ML)&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#30340;&#24615;&#33021;&#19982;&#32463;&#20856;&#30340;&#21644;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;F1&#20998;&#25968;&#26041;&#38754;&#26174;&#33879;&#20248;&#20110;&#32463;&#20856;&#27169;&#22411;&#65292;&#24182;&#22312;&#22823;&#22810;&#25968;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Textual entailment recognition is one of the basic natural language understanding(NLU) tasks. Understanding the meaning of sentences is a prerequisite before applying any natural language processing(NLP) techniques to automatically recognize the textual entailment. A text entails a hypothesis if and only if the true value of the hypothesis follows the text. Classical approaches generally utilize the feature value of each word from word embedding to represent the sentences. In this paper, we propose a novel approach to identifying the textual entailment relationship between text and hypothesis, thereby introducing a new semantic feature focusing on empirical threshold-based semantic text representation. We employ an element-wise Manhattan distance vector-based feature that can identify the semantic entailment relationship between the text-hypothesis pair. We carried out several experiments on a benchmark entailment classification(SICK-RTE) dataset. We train several machine learning(ML) 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2210.08964</link><description>&lt;p&gt;
PromptCast&#65306;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
PromptCast: A New Prompt-based Learning Paradigm for Time Series Forecasting. (arXiv:2210.08964v3 [stat.ME] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.08964
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#65292;&#23558;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#35821;&#35328;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#33539;&#24335;&#8212;&#8212;&#22522;&#20110;&#25552;&#31034;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;PromptCast&#65289;&#12290;&#22312;&#36825;&#31181;&#26032;&#30340;&#20219;&#21153;&#20013;&#65292;&#23558;&#21407;&#26469;&#30340;&#25968;&#23383;&#36755;&#20837;&#21644;&#36755;&#20986;&#36716;&#21270;&#20026;&#25552;&#31034;&#65292;&#24182;&#20197;&#21477;&#23376;&#21040;&#21477;&#23376;&#30340;&#26041;&#24335;&#25552;&#20986;&#39044;&#27979;&#20219;&#21153;&#65292;&#20351;&#24471;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#39044;&#27979;&#30340;&#30446;&#30340;&#12290;&#20026;&#20102;&#25903;&#25345;&#21644;&#20419;&#36827;&#36825;&#20010;&#20219;&#21153;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#65288;PISA&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a new perspective on time series forecasting. In existing time series forecasting methods, the models take a sequence of numerical values as input and yield numerical values as output. The existing SOTA models are largely based on the Transformer architecture, modified with multiple encoding mechanisms to incorporate the context and semantics around the historical data. Inspired by the successes of pre-trained language foundation models, we pose a question about whether these models can also be adapted to solve time-series forecasting. Thus, we propose a new forecasting paradigm: prompt-based time series forecasting (PromptCast). In this novel task, the numerical input and output are transformed into prompts and the forecasting task is framed in a sentence-to-sentence manner, making it possible to directly apply language models for forecasting purposes. To support and facilitate the research of this task, we also present a large-scale dataset (PISA) that includes th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#27010;&#24565;&#20197;&#21450;&#23427;&#20204;&#19982;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#30340;&#32039;&#24352;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#22788;&#29702;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.13012</link><description>&lt;p&gt;
&#20844;&#24179;&#24615;&#27010;&#24565;&#21450;&#20854;&#30456;&#20851;&#24352;&#21147;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Survey on Fairness Notions and Related Tensions. (arXiv:2209.13012v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.13012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#20844;&#24179;&#24615;&#30340;&#19981;&#21516;&#27010;&#24565;&#20197;&#21450;&#23427;&#20204;&#19982;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#30340;&#32039;&#24352;&#20851;&#31995;&#65292;&#24182;&#20171;&#32461;&#20102;&#22788;&#29702;&#20844;&#24179;&#24615;-&#20934;&#30830;&#24615;&#26435;&#34913;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#20915;&#31574;&#31995;&#32479;&#36234;&#26469;&#36234;&#22810;&#22320;&#29992;&#20110;&#35299;&#20915;&#25307;&#32856;&#21644;&#36151;&#27454;&#31561;&#28041;&#21450;&#37325;&#22823;&#20915;&#31574;&#30340;&#38382;&#39064;&#65292;&#24076;&#26395;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#20195;&#26367;&#20027;&#35266;&#20154;&#20026;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20915;&#31574;&#31995;&#32479;&#23481;&#26131;&#20986;&#29616;&#20559;&#35265;&#65292;&#23548;&#33268;&#19981;&#20844;&#24179;&#30340;&#20915;&#31574;&#12290;&#25991;&#29486;&#20013;&#23450;&#20041;&#20102;&#20960;&#31181;&#20844;&#24179;&#24615;&#27010;&#24565;&#20197;&#25429;&#25417;&#36825;&#20010;&#20262;&#29702;&#21644;&#31038;&#20250;&#27010;&#24565;&#30340;&#19981;&#21516;&#24494;&#22937;&#20043;&#22788;&#65288;&#20363;&#22914;&#32479;&#35745;&#24179;&#31561;&#12289;&#26426;&#20250;&#24179;&#31561;&#31561;&#65289;&#12290;&#22312;&#23398;&#20064;&#27169;&#22411;&#26102;&#38656;&#35201;&#28385;&#36275;&#20844;&#24179;&#24615;&#35201;&#27714;&#65292;&#36825;&#20135;&#29983;&#20102;&#19981;&#21516;&#30340;&#20844;&#24179;&#27010;&#24565;&#20043;&#38388;&#20197;&#21450;&#38544;&#31169;&#21644;&#20998;&#31867;&#20934;&#30830;&#24615;&#31561;&#20854;&#20182;&#26399;&#26395;&#23646;&#24615;&#20043;&#38388;&#30340;&#32039;&#24352;&#20851;&#31995;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#36890;&#24120;&#20351;&#29992;&#30340;&#20844;&#24179;&#24615;&#27010;&#24565;&#65292;&#24182;&#35752;&#35770;&#20102;&#23427;&#20204;&#19982;&#38544;&#31169;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24352;&#21147;&#12290;&#26412;&#32508;&#36848;&#20171;&#32461;&#20102;&#35299;&#20915;&#20844;&#24179;&#24615;&#19982;&#20934;&#30830;&#24615;&#26435;&#34913;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#65288;&#20998;&#20026;&#39044;&#22788;&#29702;&#12289;&#22788;&#29702;&#20013;&#12289;&#21518;&#22788;&#29702;&#21644;&#28151;&#21512;&#22235;&#31181;&#26041;&#27861;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated decision systems are increasingly used to take consequential decisions in problems such as job hiring and loan granting with the hope of replacing subjective human decisions with objective machine learning (ML) algorithms. However, ML-based decision systems are prone to bias, which results in yet unfair decisions. Several notions of fairness have been defined in the literature to capture the different subtleties of this ethical and social concept (e.g., statistical parity, equal opportunity, etc.). Fairness requirements to be satisfied while learning models created several types of tensions among the different notions of fairness and other desirable properties such as privacy and classification accuracy. This paper surveys the commonly used fairness notions and discusses the tensions among them with privacy and accuracy. Different methods to address the fairness-accuracy trade-off (classified into four approaches, namely, pre-processing, in-processing, post-processing, and hy
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#30340;&#20840;&#21442;&#25968;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#24178;&#39044;&#21518;&#30340;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;nuisance flow&#21644;target flow&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#21644;&#21452;&#37325;&#31283;&#20581;&#30340;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2209.06203</link><description>&lt;p&gt;
&#38024;&#23545;&#24178;&#39044;&#23494;&#24230;&#20272;&#35745;&#30340;&#27491;&#21017;&#21270;&#27969;
&lt;/p&gt;
&lt;p&gt;
Normalizing Flows for Interventional Density Estimation. (arXiv:2209.06203v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06203
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#30340;&#20840;&#21442;&#25968;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#24178;&#39044;&#21518;&#30340;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;nuisance flow&#21644;target flow&#65292;&#24182;&#24320;&#21457;&#20102;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#23454;&#29616;&#26377;&#25928;&#21644;&#21452;&#37325;&#31283;&#20581;&#30340;&#20272;&#35745;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#38024;&#23545;&#22240;&#26524;&#25512;&#26029;&#36890;&#24120;&#36890;&#36807;&#28508;&#22312;&#32467;&#26524;&#30340;&#22343;&#20540;&#65288;&#20363;&#22914;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65289;&#26469;&#35745;&#31639;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25968;&#37327;&#24182;&#19981;&#33021;&#23436;&#20840;&#25429;&#25417;&#28508;&#22312;&#32467;&#26524;&#20998;&#24067;&#30340;&#20840;&#37096;&#20449;&#24687;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#24178;&#39044;&#21518;&#30340;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20840;&#21442;&#25968;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32452;&#21512;&#20102;&#20004;&#31181;&#27491;&#21017;&#21270;&#27969;&#65292;&#21363;&#65288;i&#65289;&#29992;&#20110;&#20272;&#35745;&#24178;&#25200;&#21442;&#25968;&#30340;nuisance flow&#21644;&#65288;ii&#65289;&#29992;&#20110;&#21442;&#25968;&#21270;&#20272;&#35745;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#30340;target flow&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#22522;&#20110;&#21333;&#27493;&#20559;&#24046;&#26657;&#27491;&#24320;&#21457;&#20102;&#19968;&#20010;&#26131;&#20110;&#22788;&#29702;&#30340;&#20248;&#21270;&#30446;&#26631;&#65292;&#20197;&#26377;&#25928;&#21644;&#21452;&#37325;&#31283;&#20581;&#30340;&#26041;&#24335;&#20272;&#35745;&#30446;&#26631;&#27969;&#21442;&#25968;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#25552;&#20379;&#20102;&#19968;&#20010;&#27491;&#30830;&#24402;&#19968;&#21270;&#30340;&#23494;&#24230;&#20272;&#35745;&#22120;&#12290;&#22312;&#21508;&#31181;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24178;&#39044;&#27491;&#21017;&#21270;&#27969;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#29992;&#20110;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#20272;&#35745;&#28508;&#22312;&#32467;&#26524;&#23494;&#24230;&#30340;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Existing machine learning methods for causal inference usually estimate quantities expressed via the mean of potential outcomes (e.g., average treatment effect). However, such quantities do not capture the full information about the distribution of potential outcomes. In this work, we estimate the density of potential outcomes after interventions from observational data. For this, we propose a novel, fully-parametric deep learning method called Interventional Normalizing Flows. Specifically, we combine two normalizing flows, namely (i) a nuisance flow for estimating nuisance parameters and (ii) a target flow for a parametric estimation of the density of potential outcomes. We further develop a tractable optimization objective based on a one-step bias correction for an efficient and doubly robust estimation of the target flow parameters. As a result our Interventional Normalizing Flows offer a properly normalized density estimator. Across various experiments, we demonstrate that our Int
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#8212;&#8212;&#26102;&#39057;&#32593;&#32476;&#65288;TFN&#65289;&#12290;&#22312;&#20256;&#32479;&#21367;&#31215;&#23618;&#20013;&#23884;&#20837;&#20102;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#26102;&#39057;&#21464;&#25442;&#65288;TFT&#65289;&#26041;&#27861;&#20316;&#20026;&#33258;&#36866;&#24212;&#39044;&#22788;&#29702;&#23618;&#65292;&#35813;&#23618;&#19981;&#20165;&#25552;&#39640;&#20102;&#35786;&#26029;&#24615;&#33021;&#65292;&#36824;&#20351;&#24471;&#32593;&#32476;&#32467;&#26500;&#21487;&#20197;&#21487;&#35299;&#37322;&#65292;&#25925;&#38556;&#35786;&#26029;&#36807;&#31243;&#36879;&#26126;&#12290;</title><link>http://arxiv.org/abs/2209.01992</link><description>&lt;p&gt;
TFN&#65306;&#22522;&#20110;&#26102;&#39057;&#36716;&#25442;&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26234;&#33021;&#25925;&#38556;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
TFN: An Interpretable Neural Network with Time-Frequency Transform Embedded for Intelligent Fault Diagnosis. (arXiv:2209.01992v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01992
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#8212;&#8212;&#26102;&#39057;&#32593;&#32476;&#65288;TFN&#65289;&#12290;&#22312;&#20256;&#32479;&#21367;&#31215;&#23618;&#20013;&#23884;&#20837;&#20102;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#26102;&#39057;&#21464;&#25442;&#65288;TFT&#65289;&#26041;&#27861;&#20316;&#20026;&#33258;&#36866;&#24212;&#39044;&#22788;&#29702;&#23618;&#65292;&#35813;&#23618;&#19981;&#20165;&#25552;&#39640;&#20102;&#35786;&#26029;&#24615;&#33021;&#65292;&#36824;&#20351;&#24471;&#32593;&#32476;&#32467;&#26500;&#21487;&#20197;&#21487;&#35299;&#37322;&#65292;&#25925;&#38556;&#35786;&#26029;&#36807;&#31243;&#36879;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#22240;&#20854;&#24378;&#22823;&#30340;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#33021;&#21147;&#32780;&#24191;&#27867;&#24212;&#29992;&#20110;&#26426;&#26800;&#31995;&#32479;&#30340;&#25925;&#38556;&#35786;&#26029;&#12290;&#28982;&#32780;&#65292;CNN&#26159;&#19968;&#31181;&#20856;&#22411;&#30340;&#40657;&#21283;&#23376;&#27169;&#22411;&#65292;&#20854;&#20915;&#31574;&#26426;&#21046;&#19981;&#28165;&#26224;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#22312;&#39640;&#21487;&#38752;&#24615;&#30340;&#25925;&#38556;&#35786;&#26029;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;&#26102;&#39057;&#32593;&#32476;&#65288;TFN&#65289;&#65292;&#20854;&#20013;&#29289;&#29702;&#19978;&#26377;&#24847;&#20041;&#30340;&#26102;&#39057;&#21464;&#25442;&#65288;TFT&#65289;&#26041;&#27861;&#23884;&#20837;&#21040;&#20256;&#32479;&#21367;&#31215;&#23618;&#20013;&#20316;&#20026;&#33258;&#36866;&#24212;&#39044;&#22788;&#29702;&#23618;&#12290;&#36825;&#20010;&#39044;&#22788;&#29702;&#23618;&#31216;&#20026;&#26102;&#39057;&#21367;&#31215;&#65288;TFconv&#65289;&#23618;&#65292;&#21463;&#21040;&#19968;&#20010;&#31934;&#24515;&#35774;&#35745;&#30340;&#26680;&#20989;&#25968;&#30340;&#38480;&#21046;&#65292;&#29992;&#20110;&#25552;&#21462;&#19982;&#25925;&#38556;&#30456;&#20851;&#30340;&#26102;&#39057;&#20449;&#24687;&#12290;&#23427;&#19981;&#20165;&#25552;&#39640;&#20102;&#35786;&#26029;&#24615;&#33021;&#65292;&#32780;&#19988;&#25581;&#31034;&#20102;CNN&#22312;&#39057;&#29575;&#22495;&#20869;&#39044;&#27979;&#30340;&#36923;&#36753;&#22522;&#30784;&#12290;&#19981;&#21516;&#30340;TFT&#26041;&#27861;&#23545;&#24212;&#30528;TFconv&#23618;&#30340;&#19981;&#21516;&#26680;&#20989;&#25968;&#65292;&#20351;&#32593;&#32476;&#32467;&#26500;&#21487;&#35299;&#37322;&#65292;&#25925;&#38556;&#35786;&#26029;&#36807;&#31243;&#36879;&#26126;&#12290;&#36724;&#25215;&#25925;&#38556;&#25968;&#25454;&#38598;&#21644;&#40831;&#36718;&#31665;&#25391;&#21160;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;TFN&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#22522;&#20110;CNN&#30340;&#25925;&#38556;&#35786;&#26029;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#30340;&#35786;&#26029;&#24615;&#33021;&#65292;&#32780;&#25552;&#21462;&#30340;&#26102;&#39057;&#29305;&#24449;&#21487;&#20197;&#26377;&#25928;&#22320;&#25581;&#31034;&#25925;&#38556;&#39057;&#29575;&#29305;&#24449;&#21644;&#31361;&#20986;&#28508;&#22312;&#25925;&#38556;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Convolutional Neural Networks (CNNs) are widely used in fault diagnosis of mechanical systems due to their powerful feature extraction and classification capabilities. However, the CNN is a typical black-box model, and the mechanism of CNN's decision-making are not clear, which limits its application in high-reliability-required fault diagnosis scenarios. To tackle this issue, we propose a novel interpretable neural network termed as Time-Frequency Network (TFN), where the physically meaningful time-frequency transform (TFT) method is embedded into the traditional convolutional layer as an adaptive preprocessing layer. This preprocessing layer named as time-frequency convolutional (TFconv) layer, is constrained by a well-designed kernel function to extract fault-related time-frequency information. It not only improves the diagnostic performance but also reveals the logical foundation of the CNN prediction in the frequency domain. Different TFT methods correspond to different kernel fun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#24211;&#26500;&#24314;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#25163;&#21160;&#25552;&#31034;&#31574;&#30053;&#30340;&#32534;&#21046;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#24517;&#39035;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#32473;&#20986;&#19981;&#21516;&#38271;&#24230;&#30340;&#31572;&#26696;&#38598;&#65292;&#29305;&#21035;&#26159;&#21253;&#25324;&#31354;&#31572;&#26696;&#38598;&#12290;&#23454;&#20307;&#21035;&#21517;&#23383;&#20856;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24471;&#20998;&#12290;</title><link>http://arxiv.org/abs/2208.11057</link><description>&lt;p&gt;
&#25552;&#31034;&#20316;&#20026;&#25506;&#27979;&#22120;&#65306;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#24211;&#26500;&#24314;
&lt;/p&gt;
&lt;p&gt;
Prompting as Probing: Using Language Models for Knowledge Base Construction. (arXiv:2208.11057v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11057
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#30693;&#35782;&#24211;&#26500;&#24314;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#37319;&#29992;&#20102;&#22810;&#31181;&#25552;&#31034;&#25216;&#26415;&#65292;&#25163;&#21160;&#25552;&#31034;&#31574;&#30053;&#30340;&#32534;&#21046;&#33267;&#20851;&#37325;&#35201;&#65292;&#24182;&#19988;&#24517;&#39035;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#32473;&#20986;&#19981;&#21516;&#38271;&#24230;&#30340;&#31572;&#26696;&#38598;&#65292;&#29305;&#21035;&#26159;&#21253;&#25324;&#31354;&#31572;&#26696;&#38598;&#12290;&#23454;&#20307;&#21035;&#21517;&#23383;&#20856;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#19979;&#28216;&#24212;&#29992;&#20013;&#37117;&#24456;&#26377;&#29992;&#65292;&#20363;&#22914;&#25688;&#35201;&#12289;&#32763;&#35793;&#12289;&#38382;&#31572;&#21644;&#25991;&#26412;&#20998;&#31867;&#12290;&#30001;&#20110;&#23427;&#20204;&#21487;&#20197;&#23384;&#20648;&#22823;&#37327;&#20449;&#24687;&#65292;&#22240;&#27492;&#35821;&#35328;&#27169;&#22411;&#27491;&#22312;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#20013;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#24037;&#20855;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;ProP&#65288;&#25552;&#31034;&#20316;&#20026;&#25506;&#27979;&#22120;&#65289;&#65292;&#23427;&#21033;&#29992;OpenAI&#22312;2020&#24180;&#25552;&#20986;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;GPT-3&#26469;&#25191;&#34892;&#30693;&#35782;&#24211;&#26500;&#24314;&#20219;&#21153;&#12290;ProP&#37319;&#29992;&#22810;&#27493;&#39588;&#26041;&#27861;&#65292;&#32467;&#21512;&#21508;&#31181;&#25552;&#31034;&#25216;&#26415;&#26469;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25163;&#21160;&#25552;&#31034;&#31574;&#30053;&#30340;&#32534;&#21046;&#33267;&#20851;&#37325;&#35201;&#65307;&#24517;&#39035;&#40723;&#21169;&#35821;&#35328;&#27169;&#22411;&#32473;&#20986;&#19981;&#21516;&#38271;&#24230;&#30340;&#31572;&#26696;&#38598;&#65292;&#29305;&#21035;&#26159;&#21253;&#25324;&#31354;&#31572;&#26696;&#38598;&#65307;&#30495;/&#20551;&#38382;&#39064;&#26159;&#22686;&#21152;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#30340;&#24314;&#35758;&#30340;&#20934;&#30830;&#24615;&#30340;&#26377;&#29992;&#26041;&#27861;&#65307;&#35821;&#35328;&#27169;&#22411;&#30340;&#22823;&#23567;&#26159;&#19968;&#20010;&#33267;&#20851;&#37325;&#35201;&#30340;&#22240;&#32032;&#65307;&#23454;&#20307;&#21035;&#21517;&#23383;&#20856;&#21487;&#20197;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#24471;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language Models (LMs) have proven to be useful in various downstream applications, such as summarisation, translation, question answering and text classification. LMs are becoming increasingly important tools in Artificial Intelligence, because of the vast quantity of information they can store. In this work, we present ProP (Prompting as Probing), which utilizes GPT-3, a large Language Model originally proposed by OpenAI in 2020, to perform the task of Knowledge Base Construction (KBC). ProP implements a multi-step approach that combines a variety of prompting techniques to achieve this. Our results show that manual prompt curation is essential, that the LM must be encouraged to give answer sets of variable lengths, in particular including empty answer sets, that true/false questions are a useful device to increase precision on suggestions generated by the LM, that the size of the LM is a crucial factor, and that a dictionary of entity aliases improves the LM score. Our evaluation stu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#22270;&#21644;&#23454;&#20307;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#38646;&#26631;&#31614;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#23494;&#24230;&#20272;&#35745;&#27604;&#36739;&#24322;&#24120;&#21644;&#27491;&#24120;&#23454;&#20363;&#65292;&#24615;&#33021;&#20248;&#20110;&#22810;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#21644;&#21322;&#30417;&#30563;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2208.02108</link><description>&lt;p&gt;
&#38646;&#26631;&#31614;&#30340;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detecting Multivariate Time Series Anomalies with Zero Known Label. (arXiv:2208.02108v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.02108
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#21160;&#24577;&#22270;&#21644;&#23454;&#20307;&#27880;&#24847;&#21147;&#26426;&#21046;&#23454;&#29616;&#38646;&#26631;&#31614;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#30340;&#26041;&#27861;&#65292;&#20854;&#21033;&#29992;&#23494;&#24230;&#20272;&#35745;&#27604;&#36739;&#24322;&#24120;&#21644;&#27491;&#24120;&#23454;&#20363;&#65292;&#24615;&#33021;&#20248;&#20110;&#22810;&#31181;&#26080;&#30417;&#30563;&#26041;&#27861;&#21644;&#21322;&#30417;&#30563;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#24050;&#22312;&#21322;&#30417;&#30563;&#29615;&#22659;&#19979; extensively studied&#65292;&#20294;&#38656;&#35201;&#35757;&#32451;&#38598;&#20013;&#30340;&#25152;&#26377;&#27491;&#24120;&#26679;&#26412;&#65292;&#36825;&#24456;&#36153;&#21147;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#24322;&#24120;&#26816;&#27979;&#26041;&#27861;&#65292;MTGFlow&#65292;&#22522;&#20110;&#19968;&#20010;&#24191;&#27867;&#30340;&#20551;&#35774;&#65292;&#21363;&#24322;&#24120;&#23454;&#20363;&#30340;&#23494;&#24230;&#27604;&#27491;&#24120;&#23454;&#20363;&#31232;&#30095;&#12290;MTGFlow &#24314;&#31435;&#21160;&#24577;&#22270;&#25429;&#25417;&#23454;&#20307;&#38388;&#30340;&#20132;&#20114;&#65292;&#24182;&#21033;&#29992;&#23454;&#20307;&#27880;&#24847;&#26426;&#21046;&#36827;&#34892;&#23494;&#24230;&#21644;&#24322;&#24120;&#20998;&#25968;&#24314;&#27169;&#65292;&#19981;&#20165;&#33021;&#26377;&#25928;&#35299;&#20915;&#23454;&#20307;&#29305;&#24449;&#22810;&#26679;&#24615;&#21644;&#23494;&#24230;&#20272;&#35745;&#38382;&#39064;&#65292;&#32780;&#19988;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;MTGFlow &#27604;&#20960;&#31181;&#20808;&#36827;&#30340;&#26080;&#30417;&#30563;&#26041;&#27861;&#20248;&#36234;&#65292;&#24182;&#21363;&#20351;&#19982;&#21322;&#30417;&#30563;&#26041;&#27861;&#30456;&#27604;&#20063;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series anomaly detection has been extensively studied under the semi-supervised setting, where a training dataset with all normal instances is required. However, preparing such a dataset is very laborious since each single data instance should be fully guaranteed to be normal. It is, therefore, desired to explore multivariate time series anomaly detection methods based on the dataset without any label knowledge. In this paper, we propose MTGFlow, an unsupervised anomaly detection approach for multivariate time series anomaly detection via dynamic graph and entity-aware normalizing flow, leaning only on a widely accepted hypothesis that abnormal instances exhibit sparse densities than the normal. However, the complex interdependencies among entities and the diverse inherent characteristics of each entity pose significant challenges on the density estimation, let alone to detect anomalies based on the estimated possibility distribution. To tackle these problems, we prop
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Transformer&#22312;U.S Patent Phrase to Phrase Matching Dataset&#19978;&#36827;&#34892;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#25928;&#29575;&#65292;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2207.11716</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#35821;&#26009;&#24211;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#30340;&#35748;&#30693;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Cognitive Study on Semantic Similarity Analysis of Large Corpora: A Transformer-based Approach. (arXiv:2207.11716v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.11716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;Transformer&#22312;U.S Patent Phrase to Phrase Matching Dataset&#19978;&#36827;&#34892;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#25928;&#29575;&#65292;&#36798;&#21040;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#21644;&#24314;&#27169;&#26159;&#24403;&#20170;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#35768;&#22810;&#20808;&#39537;&#24212;&#29992;&#20013;&#22522;&#26412;&#35748;&#21487;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#39034;&#24207;&#27169;&#24335;&#35782;&#21035;&#30340;&#24863;&#30693;&#65292;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#65288;&#22914;RNN&#21644;LSTM&#65289;&#22312;&#35821;&#20041;&#30456;&#20284;&#24230;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#23427;&#20204;&#26080;&#27861;&#20197;&#38750;&#39034;&#24207;&#26041;&#24335;&#22788;&#29702;&#20449;&#24687;&#65292;&#22240;&#27492;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#34987;&#35748;&#20026;&#25928;&#29575;&#20302;&#19979;&#65292;&#20174;&#32780;&#23548;&#33268;&#19978;&#19979;&#25991;&#25552;&#21462;&#19981;&#24403;&#12290;Transformer&#22240;&#20854;&#38750;&#39034;&#24207;&#25968;&#25454;&#22788;&#29702;&#21644;&#33258;&#25105;&#20851;&#27880;&#31561;&#20248;&#21183;&#32780;&#25104;&#20026;&#26368;&#20808;&#36827;&#30340;&#26550;&#26500;&#12290;&#26412;&#25991;&#20351;&#29992;&#20256;&#32479;&#21644;&#22522;&#20110;transformer&#30340;&#25216;&#26415;&#23545;&#32654;&#22269;&#19987;&#21033;&#30701;&#35821;&#36827;&#34892;&#35821;&#20041;&#30456;&#20284;&#24230;&#20998;&#26512;&#21644;&#24314;&#27169;&#12290;&#25105;&#20204;&#23545;&#22235;&#31181;&#19981;&#21516;&#29256;&#26412;&#30340;&#35299;&#30721;&#22686;&#24378;BERT-DeBERTa&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#36890;&#36807;K&#25240;&#20132;&#21449;&#39564;&#35777;&#26469;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semantic similarity analysis and modeling is a fundamentally acclaimed task in many pioneering applications of natural language processing today. Owing to the sensation of sequential pattern recognition, many neural networks like RNNs and LSTMs have achieved satisfactory results in semantic similarity modeling. However, these solutions are considered inefficient due to their inability to process information in a non-sequential manner, thus leading to the improper extraction of context. Transformers function as the state-of-the-art architecture due to their advantages like non-sequential data processing and self-attention. In this paper, we perform semantic similarity analysis and modeling on the U.S Patent Phrase to Phrase Matching Dataset using both traditional and transformer-based techniques. We experiment upon four different variants of the Decoding Enhanced BERT - DeBERTa and enhance its performance by performing K-Fold Cross-Validation. The experimental results demonstrate our me
&lt;/p&gt;</description></item><item><title>&#23545;&#20110;&#39034;&#24207;&#20915;&#31574;&#32773;&#30340;&#25932;&#23545;&#25915;&#20987;&#26469;&#35828;&#65292;&#24369;&#28857;&#26159;&#32570;&#20047;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#20351;&#20854;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#65307;&#32780;R-attack&#26159;&#19968;&#31181;&#26082;&#26377;&#25928;&#21448;&#21487;&#35777;&#26126;&#26159;&#32479;&#35745;&#19981;&#21487;&#26816;&#27979;&#30340;&#25915;&#20987;&#65292;&#21487;&#20197;&#26356;&#38590;&#20197;&#20351;&#29992;&#33258;&#21160;&#21270;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#12290;</title><link>http://arxiv.org/abs/2207.10170</link><description>&lt;p&gt;
&#24187;&#35273;&#25915;&#20987;&#65306;&#23545;&#39034;&#24207;&#20915;&#31574;&#32773;&#30340;&#25932;&#23545;&#25915;&#20987;&#20013;&#21487;&#26816;&#27979;&#24615;&#24456;&#37325;&#35201;
&lt;/p&gt;
&lt;p&gt;
Illusory Attacks: Detectability Matters in Adversarial Attacks on Sequential Decision-Makers. (arXiv:2207.10170v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.10170
&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#39034;&#24207;&#20915;&#31574;&#32773;&#30340;&#25932;&#23545;&#25915;&#20987;&#26469;&#35828;&#65292;&#24369;&#28857;&#26159;&#32570;&#20047;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#20351;&#20854;&#23481;&#26131;&#34987;&#26816;&#27979;&#20986;&#26469;&#65307;&#32780;R-attack&#26159;&#19968;&#31181;&#26082;&#26377;&#25928;&#21448;&#21487;&#35777;&#26126;&#26159;&#32479;&#35745;&#19981;&#21487;&#26816;&#27979;&#30340;&#25915;&#20987;&#65292;&#21487;&#20197;&#26356;&#38590;&#20197;&#20351;&#29992;&#33258;&#21160;&#21270;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#19990;&#30028;&#20013;&#37096;&#32626;&#30340;&#33258;&#20027;&#20195;&#29702;&#38656;&#35201;&#23545;&#24863;&#23448;&#36755;&#20837;&#30340;&#25932;&#23545;&#25915;&#20987;&#20855;&#22791;&#24378;&#22823;&#30340;&#40065;&#26834;&#24615;&#12290;&#24378;&#21270;&#20195;&#29702;&#31574;&#30053;&#38656;&#35201;&#39044;&#27979;&#21487;&#33021;&#30340;&#26368;&#24378;&#25915;&#20987;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#30340;&#35266;&#27979;&#31354;&#38388;&#25915;&#20987;&#20855;&#26377;&#20849;&#21516;&#30340;&#24369;&#28857;&#65306;&#34429;&#28982;&#26377;&#25928;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#26102;&#38388;&#19978;&#30340;&#19968;&#33268;&#24615;&#65292;&#22240;&#27492;&#21487;&#20197;&#20351;&#29992;&#33258;&#21160;&#21270;&#25163;&#27573;&#25110;&#20154;&#24037;&#26816;&#26597;&#26469;&#26816;&#27979;&#12290;&#23545;&#20110;&#25932;&#25163;&#26469;&#35828;&#65292;&#21487;&#26816;&#27979;&#24615;&#26159;&#19981;&#24076;&#26395;&#20986;&#29616;&#30340;&#65292;&#22240;&#20026;&#23427;&#21487;&#33021;&#20250;&#24341;&#21457;&#23433;&#20840;&#20107;&#24577;&#21319;&#32423;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#23436;&#32654;&#30340;&#24187;&#35273;&#25915;&#20987;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#24418;&#24335;&#30340;&#39034;&#24207;&#20915;&#31574;&#32773;&#30340;&#25932;&#23545;&#25915;&#20987;&#65292;&#26082;&#26377;&#25928;&#21448;&#21487;&#35777;&#26126;&#26159;&#32479;&#35745;&#19981;&#21487;&#26816;&#27979;&#30340;&#12290;&#38543;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26356;&#21152;&#28789;&#27963;&#30340;R-attack&#65292;&#20854;&#29983;&#25104;&#30340;&#35266;&#27979;&#36716;&#25442;&#19982;&#26080;&#25932;&#23545;&#29615;&#22659;&#30340;&#29366;&#24577;&#36716;&#25442;&#20989;&#25968;&#19968;&#33268;&#19988;&#21487;&#20197;&#31471;&#21040;&#31471;&#23398;&#20064;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#19982;&#29616;&#26377;&#30340;&#25915;&#20987;&#30456;&#27604;&#65292;R-attack&#26356;&#38590;&#20197;&#20351;&#29992;&#33258;&#21160;&#21270;&#26041;&#27861;&#26816;&#27979;&#20986;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents deployed in the real world need to be robust against adversarial attacks on sensory inputs. Robustifying agent policies requires anticipating the strongest attacks possible. We demonstrate that existing observation-space attacks on reinforcement learning agents have a common weakness: while effective, their lack of temporal consistency makes them detectable using automated means or human inspection. Detectability is undesirable to adversaries as it may trigger security escalations. We introduce perfect illusory attacks, a novel form of adversarial attack on sequential decision-makers that is both effective and provably statistically undetectable. We then propose the more versatile R-attacks, which result in observation transitions that are consistent with the state-transition function of the adversary-free environment and can be learned end-to-end. Compared to existing attacks, we empirically find R-attacks to be significantly harder to detect with automated methods, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21019;&#26032;&#24615;&#26816;&#27979;&#26041;&#26696;&#65292;&#20197;&#20445;&#25252;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#26080;&#20154;&#26426;&#20813;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2206.02670</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26080;&#20154;&#26426;&#24341;&#23548;&#21644;&#35268;&#21010;&#30340;&#40065;&#26834;&#24615;&#23545;&#25239;&#25915;&#20987;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Robust Adversarial Attacks Detection based on Explainable Deep Reinforcement Learning For UAV Guidance and Planning. (arXiv:2206.02670v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21019;&#26032;&#24615;&#26816;&#27979;&#26041;&#26696;&#65292;&#20197;&#20445;&#25252;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#26080;&#20154;&#26426;&#20813;&#21463;&#23545;&#25239;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#26080;&#20154;&#26426;&#22312;&#20844;&#20849;&#39046;&#22495;&#36973;&#21463;&#23545;&#25239;&#25915;&#20987;&#30340;&#39118;&#38505;&#22686;&#21152;&#30340;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#21019;&#26032;&#24615;&#26816;&#27979;&#26041;&#26696;&#65292;&#20197;&#20445;&#25252;&#37319;&#29992;&#36825;&#20123;&#26041;&#27861;&#30340;&#26080;&#20154;&#26426;&#20813;&#21463;&#25915;&#20987;&#12290;&#35813;&#26041;&#26696;&#37319;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24341;&#23548;&#21644;&#35268;&#21010;&#65292;&#21033;&#29992;&#20154;&#24037;&#21183;&#22330;&#26469;&#25552;&#39640;&#35757;&#32451;&#25928;&#29575;&#21644;&#38556;&#30861;&#29289;&#36991;&#20813;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dangers of adversarial attacks on Uncrewed Aerial Vehicle (UAV) agents operating in public are increasing. Adopting AI-based techniques and, more specifically, Deep Learning (DL) approaches to control and guide these UAVs can be beneficial in terms of performance but can add concerns regarding the safety of those techniques and their vulnerability against adversarial attacks. Confusion in the agent's decision-making process caused by these attacks can seriously affect the safety of the UAV. This paper proposes an innovative approach based on the explainability of DL methods to build an efficient detector that will protect these DL schemes and the UAVs adopting them from attacks. The agent adopts a Deep Reinforcement Learning (DRL) scheme for guidance and planning. The agent is trained with a Deep Deterministic Policy Gradient (DDPG) with Prioritised Experience Replay (PER) DRL scheme that utilises Artificial Potential Field (APF) to improve training times and obstacle avoidance per
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#26696;&#21644;&#35745;&#31639;&#24378;&#20581;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#12289;&#19981;&#26029;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36716;&#31227;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#38416;&#36848;&#20102;&#19981;&#30830;&#23450;MDP&#65288;uMDP&#65289;&#30340;&#27010;&#24565;&#65292;&#38024;&#23545;&#24212;&#29992;&#20013;&#26377;&#38480;&#25968;&#25454;&#23548;&#33268;&#30340;&#32479;&#35745;&#35823;&#24046;&#38382;&#39064;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#35745;&#31639;&#24378;&#20581;&#31574;&#30053;&#20197;&#36981;&#24490;&#24418;&#24335;&#35268;&#33539;&#30340;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2205.15827</link><description>&lt;p&gt;
&#24378;&#20581;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#21363;&#26102;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robust Anytime Learning of Markov Decision Processes. (arXiv:2205.15827v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15827
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#20171;&#32461;&#20102;&#19968;&#31181;&#23558;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#26696;&#21644;&#35745;&#31639;&#24378;&#20581;&#31574;&#30053;&#30456;&#32467;&#21512;&#30340;&#12289;&#19981;&#26029;&#23398;&#20064;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#36716;&#31227;&#27010;&#29575;&#30340;&#26041;&#27861;&#65292;&#38416;&#36848;&#20102;&#19981;&#30830;&#23450;MDP&#65288;uMDP&#65289;&#30340;&#27010;&#24565;&#65292;&#38024;&#23545;&#24212;&#29992;&#20013;&#26377;&#38480;&#25968;&#25454;&#23548;&#33268;&#30340;&#32479;&#35745;&#35823;&#24046;&#38382;&#39064;&#25552;&#20986;&#20102;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#38598;&#30340;&#35299;&#20915;&#26041;&#27861;&#65292;&#24182;&#20171;&#32461;&#20102;&#35745;&#31639;&#24378;&#20581;&#31574;&#30053;&#20197;&#36981;&#24490;&#24418;&#24335;&#35268;&#33539;&#30340;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26159;&#32463;&#24120;&#29992;&#20110;&#39034;&#24207;&#20915;&#31574;&#30340;&#24418;&#24335;&#27169;&#22411;&#12290;MDP&#36890;&#36807;&#36716;&#31227;&#20989;&#25968;&#20013;&#30340;&#27010;&#29575;&#25429;&#33719;&#21487;&#33021;&#20986;&#29616;&#30340;&#26469;&#33258;&#19981;&#31934;&#30830;&#25191;&#34892;&#22120;&#30340;&#38543;&#26426;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#25968;&#25454;&#39537;&#21160;&#30340;&#24212;&#29992;&#20013;&#65292;&#20174;&#65288;&#26377;&#38480;&#30340;&#65289;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#20934;&#30830;&#30340;&#27010;&#29575;&#20250;&#24341;&#20837;&#32479;&#35745;&#35823;&#24046;&#65292;&#21487;&#33021;&#23548;&#33268;&#24847;&#22806;&#25110;&#19981;&#33391;&#32467;&#26524;&#12290;&#19981;&#30830;&#23450;MDP&#65288;uMDP&#65289;&#19981;&#38656;&#35201;&#20934;&#30830;&#30340;&#27010;&#29575;&#65292;&#32780;&#26159;&#20351;&#29992;&#25152;&#35859;&#30340;&#19981;&#30830;&#23450;&#24615;&#38598;&#65292;&#22312;&#36716;&#25442;&#20013;&#32771;&#34385;&#36825;&#20123;&#26377;&#38480;&#30340;&#25968;&#25454;&#12290;&#25105;&#20204;&#36890;&#36807;&#32452;&#21512;&#19987;&#38376;&#30340;&#36125;&#21494;&#26031;&#25512;&#29702;&#26041;&#26696;&#21644;&#35745;&#31639;&#24378;&#20581;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#22312;&#19968;&#20010;&#24378;&#20581;&#30340;&#21363;&#26102;&#23398;&#20064;&#26041;&#27861;&#20013;&#19981;&#26029;&#23398;&#20064;MDP&#30340;&#36716;&#31227;&#27010;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;1&#65289;&#36817;&#20284;p
&lt;/p&gt;
&lt;p&gt;
Markov decision processes (MDPs) are formal models commonly used in sequential decision-making. MDPs capture the stochasticity that may arise, for instance, from imprecise actuators via probabilities in the transition function. However, in data-driven applications, deriving precise probabilities from (limited) data introduces statistical errors that may lead to unexpected or undesirable outcomes. Uncertain MDPs (uMDPs) do not require precise probabilities but instead use so-called uncertainty sets in the transitions, accounting for such limited data. Tools from the formal verification community efficiently compute robust policies that provably adhere to formal specifications, like safety constraints, under the worst-case instance in the uncertainty set. We continuously learn the transition probabilities of an MDP in a robust anytime-learning approach that combines a dedicated Bayesian inference scheme with the computation of robust policies. In particular, our method (1) approximates p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#35777;&#26126;&#22312;&#38750;&#21442;&#25968;&#20108;&#20803;&#20998;&#31867;&#20013;&#65292;&#32570;&#20047;&#23569;&#25968;&#27966;&#26679;&#26412;&#26159;&#23398;&#20064;&#30340;&#26681;&#26412;&#38480;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#27424;&#37319;&#26679;&#31639;&#27861;&#30340;&#26368;&#23567;&#21270;&#26497;&#24046;&#39118;&#38505;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#31614;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#26368;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2205.13094</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#20998;&#31867;&#20013;&#30340;&#27424;&#37319;&#26679;&#26159;&#19968;&#31181;&#26497;&#23567;&#21270;&#26497;&#24046;&#39118;&#38505;&#30340;&#40065;&#26834;&#24615;&#24178;&#39044;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Undersampling is a Minimax Optimal Robustness Intervention in Nonparametric Classification. (arXiv:2205.13094v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.13094
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#35777;&#26126;&#22312;&#38750;&#21442;&#25968;&#20108;&#20803;&#20998;&#31867;&#20013;&#65292;&#32570;&#20047;&#23569;&#25968;&#27966;&#26679;&#26412;&#26159;&#23398;&#20064;&#30340;&#26681;&#26412;&#38480;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#27424;&#37319;&#26679;&#31639;&#27861;&#30340;&#26368;&#23567;&#21270;&#26497;&#24046;&#39118;&#38505;&#30340;&#40065;&#26834;&#24615;&#34920;&#29616;&#65292;&#29305;&#21035;&#26159;&#22312;&#26631;&#31614;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#26368;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#24191;&#27867;&#30340;&#25216;&#26415;&#26469;&#35299;&#20915;&#20998;&#24067;&#20559;&#31227;&#38382;&#39064;&#65292;&#20294;&#22312;&#20960;&#20010;&#27969;&#34892;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#22522;&#20110;&#27424;&#37319;&#26679;&#30340;&#24179;&#34913;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#36890;&#24120;&#33021;&#22815;&#23454;&#29616;&#25509;&#36817;&#26368;&#20808;&#36827;&#20934;&#30830;&#24615;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#38750;&#21442;&#25968;&#20108;&#20803;&#20998;&#31867;&#35774;&#32622;&#19979;&#65292;&#23398;&#20064;&#30340;&#22522;&#26412;&#38480;&#21046;&#26159;&#30001;&#20110;&#32570;&#20047;&#23569;&#25968;&#32676;&#20307;&#26679;&#26412;&#32780;&#20135;&#29983;&#30340;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#38500;&#38750;&#35757;&#32451;&#21644;&#27979;&#35797;&#20998;&#24067;&#20043;&#38388;&#23384;&#22312;&#39640;&#24230;&#37325;&#21472;&#65288;&#36825;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#20013;&#19981;&#22826;&#21487;&#33021;&#65289;&#65292;&#21542;&#21017;&#31639;&#27861;&#26080;&#27861;&#36229;&#36234;&#27424;&#37319;&#26679;&#65292;&#38500;&#38750;&#31639;&#27861;&#21033;&#29992;&#26377;&#20851;&#20998;&#24067;&#20559;&#31227;&#30340;&#20854;&#20182;&#32467;&#26500;&#12290;&#29305;&#21035;&#22320;&#65292;&#22312;&#26631;&#31614;&#36716;&#31227;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#24635;&#26159;&#23384;&#22312;&#19968;&#31181;&#26368;&#23567;&#21270;&#26497;&#24046;&#39118;&#38505;&#30340;&#27424;&#37319;&#26679;&#31639;&#27861;&#12290;&#22312;&#32452;&#36716;&#25442;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31867;&#26368;&#23567;&#26497;&#24046;&#39118;&#38505;&#30340;&#27424;&#37319;&#26679;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#39564;&#35777;&#23454;&#39564;&#20197;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
While a broad range of techniques have been proposed to tackle distribution shift, the simple baseline of training on an $\textit{undersampled}$ balanced dataset often achieves close to state-of-the-art-accuracy across several popular benchmarks. This is rather surprising, since undersampling algorithms discard excess majority group data. To understand this phenomenon, we ask if learning is fundamentally constrained by a lack of minority group samples. We prove that this is indeed the case in the setting of nonparametric binary classification. Our results show that in the worst case, an algorithm cannot outperform undersampling unless there is a high degree of overlap between the train and test distributions (which is unlikely to be the case in real-world datasets), or if the algorithm leverages additional structure about the distribution shift. In particular, in the case of label shift we show that there is always an undersampling algorithm that is minimax optimal. In the case of grou
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;-&#35821;&#38899;&#32852;&#21512;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#21518;&#23454;&#29616;&#20102;&#33258;&#21160;&#35789;&#35821;&#20998;&#21106;&#21644;&#32858;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2203.15081</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#38899;&#33258;&#30417;&#30563;&#27169;&#22411;&#20013;&#30340;&#35789;&#35821;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Word Discovery in Visually Grounded, Self-Supervised Speech Models. (arXiv:2203.15081v5 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15081
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;-&#35821;&#38899;&#32852;&#21512;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#22312;&#27169;&#22411;&#35757;&#32451;&#21518;&#23454;&#29616;&#20102;&#33258;&#21160;&#35789;&#35821;&#20998;&#21106;&#21644;&#32858;&#31867;&#30340;&#33021;&#21147;&#65292;&#24182;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#20687;-&#35821;&#38899;&#32852;&#21512;&#35757;&#32451;&#30340;&#33258;&#30417;&#30563;&#27169;&#22411;&#65292;&#33021;&#22815;&#23454;&#29616;&#35789;&#35821;&#30340;&#33258;&#21160;&#20998;&#21106;&#21644;&#32858;&#31867;&#65292;&#24182;&#22312; Buckeye &#35789;&#20998;&#21106;&#21644; ZeroSpeech &#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#19982;&#24403;&#21069;&#24050;&#21457;&#34920;&#30340;&#26041;&#27861;&#30456;&#24403;&#30340;&#29978;&#33267;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#31181;&#33021;&#21147;&#24182;&#26410;&#20986;&#29616;&#22312;&#22522;&#26412;&#30340; HuBERT &#21644; wav2vec2.0 &#27169;&#22411;&#20013;&#65292;&#35270;&#35273;&#32852;&#32467;&#20219;&#21153;&#26159;&#25105;&#20204;&#35266;&#23519;&#21040;&#30340;&#35789;&#35821;&#21457;&#29616;&#33021;&#21147;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a method for visually-grounded spoken term discovery. After training either a HuBERT or wav2vec2.0 model to associate spoken captions with natural images, we show that powerful word segmentation and clustering capability emerges within the model's self-attention heads. Our experiments reveal that this ability is not present to nearly the same extent in the base HuBERT and wav2vec2.0 models, suggesting that the visual grounding task is a crucial component of the word discovery capability we observe. We also evaluate our method on the Buckeye word segmentation and ZeroSpeech spoken term discovery tasks, where we perform on par with or better than currently published methods on several metrics. Code and model weights are available at https://github.com/jasonppy/word-discovery.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#23616;&#37096;&#21270;&#30340;LSTM&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#36947;&#36335;&#20043;&#38388;&#30340;&#31354;&#38388;&#19982;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#27169;&#22411;&#27604;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2112.02409</link><description>&lt;p&gt;
&#38024;&#23545;&#36947;&#36335;&#20132;&#36890;&#36895;&#24230;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#27169;&#22411;&#20013;&#21160;&#24577;&#26102;&#31354;&#32972;&#26223;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Understanding Dynamic Spatio-Temporal Contexts in Long Short-Term Memory for Road Traffic Speed Prediction. (arXiv:2112.02409v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.02409
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#23616;&#37096;&#21270;&#30340;LSTM&#27169;&#22411;&#65292;&#33021;&#22815;&#21516;&#26102;&#32771;&#34385;&#36947;&#36335;&#20043;&#38388;&#30340;&#31354;&#38388;&#19982;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#65292;&#35813;&#27169;&#22411;&#27604;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#38752;&#30340;&#20132;&#36890;&#27969;&#39044;&#27979;&#23545;&#20110;&#21019;&#24314;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#35768;&#22810;&#22522;&#20110;&#22823;&#25968;&#25454;&#30340;&#39044;&#27979;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#65292;&#20294;&#23427;&#20204;&#24182;&#27809;&#26377;&#21453;&#26144;&#20986;&#32771;&#34385;&#26102;&#38388;&#21644;&#20301;&#32622;&#30340;&#22797;&#26434;&#21160;&#24577;&#36947;&#36335;&#20132;&#20114;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21160;&#24577;&#23616;&#37096;&#21270;&#30340;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#27169;&#22411;&#65292;&#28041;&#21450;&#36947;&#36335;&#20043;&#38388;&#30340;&#31354;&#38388;&#21644;&#26102;&#38388;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#20351;&#29992;&#23616;&#37096;&#21160;&#24577;&#31354;&#38388;&#26435;&#37325;&#30697;&#38453;&#21450;&#20854;&#21160;&#24577;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;LSTM&#27169;&#22411;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#38271;&#20381;&#36182;&#24615;&#21644;&#22797;&#26434;&#38750;&#32447;&#24615;&#29305;&#24449;&#30340;&#24207;&#21015;&#25968;&#25454;&#12290;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#30456;&#27604;&#20110;&#20004;&#31181;&#19981;&#21516;&#30340;&#22522;&#32447;&#26041;&#27861;&#20855;&#26377;&#26356;&#20248;&#24322;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reliable traffic flow prediction is crucial to creating intelligent transportation systems. Many big-data-based prediction approaches have been developed but they do not reflect complicated dynamic interactions between roads considering time and location. In this study, we propose a dynamically localised long short-term memory (LSTM) model that involves both spatial and temporal dependence between roads. To do so, we use a localised dynamic spatial weight matrix along with its dynamic variation. Moreover, the LSTM model can deal with sequential data with long dependency as well as complex non-linear features. Empirical results indicated superior prediction performances of the proposed model compared to two different baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32858;&#28966;&#20110;P2P&#20849;&#20056;&#20013;&#26368;&#26680;&#24515;&#30340;&#38382;&#39064;&#65306;&#20056;&#23458;&#21644;&#21496;&#26426;&#30340;&#21305;&#37197;&#12290;&#22312;&#32771;&#34385;&#21040;&#29992;&#25143;&#20559;&#22909;&#30340;&#21069;&#25552;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#26032;&#27010;&#24565;&#20197;&#21450;&#39640;&#25928;&#21305;&#37197;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#31995;&#32479;&#33539;&#22260;&#20869;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20844;&#24179;&#21644;&#31283;&#23450;&#30340;&#20849;&#20056;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2110.01152</link><description>&lt;p&gt;
&#38750;&#21830;&#19994;P2P&#20849;&#20056;&#20013;&#30340;&#25928;&#29575;&#12289;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Efficiency, Fairness, and Stability in Non-Commercial Peer-to-Peer Ridesharing. (arXiv:2110.01152v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.01152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32858;&#28966;&#20110;P2P&#20849;&#20056;&#20013;&#26368;&#26680;&#24515;&#30340;&#38382;&#39064;&#65306;&#20056;&#23458;&#21644;&#21496;&#26426;&#30340;&#21305;&#37197;&#12290;&#22312;&#32771;&#34385;&#21040;&#29992;&#25143;&#20559;&#22909;&#30340;&#21069;&#25552;&#19979;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20851;&#20110;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#26032;&#27010;&#24565;&#20197;&#21450;&#39640;&#25928;&#21305;&#37197;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#31995;&#32479;&#33539;&#22260;&#20869;&#25928;&#29575;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;&#20844;&#24179;&#21644;&#31283;&#23450;&#30340;&#20849;&#20056;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#21830;&#19994;&#20849;&#20056;&#19981;&#21516;&#65292;&#38750;&#21830;&#19994;P2P&#20849;&#20056;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#30740;&#31350;&#65292;&#34429;&#28982;&#23427;&#21487;&#20197;&#20419;&#36827;&#38750;&#22478;&#24066;&#31038;&#21306;&#30340;&#21487;&#34892;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#20851;&#27880;P2P&#20849;&#20056;&#20013;&#30340;&#26680;&#24515;&#38382;&#39064;&#65306;&#20056;&#23458;&#21644;&#21496;&#26426;&#30340;&#21305;&#37197;&#12290;&#25105;&#20204;&#23558;&#29992;&#25143;&#30340;&#20559;&#22909;&#25552;&#21319;&#20026;&#19968;&#32423;&#32771;&#34385;&#65292;&#24182;&#22312;P2P&#20849;&#20056;&#20013;&#24341;&#20837;&#20102;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#30340;&#26032;&#27010;&#24565;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#39640;&#25928;&#21305;&#37197;&#31639;&#27861;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#29992;&#25143;&#20013;&#24515;&#21270;&#22240;&#32032;&#65292;&#21253;&#25324;&#29992;&#25143;&#30340;&#39318;&#36873;&#20986;&#21457;&#26102;&#38388;&#12289;&#20844;&#24179;&#24615;&#21644;&#31283;&#23450;&#24615;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#22312;&#21512;&#29702;&#30340;&#35745;&#31639;&#26102;&#38388;&#20869;&#33719;&#24471;&#20844;&#24179;&#21644;&#31283;&#23450;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#19988;&#21487;&#20197;&#22522;&#20110;&#31995;&#32479;&#33539;&#22260;&#20869;&#30340;&#25928;&#29575;&#29420;&#21344;&#22320;&#25913;&#21892;&#22522;&#32447;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unlike commercial ridesharing, non-commercial peer-to-peer (P2P) ridesharing has been subject to limited research -- although it can promote viable solutions in non-urban communities. This paper focuses on the core problem in P2P ridesharing: the matching of riders and drivers. We elevate users' preferences as a first-order concern and introduce novel notions of fairness and stability in P2P ridesharing. We propose algorithms for efficient matching while considering user-centric factors, including users' preferred departure time, fairness, and stability. Results suggest that fair and stable solutions can be obtained in reasonable computational times and can improve baseline outcomes based on system-wide efficiency exclusively.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25226;&#31243;&#24207;&#21512;&#25104;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#25104;&#21151;&#29575;&#12290;&#20316;&#32773;&#20351;&#29992;&#20102;&#30001;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#26500;&#24314;&#30340;Cross Aggregator&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#65292;&#23398;&#20064;&#22914;&#20309;&#32452;&#21512;&#27599;&#20010;&#26679;&#20363;&#31243;&#24207;&#35299;&#20915;&#26041;&#26696;&#65292;&#29983;&#25104;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2106.07175</link><description>&lt;p&gt;
&#23398;&#20064;&#23558;&#21608;&#20363;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#32452;&#21512;&#29992;&#20110;&#31070;&#32463;&#31243;&#24207;&#21512;&#25104;
&lt;/p&gt;
&lt;p&gt;
Learning to Combine Per-Example Solutions for Neural Program Synthesis. (arXiv:2106.07175v2 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07175
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25226;&#31243;&#24207;&#21512;&#25104;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#30340;&#26041;&#27861;&#65292;&#25552;&#39640;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#25104;&#21151;&#29575;&#12290;&#20316;&#32773;&#20351;&#29992;&#20102;&#30001;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#26500;&#24314;&#30340;Cross Aggregator&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#65292;&#23398;&#20064;&#22914;&#20309;&#32452;&#21512;&#27599;&#20010;&#26679;&#20363;&#31243;&#24207;&#35299;&#20915;&#26041;&#26696;&#65292;&#29983;&#25104;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20363;&#23376;&#31243;&#24207;&#21512;&#25104;&#30340;&#30446;&#26631;&#26159;&#25214;&#21040;&#19968;&#20010;&#19982;&#32473;&#23450;&#30340;&#36755;&#20837;&#36755;&#20986;&#26679;&#20363;&#19968;&#33268;&#30340;&#35745;&#31639;&#26426;&#31243;&#24207;&#12290;&#22823;&#22810;&#25968;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#23581;&#35797;&#25214;&#21040;&#28385;&#36275;&#25152;&#26377;&#26679;&#20363;&#30340;&#31243;&#24207;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25105;&#20204;&#30340;&#24037;&#20316;&#32771;&#34385;&#23558;&#38382;&#39064;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#30340;&#26041;&#27861;&#65306;(a)&#25214;&#21040;&#21482;&#28385;&#36275;&#19968;&#20010;&#26679;&#20363;&#30340;&#31243;&#24207;&#65292;(b)&#21033;&#29992;&#36825;&#20123;&#21333;&#20363;&#31243;&#24207;&#35299;&#20915;&#26041;&#26696;&#29983;&#25104;&#28385;&#36275;&#25152;&#26377;&#26679;&#20363;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#22810;&#22836;&#27880;&#24847;&#26426;&#21046;&#30340;Cross Aggregator&#31070;&#32463;&#32593;&#32476;&#27169;&#22359;&#65292;&#23398;&#20064;&#22914;&#20309;&#32452;&#21512;&#36825;&#20123;&#21333;&#20363;&#26041;&#26696;&#20013;&#23384;&#22312;&#30340;&#25552;&#31034;&#26469;&#21512;&#25104;&#19968;&#20010;&#20840;&#23616;&#35299;&#20915;&#26041;&#26696;&#12290;&#22312;&#19981;&#21516;&#38271;&#24230;&#30340;&#31243;&#24207;&#21644;&#20004;&#31181;&#19981;&#21516;&#23454;&#39564;&#35774;&#32622;&#19979;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;&#24403;&#22312;&#30456;&#21516;&#30340;&#26102;&#38388;&#39044;&#31639;&#19979;&#26102;&#65292;&#25105;&#20204;&#30340;&#25216;&#26415;&#26174;&#33879;&#25552;&#39640;&#20102;&#25104;&#21151;&#29575;&#65292;&#36229;&#36807;&#20102;PCCoder [Zohar&#31561;&#20154;&#65292;2018]&#21644;&#20854;&#20182;&#28040;&#34701;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#30340;&#20195;&#30721;&#12289;&#25968;&#25454;&#21644;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#22312;https://github.com/shriva&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
The goal of program synthesis from examples is to find a computer program that is consistent with a given set of input-output examples. Most learning-based approaches try to find a program that satisfies all examples at once. Our work, by contrast, considers an approach that breaks the problem into two stages: (a) find programs that satisfy only one example, and (b) leverage these per-example solutions to yield a program that satisfies all examples. We introduce the Cross Aggregator neural network module based on a multi-head attention mechanism that learns to combine the cues present in these per-example solutions to synthesize a global solution. Evaluation across programs of different lengths and under two different experimental settings reveal that when given the same time budget, our technique significantly improves the success rate over PCCoder [Zohar et. al 2018] and other ablation baselines. The code, data and trained models for our work can be found at https://github.com/shriva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#23454;&#29992;&#30340;&#31639;&#23376;&#22806;&#25512;&#27861;&#29992;&#26469;&#35299;&#20915;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#31639;&#23376;&#22806;&#25512;&#27861;&#23454;&#29616;&#38543;&#26426;&#24179;&#28369;&#21644;&#24378;&#21333;&#35843;VI&#30340;&#26368;&#20248;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2011.02987</link><description>&lt;p&gt;
&#38543;&#26426;&#21464;&#20998;&#19981;&#31561;&#24335;&#30340;&#31616;&#21333;&#32780;&#26368;&#20248;&#26041;&#27861;I&#65306;&#31639;&#23376;&#22806;&#25512;&#27861;
&lt;/p&gt;
&lt;p&gt;
Simple and optimal methods for stochastic variational inequalities, I: operator extrapolation. (arXiv:2011.02987v5 [math.OC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.02987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#23454;&#29992;&#30340;&#31639;&#23376;&#22806;&#25512;&#27861;&#29992;&#26469;&#35299;&#20915;&#21464;&#20998;&#19981;&#31561;&#24335;&#38382;&#39064;&#65292;&#21516;&#26102;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#38543;&#26426;&#31639;&#23376;&#22806;&#25512;&#27861;&#23454;&#29616;&#38543;&#26426;&#24179;&#28369;&#21644;&#24378;&#21333;&#35843;VI&#30340;&#26368;&#20248;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#23376;&#22806;&#25512;&#27861;&#65288;OE&#65289;&#26469;&#35299;&#20915;&#30830;&#23450;&#24615;&#21464;&#20998;&#19981;&#31561;&#24335;&#65288;VI&#65289;&#38382;&#39064;&#12290;&#31867;&#20284;&#20110;&#26799;&#24230;&#65288;&#31639;&#23376;&#65289;&#25237;&#24433;&#27861;&#65292;OE&#36890;&#36807;&#22312;&#27599;&#27425;&#36845;&#20195;&#20013;&#27714;&#35299;&#21333;&#20010;&#25237;&#24433;&#23376;&#38382;&#39064;&#26469;&#26356;&#26032;&#19968;&#20010;&#25628;&#32034;&#24207;&#21015;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;OE&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#31616;&#21333;&#22320;&#23454;&#29616;&#20102;&#35299;&#20915;&#21508;&#31181;VI&#38382;&#39064;&#30340;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#38543;&#26426;&#31639;&#23376;&#22806;&#25512;&#65288;SOE&#65289;&#26041;&#27861;&#65292;&#24182;&#24314;&#31435;&#20102;&#20854;&#29992;&#20110;&#35299;&#20915;&#19981;&#21516;&#38543;&#26426;VI&#38382;&#39064;&#30340;&#26368;&#20248;&#25910;&#25947;&#34892;&#20026;&#12290;&#29305;&#21035;&#22320;&#65292;SOE&#39318;&#27425;&#22312;&#25991;&#29486;&#20013;&#23454;&#29616;&#20102;&#29992;&#20110;&#35299;&#20915;&#22522;&#30784;&#38382;&#39064;&#65288;&#21363;&#38543;&#26426;&#24179;&#28369;&#21644;&#24378;&#21333;&#35843;VI&#65289;&#30340;&#26368;&#20248;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#38543;&#26426;&#22359;&#31639;&#23376;&#22806;&#25512;&#65288;SBOE&#65289;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#38477;&#20302;&#24212;&#29992;&#20110;&#20855;&#26377;&#29305;&#23450;&#22359;&#32467;&#26500;&#30340;&#22823;&#35268;&#27169;&#30830;&#23450;&#24615;VI&#30340;OE&#26041;&#27861;&#30340;&#36845;&#20195;&#25104;&#26412;&#12290;&#25968;&#20540;&#23454;&#39564;&#35777;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we first present a novel operator extrapolation (OE) method for solving deterministic variational inequality (VI) problems. Similar to the gradient (operator) projection method, OE updates one single search sequence by solving a single projection subproblem in each iteration. We show that OE can achieve the optimal rate of convergence for solving a variety of VI problems in a much simpler way than existing approaches. We then introduce the stochastic operator extrapolation (SOE) method and establish its optimal convergence behavior for solving different stochastic VI problems. In particular, SOE achieves the optimal complexity for solving a fundamental problem, i.e., stochastic smooth and strongly monotone VI, for the first time in the literature. We also present a stochastic block operator extrapolations (SBOE) method to further reduce the iteration cost for the OE method applied to large-scale deterministic VIs with a certain block structure. Numerical experiments have 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#29366;&#24577;&#21644;&#21160;&#20316;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#38646;&#38454;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#37096;&#20998;&#35266;&#27979;&#30340;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#20943;&#23567;&#36890;&#20449;&#24320;&#38144;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2006.10822</link><description>&lt;p&gt;
&#37096;&#20998;&#35266;&#27979;&#19979;&#30340;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cooperative Multi-Agent Reinforcement Learning with Partial Observations. (arXiv:2006.10822v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2006.10822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23616;&#37096;&#29366;&#24577;&#21644;&#21160;&#20316;&#20449;&#24687;&#30340;&#20998;&#24067;&#24335;&#38646;&#38454;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#21487;&#29992;&#20110;&#37096;&#20998;&#35266;&#27979;&#30340;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65292;&#20943;&#23567;&#36890;&#20449;&#24320;&#38144;&#24182;&#21462;&#24471;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#30340;&#38646;&#38454;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#12290;&#29616;&#26377;&#30340;MARL&#31639;&#27861;&#36890;&#24120;&#20551;&#35774;&#27599;&#20010;&#26234;&#33021;&#20307;&#37117;&#21487;&#20197;&#35266;&#23519;&#32593;&#32476;&#20013;&#25152;&#26377;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#12290;&#20294;&#22312;&#22823;&#35268;&#27169;&#38382;&#39064;&#20013;&#65292;&#19982;&#22810;&#36339;&#37051;&#23621;&#20849;&#20139;&#29366;&#24577;&#21644;&#21160;&#20316;&#20449;&#24687;&#21487;&#33021;&#20250;&#23548;&#33268;&#26174;&#30528;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#25552;&#20986;&#30340;&#38646;&#38454;&#31574;&#30053;&#20248;&#21270;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#65292;&#23427;&#20801;&#35768;&#26234;&#33021;&#20307;&#20165;&#22522;&#20110;&#23616;&#37096;&#30340;&#12289;&#37096;&#20998;&#30340;&#29366;&#24577;&#21644;&#21160;&#20316;&#20449;&#24687;&#26469;&#35745;&#31639;&#26412;&#22320;&#31574;&#30053;&#26799;&#24230;&#65292;&#20174;&#32780;&#26356;&#26032;&#23427;&#20204;&#30340;&#26412;&#22320;&#31574;&#30053;&#20989;&#25968;&#65292;&#24182;&#20351;&#29992;&#20849;&#35782;&#26469;&#33719;&#24471;&#20381;&#36182;&#20110;&#20840;&#23616;&#32047;&#31215;&#22870;&#21169;&#30340;&#23616;&#37096;&#20272;&#35745;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#35745;&#31639;&#26412;&#22320;&#31574;&#30053;&#26799;&#24230;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#38646;&#38454;&#31574;&#30053;&#26799;&#24230;&#20272;&#35745;&#22120;&#65292;&#23427;&#20381;&#36182;&#20110;&#19968;&#28857;&#27531;&#24046;&#21453;&#39304;&#65292; im&#21516;&#26102;&#19982;&#29616;&#26377;&#30340;&#20381;&#36182;&#20110;&#19968;&#28857;&#21453;&#39304;&#30340;&#38646;&#38454;&#20272;&#35745;&#22120;&#30456;&#27604;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#36890;&#20449;&#24320;&#38144;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#22522;&#20934;&#20219;&#21153;&#19978;&#23637;&#31034;&#20102;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#34920;&#26126;&#23427;&#32988;&#36807;&#20102;&#29616;&#26377;&#30340;&#20551;&#35774;&#20855;&#26377;&#20840;&#35266;&#23519;&#20449;&#24687;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a distributed zeroth-order policy optimization method for Multi-Agent Reinforcement Learning (MARL). Existing MARL algorithms often assume that every agent can observe the states and actions of all the other agents in the network. This can be impractical in large-scale problems, where sharing the state and action information with multi-hop neighbors may incur significant communication overhead. The advantage of the proposed zeroth-order policy optimization method is that it allows the agents to compute the local policy gradients needed to update their local policy functions using local estimates of the global accumulated rewards that depend on partial state and action information only and can be obtained using consensus. Specifically, to calculate the local policy gradients, we develop a new distributed zeroth-order policy gradient estimator that relies on one-point residual-feedback which, compared to existing zeroth-order estimators that also rely on one-poi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#28304;&#30721;&#27169;&#22411;&#30340;&#23454;&#26102;&#36866;&#24212;&#24615;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#35299;&#20915;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2003.11768</link><description>&lt;p&gt;
&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#28304;&#30721;&#27169;&#22411;&#23454;&#26102;&#36866;&#24212;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On-the-Fly Adaptation of Source Code Models using Meta-Learning. (arXiv:2003.11768v2 [cs.LG] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2003.11768
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20803;&#23398;&#20064;&#30340;&#26041;&#27861;&#29992;&#20110;&#28304;&#30721;&#27169;&#22411;&#30340;&#23454;&#26102;&#36866;&#24212;&#24615;&#30740;&#31350;&#65292;&#20197;&#25552;&#39640;&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#35299;&#20915;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36866;&#24212;&#26410;&#30693;&#30340;&#26412;&#22320;&#29615;&#22659;&#26159;&#25104;&#21151;&#30340;&#28304;&#20195;&#30721;&#27169;&#22411;&#24517;&#39035;&#20811;&#26381;&#30340;&#37325;&#35201;&#25361;&#25112;&#20043;&#19968;&#12290;&#21160;&#24577;&#35780;&#20272;&#26159;&#26368;&#27969;&#34892;&#30340;&#36866;&#24212;&#27169;&#22411;&#30340;&#26041;&#27861;&#20043;&#19968;&#65292;&#20294;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#19978;&#19979;&#25991;&#36866;&#24212;&#38382;&#39064;&#36716;&#21270;&#20026;&#20803;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#26088;&#22312;&#35757;&#32451;&#19968;&#20010;&#22522;&#26412;&#30340;&#28304;&#30721;&#27169;&#22411;&#65292;&#33021;&#22815;&#20174;&#25991;&#20214;&#20013;&#30340;&#20449;&#24687;&#25552;&#21462;&#26368;&#20339;&#30340;&#25903;&#25345;&#26631;&#35760;&#36827;&#34892;&#23398;&#20064;&#65292;&#20197;&#25552;&#20379;&#32570;&#22833;&#26631;&#35760;&#30340;&#25913;&#36827;&#39044;&#27979;&#12290;&#19982;&#21160;&#24577;&#35780;&#20272;&#19981;&#21516;&#65292;&#36825;&#20010;&#20844;&#24335;&#20801;&#35768;&#25105;&#20204;&#36873;&#25321;&#26356;&#26377;&#38024;&#23545;&#24615;&#30340;&#20449;&#24687;&#65288;&#25903;&#25345;&#26631;&#35760;&#65289;&#36827;&#34892;&#36866;&#24212;&#65292;&#21363;&#22312;&#30446;&#26631;&#25991;&#20214;&#20013;&#30340;&#30446;&#26631;&#31354;&#20301;&#32622;&#20043;&#21069;&#21644;&#20043;&#21518;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#31216;&#20026;&#34892;&#32423;&#32500;&#25252;&#30340;&#35780;&#20272;&#35774;&#32622;&#65292;&#26088;&#22312;&#21453;&#26144;IDE&#20013;&#20195;&#30721;&#33258;&#21160;&#23436;&#25104;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to adapt to unseen, local contexts is an important challenge that successful models of source code must overcome. One of the most popular approaches for the adaptation of such models is dynamic evaluation. With dynamic evaluation, when running a model on an unseen file, the model is updated immediately after having observed each token in that file. In this work, we propose instead to frame the problem of context adaptation as a meta-learning problem. We aim to train a base source code model that is best able to learn from information in a file to deliver improved predictions of missing tokens. Unlike dynamic evaluation, this formulation allows us to select more targeted information (support tokens) for adaptation, that is both before and after a target hole in a file. We consider an evaluation setting that we call line-level maintenance, designed to reflect the downstream task of code auto-completion in an IDE. Leveraging recent developments in meta-learning such as first-o
&lt;/p&gt;</description></item></channel></rss>