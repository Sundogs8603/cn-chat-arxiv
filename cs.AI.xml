<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#25512;&#26029;Transformer&#22312;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20462;&#25913;&#29616;&#26377;&#27169;&#22411;&#24182;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#21152;&#36895;&#20102;&#29616;&#26377;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10427</link><description>&lt;p&gt;
&#24182;&#34892;&#35299;&#30721;&#21152;&#36895;Transformer&#22312;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Accelerating Transformer Inference for Translation via Parallel Decoding. (arXiv:2305.10427v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10427
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24182;&#34892;&#35299;&#30721;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#25512;&#26029;Transformer&#22312;&#32763;&#35793;&#20013;&#30340;&#24212;&#29992;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20462;&#25913;&#29616;&#26377;&#27169;&#22411;&#24182;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#21152;&#36895;&#20102;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#22238;&#24402;&#35299;&#30721;&#38480;&#21046;&#20102;Transformer&#22312;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#25928;&#29575;&#12290;&#31038;&#21306;&#25552;&#20986;&#20102;&#29305;&#23450;&#30340;&#32593;&#32476;&#26550;&#26500;&#21644;&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#37117;&#24456;&#26114;&#36149;&#24182;&#19988;&#38656;&#35201;&#25913;&#21464;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#65292;&#20197;&#25512;&#23548;&#20986;&#35299;&#30721;&#36895;&#24230;&#21644;&#32763;&#35793;&#36136;&#37327;&#20043;&#38388;&#30340;&#24179;&#34913;&#12290;&#26412;&#25991;&#20174;&#35299;&#30721;&#31639;&#27861;&#30340;&#35282;&#24230;&#25552;&#20986;&#20102;&#19968;&#20010;&#35299;&#20915;&#26041;&#26696;&#65292;&#23558;&#26631;&#20934;&#30340;&#36138;&#24515;&#33258;&#22238;&#24402;&#35299;&#30721;&#36716;&#21270;&#20026;&#24182;&#34892;&#35299;&#30721;&#65292;&#24182;&#21033;&#29992;&#38597;&#20811;&#27604;&#21644;&#39640;&#26031;-&#22622;&#24503;&#23572;&#36845;&#20195;&#26041;&#27861;&#23454;&#29616;&#24555;&#36895;&#25512;&#26029;&#12290;&#35813;&#31639;&#27861;&#19981;&#38656;&#35201;&#20462;&#25913;&#29616;&#26377;&#27169;&#22411;&#65292;&#24182;&#22312;&#20445;&#25345;&#32763;&#35793;&#36136;&#37327;&#30340;&#21516;&#26102;&#21152;&#36895;&#20102;&#29616;&#26377;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#24182;&#34892;&#35299;&#30721;&#31639;&#27861;&#65292;&#24182;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#35777;&#26126;&#24182;&#34892;&#21270;&#35299;&#30721;&#30456;&#23545;&#20110;&#26631;&#20934;&#33258;&#22238;&#24402;&#35299;&#30721;&#21487;&#25552;&#39640;&#36798;38&#65285;&#30340;&#36895;&#24230;&#65292;&#24403;&#25193;&#23637;&#27169;&#22411;&#26102;&#65292;&#36895;&#24230;&#20960;&#20046;&#25552;&#39640;&#20102;2&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autoregressive decoding limits the efficiency of transformers for Machine Translation (MT). The community proposed specific network architectures and learning-based methods to solve this issue, which are expensive and require changes to the MT model, trading inference speed at the cost of the translation quality. In this paper, we propose to address the problem from the point of view of decoding algorithms, as a less explored but rather compelling direction. We propose to reframe the standard greedy autoregressive decoding of MT with a parallel formulation leveraging Jacobi and Gauss-Seidel fixed-point iteration methods for fast inference. This formulation allows to speed up existing models without training or modifications while retaining translation quality. We present three parallel decoding algorithms and test them on different languages and models showing how the parallelization introduces a speedup up to 38% w.r.t. the standard autoregressive decoding and nearly 2x when scaling t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SLiC-HF&#65292;&#21487;&#20197;&#21033;&#29992;&#24207;&#21015;&#20284;&#28982;&#26657;&#20934;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#65292;&#30456;&#36739;&#20110;&#36807;&#21435;&#30340;&#26041;&#27861;&#26356;&#21152;&#31616;&#21333;&#39640;&#25928;&#65292;&#24182;&#22312;TL;DR&#33258;&#21160;&#25688;&#35201;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#30417;&#30563;&#24494;&#35843;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.10425</link><description>&lt;p&gt;
SLiC-HF&#65306;&#20154;&#31867;&#21453;&#39304;&#30340;&#24207;&#21015;&#20284;&#28982;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
SLiC-HF: Sequence Likelihood Calibration with Human Feedback. (arXiv:2305.10425v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10425
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;SLiC-HF&#65292;&#21487;&#20197;&#21033;&#29992;&#24207;&#21015;&#20284;&#28982;&#26657;&#20934;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#65292;&#30456;&#36739;&#20110;&#36807;&#21435;&#30340;&#26041;&#27861;&#26356;&#21152;&#31616;&#21333;&#39640;&#25928;&#65292;&#24182;&#22312;TL;DR&#33258;&#21160;&#25688;&#35201;&#20219;&#21153;&#20013;&#26174;&#33879;&#25552;&#39640;&#20102;&#30417;&#30563;&#24494;&#35843;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#35777;&#26126;&#65292;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#23398;&#20064;&#21487;&#20197;&#26377;&#25928;&#22320;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;&#36807;&#21435;&#30340;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#20174;&#20154;&#31867;&#20559;&#22909;&#25968;&#25454;&#35757;&#32451;&#30340;&#22870;&#21169;&#27169;&#22411;&#20998;&#37197;&#30340;&#22870;&#21169;&#20998;&#25968;&#65292;&#21033;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26469;&#20248;&#21270;&#35821;&#35328;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#36817;&#24341;&#20837;&#30340;&#24207;&#21015;&#20284;&#28982;&#26657;&#20934;&#65288;SLiC&#65289;&#22914;&#20309;&#26377;&#25928;&#22320;&#24212;&#29992;&#20110;&#20174;&#20154;&#31867;&#20559;&#22909;&#20013;&#23398;&#20064;&#65288;SLiC-HF&#65289;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#36825;&#21487;&#20197;&#20351;&#29992;&#20026;&#19981;&#21516;&#27169;&#22411;&#25910;&#38598;&#30340;&#20154;&#31867;&#21453;&#39304;&#25968;&#25454;&#26469;&#23436;&#25104;&#65292;&#31867;&#20284;&#20110;&#31163;&#32447;RL&#25968;&#25454;&#30340;&#31163;&#32447;&#23398;&#20064;&#12290;&#33258;&#21160;&#21270;&#21644;&#20154;&#31867;&#35780;&#20272;&#23454;&#39564;&#34920;&#26126;&#65292;SLiC-HF&#26174;&#33879;&#25913;&#36827;&#20102;&#30417;&#30563;&#24494;&#35843;&#22522;&#32447;&#12290;&#27492;&#22806;&#65292;SLiC-HF&#26159;&#36807;&#21435;&#24037;&#20316;&#20013;&#20351;&#29992;&#30340;PPO RLHF&#23454;&#29616;&#30340;&#31454;&#20105;&#24615;&#26367;&#20195;&#65292;&#32780;&#19988;&#22312;&#23454;&#36341;&#20013;&#26356;&#31616;&#21333;&#12289;&#26356;&#26131;&#20110;&#35843;&#25972;&#65292;&#24182;&#20855;&#26377;&#26356;&#39640;&#30340;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from human feedback has been shown to be effective at aligning language models with human preferences. Past work has often relied on Reinforcement Learning from Human Feedback (RLHF), which optimizes the language model using reward scores assigned from a reward model trained on human preference data. In this work we show how the recently introduced Sequence Likelihood Calibration (SLiC), can also be used to effectively learn from human preferences (SLiC-HF). Furthermore, we demonstrate this can be done with human feedback data collected for a different model, similar to off-policy, offline RL data. Automatic and human evaluation experiments on the TL;DR summarization task show that SLiC-HF significantly improves supervised fine-tuning baselines. Furthermore, SLiC-HF presents a competitive alternative to the PPO RLHF implementation used in past work while being much simpler to implement, easier to tune and more computationally efficient in practice.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;AI&#36741;&#21161;&#23478;&#24237;&#36827;&#34892;&#21019;&#24847;&#32534;&#30721;&#30340;&#28508;&#21147;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24110;&#21161;&#23478;&#24237;&#29702;&#35299;&#20195;&#30721;&#12289;&#35843;&#35797;&#31243;&#24207;&#20197;&#21450;&#20026;&#26410;&#26469;&#39033;&#30446;&#29983;&#25104;&#26032;&#30340;&#24819;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#36229;&#36807;80\%&#30340;&#24635;&#20307;&#25104;&#21151;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.10417</link><description>&lt;p&gt;
Scratch Copilot &#35780;&#20272;&#65306;&#35780;&#20272;AI&#36741;&#21161;&#30340;&#20146;&#23376;&#21019;&#24847;&#32534;&#30721;
&lt;/p&gt;
&lt;p&gt;
Scratch Copilot Evaluation: Assessing AI-Assisted Creative Coding for Families. (arXiv:2305.10417v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10417
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#20351;&#29992;AI&#36741;&#21161;&#23478;&#24237;&#36827;&#34892;&#21019;&#24847;&#32534;&#30721;&#30340;&#28508;&#21147;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24110;&#21161;&#23478;&#24237;&#29702;&#35299;&#20195;&#30721;&#12289;&#35843;&#35797;&#31243;&#24207;&#20197;&#21450;&#20026;&#26410;&#26469;&#39033;&#30446;&#29983;&#25104;&#26032;&#30340;&#24819;&#27861;&#65292;&#24182;&#23454;&#29616;&#20102;&#36229;&#36807;80\%&#30340;&#24635;&#20307;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35753;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#23478;&#24237;&#21019;&#24847;&#32534;&#30721;&#20307;&#39564;&#65311;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;Scratch&#36827;&#34892;&#21019;&#24847;&#32534;&#30721;&#26102;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#28508;&#21147;&#12290;&#22522;&#20110;&#25105;&#20204;&#20043;&#21069;&#30340;&#29992;&#25143;&#30740;&#31350;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#20010;&#35780;&#20272;&#26041;&#26696;&#65292;&#20197;&#30830;&#23450;LLM&#26159;&#21542;&#33021;&#24110;&#21161;&#23478;&#24237;&#29702;&#35299;&#28216;&#25103;&#20195;&#30721;&#12289;&#35843;&#35797;&#31243;&#24207;&#21644;&#20026;&#26410;&#26469;&#39033;&#30446;&#29983;&#25104;&#26032;&#24819;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;22&#20010;Scratch&#39033;&#30446;&#26469;&#23436;&#25104;&#27599;&#20010;&#26041;&#26696;&#65292;&#29983;&#25104;LLM&#30340;&#21709;&#24212;&#65292;&#21253;&#25324;&#26377;&#21644;&#27809;&#26377;&#32451;&#20064;&#20219;&#21153;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#20135;&#29983;&#20102;120&#20010;&#21019;&#24847;&#32534;&#30721;&#25903;&#25345;&#26041;&#26696;&#25968;&#25454;&#38598;&#12290;&#27492;&#22806;&#65292;&#20316;&#32773;&#36824;&#29420;&#31435;&#35780;&#20272;&#20102;LLM&#30340;&#31934;&#30830;&#24615;&#12289;&#25945;&#23398;&#20215;&#20540;&#21644;&#36866;&#23452;&#24180;&#40836;&#30340;&#35821;&#35328;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;LLMs&#22312;&#19981;&#21516;&#30340;&#20219;&#21153;&#21644;&#35780;&#20272;&#26631;&#20934;&#19979;&#22343;&#23454;&#29616;&#20102;&#36229;&#36807;80\%&#30340;&#24635;&#20307;&#25104;&#21151;&#29575;&#12290;&#26412;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;&#20351;&#29992;LLMs&#36827;&#34892;&#23478;&#24237;&#21019;&#24847;&#32534;&#30721;&#30340;&#26377;&#20215;&#20540;&#20449;&#24687;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;AI&#25903;&#25345;&#30340;&#32534;&#30721;&#24212;&#29992;&#30340;&#35774;&#35745;&#25351;&#21335;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#31361;&#20986;&#20102;LLMs&#22312;&#21327;&#21161;&#23478;&#24237;&#20351;&#29992;Scratch&#36827;&#34892;&#21019;&#24847;&#32534;&#30721;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can AI enhance creative coding experiences for families? This study explores the potential of large language models (LLMs) in helping families with creative coding using Scratch. Based on our previous user study involving a prototype AI assistant, we devised three evaluation scenarios to determine if LLMs could help families comprehend game code, debug programs, and generate new ideas for future projects. We utilized 22 Scratch projects for each scenario and generated responses from LLMs with and without practice tasks, resulting in 120 creative coding support scenario datasets. In addition, the authors independently evaluated their precision, pedagogical value, and age-appropriate language. Our findings show that LLMs achieved an overall success rate of more than 80\% on the different tasks and evaluation criteria. This research offers valuable information on using LLMs for creative family coding and presents design guidelines for future AI-supported coding applications. Our evalu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;AI&#22312;&#23478;&#24237;&#21019;&#24847;&#32534;&#31243;&#20013;&#30340;&#35282;&#33394;&#21644;&#20316;&#29992;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;"&#20154;&#26426;&#20132;&#20114;"&#24179;&#21488;&#24110;&#21161;&#23478;&#24237;&#19982;AI&#26379;&#21451;&#21512;&#20316;&#65292;&#21457;&#29616;AI&#26379;&#21451;&#21487;&#20197;&#36890;&#36807;&#25552;&#38382;&#24110;&#21161;&#23478;&#24237;&#21019;&#20316;&#28216;&#25103;&#65292;&#22312;AI&#26379;&#21451;&#26080;&#27861;&#21327;&#21161;&#26102;&#65292;&#23478;&#38271;&#20063;&#21457;&#25381;&#20102;&#29420;&#29305;&#30340;&#20316;&#29992;&#65292;AI&#25903;&#25345;&#30340;&#24179;&#21488;&#24212;&#24378;&#35843;&#20799;&#31461;&#20026;&#20027;&#35282;&#30340;AI&#29420;&#29305;&#20114;&#21160;&#65292;&#24182;&#20851;&#27880;&#21019;&#24847;&#33258;&#25105;&#25928;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10412</link><description>&lt;p&gt;
AI&#26379;&#21451;&#65306;&#38754;&#21521;&#38738;&#23569;&#24180;&#30340;AI&#21019;&#24847;&#32534;&#31243;&#35774;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
AI Friends: A Design Framework for AI-Powered Creative Programming for Youth. (arXiv:2305.10412v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10412
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;AI&#22312;&#23478;&#24237;&#21019;&#24847;&#32534;&#31243;&#20013;&#30340;&#35282;&#33394;&#21644;&#20316;&#29992;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;"&#20154;&#26426;&#20132;&#20114;"&#24179;&#21488;&#24110;&#21161;&#23478;&#24237;&#19982;AI&#26379;&#21451;&#21512;&#20316;&#65292;&#21457;&#29616;AI&#26379;&#21451;&#21487;&#20197;&#36890;&#36807;&#25552;&#38382;&#24110;&#21161;&#23478;&#24237;&#21019;&#20316;&#28216;&#25103;&#65292;&#22312;AI&#26379;&#21451;&#26080;&#27861;&#21327;&#21161;&#26102;&#65292;&#23478;&#38271;&#20063;&#21457;&#25381;&#20102;&#29420;&#29305;&#30340;&#20316;&#29992;&#65292;AI&#25903;&#25345;&#30340;&#24179;&#21488;&#24212;&#24378;&#35843;&#20799;&#31461;&#20026;&#20027;&#35282;&#30340;AI&#29420;&#29305;&#20114;&#21160;&#65292;&#24182;&#20851;&#27880;&#21019;&#24847;&#33258;&#25105;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#33021;&#22312;&#23478;&#24237;&#20013;&#21457;&#25381;&#24590;&#26679;&#30340;&#20316;&#29992;&#65292;&#25903;&#25345;&#21644;&#38480;&#21046;&#21019;&#24847;&#32534;&#31243;&#65311;&#20026;&#20102;&#35843;&#26597;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;"&#20154;&#26426;&#20132;&#20114;"&#24179;&#21488;&#65292;&#24110;&#21161;&#23478;&#24237;&#19982;&#30001;&#30740;&#31350;&#20154;&#21592;&#25805;&#25511;&#30340;AI&#26379;&#21451;&#65292;&#36827;&#34892;&#21019;&#24847;&#32534;&#31243;&#30340;&#21512;&#20316;&#12290;&#25105;&#20204;&#19982;10&#20010;7&#33267;12&#23681;&#30340;&#20799;&#31461;&#23478;&#24237;&#21512;&#20316;&#35774;&#35745;&#20102;&#19968;&#20010;&#20026;&#26399;3&#21608;&#30340;&#32534;&#31243;&#29615;&#33410;&#12290;&#36890;&#36807;&#21019;&#24847;&#33258;&#25105;&#25928;&#33021;&#29702;&#35770;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;AI&#26379;&#21451;&#36890;&#36807;&#25552;&#38382;&#26469;&#24341;&#23548;&#28216;&#25103;&#21019;&#24847;&#26102;&#65292;&#23478;&#24237;&#26356;&#23481;&#26131;&#29983;&#25104;&#28216;&#25103;&#21019;&#24847;&#65307;&#24403;AI&#26379;&#21451;&#26080;&#27861;&#21327;&#21161;&#26102;&#65292;&#23478;&#38271;&#22312;&#25351;&#23548;&#23401;&#23376;&#23436;&#25104;&#26356;&#22797;&#26434;&#30340;&#32534;&#31243;&#20219;&#21153;&#26041;&#38754;&#36215;&#21040;&#20102;&#29420;&#29305;&#30340;&#20316;&#29992;&#65307;&#23401;&#23376;&#20204;&#26356;&#21152;&#24895;&#24847;&#20351;&#29992;AI&#26379;&#21451;&#30340;&#24110;&#21161;&#65292;&#20889;&#20986;&#26032;&#22855;&#30340;&#32534;&#31243;&#21019;&#24847;&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;AI&#25903;&#25345;&#30340;&#24179;&#21488;&#24212;&#24378;&#35843;&#23478;&#24237;&#38388;&#20197;&#20799;&#31461;&#20026;&#20027;&#35282;&#30340;AI&#29420;&#29305;&#20114;&#21160;&#65292;&#24182;&#20851;&#27880;&#21019;&#24847;&#33258;&#25105;&#25928;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
What role can AI play in supporting and constraining creative coding by families? To investigate these questions, we built a Wizard of Oz platform to help families engage in creative coding in partnership with a researcher-operated AI Friend. We designed a 3 week series of programming activities with ten children, 7 to 12 years old, and nine parents. Using a creative self efficacy lens, we observe that families found it easier to generate game ideas when prompted with questions by AI Friend; parents played a unique role in guiding children in more complex programming tasks when the AI Friend failed to help, and children were more encouraged to write code for novel ideas using the AI friend help. These findings suggest that AI supported platforms should highlight unique family AI interactions focused on children's agency and creative self-efficacy.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;</title><link>http://arxiv.org/abs/2305.10406</link><description>&lt;p&gt;
&#21464;&#20998;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Variational Classification. (arXiv:2305.10406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10406
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#21464;&#20998;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#26469;&#20248;&#21270;&#35757;&#32451;&#65292;&#20801;&#35768;&#28789;&#27963;&#30340;&#35774;&#35745;&#36873;&#25321;&#20197;&#25913;&#21892;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#23545;&#20110;&#22495;&#22806;&#25968;&#25454;&#30340;&#20998;&#31867;&#20934;&#30830;&#24615;&#24471;&#21040;&#20102;&#20445;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20256;&#32479;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#30340;&#26032;&#22411;&#25193;&#23637;&#65292;&#31216;&#20026;&#21464;&#20998;&#20998;&#31867; (VC)&#12290;&#36890;&#36807;&#24341;&#20837;&#28508;&#21464;&#37327;&#24314;&#27169;&#65292;&#31867;&#20284;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#20256;&#32479;&#33258;&#32534;&#30721;&#22120;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#19968;&#20010;&#22522;&#20110;&#35777;&#25454;&#19979;&#30028; (ELBO) &#30340;&#35757;&#32451;&#30446;&#26631;&#65292;&#37319;&#29992;&#23545;&#25239;&#24615;&#26041;&#27861;&#20248;&#21270;&#12290;&#25105;&#20204;&#30340;VC&#27169;&#22411;&#20801;&#35768;&#22312;&#35774;&#35745;&#36873;&#25321;&#26041;&#38754;&#26356;&#21152;&#28789;&#27963;&#65292;&#29305;&#21035;&#26159;&#31867;&#26465;&#20214;&#28508;&#20808;&#39564;&#65292;&#32780;&#19981;&#26159;&#22312;&#29616;&#25104;&#30340;softmax&#20998;&#31867;&#22120;&#20013;&#20570;&#20986;&#30340;&#38544;&#24335;&#20551;&#35774;&#12290;&#22312;&#22270;&#20687;&#21644;&#25991;&#26412;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20445;&#25345;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25913;&#21892;&#20102;&#20854;&#20182;&#33391;&#22909;&#29305;&#24615;&#65292;&#22914;&#26657;&#20934;&#21644;&#23545;&#25239;&#40065;&#26834;&#24615;&#65292;&#21363;&#20351;&#24212;&#29992;&#20110;&#22495;&#22806;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel extension of the traditional neural network approach to classification tasks, referred to as variational classification (VC). By incorporating latent variable modeling, akin to the relationship between variational autoencoders and traditional autoencoders, we derive a training objective based on the evidence lower bound (ELBO), optimized using an adversarial approach. Our VC model allows for more flexibility in design choices, in particular class-conditional latent priors, in place of the implicit assumptions made in off-the-shelf softmax classifiers. Empirical evaluation on image and text classification datasets demonstrates the effectiveness of our approach in terms of maintaining prediction accuracy while improving other desirable properties such as calibration and adversarial robustness, even when applied to out-of-domain data.
&lt;/p&gt;</description></item><item><title>PaLM 2 &#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;PaLM 2 &#36824;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#31283;&#23450;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#25511;&#21046;&#27602;&#24615;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.10403</link><description>&lt;p&gt;
PaLM 2 &#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
PaLM 2 Technical Report. (arXiv:2305.10403v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10403
&lt;/p&gt;
&lt;p&gt;
PaLM 2 &#26159;&#19968;&#31181;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#25552;&#20379;&#20102;&#26356;&#22909;&#30340;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#65292;&#24182;&#19988;&#36890;&#36807;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#65292;&#33719;&#24471;&#20102;&#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;PaLM 2 &#36824;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#21644;&#31283;&#23450;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#24182;&#19988;&#21487;&#20197;&#25511;&#21046;&#27602;&#24615;&#25512;&#29702;&#26102;&#38388;&#65292;&#32780;&#19981;&#20250;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; PaLM 2&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#27604;&#20854;&#21069;&#36523; PaLM &#22312;&#22810;&#35821;&#35328;&#21644;&#25512;&#29702;&#33021;&#21147;&#26041;&#38754;&#26356;&#21152;&#20986;&#33394;&#65292;&#24182;&#19988;&#35745;&#31639;&#25928;&#29575;&#26356;&#39640;&#12290;PaLM 2 &#26159;&#19968;&#31181;&#22522;&#20110; Transformer &#30340;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#31181;&#30446;&#26631;&#36827;&#34892;&#35757;&#32451;&#12290;&#36890;&#36807;&#23545;&#33521;&#35821;&#21644;&#22810;&#35821;&#35328;&#35821;&#35328;&#20197;&#21450;&#25512;&#29702;&#20219;&#21153;&#30340;&#24191;&#27867;&#35780;&#20272;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; PaLM 2 &#22312;&#19981;&#21516;&#27169;&#22411;&#22823;&#23567;&#30340;&#19979;&#28216;&#20219;&#21153;&#19978;&#20855;&#26377;&#26174;&#30528;&#30340;&#25913;&#36827;&#36136;&#37327;&#65292;&#21516;&#26102;&#23637;&#29616;&#20102;&#27604; PaLM &#26356;&#24555;&#21644;&#26356;&#26377;&#25928;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#36825;&#31181;&#25913;&#36827;&#30340;&#25928;&#29575;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24191;&#27867;&#22320;&#37096;&#32626;&#65292;&#21516;&#26102;&#20063;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#26356;&#24555;&#22320;&#21709;&#24212;&#65292;&#20197;&#33719;&#24471;&#26356;&#33258;&#28982;&#30340;&#20132;&#20114;&#33410;&#22863;&#12290;PaLM 2 &#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312; BIG-Bench &#21644;&#20854;&#20182;&#25512;&#29702;&#20219;&#21153;&#19978;&#30456;&#23545;&#20110; PaLM &#26377;&#24040;&#22823;&#30340;&#25913;&#36827;&#12290;PaLM 2 &#22312;&#19968;&#22871;&#36127;&#36131;&#20154;&#30340; AI &#35780;&#20272;&#20013;&#34920;&#29616;&#20986;&#31283;&#23450;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#27809;&#26377;&#38468;&#21152;&#36816;&#34892;&#24320;&#38144;&#25110;&#23545;&#20854;&#20182;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#23545;&#27602;&#24615;&#36827;&#34892;&#25512;&#29702;&#26102;&#38388;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Over
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10361</link><description>&lt;p&gt;
&#38750;&#21512;&#20316;&#21338;&#24328;&#20013;&#30340;&#20154;&#31867;&#36873;&#25321;&#39044;&#27979;&#65306;&#22522;&#20110;&#27169;&#25311;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Human Choice Prediction in Non-Cooperative Games: Simulation-based Off-Policy Evaluation. (arXiv:2305.10361v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#35821;&#35328;&#28216;&#25103;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35828;&#26381;&#28216;&#25103;&#22312;&#32463;&#27982;&#21644;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#24182;&#20855;&#26377;&#37325;&#35201;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#22522;&#20110;&#35821;&#35328;&#30340;&#35828;&#26381;&#28216;&#25103;&#20013;&#31163;&#32447;&#31574;&#30053;&#35780;&#20272;&#65288;OPE&#65289;&#30340;&#25361;&#25112;&#24615;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#30495;&#23454;&#21644;&#27169;&#25311;&#20154;&#31867; - &#26426;&#22120;&#20154;&#20132;&#20114;&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#26377;&#25928;&#22320;&#25972;&#21512;&#20102;&#30495;&#23454;&#20132;&#20114;&#21644;&#27169;&#25311;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Persuasion games have been fundamental in economics and AI research, and have significant practical applications. Recent works in this area have started to incorporate natural language, moving beyond the traditional stylized message setting. However, previous research has focused on on-policy prediction, where the train and test data have the same distribution, which is not representative of real-life scenarios. In this paper, we tackle the challenging problem of off-policy evaluation (OPE) in language-based persuasion games. To address the inherent difficulty of human data collection in this setup, we propose a novel approach which combines real and simulated human-bot interaction data. Our simulated data is created by an exogenous model assuming decision makers (DMs) start with a mixture of random and decision-theoretic based behaviors and improve over time. We present a deep learning training algorithm that effectively integrates real interaction and simulated data, substantially im
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#22810;&#20010;&#29983;&#29289;&#20449;&#21495;&#25968;&#25454;&#28304;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#22312;&#25152;&#26377;&#29983;&#29289;&#20449;&#21495;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#30784;&#27169;&#22411;BIOT&#12290;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#29983;&#29289;&#20449;&#21495;&#20196;&#29260;&#21270;&#20026;&#32479;&#19968;&#30340;&#8220;&#29983;&#29289;&#20449;&#21495;&#21477;&#23376;&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#36328;&#25968;&#25454;&#28304;&#30340;&#23398;&#20064;&#65292;&#22788;&#29702;&#21508;&#31181;&#26684;&#24335;&#30340;&#29983;&#29289;&#20449;&#21495;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10351</link><description>&lt;p&gt;
BIOT&#65306;&#22312;&#37326;&#22806;&#36328;&#25968;&#25454;&#28304;&#29983;&#29289;&#20449;&#21495;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
BIOT: Cross-data Biosignal Learning in the Wild. (arXiv:2305.10351v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10351
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#22810;&#20010;&#29983;&#29289;&#20449;&#21495;&#25968;&#25454;&#28304;&#19978;&#36827;&#34892;&#35757;&#32451;&#65292;&#24182;&#21487;&#22312;&#25152;&#26377;&#29983;&#29289;&#20449;&#21495;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#30784;&#27169;&#22411;BIOT&#12290;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#29983;&#29289;&#20449;&#21495;&#20196;&#29260;&#21270;&#20026;&#32479;&#19968;&#30340;&#8220;&#29983;&#29289;&#20449;&#21495;&#21477;&#23376;&#8221;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#36328;&#25968;&#25454;&#28304;&#30340;&#23398;&#20064;&#65292;&#22788;&#29702;&#21508;&#31181;&#26684;&#24335;&#30340;&#29983;&#29289;&#20449;&#21495;&#65292;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#29289;&#20449;&#21495;&#65292;&#22914;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#65292;&#22312;&#35768;&#22810;&#20020;&#24202;&#24212;&#29992;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#26174;&#31034;&#20986;&#22810;&#31181;&#25968;&#25454;&#26684;&#24335;&#21644;&#36136;&#37327;&#29305;&#24449;&#12290;&#24403;&#21069;&#30340;&#29983;&#29289;&#20449;&#21495;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36890;&#24120;&#19987;&#38376;&#38024;&#23545;&#29305;&#23450;&#25968;&#25454;&#38598;&#21644;&#20020;&#24202;&#35774;&#32622;&#36827;&#34892;&#20102;&#20248;&#21270;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;&#21463;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#22788;&#29702;&#26041;&#38754;&#30340;&#25104;&#21151;&#21551;&#21457;&#65292;&#25105;&#20204;&#25506;&#32034;&#24320;&#21457;&#22522;&#20110;&#22810;&#20010;&#25968;&#25454;&#28304;&#36827;&#34892;&#35757;&#32451;&#24182;&#21487;&#22312;&#19981;&#21516;&#22522;&#30784;&#29983;&#29289;&#20449;&#21495;&#20219;&#21153;&#19978;&#36827;&#34892;&#24494;&#35843;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#20811;&#26381;&#19982;&#21508;&#31181;&#26684;&#24335;&#30340;&#29983;&#29289;&#20449;&#21495;&#30456;&#20851;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#20363;&#22914;&#19981;&#21305;&#37197;&#30340;&#36890;&#36947;&#65292;&#21487;&#21464;&#30340;&#37319;&#26679;&#38271;&#24230;&#21644;&#26222;&#36941;&#23384;&#22312;&#30340;&#32570;&#22833;&#20540;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Biosignal Transformer&#27169;&#22411;&#65288;&#31616;&#31216;\method&#65289;&#12290;&#25152;&#25552;&#20986;&#30340;\method&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#23558;&#19981;&#21516;&#30340;&#29983;&#29289;&#20449;&#21495;&#20196;&#29260;&#21270;&#20026;&#32479;&#19968;&#30340;&#8220;&#29983;&#29289;&#20449;&#21495;&#21477;&#23376;&#8221;&#26469;&#23454;&#29616;&#36890;&#36947;&#19981;&#21305;&#37197;&#12289;&#38271;&#24230;&#21487;&#21464;&#21644;&#32570;&#22833;&#20540;&#26222;&#36941;&#23384;&#22312;&#30340;&#36328;&#25968;&#25454;&#23398;&#20064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#23558;&#27599;&#20010;&#36890;&#36947;&#20196;&#29260;&#21270;&#20026;&#22266;&#23450;&#38271;&#24230;&#30340;&#29305;&#24449;&#65292;&#24182;&#22312;&#36890;&#36947;&#20043;&#38388;&#36827;&#34892;&#32858;&#21512;&#65292;&#20197;&#24418;&#25104;&#23436;&#25972;&#30340;&#29983;&#29289;&#20449;&#21495;&#21477;&#23376;&#12290;&#22312;EEG&#20998;&#31867;&#21644;EEG&#36716;&#35821;&#38899;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#22522;&#32447;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20855;&#26377;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Biological signals, such as electroencephalograms (EEG), play a crucial role in numerous clinical applications, exhibiting diverse data formats and quality profiles. Current deep learning models for biosignals are typically specialized for specific datasets and clinical settings, limiting their broader applicability. Motivated by the success of large language models in text processing, we explore the development of foundational models that are trained from multiple data sources and can be fine-tuned on different downstream biosignal tasks.  To overcome the unique challenges associated with biosignals of various formats, such as mismatched channels, variable sample lengths, and prevalent missing values, we propose a Biosignal Transformer (\method). The proposed \method model can enable cross-data learning with mismatched channels, variable lengths, and missing values by tokenizing diverse biosignals into unified "biosignal sentences". Specifically, we tokenize each channel into fixed-le
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;GPT&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#21069;&#31471;&#65292;&#20174;&#23545;&#35805;&#20013;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20132;&#20114;&#24335;&#23545;&#35805;&#36716;&#25442;&#20026;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#36882;&#24402;&#22320;&#35201;&#27714;&#26410;&#30693;&#27493;&#39588;&#30340;&#23450;&#20041;&#65292;&#21487;&#20197;&#33719;&#21462;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#24182;&#22312;&#33258;&#28982;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#37325;&#22797;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.10349</link><description>&lt;p&gt;
&#20351;&#29992;GPT&#20174;&#23545;&#35805;&#20013;&#20132;&#20114;&#23398;&#20064;&#20998;&#23618;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Interactive Learning of Hierarchical Tasks from Dialog with GPT. (arXiv:2305.10349v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10349
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#19968;&#31181;&#20351;&#29992;GPT&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#21069;&#31471;&#65292;&#20174;&#23545;&#35805;&#20013;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;&#36890;&#36807;&#23558;&#20132;&#20114;&#24335;&#23545;&#35805;&#36716;&#25442;&#20026;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#36882;&#24402;&#22320;&#35201;&#27714;&#26410;&#30693;&#27493;&#39588;&#30340;&#23450;&#20041;&#65292;&#21487;&#20197;&#33719;&#21462;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#24182;&#22312;&#33258;&#28982;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#36827;&#34892;&#37325;&#22797;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#65292;&#21033;&#29992;GPT&#27169;&#22411;&#20316;&#20026;&#23545;&#35805;&#21069;&#31471;&#65292;&#20174;&#23545;&#35805;&#20013;&#36827;&#34892;&#21487;&#35299;&#37322;&#30340;&#31526;&#21495;&#20132;&#20114;&#24335;&#20219;&#21153;&#23398;&#20064;&#12290;&#23398;&#20064;&#21040;&#30340;&#20219;&#21153;&#34987;&#34920;&#31034;&#20026;&#35859;&#35789;-&#21442;&#25968;&#32467;&#26500;&#30340;&#20998;&#23618;&#20998;&#35299;&#65292;&#20855;&#26377;&#20316;&#29992;&#22495;&#21464;&#37327;&#21442;&#25968;&#12290;&#36890;&#36807;&#20351;&#29992;GPT&#27169;&#22411;&#23558;&#20132;&#20114;&#24335;&#23545;&#35805;&#36716;&#25442;&#20026;&#35821;&#20041;&#34920;&#31034;&#65292;&#24182;&#36882;&#24402;&#22320;&#35201;&#27714;&#26410;&#30693;&#27493;&#39588;&#30340;&#23450;&#20041;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20998;&#23618;&#20219;&#21153;&#30693;&#35782;&#21487;&#20197;&#22312;&#33258;&#28982;&#21644;&#19981;&#21463;&#38480;&#21046;&#30340;&#23545;&#35805;&#29615;&#22659;&#20013;&#34987;&#33719;&#21462;&#21644;&#37325;&#22797;&#20351;&#29992;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31995;&#32479;&#19982;&#20351;&#29992;&#26356;&#20256;&#32479;&#30340;&#35299;&#26512;&#22120;&#30340;&#31867;&#20284;&#26550;&#26500;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#31995;&#32479;&#21487;&#20197;&#23481;&#24525;&#26356;&#24191;&#27867;&#30340;&#35821;&#35328;&#21464;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a system for interpretable, symbolic, interactive task learning from dialog using a GPT model as a conversational front-end. The learned tasks are represented as hierarchical decompositions of predicate-argument structures with scoped variable arguments. By using a GPT model to convert interactive dialog into a semantic representation, and then recursively asking for definitions of unknown steps, we show that hierarchical task knowledge can be acquired and re-used in a natural and unrestrained conversational environment. We compare our system to a similar architecture using a more conventional parser and show that our system tolerates a much wider variety of linguistic variance.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#38750;&#20405;&#20837;&#24335;&#30340;4D&#20154;&#20307;&#25968;&#25454;&#38598;MM-Fi&#65292;&#29992;&#20110;&#22810;&#31181;&#26080;&#32447;&#20256;&#24863;&#20219;&#21153;&#30340;&#25903;&#25345;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;40&#21517;&#21463;&#35797;&#32773;&#30340;&#36229;&#36807;320K&#21516;&#27493;&#24103;&#30340;&#20116;&#31181;&#27169;&#24577;&#65292;&#25903;&#25345;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#21644;&#21160;&#20316;&#35782;&#21035;&#31561;&#20219;&#21153;&#30340;&#24320;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.10345</link><description>&lt;p&gt;
MM-Fi&#65306;&#29992;&#20110;&#22810;&#31181;&#26080;&#32447;&#20256;&#24863;&#30340;&#22810;&#27169;&#24577;&#38750;&#20405;&#20837;&#24335;4D&#20154;&#20307;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset for Versatile Wireless Sensing. (arXiv:2305.10345v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10345
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#38750;&#20405;&#20837;&#24335;&#30340;4D&#20154;&#20307;&#25968;&#25454;&#38598;MM-Fi&#65292;&#29992;&#20110;&#22810;&#31181;&#26080;&#32447;&#20256;&#24863;&#20219;&#21153;&#30340;&#25903;&#25345;&#12290;&#35813;&#25968;&#25454;&#38598;&#21253;&#21547;40&#21517;&#21463;&#35797;&#32773;&#30340;&#36229;&#36807;320K&#21516;&#27493;&#24103;&#30340;&#20116;&#31181;&#27169;&#24577;&#65292;&#25903;&#25345;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#21644;&#21160;&#20316;&#35782;&#21035;&#31561;&#20219;&#21153;&#30340;&#24320;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23478;&#24237;&#33258;&#21160;&#21270;&#21644;&#20803;&#23431;&#23449;&#20154;&#29289;&#27169;&#25311;&#31561;&#20247;&#22810;&#24212;&#29992;&#20013;&#65292;4D&#20154;&#20307;&#24863;&#30693;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#20381;&#36182;&#20110;&#25668;&#20687;&#22836;&#21644;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#35201;&#20040;&#20405;&#29359;&#38544;&#31169;&#65292;&#35201;&#20040;&#20351;&#29992;&#19981;&#20415;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26080;&#32447;&#20256;&#24863;&#25104;&#20026;&#28508;&#22312;&#30340;&#36873;&#25321;&#65292;&#21033;&#29992;&#28608;&#20809;&#38647;&#36798;&#12289;&#27627;&#31859;&#27874;&#38647;&#36798;&#21644;WiFi&#20449;&#21495;&#36827;&#34892;&#38750;&#35774;&#22791;&#24335;&#20154;&#20307;&#24863;&#30693;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;MM-Fi&#65292;&#31532;&#19968;&#20010;&#21253;&#21547;27&#31181;&#26085;&#24120;&#25110;&#24247;&#22797;&#21160;&#20316;&#31867;&#21035;&#30340;&#22810;&#27169;&#24577;&#38750;&#20405;&#20837;&#24335;4D&#20154;&#20307;&#25968;&#25454;&#38598;&#65292;&#20197;&#24357;&#21512;&#26080;&#32447;&#20256;&#24863;&#21644;&#39640;&#32423;&#20154;&#20307;&#24863;&#30693;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;MM-Fi&#30001;40&#21517;&#20154;&#20307;&#20027;&#20307;&#30340;&#36229;&#36807;320K&#21516;&#27493;&#24103;&#30340;&#20116;&#20010;&#27169;&#24577;&#32452;&#25104;&#12290;&#25552;&#20379;&#20102;&#21508;&#31181;&#27880;&#37322;&#20197;&#25903;&#25345;&#28508;&#22312;&#30340;&#24863;&#30693;&#20219;&#21153;&#65292;&#20363;&#22914;&#20154;&#20307;&#23039;&#24577;&#20272;&#35745;&#21644;&#21160;&#20316;&#35782;&#21035;&#12290;&#36890;&#36807;&#23545;&#22810;&#20010;&#20219;&#21153;&#30340;&#27169;&#24577;&#30340;&#24863;&#30693;&#33021;&#21147;&#36827;&#34892;&#27604;&#36739;&#65292;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;&#25105;&#20204;&#39044;&#35745;&#36825;&#20123;&#25968;&#25454;&#21644;&#25152;&#25552;&#20986;&#30340;&#35780;&#20272;&#21327;&#35758;&#23558;&#26377;&#21161;&#20110;&#24320;&#21457;&#26032;&#30340;&#26080;&#32447;&#20256;&#24863;&#31639;&#27861;&#65292;&#29992;&#20110;&#22810;&#31181;&#20154;&#20307;&#24863;&#30693;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
4D human perception plays an essential role in a myriad of applications, such as home automation and metaverse avatar simulation. However, existing solutions which mainly rely on cameras and wearable devices are either privacy intrusive or inconvenient to use. To address these issues, wireless sensing has emerged as a promising alternative, leveraging LiDAR, mmWave radar, and WiFi signals for device-free human sensing. In this paper, we propose MM-Fi, the first multi-modal non-intrusive 4D human dataset with 27 daily or rehabilitation action categories, to bridge the gap between wireless sensing and high-level human perception tasks. MM-Fi consists of over 320k synchronized frames of five modalities from 40 human subjects. Various annotations are provided to support potential sensing tasks, e.g., human pose estimation and action recognition. Extensive experiments have been conducted to compare the sensing capacity of each or several modalities in terms of multiple tasks. We envision th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;CNN&#35299;&#20915;&#20102;&#29031;&#29255;&#26041;&#21521;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;Guided Backpropagation&#33719;&#24471;&#20102;CNN&#26816;&#27979;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.10319</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#33258;&#21160;&#29031;&#29255;&#26041;&#21521;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Automatic Photo Orientation Detection with Convolutional Neural Networks. (arXiv:2305.10319v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10319
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;CNN&#35299;&#20915;&#20102;&#29031;&#29255;&#26041;&#21521;&#26816;&#27979;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#12290;&#20351;&#29992;Guided Backpropagation&#33719;&#24471;&#20102;CNN&#26816;&#27979;&#26041;&#21521;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#21033;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNN)&#26469;&#35299;&#20915;&#30830;&#23450;&#28040;&#36153;&#32773;&#29031;&#29255;&#27491;&#30830;&#26041;&#21521;(0&#176;, 90&#176;, 180&#176;&#21644;270&#176;)&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#23545;&#20110;&#27169;&#25311;&#29031;&#29255;&#30340;&#25968;&#23383;&#21270;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#22312;&#26631;&#20934;&#25968;&#25454;&#38598;&#19978;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#22312;&#26356;&#22256;&#38590;&#30340;&#28040;&#36153;&#32773;&#29031;&#29255;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#25105;&#20204;&#20351;&#29992;&#24341;&#23548;&#21453;&#21521;&#20256;&#25773;(Guided Backpropagation)&#26469;&#33719;&#24471;&#20851;&#20110;CNN&#22914;&#20309;&#26816;&#27979;&#29031;&#29255;&#26041;&#21521;&#30340;&#35265;&#35299;&#65292;&#24182;&#35299;&#37322;&#20854;&#38169;&#35823;&#12290;
&lt;/p&gt;
&lt;p&gt;
We apply convolutional neural networks (CNN) to the problem of image orientation detection in the context of determining the correct orientation (from 0, 90, 180, and 270 degrees) of a consumer photo. The problem is especially important for digitazing analog photographs. We substantially improve on the published state of the art in terms of the performance on one of the standard datasets, and test our system on a more difficult large dataset of consumer photos. We use Guided Backpropagation to obtain insights into how our CNN detects photo orientation, and to explain its mistakes.
&lt;/p&gt;</description></item><item><title>LeTI&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;LM&#29983;&#25104;&#30340;&#31243;&#24207;&#21644;&#38169;&#35823;&#28040;&#24687;&#36827;&#34892;&#20018;&#32852;&#36845;&#20195;&#24494;&#35843;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#33258;&#28982;&#21457;&#29983;&#30340;Python&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20808;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.10314</link><description>&lt;p&gt;
LeTI&#65306;&#20174;&#25991;&#26412;&#20132;&#20114;&#20013;&#23398;&#20064;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LeTI: Learning to Generate from Textual Interactions. (arXiv:2305.10314v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10314
&lt;/p&gt;
&lt;p&gt;
LeTI&#26159;&#19968;&#31181;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;LM&#29983;&#25104;&#30340;&#31243;&#24207;&#21644;&#38169;&#35823;&#28040;&#24687;&#36827;&#34892;&#20018;&#32852;&#36845;&#20195;&#24494;&#35843;&#30340;&#25216;&#26415;&#65292;&#21487;&#20197;&#29992;&#20110;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#24182;&#19988;&#22312;&#33258;&#28982;&#21457;&#29983;&#30340;Python&#25351;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26368;&#20808;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LM)&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#20808;&#21069;&#30340;&#25216;&#26415;&#36890;&#36807;&#36755;&#20837;&#36755;&#20986;&#23545;&#65288;&#20363;&#22914;&#25351;&#20196;&#24494;&#35843;&#65289;&#25110;&#29992;&#35780;&#20272;&#36755;&#20986;&#36136;&#37327;&#30340;&#25968;&#23383;&#22870;&#21169;&#65288;&#20363;&#22914;&#20174;&#20154;&#31867;&#21453;&#39304;&#20013;&#36827;&#34892;&#30340;&#24378;&#21270;&#23398;&#20064;&#65289;&#23545;&#39044;&#35757;&#32451;&#30340;LM&#36827;&#34892;&#24494;&#35843;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;LM&#20174;&#25991;&#26412;&#20132;&#20114;&#20013;&#23398;&#20064;&#30340;&#28508;&#21147;(LeTI)&#65292;&#36825;&#19981;&#20165;&#21487;&#20197;&#36890;&#36807;&#20108;&#36827;&#21046;&#26631;&#31614;&#26816;&#26597;&#20854;&#27491;&#30830;&#24615;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#36890;&#36807;&#25991;&#26412;&#21453;&#39304;&#25351;&#20986;&#21644;&#35299;&#37322;&#20854;&#36755;&#20986;&#20013;&#30340;&#38169;&#35823;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#37325;&#28857;&#26159;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;&#20854;&#20013;&#27169;&#22411;&#26681;&#25454;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#29983;&#25104;&#20195;&#30721;&#29255;&#27573;&#12290;&#36825;&#31181;&#35774;&#32622;&#21487;&#20197;&#33258;&#28982;&#19988;&#21487;&#25193;&#23637;&#22320;&#33719;&#21462;&#25991;&#26412;&#21453;&#39304;&#65306;&#20351;&#29992;Python&#35299;&#37322;&#22120;&#36827;&#34892;&#20195;&#30721;&#25191;&#34892;&#26102;&#30340;&#38169;&#35823;&#28040;&#24687;&#21644;&#22534;&#26632;&#36319;&#36394;&#12290; LeTI&#20351;&#29992;LM&#30446;&#26631;&#23545;&#33258;&#28982;&#35821;&#35328;&#25351;&#20196;&#12289;LM&#29983;&#25104;&#30340;&#31243;&#24207;&#21644;&#25991;&#26412;&#21453;&#39304;&#36827;&#34892;&#20018;&#32852;&#30340;&#36845;&#20195;&#24494;&#35843;&#65292;&#21482;&#26377;&#22312;&#29983;&#25104;&#20195;&#30721;&#26080;&#27861;&#25191;&#34892;&#26102;&#25165;&#25552;&#20379;&#25991;&#26412;&#21453;&#39304;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#21253;&#21547;58k&#20010;&#33258;&#28982;&#21457;&#29983;&#30340;Python&#25351;&#20196;&#65292;&#22686;&#21152;&#20102;&#38169;&#35823;&#28040;&#24687;&#21644;&#22534;&#26632;&#36319;&#36394;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;LeTI&#65292;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#35780;&#20272;&#25351;&#26631;&#19978;&#26174;&#33879;&#20248;&#20110;&#24378;&#22522;&#32447;&#27169;&#22411;&#65292;&#24182;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning pre-trained language models (LMs) enhances the models' capabilities. Prior techniques fine-tune a pre-trained LM on input-output pairs (e.g., instruction fine-tuning), or with numerical rewards that gauge the quality of its outputs (e.g., reinforcement learning from human feedback). We explore LMs' potential to learn from textual interactions (LeTI) that not only check their correctness with binary labels, but also pinpoint and explain errors in their outputs through textual feedback. Our investigation focuses on the code generation task, where the model produces code pieces in response to natural language instructions. This setting invites a natural and scalable way to acquire the textual feedback: the error messages and stack traces from code execution using a Python interpreter. LeTI iteratively fine-tunes the model, using the LM objective, on a concatenation of natural language instructions, LM-generated programs, and textual feedback, which is only provided when the gen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#26684;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8220;&#38543;&#26426;&#36830;&#32493;&#23884;&#20837;&#8221;&#65288;Random Continuous Embedding&#65292;RCE&#65289;&#65292;&#33021;&#22815;&#25552;&#39640; Transformer-based &#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#22823;&#24133;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20351;&#24471;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#30417;&#30563;&#34920;&#26684;&#23398;&#20064;&#20013;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.10308</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#32771;&#34385;&#34920;&#26684;&#25968;&#25454;&#25968;&#25454;&#22686;&#24378;&#30340;&#26032;&#24605;&#36335;
&lt;/p&gt;
&lt;p&gt;
Rethinking Data Augmentation for Tabular Data in Deep Learning. (arXiv:2305.10308v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#26684;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8220;&#38543;&#26426;&#36830;&#32493;&#23884;&#20837;&#8221;&#65288;Random Continuous Embedding&#65292;RCE&#65289;&#65292;&#33021;&#22815;&#25552;&#39640; Transformer-based &#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#24615;&#33021;&#65292;&#22823;&#24133;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#20351;&#24471;&#33258;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#22312;&#30417;&#30563;&#34920;&#26684;&#23398;&#20064;&#20013;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#26684;&#25968;&#25454;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#26684;&#24335;&#12290;&#34429;&#28982;&#22312;&#26377;&#30417;&#30563;&#23398;&#20064;&#20013;&#65292;&#26641;&#24418;&#26041;&#27861;&#20248;&#20110;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65307;&#20294;&#26368;&#36817;&#30340;&#25991;&#29486;&#25253;&#21578;&#31216;&#65292;Transformer-based &#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;&#22312;&#20851;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#29616;&#26377;&#25991;&#29486;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#26159;&#20027;&#23548;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#29420;&#29305;&#32467;&#26500;&#21644;&#39640;&#22797;&#26434;&#24615;&#65292;&#34920;&#26684;&#25968;&#25454;&#30340;&#25968;&#25454;&#22686;&#24378;&#19968;&#30452;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#26041;&#27861;&#23558;&#27169;&#22411;&#32467;&#26500;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#21644;&#25968;&#25454;&#22686;&#24378;&#19977;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#19968;&#36215;&#25552;&#20986;&#12290;&#22240;&#27492;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#22312;&#32508;&#21512;&#32771;&#34385;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#23545;&#27604;&#65292;&#27599;&#20010;&#32452;&#25104;&#37096;&#20998;&#23545;&#23454;&#38469;&#24615;&#33021;&#30340;&#24433;&#21709;&#36824;&#19981;&#28165;&#26970;&#12290;&#26412;&#30740;&#31350;&#20851;&#27880;&#25968;&#25454;&#22686;&#24378;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#8220;&#38543;&#26426;&#36830;&#32493;&#23884;&#20837;&#8221;&#65288;RCE&#65289;&#65292;&#36890;&#36807;&#21521;&#36830;&#32493;&#21464;&#37327;&#27880;&#20837;&#22122;&#22768;&#26469;&#29983;&#25104;&#22686;&#24378;&#30340;&#34920;&#26684;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126; RCE &#22312;&#20351;&#29992; Transformer-based &#27169;&#22411;&#36827;&#34892;&#33258;&#30417;&#30563;&#23398;&#20064;&#26102;&#19968;&#33268;&#20248;&#20110;&#29616;&#26377;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#36827;&#34892;&#31579;&#36873;&#30740;&#31350;&#20197;&#26174;&#31034; RCE &#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#35777;&#26126; RCE &#20351; Transformer-based &#27169;&#22411;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#21487;&#22312;&#30417;&#30563;&#34920;&#26684;&#23398;&#20064;&#20013;&#20248;&#20110;&#26641;&#24418;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tabular data is the most widely used data format in machine learning (ML). While tree-based methods outperform DL-based methods in supervised learning, recent literature reports that self-supervised learning with Transformer-based models outperforms tree-based methods. In the existing literature on self-supervised learning for tabular data, contrastive learning is the predominant method. In contrastive learning, data augmentation is important to generate different views. However, data augmentation for tabular data has been difficult due to the unique structure and high complexity of tabular data. In addition, three main components are proposed together in existing methods: model structure, self-supervised learning methods, and data augmentation. Therefore, previous works have compared the performance without comprehensively considering these components, and it is not clear how each component affects the actual performance.  In this study, we focus on data augmentation to address these 
&lt;/p&gt;</description></item><item><title>UniEX&#26159;&#19968;&#31181;&#33021;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24335;&#26684;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#26694;&#26550;&#65292;&#24182;&#33021;&#21516;&#26102;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#65292;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.10306</link><description>&lt;p&gt;
UniEX&#65306;&#19968;&#31181;&#22522;&#20110;&#36328;&#24230;&#25552;&#21462;&#30340;&#32479;&#19968;&#20449;&#24687;&#25277;&#21462;&#30340;&#26377;&#25928;&#39640;&#25928;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
UniEX: An Effective and Efficient Framework for Unified Information Extraction via a Span-extractive Perspective. (arXiv:2305.10306v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10306
&lt;/p&gt;
&lt;p&gt;
UniEX&#26159;&#19968;&#31181;&#33021;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24335;&#26684;&#24335;&#30340;&#20449;&#24687;&#25277;&#21462;&#26694;&#26550;&#65292;&#24182;&#33021;&#21516;&#26102;&#35299;&#20915;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#65292;&#22312;&#24615;&#33021;&#21644;&#25512;&#29702;&#36895;&#24230;&#19978;&#20248;&#20110;&#20854;&#20182;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#33539;&#24335;&#65292;&#23427;&#19982;&#20219;&#20309;&#27169;&#24335;&#26684;&#24335;&#20860;&#23481;&#65292;&#24182;&#36866;&#29992;&#20110;&#19968;&#31995;&#21015;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#65292;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#12289;&#20851;&#31995;&#25277;&#21462;&#12289;&#20107;&#20214;&#25552;&#21462;&#21644;&#24773;&#24863;&#20998;&#26512;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#20197;&#25991;&#26412;&#20026;&#22522;&#30784;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#36716;&#21270;&#20026; token-pair &#38382;&#39064;&#65292;&#20351;&#29992;&#19968;&#31181;&#32479;&#19968;&#30340;&#25552;&#21462;&#26694;&#26550; UniEX&#65292;&#23558;&#25152;&#26377;&#25552;&#21462;&#30446;&#26631;&#37117;&#32479;&#19968;&#20998;&#35299;&#20026;&#32852;&#21512;&#36328;&#24230;&#26816;&#27979;&#12289;&#20998;&#31867;&#21644;&#20851;&#32852;&#38382;&#39064;&#12290;UniEX &#21487;&#20197;&#21516;&#26102;&#32534;&#30721;&#22522;&#20110;&#27169;&#24335;&#30340;&#25552;&#31034;&#21644;&#25991;&#26412;&#20449;&#24687;&#65292;&#24182;&#20351;&#29992;&#33258;&#21160;&#32534;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#21327;&#21516;&#23398;&#20064;&#39044;&#23450;&#20041;&#20449;&#24687;&#30340;&#24191;&#20041;&#30693;&#35782;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102; traffine &#27880;&#24847;&#26426;&#21046;&#65292;&#23558;&#21253;&#25324;&#20219;&#21153;&#12289;&#26631;&#31614;&#21644;&#20869;&#37096; token &#22312;&#20869;&#30340;&#24322;&#26500;&#22240;&#32032;&#38598;&#25104;&#36215;&#26469;&#65292;&#24182;&#36890;&#36807;&#35780;&#20998;&#30697;&#38453;&#33719;&#24471;&#25552;&#21462;&#30446;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;UniEX &#22312; $14$&#20010;&#22522;&#20934;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#21644;&#25512;&#29702;&#36895;&#24230;&#37117;&#20248;&#20110;&#22522;&#20110;&#29983;&#25104;&#30340;&#36890;&#29992;&#20449;&#24687;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a new paradigm for universal information extraction (IE) that is compatible with any schema format and applicable to a list of IE tasks, such as named entity recognition, relation extraction, event extraction and sentiment analysis. Our approach converts the text-based IE tasks as the token-pair problem, which uniformly disassembles all extraction targets into joint span detection, classification and association problems with a unified extractive framework, namely UniEX. UniEX can synchronously encode schema-based prompt and textual information, and collaboratively learn the generalized knowledge from pre-defined information using the auto-encoder language models. We develop a traffine attention mechanism to integrate heterogeneous factors including tasks, labels and inside tokens, and obtain the extraction target via a scoring matrix. Experiment results show that UniEX can outperform generative universal IE models in terms of performance and inference-speed on $14$ benchmar
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19981;&#21516;&#26041;&#27861;&#20272;&#35745;&#38146;&#30005;&#27744;&#21097;&#20313;&#23551;&#21629;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#30005;&#27744;&#24615;&#33021;&#21442;&#25968;&#20934;&#30830;&#39044;&#27979;&#30005;&#27744;&#30340;&#23551;&#21629;&#12290;</title><link>http://arxiv.org/abs/2305.10298</link><description>&lt;p&gt;
&#38024;&#23545;&#38146;&#31163;&#23376;&#30005;&#27744;&#65288;&#29992;&#20110;&#30005;&#21160;&#27773;&#36710;&#65289;&#30340;&#21097;&#20313;&#23551;&#21629;&#21644;SOH&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Estimation of Remaining Useful Life and SOH of Lithium Ion Batteries (For EV Vehicles). (arXiv:2305.10298v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19981;&#21516;&#26041;&#27861;&#20272;&#35745;&#38146;&#30005;&#27744;&#21097;&#20313;&#23551;&#21629;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#21033;&#29992;&#30005;&#27744;&#24615;&#33021;&#21442;&#25968;&#20934;&#30830;&#39044;&#27979;&#30005;&#27744;&#30340;&#23551;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38146;&#31163;&#23376;&#30005;&#27744;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#39046;&#22495;&#65292;&#21253;&#25324;&#20415;&#25658;&#24335;&#30005;&#23376;&#35774;&#22791;&#12289;&#30005;&#21160;&#27773;&#36710;&#21644;&#21487;&#20877;&#29983;&#33021;&#28304;&#23384;&#20648;&#31995;&#32479;&#12290;&#20934;&#30830;&#20272;&#35745;&#36825;&#20123;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#23545;&#20110;&#30830;&#20445;&#20854;&#26368;&#20339;&#24615;&#33021;&#65292;&#39044;&#38450;&#24847;&#22806;&#25925;&#38556;&#20197;&#21450;&#38477;&#20302;&#32500;&#25252;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#23545;&#29616;&#26377;&#30340;&#20272;&#35745;&#38146;&#31163;&#23376;&#30005;&#27744;&#21097;&#20313;&#23551;&#21629;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#32508;&#21512;&#35780;&#20272;&#65292;&#21253;&#25324;&#22522;&#20110;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#12289;&#22522;&#20110;&#29289;&#29702;&#27169;&#22411;&#20197;&#21450;&#28151;&#21512;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20934;&#30830;&#39044;&#27979;&#38146;&#31163;&#23376;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#30005;&#27744;&#24615;&#33021;&#21442;&#25968;&#65292;&#21253;&#25324;&#30005;&#21387;&#12289;&#30005;&#27969;&#21644;&#28201;&#24230;&#65292;&#35757;&#32451;&#20102;&#19968;&#20010;&#39044;&#27979;&#27169;&#22411;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#20272;&#35745;&#30005;&#27744;&#30340;&#21097;&#20313;&#23551;&#21629;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#38146;&#31163;&#23376;&#30005;&#27744;&#21608;&#26399;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lithium-ion batteries are widely used in various applications, including portable electronic devices, electric vehicles, and renewable energy storage systems. Accurately estimating the remaining useful life of these batteries is crucial for ensuring their optimal performance, preventing unexpected failures, and reducing maintenance costs. In this paper, we present a comprehensive review of the existing approaches for estimating the remaining useful life of lithium-ion batteries, including data-driven methods, physics-based models, and hybrid approaches. We also propose a novel approach based on machine learning techniques for accurately predicting the remaining useful life of lithium-ion batteries. Our approach utilizes various battery performance parameters, including voltage, current, and temperature, to train a predictive model that can accurately estimate the remaining useful life of the battery. We evaluate the performance of our approach on a dataset of lithium-ion battery cycles
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#32972;&#21253;&#32422;&#26463;&#19979;&#30340;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#20004;&#31181;&#26032;&#30340;&#32447;&#24615;&#26597;&#35810;&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#20013;$\mathsf{DLA}$&#21644;$\mathsf{RLA}$&#37117;&#26377;&#24120;&#25968;&#36817;&#20284;&#27604;&#65292;&#19988;&#26597;&#35810;&#22797;&#26434;&#24230;&#20026;$O(n \log(1/\epsilon)/\epsilon)$&#12290;</title><link>http://arxiv.org/abs/2305.10292</link><description>&lt;p&gt;
&#38024;&#23545;&#32972;&#21253;&#32422;&#26463;&#19979;&#30340;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#30340;&#32447;&#24615;&#26597;&#35810;&#36924;&#36817;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Linear Query Approximation Algorithms for Non-monotone Submodular Maximization under Knapsack Constraint. (arXiv:2305.10292v1 [cs.DS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10292
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#38024;&#23545;&#32972;&#21253;&#32422;&#26463;&#19979;&#30340;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#30340;&#20004;&#31181;&#26032;&#30340;&#32447;&#24615;&#26597;&#35810;&#36924;&#36817;&#31639;&#27861;&#65292;&#20854;&#20013;$\mathsf{DLA}$&#21644;$\mathsf{RLA}$&#37117;&#26377;&#24120;&#25968;&#36817;&#20284;&#27604;&#65292;&#19988;&#26597;&#35810;&#22797;&#26434;&#24230;&#20026;$O(n \log(1/\epsilon)/\epsilon)$&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#24341;&#20837;&#20102;&#20004;&#31181;&#20855;&#26377;&#32447;&#24615;&#26597;&#35810;&#22797;&#26434;&#24230;&#30340;&#24120;&#25968;&#36817;&#20284;&#31639;&#27861;&#26469;&#35299;&#20915;&#22320;&#38754;&#38598;&#21512;&#22823;&#23567;&#20026;$n$&#65292;&#22312;&#32972;&#21253;&#32422;&#26463;&#19979;&#30340;&#38750;&#21333;&#35843;&#23376;&#27169;&#26368;&#22823;&#21270;&#38382;&#39064;&#65292;&#20998;&#21035;&#20026;$\mathsf{DLA}$&#21644;$\mathsf{RLA}$&#12290;&#20854;&#20013;$\mathsf{DLA}$&#26159;&#25552;&#20379;6+$\epsilon$&#36817;&#20284;&#27604;&#30340;&#30830;&#23450;&#24615;&#31639;&#27861;&#65292;&#32780;$\mathsf{RLA}$&#26159;&#20855;&#26377;4+$\epsilon$&#36817;&#20284;&#27604;&#30340;&#38543;&#26426;&#31639;&#27861;&#12290;&#20004;&#31181;&#31639;&#27861;&#30340;&#26597;&#35810;&#22797;&#26434;&#24230;&#22343;&#20026;$O(n \log(1/\epsilon)/\epsilon)$&#12290;&#33719;&#21462;&#32447;&#24615;&#26597;&#35810;&#19979;&#30340;&#24120;&#25968;&#36817;&#20284;&#27604;&#30340;&#20851;&#38190;&#24605;&#24819;&#22312;&#20110;: (1)&#23558;&#22320;&#38754;&#38598;&#21512;&#20998;&#20026;&#20004;&#20010;&#21512;&#36866;&#30340;&#23376;&#38598;&#65292;&#29992;&#32447;&#24615;&#26597;&#35810;&#22312;&#36825;&#20123;&#23376;&#38598;&#20013;&#25214;&#21040;&#36817;&#20284;&#26368;&#20248;&#35299;&#65292;(2)&#23558;&#38408;&#20540;&#36138;&#24515;&#19982;&#20004;&#20010;&#19981;&#30456;&#20132;&#38598;&#21512;&#25110;&#38543;&#26426;&#36873;&#25321;&#36827;&#31243;&#30340;&#24615;&#36136;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#35299;&#30340;&#36136;&#37327;&#12290;&#38500;&#20102;&#29702;&#35770;&#20998;&#26512;&#22806;&#65292;&#25105;&#20204;&#36824;&#20351;&#29992;&#19977;&#20010;&#24212;&#29992;&#31243;&#24207;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#65306;&#25910;&#20837;&#26368;&#22823;&#21270;&#65292;&#22270;&#20687;&#25688;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work, for the first time, introduces two constant factor approximation algorithms with linear query complexity for non-monotone submodular maximization over a ground set of size $n$ subject to a knapsack constraint, $\mathsf{DLA}$ and $\mathsf{RLA}$. $\mathsf{DLA}$ is a deterministic algorithm that provides an approximation factor of $6+\epsilon$ while $\mathsf{RLA}$ is a randomized algorithm with an approximation factor of $4+\epsilon$. Both run in $O(n \log(1/\epsilon)/\epsilon)$ query complexity. The key idea to obtain a constant approximation ratio with linear query lies in: (1) dividing the ground set into two appropriate subsets to find the near-optimal solution over these subsets with linear queries, and (2) combining a threshold greedy with properties of two disjoint sets or a random selection process to improve solution quality. In addition to the theoretical analysis, we have evaluated our proposed solutions with three applications: Revenue Maximization, Image Summarizat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Explain Any Concept&#65288;EAC&#65289;&#30340;&#26377;&#25928;&#32780;&#28789;&#27963;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;Segment Anything Model&#65288;SAM&#65289;&#25191;&#34892;&#23454;&#20363;&#20998;&#21106;&#65292;&#24182;&#20351;&#29992;&#30456;&#20851;&#30340;&#27010;&#24565;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;</title><link>http://arxiv.org/abs/2305.10289</link><description>&lt;p&gt;
&#35299;&#37322;&#20219;&#20309;&#27010;&#24565;&#65306;Segment Anything &#28385;&#36275;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Explain Any Concept: Segment Anything Meets Concept-Based Explanation. (arXiv:2305.10289v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10289
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Explain Any Concept&#65288;EAC&#65289;&#30340;&#26377;&#25928;&#32780;&#28789;&#27963;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#23427;&#21033;&#29992;Segment Anything Model&#65288;SAM&#65289;&#25191;&#34892;&#23454;&#20363;&#20998;&#21106;&#65292;&#24182;&#20351;&#29992;&#30456;&#20851;&#30340;&#27010;&#24565;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#24615;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#26159;&#19968;&#20010;&#20851;&#38190;&#35838;&#39064;&#65292;&#21487;&#25552;&#39640;&#23545;&#31070;&#32463;&#32593;&#32476;&#40657;&#30418;&#20869;&#37096;&#30340;&#20154;&#31867;&#29702;&#35299;&#12290;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20219;&#21153;&#65292;&#20027;&#27969;&#22522;&#20110;&#20687;&#32032;&#30340; XAI &#26041;&#27861;&#36890;&#36807;&#35782;&#21035;&#37325;&#35201;&#20687;&#32032;&#26469;&#35299;&#37322; DNN &#20915;&#31574;&#65292;&#32780;&#26032;&#20852;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340; XAI &#21017;&#25506;&#32034;&#20351;&#29992;&#27010;&#24565;&#65288;&#20363;&#22914;&#22270;&#20687;&#20013;&#30340;&#22836;&#37096;&#65289;&#26469;&#24418;&#25104;&#35299;&#37322;&#12290;&#28982;&#32780;&#65292;&#20687;&#32032;&#36890;&#24120;&#38590;&#20197;&#35299;&#37322;&#24182;&#23545; XAI &#26041;&#27861;&#30340;&#19981;&#20934;&#30830;&#24615;&#25935;&#24863;&#65292;&#32780;&#22312;&#20808;&#21069;&#30340;&#24037;&#20316;&#20013;&#65292;&#8220;&#27010;&#24565;&#8221;&#35201;&#27714;&#20154;&#24037;&#27880;&#37322;&#25110;&#20165;&#38480;&#20110;&#39044;&#23450;&#20041;&#30340;&#27010;&#24565;&#38598;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#21463;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#39537;&#21160;&#65292;Segment Anything &#27169;&#22411;&#65288;SAM&#65289;&#24050;&#32463;&#34987;&#35777;&#26126;&#26159;&#19968;&#31181;&#24378;&#22823;&#21644;&#21487;&#25512;&#24191;&#30340;&#26694;&#26550;&#65292;&#21487;&#25191;&#34892;&#31934;&#30830;&#32780;&#20840;&#38754;&#30340;&#23454;&#20363;&#20998;&#21106;&#65292;&#23454;&#29616;&#20174;&#32473;&#23450;&#22270;&#20687;&#33258;&#21160;&#20934;&#22791;&#27010;&#24565;&#38598;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#32034;&#20351;&#29992; SAM &#22686;&#24378;&#22522;&#20110;&#27010;&#24565;&#30340; XAI&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#32780;&#28789;&#27963;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#26041;&#27861;&#65292;&#21363; Explain Any Concept&#65288;EAC&#65289;&#65292;&#23427;&#21487;&#20197;&#22312;&#22270;&#20687;&#20013;&#20998;&#21106;&#20219;&#20309;&#20869;&#23481;&#65292;&#24182;&#20351;&#29992;&#30456;&#20851;&#30340;&#27010;&#24565;&#24418;&#25104;&#35299;&#37322;&#12290;EAC&#39318;&#20808;&#20351;&#29992;SAM&#29983;&#25104;&#23454;&#20363;&#20998;&#21106;&#25513;&#30721;&#65292;&#28982;&#21518;&#24212;&#29992;&#19968;&#32452;&#27010;&#24565;&#20998;&#31867;&#22120;&#26469;&#35782;&#21035;&#27599;&#20010;&#25513;&#30721;&#30340;&#30456;&#20851;&#27010;&#24565;&#65292;&#26368;&#21518;&#20351;&#29992;&#35782;&#21035;&#20986;&#30340;&#27010;&#24565;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#20004;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;XAI&#26041;&#27861;&#30456;&#27604;&#65292;EAC&#20855;&#26377;&#26174;&#30528;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
EXplainable AI (XAI) is an essential topic to improve human understanding of deep neural networks (DNNs) given their black-box internals. For computer vision tasks, mainstream pixel-based XAI methods explain DNN decisions by identifying important pixels, and emerging concept-based XAI explore forming explanations with concepts (e.g., a head in an image). However, pixels are generally hard to interpret and sensitive to the imprecision of XAI methods, whereas "concepts" in prior works require human annotation or are limited to pre-defined concept sets. On the other hand, driven by large-scale pre-training, Segment Anything Model (SAM) has been demonstrated as a powerful and promotable framework for performing precise and comprehensive instance segmentation, enabling automatic preparation of concept sets from a given image. This paper for the first time explores using SAM to augment concept-based XAI. We offer an effective and flexible concept-based explanation method, namely Explain Any 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#35780;&#20272;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22522;&#20934;&#27979;&#35797;&#20013;&#26576;&#20123;&#31995;&#32479;&#30340;&#24471;&#20998;&#32570;&#22833;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35268;&#27169;&#26356;&#22823;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2305.10284</link><description>&lt;p&gt;
&#26356;&#40065;&#26834;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#35780;&#20272;&#26041;&#27861;&#65306;&#22788;&#29702;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#32570;&#22833;&#24471;&#20998;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Towards More Robust NLP System Evaluation: Handling Missing Scores in Benchmarks. (arXiv:2305.10284v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10284
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#35780;&#20272;&#26041;&#27861;&#65292;&#21487;&#20197;&#35299;&#20915;&#22522;&#20934;&#27979;&#35797;&#20013;&#26576;&#20123;&#31995;&#32479;&#30340;&#24471;&#20998;&#32570;&#22833;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#35268;&#27169;&#26356;&#22823;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#30340;&#35780;&#20272;&#23545;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30446;&#21069;&#30340;&#22522;&#20934;&#27979;&#35797;&#26041;&#27861;&#24120;&#24120;&#20551;&#35774;&#25152;&#26377;&#31995;&#32479;&#22312;&#25152;&#26377;&#20219;&#21153;&#19978;&#37117;&#26377;&#21487;&#29992;&#30340;&#24471;&#20998;&#65292;&#36825;&#24182;&#19981;&#24635;&#26159;&#20999;&#23454;&#21487;&#34892;&#30340;&#12290;&#22312;&#29616;&#23454;&#24773;&#20917;&#19979;&#65292;&#33509;&#24178;&#22240;&#32032;&#65288;&#20363;&#22914;&#36816;&#34892;&#22522;&#32447;&#65292;&#31169;&#26377;&#31995;&#32479;&#65292;&#35745;&#31639;&#38480;&#21046;&#25110;&#19981;&#23436;&#25972;&#30340;&#25968;&#25454;&#65289;&#21487;&#33021;&#20250;&#38459;&#27490;&#26576;&#20123;&#31995;&#32479;&#22312;&#25972;&#20010;&#20219;&#21153;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#26412;&#25991;&#27491;&#24335;&#38416;&#36848;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#29616;&#26377;&#38382;&#39064;&#65306;&#22914;&#20309;&#22312;&#19968;&#20123;&#31995;&#32479;&#30340;&#20219;&#21153;&#24471;&#20998;&#32570;&#22833;&#26102;&#36827;&#34892;&#22522;&#20934;&#27979;&#35797;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35299;&#20915;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20860;&#23481;&#30340;&#37096;&#20998;&#25490;&#21517;&#26041;&#27861;&#26469;&#22635;&#34917;&#32570;&#22833;&#30340;&#25968;&#25454;&#65292;&#28982;&#21518;&#20351;&#29992;Borda&#35745;&#25968;&#26041;&#27861;&#36827;&#34892;&#32858;&#21512;&#12290;&#23427;&#21253;&#25324;&#20004;&#20010;&#29305;&#23450;&#20110;&#20219;&#21153;&#32423;&#24471;&#20998;&#25110;&#23454;&#20363;&#32423;&#24471;&#20998;&#21487;&#29992;&#30340;&#22330;&#26223;&#30340;&#32454;&#21270;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#25193;&#23637;&#22522;&#20934;&#27979;&#35797;&#65292;&#20854;&#20013;&#21253;&#21547;&#36229;&#36807;1.31&#20159;&#20010;&#24471;&#20998;&#65292;&#27604;&#29616;&#26377;&#22522;&#20934;&#27979;&#35797;&#22823;&#19968;&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#24182;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The evaluation of natural language processing (NLP) systems is crucial for advancing the field, but current benchmarking approaches often assume that all systems have scores available for all tasks, which is not always practical. In reality, several factors such as the cost of running baseline, private systems, computational limitations, or incomplete data may prevent some systems from being evaluated on entire tasks. This paper formalize an existing problem in NLP research: benchmarking when some systems scores are missing on the task, and proposes a novel approach to address it. Our method utilizes a compatible partial ranking approach to impute missing data, which is then aggregated using the Borda count method. It includes two refinements designed specifically for scenarios where either task-level or instance-level scores are available. We also introduce an extended benchmark, which contains over 131 million scores, an order of magnitude larger than existing benchmarks. We validate
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#23398;&#20064;&#24230;&#37327;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;&#65292;&#36890;&#36807;&#35757;&#32451;&#25342;&#21462;&#25104;&#21151;&#39044;&#27979;&#22120;&#21644;&#23398;&#20064;&#25342;&#21462;&#36136;&#37327;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#33021;&#22815;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#24378;&#21147;&#25235;&#25569;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.10272</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#24230;&#37327;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Large-Scale Package Manipulation via Learned Metrics of Pick Success. (arXiv:2305.10272v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#22522;&#20110;&#23398;&#20064;&#24230;&#37327;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;&#65292;&#36890;&#36807;&#35757;&#32451;&#25342;&#21462;&#25104;&#21151;&#39044;&#27979;&#22120;&#21644;&#23398;&#20064;&#25342;&#21462;&#36136;&#37327;&#24230;&#37327;&#65292;&#23454;&#29616;&#20102;&#33021;&#22815;&#22823;&#35268;&#27169;&#37096;&#32626;&#30340;&#24378;&#21147;&#25235;&#25569;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#21270;&#20179;&#20648;&#25805;&#20316;&#21487;&#20197;&#38477;&#20302;&#29289;&#27969;&#25104;&#26412;&#65292;&#26368;&#32456;&#38477;&#20302;&#28040;&#36153;&#21697;&#20215;&#26684;&#65292;&#25552;&#39640;&#20132;&#36135;&#36895;&#24230;&#65292;&#24182;&#22686;&#24378;&#23545;&#21171;&#21160;&#21147;&#27874;&#21160;&#30340;&#25269;&#25239;&#33021;&#21147;&#12290;&#36817;&#24180;&#26469;&#65292;&#33258;&#21160;&#21270;&#37325;&#22797;&#20219;&#21153;&#30340;&#20852;&#36259;&#22686;&#21152;&#65292;&#20294;&#22823;&#22810;&#25968;&#26159;&#22312;&#21463;&#25511;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#12290;&#20174;&#26434;&#20081;&#30340;&#22534;&#22534;&#20013;&#25361;&#36873;&#29289;&#21697;&#31561;&#20219;&#21153;&#30452;&#21040;&#26368;&#36817;&#25165;&#21464;&#24471;&#36275;&#22815;&#24378;&#22823;&#65292;&#21487;&#20197;&#22312;&#26368;&#23567;&#20154;&#24037;&#24178;&#39044;&#19979;&#36827;&#34892;&#22823;&#35268;&#27169;&#37096;&#32626;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#20122;&#39532;&#36874;&#26426;&#22120;&#20154;&#30340;Robot Induction&#65288;Robin&#65289;&#32676;&#30340;&#22823;&#35268;&#27169;&#21253;&#35065;&#25805;&#20316;&#65292;&#35813;&#32676;&#21033;&#29992;&#22312;&#23454;&#38469;&#29983;&#20135;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#25342;&#21462;&#25104;&#21151;&#39044;&#27979;&#22120;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#35813;&#31995;&#32479;&#22312;&#36229;&#36807;394K&#20010;&#25342;&#21462;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#23427;&#29992;&#20110;&#25226;&#27599;&#22825;&#39640;&#36798;5&#30334;&#19975;&#20010;&#21253;&#35065;&#36827;&#34892;&#20102;&#20998;&#31163;&#65292;&#26412;&#25991;&#30340;&#35780;&#20272;&#26399;&#38388;&#25805;&#20316;&#20102;&#36229;&#36807;2&#20159;&#20010;&#21253;&#35065;&#12290;&#24320;&#21457;&#30340;&#23398;&#20064;&#25342;&#21462;&#36136;&#37327;&#24230;&#37327;&#23454;&#26102;&#25490;&#21517;&#21508;&#31181;&#25342;&#21462;&#26367;&#20195;&#26041;&#26696;&#65292;&#24182;&#37319;&#29992;&#39640;&#25104;&#21151;&#29575;&#30340;&#24378;&#21147;&#25235;&#25569;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automating warehouse operations can reduce logistics overhead costs, ultimately driving down the final price for consumers, increasing the speed of delivery, and enhancing the resiliency to workforce fluctuations. The past few years have seen increased interest in automating such repeated tasks but mostly in controlled settings. Tasks such as picking objects from unstructured, cluttered piles have only recently become robust enough for large-scale deployment with minimal human intervention.  This paper demonstrates a large-scale package manipulation from unstructured piles in Amazon Robotics' Robot Induction (Robin) fleet, which utilizes a pick success predictor trained on real production data. Specifically, the system was trained on over 394K picks. It is used for singulating up to 5~million packages per day and has manipulated over 200~million packages during this paper's evaluation period.  The developed learned pick quality measure ranks various pick alternatives in real-time and p
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#30456;&#20284;&#24615;&#26041;&#27861;&#21644;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2305.10257</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#23616;&#37096;&#21644;&#20840;&#23616;&#29305;&#24449;&#25552;&#39640;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Improving Link Prediction in Social Networks Using Local and Global Features: A Clustering-based Approach. (arXiv:2305.10257v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10257
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#30456;&#20284;&#24615;&#26041;&#27861;&#21644;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#39640;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#38142;&#25509;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38142;&#25509;&#39044;&#27979;&#38382;&#39064;&#22312;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12289;&#29983;&#29289;&#20449;&#24687;&#23398;&#23454;&#39564;&#12289;&#20132;&#36890;&#32593;&#32476;&#12289;&#21009;&#20107;&#35843;&#26597;&#31561;&#35832;&#22810;&#39046;&#22495;&#20013;&#26085;&#30410;&#31361;&#20986;&#12290;&#29616;&#26377;&#30340;&#25991;&#29486;&#32570;&#20047;&#21033;&#29992;&#27599;&#31181;&#26041;&#27861;&#30340;&#20248;&#21183;&#24182;&#23558;&#23427;&#20204;&#38598;&#25104;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#26041;&#27861;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31532;&#19968;&#21644;&#31532;&#20108;&#32676;&#20307;&#26041;&#27861;&#32467;&#21512;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#20004;&#38454;&#27573;&#24320;&#21457;&#26041;&#27861;&#39318;&#20808;&#30830;&#23450;&#20102;&#19982;&#33410;&#28857;&#20043;&#38388;&#20851;&#31995;&#30456;&#20851;&#30340;&#26032;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Link prediction problem has increasingly become prominent in many domains such as social network analyses, bioinformatics experiments, transportation networks, criminal investigations and so forth. A variety of techniques has been developed for link prediction problem, categorized into 1) similarity based approaches which study a set of features to extract similar nodes; 2) learning based approaches which extract patterns from the input data; 3) probabilistic statistical approaches which optimize a set of parameters to establish a model which can best compute formation probability. However, existing literatures lack approaches which utilize strength of each approach by integrating them to achieve a much more productive one. To tackle the link prediction problem, we propose an approach based on the combination of first and second group methods; the existing studied works use just one of these categories. Our two-phase developed method firstly determines new features related to the posit
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#31350;&#20102; SAM &#27169;&#22411;&#22312;&#31165;&#19994;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126; SAM &#27169;&#22411;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#37096;&#20998;&#30340;&#40481;&#31867;&#20998;&#21106;&#20219;&#21153;&#21644;&#32418;&#22806;&#28909;&#20687;&#30340;&#20351;&#29992;&#65292;&#24182;&#29992;&#20110;&#40481;&#31867;&#36319;&#36394;&#20219;&#21153;&#20013;&#12290;</title><link>http://arxiv.org/abs/2305.10254</link><description>&lt;p&gt;
&#38754;&#21521;&#23478;&#31165;&#31185;&#23398;&#30340; SAM &#27169;&#22411;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SAM for Poultry Science. (arXiv:2305.10254v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10254
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#31350;&#20102; SAM &#27169;&#22411;&#22312;&#31165;&#19994;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126; SAM &#27169;&#22411;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#22522;&#20110;&#37096;&#20998;&#30340;&#40481;&#31867;&#20998;&#21106;&#20219;&#21153;&#21644;&#32418;&#22806;&#28909;&#20687;&#30340;&#20351;&#29992;&#65292;&#24182;&#29992;&#20110;&#40481;&#31867;&#36319;&#36394;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#22312;&#24314;&#31435;&#22823;&#35268;&#27169;&#22522;&#30784;&#27169;&#22411;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#20854;&#20013;&#65292;Meta AI Research &#25512;&#20986;&#30340;&#8220;Segment Anything Model&#8221;(SAM)&#20316;&#20026;&#29289;&#20307;&#20998;&#21106;&#20219;&#21153;&#30340;&#31361;&#30772;&#24615;&#35299;&#20915;&#26041;&#26696;&#32780;&#33073;&#39062;&#32780;&#20986;&#12290;&#34429;&#28982; SAM &#22312;&#21508;&#31181;&#20892;&#19994;&#24212;&#29992;&#20013;&#34920;&#29616;&#20986;&#20102;&#25104;&#21151;&#30340;&#24212;&#29992;&#65292;&#20294;&#20854;&#22312;&#31165;&#19994;&#65292;&#23588;&#20854;&#26159;&#22312;&#26080;&#31548;&#40481;&#30340;&#24773;&#20917;&#19979;&#30340;&#28508;&#21147;&#20173;&#26410;&#20805;&#20998;&#21457;&#25496;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35780;&#20272; SAM &#22312;&#20195;&#34920;&#24615;&#30340;&#40481;&#31867;&#20998;&#21106;&#20219;&#21153;&#19978;&#30340;&#38646;&#26679;&#26412;&#20998;&#21106;&#24615;&#33021;&#65292;&#21253;&#25324;&#22522;&#20110;&#37096;&#20998;&#30340;&#20998;&#21106;&#21644;&#32418;&#22806;&#28909;&#20687;&#30340;&#20351;&#29992;&#65292;&#24182;&#35797;&#22270;&#25506;&#32034;&#20351;&#29992; SAM &#20316;&#20026;&#20998;&#21106;&#24037;&#20855;&#36827;&#34892;&#40481;&#31867;&#36319;&#36394;&#20219;&#21153;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#30456;&#27604;&#20110; SegFormer &#21644; SETR&#65292;&#22312;&#25972;&#20307;&#21644;&#22522;&#20110;&#37096;&#20998;&#30340;&#40481;&#31867;&#20998;&#21106;&#20013;&#65292;SAM &#30340;&#24615;&#33021;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#12290;&#22522;&#20110; SAM &#30340;&#23545;&#35937;&#36319;&#36394;&#20063;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#34892;&#20026;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, the agricultural industry has witnessed significant advancements in artificial intelligence (AI), particularly with the development of large-scale foundational models. Among these foundation models, the Segment Anything Model (SAM), introduced by Meta AI Research, stands out as a groundbreaking solution for object segmentation tasks. While SAM has shown success in various agricultural applications, its potential in the poultry industry, specifically in the context of cage-free hens, remains relatively unexplored. This study aims to assess the zero-shot segmentation performance of SAM on representative chicken segmentation tasks, including part-based segmentation and the use of infrared thermal images, and to explore chicken-tracking tasks by using SAM as a segmentation tool. The results demonstrate SAM's superior performance compared to SegFormer and SETR in both whole and part-based chicken segmentation. SAM-based object tracking also provides valuable data on the beh
&lt;/p&gt;</description></item><item><title>MemoryBank &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#65292;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31867;&#20154;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#23427;&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10250</link><description>&lt;p&gt;
MemoryBank: &#29992;&#38271;&#26399;&#35760;&#24518;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MemoryBank: Enhancing Large Language Models with Long-Term Memory. (arXiv:2305.10250v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10250
&lt;/p&gt;
&lt;p&gt;
MemoryBank &#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#65292;&#26088;&#22312;&#20026;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25552;&#20379;&#31867;&#20154;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;&#23427;&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38761;&#21629;&#24615;&#36827;&#23637;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#25105;&#20204;&#19982;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#20114;&#21160;&#26041;&#24335;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#20854;&#20013;&#19968;&#20010;&#26126;&#26174;&#30340;&#19981;&#36275;&#20043;&#22788;&#26159;&#36825;&#20123;&#27169;&#22411;&#32570;&#20047;&#38271;&#26399;&#35760;&#24518;&#26426;&#21046;&#12290;&#36825;&#22312;&#38656;&#35201;&#25345;&#32493;&#20114;&#21160;&#30340;&#24773;&#20917;&#19979;&#23588;&#20026;&#26126;&#26174;&#65292;&#20363;&#22914;&#20010;&#20154;&#20276;&#20387;&#31995;&#32479;&#21644;&#24515;&#29702;&#21672;&#35810;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MemoryBank&#65292;&#36825;&#26159;&#19968;&#31181;&#19987;&#20026;LLM&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#20869;&#23384;&#26426;&#21046;&#12290;MemoryBank&#21487;&#20197;&#21484;&#21796;&#30456;&#20851;&#35760;&#24518;&#65292;&#36890;&#36807;&#25345;&#32493;&#30340;&#35760;&#24518;&#26356;&#26032;&#19981;&#26029;&#36827;&#21270;&#65292;&#36890;&#36807;&#21512;&#25104;&#36807;&#21435;&#30340;&#20114;&#21160;&#20449;&#24687;&#29702;&#35299;&#24182;&#36866;&#24212;&#29992;&#25143;&#20010;&#24615;&#12290;&#20026;&#20102;&#27169;&#20223;&#20154;&#31867;&#34892;&#20026;&#24182;&#26377;&#36873;&#25321;&#22320;&#20445;&#23384;&#35760;&#24518;&#65292;MemoryBank&#37319;&#29992;&#20102;&#21463;Ebbinghaus&#36951;&#24536;&#26354;&#32447;&#29702;&#35770;&#21551;&#21457;&#30340;&#35760;&#24518;&#26356;&#26032;&#26426;&#21046;&#65292;&#36825;&#26679;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26681;&#25454;&#26102;&#38388;&#21644;&#35760;&#24518;&#30340;&#30456;&#23545;&#37325;&#35201;&#24615;&#26469;&#36951;&#24536;&#21644;&#21152;&#24378;&#35760;&#24518;&#65292;&#20174;&#32780;&#20026;LLM&#25552;&#20379;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#38271;&#26399;&#35760;&#24518;&#12290;
&lt;/p&gt;
&lt;p&gt;
Revolutionary advancements in Large Language Models have drastically reshaped our interactions with artificial intelligence systems. Despite this, a notable hindrance remains-the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems and psychological counseling. Therefore, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user personality by synthesizing information from past interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory, which permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a hum
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;</title><link>http://arxiv.org/abs/2305.10235</link><description>&lt;p&gt;
&#35780;&#20272;LLM&#30340;&#38544;&#34255;&#39118;&#38505;&#65306;&#20851;&#20110;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Assessing Hidden Risks of LLMs: An Empirical Study on Robustness, Consistency, and Credibility. (arXiv:2305.10235v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10235
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26159;&#19968;&#39033;&#20851;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#23545;&#20027;&#27969;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#22823;&#37327;&#26597;&#35810;&#21644;&#20998;&#26512;&#65292;&#32467;&#26524;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#30528;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#23545;&#20110;&#35768;&#22810;&#39046;&#22495;&#20135;&#29983;&#20102;&#37325;&#22823;&#24433;&#21709;&#65292;&#29305;&#21035;&#26159;&#22312;&#20854;&#24320;&#25918;&#24335;&#29615;&#22659;&#65288;&#22914;API&#12289;&#24320;&#28304;&#27169;&#22411;&#21644;&#25554;&#20214;&#65289;&#20013;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;LLMs&#30340;&#24191;&#27867;&#37096;&#32626;&#65292;&#32570;&#20047;&#20840;&#38754;&#35752;&#35770;&#21644;&#20998;&#26512;&#28508;&#22312;&#39118;&#38505;&#30340;&#30740;&#31350;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#21021;&#27493;&#20294;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#65292;&#28085;&#30422;&#20102;LLMs&#31995;&#32479;&#30340;&#40065;&#26834;&#24615;&#12289;&#19968;&#33268;&#24615;&#21644;&#21487;&#20449;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#33258;&#21160;&#21270;&#24037;&#20316;&#27969;&#31243;&#26469;&#22788;&#29702;&#22823;&#37327;&#26597;&#35810;/&#21709;&#24212;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23545;&#21253;&#25324;ChatGPT&#12289;LLaMA&#21644;OPT&#22312;&#20869;&#30340;&#20027;&#27969;LLMs&#36827;&#34892;&#20102;100&#22810;&#19975;&#20010;&#26597;&#35810;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#27969;&#26680;&#24515;&#21253;&#25324;&#25968;&#25454;&#21407;&#35821;&#65292;&#38543;&#21518;&#26159;&#33258;&#21160;&#35299;&#37322;&#22120;&#65292;&#35780;&#20272;&#36825;&#20123;LLMs&#22312;&#19981;&#21516;&#30340;&#23545;&#25239;&#24615;&#24230;&#37327;&#31995;&#32479;&#19979;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#65292;&#25105;&#20204;&#24471;&#20986;&#20102;&#20960;&#20010;&#12289;&#20063;&#35768;&#26159;&#19981;&#24184;&#30340;&#32467;&#35770;&#65292;&#36825;&#20123;&#32467;&#35770;&#30456;&#24403;&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
The recent popularity of large language models (LLMs) has brought a significant impact to boundless fields, particularly through their open-ended ecosystem such as the APIs, open-sourced models, and plugins. However, with their widespread deployment, there is a general lack of research that thoroughly discusses and analyzes the potential risks concealed. In that case, we intend to conduct a preliminary but pioneering study covering the robustness, consistency, and credibility of LLMs systems. With most of the related literature in the era of LLM uncharted, we propose an automated workflow that copes with an upscaled number of queries/responses. Overall, we conduct over a million queries to the mainstream LLMs including ChatGPT, LLaMA, and OPT. Core to our workflow consists of a data primitive, followed by an automated interpreter that evaluates these LLMs under different adversarial metrical systems. As a result, we draw several, and perhaps unfortunate, conclusions that are quite unco
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24418;&#25104;&#30340;&#31751;&#65292;&#25581;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#23616;&#37096;&#23494;&#24230;&#20294;&#26080;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#65292;&#32780;&#30417;&#30563;&#23398;&#20064;&#21019;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#31867;&#22120;&#20316;&#20026;&#22788;&#29702;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;t-SNE&#21487;&#35270;&#21270;&#35777;&#26126;&#20102;&#23545;&#27604;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.10229</link><description>&lt;p&gt;
&#25506;&#32034;&#23545;&#27604;&#23398;&#20064;&#20013;&#30340;&#24402;&#32435;&#20559;&#24046;&#65306;&#20174;&#32858;&#31867;&#35282;&#24230;&#20986;&#21457;
&lt;/p&gt;
&lt;p&gt;
Exploring Inductive Biases in Contrastive Learning: A Clustering Perspective. (arXiv:2305.10229v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10229
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27604;&#36739;&#20102;&#23545;&#27604;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#24418;&#25104;&#30340;&#31751;&#65292;&#25581;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#21487;&#20197;&#29983;&#25104;&#20855;&#26377;&#23616;&#37096;&#23494;&#24230;&#20294;&#26080;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#65292;&#32780;&#30417;&#30563;&#23398;&#20064;&#21019;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#12290;&#21516;&#26102;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#20998;&#31867;&#22120;&#20316;&#20026;&#22788;&#29702;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#21033;&#29992;t-SNE&#21487;&#35270;&#21270;&#35777;&#26126;&#20102;&#23545;&#27604;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20043;&#38388;&#25968;&#25454;&#32452;&#32455;&#30340;&#24046;&#24322;&#65292;&#37325;&#28857;&#20851;&#27880;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#24341;&#20837;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#25351;&#26631;&#65292;&#30456;&#23545;&#23616;&#37096;&#23494;&#24230;&#65288;RLD&#65289;&#65292;&#29992;&#20110;&#23450;&#37327;&#27979;&#37327;&#31751;&#20869;&#30340;&#23616;&#37096;&#23494;&#24230;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#35270;&#35273;&#31034;&#20363;&#65292;&#20197;&#31361;&#20986;&#23616;&#37096;&#23494;&#38598;&#31751;&#21644;&#20840;&#23616;&#23494;&#38598;&#31751;&#20043;&#38388;&#30340;&#21306;&#21035;&#12290;&#36890;&#36807;&#23545;&#27604;&#23545;&#27604;&#23398;&#20064;&#21644;&#30417;&#30563;&#23398;&#20064;&#24418;&#25104;&#30340;&#31751;&#65292;&#25105;&#20204;&#21457;&#29616;&#23545;&#27604;&#23398;&#20064;&#29983;&#25104;&#20855;&#26377;&#23616;&#37096;&#23494;&#24230;&#32780;&#26080;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#65292;&#32780;&#30417;&#30563;&#23398;&#20064;&#21019;&#24314;&#20855;&#26377;&#23616;&#37096;&#21644;&#20840;&#23616;&#23494;&#24230;&#30340;&#31751;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#20998;&#31867;&#22120;&#20316;&#20026;&#22788;&#29702;&#23616;&#37096;&#23494;&#38598;&#31751;&#30340;&#32447;&#24615;&#20998;&#31867;&#22120;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21033;&#29992;t-SNE&#21487;&#35270;&#21270;&#26469;&#35777;&#26126;&#23545;&#27604;&#21644;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#20135;&#29983;&#30340;&#29305;&#24449;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#26469;&#30340;&#30740;&#31350;&#26041;&#21521;&#26469;&#32467;&#26463;&#26412;&#25991;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the differences in data organization between contrastive and supervised learning methods, focusing on the concept of locally dense clusters. We introduce a novel metric, Relative Local Density (RLD), to quantitatively measure local density within clusters. Visual examples are provided to highlight the distinctions between locally dense clusters and globally dense ones. By comparing the clusters formed by contrastive and supervised learning, we reveal that contrastive learning generates locally dense clusters without global density, while supervised learning creates clusters with both local and global density. We further explore the use of a Graph Convolutional Network (GCN) classifier as an alternative to linear classifiers for handling locally dense clusters. Finally, we utilize t-SNE visualizations to substantiate the differences between the features generated by contrastive and supervised learning methods. We conclude by proposing future research directions, 
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#30340;&#21487;&#20998;&#24615;&#21644;&#31163;&#25955;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;S&amp;S&#27604;&#30340;&#26377;&#25928;SVM&#27491;&#21017;&#21270;&#21442;&#25968;&#12289;&#26680;&#20989;&#25968;&#21644;&#26680;&#21442;&#25968;&#36873;&#25321;&#26041;&#27861;&#65292;&#34920;&#29616;&#36739;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#12290;</title><link>http://arxiv.org/abs/2305.10219</link><description>&lt;p&gt;
&#22522;&#20110;&#21487;&#20998;&#24615;&#21644;&#31163;&#25955;&#24230;&#27604;&#30340;SVM&#27491;&#21017;&#21270;&#21442;&#25968;&#12289;&#26680;&#20989;&#25968;&#21644;&#26680;&#21442;&#25968;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Separability and Scatteredness (S&amp;S) Ratio-Based Efficient SVM Regularization Parameter, Kernel, and Kernel Parameter Selection. (arXiv:2305.10219v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10219
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#36890;&#36807;&#20998;&#26512;&#25968;&#25454;&#30340;&#21487;&#20998;&#24615;&#21644;&#31163;&#25955;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;S&amp;S&#27604;&#30340;&#26377;&#25928;SVM&#27491;&#21017;&#21270;&#21442;&#25968;&#12289;&#26680;&#20989;&#25968;&#21644;&#26680;&#21442;&#25968;&#36873;&#25321;&#26041;&#27861;&#65292;&#34920;&#29616;&#36739;&#20256;&#32479;&#26041;&#27861;&#26356;&#20248;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;SVM&#65289;&#26159;&#19968;&#31181;&#20855;&#26377;&#24191;&#27867;&#24212;&#29992;&#30340;&#40065;&#26834;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#65292;&#21487;&#29992;&#20110;&#20998;&#31867;&#12289;&#22238;&#24402;&#21644;&#24322;&#24120;&#20540;&#26816;&#27979;&#12290;SVM&#38656;&#35201;&#35843;&#25972;&#27491;&#21017;&#21270;&#21442;&#25968;&#65288;RP&#65289;&#26469;&#25511;&#21046;&#27169;&#22411;&#23481;&#37327;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#36807;&#20132;&#21449;&#39564;&#35777;&#65288;CV&#65289;&#36807;&#31243;&#23545;&#19968;&#31995;&#21015;&#22791;&#36873;RP&#36827;&#34892;&#27604;&#36739;&#20197;&#25214;&#21040;&#26368;&#20339;RP&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#38750;&#32447;&#24615;&#21487;&#20998;&#25968;&#25454;&#65292;SVM&#20351;&#29992;&#26680;&#20989;&#25968;&#65292;&#22312;&#26680;&#20989;&#25968;&#30340;&#32593;&#26684;&#20013;&#36873;&#25321;&#19968;&#32452;&#20855;&#26377;&#19968;&#32452;&#21442;&#25968;&#30340;&#26680;&#20989;&#25968;&#12290;RP&#21644;&#26680;&#32593;&#26684;&#30340;&#26368;&#20339;&#36873;&#25321;&#26159;&#36890;&#36807;CV&#30340;&#32593;&#26684;&#25628;&#32034;&#33719;&#24471;&#30340;&#12290;&#36890;&#36807;&#38543;&#26426;&#20998;&#26512;&#27491;&#21017;&#21270;&#21442;&#25968;&#30340;&#34892;&#20026;&#65292;&#26412;&#25991;&#23637;&#31034;&#20102;SVM&#24615;&#33021;&#21487;&#20197;&#24314;&#27169;&#20026;&#25968;&#25454;&#30340;&#21487;&#20998;&#24615;&#21644;&#31163;&#25955;&#24230;&#65288;S&amp;S&#65289;&#30340;&#20989;&#25968;&#12290;&#21487;&#20998;&#24615;&#26159;&#31867;&#21035;&#20043;&#38388;&#36317;&#31163;&#30340;&#24230;&#37327;&#65292;&#31163;&#25955;&#24230;&#26159;&#25968;&#25454;&#28857;&#30340;&#20256;&#25773;&#27604;&#29575;&#12290;&#29305;&#21035;&#22320;&#65292;&#23545;&#20110;&#38128;&#38142;&#25439;&#22833;&#25104;&#26412;&#20989;&#25968;&#65292;S&amp;S&#27604;&#21487;&#20197;&#26377;&#25928;&#22320;&#20272;&#35745;&#26368;&#20248;RP&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;S&amp;S&#27604;&#30340;&#39640;&#25928;&#36873;&#25321;&#26680;&#20989;&#25968;&#21450;&#20854;&#21442;&#25968;&#26041;&#27861;&#12290;&#22312;&#21508;&#31181;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#27604;&#36739;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#19982;&#20256;&#32479;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#26041;&#27861;&#20855;&#26377;&#26356;&#23569;&#30340;&#38656;&#35201;&#35843;&#25972;&#30340;&#36229;&#21442;&#25968;&#19988;&#24615;&#33021;&#20248;&#24322;&#25110;&#21487;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
Support Vector Machine (SVM) is a robust machine learning algorithm with broad applications in classification, regression, and outlier detection. SVM requires tuning the regularization parameter (RP) which controls the model capacity and the generalization performance. Conventionally, the optimum RP is found by comparison of a range of values through the Cross-Validation (CV) procedure. In addition, for non-linearly separable data, the SVM uses kernels where a set of kernels, each with a set of parameters, denoted as a grid of kernels, are considered. The optimal choice of RP and the grid of kernels is through the grid-search of CV. By stochastically analyzing the behavior of the regularization parameter, this work shows that the SVM performance can be modeled as a function of separability and scatteredness (S&amp;S) of the data. Separability is a measure of the distance between classes, and scatteredness is the ratio of the spread of data points. In particular, for the hinge loss cost fun
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36845;&#20195;&#26799;&#24230;&#22522;&#30784;&#25237;&#24433;&#65288;IGBP&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31070;&#32463;&#34920;&#31034;&#20013;&#21024;&#38500;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#31070;&#32463;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#26576;&#20010;&#25935;&#24863;&#23646;&#24615;&#65292;&#28982;&#21518;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#19968;&#20010;&#36229;&#24179;&#38754;&#19978;&#65292;&#20351;&#24471;&#20998;&#31867;&#22120;&#23545;&#30446;&#26631;&#23646;&#24615;&#21464;&#24471;&#26080;&#24847;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#28040;&#38500;&#25935;&#24863;&#23646;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#24433;&#21709;&#24456;&#23567;&#12290;</title><link>http://arxiv.org/abs/2305.10204</link><description>&lt;p&gt;
&#25252;&#30462;&#24335;&#34920;&#31034;&#65306;&#36890;&#36807;&#36845;&#20195;&#22522;&#20110;&#26799;&#24230;&#30340;&#25237;&#24433;&#20445;&#25252;&#25935;&#24863;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
Shielded Representations: Protecting Sensitive Attributes Through Iterative Gradient-Based Projection. (arXiv:2305.10204v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36845;&#20195;&#26799;&#24230;&#22522;&#30784;&#25237;&#24433;&#65288;IGBP&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31070;&#32463;&#34920;&#31034;&#20013;&#21024;&#38500;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#65292;&#20197;&#20943;&#36731;&#27169;&#22411;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#31070;&#32463;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#26576;&#20010;&#25935;&#24863;&#23646;&#24615;&#65292;&#28982;&#21518;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#19968;&#20010;&#36229;&#24179;&#38754;&#19978;&#65292;&#20351;&#24471;&#20998;&#31867;&#22120;&#23545;&#30446;&#26631;&#23646;&#24615;&#21464;&#24471;&#26080;&#24847;&#35782;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#28040;&#38500;&#25935;&#24863;&#23646;&#24615;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#24433;&#21709;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#20542;&#21521;&#20110;&#23398;&#20064;&#21644;&#32534;&#30721;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#35299;&#20915;&#27492;&#31867;&#20559;&#24046;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#26159;&#28040;&#38500;&#27169;&#22411;&#34920;&#31034;&#20013;&#32534;&#30721;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#21024;&#38500;&#32447;&#24615;&#32534;&#30721;&#30340;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36845;&#20195;&#26799;&#24230;&#22522;&#30784;&#25237;&#24433;&#65288;IGBP&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#31070;&#32463;&#34920;&#31034;&#20013;&#21024;&#38500;&#38750;&#32447;&#24615;&#32534;&#30721;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21253;&#25324;&#36890;&#36807;&#36845;&#20195;&#35757;&#32451;&#31070;&#32463;&#20998;&#31867;&#22120;&#26469;&#39044;&#27979;&#25105;&#20204;&#35201;&#28040;&#38500;&#30340;&#29305;&#23450;&#23646;&#24615;&#65292;&#28982;&#21518;&#23558;&#34920;&#31034;&#25237;&#24433;&#21040;&#19968;&#20010;&#36229;&#24179;&#38754;&#19978;&#65292;&#20351;&#24471;&#20998;&#31867;&#22120;&#23545;&#30446;&#26631;&#23646;&#24615;&#21464;&#24471;&#26080;&#24847;&#35782;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#28040;&#38500;&#24615;&#21035;&#21644;&#31181;&#26063;&#20449;&#24687;&#20316;&#20026;&#25935;&#24863;&#23646;&#24615;&#30340;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;IGBP&#36890;&#36807;&#20869;&#22312;&#21644;&#22806;&#22312;&#35780;&#20272;&#22312;&#20943;&#36731;&#20559;&#35265;&#26041;&#38754;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#23545;&#19979;&#28216;&#20219;&#21153;&#30340;&#20934;&#30830;&#24615;&#24433;&#21709;&#24456;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language processing models tend to learn and encode social biases present in the data. One popular approach for addressing such biases is to eliminate encoded information from the model's representations. However, current methods are restricted to removing only linearly encoded information. In this work, we propose Iterative Gradient-Based Projection (IGBP), a novel method for removing non-linear encoded concepts from neural representations. Our method consists of iteratively training neural classifiers to predict a particular attribute we seek to eliminate, followed by a projection of the representation on a hypersurface, such that the classifiers become oblivious to the target attribute. We evaluate the effectiveness of our method on the task of removing gender and race information as sensitive attributes. Our results demonstrate that IGBP is effective in mitigating bias through intrinsic and extrinsic evaluations, with minimal impact on downstream task accuracy.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#30149;&#21382;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;&#23545;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#36827;&#34892;&#27515;&#20129;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#21457;&#29616;&#20020;&#24202;&#21307;&#29983;&#25152;&#20889;&#30340;SL&#20250;&#23545;AI&#24615;&#33021;&#34920;&#29616;&#19981;&#21033;&#65292;&#23588;&#20854;&#26159;&#22312;&#40657;&#20154;&#24739;&#32773;&#20013;&#34920;&#29616;&#26356;&#20026;&#26126;&#26174;&#65292;&#24378;&#35843;&#20102;&#29702;&#35299;&#20559;&#35265;&#23545;&#19979;&#28216;AI&#24615;&#33021;&#30340;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24320;&#21457;&#26356;&#20855;&#20844;&#24179;&#21644;&#27491;&#20041;&#30340;&#21307;&#30103;&#31995;&#32479;&#12290;</title><link>http://arxiv.org/abs/2305.10201</link><description>&lt;p&gt;
&#20154;&#20204;&#20132;&#35848;&#65292;AI&#20542;&#21548;&#65306;&#30005;&#23376;&#30149;&#21382;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;&#23545;AI&#21028;&#26029;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
People Talking and AI Listening: How Stigmatizing Language in EHR Notes Affect AI Performance. (arXiv:2305.10201v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#30005;&#23376;&#30149;&#21382;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;&#23545;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#36827;&#34892;&#27515;&#20129;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#21457;&#29616;&#20020;&#24202;&#21307;&#29983;&#25152;&#20889;&#30340;SL&#20250;&#23545;AI&#24615;&#33021;&#34920;&#29616;&#19981;&#21033;&#65292;&#23588;&#20854;&#26159;&#22312;&#40657;&#20154;&#24739;&#32773;&#20013;&#34920;&#29616;&#26356;&#20026;&#26126;&#26174;&#65292;&#24378;&#35843;&#20102;&#29702;&#35299;&#20559;&#35265;&#23545;&#19979;&#28216;AI&#24615;&#33021;&#30340;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24320;&#21457;&#26356;&#20855;&#20844;&#24179;&#21644;&#27491;&#20041;&#30340;&#21307;&#30103;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#30149;&#21382;(EHRs)&#26159;&#26399;&#26395;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;(AI)-&#39537;&#21160;&#30340;&#21307;&#30103;&#36716;&#22411;&#30340;&#37325;&#35201;&#25968;&#25454;&#26469;&#28304;&#12290;&#28982;&#32780;&#65292;&#21453;&#26144;&#22312;EHR&#31508;&#35760;&#20013;&#30340;&#20020;&#24202;&#21307;&#24072;&#20559;&#35265;&#21487;&#33021;&#20250;&#23548;&#33268;AI&#27169;&#22411;&#32487;&#25215;&#24182;&#25918;&#22823;&#36825;&#20123;&#20559;&#35265;&#65292;&#20174;&#32780;&#19981;&#26029;&#21152;&#21095;&#20581;&#24247;&#19978;&#30340;&#19981;&#24179;&#31561;&#12290;&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;EHR&#31508;&#35760;&#20013;&#27745;&#21517;&#21270;&#35821;&#35328;(SL)&#23545;&#22522;&#20110;Transformer&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21644;&#21487;&#35299;&#37322;AI(XAI)&#25216;&#26415;&#36827;&#34892;&#27515;&#20129;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#20020;&#24202;&#21307;&#29983;&#25152;&#20889;&#30340;SL&#19981;&#21033;&#20110;AI&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#23588;&#20854;&#26159;&#22312;&#40657;&#20154;&#24739;&#32773;&#20013;&#34920;&#29616;&#26356;&#20026;&#26126;&#26174;&#65292;&#31361;&#20986;&#20102;SL&#20316;&#20026;AI&#27169;&#22411;&#21457;&#23637;&#20013;&#31181;&#26063;&#24046;&#24322;&#30340;&#19968;&#31181;&#26469;&#28304;&#12290;&#20026;&#25506;&#32034;&#19968;&#31181;&#25805;&#20316;&#19978;&#26377;&#25928;&#30340;&#32531;&#35299;SL&#24433;&#21709;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20020;&#24202;&#21307;&#29983;&#21327;&#20316;&#32593;&#32476;&#20013;SL&#29983;&#25104;&#30340;&#27169;&#24335;&#65292;&#21457;&#29616;&#20013;&#22830;&#21307;&#29983;&#23545;AI&#27169;&#22411;&#20013;&#30340;&#31181;&#26063;&#24046;&#24322;&#20855;&#26377;&#26356;&#24378;&#30340;&#24433;&#21709;&#21147;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#21024;&#38500;&#20013;&#22830;&#20020;&#24202;&#21307;&#29983;&#25776;&#20889;&#30340;SL&#26159;&#30456;&#23545;&#20110;&#38543;&#26426;&#36873;&#25321;&#20020;&#24202;&#21307;&#29983;&#32780;&#35328;&#65292;&#32531;&#35299;SL&#23545;AI&#24615;&#33021;&#24433;&#21709;&#30340;&#26356;&#20026;&#26377;&#25928;&#30340;&#31574;&#30053;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#29702;&#35299;&#21453;&#26144;&#22312;EHR&#31508;&#35760;&#20013;&#30340;&#20020;&#24202;&#21307;&#24072;&#20559;&#35265;&#23545;&#19979;&#28216;AI&#24615;&#33021;&#30340;&#24433;&#21709;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#24320;&#21457;&#26356;&#20855;&#20844;&#24179;&#21644;&#27491;&#20041;&#30340;&#21307;&#30103;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electronic health records (EHRs) serve as an essential data source for the envisioned artificial intelligence (AI)-driven transformation in healthcare. However, clinician biases reflected in EHR notes can lead to AI models inheriting and amplifying these biases, perpetuating health disparities. This study investigates the impact of stigmatizing language (SL) in EHR notes on mortality prediction using a Transformer-based deep learning model and explainable AI (XAI) techniques. Our findings demonstrate that SL written by clinicians adversely affects AI performance, particularly so for black patients, highlighting SL as a source of racial disparity in AI model development. To explore an operationally efficient way to mitigate SL's impact, we investigate patterns in the generation of SL through a clinicians' collaborative network, identifying central clinicians as having a stronger impact on racial disparity in the AI model. We find that removing SL written by central clinicians is a more 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26377;&#25928;&#22320;&#35745;&#31639;&#26080;&#24207;&#36879;&#26126;&#24230;&#65292;&#20855;&#22791;&#24555;&#36895;&#24615;&#12289;&#21344;&#29992;&#20869;&#23384;&#23569;&#20197;&#21450;&#26356;&#20026;&#20934;&#30830;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#25152;&#26377;&#22330;&#26223;&#65292;&#26080;&#38656;&#36229;&#21069;&#35774;&#32622;&#65292;&#19988;&#22312;&#36816;&#29992;&#26222;&#21450;GPU&#30340;&#25152;&#26377;&#24179;&#21488;&#19978;&#37117;&#26377;&#20415;&#25463;&#30340;&#31227;&#26893;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10197</link><description>&lt;p&gt;
&#28145;&#24230;&#24555;&#36895;&#30340;&#36817;&#20284;&#26080;&#24207;&#36879;&#26126;&#24230;
&lt;/p&gt;
&lt;p&gt;
Deep and Fast Approximate Order Independent Transparency. (arXiv:2305.10197v1 [cs.GR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26377;&#25928;&#22320;&#35745;&#31639;&#26080;&#24207;&#36879;&#26126;&#24230;&#65292;&#20855;&#22791;&#24555;&#36895;&#24615;&#12289;&#21344;&#29992;&#20869;&#23384;&#23569;&#20197;&#21450;&#26356;&#20026;&#20934;&#30830;&#12290;&#35813;&#26041;&#27861;&#36866;&#29992;&#20110;&#25152;&#26377;&#22330;&#26223;&#65292;&#26080;&#38656;&#36229;&#21069;&#35774;&#32622;&#65292;&#19988;&#22312;&#36816;&#29992;&#26222;&#21450;GPU&#30340;&#25152;&#26377;&#24179;&#21488;&#19978;&#37117;&#26377;&#20415;&#25463;&#30340;&#31227;&#26893;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#26377;&#25928;&#35745;&#31639;&#26080;&#24207;&#36879;&#26126;&#24230;(OIT)&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24555;&#36895;&#12289;&#38656;&#35201;&#26497;&#23569;&#30340;&#20869;&#23384;(&#20165;&#21462;&#20915;&#20110;&#23631;&#24149;&#20998;&#36776;&#29575;&#32780;&#19981;&#26159;&#19977;&#35282;&#24418;&#25110;&#36879;&#26126;&#23618;&#25968;&#37327;)&#65292;&#19982;&#20808;&#21069;&#30340;&#36817;&#20284;&#26041;&#27861;&#30456;&#27604;&#26356;&#31934;&#30830;&#65292;&#36866;&#29992;&#20110;&#27599;&#20010;&#22330;&#26223;&#32780;&#26080;&#38656;&#35774;&#32622;&#65292;&#24182;&#19988;&#21487;&#22312;&#36816;&#34892;&#29978;&#33267;&#20351;&#29992;&#21830;&#21697;GPU&#30340;&#25152;&#26377;&#24179;&#21488;&#19978;&#31227;&#26893;&#12290;&#25105;&#20204;&#38656;&#35201;&#28210;&#26579;&#36890;&#34892;&#35777;&#26469;&#25552;&#21462;&#25152;&#26377;&#29305;&#24449;&#65292;&#38543;&#21518;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#39044;&#27979;&#25972;&#20010;OIT&#20687;&#32032;&#39068;&#33394;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#27604;&#36739;&#24615;&#30340;&#23454;&#39564;&#35780;&#20272;&#21644;&#25152;&#26377;&#26041;&#27861;&#30340;&#30528;&#33394;&#22120;&#28304;&#20195;&#30721;&#20197;&#22797;&#21046;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a machine learning approach for efficiently computing order independent transparency (OIT). Our method is fast, requires a small constant amount of memory (depends only on the screen resolution and not on the number of triangles or transparent layers), is more accurate as compared to previous approximate methods, works for every scene without setup and is portable to all platforms running even with commodity GPUs. Our method requires a rendering pass to extract all features that are subsequently used to predict the overall OIT pixel color with a pre-trained neural network. We provide a comparative experimental evaluation and shader source code of all methods for reproduction of the experiments.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24635;&#32467;&#20102;&#38646;&#20195;&#35789;&#32763;&#35793;&#65288;ZPT&#65289;&#39046;&#22495;&#31070;&#32463;&#32593;&#32476;&#20840;&#38754;&#25512;&#24191;&#21518;&#30340;&#37325;&#35201;&#24037;&#20316;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#20219;&#21153;&#25110;&#36801;&#31227;&#23398;&#20064;&#37117;&#21487;&#20197;&#23454;&#29616;ZPT&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2305.10196</link><description>&lt;p&gt;
&#38646;&#20195;&#35789;&#32763;&#35793;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey on Zero Pronoun Translation. (arXiv:2305.10196v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10196
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24635;&#32467;&#20102;&#38646;&#20195;&#35789;&#32763;&#35793;&#65288;ZPT&#65289;&#39046;&#22495;&#31070;&#32463;&#32593;&#32476;&#20840;&#38754;&#25512;&#24191;&#21518;&#30340;&#37325;&#35201;&#24037;&#20316;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#22810;&#20219;&#21153;&#25110;&#36801;&#31227;&#23398;&#20064;&#37117;&#21487;&#20197;&#23454;&#29616;ZPT&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38646;&#20195;&#35789;&#65288;ZP&#65289;&#36890;&#24120;&#22312;&#31867;&#20284;&#20013;&#25991;&#12289;&#21256;&#29273;&#21033;&#35821;&#21644;&#21360;&#22320;&#35821;&#36825;&#26679;&#30340;&#20002;&#30465;&#30053;&#65292;&#32780;&#22312;&#38750;&#20002;&#22833;&#30465;&#20221;&#35832;&#22914;&#33521;&#35821;&#20013;&#65292;&#24212;&#24403;&#36827;&#34892;&#22238;&#24212;&#12290;&#36825;&#19968;&#29616;&#35937;&#22312;&#26426;&#22120;&#32763;&#35793;&#65288;MT&#65289;&#39046;&#22495;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#65292;&#22240;&#20026;&#23427;&#24456;&#38590;&#30830;&#23450;&#20195;&#35789;&#30340;&#27491;&#30830;&#20808;&#34892;&#35789;&#65292;&#36825;&#26159;MT&#31995;&#32479;&#38754;&#20020;&#30340;&#37325;&#35201;&#25361;&#25112;&#12290;&#26412;&#25991;&#24635;&#32467;&#20102;&#31070;&#32463;&#32593;&#32476;&#20840;&#38754;&#25512;&#23637;&#20043;&#21518;&#22312;&#38646;&#20195;&#35789;&#32763;&#35793;&#65288;ZPT&#65289;&#26041;&#38754;&#25152;&#20570;&#30340;&#37325;&#35201;&#24037;&#20316;&#65292;&#20197;&#20415;&#30740;&#31350;&#20154;&#21592;&#20102;&#35299;&#24403;&#21069;&#29366;&#24577;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;&#25105;&#20204;&#26681;&#25454;&#28436;&#21464;&#12289;&#25968;&#25454;&#38598;&#12289;&#26041;&#27861;&#21644;&#35780;&#20272;&#25552;&#20379;&#20102;&#19968;&#20221;&#25991;&#29486;&#32452;&#32455;&#24418;&#24335;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#27604;&#36739;&#21644;&#20998;&#26512;&#20102;&#22312;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#31454;&#20105;&#27169;&#22411;&#21644;&#35780;&#20272;&#25351;&#26631;&#12290;&#25105;&#20204;&#25366;&#25496;&#20102;&#19968;&#20123;&#26377;&#30410;&#30340;&#21457;&#29616;&#65292;&#20363;&#22914;&#65306;1&#65289;ZPT&#31526;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21457;&#23637;&#36235;&#21183;&#65307;2&#65289;&#25968;&#25454;&#38480;&#21046;&#20250;&#22312;&#19981;&#21516;&#35821;&#35328;&#21644;&#39046;&#22495;&#20013;&#20135;&#29983;&#23398;&#20064;&#20559;&#24046;&#65307;3&#65289;&#36890;&#36807;&#22810;&#20219;&#21153;&#25110;&#36801;&#31227;&#23398;&#20064;&#21487;&#20197;&#23454;&#29616;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
Zero pronouns (ZPs) are frequently omitted in pro-drop languages (e.g. Chinese, Hungarian, and Hindi), but should be recalled in non-pro-drop languages (e.g. English). This phenomenon has been studied extensively in machine translation (MT), as it poses a significant challenge for MT systems due to the difficulty in determining the correct antecedent for the pronoun. This survey paper highlights the major works that have been undertaken in zero pronoun translation (ZPT) after the neural revolution, so that researchers can recognise the current state and future directions of this field. We provide an organisation of the literature based on evolution, dataset, method and evaluation. In addition, we compare and analyze competing models and evaluation metrics on different benchmarks. We uncover a number of insightful findings such as: 1) ZPT is in line with the development trend of large language model; 2) data limitation causes learning bias in languages and domains; 3) performance improv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#20256;&#36882;&#21338;&#24328;&#21644;&#29702;&#24615;&#35328;&#35821;&#34892;&#20026;&#26694;&#26550;&#30340;&#21464;&#20307;sRSA&#65292;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#39046;&#22495;&#20013;&#30340;&#23454;&#29992;&#25512;&#29702;&#38382;&#39064;&#12290;&#22312;&#39068;&#33394;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#37319;&#29992;sRSA&#30340;&#20195;&#29702;&#27604;&#20256;&#32479;RSA&#21644;&#20165;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#29702;&#26356;&#25509;&#36817;&#20110;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2305.10167</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#20256;&#36882;&#21338;&#24328;&#20013;&#30340;&#23454;&#29992;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Pragmatic Reasoning in Structured Signaling Games. (arXiv:2305.10167v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10167
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#32467;&#26500;&#21270;&#20256;&#36882;&#21338;&#24328;&#21644;&#29702;&#24615;&#35328;&#35821;&#34892;&#20026;&#26694;&#26550;&#30340;&#21464;&#20307;sRSA&#65292;&#24212;&#29992;&#20110;&#32467;&#26500;&#21270;&#39046;&#22495;&#20013;&#30340;&#23454;&#29992;&#25512;&#29702;&#38382;&#39064;&#12290;&#22312;&#39068;&#33394;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#37319;&#29992;sRSA&#30340;&#20195;&#29702;&#27604;&#20256;&#32479;RSA&#21644;&#20165;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#29702;&#26356;&#25509;&#36817;&#20110;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#24102;&#26377;&#30456;&#20284;&#24615;&#32467;&#26500;&#30340;&#32467;&#26500;&#21270;&#20256;&#36882;&#21338;&#24328;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#29702;&#24615;&#35328;&#35821;&#34892;&#20026;&#26694;&#26550;&#30340;&#21464;&#20307;&#65292;&#31216;&#20026;&#32467;&#26500;&#21270;&#29702;&#24615;&#35328;&#35821;&#34892;&#20026;&#65288;sRSA&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#32467;&#26500;&#21270;&#39046;&#22495;&#30340;&#23454;&#29992;&#25512;&#29702;&#38382;&#39064;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#22312;&#39068;&#33394;&#39046;&#22495;&#20013;&#37319;&#29992;sRSA&#30340;&#29702;&#24615;&#26234;&#33021;&#20195;&#29702;&#30340;&#34892;&#20026;&#65292;&#35777;&#26126;&#20102;&#37319;&#29992;World Color Survey&#24471;&#20986;&#30340;&#35821;&#20041;&#34920;&#31034;&#30340;&#32467;&#26500;&#21270;&#20195;&#29702;&#27604;&#20256;&#32479;RSA&#21644;&#20165;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#20195;&#29702;&#26356;&#25509;&#36817;&#20110;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#65292;&#19988;&#32463;&#36807;1&#25110;2&#27425;&#36882;&#24402;&#30340;&#35757;&#32451;&#23601;&#33021;&#22815;&#36798;&#21040;&#25928;&#29575;&#26497;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#23454;&#29992;&#25512;&#29702;&#21644;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20013;&#30340;&#23398;&#20064;&#36807;&#31243;&#21644;&#30456;&#20114;&#20316;&#29992;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;sRSA&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#21457;&#23637;&#20986;&#30340;&#36890;&#20449;&#31574;&#30053;&#26356;&#25509;&#36817;&#20110;&#20449;&#24687;&#29702;&#35770;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we introduce a structured signaling game, an extension of the classical signaling game with a similarity structure between meanings in the context, along with a variant of the Rational Speech Act (RSA) framework which we call structured-RSA (sRSA) for pragmatic reasoning in structured domains. We explore the behavior of the sRSA in the domain of color and show that pragmatic agents using sRSA on top of semantic representations, derived from the World Color Survey, attain efficiency very close to the information theoretic limit after only 1 or 2 levels of recursion. We also explore the interaction between pragmatic reasoning and learning in multi-agent reinforcement learning framework. Our results illustrate that artificial agents using sRSA develop communication closer to the information theoretic frontier compared to agents using RSA and just reinforcement learning. We also find that the ambiguity of the semantic representation increases as the pragmatic agents are allowe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10163</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#30340;&#29983;&#25104;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#20013;&#22269;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#19978;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Qualifying Chinese Medical Licensing Examination with Knowledge Enhanced Generative Pre-training Model. (arXiv:2305.10163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#22312;ChatGPT&#20013;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#20013;&#21462;&#24471;&#25104;&#21151;&#65292;&#36825;&#20026;&#24314;&#31435;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#30340;&#21019;&#26032;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;GPT&#65289;&#65292;&#22914;ChatGPT&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;ChatGPT&#24050;&#34987;&#25972;&#21512;&#21040;&#21508;&#20010;&#39046;&#22495;&#30340;&#24037;&#20316;&#27969;&#20013;&#20197;&#25552;&#39640;&#25928;&#29575;&#65292;&#20294;&#20854;&#24494;&#35843;&#36807;&#31243;&#30340;&#28789;&#27963;&#24615;&#19981;&#36275;&#65292;&#38459;&#30861;&#20102;&#20854;&#22312;&#38656;&#35201;&#24191;&#27867;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#21644;&#35821;&#20041;&#30693;&#35782;&#30340;&#39046;&#22495;&#65292;&#22914;&#21307;&#30103;&#20445;&#20581;&#65292;&#30340;&#24212;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;ChatGPT&#22312;&#20013;&#22269;&#22269;&#23478;&#21307;&#23398;&#25191;&#19994;&#21307;&#24072;&#36164;&#26684;&#32771;&#35797;&#65288;CNMLE&#65289;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#25913;&#36827;ChatGPT&#65292;&#21363;&#20174;&#20004;&#20010;&#26041;&#38754;&#38598;&#25104;&#21307;&#23398;&#39046;&#22495;&#30693;&#35782;&#21644;&#21551;&#29992;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26816;&#32034;&#26041;&#27861;&#65292;&#23558;&#21307;&#23398;&#32972;&#26223;&#30693;&#35782;&#25552;&#21462;&#20026;&#35821;&#20041;&#25351;&#20196;&#26469;&#25351;&#23548;ChatGPT&#30340;&#25512;&#26029;&#12290;&#31867;&#20284;&#22320;&#65292;&#30456;&#20851;&#30340;&#21307;&#30103;&#38382;&#39064;&#34987;&#35782;&#21035;&#24182;&#20316;&#20026;&#28436;&#31034;&#36755;&#20837;&#32473;ChatGPT&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30452;&#25509;&#24212;&#29992;ChatGPT&#26080;&#27861;&#22312;CNMLE&#19978;&#33719;&#24471;&#21512;&#26684;&#20998;&#25968;&#65288;51&#20998;&#65289;&#65292;&#21482;&#26377;&#22522;&#20110;&#30693;&#35782;&#22686;&#24378;&#35757;&#32451;&#30340;&#27169;&#22411;&#25104;&#21151;&#36890;&#36807;&#32771;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-Training (GPT) models like ChatGPT have demonstrated exceptional performance in various Natural Language Processing (NLP) tasks. Although ChatGPT has been integrated into the overall workflow to boost efficiency in many domains, the lack of flexibility in the finetuning process hinders its applications in areas that demand extensive domain expertise and semantic knowledge, such as healthcare. In this paper, we evaluate ChatGPT on the China National Medical Licensing Examination (CNMLE) and propose a novel approach to improve ChatGPT from two perspectives: integrating medical domain knowledge and enabling few-shot learning. By using a simple but effective retrieval method, medical background knowledge is extracted as semantic instructions to guide the inference of ChatGPT. Similarly, relevant medical questions are identified and fed as demonstrations to ChatGPT. Experimental results show that directly applying ChatGPT fails to qualify the CNMLE at a score of 51 (i.e., onl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#35759;&#30340;&#22810;&#20195;&#29702;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22823;&#22411;&#39118;&#30005;&#22330;&#22810;&#21464;&#37327;&#25511;&#21046;&#26041;&#27861;&#65292;&#23558;&#39118;&#30005;&#22330;&#20998;&#25104;&#22810;&#20010;&#32858;&#21512;&#22120;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#36890;&#35759;&#36827;&#34892;&#20449;&#24687;&#20132;&#25442;&#26469;&#26368;&#22823;&#21270;&#39118;&#30005;&#22330;&#21151;&#29575;&#36755;&#20986;&#65292;&#24182;&#22312;&#20223;&#30495;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10161</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#36890;&#35759;&#22810;&#20195;&#29702;PPO&#30340;&#38598;&#21512;&#22823;&#22411;&#39118;&#30005;&#22330;&#22810;&#21464;&#37327;&#21151;&#29575;&#36755;&#20986;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Collective Large-scale Wind Farm Multivariate Power Output Control Based on Hierarchical Communication Multi-Agent Proximal Policy Optimization. (arXiv:2305.10161v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36890;&#35759;&#30340;&#22810;&#20195;&#29702;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22823;&#22411;&#39118;&#30005;&#22330;&#22810;&#21464;&#37327;&#25511;&#21046;&#26041;&#27861;&#65292;&#23558;&#39118;&#30005;&#22330;&#20998;&#25104;&#22810;&#20010;&#32858;&#21512;&#22120;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#36890;&#35759;&#36827;&#34892;&#20449;&#24687;&#20132;&#25442;&#26469;&#26368;&#22823;&#21270;&#39118;&#30005;&#22330;&#21151;&#29575;&#36755;&#20986;&#65292;&#24182;&#22312;&#20223;&#30495;&#20013;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39118;&#30005;&#25104;&#20026;&#20840;&#29699;&#36234;&#26469;&#36234;&#37325;&#35201;&#30340;&#21487;&#20877;&#29983;&#33021;&#28304;&#65292;&#20294;&#30001;&#20110;&#39118;&#30005;&#22330;&#30340;&#39640;&#31995;&#32479;&#22797;&#26434;&#24615;&#65292;&#39118;&#30005;&#22330;&#21151;&#29575;&#25511;&#21046;&#38754;&#20020;&#24040;&#22823;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#36890;&#35759;&#30340;&#22810;&#20195;&#29702;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22823;&#22411;&#39118;&#30005;&#22330;&#22810;&#21464;&#37327;&#25511;&#21046;&#26041;&#27861;&#26469;&#22788;&#29702;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#26368;&#22823;&#21270;&#39118;&#30005;&#22330;&#21151;&#29575;&#36755;&#20986;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#39118;&#30005;&#22330;&#22810;&#21464;&#37327;&#21151;&#29575;&#27169;&#22411;&#26469;&#30740;&#31350;&#39118;&#21147;&#21457;&#30005;&#26426;&#23545;&#21151;&#29575;&#30340;&#24433;&#21709;&#12290;&#35813;&#22810;&#21464;&#37327;&#27169;&#22411;&#21253;&#25324;&#36724;&#21521;&#24863;&#24212;&#22240;&#23376;&#12289;&#20559;&#33322;&#35282;&#21644;&#20542;&#26012;&#35282;&#25511;&#21046;&#21464;&#37327;&#12290;&#25552;&#20986;&#20102;&#20998;&#23618;&#36890;&#35759;&#22810;&#20195;&#29702;PPO&#31639;&#27861;&#26469;&#21327;&#35843;&#22810;&#21464;&#37327;&#22823;&#22411;&#39118;&#30005;&#22330;&#30340;&#36830;&#32493;&#25511;&#21046;&#12290;&#23558;&#22823;&#22411;&#39118;&#30005;&#22330;&#21010;&#20998;&#20026;&#22810;&#20010;&#39118;&#21147;&#21457;&#30005;&#26426;&#32858;&#21512;&#22120;&#65292;&#24182;&#36890;&#36807;&#20998;&#23618;&#36890;&#35759;&#36827;&#34892;&#20449;&#24687;&#20132;&#25442;&#20197;&#26368;&#22823;&#21270;&#39118;&#30005;&#22330;&#21151;&#29575;&#36755;&#20986;&#12290;&#20223;&#30495;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;HCMAPPO&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#20248;&#21270;&#39118;&#30005;&#22330;&#21151;&#29575;&#36755;&#20986;&#65292;&#24182;&#20248;&#20110;&#29616;&#26377;&#30340;&#20854;&#20182;&#25511;&#21046;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Wind power is becoming an increasingly important source of renewable energy worldwide. However, wind farm power control faces significant challenges due to the high system complexity inherent in these farms. A novel communication-based multi-agent deep reinforcement learning large-scale wind farm multivariate control is proposed to handle this challenge and maximize power output. A wind farm multivariate power model is proposed to study the influence of wind turbines (WTs) wake on power. The multivariate model includes axial induction factor, yaw angle, and tilt angle controllable variables. The hierarchical communication multi-agent proximal policy optimization (HCMAPPO) algorithm is proposed to coordinate the multivariate large-scale wind farm continuous controls. The large-scale wind farm is divided into multiple wind turbine aggregators (WTAs), and neighboring WTAs can exchange information through hierarchical communication to maximize the wind farm power output. Simulation results
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#39046;&#22495;&#20869;&#23578;&#26410;&#30740;&#31350;&#30340;&#38382;&#39064;&#65306;&#24773;&#26223;&#21644;&#32454;&#33268;&#22320;&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10156</link><description>&lt;p&gt;
&#38405;&#35835;&#36807;&#31243;&#20013;&#23545;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Personality Understanding of Fictional Characters during Book Reading. (arXiv:2305.10156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#39046;&#22495;&#20869;&#23578;&#26410;&#30740;&#31350;&#30340;&#38382;&#39064;&#65306;&#24773;&#26223;&#21644;&#32454;&#33268;&#22320;&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#26159;&#38405;&#35835;&#25925;&#20107;&#30340;&#20851;&#38190;&#12290;&#38543;&#30528;&#35835;&#32773;&#19982;&#25925;&#20107;&#30340;&#20114;&#21160;&#65292;&#20182;&#20204;&#23545;&#19968;&#20010;&#20154;&#29289;&#30340;&#29702;&#35299;&#20250;&#26681;&#25454;&#26032;&#30340;&#20107;&#20214;&#21644;&#20449;&#24687;&#32780;&#28436;&#21464;&#65307;&#24182;&#19988;&#21487;&#20197;&#24863;&#30693;&#21040;&#22810;&#20010;&#31934;&#32454;&#30340;&#20010;&#24615;&#26041;&#38754;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#65306;&#24773;&#22659;&#21644;&#31934;&#32454;&#30340;&#20010;&#24615;&#29702;&#35299;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;NLP&#39046;&#22495;&#20013;&#27809;&#26377;&#24471;&#21040;&#30740;&#31350;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#27169;&#20223;&#38405;&#35835;&#36807;&#31243;&#30340;&#36866;&#24403;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#27880;&#37322;&#31574;&#30053;&#28041;&#21450;&#29992;&#22312;&#32447;&#38405;&#35835;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#31508;&#35760;&#20316;&#20026;&#21407;&#22987;&#20070;&#31821;&#30340;&#20195;&#29702;&#36827;&#34892;&#27880;&#37322;&#12290;&#23454;&#39564;&#21644;&#20154;&#20307;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#26082;&#26377;&#25928;&#21448;&#20934;&#30830;&#65307;&#25105;&#20204;&#30340;&#20219;&#21153;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#38271;&#26399;&#30340;&#19978;&#19979;&#25991;&#20197;&#23454;&#29616;&#23545;&#26426;&#22120;&#21644;&#20154;&#31867;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/Gorov/personet_acl23&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comprehending characters' personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situated and fine-grained personality understanding. The problem has not been studied in the NLP field, primarily due to the lack of appropriate datasets mimicking the process of book reading. We present the first labeled dataset PersoNet for this problem. Our novel annotation strategy involves annotating user notes from online reading apps as a proxy for the original books. Experiments and human studies indicate that our dataset construction is both efficient and accurate; and our task heavily relies on long-term context to achieve accurate predictions for both machines and humans. The dataset is available at https://github.com/Gorov/personet_acl23.
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#36951;&#24536;&#26041;&#27861;&#65292;&#21363;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#23454;&#29616;&#21487;&#25511;&#30340;&#36951;&#24536;&#65292;&#29992;&#25143;&#21487;&#25351;&#23450;&#28040;&#38500;&#21738;&#20123;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2305.10120</link><description>&lt;p&gt;
&#36873;&#25321;&#24615;&#36951;&#24536;&#65306;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Selective Amnesia: A Continual Learning Approach to Forgetting in Deep Generative Models. (arXiv:2305.10120v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10120
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#20869;&#23481;&#30340;&#38382;&#39064;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#36951;&#24536;&#26041;&#27861;&#65292;&#21363;&#25345;&#32493;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#22312;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#23454;&#29616;&#21487;&#25511;&#30340;&#36951;&#24536;&#65292;&#29992;&#25143;&#21487;&#25351;&#23450;&#28040;&#38500;&#21738;&#20123;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#30340;&#24191;&#27867;&#20351;&#29992;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#34987;&#35823;&#29992;&#29983;&#25104;&#26377;&#23475;&#12289;&#35823;&#23548;&#25110;&#19981;&#24403;&#20869;&#23481;&#30340;&#25285;&#24551;&#12290;&#21463;&#27492;&#38382;&#39064;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#25345;&#32493;&#23398;&#20064;&#21551;&#21457;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#26377;&#36873;&#25321;&#24615;&#22320;&#36951;&#24536;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#20013;&#30340;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#31216;&#20026;&#36873;&#25321;&#24615;&#36951;&#24536;&#65292;&#21487;&#20197;&#23454;&#29616;&#21487;&#25511;&#30340;&#36951;&#24536;&#65292;&#29992;&#25143;&#21487;&#20197;&#25351;&#23450;&#35813;&#22914;&#20309;&#36951;&#24536;&#19968;&#20010;&#27010;&#24565;&#12290;&#36873;&#25321;&#24615;&#36951;&#24536;&#21487;&#24212;&#29992;&#20110;&#21464;&#20998;&#20284;&#28982;&#27169;&#22411;&#65292;&#28085;&#30422;&#20102;&#21508;&#31181;&#27969;&#34892;&#30340;&#28145;&#24230;&#29983;&#25104;&#26694;&#26550;&#65292;&#21253;&#25324;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#12290;&#19981;&#21516;&#27169;&#22411;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#35825;&#23548;&#36951;&#24536;&#21508;&#31181;&#27010;&#24565;&#65292;&#20174;&#26631;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#25972;&#20010;&#31867;&#21035;&#21040;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#30340;&#21517;&#20154;&#21644;&#35064;&#20307;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#21487;&#20844;&#24320;&#35775;&#38382;&#65292;&#32593;&#22336;&#20026;https://github.com/clear-nus/selective-amnesia&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent proliferation of large-scale text-to-image models has led to growing concerns that such models may be misused to generate harmful, misleading, and inappropriate content. Motivated by this issue, we derive a technique inspired by continual learning to selectively forget concepts in pretrained deep generative models. Our method, dubbed Selective Amnesia, enables controllable forgetting where a user can specify how a concept should be forgotten. Selective Amnesia can be applied to conditional variational likelihood models, which encompass a variety of popular deep generative frameworks, including variational autoencoders and large-scale text-to-image diffusion models. Experiments across different models demonstrate that our approach induces forgetting on a variety of concepts, from entire classes in standard datasets to celebrity and nudity prompts in text-to-image models. Our code is publicly available at https://github.com/clear-nus/selective-amnesia.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#31216;&#20854;&#20026;Gap Filler (GaFi)&#27969;&#31243;&#24182;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2305.10118</link><description>&lt;p&gt;
&#26550;&#36215;&#26725;&#26753;&#65306;&#36890;&#36807;&#21518;&#22788;&#29702;&#25216;&#26415;&#22686;&#24378;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#29992;&#24615;
&lt;/p&gt;
&lt;p&gt;
Bridging the Gap: Enhancing the Utility of Synthetic Data via Post-Processing Techniques. (arXiv:2305.10118v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10118
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#24182;&#36890;&#36807;&#19977;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#38598;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#26041;&#27861;&#12290;&#20316;&#32773;&#31216;&#20854;&#20026;Gap Filler (GaFi)&#27969;&#31243;&#24182;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33719;&#21462;&#21644;&#27880;&#37322;&#29992;&#20110;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#21512;&#36866;&#25968;&#25454;&#38598;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#29983;&#25104;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#29983;&#25104;&#26367;&#20195;&#25110;&#22686;&#24378;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#21512;&#25104;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#20854;&#19981;&#33021;&#23436;&#20840;&#25429;&#25417;&#29616;&#23454;&#19990;&#30028;&#25968;&#25454;&#30340;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#29983;&#25104;&#29992;&#20110;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#38543;&#21518;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#36827;&#34892;&#35780;&#20272;&#12290;&#20026;&#20102;&#25913;&#36827;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26032;&#39062;&#30340;&#21518;&#22788;&#29702;&#25216;&#26415;&#65306;&#21160;&#24577;&#26679;&#26412;&#36807;&#28388;&#65292;&#21160;&#24577;&#25968;&#25454;&#38598;&#22238;&#25910;&#21644;&#25193;&#23637;&#25216;&#24039;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220; Gap Filler (GaFi)&#8221;&#30340;&#27969;&#31243;&#65292;&#22312;&#26368;&#20339;&#21644;&#21327;&#35843;&#30340;&#26041;&#24335;&#19979;&#24212;&#29992;&#36825;&#20123;&#25216;&#26415;&#65292;&#20197;&#26368;&#22823;&#31243;&#24230;&#22320;&#25552;&#39640;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Acquiring and annotating suitable datasets for training deep learning models is challenging. This often results in tedious and time-consuming efforts that can hinder research progress. However, generative models have emerged as a promising solution for generating synthetic datasets that can replace or augment real-world data. Despite this, the effectiveness of synthetic data is limited by their inability to fully capture the complexity and diversity of real-world data. To address this issue, we explore the use of Generative Adversarial Networks to generate synthetic datasets for training classifiers that are subsequently evaluated on real-world images. To improve the quality and diversity of the synthetic dataset, we propose three novel post-processing techniques: Dynamic Sample Filtering, Dynamic Dataset Recycle, and Expansion Trick. In addition, we introduce a pipeline called Gap Filler (GaFi), which applies these techniques in an optimal and coordinated manner to maximise classifica
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#30340;AI&#26041;&#27861;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#31572;&#26696;&#38598;&#32534;&#31243;&#26469;&#33258;&#21160;&#21270;&#30005;&#27668;&#25511;&#21046;&#38754;&#26495;&#30340;&#21512;&#35268;&#24615;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#35782;&#21035;&#21487;&#33021;&#23384;&#22312;&#30340;&#24322;&#24120;&#21644;&#38169;&#35823;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.10113</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#30340;AI&#29992;&#20110;&#30005;&#27668;&#25511;&#21046;&#38754;&#26495;&#30340;&#21512;&#35268;&#26816;&#26597;
&lt;/p&gt;
&lt;p&gt;
Neuro-Symbolic AI for Compliance Checking of Electrical Control Panels. (arXiv:2305.10113v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10113
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#30340;AI&#26041;&#27861;&#65292;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#31572;&#26696;&#38598;&#32534;&#31243;&#26469;&#33258;&#21160;&#21270;&#30005;&#27668;&#25511;&#21046;&#38754;&#26495;&#30340;&#21512;&#35268;&#24615;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#21482;&#26377;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#19979;&#35782;&#21035;&#21487;&#33021;&#23384;&#22312;&#30340;&#24322;&#24120;&#21644;&#38169;&#35823;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25903;&#25345;&#21644;&#25913;&#21892;&#26234;&#33021;&#21046;&#36896;&#21644;&#24037;&#19994;4.0&#26041;&#38754;&#21457;&#25381;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#20351;&#39046;&#22495;&#19987;&#23478;&#25163;&#21160;&#25191;&#34892;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#20219;&#21153;&#33258;&#21160;&#21270;&#12290;&#29305;&#21035;&#26159;&#65292;&#35780;&#20272;&#20135;&#21697;&#19982;&#30456;&#23545;&#21407;&#29702;&#22270;&#30340;&#31526;&#21512;&#24615;&#26159;&#19968;&#39033;&#32791;&#26102;&#19988;&#23481;&#26131;&#20986;&#38169;&#30340;&#36807;&#31243;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#29305;&#23450;&#30340;&#24037;&#19994;&#22330;&#26223;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#30005;&#27668;&#25511;&#21046;&#38754;&#26495;&#30340;&#21512;&#35268;&#24615;&#39564;&#35777;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#21644;&#31572;&#26696;&#38598;&#32534;&#31243;&#65288;ASP&#65289;&#30340;&#32452;&#21512;&#65292;&#21363;&#20351;&#21482;&#26377;&#38750;&#24120;&#26377;&#38480;&#30340;&#35757;&#32451;&#25968;&#25454;&#65292;&#20063;&#21487;&#20197;&#35782;&#21035;&#20986;&#26368;&#32456;&#20135;&#21697;&#20013;&#21487;&#33021;&#23384;&#22312;&#30340;&#24322;&#24120;&#21644;&#38169;&#35823;&#12290;&#36890;&#36807;&#24847;&#22823;&#21033;&#19968;&#23478;&#20174;&#20107;&#30005;&#27668;&#25511;&#21046;&#38754;&#26495;&#29983;&#20135;&#30340;&#20844;&#21496;&#25552;&#20379;&#30340;&#23454;&#38469;&#27979;&#35797;&#26696;&#20363;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence plays a main role in supporting and improving smart manufacturing and Industry 4.0, by enabling the automation of different types of tasks manually performed by domain experts. In particular, assessing the compliance of a product with the relative schematic is a time-consuming and prone-to-error process. In this paper, we address this problem in a specific industrial scenario. In particular, we define a Neuro-Symbolic approach for automating the compliance verification of the electrical control panels. Our approach is based on the combination of Deep Learning techniques with Answer Set Programming (ASP), and allows for identifying possible anomalies and errors in the final product even when a very limited amount of training data is available. The experiments conducted on a real test case provided by an Italian Company operating in electrical control panel production demonstrate the effectiveness of the proposed approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;TweetGage&#65292;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#34920;&#31034;&#21457;&#24067;&#24086;&#23376;&#38388;&#30340;&#35821;&#20041;&#20851;&#32852;&#26469;&#39044;&#27979;&#29992;&#25143;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20114;&#21160;&#65292;&#30456;&#23545;&#20854;&#20182;&#30740;&#31350;&#21482;&#32771;&#34385;&#24086;&#23376;&#25991;&#26412;&#21644;&#21457;&#24067;&#29992;&#25143;&#31561;&#22240;&#32032;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10103</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#25512;&#29305;&#20114;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Predicting Tweet Engagement with Graph Neural Networks. (arXiv:2305.10103v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10103
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;TweetGage&#65292;&#19968;&#20010;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#34920;&#31034;&#21457;&#24067;&#24086;&#23376;&#38388;&#30340;&#35821;&#20041;&#20851;&#32852;&#26469;&#39044;&#27979;&#29992;&#25143;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20114;&#21160;&#65292;&#30456;&#23545;&#20854;&#20182;&#30740;&#31350;&#21482;&#32771;&#34385;&#24086;&#23376;&#25991;&#26412;&#21644;&#21457;&#24067;&#29992;&#25143;&#31561;&#22240;&#32032;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#32593;&#32476;&#26159;&#26368;&#37325;&#35201;&#30340;&#22312;&#32447;&#20869;&#23481;&#20998;&#20139;&#24179;&#21488;&#20043;&#19968;&#65292;&#39044;&#27979;&#21457;&#24067;&#20869;&#23481;&#30340;&#20114;&#21160;&#24773;&#20917;&#26159;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#23454;&#29616;&#30408;&#21033;&#30340;&#20851;&#38190;&#12290;&#26412;&#25991;&#35748;&#20026;&#21457;&#24067;&#30340;&#20869;&#23481;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#32852;&#20063;&#26159;&#20114;&#21160;&#25968;&#22686;&#38271;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TweetGage&#65292;&#19968;&#20010;&#22522;&#20110;&#22270;&#30340;&#31070;&#32463;&#32593;&#32476;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#27169;&#22411;&#26469;&#34920;&#31034;&#24086;&#23376;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#39044;&#27979;&#29992;&#25143;&#30340;&#20114;&#21160;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#25552;&#35758;&#65292;&#25105;&#20204;&#38024;&#23545;Twitter&#24179;&#21488;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#20248;&#33391;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social Networks represent one of the most important online sources to share content across a world-scale audience. In this context, predicting whether a post will have any impact in terms of engagement is of crucial importance to drive the profitable exploitation of these media. In the literature, several studies address this issue by leveraging direct features of the posts, typically related to the textual content and the user publishing it. In this paper, we argue that the rise of engagement is also related to another key component, which is the semantic connection among posts published by users in social media. Hence, we propose TweetGage, a Graph Neural Network solution to predict the user engagement based on a novel graph-based model that represents the relationships among posts. To validate our proposal, we focus on the Twitter platform and perform a thorough experimental campaign providing evidence of its quality.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31227;&#24773;&#21453;&#24212;&#24847;&#22270;&#20998;&#31867;&#27861;&#26469;&#25511;&#21046;&#21644;&#35299;&#37322;&#31070;&#32463;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#20849;&#24773;&#22238;&#24212;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20135;&#29983;&#21487;&#25511;&#21644;&#21487;&#35299;&#37322;&#30340;&#20849;&#24773;&#22238;&#24212;&#12290;</title><link>http://arxiv.org/abs/2305.10096</link><description>&lt;p&gt;
&#20351;&#29992;&#31227;&#24773;&#21453;&#24212;&#24847;&#22270;&#20998;&#31867;&#27861;&#26469;&#25511;&#21046;&#21644;&#35299;&#37322;&#31070;&#32463;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#20849;&#24773;&#12290;
&lt;/p&gt;
&lt;p&gt;
Use of a Taxonomy of Empathetic Response Intents to Control and Interpret Empathy in Neural Chatbots. (arXiv:2305.10096v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10096
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#31227;&#24773;&#21453;&#24212;&#24847;&#22270;&#20998;&#31867;&#27861;&#26469;&#25511;&#21046;&#21644;&#35299;&#37322;&#31070;&#32463;&#32842;&#22825;&#26426;&#22120;&#20154;&#20013;&#30340;&#20849;&#24773;&#22238;&#24212;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#20135;&#29983;&#21487;&#25511;&#21644;&#21487;&#35299;&#37322;&#30340;&#20849;&#24773;&#22238;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#20195;&#29702;&#30340;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#26368;&#36817;&#30340;&#36235;&#21183;&#26159;&#35753;&#23427;&#20204;&#33021;&#22815;&#23545;&#24773;&#24863;&#25552;&#31034;&#36827;&#34892;&#21516;&#24773;&#24335;&#30340;&#23545;&#35805;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#35201;&#20040;&#36981;&#24490;&#31471;&#21040;&#31471;&#30340;&#26041;&#27861;&#65292;&#35201;&#20040;&#22312;&#30456;&#20284;&#30340;&#24773;&#24863;&#26631;&#31614;&#19978;&#36827;&#34892;&#26465;&#20214;&#21453;&#24212;&#20197;&#20135;&#29983;&#20849;&#24773;&#24335;&#30340;&#22238;&#31572;&#12290;&#20294;&#20849;&#24773;&#26159;&#19968;&#20010;&#24191;&#27867;&#30340;&#27010;&#24565;&#65292;&#23427;&#25351;&#30340;&#26159;&#20010;&#20307;&#23545;&#21478;&#19968;&#20010;&#20154;&#35266;&#23519;&#21040;&#30340;&#32463;&#21382;&#30340;&#35748;&#30693;&#21644;&#24773;&#24863;&#21453;&#24212;&#65292;&#23427;&#27604;&#21333;&#32431;&#30340;&#24773;&#24863;&#27169;&#20223;&#26356;&#21152;&#22797;&#26434;&#12290;&#22240;&#27492;&#65292;&#38500;&#20102;&#36890;&#29992;&#24773;&#24863;&#22806;&#65292;&#36824;&#38656;&#35201;&#35782;&#21035;&#22797;&#26434;&#30340;&#20154;&#31867;&#23545;&#35805;&#31574;&#30053;&#21644;&#21160;&#24577;&#26469;&#25511;&#21046;&#21644;&#35299;&#37322;&#32842;&#22825;&#26426;&#22120;&#20154;&#30340;&#20849;&#24773;&#22238;&#24212;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#20843;&#31181;&#20849;&#24773;&#21453;&#24212;&#24847;&#22270;&#30340;&#20998;&#31867;&#27861;&#20197;&#21450;&#36890;&#29992;&#24773;&#24863;&#31867;&#21035;&#26469;&#24314;&#31435;&#19968;&#20010;&#23545;&#35805;&#21709;&#24212;&#29983;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#20197;&#21487;&#25511;&#21644;&#21487;&#35299;&#37322;&#30340;&#26041;&#24335;&#20135;&#29983;&#20849;&#24773;&#22238;&#24212;&#12290;&#23427;&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;1&#65289;&#21709;&#24212;&#24773;&#24863;/&#24847;&#22270;&#39044;&#27979;&#27169;&#22359;&#65307;&#20197;&#21450;2&#65289;&#21709;&#24212;&#29983;&#25104;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
A recent trend in the domain of open-domain conversational agents is enabling them to converse empathetically to emotional prompts. Current approaches either follow an end-to-end approach or condition the responses on similar emotion labels to generate empathetic responses. But empathy is a broad concept that refers to the cognitive and emotional reactions of an individual to the observed experiences of another and it is more complex than mere mimicry of emotion. Hence, it requires identifying complex human conversational strategies and dynamics in addition to generic emotions to control and interpret empathetic responding capabilities of chatbots. In this work, we make use of a taxonomy of eight empathetic response intents in addition to generic emotion categories in building a dialogue response generation model capable of generating empathetic responses in a controllable and interpretable manner. It consists of two modules: 1) a response emotion/intent prediction module; and 2) a res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#19982;&#24212;&#29992;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#21313;&#24180;&#30340;&#30740;&#31350;&#36235;&#21183;&#19982;&#21069;&#26223;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#26410;&#26469;&#21313;&#24180;&#20013;&#65292;&#21487;&#20449;&#36182;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23558;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#20154;&#26426;&#20132;&#20114;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#31038;&#20250;&#20013;&#23454;&#38469;&#24212;&#29992;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#65292; &#22240;&#27492;&#65292;&#26412;&#25991;&#36824;&#20998;&#26512;&#20102;&#24212;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.10091</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65306;&#26041;&#27861;&#12289;&#24212;&#29992;&#12289;&#21069;&#26223;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Reinforcement Learning: Methods, Applications, Visionary Prospects, and Challenges. (arXiv:2305.10091v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10091
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#19982;&#24212;&#29992;&#65292;&#24182;&#25351;&#20986;&#20102;&#26410;&#26469;&#21313;&#24180;&#30340;&#30740;&#31350;&#36235;&#21183;&#19982;&#21069;&#26223;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#26410;&#26469;&#21313;&#24180;&#20013;&#65292;&#21487;&#20449;&#36182;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23558;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#27492;&#22806;&#65292;&#32771;&#34385;&#20154;&#26426;&#20132;&#20114;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#31038;&#20250;&#20013;&#23454;&#38469;&#24212;&#29992;&#30340;&#37325;&#35201;&#32771;&#34385;&#22240;&#32032;&#65292; &#22240;&#27492;&#65292;&#26412;&#25991;&#36824;&#20998;&#26512;&#20102;&#24212;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#38656;&#35201;&#35299;&#20915;&#21487;&#20280;&#32553;&#24615;&#12289;&#38750;&#38745;&#24577;&#24615;&#21644;&#21487;&#20449;&#24615;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#39038;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#19982;&#24212;&#29992;&#65292;&#24182;&#25351;&#20986;&#26410;&#26469;&#21313;&#24180;&#30340;&#30740;&#31350;&#36235;&#21183;&#21644;&#21069;&#26223;&#12290;&#39318;&#20808;&#65292;&#26412;&#25991;&#24635;&#32467;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#22522;&#26412;&#26041;&#27861;&#21644;&#24212;&#29992;&#22330;&#26223;&#12290;&#20854;&#27425;&#65292;&#26412;&#25991;&#27010;&#36848;&#20102;&#30456;&#24212;&#30340;&#30740;&#31350;&#26041;&#27861;&#21450;&#20854;&#22312;&#23433;&#20840;&#24615;&#12289;&#40065;&#26834;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#36947;&#24503;&#32422;&#26463;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#65292;&#36825;&#20123;&#23616;&#38480;&#24615;&#38656;&#35201;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#24471;&#21040;&#35299;&#20915;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20540;&#24471;&#20851;&#27880;&#30340;&#26159;&#65292;&#22312;&#26410;&#26469;&#21313;&#24180;&#20013;&#65292;&#21487;&#20449;&#36182;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#23558;&#25104;&#20026;&#19968;&#20010;&#28909;&#38376;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24314;&#35758;&#22312;&#32771;&#34385;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#22312;&#21508;&#31181;&#31038;&#20250;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#26102;&#24212;&#32771;&#34385;&#20154;&#26426;&#20114;&#21160;&#65292;&#22240;&#27492;&#65292;&#26412;&#25991;&#36824;&#20998;&#26512;&#20102;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#26102;&#25152;&#38754;&#20020;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning (MARL) is a widely used Artificial Intelligence (AI) technique. However, current studies and applications need to address its scalability, non-stationarity, and trustworthiness. This paper aims to review methods and applications and point out research trends and visionary prospects for the next decade. First, this paper summarizes the basic methods and application scenarios of MARL. Second, this paper outlines the corresponding research methods and their limitations on safety, robustness, generalization, and ethical constraints that need to be addressed in the practical applications of MARL. In particular, we believe that trustworthy MARL will become a hot research topic in the next decade. In addition, we suggest that considering human interaction is essential for the practical application of MARL in various societies. Therefore, this paper also analyzes the challenges while MARL is applied to human-machine interaction.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35777;&#26126;&#20102;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#35753;&#23398;&#20064;&#32773;&#30340;&#22870;&#21169;&#20540;&#21644;&#26368;&#20248;&#35299;&#27169;&#20223;&#19987;&#23478;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2305.10089</link><description>&lt;p&gt;
&#19968;&#31181;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#30340;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
A proof of imitation of Wasserstein inverse reinforcement learning for multi-objective optimization. (arXiv:2305.10089v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10089
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#36866;&#29992;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#21487;&#35753;&#23398;&#20064;&#32773;&#30340;&#22870;&#21169;&#20540;&#21644;&#26368;&#20248;&#35299;&#27169;&#20223;&#19987;&#23478;&#65292;&#20855;&#26377;&#19968;&#23450;&#30340;&#23454;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35777;&#26126;&#20102;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#22312;&#26377;&#38480;&#27425;&#36845;&#20195;&#20013;&#35753;&#23398;&#20064;&#32773;&#30340;&#22870;&#21169;&#20540;&#27169;&#20223;&#19987;&#23478;&#30340;&#22870;&#21169;&#20540;&#65292;&#24182;&#35777;&#26126;&#20102;&#22312;&#35789;&#20856;&#24207;&#30340;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#65292;Wasserstein&#21453;&#21521;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#35753;&#23398;&#20064;&#32773;&#30340;&#26368;&#20248;&#35299;&#27169;&#20223;&#19987;&#23478;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
We prove Wasserstein inverse reinforcement learning enables the learner's reward values to imitate the expert's reward values in a finite iteration for multi-objective optimizations. Moreover, we prove Wasserstein inverse reinforcement learning enables the learner's optimal solutions to imitate the expert's optimal solutions for multi-objective optimizations with lexicographic order.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#25105;&#30417;&#30563;&#12289;&#32858;&#31867;&#21644;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#65292;&#35299;&#20915;&#20919;&#21551;&#21160;&#25110;&#26080;&#30417;&#30563;&#36873;&#25321;&#26631;&#35760;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10071</link><description>&lt;p&gt;
&#20919;&#21551;&#21160;&#38382;&#39064;&#65306;&#26080;&#30417;&#30563;&#30340;&#31867;&#21035;&#21457;&#29616;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cold PAWS: Unsupervised class discovery and the cold-start problem. (arXiv:2305.10071v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10071
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#33258;&#25105;&#30417;&#30563;&#12289;&#32858;&#31867;&#21644;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#65292;&#35299;&#20915;&#20919;&#21551;&#21160;&#25110;&#26080;&#30417;&#30563;&#36873;&#25321;&#26631;&#35760;&#38382;&#39064;&#65292;&#24182;&#22312;&#22810;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#33719;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#65292;&#26631;&#35760;&#25968;&#25454;&#38598;&#24120;&#24120;&#26159;&#19968;&#39033;&#33392;&#33510;&#19988;&#32791;&#26102;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#30740;&#31350;&#34920;&#26126;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#21487;&#20197;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#20013;&#20351;&#29992;&#38750;&#24120;&#23569;&#30340;&#26631;&#31614;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#65292;&#20294;&#24456;&#23569;&#26377;&#20154;&#20851;&#27880;&#22914;&#20309;&#36873;&#25321;&#25968;&#25454;&#38598;&#20013;&#30340;&#22270;&#20687;&#36827;&#34892;&#26631;&#35760;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#12289;&#32858;&#31867;&#21644;&#27969;&#24418;&#23398;&#20064;&#25216;&#26415;&#30340;&#26032;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#39318;&#27425;&#36873;&#25321;&#20449;&#24687;&#22270;&#20687;&#23376;&#38598;&#36827;&#34892;&#26631;&#35760;&#30340;&#25361;&#25112;&#65292;&#21363;&#20919;&#21551;&#21160;&#25110;&#26080;&#30417;&#30563;&#36873;&#25321;&#26631;&#35760;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#20960;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#65288;&#21253;&#25324;CIFAR10&#12289;Imagenette&#12289;DeepWeeds&#21644;EuroSAT&#65289;&#27979;&#35797;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35266;&#23519;&#21040;&#24403;&#20351;&#29992;&#25105;&#20204;&#30340;&#26631;&#31614;&#36873;&#25321;&#31574;&#30053;&#26102;&#65292;&#19982;&#38543;&#26426;&#25277;&#26679;&#30456;&#27604;&#65292;&#22312;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#31574;&#30053;&#22343;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#36824;&#22312;d&#26041;&#38754;&#33719;&#24471;&#20102;&#26356;&#20248;&#31168;&#30340;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;
In many machine learning applications, labeling datasets can be an arduous and time-consuming task. Although research has shown that semi-supervised learning techniques can achieve high accuracy with very few labels within the field of computer vision, little attention has been given to how images within a dataset should be selected for labeling. In this paper, we propose a novel approach based on well-established self-supervised learning, clustering, and manifold learning techniques that address this challenge of selecting an informative image subset to label in the first instance, which is known as the cold-start or unsupervised selective labelling problem. We test our approach using several publicly available datasets, namely CIFAR10, Imagenette, DeepWeeds, and EuroSAT, and observe improved performance with both supervised and semi-supervised learning strategies when our label selection strategy is used, in comparison to random sampling. We also obtain superior performance for the d
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#23601;&#19994;&#39046;&#22495;&#30340;&#22810;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#22686;&#24378;&#20915;&#31574;&#25903;&#25345;&#12289;&#36981;&#23432;&#27861;&#24459;&#35201;&#27714;&#12289;&#24341;&#23548;&#21463;&#25511;&#21464;&#21270;&#21644;&#20998;&#26512;&#26032;&#39062;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2305.10069</link><description>&lt;p&gt;
&#25581;&#31034;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#23601;&#19994;&#39046;&#22495;&#30340;&#28508;&#21147;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Potential of Counterfactuals Explanations in Employability. (arXiv:2305.10069v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10069
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#21453;&#20107;&#23454;&#35299;&#37322;&#22312;&#23601;&#19994;&#39046;&#22495;&#30340;&#22810;&#31181;&#24212;&#29992;&#65292;&#21253;&#25324;&#22686;&#24378;&#20915;&#31574;&#25903;&#25345;&#12289;&#36981;&#23432;&#27861;&#24459;&#35201;&#27714;&#12289;&#24341;&#23548;&#21463;&#25511;&#21464;&#21270;&#21644;&#20998;&#26512;&#26032;&#39062;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#20013;&#65292;&#21453;&#20107;&#23454;&#35299;&#37322;&#34987;&#35748;&#20026;&#21487;&#20197;&#20026;&#22797;&#26434;&#30340;&#27169;&#22411;&#20915;&#31574;&#25552;&#20379;&#31616;&#21333;&#12289;&#31616;&#27905;&#12289;&#26131;&#25026;&#30340;&#29702;&#30001;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#36824;&#27809;&#26377;&#30475;&#21040;&#26356;&#22810;&#24212;&#29992;&#30740;&#31350;&#65292;&#23558;&#23427;&#20204;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#24773;&#20917;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#30740;&#31350;&#20391;&#37325;&#20110;&#23637;&#31034;&#22914;&#20309;&#23558;&#21453;&#20107;&#23454;&#29992;&#20110;&#19982;&#23601;&#19994;&#30456;&#20851;&#30340;&#38382;&#39064;&#65292;&#20854;&#20013;&#28041;&#21450;&#22797;&#26434;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#12290;&#23545;&#20110;&#36825;&#20123;&#29992;&#20363;&#65292;&#25105;&#20204;&#20351;&#29992;&#20174;&#27604;&#21033;&#26102;&#20844;&#20849;&#23601;&#19994;&#26426;&#26500;&#65288;VDAB&#65289;&#33719;&#24471;&#30340;&#30495;&#23454;&#25968;&#25454;&#12290;&#25152;&#25552;&#20986;&#30340;&#29992;&#20363;&#36229;&#36234;&#20102;&#20165;&#23558;&#21453;&#20107;&#23454;&#29992;&#20316;&#35299;&#37322;&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#22914;&#20309;&#22686;&#24378;&#20915;&#31574;&#25903;&#25345;&#65292;&#36981;&#23432;&#27861;&#24459;&#35201;&#27714;&#65292;&#24341;&#23548;&#21463;&#25511;&#21464;&#21270;&#24182;&#20998;&#26512;&#26032;&#39062;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
In eXplainable Artificial Intelligence (XAI), counterfactual explanations are known to give simple, short, and comprehensible justifications for complex model decisions. However, we are yet to see more applied studies in which they are applied in real-world cases. To fill this gap, this study focuses on showing how counterfactuals are applied to employability-related problems which involve complex machine learning algorithms. For these use cases, we use real data obtained from a public Belgian employment institution (VDAB). The use cases presented go beyond the mere application of counterfactuals as explanations, showing how they can enhance decision support, comply with legal requirements, guide controlled changes, and analyze novel insights.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#26680;&#30340;&#28151;&#21512;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#12289;&#26680;&#26041;&#27861;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#36873;&#25321;&#20174;ATM&#20107;&#20214;&#26085;&#24535;&#20013;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#65292;&#29992;&#20110;&#26089;&#26399;ATM&#25925;&#38556;&#39044;&#27979;&#65292;&#22312;&#30495;&#23454;&#30340;ATM&#20107;&#20214;&#26085;&#24535;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.10059</link><description>&lt;p&gt;
&#22522;&#20110;&#21367;&#31215;&#26680;&#30340;&#28151;&#21512;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;ATM&#25925;&#38556;&#39044;&#27979;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A hybrid feature learning approach based on convolutional kernels for ATM fault prediction using event-log data. (arXiv:2305.10059v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10059
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21367;&#31215;&#26680;&#30340;&#28151;&#21512;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#12289;&#26680;&#26041;&#27861;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#36873;&#25321;&#20174;ATM&#20107;&#20214;&#26085;&#24535;&#20013;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#65292;&#29992;&#20110;&#26089;&#26399;ATM&#25925;&#38556;&#39044;&#27979;&#65292;&#22312;&#30495;&#23454;&#30340;ATM&#20107;&#20214;&#26085;&#24535;&#25968;&#25454;&#19978;&#36827;&#34892;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#20854;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#27979;&#24615;&#32500;&#25252;&#65288;PdM&#65289;&#26041;&#27861;&#26088;&#22312;&#22312;&#35774;&#22791;&#25925;&#38556;&#20043;&#21069;&#23433;&#25490;&#32500;&#25252;&#24037;&#20316;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#33258;&#21160;&#21462;&#27454;&#26426;&#65288;ATM&#65289;&#30340;&#26089;&#26399;&#25925;&#38556;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#22240;&#20026;&#36825;&#20123;&#26426;&#22120;&#26131;&#21463;&#21508;&#31181;&#19981;&#21487;&#39044;&#27979;&#30340;&#25925;&#38556;&#24433;&#21709;&#12290;ATM&#36890;&#36807;&#29983;&#25104;&#22823;&#37327;&#30340;&#20107;&#20214;&#26085;&#24535;&#25968;&#25454;&#26469;&#36319;&#36394;&#25191;&#34892;&#29366;&#24577;&#65292;&#36825;&#20123;&#25968;&#25454;&#25910;&#38598;&#19982;&#25925;&#38556;&#20107;&#20214;&#26080;&#20851;&#30340;&#31995;&#32479;&#28040;&#24687;&#12290; &#22522;&#20110;&#20107;&#20214;&#26085;&#24535;&#39044;&#27979;&#25925;&#38556;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#25361;&#25112;&#65292;&#20027;&#35201;&#22312;&#20110;&#25552;&#21462;&#21487;&#33021;&#34920;&#31034;&#21363;&#23558;&#21457;&#29983;&#25925;&#38556;&#30340;&#20107;&#20214;&#24207;&#21015;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#29305;&#24449;&#23398;&#20064;&#26041;&#27861;&#30446;&#21069;&#27491;&#22312;PdM&#20013;&#20351;&#29992;&#65292;&#20854;&#20013;&#20174;&#26368;&#23567;&#22788;&#29702;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#20013;&#33258;&#21160;&#23398;&#20064;&#26377;&#20449;&#24687;&#37327;&#30340;&#29305;&#24449;&#12290;&#20294;&#26159;&#65292;&#20173;&#28982;&#23384;&#22312;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#26041;&#27861;&#26469;&#20174;&#22522;&#20110;&#20107;&#20214;&#26085;&#24535;&#30340;&#25968;&#25454;&#20013;&#23548;&#20986;&#30456;&#20851;&#29305;&#24449;&#30340;&#31354;&#30333;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#21367;&#31215;&#26680;&#30340;&#39044;&#27979;&#27169;&#22411;&#65288;MiniROCKET&#65289;&#29992;&#20110;&#26089;&#26399;ATM&#25925;&#38556;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#21253;&#25324;&#19968;&#31181;&#28151;&#21512;&#29305;&#24449;&#23398;&#20064;&#25216;&#26415;&#65292;&#36890;&#36807;&#32467;&#21512;&#28145;&#24230;&#23398;&#20064;&#12289;&#26680;&#26041;&#27861;&#21644;&#22522;&#20110;&#35268;&#21017;&#30340;&#29305;&#24449;&#36873;&#25321;&#20174;ATM&#20107;&#20214;&#26085;&#24535;&#20013;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#12290;&#25105;&#20204;&#23545;&#30495;&#23454;&#30340;ATM&#20107;&#20214;&#26085;&#24535;&#25968;&#25454;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#20998;&#31867;&#24615;&#33021;&#12289;&#31283;&#23450;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predictive Maintenance (PdM) methods aim to facilitate the scheduling of maintenance work before equipment failure. In this context, detecting early faults in automated teller machines (ATMs) has become increasingly important since these machines are susceptible to various types of unpredictable failures. ATMs track execution status by generating massive event-log data that collect system messages unrelated to the failure event. Predicting machine failure based on event logs poses additional challenges, mainly in extracting features that might represent sequences of events indicating impending failures. Accordingly, feature learning approaches are currently being used in PdM, where informative features are learned automatically from minimally processed sensor data. However, a gap remains to be seen on how these approaches can be exploited for deriving relevant features from event-log-based data. To fill this gap, we present a predictive model based on a convolutional kernel (MiniROCKET
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22522;&#20110;&#21442;&#25968;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;"&#21306;&#22495;&#39564;&#35777;"&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#20915;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;&#30340;$\epsilon$-close&#21442;&#25968;&#35843;&#25972;&#38382;&#39064;&#65292;&#20026;&#22788;&#29702;&#21442;&#25968;&#21270;BN&#30340;&#23376;&#31867;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10051</link><description>&lt;p&gt;
&#22312;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;&#25214;&#21040;$\epsilon$-close&#21464;&#20998;&#21442;&#25968;
&lt;/p&gt;
&lt;p&gt;
Finding an $\epsilon$-close Variation of Parameters in Bayesian Networks. (arXiv:2305.10051v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#30340;&#31639;&#27861;&#22522;&#20110;&#21442;&#25968;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;"&#21306;&#22495;&#39564;&#35777;"&#25216;&#26415;&#65292;&#21487;&#20197;&#35299;&#20915;&#36125;&#21494;&#26031;&#32593;&#32476;&#20013;&#30340;$\epsilon$-close&#21442;&#25968;&#35843;&#25972;&#38382;&#39064;&#65292;&#20026;&#22788;&#29702;&#21442;&#25968;&#21270;BN&#30340;&#23376;&#31867;&#25552;&#20379;&#20102;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#36125;&#21494;&#26031;&#32593;&#32476;&#65288;BNs&#65289;&#20013;&#30340;$\epsilon$-close&#21442;&#25968;&#35843;&#25972;&#38382;&#39064;&#36827;&#34892;&#20102;&#30740;&#31350;&#65306;&#25214;&#21040;&#32473;&#23450;&#26465;&#20214;&#19979;&#26368;&#23567;&#30340;$\epsilon$-close&#27010;&#29575;&#20462;&#27491;&#65292;&#20351;BNs&#26377;&#25928;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21442;&#25968;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;"&#21306;&#22495;&#39564;&#35777;"&#25216;&#26415;&#30340;&#31639;&#27861;&#65292;&#20854;&#33021;&#21147;&#36229;&#36234;&#20219;&#20309;&#29616;&#26377;&#25216;&#26415;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#21487;&#20197;&#23545;&#26368;&#22810;&#26377;8&#20010;&#21442;&#25968;&#30340;&#22823;&#22411;BN&#22522;&#20934;&#36827;&#34892;$\epsilon$-close&#35843;&#25972;&#12290;&#29305;&#21035;&#22320;&#65292;&#36890;&#36807;&#20801;&#35768;&#65288;i&#65289;&#22810;&#20010;CPT&#20013;&#30340;&#19981;&#21516;&#21442;&#25968;&#21644;&#65288;ii&#65289;CPT&#20043;&#38388;&#30340;&#21442;&#25968;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#22788;&#29702;&#20102;&#36804;&#20170;&#20026;&#27490;&#25910;&#21040;&#24456;&#23569;&#20851;&#27880;&#30340;&#21442;&#25968;&#21270;BN&#30340;&#23376;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper addresses the $\epsilon$-close parameter tuning problem for Bayesian Networks (BNs): find a minimal $\epsilon$-close amendment of probability entries in a given set of (rows in) conditional probability tables that make a given quantitative constraint on the BN valid. Based on the state-of-the-art "region verification" techniques for parametric Markov chains, we propose an algorithm whose capabilities go beyond any existing techniques. Our experiments show that $\epsilon$-close tuning of large BN benchmarks with up to 8 parameters is feasible. In particular, by allowing (i) varied parameters in multiple CPTs and (ii) inter-CPT parameter dependencies, we treat subclasses of parametric BNs that have received scant attention so far.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25193;&#23637;&#20102;&#26368;&#26032;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#20174;&#22810;&#20013;&#24515;&#20020;&#24202;&#30740;&#31350;&#30340;&#32570;&#22833;&#25968;&#25454;&#20013;&#20998;&#26512;&#20102;&#19981;&#21516;&#32570;&#22833;&#26426;&#21046;&#23545;&#24674;&#22797;&#30340;&#22240;&#26524;&#22270;&#30340;&#24433;&#21709;&#65292;&#39564;&#35777;&#20102;&#25152;&#24674;&#22797;&#22240;&#26524;&#22270;&#30340;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;&#24182;&#29992;&#22270;&#24418;&#20998;&#31163;&#26469;&#39564;&#35777;&#22240;&#26524;&#36890;&#36335;&#65292;&#35752;&#35770;&#20102;&#22240;&#26524;&#22270;&#30340;&#25311;&#21512;&#24230;&#21644;&#20174;&#20020;&#24202;&#20915;&#31574;&#35282;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10050</link><description>&lt;p&gt;
&#22312;&#22810;&#20013;&#24515;&#20020;&#24202;&#30740;&#31350;&#20013;&#22788;&#29702;&#32570;&#22833;&#25968;&#25454;&#30340;&#22240;&#26524;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery with Missing Data in a Multicentric Clinical Study. (arXiv:2305.10050v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10050
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25193;&#23637;&#20102;&#26368;&#26032;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#20174;&#22810;&#20013;&#24515;&#20020;&#24202;&#30740;&#31350;&#30340;&#32570;&#22833;&#25968;&#25454;&#20013;&#20998;&#26512;&#20102;&#19981;&#21516;&#32570;&#22833;&#26426;&#21046;&#23545;&#24674;&#22797;&#30340;&#22240;&#26524;&#22270;&#30340;&#24433;&#21709;&#65292;&#39564;&#35777;&#20102;&#25152;&#24674;&#22797;&#22240;&#26524;&#22270;&#30340;&#20020;&#24202;&#30456;&#20851;&#24615;&#65292;&#24182;&#29992;&#22270;&#24418;&#20998;&#31163;&#26469;&#39564;&#35777;&#22240;&#26524;&#36890;&#36335;&#65292;&#35752;&#35770;&#20102;&#22240;&#26524;&#22270;&#30340;&#25311;&#21512;&#24230;&#21644;&#20174;&#20020;&#24202;&#20915;&#31574;&#35282;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#35266;&#27979;&#25968;&#25454;&#30340;&#25968;&#25454;&#29983;&#25104;&#27169;&#22411;&#21644;&#30456;&#20851;&#32852;&#30340;&#22240;&#26524;&#22270;&#36890;&#24120;&#19981;&#23384;&#22312;&#65292;&#22240;&#27492;&#20174;&#35266;&#27979;&#25968;&#25454;&#20013;&#26816;&#39564;&#20020;&#24202;&#20551;&#35774;&#30340;&#22240;&#26524;&#25512;&#26029;&#38754;&#20020;&#35768;&#22810;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#35266;&#27979;&#25968;&#25454;&#21487;&#33021;&#21253;&#21547;&#32570;&#22833;&#30340;&#20540;&#65292;&#36825;&#20250;&#24433;&#21709;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#24674;&#22797;&#22240;&#26524;&#22270;&#30340;&#25928;&#26524;&#65292;&#36825;&#26159;&#20020;&#24202;&#30740;&#31350;&#20013;&#32463;&#24120;&#34987;&#24573;&#30053;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#23376;&#23467;&#20869;&#33180;&#30284;&#30340;&#22810;&#20013;&#24515;&#30740;&#31350;&#25968;&#25454;&#65292;&#20998;&#26512;&#19981;&#21516;&#32570;&#22833;&#26426;&#21046;&#23545;&#24674;&#22797;&#30340;&#22240;&#26524;&#22270;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#25193;&#23637;&#20102;&#26368;&#20808;&#36827;&#30340;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#65292;&#21033;&#29992;&#19987;&#23478;&#30693;&#35782;&#32780;&#19981;&#25439;&#22833;&#29702;&#35770;&#30340;&#20005;&#35880;&#24615;&#65292;&#39564;&#35777;&#20102;&#25152;&#24674;&#22797;&#22240;&#26524;&#22270;&#30340;&#20020;&#24202;&#30456;&#20851;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#22270;&#24418;&#20998;&#31163;&#26469;&#39564;&#35777;&#22240;&#26524;&#36890;&#36335;&#65292;&#35752;&#35770;&#20102;&#22240;&#26524;&#22270;&#30340;&#25311;&#21512;&#24230;&#21644;&#20174;&#20020;&#24202;&#20915;&#31574;&#35282;&#24230;&#30340;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal inference for testing clinical hypotheses from observational data presents many difficulties because the underlying data-generating model and the associated causal graph are not usually available. Furthermore, observational data may contain missing values, which impact the recovery of the causal graph by causal discovery algorithms: a crucial issue often ignored in clinical studies. In this work, we use data from a multi-centric study on endometrial cancer to analyze the impact of different missingness mechanisms on the recovered causal graph. This is achieved by extending state-of-the-art causal discovery algorithms to exploit expert knowledge without sacrificing theoretical soundness. We validate the recovered graph with expert physicians, showing that our approach finds clinically-relevant solutions. Finally, we discuss the goodness of fit of our graph and its consistency from a clinical decision-making perspective using graphical separation to validate causal pathways.
&lt;/p&gt;</description></item><item><title>&#23376;&#23467;&#20869;&#33180;&#30284;&#28107;&#24052;&#32467;&#36716;&#31227;&#39118;&#38505;&#35780;&#20272;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#36136;&#37327;&#38382;&#39064;&#12289;&#32570;&#22833;&#20540;&#21644;&#39640;&#32500;&#24230;&#31561;&#38480;&#21046;&#65292;&#30740;&#31350;&#32773;&#37319;&#29992;&#22240;&#26524;&#24615;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#23398;&#20064;&#35780;&#20272;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#20808;&#21069;&#30693;&#35782;&#21033;&#29992;&#21644;&#20559;&#24046;&#32531;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10041</link><description>&lt;p&gt;
&#23376;&#23467;&#20869;&#33180;&#30284;&#24739;&#32773;&#28107;&#24052;&#32467;&#36716;&#31227;&#30340;&#39118;&#38505;&#35780;&#20272;&#65306;&#19968;&#31181;&#22240;&#26524;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Risk Assessment of Lymph Node Metastases in Endometrial Cancer Patients: A Causal Approach. (arXiv:2305.10041v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10041
&lt;/p&gt;
&lt;p&gt;
&#23376;&#23467;&#20869;&#33180;&#30284;&#28107;&#24052;&#32467;&#36716;&#31227;&#39118;&#38505;&#35780;&#20272;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#30001;&#20110;&#36136;&#37327;&#38382;&#39064;&#12289;&#32570;&#22833;&#20540;&#21644;&#39640;&#32500;&#24230;&#31561;&#38480;&#21046;&#65292;&#30740;&#31350;&#32773;&#37319;&#29992;&#22240;&#26524;&#24615;&#36125;&#21494;&#26031;&#32593;&#32476;&#26469;&#23398;&#20064;&#35780;&#20272;&#27169;&#22411;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#20808;&#21069;&#30693;&#35782;&#21033;&#29992;&#21644;&#20559;&#24046;&#32531;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#23376;&#23467;&#20869;&#33180;&#30284;&#24739;&#32773;&#26415;&#21069;&#28107;&#24052;&#32467;&#36716;&#31227;&#39118;&#38505;&#36827;&#34892;&#35780;&#20272;&#26159;&#19968;&#39033;&#22797;&#26434;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#22312;&#21407;&#21017;&#19978;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36275;&#22815;&#28789;&#27963;&#21644;&#34920;&#36798;&#65292;&#21487;&#20197;&#25429;&#25417;&#20020;&#24202;&#39118;&#38505;&#35780;&#20272;&#21160;&#24577;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#21482;&#33021;&#20351;&#29992;&#23384;&#22312;&#36136;&#37327;&#38382;&#39064;&#12289;&#32570;&#22833;&#20540;&#12289;&#26679;&#26412;&#23567;&#21644;&#39640;&#32500;&#24230;&#30340;&#35266;&#27979;&#25968;&#25454;&#65306;&#25105;&#20204;&#19981;&#33021;&#20174;&#21463;&#21040;&#36825;&#20123;&#20559;&#24046;&#24433;&#21709;&#30340;&#26377;&#38480;&#35266;&#27979;&#25968;&#25454;&#20013;&#21487;&#38752;&#22320;&#23398;&#20064;&#36825;&#20123;&#27169;&#22411;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#36873;&#25321;&#23398;&#20064;&#22240;&#26524;&#24615;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#20197;&#32531;&#35299;&#19978;&#36848;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20020;&#24202;&#21307;&#29983;&#21644;&#21307;&#24072;&#23545;&#23376;&#23467;&#20869;&#33180;&#30284;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#29992;&#20110;&#22240;&#26524;&#24615;&#36125;&#21494;&#26031;&#32593;&#32476;&#65292;&#22522;&#20110;Bootstrap&#37325;&#25277;&#26679;&#65292;&#32780;&#19981;&#26159;&#31867;&#20284;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#21333;&#19968;&#25554;&#34917;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21253;&#25324;&#19968;&#20010;&#19978;&#19979;&#25991;&#21464;&#37327;&#65292;&#20197;&#35780;&#20272;&#36873;&#25321;&#20559;&#24046;&#26159;&#21542;&#20250;&#23548;&#33268;&#23398;&#20064;&#34394;&#20551;&#20851;&#32852;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#26041;&#27861;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Assessing the pre-operative risk of lymph node metastases in endometrial cancer patients is a complex and challenging task. In principle, machine learning and deep learning models are flexible and expressive enough to capture the dynamics of clinical risk assessment. However, in this setting we are limited to observational data with quality issues, missing values, small sample size and high dimensionality: we cannot reliably learn such models from limited observational data with these sources of bias. Instead, we choose to learn a causal Bayesian network to mitigate the issues above and to leverage the prior knowledge on endometrial cancer available from clinicians and physicians. We introduce a causal discovery algorithm for causal Bayesian networks based on bootstrap resampling, as opposed to the single imputation used in related works. Moreover, we include a context variable to evaluate whether selection bias results in learning spurious associations. Finally, we discuss the strengt
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#22270;&#24418;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM(GPT-3/4)&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10037</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#20915;&#22270;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Solve Graph Problems in Natural Language?. (arXiv:2305.10037v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#22270;&#24418;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM(GPT-3/4)&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#19968;&#20123;&#20855;&#26377;&#38544;&#24335;&#22270;&#24418;&#32467;&#26500;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#35268;&#21010;&#12289;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#25110;&#30693;&#35782;&#25506;&#32034;&#12289;&#32467;&#26500;&#21270;&#24120;&#35782;&#25512;&#29702;&#31561;&#31561;&#12290;&#34429;&#28982;LLM&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;LLM&#26159;&#21542;&#33021;&#22815;&#26174;&#24335;&#22788;&#29702;&#22270;&#24418;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;&#22522;&#20110;&#27010;&#24565;&#30340;&#31354;&#38388;&#20013;&#65292;&#24182;&#25191;&#34892;&#32467;&#26500;&#21270;&#25805;&#20316;&#20173;&#28982;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#23427;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#20840;&#38754;&#27979;&#35797;&#12290;NLGraph&#21253;&#21547;29,370&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20843;&#20010;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65292;&#20174;&#31616;&#21333;&#30340;&#36830;&#25509;&#21644;&#26368;&#30701;&#36335;&#24452;&#21040;&#22797;&#26434;&#30340;&#26368;&#22823;&#27969;&#21644;&#27169;&#25311;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#20219;&#21153;&#19981;&#31561;&#12290;&#25105;&#20204;&#22312;NLGraph&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;LLM(GPT-3/4)&#65292;&#24182;&#21457;&#29616;1)&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65307;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#21457;&#29616;&#30340;&#29702;&#35770;&#12289;&#23454;&#36341;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#22240;&#26524;&#22270;&#24674;&#22797;&#31639;&#27861;&#12289;&#23454;&#38469;&#24212;&#29992;&#21450;&#20854;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.10032</link><description>&lt;p&gt;
&#22240;&#26524;&#25506;&#32034;&#32508;&#36848;&#65306;&#29702;&#35770;&#19982;&#23454;&#36341;&#65288;arXiv:2305.10032v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
A Survey on Causal Discovery: Theory and Practice. (arXiv:2305.10032v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10032
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#21457;&#29616;&#30340;&#29702;&#35770;&#12289;&#23454;&#36341;&#21644;&#26368;&#26032;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#22240;&#26524;&#22270;&#24674;&#22797;&#31639;&#27861;&#12289;&#23454;&#38469;&#24212;&#29992;&#21450;&#20854;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#25511;&#21046;&#29616;&#35937;&#30340;&#35268;&#24459;&#26159;&#31185;&#23398;&#36827;&#27493;&#30340;&#26680;&#24515;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#20197;&#22240;&#26524;&#26041;&#24335;&#24314;&#27169;&#19981;&#21516;&#26041;&#38754;&#30340;&#30456;&#20114;&#20316;&#29992;&#20026;&#30446;&#26631;&#26102;&#65292;&#36825;&#19968;&#28857;&#26356;&#20026;&#37325;&#35201;&#12290;&#20107;&#23454;&#19978;&#65292;&#22240;&#26524;&#25512;&#26029;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#37327;&#21270;&#23548;&#33268;&#20854;&#25928;&#24212;&#30340;&#22522;&#26412;&#20851;&#31995;&#12290;&#22240;&#26524;&#21457;&#29616;&#26159;&#26356;&#24191;&#27867;&#30340;&#22240;&#26524;&#20851;&#31995;&#39046;&#22495;&#30340;&#19968;&#20010;&#20998;&#25903;&#65292;&#22312;&#20854;&#20013;&#20174;&#25968;&#25454;&#20013;&#24674;&#22797;&#22240;&#26524;&#22270;&#65288;&#22312;&#21487;&#33021;&#30340;&#24773;&#20917;&#19979;&#65289;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#22240;&#26524;&#25928;&#24212;&#30340;&#35782;&#21035;&#21644;&#20272;&#35745;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#25506;&#35752;&#20102;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20379;&#20102;&#23545;&#19981;&#21516;&#35774;&#32622;&#19979;&#24050;&#24320;&#21457;&#31639;&#27861;&#30340;&#19968;&#33268;&#27010;&#36848;&#65292;&#25253;&#21578;&#20102;&#26377;&#29992;&#30340;&#24037;&#20855;&#21644;&#25968;&#25454;&#65292;&#24182;&#25552;&#20379;&#23454;&#38469;&#24212;&#29992;&#20197;&#29702;&#35299;&#36825;&#20123;&#26041;&#27861;&#20026;&#20160;&#20040;&#20197;&#21450;&#22914;&#20309;&#24471;&#21040;&#20016;&#23500;&#30340;&#21033;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the laws that govern a phenomenon is the core of scientific progress. This is especially true when the goal is to model the interplay between different aspects in a causal fashion. Indeed, causal inference itself is specifically designed to quantify the underlying relationships that connect a cause to its effect. Causal discovery is a branch of the broader field of causality in which causal graphs is recovered from data (whenever possible), enabling the identification and estimation of causal effects. In this paper, we explore recent advancements in a unified manner, provide a consistent overview of existing algorithms developed under different settings, report useful tools and data, present real-world applications to understand why and how these methods can be fruitfully exploited.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;qasp&#24605;&#24819;&#30340;ASP(Q)&#27714;&#35299;&#22120;&#65292;&#24341;&#20837;&#20102;&#26356;&#26377;&#25928;&#30340;&#32534;&#30721;&#36807;&#31243;&#21644;&#26032;&#30340;&#20248;&#21270;&#32534;&#30721;&#65292;&#24182;&#20351;&#29992;&#31639;&#27861;&#36873;&#25321;&#31574;&#30053;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20248;&#20110;qasp&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.10021</link><description>&lt;p&gt;
ASP(Q)&#30340;&#39640;&#25928;&#27714;&#35299;&#22120;
&lt;/p&gt;
&lt;p&gt;
An efficient solver for ASP(Q). (arXiv:2305.10021v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10021
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;qasp&#24605;&#24819;&#30340;ASP(Q)&#27714;&#35299;&#22120;&#65292;&#24341;&#20837;&#20102;&#26356;&#26377;&#25928;&#30340;&#32534;&#30721;&#36807;&#31243;&#21644;&#26032;&#30340;&#20248;&#21270;&#32534;&#30721;&#65292;&#24182;&#20351;&#29992;&#31639;&#27861;&#36873;&#25321;&#31574;&#30053;&#26469;&#25552;&#39640;&#24615;&#33021;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20248;&#20110;qasp&#21644;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24102;&#37327;&#35789;&#30340;&#31572;&#26696;&#38598;&#32534;&#31243;ASP(Q)&#36890;&#36807;&#23558;&#38382;&#39064;&#27169;&#22359;&#21270;&#21644;&#22768;&#26126;&#21270;&#65292;&#23558;&#31572;&#26696;&#38598;&#32534;&#31243;&#65288;ASP&#65289;&#25193;&#23637;&#21040;&#25972;&#20010;&#22810;&#39033;&#24335;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#38382;&#39064;&#24314;&#27169;&#12290;&#31532;&#19968;&#20010;&#23454;&#29616;ASP(Q)&#30340;&#24037;&#20855;qasp&#22522;&#20110;&#37327;&#21270;&#24067;&#23572;&#20844;&#24335;&#65288;QBF&#65289;&#30340;&#32763;&#35793;&#65292;&#26088;&#22312;&#21033;&#29992;&#25104;&#29087;&#30340;QBF&#27714;&#35299;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;qasp&#20013;&#37319;&#29992;&#30340;QBF&#32534;&#30721;&#23454;&#29616;&#38750;&#24120;&#36890;&#29992;&#65292;&#30001;&#20110;&#31526;&#21495;&#21644;&#23376;&#21477;&#30340;&#25968;&#37327;&#24222;&#22823;&#65292;&#21487;&#33021;&#38590;&#20197;&#35780;&#20272;&#29616;&#26377;&#30340;QBF&#27714;&#35299;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23454;&#29616;&#26041;&#27861;&#65292;&#24314;&#31435;&#22312;qasp&#30340;&#24605;&#24819;&#22522;&#30784;&#19978;&#65292;&#20855;&#26377;&#26356;&#39640;&#25928;&#30340;&#32534;&#30721;&#36807;&#31243;&#21644;ASP(Q)&#31243;&#24207;&#22312;QBF&#20013;&#30340;&#26032;&#20248;&#21270;&#32534;&#30721;&#12290;&#26032;&#30340;&#32534;&#30721;&#20135;&#29983;&#26356;&#23567;&#30340;&#20844;&#24335;&#65288;&#22312;&#37327;&#35789;&#12289;&#21464;&#37327;&#21644;&#23376;&#21477;&#30340;&#25968;&#37327;&#26041;&#38754;&#65289;&#65292;&#24182;&#23548;&#33268;&#26356;&#39640;&#25928;&#30340;&#35780;&#20272;&#36807;&#31243;&#12290;&#31639;&#27861;&#36873;&#25321;&#31574;&#30053;&#33258;&#21160;&#32452;&#21512;&#22810;&#20010;QBF&#27714;&#35299;&#22120;&#65292;&#20197;&#25214;&#21040;&#26368;&#20339;&#30340;&#27714;&#35299;&#22120;&#26469;&#35780;&#20272;&#32534;&#30721;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#25105;&#20204;&#27714;&#35299;&#22120;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27714;&#35299;&#22120;&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#28040;&#32791;&#26041;&#38754;&#20248;&#20110;qasp&#21644;&#20854;&#20182;ASP(Q)&#30340;&#26368;&#20808;&#36827;&#30340;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Answer Set Programming with Quantifiers ASP(Q) extends Answer Set Programming (ASP) to allow for declarative and modular modeling of problems from the entire polynomial hierarchy. The first implementation of ASP(Q), called qasp, was based on a translation to Quantified Boolean Formulae (QBF) with the aim of exploiting the well-developed and mature QBF-solving technology. However, the implementation of the QBF encoding employed in qasp is very general and might produce formulas that are hard to evaluate for existing QBF solvers because of the large number of symbols and sub-clauses. In this paper, we present a new implementation that builds on the ideas of qasp and features both a more efficient encoding procedure and new optimized encodings of ASP(Q) programs in QBF. The new encodings produce smaller formulas (in terms of the number of quantifiers, variables, and clauses) and result in a more efficient evaluation process. An algorithm selection strategy automatically combines several Q
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#23545;ViT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#29992;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#30340;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#65292;&#27604;&#20256;&#32479;CNN&#21644;ViT&#37117;&#34920;&#29616;&#26356;&#22909;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#20013;&#22270;&#20687;&#26631;&#27880;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10018</link><description>&lt;p&gt;
&#21033;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#21644;&#35270;&#35273;Transformer&#36827;&#34892;&#32454;&#31890;&#24230;&#20998;&#31867;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning for Fine-grained Classification Using Semi-supervised Learning and Visual Transformers. (arXiv:2305.10018v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10018
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#23545;ViT&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21487;&#29992;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#30340;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#65292;&#27604;&#20256;&#32479;CNN&#21644;ViT&#37117;&#34920;&#29616;&#26356;&#22909;&#65292;&#26377;&#21161;&#20110;&#35299;&#20915;&#30005;&#23376;&#21830;&#21153;&#20013;&#22270;&#20687;&#26631;&#27880;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32454;&#31890;&#24230;&#20998;&#31867;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#35782;&#21035;&#21516;&#19968;&#31867;&#21035;&#20869;&#29289;&#20307;&#20043;&#38388;&#24494;&#23567;&#24046;&#24322;&#12290;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#39033;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#30001;&#20110;&#20854;&#20351;&#29992;&#33258;&#27880;&#24847;&#21147;&#26426;&#21046;&#23398;&#20064;&#35270;&#35273;&#25968;&#25454;&#39640;&#24230;&#34920;&#29616;&#21147;&#30340;&#33021;&#21147;&#65292;&#35270;&#35273;Transformer&#65288;ViT&#65289;&#26368;&#36817;&#24050;&#25104;&#20026;&#22270;&#20687;&#20998;&#31867;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#21322;&#30417;&#30563;ViT&#65292;&#19968;&#31181;&#24212;&#29992;&#20110;&#32570;&#20047;&#27880;&#37322;&#25968;&#25454;&#24773;&#20917;&#19979;&#30340;ViT&#27169;&#22411;&#24494;&#35843;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#65292;&#36825;&#22312;&#30005;&#23376;&#21830;&#21153;&#20013;&#29305;&#21035;&#24120;&#35265;&#65292;&#20854;&#20013;&#22270;&#20687;&#26131;&#20110;&#33719;&#21462;&#20294;&#26631;&#31614;&#26377;&#22122;&#38899;&#12289;&#19981;&#23384;&#22312;&#25110;&#33719;&#21462;&#26114;&#36149;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#20351;&#29992;&#26377;&#38480;&#30340;&#27880;&#37322;&#25968;&#25454;&#36827;&#34892;&#24494;&#35843;&#65292;&#21322;&#30417;&#30563;ViT&#20173;&#28982;&#20248;&#20110;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;ViT&#12290;&#36825;&#20123;&#21457;&#29616;&#34920;&#26126;&#65292;&#21322;&#30417;&#30563;ViT&#22312;&#38656;&#35201;&#31934;&#30830;&#21644;&#32454;&#31890;&#24230;&#20998;&#31867;&#30340;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-grained classification is a challenging task that involves identifying subtle differences between objects within the same category. This task is particularly challenging in scenarios where data is scarce. Visual transformers (ViT) have recently emerged as a powerful tool for image classification, due to their ability to learn highly expressive representations of visual data using self-attention mechanisms. In this work, we explore Semi-ViT, a ViT model fine tuned using semi-supervised learning techniques, suitable for situations where we have lack of annotated data. This is particularly common in e-commerce, where images are readily available but labels are noisy, nonexistent, or expensive to obtain. Our results demonstrate that Semi-ViT outperforms traditional convolutional neural networks (CNN) and ViTs, even when fine-tuned with limited annotated data. These findings indicate that Semi-ViTs hold significant promise for applications that require precise and fine-grained classifi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;RAHC&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22788;&#29702;&#20219;&#24847;&#22797;&#26434;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#22270;&#20687;&#24674;&#22797;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;HAC&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#22522;&#20934;&#27979;&#35797;&#28151;&#21512;&#26465;&#20214;&#30340;&#22270;&#20687;&#24674;&#22797;&#12290;</title><link>http://arxiv.org/abs/2305.09996</link><description>&lt;p&gt;
&#19968;&#27425;&#24615;&#36824;&#21407;&#25429;&#33719;&#20110;&#20219;&#24847;&#22797;&#26434;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#22270;&#20687;
&lt;/p&gt;
&lt;p&gt;
Restoring Images Captured in Arbitrary Hybrid Adverse Weather Conditions in One Go. (arXiv:2305.09996v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09996
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;RAHC&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22788;&#29702;&#20219;&#24847;&#22797;&#26434;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#30340;&#22270;&#20687;&#24674;&#22797;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;HAC&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#22522;&#20934;&#27979;&#35797;&#28151;&#21512;&#26465;&#20214;&#30340;&#22270;&#20687;&#24674;&#22797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24694;&#21155;&#22825;&#27668;&#26465;&#20214;&#19979;&#65292;&#22270;&#20687;&#24448;&#24448;&#20250;&#21463;&#21040;&#38543;&#26426;&#28151;&#21512;&#30340;&#22825;&#27668;&#24433;&#21709;&#65288;&#20363;&#22914;&#65292;&#38632;&#22825;&#21644;&#38654;&#38718;&#22812;&#26202;&#65289;&#65292;&#32780;&#29616;&#26377;&#30340;&#22270;&#20687;&#24674;&#22797;&#31639;&#27861;&#39044;&#35745;&#22825;&#27668;&#24433;&#21709;&#26159;&#30456;&#20114;&#29420;&#31435;&#30340;&#65292;&#22240;&#27492;&#21487;&#33021;&#26080;&#27861;&#22788;&#29702;&#22797;&#26434;&#30340;&#29616;&#23454;&#24773;&#20917;&#12290;&#27492;&#22806;&#65292;&#30001;&#20110;&#32570;&#20047;&#20840;&#38754;&#23545;&#24212;&#30340;&#25968;&#25454;&#38598;&#26469;&#25551;&#36848;&#22797;&#26434;&#30340;&#28151;&#21512;&#22825;&#27668;&#29366;&#20917;&#65292;&#30417;&#30563;&#35757;&#32451;&#19981;&#21487;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#31574;&#30053;&#8212;&#8212;&#26694;&#26550;&#21644;&#25968;&#25454;&#8212;&#8212;&#26469;&#24357;&#34917;&#19978;&#36848;&#38480;&#21046;&#12290;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#31216;&#20026;RAHC&#65292;&#21487;&#20197;&#33298;&#36866;&#22320;&#22788;&#29702;&#28151;&#21512;&#24773;&#20917;&#65292;&#24182;&#36890;&#36807;&#21333;&#20010;&#35757;&#32451;&#27169;&#22411;&#28789;&#27963;&#22320;&#24674;&#22797;&#20219;&#24847;&#28151;&#21512;&#26465;&#20214;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#31216;&#20026;HAC&#65292;&#29992;&#20110;&#23398;&#20064;&#21644;&#22522;&#20934;&#27979;&#35797;&#20219;&#24847;&#28151;&#21512;&#26465;&#20214;&#30340;&#22270;&#20687;&#24674;&#22797;&#12290;HAC&#21253;&#21547;31&#31181;&#24773;&#26223;&#65292;&#21253;&#25324;&#20219;&#24847;&#32452;&#21512;&#30340;&#38632;&#22825;&#12289;&#38634;&#22825;&#12289;&#38654;&#38718;&#12289;&#38654;&#22825;&#21644;&#34180;&#38654;&#22825;&#27668;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adverse conditions typically suffer from stochastic hybrid weather degradations (e.g., rainy and hazy night), while existing image restoration algorithms envisage that weather degradations occur independently, thus may fail to handle real-world complicated scenarios. Besides, supervised training is not feasible due to the lack of comprehensive paired dataset to characterize hybrid conditions. To this end, we have advanced the forementioned limitations with two tactics: framework and data. On the one hand, we present a novel unified framework, dubbed RAHC, to Restore Arbitrary Hybrid adverse weather Conditions in one go, which can comfortably cope with hybrid scenarios with insufficient remaining background constituents and restore arbitrary hybrid conditions with a single trained model flexibly. On the other hand, we establish a new dataset, termed HAC, for learning and benchmarking arbitrary Hybrid Adverse Conditions restoration. HAC contains 31 scenarios composed of an arbitrary comb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21548;&#32773;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#35821;&#38899;&#22686;&#24378;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#22810;&#35828;&#35805;&#20154;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#30446;&#26631;&#21457;&#35328;&#32773;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09994</link><description>&lt;p&gt;
&#37319;&#29992;&#36328;&#36890;&#36947;&#21367;&#31215;&#20132;&#21449;&#27880;&#24847;&#21147;&#30340;&#22522;&#20110;&#26102;&#38388;&#22495;&#33041;&#27874;&#36741;&#21161;&#30340;&#35821;&#38899;&#22686;&#24378;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
BASEN: Time-Domain Brain-Assisted Speech Enhancement Network with Convolutional Cross Attention in Multi-talker Conditions. (arXiv:2305.09994v1 [eess.AS])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21548;&#32773;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#35821;&#38899;&#22686;&#24378;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#22810;&#35828;&#35805;&#20154;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#30446;&#26631;&#21457;&#35328;&#32773;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#35828;&#35805;&#20154;&#30340;&#24773;&#20917;&#19979;&#65292;&#26102;&#38388;&#22495;&#21333;&#36890;&#36947;&#35821;&#38899;&#22686;&#24378;&#65288;SE&#65289;&#20173;&#28982;&#38590;&#20197;&#25552;&#21462;&#30446;&#26631;&#21457;&#35328;&#32773;&#32780;&#19981;&#20855;&#22791;&#20219;&#20309;&#20808;&#21069;&#30340;&#20449;&#24687;&#12290;&#36890;&#36807;&#21548;&#35273;&#20851;&#27880;&#35299;&#30721;&#24050;&#32463;&#34920;&#26126;&#65292;&#21548;&#32773;&#30340;&#22823;&#33041;&#27963;&#21160;&#21253;&#21547;&#20102;&#25152;&#20851;&#27880;&#35828;&#35805;&#32773;&#30340;&#21548;&#35273;&#20449;&#24687;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#26102;&#38388;&#22495;&#33041;&#27874;&#36741;&#21161;&#30340;SE&#32593;&#32476;&#65288;BASEN&#65289;&#65292;&#35813;&#32593;&#32476;&#21253;&#21547;&#20174;&#21548;&#32773;&#35760;&#24405;&#30340;&#33041;&#30005;&#22270;&#65288;EEG&#65289;&#20449;&#21495;&#65292;&#21487;&#20197;&#20174;&#21333;&#22768;&#36947;&#35821;&#38899;&#28151;&#21512;&#29289;&#20013;&#25552;&#21462;&#30446;&#26631;&#35828;&#35805;&#32773;&#12290;&#25152;&#25552;&#20986;&#30340;BASEN&#22522;&#20110;&#20840;&#21367;&#31215;&#26102;&#38388;&#22495;&#38899;&#39057;&#20998;&#31163;&#32593;&#32476;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#33041;&#30005;&#22270;&#20449;&#21495;&#20013;&#21253;&#21547;&#30340;&#20114;&#34917;&#20449;&#24687;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#20010;&#21367;&#31215;&#22810;&#23618;&#20132;&#21449;&#27880;&#24847;&#27169;&#22359;&#65292;&#20197;&#34701;&#21512;&#21452;&#20998;&#25903;&#29305;&#24449;&#12290;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;&#22810;&#20010;&#35780;&#20272;&#24230;&#37327;&#19978;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#20195;&#30721;&#22312;&#25509;&#21463;&#21518;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time-domain single-channel speech enhancement (SE) still remains challenging to extract the target speaker without any prior information on multi-talker conditions. It has been shown via auditory attention decoding that the brain activity of the listener contains the auditory information of the attended speaker. In this paper, we thus propose a novel time-domain brain-assisted SE network (BASEN) incorporating electroencephalography (EEG) signals recorded from the listener for extracting the target speaker from monaural speech mixtures. The proposed BASEN is based on the fully-convolutional time-domain audio separation network. In order to fully leverage the complementary information contained in the EEG signals, we further propose a convolutional multi-layer cross attention module to fuse the dual-branch features. Experimental results on a public dataset show that the proposed model outperforms the state-of-the-art method in several evaluation metrics. The reproducible code is availabl
&lt;/p&gt;</description></item><item><title>Reprompting&#26159;&#19968;&#31181;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#37319;&#26679;&#26032;&#37197;&#26041;&#35299;&#20915;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#65292;&#27604;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#65292;&#36824;&#21487;&#20197;&#25552;&#39640;&#36739;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09993</link><description>&lt;p&gt;
Reprompting: &#36890;&#36807;&#21513;&#24067;&#26031;&#37319;&#26679;&#33258;&#21160;&#25512;&#26029;&#24605;&#32500;&#38142;&#30340;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Reprompting: Automated Chain-of-Thought Prompt Inference Through Gibbs Sampling. (arXiv:2305.09993v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09993
&lt;/p&gt;
&lt;p&gt;
Reprompting&#26159;&#19968;&#31181;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#36845;&#20195;&#37319;&#26679;&#26032;&#37197;&#26041;&#35299;&#20915;&#22810;&#27493;&#25512;&#29702;&#20219;&#21153;&#65292;&#27604;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#34920;&#29616;&#26356;&#22909;&#65292;&#36824;&#21487;&#20197;&#25552;&#39640;&#36739;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;Reprompting&#65292;&#36825;&#26159;&#19968;&#31181;&#36845;&#20195;&#37319;&#26679;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#25628;&#32034;&#32473;&#23450;&#20219;&#21153;&#30340;&#24605;&#32500;&#38142;&#37197;&#26041;&#12290;&#36890;&#36807;&#21513;&#24067;&#26031;&#37319;&#26679;&#65292;&#25105;&#20204;&#25512;&#26029;&#36866;&#29992;&#20110;&#19968;&#32452;&#35757;&#32451;&#26679;&#20363;&#30340;&#24605;&#32500;&#38142;&#37197;&#26041;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20808;&#21069;&#37319;&#26679;&#30340;&#35299;&#20316;&#20026;&#29238;&#25552;&#31034;&#65292;&#36845;&#20195;&#22320;&#37319;&#26679;&#26032;&#30340;&#37197;&#26041;&#26469;&#35299;&#20915;&#20854;&#20182;&#35757;&#32451;&#38382;&#39064;&#12290;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#20116;&#20010;Big-Bench Hard&#20219;&#21153;&#20013;&#65292;Reprompting&#30340;&#34920;&#29616;&#22987;&#32456;&#20248;&#20110;&#38646;&#26679;&#26412;&#12289;&#23569;&#26679;&#26412;&#21644;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#22522;&#32447;&#12290;Reprompting&#36824;&#21487;&#20197;&#20419;&#36827;&#30693;&#35782;&#20174;&#19968;&#20010;&#26356;&#24378;&#30340;&#27169;&#22411;&#21040;&#19968;&#20010;&#36739;&#24369;&#30340;&#27169;&#22411;&#30340;&#36716;&#31227;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#36739;&#24369;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;Reprompting&#30456;&#23545;&#20110;&#20351;&#29992;&#20154;&#31867;&#32534;&#20889;&#30340;&#24605;&#32500;&#38142;&#25552;&#31034;&#30340;&#20808;&#21069;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24102;&#26469;&#20102;&#39640;&#36798;+17&#20010;&#24615;&#33021;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Reprompting, an iterative sampling algorithm that searches for the Chain-of-Thought (CoT) recipes for a given task without human intervention. Through Gibbs sampling, we infer CoT recipes that work consistently well for a set of training samples. Our method iteratively samples new recipes using previously sampled solutions as parent prompts to solve other training problems. On five Big-Bench Hard tasks that require multi-step reasoning, Reprompting achieves consistently better performance than the zero-shot, few-shot, and human-written CoT baselines. Reprompting can also facilitate transfer of knowledge from a stronger model to a weaker model leading to substantially improved performance of the weaker model. Overall, Reprompting brings up to +17 point improvements over the previous state-of-the-art method that uses human-written CoT prompts.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#27169;&#22411;--&#34701;&#21512;&#23431;&#23449;(FU)&#65292;&#23427;&#23558;&#34394;&#25311;&#12289;&#29289;&#29702;&#21644;&#35748;&#30693;&#19990;&#30028;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#30740;&#31350;&#20102;&#20960;&#20010;&#24433;&#21709;&#27785;&#28024;&#24335;&#21644;&#20132;&#20114;&#24335;&#20307;&#39564;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#34701;&#21512;&#23431;&#23449;&#30340;&#22522;&#26412;&#21407;&#21017;&#65292;&#21487;&#20197;&#20351;&#29289;&#29702;&#21644;&#34394;&#25311;&#19990;&#30028;&#26080;&#32541;&#22320;&#34701;&#21512;&#36215;&#26469;&#12290;</title><link>http://arxiv.org/abs/2305.09992</link><description>&lt;p&gt;
&#19968;&#20010;&#34701;&#21512;&#27169;&#22411;: &#23454;&#29616;&#34394;&#25311;&#12289;&#29289;&#29702;&#21644;&#35748;&#30693;&#30340;&#19968;&#20307;&#21270;&#21450;&#20854;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Fusion Model: Towards a Virtual, Physical and Cognitive Integration and its Principles. (arXiv:2305.09992v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#34701;&#21512;&#27169;&#22411;--&#34701;&#21512;&#23431;&#23449;(FU)&#65292;&#23427;&#23558;&#34394;&#25311;&#12289;&#29289;&#29702;&#21644;&#35748;&#30693;&#19990;&#30028;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#30740;&#31350;&#20102;&#20960;&#20010;&#24433;&#21709;&#27785;&#28024;&#24335;&#21644;&#20132;&#20114;&#24335;&#20307;&#39564;&#30340;&#22240;&#32032;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#34701;&#21512;&#23431;&#23449;&#30340;&#22522;&#26412;&#21407;&#21017;&#65292;&#21487;&#20197;&#20351;&#29289;&#29702;&#21644;&#34394;&#25311;&#19990;&#30028;&#26080;&#32541;&#22320;&#34701;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34394;&#25311;&#29616;&#23454;(VR)&#12289;&#22686;&#24378;&#29616;&#23454;(AR)&#12289;&#28151;&#21512;&#29616;&#23454;(MR)&#12289;&#25968;&#23383;&#23402;&#29983;&#12289;&#20803;&#23431;&#23449;&#21644;&#20854;&#20182;&#30456;&#20851;&#25968;&#23383;&#25216;&#26415;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#20123;&#26032;&#20852;&#25216;&#26415;&#27491;&#22312;&#26174;&#33879;&#25913;&#21464;&#30528;&#19990;&#30028;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#34701;&#21512;&#27169;&#22411;&#8212;&#8212;&#34701;&#21512;&#23431;&#23449;(FU)&#65292;&#22312;&#36825;&#20010;&#27169;&#22411;&#20013;&#65292;&#34394;&#25311;&#12289;&#29289;&#29702;&#21644;&#35748;&#30693;&#19990;&#30028;&#34701;&#21512;&#22312;&#19968;&#36215;&#12290;&#22240;&#27492;&#65292;&#24314;&#31435;&#19968;&#20010;&#31526;&#21512;&#25105;&#20204;&#29289;&#29702;&#23431;&#23449;&#35268;&#24459;&#21644;&#21407;&#21017;&#30340;&#34701;&#21512;&#27169;&#22411;&#30340;&#19968;&#22871;&#21407;&#21017;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20960;&#20010;&#21487;&#33021;&#24433;&#21709;&#27785;&#28024;&#24335;&#21644;&#20132;&#20114;&#24335;&#20307;&#39564;&#30340;&#22240;&#32032;; &#24182;&#25552;&#20986;&#20102;&#34701;&#21512;&#23431;&#23449;&#30340;&#22522;&#26412;&#21407;&#21017;&#65292;&#21487;&#20197;&#23558;&#29289;&#29702;&#21644;&#34394;&#25311;&#19990;&#30028;&#26080;&#32541;&#22320;&#34701;&#21512;&#36215;&#26469;&#12290;
&lt;/p&gt;
&lt;p&gt;
Virtual Reality (VR), Augmented Reality (AR), Mixed Reality (MR), digital twin, Metaverse and other related digital technologies have attracted much attention in recent years. These new emerging technologies are changing the world significantly. This research introduces a fusion model, i.e. Fusion Universe (FU), where the virtual, physical, and cognitive worlds are merged together. Therefore, it is crucial to establish a set of principles for the fusion model that is compatible with our physical universe laws and principles. This paper investigates several aspects that could affect immersive and interactive experience; and proposes the fundamental principles for Fusion Universe that can integrate physical and virtual world seamlessly.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#32463;&#20856;&#30340;SGD&#26694;&#26550;&#19979;&#23454;&#29616;&#33258;&#36866;&#24212;&#27493;&#38271;&#36873;&#25321;&#65292;&#22312;&#36923;&#36753;&#22238;&#24402;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19982;&#25163;&#21160;&#35843;&#25972;&#24471;&#21040;&#30340;&#26368;&#20339;&#27493;&#38271;&#30456;&#24403;&#30340;&#27493;&#38271;&#12290;</title><link>http://arxiv.org/abs/2305.09978</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#30340;&#38543;&#26426;&#27604;&#29575;&#36319;&#36394;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Ratios Tracking Algorithm for Large Scale Machine Learning Problems. (arXiv:2305.09978v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09978
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#22312;&#32463;&#20856;&#30340;SGD&#26694;&#26550;&#19979;&#23454;&#29616;&#33258;&#36866;&#24212;&#27493;&#38271;&#36873;&#25321;&#65292;&#22312;&#36923;&#36753;&#22238;&#24402;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19982;&#25163;&#21160;&#35843;&#25972;&#24471;&#21040;&#30340;&#26368;&#20339;&#27493;&#38271;&#30456;&#24403;&#30340;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#21644;&#20219;&#21153;&#37117;&#20381;&#36182;&#20110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#31639;&#27861;&#21450;&#20854;&#21464;&#20307;&#12290;&#26377;&#25928;&#30340;&#27493;&#38271;&#36873;&#25321;&#23545;&#31639;&#27861;&#30340;&#25104;&#21151;&#33267;&#20851;&#37325;&#35201;&#65292;&#36825;&#20419;&#36827;&#20102;&#35832;&#22914;ADAM&#25110;AdaGrad&#20043;&#31867;&#30340;&#31639;&#27861;&#30340;&#21457;&#23637;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31639;&#27861;&#65292;&#22312;&#32463;&#20856;&#30340;SGD&#26694;&#26550;&#19979;&#23454;&#29616;&#33258;&#36866;&#24212;&#27493;&#38271;&#36873;&#25321;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#36866;&#24212;&#20854;&#20182;&#38543;&#26426;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#28789;&#24863;&#26469;&#33258;&#20256;&#32479;&#30340;&#38750;&#32447;&#24615;&#20248;&#21270;&#25216;&#26415;&#65292;&#24182;&#21463;&#21040;&#20998;&#26512;&#21457;&#29616;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#21512;&#29702;&#26465;&#20214;&#19979;&#65292;&#35813;&#31639;&#27861;&#20135;&#29983;&#31526;&#21512;&#33391;&#22909;&#29702;&#35770;&#35201;&#27714;&#30340;&#27493;&#38271;&#65292;&#24182;&#22312;&#26399;&#26395;&#19979;&#29983;&#25104;&#25910;&#25947;&#20110;&#35299;&#30340;&#38745;&#27490;&#37051;&#22495;&#30340;&#36845;&#20195;&#12290;&#25105;&#20204;&#22312;&#36923;&#36753;&#22238;&#24402;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#27979;&#35797;&#20102;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#21487;&#20197;&#29983;&#25104;&#19982;&#25163;&#21160;&#35843;&#25972;&#24471;&#21040;&#30340;&#26368;&#20339;&#27493;&#38271;&#30456;&#24403;&#30340;&#27493;&#38271;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many machine learning applications and tasks rely on the stochastic gradient descent (SGD) algorithm and its variants. Effective step length selection is crucial for the success of these algorithms, which has motivated the development of algorithms such as ADAM or AdaGrad. In this paper, we propose a novel algorithm for adaptive step length selection in the classical SGD framework, which can be readily adapted to other stochastic algorithms. Our proposed algorithm is inspired by traditional nonlinear optimization techniques and is supported by analytical findings. We show that under reasonable conditions, the algorithm produces step lengths in line with well-established theoretical requirements, and generates iterates that converge to a stationary neighborhood of a solution in expectation. We test the proposed algorithm on logistic regressions and deep neural networks and demonstrate that the algorithm can generate step lengths comparable to the best step length obtained from manual tu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#28183;&#36879;&#30340;&#23884;&#20837;&#25216;&#26415;&#65292;&#36890;&#36807;&#32500;&#25252;&#26368;&#30701;&#36335;&#24452;&#21644;&#28040;&#38500;&#20887;&#20313;&#36335;&#24452;&#26469;&#26368;&#23567;&#21270;&#29109;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.09974</link><description>&lt;p&gt;
&#26080;&#24402;&#27827;&#27969;&#65306;&#22522;&#20110;&#22270;&#28183;&#36879;&#23884;&#20837;&#30340;&#39640;&#25928;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
River of No Return: Graph Percolation Embeddings for Efficient Knowledge Graph Reasoning. (arXiv:2305.09974v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09974
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#28183;&#36879;&#30340;&#23884;&#20837;&#25216;&#26415;&#65292;&#36890;&#36807;&#32500;&#25252;&#26368;&#30701;&#36335;&#24452;&#21644;&#28040;&#38500;&#20887;&#20313;&#36335;&#24452;&#26469;&#26368;&#23567;&#21270;&#29109;&#30340;&#28040;&#24687;&#20256;&#36882;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#30693;&#35782;&#22270;&#35889;&#25512;&#29702;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#25512;&#29702;&#23884;&#20837;&#25216;&#26415;&#12290;&#25105;&#20204;&#39318;&#27425;&#23558;&#36335;&#24452;&#32534;&#30721;&#21644;&#28040;&#24687;&#20256;&#36882;&#30340;&#26368;&#20808;&#36827;KG&#25512;&#29702;&#27169;&#22411;&#20013;&#30340;&#36335;&#24452;&#20887;&#20313;&#38382;&#39064;&#19982;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#21464;&#25442;&#35823;&#24046;&#32852;&#31995;&#36215;&#26469;&#65292;&#36825;&#20026;&#25105;&#20204;&#24102;&#26469;&#20102;&#23545;KG&#25512;&#29702;&#30340;&#26032;&#30340;&#29702;&#35770;&#27934;&#35265;&#65292;&#20197;&#21450;&#22312;&#23454;&#36341;&#20013;&#20855;&#26377;&#39640;&#25928;&#24615;&#12290;&#22312;&#29702;&#35770;&#26041;&#38754;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;KG&#36335;&#24452;&#21464;&#25442;&#35823;&#24046;&#30340;&#29109;&#65292;&#24182;&#25351;&#20986;&#20102;&#26597;&#35810;&#29305;&#23450;&#20887;&#20313;&#36335;&#24452;&#20250;&#24341;&#36215;&#29109;&#30340;&#22686;&#21152;&#12290;&#36825;&#20123;&#21457;&#29616;&#25351;&#23548;&#25105;&#20204;&#32500;&#25252;&#26368;&#30701;&#36335;&#24452;&#65292;&#24182;&#28040;&#38500;&#20887;&#20313;&#36335;&#24452;&#20197;&#26368;&#23567;&#21270;&#29109;&#30340;&#28040;&#24687;&#20256;&#36882;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#22312;&#23454;&#36341;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#22270;&#28183;&#36879;&#36807;&#31243;&#65292;&#35813;&#36807;&#31243;&#21463;&#27969;&#20307;&#21147;&#23398;&#20013;&#28183;&#36879;&#27169;&#22411;&#30340;&#21551;&#21457;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#22522;&#20110;GNN&#30340;KG&#25512;&#29702;&#26694;&#26550;&#65292;&#31216;&#20026;&#22270;&#28183;&#36879;&#23884;&#20837;&#65288;GraPE&#65289;&#12290;GraPE&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;WN18RR&#21644;FB15K237&#19978;&#30340;&#24402;&#32435;&#24335;&#21644;&#20256;&#36882;&#24335;KG&#25512;&#29702;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study Graph Neural Networks (GNNs)-based embedding techniques for knowledge graph (KG) reasoning. For the first time, we link the path redundancy issue in the state-of-the-art KG reasoning models based on path encoding and message passing to the transformation error in model training, which brings us new theoretical insights into KG reasoning, as well as high efficacy in practice. On the theoretical side, we analyze the entropy of transformation error in KG paths and point out query-specific redundant paths causing entropy increases. These findings guide us to maintain the shortest paths and remove redundant paths for minimized-entropy message passing. To achieve this goal, on the practical side, we propose an efficient Graph Percolation Process motivated by the percolation model in Fluid Mechanics, and design a lightweight GNN-based KG reasoning framework called Graph Percolation Embeddings (GraPE). GraPE outperforms previous state-of-the-art methods in both transductive and induct
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HOI&#26816;&#27979;&#25968;&#25454;&#25286;&#20998;&#65292;&#26088;&#22312;&#35780;&#20272;&#31995;&#32479;&#24615;&#27867;&#21270;&#12290;&#22312;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;HOI&#26816;&#27979;&#27169;&#22411;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#21644;&#20132;&#20114;&#32452;&#21512;&#30340;&#27867;&#21270;&#21313;&#20998;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.09948</link><description>&lt;p&gt;
HICO-DET-SG&#21644;V-COCO-SG&#65306;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#29992;&#20110;&#35780;&#20272;&#20154;-&#29289;&#20132;&#20114;&#26816;&#27979;&#20013;&#30340;&#31995;&#32479;&#24615;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
HICO-DET-SG and V-COCO-SG: New Data Splits to Evaluate Systematic Generalization in Human-Object Interaction Detection. (arXiv:2305.09948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HOI&#26816;&#27979;&#25968;&#25454;&#25286;&#20998;&#65292;&#26088;&#22312;&#35780;&#20272;&#31995;&#32479;&#24615;&#27867;&#21270;&#12290;&#22312;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;HOI&#26816;&#27979;&#27169;&#22411;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#21644;&#20132;&#20114;&#32452;&#21512;&#30340;&#27867;&#21270;&#21313;&#20998;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;-&#29289;&#20132;&#20114;&#26816;&#27979;&#26159;&#19968;&#31181;&#39044;&#27979;&#22270;&#20687;&#20013;&#20154;&#19982;&#29289;&#21697;&#20043;&#38388;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#23545;HOI&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#24615;&#30340;&#27867;&#21270;&#65292;&#21363;&#27867;&#21270;&#21040;&#26032;&#30340;&#23545;&#35937;&#21644;&#20132;&#20114;&#32452;&#21512;&#19978;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20165;&#21487;&#33021;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#32452;&#21512;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27809;&#26377;&#24320;&#25918;&#30340;&#22522;&#20934;&#27979;&#35797;&#25110;&#29616;&#26377;&#24037;&#20316;&#35780;&#20272;HOI&#26816;&#27979;&#20013;&#30340;&#31995;&#32479;&#24615;&#27867;&#21270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;HICO-DET&#21644;V-COCO&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#20004;&#20010;&#21517;&#20026;HICO-DET-SG&#21644;V-COCO-SG&#30340;&#26032;&#30340;HOI&#26816;&#27979;&#25968;&#25454;&#25286;&#20998;&#12290;&#25105;&#20204;&#22312;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#19978;&#35780;&#20272;&#20102;&#20195;&#34920;&#24615;&#30340;HOI&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#27979;&#35797;&#24615;&#33021;&#26377;&#24456;&#22823;&#30340;&#38477;&#20302;&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#31995;&#32479;&#24615;&#27867;&#21270;&#26159;HOI&#26816;&#27979;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#25286;&#20998;&#33021;&#22815;&#40723;&#21169;&#26356;&#22810;&#30340;&#30740;&#31350;&#26397;&#30528;&#36825;&#20010;&#30446;&#26631;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-Object Interaction (HOI) detection is a task to predict interactions between humans and objects in an image. In real-world scenarios, HOI detection models are required systematic generalization, i.e., generalization to novel combinations of objects and interactions, because it is highly probable that the train data only cover a limited portion of all possible combinations. However, to our knowledge, no open benchmark or existing work evaluates the systematic generalization in HOI detection. To address this issue, we created two new sets of HOI detection data splits named HICO-DET-SG and V-COCO-SG based on HICO-DET and V-COCO datasets. We evaluated representative HOI detection models on the new data splits and observed large degradation in the test performances compared to those on the original datasets. This result shows that systematic generalization is a challenging goal in HOI detection. We hope our new data splits encourage more research toward this goal.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#21305;&#20857;&#22561;&#23398;&#20064;&#20998;&#31867;&#22120;&#31995;&#32479;&#65292;&#19982;&#32463;&#20856;&#30340;&#23494;&#27463;&#26681;&#31995;&#32479; XCS &#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426; FrozenLake &#29615;&#22659;&#20013;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09945</link><description>&lt;p&gt;
&#21487;&#35299;&#37322;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21305;&#20857;&#22561;&#23398;&#20064;&#20998;&#31867;&#22120;&#31995;&#32479;&#65306;&#19982; XCS &#30340;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
Pittsburgh Learning Classifier Systems for Explainable Reinforcement Learning: Comparing with XCS. (arXiv:2305.09945v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09945
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#20010;&#26032;&#30340;&#21305;&#20857;&#22561;&#23398;&#20064;&#20998;&#31867;&#22120;&#31995;&#32479;&#65292;&#19982;&#32463;&#20856;&#30340;&#23494;&#27463;&#26681;&#31995;&#32479; XCS &#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#24182;&#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426; FrozenLake &#29615;&#22659;&#20013;&#33719;&#24471;&#20102;&#36739;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24212;&#29992;&#65292;&#23545;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#30340;&#20852;&#36259;&#26377;&#25152;&#22686;&#21152;&#65292;&#20294;&#26159;&#19982;&#35937;&#24449;&#24615;&#31995;&#32479;&#30456;&#27604;&#65292;&#36825;&#20123;&#36830;&#25509;&#20027;&#20041;&#26041;&#27861;&#19981;&#36879;&#26126;&#12290;&#23398;&#20064;&#20998;&#31867;&#22120;&#31995;&#32479; (LCS) &#26159;&#36827;&#21270;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#65292;&#21487;&#34987;&#24402;&#31867;&#20026;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021; (XAI)&#65292;&#22240;&#20026;&#23427;&#20204;&#22522;&#20110;&#35268;&#21017;&#30340;&#26412;&#36136;&#12290;&#23494;&#27463;&#26681; LCS &#36890;&#24120;&#22312; RL &#39046;&#22495;&#20013;&#20351;&#29992;&#65292;&#32780;&#21305;&#20857;&#22561;&#31995;&#32479;&#65288;&#20363;&#22914; SAMUEL&#65289;&#30001;&#20110;&#22797;&#26434;&#30340;&#31639;&#27861;&#35774;&#35745;&#21644;&#39640;&#35745;&#31639;&#35201;&#27714;&#32780;&#24456;&#23569;&#20351;&#29992;&#65307;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#20197;&#20135;&#29983;&#27604;&#23494;&#27463;&#26681;&#31995;&#32479;&#26356;&#31616;&#27905;/&#21487;&#35299;&#37322;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#20004;&#20010;&#26032;&#30340;&#21305;&#20857;&#22561; LCS&#65292;&#20197;&#35299;&#20915; RL &#39046;&#22495;&#30340;&#38382;&#39064;&#65306;PPL-DL &#21644; PPL-ST&#12290;&#21069;&#32773;&#20805;&#24403;&#8220;&#38646;&#32423;&#8221;&#31995;&#32479;&#65292;&#21518;&#32773;&#37325;&#35775;&#20102; SAMUEL &#30340;&#26680;&#24515;&#33945;&#29305;&#21345;&#32599;&#23398;&#20064;&#26426;&#21046;&#65292;&#29992;&#20110;&#20272;&#35745;&#35268;&#21017;&#24378;&#24230;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20004;&#20010;&#21305;&#20857;&#22561;&#31995;&#32479;&#19982;&#23494;&#27463;&#26681;&#31995;&#32479; XCS &#22312;&#30830;&#23450;&#24615;&#21644;&#38543;&#26426; FrozenLake &#29615;&#22659;&#20013;&#36827;&#34892;&#27604;&#36739;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;PPL-ST&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Interest in reinforcement learning (RL) has recently surged due to the application of deep learning techniques, but these connectionist approaches are opaque compared with symbolic systems. Learning Classifier Systems (LCSs) are evolutionary machine learning systems that can be categorised as eXplainable AI (XAI) due to their rule-based nature. Michigan LCSs are commonly used in RL domains as the alternative Pittsburgh systems (e.g. SAMUEL) suffer from complex algorithmic design and high computational requirements; however they can produce more compact/interpretable solutions than Michigan systems. We aim to develop two novel Pittsburgh LCSs to address RL domains: PPL-DL and PPL-ST. The former acts as a "zeroth-level" system, and the latter revisits SAMUEL's core Monte Carlo learning mechanism for estimating rule strength. We compare our two Pittsburgh systems to the Michigan system XCS across deterministic and stochastic FrozenLake environments. Results show that PPL-ST performs on-pa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#28436;&#31034;&#30340;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;IBC&#65289;&#65292;&#36890;&#36807;&#36741;&#21161;&#20195;&#29702;&#21644;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#21452;&#21521;&#30446;&#26631;&#35838;&#31243;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#20808;&#21069;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20174;&#38750;&#21608;&#26399;&#24615;&#20132;&#20114;&#20013;&#23398;&#20064;&#65292;&#24182;&#22312;&#31232;&#30095;&#20219;&#21153;&#30456;&#20851;&#20132;&#20114;&#30340;&#29615;&#22659;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09943</link><description>&lt;p&gt;
&#26080;&#38656;&#28436;&#31034;&#30340;&#33258;&#20027;&#22686;&#24378;&#23398;&#20064;&#65306;&#38544;&#24335;&#21452;&#21521;&#35838;&#31243;&#27861;
&lt;/p&gt;
&lt;p&gt;
Demonstration-free Autonomous Reinforcement Learning via Implicit and Bidirectional Curriculum. (arXiv:2305.09943v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09943
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#28436;&#31034;&#30340;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65288;IBC&#65289;&#65292;&#36890;&#36807;&#36741;&#21161;&#20195;&#29702;&#21644;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#21452;&#21521;&#30446;&#26631;&#35838;&#31243;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#20808;&#21069;&#25968;&#25454;&#20381;&#36182;&#30340;&#24773;&#20917;&#19979;&#65292;&#23454;&#29616;&#20174;&#38750;&#21608;&#26399;&#24615;&#20132;&#20114;&#20013;&#23398;&#20064;&#65292;&#24182;&#22312;&#31232;&#30095;&#20219;&#21153;&#30456;&#20851;&#20132;&#20114;&#30340;&#29615;&#22659;&#20013;&#21462;&#24471;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24378;&#21270;&#23398;&#20064;&#22312;&#20165;&#36890;&#36807;&#19982;&#29615;&#22659;&#20132;&#20114;&#26469;&#33719;&#24471;&#22797;&#26434;&#25216;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#23427;&#20551;&#35774;&#22312;&#27599;&#20010;&#21608;&#26399;&#32467;&#26463;&#26102;&#37117;&#21487;&#20197;&#36731;&#26131;&#22320;&#22238;&#21040;&#21021;&#22987;&#29366;&#24577;&#12290;&#36825;&#31181;&#20551;&#35774;&#22952;&#30861;&#20102;&#20855;&#36523;&#20195;&#29702;&#30340;&#33258;&#20027;&#23398;&#20064;&#65292;&#22240;&#20026;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#36827;&#34892;&#37325;&#32622;&#38656;&#35201;&#32791;&#36153;&#26102;&#38388;&#21644;&#32321;&#29712;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22240;&#27492;&#65292;&#23545;&#20110;&#33021;&#22815;&#20174;&#38750;&#21608;&#26399;&#24615;&#20132;&#20114;&#20013;&#23398;&#20064;&#30340;&#33258;&#20027;&#24378;&#21270;&#23398;&#20064;&#65288;ARL&#65289;&#26041;&#27861;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;ARL&#26041;&#27861;&#21463;&#21040;&#20854;&#23545;&#20808;&#21069;&#25968;&#25454;&#30340;&#20381;&#36182;&#30340;&#38480;&#21046;&#65292;&#26080;&#27861;&#22312;&#20219;&#21153;&#30456;&#20851;&#20132;&#20114;&#31232;&#30095;&#30340;&#29615;&#22659;&#20013;&#23398;&#20064;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#38544;&#24335;&#21644;&#21452;&#21521;&#35838;&#31243;&#30340;&#26080;&#28436;&#31034;ARL&#31639;&#27861;&#65288;IBC&#65289;&#12290;&#36890;&#36807;&#36741;&#21161;&#20195;&#29702;&#20197;&#21450;&#22522;&#20110;&#26368;&#20248;&#36755;&#36816;&#30340;&#21452;&#21521;&#30446;&#26631;&#35838;&#31243;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#34920;&#29616;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#29978;&#33267;&#27604;&#21033;&#29992;&#28436;&#31034;&#30340;&#26041;&#27861;&#36824;&#35201;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
While reinforcement learning (RL) has achieved great success in acquiring complex skills solely from environmental interactions, it assumes that resets to the initial state are readily available at the end of each episode. Such an assumption hinders the autonomous learning of embodied agents due to the time-consuming and cumbersome workarounds for resetting in the physical world. Hence, there has been a growing interest in autonomous RL (ARL) methods that are capable of learning from non-episodic interactions. However, existing works on ARL are limited by their reliance on prior data and are unable to learn in environments where task-relevant interactions are sparse. In contrast, we propose a demonstration-free ARL algorithm via Implicit and Bi-directional Curriculum (IBC). With an auxiliary agent that is conditionally activated upon learning progress and a bidirectional goal curriculum based on optimal transport, our method outperforms previous methods, even the ones that leverage dem
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09941</link><description>&lt;p&gt;
&#8220;&#25105;&#20840;&#28982;&#25104;&#20026;&#25105;&#33258;&#24049;&#8221;&#65306;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
"I'm fully who I am": Towards Centering Transgender and Non-Binary Voices to Measure Biases in Open Language Generation. (arXiv:2305.09941v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09941
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#20197;TGNB&#20154;&#32676;&#30340;&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#35780;&#20272;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#20013;&#30340;&#20559;&#35265;&#12290;&#36890;&#36807;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25552;&#20986;&#20102;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#30340;OLG&#31995;&#32479;&#35780;&#20272;&#26694;&#26550;&#65292;&#24182;&#19988;&#21253;&#25324;&#19968;&#20010;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#21644;&#20998;&#26512;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36328;&#24615;&#21035;&#21644;&#38750;&#20108;&#20803;&#65288;TGNB&#65289;&#20154;&#32676;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#32463;&#21382;&#20102;&#19981;&#25104;&#27604;&#20363;&#30340;&#27495;&#35270;&#21644;&#25490;&#26021;&#12290;&#38543;&#30528;&#35821;&#35328;&#29983;&#25104;&#25216;&#26415;&#30340;&#26085;&#30410;&#26222;&#21450;&#21644;&#24212;&#29992;&#65292;&#36827;&#19968;&#27493;&#36793;&#32536;&#21270;&#36825;&#19968;&#20154;&#32676;&#30340;&#21487;&#33021;&#24615;&#20063;&#22312;&#22686;&#21152;&#12290;&#34429;&#28982;&#22823;&#37327;&#30340;NLP&#20844;&#24179;&#25991;&#29486;&#30528;&#37325;&#20110;&#38416;&#26126;&#21644;&#35299;&#20915;&#24615;&#21035;&#20559;&#35265;&#65292;&#20294;&#35780;&#20272;TGNB&#36523;&#20221;&#25152;&#24102;&#26469;&#30340;&#24615;&#21035;&#20260;&#23475;&#38656;&#35201;&#29702;&#35299;&#36825;&#20123;&#36523;&#20221;&#22914;&#20309;&#29420;&#29305;&#22320;&#19982;&#31038;&#20250;&#24615;&#21035;&#35268;&#33539;&#20114;&#21160;&#20197;&#21450;&#19982;&#24615;&#21035;&#20108;&#20803;&#20013;&#24515;&#30340;&#35270;&#35282;&#30456;&#21306;&#20998;&#12290;&#36825;&#26679;&#30340;&#27979;&#37327;&#26694;&#26550;&#26412;&#36136;&#19978;&#38656;&#35201;&#20197;TGNB&#22768;&#38899;&#20026;&#20013;&#24515;&#65292;&#24110;&#21161;&#25351;&#23548;&#21253;&#23481;&#24615;&#21035;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#35813;&#20026;&#35841;&#26381;&#21153;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20197;TGNB&#31038;&#21306;&#21644;&#29616;&#26377;&#30340;&#36328;&#23398;&#31185;&#25991;&#29486;&#20026;&#22522;&#30784;&#65292;&#35780;&#20272;&#20102;TGNB&#20010;&#20307;&#32463;&#21382;&#36793;&#32536;&#21270;&#25152;&#24418;&#25104;&#30340;&#31038;&#20250;&#29616;&#23454;&#26159;&#22914;&#20309;&#24433;&#21709;&#21644;&#23384;&#22312;&#20110;&#24320;&#25918;&#24335;&#35821;&#35328;&#29983;&#25104;&#65288;OLG&#65289;&#20013;&#12290;&#39318;&#20808;&#29702;&#35299;TGNB&#20010;&#20307;&#30340;&#32463;&#21382;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#35780;&#20272;OLG&#31995;&#32479;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20197;TGNB&#20154;&#32676;&#20026;&#20013;&#24515;&#65292;&#24230;&#37327;&#19982;&#35813;&#20154;&#32676;&#30456;&#20851;&#30340;&#20559;&#35265;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#29305;&#21035;&#20026;TGNB&#20154;&#32676;&#35774;&#35745;&#30340;&#35843;&#26597;&#24037;&#20855;&#65292;&#20197;&#21450;&#20132;&#21449;&#20998;&#26512;&#32467;&#26524;&#30340;&#20132;&#21449;&#26041;&#27861;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#36825;&#39033;&#24037;&#20316;&#23558;&#26377;&#21161;&#20110;&#23454;&#29616;&#26356;&#20844;&#24179;&#12289;&#26356;&#21253;&#23481;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31038;&#21306;&#65292;&#24182;&#28508;&#22312;&#22320;&#35299;&#20915;NLP&#30740;&#31350;&#20013;&#24191;&#27867;&#30340;&#20132;&#21449;&#36523;&#20221;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transgender and non-binary (TGNB) individuals disproportionately experience discrimination and exclusion from daily life. Given the recent popularity and adoption of language generation technologies, the potential to further marginalize this population only grows. Although a multitude of NLP fairness literature focuses on illuminating and addressing gender biases, assessing gender harms for TGNB identities requires understanding how such identities uniquely interact with societal gender norms and how they differ from gender binary-centric perspectives. Such measurement frameworks inherently require centering TGNB voices to help guide the alignment between gender-inclusive NLP and whom they are intended to serve. Towards this goal, we ground our work in the TGNB community and existing interdisciplinary literature to assess how the social reality surrounding experienced marginalization by TGNB persons contributes to and persists within Open Language Generation (OLG). By first understandi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#22240;&#27169;&#31946;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#28436;&#21270;&#20986;&#21487;&#35299;&#37322;&#30340;&#31616;&#32422;&#31574;&#30053;&#65292;&#24182;&#33021;&#26377;&#25928;&#24179;&#34913;&#31574;&#30053;&#24615;&#33021;&#19982;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09922</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#21487;&#35299;&#37322;&#21644;&#31616;&#32422;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#30340;&#22522;&#22240;&#27169;&#31946;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
A Genetic Fuzzy System for Interpretable and Parsimonious Reinforcement Learning Policies. (arXiv:2305.09922v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09922
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22522;&#22240;&#27169;&#31946;&#31995;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#65292;&#21487;&#28436;&#21270;&#20986;&#21487;&#35299;&#37322;&#30340;&#31616;&#32422;&#31574;&#30053;&#65292;&#24182;&#33021;&#26377;&#25928;&#24179;&#34913;&#31574;&#30053;&#24615;&#33021;&#19982;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27491;&#32463;&#21382;&#30528;&#30740;&#31350;&#20852;&#36259;&#30340;&#22797;&#33487;&#65292;&#23398;&#20064;&#20998;&#31867;&#22120;&#31995;&#32479;&#65288;LCS&#65289;&#24050;&#32463;&#34987;&#24212;&#29992;&#22810;&#24180;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#23494;&#27463;&#26681;&#26041;&#27861;&#24448;&#24448;&#20250;&#28436;&#21270;&#25104;&#22823;&#37327;&#30340;&#35268;&#21017;&#24211;&#65292;&#36825;&#20123;&#35268;&#21017;&#24211;&#38590;&#20197;&#35299;&#37322;&#25110;&#25193;&#23637;&#21040;&#36229;&#20986;&#26631;&#20934;&#36855;&#23467;&#20043;&#22806;&#30340;&#39046;&#22495;&#12290;&#25552;&#20986;&#20102;&#19968;&#31181;&#21305;&#20857;&#22561;&#22522;&#22240;&#27169;&#31946;&#31995;&#32479;&#65292;&#21517;&#20026;Fuzzy MoCoCo&#65292;&#20854;&#21033;&#29992;&#22810;&#30446;&#26631;&#21644;&#21512;&#20316;&#21327;&#21516;&#36827;&#21270;&#26426;&#21046;&#65292;&#20026;RL&#29615;&#22659;&#28436;&#21270;&#27169;&#31946;&#35268;&#21017;&#31574;&#30053;&#12290;&#31995;&#32479;&#20013;&#30340;&#22810;&#30446;&#26631;&#19982;&#31574;&#30053;&#24615;&#33021;&#19982;&#22797;&#26434;&#24615;&#26377;&#20851;&#12290;&#36830;&#32493;&#29366;&#24577;&#30340;RL&#29615;&#22659;Mountain Car&#34987;&#29992;&#20316;&#27979;&#35797;&#22522;&#30784;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#33021;&#22815;&#26377;&#25928;&#25506;&#32034;&#31574;&#30053;&#24615;&#33021;&#19982;&#22797;&#26434;&#24615;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#24182;&#23398;&#20064;&#20986;&#21487;&#35299;&#37322;&#30340;&#65292;&#39640;&#24615;&#33021;&#30340;&#31574;&#30053;&#65292;&#24182;&#23613;&#21487;&#33021;&#23569;&#22320;&#20351;&#29992;&#35268;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) is experiencing a resurgence in research interest, where Learning Classifier Systems (LCSs) have been applied for many years. However, traditional Michigan approaches tend to evolve large rule bases that are difficult to interpret or scale to domains beyond standard mazes. A Pittsburgh Genetic Fuzzy System (dubbed Fuzzy MoCoCo) is proposed that utilises both multiobjective and cooperative coevolutionary mechanisms to evolve fuzzy rule-based policies for RL environments. Multiobjectivity in the system is concerned with policy performance vs. complexity. The continuous state RL environment Mountain Car is used as a testing bed for the proposed system. Results show the system is able to effectively explore the trade-off between policy performance and complexity, and learn interpretable, high-performing policies that use as few rules as possible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#21450;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22914;&#20309;&#23398;&#20064;&#24178;&#39044;&#36873;&#39033;&#36873;&#25321;&#31574;&#30053;&#65292;&#32467;&#26524;&#34920;&#26126;&#19978;&#19979;&#25991;&#25512;&#26029;&#35823;&#24046;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#23545;&#23398;&#20064;&#26377;&#25928;&#31574;&#30053;&#30340;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#19981;&#30830;&#23450;&#24615;&#22686;&#21152;&#26102;&#20174;&#19978;&#19979;&#25991;&#25512;&#26029;&#20013;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#25552;&#39640;&#24178;&#39044;&#25928;&#26524;&#65292;&#32780;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#23545;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#29366;&#24577;&#20449;&#24687;&#30340;&#38750;&#20961;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09913</link><description>&lt;p&gt;
&#35780;&#20272;&#19978;&#19979;&#25991;&#25512;&#26029;&#35823;&#24046;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#23545;&#21450;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;RL&#26041;&#27861;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Assessing the Impact of Context Inference Error and Partial Observability on RL Methods for Just-In-Time Adaptive Interventions. (arXiv:2305.09913v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#21450;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22914;&#20309;&#23398;&#20064;&#24178;&#39044;&#36873;&#39033;&#36873;&#25321;&#31574;&#30053;&#65292;&#32467;&#26524;&#34920;&#26126;&#19978;&#19979;&#25991;&#25512;&#26029;&#35823;&#24046;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#23545;&#23398;&#20064;&#26377;&#25928;&#31574;&#30053;&#30340;&#33021;&#21147;&#20135;&#29983;&#24433;&#21709;&#65292;&#36890;&#36807;&#22312;&#19978;&#19979;&#25991;&#19981;&#30830;&#23450;&#24615;&#22686;&#21152;&#26102;&#20174;&#19978;&#19979;&#25991;&#25512;&#26029;&#20013;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#25552;&#39640;&#24178;&#39044;&#25928;&#26524;&#65292;&#32780;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#23545;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#29366;&#24577;&#20449;&#24687;&#30340;&#38750;&#20961;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21450;&#26102;&#33258;&#36866;&#24212;&#24178;&#39044;(JITAIs)&#26159;&#34892;&#20026;&#31185;&#23398;&#30028;&#24320;&#21457;&#30340;&#19968;&#31867;&#20010;&#24615;&#21270;&#20581;&#24247;&#24178;&#39044;&#12290; JITAIs&#26088;&#22312;&#36890;&#36807;&#20174;&#39044;&#23450;&#20041;&#30340;&#32452;&#20214;&#38598;&#20013;&#36845;&#20195;&#36873;&#25321;&#24178;&#39044;&#36873;&#39033;&#24207;&#21015;&#26469;&#21709;&#24212;&#27599;&#20010;&#20010;&#20307;&#30340;&#26102;&#38388;&#21464;&#21270;&#29366;&#24577;&#65292;&#20197;&#25552;&#20379;&#27491;&#30830;&#31867;&#22411;&#21644;&#25968;&#37327;&#30340;&#25903;&#25345;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#23398;&#20064;&#24178;&#39044;&#36873;&#39033;&#36873;&#25321;&#31574;&#30053;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#25512;&#26029;&#35823;&#24046;&#21644;&#37096;&#20998;&#21487;&#35266;&#23519;&#24615;&#23545;&#23398;&#20064;&#26377;&#25928;&#31574;&#30053;&#30340;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#19978;&#19979;&#25991;&#19981;&#30830;&#23450;&#24615;&#22686;&#21152;&#26102;&#65292;&#20174;&#19978;&#19979;&#25991;&#25512;&#26029;&#20013;&#20256;&#25773;&#30340;&#19981;&#30830;&#23450;&#24615;&#23545;&#25552;&#39640;&#24178;&#39044;&#25928;&#26524;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#21487;&#20197;&#25552;&#20379;&#23545;&#37096;&#20998;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#29366;&#24577;&#20449;&#24687;&#30340;&#38750;&#20961;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Just-in-Time Adaptive Interventions (JITAIs) are a class of personalized health interventions developed within the behavioral science community. JITAIs aim to provide the right type and amount of support by iteratively selecting a sequence of intervention options from a pre-defined set of components in response to each individual's time varying state. In this work, we explore the application of reinforcement learning methods to the problem of learning intervention option selection policies. We study the effect of context inference error and partial observability on the ability to learn effective policies. Our results show that the propagation of uncertainty from context inferences is critical to improving intervention efficacy as context uncertainty increases, while policy gradient algorithms can provide remarkable robustness to partially observed behavioral state information.
&lt;/p&gt;</description></item><item><title>ClusterNS &#26159;&#19968;&#31181;&#23558;&#32858;&#31867;&#20449;&#24687;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340; K &#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#25552;&#20379;&#38590;&#36127;&#20363;&#24182;&#35782;&#21035;&#38169;&#35823;&#36127;&#20363;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#35299;&#20915;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#12290;</title><link>http://arxiv.org/abs/2305.09892</link><description>&lt;p&gt;
&#38754;&#21521;&#32858;&#31867;&#30340;&#36127;&#37319;&#26679;&#29992;&#20110;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Clustering-Aware Negative Sampling for Unsupervised Sentence Representation. (arXiv:2305.09892v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09892
&lt;/p&gt;
&lt;p&gt;
ClusterNS &#26159;&#19968;&#31181;&#23558;&#32858;&#31867;&#20449;&#24687;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#30340; K &#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#25552;&#20379;&#38590;&#36127;&#20363;&#24182;&#35782;&#21035;&#38169;&#35823;&#36127;&#20363;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#35299;&#20915;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#27604;&#23398;&#20064;&#22312;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#24191;&#27867;&#30740;&#31350;&#65292;&#28982;&#32780;&#26089;&#26399;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#27491;&#20363;&#30340;&#26500;&#24314;&#19978;&#65292;&#32780; batch &#20869;&#30340;&#26679;&#26412;&#36890;&#24120;&#34987;&#35270;&#20026;&#36127;&#20363;&#12290;&#36825;&#31181;&#26041;&#27861;&#24573;&#35270;&#20102;&#36873;&#25321;&#21512;&#36866;&#30340;&#36127;&#20363;&#30340;&#37325;&#35201;&#24615;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#38590;&#36127;&#20363;&#30340;&#31232;&#32570;&#24615;&#21644;&#38169;&#35823;&#36127;&#20363;&#30340;&#21253;&#21547;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; ClusterNS&#65288;&#38754;&#21521;&#32858;&#31867;&#30340;&#36127;&#37319;&#26679;&#65289;&#65292;&#19968;&#31181;&#23558;&#32858;&#31867;&#20449;&#24687;&#24341;&#20837;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#30340;&#26032;&#26041;&#27861;&#12290;&#25105;&#20204;&#24212;&#29992;&#25913;&#36827;&#30340; K &#22343;&#20540;&#32858;&#31867;&#31639;&#27861;&#26469;&#25552;&#20379;&#38590;&#36127;&#20363;&#24182;&#35782;&#21035;&#35757;&#32451;&#36807;&#31243;&#20013;&#30340;&#38169;&#35823;&#36127;&#20363;&#65292;&#26088;&#22312;&#36890;&#36807;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#22312;&#35821;&#20041;&#25991;&#26412;&#30456;&#20284;&#24230;&#65288;STS&#65289;&#20219;&#21153;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340; ClusterNS &#22312;&#26080;&#30417;&#30563;&#21477;&#23376;&#34920;&#31034;&#23398;&#20064;&#20013;&#34920;&#29616;&#20248;&#20110;&#22522;&#32447;&#12290;&#25105;&#20204;&#30340;&#20195;&#30721;&#24050;&#19978;&#20256;&#21040; https://github.com/xxxxxx&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive learning has been widely studied in sentence representation learning. However, earlier works mainly focus on the construction of positive examples, while in-batch samples are often simply treated as negative examples. This approach overlooks the importance of selecting appropriate negative examples, potentially leading to a scarcity of hard negatives and the inclusion of false negatives. To address these issues, we propose ClusterNS (Clustering-aware Negative Sampling), a novel method that incorporates cluster information into contrastive learning for unsupervised sentence representation learning. We apply a modified K-means clustering algorithm to supply hard negatives and recognize in-batch false negatives during training, aiming to solve the two issues in one unified framework. Experiments on semantic textual similarity (STS) tasks demonstrate that our proposed ClusterNS compares favorably with baselines in unsupervised sentence representation learning. Our code has been
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20851;&#38190;&#35789;&#24863;&#30693;&#20132;&#21449;&#32534;&#30721;&#22120;&#25490;&#24207;&#25688;&#35201;&#31243;&#24207;&#65292;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#24230;&#37327; GIS&amp;T BoK &#35805;&#39064;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#20197;&#35299;&#20915;&#25163;&#21160;&#23450;&#20041;&#35805;&#39064;&#20851;&#31995;&#24102;&#26469;&#30340;&#19981;&#23436;&#25972;&#35780;&#20272;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#37327;&#35805;&#39064;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545; GIS&amp;T &#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2305.09877</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#21644;&#20851;&#38190;&#35789;&#24863;&#30693;&#20132;&#21449;&#32534;&#30721;&#22120;&#25490;&#24207;&#25688;&#35201;&#31243;&#24207;&#30340;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#35821;&#20041;&#30456;&#20284;&#24230;&#24230;&#37327;&#8212;&#8212;&#20197;UCGIS GIS&amp;T&#30693;&#35782;&#20307;&#31995;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Semantic Similarity Measure of Natural Language Text through Machine Learning and a Keyword-Aware Cross-Encoder-Ranking Summarizer -- A Case Study Using UCGIS GIS&amp;T Body of Knowledge. (arXiv:2305.09877v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09877
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20851;&#38190;&#35789;&#24863;&#30693;&#20132;&#21449;&#32534;&#30721;&#22120;&#25490;&#24207;&#25688;&#35201;&#31243;&#24207;&#65292;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#24230;&#37327; GIS&amp;T BoK &#35805;&#39064;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#65292;&#20197;&#35299;&#20915;&#25163;&#21160;&#23450;&#20041;&#35805;&#39064;&#20851;&#31995;&#24102;&#26469;&#30340;&#19981;&#23436;&#25972;&#35780;&#20272;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#22312;&#20934;&#30830;&#24230;&#37327;&#35805;&#39064;&#20851;&#31995;&#26041;&#38754;&#34920;&#29616;&#33391;&#22909;&#65292;&#23545; GIS&amp;T &#39046;&#22495;&#30340;&#30740;&#31350;&#21644;&#23454;&#36341;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
GIS&amp;T &#30693;&#35782;&#20307;&#31995;&#26159;&#30001;&#22320;&#29702;&#20449;&#24687;&#31185;&#23398;&#19982;&#25216;&#26415;&#30456;&#20851;&#22242;&#20307;&#21457;&#36215;&#30340;&#19968;&#20010;&#31038;&#21306;&#39033;&#30446;&#65292;&#26088;&#22312;&#23450;&#20041;&#12289;&#24320;&#21457;&#21644;&#35760;&#24405;&#22320;&#29702;&#20449;&#24687;&#31185;&#23398;&#19982;&#25216;&#26415;&#30456;&#20851;&#35805;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#37319;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;&#20851;&#38190;&#35789;&#24863;&#30693;&#20132;&#21449;&#32534;&#30721;&#22120;&#25490;&#24207;&#25688;&#35201;&#31243;&#24207;&#65292;&#20174;&#25991;&#26412;&#20869;&#23481;&#20013;&#25552;&#21462;&#35821;&#20041;&#20449;&#24687;&#65292;&#24182;&#24230;&#37327; BoK &#35805;&#39064;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#35782;&#21035; BoK &#35805;&#39064;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24230;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182; NLP &#25216;&#26415;&#12290;&#35813;&#26041;&#27861;&#33021;&#22815;&#33258;&#21160;&#19988;&#20934;&#30830;&#22320;&#24230;&#37327;&#35805;&#39064;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#20174;&#32780;&#20351; GIS&amp;T &#39046;&#22495;&#30340;&#30740;&#31350;&#20154;&#21592;&#21644;&#23454;&#36341;&#32773;&#21463;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
Initiated by the University Consortium of Geographic Information Science (UCGIS), GIS&amp;T Body of Knowledge (BoK) is a community-driven endeavor to define, develop, and document geospatial topics related to geographic information science and technologies (GIS&amp;T). In recent years, GIS&amp;T BoK has undergone rigorous development in terms of its topic re-organization and content updating, resulting in a new digital version of the project. While the BoK topics provide useful materials for researchers and students to learn about GIS, the semantic relationships among the topics, such as semantic similarity, should also be identified so that a better and automated topic navigation can be achieved. Currently, the related topics are either defined manually by editors or authors, which may result in an incomplete assessment of topic relationship. To address this challenge, our research evaluates the effectiveness of multiple natural language processing (NLP) techniques in extracting semantics from te
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Summarize and Score&#65288;SASC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#30740;&#31350;&#32773;&#20204;&#24050;&#32463;&#22312;&#21512;&#25104;&#27169;&#22359;&#21644;BERT&#27169;&#22411;&#20013;&#20351;&#29992;SASC&#65292;&#35753;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27169;&#22359;&#30340;&#36873;&#25321;&#24615;&#65292;&#36825;&#23545;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;</title><link>http://arxiv.org/abs/2305.09863</link><description>&lt;p&gt;
&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
Explaining black box text modules in natural language with language models. (arXiv:2305.09863v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09863
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Summarize and Score&#65288;SASC&#65289;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#30740;&#31350;&#32773;&#20204;&#24050;&#32463;&#22312;&#21512;&#25104;&#27169;&#22359;&#21644;BERT&#27169;&#22411;&#20013;&#20351;&#29992;SASC&#65292;&#35753;&#25105;&#20204;&#21487;&#20197;&#35299;&#37322;&#27169;&#22359;&#30340;&#36873;&#25321;&#24615;&#65292;&#36825;&#23545;&#20110;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#38750;&#24120;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#35777;&#26126;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#19981;&#36879;&#26126;&#24615;&#24050;&#32463;&#24341;&#36215;&#20102;&#23545;&#21487;&#35299;&#37322;&#24615;&#30340;&#38656;&#27714;&#12290;&#26412;&#25991;&#35810;&#38382;&#26159;&#21542;&#21487;&#20197;&#33258;&#21160;&#33719;&#21462;&#40657;&#30418;&#25991;&#26412;&#27169;&#22359;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#12290;&#19968;&#20010;&#8220;&#25991;&#26412;&#27169;&#22359;&#8221;&#26159;&#23558;&#25991;&#26412;&#26144;&#23556;&#21040;&#26631;&#37327;&#36830;&#32493;&#20540;&#30340;&#20219;&#20309;&#20989;&#25968;&#65292;&#20363;&#22914;LLM&#20869;&#30340;&#23376;&#27169;&#22359;&#25110;&#22823;&#33041;&#21306;&#22495;&#30340;&#25311;&#21512;&#27169;&#22411;&#12290;&#8220;&#40657;&#30418;&#8221;&#34920;&#31034;&#25105;&#20204;&#21482;&#33021;&#35775;&#38382;&#27169;&#22359;&#30340;&#36755;&#20837;/&#36755;&#20986;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Summarize and Score&#65288;SASC&#65289;&#26041;&#27861;&#65292;&#23427;&#25509;&#21463;&#25991;&#26412;&#27169;&#22359;&#24182;&#36820;&#22238;&#27169;&#22359;&#36873;&#25321;&#24615;&#30340;&#33258;&#28982;&#35821;&#35328;&#35299;&#37322;&#20197;&#21450;&#35299;&#37322;&#21487;&#38752;&#31243;&#24230;&#30340;&#20998;&#25968;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#19978;&#19979;&#25991;&#20013;&#30740;&#31350;SASC&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#27169;&#22359;&#19978;&#35780;&#20272;SASC&#65292;&#24182;&#21457;&#29616;&#23427;&#32463;&#24120;&#24674;&#22797;&#22522;&#26412;&#30495;&#30456;&#35828;&#26126;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;SASC&#26469;&#35299;&#37322;&#39044;&#35757;&#32451;BERT&#27169;&#22411;&#20013;&#30340;&#27169;&#22359;&#65292;&#20351;&#24471;&#26816;&#26597;BERT&#30340;&#27169;&#22359;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable prediction performance for a growing array of tasks. However, their rapid proliferation and increasing opaqueness have created a growing need for interpretability. Here, we ask whether we can automatically obtain natural language explanations for black box text modules. A "text module" is any function that maps text to a scalar continuous value, such as a submodule within an LLM or a fitted model of a brain region. "Black box" indicates that we only have access to the module's inputs/outputs.  We introduce Summarize and Score (SASC), a method that takes in a text module and returns a natural language explanation of the module's selectivity along with a score for how reliable the explanation is. We study SASC in 3 contexts. First, we evaluate SASC on synthetic modules and find that it often recovers ground truth explanations. Second, we use SASC to explain modules found within a pre-trained BERT model, enabling inspection of the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#20102;epsilon&#37319;&#26679;&#26041;&#24335;&#33021;&#22815;&#20351;&#24471;&#35299;&#30721;&#32467;&#26524;&#26174;&#33879;&#22320;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#24335;&#21644;&#26463;&#25628;&#32034;&#35299;&#30721;&#12290;</title><link>http://arxiv.org/abs/2305.09860</link><description>&lt;p&gt;
Epsilon Sampling Rocks: &#30740;&#31350;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#37319;&#26679;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Epsilon Sampling Rocks: Investigating Sampling Strategies for \\Minimum Bayes Risk Decoding for Machine Translation. (arXiv:2305.09860v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#20102;epsilon&#37319;&#26679;&#26041;&#24335;&#33021;&#22815;&#20351;&#24471;&#35299;&#30721;&#32467;&#26524;&#26174;&#33879;&#22320;&#20248;&#20110;&#20854;&#20182;&#25152;&#26377;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#24335;&#21644;&#26463;&#25628;&#32034;&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#35299;&#30721;&#24050;&#32463;&#26174;&#31034;&#20986;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26367;&#20195;&#26463;&#25628;&#32034;&#35299;&#30721;&#30340;&#26041;&#27861;&#65292;&#23588;&#20854;&#26159;&#19982;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#25928;&#29992;&#20989;&#25968;&#30456;&#32467;&#21512;&#26102;&#12290;&#28982;&#32780;&#65292;MBR&#35299;&#30721;&#30340;&#24615;&#33021;&#20005;&#37325;&#20381;&#36182;&#20110;&#20174;&#27169;&#22411;&#20013;&#37319;&#26679;&#30340;&#26041;&#27861;&#21644;&#25968;&#37327;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;MBR&#35299;&#30721;&#30340;&#19981;&#21516;&#37319;&#26679;&#26041;&#27861;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#37319;&#26679;&#26041;&#27861;&#65292;&#20363;&#22914;&#31062;&#20808;&#37319;&#26679;&#65292;&#26680;&#37319;&#26679;&#21644;top-k&#37319;&#26679;&#12290;&#22522;&#20110;&#25105;&#20204;&#23545;&#23427;&#20204;&#23616;&#38480;&#24615;&#30340;&#35748;&#35782;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#26368;&#36817;&#25552;&#20986;&#30340;epsilon&#37319;&#26679;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20462;&#21098;&#25152;&#26377;&#23567;&#20110;epsilon&#30340;&#26631;&#35760;&#65292;&#20197;&#30830;&#20445;&#26679;&#26412;&#20013;&#30340;&#27599;&#20010;&#26631;&#35760;&#33719;&#24471;&#20844;&#24179;&#30340;&#27010;&#29575;&#36136;&#37327;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#20154;&#31867;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22522;&#20110;epsilon&#37319;&#26679;&#30340;MBR&#35299;&#30721;&#26174;&#33879;&#20248;&#20110;&#19981;&#20165;&#26159;&#26463;&#25628;&#32034;&#35299;&#30721;&#65292;&#32780;&#19988;&#36824;&#20248;&#20110;&#25152;&#26377;&#20854;&#20182;&#24050;&#27979;&#35797;&#30340;&#37319;&#26679;&#26041;&#27861;&#30340;MBR&#35299;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in machine translation (MT) have shown that Minimum Bayes Risk (MBR) decoding can be a powerful alternative to beam search decoding, especially when combined with neural-based utility functions. However, the performance of MBR decoding depends heavily on how and how many candidates are sampled from the model. In this paper, we explore how different sampling approaches for generating candidate lists for MBR decoding affect performance. We evaluate popular sampling approaches, such as ancestral, nucleus, and top-k sampling. Based on our insights into their limitations, we experiment with the recently proposed epsilon-sampling approach, which prunes away all tokens with a probability smaller than epsilon, ensuring that each token in a sample receives a fair probability mass. Through extensive human evaluations, we demonstrate that MBR decoding based on epsilon-sampling significantly outperforms not only beam search decoding, but also MBR decoding with all other tested samp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#20851;&#31995;&#26631;&#27880;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19979;&#39044;&#27979;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09858</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#27169;&#22411;&#26159;&#23569;&#26679;&#26412;&#23398;&#20064;&#32773;&#65306;&#20197; LLMS &#22312;&#30005;&#21830;&#20013;&#30340;&#20851;&#31995;&#26631;&#27880;&#20026;&#20363;&#30340;&#32463;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Completion Models are Few-shot Learners: An Empirical Study of Relation Labeling in E-commerce with LLMs. (arXiv:2305.09858v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23545;&#30693;&#35782;&#22270;&#35889;&#20013;&#20851;&#31995;&#26631;&#27880;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24378;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#22312;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#19979;&#39044;&#27979;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#22312;&#22686;&#24378;&#30005;&#23376;&#21830;&#21153;&#31995;&#32479;&#24615;&#33021;&#26041;&#38754;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#23454;&#20307;&#21450;&#20854;&#20851;&#31995;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#65292;&#20363;&#22914;&#20135;&#21697;&#25110;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#30340;&#20114;&#34917;&#25110;&#26367;&#20195;&#20851;&#31995;&#65292;&#36825;&#20123;&#20449;&#24687;&#21487;&#20197;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21033;&#29992;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#30340;&#21160;&#24577;&#24615;&#21644;&#20154;&#21147;&#25104;&#26412;&#30456;&#20851;&#30340;&#21407;&#22240;&#65292;&#30693;&#35782;&#22270;&#35889;&#20013;&#30340;&#20851;&#31995;&#26631;&#27880;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#31361;&#30772;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#20986;&#20046;&#24847;&#26009;&#30340;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20851;&#20110; LLM &#22312;&#30005;&#23376;&#21830;&#21153;&#30693;&#35782;&#22270;&#35889;&#20013;&#36827;&#34892;&#20851;&#31995;&#26631;&#27880;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#30740;&#31350;&#23427;&#20204;&#22312;&#33258;&#28982;&#35821;&#35328;&#26041;&#38754;&#24378;&#22823;&#30340;&#23398;&#20064;&#33021;&#21147;&#20197;&#21450;&#22312;&#26377;&#38480;&#26631;&#35760;&#25968;&#25454;&#19979;&#39044;&#27979;&#20135;&#21697;&#31867;&#22411;&#20043;&#38388;&#20851;&#31995;&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#21508;&#31181; LLM&#65292;&#21253;&#25324; PaLM &#21644; GPT-3.5&#65292;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#65292;&#35777;&#26126;&#23427;&#20204;&#33021;&#22815;&#36798;&#21040;&#19982;&#20154;&#31867;&#30456;&#24403;&#30340;&#20851;&#31995;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs (KGs) play a crucial role in enhancing e-commerce system performance by providing structured information about entities and their relationships, such as complementary or substitutable relations between products or product types, which can be utilized in recommender systems. However, relation labeling in KGs remains a challenging task due to the dynamic nature of e-commerce domains and the associated cost of human labor. Recently, breakthroughs in Large Language Models (LLMs) have shown surprising results in numerous natural language processing tasks. In this paper, we conduct an empirical study of LLMs for relation labeling in e-commerce KGs, investigating their powerful learning capabilities in natural language and effectiveness in predicting relations between product types with limited labeled data. We evaluate various LLMs, including PaLM and GPT-3.5, on benchmark datasets, demonstrating their ability to achieve competitive performance compared to humans on relation
&lt;/p&gt;</description></item><item><title>CoEdIT&#26159;&#19968;&#31181;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#32534;&#36753;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#39640;&#29992;&#25143;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#25552;&#39640;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.09857</link><description>&lt;p&gt;
CoEdIT&#65306;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
CoEdIT: Text Editing by Task-Specific Instruction Tuning. (arXiv:2305.09857v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09857
&lt;/p&gt;
&lt;p&gt;
CoEdIT&#26159;&#19968;&#31181;&#36890;&#36807;&#20219;&#21153;&#29305;&#23450;&#25351;&#20196;&#35843;&#25972;&#23454;&#29616;&#25991;&#26412;&#32534;&#36753;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#33021;&#22815;&#25552;&#39640;&#29992;&#25143;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#25552;&#39640;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#32534;&#36753;&#25110;&#20462;&#35746;&#26159;&#20154;&#31867;&#20889;&#20316;&#36807;&#31243;&#20013;&#24517;&#19981;&#21487;&#23569;&#30340;&#21151;&#33021;&#12290;&#29702;&#35299;LLMs&#22312;&#36827;&#34892;&#39640;&#36136;&#37327;&#20462;&#35746;&#21644;&#19982;&#20154;&#31867;&#20889;&#20316;&#32773;&#21327;&#20316;&#26041;&#38754;&#30340;&#33021;&#21147;&#26159;&#26500;&#24314;&#26377;&#25928;&#20889;&#20316;&#21161;&#25163;&#30340;&#20851;&#38190;&#27493;&#39588;&#12290;&#22312;LLMs&#21644;&#25351;&#20196;&#35843;&#25972;&#30340;&#20808;&#21069;&#25104;&#21151;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#21033;&#29992;&#32463;&#36807;&#25351;&#20196;&#35843;&#25972;&#30340;LLMs&#36827;&#34892;&#25991;&#26412;&#20462;&#35746;&#65292;&#20197;&#25552;&#39640;&#29992;&#25143;&#29983;&#25104;&#25991;&#26412;&#30340;&#36136;&#37327;&#21644;&#25552;&#39640;&#27969;&#31243;&#30340;&#25928;&#29575;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;CoEdIT&#65292;&#36825;&#26159;&#19968;&#27454;&#29992;&#20110;&#20889;&#20316;&#36741;&#21161;&#30340;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#32534;&#36753;&#27169;&#22411;&#12290;CoEdIT&#20174;&#29992;&#25143;&#37027;&#37324;&#33719;&#21462;&#25351;&#20196;&#65292;&#25351;&#23450;&#25152;&#38656;&#25991;&#26412;&#30340;&#23646;&#24615;&#65292;&#20363;&#22914;&#8220;&#20351;&#21477;&#23376;&#26356;&#31616;&#21333;&#8221;&#25110;&#8220;&#20197;&#26356;&#20013;&#31435;&#30340;&#39118;&#26684;&#20889;&#20316;&#8221;&#65292;&#24182;&#36755;&#20986;&#32534;&#36753;&#21518;&#30340;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#21508;&#31181;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#65288;1&#65289;&#22312;&#21508;&#31181;&#25991;&#26412;&#32534;&#36753;&#22522;&#20934;&#27979;&#35797;&#19978;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#65288;2&#65289;&#19982;&#20844;&#24320;&#21487;&#29992;&#30340;&#27169;&#22411;&#30456;&#27604;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text editing or revision is an essential function of the human writing process. Understanding the capabilities of LLMs for making high-quality revisions and collaborating with human writers is a critical step toward building effective writing assistants. With the prior success of LLMs and instruction tuning, we leverage instruction-tuned LLMs for text revision to improve the quality of user-generated text and improve the efficiency of the process. We introduce CoEdIT, a state-of-the-art text editing model for writing assistance. CoEdIT takes instructions from the user specifying the attributes of the desired text, such as "Make the sentence simpler" or "Write it in a more neutral style," and outputs the edited text. We present a large language model fine-tuned on a diverse collection of task-specific instructions for text editing (a total of 82K instructions). Our model (1) achieves state-of-the-art performance on various text editing benchmarks, (2) is competitive with publicly availa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#20855;&#26377;&#19981;&#21487;&#38752;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#23481;&#38169;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#30456;&#23545;&#36739;&#31616;&#21333;&#30340;FL&#31639;&#27861;&#22312;&#27492;&#24773;&#22659;&#19979;&#20063;&#33021;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2305.09856</link><description>&lt;p&gt;
&#31616;&#21333;&#26131;&#29992;&#65306;&#20855;&#26377;&#19981;&#21487;&#38752;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;&#23481;&#38169;&#24615;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Keep It Simple: Fault Tolerance Evaluation of Federated Learning with Unreliable Clients. (arXiv:2305.09856v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#20855;&#26377;&#19981;&#21487;&#38752;&#23458;&#25143;&#31471;&#30340;&#32852;&#37030;&#23398;&#20064;&#30340;&#23481;&#38169;&#24615;&#65292;&#30740;&#31350;&#34920;&#26126;&#30456;&#23545;&#36739;&#31616;&#21333;&#30340;FL&#31639;&#27861;&#22312;&#27492;&#24773;&#22659;&#19979;&#20063;&#33021;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#65292;&#32852;&#37030;&#23398;&#20064;&#65288;FL&#65289;&#21487;&#20197;&#22312;&#22810;&#20010;&#35774;&#22791;&#19978;&#36827;&#34892;&#20998;&#25955;&#27169;&#22411;&#35757;&#32451;&#65292;&#32780;&#19981;&#27844;&#38706;&#26412;&#22320;&#35757;&#32451;&#25968;&#25454;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#30740;&#31350;&#25552;&#20986;&#20102;&#25552;&#39640;FL&#23481;&#38169;&#24615;&#30340;&#26041;&#27861;&#65292;&#20294;&#29616;&#23454;&#24212;&#29992;&#20013;&#19981;&#21487;&#38752;&#35774;&#22791;&#65288;&#20363;&#22914;&#25481;&#32447;&#12289;&#38169;&#35823;&#37197;&#32622;&#12289;&#24046;&#25968;&#25454;&#36136;&#37327;&#65289;&#30340;&#30495;&#23454;&#24433;&#21709;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#35843;&#26597;&#12290;&#25105;&#20204;&#31934;&#24515;&#36873;&#25321;&#20102;&#20004;&#20010;&#20855;&#26377;&#26377;&#38480;&#23458;&#25143;&#31471;&#30340;&#20195;&#34920;&#24615;&#23454;&#38469;&#20998;&#31867;&#38382;&#39064;&#65292;&#20197;&#26356;&#22909;&#22320;&#20998;&#26512;FL&#23481;&#38169;&#24615;&#12290;&#19982;&#30452;&#35273;&#30456;&#21453;&#65292;&#31616;&#21333;&#30340;FL&#31639;&#27861;&#22312;&#23384;&#22312;&#19981;&#21487;&#38752;&#23458;&#25143;&#31471;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#20986;&#22855;&#22320;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL), as an emerging artificial intelligence (AI) approach, enables decentralized model training across multiple devices without exposing their local training data. FL has been increasingly gaining popularity in both academia and industry. While research works have been proposed to improve the fault tolerance of FL, the real impact of unreliable devices (e.g., dropping out, misconfiguration, poor data quality) in real-world applications is not fully investigated. We carefully chose two representative, real-world classification problems with a limited numbers of clients to better analyze FL fault tolerance. Contrary to the intuition, simple FL algorithms can perform surprisingly well in the presence of unreliable clients.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;MCTS/THTS&#31639;&#27861;GreedyUCT-Normal&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#37319;&#29992;&#22870;&#21169;&#21464;&#21270;&#30340;&#23610;&#24230;&#22788;&#29702;&#19981;&#21516;&#23610;&#24230;&#30340;&#20998;&#24067;&#65292;&#20197;&#22312;&#32463;&#20856;&#35745;&#21010;&#20013;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2305.09840</link><description>&lt;p&gt;
&#32463;&#20856;&#35268;&#21010;&#20013;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#33258;&#36866;&#24212;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Scale-Adaptive Balancing of Exploration and Exploitation in Classical Planning. (arXiv:2305.09840v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;MCTS/THTS&#31639;&#27861;GreedyUCT-Normal&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#37319;&#29992;&#22870;&#21169;&#21464;&#21270;&#30340;&#23610;&#24230;&#22788;&#29702;&#19981;&#21516;&#23610;&#24230;&#30340;&#20998;&#24067;&#65292;&#20197;&#22312;&#32463;&#20856;&#35745;&#21010;&#20013;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28216;&#25103;&#26641;&#25628;&#32034;&#21644;&#33258;&#21160;&#21270;&#35268;&#21010;&#20013;&#65292;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#20010;&#38382;&#39064;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#25991;&#29486;&#20013;&#24050;&#32463;&#34987;&#24191;&#27867;&#20998;&#26512;&#65292;&#20294;&#35268;&#21010;&#31038;&#21306;&#22312;&#35797;&#22270;&#24212;&#29992;&#36825;&#20123;&#32467;&#26524;&#26102;&#21462;&#24471;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MAB&#25991;&#29486;&#26356;&#35814;&#32454;&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#21161;&#20110;&#25913;&#36827;&#22522;&#20110;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;/&#22522;&#20110;&#35797;&#39564;&#30340;&#21551;&#21457;&#24335;&#26641;&#25628;&#32034;&#65288;THTS&#65289;&#30340;&#29616;&#26377;&#35268;&#21010;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;THTS&#22312;&#19968;&#31181;&#20020;&#26102;&#26041;&#27861;&#20013;&#20351;&#29992;UCB1 MAB&#31639;&#27861;&#65292;&#22240;&#20026;&#22312;&#21551;&#21457;&#24335;&#25628;&#32034;&#20013;UCB1&#29702;&#35770;&#19978;&#38656;&#35201;&#26377;&#30028;&#25903;&#25345;&#22870;&#21169;&#20998;&#24067;&#30340;&#35201;&#27714;&#22312;&#32463;&#20856;&#35268;&#21010;&#20013;&#19981;&#34987;&#28385;&#36275;&#12290;&#26680;&#24515;&#38382;&#39064;&#22312;&#20110;UCB1&#32570;&#20047;&#23545;&#19981;&#21516;&#22870;&#21169;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GreedyUCT-Normal&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;UCB1-Normal&#36172;&#21338;&#26426;&#30340;MCTS/THTS&#31639;&#27861;&#65292;&#29992;&#20110;&#25935;&#25463;&#32463;&#20856;&#35745;&#21010;&#65292;&#23427;&#36890;&#36807;&#37319;&#29992;&#22870;&#21169;&#21464;&#21270;&#30340;&#23610;&#24230;&#22788;&#29702;&#19981;&#21516;&#23610;&#24230;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Balancing exploration and exploitation has been an important problem in both game tree search and automated planning. However, while the problem has been extensively analyzed within the Multi-Armed Bandit (MAB) literature, the planning community has had limited success when attempting to apply those results. We show that a more detailed theoretical understanding of MAB literature helps improve existing planning algorithms that are based on Monte Carlo Tree Search (MCTS) / Trial Based Heuristic Tree Search (THTS). In particular, THTS uses UCB1 MAB algorithms in an ad hoc manner, as UCB1's theoretical requirement of fixed bounded support reward distributions is not satisfied within heuristic search for classical planning. The core issue lies in UCB1's lack of adaptations to the different scales of the rewards. We propose GreedyUCT-Normal, a MCTS/THTS algorithm with UCB1-Normal bandit for agile classical planning, which handles distributions with different scales by taking the reward vari
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#32780;&#28789;&#27963;&#30340;&#21512;&#20316;&#26234;&#33021;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#24322;&#27493;&#35745;&#31639;&#32593;&#32476;&#19981;&#21516;&#37096;&#20998;&#12289;&#21560;&#25910;&#21453;&#21521;&#20256;&#25773;&#19981;&#33021;&#20351;&#29992;&#30340;&#19981;&#21487;&#24494;&#32452;&#20214;&#12289;&#25506;&#32034;&#21644;/&#25110;&#26102;&#24577;&#25277;&#35937;&#30340;&#20998;&#23618;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#20998;&#24067;&#24335;&#21644;&#24182;&#34892;&#23398;&#20064;&#12290;&#22312;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#27169;&#25311;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;</title><link>http://arxiv.org/abs/2305.09838</link><description>&lt;p&gt;
&#21512;&#20316;&#26234;&#33021;&#32593;&#32476;&#65306;&#27867;&#21270;&#21644;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Coagent Networks: Generalized and Scaled. (arXiv:2305.09838v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09838
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#22823;&#32780;&#28789;&#27963;&#30340;&#21512;&#20316;&#26234;&#33021;&#32593;&#32476;&#26694;&#26550;&#65292;&#21487;&#20197;&#24322;&#27493;&#35745;&#31639;&#32593;&#32476;&#19981;&#21516;&#37096;&#20998;&#12289;&#21560;&#25910;&#21453;&#21521;&#20256;&#25773;&#19981;&#33021;&#20351;&#29992;&#30340;&#19981;&#21487;&#24494;&#32452;&#20214;&#12289;&#25506;&#32034;&#21644;/&#25110;&#26102;&#24577;&#25277;&#35937;&#30340;&#20998;&#23618;&#32593;&#32476;&#65292;&#24182;&#20351;&#29992;&#39640;&#25928;&#31639;&#27861;&#36827;&#34892;&#20998;&#24067;&#24335;&#21644;&#24182;&#34892;&#23398;&#20064;&#12290;&#22312;&#22522;&#20934;&#38382;&#39064;&#19978;&#30340;&#27169;&#25311;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#26377;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21512;&#20316;&#26234;&#33021;&#32593;&#32476;&#20026;&#20219;&#24847;&#38543;&#26426;&#31070;&#32463;&#32593;&#32476;&#30340;&#21407;&#21017;&#24615;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#24378;&#22823;&#32780;&#28789;&#27963;&#30340;&#26694;&#26550;&#12290;&#23427;&#19981;&#20165;&#33021;&#22815;&#24322;&#27493;&#35745;&#31639;&#32593;&#32476;&#30340;&#19981;&#21516;&#37096;&#20998;&#65292;&#36824;&#33021;&#22815;&#21560;&#25910;&#19968;&#20123;&#21453;&#21521;&#20256;&#25773;&#19981;&#33021;&#20351;&#29992;&#30340;&#19981;&#21487;&#24494;&#32452;&#20214;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#21487;&#20197;&#22312;&#21160;&#20316;&#31354;&#38388;&#32423;&#21035;&#20197;&#19978;&#36827;&#34892;&#25506;&#32034;&#65292;&#21363;&#21487;&#20197;&#35774;&#35745;&#20026;&#25506;&#32034;&#21644;/&#25110;&#26102;&#24577;&#25277;&#35937;&#30340;&#20998;&#23618;&#32593;&#32476;&#12290;&#26412;&#25991;&#23558;&#21327;&#20316;&#29702;&#35770;&#21644;&#23398;&#20064;&#35268;&#21017;&#25512;&#24191;&#21040;&#20219;&#24847;&#32593;&#32476;&#25299;&#25169;&#65292;&#24182;&#36890;&#36807;&#20351;&#29992;&#20998;&#24067;&#24335;&#21644;&#24182;&#34892;&#23398;&#20064;&#30340;&#39640;&#25928;&#31639;&#27861;&#26469;&#25193;&#23637;&#23427;&#12290;&#25105;&#20204;&#22312;&#22522;&#20934;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#27169;&#25311;&#65292;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#22312;&#24615;&#33021;&#19978;&#30340;&#26174;&#33879;&#25552;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
Coagent networks for reinforcement learning (RL) [Thomas and Barto, 2011] provide a powerful and flexible framework for deriving principled learning rules for arbitrary stochastic neural networks. The coagent framework offers an alternative to backpropagation-based deep learning (BDL) that overcomes some of backpropagation's main limitations. For example, coagent networks can compute different parts of the network \emph{asynchronously} (at different rates or at different times), can incorporate non-differentiable components that cannot be used with backpropagation, and can explore at levels higher than their action spaces (that is, they can be designed as hierarchical networks for exploration and/or temporal abstraction). However, the coagent framework is not just an alternative to BDL; the two approaches can be blended: BDL can be combined with coagent learning rules to create architectures with the advantages of both approaches. This work generalizes the coagent theory and learning r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#20013;&#30340;&#20219;&#21153;&#37096;&#32626;&#21644;&#36793;&#32536;&#36164;&#28304;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09832</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36793;&#32536;&#36164;&#28304;&#20219;&#21153;&#37096;&#32626;&#21644;&#25193;&#23637;&#26041;&#27861;&#29992;&#20110;&#36710;&#36733;&#32593;&#32476;&#26381;&#21153;&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
A Deep RL Approach on Task Placement and Scaling of Edge Resources for Cellular Vehicle-to-Network Service Provisioning. (arXiv:2305.09832v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#25955;&#24335;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#36710;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#20013;&#30340;&#20219;&#21153;&#37096;&#32626;&#21644;&#36793;&#32536;&#36164;&#28304;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#36710;&#32852;&#32593;&#8221;&#27491;&#22788;&#20110;&#25105;&#20204;&#31038;&#20250;&#25968;&#23383;&#21270;&#36716;&#22411;&#30340;&#21069;&#27839;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#25955;&#24335;&#26041;&#27861;&#29992;&#20110;&#25552;&#20379;&#36710;&#36742;&#36890;&#32852;&#32593;&#65288;C-V2N&#65289;&#26381;&#21153;&#65292;&#35299;&#20915;&#26381;&#21153;&#20219;&#21153;&#37096;&#32626;&#21644;&#36793;&#32536;&#36164;&#28304;&#30340;&#25193;&#23637;&#38382;&#39064;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#32852;&#21512;&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#20010;&#38382;&#39064;&#30340;&#32852;&#25509;&#26041;&#24335;&#65292;&#37319;&#29992;&#20102;&#22522;&#20110;&#36138;&#24515;&#31639;&#27861;&#30340;&#20851;&#20110;&#20219;&#21153;&#37096;&#32626;&#30340;&#26041;&#27861;&#21644;&#22522;&#20110; Deep Deterministic Policy Gradient (DDPG) &#30340;&#25193;&#23637;&#26041;&#27861;&#12290;&#26412;&#25991;&#36824;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#25193;&#23637;&#20195;&#29702;&#19982;&#22810;&#20010;&#29366;&#24577;&#19979;&#26368;&#20808;&#36827;&#30340;&#25193;&#23637;&#26041;&#27861;&#30340;&#24615;&#33021;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cellular-Vehicle-to-Everything (C-V2X) is currently at the forefront of the digital transformation of our society. By enabling vehicles to communicate with each other and with the traffic environment using cellular networks, we redefine transportation, improving road safety and transportation services, increasing efficiency of traffic flows, and reducing environmental impact. This paper proposes a decentralized approach for provisioning Cellular Vehicular-to-Network (C-V2N) services, addressing the coupled problems of service task placement and scaling of edge resources. We formalize the joint problem and prove its complexity. We propose an approach to tackle it, linking the two problems, employing decentralized decision-making using (i) a greedy approach for task placement and (ii) a Deep Deterministic Policy Gradient (DDPG) based approach for scaling. We benchmark the performance of our approach, focusing on the scaling agent, against several State-of-the-Art (SoA) scaling approaches
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#21517;&#20026;&#25311;&#24577;&#21021;&#22987;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20165;&#35843;&#25972;&#33258;&#27880;&#24847;&#21147;&#23618;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#65292;&#21363;&#21487;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#22823;&#22823;&#25552;&#39640;Transformer&#30340;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.09828</link><description>&lt;p&gt;
&#33258;&#27880;&#24847;&#21147;&#23618;&#30340;&#25311;&#24577;&#21021;&#22987;&#21270;
&lt;/p&gt;
&lt;p&gt;
Mimetic Initialization of Self-Attention Layers. (arXiv:2305.09828v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#19968;&#31181;&#21517;&#20026;&#25311;&#24577;&#21021;&#22987;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20165;&#20165;&#35843;&#25972;&#33258;&#27880;&#24847;&#21147;&#23618;&#30340;&#26435;&#37325;&#21021;&#22987;&#21270;&#65292;&#21363;&#21487;&#22312;&#35270;&#35273;&#20219;&#21153;&#20013;&#22823;&#22823;&#25552;&#39640;Transformer&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23567;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;Transformer&#21313;&#20998;&#22256;&#38590;&#12290;&#36890;&#24120;&#38656;&#35201;&#20197;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#27169;&#22411;&#20316;&#20026;&#36215;&#28857;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#36825;&#20123;&#39044;&#35757;&#32451;Transformer&#30340;&#26435;&#37325;&#65288;&#23588;&#20854;&#26159;&#29992;&#20110;&#35270;&#35273;&#20219;&#21153;&#65289;&#65292;&#35797;&#22270;&#25214;&#21040;&#36896;&#25104;&#36825;&#31181;&#24046;&#24322;&#30340;&#21407;&#22240;&#12290;&#24778;&#35766;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#20165;&#20165;&#36890;&#36807;&#21021;&#22987;&#21270;&#33258;&#27880;&#24847;&#21147;&#23618;&#30340;&#26435;&#37325;&#65292;&#20351;&#20854;&#8220;&#30475;&#36215;&#26469;&#8221;&#26356;&#20687;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#23601;&#33021;&#22815;&#26356;&#24555;&#19988;&#26356;&#39640;&#31934;&#24230;&#22320;&#35757;&#32451;&#26222;&#36890;Transformer&#65292;&#23588;&#20854;&#26159;&#22312;&#20687;CIFAR-10&#21644;ImageNet&#20998;&#31867;&#36825;&#26679;&#30340;&#35270;&#35273;&#20219;&#21153;&#19978;&#65292;&#25105;&#20204;&#30340;&#31934;&#24230;&#25552;&#39640;&#36229;&#36807;5&#65285;&#21644;4&#65285;&#12290;&#25105;&#20204;&#30340;&#21021;&#22987;&#21270;&#26041;&#26696;&#26159;&#38381;&#24335;&#30340;&#12289;&#26080;&#38656;&#23398;&#20064;&#30340;&#12289;&#38750;&#24120;&#31616;&#21333;&#65306;&#25105;&#20204;&#23558;&#26597;&#35810;&#21644;&#38190;&#26435;&#37325;&#30340;&#20056;&#31215;&#35774;&#32622;&#20026;&#36817;&#20284;&#20110;&#26631;&#35782;&#65292;&#23558;&#20540;&#21644;&#25237;&#24433;&#26435;&#37325;&#30340;&#20056;&#31215;&#36817;&#20284;&#20110;&#36127;&#26631;&#35782;&#12290;&#30001;&#20110;&#36825;&#31867;&#20284;&#20110;&#25105;&#20204;&#22312;&#39044;&#35757;&#32451;Transformer&#20013;&#30475;&#21040;&#30340;&#27169;&#24335;&#65292;&#25152;&#20197;&#25105;&#20204;&#31216;&#20026;&#8220;&#25311;&#24577;&#21021;&#22987;&#21270;&#8221;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is notoriously difficult to train Transformers on small datasets; typically, large pre-trained models are instead used as the starting point. We explore the weights of such pre-trained Transformers (particularly for vision) to attempt to find reasons for this discrepancy. Surprisingly, we find that simply initializing the weights of self-attention layers so that they "look" more like their pre-trained counterparts allows us to train vanilla Transformers faster and to higher final accuracies, particularly on vision tasks such as CIFAR-10 and ImageNet classification, where we see gains in accuracy of over 5% and 4%, respectively. Our initialization scheme is closed form, learning-free, and very simple: we set the product of the query and key weights to be approximately the identity, and the product of the value and projection weights to approximately the negative identity. As this mimics the patterns we saw in pre-trained Transformers, we call the technique "mimetic initialization".
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#35282;&#33394;&#22270;&#20687;&#29305;&#24449;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#25552;&#20379;&#35282;&#33394;&#30340;&#22270;&#29255;&#65292;&#29983;&#25104;&#21305;&#37197;&#26399;&#26395;&#30340;&#22270;&#20687;&#24182;&#35843;&#25972;&#21508;&#31181;&#32454;&#33410;&#65292;&#26080;&#38656;&#20026;&#27599;&#20010;&#20010;&#20307;/&#21160;&#30011;&#35282;&#33394;&#22270;&#20687;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.09817</link><description>&lt;p&gt;
&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#20154;&#20687;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Method for Training-free Person Image Picture Generation. (arXiv:2305.09817v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#35282;&#33394;&#22270;&#20687;&#29305;&#24449;&#32534;&#30721;&#22120;&#27169;&#22411;&#65292;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#25552;&#20379;&#35282;&#33394;&#30340;&#22270;&#29255;&#65292;&#29983;&#25104;&#21305;&#37197;&#26399;&#26395;&#30340;&#22270;&#20687;&#24182;&#35843;&#25972;&#21508;&#31181;&#32454;&#33410;&#65292;&#26080;&#38656;&#20026;&#27599;&#20010;&#20010;&#20307;/&#21160;&#30011;&#35282;&#33394;&#22270;&#20687;&#21333;&#29420;&#35757;&#32451;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25193;&#25955;&#27169;&#22411;&#24050;&#32463;&#22312;&#29983;&#25104;&#22270;&#20687;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#22270;&#29255;&#37117;&#26159;&#21333;&#35843;&#30340;&#65292;&#22823;&#22810;&#25968;&#37117;&#26159;&#35757;&#32451;&#38598;&#20013;&#20154;&#29289;&#22270;&#29255;&#30340;&#20998;&#24067;&#32467;&#26524;&#65292;&#38590;&#20197;&#20026;&#22266;&#23450;&#25968;&#37327;&#30340;&#20010;&#20307;&#29983;&#25104;&#22810;&#24352;&#22270;&#29255;&#12290;&#22914;&#26524;&#35201;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36890;&#24120;&#24517;&#39035;&#36890;&#36807;&#24494;&#35843;&#27169;&#22411;&#12290;&#36825;&#24847;&#21619;&#30528;&#65292;&#22914;&#26524;&#35201;&#32472;&#21046;&#27599;&#20010;&#20010;&#20307;/&#21160;&#30011;&#35282;&#33394;&#22270;&#20687;&#65292;&#24517;&#39035;&#23545;&#20854;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#31181;&#35757;&#32451;&#30340;&#30828;&#20214;&#21644;&#25104;&#26412;&#24120;&#24120;&#36229;&#20986;&#20102;&#26222;&#36890;&#29992;&#25143;&#30340;&#33021;&#21147;&#65292;&#32780;&#26222;&#36890;&#29992;&#25143;&#23454;&#38469;&#19978;&#21344;&#20102;&#20154;&#25968;&#26368;&#22810;&#30340;&#19968;&#37096;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#35282;&#33394;&#22270;&#20687;&#29305;&#24449;&#32534;&#30721;&#22120;&#27169;&#22411;&#20351;&#24471;&#29992;&#25143;&#21487;&#20197;&#36890;&#36807;&#31616;&#21333;&#25552;&#20379;&#35282;&#33394;&#30340;&#22270;&#29255;&#65292;&#20351;&#29983;&#25104;&#30340;&#22270;&#20687;&#20013;&#30340;&#35282;&#33394;&#19982;&#26399;&#26395;&#21305;&#37197;&#12290;&#27492;&#22806;&#65292;&#22312;&#36807;&#31243;&#20013;&#21487;&#20197;&#20351;&#29992;&#25552;&#31034;&#35843;&#25972;&#21508;&#31181;&#32454;&#33410;&#12290;&#19982;&#20256;&#32479;&#30340;&#22270;&#20687;&#21040;&#22270;&#20687;&#27169;&#22411;&#19981;&#21516;&#65292;&#26412;&#25991;&#25552;&#20986;&#30340;&#35282;&#33394;&#22270;&#20687;&#29305;&#24449;&#32534;&#30721;&#22120;&#27169;&#22411;&#19981;&#20381;&#36182;&#20110;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#35757;&#32451;&#65292;&#22240;&#27492;&#26159;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#20154;&#20687;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The current state-of-the-art Diffusion model has demonstrated excellent results in generating images. However, the images are monotonous and are mostly the result of the distribution of images of people in the training set, making it challenging to generate multiple images for a fixed number of individuals. This problem can often only be solved by fine-tuning the training of the model. This means that each individual/animated character image must be trained if it is to be drawn, and the hardware and cost of this training is often beyond the reach of the average user, who accounts for the largest number of people. To solve this problem, the Character Image Feature Encoder model proposed in this paper enables the user to use the process by simply providing a picture of the character to make the image of the character in the generated image match the expectation. In addition, various details can be adjusted during the process using prompts. Unlike traditional Image-to-Image models, the Ch
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#20316;&#20026;&#33258;&#38381;&#30151;&#24739;&#32773;&#36741;&#21161;&#25216;&#26415;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807; Reddit &#35780;&#35770;&#30340;&#35843;&#26597;&#24471;&#20986;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2305.09815</link><description>&lt;p&gt;
&#25506;&#32034;&#22522;&#20110;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#36741;&#21161;&#25216;&#26415;&#22312;&#33258;&#38381;&#30151;&#24739;&#32773;&#20013;&#30340;&#24212;&#29992;&#21069;&#26223;
&lt;/p&gt;
&lt;p&gt;
Exploring outlooks towards generative AI-based assistive technologies for people with Autism. (arXiv:2305.09815v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09815
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#23558;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#20316;&#20026;&#33258;&#38381;&#30151;&#24739;&#32773;&#36741;&#21161;&#25216;&#26415;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807; Reddit &#35780;&#35770;&#30340;&#35843;&#26597;&#24471;&#20986;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#30340;&#20854;&#20013;&#19968;&#20010;&#24212;&#29992;&#8212;&#8212;&#28145;&#24230;&#20266;&#36896;&#65292;&#26159;&#19968;&#31181;&#21512;&#25104;&#35270;&#39057;&#12290;&#20294;&#26159;&#65292;&#28145;&#24230;&#20266;&#36896;&#30340;&#36127;&#38754;&#24212;&#29992;&#8212;&#8212;&#34394;&#20551;&#26032;&#38395;&#21644;&#33394;&#24773;&#20869;&#23481;&#65292;&#21364;&#25104;&#20026;&#20102;&#25968;&#23383;&#29983;&#24577;&#31995;&#32479;&#20013;&#26368;&#27969;&#34892;&#30340;&#20004;&#31181;&#24418;&#24335;&#12290;&#20154;&#20204;&#24050;&#32463;&#24320;&#22987;&#24605;&#32771;&#22312;&#30005;&#24433;&#21046;&#20316;&#12289;&#25945;&#23398;&#31561;&#39046;&#22495;&#20013;&#20351;&#29992;&#28145;&#24230;&#20266;&#36896;&#30340;&#20248;&#21183;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#27531;&#30142;&#20154;&#22763;&#20013;&#30340;&#28145;&#24230;&#20266;&#36896;&#25216;&#26415;&#28508;&#21147;&#30340;&#30740;&#31350;&#21364;&#38750;&#24120;&#23569;&#25110;&#26681;&#26412;&#27809;&#26377;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#20266;&#36896;&#20316;&#20026;&#19968;&#39033;&#36741;&#21161;&#25216;&#26415;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#26377;&#20851; Nvdia &#26032;&#30340;&#35270;&#39057;&#20250;&#35758;&#21151;&#33021;&#30340; Reddit &#20132;&#27969;&#65292;&#35813;&#21151;&#33021;&#20801;&#35768;&#21442;&#19982;&#32773;&#22312;&#22312;&#32447;&#20250;&#35758;&#26399;&#38388;&#20445;&#25345;&#30446;&#20809;&#25509;&#35302;&#12290;&#36890;&#36807;&#25163;&#21160;&#32593;&#39029;&#25235;&#21462;&#21644;&#23450;&#24615;&#32534;&#30721;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102; 162 &#26465;&#30456;&#20851;&#35780;&#35770;&#65292;&#35752;&#35770;&#20102;&#36825;&#39033;&#25216;&#26415;&#23545;&#20110;&#33258;&#38381;&#30151;&#20154;&#22763;&#30340;&#30456;&#20851;&#24615;&#21644;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The last few years have significantly increased global interest in generative artificial intelligence. Deepfakes, which are synthetically created videos, emerged as an application of generative artificial intelligence. Fake news and pornographic content have been the two most prevalent negative use cases of deepfakes in the digital ecosystem. Deepfakes have some advantageous applications that experts in the subject have thought of in the areas of filmmaking, teaching, etc. Research on the potential of deepfakes among people with disabilities is, however, scarce or nonexistent. This workshop paper explores the potential of deepfakes as an assistive technology. We examined Reddit conversations regarding Nvdia's new videoconferencing feature which allows participants to maintain eye contact during online meetings. Through manual web scraping and qualitative coding, we found 162 relevant comments discussing the relevance and appropriateness of the technology for people with Autism. The the
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26234;&#33021;&#23478;&#23621;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#29992;&#25143;&#21629;&#20196;&#30340;&#21019;&#24847;&#30446;&#26631;&#23548;&#21521;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21019;&#36896;&#24615;&#22320;&#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2305.09802</link><description>&lt;p&gt;
Sasha: &#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#26234;&#33021;&#23478;&#23621;&#20013;&#30340;&#21019;&#24847;&#30446;&#26631;&#23548;&#21521;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Sasha: creative goal-oriented reasoning in smart homes with large language models. (arXiv:2305.09802v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#26234;&#33021;&#23478;&#23621;&#20013;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#29992;&#25143;&#21629;&#20196;&#30340;&#21019;&#24847;&#30446;&#26631;&#23548;&#21521;&#25512;&#29702;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#21019;&#36896;&#24615;&#22320;&#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#23478;&#23621;&#30340;&#20351;&#29992;&#32773;&#19982;&#35774;&#22791;&#30340;&#20132;&#20114;&#37117;&#26377;&#26126;&#30830;&#25110;&#38544;&#21547;&#30340;&#30446;&#26631;&#12290;&#29616;&#26377;&#30340;&#23478;&#24237;&#21161;&#25163;&#33021;&#22815;&#36731;&#26494;&#22320;&#23454;&#29616;&#26126;&#30830;&#30340;&#30446;&#26631;&#65292;&#20363;&#22914;"&#25171;&#24320;&#28783;"&#12290;&#28982;&#32780;&#65292;&#22312;&#26356;&#33258;&#28982;&#30340;&#20132;&#27969;&#20013;&#65292;&#20154;&#20204;&#24448;&#24448;&#20250;&#25551;&#36848;&#38544;&#21547;&#30340;&#30446;&#26631;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21487;&#20197;&#35753;&#21035;&#20154;"&#35753;&#25151;&#38388;&#21464;&#24471;&#33298;&#36866;"&#32780;&#19981;&#26159;&#25551;&#36848;&#20855;&#20307;&#30340;&#27493;&#39588;&#12290;&#24403;&#21069;&#30340;&#31995;&#32479;&#24456;&#38590;&#35299;&#20915;&#36825;&#31181;&#27495;&#20041;&#65292;&#22240;&#20026;&#38656;&#35201;&#23558;&#27169;&#31946;&#30340;&#24847;&#22270;&#19982;&#20855;&#20307;&#30340;&#35774;&#22791;&#32852;&#31995;&#36215;&#26469;&#12290;&#25105;&#20204;&#20174;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#20123;&#27169;&#22411;&#32463;&#36807;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#35757;&#32451;&#65292;&#24182;&#36890;&#36807;&#28789;&#27963;&#30340;&#26041;&#24335;&#36866;&#24212;&#19979;&#28216;&#20219;&#21153;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#20351;&#29992;LLMs&#25511;&#21046;&#35774;&#22791;&#24182;&#21019;&#24314;&#33258;&#21160;&#21270;&#31243;&#24207;&#20197;&#28385;&#36275;&#29992;&#25143;&#21629;&#20196;&#30340;&#38544;&#21547;&#30446;&#26631;&#12290;&#22312;&#20197;&#29992;&#25143;&#20026;&#20013;&#24515;&#30340;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;LLMs&#21487;&#20197;&#21019;&#36896;&#24615;&#22320;&#25512;&#29702;&#65292;&#20197;&#23454;&#29616;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#38477;&#20302;&#20854;&#26377;&#29992;&#24615;&#30340;&#24046;&#36317;&#12290;&#25105;&#20204;&#20351;&#29992;Sasha&#35299;&#20915;&#20102;&#36825;&#20123;&#24046;&#36317;&#65306;&#36825;&#26159;&#19968;&#20010;&#22312;&#26234;&#33021;&#23478;&#23621;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;&#28789;&#27963;&#35299;&#37322;&#29992;&#25143;&#21629;&#20196;&#21644;&#35774;&#22791;&#25511;&#21046;&#30340;&#21019;&#24847;&#30446;&#26631;&#23548;&#21521;&#25512;&#29702;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Every smart home user interaction has an explicit or implicit goal. Existing home assistants easily achieve explicit goals, e.g., "turn on the light". In more natural communication, however, humans tend to describe implicit goals. We can, for example, ask someone to "make it cozy" rather than describe the specific steps involved. Current systems struggle with this ambiguity since it requires them to relate vague intent to specific devices. We approach this problem of flexibly achieving user goals from the perspective of general-purpose large language models (LLMs) trained on gigantic corpora and adapted to downstream tasks with remarkable flexibility. We explore the use of LLMs for controlling devices and creating automation routines to meet the implicit goals of user commands. In a user-focused study, we find that LLMs can reason creatively to achieve challenging goals, while also revealing gaps that diminish their usefulness. We address these gaps with Sasha: a system for creative, g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#23631;&#38556;&#20989;&#25968;&#21450;LBAC&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#36798;&#24615;&#26465;&#20214;&#19979;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;&#22312;&#23454;&#38469;2D&#22235;&#26059;&#32764;&#23548;&#33322;&#20219;&#21153;&#20013;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#26080;&#20851;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09793</link><description>&lt;p&gt;
&#24212;&#29992;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#23631;&#38556;&#20989;&#25968;&#30340;&#24378;&#21270;&#23398;&#20064;&#23433;&#20840;&#26426;&#22120;&#20154;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Safe Robot Control using Control Lyapunov Barrier Functions. (arXiv:2305.09793v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09793
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#23631;&#38556;&#20989;&#25968;&#21450;LBAC&#31639;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#25968;&#25454;&#30340;&#23433;&#20840;&#24615;&#21644;&#21487;&#36798;&#24615;&#26465;&#20214;&#19979;&#26426;&#22120;&#20154;&#25511;&#21046;&#12290;&#22312;&#23454;&#38469;2D&#22235;&#26059;&#32764;&#23548;&#33322;&#20219;&#21153;&#20013;&#39564;&#35777;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20248;&#20110;&#20854;&#20182;&#27169;&#22411;&#26080;&#20851;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#38754;&#23545;&#22797;&#26434;&#30340;&#26426;&#22120;&#20154;&#25511;&#21046;&#20219;&#21153;&#26102;&#65292;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#23637;&#29616;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#24378;&#22823;&#30340;&#23433;&#20840;&#20445;&#38556;&#65292;&#20854;&#22312;&#29289;&#29702;&#26426;&#22120;&#20154;&#19978;&#30340;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#26412;&#25991;&#25506;&#35752;&#20102;&#25511;&#21046;&#26446;&#20122;&#26222;&#35834;&#22827;&#23631;&#38556;&#20989;&#25968;&#65288;CLBF&#65289;&#65292;&#20165;&#22522;&#20110;&#25968;&#25454;&#20998;&#26512;&#23433;&#20840;&#24615;&#21644;&#21487;&#36798;&#24615;&#65292;&#32780;&#26080;&#38656;&#26126;&#30830;&#20351;&#29992;&#21160;&#24577;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;LyapunovBarrierActor-Critic&#65288;LBAC&#65289;&#31639;&#27861;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#23547;&#25214;&#28385;&#36275;&#22522;&#20110;&#25968;&#25454;&#30340;&#23433;&#20840;&#21644;&#21487;&#36798;&#24615;&#26465;&#20214;&#30340;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#36890;&#36807;&#20223;&#30495;&#21644;&#30495;&#23454;&#26426;&#22120;&#20154;&#25511;&#21046;&#23454;&#39564;&#65292;&#21363;2D&#22235;&#26059;&#32764;&#23548;&#33322;&#20219;&#21153;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#22312;&#21487;&#36798;&#24615;&#21644;&#23433;&#20840;&#24615;&#26041;&#38754;&#30340;&#25928;&#26524;&#20248;&#20110;&#20854;&#20182;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) exhibits impressive performance when managing complicated control tasks for robots. However, its wide application to physical robots is limited by the absence of strong safety guarantees. To overcome this challenge, this paper explores the control Lyapunov barrier function (CLBF) to analyze the safety and reachability solely based on data without explicitly employing a dynamic model. We also proposed the Lyapunov barrier actor-critic (LBAC), a model-free RL algorithm, to search for a controller that satisfies the data-based approximation of the safety and reachability conditions. The proposed approach is demonstrated through simulation and real-world robot control experiments, i.e., a 2D quadrotor navigation task. The experimental findings reveal this approach's effectiveness in reachability and safety, surpassing other model-free RL methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#24341;&#23548;&#36710;&#36742;&#65288;AGV&#65289;&#25511;&#21046;&#12289;&#36793;&#32536;&#26234;&#33021;&#21644;&#20154;&#31867;&#36755;&#20837;&#30340;&#21644;&#35856;&#35774;&#35745;&#65292;&#20197;&#23454;&#29616;&#24037;&#19994;&#29615;&#22659;&#20013;&#30340;&#33258;&#20027;&#36816;&#36755;&#65292;&#20854;&#26680;&#24515;&#25216;&#26415;&#26159;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#36830;&#25509;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;AGV&#23454;&#29616;&#20154;&#26426;&#21327;&#21516;&#12290;</title><link>http://arxiv.org/abs/2305.09788</link><description>&lt;p&gt;
&#36793;&#32536;&#26234;&#33021;&#19982;&#33258;&#21160;&#24341;&#23548;&#36710;&#36742;&#25511;&#21046;&#30340;&#21327;&#21516;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Codesign of Edge Intelligence and Automated Guided Vehicle Control. (arXiv:2305.09788v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#24341;&#23548;&#36710;&#36742;&#65288;AGV&#65289;&#25511;&#21046;&#12289;&#36793;&#32536;&#26234;&#33021;&#21644;&#20154;&#31867;&#36755;&#20837;&#30340;&#21644;&#35856;&#35774;&#35745;&#65292;&#20197;&#23454;&#29616;&#24037;&#19994;&#29615;&#22659;&#20013;&#30340;&#33258;&#20027;&#36816;&#36755;&#65292;&#20854;&#26680;&#24515;&#25216;&#26415;&#26159;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#36830;&#25509;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;AGV&#23454;&#29616;&#20154;&#26426;&#21327;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#20027;&#24341;&#23548;&#36710;&#36742;&#65288;AGV&#65289;&#25511;&#21046;&#12289;&#36793;&#32536;&#26234;&#33021;&#21644;&#20154;&#31867;&#36755;&#20837;&#30340;&#21644;&#35856;&#35774;&#35745;&#65292;&#20197;&#23454;&#29616;&#24037;&#19994;&#29615;&#22659;&#20013;&#30340;&#33258;&#20027;&#36816;&#36755;&#12290;&#35813;AGV&#20855;&#26377;&#22312;&#28304;&#21644;&#30446;&#26631;&#20043;&#38388;&#23548;&#33322;&#24182;&#25342;&#21462;/&#25918;&#32622;&#29289;&#21697;&#30340;&#33021;&#21147;&#12290;&#20154;&#31867;&#36755;&#20837;&#38544;&#21547;&#22320;&#25552;&#20379;&#20102;&#30446;&#30340;&#22320;&#21644;&#20934;&#30830;&#30340;&#21368;&#36135;&#28857;&#30340;&#20559;&#22909;&#65292;&#36825;&#20123;&#20559;&#22909;&#26469;&#33258;&#20110;&#32593;&#32476;&#36793;&#32536;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#26080;&#32447;&#32593;&#32476;&#19982;AGV&#20849;&#20139;&#12290;&#28436;&#31034;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#30828;&#20214;&#12289;&#36719;&#20214;&#21644;AI&#35774;&#35745;&#30340;&#32508;&#21512;&#35774;&#35745;&#36798;&#21040;&#20102;&#25216;&#26415;&#25104;&#29087;&#24230;&#27700;&#24179;&#65288;TRL&#65289;4-5&#30340;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a harmonic design of autonomous guided vehicle (AGV) control, edge intelligence, and human input to enable autonomous transportation in industrial environments. The AGV has the capability to navigate between a source and destinations and pick/place objects. The human input implicitly provides preferences of the destination and exact drop point, which are derived from an artificial intelligence (AI) module at the network edge and shared with the AGV over a wireless network. The demonstration indicates that the proposed integrated design of hardware, software, and AI design achieve a technology readiness level (TRL) of range 4-5
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25209;&#35780;&#24615;&#22320;&#26816;&#26597;&#21644;&#23457;&#26597;&#20102;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340;VQA&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#25991;&#26412;&#35821;&#20041;&#29983;&#25104;&#12289;&#23545;&#35937;&#35782;&#21035;&#21644;&#31572;&#26696;&#20998;&#31867;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2305.09782</link><description>&lt;p&gt;
&#24102;&#26377;&#27880;&#24847;&#21147;&#27169;&#22411;&#30340;&#35270;&#35273;&#38382;&#31572;&#31639;&#27861;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Analysis of Visual Question Answering Algorithms with attention model. (arXiv:2305.09782v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09782
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25209;&#35780;&#24615;&#22320;&#26816;&#26597;&#21644;&#23457;&#26597;&#20102;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340;VQA&#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;&#25991;&#26412;&#35821;&#20041;&#29983;&#25104;&#12289;&#23545;&#35937;&#35782;&#21035;&#21644;&#31572;&#26696;&#20998;&#31867;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38382;&#31572;&#65288;VQA&#65289;&#20351;&#29992;&#22270;&#20687;&#22788;&#29702;&#31639;&#27861;&#22788;&#29702;&#22270;&#20687;&#65292;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#29702;&#35299;&#24182;&#22238;&#31572;&#38382;&#39064;&#12290; VQA &#23545;&#35270;&#35273;&#21463;&#25439;&#32773;&#26377;&#24110;&#21161;&#65292;&#21487;&#29992;&#20110;&#23433;&#20840;&#30417;&#25511;&#31995;&#32479;&#21644;&#20174;&#32593;&#32476;&#20013;&#23398;&#20064;&#30340;&#22312;&#32447;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290; &#23427;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#27861;&#23398;&#20064;&#38382;&#39064;&#30340;&#35821;&#20041;&#24182;&#25552;&#21462;&#25991;&#26412;&#29305;&#24449;&#12290; &#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#29992;&#20110;&#20197;&#19968;&#31181;&#33021;&#22815;&#35782;&#21035;&#25152;&#38382;&#38382;&#39064;&#28041;&#21450;&#30340;&#29289;&#20307;&#30340;&#26041;&#24335;&#29983;&#25104;&#22270;&#20687;&#34920;&#31034;&#12290; &#27880;&#24847;&#21147;&#27169;&#22411;&#35797;&#22270;&#27169;&#20223;&#20154;&#31867;&#26681;&#25454;&#35821;&#22659;&#20851;&#27880;&#22270;&#20687;&#19981;&#21516;&#21306;&#22495;&#30340;&#34892;&#20026;&#12290; &#26412;&#25991;&#25209;&#35780;&#24615;&#22320;&#26816;&#26597;&#21644;&#23457;&#26597;&#20102;&#20351;&#29992;&#20849;&#21516;&#27880;&#24847;&#21147;&#26041;&#27861;&#30340; VQA &#31639;&#27861;&#30340;&#26041;&#27861;&#65292;&#20363;&#22914;&#29983;&#25104;&#25991;&#26412;&#35821;&#20041;&#65292;&#35782;&#21035;&#23545;&#35937;&#21644;&#31572;&#26696;&#20998;&#31867;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Visual question answering (VQA) usesimage processing algorithms to process the image and natural language processing methods to understand and answer the question. VQA is helpful to a visually impaired person, can be used for the security surveillance system and online chatbots that learn from the web. It uses NLP methods to learn the semantic of the question and to derive the textual features. Computer vision techniques are used for generating image representation in such a way that they can identify the objects about which question is asked. The Attention model tries to mimic the human behavior of giving attention to a different region of an image according to our understanding of its context. This paper critically examines and reviews methods of VQA algorithm such as generation of semantics of text, identification of objects and answer classification techniques that use the co-attention approach.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#25193;&#23637;Walsh-Hadamard&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#36991;&#20813;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20302;&#38454;&#39057;&#29575;&#20197;&#21450;&#35823;&#35782;&#21035;&#20302;&#38454;&#39057;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09779</link><description>&lt;p&gt;
&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;Walsh-Hadamard&#27491;&#21017;&#21270;&#22120;&#65292;&#20197;&#20811;&#26381;&#31070;&#32463;&#32593;&#32476;&#30340;&#20302;&#38454;&#35889;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
A Scalable Walsh-Hadamard Regularizer to Overcome the Low-degree Spectral Bias of Neural Networks. (arXiv:2305.09779v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#21487;&#25193;&#23637;Walsh-Hadamard&#27491;&#21017;&#21270;&#22120;&#65292;&#21487;&#36991;&#20813;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#20302;&#38454;&#39057;&#29575;&#20197;&#21450;&#35823;&#35782;&#21035;&#20302;&#38454;&#39057;&#29575;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#23398;&#20064;&#20219;&#24847;&#20989;&#25968;&#30340;&#33021;&#21147;&#65292;&#20294;&#36890;&#36807;&#26799;&#24230;&#19979;&#38477;&#35757;&#32451;&#30340;&#27169;&#22411;&#24120;&#24120;&#34920;&#29616;&#20986;&#23545;&#8220;&#26356;&#31616;&#21333;&#8221;&#20989;&#25968;&#30340;&#20559;&#22909;&#12290;&#26412;&#25991;&#36890;&#36807;&#20613;&#37324;&#21494;&#65288;Walsh-Hadamard&#65289;&#21464;&#25442;&#65292;&#20174;&#31163;&#25955;&#65288;&#38646;&#19968;&#65289;&#36755;&#20837;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#35282;&#24230;&#25506;&#35752;&#20102;&#31616;&#21333;&#24615;&#30340;&#27010;&#24565;&#65292;&#20854;&#20013;&#21487;&#20197;&#36890;&#36807;&#20613;&#37324;&#21494;&#31995;&#25968;&#30340;&#8220;&#38454;&#8221;&#26469;&#25429;&#25417;&#31616;&#21333;&#24615;&#27010;&#24565;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#31070;&#32463;&#32593;&#32476;&#26377;&#23398;&#20064;&#36739;&#20302;&#38454;&#39057;&#29575;&#30340;&#36235;&#21183;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#35889;&#20559;&#24046;&#21521;&#36739;&#31616;&#21333;&#29305;&#24449;&#30340;&#36235;&#21183;&#23454;&#38469;&#19978;&#20250;&#25439;&#23475;&#31070;&#32463;&#32593;&#32476;&#22312;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#25193;&#23637;&#30340;&#21151;&#33021;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#20197;&#24110;&#21161;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#26356;&#39640;&#30340;&#38454;&#39057;&#29575;&#12290;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#22120;&#36824;&#26377;&#21161;&#20110;&#36991;&#20813;&#23545;&#20302;&#38454;&#39057;&#29575;&#30340;&#38169;&#35823;&#35782;&#21035;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35821;&#38899;&#35782;&#21035;&#20013;&#24212;&#29992;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#36827;&#34892;&#20998;&#31867;&#20219;&#21153;&#30340;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27491;&#21017;&#21270;&#22120;&#22312;&#20302;&#25968;&#25454;&#37327;&#29615;&#22659;&#19979;&#26174;&#33879;&#25552;&#39640;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the capacity of neural nets to learn arbitrary functions, models trained through gradient descent often exhibit a bias towards ``simpler'' functions. Various notions of simplicity have been introduced to characterize this behavior. Here, we focus on the case of neural networks with discrete (zero-one) inputs through the lens of their Fourier (Walsh-Hadamard) transforms, where the notion of simplicity can be captured through the \emph{degree} of the Fourier coefficients. We empirically show that neural networks have a tendency to learn lower-degree frequencies. We show how this spectral bias towards simpler features can in fact \emph{hurt} the neural network's generalization on real-world datasets. To remedy this we propose a new scalable functional regularization scheme that aids the neural network to learn higher degree frequencies. Our regularizer also helps avoid erroneous identification of low-degree frequencies, which further improves generalization. We extensively evaluat
&lt;/p&gt;</description></item><item><title>&#21033;&#29992;&#30524;&#21160;&#25968;&#25454;&#23545;&#20154;&#31867;&#30340;&#27880;&#24847;&#21147;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#28304;&#20195;&#30721;&#25688;&#35201;&#20013;&#65292;&#39044;&#27979;&#28304;&#20195;&#30721;&#20013;&#26368;&#37325;&#35201;&#30340;&#21333;&#35789;&#24182;&#22686;&#24378;&#20102;&#22522;&#32447;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09773</link><description>&lt;p&gt;
&#22522;&#20110;&#30524;&#21160;&#25968;&#25454;&#30340;&#20154;&#31867;&#27880;&#24847;&#21147;&#24314;&#27169;&#22312;&#31070;&#32463;&#28304;&#20195;&#30721;&#25688;&#35201;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Towards Modeling Human Attention from Eye Movements for Neural Source Code Summarization. (arXiv:2305.09773v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09773
&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#30524;&#21160;&#25968;&#25454;&#23545;&#20154;&#31867;&#30340;&#27880;&#24847;&#21147;&#36827;&#34892;&#24314;&#27169;&#65292;&#24182;&#23558;&#27169;&#22411;&#24212;&#29992;&#20110;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#28304;&#20195;&#30721;&#25688;&#35201;&#20013;&#65292;&#39044;&#27979;&#28304;&#20195;&#30721;&#20013;&#26368;&#37325;&#35201;&#30340;&#21333;&#35789;&#24182;&#22686;&#24378;&#20102;&#22522;&#32447;&#27169;&#22411;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#28304;&#20195;&#30721;&#25688;&#35201;&#26159;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#29983;&#25104;&#28304;&#20195;&#30721;&#34892;&#20026;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#30340;&#20219;&#21153;&#12290;&#22823;&#22810;&#25968;&#31070;&#32463;&#27169;&#22411;&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998;&#26159;&#27880;&#24847;&#26426;&#21046;&#12290;&#27880;&#24847;&#26426;&#21046;&#23398;&#20064;&#23558;&#28304;&#20195;&#30721;&#20013;&#30340;&#29305;&#24449;&#19982;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#26102;&#35201;&#20351;&#29992;&#30340;&#29305;&#23450;&#21333;&#35789;&#36830;&#25509;&#36215;&#26469;&#12290;&#20154;&#31867;&#22312;&#32534;&#30721;&#20013;&#20063;&#20250;&#26356;&#21152;&#20851;&#27880;&#26576;&#20123;&#29305;&#23450;&#30340;&#29305;&#24449;&#12290;&#36825;&#31181;&#20154;&#31867;&#20851;&#27880;&#21453;&#26144;&#20102;&#32463;&#39564;&#21644;&#39640;&#27700;&#24179;&#35748;&#30693;&#65292;&#36828;&#36229;&#20219;&#20309;&#24403;&#21069;&#31070;&#32463;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#26412;&#25991;&#21033;&#29992;&#24050;&#21457;&#24067;&#30340;&#30524;&#21160;&#23454;&#39564;&#25968;&#25454;&#21019;&#24314;&#20102;&#20154;&#31867;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#24182;&#39044;&#27979;&#28304;&#20195;&#30721;&#20013;&#26368;&#37325;&#35201;&#30340;&#21333;&#35789;&#65292;&#20197;&#22686;&#24378;&#22522;&#32447;&#31070;&#32463;&#20195;&#30721;&#25688;&#35201;&#26041;&#27861;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#22686;&#24378;&#26041;&#27861;&#30340;&#39044;&#27979;&#24615;&#33021;&#26377;&#25152;&#25552;&#21319;&#65292;&#36825;&#19982;&#20854;&#20182;&#29983;&#29289;&#21551;&#21457;&#24335;&#31070;&#32463;&#27169;&#22411;&#30340;&#34920;&#29616;&#30456;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural source code summarization is the task of generating natural language descriptions of source code behavior using neural networks. A fundamental component of most neural models is an attention mechanism. The attention mechanism learns to connect features in source code to specific words to use when generating natural language descriptions. Humans also pay attention to some features in code more than others. This human attention reflects experience and high-level cognition well beyond the capability of any current neural model. In this paper, we use data from published eye-tracking experiments to create a model of this human attention. The model predicts which words in source code are the most important for code summarization. Next, we augment a baseline neural code summarization approach using our model of human attention. We observe an improvement in prediction performance of the augmented approach in line with other bio-inspired neural models.
&lt;/p&gt;</description></item><item><title>ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.09770</link><description>&lt;p&gt;
ConvXAI&#65306;&#36890;&#36807;&#23545;&#35805;&#25552;&#20379;&#24322;&#26500;&#30340;AI&#35299;&#37322;&#65292;&#25903;&#25345;&#20154;&#26426;&#31185;&#25216;&#20889;&#20316;
&lt;/p&gt;
&lt;p&gt;
ConvXAI: Delivering Heterogeneous AI Explanations via Conversations to Support Human-AI Scientific Writing. (arXiv:2305.09770v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09770
&lt;/p&gt;
&lt;p&gt;
ConvXAI&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#38598;&#25104;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#23884;&#20837;&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#65288;XAI&#65289;&#26041;&#27861;&#26469;&#35299;&#37322;AI&#31995;&#32479;&#65292;&#20294;&#30446;&#21069;&#30340;&#26041;&#27861;&#26159;&#21542;&#23545;&#20154;&#31867;&#23454;&#29992;&#20173;&#23384;&#22312;&#19981;&#19968;&#33268;&#30340;&#21457;&#29616;&#12290;&#20026;&#20102;&#25913;&#21892;XAI&#26041;&#27861;&#30340;&#23454;&#29992;&#24615;&#65292;&#19968;&#31995;&#21015;&#30740;&#31350;&#30830;&#23450;&#20102;&#29616;&#23454;&#19990;&#30028;&#20013;&#22810;&#26679;&#21270;&#21644;&#21160;&#24577;&#30340;&#29992;&#25143;&#38656;&#27714;&#19982;&#29616;&#26377;XAI&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#34429;&#28982;&#20043;&#21069;&#30340;&#30740;&#31350;&#35774;&#24819;&#23558;&#22810;&#31181;XAI&#26041;&#27861;&#38598;&#25104;&#21040;&#36890;&#29992;XAI&#30028;&#38754;&#65288;&#20363;&#22914;&#65292;&#22522;&#20110;&#23545;&#35805;&#25110;GUI&#30340;XAI&#31995;&#32479;&#65289;&#20013;&#20197;&#20943;&#36731;&#36825;&#20123;&#24046;&#36317;&#65292;&#20294;&#32570;&#23569;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#22914;&#20309;&#35774;&#35745;&#20197;&#28385;&#36275;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ConvXAI&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#23545;&#35805;&#30340;XAI&#31995;&#32479;&#65292;&#23427;&#32467;&#21512;&#20102;&#22810;&#31181;XAI&#31867;&#22411;&#65292;&#24182;&#36171;&#20104;&#29992;&#25143;&#36890;&#36807;&#36890;&#29992;&#30340;XAI&#23545;&#35805;&#30028;&#38754;&#25552;&#20986;&#21508;&#31181;XAI&#38382;&#39064;&#30340;&#33021;&#21147;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#21019;&#26032;&#22320;&#23558;&#23454;&#38469;&#29992;&#25143;&#38656;&#27714;&#65288;&#21363;&#65292;&#22522;&#20110;&#26684;&#24335;&#30740;&#31350;&#30340;&#22235;&#20010;&#21407;&#21017;&#65289;&#23884;&#20837;ConvXAI&#35774;&#35745;&#20013;&#65292;&#20197;&#25552;&#39640;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
While various AI explanation (XAI) methods have been proposed to interpret AI systems, whether the state-of-the-art XAI methods are practically useful for humans remains inconsistent findings. To improve the usefulness of XAI methods, a line of studies identifies the gaps between the diverse and dynamic real-world user needs with the status quo of XAI methods. Although prior studies envision mitigating these gaps by integrating multiple XAI methods into the universal XAI interfaces (e.g., conversational or GUI-based XAI systems), there is a lack of work investigating how these systems should be designed to meet practical user needs. In this study, we present ConvXAI, a conversational XAI system that incorporates multiple XAI types, and empowers users to request a variety of XAI questions via a universal XAI dialogue interface. Particularly, we innovatively embed practical user needs (i.e., four principles grounding on the formative study) into ConvXAI design to improve practical useful
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;CNN&#30340;&#37327;&#23376;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#37322;&#21738;&#20123;&#29305;&#24449;&#23545;&#20110;&#20998;&#31867;&#26368;&#37325;&#35201;&#26469;&#36991;&#20813;&#36951;&#24536;&#65292;&#24182;&#22768;&#31216;&#22914;&#26524;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#21017;&#20250;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09738</link><description>&lt;p&gt;
CQural&#65306;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;CNN&#30340;&#37327;&#23376;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
CQural: A Novel CNN based Hybrid Architecture for Quantum Continual Machine Learning. (arXiv:2305.09738v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09738
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;CNN&#30340;&#37327;&#23376;&#25345;&#32493;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#21487;&#20197;&#36890;&#36807;&#35299;&#37322;&#21738;&#20123;&#29305;&#24449;&#23545;&#20110;&#20998;&#31867;&#26368;&#37325;&#35201;&#26469;&#36991;&#20813;&#36951;&#24536;&#65292;&#24182;&#22768;&#31216;&#22914;&#26524;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#21017;&#20250;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22686;&#37327;&#24335;&#23398;&#20064;&#20013;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#19981;&#20165;&#24456;&#37325;&#35201;&#65292;&#32780;&#19988;&#26159;&#23454;&#29616;&#20154;&#24037;&#36890;&#29992;&#26234;&#33021;&#30340;&#26377;&#25928;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#22312;&#25345;&#32493;&#23398;&#20064;&#26041;&#38754;&#24456;&#23481;&#26131;&#20986;&#29616;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#38382;&#39064;&#12290;&#35768;&#22810;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#35768;&#22810;&#25216;&#26415;&#26469;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#30340;&#36951;&#24536;&#24433;&#21709;&#65292;&#20294;&#26159;&#25152;&#26377;&#30340;&#25216;&#26415;&#37117;&#26159;&#22312;&#32463;&#20856;&#23398;&#20064;&#19978;&#30740;&#31350;&#30340;&#65292;&#24456;&#23569;&#26377;&#20154;&#20851;&#27880;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#32467;&#26500;&#30340;&#25913;&#21464;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#26032;&#30340;&#28151;&#21512;&#32463;&#20856; - &#37327;&#23376;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36991;&#20813;&#25345;&#32493;&#23398;&#20064;&#20013;&#30340;&#28798;&#38590;&#24615;&#36951;&#24536;&#65292;&#24182;&#35299;&#37322;&#20102;&#21738;&#20123;&#21151;&#33021;&#23545;&#20110;&#20998;&#31867;&#26368;&#37325;&#35201;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22768;&#31216;&#22914;&#26524;&#20351;&#29992;&#36825;&#20123;&#35299;&#37322;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#21017;&#20250;&#33719;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training machine learning models in an incremental fashion is not only important but also an efficient way to achieve artificial general intelligence. The ability that humans possess of continuous or lifelong learning helps them to not forget previously learned tasks. However, current neural network models are prone to catastrophic forgetting when it comes to continual learning. Many researchers have come up with several techniques in order to reduce the effect of forgetting from neural networks, however, all techniques are studied classically with a very less focus on changing the machine learning model architecture. In this research paper, we show that it is not only possible to circumvent catastrophic forgetting in continual learning with novel hybrid classical-quantum neural networks, but also explains what features are most important to learn for classification. In addition, we also claim that if the model is trained with these explanations, it tends to give better performance and
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20025;&#40614;&#25163;&#35821;&#27880;&#37322;&#25968;&#25454;&#38598;&#65288;ADDSL&#65289;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;YOLOv5&#30340;&#25163;&#21183;&#26816;&#27979;&#21644;&#23383;&#27597;&#25968;&#23383;&#35782;&#21035;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#26368;&#39640;&#21487;&#36798;92%&#12290;&#19982;&#21516;&#39046;&#22495;&#29616;&#26377;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#26356;&#39640;&#25928;&#21644;&#20934;&#30830;&#12290;</title><link>http://arxiv.org/abs/2305.09736</link><description>&lt;p&gt;
ADDSL: &#22522;&#20110;&#26631;&#27880;&#30340;&#20025;&#40614;&#25163;&#35821;&#30340;&#25163;&#21183;&#26816;&#27979;&#19982;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
ADDSL: Hand Gesture Detection and Sign Language Recognition on Annotated Danish Sign Language. (arXiv:2305.09736v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09736
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#20025;&#40614;&#25163;&#35821;&#27880;&#37322;&#25968;&#25454;&#38598;&#65288;ADDSL&#65289;&#24182;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#35757;&#32451;&#20102;&#19968;&#20010;&#22522;&#20110;YOLOv5&#30340;&#25163;&#21183;&#26816;&#27979;&#21644;&#23383;&#27597;&#25968;&#23383;&#35782;&#21035;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#26368;&#39640;&#21487;&#36798;92%&#12290;&#19982;&#21516;&#39046;&#22495;&#29616;&#26377;&#24037;&#20316;&#30456;&#27604;&#65292;&#35813;&#27169;&#22411;&#26356;&#39640;&#25928;&#21644;&#20934;&#30830;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#20197;&#26469;&#65292;&#23558;&#25163;&#21183;&#26816;&#27979;&#24182;&#23558;&#20854;&#35782;&#21035;&#20026;&#23383;&#27597;&#25110;&#25968;&#23383;&#19968;&#30452;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#32473;&#27531;&#38556;&#20154;&#22763;&#24102;&#26469;&#20102;&#27807;&#36890;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21363;&#20025;&#40614;&#25163;&#35821;&#27880;&#37322;&#25968;&#25454;&#38598;&#65288;ADDSL&#65289;&#12290;&#20351;&#29992;&#24320;&#28304;&#24037;&#20855;LabelImg&#22312;YOLO&#26684;&#24335;&#20013;&#21046;&#20316;&#20102;&#25968;&#25454;&#38598;&#30340;&#27880;&#37322;&#12290;&#21033;&#29992;&#27492;&#25968;&#25454;&#38598;&#65292;&#20351;&#29992;CSP-DarkNet53&#39592;&#24178;&#21644;YOLOv3&#22836;&#30340;&#21333;&#38454;&#27573;&#30446;&#26631;&#26816;&#27979;&#22120;&#27169;&#22411;&#65288;YOLOv5&#65289;&#36890;&#36807;&#27599;&#31867;&#20165;&#20351;&#29992;&#19971;&#20010;&#29420;&#29305;&#30340;&#22270;&#20687;&#65288;&#19981;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65289;&#26469;&#35757;&#32451;&#65292;&#20197;&#35782;&#21035;&#23383;&#27597;&#65288;A-Z&#65289;&#21644;&#25968;&#23383;&#65288;0-9&#65289;&#12290;&#35757;&#32451;&#20116;&#20010;&#27169;&#22411;&#65292;&#20849;350&#20010;&#21608;&#26399;&#65292;&#24471;&#21040;&#27599;&#24352;&#22270;&#20687;&#30340;&#24179;&#22343;&#25512;&#26029;&#26102;&#38388;&#20026;9.02ms&#65292;&#19982;&#20043;&#21069;&#30340;&#30740;&#31350;&#30456;&#27604;&#65292;&#26368;&#20339;&#20934;&#30830;&#29575;&#20026;92%&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20462;&#25913;&#21518;&#30340;&#27169;&#22411;&#27604;&#21516;&#39046;&#22495;&#30340;&#29616;&#26377;&#24037;&#20316;&#26356;&#39640;&#25928;&#21644;&#20934;&#30830;&#12290;&#25105;&#20204;&#27169;&#22411;&#30340;&#20195;&#30721;&#24211;&#21487;&#22312;GitHub&#23384;&#20648;&#24211;https://github.com/s4nyam/pvt-addsl &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a long time, detecting hand gestures and recognizing them as letters or numbers has been a challenging task. This creates communication barriers for individuals with disabilities. This paper introduces a new dataset, the Annotated Dataset for Danish Sign Language (ADDSL). Annota-tions for the dataset were made using the open-source tool LabelImg in the YOLO format. Using this dataset, a one-stage ob-ject detector model (YOLOv5) was trained with the CSP-DarkNet53 backbone and YOLOv3 head to recognize letters (A-Z) and numbers (0-9) using only seven unique images per class (without augmen-tation). Five models were trained with 350 epochs, resulting in an average inference time of 9.02ms per image and a best accu-racy of 92% when compared to previous research. Our results show that modified model is efficient and more accurate than existing work in the same field. The code repository for our model is available at the GitHub repository https://github.com/s4nyam/pvt-addsl.
&lt;/p&gt;</description></item><item><title>FedHGN&#26159;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#27169;&#24335;&#26435;&#37325;&#35299;&#32806;&#21644;&#31995;&#25968;&#23545;&#40784;&#25216;&#26415;&#65292;&#20351;&#24471;&#19981;&#21516;&#23458;&#25143;&#31471;&#21487;&#20197;&#20849;&#20139;&#30693;&#35782;&#32780;&#19981;&#27844;&#38706;&#38544;&#31169;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#21152;&#20248;&#31168;&#12290;</title><link>http://arxiv.org/abs/2305.09729</link><description>&lt;p&gt;
FedHGN&#65306;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedHGN: A Federated Framework for Heterogeneous Graph Neural Networks. (arXiv:2305.09729v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09729
&lt;/p&gt;
&lt;p&gt;
FedHGN&#26159;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#37319;&#29992;&#27169;&#24335;&#26435;&#37325;&#35299;&#32806;&#21644;&#31995;&#25968;&#23545;&#40784;&#25216;&#26415;&#65292;&#20351;&#24471;&#19981;&#21516;&#23458;&#25143;&#31471;&#21487;&#20197;&#20849;&#20139;&#30693;&#35782;&#32780;&#19981;&#27844;&#38706;&#38544;&#31169;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#34920;&#29616;&#26356;&#21152;&#20248;&#31168;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19982;&#20256;&#32479;GNN&#30456;&#27604;&#65292;&#24322;&#26500;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HGNN&#65289;&#21487;&#20197;&#26356;&#26377;&#25928;&#22320;&#20174;&#31867;&#22411;&#21270;&#21644;&#20851;&#31995;&#21270;&#22270;&#25968;&#25454;&#20013;&#23398;&#20064;&#12290;&#30001;&#20110;&#38544;&#31169;&#27861;&#35268;&#65288;&#20363;&#22914;GDPR&#65289;&#65292;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#35757;&#32451;&#25968;&#25454;&#24448;&#24448;&#24456;&#23569;&#65292;&#32780;&#20351;&#29992;&#26356;&#22823;&#30340;&#21442;&#25968;&#31354;&#38388;&#21487;&#33021;&#38656;&#35201;&#26356;&#22810;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#32852;&#37030;&#22270;&#23398;&#20064;&#65288;FGL&#65289;&#20351;&#22810;&#20010;&#23458;&#25143;&#31471;&#20849;&#21516;&#35757;&#32451;GNN&#32780;&#19981;&#20849;&#20139;&#26412;&#22320;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;FGL&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#21516;&#26500;GNN&#25110;&#30693;&#35782;&#22270;&#23884;&#20837;&#19978;&#65307;&#24456;&#23569;&#32771;&#34385;&#24322;&#26500;&#22270;&#21644;HGNN&#12290;&#22312;&#32852;&#37030;&#24322;&#26500;&#22270;&#23398;&#20064;&#20013;&#65292;&#23458;&#25143;&#31471;&#21487;&#33021;&#25317;&#26377;&#31169;&#26377;&#22270;&#27169;&#24335;&#65292;&#23581;&#35797;&#23450;&#20041;&#20840;&#23616;HGNN&#27169;&#22411;&#30340;&#20256;&#32479;FL/FGL&#26041;&#27861;&#20250;&#20405;&#29359;&#27169;&#24335;&#38544;&#31169;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedHGN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;HGNN FGL&#26694;&#26550;&#12290;FedHGN&#37319;&#29992;&#27169;&#24335;&#26435;&#37325;&#35299;&#32806;&#26469;&#23454;&#29616;&#29420;&#31435;&#20110;&#27169;&#24335;&#30340;&#30693;&#35782;&#20849;&#20139;&#65292;&#24182;&#37319;&#29992;&#31995;&#25968;&#23545;&#40784;&#26469;&#31283;&#23450;&#35757;&#32451;&#36807;&#31243;&#21644;&#25552;&#39640;HGNN&#27867;&#21270;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#29616;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;FedHGN&#30456;&#23545;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous graph neural networks (HGNNs) can learn from typed and relational graph data more effectively than conventional GNNs. With larger parameter spaces, HGNNs may require more training data, which is often scarce in real-world applications due to privacy regulations (e.g., GDPR). Federated graph learning (FGL) enables multiple clients to train a GNN collaboratively without sharing their local data. However, existing FGL methods mainly focus on homogeneous GNNs or knowledge graph embeddings; few have considered heterogeneous graphs and HGNNs. In federated heterogeneous graph learning, clients may have private graph schemas. Conventional FL/FGL methods attempting to define a global HGNN model would violate schema privacy. To address these challenges, we propose FedHGN, a novel and general FGL framework for HGNNs. FedHGN adopts schema-weight decoupling to enable schema-agnostic knowledge sharing and employs coefficients alignment to stabilize the training process and improve HGNN
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#30340;&#25193;&#25955;&#21464;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26102;&#31354;&#39044;&#27979;&#65292;&#22312;&#21160;&#24577;&#22270;&#26500;&#24314;&#19978;&#32771;&#34385;&#20102;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#35299;&#20915;&#20102;&#21160;&#24577;&#22270;&#31639;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09703</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#30340;&#25193;&#25955;&#21464;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#26102;&#31354;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Dynamic Causal Explanation Based Diffusion-Variational Graph Neural Network for Spatio-temporal Forecasting. (arXiv:2305.09703v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#30340;&#25193;&#25955;&#21464;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#26102;&#31354;&#39044;&#27979;&#65292;&#22312;&#21160;&#24577;&#22270;&#26500;&#24314;&#19978;&#32771;&#34385;&#20102;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#21644;&#19981;&#30830;&#23450;&#24615;&#65292;&#35299;&#20915;&#20102;&#21160;&#24577;&#22270;&#31639;&#27861;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#21160;&#24577;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#24050;&#32463;&#25104;&#20026;&#26102;&#31354;&#39044;&#27979;&#38382;&#39064;&#20013;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#34429;&#28982;&#35768;&#22810;&#21160;&#24577;&#22270;&#26500;&#24314;&#26041;&#27861;&#24050;&#32463;&#34987;&#24320;&#21457;&#65292;&#20294;&#20854;&#20013;&#30456;&#23545;&#36739;&#23569;&#30340;&#25506;&#32034;&#20102;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#30001;&#27492;&#20135;&#29983;&#30340;&#27169;&#22411;&#32570;&#20047;&#23545;&#21160;&#24577;&#29983;&#25104;&#22270;&#30340;&#37051;&#23621;&#33410;&#28857;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#24378;&#22823;&#21487;&#35299;&#37322;&#24615;&#65292;&#36825;&#24456;&#23481;&#26131;&#23548;&#33268;&#21518;&#32493;&#20915;&#31574;&#30340;&#39118;&#38505;&#12290;&#27492;&#22806;&#65292;&#24456;&#23569;&#26377;&#20154;&#32771;&#34385;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#30340;&#21160;&#24577;&#22270;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#22122;&#22768;&#65292;&#32780;&#36825;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#22270;&#32467;&#26500;&#32593;&#32476;&#20013;&#26159;&#26222;&#36941;&#23384;&#22312;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#21464;&#20998;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;DVGNN&#65289;&#30340;&#26102;&#31354;&#39044;&#27979;&#26041;&#27861;&#12290;&#23545;&#20110;&#21160;&#24577;&#22270;&#26500;&#24314;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#29983;&#25104;&#27169;&#22411;&#12290;&#22312;&#32534;&#30721;&#22120;&#38454;&#27573;&#65292;&#24212;&#29992;&#20004;&#23618;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#26469;&#35745;&#31639;&#28508;&#22312;&#33410;&#28857;&#23884;&#20837;&#30340;&#21518;&#39564;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs), especially dynamic GNNs, have become a research hotspot in spatio-temporal forecasting problems. While many dynamic graph construction methods have been developed, relatively few of them explore the causal relationship between neighbour nodes. Thus, the resulting models lack strong explainability for the causal relationship between the neighbour nodes of the dynamically generated graphs, which can easily lead to a risk in subsequent decisions. Moreover, few of them consider the uncertainty and noise of dynamic graphs based on the time series datasets, which are ubiquitous in real-world graph structure networks. In this paper, we propose a novel Dynamic Diffusion-Variational Graph Neural Network (DVGNN) for spatio-temporal forecasting. For dynamic graph construction, an unsupervised generative model is devised. Two layers of graph convolutional network (GCN) are applied to calculate the posterior distribution of the latent node embeddings in the encoder sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;TapTap&#65292;&#19968;&#31181;&#36890;&#36807;&#34920;&#26684;&#39044;&#35757;&#32451;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#34920;&#26684;&#26469;&#25552;&#39640;&#34920;&#26684;&#39044;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#22312;12&#20010;&#25968;&#25454;&#38598;&#23454;&#39564;&#20013;&#65292;TapTap&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20248;&#20110;16&#20010;&#22522;&#32447;&#65292;&#24182;&#21487;&#20197;&#19982;&#22810;&#20010;&#39592;&#24178;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.09696</link><description>&lt;p&gt;
&#29983;&#25104;&#24335;&#34920;&#26684;&#39044;&#35757;&#32451;&#22686;&#24378;&#20102;&#34920;&#26684;&#39044;&#27979;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Generative Table Pre-training Empowers Models for Tabular Prediction. (arXiv:2305.09696v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09696
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;TapTap&#65292;&#19968;&#31181;&#36890;&#36807;&#34920;&#26684;&#39044;&#35757;&#32451;&#29983;&#25104;&#39640;&#36136;&#37327;&#21512;&#25104;&#34920;&#26684;&#26469;&#25552;&#39640;&#34920;&#26684;&#39044;&#27979;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;&#22312;12&#20010;&#25968;&#25454;&#38598;&#23454;&#39564;&#20013;&#65292;TapTap&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20248;&#20110;16&#20010;&#22522;&#32447;&#65292;&#24182;&#21487;&#20197;&#19982;&#22810;&#20010;&#39592;&#24178;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#34920;&#26684;&#39044;&#35757;&#32451;&#24050;&#32463;&#25104;&#20026;&#30740;&#31350;&#30340;&#28909;&#28857;&#65292;&#20294;&#22914;&#20309;&#21033;&#29992;&#34920;&#26684;&#39044;&#35757;&#32451;&#26469;&#25552;&#39640;&#34920;&#26684;&#39044;&#27979;&#30340;&#24615;&#33021;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;TapTap&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#21033;&#29992;&#34920;&#26684;&#39044;&#35757;&#32451;&#26469;&#22686;&#24378;&#34920;&#26684;&#39044;&#27979;&#27169;&#22411;&#30340;&#23581;&#35797;&#12290;&#22312;&#23545;&#22823;&#37327;&#23454;&#38469;&#19990;&#30028;&#30340;&#34920;&#26684;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#21518;&#65292;TapTap&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#21512;&#25104;&#34920;&#26684;&#65292;&#20197;&#25903;&#25345;&#21508;&#31181;&#34920;&#26684;&#25968;&#25454;&#24212;&#29992;&#65292;&#21253;&#25324;&#38544;&#31169;&#20445;&#25252;&#12289;&#20302;&#36164;&#28304;&#29615;&#22659;&#12289;&#32570;&#22833;&#20540;&#25554;&#34917;&#21644;&#22833;&#34913;&#20998;&#31867;&#12290;&#22312;12&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#24191;&#27867;&#23454;&#39564;&#34920;&#26126;&#65292;TapTap&#22312;&#19981;&#21516;&#22330;&#26223;&#19979;&#20248;&#20110;16&#20010;&#22522;&#32447;&#12290;&#21516;&#26102;&#65292;&#23427;&#21487;&#20197;&#36731;&#26494;&#22320;&#19982;&#21508;&#31181;&#39592;&#24178;&#27169;&#22411;&#32467;&#21512;&#20351;&#29992;&#65292;&#21253;&#25324;LightGBM&#12289;&#22810;&#23618;&#24863;&#30693;&#26426;&#65288;MLP&#65289;&#21644;Transformer&#12290;&#27492;&#22806;&#65292;&#22312;&#34920;&#26684;&#39044;&#35757;&#32451;&#30340;&#24110;&#21161;&#19979;&#65292;&#20351;&#29992;TapTap&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#30340;&#27169;&#22411;&#29978;&#33267;&#21487;&#20197;&#19982;&#20351;&#29992;&#21407;&#22987;&#30495;&#23454;&#25968;&#25454;&#30340;&#27169;&#22411;&#31454;&#20105;&#65292;&#23454;&#29616;&#20102;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, the topic of table pre-training has attracted considerable research interest. However, how to employ table pre-training to boost the performance of tabular prediction remains an open challenge. In this paper, we propose TapTap, the first attempt that leverages table pre-training to empower models for tabular prediction. After pre-training on a large corpus of real-world tabular data, TapTap can generate high-quality synthetic tables to support various applications on tabular data, including privacy protection, low resource regime, missing value imputation, and imbalanced classification. Extensive experiments on 12 datasets demonstrate that TapTap outperforms a total of 16 baselines in different scenarios. Meanwhile, it can be easily combined with various backbone models, including LightGBM, Multilayer Perceptron (MLP) and Transformer. Moreover, with the aid of table pre-training, models trained using synthetic data generated by TapTap can even compete with models using the or
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24102;&#34928;&#20943;&#20989;&#25968;&#30340;&#28857;&#35843;&#25972;&#21327;&#35758;&#65288;PAdf&#65289;&#20197;&#35299;&#20915;&#29616;&#26377;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#35780;&#20272;&#26041;&#24335;&#39640;&#20272;&#25110;&#20302;&#20272;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#37325;&#26032;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;PAdf&#21327;&#35758;&#19981;&#20165;&#32771;&#34385;&#35201;&#26597;&#25214;&#23613;&#21487;&#33021;&#22810;&#30340;&#27573;&#25968;&#65292;&#36824;&#32771;&#34385;&#24555;&#36895;&#20934;&#30830;&#22320;&#26816;&#27979;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2305.09691</link><description>&lt;p&gt;
&#24102;&#34928;&#20943;&#20989;&#25968;&#30340;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#35780;&#20272;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Evaluation Strategy of Time-series Anomaly Detection with Decay Function. (arXiv:2305.09691v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09691
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24102;&#34928;&#20943;&#20989;&#25968;&#30340;&#28857;&#35843;&#25972;&#21327;&#35758;&#65288;PAdf&#65289;&#20197;&#35299;&#20915;&#29616;&#26377;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#35780;&#20272;&#26041;&#24335;&#39640;&#20272;&#25110;&#20302;&#20272;&#24615;&#33021;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#37325;&#26032;&#35780;&#20272;&#65292;&#25105;&#20204;&#21457;&#29616;PAdf&#21327;&#35758;&#19981;&#20165;&#32771;&#34385;&#35201;&#26597;&#25214;&#23613;&#21487;&#33021;&#22810;&#30340;&#27573;&#25968;&#65292;&#36824;&#32771;&#34385;&#24555;&#36895;&#20934;&#30830;&#22320;&#26816;&#27979;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#19968;&#33324;&#37319;&#29992;&#28857;&#35843;&#25972;&#21327;&#35758;&#26469;&#35780;&#20272;&#20854;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#21327;&#35758;&#23481;&#26131;&#39640;&#20272;&#26816;&#27979;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#22240;&#20026;&#23427;&#21482;&#32771;&#34385;&#26816;&#27979;&#21040;&#30340;&#24322;&#24120;&#27573;&#25968;&#21644;&#22823;&#23567;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35780;&#20272;&#21327;&#35758;&#8212;&#8212;&#24102;&#34928;&#20943;&#20989;&#25968;&#30340;&#28857;&#35843;&#25972;&#21327;&#35758;&#65288;PAdf&#65289;&#65292;&#20197;&#35780;&#20272;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#35813;&#21327;&#35758;&#32771;&#34385;&#20102;&#24555;&#36895;&#20934;&#30830;&#22320;&#26816;&#27979;&#24322;&#24120;&#21644;&#36991;&#20813;&#35823;&#25253;&#30340;&#29702;&#24819;&#35201;&#27714;&#12290;&#26412;&#25991;&#20174;&#29702;&#35770;&#21644;&#23454;&#39564;&#20004;&#20010;&#26041;&#38754;&#35777;&#26126;&#20102;PAdf&#21327;&#35758;&#35299;&#20915;&#20102;&#29616;&#26377;&#21327;&#35758;&#22914;PA&#21644;PA\%K&#31561;&#30340;&#39640;&#20272;&#21644;&#20302;&#20272;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#37325;&#26032;&#35780;&#20272;SOTA&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;PA&#21327;&#35758;&#21482;&#32771;&#34385;&#26597;&#25214;&#23613;&#21487;&#33021;&#22810;&#30340;&#24322;&#24120;&#27573;&#65292;&#32780;PAdf&#21327;&#35758;&#21017;&#19981;&#20165;&#32771;&#34385;&#26597;&#25214;&#23613;&#21487;&#33021;&#22810;&#30340;&#27573;&#25968;&#65292;&#21516;&#26102;&#36824;&#32771;&#34385;&#24555;&#36895;&#26816;&#27979;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent algorithms of time-series anomaly detection have been evaluated by applying a Point Adjustment (PA) protocol. However, the PA protocol has a problem of overestimating the performance of the detection algorithms because it only depends on the number of detected abnormal segments and their size. We propose a novel evaluation protocol called the Point-Adjusted protocol with decay function (PAdf) to evaluate the time-series anomaly detection algorithm by reflecting the following ideal requirements: detect anomalies quickly and accurately without false alarms. This paper theoretically and experimentally shows that the PAdf protocol solves the over- and under-estimation problems of existing protocols such as PA and PA\%K. By conducting re-evaluations of SOTA models in benchmark datasets, we show that the PA protocol only focuses on finding many anomalous segments, whereas the score of the PAdf protocol considers not only finding many segments but also detecting anomalies quickly witho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35762;&#36848;&#20102;&#25968;&#25454;&#20559;&#24046;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12289;&#24433;&#21709;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;</title><link>http://arxiv.org/abs/2305.09686</link><description>&lt;p&gt;
&#25968;&#25454;&#20559;&#24046;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Data Bias Management. (arXiv:2305.09686v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35762;&#36848;&#20102;&#25968;&#25454;&#20559;&#24046;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;&#12289;&#24433;&#21709;&#21450;&#21487;&#33021;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#25968;&#25454;&#39537;&#21160;&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20559;&#24046;&#21644;&#20844;&#24179;&#31561;&#27010;&#24565;&#22312;&#31185;&#30740;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#65292;&#26080;&#35770;&#26159;&#22312;&#20135;&#19994;&#30028;&#36824;&#26159;&#23398;&#26415;&#30028;&#20013;&#65292;&#37117;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#36825;&#20123;&#38382;&#39064;&#36890;&#24120;&#28304;&#20110;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#31995;&#32479;&#30340;&#25968;&#25454;&#36136;&#37327;&#19981;&#21516;&#12290;&#38543;&#30528;&#36825;&#20123;&#31995;&#32479;&#34987;&#21830;&#19994;&#21270;&#21644;&#37096;&#32626;&#65292;&#26377;&#26102;&#34987;&#22996;&#25176;&#20570;&#20986;&#25913;&#21464;&#29983;&#27963;&#30340;&#20915;&#31574;&#65292;&#20154;&#20204;&#27491;&#22312;&#20570;&#20986;&#37325;&#22823;&#21162;&#21147;&#26469;&#30830;&#23450;&#21644;&#28040;&#38500;&#21487;&#33021;&#23548;&#33268;&#25968;&#25454;&#20559;&#24046;&#30340;&#26469;&#28304;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#30740;&#31350;&#32467;&#26524;&#65292;&#23637;&#31034;&#25968;&#25454;&#20559;&#35265;&#22914;&#20309;&#24433;&#21709;&#26368;&#32456;&#29992;&#25143;&#65292;&#20559;&#24046;&#30340;&#36215;&#28304;&#20197;&#21450;&#25105;&#20204;&#24212;&#35813;&#22914;&#20309;&#35299;&#20915;&#35813;&#38382;&#39064;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#19981;&#24517;&#22312;&#25152;&#26377;&#24773;&#20917;&#19979;&#28040;&#38500;&#25968;&#25454;&#20559;&#24046;&#65292;&#32780;&#26159;&#24212;&#23558;&#30740;&#31350;&#37325;&#28857;&#36716;&#21521;&#20559;&#35265;&#30340;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Due to the widespread use of data-powered systems in our everyday lives, concepts like bias and fairness gained significant attention among researchers and practitioners, in both industry and academia. Such issues typically emerge from the data, which comes with varying levels of quality, used to train supervised machine learning systems. With the commercialization and deployment of such systems that are sometimes delegated to make life-changing decisions, significant efforts are being made towards the identification and removal of possible sources of data bias that may resurface to the final end user or in the decisions being made. In this paper, we present research results that show how bias in data affects end users, where bias is originated, and provide a viewpoint about what we should do about it. We argue that data bias is not something that should necessarily be removed in all cases, and that research attention should instead shift from bias removal towards the identification, m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#36845;&#20195;&#31639;&#27861;&#30340;&#33030;&#24369;&#27700;&#21360;&#25216;&#26415;&#65292;&#33021;&#23558;&#26222;&#36890;&#35757;&#32451;&#26679;&#26412;&#36716;&#21270;&#20026;&#23545;&#27169;&#22411;&#26356;&#25935;&#24863;&#30340;&#33030;&#24369;&#26679;&#26412;&#65292;&#29992;&#20110;&#27169;&#22411;&#23436;&#25972;&#24615;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2305.09684</link><description>&lt;p&gt;
&#22522;&#20110;&#20915;&#31574;&#36845;&#20195;&#31639;&#27861;&#30340;&#33030;&#24369;&#27700;&#21360;&#25216;&#26415;&#29992;&#20110;&#27169;&#22411;&#23436;&#25972;&#24615;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
Decision-based iterative fragile watermarking for model integrity verification. (arXiv:2305.09684v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09684
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#36845;&#20195;&#31639;&#27861;&#30340;&#33030;&#24369;&#27700;&#21360;&#25216;&#26415;&#65292;&#33021;&#23558;&#26222;&#36890;&#35757;&#32451;&#26679;&#26412;&#36716;&#21270;&#20026;&#23545;&#27169;&#22411;&#26356;&#25935;&#24863;&#30340;&#33030;&#24369;&#26679;&#26412;&#65292;&#29992;&#20110;&#27169;&#22411;&#23436;&#25972;&#24615;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#20113;&#26381;&#21153;&#22120;&#30340;&#26381;&#21153;&#38656;&#27714;&#65292;&#22522;&#30784;&#27169;&#22411;&#36890;&#24120;&#37096;&#32626;&#22312;&#20113;&#26381;&#21153;&#22120;&#19978;&#12290;&#28982;&#32780;&#65292;&#36825;&#20063;&#23558;&#27169;&#22411;&#26292;&#38706;&#22312;&#23433;&#20840;&#39118;&#38505;&#20013;&#65292;&#25915;&#20987;&#32773;&#21487;&#20197;&#22312;&#19978;&#20256;&#25110;&#20256;&#36755;&#21040;&#20113;&#26381;&#21153;&#22120;&#21518;&#20462;&#25913;&#23427;&#20204;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#30340;&#12289;&#22522;&#20110;&#20915;&#31574;&#30340;&#33030;&#24369;&#24615;&#27700;&#21360;&#31639;&#27861;&#65292;&#23558;&#26222;&#36890;&#35757;&#32451;&#26679;&#26412;&#36716;&#21270;&#20026;&#23545;&#27169;&#22411;&#26356;&#25935;&#24863;&#30340;&#33030;&#24369;&#26679;&#26412;&#12290;&#22312;&#39564;&#35777;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#23558;&#21407;&#22987;&#27169;&#22411;&#30340;&#25935;&#24863;&#26679;&#26412;&#36755;&#20986;&#19982;&#34987;&#25915;&#20987;&#27169;&#22411;&#30340;&#36755;&#20986;&#36827;&#34892;&#27604;&#36739;&#65292;&#20197;&#35780;&#20272;&#27169;&#22411;&#30340;&#23436;&#25972;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#33030;&#24369;&#27700;&#21360;&#31639;&#27861;&#26159;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#65292;&#26088;&#22312;&#26368;&#23567;&#21270;&#23558;&#36716;&#25442;&#26679;&#26412;&#36755;&#20837;&#21040;&#30446;&#26631;&#27169;&#22411;&#26102;&#39044;&#27979;&#27010;&#29575;&#20998;&#24067;&#30340;&#26041;&#24046;&#12290;&#25105;&#20204;&#36890;&#36807;&#22810;&#27425;&#36845;&#20195;&#23558;&#26222;&#36890;&#26679;&#26412;&#36716;&#25442;&#20026;&#33030;&#24369;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19968;&#20123;&#20248;&#28857;&#65306;&#65288;1&#65289;&#20915;&#31574;&#40657;&#21283;&#23376;&#26041;&#24335;&#19979;&#30340;&#36845;&#20195;&#26356;&#26032;&#26679;&#26412;&#65307;&#65288;2&#65289;&#31639;&#27861;&#24212;&#29992;&#22312;&#27169;&#22411;&#23436;&#25972;&#24615;&#39564;&#35777;&#39046;&#22495;&#65292;&#32780;&#38750;&#21152;&#23494;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Typically, foundation models are hosted on cloud servers to meet the high demand for their services. However, this exposes them to security risks, as attackers can modify them after uploading to the cloud or transferring from a local system. To address this issue, we propose an iterative decision-based fragile watermarking algorithm that transforms normal training samples into fragile samples that are sensitive to model changes. We then compare the output of sensitive samples from the original model to that of the compromised model during validation to assess the model's completeness.The proposed fragile watermarking algorithm is an optimization problem that aims to minimize the variance of the predicted probability distribution outputed by the target model when fed with the converted sample.We convert normal samples to fragile samples through multiple iterations. Our method has some advantages: (1) the iterative update of samples is done in a decision-based black-box manner, relying s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;&#65292;&#20854;&#21487;&#22312;&#35782;&#21035;&#21644;&#20998;&#31867;&#21508;&#31181;&#31867;&#22411;&#30340;&#28431;&#27934;&#26041;&#38754;&#36798;&#21040;&#39640;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20256;&#32479;SAST&#21644;DAST&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09673</link><description>&lt;p&gt;
&#20351;&#29992;&#21452;&#38454;&#27573;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#28431;&#27934;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Vulnerability Detection Using Two-Stage Deep Learning Models. (arXiv:2305.09673v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09673
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#65292;&#37319;&#29992;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#29992;&#20110;&#28431;&#27934;&#26816;&#27979;&#65292;&#20854;&#21487;&#22312;&#35782;&#21035;&#21644;&#20998;&#31867;&#21508;&#31181;&#31867;&#22411;&#30340;&#28431;&#27934;&#26041;&#38754;&#36798;&#21040;&#39640;&#20934;&#30830;&#29575;&#65292;&#20248;&#20110;&#20256;&#32479;SAST&#21644;DAST&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24212;&#29992;&#31243;&#24207;&#23433;&#20840;&#26159;&#29616;&#20195;&#36719;&#20214;&#24320;&#21457;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#35768;&#22810;&#25915;&#20987;&#21462;&#20915;&#20110;&#36719;&#20214;&#20013;&#30340;&#28431;&#27934;&#12290;&#30001;&#20110;&#25216;&#26415;&#36827;&#27493;&#65292;&#25915;&#20987;&#25968;&#37327;&#27491;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#22686;&#21152;&#12290;&#20844;&#21496;&#24517;&#39035;&#22312;&#24320;&#21457;&#12289;&#27979;&#35797;&#21644;&#37096;&#32626;&#36719;&#20214;&#30340;&#27599;&#20010;&#38454;&#27573;&#20013;&#37117;&#21253;&#21547;&#23433;&#20840;&#21151;&#33021;&#65292;&#20197;&#38450;&#27490;&#25968;&#25454;&#27844;&#38706;&#12290;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#30340;&#26041;&#27861;&#26377;&#35768;&#22810;&#31181;&#65292;&#22914;&#38750;AI&#26041;&#27861;(&#22914;SAST&#21644;DAST)&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#22823;&#37327;&#30340;&#35823;&#25253;&#21644;&#28431;&#25253;&#12290;&#19982;&#27492;&#30456;&#23545;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#33268;&#21147;&#20110;&#24320;&#21457;&#22522;&#20110;AI&#30340;&#28431;&#27934;&#26816;&#27979;&#31995;&#32479;&#65292;&#37319;&#29992;&#20102;BERT&#12289;BLSTM&#31561;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#38454;&#27573;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;C/C++&#28304;&#20195;&#30721;&#30340;&#28431;&#27934;&#26816;&#27979;&#12290;&#31532;&#19968;&#38454;&#27573;&#26159;CNN&#65292;&#29992;&#20110;&#26816;&#27979;&#28304;&#20195;&#30721;&#26159;&#21542;&#21253;&#21547;&#20219;&#20309;&#28431;&#27934;(&#20108;&#20803;&#20998;&#31867;)&#65292;&#31532;&#20108;&#38454;&#27573;&#26159;&#22522;&#20110;LSTM&#30340;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#29305;&#23450;&#31867;&#22411;&#30340;&#28431;&#27934;(&#22810;&#31867;&#20998;&#31867;)&#12290;&#25152;&#25552;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#22312;&#26816;&#27979;&#21644;&#20998;&#31867;&#21508;&#31181;&#31867;&#22411;&#30340;&#28431;&#27934;&#26041;&#38754;&#23454;&#29616;&#20102;&#39640;&#31934;&#24230;&#29575;&#65292;&#20248;&#20110;&#20256;&#32479;&#30340;SAST&#21644;DAST&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Application security is an essential part of developing modern software, as lots of attacks depend on vulnerabilities in software. The number of attacks is increasing globally due to technological advancements. Companies must include security in every stage of developing, testing, and deploying their software in order to prevent data breaches. There are several methods to detect software vulnerability Non-AI-based such as Static Application Security Testing (SAST) and Dynamic Application Security Testing (DAST). However, these approaches have substantial false-positive and false-negative rates. On the other side, researchers have been interested in developing an AI-based vulnerability detection system employing deep learning models like BERT, BLSTM, etc. In this paper, we proposed a two-stage solution, two deep learning models were proposed for vulnerability detection in C/C++ source codes, the first stage is CNN which detects if the source code contains any vulnerability (binary class
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21644;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.09656</link><description>&lt;p&gt;
&#22768;&#26126;&#25552;&#31034;&#19979;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Satisfiability-Aided Language Models Using Declarative Prompting. (arXiv:2305.09656v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09656
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#21644;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28385;&#36275;&#24615;&#36741;&#21161;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#65292;&#29992;&#20110;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#19968;&#20010;&#22768;&#26126;&#24615;&#20219;&#21153;&#35268;&#33539;&#65292;&#24182;&#21033;&#29992;&#19968;&#20010;&#29616;&#25104;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#24471;&#20986;&#26368;&#32456;&#31572;&#26696;&#12290;&#35813;&#26041;&#27861;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#20248;&#28857;&#65306;&#31532;&#19968;&#65292;&#22768;&#26126;&#24615;&#35268;&#33539;&#27604;&#25512;&#29702;&#27493;&#39588;&#26356;&#25509;&#36817;&#38382;&#39064;&#25551;&#36848;&#65292;&#22240;&#27492;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35299;&#26512;&#23427;&#65307;&#31532;&#20108;&#65292;&#36890;&#36807;&#23558;&#23454;&#38469;&#25512;&#29702;&#20219;&#21153;&#22996;&#25176;&#32473;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#20445;&#35777;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prior work has combined chain-of-thought prompting in large language models (LLMs) with programmatic representations to perform effective and transparent reasoning. While such an approach works very well for tasks that only require forward reasoning (e.g., straightforward arithmetic), it is less effective for constraint solving tasks that require more sophisticated planning and search. In this paper, we propose a new satisfiability-aided language modeling approach for improving the reasoning capabilities of LLMs. We use an LLM to generate a declarative task specification rather than an imperative program and leverage an off-the-shelf automated theorem prover to derive the final answer. This approach has two key advantages. The declarative specification is closer to the problem description than the reasoning steps are, so the LLM can parse it more accurately. Furthermore, by offloading the actual reasoning task to an automated theorem prover, our approach can guarantee the correctness o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.09480</link><description>&lt;p&gt;
&#20132;&#21449;&#38376;&#25511;&#22810;&#23618;&#24863;&#30693;&#26426;&#19979;&#30340;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#26159;&#19968;&#31181;&#19968;&#27425;&#24615;&#25239;&#20307;&#35774;&#35745;&#22120;
&lt;/p&gt;
&lt;p&gt;
Protein Complex Invariant Embedding with Cross-Gate MLP is A One-Shot Antibody Designer. (arXiv:2305.09480v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09480
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;&#25239;&#20307;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#65292;&#35299;&#20915;&#20960;&#20309;&#24314;&#27169;&#21644;&#20302;&#25928;&#25512;&#26029;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25239;&#20307;&#26159;&#30001;&#20813;&#30123;&#31995;&#32479;&#20135;&#29983;&#30340;&#38024;&#23545;&#22806;&#26469;&#29289;&#36136;&#25110;&#25239;&#21407;&#30340;&#37325;&#35201;&#34507;&#30333;&#36136;&#12290;&#25239;&#20307;&#30340;&#29305;&#24322;&#24615;&#30001;&#20854;&#20114;&#34917;&#20915;&#23450;&#21306;&#65288;CDR&#65289;&#20915;&#23450;&#65292;CDR&#20301;&#20110;&#25239;&#20307;&#38142;&#30340;&#21487;&#21464;&#21306;&#22495;&#20013;&#65292;&#24418;&#25104;&#19982;&#25239;&#21407;&#32467;&#21512;&#30340;&#20301;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#21033;&#29992;&#22797;&#26434;&#30340;&#25216;&#26415;&#29983;&#25104;CDR&#65292;&#20294;&#23427;&#20204;&#36973;&#21463;&#20102;&#20960;&#20309;&#24314;&#27169;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#24120;&#35265;&#30340;&#36845;&#20195;&#31934;&#21270;&#31574;&#30053;&#23548;&#33268;&#20102;&#20302;&#25928;&#30340;&#25512;&#26029;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#29983;&#25104;&#27169;&#22411;&#65292;&#21487;&#20197;&#19968;&#27425;&#24615;&#22320;&#20849;&#21516;&#35774;&#35745;CDR&#30340;1D&#24207;&#21015;&#21644;3D&#32467;&#26500;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23558;&#25239;&#20307;CDR&#35774;&#35745;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#65288;i&#65289;&#34507;&#30333;&#36136;&#32467;&#26500;&#30340;&#20960;&#20309;&#24314;&#27169;&#21644;&#65288;ii&#65289;&#24207;&#21015;&#32467;&#26500;&#20849;&#23398;&#20064;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#34507;&#30333;&#36136;&#22797;&#21512;&#29289;&#19981;&#21464;&#23884;&#20837;&#65292;&#21487;&#25429;&#25417;&#34507;&#30333;&#36136;&#39592;&#26550;&#21407;&#23376;&#65288;&#21253;&#25324;C&#945;&#12289;N&#12289;C&#21644;O&#21407;&#23376;&#65289;&#20043;&#38388;&#30340;&#20869;&#37096;&#21644;&#22806;&#37096;&#32452;&#20998;&#30456;&#20114;&#20316;&#29992;&#65292;&#20197;&#23454;&#29616;&#20840;&#38754;&#30340;&#20960;&#20309;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Antibodies are crucial proteins produced by the immune system in response to foreign substances or antigens. The specificity of an antibody is determined by its complementarity-determining regions (CDRs), which are located in the variable domains of the antibody chains and form the antigen-binding site. Previous studies have utilized complex techniques to generate CDRs, but they suffer from inadequate geometric modeling. Moreover, the common iterative refinement strategies lead to an inefficient inference. In this paper, we propose a deep generative model that can co-design 1D sequences and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple the antibody CDR design into two stages: (i) geometric modeling of protein structures and (ii) sequence-structure co-learning. We develop a protein complex invariant embedding that captures both intra- and inter-component interactions among the backbone atoms including C$\alpha$, N, C, and O atoms to achieve comprehensive geome
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#22810;&#36890;&#36947;&#30005;&#38459;&#25239;&#34880;&#27969;&#21160;&#21147;&#23398;&#30417;&#27979;&#20013;&#24515;&#32954;&#23481;&#31215;&#20449;&#21495;&#65288;CVS&#65289;&#30340;&#36816;&#21160;&#35825;&#23548;&#21487;&#38752;&#24615;&#38477;&#20302;&#12290;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#32534;&#30721;&#22120; - &#35299;&#30721;&#22120;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21040;&#26242;&#24577; CVS &#24207;&#21015;&#20013;&#23384;&#22312;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#36816;&#21160;&#24433;&#21709;&#20197;&#21450;&#32570;&#20047;&#22312; CVS &#38543;&#26102;&#38388;&#20986;&#29616;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36816;&#21160;&#24341;&#36215;&#24322;&#24120;&#30340;&#26174;&#24335;&#26426;&#21046;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.09368</link><description>&lt;p&gt;
&#22810;&#36890;&#36947;&#30005;&#38459;&#25239;&#34880;&#27969;&#21160;&#21147;&#23398;&#30417;&#27979;&#20013;&#30340;&#33258;&#21160;&#20449;&#21495;&#36136;&#37327;&#35780;&#20272;&#30340;&#26080;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Unsupervised sequence-to-sequence learning for automatic signal quality assessment in multi-channel electrical impedance-based hemodynamic monitoring. (arXiv:2305.09368v1 [eess.SP])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09368
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#22810;&#36890;&#36947;&#30005;&#38459;&#25239;&#34880;&#27969;&#21160;&#21147;&#23398;&#30417;&#27979;&#20013;&#24515;&#32954;&#23481;&#31215;&#20449;&#21495;&#65288;CVS&#65289;&#30340;&#36816;&#21160;&#35825;&#23548;&#21487;&#38752;&#24615;&#38477;&#20302;&#12290;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#32534;&#30721;&#22120; - &#35299;&#30721;&#22120;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21040;&#26242;&#24577; CVS &#24207;&#21015;&#20013;&#23384;&#22312;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#26412;&#26041;&#27861;&#19981;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#36816;&#21160;&#24433;&#21709;&#20197;&#21450;&#32570;&#20047;&#22312; CVS &#38543;&#26102;&#38388;&#20986;&#29616;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36816;&#21160;&#24341;&#36215;&#24322;&#24120;&#30340;&#26174;&#24335;&#26426;&#21046;&#65292;&#20855;&#26377;&#31454;&#20105;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#30417;&#30563;&#30340;&#24207;&#21015;&#21040;&#24207;&#21015;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#35780;&#20272;&#22810;&#36890;&#36947;&#30005;&#38459;&#25239;&#34880;&#27969;&#21160;&#21147;&#23398;&#30417;&#27979;&#20013;&#24515;&#32954;&#23481;&#31215;&#20449;&#21495;&#65288;CVS&#65289;&#30340;&#36816;&#21160;&#35825;&#23548;&#21487;&#38752;&#24615;&#38477;&#20302;&#12290;&#35813;&#26041;&#27861;&#35797;&#22270;&#35299;&#20915;&#29616;&#26377;&#23398;&#20064;&#22411;&#35780;&#20272;&#26041;&#27861;&#30340;&#32570;&#28857;&#65292;&#20363;&#22914;&#38656;&#35201;&#25163;&#21160;&#27880;&#37322;&#36816;&#21160;&#24433;&#21709;&#20197;&#21450;&#32570;&#20047;&#22312; CVS &#38543;&#26102;&#38388;&#20986;&#29616;&#19978;&#19979;&#25991;&#21464;&#21270;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#36816;&#21160;&#24341;&#36215;&#24322;&#24120;&#30340;&#26174;&#24335;&#26426;&#21046;&#12290;&#36890;&#36807;&#21033;&#29992;&#38271;&#30701;&#26102;&#35760;&#24518;&#21644;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#32467;&#26500;&#65292;&#32534;&#30721;&#22120; - &#35299;&#30721;&#22120;&#27169;&#22411;&#19981;&#20165;&#34987;&#35757;&#32451;&#25104;&#33258;&#25105;&#37325;&#29616; CVS &#30340;&#36755;&#20837;&#24207;&#21015;&#65292;&#32780;&#19988;&#36824;&#34987;&#35757;&#32451;&#25104;&#20197;&#24179;&#34892;&#26041;&#24335;&#22806;&#25512;&#26410;&#26469;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#27169;&#22411;&#21487;&#20197;&#25429;&#25417;&#21040;&#26242;&#24577; CVS &#24207;&#21015;&#20013;&#23384;&#22312;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#21516;&#26102;&#34987;&#35268;&#33539;&#21270;&#20197;&#25506;&#32034;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#30340;&#19968;&#33324;&#20851;&#31995;&#12290;&#26681;&#25454;&#23454;&#38469;&#21644;&#39044;&#27979; CVS &#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#26816;&#27979;&#20986;&#20302;&#36136;&#37327;&#30340;&#21463;&#36816;&#21160;&#24433;&#21709;&#30340; CVS&#12290;&#22810;&#36890;&#36947; EIT &#22522;&#24515;&#34880;&#31649;&#30417;&#27979;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#23454;&#39564;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#36816;&#21160;&#24341;&#36215;&#30340;&#27979;&#37327;&#36136;&#37327;&#35780;&#20272;&#26041;&#27861;&#30340;&#31454;&#20105;&#24615;&#33021;&#19978;&#20855;&#26377;&#31454;&#20105;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
This study proposes an unsupervised sequence-to-sequence learning approach that automatically assesses the motion-induced reliability degradation of the cardiac volume signal (CVS) in multi-channel electrical impedance-based hemodynamic monitoring. The proposed method attempts to tackle shortcomings in existing learning-based assessment approaches, such as the requirement of manual annotation for motion influence and the lack of explicit mechanisms for realizing motion-induced abnormalities under contextual variations in CVS over time. By utilizing long-short term memory and variational auto-encoder structures, an encoder--decoder model is trained not only to self-reproduce an input sequence of the CVS but also to extrapolate the future in a parallel fashion. By doing so, the model can capture contextual knowledge lying in a temporal CVS sequence while being regularized to explore a general relationship over the entire time-series. A motion-influenced CVS of low-quality is detected, ba
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#65292;&#24182;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.09329</link><description>&lt;p&gt;
BERTTM: &#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#36827;&#34892;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
BERTTM: Leveraging Contextualized Word Embeddings from Pre-trained Language Models for Neural Topic Modeling. (arXiv:2305.09329v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#21033;&#29992;&#26469;&#33258;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#65292;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#65292;&#24182;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36817;&#24180;&#26469;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#20027;&#39064;&#24314;&#27169;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20013;&#25198;&#28436;&#30528;&#26085;&#30410;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20027;&#39064;&#27169;&#22411;&#20173;&#28982;&#20381;&#36182;&#20110;&#35789;&#34955;&#65288;BoW&#65289;&#20449;&#24687;&#65292;&#26080;&#35770;&#26159;&#20316;&#20026;&#35757;&#32451;&#36755;&#20837;&#36824;&#26159;&#35757;&#32451;&#30446;&#26631;&#12290;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#25429;&#25417;&#25991;&#26723;&#20013;&#30340;&#21333;&#35789;&#39034;&#24207;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#24182;&#23548;&#33268;&#23427;&#20204;&#22312;&#22788;&#29702;&#26032;&#25991;&#26723;&#20013;&#30340;&#26410;&#35266;&#23519;&#21040;&#30340;&#21333;&#35789;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#21521;&#37327;&#22312;&#35789;&#20041;&#28040;&#27495;&#30340;&#33021;&#21147;&#19978;&#34920;&#29616;&#20248;&#36234;&#65292;&#24182;&#35777;&#26126;&#20102;&#23427;&#20204;&#22312;&#22788;&#29702;OOV&#21333;&#35789;&#26102;&#26159;&#26377;&#25928;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;BERT&#30340;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#12290;&#35813;&#27169;&#22411;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20219;&#20309;BoW&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#25512;&#26029;&#20986;&#25991;&#26723;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#27492;&#22806;&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#30452;&#25509;&#20174;&#19978;&#19979;&#25991;&#21270;&#35789;&#23884;&#20837;&#20013;&#25512;&#26029;&#20986;&#25991;&#26723;&#20013;&#27599;&#20010;&#21333;&#35789;&#30340;&#20027;&#39064;&#20998;&#24067;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20165;&#20381;&#36182;BoW&#34920;&#31034;&#21644;&#20854;&#20182;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the development of neural topic models in recent years, topic modelling is playing an increasingly important role in natural language understanding. However, most existing topic models still rely on bag-of-words (BoW) information, either as training input or training target. This limits their ability to capture word order information in documents and causes them to suffer from the out-of-vocabulary (OOV) issue, i.e. they cannot handle unobserved words in new documents. Contextualized word embeddings from pre-trained language models show superiority in the ability of word sense disambiguation and prove to be effective in dealing with OOV words. In this work, we developed a novel neural topic model combining contextualized word embeddings from the pre-trained language model BERT. The model can infer the topic distribution of a document without using any BoW information. In addition, the model can infer the topic distribution of each word in a document directly from the contextualize
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#32771;&#34385;&#23569;&#37327;&#25968;&#25454;&#20043;&#38388;&#30340;&#31867;&#20869;&#36317;&#31163;&#21644;&#31867;&#38388;&#36317;&#31163;&#26469;&#32771;&#34385;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#22686;&#24378;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#23454;&#29616;&#20248;&#31168;&#30340;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.09062</link><description>&lt;p&gt;
SuSana Distance&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#65306;&#36890;&#36807;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;&#23454;&#29616;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#29992;&#20110;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SuSana Distancia is all you need: Enforcing class separability in metric learning via two novel distance-based loss functions for few-shot image classification. (arXiv:2305.09062v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09062
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#22522;&#20110;&#36317;&#31163;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#32771;&#34385;&#23569;&#37327;&#25968;&#25454;&#20043;&#38388;&#30340;&#31867;&#20869;&#36317;&#31163;&#21644;&#31867;&#38388;&#36317;&#31163;&#26469;&#32771;&#34385;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#22686;&#24378;&#24230;&#37327;&#23398;&#20064;&#20013;&#30340;&#31867;&#21035;&#21487;&#20998;&#24615;&#65292;&#23454;&#29616;&#20248;&#31168;&#30340;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30740;&#31350;&#39046;&#22495;&#65292;&#26088;&#22312;&#20165;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#26679;&#26412;&#23398;&#20064;&#26032;&#30340;&#27010;&#24565;&#12290;&#22522;&#20110;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#36817;&#24037;&#20316;&#21033;&#29992;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#20854;&#20013;&#21253;&#25324;&#25903;&#25345;&#65288;&#35757;&#32451;&#65289;&#21644;&#26597;&#35810;&#38598;&#65288;&#27979;&#35797;&#65289;&#65292;&#26088;&#22312;&#23398;&#20064;&#36825;&#20123;&#38598;&#21512;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#27604;&#36739;&#24230;&#37327;&#12290;&#30001;&#20110;&#25968;&#25454;&#32570;&#20047;&#65292;&#23884;&#20837;&#32593;&#32476;&#30340;&#23398;&#20064;&#36807;&#31243;&#25104;&#20026;&#23569;&#26679;&#26412;&#20219;&#21153;&#30340;&#37325;&#35201;&#37096;&#20998;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#20351;&#29992;&#24230;&#37327;&#23398;&#20064;&#26041;&#27861;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#24213;&#23618;&#28508;&#22312;&#31354;&#38388;&#30340;&#23646;&#24615;&#21644;&#31867;&#21035;&#20043;&#38388;&#30340;&#21487;&#20998;&#24615;&#24182;&#26410;&#23436;&#20840;&#24378;&#21046;&#25191;&#34892;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#32771;&#34385;&#23569;&#37327;&#25968;&#25454;&#20043;&#38388;&#30340;&#31867;&#20869;&#36317;&#31163;&#21644;&#31867;&#38388;&#36317;&#31163;&#26469;&#32771;&#34385;&#23884;&#20837;&#21521;&#37327;&#30340;&#37325;&#35201;&#24615;&#12290;&#31532;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#26159;Proto-Triplet Loss&#65292;&#23427;&#22522;&#20110;&#21407;&#22987;&#19977;&#20803;&#32452;&#25439;&#22833;&#65292;&#24182;&#28155;&#21152;&#20102;&#21407;&#22411;&#21521;&#37327;&#12290;&#31532;&#20108;&#20010;&#25439;&#22833;&#20989;&#25968;&#26159;SuSana Distance Loss&#65292;&#23427;&#32771;&#34385;&#20102;&#21516;&#31867;&#26679;&#26412;&#21644;&#19981;&#21516;&#31867;&#26679;&#26412;&#20043;&#38388;&#30340;&#36317;&#31163;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23569;&#26679;&#26412;&#22270;&#20687;&#20998;&#31867;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#29616;&#31867;&#21035;&#21487;&#20998;&#24615;&#21644;&#36798;&#21040;&#26368;&#20808;&#36827;&#24615;&#33021;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot learning is a challenging area of research that aims to learn new concepts with only a few labeled samples of data. Recent works based on metric-learning approaches leverage the meta-learning approach, which is encompassed by episodic tasks that make use a support (training) and query set (test) with the objective of learning a similarity comparison metric between those sets. Due to the lack of data, the learning process of the embedding network becomes an important part of the few-shot task. Previous works have addressed this problem using metric learning approaches, but the properties of the underlying latent space and the separability of the difference classes on it was not entirely enforced. In this work, we propose two different loss functions which consider the importance of the embedding vectors by looking at the intra-class and inter-class distance between the few data. The first loss function is the Proto-Triplet Loss, which is based on the original triplet loss with 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HumanMotionQA&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20154;&#20307;&#36816;&#21160;&#24207;&#21015;&#19978;&#36827;&#34892;&#22797;&#26434;&#12289;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;NSPose&#26041;&#27861;&#65292;&#21033;&#29992;&#31526;&#21495;&#21270;&#25512;&#29702;&#21644;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#20013;&#65292;&#24182;&#36229;&#36807;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.08953</link><description>&lt;p&gt;
&#22522;&#20110;&#27169;&#22359;&#21270;&#21160;&#20316;&#31243;&#24207;&#30340;&#21160;&#20316;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Motion Question Answering via Modular Motion Programs. (arXiv:2305.08953v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08953
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;HumanMotionQA&#20219;&#21153;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#22312;&#22797;&#26434;&#30340;&#20154;&#20307;&#36816;&#21160;&#24207;&#21015;&#19978;&#36827;&#34892;&#22797;&#26434;&#12289;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;NSPose&#26041;&#27861;&#65292;&#21033;&#29992;&#31526;&#21495;&#21270;&#25512;&#29702;&#21644;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35813;&#20219;&#21153;&#20013;&#65292;&#24182;&#36229;&#36807;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#24863;&#30693;&#21644;&#29702;&#35299;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#39318;&#20808;&#35774;&#35745;&#27169;&#22411;&#65292;&#23545;&#21160;&#20316;&#24207;&#21015;&#36827;&#34892;&#22797;&#26434;&#30340;&#26102;&#31354;&#25512;&#29702;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;HumanMotionQA&#20219;&#21153;&#65292;&#35780;&#20272;&#27169;&#22411;&#22312;&#38271;&#26102;&#38388;&#20154;&#31867;&#36816;&#21160;&#24207;&#21015;&#19978;&#36827;&#34892;&#22797;&#26434;&#12289;&#22810;&#27493;&#25512;&#29702;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#29983;&#25104;&#20102;&#19968;&#20010;&#38382;&#31572;&#23545;&#25968;&#25454;&#38598;&#65292;&#38656;&#35201;&#22312;&#21160;&#20316;&#24207;&#21015;&#30340;&#23567;&#37096;&#20998;&#20013;&#26816;&#27979;&#36816;&#21160;&#32447;&#32034;&#65292;&#23545;&#20107;&#20214;&#21457;&#29983;&#30340;&#26102;&#38388;&#36827;&#34892;&#25512;&#29702;&#65292;&#24182;&#26597;&#35810;&#29305;&#23450;&#30340;&#36816;&#21160;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;NSPose&#65292;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#26041;&#27861;&#65292;&#29992;&#20110;&#22788;&#29702;&#35813;&#20219;&#21153;&#65292;&#23427;&#21033;&#29992;&#31526;&#21495;&#21270;&#25512;&#29702;&#21644;&#27169;&#22359;&#21270;&#35774;&#35745;&#65292;&#36890;&#36807;&#23398;&#20064;&#36816;&#21160;&#27010;&#24565;&#12289;&#23646;&#24615;&#31070;&#32463;&#25805;&#20316;&#31526;&#21644;&#26102;&#38388;&#20851;&#31995;&#65292;&#26469;&#22788;&#29702;&#21160;&#20316;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;NSPose&#22312;HumanMotionQA&#20219;&#21153;&#20013;&#30340;&#36866;&#29992;&#24615;&#65292;&#32988;&#36807;&#20102;&#25152;&#26377;&#22522;&#32447;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In order to build artificial intelligence systems that can perceive and reason with human behavior in the real world, we must first design models that conduct complex spatio-temporal reasoning over motion sequences. Moving towards this goal, we propose the HumanMotionQA task to evaluate complex, multi-step reasoning abilities of models on long-form human motion sequences. We generate a dataset of question-answer pairs that require detecting motor cues in small portions of motion sequences, reasoning temporally about when events occur, and querying specific motion attributes. In addition, we propose NSPose, a neuro-symbolic method for this task that uses symbolic reasoning and a modular design to ground motion through learning motion concepts, attribute neural operators, and temporal relations. We demonstrate the suitability of NSPose for the HumanMotionQA task, outperforming all baseline methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#21644;&#25512;&#29702;&#20808;&#21069;&#21644;&#23398;&#20064;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#25506;&#32034;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2305.08876</link><description>&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#21450;&#20854;&#20998;&#31867;&#27861;&#65306;&#19968;&#39033;&#35843;&#26597;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI and its Taxonomy: a survey. (arXiv:2305.08876v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#30740;&#31350;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#30740;&#31350;&#65292;&#25506;&#32034;&#20102;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#21644;&#25512;&#29702;&#20808;&#21069;&#21644;&#23398;&#20064;&#30693;&#35782;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#20197;&#25506;&#32034;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#24615;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#28041;&#21450;&#32452;&#21512;&#31526;&#21495;&#22788;&#29702;&#65288;&#22914;&#32463;&#20856;&#20154;&#24037;&#26234;&#33021;&#65289;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#27169;&#22411;&#65292;&#26159;&#19968;&#20010;&#38750;&#24120;&#25104;&#29087;&#30340;&#39046;&#22495;&#12290;&#36825;&#20123;&#27169;&#22411;&#20316;&#20026;&#23454;&#29616;&#20154;&#24037;&#26234;&#33021;&#36890;&#29992;&#24615;&#30340;&#19968;&#31181;&#23581;&#35797;&#65292;&#22312;&#25506;&#32034;&#38500;&#20102;&#22686;&#21152;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#23610;&#23544;&#20197;&#22806;&#30340;&#26367;&#20195;&#26041;&#26696;&#20197;&#21450;&#23558;&#23398;&#20064;&#25968;&#25454;&#20998;&#24067;&#21644;&#25512;&#29702;&#20808;&#21069;&#21644;&#23398;&#20064;&#30693;&#35782;&#30456;&#32467;&#21512;&#26041;&#38754;&#20855;&#26377;&#29420;&#29305;&#20316;&#29992;&#12290;&#26412;&#27425;&#35843;&#26597;&#30740;&#31350;&#20102;&#36825;&#19968;&#39046;&#22495;&#36817;&#24180;&#26469;&#30340;&#30740;&#31350;&#35770;&#25991;&#65292;&#24182;&#25552;&#20379;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#20998;&#31867;&#21644;&#27604;&#36739;&#65292;&#21516;&#26102;&#20171;&#32461;&#20102;&#24212;&#29992;&#26696;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic AI deals with models that combine symbolic processing, like classic AI, and neural networks, as it's a very established area. These models are emerging as an effort toward Artificial General Intelligence (AGI) by both exploring an alternative to just increasing datasets' and models' sizes and combining Learning over the data distribution, Reasoning on prior and learned knowledge, and by symbiotically using them. This survey investigates research papers in this area during recent years and brings classification and comparison between the presented models as well as applications.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#30446;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24635;&#32467;&#20102;&#37096;&#32626;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#20173;&#38656;&#20811;&#26381;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2305.07348</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#30446;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#32508;&#36848;&#65306;&#24403;&#21069;&#29366;&#24577;&#12289;&#38480;&#21046;&#21644;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
A Survey on Deep Learning-Based Monocular Spacecraft Pose Estimation: Current State, Limitations and Prospects. (arXiv:2305.07348v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07348
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35780;&#20272;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#21333;&#30446;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#30340;&#29616;&#26377;&#26041;&#27861;&#65292;&#24635;&#32467;&#20102;&#37096;&#32626;&#35813;&#26041;&#27861;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#20173;&#38656;&#20811;&#26381;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20272;&#31639;&#19981;&#37197;&#21512;&#30340;&#33322;&#22825;&#22120;&#23039;&#24577;&#26159;&#19968;&#39033;&#37325;&#35201;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#38382;&#39064;&#65292;&#21487;&#20197;&#23454;&#29616;&#22312;&#36712;&#33258;&#21160;&#35270;&#35273;&#31995;&#32479;&#30340;&#37096;&#32626;&#65292;&#20854;&#24212;&#29992;&#33539;&#22260;&#20174;&#36712;&#36947;&#32500;&#20462;&#21040;&#22826;&#31354;&#30862;&#29255;&#28165;&#38500;&#12290;&#38543;&#30528;&#35745;&#31639;&#26426;&#35270;&#35273;&#30340;&#36235;&#21183;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#24037;&#20316;&#22312;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#26041;&#27861;&#26469;&#35299;&#20915;&#27492;&#38382;&#39064;&#12290;&#20294;&#26159;&#23613;&#31649;&#22312;&#30740;&#31350;&#38454;&#27573;&#33719;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20173;&#23384;&#22312;&#38459;&#27490;&#36825;&#31181;&#26041;&#27861;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#20351;&#29992;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#29305;&#21035;&#26159;&#65292;&#37096;&#32626;&#36825;&#31181;&#35745;&#31639;&#23494;&#38598;&#22411;&#31639;&#27861;&#20173;&#28982;&#21463;&#21040;&#23569;&#37327;&#30740;&#31350;&#65292;&#32780;&#22312;&#21512;&#25104;&#22270;&#20687;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#22312;&#30495;&#23454;&#22270;&#20687;&#19978;&#36827;&#34892;&#27979;&#35797;&#26102;&#24615;&#33021;&#19979;&#38477;&#20173;&#28982;&#38656;&#35201;&#20943;&#36731;&#12290;&#26412;&#25991;&#20027;&#35201;&#30446;&#30340;&#26159;&#20840;&#38754;&#25551;&#36848;&#24403;&#21069;&#22522;&#20110;DL&#30340;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#26041;&#27861;&#65292;&#36741;&#21161;&#30830;&#23450;&#26377;&#25928;&#37096;&#32626;DL&#30340;&#33322;&#22825;&#22120;&#23039;&#24577;&#20272;&#35745;&#22312;&#29616;&#23454;&#20219;&#21153;&#20013;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Estimating the pose of an uncooperative spacecraft is an important computer vision problem for enabling the deployment of automatic vision-based systems in orbit, with applications ranging from on-orbit servicing to space debris removal. Following the general trend in computer vision, more and more works have been focusing on leveraging Deep Learning (DL) methods to address this problem. However and despite promising research-stage results, major challenges preventing the use of such methods in real-life missions still stand in the way. In particular, the deployment of such computation-intensive algorithms is still under-investigated, while the performance drop when training on synthetic and testing on real images remains to mitigate. The primary goal of this survey is to describe the current DL-based methods for spacecraft pose estimation in a comprehensive manner. The secondary goal is to help define the limitations towards the effective deployment of DL-based spacecraft pose estimat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06360</link><description>&lt;p&gt;
&#25506;&#32034;&#26426;&#22120;&#36951;&#24536;&#30340;&#39046;&#22495;&#65306;&#19968;&#31687;&#32508;&#36848;&#19982;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Exploring the Landscape of Machine Unlearning: A Survey and Taxonomy. (arXiv:2305.06360v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06360
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#29616;&#29366;&#21644;&#25216;&#26415;&#24212;&#29992;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#65292;&#35752;&#35770;&#20102;MU&#22312;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#31561;&#39046;&#22495;&#30340;&#28508;&#22312;&#30410;&#22788;&#65292;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#36951;&#24536;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#22240;&#20026;&#38656;&#35201;&#21024;&#38500;&#25110;&#20462;&#25913;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25152;&#20570;&#20986;&#30340;&#39044;&#27979;&#12290;&#34429;&#28982;&#35757;&#32451;&#27169;&#22411;&#21464;&#24471;&#26356;&#21152;&#26377;&#25928;&#21644;&#20934;&#30830;&#65292;&#20294;&#22312;&#26576;&#20123;&#39046;&#22495;&#65288;&#22914;&#38544;&#31169;&#12289;&#23433;&#20840;&#21644;&#20844;&#27491;&#24615;&#65289;&#65292;&#36951;&#24536;&#20808;&#21069;&#23398;&#21040;&#30340;&#20449;&#24687;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#36234;&#26469;&#36234;&#26174;&#33879;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#26426;&#22120;&#36951;&#24536;&#30340;&#32508;&#36848;&#65292;&#28085;&#30422;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#25968;&#25454;&#21024;&#38500;&#12289;&#25200;&#21160;&#21644;&#27169;&#22411;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25991;&#20013;&#36824;&#20171;&#32461;&#20102;&#24120;&#29992;&#30340;&#24230;&#37327;&#26631;&#20934;&#21644;&#25968;&#25454;&#38598;&#12290;&#25991;&#31456;&#36824;&#24378;&#35843;&#20102;&#38656;&#35201;&#35299;&#20915;&#30340;&#25361;&#25112;&#65292;&#21253;&#25324;&#25915;&#20987;&#22797;&#26434;&#24615;&#12289;&#26631;&#20934;&#21270;&#12289;&#21487;&#36716;&#31227;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#12289;&#35757;&#32451;&#25968;&#25454;&#21644;&#36164;&#28304;&#38480;&#21046;&#12290;&#26412;&#25991;&#30340;&#36129;&#29486;&#21253;&#25324;&#35752;&#35770;MU&#30340;&#28508;&#22312;&#30410;&#22788;&#20197;&#21450;&#23427;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine unlearning (MU) is a field that is gaining increasing attention due to the need to remove or modify predictions made by machine learning (ML) models. While training models have become more efficient and accurate, the importance of unlearning previously learned information has become increasingly significant in fields such as privacy, security, and fairness. This paper presents a comprehensive survey of MU, covering current state-of-the-art techniques and approaches, including data deletion, perturbation, and model updates. In addition, commonly used metrics and datasets are also presented. The paper also highlights the challenges that need to be addressed, including attack sophistication, standardization, transferability, interpretability, training data, and resource constraints. The contributions of this paper include discussions about the potential benefits of MU and its future directions in Natural Language Processing, Computer vision, and Recommender Systems. Additionally, 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29702;&#35770;&#23618;&#38754;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#24120;&#35268;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.06137</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A proof of convergence of inverse reinforcement learning for multi-objective optimization. (arXiv:2305.06137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29702;&#35770;&#23618;&#38754;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#24120;&#35268;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#31561;&#25928;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;WIRL&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;&#19982;&#25237;&#24433;&#27425;&#26799;&#24230;&#27861;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;WIRL&#65289;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;&#26368;&#22823;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#65292;&#23548;&#24341;&#25104;&#26412;&#23398;&#20064;&#65289;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show the convergence of Wasserstein inverse reinforcement learning (WIRL) for multi-objective optimizations with the projective subgradient method by formulating an inverse problem of the optimization problem that is equivalent to WIRL for multi-objective optimizations.  In addition, we prove convergence of inverse reinforcement learning (maximum entropy inverse reinforcement learning, guid cost learning) for multi-objective optimization with the projective subgradient method.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#24182;&#21487;&#33021;&#25918;&#22823;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;</title><link>http://arxiv.org/abs/2305.03355</link><description>&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#32508;&#21512;&#30740;&#31350;&#65306;&#24615;&#33021;&#12289;&#38544;&#31169;&#12289;&#40065;&#26834;&#24615;&#20197;&#21450;&#20844;&#24179;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Comprehensive Study on Dataset Distillation: Performance, Privacy, Robustness and Fairness. (arXiv:2305.03355v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03355
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20854;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#24182;&#21487;&#33021;&#25918;&#22823;&#27169;&#22411;&#30340;&#19981;&#20844;&#24179;&#24615;&#65292;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#38598;&#21387;&#32553;&#26088;&#22312;&#23558;&#21407;&#22987;&#25968;&#25454;&#38598;&#30340;&#20016;&#23500;&#29305;&#24449;&#32534;&#30721;&#25104;&#23567;&#22411;&#25968;&#25454;&#38598;&#65292;&#26159;&#19968;&#31181;&#21152;&#36895;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#21644;&#30456;&#20851;&#30740;&#31350;&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#21387;&#32553;&#22270;&#20687;&#30340;&#20449;&#24687;&#24615;&#21644;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36824;&#27809;&#26377;&#20174;&#23433;&#20840;&#24615;&#35282;&#24230;&#20840;&#38754;&#20998;&#26512;&#36825;&#19968;&#25216;&#26415;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#23545;&#28508;&#22312;&#39118;&#38505;&#32570;&#20047;&#31995;&#32479;&#29702;&#35299;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#35780;&#20272;&#20102;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#12290;&#25105;&#20204;&#25104;&#21151;&#20351;&#29992;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#26469;&#26174;&#31034;&#20173;&#28982;&#23384;&#22312;&#38544;&#31169;&#39118;&#38505;&#12290;&#26412;&#25991;&#36824;&#34920;&#26126;&#65292;&#25968;&#25454;&#38598;&#21387;&#32553;&#22312;&#27169;&#22411;&#40065;&#26834;&#24615;&#26041;&#38754;&#21487;&#33021;&#20250;&#20135;&#29983;&#19981;&#21516;&#31243;&#24230;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#25918;&#22823;&#31867;&#21035;&#38388;&#30340;&#27169;&#22411;&#19981;&#20844;&#24179;&#24615;&#12290;&#26412;&#30740;&#31350;&#20026;&#25968;&#25454;&#38598;&#21387;&#32553;&#35780;&#20272;&#25552;&#20379;&#20102;&#22823;&#35268;&#27169;&#30340;&#22522;&#20934;&#27979;&#35797;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of dataset distillation is to encode the rich features of an original dataset into a tiny dataset. It is a promising approach to accelerate neural network training and related studies. Different approaches have been proposed to improve the informativeness and generalization performance of distilled images. However, no work has comprehensively analyzed this technique from a security perspective and there is a lack of systematic understanding of potential risks. In this work, we conduct extensive experiments to evaluate current state-of-the-art dataset distillation methods. We successfully use membership inference attacks to show that privacy risks still remain. Our work also demonstrates that dataset distillation can cause varying degrees of impact on model robustness and amplify model unfairness across classes when making predictions. This work offers a large-scale benchmarking framework for dataset distillation evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;Selfmem&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#33258;&#25105;&#35760;&#24518;&#27744;&#24182;&#37319;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#65292;&#20351;&#26816;&#32034;&#26356;&#21152;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.02437</link><description>&lt;p&gt;
&#36816;&#29992;&#33258;&#25105;&#35760;&#24518;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lift Yourself Up: Retrieval-augmented Text Generation with Self Memory. (arXiv:2305.02437v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02437
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;Selfmem&#65292;&#36890;&#36807;&#36845;&#20195;&#29983;&#25104;&#33258;&#25105;&#35760;&#24518;&#27744;&#24182;&#37319;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#65292;&#20351;&#26816;&#32034;&#26356;&#21152;&#33258;&#36866;&#24212;&#65292;&#25552;&#39640;&#20102;&#25991;&#26412;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30456;&#36739;&#20110;&#20256;&#32479;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#65292;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#33021;&#22815;&#30452;&#25509;&#36845;&#20195;&#20154;&#31867;&#32534;&#20889;&#30340;&#21442;&#32771;&#24211;&#65292;&#24182;&#20174;&#20013;&#26816;&#32034;&#20986;&#30456;&#24212;&#30340;&#20449;&#24687;&#65292;&#20197;&#29983;&#25104;&#26356;&#20248;&#36136;&#30340;&#25991;&#26412;&#12290;&#20294;&#24403;&#21069;&#25991;&#29486;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#26816;&#32034;&#21040;&#30340;&#35760;&#24518;&#26469;&#33258;&#20110;&#22266;&#23450;&#30340;&#35821;&#26009;&#24211;&#65292;&#20854;&#36136;&#37327;&#23384;&#22312;&#19968;&#23450;&#23616;&#38480;&#24615;&#65292;&#21487;&#33021;&#20250;&#38480;&#21046;&#35760;&#24518;&#22686;&#24378;&#27169;&#22411;&#30340;&#28508;&#21147;&#12290;&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#21517;&#20026;Selfmem&#30340;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#36845;&#20195;&#22320;&#37319;&#29992;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#22120;&#33258;&#36523;&#20197;&#29983;&#25104;&#26080;&#38480;&#21046;&#30340;&#33258;&#25105;&#35760;&#24518;&#27744;&#65292;&#24182;&#20351;&#29992;&#35760;&#24518;&#36873;&#25321;&#22120;&#20026;&#19979;&#19968;&#36718;&#29983;&#25104;&#36873;&#25321;&#19968;&#20010;&#29983;&#25104;&#30340;&#35760;&#24518;&#12290;&#30456;&#32467;&#21512;&#65292;&#36825;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#25552;&#20986;&#20102;&#36816;&#29992;&#33258;&#25105;&#35760;&#24518;&#30340;&#26816;&#32034;&#22686;&#24378;&#25991;&#26412;&#29983;&#25104;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
With direct access to human-written reference as memory, retrieval-augmented generation has achieved much progress in a wide range of text generation tasks. Since better memory would typically prompt better generation~(we define this as primal problem), previous works mainly focus on how to retrieve better memory. However, one fundamental limitation exists for current literature: the memory is retrieved from a fixed corpus and is bounded by the quality of the corpus. Due to the finite retrieval space, bounded memory would greatly limit the potential of the memory-augmented generation model. In this paper, by exploring the duality of the primal problem: better generation also prompts better memory, we propose a framework called Selfmem, which iteratively adopts a retrieval-augmented generator itself to generate an unbounded memory pool and uses a memory selector to pick one generated memory for the next generation round. By combining the primal and dual problem, a retrieval-augmented ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#22238;&#31572;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#38382;&#39064;&#19978;&#30340;&#19981;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23398;&#26415;&#30028;&#20351;&#29992;ChatGPT&#30340;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2304.14993</link><description>&lt;p&gt;
ChatGPT - &#23545;&#20110;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#23398;&#29983;&#21644;&#25945;&#24072;&#26159;&#31119;&#26159;&#31096;?
&lt;/p&gt;
&lt;p&gt;
ChatGPT -- a Blessing or a Curse for Undergraduate Computer Science Students and Instructors?. (arXiv:2304.14993v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14993
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;ChatGPT&#22312;&#22238;&#31572;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#38382;&#39064;&#19978;&#30340;&#19981;&#21487;&#38752;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#22312;&#23398;&#26415;&#30028;&#20351;&#29992;ChatGPT&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#30001;OpenAI&#24320;&#21457;&#30340;AI&#35821;&#35328;&#27169;&#22411;&#65292;&#21487;&#20197;&#29702;&#35299;&#21644;&#29983;&#25104;&#31867;&#20154;&#25991;&#26412;&#12290;&#23427;&#21487;&#29992;&#20110;&#35821;&#35328;&#29983;&#25104;&#12289;&#38382;&#31572;&#12289;&#25991;&#26412;&#25688;&#35201;&#12289;&#32842;&#22825;&#26426;&#22120;&#20154;&#24320;&#21457;&#12289;&#35821;&#35328;&#32763;&#35793;&#12289;&#24773;&#24863;&#20998;&#26512;&#12289;&#20869;&#23481;&#21019;&#20316;&#12289;&#20010;&#24615;&#21270;&#12289;&#25991;&#26412;&#23436;&#25104;&#21644;&#25925;&#20107;&#21465;&#36848;&#31561;&#22810;&#31181;&#29992;&#36884;&#12290;&#34429;&#28982;ChatGPT&#21463;&#21040;&#20102;&#30456;&#24403;&#31215;&#26497;&#30340;&#20851;&#27880;&#65292;&#20294;&#22312;&#23398;&#26415;&#30028;&#20063;&#24341;&#36215;&#20102;&#19968;&#31181;&#25285;&#24551;&#21644;&#19981;&#30830;&#23450;&#24863;&#12290;&#23384;&#22312;&#25285;&#24551;&#23398;&#29983;&#21487;&#33021;&#20250;&#21033;&#29992;ChatGPT&#23436;&#25104;&#35838;&#22806;&#20316;&#19994;&#21644;&#32771;&#35797;&#65292;&#24182;&#33719;&#24471;&#26377;&#21033;&#30340;&#25104;&#32489;&#65292;&#32780;&#19981;&#30495;&#27491;&#33719;&#24471;&#30693;&#35782;&#12290;&#26412;&#25991;&#37319;&#29992;&#23450;&#37327;&#26041;&#27861;&#65292;&#23637;&#31034;&#20102;ChatGPT&#22312;&#22238;&#31572;&#26412;&#31185;&#35745;&#31639;&#26426;&#31185;&#23398;&#33539;&#22260;&#20869;&#30340;&#21508;&#31181;&#38382;&#39064;&#19978;&#20855;&#26377;&#39640;&#24230;&#30340;&#19981;&#21487;&#38752;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#23398;&#29983;&#30450;&#30446;&#20381;&#36182;ChatGPT&#23436;&#25104;&#20316;&#19994;&#21644;&#32771;&#35797;&#21487;&#33021;&#20250;&#33258;&#27585;&#21069;&#31243;&#12290;&#25105;&#20204;&#22312;&#36825;&#20010;&#20998;&#26512;&#22522;&#30784;&#19978;&#25552;&#20986;&#20102;&#25945;&#24072;&#21644;&#23398;&#29983;&#22914;&#20309;&#22312;&#23398;&#26415;&#30028;&#20351;&#29992;ChatGPT&#30340;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT is an AI language model developed by OpenAI that can understand and generate human-like text. It can be used for a variety of use cases such as language generation, question answering, text summarization, chatbot development, language translation, sentiment analysis, content creation, personalization, text completion, and storytelling. While ChatGPT has garnered significant positive attention, it has also generated a sense of apprehension and uncertainty in academic circles. There is concern that students may leverage ChatGPT to complete take-home assignments and exams and obtain favorable grades without genuinely acquiring knowledge. This paper adopts a quantitative approach to demonstrate ChatGPT's high degree of unreliability in answering a diverse range of questions pertaining to topics in undergraduate computer science. Our analysis shows that students may risk self-sabotage by blindly depending on ChatGPT to complete assignments and exams. We build upon this analysis to p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23433;&#25490;&#22320;&#38081;&#26102;&#21051;&#34920;&#21644;&#35843;&#25972;&#21015;&#36710;&#30340;&#20572;&#38752;&#26102;&#38388;&#21644;&#24033;&#33322;&#36895;&#24230;&#65292;&#20248;&#21270;&#25200;&#21160;&#19979;&#30340;&#22320;&#38081;&#31995;&#32479;&#33021;&#28304;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#29615;&#22659;&#19979;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#36798;&#38477;&#20302;10.9%&#30340;&#29301;&#24341;&#33021;&#37327;&#28040;&#32791;&#21644;&#26368;&#39640;&#36798;&#25552;&#39640;47.9%&#30340;&#20877;&#29983;&#21046;&#21160;&#33021;&#37327;&#21033;&#29992;&#29575;&#65292;&#20026;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#30340;&#33410;&#33021;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2304.13443</link><description>&lt;p&gt;
&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#20248;&#21270;&#22320;&#38081;&#31995;&#32479;&#33021;&#28304;&#25928;&#29575;&#22312;&#19981;&#30830;&#23450;&#25200;&#21160;&#19979;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
Optimizing Energy Efficiency in Metro Systems Under Uncertainty Disturbances Using Reinforcement Learning. (arXiv:2304.13443v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.13443
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#23433;&#25490;&#22320;&#38081;&#26102;&#21051;&#34920;&#21644;&#35843;&#25972;&#21015;&#36710;&#30340;&#20572;&#38752;&#26102;&#38388;&#21644;&#24033;&#33322;&#36895;&#24230;&#65292;&#20248;&#21270;&#25200;&#21160;&#19979;&#30340;&#22320;&#38081;&#31995;&#32479;&#33021;&#28304;&#25928;&#29575;&#65292;&#35813;&#26041;&#27861;&#22312;&#27169;&#25311;&#29615;&#22659;&#19979;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#36798;&#38477;&#20302;10.9%&#30340;&#29301;&#24341;&#33021;&#37327;&#28040;&#32791;&#21644;&#26368;&#39640;&#36798;&#25552;&#39640;47.9%&#30340;&#20877;&#29983;&#21046;&#21160;&#33021;&#37327;&#21033;&#29992;&#29575;&#65292;&#20026;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#30340;&#33410;&#33021;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22478;&#24066;&#20132;&#36890;&#39046;&#22495;&#65292;&#22320;&#38081;&#31995;&#32479;&#26159;&#20851;&#38190;&#30340;&#21487;&#25345;&#32493;&#20844;&#20849;&#20132;&#36890;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#24040;&#22823;&#33021;&#28304;&#28040;&#32791;&#23545;&#21487;&#25345;&#32493;&#24615;&#30446;&#26631;&#26500;&#25104;&#20102;&#25361;&#25112;&#12290;&#24310;&#35823;&#21644;&#20056;&#23458;&#27969;&#21464;&#21270;&#31561;&#25200;&#21160;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#22240;&#20026;&#23427;&#20204;&#20250;&#23545;&#22320;&#38081;&#31995;&#32479;&#30340;&#33021;&#28304;&#25928;&#29575;&#20135;&#29983;&#36127;&#38754;&#24433;&#21709;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31574;&#30053;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#35843;&#25972;&#21015;&#36710;&#30340;&#20572;&#38752;&#26102;&#38388;&#21644;&#24033;&#33322;&#36895;&#24230;&#65292;&#37325;&#26032;&#23433;&#25490;&#22320;&#38081;&#26102;&#21051;&#34920;&#65292;&#24182;&#20248;&#21270;&#21463;&#25200;&#21160;&#24433;&#21709;&#30340;&#22320;&#38081;&#31995;&#32479;&#30340;&#33021;&#28304;&#25928;&#29575;&#12290;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#22522;&#32447;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#39640;&#36798;10.9&#65285;&#30340;&#29301;&#24341;&#33021;&#37327;&#28040;&#32791;&#38477;&#20302;&#21644;&#26368;&#39640;&#36798;47.9&#65285;&#30340;&#20877;&#29983;&#21046;&#21160;&#33021;&#37327;&#21033;&#29992;&#29575;&#25552;&#39640;&#12290;&#26412;&#30740;&#31350;&#20026;&#22478;&#24066;&#36712;&#36947;&#20132;&#36890;&#33410;&#33021;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the realm of urban transportation, metro systems serve as crucial and sustainable means of public transit. However, their substantial energy consumption poses a challenge to the goal of sustainability. Disturbances such as delays and passenger flow changes can further exacerbate this issue by negatively affecting energy efficiency in metro systems. To tackle this problem, we propose a policy-based reinforcement learning approach that reschedules the metro timetable and optimizes energy efficiency in metro systems under disturbances by adjusting the dwell time and cruise speed of trains. Our experiments conducted in a simulation environment demonstrate the superiority of our method over baseline methods, achieving a traction energy consumption reduction of up to 10.9% and an increase in regenerative braking energy utilization of up to 47.9%. This study provides an effective solution to the energy-saving problem of urban rail transit.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;SAM&#22312;&#21508;&#31181;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#28857;&#25552;&#31034;&#19979;&#20854;&#34920;&#29616;&#39640;&#24230;&#21464;&#21270;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;</title><link>http://arxiv.org/abs/2304.10517</link><description>&lt;p&gt;
&#21307;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#20219;&#24847;&#20998;&#21106;&#27169;&#22411;&#65306;&#19968;&#39033;&#23454;&#39564;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model for Medical Image Analysis: an Experimental Study. (arXiv:2304.10517v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10517
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;SAM&#22312;&#21508;&#31181;&#19981;&#21516;&#24773;&#20917;&#19979;&#30340;&#34920;&#29616;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#22312;&#21333;&#28857;&#25552;&#31034;&#19979;&#20854;&#34920;&#29616;&#39640;&#24230;&#21464;&#21270;&#65292;&#26159;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24037;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#25454;&#27880;&#37322;&#30340;&#26377;&#38480;&#21487;&#29992;&#24615;&#21644;&#33719;&#21462;&#25104;&#26412;&#65292;&#35757;&#32451;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;Segment Anything Model&#65288;SAM&#65289;&#26159;&#19968;&#31181;&#22522;&#30784;&#27169;&#22411;&#65292;&#32463;&#36807;&#36229;&#36807;10&#20159;&#20010;&#27880;&#37322;&#30340;&#35757;&#32451;&#65292;&#20027;&#35201;&#29992;&#20110;&#33258;&#28982;&#22270;&#20687;&#65292;&#26088;&#22312;&#33021;&#22815;&#20197;&#20132;&#20114;&#26041;&#24335;&#20998;&#21106;&#29992;&#25143;&#23450;&#20041;&#30340;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#23613;&#31649;SAM&#22312;&#33258;&#28982;&#22270;&#20687;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#19981;&#28165;&#26970;&#35813;&#27169;&#22411;&#22312;&#36716;&#25442;&#21040;&#21307;&#23398;&#22270;&#20687;&#39046;&#22495;&#26102;&#20250;&#21463;&#21040;&#22810;&#22823;&#24433;&#21709;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23545;SAM&#22312;&#21508;&#31181;&#27169;&#24577;&#21644;&#35299;&#21078;&#23398;&#30340;11&#20010;&#21307;&#23398;&#22270;&#20687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#35780;&#20272;&#12290;&#22312;&#25105;&#20204;&#30340;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#26041;&#27861;&#29983;&#25104;&#28857;&#25552;&#31034;&#26469;&#27169;&#25311;&#20132;&#20114;&#20998;&#21106;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SAM&#22522;&#20110;&#21333;&#28857;&#25552;&#31034;&#30340;&#34920;&#29616;&#22312;&#20219;&#21153;&#21644;&#25968;&#25454;&#38598;&#26041;&#38754;&#39640;&#24230;&#21464;&#21270;&#65292;&#21363;&#20174;&#33034;&#26609;MRI&#25968;&#25454;&#38598;&#30340;0.1135&#21040;&#39627;&#20851;&#33410;X&#23556;&#32447;&#25968;&#25454;&#38598;&#30340;0.8650&#12290;
&lt;/p&gt;
&lt;p&gt;
Training segmentation models for medical images continues to be challenging due to the limited availability and acquisition expense of data annotations. Segment Anything Model (SAM) is a foundation model trained on over 1 billion annotations, predominantly for natural images, that is intended to be able to segment the user-defined object of interest in an interactive manner. Despite its impressive performance on natural images, it is unclear how the model is affected when shifting to medical image domains. Here, we perform an extensive evaluation of SAM's ability to segment medical images on a collection of 11 medical imaging datasets from various modalities and anatomies. In our experiments, we generated point prompts using a standard method that simulates interactive segmentation. Experimental results show that SAM's performance based on single prompts highly varies depending on the task and the dataset, i.e., from 0.1135 for a spine MRI dataset to 0.8650 for a hip x-ray dataset, eva
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#32593;&#32476;&#27969;&#37327;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;NetGPT&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20248;&#21270;&#32593;&#32476;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.09513</link><description>&lt;p&gt;
NetGPT&#65306;&#32593;&#32476;&#27969;&#37327;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NetGPT: Generative Pretrained Transformer for Network Traffic. (arXiv:2304.09513v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09513
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#39318;&#20010;&#32593;&#32476;&#27969;&#37327;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;NetGPT&#65292;&#35813;&#27169;&#22411;&#21487;&#20197;&#20248;&#21270;&#32593;&#32476;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#22823;&#35268;&#27169;&#30340;&#21407;&#22987;&#25968;&#25454;&#23398;&#20064;&#32593;&#32476;&#27969;&#37327;&#30340;&#22522;&#26412;&#29305;&#24449;&#65292;&#24182;&#20026;&#36755;&#20837;&#27969;&#37327;&#29983;&#25104;&#21487;&#21306;&#20998;&#30340;&#32467;&#26524;&#65292;&#32780;&#19981;&#32771;&#34385;&#29305;&#23450;&#30340;&#19979;&#28216;&#20219;&#21153;&#12290;&#26377;&#25928;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#20197;&#26174;&#33879;&#20248;&#21270;&#19979;&#28216;&#20219;&#21153;&#30340;&#35757;&#32451;&#25928;&#29575;&#21644;&#26377;&#25928;&#24615;&#65292;&#20363;&#22914;&#27969;&#37327;&#20998;&#31867;&#12289;&#25915;&#20987;&#26816;&#27979;&#12289;&#36164;&#28304;&#35843;&#24230;&#12289;&#21327;&#35758;&#20998;&#26512;&#21644;&#27969;&#37327;&#29983;&#25104;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;NetGPT&#65292;&#26088;&#22312;&#20026;&#32593;&#32476;&#27969;&#37327;&#26500;&#24314;&#39044;&#35757;&#32451;&#27169;&#22411;&#24182;&#35299;&#20915;&#22810;&#26679;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pretrained models for network traffic can utilize large-scale raw data to learn the essential characteristics of network traffic, and generate distinguishable results for input traffic without considering specific downstream tasks. Effective pretrained models can significantly optimize the training efficiency and effectiveness of downstream tasks, such as traffic classification, attack detection, resource scheduling, protocol analysis, and traffic generation. Despite the great success of pretraining in natural language processing, there is no work in the network field. Considering the diverse demands and characteristics of network traffic and network tasks, it is non-trivial to build a pretrained model for network traffic and we face various challenges, especially the heterogeneous headers and payloads in the multi-pattern network traffic and the different dependencies for contexts of diverse downstream network tasks.  To tackle these challenges, in this paper, we make the first attemp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.08649</link><description>&lt;p&gt;
&#22522;&#20110;BERT&#30340;&#25216;&#26415;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Classification of US Supreme Court Cases using BERT-Based Techniques. (arXiv:2304.08649v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22522;&#20110;BERT&#25216;&#26415;&#25506;&#31350;&#20102;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#26696;&#20363;&#36827;&#34892;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20351;&#29992;BERT&#27169;&#22411;&#19982;&#20854;&#20182;&#20808;&#36827;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#32456;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;80%&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#21462;&#24471;&#20102;60%&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#21452;&#21521;&#32534;&#30721;&#22120;&#34920;&#31034;&#26469;&#33258;&#21464;&#21387;&#22120;&#30340;&#27169;&#22411;&#65288;BERT&#65289;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#65288;&#22914;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#65292;&#35789;&#24615;&#65288;POS&#65289;&#26631;&#35760;&#31561;&#65289;&#19978;&#20135;&#29983;&#20102;&#26368;&#26032;&#25216;&#26415;&#65288;SOTA&#65289;&#32467;&#26524;&#12290;&#24403;&#20998;&#31867;&#38271;&#25991;&#26723;&#65288;&#20363;&#22914;&#26469;&#33258;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#30340;&#25991;&#26723;&#65289;&#26102;&#65292;&#20351;&#29992;BERT&#27169;&#22411;&#21487;&#33021;&#27604;&#36739;&#22256;&#38590;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23581;&#35797;&#20102;&#20960;&#31181;&#22522;&#20110;BERT&#30340;&#20998;&#31867;&#25216;&#26415;&#65292;&#29992;&#20110;&#23545;&#32654;&#22269;&#26368;&#39640;&#27861;&#38498;&#20915;&#23450;&#25110;&#26368;&#39640;&#27861;&#38498;&#25968;&#25454;&#24211;&#65288;SCDB&#65289;&#36827;&#34892;&#20998;&#31867;&#65292;&#24182;&#23558;&#20854;&#19982;&#20808;&#21069;&#30340;SOTA&#32467;&#26524;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#36824;&#23558;&#25105;&#20204;&#30340;&#32467;&#26524;&#19982;&#38024;&#23545;&#38271;&#25991;&#26723;&#30340;SOTA&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#20998;&#31867;&#20219;&#21153;&#36827;&#34892;&#20102;&#27604;&#36739;&#65306;&#65288;1&#65289;&#24191;&#27867;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;15&#20010;&#31867;&#21035;&#65307;&#65288;2&#65289;&#32454;&#31890;&#24230;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#20855;&#26377;279&#20010;&#31867;&#21035;&#12290;&#25105;&#20204;&#30340;&#26368;&#20339;&#32467;&#26524;&#22312;15&#20010;&#24191;&#27867;&#31867;&#21035;&#19978;&#20135;&#29983;80&#65285;&#30340;&#20934;&#30830;&#24230;&#65292;&#22312;279&#20010;&#32454;&#31890;&#24230;&#31867;&#21035;&#19978;&#20135;&#29983;60&#65285;&#30340;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models based on bidirectional encoder representations from transformers (BERT) produce state of the art (SOTA) results on many natural language processing (NLP) tasks such as named entity recognition (NER), part-of-speech (POS) tagging etc. An interesting phenomenon occurs when classifying long documents such as those from the US supreme court where BERT-based models can be considered difficult to use on a first-pass or out-of-the-box basis. In this paper, we experiment with several BERT-based classification techniques for US supreme court decisions or supreme court database (SCDB) and compare them with the previous SOTA results. We then compare our results specifically with SOTA models for long documents. We compare our results for two classification tasks: (1) a broad classification task with 15 categories and (2) a fine-grained classification task with 279 categories. Our best result produces an accuracy of 80\% on the 15 broad categories and 60\% on the fine-grained 279 categories 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q-&#30697;&#38453;&#30340;&#27880;&#24847;&#21147;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19981;&#23384;&#22312;&#20107;&#20808;&#30830;&#23450;&#30340;&#25216;&#33021;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22312;&#32447;&#25945;&#32946;&#24179;&#21488;&#12290;</title><link>http://arxiv.org/abs/2304.08168</link><description>&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#30340;&#27880;&#24847;&#21147;Q-&#30697;&#38453;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Attentive Q-Matrix Learning for Knowledge Tracing. (arXiv:2304.08168v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Q-&#30697;&#38453;&#30340;&#27880;&#24847;&#21147;&#30693;&#35782;&#36861;&#36394;&#27169;&#22411;&#65292;&#33021;&#22815;&#22312;&#19981;&#23384;&#22312;&#20107;&#20808;&#30830;&#23450;&#30340;&#25216;&#33021;&#26631;&#31614;&#30340;&#24773;&#20917;&#19979;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22312;&#32447;&#25945;&#32946;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26234;&#33021;&#25945;&#23398;&#31995;&#32479;&#30340;&#36805;&#29467;&#21457;&#23637;&#20013;&#65292;&#36861;&#36394;&#23398;&#29983;&#30340;&#30693;&#35782;&#29366;&#24577;&#36234;&#26469;&#36234;&#37325;&#35201;&#65292;&#20197;&#20415;&#25552;&#20379;&#20010;&#24615;&#21270;&#30340;&#23398;&#20064;&#25351;&#23548;&#12290;&#36825;&#26159;&#30693;&#35782;&#36861;&#36394; (KT) &#30340;&#20027;&#35201;&#24605;&#24819;&#65292;&#23427;&#26681;&#25454;&#23398;&#29983;&#22312;&#24179;&#21488;&#19978;&#30340;&#36807;&#21435;&#20132;&#20114;&#26469;&#24314;&#31435;&#27169;&#22411;&#65292;&#20197;&#27169;&#25311;&#23398;&#29983;&#25484;&#25569;&#30693;&#35782;&#27010;&#24565; (KCs&#65292;&#35299;&#20915;&#38382;&#39064;&#25152;&#38656;&#30340;&#25216;&#33021;)&#12290;&#34429;&#28982;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;KT&#27169;&#22411;&#65292;&#24182;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#20854;&#20013;&#22823;&#37096;&#20998;&#27169;&#22411;&#20351;&#29992;&#27010;&#24565;&#26469;&#32034;&#24341;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#38656;&#35201;&#39044;&#20808;&#30830;&#23450;&#27599;&#20010;&#38382;&#39064;&#25152;&#38656;&#30340;&#25216;&#33021;&#26631;&#31614;&#65292;&#20197;&#25351;&#31034;&#27491;&#30830;&#22238;&#31572;&#35813;&#38382;&#39064;&#25152;&#38656;&#30340;KC&#12290;&#36825;&#20351;&#24471;&#36825;&#20123;&#27169;&#22411;&#24456;&#38590;&#24212;&#29992;&#20110;&#22823;&#35268;&#27169;&#30340;&#22312;&#32447;&#25945;&#32946;&#24179;&#21488;&#65292;&#22240;&#20026;&#38382;&#39064;&#36890;&#24120;&#27809;&#26377;&#25353;&#29031;&#25216;&#33021;&#26631;&#31614;&#36827;&#34892;&#24456;&#22909;&#30340;&#32452;&#32455;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;Q-&#30697;&#38453;&#30340;&#27880;&#24847;&#21147;&#30693;&#35782;&#36861;&#36394; (QAKT)&#65292;&#36825;&#26159;&#19968;&#31181;&#31471;&#21040;&#31471;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#23558;&#27880;&#24847;&#21147;&#26041;&#27861;&#24212;&#29992;&#20110;&#22330;&#26223;&#20013;&#65292;&#20854;&#20013;&#19981;&#23384;&#22312;&#20107;&#20808;&#30830;&#23450;&#30340;&#25216;&#33021;&#26631;&#31614;&#12290;
&lt;/p&gt;
&lt;p&gt;
As the rapid development of Intelligent Tutoring Systems (ITS) in the past decade, tracing the students' knowledge state has become more and more important in order to provide individualized learning guidance. This is the main idea of Knowledge Tracing (KT), which models students' mastery of knowledge concepts (KCs, skills needed to solve a question) based on their past interactions on platforms. Plenty of KT models have been proposed and have shown remarkable performance recently. However, the majority of these models use concepts to index questions, which means the predefined skill tags for each question are required in advance to indicate the KCs needed to answer that question correctly. This makes it pretty hard to apply on large-scale online education platforms where questions are often not well-organized by skill tags. In this paper, we propose Q-matrix-based Attentive Knowledge Tracing (QAKT), an end-to-end style model that is able to apply the attentive method to scenes where n
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.02396</link><description>&lt;p&gt;
AutoRL&#36229;&#21442;&#25968;&#26223;&#35266;
&lt;/p&gt;
&lt;p&gt;
AutoRL Hyperparameter Landscapes. (arXiv:2304.02396v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02396
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#22810;&#27425;&#24314;&#31435;&#21644;&#20998;&#26512;AutoRL&#36229;&#21442;&#25968;&#30340;&#26223;&#35266;&#65292;&#35777;&#26126;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#21462;&#24471;&#20196;&#20154;&#30633;&#30446;&#25104;&#26524;&#30340;&#21516;&#26102;&#65292;&#20854;&#36229;&#21442;&#25968;&#23545;&#24615;&#33021;&#30340;&#24433;&#21709;&#38480;&#21046;&#20102;&#20854;&#24212;&#29992;&#33539;&#22260;&#12290;&#36825;&#32463;&#24120;&#20351;&#24471;&#22312;&#23454;&#36341;&#20013;&#38590;&#20197;&#33719;&#24471;&#33391;&#22909;&#30340;&#32467;&#26524;&#12290;&#33258;&#21160;&#21270;RL&#65288;AutoRL&#65289;&#35299;&#20915;&#20102;&#36825;&#20010;&#38590;&#39064;&#65292;&#20294;&#26377;&#20851;&#36229;&#21442;&#25968;&#20248;&#21270;&#65288;HPO&#65289;&#26041;&#27861;&#22312;&#25628;&#32034;&#26368;&#20339;&#37197;&#32622;&#26102;&#25152;&#36941;&#21382;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#21160;&#24577;&#21464;&#21270;&#30340;&#20449;&#24687;&#24456;&#23569;&#12290;&#37492;&#20110;&#29616;&#26377;AutoRL&#26041;&#27861;&#21160;&#24577;&#35843;&#25972;&#36229;&#21442;&#25968;&#37197;&#32622;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#19981;&#20165;&#22312;&#19968;&#20010;&#26102;&#38388;&#28857;&#65292;&#32780;&#19988;&#22312;&#22810;&#20010;&#26102;&#38388;&#28857;&#19978;&#24314;&#31435;&#21644;&#20998;&#26512;&#36825;&#20123;&#36229;&#21442;&#25968;&#26223;&#35266;&#12290;&#38024;&#23545;&#20851;&#20110;&#36825;&#31181;&#21160;&#24577;AutoRL&#26041;&#27861;&#21512;&#27861;&#24615;&#30340;&#19968;&#20010;&#37325;&#35201;&#24320;&#25918;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20805;&#20998;&#30340;&#35777;&#25454;&#65292;&#34920;&#26126;&#22312;&#19981;&#21516;&#31181;&#31867;&#30340;&#29615;&#22659;&#65288;Cartpole&#21644;Pendulum&#65289;&#20013;&#65292;&#26469;&#33258;RL&#25991;&#29486;&#30340;&#20195;&#34920;&#31639;&#27861;&#65288;DQN&#21644;SAC&#65289;&#30340;&#36229;&#21442;&#25968;&#26223;&#35266;&#20250;&#38543;&#26102;&#38388;&#32780;&#24378;&#28872;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Reinforcement Learning (RL) has shown to be capable of producing impressive results, its use is limited by the impact of its hyperparameters on performance. This often makes it difficult to achieve good results in practice. Automated RL (AutoRL) addresses this difficulty, yet little is known about the dynamics of the hyperparameter landscapes that hyperparameter optimization (HPO) methods traverse in search of optimal configurations. In view of existing AutoRL approaches dynamically adjusting hyperparameter configurations, we propose an approach to build and analyze these hyperparameter landscapes not just for one point in time but at multiple points in time throughout training. Addressing an important open question on the legitimacy of such dynamic AutoRL approaches, we provide thorough empirical evidence that the hyperparameter landscapes strongly vary over time across representative algorithms from RL literature (DQN and SAC) in different kinds of environments (Cartpole and
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;PAC-S&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26631;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65307;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#20844;&#24320;&#12290;</title><link>http://arxiv.org/abs/2303.12112</link><description>&lt;p&gt;
&#22522;&#20110;&#27491;&#26679;&#26412;&#22686;&#24378;&#23545;&#27604;&#23398;&#20064;&#30340;&#22270;&#20687;&#35270;&#39057;&#26631;&#39064;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Positive-Augmented Constrastive Learning for Image and Video Captioning Evaluation. (arXiv:2303.12112v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;PAC-S&#65292;&#21487;&#20197;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#22270;&#20687;&#21644;&#35270;&#39057;&#30340;&#26631;&#39064;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#30340;&#25351;&#26631;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#65307;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#24050;&#32463;&#20844;&#24320;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;CLIP&#27169;&#22411;&#22312;&#24456;&#22810;&#36328;&#27169;&#24577;&#20219;&#21153;&#19978;&#37117;&#38750;&#24120;&#26377;&#25928;&#65292;&#21253;&#25324;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#32467;&#26500;&#20013;&#29983;&#25104;&#30340;&#26631;&#39064;&#35780;&#20272;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#23545;&#27604;&#24230;&#30340;&#22270;&#20687;&#26631;&#39064;&#35780;&#20272;&#25351;&#26631;&#37197;&#26041;&#65292;&#21363;&#27491;&#26679;&#26412;&#22686;&#24378;&#30340;&#23545;&#27604;&#24230;&#23398;&#20064;&#20998;&#25968;&#65288;PAC-S&#65289;&#65292;&#20197;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#24335;&#32479;&#19968;&#20102;&#23545;&#27604;&#24230;&#35270;&#35273;-&#35821;&#20041;&#31354;&#38388;&#30340;&#23398;&#20064;&#21644;&#31574;&#23637;&#25968;&#25454;&#19978;&#29983;&#25104;&#30340;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#28155;&#21152;&#12290;&#36328;&#36234;&#22810;&#20010;&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26032;&#25351;&#26631;&#22312;&#22270;&#20687;&#21644;&#35270;&#39057;&#19978;&#19982;&#20154;&#31867;&#21028;&#26029;&#30340;&#30456;&#20851;&#24615;&#26368;&#39640;&#65292;&#20248;&#20110;&#29616;&#26377;&#21442;&#32771;&#25351;&#26631;&#65288;&#22914;CIDEr&#21644;SPICE&#65289;&#21644;&#26080;&#21442;&#32771;&#25351;&#26631;&#65288;&#22914;CLIP-Score&#65289;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#27969;&#34892;&#30340;&#22270;&#20687;&#26631;&#39064;&#26041;&#27861;&#65292;&#24182;&#35780;&#20272;&#20102;&#37319;&#29992;&#19981;&#21516;&#36328;&#27169;&#24577;&#29305;&#24449;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#28304;&#20195;&#30721;&#21644;&#35757;&#32451;&#27169;&#22411;&#26159;&#20844;&#24320;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The CLIP model has been recently proven to be very effective for a variety of cross-modal tasks, including the evaluation of captions generated from vision-and-language architectures. In this paper, we propose a new recipe for a contrastive-based evaluation metric for image captioning, namely Positive-Augmented Contrastive learning Score (PAC-S), that in a novel way unifies the learning of a contrastive visual-semantic space with the addition of generated images and text on curated data. Experiments spanning several datasets demonstrate that our new metric achieves the highest correlation with human judgments on both images and videos, outperforming existing reference-based metrics like CIDEr and SPICE and reference-free metrics like CLIP-Score. Finally, we test the system-level correlation of the proposed metric when considering popular image captioning approaches, and assess the impact of employing different cross-modal features. Our source code and trained models are publicly availa
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#22810;&#26679;&#21270;&#30340;&#32422;&#26463;&#24182;&#19988;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#30340;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.01141</link><description>&lt;p&gt;
DeepSaDe: &#23398;&#20064;&#30830;&#20445;&#28385;&#36275;&#39046;&#22495;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DeepSaDe: Learning Neural Networks that Guarantee Domain Constraint Satisfaction. (arXiv:2303.01141v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#35813;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#22810;&#26679;&#21270;&#30340;&#32422;&#26463;&#24182;&#19988;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#30340;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#26222;&#21450;&#65292;&#23588;&#20854;&#26159;&#31070;&#32463;&#32593;&#32476;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20851;&#27880;&#23427;&#20204;&#30340;&#21487;&#20449;&#24230;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#34892;&#20026;&#24517;&#39035;&#26159;&#23433;&#20840;&#30340;&#12290;&#24403;&#21069;&#19968;&#20123;&#26041;&#27861;&#21487;&#20197;&#23545;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32422;&#26463;&#65292;&#20294;&#23427;&#20204;&#19981;&#33021;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#30340;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#38480;&#21046;&#65288;&#21363;&#20351;&#22312;&#26410;&#30475;&#36807;&#30340;&#25968;&#25454;&#19978;&#65289;&#65292;&#25110;&#32773;&#23427;&#20204;&#23545;&#21487;&#24378;&#21046;&#25191;&#34892;&#30340;&#32422;&#26463;&#31867;&#22411;&#26377;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#21487;&#20197;&#24378;&#21046;&#25191;&#34892;&#24191;&#27867;&#32422;&#26463;&#24182;&#20445;&#35777;&#25152;&#26377;&#21487;&#33021;&#39044;&#27979;&#37117;&#28385;&#36275;&#32422;&#26463;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#35813;&#26041;&#27861;&#22522;&#20110;&#20197;&#24448;&#23558;&#23398;&#20064;&#32447;&#24615;&#27169;&#22411;&#35270;&#20026;&#32422;&#26463;&#28385;&#36275;&#38382;&#39064;&#65288;CSP&#65289;&#30340;&#24037;&#20316;&#12290;&#20026;&#20102;&#23558;&#36825;&#20010;&#24819;&#27861;&#24212;&#29992;&#20110;&#31070;&#32463;&#32593;&#32476;&#65292;&#26412;&#25991;&#22686;&#21152;&#20102;&#20004;&#20010;&#20851;&#38190;&#30340;&#26032;&#20803;&#32032;&#65306;&#32593;&#32476;&#23618;&#19978;&#30340;&#32422;&#26463;&#20256;&#25773;&#21644;&#26435;&#37325;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
As machine learning models, specifically neural networks, are becoming increasingly popular, there are concerns regarding their trustworthiness, specially in safety-critical applications, e.g. actions of an autonomous vehicle must be safe. There are approaches that can train neural networks where such domain requirements are enforced as constraints, but they either cannot guarantee that the constraint will be satisfied by all possible predictions (even on unseen data) or they are limited in the type of constraints that can be enforced. In this paper, we present an approach to train neural networks which can enforce a wide variety of constraints and guarantee that the constraint is satisfied by all possible predictions. The approach builds on earlier work where learning linear models is formulated as a constraint satisfaction problem (CSP). To make this idea applicable to neural networks, two crucial new elements are added: constraint propagation over the network layers, and weight upda
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23450;&#24615;&#29702;&#35770;&#20013;&#30340;&#25429;&#33719;&#21306;&#22495;&#27010;&#24565;&#65292;&#21019;&#24314;&#20986;&#23433;&#20840;&#38598;&#65292;&#20197;&#30830;&#20445;&#20998;&#25955;&#24335;&#23398;&#20064;&#22312;&#32852;&#21512;&#31574;&#30053;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.13844</link><description>&lt;p&gt;
&#36890;&#36807;&#25429;&#33719;&#21306;&#22495;&#23454;&#29616;&#23433;&#20840;&#30340;&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Safe Multi-agent Learning via Trapping Regions. (arXiv:2302.13844v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#23450;&#24615;&#29702;&#35770;&#20013;&#30340;&#25429;&#33719;&#21306;&#22495;&#27010;&#24565;&#65292;&#21019;&#24314;&#20986;&#23433;&#20840;&#38598;&#65292;&#20197;&#30830;&#20445;&#20998;&#25955;&#24335;&#23398;&#20064;&#22312;&#32852;&#21512;&#31574;&#30053;&#31354;&#38388;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#23398;&#20064;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#22312;&#20110;&#30830;&#31435;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;&#65292;&#22240;&#20026;&#22312;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#19968;&#32452;&#33258;&#31169;&#30340;&#20010;&#20307;&#26234;&#33021;&#20307;&#26080;&#27861;&#20445;&#35777;&#19982;&#20854;&#32852;&#21512;&#31574;&#30053;&#21516;&#26102;&#23398;&#20064;&#26102;&#20250;&#25910;&#25947;&#12290;&#36825;&#19982;&#22823;&#22810;&#25968;&#21333;&#26234;&#33021;&#20307;&#29615;&#22659;&#24418;&#25104;&#40092;&#26126;&#23545;&#27604;&#65292;&#24182;&#20026;&#23454;&#38469;&#24212;&#29992;&#37096;&#32626;&#24102;&#26469;&#20102;&#31105;&#38178;&#38556;&#30861;&#65292;&#22240;&#20026;&#23427;&#22312;&#31995;&#32479;&#30340;&#38271;&#26399;&#34892;&#20026;&#20013;&#24341;&#20837;&#20102;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24212;&#29992;&#20102;&#20174;&#21160;&#24577;&#31995;&#32479;&#30340;&#23450;&#24615;&#29702;&#35770;&#20013;&#24050;&#30693;&#30340;&#25429;&#33719;&#21306;&#22495;&#27010;&#24565;&#65292;&#20026;&#20998;&#25955;&#24335;&#23398;&#20064;&#22312;&#32852;&#21512;&#31574;&#30053;&#31354;&#38388;&#20013;&#24314;&#31435;&#23433;&#20840;&#38598;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20108;&#36827;&#21046;&#20998;&#21306;&#31639;&#27861;&#26469;&#39564;&#35777;&#20505;&#36873;&#38598;&#26159;&#21542;&#24418;&#25104;&#20102;&#31995;&#32479;&#20869;&#24050;&#30693;&#23398;&#20064;&#21160;&#24577;&#30340;&#25429;&#33719;&#21306;&#22495;&#65292;&#20197;&#21450;&#19968;&#31181;&#21551;&#21457;&#24335;&#37319;&#26679;&#31639;&#27861;&#65292;&#29992;&#20110;&#26410;&#30693;&#23398;&#20064;&#21160;&#24577;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#28436;&#31034;&#20102;&#22312;&#35268;&#33539;&#21270;&#30340;Dirac&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#21644;&#20132;&#36890;&#25511;&#21046;&#22330;&#26223;&#20013;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the main challenges of multi-agent learning lies in establishing convergence of the algorithms, as, in general, a collection of individual, self-serving agents is not guaranteed to converge with their joint policy, when learning concurrently. This is in stark contrast to most single-agent environments, and sets a prohibitive barrier for deployment in practical applications, as it induces uncertainty in long term behavior of the system. In this work, we apply the concept of trapping regions, known from qualitative theory of dynamical systems, to create safety sets in the joint strategy space for decentralized learning. We propose a binary partitioning algorithm for verification that candidate sets form trapping regions in systems with known learning dynamics, and a heuristic sampling algorithm for scenarios where learning dynamics are not known. We demonstrate the applications to a regularized version of Dirac Generative Adversarial Network, a four-intersection traffic control sc
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2302.11939</link><description>&lt;p&gt;
&#19968;&#31449;&#24335;&#35299;&#20915;&#26041;&#26696;&#65306;&#21033;&#29992;&#39044;&#35757;&#32451; LM &#36827;&#34892;&#24378;&#22823;&#30340;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
One Fits All:Power General Time Series Analysis by Pretrained LM. (arXiv:2302.11939v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11939
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026; Frozen Pretrained Transformer (FPT) &#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#20219;&#21153;&#30340;&#24494;&#35843;&#65292;&#36827;&#32780;&#20351;&#20854;&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#22791;&#30528;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#39044;&#35757;&#32451;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702; (NLP) &#21644;&#35745;&#31639;&#26426;&#35270;&#35273; (CV) &#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#65292;&#20294;&#22312;&#36890;&#29992;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#39046;&#22495;&#21462;&#24471;&#30340;&#36827;&#23637;&#26377;&#38480;&#12290;&#19982; NLP &#21644; CV &#19981;&#21516;&#30340;&#26159;&#65292;&#36825;&#20123;&#39046;&#22495;&#37319;&#29992;&#32479;&#19968;&#27169;&#22411;&#21363;&#21487;&#25191;&#34892;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#32780;&#22312;&#27599;&#20010;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20219;&#21153;&#20013;&#65292;&#19987;&#38376;&#35774;&#35745;&#30340;&#26041;&#27861;&#20173;&#28982;&#21344;&#25454;&#20027;&#23548;&#22320;&#20301;&#65292;&#22914;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#12290;&#38459;&#30861;&#39044;&#35757;&#32451;&#27169;&#22411;&#21457;&#23637;&#30340;&#20027;&#35201;&#25361;&#25112;&#26159;&#32570;&#20047;&#22823;&#37327;&#29992;&#20110;&#35757;&#32451;&#30340;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#21033;&#29992;&#20174;&#25968;&#21313;&#20159;&#26631;&#35760;&#35757;&#32451;&#20986;&#26469;&#30340;&#35821;&#35328;&#25110; CV &#27169;&#22411;&#65292;&#26469;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36991;&#20813;&#25913;&#21464;&#39044;&#35757;&#32451;&#35821;&#35328;&#25110;&#22270;&#20687;&#27169;&#22411;&#20013;&#27531;&#24046;&#22359;&#20013;&#30340;&#33258;&#27880;&#24847;&#21147;&#21644;&#21069;&#21521;&#20256;&#36882;&#23618;&#12290;&#36825;&#31181;&#27169;&#22411;&#34987;&#31216;&#20026;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120; (FPT)&#65292;&#36890;&#36807;&#23545;&#28041;&#21450;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#25152;&#26377;&#20027;&#35201;&#31867;&#22411;&#30340;&#20219;&#21153;&#36827;&#34892;&#24494;&#35843;&#36827;&#34892;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#31867;&#12289;&#24322;&#24120;&#26816;&#27979;&#12289;&#39044;&#27979;&#21644;&#23569;&#26679;&#26412;&#23398;&#20064;&#31561;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;FPT &#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#37117;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although we have witnessed great success of pre-trained models in natural language processing (NLP) and computer vision (CV), limited progress has been made for general time series analysis. Unlike NLP and CV where a unified model can be used to perform different tasks, specially designed approach still dominates in each time series analysis task such as classification, anomaly detection, forecasting, and few-shot learning. The main challenge that blocks the development of pre-trained model for time series analysis is the lack of a large amount of data for training. In this work, we address this challenge by leveraging language or CV models, pre-trained from billions of tokens, for time series analysis. Specifically, we refrain from altering the self-attention and feedforward layers of the residual blocks in the pre-trained language or image model. This model, known as the Frozen Pretrained Transformer (FPT), is evaluated through fine-tuning on all major types of tasks involving time s
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;"Growing Steerable Neural Cellular Automata"&#65292;&#36890;&#36807;&#20351;&#27599;&#20010;&#32454;&#32990;&#36127;&#36131;&#20854;&#33258;&#24049;&#30340;&#26041;&#21521;&#65292;&#20135;&#29983;&#20102;&#20855;&#26377;&#21464;&#21270;&#26041;&#21521;&#30340;&#32454;&#32990;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2302.10197</link><description>&lt;p&gt;
&#29983;&#38271;&#21487;&#25805;&#32437;&#31070;&#32463;&#20803;&#32454;&#32990;&#33258;&#21160;&#26426;
&lt;/p&gt;
&lt;p&gt;
Growing Steerable Neural Cellular Automata. (arXiv:2302.10197v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10197
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;"Growing Steerable Neural Cellular Automata"&#65292;&#36890;&#36807;&#20351;&#27599;&#20010;&#32454;&#32990;&#36127;&#36131;&#20854;&#33258;&#24049;&#30340;&#26041;&#21521;&#65292;&#20135;&#29983;&#20102;&#20855;&#26377;&#21464;&#21270;&#26041;&#21521;&#30340;&#32454;&#32990;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#20803;&#32454;&#32990;&#33258;&#21160;&#26426;&#65288;NCA&#65289;&#27169;&#22411;&#26174;&#31034;&#20986;&#20102;&#24778;&#20154;&#30340;&#27169;&#24335;&#24418;&#25104;&#33021;&#21147;&#21644;&#28304;&#20110;&#23616;&#37096;&#21327;&#35843;&#30340;&#22797;&#26434;&#20840;&#23616;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22312;&#21407;&#22987;&#30340;NCA&#23454;&#29616;&#20013;&#65292;&#32454;&#32990;&#26080;&#27861;&#35843;&#25972;&#33258;&#24049;&#30340;&#26041;&#21521;&#65292;&#23450;&#21521;&#26159;&#30001;&#27169;&#22411;&#35774;&#35745;&#24072;&#22312;&#22806;&#37096;&#23450;&#21521;&#12290;&#26368;&#36817;&#30340;&#21508;&#21521;&#21516;&#24615;NCA&#21464;&#20307;&#65288;Growing Isotropic Neural Cellular Automata&#65289;&#36890;&#36807;&#28040;&#38500;&#23545;&#20854;&#37051;&#22495;&#20013;&#31354;&#38388;&#29366;&#24577;&#26799;&#24230;&#24863;&#30693;&#30340;&#20381;&#36182;&#24615;&#65292;&#20351;&#27169;&#22411;&#19982;&#23450;&#21521;&#26080;&#20851; - &#32454;&#32990;&#26080;&#27861;&#20174;&#19978;&#21040;&#19979;&#25110;&#20174;&#24038;&#21040;&#21491;&#65292;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#19981;&#21516;&#30340;&#26041;&#27861;&#37325;&#26032;&#23457;&#35270;NCA&#65306;&#25105;&#20204;&#20351;&#27599;&#20010;&#32454;&#32990;&#36127;&#36131;&#20854;&#33258;&#24049;&#30340;&#23450;&#21521;&#65292;&#20801;&#35768;&#20854;&#26681;&#25454;&#21487;&#35843;&#25972;&#30340;&#20869;&#37096;&#29366;&#24577;&#8220;&#36716;&#21521;&#8221;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#21487;&#25805;&#32437;NCA&#21253;&#21547;&#23884;&#20837;&#21516;&#19968;&#27169;&#24335;&#20013;&#20855;&#26377;&#19981;&#21516;&#26041;&#21521;&#30340;&#32454;&#32990;&#12290; &#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#34429;&#28982;&#21508;&#21521;&#21516;&#24615;NCA&#27809;&#26377;&#23450;&#21521;&#65292;&#21487;&#25805;&#32437;&#30340;NCA&#20855;&#26377;&#25163;&#24615;&#65306;&#23427;&#20204;&#26377;&#39044;&#23450;
&lt;/p&gt;
&lt;p&gt;
Neural Cellular Automata (NCA) models have shown remarkable capacity for pattern formation and complex global behaviors stemming from local coordination. However, in the original implementation of NCA, cells are incapable of adjusting their own orientation, and it is the responsibility of the model designer to orient them externally. A recent isotropic variant of NCA (Growing Isotropic Neural Cellular Automata) makes the model orientation-independent - cells can no longer tell up from down, nor left from right - by removing its dependency on perceiving the gradient of spatial states in its neighborhood. In this work, we revisit NCA with a different approach: we make each cell responsible for its own orientation by allowing it to "turn" as determined by an adjustable internal state. The resulting Steerable NCA contains cells of varying orientation embedded in the same pattern. We observe how, while Isotropic NCA are orientation-agnostic, Steerable NCA have chirality: they have a predete
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#36755;&#20837;&#30340;&#27491;&#30830;&#20272;&#35745;&#65292;&#36890;&#36807;&#25193;&#23637;InfoNCE&#30446;&#26631;&#21644;&#32534;&#30721;&#22120;&#20197;&#39044;&#27979;&#28508;&#21464;&#37327;&#20998;&#24067;&#26469;&#23454;&#29616;&#65292;&#22312;&#35745;&#31639;&#24050;&#30693;&#26597;&#35810;&#22270;&#20687;&#30340;&#21487;&#20449;&#21306;&#38388;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2302.02865</link><description>&lt;p&gt;
&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#24674;&#22797;&#20102;&#19981;&#30830;&#23450;&#24615;&#36755;&#20837;&#30340;&#27491;&#30830;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs. (arXiv:2302.02865v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02865
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21033;&#29992;&#27010;&#29575;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#24674;&#22797;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#36755;&#20837;&#30340;&#27491;&#30830;&#20272;&#35745;&#65292;&#36890;&#36807;&#25193;&#23637;InfoNCE&#30446;&#26631;&#21644;&#32534;&#30721;&#22120;&#20197;&#39044;&#27979;&#28508;&#21464;&#37327;&#20998;&#24067;&#26469;&#23454;&#29616;&#65292;&#22312;&#35745;&#31639;&#24050;&#30693;&#26597;&#35810;&#22270;&#20687;&#30340;&#21487;&#20449;&#21306;&#38388;&#26041;&#38754;&#20855;&#26377;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#27604;&#23398;&#20064;&#32534;&#30721;&#22120;&#34987;&#35777;&#26126;&#21487;&#20197;&#32763;&#36716;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#65306;&#23427;&#20204;&#21487;&#20197;&#23558;&#27599;&#20010;&#36755;&#20837;&#65288;&#22914;&#22270;&#20687;&#65289;&#32534;&#30721;&#25104;&#29983;&#25104;&#35813;&#22270;&#20687;&#30340;&#30495;&#23454;&#28508;&#21464;&#37327;&#65288;Zimmermann&#31561;&#20154;&#65292;2021&#65289;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#30340;&#35266;&#23519;&#32467;&#26524;&#36890;&#24120;&#23384;&#22312;&#20869;&#22312;&#30340;&#27169;&#31946;&#24615;&#12290;&#20363;&#22914;&#65292;&#22270;&#20687;&#21487;&#33021;&#27169;&#31946;&#25110;&#21482;&#26174;&#31034;3D&#29289;&#20307;&#30340;2D&#35270;&#22270;&#65292;&#22240;&#27492;&#21487;&#33021;&#26377;&#22810;&#20010;&#28508;&#21464;&#37327;&#29983;&#25104;&#23427;&#20204;&#12290;&#36825;&#20351;&#24471;&#28508;&#21464;&#37327;&#30340;&#30495;&#23454;&#21518;&#39564;&#27010;&#29575;&#20855;&#26377;&#24322;&#26041;&#24046;&#19981;&#30830;&#23450;&#24615;&#12290;&#22312;&#36825;&#31181;&#35774;&#32622;&#19979;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#24120;&#35265;&#30340;InfoNCE&#30446;&#26631;&#21644;&#32534;&#30721;&#22120;&#65292;&#20197;&#39044;&#27979;&#28508;&#21464;&#37327;&#20998;&#24067;&#32780;&#19981;&#26159;&#28857;&#12290;&#25105;&#20204;&#35777;&#26126;&#36825;&#20123;&#20998;&#24067;&#24674;&#22797;&#20102;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#27491;&#30830;&#21518;&#39564;&#20998;&#24067;&#65292;&#21253;&#25324;&#20854;&#19981;&#30830;&#23450;&#24615;&#27700;&#24179;&#30340;&#20272;&#35745;&#65292;&#35813;&#20272;&#35745;&#23384;&#22312;&#28508;&#21464;&#37327;&#31354;&#38388;&#30340;&#26059;&#36716;&#12290;&#38500;&#20102;&#25552;&#20379;&#26657;&#20934;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#20043;&#22806;&#65292;&#36825;&#20123;&#21518;&#39564;&#20998;&#24067;&#36824;&#20801;&#35768;&#22312;&#22270;&#20687;&#26816;&#32034;&#20013;&#35745;&#31639;&#21487;&#20449;&#21306;&#38388;&#12290;&#23427;&#20204;&#21253;&#25324;&#20855;&#26377;&#19982;&#32473;&#23450;&#26597;&#35810;&#30456;&#21516;&#30340;&#28508;&#21464;&#37327;&#30340;&#22270;&#20687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastively trained encoders have recently been proven to invert the data-generating process: they encode each input, e.g., an image, into the true latent vector that generated the image (Zimmermann et al., 2021). However, real-world observations often have inherent ambiguities. For instance, images may be blurred or only show a 2D view of a 3D object, so multiple latents could have generated them. This makes the true posterior for the latent vector probabilistic with heteroscedastic uncertainty. In this setup, we extend the common InfoNCE objective and encoders to predict latent distributions instead of points. We prove that these distributions recover the correct posteriors of the data-generating process, including its level of aleatoric uncertainty, up to a rotation of the latent space. In addition to providing calibrated uncertainty estimates, these posteriors allow the computation of credible intervals in image retrieval. They comprise images with the same latent as a given quer
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#33258;&#25105;&#24847;&#35782;&#20215;&#20540;&#20998;&#35299;&#26694;&#26550;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23436;&#20840;&#25682;&#24323;&#20102;&#20010;&#20307;&#20840;&#23616;&#26368;&#22823;&#20540;&#30340;&#21069;&#25552;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.02180</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#37325;&#33258;&#25105;&#24847;&#35782;&#20215;&#20540;&#20998;&#35299;&#26694;&#26550;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Dual Self-Awareness Value Decomposition Framework without Individual Global Max for Cooperative Multi-Agent Reinforcement Learning. (arXiv:2302.02180v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.02180
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#37325;&#33258;&#25105;&#24847;&#35782;&#20215;&#20540;&#20998;&#35299;&#26694;&#26550;&#30340;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23436;&#20840;&#25682;&#24323;&#20102;&#20010;&#20307;&#20840;&#23616;&#26368;&#22823;&#20540;&#30340;&#21069;&#25552;&#65292;&#20855;&#26377;&#33391;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#22810;&#26234;&#20307;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#20215;&#20540;&#20998;&#35299;&#26041;&#27861;&#24050;&#32463;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#12290;&#28982;&#32780;&#65292;&#20960;&#20046;&#25152;&#26377;&#29616;&#26377;&#26041;&#27861;&#37117;&#36981;&#24490;&#20010;&#20307;&#20840;&#23616;&#26368;&#22823;&#20540;&#65288;IGM&#65289;&#25110;&#20854;&#21464;&#20307;&#30340;&#21407;&#21017;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#38382;&#39064;&#35299;&#20915;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#24515;&#29702;&#23398;&#20013;&#21452;&#37325;&#33258;&#25105;&#24847;&#35782;&#27010;&#24565;&#30340;&#21452;&#37325;&#33258;&#25105;&#24847;&#35782;&#20215;&#20540;&#20998;&#35299;&#26694;&#26550;&#65292;&#23436;&#20840;&#25682;&#24323;&#20102;IGM&#21069;&#25552;&#12290;&#27599;&#20010;&#26234;&#33021;&#20307;&#21253;&#25324;&#33258;&#25105;&#31574;&#30053;&#20197;&#36827;&#34892;&#21160;&#20316;&#36873;&#25321;&#21644;&#26367;&#36523;&#20215;&#20540;&#20989;&#25968;&#20197;&#35299;&#20915;&#20449;&#29992;&#20998;&#37197;&#38382;&#39064;&#12290;&#20215;&#20540;&#20989;&#25968;&#20998;&#35299;&#21487;&#20197;&#21033;&#29992;&#26174;&#24335;&#25628;&#32034;&#36807;&#31243;&#24573;&#30053;IGM&#20551;&#35774;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21453;&#33258;&#25105;&#25506;&#32034;&#26426;&#21046;&#65292;&#20197;&#36991;&#20813;&#31639;&#27861;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#12290;&#20316;&#20026;&#31532;&#19968;&#20010;&#23436;&#20840;&#19981;&#29992;IGM&#30340;&#20215;&#20540;&#20998;&#35299;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#21508;&#31181;&#21327;&#20316;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26399;&#26395;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Value decomposition methods have gained popularity in the field of cooperative multi-agent reinforcement learning. However, almost all existing methods follow the principle of Individual Global Max (IGM) or its variants, which limits their problem-solving capabilities. To address this, we propose a dual self-awareness value decomposition framework, inspired by the notion of dual self-awareness in psychology, that entirely rejects the IGM premise. Each agent consists of an ego policy for action selection and an alter ego value function to solve the credit assignment problem. The value function factorization can ignore the IGM assumption by utilizing an explicit search procedure. On the basis of the above, we also suggest a novel anti-ego exploration mechanism to avoid the algorithm becoming stuck in a local optimum. As the first fully IGM-free value decomposition method, our proposed framework achieves desirable performance in various cooperative tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#24102;&#24179;&#22343;&#26631;&#20934;&#32422;&#26463; MDP &#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2302.00808</link><description>&lt;p&gt;
&#24179;&#22343;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Average-Constrained Policy Optimization. (arXiv:2302.00808v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00808
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#24102;&#24179;&#22343;&#26631;&#20934;&#32422;&#26463; MDP &#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65292;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#21046;&#26465;&#20214;&#30340;&#24378;&#21270;&#23398;&#20064;&#23545;&#20110;&#21508;&#31181;&#24212;&#29992;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#36890;&#24120;&#65292;&#24179;&#22343;&#26631;&#20934;&#27604;&#25240;&#25187;&#26631;&#20934;&#26356;&#21512;&#36866;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;&#24179;&#22343;&#38480;&#21046; CMDP &#30340;&#24378;&#21270;&#23398;&#20064;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#38024;&#23545;&#25240;&#25187;&#38480;&#21046; RL &#38382;&#39064;&#35774;&#35745;&#30340;&#31639;&#27861;&#36890;&#24120;&#22312;&#24179;&#22343; CMDP &#29615;&#22659;&#19979;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#20989;&#25968;&#36924;&#36817;&#31639;&#27861;&#30340;&#24102;&#24179;&#22343;&#26631;&#20934;&#32422;&#26463; MDP &#30340;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#12290;&#24179;&#22343;&#38480;&#21046;&#31574;&#30053;&#20248;&#21270;&#65288;ACPO&#65289;&#31639;&#27861;&#30340;&#28789;&#24863;&#26469;&#33258;&#22522;&#20110;&#20449;&#20219;&#21306;&#22495;&#26041;&#27861;&#30340;&#33879;&#21517; PPO &#31867;&#31639;&#27861;&#12290;&#25105;&#20204;&#21457;&#23637;&#20102;&#22522;&#26412;&#30340;&#24179;&#22343; MDP &#25935;&#24863;&#24615;&#29702;&#35770;&#65292;&#28982;&#21518;&#22312;&#31639;&#27861;&#35774;&#35745;&#20013;&#20351;&#29992;&#30456;&#24212;&#30340;&#30028;&#38480;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20854;&#24615;&#33021;&#30340;&#29702;&#35770;&#20445;&#35777;&#65292;&#24182;&#36890;&#36807;&#22312;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340; MuJoCo &#29615;&#22659;&#20013;&#36827;&#34892;&#22823;&#37327;&#23454;&#39564;&#24037;&#20316;&#65292;&#23637;&#31034;&#20102;&#35813;&#31639;&#27861;&#19982;&#20854;&#20182;&#24120;&#35268;&#31639;&#27861;&#30456;&#27604;&#30340;&#21331;&#36234;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) with constraints is becoming an increasingly important problem for various applications. Often, the average criterion is more suitable than the discounted criterion. Yet, RL for average criterion-constrained MDPs remains a challenging problem. Algorithms designed for discounted constrained RL problems often do not perform well for the average CMDP setting. In this paper, we introduce a new policy optimization with function approximation algorithm for constrained MDPs with the average criterion. The Average-Constrained Policy Optimization (ACPO) algorithm is inspired by the famed PPO-type algorithms based on trust region methods. We develop basic sensitivity theory for average MDPs, and then use the corresponding bounds in the design of the algorithm. We provide theoretical guarantees on its performance, and through extensive experimental work in various challenging MuJoCo environments, show the superior performance of the algorithm when compared to other sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33976;&#39311;&#20248;&#21183;&#22312;&#21033;&#29992;&#36807;&#21435;&#32463;&#39564;&#30340;&#21516;&#26102;&#36981;&#24490;&#31283;&#23450;&#30340;&#22312;&#32447;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;</title><link>http://arxiv.org/abs/2302.00533</link><description>&lt;p&gt;
&#33976;&#39311;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Distillation Policy Optimization. (arXiv:2302.00533v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00533
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#19968;&#31181;&#28436;&#21592;-&#35780;&#35770;&#23478;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#33976;&#39311;&#20248;&#21183;&#22312;&#21033;&#29992;&#36807;&#21435;&#32463;&#39564;&#30340;&#21516;&#26102;&#36981;&#24490;&#31283;&#23450;&#30340;&#22312;&#32447;&#31574;&#30053;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#20197;&#36866;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#28436;&#21592;-&#35780;&#35770;&#23478;&#23398;&#20064;&#26694;&#26550;&#65292;&#23427;&#20511;&#37492;&#20102;&#20998;&#24067;&#24335;&#23398;&#20064;&#30340;&#35270;&#35282;&#21644;&#20004;&#31181;&#31574;&#30053;&#25913;&#36827;&#25968;&#25454;&#30340;&#20132;&#21449;&#34701;&#21512;&#65292;&#23454;&#29616;&#20102;&#24555;&#36895;&#23398;&#20064;&#24182;&#21487;&#24212;&#29992;&#20110;&#24191;&#27867;&#30340;&#31639;&#27861;&#31867;&#21035;&#12290;&#22312;&#35813;&#26694;&#26550;&#20013;&#65292;&#39318;&#20808;&#25552;&#20986;&#20102;&#26041;&#24046;&#20943;&#23569;&#26426;&#21046;&#65292;&#20363;&#22914;&#32479;&#19968;&#20248;&#21183;&#20272;&#35745;&#22120; (UAE) &#21644;&#19968;&#20010;&#23398;&#20064;&#30340;&#22522;&#32447;&#65292;&#19981;&#20165;&#26159;&#36830;&#25509;&#21040;&#21160;&#20316;&#20540;&#20989;&#25968;&#30340;&#26725;&#26753;&#65292;&#36824;&#33021;&#25552;&#28860;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
On-policy algorithms are supposed to be stable, however, sample-intensive yet. Off-policy algorithms utilizing past experiences are deemed to be sample-efficient, nevertheless, unstable in general. Can we design an algorithm that can employ the off-policy data, while exploit the stable learning by sailing along the course of the on-policy walkway? In this paper, we present an actor-critic learning framework that borrows the distributional perspective of interest to evaluate, and cross-breeds two sources of the data for policy improvement, which enables fast learning and can be applied to a wide class of algorithms. In its backbone, the variance reduction mechanisms, such as unified advantage estimator (UAE), that extends generalized advantage estimator (GAE) to be applicable on any state-dependent baseline, and a learned baseline, that is competent to stabilize the policy gradient, are firstly put forward to not merely be a bridge to the action-value function but also distill the advan
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#29305;&#24449;&#32447;&#24615;&#35843;&#21046;&#30340;&#38543;&#26426;&#32593;&#32476;&#30910;&#21387;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#25506;&#32034;&#65292;&#36991;&#20813;&#20102;&#21306;&#20998;&#24230;&#30340;&#38382;&#39064;&#65292;&#22312; D4RL &#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#21487;&#19982;&#38598;&#25104;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.13616</link><description>&lt;p&gt;
&#38543;&#26426;&#32593;&#32476;&#30910;&#21387;&#23545;&#38450;&#27490;&#25506;&#32034;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Anti-Exploration by Random Network Distillation. (arXiv:2301.13616v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.13616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#29305;&#24449;&#32447;&#24615;&#35843;&#21046;&#30340;&#38543;&#26426;&#32593;&#32476;&#30910;&#21387;&#31639;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#38450;&#27490;&#25506;&#32034;&#65292;&#36991;&#20813;&#20102;&#21306;&#20998;&#24230;&#30340;&#38382;&#39064;&#65292;&#22312; D4RL &#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#21487;&#19982;&#38598;&#25104;&#26041;&#27861;&#30456;&#23218;&#32654;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#38543;&#26426;&#32593;&#32476;&#30910;&#21387; (RND) &#22312;&#21508;&#31181;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#22312;&#29992;&#20316;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#24809;&#32602;&#36234;&#30028;&#25805;&#20316;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#22120;&#26102;&#65292;&#23427;&#34987;&#35777;&#26126;&#19981;&#20855;&#26377;&#36275;&#22815;&#30340;&#21306;&#20998;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#23457;&#35270;&#20102;&#36825;&#20123;&#32467;&#26524;&#65292;&#24182;&#34920;&#26126;&#65292;&#36890;&#36807;&#23545; RND &#20808;&#39564;&#36827;&#34892;&#26420;&#32032;&#30340;&#35843;&#33410;&#36873;&#25321;&#65292;&#28436;&#21592;&#26377;&#25928;&#22320;&#26368;&#23567;&#21270;&#21453;&#25506;&#32034;&#22870;&#21169;&#21464;&#24471;&#19981;&#21487;&#34892;&#65292;&#24182;&#19988;&#21306;&#20998;&#24230;&#19981;&#20877;&#26159;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#36890;&#36807;&#22522;&#20110;&#29305;&#24449;&#32447;&#24615;&#35843;&#21046; (FiLM) &#30340;&#35843;&#33410;&#26469;&#36991;&#20813;&#36825;&#31181;&#23616;&#38480;&#24615;&#65292;&#20174;&#32780;&#24471;&#21040;&#19968;&#20010;&#31616;&#21333;&#32780;&#39640;&#25928;&#30340;&#22522;&#20110;&#36719;&#34892;&#20026;&#32773;-&#35780;&#35770;&#23478;&#31639;&#27861;&#30340;&#26080;&#38598;&#25104;&#31639;&#27861;&#12290;&#25105;&#20204;&#22312; D4RL &#22522;&#20934;&#27979;&#35797;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#23427;&#33021;&#22815;&#23454;&#29616;&#19982;&#22522;&#20110;&#38598;&#25104;&#30340;&#26041;&#27861;&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#24182;&#26174;&#33879;&#20248;&#20110;&#26080;&#38598;&#25104;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of Random Network Distillation (RND) in various domains, it was shown as not discriminative enough to be used as an uncertainty estimator for penalizing out-of-distribution actions in offline reinforcement learning. In this paper, we revisit these results and show that, with a naive choice of conditioning for the RND prior, it becomes infeasible for the actor to effectively minimize the anti-exploration bonus and discriminativity is not an issue. We show that this limitation can be avoided with conditioning based on Feature-wise Linear Modulation (FiLM), resulting in a simple and efficient ensemble-free algorithm based on Soft Actor-Critic. We evaluate it on the D4RL benchmark, showing that it is capable of achieving performance comparable to ensemble-based methods and outperforming ensemble-free approaches by a wide margin.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#37319;&#29992;&#21069;&#32512;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11719</link><description>&lt;p&gt;
&#23558;&#30693;&#35782;&#32435;&#20837;&#25991;&#26723;&#25688;&#35201;&#29983;&#25104;&#20013;&#65306;&#22522;&#20110;GPT-2&#30340;&#21069;&#32512;&#35843;&#25972;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Incorporating Knowledge into Document Summarisation: an Application of Prefix-Tuning on GPT-2. (arXiv:2301.11719v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#65292;&#20855;&#20307;&#37319;&#29992;&#21069;&#32512;&#35843;&#25972;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#27492;&#26041;&#27861;&#21487;&#20197;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#32780;&#19988;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29616;&#22312;&#25991;&#26723;&#25688;&#35201;&#25216;&#26415;&#24471;&#21040;&#20102;&#24456;&#22823;&#30340;&#21457;&#23637;&#65292;&#20294;&#26159;&#29983;&#25104;&#30340;&#25688;&#35201;&#21644;&#21407;&#22987;&#25991;&#26412;&#20043;&#38388;&#30340;&#20107;&#23454;&#19981;&#19968;&#33268;&#20173;&#28982;&#26102;&#26377;&#21457;&#29983;&#12290;&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#37319;&#29992;&#25552;&#31034;&#26469;&#23558;&#20107;&#23454;&#30693;&#35782;&#32435;&#20837;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20855;&#20307;&#30740;&#31350;&#20102;&#21069;&#32512;&#35843;&#25972;&#65292;&#23427;&#20351;&#29992;&#19968;&#32452;&#21487;&#35757;&#32451;&#30340;&#36830;&#32493;&#21069;&#32512;&#25552;&#31034;&#21644;&#31163;&#25955;&#33258;&#28982;&#35821;&#35328;&#25552;&#31034;&#26469;&#24110;&#21161;&#25688;&#35201;&#29983;&#25104;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#35757;&#32451;&#30340;&#21069;&#32512;&#21487;&#20197;&#24110;&#21161;&#25688;&#35201;&#27169;&#22411;&#20934;&#30830;&#22320;&#20174;&#31163;&#25955;&#25552;&#31034;&#20013;&#25552;&#21462;&#20449;&#24687;&#65292;&#20174;&#32780;&#29983;&#25104;&#20445;&#30041;&#30693;&#35782;&#30340;&#25688;&#35201;&#65292;&#36825;&#20123;&#25688;&#35201;&#22312;&#20107;&#23454;&#19978;&#19982;&#31163;&#25955;&#25552;&#31034;&#19968;&#33268;&#12290;&#29983;&#25104;&#30340;&#25688;&#35201;&#30340;ROUGE&#25913;&#36827;&#34920;&#26126;&#65292;&#23558;&#20107;&#23454;&#30693;&#35782;&#26126;&#30830;&#22320;&#28155;&#21152;&#21040;&#25688;&#35201;&#29983;&#25104;&#36807;&#31243;&#20013;&#21487;&#20197;&#25552;&#21319;&#25972;&#20307;&#24615;&#33021;&#65292;&#26174;&#31034;&#20986;&#22312;&#20854;&#20182;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#24212;&#29992;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the great development of document summarisation techniques nowadays, factual inconsistencies between the generated summaries and the original texts still occur from time to time. This study explores the possibility of adopting prompts to incorporate factual knowledge into generated summaries. We specifically study prefix-tuning that uses a set of trainable continuous prefix prompts together with discrete natural language prompts to aid summary generation. Experimental results demonstrate that the trainable prefixes can help the summarisation model extract information from discrete prompts precisely, thus generating knowledge-preserving summaries that are factually consistent with the discrete prompts. The ROUGE improvements of the generated summaries indicate that explicitly adding factual knowledge into the summarisation process could boost the overall performance, showing great potential for applying it to other natural language processing tasks.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#30340;RNA&#35774;&#35745;&#27969;&#31243;&#65292;&#36890;&#36807;&#26500;&#24314;&#22823;&#22411;&#25968;&#25454;&#38598;&#24182;&#35774;&#35745;&#20840;&#38754;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;RNA&#24207;&#21015;&#35774;&#35745;&#12290;</title><link>http://arxiv.org/abs/2301.10774</link><description>&lt;p&gt;
&#22522;&#20110;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#30340;RNA&#19977;&#32423;&#32467;&#26500;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design. (arXiv:2301.10774v2 [q-bio.BM] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10774
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#30340;RNA&#35774;&#35745;&#27969;&#31243;&#65292;&#36890;&#36807;&#26500;&#24314;&#22823;&#22411;&#25968;&#25454;&#38598;&#24182;&#35774;&#35745;&#20840;&#38754;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#27861;&#65292;&#23454;&#29616;&#20102;&#26356;&#39640;&#25928;&#30340;RNA&#24207;&#21015;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#20154;&#24037;&#26234;&#33021;&#24050;&#22312;&#25581;&#31034;&#29983;&#29289;&#22823;&#20998;&#23376;&#30340;&#19968;&#32423;&#24207;&#21015;&#19982;&#19977;&#32423;&#32467;&#26500;&#20043;&#38388;&#30340;&#20851;&#31995;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#30528;&#36827;&#23637;&#65292;&#20294;&#22522;&#20110;&#29305;&#23450;&#19977;&#32423;&#32467;&#26500;&#35774;&#35745;RNA&#24207;&#21015;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34429;&#28982;&#34507;&#30333;&#36136;&#35774;&#35745;&#20013;&#30340;&#29616;&#26377;&#26041;&#27861;&#24050;&#32463;&#24443;&#24213;&#25506;&#32034;&#20102;&#34507;&#30333;&#36136;&#20013;&#32467;&#26500;&#21040;&#24207;&#21015;&#30340;&#20381;&#36182;&#24615;&#65292;&#20294;RNA&#35774;&#35745;&#20173;&#38754;&#20020;&#32467;&#26500;&#22797;&#26434;&#24615;&#21644;&#25968;&#25454;&#31232;&#32570;&#24615;&#30340;&#22256;&#38590;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#34429;&#28982;RNA&#19982;&#34507;&#30333;&#36136;&#20849;&#20139;&#31867;&#20284;&#30340;&#32467;&#26500;&#32452;&#20998;&#65292;&#20294;&#30452;&#25509;&#23558;&#34507;&#30333;&#36136;&#35774;&#35745;&#26041;&#27861;&#31227;&#26893;&#21040;RNA&#35774;&#35745;&#20013;&#21364;&#26080;&#27861;&#21462;&#24471;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#31995;&#32479;&#26500;&#24314;&#25968;&#25454;&#39537;&#21160;&#30340;RNA&#35774;&#35745;&#27969;&#31243;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#22823;&#22411;&#12289;&#31934;&#24515;&#31574;&#21010;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#32467;&#26500;&#24314;&#27169;&#26041;&#27861;&#26469;&#34920;&#31034;&#22797;&#26434;&#30340;RNA&#19977;&#32423;&#32467;&#26500;&#12290;&#26356;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23618;&#27425;&#25968;&#25454;&#26377;&#25928;&#34920;&#31034;&#23398;&#20064;&#26694;&#26550;&#65292;&#23398;&#20064;&#32467;&#26500;&#34920;&#31034;&#30340;&#22810;&#20010;&#23618;&#27425;&#29305;&#24449;&#65292;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;RNA&#24207;&#21015;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
While artificial intelligence has made remarkable strides in revealing the relationship between biological macromolecules' primary sequence and tertiary structure, designing RNA sequences based on specified tertiary structures remains challenging. Though existing approaches in protein design have thoroughly explored structure-to-sequence dependencies in proteins, RNA design still confronts difficulties due to structural complexity and data scarcity. Adding to the problem, direct transplantation of protein design methodologies into RNA design fails to achieve satisfactory outcomes although sharing similar structural components. In this study, we aim to systematically construct a data-driven RNA design pipeline. We crafted a large, well-curated benchmark dataset and designed a comprehensive structural modeling approach to represent the complex RNA tertiary structure. More importantly, we proposed a hierarchical data-efficient representation learning framework that learns structural repre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;A-NeSI&#30340;&#26032;&#39062;PNL&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#36817;&#20284;&#25512;&#29702;&#65292;&#33021;&#22815;&#20445;&#35777;&#27010;&#29575;&#36923;&#36753;&#35821;&#20041;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;PNL&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#30340;&#28385;&#36275;&#12290;</title><link>http://arxiv.org/abs/2212.12393</link><description>&lt;p&gt;
A-NeSI: &#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#36817;&#20284;&#26041;&#27861;&#29992;&#20110;&#27010;&#29575;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
A-NeSI: A Scalable Approximate Method for Probabilistic Neurosymbolic Inference. (arXiv:2212.12393v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12393
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;A-NeSI&#30340;&#26032;&#39062;PNL&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#23454;&#29616;&#20102;&#36817;&#20284;&#25512;&#29702;&#65292;&#33021;&#22815;&#20445;&#35777;&#27010;&#29575;&#36923;&#36753;&#35821;&#20041;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;PNL&#30340;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#65292;&#33021;&#22815;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#30340;&#28385;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23558;&#31070;&#32463;&#32593;&#32476;&#19982;&#31526;&#21495;&#25512;&#29702;&#30456;&#32467;&#21512;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#24341;&#20837;&#30340;&#27010;&#29575;&#31070;&#32463;&#31526;&#21495;&#23398;&#20064;&#65288;PNL&#65289;&#26694;&#26550;&#65292;&#22914;DeepProbLog&#65292;&#25191;&#34892;&#25351;&#25968;&#26102;&#38388;&#30340;&#31934;&#30830;&#25512;&#29702;&#65292;&#38480;&#21046;&#20102;PNL&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#36817;&#20284;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#65288;A-NeSI&#65289;&#65306;&#19968;&#31181;&#26032;&#30340;PNL&#26694;&#26550;&#65292;&#23427;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#21487;&#25193;&#23637;&#30340;&#36817;&#20284;&#25512;&#29702;&#12290;A-NeSI 1) &#22312;&#19981;&#25913;&#21464;&#27010;&#29575;&#36923;&#36753;&#35821;&#20041;&#30340;&#24773;&#20917;&#19979;&#65292;&#20197;&#22810;&#39033;&#24335;&#26102;&#38388;&#25191;&#34892;&#36817;&#20284;&#25512;&#29702;&#65307;2) &#20351;&#29992;&#30001;&#32972;&#26223;&#30693;&#35782;&#29983;&#25104;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65307;3) &#21487;&#20197;&#29983;&#25104;&#26377;&#20851;&#39044;&#27979;&#30340;&#31526;&#21495;&#35299;&#37322;&#65307;4) &#21487;&#20197;&#22312;&#27979;&#35797;&#26102;&#38388;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#30340;&#28385;&#36275;&#65292;&#36825;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#38750;&#24120;&#37325;&#35201;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;A-NeSI&#26159;&#31532;&#19968;&#20010;&#33021;&#22815;&#35299;&#20915;&#20855;&#26377;&#25351;&#25968;&#32452;&#21512;&#25193;&#23637;&#30340;&#19977;&#31181;&#31070;&#32463;&#31526;&#21495;&#20219;&#21153;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;A-NeSI&#23454;&#29616;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#23433;&#20840;&#24615;&#65292;&#32780;&#27809;&#26377;&#24809;&#32602;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the problem of combining neural networks with symbolic reasoning. Recently introduced frameworks for Probabilistic Neurosymbolic Learning (PNL), such as DeepProbLog, perform exponential-time exact inference, limiting the scalability of PNL solutions. We introduce Approximate Neurosymbolic Inference (A-NeSI): a new framework for PNL that uses neural networks for scalable approximate inference. A-NeSI 1) performs approximate inference in polynomial time without changing the semantics of probabilistic logics; 2) is trained using data generated by the background knowledge; 3) can generate symbolic explanations of predictions; and 4) can guarantee the satisfaction of logical constraints at test time, which is vital in safety-critical applications. Our experiments show that A-NeSI is the first end-to-end method to solve three neurosymbolic tasks with exponential combinatorial scaling. Finally, our experiments show that A-NeSI achieves explainability and safety without a penalty in p
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21518;&#35265;&#20449;&#29992;&#20998;&#37197;&#36825;&#31181;&#20998;&#37197;&#20449;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22240;&#26524;&#32467;&#26500;&#30340;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2212.11636</link><description>&lt;p&gt;
&#26397;&#21521;&#22240;&#26524;&#36131;&#20219;&#21010;&#20998;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Causal Credit Assignment. (arXiv:2212.11636v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.11636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21518;&#35265;&#20449;&#29992;&#20998;&#37197;&#36825;&#31181;&#20998;&#37197;&#20449;&#29992;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#22240;&#26524;&#32467;&#26500;&#30340;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#22914;&#20309;&#26681;&#25454;&#34892;&#20026;&#30340;&#36129;&#29486;&#36866;&#24403;&#22320;&#23558;&#20449;&#29992;&#20998;&#37197;&#32473;&#26410;&#26469;&#30340;&#32467;&#26524;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#30446;&#21069;&#24191;&#27867;&#20351;&#29992;&#30340;&#20449;&#29992;&#20998;&#37197;&#26041;&#27861;&#30340;&#20551;&#35774;&#22312;&#20915;&#31574;&#25928;&#26524;&#19981;&#31435;&#21363;&#26174;&#29616;&#30340;&#20219;&#21153;&#20013;&#20855;&#26377;&#21155;&#21183;&#12290;&#27492;&#22806;&#65292;&#35813;&#26041;&#27861;&#21482;&#33021;&#35780;&#20272;&#20195;&#29702;&#24050;&#36873;&#25321;&#30340;&#21160;&#20316;&#65292;&#22240;&#27492;&#25928;&#29575;&#26497;&#20302;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#21518;&#35265;&#20449;&#29992;&#20998;&#37197;&#30340;&#28508;&#21147;&#21644;&#23616;&#38480;&#65292;&#24182;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#29615;&#22659;&#22240;&#26524;&#32467;&#26500;&#30340;&#20998;&#35299;&#29366;&#24577;&#34920;&#31034;&#26469;&#25552;&#39640;&#21518;&#35265;&#20449;&#29992;&#20998;&#37197;&#25928;&#29575;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adequately assigning credit to actions for future outcomes based on their contributions is a long-standing open challenge in Reinforcement Learning. The assumptions of the most commonly used credit assignment method are disadvantageous in tasks where the effects of decisions are not immediately evident. Furthermore, this method can only evaluate actions that have been selected by the agent, making it highly inefficient. Still, no alternative methods have been widely adopted in the field. Hindsight Credit Assignment is a promising, but still unexplored candidate, which aims to solve the problems of both long-term and counterfactual credit assignment. In this thesis, we empirically investigate Hindsight Credit Assignment to identify its main benefits, and key points to improve. Then, we apply it to factored state representations, and in particular to state representations based on the causal structure of the environment. In this setting, we propose a variant of Hindsight Credit Assignmen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; KGLM &#26550;&#26500;&#65292;&#23558;&#26032;&#30340;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#23618;&#25972;&#21512;&#36827;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#12290;&#20351;&#29992;&#20174;&#30693;&#35782;&#22270;&#35889;&#25552;&#21462;&#30340;&#19977;&#20803;&#32452;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#24471;&#21040;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2211.02744</link><description>&lt;p&gt;
KGLM: &#23558;&#30693;&#35782;&#22270;&#35889;&#32467;&#26500;&#25972;&#21512;&#36827;&#35821;&#35328;&#27169;&#22411;&#20197;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
KGLM: Integrating Knowledge Graph Structure in Language Models for Link Prediction. (arXiv:2211.02744v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.02744
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; KGLM &#26550;&#26500;&#65292;&#23558;&#26032;&#30340;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#23618;&#25972;&#21512;&#36827;&#35821;&#35328;&#27169;&#22411;&#65292;&#20351;&#20854;&#33021;&#22815;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#12290;&#20351;&#29992;&#20174;&#30693;&#35782;&#22270;&#35889;&#25552;&#21462;&#30340;&#19977;&#20803;&#32452;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#24182;&#36827;&#34892;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#24471;&#21040;&#20102;&#33391;&#22909;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#33021;&#22815;&#22312;&#22823;&#35268;&#27169;&#24773;&#20917;&#19979;&#34920;&#31034;&#22797;&#26434;&#20851;&#31995;&#65292;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#30693;&#35782;&#34920;&#31034;&#12289;&#38382;&#31572;&#21644;&#25512;&#33616;&#31995;&#32479;&#31561;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22270;&#35889;&#36890;&#24120;&#23384;&#22312;&#20449;&#24687;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#38656;&#35201;&#36827;&#34892;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#12290;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20986;&#33394;&#30340;&#25928;&#26524;&#65292;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#24573;&#30053;&#20102;&#30693;&#35782;&#22270;&#35889;&#25152;&#34164;&#21547;&#30340;&#22266;&#26377;&#20449;&#24687;&#65292;&#21363;&#23454;&#20307;&#21644;&#20851;&#31995;&#31867;&#22411;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;KGLM&#65288;Knowledge Graph Language Model&#65289;&#26550;&#26500;&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#23454;&#20307;/&#20851;&#31995;&#23884;&#20837;&#23618;&#65292;&#23427;&#23398;&#20064;&#21306;&#20998;&#19981;&#21516;&#30340;&#23454;&#20307;&#21644;&#20851;&#31995;&#31867;&#22411;&#65292;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#23398;&#20064;&#30693;&#35782;&#22270;&#35889;&#30340;&#32467;&#26500;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20351;&#29992;&#20174;&#30693;&#35782;&#22270;&#35889;&#25552;&#21462;&#30340;&#19977;&#20803;&#32452;&#36827;&#19968;&#27493;&#39044;&#35757;&#32451;&#36825;&#20123;&#39069;&#22806;&#23884;&#20837;&#23618;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#37319;&#29992;&#21518;&#32493;&#30340; link prediction &#20219;&#21153;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability of knowledge graphs to represent complex relationships at scale has led to their adoption for various needs including knowledge representation, question-answering, and recommendation systems. Knowledge graphs are often incomplete in the information they represent, necessitating the need for knowledge graph completion tasks. Pre-trained and fine-tuned language models have shown promise in these tasks although these models ignore the intrinsic information encoded in the knowledge graph, namely the entity and relation types. In this work, we propose the Knowledge Graph Language Model (KGLM) architecture, where we introduce a new entity/relation embedding layer that learns to differentiate distinctive entity and relation types, therefore allowing the model to learn the structure of the knowledge graph. In this work, we show that further pre-training the language models with this additional embedding layer using the triples extracted from the knowledge graph, followed by the sta
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#25104;&#20687;&#20998;&#26512;&#20013;&#22788;&#29702;3D&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#23558;&#20307;&#31215;&#25968;&#25454;&#36716;&#25442;&#20026;2D&#36229;&#32423;&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;2D&#32593;&#32476;&#36827;&#34892;&#20998;&#21106;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#26377;&#25928;&#22320;&#23884;&#20837;3D&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2205.02847</link><description>&lt;p&gt;
&#36229;&#32423;&#22270;&#20687;&#8212;&#8212;3D&#21307;&#23398;&#25104;&#20687;&#20998;&#26512;&#30340;&#20840;&#26032;&#20108;&#32500;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Super Images -- A New 2D Perspective on 3D Medical Imaging Analysis. (arXiv:2205.02847v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.02847
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#23398;&#25104;&#20687;&#20998;&#26512;&#20013;&#22788;&#29702;3D&#25968;&#25454;&#30340;&#26032;&#26041;&#27861;&#65292;&#21363;&#23558;&#20307;&#31215;&#25968;&#25454;&#36716;&#25442;&#20026;2D&#36229;&#32423;&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;2D&#32593;&#32476;&#36827;&#34892;&#20998;&#21106;&#65292;&#20197;&#25552;&#39640;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#35757;&#32451;&#20013;&#26377;&#25928;&#22320;&#23884;&#20837;3D&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#25104;&#20687;&#20998;&#26512;&#20013;&#65292;&#28145;&#24230;&#23398;&#20064;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#32463;&#24120;&#20381;&#36182;&#20307;&#31215;&#25968;&#25454;&#26469;&#20998;&#21106;&#21307;&#23398;&#22270;&#20687;&#65292;&#38656;&#35201;&#20351;&#29992;3D&#26550;&#26500;&#65292;&#36825;&#20123;&#26550;&#26500;&#22240;&#20854;&#25429;&#25417;interslice context&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#36190;&#36175;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#36825;&#20123;&#32593;&#32476;&#20013;&#20351;&#29992;&#20102;3D&#21367;&#31215;&#12289;&#26368;&#22823;&#27744;&#21270;&#12289;up-convolutions&#21644;&#20854;&#20182;&#25805;&#20316;&#65292;&#36825;&#20123;&#26550;&#26500;&#22312;&#26102;&#38388;&#21644;&#35745;&#31639;&#26041;&#38754;&#36890;&#24120;&#27604;&#23427;&#20204;&#30340;2D&#31561;&#20215;&#29289;&#26356;&#20302;&#25928;&#12290;&#27492;&#22806;&#65292;&#24456;&#23569;&#26377;3D&#39044;&#35757;&#32451;&#27169;&#22411;&#26435;&#37325;&#65292;&#39044;&#35757;&#32451;&#36890;&#24120;&#24456;&#38590;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#20108;&#32500;&#26041;&#27861;&#26469;&#26377;&#25928;&#22320;&#23884;&#20837;3D&#30693;&#35782;&#65292;&#21516;&#26102;&#22788;&#29702;3D&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#23558;&#20307;&#31215;&#25968;&#25454;&#36716;&#25442;&#20026;2D&#36229;&#32423;&#22270;&#20687;&#65292;&#24182;&#20351;&#29992;2D&#32593;&#32476;&#36827;&#34892;&#20998;&#21106;&#20197;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#23558;3D&#22270;&#20687;&#30340;&#20999;&#29255;&#24182;&#25490;&#25340;&#25509;&#26469;&#29983;&#25104;&#36229;&#20998;&#36776;&#29575;&#22270;&#20687;&#12290;&#25105;&#20204;&#26399;&#26395;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33021;&#22815;&#22312;&#31354;&#38388;&#19978;&#25429;&#25417;&#21644;&#23398;&#20064;&#36825;&#20123;&#29305;&#24615;&#65292;&#23613;&#31649;&#21487;&#33021;&#20250;&#20002;&#22833;interslice context&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
In medical imaging analysis, deep learning has shown promising results. We frequently rely on volumetric data to segment medical images, necessitating the use of 3D architectures, which are commended for their capacity to capture interslice context. However, because of the 3D convolutions, max pooling, up-convolutions, and other operations utilized in these networks, these architectures are often more inefficient in terms of time and computation than their 2D equivalents. Furthermore, there are few 3D pretrained model weights, and pretraining is often difficult. We present a simple yet effective 2D method to handle 3D data while efficiently embedding the 3D knowledge during training. We propose transforming volumetric data into 2D super images and segmenting with 2D networks to solve these challenges. Our method generates a super-resolution image by stitching slices side by side in the 3D image. We expect deep neural networks to capture and learn these properties spatially despite losi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#35270;&#35273;&#20219;&#21153;&#30340;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#26426;&#21046;&#8212;&#8212;&#37051;&#22495;&#20851;&#27880;&#65288;NA&#65289;&#12290;&#22522;&#20110;NA&#65292;&#24320;&#21457;&#20102;NAT&#65292;NAT-Tiny&#22312;ImageNet&#19978;&#36798;&#21040;&#20102;83.2&#65285;&#30340;top-1&#20934;&#30830;&#29575;&#65292;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2204.07143</link><description>&lt;p&gt;
&#12298;&#37051;&#22495;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#12299;
&lt;/p&gt;
&lt;p&gt;
Neighborhood Attention Transformer. (arXiv:2204.07143v5 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.07143
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#35270;&#35273;&#20219;&#21153;&#30340;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#26426;&#21046;&#8212;&#8212;&#37051;&#22495;&#20851;&#27880;&#65288;NA&#65289;&#12290;&#22522;&#20110;NA&#65292;&#24320;&#21457;&#20102;NAT&#65292;NAT-Tiny&#22312;ImageNet&#19978;&#36798;&#21040;&#20102;83.2&#65285;&#30340;top-1&#20934;&#30830;&#29575;&#65292;&#33021;&#22815;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#8220;&#37051;&#22495;&#20851;&#27880;&#8221;&#65288;NA&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#31181;&#38024;&#23545;&#35270;&#35273;&#20219;&#21153;&#30340;&#39640;&#25928;&#21644;&#21487;&#25193;&#23637;&#30340;&#28369;&#21160;&#31383;&#21475;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;NA&#26159;&#19968;&#31181;&#20687;&#32032;&#32423;&#36816;&#31639;&#65292;&#23558;&#33258;&#27880;&#24847;&#21147;&#65288;SA&#65289;&#23616;&#38480;&#20110;&#26368;&#36817;&#30340;&#30456;&#37051;&#20687;&#32032;&#65292;&#22240;&#27492;&#19982;SA&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#30456;&#27604;&#65292;&#20855;&#26377;&#32447;&#24615;&#30340;&#26102;&#38388;&#21644;&#31354;&#38388;&#22797;&#26434;&#24230;&#12290;&#28369;&#21160;&#31383;&#21475;&#27169;&#24335;&#20351;NA&#30340;&#24863;&#21463;&#37326;&#33021;&#22815;&#22686;&#38271;&#32780;&#19981;&#38656;&#35201;&#39069;&#22806;&#30340;&#20687;&#32032;&#31227;&#20301;&#65292;&#24182;&#19988;&#20445;&#30041;&#20102;&#24179;&#31227;&#31561;&#21464;&#24615;&#65292;&#36825;&#19982;Swin Transformer&#30340;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65288;WSA&#65289;&#19981;&#21516;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;NATTEN&#65288;&#37051;&#22495;&#20851;&#27880;&#25193;&#23637;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#20855;&#26377;&#39640;&#25928;&#30340;C++&#21644;CUDA&#20869;&#26680;&#30340;Python&#21253;&#65292;&#20351;NA&#30340;&#36816;&#34892;&#36895;&#24230;&#27604;Swin&#30340;WSA&#24555;&#39640;&#36798;40&#65285;&#65292;&#21516;&#26102;&#20351;&#29992;&#30340;&#20869;&#23384;&#23569;&#20102;25&#65285;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#22522;&#20110;NA&#30340;&#26032;&#23618;&#27425;&#32467;&#26500;&#21464;&#25442;&#22120;&#35774;&#35745;&#8212;&#8212;&#37051;&#22495;&#20851;&#27880;&#21464;&#25442;&#22120;&#65288;NAT&#65289;&#65292;&#20197;&#25552;&#39640;&#22270;&#20687;&#20998;&#31867;&#21644;&#19979;&#28216;&#35270;&#35273;&#24615;&#33021;&#12290;&#22312;NAT&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20855;&#26377;&#31454;&#20105;&#21147;&#65292;NAT-Tiny&#22312;ImageNet&#19978;&#36798;&#21040;&#20102;83.2&#65285;&#30340;top-1&#20934;&#30830;&#29575;&#21644;51.4&#65285;&#30340;mAP&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on M
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25955;&#24230;&#23548;&#21521;&#24418;&#29366;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#38656;&#36755;&#20837;&#27861;&#21521;&#37327;&#65292;&#20197;&#36719;&#32422;&#26463;&#20542;&#21521;&#20110;&#24179;&#28369;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#38752;&#22320;&#23450;&#20301;&#27599;&#20010;&#28857;&#30340;&#26410;&#30693;&#27861;&#21521;&#37327;&#30340;&#26799;&#24230;&#65292;&#29978;&#33267;&#27604;&#20351;&#29992;&#30495;&#23454;&#27861;&#21521;&#37327;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;INR&#20960;&#20309;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25910;&#25947;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.10811</link><description>&lt;p&gt;
DiGS&#65306;&#22522;&#20110;&#25955;&#24230;&#23548;&#21521;&#30340;&#26410;&#23450;&#21521;&#28857;&#20113;&#24418;&#29366;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
DiGS : Divergence guided shape implicit neural representation for unoriented point clouds. (arXiv:2106.10811v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.10811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25955;&#24230;&#23548;&#21521;&#24418;&#29366;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#26080;&#38656;&#36755;&#20837;&#27861;&#21521;&#37327;&#65292;&#20197;&#36719;&#32422;&#26463;&#20542;&#21521;&#20110;&#24179;&#28369;&#35299;&#20915;&#26041;&#26696;&#65292;&#21487;&#38752;&#22320;&#23450;&#20301;&#27599;&#20010;&#28857;&#30340;&#26410;&#30693;&#27861;&#21521;&#37327;&#30340;&#26799;&#24230;&#65292;&#29978;&#33267;&#27604;&#20351;&#29992;&#30495;&#23454;&#27861;&#21521;&#37327;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;INR&#20960;&#20309;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24418;&#29366;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#65288;INR&#65289;&#26368;&#36817;&#24050;&#32463;&#22312;&#24418;&#29366;&#20998;&#26512;&#21644;&#37325;&#24314;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#29616;&#26377;&#30340;INR&#38656;&#35201;&#20960;&#20309;&#22352;&#26631;&#26469;&#23398;&#20064;&#24418;&#29366;&#30340;&#38544;&#24335;&#27700;&#24179;&#38598;&#12290;&#24403;&#27599;&#20010;&#28857;&#37117;&#26377;&#27861;&#21521;&#37327;&#26102;&#65292;&#21487;&#20197;&#23398;&#20064;&#21040;&#26356;&#39640;&#20445;&#30495;&#24230;&#30340;&#34920;&#31034;&#65292;&#28982;&#32780;&#36890;&#24120;&#21407;&#22987;&#25968;&#25454;&#20013;&#24456;&#23569;&#25552;&#20379;&#27861;&#21521;&#37327;&#12290;&#27492;&#22806;&#65292;&#24050;&#32463;&#34920;&#26126;&#26041;&#27861;&#30340;&#21021;&#22987;&#21270;&#23545;&#20110;&#34920;&#38754;&#37325;&#26500;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19981;&#38656;&#35201;&#27861;&#21521;&#37327;&#20316;&#20026;&#36755;&#20837;&#30340;&#25955;&#24230;&#23548;&#21521;&#24418;&#29366;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#36317;&#31163;&#20989;&#25968;&#30340;&#25955;&#24230;&#19978;&#28155;&#21152;&#36719;&#32422;&#26463;&#20250;&#20542;&#21521;&#20110;&#24179;&#28369;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#21487;&#38752;&#22320;&#23450;&#20301;&#27599;&#20010;&#28857;&#30340;&#26410;&#30693;&#27861;&#21521;&#37327;&#30340;&#26799;&#24230;&#65292;&#26576;&#20123;&#24773;&#20917;&#19979;&#29978;&#33267;&#27604;&#30452;&#25509;&#20351;&#29992;&#22320;&#38754;&#30495;&#23454;&#27861;&#21521;&#37327;&#30340;&#26041;&#27861;&#34920;&#29616;&#24471;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27491;&#24358;INR&#20960;&#20309;&#21021;&#22987;&#21270;&#26041;&#27861;&#65292;&#36827;&#19968;&#27493;&#25913;&#21892;&#20102;&#25910;&#25947;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Shape implicit neural representations (INRs) have recently shown to be effective in shape analysis and reconstruction tasks. Existing INRs require point coordinates to learn the implicit level sets of the shape. When a normal vector is available for each point, a higher fidelity representation can be learned, however normal vectors are often not provided as raw data. Furthermore, the method's initialization has been shown to play a crucial role for surface reconstruction. In this paper, we propose a divergence guided shape representation learning approach that does not require normal vectors as input. We show that incorporating a soft constraint on the divergence of the distance function favours smooth solutions that reliably orients gradients to match the unknown normal at each point, in some cases even better than approaches that use ground truth normal vectors directly. Additionally, we introduce a novel geometric initialization method for sinusoidal INRs that further improves conve
&lt;/p&gt;</description></item></channel></rss>