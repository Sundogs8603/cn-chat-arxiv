<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;ModTr&#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#22411;&#36716;&#25442;&#32593;&#32476;&#35843;&#25972;&#36755;&#20837;&#20197;&#26368;&#23567;&#21270;&#26816;&#27979;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20174;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#21040;&#21478;&#19968;&#20010;&#30340;&#26377;&#25928;&#36866;&#24212;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2404.01492</link><description>&lt;p&gt;
&#19981;&#36951;&#24536;&#20808;&#39564;&#30693;&#35782;&#30340;&#30446;&#26631;&#26816;&#27979;&#36866;&#24212;&#27169;&#24577;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
Modality Translation for Object Detection Adaptation Without Forgetting Prior Knowledge
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.01492
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;ModTr&#26041;&#27861;&#65292;&#36890;&#36807;&#23567;&#22411;&#36716;&#25442;&#32593;&#32476;&#35843;&#25972;&#36755;&#20837;&#20197;&#26368;&#23567;&#21270;&#26816;&#27979;&#25439;&#22833;&#65292;&#23454;&#29616;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#20174;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#21040;&#21478;&#19968;&#20010;&#30340;&#26377;&#25928;&#36866;&#24212;&#65292;&#32780;&#26080;&#38656;&#24494;&#35843;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#20013;&#24120;&#35265;&#30340;&#20570;&#27861;&#26159;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#20197;&#22312;&#19981;&#21516;&#39046;&#22495;&#21644;&#20219;&#21153;&#20013;&#20934;&#30830;&#25191;&#34892;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#21482;&#36866;&#29992;&#20110;&#36328;&#27169;&#24577;&#65292;&#22240;&#20026;&#20351;&#29992;&#19981;&#21516;&#20256;&#24863;&#22120;&#25429;&#33719;&#30340;&#25968;&#25454;&#23384;&#22312;&#26356;&#22823;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#26412;&#25991;&#19987;&#27880;&#20110;&#23558;&#22823;&#22411;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#35843;&#25972;&#21040;&#19968;&#20010;&#25110;&#22810;&#20010;&#27169;&#24577;&#30340;&#38382;&#39064;&#65292;&#21516;&#26102;&#20445;&#25345;&#39640;&#25928;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ModTr&#20316;&#20026;&#26222;&#36941;&#20570;&#27861;&#24494;&#35843;&#22823;&#22411;&#27169;&#22411;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;ModTr&#21253;&#25324;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#36716;&#25442;&#32593;&#32476;&#35843;&#25972;&#36755;&#20837;&#65292;&#35813;&#32593;&#32476;&#32463;&#36807;&#35757;&#32451;&#65292;&#30452;&#25509;&#20351;&#26816;&#27979;&#25439;&#22833;&#26368;&#23567;&#21270;&#12290;&#22240;&#27492;&#65292;&#21407;&#22987;&#27169;&#22411;&#21487;&#20197;&#22312;&#36716;&#25442;&#21518;&#30340;&#36755;&#20837;&#19978;&#24037;&#20316;&#65292;&#26080;&#38656;&#36827;&#34892;&#20219;&#20309;&#36827;&#19968;&#27493;&#30340;&#26356;&#25913;&#25110;&#21442;&#25968;&#24494;&#35843;&#12290;&#23545;&#20004;&#20010;&#30693;&#21517;&#25968;&#25454;&#38598;&#19978;&#20174;&#32418;&#22806;&#21040;RGB&#22270;&#20687;&#30340;&#36716;&#25442;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#31181;&#31616;&#21333;&#30340;ModTr&#26041;&#27861;&#25552;&#20379;&#20102;&#26816;&#27979;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.01492v1 Announce Type: cross  Abstract: A common practice in deep learning consists of training large neural networks on massive datasets to perform accurately for different domains and tasks. While this methodology may work well in numerous application areas, it only applies across modalities due to a larger distribution shift in data captured using different sensors. This paper focuses on the problem of adapting a large object detection model to one or multiple modalities while being efficient. To do so, we propose ModTr as an alternative to the common approach of fine-tuning large models. ModTr consists of adapting the input with a small transformation network trained to minimize the detection loss directly. The original model can therefore work on the translated inputs without any further change or fine-tuning to its parameters. Experimental results on translating from IR to RGB images on two well-known datasets show that this simple ModTr approach provides detectors tha
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#37325;&#24314;&#38454;&#27573;&#21644;&#37325;&#24314;&#25439;&#22833;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#21644;&#29305;&#24449;&#34920;&#31034;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20849;&#21516;&#20449;&#24687;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35299;&#20915;&#30456;&#20851;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2404.00505</link><description>&lt;p&gt;
&#20855;&#26377;&#37325;&#24314;&#25439;&#22833;&#30340;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning with Reconstruction Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2404.00505
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#37325;&#24314;&#38454;&#27573;&#21644;&#37325;&#24314;&#25439;&#22833;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#21644;&#29305;&#24449;&#34920;&#31034;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#24314;&#31435;&#20102;&#20849;&#21516;&#20449;&#24687;&#30340;&#27010;&#24565;&#65292;&#29992;&#20110;&#35299;&#20915;&#30456;&#20851;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22810;&#25968;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#25968;&#23398;&#20248;&#21270;&#30340;&#24212;&#29992;&#20013;&#65292;&#36890;&#24120;&#20026;&#27599;&#20010;&#29305;&#23450;&#20248;&#21270;&#30446;&#26631;&#35757;&#32451;&#19968;&#20010;&#19987;&#29992;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#22330;&#26223;&#20013;&#65292;&#21516;&#19968;&#32452;&#38382;&#39064;&#36755;&#20837;&#19978;&#32463;&#24120;&#38656;&#35201;&#20248;&#21270;&#20960;&#20010;&#19981;&#21516;&#20294;&#30456;&#20851;&#30340;&#30446;&#26631;&#25110;&#20219;&#21153;&#12290;&#19982;&#20026;&#27599;&#20010;&#38382;&#39064;&#21333;&#29420;&#35757;&#32451;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#30456;&#27604;&#65292;&#26356;&#26377;&#25928;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#36825;&#20123;&#30446;&#26631;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#20351;&#29992;&#20849;&#20139;&#27169;&#22411;&#21442;&#25968;&#21644;&#29305;&#24449;&#34920;&#31034;&#35757;&#32451;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#26412;&#25991;&#39318;&#20808;&#24314;&#31435;&#20102;&#20849;&#21516;&#20449;&#24687;&#30340;&#27010;&#24565;&#65306;&#35299;&#20915;&#30456;&#20851;&#20219;&#21153;&#25152;&#38656;&#30340;&#20849;&#20139;&#30693;&#35782;&#65292;&#28982;&#21518;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#27169;&#22411;&#35757;&#32451;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#22411;&#20013;&#28155;&#21152;&#19968;&#20010;&#39069;&#22806;&#30340;&#37325;&#24314;&#38454;&#27573;&#20197;&#21450;&#30456;&#20851;&#30340;&#26032;&#37325;&#24314;&#25439;&#22833;&#12290;&#35813;&#25439;&#22833;&#29992;&#20110;&#20174;&#36873;&#25321;&#30340;&#38544;&#34255;&#29366;&#24577;&#24320;&#22987;&#37325;&#26032;&#26500;&#24314;&#20849;&#21516;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2404.00505v1 Announce Type: cross  Abstract: In most applications of utilizing neural networks for mathematical optimization, a dedicated model is trained for each specific optimization objective. However, in many scenarios, several distinct yet correlated objectives or tasks often need to be optimized on the same set of problem inputs. Instead of independently training a different neural network for each problem separately, it would be more efficient to exploit the correlations between these objectives and to train multiple neural network models with shared model parameters and feature representations. To achieve this, this paper first establishes the concept of common information: the shared knowledge required for solving the correlated tasks, then proposes a novel approach for model training by adding into the model an additional reconstruction stage associated with a new reconstruction loss. This loss is for reconstructing the common information starting from a selected hidde
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;</title><link>https://arxiv.org/abs/2403.18998</link><description>&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#36328;&#31995;&#32479;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Few-Shot Cross-System Anomaly Trace Classification for Microservice-based systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.18998
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#30340;&#26032;&#26694;&#26550;&#65292;&#21033;&#29992;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#24182;&#24212;&#29992;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#36827;&#34892;&#39640;&#25928;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24494;&#26381;&#21153;&#31995;&#32479;&#65288;MSS&#65289;&#30001;&#20110;&#20854;&#22797;&#26434;&#21644;&#21160;&#24577;&#30340;&#29305;&#24615;&#21487;&#33021;&#22312;&#21508;&#31181;&#25925;&#38556;&#31867;&#21035;&#20013;&#20986;&#29616;&#25925;&#38556;&#12290;&#20026;&#20102;&#26377;&#25928;&#22788;&#29702;&#25925;&#38556;&#65292;AIOps&#24037;&#20855;&#21033;&#29992;&#22522;&#20110;&#36319;&#36394;&#30340;&#24322;&#24120;&#26816;&#27979;&#21644;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#24494;&#26381;&#21153;&#31995;&#32479;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#65306;&#65288;1&#65289;&#22810;&#22836;&#27880;&#24847;&#21147;&#33258;&#32534;&#30721;&#22120;&#29992;&#20110;&#26500;&#24314;&#31995;&#32479;&#29305;&#23450;&#30340;&#36319;&#36394;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#65288;2&#65289;&#22522;&#20110;Transformer&#32534;&#30721;&#22120;&#30340;&#27169;&#22411;&#26080;&#20851;&#20803;&#23398;&#20064;&#65292;&#20197;&#36827;&#34892;&#26377;&#25928;&#21644;&#39640;&#25928;&#30340;&#23569;&#26679;&#26412;&#24322;&#24120;&#36319;&#36394;&#20998;&#31867;&#12290;&#35813;&#26694;&#26550;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#30340;MSS&#65292;Trainticket&#21644;OnlineBoutique&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#20351;&#29992;&#24320;&#25918;&#25968;&#25454;&#38598;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#35843;&#25972;&#23398;&#21040;&#30340;&#30693;&#35782;&#65292;&#20197;&#23545;&#26032;&#30340;&#12289;&#26410;&#35265;&#30340;&#26032;&#39062;&#25925;&#38556;&#31867;&#21035;&#30340;&#24322;&#24120;&#36319;&#36394;&#36827;&#34892;&#20998;&#31867;&#65292;&#26080;&#35770;&#26159;&#22312;&#26368;&#21021;&#35757;&#32451;&#30340;&#21516;&#19968;&#31995;&#32479;&#20869;&#65292;&#36824;&#26159;&#22312;&#20854;&#20182;&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.18998v1 Announce Type: cross  Abstract: Microservice-based systems (MSS) may experience failures in various fault categories due to their complex and dynamic nature. To effectively handle failures, AIOps tools utilize trace-based anomaly detection and root cause analysis. In this paper, we propose a novel framework for few-shot abnormal trace classification for MSS. Our framework comprises two main components: (1) Multi-Head Attention Autoencoder for constructing system-specific trace representations, which enables (2) Transformer Encoder-based Model-Agnostic Meta-Learning to perform effective and efficient few-shot learning for abnormal trace classification. The proposed framework is evaluated on two representative MSS, Trainticket and OnlineBoutique, with open datasets. The results show that our framework can adapt the learned knowledge to classify new, unseen abnormal traces of novel fault categories both within the same system it was initially trained on and even in the 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15412</link><description>&lt;p&gt;
&#22312;LLMs&#20013;&#27979;&#37327;&#21644;&#24314;&#27169;&#8220;&#25991;&#21270;&#8221;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Measuring and Modeling "Culture" in LLMs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15412
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21576;&#29616;&#20102;&#23545;39&#31687;&#26368;&#26032;&#35770;&#25991;&#30340;&#35843;&#26597;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#27809;&#26377;&#19968;&#31687;&#30740;&#31350;&#23450;&#20041;&#8220;&#25991;&#21270;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#12289;&#22810;&#23618;&#38754;&#30340;&#27010;&#24565;&#65307;&#30456;&#21453;&#65292;&#23427;&#20204;&#22312;&#19968;&#20123;&#29305;&#21035;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20195;&#34920;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#38754;&#31216;&#20026;&#25991;&#21270;&#30340;&#20195;&#29702;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#22312;&#20154;&#21475;&#32479;&#35745;&#12289;&#35821;&#20041;&#21644;&#35821;&#35328;&#25991;&#21270;&#20132;&#20114;&#20195;&#29702;&#30340;&#19977;&#20010;&#32500;&#24230;&#19978;&#12290;&#25105;&#20204;&#36824;&#23545;&#37319;&#29992;&#30340;&#25506;&#26597;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21482;&#26377;&#8220;&#25991;&#21270;&#8221;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#22914;&#20215;&#20540;&#35266;&#21644;&#30446;&#26631;&#65292;&#34987;&#30740;&#31350;&#20102;&#65292;&#30041;&#19979;&#20102;&#20960;&#20010;&#20854;&#20182;&#26377;&#36259;&#19988;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22823;&#37327;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#65288;Hershcovich&#31561;&#20154;&#65292;2022&#65289;&#30340;&#26410;&#34987;&#25506;&#31350;&#12290;&#21478;&#22806;&#20004;&#20010;&#20851;&#38190;&#30340;&#31354;&#30333;&#26159;&#30446;&#21069;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#24773;&#22659;&#24615;&#30340;&#32570;&#20047;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15412v1 Announce Type: cross  Abstract: We present a survey of 39 recent papers that aim to study cultural representation and inclusion in large language models. We observe that none of the studies define "culture," which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture." We call these aspects the proxies of cultures, and organize them across three dimensions of demographic, semantic and linguistic-cultural interaction proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of "culture," such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situatedness of the current methods. Based on these observations
&lt;/p&gt;</description></item><item><title>PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15388</link><description>&lt;p&gt;
LLaVA-PruMerge: &#33258;&#36866;&#24212;&#20196;&#29260;&#20943;&#23569;&#29992;&#20110;&#39640;&#25928;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15388
&lt;/p&gt;
&lt;p&gt;
PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#36890;&#36807;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;LMMs&#21253;&#25324;&#20102;&#26356;&#22797;&#26434;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22914;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#20196;&#29260;&#20943;&#23569;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#31867;&#20284;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#35768;&#22810;&#35270;&#35273;&#20196;&#29260;&#22312;&#31354;&#38388;&#19978;&#26159;&#20887;&#20313;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PruMerge&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21487;&#27604;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15388v1 Announce Type: cross  Abstract: Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#34920;&#24449;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#22312;&#22270;&#22686;&#24378;&#21518;&#21407;&#26377;&#20551;&#35774;&#19981;&#20877;&#25104;&#31435;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.02719</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Subgraph Contrastive Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02719
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#34920;&#24449;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#22312;&#22270;&#22686;&#24378;&#21518;&#21407;&#26377;&#20551;&#35774;&#19981;&#20877;&#25104;&#31435;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32423;&#23545;&#27604;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#23545;&#27604;&#20004;&#20010;&#22686;&#24378;&#22270;&#26469;&#23398;&#20064;&#27599;&#20010;&#22270;&#30340;&#34920;&#31034;&#65292;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#36890;&#24120;&#31616;&#21333;&#22320;&#20551;&#35774;&#19968;&#20010;&#22270;&#21450;&#20854;&#22686;&#24378;&#22270;&#20026;&#27491;&#23545;&#65292;&#21542;&#21017;&#20026;&#36127;&#23545;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;&#22270;&#32467;&#26500;&#36890;&#24120;&#22797;&#26434;&#19988;&#22810;&#23610;&#24230;&#65292;&#36825;&#24102;&#26469;&#20102;&#19968;&#20010;&#26681;&#26412;&#38382;&#39064;&#65306;&#22312;&#22270;&#22686;&#24378;&#21518;&#65292;&#20808;&#21069;&#30340;&#20551;&#35774;&#26159;&#21542;&#20173;&#28982;&#25104;&#31435;&#65311;&#36890;&#36807;&#23454;&#39564;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#24378;&#22270;&#32467;&#26500;&#30340;&#35821;&#20041;&#20449;&#24687;&#21487;&#33021;&#19981;&#19968;&#33268;&#20110;&#21407;&#22987;&#22270;&#32467;&#26500;&#65292;&#24182;&#19988;&#20004;&#20010;&#22686;&#24378;&#22270;&#26159;&#27491;&#23545;&#36824;&#26159;&#36127;&#23545;&#19982;&#22810;&#23610;&#24230;&#32467;&#26500;&#23494;&#20999;&#30456;&#20851;&#12290;&#22522;&#20110;&#36825;&#19968;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#34920;&#24449;&#32454;&#31890;&#24230;&#35821;&#20041;&#20449;&#24687;&#30340;&#22810;&#23610;&#24230;&#23376;&#22270;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#29983;&#25104;&#20840;&#23616;&#21644;&#23616;&#37096;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02719v1 Announce Type: new  Abstract: Graph-level contrastive learning, aiming to learn the representations for each graph by contrasting two augmented graphs, has attracted considerable attention. Previous studies usually simply assume that a graph and its augmented graph as a positive pair, otherwise as a negative pair. However, it is well known that graph structure is always complex and multi-scale, which gives rise to a fundamental question: after graph augmentation, will the previous assumption still hold in reality? By an experimental analysis, we discover the semantic information of an augmented graph structure may be not consistent as original graph structure, and whether two augmented graphs are positive or negative pairs is highly related with the multi-scale structures. Based on this finding, we propose a multi-scale subgraph contrastive learning method which is able to characterize the fine-grained semantic information. Specifically, we generate global and local 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Ex&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#26469;&#20462;&#27491;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;</title><link>https://arxiv.org/abs/2402.17097</link><description>&lt;p&gt;
&#20462;&#22797;: &#22312;&#35828;&#26126;&#21518;&#20462;&#27491;LLM&#21709;&#24212;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;
&lt;/p&gt;
&lt;p&gt;
Re-Ex: Revising after Explanation Reduces the Factual Errors in LLM Responses
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17097
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Re-Ex&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#26469;&#20462;&#27491;LLM&#29983;&#25104;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#26469;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32531;&#35299;&#24187;&#35273;&#38382;&#39064;&#26159;LLM&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#65292;&#25105;&#20204;&#38656;&#35201;&#20811;&#26381;&#36825;&#19968;&#25361;&#25112;&#65292;&#20197;&#20415;&#21487;&#38752;&#22320;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20351;&#29992;&#23427;&#20204;&#12290;&#26368;&#36817;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#26041;&#27861;&#26469;&#26816;&#26597;LLM&#29983;&#25104;&#30340;&#25991;&#26412;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#24182;&#30456;&#24212;&#22320;&#36827;&#34892;&#20462;&#35746;&#65292;&#20197;&#20943;&#23569;&#24187;&#35273;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Re-Ex&#65292;&#19968;&#31181;&#20462;&#35746;LLM&#29983;&#25104;&#25991;&#26412;&#30340;&#26041;&#27861;&#65292;&#23427;&#24341;&#20837;&#20102;&#19968;&#20010;&#31216;&#20026;&#20107;&#23454;&#38169;&#35823;&#35828;&#26126;&#27493;&#39588;&#30340;&#26032;&#27493;&#39588;&#12290; Re-Ex&#20351;&#29992;3&#20010;&#27493;&#39588;&#23545;LLM&#30340;&#21021;&#22987;&#21709;&#24212;&#36827;&#34892;&#20462;&#35746;&#65306;&#39318;&#20808;&#65292;&#20351;&#29992;&#22806;&#37096;&#24037;&#20855;&#33719;&#21462;&#21709;&#24212;&#20013;&#20107;&#23454;&#38169;&#35823;&#30340;&#35777;&#25454;&#65307;&#31532;&#20108;&#65292;&#35201;&#27714;LLM&#26681;&#25454;&#31532;&#19968;&#27493;&#20013;&#25910;&#38598;&#30340;&#35777;&#25454;&#35299;&#37322;&#21709;&#24212;&#20013;&#30340;&#38382;&#39064;&#37096;&#20998;&#65307;&#26368;&#21518;&#65292;LLM&#20351;&#29992;&#22312;&#31532;&#20108;&#27493;&#20013;&#33719;&#24471;&#30340;&#35299;&#37322;&#23545;&#21709;&#24212;&#36827;&#34892;&#20462;&#35746;&#12290;&#38500;&#20102;&#35828;&#26126;&#27493;&#39588;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#26032;&#30340;&#25552;&#31034;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#25152;&#38656;&#30340;&#26631;&#35760;&#25968;&#37327;&#21644;&#25346;&#38047;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17097v1 Announce Type: cross  Abstract: Mitigating hallucination issues is one of the main challenges of LLMs we need to overcome, in order to reliably use them in real-world scenarios. Recently, various methods are proposed to check the factual errors in the LLM-generated texts and revise them accordingly, to reduce the hallucination issue. In this paper, we propose Re-Ex, a method of revising LLM-generated texts, which introduces a novel step dubbed as the factual error explanation step. Re-Ex revises the initial response of LLMs using 3-steps: first, external tools are used to get the evidences on the factual errors in the response; second, LLMs are instructed to explain the problematic parts of the response based on the evidences gathered in the first step; finally, LLMs revise the response using the explanation obtained in the second step. In addition to the explanation step, we propose new prompting techniques to reduce the amount of tokens and wall-clock time required
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36328;&#38382;&#39064;&#27867;&#21270;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;VRPs&#23450;&#20041;&#20026;&#20849;&#20139;&#22522;&#30784;&#23646;&#24615;&#30340;&#19981;&#21516;&#32452;&#21512;&#65292;&#24182;&#36890;&#36807;&#23646;&#24615;&#32452;&#21512;&#21516;&#26102;&#35299;&#20915;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#36335;&#24452;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.16891</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#29992;&#20110;&#20855;&#26377;&#36328;&#38382;&#39064;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#36335;&#24452;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning for Routing Problem with Cross-Problem Zero-Shot Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.16891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36328;&#38382;&#39064;&#27867;&#21270;&#30340;&#20851;&#38190;&#25361;&#25112;&#65292;&#36890;&#36807;&#23558;VRPs&#23450;&#20041;&#20026;&#20849;&#20139;&#22522;&#30784;&#23646;&#24615;&#30340;&#19981;&#21516;&#32452;&#21512;&#65292;&#24182;&#36890;&#36807;&#23646;&#24615;&#32452;&#21512;&#21516;&#26102;&#35299;&#20915;&#23427;&#20204;&#65292;&#23454;&#29616;&#20102;&#38646;&#26679;&#26412;&#27867;&#21270;&#30340;&#36335;&#24452;&#38382;&#39064;&#35299;&#20915;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRPs&#65289;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#37117;&#33021;&#25214;&#21040;&#65292;&#24050;&#32463;&#25104;&#20026;&#20960;&#21313;&#24180;&#30340;&#37325;&#35201;&#30740;&#31350;&#35838;&#39064;&#12290;&#26368;&#36817;&#65292;&#21033;&#29992;&#22522;&#20110;&#23398;&#20064;&#30340;&#27169;&#22411;&#26469;&#35299;&#20915;VRPs&#30340;&#31070;&#32463;&#32452;&#21512;&#20248;&#21270;&#65288;NCO&#65289;&#26041;&#27861;&#24341;&#36215;&#20102;&#30456;&#24403;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;NCO&#26041;&#27861;&#36890;&#24120;&#38656;&#35201;&#20026;&#27599;&#20010;&#36335;&#24452;&#38382;&#39064;&#26500;&#24314;&#19968;&#20010;&#27169;&#22411;&#65292;&#36825;&#26174;&#33879;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#20855;&#26377;&#19981;&#21516;&#23646;&#24615;&#30340;&#30495;&#23454;&#24037;&#19994;&#38382;&#39064;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36328;&#38382;&#39064;&#27867;&#21270;&#30340;&#20851;&#38190;&#25361;&#25112;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;VRPs&#23450;&#20041;&#20026;&#19968;&#32452;&#20849;&#20139;&#30340;&#22522;&#30784;&#23646;&#24615;&#30340;&#19981;&#21516;&#32452;&#21512;&#65292;&#24182;&#36890;&#36807;&#23646;&#24615;&#32452;&#21512;&#21516;&#26102;&#36890;&#36807;&#21333;&#19968;&#27169;&#22411;&#35299;&#20915;&#23427;&#20204;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#33021;&#22815;&#25104;&#21151;&#35299;&#20915;&#20855;&#26377;&#26410;&#35265;&#23646;&#24615;&#32452;&#21512;&#30340;VRPs&#65292;&#23454;&#29616;&#38646;&#26679;&#26412;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.16891v1 Announce Type: cross  Abstract: Vehicle routing problems (VRPs), which can be found in numerous real-world applications, have been an important research topic for several decades. Recently, the neural combinatorial optimization (NCO) approach that leverages a learning-based model to solve VRPs without manual algorithm design has gained substantial attention. However, current NCO methods typically require building one model for each routing problem, which significantly hinders their practical application for real-world industry problems with diverse attributes. In this work, we make the first attempt to tackle the crucial challenge of cross-problem generalization. In particular, we formulate VRPs as different combinations of a set of shared underlying attributes and solve them simultaneously via a single model through attribute composition. In this way, our proposed model can successfully solve VRPs with unseen attribute combinations in a zero-shot generalization mann
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#20219;&#21153;&#30340;&#20004;&#31181;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#37096;&#20998;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.12730</link><description>&lt;p&gt;
UMBCLU&#22312;SemEval-2024&#20219;&#21153;1A&#21644;1C&#20013;&#30340;&#34920;&#29616;&#65306;&#24102;&#26377;&#21644;&#19981;&#24102;&#26377;&#26426;&#22120;&#32763;&#35793;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;
&lt;/p&gt;
&lt;p&gt;
UMBCLU at SemEval-2024 Task 1A and 1C: Semantic Textual Relatedness with and without machine translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12730
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#26412;&#25991;&#24320;&#21457;&#20102;&#29992;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#20219;&#21153;&#30340;&#20004;&#31181;&#27169;&#22411;&#65292;&#21462;&#24471;&#20102;&#27604;&#37096;&#20998;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;&#25105;&#20204;&#20026;SemEval-2024&#20219;&#21153;1&#24320;&#21457;&#30340;&#31995;&#32479;&#65292;&#8220;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#8221;&#12290; &#35813;&#20219;&#21153;&#30340;&#30446;&#26631;&#26159;&#26500;&#24314;&#19968;&#20010;&#33021;&#22815;&#35782;&#21035;&#30446;&#26631;&#35821;&#35328;&#20013;&#23646;&#20110;&#38750;&#27954;&#21644;&#20122;&#27954;&#35821;&#35328;&#38598;&#21512;&#30340;&#20004;&#20010;&#21477;&#23376;&#20043;&#38388;&#30340;&#35821;&#20041;&#25991;&#26412;&#30456;&#20851;&#24615;&#65288;STR&#65289;&#30340;&#27169;&#22411;&#12290; &#25105;&#20204;&#21442;&#19982;&#20102;&#23376;&#20219;&#21153;A&#21644;C&#65292;&#24182;&#25506;&#32034;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36827;&#34892;&#30417;&#30563;&#21644;&#36328;&#35821;&#35328;&#35757;&#32451;&#12290; &#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#26426;&#22120;&#32763;&#35793;&#21644;&#35821;&#20041;&#30456;&#20284;&#24615;&#12290; &#20351;&#29992;&#26426;&#22120;&#32763;&#35793;&#21644;&#21477;&#23376;&#23884;&#20837;LLMs&#30340;&#32452;&#21512;&#65292;&#25105;&#20204;&#20026;&#23376;&#20219;&#21153;A&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;STR&#27169;&#22411;&#65292;TranSem&#65292;&#24182;&#23545;STR&#25968;&#25454;&#19978;&#30340;T5&#31995;&#21015;&#27169;&#22411;&#36827;&#34892;&#20102;&#24494;&#35843;&#65292;&#29992;&#20110;&#23376;&#20219;&#21153;C&#30340;FineSem&#12290; &#25105;&#20204;&#22312;&#23376;&#20219;&#21153;A&#20013;7&#31181;&#35821;&#35328;&#30340;&#27169;&#22411;&#32467;&#26524;&#27604;3&#31181;&#35821;&#35328;&#30340;&#23448;&#26041;&#22522;&#20934;&#26356;&#22909;&#65292;&#32780;&#19982;&#20854;&#20182;4&#31181;&#35821;&#35328;&#30340;&#22522;&#20934;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12730v1 Announce Type: cross  Abstract: This paper describes the system we developed for SemEval-2024 Task 1, "Semantic Textual Relatedness for African and Asian Languages." The aim of the task is to build a model that can identify semantic textual relatedness (STR) between two sentences of a target language belonging to a collection of African and Asian languages. We participated in Subtasks A and C and explored supervised and cross-lingual training leveraging large language models (LLMs). Pre-trained large language models have been extensively used for machine translation and semantic similarity. Using a combination of machine translation and sentence embedding LLMs, we developed a unified STR model, TranSem, for subtask A and fine-tuned the T5 family of models on the STR data, FineSem, for use in subtask C. Our model results for 7 languages in subtask A were better than the official baseline for 3 languages and on par with the baseline for the remaining 4 languages. Our m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;358&#21517;&#32654;&#22269;&#21442;&#19982;&#32773;&#65292;&#21457;&#29616;&#31038;&#20250;&#24433;&#21709;&#12289;&#24615;&#33021;&#26399;&#26395;&#12289;&#20139;&#21463;&#21160;&#26426;&#12289;&#20415;&#21033;&#26465;&#20214;&#21644;&#21162;&#21147;&#26399;&#26395;&#26159;&#24433;&#21709;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25509;&#21463;&#24230;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;</title><link>https://arxiv.org/abs/2402.11444</link><description>&lt;p&gt;
&#22312;&#32654;&#22269;&#35780;&#20272;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#20844;&#20247;&#25509;&#21463;&#24230;
&lt;/p&gt;
&lt;p&gt;
Gauging Public Acceptance of Conditionally Automated Cars in the United States
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#35843;&#26597;358&#21517;&#32654;&#22269;&#21442;&#19982;&#32773;&#65292;&#21457;&#29616;&#31038;&#20250;&#24433;&#21709;&#12289;&#24615;&#33021;&#26399;&#26395;&#12289;&#20139;&#21463;&#21160;&#26426;&#12289;&#20415;&#21033;&#26465;&#20214;&#21644;&#21162;&#21147;&#26399;&#26395;&#26159;&#24433;&#21709;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25509;&#21463;&#24230;&#30340;&#37325;&#35201;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26234;&#33021;&#22478;&#24066;&#30340;&#19968;&#20010;&#20803;&#32032;&#65292;&#21363;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;SAE Level 3&#65289;&#65292;&#30740;&#31350;&#20102;&#24433;&#21709;&#32654;&#22269;&#20844;&#20247;&#25509;&#21463;&#24230;&#30340;&#22240;&#32032;&#12290;&#25105;&#20204;&#24212;&#29992;&#20102;UTUAT2&#27169;&#22411;&#30340;&#25913;&#32534;&#29256;&#12290;&#36890;&#36807;&#23454;&#39564;&#26041;&#27861;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;358&#21517;&#32654;&#22269;&#21442;&#19982;&#32773;&#65292;&#21521;&#20182;&#20204;&#23637;&#31034;&#20102;&#19968;&#20010;&#27010;&#36848;L3&#25216;&#26415;&#30340;&#30701;&#31687;&#25925;&#20107;&#65292;&#28982;&#21518;&#25552;&#20986;&#19968;&#31995;&#21015;&#38382;&#39064;&#65292;&#20197;&#25429;&#25417;&#20182;&#20204;&#23545;&#26377;&#26465;&#20214;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#24863;&#30693;&#12290;&#37319;&#29992;PLS-SEM&#23545;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25216;&#26415;&#30340;&#25509;&#21463;&#24230;&#65292;&#25353;&#37325;&#35201;&#24615;&#36882;&#20943;&#30340;&#39034;&#24207;&#65292;&#21463;&#31038;&#20250;&#24433;&#21709;&#12289;&#24615;&#33021;&#26399;&#26395;&#12289;&#20139;&#21463;&#21160;&#26426;&#12289;&#20415;&#21033;&#26465;&#20214;&#21644;&#21162;&#21147;&#26399;&#26395;&#30340;&#24433;&#21709;&#12290;&#27492;&#22806;&#65292;&#20139;&#21463;&#21160;&#26426;&#12289;&#31038;&#20250;&#24433;&#21709;&#12289;&#20415;&#21033;&#26465;&#20214;&#21644;&#21162;&#21147;&#26399;&#26395;&#37117;&#23545;&#25216;&#26415;&#30340;&#23454;&#29992;&#24615;&#24863;&#30693;&#26377;&#31215;&#26497;&#24433;&#21709;&#65307;&#20415;&#21033;&#26465;&#20214;&#12289;&#20139;&#21463;&#21160;&#26426;&#21644;&#31038;&#20250;&#24433;&#21709;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11444v1 Announce Type: cross  Abstract: In this work we look at an element of smart cities, conditionally automated cars (SAE Level 3), investigating the factors influencing public acceptance in the United States. We apply an adaptation of the UTUAT2 model. Taking an experimental approach study 358 participants in the US were presented with a vignette outlining the L3 technology followed by a series of questions to capture their perceptions of conditionally automated cars. PLS-SEM was used to analyze the collected data. The results reveal that the acceptance of the technology, in order of decreasing importance, was determined by social influence, performance expectancy, hedonic motivation, facilitating conditions, and effort expectancy. Furthermore, hedonic motivation, social influence, facilitating conditions and effort expectancy all have a positive influence on the perception of how useful the technology is; facilitating conditions, hedonic motivation, and social influenc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;CVLA&#65292;&#29992;&#20110;&#30701;&#35270;&#39057;&#24189;&#40664;&#26816;&#27979;&#12290;CVLA&#19981;&#20165;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24577;&#20449;&#21495;&#65292;&#36824;&#33021;&#36890;&#36807;&#22312;&#19968;&#33268;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#23545;&#40784;&#35270;&#39057;&#21644;&#35821;&#35328;&#32452;&#20214;&#20135;&#29983;&#36866;&#21512;&#30340;&#22810;&#27169;&#24335;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CVLA&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#21644;&#20960;&#20010;&#31454;&#20105;&#22522;&#20934;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2402.09055</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#21033;&#29992;&#35780;&#35770;&#36741;&#21161;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;&#29992;&#20110;&#30701;&#35270;&#39057;&#24189;&#40664;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Comment-aided Video-Language Alignment via Contrastive Pre-training for Short-form Video Humor Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36890;&#36807;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#21629;&#21517;&#20026;CVLA&#65292;&#29992;&#20110;&#30701;&#35270;&#39057;&#24189;&#40664;&#26816;&#27979;&#12290;CVLA&#19981;&#20165;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24577;&#20449;&#21495;&#65292;&#36824;&#33021;&#36890;&#36807;&#22312;&#19968;&#33268;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#23545;&#40784;&#35270;&#39057;&#21644;&#35821;&#35328;&#32452;&#20214;&#20135;&#29983;&#36866;&#21512;&#30340;&#22810;&#27169;&#24335;&#34920;&#31034;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CVLA&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#21644;&#20960;&#20010;&#31454;&#20105;&#22522;&#20934;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#30701;&#35270;&#39057;&#22312;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#24433;&#21709;&#21147;&#26085;&#30410;&#25193;&#22823;&#65292;&#22810;&#27169;&#24335;&#24189;&#40664;&#26816;&#27979;&#22312;&#24773;&#24863;&#35745;&#31639;&#20013;&#30340;&#37325;&#35201;&#24615;&#20063;&#26085;&#30410;&#22686;&#21152;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#30701;&#35270;&#39057;&#24189;&#40664;&#26816;&#27979;&#30340;&#20004;&#23618;&#20998;&#23618;&#27169;&#22411;&#65292;&#21629;&#21517;&#20026;&#36890;&#36807;&#25968;&#25454;&#22686;&#24378;&#30340;&#22810;&#27169;&#24335;&#23545;&#27604;&#39044;&#35757;&#32451;&#30340;&#35780;&#35770;&#36741;&#21161;&#35270;&#39057;&#35821;&#35328;&#23545;&#40784;&#65288;CVLA&#65289;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;CVLA&#19981;&#20165;&#36866;&#29992;&#20110;&#21508;&#31181;&#27169;&#24577;&#20449;&#21495;&#65292;&#24182;&#19988;&#36890;&#36807;&#22312;&#19968;&#33268;&#30340;&#35821;&#20041;&#31354;&#38388;&#20013;&#23545;&#40784;&#35270;&#39057;&#21644;&#35821;&#35328;&#32452;&#20214;&#65292;&#20135;&#29983;&#19968;&#20010;&#36866;&#21512;&#30340;&#22810;&#27169;&#24335;&#34920;&#31034;&#12290;&#22312;&#20004;&#20010;&#24189;&#40664;&#26816;&#27979;&#25968;&#25454;&#38598;DY11k&#21644;UR-FUNNY&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;CVLA&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#21644;&#20960;&#20010;&#31454;&#20105;&#22522;&#20934;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#12289;&#20195;&#30721;&#21644;&#27169;&#22411;&#21457;&#24067;&#22312;https://github.com/yliu-cs/CVLA&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09055v1 Announce Type: cross Abstract: The growing importance of multi-modal humor detection within affective computing correlates with the expanding influence of short-form video sharing on social media platforms. In this paper, we propose a novel two-branch hierarchical model for short-form video humor detection (SVHD), named Comment-aided Video-Language Alignment (CVLA) via data-augmented multi-modal contrastive pre-training. Notably, our CVLA not only operates on raw signals across various modal channels but also yields an appropriate multi-modal representation by aligning the video and language components within a consistent semantic space. The experimental results on two humor detection datasets, including DY11k and UR-FUNNY, demonstrate that CVLA dramatically outperforms state-of-the-art and several competitive baseline approaches. Our dataset, code and model release at https://github.com/yliu-cs/CVLA.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;UWB&#30340;&#38745;&#24577;&#25163;&#21183;&#35782;&#21035;&#30340;&#31283;&#20581;&#26694;&#26550;&#65292;&#32463;&#36807;&#25968;&#25454;&#38598;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#35757;&#32451;&#27169;&#22411;&#36798;&#21040;96.78%&#20934;&#30830;&#29575;&#65292;&#24182;&#24320;&#21457;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;GUI&#26694;&#26550;&#65292;&#20026;&#23558;UWB&#25216;&#26415;&#24212;&#29992;&#20110;&#38745;&#24577;&#25163;&#21183;&#35782;&#21035;&#24102;&#26469;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2310.15036</link><description>&lt;p&gt;
&#22522;&#20110;UWB&#30340;&#38745;&#24577;&#25163;&#21183;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
UWB Based Static Gesture Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.15036
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;UWB&#30340;&#38745;&#24577;&#25163;&#21183;&#35782;&#21035;&#30340;&#31283;&#20581;&#26694;&#26550;&#65292;&#32463;&#36807;&#25968;&#25454;&#38598;&#25910;&#38598;&#21644;&#22788;&#29702;&#65292;&#35757;&#32451;&#27169;&#22411;&#36798;&#21040;96.78%&#20934;&#30830;&#29575;&#65292;&#24182;&#24320;&#21457;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;GUI&#26694;&#26550;&#65292;&#20026;&#23558;UWB&#25216;&#26415;&#24212;&#29992;&#20110;&#38745;&#24577;&#25163;&#21183;&#35782;&#21035;&#24102;&#26469;&#20102;&#37325;&#35201;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30340;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31283;&#20581;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22522;&#20110;UWB&#30340;&#38745;&#24577;&#25163;&#21183;&#35782;&#21035;&#65292;&#21033;&#29992;&#19987;&#26377;&#30340;UWB&#38647;&#36798;&#20256;&#24863;&#22120;&#25216;&#26415;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#25968;&#25454;&#25910;&#38598;&#24037;&#20316;&#65292;&#32534;&#21046;&#20102;&#21253;&#21547;&#20116;&#31181;&#24120;&#29992;&#25163;&#21183;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;&#20840;&#38754;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#27969;&#31243;&#65292;&#21253;&#25324;&#24322;&#24120;&#20540;&#22788;&#29702;&#12289;&#20445;&#25345;&#38271;&#23485;&#27604;&#30340;&#35843;&#25972;&#22823;&#23567;&#21644;&#20266;&#24425;&#33394;&#22270;&#20687;&#36716;&#25442;&#12290;&#25105;&#20204;&#23545;&#22788;&#29702;&#21518;&#30340;&#22270;&#20687;&#35757;&#32451;&#20102;CNN&#21644;MobileNet&#27169;&#22411;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#34920;&#29616;&#26368;&#20339;&#30340;&#27169;&#22411;&#36798;&#21040;&#20102;96.78%&#30340;&#20934;&#30830;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;GUI&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#30340;&#31995;&#32479;&#36164;&#28304;&#20351;&#29992;&#24773;&#20917;&#21644;&#22788;&#29702;&#26102;&#38388;&#65292;&#26174;&#31034;&#20986;&#20302;&#20869;&#23384;&#21033;&#29992;&#21644;&#22312;&#19981;&#21040;&#19968;&#31186;&#30340;&#23454;&#26102;&#20219;&#21153;&#23436;&#25104;&#12290;&#36825;&#39033;&#30740;&#31350;&#26159;&#22312;&#20351;&#29992;UWB&#25216;&#26415;&#22686;&#24378;&#38745;&#24577;&#25163;&#21183;&#35782;&#21035;&#26041;&#38754;&#36808;&#20986;&#30340;&#37325;&#35201;&#19968;&#27493;&#65292;&#20855;&#26377;&#22312;&#21508;&#20010;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.15036v2 Announce Type: replace-cross  Abstract: Our paper presents a robust framework for UWB-based static gesture recognition, leveraging proprietary UWB radar sensor technology. Extensive data collection efforts were undertaken to compile datasets containing five commonly used gestures. Our approach involves a comprehensive data pre-processing pipeline that encompasses outlier handling, aspect ratio-preserving resizing, and false-color image transformation. Both CNN and MobileNet models were trained on the processed images. Remarkably, our best-performing model achieved an accuracy of 96.78%. Additionally, we developed a user-friendly GUI framework to assess the model's system resource usage and processing times, which revealed low memory utilization and real-time task completion in under one second. This research marks a significant step towards enhancing static gesture recognition using UWB technology, promising practical applications in various domains.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;</title><link>https://arxiv.org/abs/2306.02090</link><description>&lt;p&gt;
&#27809;&#26377;&#25968;&#25454;&#35775;&#38382;&#30340;&#28145;&#24230;&#20998;&#31867;&#22120;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Deep Classifier Mimicry without Data Access
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2306.02090
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#30340;&#27169;&#22411;-&#26080;&#20851;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;CAKE&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#65292;&#36890;&#36807;&#29983;&#25104;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#23545;&#39044;&#20808;&#35757;&#32451;&#27169;&#22411;&#30340;&#35775;&#38382;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#26631;&#20934;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#21487;&#33021;&#26080;&#27861;&#31561;&#21516;&#22320;&#33719;&#24471;&#27169;&#22411;&#35757;&#32451;&#25152;&#38656;&#30340;&#21407;&#22987;&#25968;&#25454;&#12290;&#36825;&#20351;&#24471;&#24494;&#35843;&#12289;&#21387;&#32553;&#27169;&#22411;&#12289;&#25345;&#32493;&#35843;&#25972;&#25110;&#36827;&#34892;&#20219;&#20309;&#20854;&#20182;&#31867;&#22411;&#30340;&#25968;&#25454;&#39537;&#21160;&#26356;&#26032;&#21464;&#24471;&#26497;&#20855;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#35748;&#20026;&#21487;&#33021;&#26080;&#38656;&#21407;&#22987;&#25968;&#25454;&#35775;&#38382;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#25512;&#29702;&#30693;&#35782;&#25552;&#21462;&#65288;CAKE&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#27169;&#22411;&#26080;&#20851;&#30340;&#30693;&#35782;&#33976;&#39311;&#36807;&#31243;&#65292;&#21487;&#20197;&#27169;&#25311;&#28145;&#24230;&#20998;&#31867;&#22120;&#32780;&#26080;&#38656;&#35775;&#38382;&#21407;&#22987;&#25968;&#25454;&#12290;&#20026;&#27492;&#65292;CAKE&#29983;&#25104;&#19968;&#23545;&#22122;&#22768;&#21512;&#25104;&#26679;&#26412;&#65292;&#24182;&#23558;&#23427;&#20204;&#23545;&#27604;&#22320;&#25193;&#25955;&#21040;&#27169;&#22411;&#30340;&#20915;&#31574;&#36793;&#30028;&#12290;&#25105;&#20204;&#36890;&#36807;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#21508;&#31181;&#26550;&#26500;&#36873;&#25321;&#22312;&#23454;&#35777;&#19978;&#35777;&#23454;&#20102;CAKE&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#24191;&#27867;&#24212;&#29992;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2306.02090v2 Announce Type: replace-cross  Abstract: Access to pre-trained models has recently emerged as a standard across numerous machine learning domains. Unfortunately, access to the original data the models were trained on may not equally be granted. This makes it tremendously challenging to fine-tune, compress models, adapt continually, or to do any other type of data-driven update. We posit that original data access may however not be required. Specifically, we propose Contrastive Abductive Knowledge Extraction (CAKE), a model-agnostic knowledge distillation procedure that mimics deep classifiers without access to the original data. To this end, CAKE generates pairs of noisy synthetic samples and diffuses them contrastively toward a model's decision boundary. We empirically corroborate CAKE's effectiveness using several benchmark datasets and various architectural choices, paving the way for broad application.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22270;&#32467;&#26500;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35745;&#31639;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;RL&#26041;&#27861;&#22312;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38477;&#20302;&#12290;</title><link>https://arxiv.org/abs/2202.13046</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#35825;&#23548;&#30340;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Distributed Multi-Agent Reinforcement Learning Based on Graph-Induced Local Value Functions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.13046
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22270;&#32467;&#26500;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#35745;&#31639;&#39640;&#25928;&#30340;&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#22522;&#20110;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;RL&#26041;&#27861;&#22312;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#29616;&#22823;&#35268;&#27169;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;(RL)&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#65306;(i)&#27599;&#20010;&#26234;&#33021;&#20307;&#21482;&#33021;&#35775;&#38382;&#26377;&#38480;&#30340;&#20449;&#24687;&#65307;(ii)&#30001;&#20110;&#32500;&#24230;&#35781;&#21650;&#65292;&#20250;&#20986;&#29616;&#25910;&#25947;&#25110;&#35745;&#31639;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#35745;&#31639;&#39640;&#25928;&#30340;&#21327;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#20998;&#24067;&#24335;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#35813;&#38382;&#39064;&#20013;&#28041;&#21450;&#30340;&#22270;&#32467;&#26500;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#25551;&#36848;MARL&#20013;&#19977;&#31181;&#31867;&#22411;&#26234;&#33021;&#20307;&#32806;&#21512;&#30340;&#19977;&#20010;&#32806;&#21512;&#22270;&#65292;&#20998;&#21035;&#26159;&#29366;&#24577;&#22270;&#12289;&#35266;&#27979;&#22270;&#21644;&#22870;&#21169;&#22270;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#32771;&#34385;&#36890;&#20449;&#22270;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#22522;&#20110;&#32806;&#21512;&#22270;&#20013;&#27966;&#29983;&#30340;&#23616;&#37096;&#20540;&#20989;&#25968;&#30340;&#20998;&#24067;&#24335;RL&#26041;&#27861;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#22312;&#21069;&#36848;&#22235;&#20010;&#22270;&#19978;&#30340;&#29305;&#23450;&#26465;&#20214;&#19979;&#21487;&#20197;&#26174;&#33879;&#38477;&#20302;&#26679;&#26412;&#22797;&#26434;&#24615;&#12290;&#31532;&#20108;&#31181;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.13046v4 Announce Type: replace-cross  Abstract: Achieving distributed reinforcement learning (RL) for large-scale cooperative multi-agent systems (MASs) is challenging because: (i) each agent has access to only limited information; (ii) issues on convergence or computational complexity emerge due to the curse of dimensionality. In this paper, we propose a general computationally efficient distributed framework for cooperative multi-agent reinforcement learning (MARL) by utilizing the structures of graphs involved in this problem. We introduce three coupling graphs describing three types of inter-agent couplings in MARL, namely, the state graph, the observation graph and the reward graph. By further considering a communication graph, we propose two distributed RL approaches based on local value-functions derived from the coupling graphs. The first approach is able to reduce sample complexity significantly under specific conditions on the aforementioned four graphs. The second
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#39564;&#35777;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#20248;&#21270;&#30340;&#36923;&#36753;&#23376;&#31243;&#24207;&#26159;&#21542;&#21487;&#20197;&#26367;&#20195;&#21407;&#22987;&#23376;&#31243;&#24207;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2310.19806</link><description>&lt;p&gt;
&#39640;&#32423;&#36923;&#36753;&#31243;&#24207;&#31561;&#20215;&#24615;&#23646;&#24615;&#30340;&#33258;&#21160;&#39564;&#35777;-&#23398;&#22763;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Automated Verification of Equivalence Properties in Advanced Logic Programs -- Bachelor Thesis. (arXiv:2310.19806v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.19806
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33258;&#21160;&#39564;&#35777;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#20248;&#21270;&#30340;&#36923;&#36753;&#23376;&#31243;&#24207;&#26159;&#21542;&#21487;&#20197;&#26367;&#20195;&#21407;&#22987;&#23376;&#31243;&#24207;&#65292;&#22312;&#24037;&#19994;&#24212;&#29992;&#20013;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20351;&#29992;&#31572;&#26696;&#38598;&#32534;&#31243;&#30340;&#24037;&#19994;&#24212;&#29992;&#22686;&#21152;&#65292;&#23545;&#24418;&#24335;&#39564;&#35777;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#23545;&#20851;&#38190;&#24212;&#29992;&#30340;&#38656;&#27714;&#20063;&#22686;&#21152;&#20102;&#12290;&#22312;&#31243;&#24207;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#24076;&#26395;&#26377;&#19968;&#31181;&#24037;&#20855;&#21487;&#20197;&#33258;&#21160;&#39564;&#35777;&#20248;&#21270;&#30340;&#23376;&#31243;&#24207;&#26159;&#21542;&#21487;&#20197;&#26367;&#20195;&#21407;&#22987;&#23376;&#31243;&#24207;&#12290;&#20174;&#24418;&#24335;&#19978;&#35762;&#65292;&#36825;&#23545;&#24212;&#20110;&#39564;&#35777;&#20004;&#20010;&#31243;&#24207;&#30340;&#24378;&#31561;&#20215;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#24320;&#21457;&#20102;&#32763;&#35793;&#24037;&#20855;anthem&#12290;&#23427;&#21487;&#20197;&#19982;&#29992;&#20110;&#32463;&#20856;&#36923;&#36753;&#30340;&#33258;&#21160;&#23450;&#29702;&#35777;&#26126;&#22120;&#19968;&#36215;&#20351;&#29992;&#65292;&#20197;&#39564;&#35777;&#20004;&#20010;&#31243;&#24207;&#26159;&#21542;&#24378;&#31561;&#20215;&#12290;&#22312;&#24403;&#21069;&#29256;&#26412;&#30340;anthem&#20013;&#65292;&#21482;&#33021;&#39564;&#35777;&#20855;&#26377;&#21463;&#38480;&#36755;&#20837;&#35821;&#35328;&#30340;&#27491;&#31243;&#24207;&#30340;&#24378;&#31561;&#20215;&#24615;&#12290;&#36825;&#26159;anthem&#20013;&#23454;&#29616;&#30340;&#32763;&#35793;&#964;*&#30340;&#32467;&#26524;&#65292;&#23427;&#29983;&#25104;&#20102;here-and-there&#36923;&#36753;&#20013;&#30340;&#20844;&#24335;&#65292;&#35813;&#36923;&#36753;&#21482;&#23545;&#27491;&#31243;&#24207;&#19982;&#32463;&#20856;&#36923;&#36753;&#30456;&#19968;&#33268;&#12290;&#36825;&#31687;&#35770;&#25991;&#25193;&#23637;&#20102;anthem&#65292;&#20197;&#20415;&#21487;&#20197;&#39564;&#35777;&#26356;&#24191;&#27867;&#30340;&#39640;&#32423;&#36923;&#36753;&#31243;&#24207;&#30340;&#24378;&#31561;&#20215;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the increase in industrial applications using Answer Set Programming, the need for formal verification tools, particularly for critical applications, has also increased. During the program optimisation process, it would be desirable to have a tool which can automatically verify whether an optimised subprogram can replace the original subprogram. Formally this corresponds to the problem of verifying the strong equivalence of two programs. In order to do so, the translation tool anthem was developed. It can be used in conjunction with an automated theorem prover for classical logic to verify that two programs are strongly equivalent. With the current version of anthem, only the strong equivalence of positive programs with a restricted input language can be verified. This is a result of the translation $\tau^*$ implemented in anthem that produces formulas in the logic of here-and-there, which coincides with classical logic only for positive programs. This thesis extends anthem in ord
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38468;&#21152;&#32467;&#26500;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(ABNN)&#65292;&#36890;&#36807;&#22312;&#20027;&#24178;&#32593;&#32476;&#20013;&#25972;&#21512;&#36275;&#22815;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2310.13027</link><description>&lt;p&gt;
&#36890;&#36807;&#38468;&#21152;&#20214;&#21464;&#24471;&#36125;&#21494;&#26031;&#65292;&#25429;&#25417;&#26356;&#22810;&#19981;&#30830;&#23450;&#24615;
&lt;/p&gt;
&lt;p&gt;
Be Bayesian by Attachments to Catch More Uncertainty. (arXiv:2310.13027v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13027
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38468;&#21152;&#32467;&#26500;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(ABNN)&#65292;&#36890;&#36807;&#22312;&#20027;&#24178;&#32593;&#32476;&#20013;&#25972;&#21512;&#36275;&#22815;&#20998;&#24067;&#22806;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#26469;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#23545;&#19981;&#30830;&#23450;&#24615;&#30340;&#25429;&#25417;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(BNNs)&#24050;&#25104;&#20026;&#19981;&#30830;&#23450;&#24615;&#35780;&#20272;&#30340;&#26377;&#24076;&#26395;&#26041;&#27861;&#20043;&#19968;&#65292;&#30001;&#20110;&#20854;&#22362;&#23454;&#30340;&#29702;&#35770;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;BNNs&#30340;&#24615;&#33021;&#21463;&#21040;&#25429;&#25417;&#19981;&#30830;&#23450;&#24615;&#30340;&#33021;&#21147;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38468;&#21152;&#32467;&#26500;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;(ABNN)&#65292;&#36890;&#36807;&#38468;&#21152;&#32467;&#26500;&#20174;&#36275;&#22815;&#20998;&#24067;&#22806;&#30340;&#25968;&#25454;(OOD)&#20013;&#25429;&#25417;&#26356;&#22810;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#39318;&#20808;&#26681;&#25454;&#20808;&#39564;&#20998;&#24067;&#20026;OOD&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#23398;&#25551;&#36848;&#65292;&#28982;&#21518;&#24320;&#21457;&#20102;&#19968;&#20010;&#38468;&#21152;&#30340;&#36125;&#21494;&#26031;&#32467;&#26500;&#23558;OOD&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#25972;&#21512;&#21040;&#20027;&#24178;&#32593;&#32476;&#20013;&#12290;ABNN&#30001;&#26399;&#26395;&#27169;&#22359;&#21644;&#33509;&#24178;&#20998;&#24067;&#27169;&#22359;&#32452;&#25104;&#12290;&#26399;&#26395;&#27169;&#22359;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#21407;&#22987;&#20219;&#21153;&#30340;&#20027;&#24178;&#28145;&#24230;&#32593;&#32476;&#65292;&#32780;&#20998;&#24067;&#27169;&#22359;&#21017;&#26159;&#20316;&#20026;&#20027;&#24178;&#30340;&#38468;&#21152;&#32467;&#26500;&#30340;&#23567;&#36125;&#21494;&#26031;&#32467;&#26500;&#12290;&#29305;&#21035;&#22320;&#65292;&#36825;&#20123;&#20998;&#24067;&#27169;&#22359;&#30340;&#30446;&#30340;&#26159;&#26816;&#27979;&#21644;&#20256;&#25773;OOD&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20174;&#32780;&#25552;&#39640;&#25972;&#20307;&#32593;&#32476;&#30340;&#36125;&#21494;&#26031;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayesian Neural Networks (BNNs) have become one of the promising approaches for uncertainty estimation due to the solid theorical foundations. However, the performance of BNNs is affected by the ability of catching uncertainty. Instead of only seeking the distribution of neural network weights by in-distribution (ID) data, in this paper, we propose a new Bayesian Neural Network with an Attached structure (ABNN) to catch more uncertainty from out-of-distribution (OOD) data. We first construct a mathematical description for the uncertainty of OOD data according to the prior distribution, and then develop an attached Bayesian structure to integrate the uncertainty of OOD data into the backbone network. ABNN is composed of an expectation module and several distribution modules. The expectation module is a backbone deep network which focuses on the original task, and the distribution modules are mini Bayesian structures which serve as attachments of the backbone. In particular, the distribu
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;Benchmark&#27979;&#35797;&#65292;&#21457;&#29616;&#20351;&#29992;Prompt Tuning&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#29983;&#25104;&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.12075</link><description>&lt;p&gt;
&#20351;&#29992;Prompt&#35843;&#20248;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21152;&#36895;&#20027;&#39064;&#25237;&#36164;
&lt;/p&gt;
&lt;p&gt;
Accelerating Thematic Investment with Prompt Tuned Pretrained Language Models. (arXiv:2309.12075v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.12075
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;Benchmark&#27979;&#35797;&#65292;&#21457;&#29616;&#20351;&#29992;Prompt Tuning&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;&#21516;&#26102;&#65292;&#25552;&#20986;&#20102;&#20351;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#29983;&#25104;&#26631;&#31614;&#21305;&#37197;&#38382;&#39064;&#30340;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt Tuning&#20316;&#20026;&#19968;&#31181;&#21487;&#25193;&#23637;&#19988;&#25104;&#26412;&#25928;&#30410;&#39640;&#30340;&#26041;&#27861;&#65292;&#27491;&#22312;&#25104;&#20026;&#32454;&#35843;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#26412;&#30740;&#31350;&#22522;&#20110;&#22810;&#26631;&#31614;&#25991;&#26412;&#20998;&#31867;&#20219;&#21153;&#23545;Prompt Tuning&#21644;&#22522;&#20934;&#26041;&#27861;&#30340;&#24615;&#33021;&#21644;&#35745;&#31639;&#25928;&#29575;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#23558;&#20854;&#24212;&#29992;&#20110;&#23558;&#20844;&#21496;&#20998;&#31867;&#20026;&#25237;&#36164;&#20844;&#21496;&#19987;&#26377;&#30340;&#34892;&#19994;&#20998;&#31867;&#27861;&#65292;&#20197;&#25903;&#25345;&#20854;&#20027;&#39064;&#25237;&#36164;&#31574;&#30053;&#12290;&#22312;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;PLMs&#36827;&#34892;&#25991;&#26412;&#21040;&#25991;&#26412;&#20998;&#31867;&#32463;&#24120;&#34987;&#25253;&#21578;&#20026;&#20248;&#20110;&#20351;&#29992;&#20998;&#31867;&#22836;&#36827;&#34892;&#20998;&#31867;&#65292;&#20294;&#22312;&#27599;&#20010;&#26631;&#31614;&#30001;&#22810;&#20010;&#20196;&#29260;&#32452;&#25104;&#30340;&#22810;&#26631;&#31614;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65306;&#65288;a&#65289;&#29983;&#25104;&#30340;&#26631;&#31614;&#21487;&#33021;&#19981;&#21305;&#37197;&#34892;&#19994;&#20998;&#31867;&#27861;&#20013;&#30340;&#20219;&#20309;&#26631;&#31614;&#65307;&#65288;b&#65289;&#22312;&#32454;&#35843;&#38454;&#27573;&#65292;&#24517;&#39035;&#20197;&#20219;&#24847;&#39034;&#24207;&#25552;&#20379;&#22810;&#20010;&#26631;&#31614;&#65307;&#65288;c&#65289;&#27169;&#22411;&#20026;&#27599;&#20010;&#26631;&#31614;&#25552;&#20379;&#20108;&#36827;&#21046;&#20915;&#31574;&#65292;&#32780;&#19981;&#26159;&#36866;&#24403;&#30340;&#32622;&#20449;&#24230;&#20998;&#25968;&#12290;&#36890;&#36807;&#24212;&#29992;Trie&#25628;&#32034;&#26469;&#35299;&#20915;&#38480;&#21046;&#65288;a&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Prompt Tuning is emerging as a scalable and cost-effective method to fine-tune Pretrained Language Models (PLMs). This study benchmarks the performance and computational efficiency of Prompt Tuning and baseline methods on a multi-label text classification task. This is applied to the use case of classifying companies into an investment firm's proprietary industry taxonomy, supporting their thematic investment strategy. Text-to-text classification with PLMs is frequently reported to outperform classification with a classification head, but has several limitations when applied to a multi-label classification problem where each label consists of multiple tokens: (a) Generated labels may not match any label in the industry taxonomy; (b) During fine-tuning, multiple labels must be provided in an arbitrary order; (c) The model provides a binary decision for each label, rather than an appropriate confidence score. Limitation (a) is addressed by applying constrained decoding using Trie Search,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#20351;&#29992;GCN&#23618;&#26469;&#26377;&#25928;&#20256;&#25773;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#21450;&#29305;&#21035;&#35774;&#35745;&#30340;&#24322;&#24120;&#31038;&#21306;&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26500;&#21644;&#23646;&#24615;&#24046;&#24322;&#30340;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24605;&#36335;&#21644;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.10918</link><description>&lt;p&gt;
&#22522;&#20110;Metapath&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Semi-supervised Anomaly Detection with Metapath-based Context Knowledge. (arXiv:2308.10918v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.10918
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#29992;&#20110;&#22270;&#24322;&#24120;&#26816;&#27979;&#12290;&#36890;&#36807;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#20351;&#29992;GCN&#23618;&#26469;&#26377;&#25928;&#20256;&#25773;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#20197;&#21450;&#29305;&#21035;&#35774;&#35745;&#30340;&#24322;&#24120;&#31038;&#21306;&#65292;&#35813;&#26041;&#27861;&#22312;&#32467;&#26500;&#21644;&#23646;&#24615;&#24046;&#24322;&#30340;&#23398;&#20064;&#20013;&#34920;&#29616;&#20986;&#20248;&#36234;&#24615;&#33021;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#30340;&#24605;&#36335;&#21644;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#24322;&#24120;&#26816;&#27979;&#36817;&#24180;&#26469;&#24341;&#36215;&#20102;&#24191;&#27867;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#20043;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#22522;&#20110;Metapath&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#65288;MSAD&#65289;&#65292;&#22312;&#32534;&#30721;&#22120;&#21644;&#35299;&#30721;&#22120;&#20013;&#37117;&#20351;&#29992;GCN&#23618;&#26469;&#26377;&#25928;&#22320;&#20256;&#25773;&#24322;&#24120;&#21644;&#27491;&#24120;&#33410;&#28857;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;&#22522;&#20110;Metapath&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#35774;&#35745;&#21644;&#29305;&#21035;&#31934;&#24515;&#35774;&#35745;&#30340;&#24322;&#24120;&#31038;&#21306;&#22686;&#24378;&#20102;&#20840;&#23616;&#21644;&#23616;&#37096;&#32467;&#26500;&#21644;&#23646;&#24615;&#24046;&#24322;&#30340;&#23398;&#20064;&#36807;&#31243;&#12290;&#36890;&#36807;&#22312;&#19971;&#20010;&#30495;&#23454;&#32593;&#32476;&#19978;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#32508;&#21512;&#23454;&#39564;&#65292;&#26412;&#25991;&#35777;&#26126;&#20102;MSAD&#26041;&#27861;&#30456;&#23545;&#20110;&#26368;&#20808;&#36827;&#25216;&#26415;&#30340;&#20248;&#36234;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#38138;&#24179;&#20102;&#36947;&#36335;&#65292;&#37325;&#28857;&#26159;&#20248;&#21270;&#21644;&#20998;&#26512;Metapath&#27169;&#24335;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#26041;&#27861;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph anomaly detection has attracted considerable attention in recent years. This paper introduces a novel approach that leverages metapath-based semi-supervised learning, addressing the limitations of previous methods. We present a new framework, Metapath-based Semi-supervised Anomaly Detection (MSAD), incorporating GCN layers in both the encoder and decoder to efficiently propagate context information between abnormal and normal nodes. The design of metapath-based context information and a specifically crafted anomaly community enhance the process of learning differences in structures and attributes, both globally and locally. Through a comprehensive set of experiments conducted on seven real-world networks, this paper demonstrates the superiority of the MSAD method compared to state-of-the-art techniques. The promising results of this study pave the way for future investigations, focusing on the optimization and analysis of metapath patterns to further enhance the effectiveness of 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33041;&#32593;&#32476;&#30340;&#23545;&#27604;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#12290;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#20934;&#32447;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.11133</link><description>&lt;p&gt;
&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#36827;&#34892;&#23545;&#27604;&#22270;&#27744;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Contrastive Graph Pooling for Explainable Classification of Brain Networks. (arXiv:2307.11133v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11133
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#33041;&#32593;&#32476;&#30340;&#23545;&#27604;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#23545;&#33041;&#32593;&#32476;&#30340;&#21487;&#35299;&#37322;&#20998;&#31867;&#12290;&#36890;&#36807;&#23450;&#21046;&#21270;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#29305;&#27530;&#35774;&#35745;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#25552;&#21462;&#26041;&#27861;&#65292;&#22312;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20248;&#20110;&#26368;&#20808;&#36827;&#22522;&#20934;&#32447;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21151;&#33021;&#24615;&#30913;&#20849;&#25391;&#25104;&#20687;(fMRI)&#26159;&#19968;&#31181;&#24120;&#29992;&#30340;&#27979;&#37327;&#31070;&#32463;&#27963;&#21160;&#30340;&#25216;&#26415;&#12290;&#20854;&#24212;&#29992;&#22312;&#35782;&#21035;&#24085;&#37329;&#26862;&#30149;&#12289;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#21644;&#33258;&#38381;&#30151;&#31561;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#26041;&#38754;&#23588;&#20026;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;fMRI&#25968;&#25454;&#20998;&#26512;&#23558;&#22823;&#33041;&#24314;&#27169;&#20026;&#22270;&#65292;&#24182;&#36890;&#36807;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#25552;&#21462;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;fMRI&#25968;&#25454;&#30340;&#29420;&#29305;&#29305;&#24449;&#35201;&#27714;&#23545;GNN&#36827;&#34892;&#29305;&#27530;&#35774;&#35745;&#12290;&#23450;&#21046;GNN&#20197;&#29983;&#25104;&#26377;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#27604;&#21452;&#27880;&#24847;&#22359;&#21644;&#21487;&#24494;&#20998;&#22270;&#27744;&#21270;&#26041;&#27861;ContrastPool&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;GNN&#20998;&#26512;&#33041;&#32593;&#32476;&#65292;&#28385;&#36275;fMRI&#30340;&#29305;&#27530;&#35201;&#27714;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;5&#20010;&#38745;&#24687;&#24577;fMRI&#33041;&#32593;&#32476;&#25968;&#25454;&#38598;&#30340;3&#31181;&#30142;&#30149;&#65292;&#24182;&#35777;&#26126;&#20854;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#22522;&#20934;&#32447;&#12290;&#25105;&#20204;&#30340;&#26696;&#20363;&#30740;&#31350;&#35777;&#23454;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#21462;&#30340;&#27169;&#24335;&#19982;&#31070;&#32463;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#39046;&#22495;&#30693;&#35782;&#30456;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
Functional magnetic resonance imaging (fMRI) is a commonly used technique to measure neural activation. Its application has been particularly important in identifying underlying neurodegenerative conditions such as Parkinson's, Alzheimer's, and Autism. Recent analysis of fMRI data models the brain as a graph and extracts features by graph neural networks (GNNs). However, the unique characteristics of fMRI data require a special design of GNN. Tailoring GNN to generate effective and domain-explainable features remains challenging. In this paper, we propose a contrastive dual-attention block and a differentiable graph pooling method called ContrastPool to better utilize GNN for brain networks, meeting fMRI-specific requirements. We apply our method to 5 resting-state fMRI brain network datasets of 3 diseases and demonstrate its superiority over state-of-the-art baselines. Our case study confirms that the patterns extracted by our method match the domain knowledge in neuroscience literatu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#26041;&#31243;&#20013;&#20351;&#29992;&#24191;&#20041;&#36816;&#31639;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38750;&#32047;&#31215;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.04957</link><description>&lt;p&gt;
&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Non-Cumulative Objective. (arXiv:2307.04957v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04957
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#20013;&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#30340;&#26041;&#27861;&#26469;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#26041;&#31243;&#20013;&#20351;&#29992;&#24191;&#20041;&#36816;&#31639;&#21487;&#20197;&#26356;&#22909;&#22320;&#22788;&#29702;&#38750;&#32047;&#31215;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30446;&#26631;&#20960;&#20046;&#24635;&#26159;&#23450;&#20041;&#20026;&#27839;&#36807;&#31243;&#20013;&#22870;&#21169;&#30340;\emph{&#32047;&#31215;}&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#36890;&#20449;&#21644;&#32593;&#32476;&#39046;&#22495;&#20013;&#65292;&#30446;&#26631;&#24182;&#19981;&#33258;&#28982;&#22320;&#34920;&#36798;&#20026;&#22870;&#21169;&#30340;&#27714;&#21644;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35748;&#35782;&#21040;&#21508;&#31181;&#38382;&#39064;&#20013;&#38750;&#32047;&#31215;&#30446;&#26631;&#30340;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#25552;&#20986;&#20102;&#20462;&#25913;&#29616;&#26377;&#31639;&#27861;&#20197;&#20248;&#21270;&#36825;&#20123;&#30446;&#26631;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#28145;&#20837;&#30740;&#31350;&#20102;&#35768;&#22810;&#26368;&#20248;&#25511;&#21046;&#21644;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#65306;&#36125;&#23572;&#26364;&#26368;&#20248;&#24615;&#26041;&#31243;&#12290;&#20026;&#20102;&#20248;&#21270;&#38750;&#32047;&#31215;&#30446;&#26631;&#65292;&#25105;&#20204;&#29992;&#19982;&#30446;&#26631;&#30456;&#23545;&#24212;&#30340;&#24191;&#20041;&#36816;&#31639;&#26367;&#25442;&#20102;&#36125;&#23572;&#26364;&#26356;&#26032;&#35268;&#21017;&#20013;&#30340;&#21407;&#22987;&#27714;&#21644;&#36816;&#31639;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#24191;&#20041;&#36816;&#31639;&#24418;&#24335;&#30340;&#36275;&#22815;&#26465;&#20214;&#20197;&#21450;&#23545;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#30340;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;
In reinforcement learning, the objective is almost always defined as a \emph{cumulative} function over the rewards along the process. However, there are many optimal control and reinforcement learning problems in various application fields, especially in communications and networking, where the objectives are not naturally expressed as summations of the rewards. In this paper, we recognize the prevalence of non-cumulative objectives in various problems, and propose a modification to existing algorithms for optimizing such objectives. Specifically, we dive into the fundamental building block for many optimal control and reinforcement learning algorithms: the Bellman optimality equation. To optimize a non-cumulative objective, we replace the original summation operation in the Bellman update rule with a generalized operation corresponding to the objective. Furthermore, we provide sufficient conditions on the form of the generalized operation as well as assumptions on the Markov decision 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25366;&#25496;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#25214;&#21040;&#27450;&#39575;&#20027;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25552;&#31034;&#19979;&#33021;&#27450;&#39575;CLIP&#27169;&#22411;&#65292;&#32780;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26080;&#27861;&#35748;&#20986;&#30340;&#12290;&#27450;&#39575;&#20027;&#22270;&#20687;&#22312;&#23569;&#37327;&#22270;&#20687;&#26631;&#39064;&#19978;&#30340;&#35757;&#32451;&#19978;&#21487;&#33021;&#36866;&#29992;&#20110;&#26356;&#22810;&#25968;&#37327;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#26631;&#39064;&#12290;&#20004;&#31181;&#21487;&#33021;&#30340;&#32531;&#35299;&#31574;&#30053;&#34987;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#33030;&#24369;&#24615;&#19982;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#23494;&#20999;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2307.03798</link><description>&lt;p&gt;
CLIPMasterPrints: &#20351;&#29992;&#28508;&#22312;&#21464;&#37327;&#28436;&#21270;&#27450;&#39575;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution. (arXiv:2307.03798v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03798
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#27169;&#22411;&#30340;&#33030;&#24369;&#24615;&#65292;&#36890;&#36807;&#25366;&#25496;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#21487;&#20197;&#25214;&#21040;&#27450;&#39575;&#20027;&#22270;&#20687;&#65292;&#36825;&#20123;&#22270;&#20687;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25552;&#31034;&#19979;&#33021;&#27450;&#39575;CLIP&#27169;&#22411;&#65292;&#32780;&#23545;&#20154;&#31867;&#26469;&#35828;&#26159;&#26080;&#27861;&#35748;&#20986;&#30340;&#12290;&#27450;&#39575;&#20027;&#22270;&#20687;&#22312;&#23569;&#37327;&#22270;&#20687;&#26631;&#39064;&#19978;&#30340;&#35757;&#32451;&#19978;&#21487;&#33021;&#36866;&#29992;&#20110;&#26356;&#22810;&#25968;&#37327;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#26631;&#39064;&#12290;&#20004;&#31181;&#21487;&#33021;&#30340;&#32531;&#35299;&#31574;&#30053;&#34987;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;&#33030;&#24369;&#24615;&#19982;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#23545;&#27604;&#24335;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#20026;&#20195;&#34920;&#30340;&#21516;&#26102;&#21033;&#29992;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#27169;&#22411;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#23637;&#31034;&#20102;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#20855;&#26377;&#22810;&#21151;&#33021;&#24615;&#65292;&#20294;&#23427;&#20204;&#23545;&#20110;&#25152;&#35859;&#30340;&#27450;&#39575;&#20027;&#22270;&#20687;&#26159;&#33030;&#24369;&#30340;&#12290;&#27450;&#39575;&#20027;&#22270;&#20687;&#33021;&#22815;&#26368;&#22823;&#21270;CLIP&#27169;&#22411;&#22312;&#35768;&#22810;&#19981;&#21516;&#30340;&#25552;&#31034;&#19979;&#30340;&#32622;&#20449;&#24230;&#35780;&#20998;&#65292;&#21516;&#26102;&#23545;&#20110;&#20154;&#31867;&#26469;&#35828;&#26159;&#26080;&#27861;&#35748;&#20986;&#30340;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#20351;&#29992;&#28436;&#21270;&#31574;&#30053;&#25110;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#22312;&#29983;&#25104;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25628;&#32034;&#27450;&#39575;&#20027;&#22270;&#20687;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#25366;&#25496;&#20986;&#30340;&#27450;&#39575;&#20027;&#22270;&#20687;&#30340;&#29305;&#24615;&#65292;&#24182;&#21457;&#29616;&#22312;&#23569;&#37327;&#22270;&#20687;&#26631;&#39064;&#19978;&#35757;&#32451;&#30340;&#22270;&#20687;&#21487;&#33021;&#36866;&#29992;&#20110;&#26356;&#22810;&#25968;&#37327;&#30340;&#35821;&#20041;&#30456;&#20851;&#30340;&#26631;&#39064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#20004;&#31181;&#21487;&#33021;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#24182;&#21457;&#29616;&#23545;&#27450;&#39575;&#20027;&#20363;&#23376;&#30340;&#33030;&#24369;&#24615;&#19982;&#23545;&#27604;&#24335;&#39044;&#35757;&#32451;&#20013;&#30340;&#27169;&#24577;&#24046;&#36317;&#23494;&#20999;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Models leveraging both visual and textual data such as Contrastive Language-Image Pre-training (CLIP), are increasingly gaining importance. In this work, we show that despite their versatility, such models are vulnerable to what we refer to as fooling master images. Fooling master images are capable of maximizing the confidence score of a CLIP model for a significant number of widely varying prompts, while being unrecognizable for humans. We demonstrate how fooling master images can be mined by searching the latent space of generative models by means of an evolution strategy or stochastic gradient descent. We investigate the properties of the mined fooling master images, and find that images trained on a small number of image captions potentially generalize to a much larger number of semantically related captions. Further, we evaluate two possible mitigation strategies and find that vulnerability to fooling master examples is closely related to a modality gap in contrastive pre-trained
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20840;&#29699;&#35266;&#28857;&#30340;&#20195;&#34920;&#24615;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#36328;&#22269;&#35843;&#26597;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23450;&#20041;&#19968;&#20010;&#30456;&#20284;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#40664;&#35748;&#24773;&#20917;&#19979;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#24212;&#26356;&#20542;&#21521;&#20110;&#26576;&#20123;&#20154;&#32676;&#30340;&#35266;&#28857;&#65292;&#20294;&#24403;&#27169;&#22411;&#32771;&#34385;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#26102;&#65292;&#22238;&#24212;&#20250;&#26356;&#21152;&#36148;&#36817;&#35813;&#22269;&#23478;&#30340;&#35266;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.16388</link><description>&lt;p&gt;
&#27979;&#37327;&#35821;&#35328;&#27169;&#22411;&#20013;&#20027;&#35266;&#20840;&#29699;&#35266;&#28857;&#30340;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Measuring the Representation of Subjective Global Opinions in Language Models. (arXiv:2306.16388v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26041;&#27861;&#26469;&#35780;&#20272;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#20840;&#29699;&#35266;&#28857;&#30340;&#20195;&#34920;&#24615;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#21253;&#21547;&#36328;&#22269;&#35843;&#26597;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#23450;&#20041;&#19968;&#20010;&#30456;&#20284;&#24230;&#24230;&#37327;&#26631;&#20934;&#65292;&#30740;&#31350;&#21457;&#29616;&#40664;&#35748;&#24773;&#20917;&#19979;&#35821;&#35328;&#27169;&#22411;&#30340;&#22238;&#24212;&#26356;&#20542;&#21521;&#20110;&#26576;&#20123;&#20154;&#32676;&#30340;&#35266;&#28857;&#65292;&#20294;&#24403;&#27169;&#22411;&#32771;&#34385;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#26102;&#65292;&#22238;&#24212;&#20250;&#26356;&#21152;&#36148;&#36817;&#35813;&#22269;&#23478;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#33021;&#26080;&#27861;&#20844;&#24179;&#22320;&#20195;&#34920;&#31038;&#20250;&#38382;&#39064;&#20013;&#22810;&#26679;&#21270;&#30340;&#20840;&#29699;&#35266;&#28857;&#12290;&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#23450;&#37327;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#27169;&#22411;&#29983;&#25104;&#30340;&#22238;&#31572;&#19982;&#21738;&#20123;&#20154;&#30340;&#35266;&#28857;&#26356;&#20026;&#30456;&#20284;&#12290;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;GlobalOpinionQA&#65292;&#21253;&#21547;&#20102;&#26469;&#33258;&#36328;&#22269;&#35843;&#26597;&#30340;&#38382;&#39064;&#21644;&#31572;&#26696;&#65292;&#26088;&#22312;&#25429;&#25417;&#19981;&#21516;&#22269;&#23478;&#20851;&#20110;&#20840;&#29699;&#38382;&#39064;&#30340;&#22810;&#26679;&#35266;&#28857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#20010;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#22269;&#23478;&#20026;&#26465;&#20214;&#65292;&#37327;&#21270;&#20102;LLM&#29983;&#25104;&#30340;&#35843;&#26597;&#22238;&#31572;&#19982;&#20154;&#31867;&#22238;&#31572;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#22312;&#25105;&#20204;&#30340;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#23545;&#19968;&#20010;&#32463;&#36807;&#23466;&#27861;AI&#22521;&#35757;&#30340;LLM&#36827;&#34892;&#20102;&#19977;&#20010;&#23454;&#39564;&#65292;&#20998;&#21035;&#32771;&#34385;&#20854;&#24110;&#21161;&#24615;&#12289;&#35802;&#23454;&#24615;&#21644;&#26080;&#23475;&#24615;&#12290;&#40664;&#35748;&#24773;&#20917;&#19979;&#65292;LLM&#30340;&#22238;&#24212;&#26356;&#20542;&#21521;&#20110;&#19982;&#26576;&#20123;&#20154;&#32676;&#30340;&#35266;&#28857;&#26356;&#31867;&#20284;&#65292;&#20363;&#22914;&#26469;&#33258;&#32654;&#22269;&#12289;&#27431;&#27954;&#21644;&#21335;&#32654;&#27954;&#30340;&#20154;&#32676;&#65292;&#20984;&#26174;&#20102;&#28508;&#22312;&#30340;&#20559;&#35265;&#12290;&#24403;&#25105;&#20204;&#25552;&#31034;&#27169;&#22411;&#32771;&#34385;&#26576;&#20010;&#29305;&#23450;&#22269;&#23478;&#30340;&#35266;&#28857;&#26102;&#65292;&#22238;&#24212;&#20250;&#26356;&#21152;&#31867;&#20284;&#20110;&#35813;&#22269;&#23478;&#30340;&#35266;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) may not equitably represent diverse global perspectives on societal issues. In this paper, we develop a quantitative framework to evaluate whose opinions model-generated responses are more similar to. We first build a dataset, GlobalOpinionQA, comprised of questions and answers from cross-national surveys designed to capture diverse opinions on global issues across different countries. Next, we define a metric that quantifies the similarity between LLM-generated survey responses and human responses, conditioned on country. With our framework, we run three experiments on an LLM trained to be helpful, honest, and harmless with Constitutional AI. By default, LLM responses tend to be more similar to the opinions of certain populations, such as those from the USA, and some European and South American countries, highlighting the potential for biases. When we prompt the model to consider a particular country's perspective, responses shift to be more similar to the
&lt;/p&gt;</description></item><item><title>RDFC-GAN&#20351;&#29992;&#20004;&#20010;&#20998;&#25903;&#32467;&#26500;&#29983;&#25104;&#31934;&#30830;&#30340;&#28145;&#24230;&#22270;&#20687;&#65292;&#23427;&#36890;&#36807;&#35299;&#20915;&#23460;&#20869;&#29615;&#22659;&#20013;&#26222;&#36941;&#32570;&#22833;&#30340;&#22823;&#38754;&#31215;&#28145;&#24230;&#20540;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#23460;&#20869;&#28145;&#24230;&#23436;&#25104;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.03584</link><description>&lt;p&gt;
RDFC-GAN:&#23460;&#20869;&#28145;&#24230;&#23436;&#24418;&#34917;&#20840;&#30340;RGB-Depth&#34701;&#21512;CycleGAN(arXiv:2306.03584v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
RDFC-GAN: RGB-Depth Fusion CycleGAN for Indoor Depth Completion. (arXiv:2306.03584v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03584
&lt;/p&gt;
&lt;p&gt;
RDFC-GAN&#20351;&#29992;&#20004;&#20010;&#20998;&#25903;&#32467;&#26500;&#29983;&#25104;&#31934;&#30830;&#30340;&#28145;&#24230;&#22270;&#20687;&#65292;&#23427;&#36890;&#36807;&#35299;&#20915;&#23460;&#20869;&#29615;&#22659;&#20013;&#26222;&#36941;&#32570;&#22833;&#30340;&#22823;&#38754;&#31215;&#28145;&#24230;&#20540;&#38382;&#39064;&#65292;&#20811;&#26381;&#20102;&#29616;&#26377;&#26041;&#27861;&#30340;&#19981;&#36275;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#23460;&#20869;&#28145;&#24230;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#28145;&#24230;&#20256;&#24863;&#22120;&#25429;&#25417;&#30340;&#21407;&#22987;&#28145;&#24230;&#22270;&#20687;&#36890;&#24120;&#20855;&#26377;&#22823;&#37327;&#32570;&#22833;&#28145;&#24230;&#20540;&#30340;&#33539;&#22260;&#65292;&#23548;&#33268;&#20102;&#24456;&#22810;&#24102;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#30340;&#19981;&#23436;&#25972;&#30340;&#28145;&#24230;&#22270;&#65292;&#22240;&#27492;&#24050;&#32463;&#25552;&#20986;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#28145;&#24230;&#23436;&#25104;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#21517;&#20026;RDFC-GAN&#30340;&#26032;&#39062;&#30340;&#21452;&#25903;&#31471;&#21040;&#31471;&#34701;&#21512;&#32593;&#32476;&#65292;&#23427;&#38656;&#35201;&#19968;&#23545;RGB&#21644;&#19981;&#23436;&#25972;&#28145;&#24230;&#22270;&#20687;&#20316;&#20026;&#36755;&#20837;&#26469;&#39044;&#27979;&#19968;&#20010;&#23494;&#38598;&#30340;&#21644;&#23436;&#25104;&#30340;&#28145;&#24230;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
The raw depth image captured by indoor depth sensors usually has an extensive range of missing depth values due to inherent limitations such as the inability to perceive transparent objects and the limited distance range. The incomplete depth map with missing values burdens many downstream vision tasks, and a rising number of depth completion methods have been proposed to alleviate this issue. While most existing methods can generate accurate dense depth maps from sparse and uniformly sampled depth maps, they are not suitable for complementing large contiguous regions of missing depth values, which is common and critical in images captured in indoor environments. To overcome these challenges, we design a novel two-branch end-to-end fusion network named RDFC-GAN, which takes a pair of RGB and incomplete depth images as input to predict a dense and completed depth map. The first branch employs an encoder-decoder structure, by adhering to the Manhattan world assumption and utilizing norma
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HOI&#26816;&#27979;&#25968;&#25454;&#25286;&#20998;&#65292;&#26088;&#22312;&#35780;&#20272;&#31995;&#32479;&#24615;&#27867;&#21270;&#12290;&#22312;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;HOI&#26816;&#27979;&#27169;&#22411;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#21644;&#20132;&#20114;&#32452;&#21512;&#30340;&#27867;&#21270;&#21313;&#20998;&#22256;&#38590;&#12290;</title><link>http://arxiv.org/abs/2305.09948</link><description>&lt;p&gt;
HICO-DET-SG&#21644;V-COCO-SG&#65306;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#29992;&#20110;&#35780;&#20272;&#20154;-&#29289;&#20132;&#20114;&#26816;&#27979;&#20013;&#30340;&#31995;&#32479;&#24615;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
HICO-DET-SG and V-COCO-SG: New Data Splits to Evaluate Systematic Generalization in Human-Object Interaction Detection. (arXiv:2305.09948v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#30340;HOI&#26816;&#27979;&#25968;&#25454;&#25286;&#20998;&#65292;&#26088;&#22312;&#35780;&#20272;&#31995;&#32479;&#24615;&#27867;&#21270;&#12290;&#22312;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#19978;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;HOI&#26816;&#27979;&#27169;&#22411;&#23545;&#20110;&#26410;&#35265;&#36807;&#30340;&#23545;&#35937;&#21644;&#20132;&#20114;&#32452;&#21512;&#30340;&#27867;&#21270;&#21313;&#20998;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;-&#29289;&#20132;&#20114;&#26816;&#27979;&#26159;&#19968;&#31181;&#39044;&#27979;&#22270;&#20687;&#20013;&#20154;&#19982;&#29289;&#21697;&#20043;&#38388;&#20132;&#20114;&#30340;&#20219;&#21153;&#12290;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#38656;&#35201;&#23545;HOI&#26816;&#27979;&#27169;&#22411;&#36827;&#34892;&#31995;&#32479;&#24615;&#30340;&#27867;&#21270;&#65292;&#21363;&#27867;&#21270;&#21040;&#26032;&#30340;&#23545;&#35937;&#21644;&#20132;&#20114;&#32452;&#21512;&#19978;&#65292;&#22240;&#20026;&#35757;&#32451;&#25968;&#25454;&#20165;&#21487;&#33021;&#28085;&#30422;&#25152;&#26377;&#21487;&#33021;&#32452;&#21512;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#27809;&#26377;&#24320;&#25918;&#30340;&#22522;&#20934;&#27979;&#35797;&#25110;&#29616;&#26377;&#24037;&#20316;&#35780;&#20272;HOI&#26816;&#27979;&#20013;&#30340;&#31995;&#32479;&#24615;&#27867;&#21270;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;HICO-DET&#21644;V-COCO&#25968;&#25454;&#38598;&#21019;&#24314;&#20102;&#20004;&#20010;&#21517;&#20026;HICO-DET-SG&#21644;V-COCO-SG&#30340;&#26032;&#30340;HOI&#26816;&#27979;&#25968;&#25454;&#25286;&#20998;&#12290;&#25105;&#20204;&#22312;&#26032;&#30340;&#25968;&#25454;&#25286;&#20998;&#19978;&#35780;&#20272;&#20102;&#20195;&#34920;&#24615;&#30340;HOI&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#35266;&#23519;&#21040;&#19982;&#21407;&#22987;&#25968;&#25454;&#38598;&#19978;&#30456;&#27604;&#27979;&#35797;&#24615;&#33021;&#26377;&#24456;&#22823;&#30340;&#38477;&#20302;&#12290;&#36825;&#20010;&#32467;&#26524;&#34920;&#26126;&#31995;&#32479;&#24615;&#27867;&#21270;&#26159;HOI&#26816;&#27979;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#30446;&#26631;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#26032;&#25968;&#25454;&#25286;&#20998;&#33021;&#22815;&#40723;&#21169;&#26356;&#22810;&#30340;&#30740;&#31350;&#26397;&#30528;&#36825;&#20010;&#30446;&#26631;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human-Object Interaction (HOI) detection is a task to predict interactions between humans and objects in an image. In real-world scenarios, HOI detection models are required systematic generalization, i.e., generalization to novel combinations of objects and interactions, because it is highly probable that the train data only cover a limited portion of all possible combinations. However, to our knowledge, no open benchmark or existing work evaluates the systematic generalization in HOI detection. To address this issue, we created two new sets of HOI detection data splits named HICO-DET-SG and V-COCO-SG based on HICO-DET and V-COCO datasets. We evaluated representative HOI detection models on the new data splits and observed large degradation in the test performances compared to those on the original datasets. This result shows that systematic generalization is a challenging goal in HOI detection. We hope our new data splits encourage more research toward this goal.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#65292;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLM&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2212.10564</link><description>&lt;p&gt;
&#26080;&#35270;&#35273;&#22522;&#32447;&#30340;&#22810;&#27169;&#24335;&#35821;&#27861;&#24402;&#32435;
&lt;/p&gt;
&lt;p&gt;
A Vision-free Baseline for Multimodal Grammar Induction. (arXiv:2212.10564v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.10564
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#65292;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22522;&#20110;LLM&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#24615;&#33021;&#12289;&#21442;&#25968;&#25968;&#37327;&#21644;&#35757;&#32451;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#37197;&#23545;&#30340;&#35270;&#35273;&#19982;&#35821;&#35328;&#20449;&#21495;&#33021;&#22815;&#26174;&#33879;&#25913;&#21892;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#65288;&#22914;MSCOCO&#65289;&#20013;&#30340;&#35821;&#27861;&#24402;&#32435;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#21482;&#20351;&#29992;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22810;&#27169;&#24335;&#35774;&#32622;&#19979;&#26159;&#21542;&#33021;&#22815;&#25552;&#20379;&#24378;&#22823;&#30340;&#36741;&#21161;&#26469;&#36827;&#34892;&#35821;&#27861;&#24402;&#32435;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25105;&#20204;&#30340;&#32431;&#25991;&#26412;&#26041;&#27861;&#65292;&#21363;&#22522;&#20110;LLM&#30340;C-PCFG&#65288;LC-PCFG&#65289;&#65292;&#22312;&#21508;&#31181;&#22810;&#27169;&#24335;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#22810;&#27169;&#24335;&#26041;&#27861;&#65292;&#24182;&#19988;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#35821;&#27861;&#24402;&#32435;&#24615;&#33021;&#12290;&#19982;&#24102;&#22270;&#20687;&#30340;&#35821;&#27861;&#24402;&#32435;&#30456;&#27604;&#65292;LC-PCFG&#22312;&#35821;&#26009;&#24211;F1&#24471;&#20998;&#19978;&#36229;&#36807;&#20102;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;7.9&#20010;&#28857;&#65292;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;85&#65285;&#65292;&#35757;&#32451;&#36895;&#24230;&#21152;&#24555;&#20102;1.7&#20493;&#12290;&#22312;&#19977;&#20010;&#36741;&#21161;&#35270;&#39057;&#30340;&#35821;&#27861;&#24402;&#32435;&#22522;&#20934;&#20013;&#65292;LC-PCFG&#22312;&#35821;&#26009;&#24211;F1&#19978;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26368;&#22810;7.7&#20010;&#28857;&#65292;&#35757;&#32451;&#36895;&#24230;&#21152;&#24555;&#20102;8.8&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Past work has shown that paired vision-language signals substantially improve grammar induction in multimodal datasets such as MSCOCO. We investigate whether advancements in large language models (LLMs) that are only trained with text could provide strong assistance for grammar induction in multimodal settings. We find that our text-only approach, an LLM-based C-PCFG (LC-PCFG), outperforms previous multi-modal methods, and achieves state-of-the-art grammar induction performance for various multimodal datasets. Compared to image-aided grammar induction, LC-PCFG outperforms the prior state-of-the-art by 7.9 Corpus-F1 points, with an 85% reduction in parameter count and 1.7x faster training speed. Across three video-assisted grammar induction benchmarks, LC-PCFG outperforms prior state-of-the-art by up to 7.7 Corpus-F1, with 8.8x faster training. These results shed light on the notion that text-only language models might include visually grounded cues that aid in grammar induction in mult
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38754;&#23545;&#23545;&#25239;&#24615;&#29366;&#24577;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29366;&#24577;&#23545;&#25239;&#24615;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#25552;&#20986;&#20102;&#40065;&#26834;&#26234;&#33021;&#20307;&#31574;&#30053;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#26377;&#38480;&#29366;&#24577;&#21644;&#26377;&#38480;&#21160;&#20316;&#24773;&#20917;&#19979;&#30340;&#23384;&#22312;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#23545;&#25239;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#40065;&#26834;&#24615;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.02705</link><description>&lt;p&gt;
&#24590;&#26679;&#35299;&#20915;&#38754;&#23545;&#23545;&#25239;&#29366;&#24577;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
What is the Solution for State-Adversarial Multi-Agent Reinforcement Learning?. (arXiv:2212.02705v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02705
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#20915;&#38754;&#23545;&#23545;&#25239;&#24615;&#29366;&#24577;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#29366;&#24577;&#23545;&#25239;&#24615;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;&#65292;&#25552;&#20986;&#20102;&#40065;&#26834;&#26234;&#33021;&#20307;&#31574;&#30053;&#30340;&#27010;&#24565;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#22312;&#26377;&#38480;&#29366;&#24577;&#21644;&#26377;&#38480;&#21160;&#20316;&#24773;&#20917;&#19979;&#30340;&#23384;&#22312;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#23545;&#25239;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#40065;&#26834;&#24615;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#20013;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#26041;&#27861;&#65292;&#20551;&#35774;&#26234;&#33021;&#20307;&#30340;&#31574;&#30053;&#22522;&#20110;&#20934;&#30830;&#30340;&#29366;&#24577;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#23398;&#20064;&#30340;&#31574;&#30053;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#29366;&#24577;&#25200;&#21160;&#25915;&#20987;&#30340;&#24433;&#21709;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29366;&#24577;&#23545;&#25239;&#24615;&#39532;&#23572;&#31185;&#22827;&#21338;&#24328;(SAMG)&#65292;&#24182;&#39318;&#27425;&#23581;&#35797;&#30740;&#31350;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#19979;MARL&#30340;&#22522;&#26412;&#23646;&#24615;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22312;SAMG&#20013;&#65292;&#36890;&#24120;&#20351;&#29992;&#30340;&#26368;&#20248;&#26234;&#33021;&#20307;&#31574;&#30053;&#21644;&#40065;&#26834;&#32435;&#20160;&#22343;&#34913;&#35299;&#20915;&#27010;&#24565;&#24182;&#19981;&#24635;&#26159;&#23384;&#22312;&#30340;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#22256;&#38590;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#31181;&#31216;&#20026;&#40065;&#26834;&#26234;&#33021;&#20307;&#31574;&#30053;&#30340;&#26032;&#35299;&#20915;&#27010;&#24565;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#30340;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#26368;&#22351;&#24773;&#20917;&#19979;&#30340;&#39044;&#26399;&#29366;&#24577;&#20540;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#26377;&#38480;&#29366;&#24577;&#21644;&#26377;&#38480;&#21160;&#20316;SAMG&#20013;&#23384;&#22312;&#40065;&#26834;&#26234;&#33021;&#20307;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#40065;&#26834;&#22810;&#26234;&#33021;&#20307;&#23545;&#25239;&#24615;&#28436;&#21592;-&#35780;&#35770;&#23478;(RMA3C)&#31639;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#22312;&#29366;&#24577;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;MARL&#26234;&#33021;&#20307;&#30340;&#40065;&#26834;&#24615;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Various methods for Multi-Agent Reinforcement Learning (MARL) have been developed with the assumption that agents' policies are based on accurate state information. However, policies learned through Deep Reinforcement Learning (DRL) are susceptible to adversarial state perturbation attacks. In this work, we propose a State-Adversarial Markov Game (SAMG) and make the first attempt to investigate the fundamental properties of MARL under state uncertainties. Our analysis shows that the commonly used solution concepts of optimal agent policy and robust Nash equilibrium do not always exist in SAMGs. To circumvent this difficulty, we consider a new solution concept called robust agent policy, where agents aim to maximize the worst-case expected state value. We prove the existence of robust agent policy for finite state and finite action SAMGs. Additionally, we propose a Robust Multi-Agent Adversarial Actor-Critic (RMA3C) algorithm to learn robust policies for MARL agents under state uncertai
&lt;/p&gt;</description></item></channel></rss>