<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;</title><link>https://rss.arxiv.org/abs/2402.01295</link><description>&lt;p&gt;
ExtremeCast: &#25552;&#21319;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#30340;&#26497;&#20540;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01295
&lt;/p&gt;
&lt;p&gt;
ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#25253;&#22312;&#20840;&#29699;&#20013;&#26399;&#39044;&#25253;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#39044;&#27979;&#26497;&#31471;&#22825;&#27668;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26497;&#31471;&#20540;&#39044;&#27979;&#19982;&#27492;&#23494;&#20999;&#30456;&#20851;&#12290;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#23545;&#31216;&#25439;&#22833;&#65292;&#22914;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#20250;&#23548;&#33268;&#39044;&#27979;&#26377;&#20559;&#24046;&#24182;&#20302;&#20272;&#26497;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Exloss&#65292;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#20248;&#21270;&#31361;&#20986;&#26497;&#20540;&#65292;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#26497;&#31471;&#22825;&#27668;&#39044;&#25253;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#23427;&#22686;&#21152;&#20102;&#20687;&#32032;&#20540;&#30340;&#26041;&#24046;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;&#12290;&#32467;&#21512;&#20808;&#36827;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-driven weather forecast based on machine learning (ML) has experienced rapid development and demonstrated superior performance in the global medium-range forecast compared to traditional physics-based dynamical models. However, most of these ML models struggle with accurately predicting extreme weather, which is closely related to the extreme value prediction. Through mathematical analysis, we prove that the use of symmetric losses, such as the Mean Squared Error (MSE), leads to biased predictions and underestimation of extreme values. To address this issue, we introduce Exloss, a novel loss function that performs asymmetric optimization and highlights extreme values to obtain accurate extreme weather forecast. Furthermore, we introduce a training-free extreme value enhancement strategy named ExEnsemble, which increases the variance of pixel values and improves the forecast robustness. Combined with an advanced global weather forecast model, extensive experiments show that our sol
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#25955;&#24335;&#28608;&#21169;&#26426;&#21046;&#65292;&#26088;&#22312;&#22312;&#36710;&#32852;&#32593;&#29615;&#22659;&#20013;&#20026;&#31227;&#21160;AIGC&#26381;&#21153;&#20998;&#37197;&#30340;&#20379;&#38656;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2403.20151</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#25955;&#24335;&#36710;&#32852;&#32593;&#31227;&#21160;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#26381;&#21153;&#28608;&#21169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
A Learning-based Incentive Mechanism for Mobile AIGC Service in Decentralized Internet of Vehicles
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.20151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#25955;&#24335;&#28608;&#21169;&#26426;&#21046;&#65292;&#26088;&#22312;&#22312;&#36710;&#32852;&#32593;&#29615;&#22659;&#20013;&#20026;&#31227;&#21160;AIGC&#26381;&#21153;&#20998;&#37197;&#30340;&#20379;&#38656;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20151v1 &#20844;&#21578;&#31867;&#22411;: &#26032;&#30340; &#25688;&#35201;: &#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#20869;&#23481;&#65288;AIGC&#65289;&#25351;&#30340;&#26159;&#21033;&#29992;AI&#27169;&#22411;&#36827;&#34892;&#33258;&#21160;&#21270;&#20869;&#23481;&#29983;&#25104;&#30340;&#33539;&#24335;&#12290;&#36710;&#32852;&#32593;&#65288;IoV&#65289;&#32593;&#32476;&#20013;&#30340;&#31227;&#21160;AIGC&#26381;&#21153;&#27604;&#20256;&#32479;&#22522;&#20110;&#20113;&#30340;AIGC&#26381;&#21153;&#20855;&#26377;&#35832;&#22810;&#20248;&#21183;&#65292;&#21253;&#25324;&#22686;&#24378;&#30340;&#32593;&#32476;&#25928;&#29575;&#12289;&#26356;&#22909;&#30340;&#21487;&#37325;&#26500;&#24615;&#65292;&#20197;&#21450;&#26356;&#24378;&#30340;&#25968;&#25454;&#23433;&#20840;&#21644;&#38544;&#31169;&#24615;&#12290;&#28982;&#32780;&#65292;AIGC&#26381;&#21153;&#25552;&#20379;&#32463;&#24120;&#38656;&#35201;&#22823;&#37327;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#36164;&#28304;&#21463;&#38480;&#30340;&#36335;&#36793;&#21333;&#20803;&#65288;RSUs&#65289;&#38754;&#20020;&#30528;&#22312;&#19981;&#38477;&#20302;&#25972;&#20307;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#32500;&#25252;&#22810;&#26679;&#21270;AIGC&#26381;&#21153;&#27744;&#24182;&#28385;&#36275;&#25152;&#26377;&#29992;&#25143;&#26381;&#21153;&#35831;&#27714;&#30340;&#25361;&#25112;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#31227;&#21160;AIGC&#26381;&#21153;&#20998;&#37197;&#30340;&#20998;&#25955;&#28608;&#21169;&#26426;&#21046;&#65292;&#21033;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#25214;&#21040;RSUs&#19978;AIGC&#26381;&#21153;&#20379;&#24212;&#21644;IoV&#29615;&#22659;&#20013;&#29992;&#25143;&#26381;&#21153;&#38656;&#27714;&#20043;&#38388;&#30340;&#24179;&#34913;&#65292;&#20248;&#21270;&#29992;&#25143;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.20151v1 Announce Type: new  Abstract: Artificial Intelligence-Generated Content (AIGC) refers to the paradigm of automated content generation utilizing AI models. Mobile AIGC services in the Internet of Vehicles (IoV) network have numerous advantages over traditional cloud-based AIGC services, including enhanced network efficiency, better reconfigurability, and stronger data security and privacy. Nonetheless, AIGC service provisioning frequently demands significant resources. Consequently, resource-constrained roadside units (RSUs) face challenges in maintaining a heterogeneous pool of AIGC services and addressing all user service requests without degrading overall performance. Therefore, in this paper, we propose a decentralized incentive mechanism for mobile AIGC service allocation, employing multi-agent deep reinforcement learning to find the balance between the supply of AIGC services on RSUs and user demand for services within the IoV context, optimizing user experience
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#36731;&#37327;&#32423;&#12289;&#22810;&#24103;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; EM-VLM4AD&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#35270;&#35273;&#38382;&#31572;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#20869;&#23384;&#21644;&#28014;&#28857;&#36816;&#31639;&#38656;&#27714;&#33267;&#23569;&#20943;&#23569;&#21313;&#20493;&#65292;&#24182;&#19988;&#22312;BLEU-4&#12289;METEOR&#12289;CIDEr&#21644;ROGUE&#20998;&#25968;&#19978;&#22343;&#21462;&#24471;&#26356;&#39640;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.19838</link><description>&lt;p&gt;
&#22810;&#24103;&#12289;&#36731;&#37327;&#32423;&#21644;&#39640;&#25928;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Multi-Frame, Lightweight &amp; Efficient Vision-Language Models for Question Answering in Autonomous Driving
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.19838
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#12289;&#36731;&#37327;&#32423;&#12289;&#22810;&#24103;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411; EM-VLM4AD&#65292;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#30340;&#35270;&#35273;&#38382;&#31572;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#65292;&#20869;&#23384;&#21644;&#28014;&#28857;&#36816;&#31639;&#38656;&#27714;&#33267;&#23569;&#20943;&#23569;&#21313;&#20493;&#65292;&#24182;&#19988;&#22312;BLEU-4&#12289;METEOR&#12289;CIDEr&#21644;ROGUE&#20998;&#25968;&#19978;&#22343;&#21462;&#24471;&#26356;&#39640;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#21644;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;MMLMs&#65289;&#24050;&#32463;&#22312;&#33258;&#21160;&#39550;&#39542;&#30740;&#31350;&#20013;&#21464;&#24471;&#31361;&#20986;&#65292;&#22240;&#20026;&#36825;&#20123;&#27169;&#22411;&#21487;&#20197;&#21033;&#29992;&#20132;&#36890;&#22330;&#26223;&#22270;&#20687;&#21644;&#20854;&#20182;&#25968;&#25454;&#27169;&#24577;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#25991;&#26412;&#25512;&#29702;&#21644;&#21709;&#24212;&#65292;&#29992;&#20110;&#31471;&#21040;&#31471;&#33258;&#21160;&#39550;&#39542;&#23433;&#20840;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#38024;&#23545;&#36825;&#20123;&#31995;&#32479;&#30340;&#26041;&#27861;&#20351;&#29992;&#26114;&#36149;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39592;&#24178;&#21644;&#22270;&#20687;&#32534;&#30721;&#22120;&#65292;&#20351;&#24471;&#36825;&#20123;&#31995;&#32479;&#19981;&#36866;&#21512;&#20855;&#26377;&#20005;&#26684;&#20869;&#23384;&#38480;&#21046;&#21644;&#38656;&#35201;&#24555;&#36895;&#25512;&#29702;&#26102;&#38388;&#30340;&#23454;&#26102;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#20808;&#21069;&#38382;&#39064;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;EM-VLM4AD&#65292;&#19968;&#31181;&#39640;&#25928;&#12289;&#36731;&#37327;&#32423;&#12289;&#22810;&#24103;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#25191;&#34892;&#33258;&#21160;&#39550;&#39542;&#30340;&#35270;&#35273;&#38382;&#31572;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.19838v1 Announce Type: cross  Abstract: Vision-Language Models (VLMs) and Multi-Modal Language models (MMLMs) have become prominent in autonomous driving research, as these models can provide interpretable textual reasoning and responses for end-to-end autonomous driving safety tasks using traffic scene images and other data modalities. However, current approaches to these systems use expensive large language model (LLM) backbones and image encoders, making such systems unsuitable for real-time autonomous driving systems where tight memory constraints exist and fast inference time is necessary. To address these previous issues, we develop EM-VLM4AD, an efficient, lightweight, multi-frame vision language model which performs Visual Question Answering for autonomous driving. In comparison to previous approaches, EM-VLM4AD requires at least 10 times less memory and floating point operations, while also achieving higher BLEU-4, METEOR, CIDEr, and ROGUE scores than the existing b
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#23436;&#20840;&#20108;&#36827;&#21046;&#28857;&#20113;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21387;&#32553;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#20026;1&#20301;&#20108;&#36827;&#21046;&#20540;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#28857;&#20113;&#22788;&#29702;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23384;&#20648;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.09998</link><description>&lt;p&gt;
FBPT&#65306;&#19968;&#20010;&#23436;&#20840;&#20108;&#36827;&#21046;&#28857;&#20113;Transformer
&lt;/p&gt;
&lt;p&gt;
FBPT: A Fully Binary Point Transformer
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09998
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#23436;&#20840;&#20108;&#36827;&#21046;&#28857;&#20113;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#21387;&#32553;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#20026;1&#20301;&#20108;&#36827;&#21046;&#20540;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#28857;&#20113;&#22788;&#29702;&#20219;&#21153;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23384;&#20648;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Fully Binary Point Cloud Transformer&#65288;FBPT&#65289;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#22312;&#26426;&#22120;&#20154;&#21644;&#31227;&#21160;&#35774;&#22791;&#39046;&#22495;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21644;&#25193;&#23637;&#28508;&#21147;&#12290;&#36890;&#36807;&#23558;32&#20301;&#20840;&#31934;&#24230;&#32593;&#32476;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#21387;&#32553;&#20026;1&#20301;&#20108;&#36827;&#21046;&#20540;&#65292;&#25152;&#25552;&#20986;&#30340;&#20108;&#36827;&#21046;&#28857;&#20113;Transformer&#32593;&#32476;&#26174;&#33879;&#38477;&#20302;&#20102;&#29992;&#20110;&#28857;&#20113;&#22788;&#29702;&#20219;&#21153;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#23384;&#20648;&#21344;&#29992;&#21644;&#35745;&#31639;&#36164;&#28304;&#38656;&#27714;&#65292;&#30456;&#36739;&#20110;&#20840;&#31934;&#24230;&#28857;&#20113;&#32593;&#32476;&#12290;&#28982;&#32780;&#65292;&#23454;&#29616;&#23436;&#20840;&#30340;&#20108;&#36827;&#21046;&#28857;&#20113;Transformer&#32593;&#32476;&#65292;&#20854;&#20013;&#38500;&#20102;&#19982;&#20219;&#21153;&#29305;&#23450;&#30340;&#27169;&#22359;&#22806;&#20854;&#20182;&#25152;&#26377;&#37096;&#20998;&#37117;&#26159;&#20108;&#36827;&#21046;&#30340;&#65292;&#20250;&#22312;&#37327;&#21270;&#27880;&#24847;&#21147;&#27169;&#22359;&#20013;&#30340;Q&#12289;K&#12289;V&#21644;&#33258;&#27880;&#24847;&#21147;&#28608;&#27963;&#26102;&#38754;&#20020;&#25361;&#25112;&#21644;&#29942;&#39048;&#65292;&#22240;&#20026;&#23427;&#20204;&#19981;&#31526;&#21512;&#31616;&#21333;&#30340;&#27010;&#29575;&#20998;&#24067;&#65292;&#24182;&#19988;&#21487;&#33021;&#38543;&#36755;&#20837;&#25968;&#25454;&#21464;&#21270;&#32780;&#21464;&#21270;&#12290;&#27492;&#22806;&#65292;&#22312;&#25105;&#20204;&#30340;&#32593;&#32476;&#20013;&#65292;&#20108;&#36827;&#21046;&#27880;&#24847;&#21147;&#27169;&#22359;&#32463;&#21382;&#20102;&#34928;&#20943;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09998v1 Announce Type: cross  Abstract: This paper presents a novel Fully Binary Point Cloud Transformer (FBPT) model which has the potential to be widely applied and expanded in the fields of robotics and mobile devices. By compressing the weights and activations of a 32-bit full-precision network to 1-bit binary values, the proposed binary point cloud Transformer network significantly reduces the storage footprint and computational resource requirements of neural network models for point cloud processing tasks, compared to full-precision point cloud networks. However, achieving a fully binary point cloud Transformer network, where all parts except the modules specific to the task are binary, poses challenges and bottlenecks in quantizing the activations of Q, K, V and self-attention in the attention module, as they do not adhere to simple probability distributions and can vary with input data. Furthermore, in our network, the binary attention module undergoes a degradation
&lt;/p&gt;</description></item><item><title>Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#65292;&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#24182;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#23637;&#29616;&#20986;&#23454;&#20363;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#65292;&#20026;&#23558;&#26469;&#30740;&#31350;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.03835</link><description>&lt;p&gt;
Cobweb&#65306;&#19968;&#31181;&#22686;&#37327;&#21644;&#20998;&#23618;&#24335;&#30340;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Cobweb: An Incremental and Hierarchical Model of Human-Like Category Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03835
&lt;/p&gt;
&lt;p&gt;
Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#37319;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#65292;&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#24182;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#23637;&#29616;&#20986;&#23454;&#20363;&#21644;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#65292;&#20026;&#23558;&#26469;&#30740;&#31350;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Cobweb&#26159;&#19968;&#31181;&#31867;&#20284;&#20154;&#31867;&#30340;&#31867;&#21035;&#23398;&#20064;&#31995;&#32479;&#65292;&#19982;&#20854;&#20182;&#22686;&#37327;&#20998;&#31867;&#27169;&#22411;&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#21033;&#29992;&#31867;&#21035;&#25928;&#29992;&#24230;&#37327;&#26500;&#24314;&#20998;&#23618;&#32452;&#32455;&#30340;&#31867;&#20284;&#26641;&#29366;&#32467;&#26500;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;Cobweb&#33021;&#22815;&#25429;&#25417;&#24515;&#29702;&#25928;&#24212;&#65292;&#22914;&#22522;&#26412;&#27700;&#24179;&#12289;&#20856;&#22411;&#24615;&#21644;&#25159;&#24418;&#25928;&#24212;&#12290;&#28982;&#32780;&#65292;&#23545;Cobweb&#20316;&#20026;&#20154;&#31867;&#20998;&#31867;&#27169;&#22411;&#30340;&#26356;&#24191;&#27867;&#35780;&#20272;&#20173;&#28982;&#32570;&#20047;&#12290;&#26412;&#30740;&#31350;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#30333;&#12290;&#23427;&#30830;&#23450;&#20102;Cobweb&#19982;&#32463;&#20856;&#30340;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#25928;&#24212;&#30340;&#19968;&#33268;&#24615;&#12290;&#36824;&#25506;&#35752;&#20102;Cobweb&#23637;&#29616;&#20986;&#22312;&#21333;&#19968;&#27169;&#22411;&#20013;&#26082;&#26377;&#23454;&#20363;&#21448;&#26377;&#21407;&#22411;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#23558;&#26469;&#30740;&#31350;Cobweb&#20316;&#20026;&#20154;&#31867;&#31867;&#21035;&#23398;&#20064;&#30340;&#32508;&#21512;&#27169;&#22411;&#22880;&#23450;&#20102;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03835v1 Announce Type: cross  Abstract: Cobweb, a human like category learning system, differs from other incremental categorization models in constructing hierarchically organized cognitive tree-like structures using the category utility measure. Prior studies have shown that Cobweb can capture psychological effects such as the basic level, typicality, and fan effects. However, a broader evaluation of Cobweb as a model of human categorization remains lacking. The current study addresses this gap. It establishes Cobweb's alignment with classical human category learning effects. It also explores Cobweb's flexibility to exhibit both exemplar and prototype like learning within a single model. These findings set the stage for future research on Cobweb as a comprehensive model of human category learning.
&lt;/p&gt;</description></item><item><title>PaperWeaver&#36890;&#36807;&#23558;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#19982;&#25512;&#33616;&#35770;&#25991;&#19978;&#19979;&#25991;&#21270;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#20027;&#39064;&#35770;&#25991;&#25552;&#37266;</title><link>https://arxiv.org/abs/2403.02939</link><description>&lt;p&gt;
PaperWeaver&#65306;&#36890;&#36807;&#23558;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#19982;&#25512;&#33616;&#35770;&#25991;&#19978;&#19979;&#25991;&#21270;&#65292;&#20016;&#23500;&#20027;&#39064;&#35770;&#25991;&#25552;&#37266;
&lt;/p&gt;
&lt;p&gt;
PaperWeaver: Enriching Topical Paper Alerts by Contextualizing Recommended Papers with User-collected Papers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.02939
&lt;/p&gt;
&lt;p&gt;
PaperWeaver&#36890;&#36807;&#23558;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#19982;&#25512;&#33616;&#35770;&#25991;&#19978;&#19979;&#25991;&#21270;&#65292;&#20026;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#20102;&#26356;&#20016;&#23500;&#30340;&#20027;&#39064;&#35770;&#25991;&#25552;&#37266;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#23398;&#26415;&#26723;&#26696;&#30340;&#36805;&#36895;&#22686;&#38271;&#65292;&#30740;&#31350;&#20154;&#21592;&#35746;&#38405;&#8220;&#35770;&#25991;&#25552;&#37266;&#8221;&#31995;&#32479;&#65292;&#23450;&#26399;&#20026;&#20182;&#20204;&#25512;&#33616;&#26368;&#36817;&#21457;&#34920;&#30340;&#19982;&#20043;&#21069;&#25910;&#38598;&#30340;&#35770;&#25991;&#30456;&#20284;&#30340;&#35770;&#25991;&#12290;&#28982;&#32780;&#65292;&#30740;&#31350;&#20154;&#21592;&#26377;&#26102;&#24456;&#38590;&#29702;&#35299;&#25512;&#33616;&#35770;&#25991;&#19982;&#20182;&#20204;&#33258;&#24049;&#30740;&#31350;&#32972;&#26223;&#20043;&#38388;&#24494;&#22937;&#30340;&#32852;&#31995;&#65292;&#22240;&#20026;&#29616;&#26377;&#31995;&#32479;&#21482;&#21576;&#29616;&#35770;&#25991;&#26631;&#39064;&#21644;&#25688;&#35201;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21457;&#29616;&#36825;&#20123;&#32852;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PaperWeaver&#65292;&#36825;&#26159;&#19968;&#20010;&#20016;&#23500;&#30340;&#35770;&#25991;&#25552;&#37266;&#31995;&#32479;&#65292;&#26681;&#25454;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#25552;&#20379;&#25512;&#33616;&#35770;&#25991;&#30340;&#19978;&#19979;&#25991;&#21270;&#25991;&#26412;&#25551;&#36848;&#12290;PaperWeaver&#37319;&#29992;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#20174;&#29992;&#25143;&#25910;&#38598;&#30340;&#35770;&#25991;&#20013;&#25512;&#26029;&#29992;&#25143;&#30340;&#30740;&#31350;&#20852;&#36259;&#65292;&#25552;&#21462;&#35770;&#25991;&#30340;&#29305;&#23450;&#32972;&#26223;&#65292;&#24182;&#22312;&#36825;&#20123;&#32972;&#26223;&#19978;&#27604;&#36739;&#25512;&#33616;&#35770;&#25991;&#21644;&#25910;&#38598;&#30340;&#35770;&#25991;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#65288;N=15&#65289;&#34920;&#26126;&#65292;&#20351;&#29992;PaperWeaver&#30340;&#21442;&#19982;&#32773;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.02939v1 Announce Type: cross  Abstract: With the rapid growth of scholarly archives, researchers subscribe to "paper alert" systems that periodically provide them with recommendations of recently published papers that are similar to previously collected papers. However, researchers sometimes struggle to make sense of nuanced connections between recommended papers and their own research context, as existing systems only present paper titles and abstracts. To help researchers spot these connections, we present PaperWeaver, an enriched paper alerts system that provides contextualized text descriptions of recommended papers based on user-collected papers. PaperWeaver employs a computational method based on Large Language Models (LLMs) to infer users' research interests from their collected papers, extract context-specific aspects of papers, and compare recommended and collected papers on these aspects. Our user study (N=15) showed that participants using PaperWeaver were able to
&lt;/p&gt;</description></item><item><title>DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;</title><link>https://arxiv.org/abs/2403.01954</link><description>&lt;p&gt;
DECIDERS&#65306;&#19968;&#31181;&#36890;&#36807;&#27169;&#20223;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#23454;&#29616;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#30340;&#35821;&#35328;&#29983;&#25104;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
DECIDER: A Rule-Controllable Decoding Strategy for Language Generation by Imitating Dual-System Cognitive Theory
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01954
&lt;/p&gt;
&lt;p&gt;
DECIDER&#26159;&#19968;&#31181;&#21463;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#35268;&#21017;&#20197;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35789;&#20856;&#32422;&#26463;&#35299;&#30721;&#26041;&#27861;&#26088;&#22312;&#36890;&#36807;&#26576;&#20123;&#30446;&#26631;&#27010;&#24565;&#25511;&#21046;&#25152;&#29983;&#25104;&#25991;&#26412;&#30340;&#24847;&#20041;&#25110;&#39118;&#26684;&#12290;&#29616;&#26377;&#26041;&#27861;&#36807;&#20110;&#20851;&#27880;&#36825;&#20123;&#30446;&#26631;&#26412;&#36523;&#65292;&#23548;&#33268;&#32570;&#20047;&#20851;&#20110;&#22914;&#20309;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#30340;&#39640;&#23618;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#36890;&#24120;&#36890;&#36807;&#36981;&#24490;&#26576;&#20123;&#35268;&#21017;&#26469;&#22788;&#29702;&#20219;&#21153;&#65292;&#36825;&#20123;&#35268;&#21017;&#19981;&#20165;&#20851;&#27880;&#20110;&#30446;&#26631;&#26412;&#36523;&#65292;&#36824;&#20851;&#27880;&#20110;&#24341;&#21457;&#30446;&#26631;&#21457;&#29983;&#30340;&#35821;&#20041;&#30456;&#20851;&#27010;&#24565;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DECIDER&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#31995;&#32479;&#35748;&#30693;&#29702;&#35770;&#21551;&#21457;&#30340;&#32422;&#26463;&#35821;&#35328;&#29983;&#25104;&#30340;&#35268;&#21017;&#21487;&#25511;&#35299;&#30721;&#31574;&#30053;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;DECIDER&#20013;&#65292;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#37197;&#22791;&#20102;&#19968;&#20010;&#36923;&#36753;&#25512;&#29702;&#22120;&#65292;&#20197;&#39640;&#23618;&#35268;&#21017;&#20316;&#20026;&#36755;&#20837;&#12290;&#28982;&#21518;&#65292;DECIDER&#20801;&#35768;&#35268;&#21017;&#20449;&#21495;&#22312;&#27599;&#20010;&#35299;&#30721;&#27493;&#39588;&#20013;&#27969;&#20837;PLM&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DECIDER&#33021;&#22815;&#26377;&#25928;&#22320;&#36981;&#24490;&#32473;&#23450;&#30340;&#35268;&#21017;&#65292;&#24341;&#23548;&#29983;&#25104;&#26041;&#21521;&#26397;&#21521;&#30446;&#26631;&#36827;&#34892;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01954v1 Announce Type: cross  Abstract: Lexicon-based constrained decoding approaches aim to control the meaning or style of the generated text through certain target concepts. Existing approaches over-focus the targets themselves, leading to a lack of high-level reasoning about how to achieve them. However, human usually tackles tasks by following certain rules that not only focuses on the targets but also on semantically relevant concepts that induce the occurrence of targets. In this work, we present DECIDER, a rule-controllable decoding strategy for constrained language generation inspired by dual-system cognitive theory. Specifically, in DECIDER, a pre-trained language model (PLM) is equiped with a logic reasoner that takes high-level rules as input. Then, the DECIDER allows rule signals to flow into the PLM at each decoding step. Extensive experimental results demonstrate that DECIDER can effectively follow given rules to guide generation direction toward the targets i
&lt;/p&gt;</description></item><item><title>ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.18609</link><description>&lt;p&gt;
ICE-SEARCH: &#19968;&#31181;&#22522;&#20110;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#29305;&#24449;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH: A Language Model-Driven Feature Selection Approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.18609
&lt;/p&gt;
&lt;p&gt;
ICE-SEARCH&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;&#20219;&#21153;&#30340;&#26041;&#27861;&#65292;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;State-of-the-Art(SOTA)&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25581;&#31034;&#20102;In-Context Evolutionary Search (ICE-SEARCH)&#26041;&#27861;&#65292;&#36825;&#26159;&#39318;&#20010;&#23558;&#35821;&#35328;&#27169;&#22411;(LMs)&#19982;&#36827;&#21270;&#31639;&#27861;&#30456;&#32467;&#21512;&#29992;&#20110;&#29305;&#24449;&#36873;&#25321;(FS)&#20219;&#21153;&#30340;&#24037;&#20316;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#22312;&#21307;&#23398;&#39044;&#27979;&#20998;&#26512;(MPA)&#24212;&#29992;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;ICE-SEARCH&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#20013;&#22266;&#26377;&#30340;&#20132;&#21449;&#21644;&#31361;&#21464;&#33021;&#21147;&#65292;&#22312;&#19968;&#20010;&#36827;&#21270;&#26694;&#26550;&#20869;&#26174;&#30528;&#25913;&#36827;&#29305;&#24449;&#36873;&#25321;&#65292;&#36890;&#36807;&#27169;&#22411;&#30340;&#20840;&#38754;&#19990;&#30028;&#30693;&#35782;&#21644;&#20854;&#36866;&#24212;&#21508;&#31181;&#35282;&#33394;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23545;&#35813;&#26041;&#27861;&#30340;&#35780;&#20272;&#28085;&#30422;&#20102;&#19977;&#20010;&#20851;&#38190;&#30340;MPA&#20219;&#21153;&#65306;&#20013;&#39118;&#12289;&#24515;&#34880;&#31649;&#30142;&#30149;&#21644;&#31958;&#23615;&#30149;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;ICE-SEARCH&#22312;&#30830;&#23450;&#21307;&#23398;&#24212;&#29992;&#30340;&#20851;&#38190;&#29305;&#24449;&#26041;&#38754;&#20248;&#20110;&#20256;&#32479;&#30340;FS&#26041;&#27861;&#12290;ICE-SEARCH&#22312;&#20013;&#39118;&#39044;&#27979;&#21644;&#31958;&#23615;&#30149;&#39044;&#27979;&#20013;&#23454;&#29616;&#20102;&#39046;&#20808;&#27700;&#24179;&#65307;&#20915;&#31574;&#38543;&#26426;&#21270;ICE-SEARCH&#22312;&#24515;&#34880;&#31649;&#30142;&#30149;&#39044;&#27979;&#20013;&#25490;&#21517;&#20026;&#39046;&#20808;&#27700;&#24179;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#19981;&#20165;&#35777;&#26126;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.18609v1 Announce Type: cross  Abstract: This study unveils the In-Context Evolutionary Search (ICE-SEARCH) method, the first work that melds language models (LMs) with evolutionary algorithms for feature selection (FS) tasks and demonstrates its effectiveness in Medical Predictive Analytics (MPA) applications. ICE-SEARCH harnesses the crossover and mutation capabilities inherent in LMs within an evolutionary framework, significantly improving FS through the model's comprehensive world knowledge and its adaptability to a variety of roles. Our evaluation of this methodology spans three crucial MPA tasks: stroke, cardiovascular disease, and diabetes, where ICE-SEARCH outperforms traditional FS methods in pinpointing essential features for medical applications. ICE-SEARCH achieves State-of-the-Art (SOTA) performance in stroke prediction and diabetes prediction; the Decision-Randomized ICE-SEARCH ranks as SOTA in cardiovascular disease prediction. Our results not only demonstrate
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2402.11472</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#65306;DDIPrompt
&lt;/p&gt;
&lt;p&gt;
DDIPrompt: Drug-Drug Interaction Event Prediction based on Graph Prompt Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11472
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#25552;&#31034;&#23398;&#20064;&#30340;DDIPrompt&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#39044;&#27979;&#20013;&#30340;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#21644;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#31232;&#32570;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#20854;&#22312;&#24314;&#27169;&#33647;&#29289;&#20998;&#23376;&#20869;&#37096;&#21644;&#20043;&#38388;&#21407;&#23376;&#21644;&#21151;&#33021;&#22242;&#20043;&#38388;&#22797;&#26434;&#20851;&#32852;&#26041;&#38754;&#30340;&#29087;&#32451;&#34920;&#29616;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#39044;&#27979;&#33647;&#29289;&#30456;&#20114;&#20316;&#29992;&#20107;&#20214;&#65288;DDI&#65289;&#26041;&#38754;&#21464;&#24471;&#26085;&#30410;&#26222;&#36941;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20173;&#28982;&#21463;&#21040;&#20004;&#20010;&#37325;&#22823;&#25361;&#25112;&#30340;&#21046;&#32422;&#65306;&#65288;1&#65289;&#39640;&#24230;&#19981;&#24179;&#34913;&#20107;&#20214;&#20998;&#24067;&#30340;&#38382;&#39064;&#65292;&#22312;&#21307;&#23398;&#25968;&#25454;&#38598;&#20013;&#36825;&#26159;&#19968;&#20010;&#24120;&#35265;&#20294;&#20851;&#38190;&#30340;&#38382;&#39064;&#65292;&#26576;&#20123;&#30456;&#20114;&#20316;&#29992;&#34987;&#24191;&#27867;&#22320;&#20302;&#20272;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#23545;&#23454;&#29616;&#20934;&#30830;&#21487;&#38752;&#30340;DDI&#39044;&#27979;&#26500;&#25104;&#20102;&#37325;&#22823;&#38556;&#30861;&#12290;&#65288;2&#65289;&#32597;&#35265;&#20107;&#20214;&#26631;&#35760;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#65292;&#22312;&#21307;&#23398;&#39046;&#22495;&#26159;&#19968;&#20010;&#26222;&#36941;&#38382;&#39064;&#65292;&#30001;&#20110;&#25968;&#25454;&#26377;&#38480;&#65292;&#24448;&#24448;&#24573;&#35270;&#25110;&#30740;&#31350;&#19981;&#36275;&#30340;&#32597;&#35265;&#20294;&#28508;&#22312;&#20851;&#38190;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;DDIPrompt&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#26368;&#36817;&#22270;&#25552;&#31034;&#23398;&#36827;&#23637;&#21551;&#21457;&#30340;&#21019;&#26032;&#33391;&#26041;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11472v1 Announce Type: cross  Abstract: Recently, Graph Neural Networks have become increasingly prevalent in predicting adverse drug-drug interactions (DDI) due to their proficiency in modeling the intricate associations between atoms and functional groups within and across drug molecules. However, they are still hindered by two significant challenges: (1) the issue of highly imbalanced event distribution, which is a common but critical problem in medical datasets where certain interactions are vastly underrepresented. This imbalance poses a substantial barrier to achieving accurate and reliable DDI predictions. (2) the scarcity of labeled data for rare events, which is a pervasive issue in the medical field where rare yet potentially critical interactions are often overlooked or under-studied due to limited available data. In response, we offer DDIPrompt, an innovative panacea inspired by the recent advancements in graph prompting. Our framework aims to address these issue
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;</title><link>https://arxiv.org/abs/2402.07818</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Zeroth-Order Methods for Scalable Large Language Model Finetuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.07818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24046;&#20998;&#38544;&#31169;&#38646;&#38454;&#26041;&#27861;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#20013;&#30340;&#24212;&#29992;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#20351;&#29992;&#38646;&#38454;&#26799;&#24230;&#26469;&#36991;&#20813;&#20256;&#32479;&#20248;&#21270;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#65292;&#23454;&#29616;&#20102;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#30340;&#33391;&#22909;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29305;&#23450;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24494;&#35843;&#26159;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#24378;&#22823;&#33021;&#21147;&#36827;&#34892;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#30340;&#24191;&#27867;&#25509;&#21463;&#30340;&#33539;&#20363;&#12290;&#30001;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#30340;&#26222;&#21450;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20197;&#20445;&#25252;&#29305;&#23450;&#20219;&#21153;&#25968;&#25454;&#38598;&#30340;&#38544;&#31169;&#12290;&#24046;&#20998;&#38544;&#31169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#24494;&#35843;&#26041;&#27861;&#30340;&#35774;&#35745;&#26680;&#24515;&#26159;&#22312;&#38544;&#31169;&#12289;&#25928;&#29992;&#21644;&#21487;&#25193;&#23637;&#24615;&#20043;&#38388;&#36798;&#21040;&#28385;&#24847;&#30340;&#26435;&#34913;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#37117;&#26159;&#22522;&#20110;DP-SGD&#30340;&#21019;&#26032;&#24615;&#24037;&#20316;&#12290;&#23613;&#31649;&#23558;DP-SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#25512;&#21040;&#20102;&#26497;&#38480;&#65292;&#20294;&#22522;&#20110;DP-SGD&#30340;&#24494;&#35843;&#26041;&#27861;&#19981;&#24184;&#22320;&#21463;&#21040;&#20102;SGD&#22266;&#26377;&#20302;&#25928;&#29575;&#30340;&#38480;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;DP&#38646;&#38454;&#26041;&#27861;&#22312;LLM&#39044;&#35757;&#32451;&#20013;&#30340;&#28508;&#21147;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#29992;&#26356;&#39640;&#25928;&#30340;&#38646;&#38454;&#26799;&#24230;&#26469;&#36817;&#20284;&#26799;&#24230;&#65292;&#36991;&#20813;&#20102;SGD&#30340;&#21487;&#25193;&#23637;&#24615;&#29942;&#39048;&#12290;&#19982;&#23558;&#38646;&#38454;&#26041;&#27861;&#20316;&#20026;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#19981;&#21516;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#21106;&#25509;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#33021;&#22815;&#20197;&#38750;&#24120;&#25509;&#36817;&#30340;&#26041;&#24335;&#27169;&#25311;DP-SGD&#30340;&#22522;&#26412;&#25805;&#20316;&#65292;&#28982;&#21518;&#21033;&#29992;&#38646;&#38454;&#20248;&#21270;&#26041;&#27861;&#26469;&#36817;&#20284;&#26799;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finetuning on task-specific datasets is a widely-embraced paradigm of harnessing the powerful capability of pretrained LLMs for various downstream tasks. Due to the popularity of LLMs finetuning and its accompanying privacy concerns, differentially private (DP) finetuning of pretrained LLMs has garnered increasing attention to safeguarding the privacy of task-specific datasets. Lying at the design core of DP LLM finetuning methods is the satisfactory tradeoff between privacy, utility, and scalability. Most existing methods build upon the seminal work of DP-SGD. Despite pushing the scalability of DP-SGD to its limit, DP-SGD-based finetuning methods are unfortunately limited by the inherent inefficiency of SGD. In this paper, we investigate the potential of DP zeroth-order methods for LLM pretraining, which avoids the scalability bottleneck of SGD by approximating the gradient with the more efficient zeroth-order gradient. Rather than treating the zeroth-order method as a drop-in replace
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;&#24189;&#28789;&#30340;&#28508;&#22312;&#23454;&#26045;&#35774;&#35745;&#31354;&#38388;&#21644;&#20854;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#30740;&#31350;&#35758;&#31243;&#20197;&#20415;&#20351;&#20154;&#20204;&#33021;&#22815;&#23433;&#20840;&#32780;&#26377;&#30410;&#22320;&#21019;&#24314;&#21644;&#19982;&#20154;&#24037;&#26234;&#33021;&#26469;&#19990;&#36827;&#34892;&#20114;&#21160;&#12290;</title><link>https://arxiv.org/abs/2402.01662</link><description>&lt;p&gt;
&#29983;&#25104;&#24189;&#28789;&#65306;&#39044;&#27979;&#20154;&#24037;&#26234;&#33021;&#26469;&#19990;&#30340;&#30410;&#22788;&#21644;&#39118;&#38505;
&lt;/p&gt;
&lt;p&gt;
Generative Ghosts: Anticipating Benefits and Risks of AI Afterlives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.01662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35752;&#35770;&#20102;&#29983;&#25104;&#24189;&#28789;&#30340;&#28508;&#22312;&#23454;&#26045;&#35774;&#35745;&#31354;&#38388;&#21644;&#20854;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#30740;&#31350;&#35758;&#31243;&#20197;&#20415;&#20351;&#20154;&#20204;&#33021;&#22815;&#23433;&#20840;&#32780;&#26377;&#30410;&#22320;&#21019;&#24314;&#21644;&#19982;&#20154;&#24037;&#26234;&#33021;&#26469;&#19990;&#36827;&#34892;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#24615;&#33021;&#30340;&#24191;&#24230;&#21644;&#28145;&#24230;&#19978;&#36805;&#36895;&#25552;&#21319;&#65292;&#23427;&#20204;&#36234;&#26469;&#36234;&#36866;&#21512;&#21019;&#24314;&#21151;&#33021;&#24378;&#22823;&#12289;&#36924;&#30495;&#30340;&#20195;&#29702;&#20154;&#65292;&#21253;&#25324;&#22522;&#20110;&#29305;&#23450;&#20154;&#29289;&#24314;&#27169;&#30340;&#20195;&#29702;&#20154;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#39044;&#35745;&#65292;&#22312;&#25105;&#20204;&#26377;&#29983;&#20043;&#24180;&#65292;&#20154;&#20204;&#21487;&#33021;&#20250;&#26222;&#36941;&#20351;&#29992;&#23450;&#21046;&#30340;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20154;&#19982;&#29233;&#30340;&#20154;&#21644;/&#25110;&#26356;&#24191;&#22823;&#30340;&#19990;&#30028;&#36827;&#34892;&#20114;&#21160;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#29983;&#25104;&#24189;&#28789;&#65292;&#22240;&#20026;&#36825;&#20123;&#20195;&#29702;&#20154;&#23558;&#33021;&#22815;&#29983;&#25104;&#26032;&#39062;&#30340;&#20869;&#23481;&#65292;&#32780;&#19981;&#21482;&#26159;&#22797;&#36848;&#20854;&#21019;&#20316;&#32773;&#22312;&#29983;&#21069;&#30340;&#20869;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#29983;&#25104;&#24189;&#28789;&#28508;&#22312;&#23454;&#26045;&#30340;&#35774;&#35745;&#31354;&#38388;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#29983;&#25104;&#24189;&#28789;&#30340;&#23454;&#38469;&#21644;&#20262;&#29702;&#24433;&#21709;&#65292;&#21253;&#25324;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#30340;&#28508;&#22312;&#31215;&#26497;&#21644;&#28040;&#26497;&#24433;&#21709;&#12290;&#22522;&#20110;&#36825;&#20123;&#32771;&#34385;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#30740;&#31350;&#35758;&#31243;&#65292;&#26088;&#22312;&#20351;&#20154;&#20204;&#33021;&#22815;&#23433;&#20840;&#32780;&#26377;&#30410;&#22320;&#21019;&#24314;&#21644;&#19982;&#20154;&#24037;&#26234;&#33021;&#26469;&#19990;&#36827;&#34892;&#20114;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
As AI systems quickly improve in both breadth and depth of performance, they lend themselves to creating increasingly powerful and realistic agents, including the possibility of agents modeled on specific people. We anticipate that within our lifetimes it may become common practice for people to create a custom AI agent to interact with loved ones and/or the broader world after death. We call these generative ghosts, since such agents will be capable of generating novel content rather than merely parroting content produced by their creator while living. In this paper, we first discuss the design space of potential implementations of generative ghosts. We then discuss the practical and ethical implications of generative ghosts, including potential positive and negative impacts on individuals and society. Based on these considerations, we lay out a research agenda for the AI and HCI research communities to empower people to create and interact with AI afterlives in a safe and beneficial 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;MARL&#20195;&#29702;&#65292;&#30740;&#31350;&#20102;&#20182;&#20204;&#23398;&#20064;&#36991;&#35753;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#65292;&#22312;&#23494;&#24230;&#19981;&#22826;&#39640;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2312.11834</link><description>&lt;p&gt;
&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#22312;&#34892;&#20154;&#21160;&#24577;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11834
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;MARL&#20195;&#29702;&#65292;&#30740;&#31350;&#20102;&#20182;&#20204;&#23398;&#20064;&#36991;&#35753;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#65292;&#22312;&#23494;&#24230;&#19981;&#22826;&#39640;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#27169;&#25311;&#34892;&#20154;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#36947;&#36335;&#65292;&#24182;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#21644;&#26368;&#23567;&#20108;&#20056;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#30340;MARL&#20195;&#29702;&#12290;&#22312;&#36825;&#20010;&#29615;&#22659;&#19979;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#20195;&#29702;&#23398;&#20064;&#36991;&#24320;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#20219;&#21153;&#65306;&#31364;&#30452;&#25509;&#36335;&#24452;&#21644;&#23485;&#32469;&#36947;&#20043;&#38388;&#30340;&#36873;&#25321;&#65292;&#20197;&#21450;&#36208;&#24266;&#20013;&#30340;&#21452;&#21521;&#34892;&#20154;&#27969;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20195;&#29702;&#23494;&#24230;&#19981;&#22826;&#39640;&#26102;&#65292;&#23398;&#20064;&#26159;&#25104;&#21151;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11834v2 Announce Type: replace-cross  Abstract: In recent years, simulations of pedestrians using the multi-agent reinforcement learning (MARL) have been studied. This study considered the roads on a grid-world environment, and implemented pedestrians as MARL agents using an echo-state network and the least squares policy iteration method. Under this environment, the ability of these agents to learn to move forward by avoiding other agents was investigated. Specifically, we considered two types of tasks: the choice between a narrow direct route and a broad detour, and the bidirectional pedestrian flow in a corridor. The simulations results indicated that the learning was successful when the density of the agents was not that high.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31435;&#38656;&#35201;&#26356;&#22909;&#22320;&#25972;&#21512;&#20154;&#31867;&#32972;&#26223;&#65292;&#24182;&#38754;&#20020;&#30528;&#22914;&#20309;&#25429;&#25417;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#20197;&#21450;&#22914;&#20309;&#24314;&#27169;&#30340;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2312.07751</link><description>&lt;p&gt;
&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#65306;&#38656;&#27714;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Large Human Language Models: A Need and the Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07751
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#30340;&#24314;&#31435;&#38656;&#35201;&#26356;&#22909;&#22320;&#25972;&#21512;&#20154;&#31867;&#32972;&#26223;&#65292;&#24182;&#38754;&#20020;&#30528;&#22914;&#20309;&#25429;&#25417;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#20197;&#21450;&#22914;&#20309;&#24314;&#27169;&#30340;&#19968;&#31995;&#21015;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#31867;&#20013;&#24515;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#30340;&#36827;&#23637;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#24847;&#35782;&#21040;&#23558;&#20154;&#31867;&#21644;&#31038;&#20250;&#22240;&#32032;&#32435;&#20837;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#24050;&#32463;&#20005;&#37325;&#20381;&#36182;&#20110;LLM&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#24182;&#27809;&#26377;&#23545;&#20316;&#32773;&#36827;&#34892;&#24314;&#27169;&#12290;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#30495;&#27491;&#29702;&#35299;&#20154;&#31867;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#26356;&#22909;&#22320;&#23558;&#20154;&#31867;&#32972;&#26223;&#25972;&#21512;&#21040;LLM&#20013;&#12290;&#36825;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#35774;&#35745;&#32771;&#34385;&#21644;&#25361;&#25112;&#65292;&#28041;&#21450;&#21040;&#35201;&#25429;&#25417;&#21738;&#20123;&#20154;&#31867;&#22240;&#32032;&#12289;&#22914;&#20309;&#34920;&#31034;&#23427;&#20204;&#20197;&#21450;&#35201;&#37319;&#29992;&#20309;&#31181;&#24314;&#27169;&#31574;&#30053;&#31561;&#38382;&#39064;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20513;&#20174;&#24515;&#29702;&#23398;&#21644;&#34892;&#20026;&#31185;&#23398;&#30340;&#27010;&#24565;&#20986;&#21457;&#65292;&#25903;&#25345;&#19977;&#20010;&#31435;&#22330;&#26469;&#21019;&#24314;&#22823;&#22411;&#20154;&#31867;&#35821;&#35328;&#27169;&#22411;&#65288;LHLMs&#65289;&#65306;&#39318;&#20808;&#65292;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#24212;&#21253;&#25324;&#20154;&#31867;&#32972;&#26223;&#12290;&#20854;&#27425;&#65292;LHLMs&#24212;&#35813;&#24847;&#35782;&#21040;&#20154;&#19981;&#20165;&#20165;&#26159;&#20182;&#20204;&#25152;&#23646;&#30340;&#32676;&#20307;&#12290;&#31532;&#19977;&#65292;LHLMs&#24212;&#35813;&#33021;&#22815;&#32771;&#34385;&#21040;&#20154;&#31867;&#32972;&#26223;&#30340;&#21160;&#24577;&#21644;&#26102;&#38388;&#20381;&#36182;&#24615;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07751v2 Announce Type: replace-cross  Abstract: As research in human-centered NLP advances, there is a growing recognition of the importance of incorporating human and social factors into NLP models. At the same time, our NLP systems have become heavily reliant on LLMs, most of which do not model authors. To build NLP systems that can truly understand human language, we must better integrate human contexts into LLMs. This brings to the fore a range of design considerations and challenges in terms of what human aspects to capture, how to represent them, and what modeling strategies to pursue. To address these, we advocate for three positions toward creating large human language models (LHLMs) using concepts from psychological and behavioral sciences: First, LM training should include the human context. Second, LHLMs should recognize that people are more than their group(s). Third, LHLMs should be able to account for the dynamic and temporally-dependent nature of the human con
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20248;&#21270;&#30340;&#20999;&#29255;&#20998;&#24067;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24555;&#36895;&#36827;&#34892;&#33945;&#29305;&#21345;&#27931;&#26399;&#26395;&#20272;&#35745;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#21521;&#37327;&#20043;&#38388;&#30340;&#24402;&#19968;&#21270;&#24046;&#24322;&#26500;&#24314;&#38543;&#26426;&#36335;&#24452;&#25237;&#24433;&#26041;&#21521;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#38543;&#26426;&#36335;&#24452;&#20999;&#29255;&#20998;&#24067;&#21644;&#20004;&#20010;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#30340;&#21464;&#31181;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#25299;&#25169;&#12289;&#32479;&#35745;&#21644;&#35745;&#31639;&#24615;&#36136;&#19978;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2401.15889</link><description>&lt;p&gt;
&#24102;&#26377;&#38543;&#26426;&#36335;&#24452;&#25237;&#24433;&#26041;&#21521;&#30340;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sliced Wasserstein with Random-Path Projecting Directions. (arXiv:2401.15889v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20248;&#21270;&#30340;&#20999;&#29255;&#20998;&#24067;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#24555;&#36895;&#36827;&#34892;&#33945;&#29305;&#21345;&#27931;&#26399;&#26395;&#20272;&#35745;&#12290;&#36890;&#36807;&#21033;&#29992;&#38543;&#26426;&#21521;&#37327;&#20043;&#38388;&#30340;&#24402;&#19968;&#21270;&#24046;&#24322;&#26500;&#24314;&#38543;&#26426;&#36335;&#24452;&#25237;&#24433;&#26041;&#21521;&#65292;&#20174;&#32780;&#24471;&#21040;&#20102;&#38543;&#26426;&#36335;&#24452;&#20999;&#29255;&#20998;&#24067;&#21644;&#20004;&#20010;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#30340;&#21464;&#31181;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#25299;&#25169;&#12289;&#32479;&#35745;&#21644;&#35745;&#31639;&#24615;&#36136;&#19978;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24212;&#29992;&#20013;&#65292;&#20999;&#29255;&#20998;&#24067;&#36873;&#25321;&#24050;&#34987;&#29992;&#20316;&#25552;&#39640;&#22522;&#20110;&#26368;&#23567;&#21270;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#36317;&#31163;&#30340;&#21442;&#25968;&#20272;&#35745;&#22120;&#24615;&#33021;&#30340;&#26377;&#25928;&#25216;&#26415;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#35201;&#20040;&#21033;&#29992;&#26114;&#36149;&#30340;&#20248;&#21270;&#26469;&#36873;&#25321;&#20999;&#29255;&#20998;&#24067;&#65292;&#35201;&#20040;&#20351;&#29992;&#38656;&#35201;&#26114;&#36149;&#30340;&#25277;&#26679;&#26041;&#27861;&#30340;&#20999;&#29255;&#20998;&#24067;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#20248;&#21270;&#30340;&#20999;&#29255;&#20998;&#24067;&#65292;&#21487;&#20197;&#24555;&#36895;&#36827;&#34892;&#33945;&#29305;&#21345;&#27931;&#26399;&#26395;&#20272;&#35745;&#30340;&#25277;&#26679;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#38543;&#26426;&#36335;&#24452;&#25237;&#24433;&#26041;&#21521;&#65288;RPD&#65289;&#65292;&#23427;&#26159;&#36890;&#36807;&#21033;&#29992;&#20004;&#20010;&#36755;&#20837;&#27979;&#37327;&#20013;&#20004;&#20010;&#38543;&#26426;&#21521;&#37327;&#20043;&#38388;&#30340;&#24402;&#19968;&#21270;&#24046;&#24322;&#26500;&#24314;&#30340;&#12290;&#20174;RPD&#20013;&#65292;&#25105;&#20204;&#24471;&#21040;&#20102;&#38543;&#26426;&#36335;&#24452;&#20999;&#29255;&#20998;&#24067;&#65288;RPSD&#65289;&#21644;&#20004;&#20010;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#30340;&#21464;&#31181;&#65292;&#21363;&#38543;&#26426;&#36335;&#24452;&#25237;&#24433;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;RPSW&#65289;&#21644;&#37325;&#35201;&#24615;&#21152;&#26435;&#38543;&#26426;&#36335;&#24452;&#25237;&#24433;&#20999;&#29255;&#29926;&#29791;&#26031;&#22374;&#65288;IWRPSW&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#35752;&#35770;&#20102;&#25299;&#25169;&#12289;&#32479;&#35745;&#21644;&#35745;&#31639;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Slicing distribution selection has been used as an effective technique to improve the performance of parameter estimators based on minimizing sliced Wasserstein distance in applications. Previous works either utilize expensive optimization to select the slicing distribution or use slicing distributions that require expensive sampling methods. In this work, we propose an optimization-free slicing distribution that provides a fast sampling for the Monte Carlo estimation of expectation. In particular, we introduce the random-path projecting direction (RPD) which is constructed by leveraging the normalized difference between two random vectors following the two input measures. From the RPD, we derive the random-path slicing distribution (RPSD) and two variants of sliced Wasserstein, i.e., the Random-Path Projection Sliced Wasserstein (RPSW) and the Importance Weighted Random-Path Projection Sliced Wasserstein (IWRPSW). We then discuss the topological, statistical, and computational propert
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#30340;&#20381;&#36182;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36866;&#24403;&#20381;&#36182;&#12290;&#35813;&#23450;&#20041;&#20998;&#31163;&#20102;&#20381;&#36182;&#30340;&#27010;&#24565;&#21644;&#20154;&#31867;&#22312;&#24418;&#25104;&#20934;&#30830;&#20449;&#24565;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20026;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20114;&#34917;&#24615;&#21644;&#20381;&#36182;&#24615;&#30340;&#30740;&#31350;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2401.15356</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#20381;&#36182;&#30340;&#32479;&#35745;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Statistical Framework for Measuring AI Reliance. (arXiv:2401.15356v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15356
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#30340;&#20381;&#36182;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#29992;&#20110;&#34913;&#37327;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#36866;&#24403;&#20381;&#36182;&#12290;&#35813;&#23450;&#20041;&#20998;&#31163;&#20102;&#20381;&#36182;&#30340;&#27010;&#24565;&#21644;&#20154;&#31867;&#22312;&#24418;&#25104;&#20934;&#30830;&#20449;&#24565;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20026;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20114;&#34917;&#24615;&#21644;&#20381;&#36182;&#24615;&#30340;&#30740;&#31350;&#35774;&#35745;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#32463;&#24120;&#22312;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#24110;&#21161;&#19979;&#20570;&#20915;&#31574;&#12290;&#19968;&#20010;&#24120;&#35265;&#27169;&#24335;&#26159;&#20154;&#24037;&#26234;&#33021;&#21521;&#20154;&#31867;&#25512;&#33616;&#34892;&#21160;&#65292;&#32780;&#20154;&#31867;&#20445;&#30041;&#23545;&#26368;&#32456;&#20915;&#31574;&#30340;&#25511;&#21046;&#26435;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#30830;&#35748;&#65292;&#30830;&#20445;&#20154;&#31867;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#36866;&#24403;&#20381;&#36182;&#26159;&#23454;&#29616;&#20114;&#34917;&#24615;&#33021;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#30446;&#21069;&#22312;&#36825;&#26041;&#38754;&#30340;&#30740;&#31350;&#20013;&#20351;&#29992;&#30340;&#36866;&#24403;&#20381;&#36182;&#30340;&#23450;&#20041;&#32570;&#20047;&#24418;&#24335;&#21270;&#30340;&#32479;&#35745;&#22522;&#30784;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#30683;&#30462;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#32479;&#35745;&#20915;&#31574;&#29702;&#35770;&#30340;&#20381;&#36182;&#30340;&#24418;&#24335;&#23450;&#20041;&#65292;&#23427;&#23558;&#20381;&#36182;&#30340;&#27010;&#24565;&#19982;&#20154;&#31867;&#22312;&#21306;&#20998;&#20449;&#21495;&#24182;&#24418;&#25104;&#20934;&#30830;&#20449;&#24565;&#30340;&#25361;&#25112;&#20998;&#24320;&#12290;&#25105;&#20204;&#30340;&#23450;&#20041;&#20135;&#29983;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#29992;&#26469;&#25351;&#23548;&#20154;&#31867;&#19982;&#20154;&#24037;&#26234;&#33021;&#20114;&#34917;&#24615;&#21644;&#20381;&#36182;&#24615;&#30340;&#30740;&#31350;&#35774;&#35745;&#21644;&#35299;&#37322;&#12290;&#21033;&#29992;&#26368;&#36817;&#30340;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#20915;&#31574;&#30740;&#31350;...
&lt;/p&gt;
&lt;p&gt;
Humans frequently make decisions with the aid of artificially intelligent (AI) systems. A common pattern is for the AI to recommend an action to the human who retains control over the final decision. Researchers have identified ensuring that a human has appropriate reliance on an AI as a critical component of achieving complementary performance. We argue that the current definition of appropriate reliance used in such research lacks formal statistical grounding and can lead to contradictions. We propose a formal definition of reliance, based on statistical decision theory, which separates the concepts of reliance as the probability the decision-maker follows the AI's prediction from challenges a human may face in differentiating the signals and forming accurate beliefs about the situation. Our definition gives rise to a framework that can be used to guide the design and interpretation of studies on human-AI complementarity and reliance. Using recent AI-advised decision making studies f
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20223;&#30495;&#25216;&#26415;&#30340;&#21457;&#23637;&#19982;&#31185;&#23398;&#33539;&#24335;&#30340;&#28436;&#21464;&#65292;&#24182;&#25552;&#20986;&#20102;&#34892;&#20026;&#20223;&#30495;&#30340;&#27010;&#24565;&#65292;&#20195;&#34920;&#20102;&#26356;&#39640;&#31243;&#24230;&#30340;&#33539;&#24335;&#25972;&#21512;&#12290;</title><link>http://arxiv.org/abs/2401.09851</link><description>&lt;p&gt;
&#20223;&#30495;&#34892;&#20026;&#65306;&#25506;&#32034;&#31185;&#23398;&#30340;&#21487;&#33021;&#19979;&#19968;&#33539;&#24335;
&lt;/p&gt;
&lt;p&gt;
Behavioral Simulation: Exploring A Possible Next Paradigm for Science. (arXiv:2401.09851v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.09851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20223;&#30495;&#25216;&#26415;&#30340;&#21457;&#23637;&#19982;&#31185;&#23398;&#33539;&#24335;&#30340;&#28436;&#21464;&#65292;&#24182;&#25552;&#20986;&#20102;&#34892;&#20026;&#20223;&#30495;&#30340;&#27010;&#24565;&#65292;&#20195;&#34920;&#20102;&#26356;&#39640;&#31243;&#24230;&#30340;&#33539;&#24335;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20223;&#30495;&#25216;&#26415;&#24050;&#24191;&#27867;&#24212;&#29992;&#20110;&#35768;&#22810;&#31185;&#23398;&#30740;&#31350;&#39046;&#22495;&#65292;&#22914;&#22825;&#27668;&#39044;&#25253;&#12289;&#27969;&#20307;&#21147;&#23398;&#21644;&#29983;&#29289;&#31181;&#32676;&#12290;&#23427;&#26159;&#22788;&#29702;&#22797;&#26434;&#31995;&#32479;&#38382;&#39064;&#30340;&#26368;&#20339;&#24037;&#20855;&#65292;&#22312;&#34920;&#31034;&#31354;&#38388;&#20013;&#26080;&#27861;&#20351;&#29992;&#38381;&#21512;&#24418;&#24335;&#34920;&#36798;&#24335;&#19988;&#30446;&#26631;&#20998;&#24067;&#36807;&#20110;&#22797;&#26434;&#32780;&#26080;&#27861;&#23436;&#20840;&#30001;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34920;&#31034;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20223;&#30495;&#25216;&#26415;&#30340;&#21457;&#23637;&#19982;&#31185;&#23398;&#33539;&#24335;&#26159;&#19968;&#33268;&#30340;&#12290;&#26412;&#25991;&#20174;&#25968;&#25454;&#12289;&#31639;&#27861;&#21644;&#35745;&#31639;&#33021;&#21147;&#30340;&#35282;&#24230;&#24402;&#32435;&#20102;&#31185;&#23398;&#33539;&#24335;&#30340;&#28436;&#21464;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#23558;&#20223;&#30495;&#25216;&#26415;&#20998;&#20026;&#19977;&#20010;&#38454;&#27573;&#65292;&#19982;&#26032;&#33539;&#24335;&#30340;&#20986;&#29616;&#30456;&#36866;&#24212;&#65292;&#24182;&#21457;&#29616;&#20808;&#36827;&#30340;&#20223;&#30495;&#25216;&#26415;&#26159;&#33539;&#24335;&#25972;&#21512;&#30340;&#20856;&#22411;&#23454;&#20363;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#34892;&#20026;&#20223;&#30495;&#65288;BS&#65289;&#30340;&#27010;&#24565;&#65292;&#29305;&#21035;&#26159;&#22797;&#26434;&#34892;&#20026;&#20223;&#30495;&#65288;SBS&#65289;&#65292;&#20195;&#34920;&#20102;&#26356;&#39640;&#31243;&#24230;&#30340;&#33539;&#24335;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Simulation technologies have been widely utilized in many scientific research fields such as weather forecasting, fluid mechanics and biological populations. It is the best tool to handle problems in complex systems, where closed-form expressions are unavailable and the target distribution in the representation space is too complex to be fully represented by a deep learning (DL) model. We believe that the development of simulation technologies is consistent with scientific paradigms. This paper induces the evolution of scientific paradigms from the perspective of data, algorithms, and computational power. Building upon this perspective, we divide simulation technologies into three stages aligning with the emergence of new paradigms, and find that advanced simulation technologies are typical instances of paradigms integration. Moreover, we propose the concept of behavioral simulation (BS), specifically sophisticated behavioral simulation (SBS), representing a higher degree of paradigms 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SlimPajama&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#19981;&#21516;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20840;&#23616;&#21435;&#37325;&#21644;&#23616;&#37096;&#21435;&#37325;&#30340;&#27604;&#36739;&#21644;&#39640;&#36136;&#37327;&#22810;&#28304;&#25968;&#25454;&#38598;&#30340;&#27604;&#20363;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2309.10818</link><description>&lt;p&gt;
SlimPajama-DC: &#29702;&#35299;LLM&#35757;&#32451;&#20013;&#30340;&#25968;&#25454;&#32452;&#21512;
&lt;/p&gt;
&lt;p&gt;
SlimPajama-DC: Understanding Data Combinations for LLM Training. (arXiv:2309.10818v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10818
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#20351;&#29992;SlimPajama&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#19981;&#21516;&#25968;&#25454;&#32452;&#21512;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#20840;&#23616;&#21435;&#37325;&#21644;&#23616;&#37096;&#21435;&#37325;&#30340;&#27604;&#36739;&#21644;&#39640;&#36136;&#37327;&#22810;&#28304;&#25968;&#25454;&#38598;&#30340;&#27604;&#20363;&#23545;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#20102;&#35299;&#20351;&#29992;SlimPajama&#36827;&#34892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#26102;&#21508;&#31181;&#25968;&#25454;&#32452;&#21512;&#65288;&#22914;&#32593;&#32476;&#25991;&#26412;&#12289;&#32500;&#22522;&#30334;&#31185;&#12289;GitHub&#12289;&#22270;&#20070;&#65289;&#23545;&#20854;&#35757;&#32451;&#30340;&#24433;&#21709;&#12290;SlimPajama&#26159;&#19968;&#20010;&#32463;&#36807;&#20005;&#26684;&#21435;&#37325;&#30340;&#22810;&#28304;&#25968;&#25454;&#38598;&#65292;&#20174;Together&#36129;&#29486;&#30340;1.2T&#20010;token&#30340;RedPajama&#25968;&#25454;&#38598;&#20013;&#31934;&#32454;&#32452;&#21512;&#21644;&#21435;&#37325;&#65292;&#24635;&#20849;&#24471;&#21040;&#20102;627B&#20010;tokens&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#30740;&#31350;&#31216;&#20026;SlimPajama-DC&#65292;&#36825;&#26159;&#19968;&#39033;&#26088;&#22312;&#25581;&#31034;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#20013;&#20351;&#29992;SlimPajama&#25152;&#28041;&#21450;&#30340;&#22522;&#26412;&#29305;&#24449;&#21644;&#26368;&#20339;&#23454;&#36341;&#30340;&#32463;&#39564;&#20998;&#26512;&#12290;&#22312;&#25105;&#20204;&#20351;&#29992;SlimPajama&#36827;&#34892;&#30740;&#31350;&#30340;&#36807;&#31243;&#20013;&#65292;&#20986;&#29616;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#32467;&#26524;&#65306;&#65288;1&#65289;&#20840;&#23616;&#21435;&#37325; vs. &#23616;&#37096;&#21435;&#37325;&#12290;&#25105;&#20204;&#20998;&#26512;&#21644;&#35752;&#35770;&#20102;&#20840;&#23616;&#21435;&#37325;&#65288;&#36328;&#19981;&#21516;&#25968;&#25454;&#38598;&#28304;&#65289;&#21644;&#23616;&#37096;&#21435;&#37325;&#65288;&#22312;&#21333;&#20010;&#25968;&#25454;&#38598;&#28304;&#20869;&#37096;&#65289;&#23545;&#35757;&#32451;&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#65288;2&#65289;&#39640;&#36136;&#37327;/&#39640;&#24230;&#21435;&#37325;&#30340;&#22810;&#28304;&#25968;&#25454;&#38598;&#22312;&#32452;&#21512;&#20013;&#30340;&#27604;&#20363;&#12290;&#20026;&#20102;&#30740;&#31350;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;...
&lt;/p&gt;
&lt;p&gt;
This paper aims to understand the impacts of various data combinations (e.g., web text, wikipedia, github, books) on the training of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T tokens RedPajama dataset contributed by Together. We've termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of high-quality/highly-deduplicated multi-source datasets in the combination. To study this, we cons
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2308.00264</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-Modality Multi-Loss Fusion Network. (arXiv:2308.00264v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.00264
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#22810;&#25439;&#22833;&#34701;&#21512;&#32593;&#32476;&#36890;&#36807;&#26368;&#20339;&#36873;&#25321;&#21644;&#34701;&#21512;&#22810;&#20010;&#27169;&#24577;&#30340;&#29305;&#24449;&#65292;&#25552;&#39640;&#20102;&#24773;&#24863;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#20102;&#29992;&#20110;&#22686;&#24378;&#31070;&#32463;&#32593;&#32476;&#20013;&#24773;&#24863;&#26816;&#27979;&#30340;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#20248;&#21270;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#36328;&#22810;&#20010;&#27169;&#24577;&#36873;&#25321;&#21644;&#34701;&#21512;&#29305;&#24449;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#32452;&#21512;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#20197;&#25913;&#21892;&#24773;&#24863;&#26816;&#27979;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#34701;&#21512;&#26041;&#27861;&#24182;&#19988;&#30740;&#31350;&#20102;&#22810;&#25439;&#22833;&#35757;&#32451;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#20013;&#30340;&#24433;&#21709;&#65292;&#20174;&#32780;&#30830;&#23450;&#20102;&#19982;&#23376;&#32593;&#24615;&#33021;&#30456;&#20851;&#30340;&#26377;&#29992;&#21457;&#29616;&#12290;&#25105;&#20204;&#26368;&#22909;&#30340;&#27169;&#22411;&#22312;&#19977;&#20010;&#25968;&#25454;&#38598;&#65288;CMU-MOSI&#12289;CMU-MOSEI&#21644;CH-SIMS&#65289;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;&#22823;&#22810;&#25968;&#25351;&#26631;&#19978;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#35757;&#32451;&#22810;&#27169;&#24577;&#29305;&#24449;&#21487;&#20197;&#25913;&#21892;&#21333;&#27169;&#24577;&#27979;&#35797;&#65292;&#24182;&#19988;&#22522;&#20110;&#25968;&#25454;&#38598;&#27880;&#37322;&#27169;&#24335;&#35774;&#35745;&#34701;&#21512;&#26041;&#27861;&#21487;&#20197;&#22686;&#24378;&#27169;&#22411;&#24615;&#33021;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#20102;&#22312;&#31070;&#32463;&#32593;&#32476;&#20013;&#22686;&#24378;&#24773;&#24863;&#26816;&#27979;&#30340;&#20248;&#21270;&#29305;&#24449;&#36873;&#25321;&#21644;&#34701;&#21512;&#26041;&#27861;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work we investigate the optimal selection and fusion of features across multiple modalities and combine these in a neural network to improve emotion detection. We compare different fusion methods and examine the impact of multi-loss training within the multi-modality fusion network, identifying useful findings relating to subnet performance. Our best model achieves state-of-the-art performance for three datasets (CMU-MOSI, CMU-MOSEI and CH-SIMS), and outperforms the other methods in most metrics. We have found that training on multimodal features improves single modality testing and designing fusion methods based on dataset annotation schema enhances model performance. These results suggest a roadmap towards an optimized feature selection and fusion approach for enhancing emotion detection in neural networks.
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;</title><link>http://arxiv.org/abs/2307.06945</link><description>&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
In-context Autoencoder for Context Compression in a Large Language Model. (arXiv:2307.06945v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06945
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;In-context Autoencoder (ICAE)&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65292;&#23427;&#36890;&#36807;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#23454;&#29616;&#20102;$4\times$&#30340;&#19978;&#19979;&#25991;&#21387;&#32553;&#65292;&#24182;&#33021;&#22815;&#26681;&#25454;&#20869;&#23384;&#27133;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#19978;&#19979;&#25991;&#33258;&#32534;&#30721;&#22120;&#65288;ICAE&#65289;&#12290; ICAE&#26377;&#20004;&#20010;&#27169;&#22359;&#65306;&#19968;&#20010;&#21487;&#23398;&#20064;&#30340;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#20174;LLM&#20013;&#37319;&#29992;LoRA&#26041;&#24335;&#23558;&#38271;&#19978;&#19979;&#25991;&#21387;&#32553;&#20026;&#26377;&#38480;&#25968;&#37327;&#30340;&#20869;&#23384;&#27133;&#65292;&#20197;&#21450;&#19968;&#20010;&#22266;&#23450;&#30340;&#35299;&#30721;&#22120;&#65292;&#20316;&#20026;&#30446;&#26631;LLM&#65292;&#21487;&#20197;&#26681;&#25454;&#20869;&#23384;&#27133;&#26469;&#36827;&#34892;&#21508;&#31181;&#30446;&#30340;&#30340;&#26465;&#20214;&#22788;&#29702;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;&#33258;&#32534;&#30721;&#21644;&#35821;&#35328;&#24314;&#27169;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#25968;&#25454;&#19978;&#39044;&#35757;&#32451;ICAE&#65292;&#20351;&#20854;&#33021;&#22815;&#29983;&#25104;&#20934;&#30830;&#21644;&#20840;&#38754;&#34920;&#31034;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#20869;&#23384;&#27133;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#23569;&#37327;&#25351;&#23548;&#25968;&#25454;&#23545;&#39044;&#35757;&#32451;&#30340;ICAE&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#22686;&#24378;&#20854;&#19982;&#21508;&#31181;&#25552;&#31034;&#30340;&#20132;&#20114;&#65292;&#20174;&#32780;&#20135;&#29983;&#29702;&#24819;&#30340;&#21709;&#24212;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#25105;&#20204;&#25552;&#20986;&#30340;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#33539;&#24335;&#23398;&#20064;&#30340;ICAE&#21487;&#20197;&#26377;&#25928;&#22320;&#20135;&#29983;$4\times$&#19978;&#19979;&#25991;&#21387;&#32553;&#30340;&#20869;&#23384;&#27133;&#65292;&#30446;&#26631;LLM&#21487;&#20197;&#24456;&#22909;&#22320;&#23545;&#20854;&#36827;&#34892;&#26465;&#20214;&#22788;&#29702;&#65292;&#20197;&#21709;&#24212;&#21508;&#31181;&#25552;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose the In-context Autoencoder (ICAE) for context compression in a large language model (LLM). The ICAE has two modules: a learnable encoder adapted with LoRA from an LLM for compressing a long context into a limited number of memory slots, and a fixed decoder which is the target LLM that can condition on the memory slots for various purposes. We first pretrain the ICAE using both autoencoding and language modeling objectives on massive text data, enabling it to generate memory slots that accurately and comprehensively represent the original context. Then, we fine-tune the pretrained ICAE on a small amount of instruct data to enhance its interaction with various prompts for producing desirable responses. Our experimental results demonstrate that the ICAE learned with our proposed pretraining and fine-tuning paradigm can effectively produce memory slots with $4\times$ context compression, which can be well conditioned on by the target LLM to respond to various prompts. The promis
&lt;/p&gt;</description></item><item><title>TransCoder&#26159;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#20195;&#30721;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#35843;&#25972;&#30340;&#21069;&#32512;&#32534;&#30721;&#22120;&#20316;&#20026;&#20803;&#23398;&#20064;&#22120;&#26469;&#25429;&#25417;&#36328;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#30340;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#30340;&#20195;&#30721;&#30456;&#20851;&#20803;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;remarkably&#22320;&#25552;&#39640;&#35757;&#32451;&#26679;&#26412;&#37327;&#36739;&#23567;&#21644;&#35821;&#26009;&#24211;&#36739;&#23567;&#30340;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2306.07285</link><description>&lt;p&gt;
TransCoder&#65306;&#21463;&#20154;&#31867;&#25216;&#33021;&#21551;&#21457;&#30340;&#32479;&#19968;&#21487;&#36716;&#31227;&#20195;&#30721;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
TransCoder: Towards Unified Transferable Code Representation Learning Inspired by Human Skills. (arXiv:2306.07285v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07285
&lt;/p&gt;
&lt;p&gt;
TransCoder&#26159;&#19968;&#31181;&#21487;&#36716;&#31227;&#30340;&#20195;&#30721;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#30340;&#24494;&#35843;&#31574;&#30053;&#65292;&#36890;&#36807;&#21487;&#35843;&#25972;&#30340;&#21069;&#32512;&#32534;&#30721;&#22120;&#20316;&#20026;&#20803;&#23398;&#20064;&#22120;&#26469;&#25429;&#25417;&#36328;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#30340;&#21487;&#36716;&#31227;&#30693;&#35782;&#65292;&#20351;&#27169;&#22411;&#23398;&#20064;&#26356;&#22909;&#30340;&#20195;&#30721;&#30456;&#20851;&#20803;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;remarkably&#22320;&#25552;&#39640;&#35757;&#32451;&#26679;&#26412;&#37327;&#36739;&#23567;&#21644;&#35821;&#26009;&#24211;&#36739;&#23567;&#30340;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20195;&#30721;&#39044;&#35757;&#32451;&#27169;&#22411;&#65288;CodePTMs&#65289;&#24050;&#32463;&#34920;&#29616;&#20986;&#22312;&#22788;&#29702;&#21508;&#31181;&#36719;&#20214;&#26234;&#33021;&#20219;&#21153;&#26041;&#38754;&#30340;&#25166;&#23454;&#33021;&#21147;&#65292;&#20363;&#22914;&#65292;&#20195;&#30721;&#20811;&#38534;&#26816;&#27979;&#12289;&#20195;&#30721;&#32763;&#35793;&#21644;&#20195;&#30721;&#25688;&#35201;&#12290;&#30446;&#21069;&#23558;&#36825;&#20123;&#27169;&#22411;&#37096;&#32626;&#21040;&#19979;&#28216;&#20219;&#21153;&#30340;&#20027;&#27969;&#26041;&#27861;&#26159;&#22312;&#21333;&#20010;&#20219;&#21153;&#19978;&#23545;&#23427;&#20204;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#36890;&#24120;&#26159;&#26114;&#36149;&#30340;&#65292;&#24182;&#19988;&#38656;&#35201;&#22823;&#22411;&#27169;&#22411;&#30340;&#20805;&#36275;&#25968;&#25454;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TransCoder&#65292;&#36825;&#26159;&#19968;&#31181;&#32479;&#19968;&#30340;&#21487;&#36716;&#31227;&#30340;&#20195;&#30721;&#34920;&#31034;&#23398;&#20064;&#20219;&#21153;&#30340;&#24494;&#35843;&#31574;&#30053;&#12290;&#21463;&#20154;&#31867;&#20869;&#22312;&#30693;&#35782;&#27867;&#21270;&#25216;&#33021;&#30340;&#21551;&#21457;&#65292;TransCoder&#39537;&#21160;&#27169;&#22411;&#20687;&#20154;&#31867;&#31243;&#24207;&#21592;&#19968;&#26679;&#23398;&#20064;&#26356;&#22909;&#30340;&#20195;&#30721;&#30456;&#20851;&#20803;&#30693;&#35782;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#37319;&#29992;&#21487;&#35843;&#25972;&#30340;&#21069;&#32512;&#32534;&#30721;&#22120;&#20316;&#20026;&#20803;&#23398;&#20064;&#22120;&#65292;&#20998;&#21035;&#25429;&#25417;&#36328;&#20219;&#21153;&#21644;&#36328;&#35821;&#35328;&#30340;&#21487;&#36716;&#31227;&#30693;&#35782;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;remarkably&#22320;&#25552;&#39640;&#35757;&#32451;&#26679;&#26412;&#37327;&#36739;&#23567;&#21644;&#35821;&#26009;&#24211;&#36739;&#23567;&#30340;&#20219;&#21153;&#21644;&#35821;&#35328;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code pre-trained models (CodePTMs) have recently demonstrated a solid capacity to process various software intelligence tasks, e.g., code clone detection, code translation, and code summarization. The current mainstream method that deploys these models to downstream tasks is to fine-tune them on individual tasks, which is generally costly and needs sufficient data for large models. To tackle the issue, in this paper, we present TransCoder, a unified Transferable fine-tuning strategy for Code representation learning. Inspired by human inherent skills of knowledge generalization, TransCoder drives the model to learn better code-related meta-knowledge like human programmers. Specifically, we employ a tunable prefix encoder as the meta-learner to capture cross-task and cross-language transferable knowledge, respectively. Besides, tasks with minor training sample sizes and languages with small corpus can be remarkably benefited from our approach. Extensive experiments conducted on benchmark
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#22522;&#20110;&#20195;&#29702;&#20154;&#32676;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23398;&#20064;&#22266;&#23450;&#32500;&#24230;&#30340;&#23884;&#20837;&#65292;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#22312;&#19968;&#23567;&#32452;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#26469;&#39044;&#27979;&#20854;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#20219;&#21153;&#36873;&#39033;&#20013;&#36873;&#25321;&#20855;&#26377;&#25152;&#38656;&#29305;&#24449;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2306.03311</link><description>&lt;p&gt;
&#20351;&#29992;&#20195;&#29702;&#20154;&#32676;&#20307;&#23398;&#20064;&#24207;&#21015;&#20219;&#21153;&#30340;&#23884;&#20837;
&lt;/p&gt;
&lt;p&gt;
Learning Embeddings for Sequential Tasks Using Population of Agents. (arXiv:2306.03311v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03311
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#22522;&#20110;&#20195;&#29702;&#20154;&#32676;&#20307;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23398;&#20064;&#22266;&#23450;&#32500;&#24230;&#30340;&#23884;&#20837;&#65292;&#21487;&#20197;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#22312;&#19968;&#23567;&#32452;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#26469;&#39044;&#27979;&#20854;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#19988;&#21487;&#20197;&#20174;&#32473;&#23450;&#30340;&#20219;&#21153;&#36873;&#39033;&#20013;&#36873;&#25321;&#20855;&#26377;&#25152;&#38656;&#29305;&#24449;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20449;&#24687;&#29702;&#35770;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#23398;&#20064;&#22266;&#23450;&#32500;&#24230;&#30340;&#23884;&#20837;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#26679;&#30340;&#24819;&#27861;&#65306;&#22914;&#26524;&#35266;&#23519;&#19968;&#20010;&#20195;&#29702;&#22312;&#19968;&#20010;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20943;&#23569;&#20102;&#25105;&#20204;&#20851;&#20110;&#20182;&#22312;&#21478;&#19968;&#20010;&#20219;&#21153;&#19978;&#34920;&#29616;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#37027;&#20040;&#20004;&#20010;&#20219;&#21153;&#23601;&#30456;&#20284;&#12290;&#25105;&#20204;&#30340;&#20449;&#24687;&#29702;&#35770;&#20934;&#21017;&#25429;&#25417;&#20102;&#36825;&#31181;&#30452;&#35273;&#65292;&#20351;&#29992;&#22810;&#26679;&#21270;&#30340;&#20195;&#29702;&#20154;&#32676;&#20307;&#26469;&#27979;&#37327;&#24207;&#21015;&#20915;&#31574;&#29615;&#22659;&#20013;&#20219;&#21153;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#38500;&#20102;&#23450;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;&#20004;&#20010;&#24212;&#29992;&#22330;&#26223;&#36827;&#34892;&#37327;&#21270;&#27604;&#36739;&#65292;&#22522;&#20110;&#20219;&#21153;&#23884;&#20837;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#65306;&#36890;&#36807;&#35266;&#23519;&#20195;&#29702;&#22312;&#19968;&#23567;&#32452;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#26469;&#39044;&#27979;&#20854;&#22312;&#27979;&#35797;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65307;&#20174;&#32473;&#23450;&#30340;&#20219;&#21153;&#36873;&#39033;&#20013;&#36873;&#25321;&#20855;&#26377;&#25152;&#38656;&#29305;&#24449;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an information-theoretic framework to learn fixed-dimensional embeddings for tasks in reinforcement learning. We leverage the idea that two tasks are similar to each other if observing an agent's performance on one task reduces our uncertainty about its performance on the other. This intuition is captured by our information-theoretic criterion which uses a diverse population of agents to measure similarity between tasks in sequential decision-making settings. In addition to qualitative assessment, we empirically demonstrate the effectiveness of our techniques based on task embeddings by quantitative comparisons against strong baselines on two application scenarios: predicting an agent's performance on a test task by observing its performance on a small quiz of tasks, and selecting tasks with desired characteristics from a given set of options.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#26102;&#38388;&#36923;&#36753;&#65292;&#36890;&#36807;&#20351;&#29992;Krohn&#21644;Rhodes&#30340;&#32423;&#32852;&#29702;&#35770;&#65292;&#25193;&#23637;&#20102;&#36807;&#21435;&#30340;LTL&#34920;&#36798;&#33021;&#21147;&#65292;&#20854;&#20013;&#21253;&#25324;&#21487;&#20197;&#25429;&#33719;&#20854;&#20182;prime automata&#30340;&#26032;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#12290;</title><link>http://arxiv.org/abs/2304.09639</link><description>&lt;p&gt;
Krohn-Rhodes&#36923;&#36753;
&lt;/p&gt;
&lt;p&gt;
The Krohn-Rhodes Logics. (arXiv:2304.09639v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.09639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#26102;&#38388;&#36923;&#36753;&#65292;&#36890;&#36807;&#20351;&#29992;Krohn&#21644;Rhodes&#30340;&#32423;&#32852;&#29702;&#35770;&#65292;&#25193;&#23637;&#20102;&#36807;&#21435;&#30340;LTL&#34920;&#36798;&#33021;&#21147;&#65292;&#20854;&#20013;&#21253;&#25324;&#21487;&#20197;&#25429;&#33719;&#20854;&#20182;prime automata&#30340;&#26032;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#26032;&#30340;&#36807;&#21435;&#30340;&#27169;&#24577;&#26102;&#38388;&#36923;&#36753;&#65292;&#36890;&#36807;&#20351;&#29992;Krohn&#21644;Rhodes&#30340;&#33258;&#21160;&#26426;&#32423;&#32852;&#29702;&#35770;&#65292;&#22522;&#20110;Past LTL&#25193;&#23637;&#19968;&#32452;&#20016;&#23500;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#32780;&#33719;&#24471;&#12290;&#35813;&#29702;&#35770;&#25351;&#20986;&#65292;&#27599;&#20010;&#33258;&#21160;&#26426;&#37117;&#21487;&#20197;&#34920;&#31034;&#20026;&#19968;&#20123;&#31216;&#20026;prime automata&#30340;&#22522;&#26412;&#33258;&#21160;&#26426;&#30340;&#32423;&#32852;&#12290;&#20182;&#20204;&#26159;&#25152;&#26377;&#33258;&#21160;&#26426;&#30340;&#26500;&#24314;&#22359;&#65292;&#31867;&#20284;&#20110;&#36136;&#25968;&#26159;&#25152;&#26377;&#33258;&#28982;&#25968;&#30340;&#26500;&#24314;&#22359;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36807;&#21435;&#30340;LTL&#23545;&#24212;&#20110;&#31216;&#20026;flip-flops&#30340;&#19968;&#31181;prime automata&#30340;&#32423;&#32852;&#12290;&#29305;&#21035;&#22320;&#65292;Past LTL&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#30001;flip-flops&#25429;&#33719;&#65292;&#24182;&#19988;&#23427;&#20204;&#19981;&#33021;&#25429;&#33719;&#20219;&#20309;&#20854;&#20182;prime automata&#65292;&#23558;&#34920;&#36798;&#33021;&#21147;&#38480;&#21046;&#22312;&#26143;&#21495;&#33258;&#30001;&#27491;&#21017;&#35821;&#35328;&#20869;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#26032;&#30340;&#26102;&#38388;&#36816;&#31639;&#31526;&#65292;&#21487;&#20197;&#25429;&#33719;&#20854;&#20182;prime automata&#65292;&#20174;&#32780;&#25193;&#23637;&#20102;Past LTL&#30340;&#34920;&#36798;&#33021;&#21147;&#12290;&#36825;&#20123;&#36816;&#31639;&#31526;&#26159;&#26080;&#31351;&#22810;&#30340;&#65292;&#24182;&#19988;&#23427;&#20204;&#20135;&#29983;&#20102;&#26080;&#38480;&#25968;&#37327;&#30340;&#36923;&#36753;&#65292;&#25429;&#33719;&#20102;&#27491;&#21017;&#35821;&#35328;&#30340;&#26080;&#38480;&#25968;&#37327;&#30340;&#19981;&#21516;&#29255;&#27573;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new family of modal temporal logics of the past, obtained by extending Past LTL with a rich set of temporal operators based on the theory by Krohn and Rhodes for automata cascades. The theory says that every automaton can be expressed as a cascade of some basic automata called prime automata. They are the building blocks of all automata, analogously to prime numbers being the building blocks of all natural numbers. We show that Past LTL corresponds to cascades of one kind of prime automata called flip-flops. In particular, the temporal operators of Past LTL are captured by flip-flops, and they cannot capture any other prime automaton, confining the expressivity within the star-free regular languages. We propose novel temporal operators that can capture other prime automata, and hence extend the expressivity of Past LTL. Such operators are infinitely-many, and they yield an infinite number of logics capturing an infinite number of distinct fragments of the regular languages
&lt;/p&gt;</description></item></channel></rss>