<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>Species196&#26159;&#19968;&#20010;&#21253;&#21547;196&#20010;&#31867;&#21035;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#29289;&#31181;&#35782;&#21035;&#12290;&#23427;&#25552;&#20379;&#20102;&#22235;&#31181;&#23454;&#39564;&#35774;&#32622;&#65292;&#21487;&#20197;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#29616;&#26377;&#27169;&#22411;&#21644;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2309.14183</link><description>&lt;p&gt;
Species196&#65306;&#19968;&#30334;&#19975;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#29992;&#20110;&#32454;&#31890;&#24230;&#29289;&#31181;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Species196: A One-Million Semi-supervised Dataset for Fine-grained Species Recognition. (arXiv:2309.14183v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14183
&lt;/p&gt;
&lt;p&gt;
Species196&#26159;&#19968;&#20010;&#21253;&#21547;196&#20010;&#31867;&#21035;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#32454;&#31890;&#24230;&#29289;&#31181;&#35782;&#21035;&#12290;&#23427;&#25552;&#20379;&#20102;&#22235;&#31181;&#23454;&#39564;&#35774;&#32622;&#65292;&#21487;&#20197;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#29616;&#26377;&#27169;&#22411;&#21644;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#35270;&#35273;&#27169;&#22411;&#30340;&#21457;&#23637;&#65292;&#26222;&#36890;&#35270;&#35273;&#35782;&#21035;&#24050;&#32463;&#36798;&#21040;&#20102;&#19968;&#20010;&#24456;&#39640;&#30340;&#27700;&#24179;&#65292;&#20294;&#26159;&#19981;&#33021;&#24456;&#22909;&#22320;&#22788;&#29702;&#19987;&#38376;&#39046;&#22495;&#20013;&#30340;&#32454;&#31890;&#24230;&#35782;&#21035;&#65292;&#27604;&#22914;&#20837;&#20405;&#29289;&#31181;&#20998;&#31867;&#12290;&#35782;&#21035;&#21644;&#31649;&#29702;&#20837;&#20405;&#29289;&#31181;&#20855;&#26377;&#24456;&#24378;&#30340;&#31038;&#20250;&#21644;&#29983;&#24577;&#20215;&#20540;&#12290;&#30446;&#21069;&#65292;&#22823;&#22810;&#25968;&#20837;&#20405;&#29289;&#31181;&#25968;&#25454;&#38598;&#30340;&#35268;&#27169;&#26377;&#38480;&#65292;&#35206;&#30422;&#30340;&#29289;&#31181;&#33539;&#22260;&#29421;&#31364;&#65292;&#36825;&#38480;&#21046;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20837;&#20405;&#29983;&#29289;&#35745;&#37327;&#31995;&#32479;&#30340;&#21457;&#23637;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#39046;&#22495;&#30340;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Species196&#65292;&#19968;&#20010;&#21253;&#21547;196&#20010;&#31867;&#21035;&#30340;&#22823;&#35268;&#27169;&#21322;&#30417;&#30563;&#25968;&#25454;&#38598;&#12290;&#23427;&#25910;&#38598;&#20102;&#36229;&#36807;19K&#24102;&#26377;&#19987;&#23478;&#32423;&#20934;&#30830;&#27880;&#37322;&#30340;&#22270;&#20687;Species196-L&#65292;&#20197;&#21450;120&#19975;&#24352;&#26410;&#26631;&#35760;&#30340;&#20837;&#20405;&#29289;&#31181;&#22270;&#20687;Species196-U&#12290;&#35813;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#22235;&#31181;&#23454;&#39564;&#35774;&#32622;&#65292;&#29992;&#20110;&#22522;&#20934;&#27979;&#35797;&#29616;&#26377;&#30340;&#27169;&#22411;&#21644;&#31639;&#27861;&#65292;&#20998;&#21035;&#26159;&#30417;&#30563;&#23398;&#20064;&#12289;&#21322;&#30417;&#30563;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The development of foundation vision models has pushed the general visual recognition to a high level, but cannot well address the fine-grained recognition in specialized domain such as invasive species classification. Identifying and managing invasive species has strong social and ecological value. Currently, most invasive species datasets are limited in scale and cover a narrow range of species, which restricts the development of deep-learning based invasion biometrics systems. To fill the gap of this area, we introduced Species196, a large-scale semi-supervised dataset of 196-category invasive species. It collects over 19K images with expert-level accurate annotations Species196-L, and 1.2M unlabeled images of invasive species Species196-U. The dataset provides four experimental settings for benchmarking the existing models and algorithms, namely, supervised learning, semi-supervised learning, self-supervised pretraining and zero-shot inference ability of large multi-modal models. T
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#24490;&#29615;&#27169;&#22411;&#32467;&#21512;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;PO&#36830;&#32493;&#25511;&#21046;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#23545;&#20110;&#19981;&#35268;&#21017;&#35266;&#23519;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.14078</link><description>&lt;p&gt;
&#22522;&#20110;ODE&#30340;&#26080;&#27169;&#22411;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;POMDPs
&lt;/p&gt;
&lt;p&gt;
ODE-based Recurrent Model-free Reinforcement Learning for POMDPs. (arXiv:2309.14078v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14078
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;ODE&#30340;&#24490;&#29615;&#27169;&#22411;&#32467;&#21512;&#26080;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#26469;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;PO&#36830;&#32493;&#25511;&#21046;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#23545;&#20110;&#19981;&#35268;&#21017;&#35266;&#23519;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#24120;&#24494;&#20998;&#26041;&#31243;&#65288;ODEs&#65289;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#24314;&#27169;&#29289;&#29702;&#26426;&#21046;&#30340;&#26631;&#20934;&#65292;&#23427;&#20204;&#26377;&#21161;&#20110;&#22312;&#26410;&#30693;&#30340;&#29289;&#29702;&#25110;&#29983;&#29289;&#29615;&#22659;&#20013;&#36827;&#34892;&#36817;&#20284;&#25512;&#26029;&#12290;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#65288;PO&#65289;&#29615;&#22659;&#20013;&#65292;&#22914;&#20309;&#20174;&#21407;&#22987;&#35266;&#27979;&#20013;&#25512;&#26029;&#30475;&#19981;&#35265;&#30340;&#20449;&#24687;&#22256;&#25200;&#30528;&#20195;&#29702;&#20154;&#12290;&#36890;&#36807;&#20351;&#29992;&#20855;&#26377;&#32039;&#20945;&#19978;&#19979;&#25991;&#30340;&#24490;&#29615;&#31574;&#30053;&#65292;&#22522;&#20110;&#19978;&#19979;&#25991;&#30340;&#24378;&#21270;&#23398;&#20064;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#30340;&#26041;&#27861;&#26469;&#20174;&#21382;&#21490;&#36716;&#25442;&#20013;&#25552;&#21462;&#19981;&#21487;&#35266;&#23519;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#24110;&#21161;&#20195;&#29702;&#20154;&#25552;&#21462;&#26356;&#22810;&#19982;&#21160;&#24577;&#30456;&#20851;&#30340;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;ODE&#30340;&#24490;&#29615;&#27169;&#22411;&#65292;&#24182;&#32467;&#21512;&#20102;&#26080;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#26469;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;POMDPs&#65289;&#12290;&#25105;&#20204;&#36890;&#36807;&#21508;&#31181;PO&#36830;&#32493;&#25511;&#21046;&#21644;&#20803;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#23454;&#39564;&#39564;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#30001;&#20110;ODEs&#20855;&#26377;&#24314;&#27169;&#19981;&#35268;&#21017;&#24615;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23545;&#20110;&#19981;&#35268;&#21017;&#35266;&#23519;&#20855;&#26377;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural ordinary differential equations (ODEs) are widely recognized as the standard for modeling physical mechanisms, which help to perform approximate inference in unknown physical or biological environments. In partially observable (PO) environments, how to infer unseen information from raw observations puzzled the agents. By using a recurrent policy with a compact context, context-based reinforcement learning provides a flexible way to extract unobservable information from historical transitions. To help the agent extract more dynamics-related information, we present a novel ODE-based recurrent model combines with model-free reinforcement learning (RL) framework to solve partially observable Markov decision processes (POMDPs). We experimentally demonstrate the efficacy of our methods across various PO continuous control and meta-RL tasks. Furthermore, our experiments illustrate that our method is robust against irregular observations, owing to the ability of ODEs to model irregularl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SpikingNeRF&#65292;&#23427;&#36890;&#36807;&#23558;&#36752;&#23556;&#20809;&#32447;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#32500;&#24230;&#23545;&#40784;&#65292;&#20197;&#33410;&#30465;&#33021;&#37327;&#24182;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;</title><link>http://arxiv.org/abs/2309.10987</link><description>&lt;p&gt;
Spiking NeRF&#65306;&#20351;&#29983;&#29289;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#31359;&#36879;&#29616;&#23454;&#19990;&#30028;
&lt;/p&gt;
&lt;p&gt;
Spiking NeRF: Making Bio-inspired Neural Networks See through the Real World. (arXiv:2309.10987v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10987
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SpikingNeRF&#65292;&#23427;&#36890;&#36807;&#23558;&#36752;&#23556;&#20809;&#32447;&#19982;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#26102;&#38388;&#32500;&#24230;&#23545;&#40784;&#65292;&#20197;&#33410;&#30465;&#33021;&#37327;&#24182;&#20943;&#23569;&#35745;&#31639;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#21033;&#29992;&#20854;&#20855;&#26377;&#28508;&#22312;&#29983;&#29289;&#23398;&#21487;&#34892;&#24615;&#30340;&#33021;&#37327;&#25928;&#29575;&#21644;&#28508;&#21147;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#20197;&#22823;&#37327;&#33021;&#37327;&#28040;&#32791;&#28210;&#26579;&#39640;&#36136;&#37327;&#30340;3D&#22330;&#26223;&#65292;&#20294;&#24456;&#23569;&#26377;&#30740;&#31350;&#28145;&#20837;&#25506;&#32034;&#20197;&#29983;&#29289;&#21551;&#21457;&#30340;&#26041;&#27861;&#36827;&#34892;&#33410;&#33021;&#35299;&#20915;&#26041;&#26696;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#33033;&#20914;NeRF&#65288;SpikingNeRF&#65289;&#65292;&#23558;&#36752;&#23556;&#20809;&#32447;&#19982;SNN&#30340;&#26102;&#38388;&#32500;&#24230;&#23545;&#40784;&#65292;&#20197;&#33258;&#28982;&#22320;&#36866;&#24212;SNN&#23545;&#36752;&#23556;&#22330;&#30340;&#37325;&#24314;&#12290;&#22240;&#27492;&#65292;&#35745;&#31639;&#20197;&#22522;&#20110;&#33033;&#20914;&#12289;&#26080;&#20056;&#27861;&#30340;&#26041;&#24335;&#36827;&#34892;&#65292;&#20174;&#32780;&#20943;&#23569;&#33021;&#37327;&#28040;&#32791;&#12290;&#22312;SpikingNeRF&#20013;&#65292;&#20809;&#32447;&#19978;&#30340;&#27599;&#20010;&#37319;&#26679;&#28857;&#21305;&#37197;&#21040;&#29305;&#23450;&#30340;&#26102;&#38388;&#27493;&#65292;&#24182;&#20197;&#28151;&#21512;&#26041;&#24335;&#34920;&#31034;&#65292;&#20854;&#20013;&#20307;&#32032;&#32593;&#26684;&#20063;&#24471;&#21040;&#32500;&#25252;&#12290;&#22522;&#20110;&#20307;&#32032;&#32593;&#26684;&#65292;&#30830;&#23450;&#37319;&#26679;&#28857;&#26159;&#21542;&#22312;&#35757;&#32451;&#21644;&#25512;&#26029;&#36807;&#31243;&#20013;&#34987;&#23631;&#34109;&#20197;&#36827;&#34892;&#26356;&#22909;&#30340;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#25805;&#20316;&#20063;&#20250;&#20135;&#29983;&#19981;&#21487;&#36870;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking neuron networks (SNNs) have been thriving on numerous tasks to leverage their promising energy efficiency and exploit their potentialities as biologically plausible intelligence. Meanwhile, the Neural Radiance Fields (NeRF) render high-quality 3D scenes with massive energy consumption, and few works delve into the energy-saving solution with a bio-inspired approach. In this paper, we propose spiking NeRF (SpikingNeRF), which aligns the radiance ray with the temporal dimension of SNN, to naturally accommodate the SNN to the reconstruction of Radiance Fields. Thus, the computation turns into a spike-based, multiplication-free manner, reducing the energy consumption. In SpikingNeRF, each sampled point on the ray is matched onto a particular time step, and represented in a hybrid manner where the voxel grids are maintained as well. Based on the voxel grids, sampled points are determined whether to be masked for better training and inference. However, this operation also incurs irre
&lt;/p&gt;</description></item><item><title>Data Formulator&#26159;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#35270;&#21270;&#21019;&#20316;&#24037;&#20855;&#65292;&#37319;&#29992;&#27010;&#24565;&#32465;&#23450;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#39640;&#32423;&#21487;&#35270;&#21270;&#24847;&#22270;&#21644;&#20302;&#32423;&#25968;&#25454;&#36716;&#25442;&#27493;&#39588;&#20998;&#31163;&#65292;&#21033;&#29992;AI&#20195;&#29702;&#33258;&#21160;&#36716;&#25442;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#25152;&#38656;&#30340;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2309.10094</link><description>&lt;p&gt;
&#25968;&#25454;&#20844;&#24335;&#21270;: &#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#39537;&#21160;&#30340;&#27010;&#24565;&#39537;&#21160;&#21487;&#35270;&#21270;&#21019;&#20316;
&lt;/p&gt;
&lt;p&gt;
Data Formulator: AI-powered Concept-driven Visualization Authoring. (arXiv:2309.10094v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.10094
&lt;/p&gt;
&lt;p&gt;
Data Formulator&#26159;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#35270;&#21270;&#21019;&#20316;&#24037;&#20855;&#65292;&#37319;&#29992;&#27010;&#24565;&#32465;&#23450;&#33539;&#24335;&#65292;&#36890;&#36807;&#23558;&#39640;&#32423;&#21487;&#35270;&#21270;&#24847;&#22270;&#21644;&#20302;&#32423;&#25968;&#25454;&#36716;&#25442;&#27493;&#39588;&#20998;&#31163;&#65292;&#21033;&#29992;AI&#20195;&#29702;&#33258;&#21160;&#36716;&#25442;&#36755;&#20837;&#25968;&#25454;&#65292;&#24182;&#29983;&#25104;&#25152;&#38656;&#30340;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#29616;&#20195;&#21487;&#35270;&#21270;&#24037;&#20855;&#38656;&#35201;&#20316;&#32773;&#23558;&#20854;&#25968;&#25454;&#36716;&#25442;&#20026;&#25972;&#27905;&#26684;&#24335;&#20197;&#21019;&#24314;&#25152;&#38656;&#30340;&#21487;&#35270;&#21270;&#12290;&#30001;&#20110;&#36825;&#38656;&#35201;&#20855;&#22791;&#32534;&#31243;&#25110;&#21333;&#29420;&#30340;&#25968;&#25454;&#22788;&#29702;&#24037;&#20855;&#30340;&#32463;&#39564;&#65292;&#25968;&#25454;&#36716;&#25442;&#20173;&#28982;&#26159;&#21487;&#35270;&#21270;&#21019;&#20316;&#30340;&#19968;&#20010;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#35270;&#21270;&#33539;&#24335;&#65292;&#21363;&#27010;&#24565;&#32465;&#23450;&#65292;&#23427;&#23558;&#39640;&#32423;&#21487;&#35270;&#21270;&#24847;&#22270;&#21644;&#20302;&#32423;&#25968;&#25454;&#36716;&#25442;&#27493;&#39588;&#20998;&#24320;&#65292;&#24182;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#22312;Data Formulator&#20013;&#23454;&#29616;&#20102;&#36825;&#20010;&#33539;&#24335;&#65292;&#36825;&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#30340;&#21487;&#35270;&#21270;&#21019;&#20316;&#24037;&#20855;&#12290;&#20351;&#29992;Data Formulator&#65292;&#20316;&#32773;&#39318;&#20808;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#25110;&#31034;&#20363;&#23450;&#20041;&#20182;&#20204;&#35745;&#21010;&#21487;&#35270;&#21270;&#30340;&#25968;&#25454;&#27010;&#24565;&#65292;&#28982;&#21518;&#23558;&#23427;&#20204;&#32465;&#23450;&#21040;&#21487;&#35270;&#36890;&#36947;&#12290;Data Formulator&#28982;&#21518;&#23558;&#20854;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#27966;&#36963;&#20986;&#21435;&#65292;&#33258;&#21160;&#23558;&#36755;&#20837;&#25968;&#25454;&#36716;&#25442;&#20026;&#21576;&#29616;&#36825;&#20123;&#27010;&#24565;&#21644;&#29983;&#25104;&#25152;&#38656;&#21487;&#35270;&#21270;&#30340;&#25968;&#25454;&#12290;&#22312;&#21521;Data Formulator&#23637;&#31034;&#26469;&#33258;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#32467;&#26524;(&#36716;&#25442;&#21518;&#30340;&#34920;&#26684;&#21644;&#36755;&#20986;&#30340;&#21487;&#35270;&#21270;)&#26102;&#65292;Data Formulator&#25552;&#20379;&#21453;&#39304;&#12290;
&lt;/p&gt;
&lt;p&gt;
With most modern visualization tools, authors need to transform their data into tidy formats to create visualizations they want. Because this requires experience with programming or separate data processing tools, data transformation remains a barrier in visualization authoring. To address this challenge, we present a new visualization paradigm, concept binding, that separates high-level visualization intents and low-level data transformation steps, leveraging an AI agent. We realize this paradigm in Data Formulator, an interactive visualization authoring tool. With Data Formulator, authors first define data concepts they plan to visualize using natural languages or examples, and then bind them to visual channels. Data Formulator then dispatches its AI-agent to automatically transform the input data to surface these concepts and generate desired visualizations. When presenting the results (transformed table and output visualizations) from the AI agent, Data Formulator provides feedback
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#31616;&#21270;&#30340;&#38598;&#21512;&#32467;&#26500;BEA&#29992;&#20110;&#38170;&#28857;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#26657;&#20934;&#19982;&#38477;&#20302;&#19981;&#30830;&#23450;&#24615;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2309.08036</link><description>&lt;p&gt;
BEA: &#37325;&#26032;&#23457;&#35270;&#20351;&#29992;Budding Ensemble Architecture&#30340;&#22522;&#20110;&#38170;&#28857;&#30340;&#30446;&#26631;&#26816;&#27979;DNN
&lt;/p&gt;
&lt;p&gt;
BEA: Revisiting anchor-based object detection DNN using Budding Ensemble Architecture. (arXiv:2309.08036v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08036
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#31616;&#21270;&#30340;&#38598;&#21512;&#32467;&#26500;BEA&#29992;&#20110;&#38170;&#28857;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#65292;&#24182;&#36890;&#36807;&#25913;&#36827;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#26657;&#20934;&#19982;&#38477;&#20302;&#19981;&#30830;&#23450;&#24615;&#35823;&#24046;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Budding Ensemble Architecture&#65288;BEA&#65289;&#65292;&#19968;&#31181;&#29992;&#20110;&#38170;&#28857;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#26032;&#22411;&#31616;&#21270;&#38598;&#21512;&#32467;&#26500;&#12290;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#20219;&#21153;&#20013;&#38750;&#24120;&#37325;&#35201;&#65292;&#29305;&#21035;&#26159;&#22312;&#33258;&#20027;&#31995;&#32479;&#20013;&#12290;&#23427;&#20204;&#24212;&#35813;&#25552;&#20379;&#31934;&#30830;&#30340;&#36793;&#30028;&#26694;&#26816;&#27979;&#65292;&#24182;&#26657;&#20934;&#20854;&#39044;&#27979;&#30340;&#32622;&#20449;&#24230;&#24471;&#20998;&#65292;&#20197;&#33719;&#24471;&#26356;&#39640;&#36136;&#37327;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20551;&#38451;&#24615;&#25509;&#25910;&#21040;&#39640;&#20998;&#25110;&#30495;&#38451;&#24615;&#30001;&#20110;&#20302;&#20998;&#32780;&#34987;&#20002;&#24323;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21487;&#33021;&#20250;&#20570;&#20986;&#38169;&#35823;&#30340;&#20915;&#31574;&#12290;BEA&#26088;&#22312;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;BEA&#30340;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#25913;&#21892;&#20102;&#32622;&#20449;&#24230;&#24471;&#20998;&#30340;&#26657;&#20934;&#21644;&#38477;&#20302;&#20102;&#19981;&#30830;&#23450;&#24615;&#35823;&#24046;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#21306;&#20998;&#30495;&#38451;&#24615;&#21644;&#20551;&#38451;&#24615;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;&#20351;&#29992;BEA&#26041;&#27861;&#21644;&#20854;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#23545;Base-YOLOv3&#21644;SSD&#27169;&#22411;&#36827;&#34892;&#20102;&#25913;&#36827;&#12290;BEA&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;Base-YOLOv3&#32467;&#26524;&#20013;&#65292;&#31934;&#24230;&#20998;&#21035;&#25552;&#39640;&#20102;6%&#21644;3.7%&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces the Budding Ensemble Architecture (BEA), a novel reduced ensemble architecture for anchor-based object detection models. Object detection models are crucial in vision-based tasks, particularly in autonomous systems. They should provide precise bounding box detections while also calibrating their predicted confidence scores, leading to higher-quality uncertainty estimates. However, current models may make erroneous decisions due to false positives receiving high scores or true positives being discarded due to low scores. BEA aims to address these issues. The proposed loss functions in BEA improve the confidence score calibration and lower the uncertainty error, which results in a better distinction of true and false positives and, eventually, higher accuracy of the object detection models. Both Base-YOLOv3 and SSD models were enhanced using the BEA method and its proposed loss functions. The BEA on Base-YOLOv3 trained on the KITTI dataset results in a 6% and 3.7% i
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#20013;&#30340;&#24494;&#22937;&#36873;&#25321;&#65292;&#29305;&#21035;&#20851;&#27880;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#21457;&#29616;&#31232;&#30095;&#21021;&#22987;&#21270;&#21644;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19988;&#21512;&#25104;&#31232;&#30095;&#22855;&#20598;&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#30495;&#23454;&#38382;&#39064;&#30340;&#20195;&#29702;&#12290;</title><link>http://arxiv.org/abs/2309.03800</link><description>&lt;p&gt;
&#31070;&#32463;&#29305;&#24449;&#23398;&#20064;&#20013;&#30340;&#24085;&#32047;&#25176;&#21069;&#27839;&#65306;&#25968;&#25454;&#12289;&#35745;&#31639;&#12289;&#23485;&#24230;&#21644;&#36816;&#27668;
&lt;/p&gt;
&lt;p&gt;
Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck. (arXiv:2309.03800v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.03800
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#35774;&#35745;&#20013;&#30340;&#24494;&#22937;&#36873;&#25321;&#65292;&#29305;&#21035;&#20851;&#27880;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#65292;&#21457;&#29616;&#31232;&#30095;&#21021;&#22987;&#21270;&#21644;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#21487;&#20197;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#65292;&#24182;&#19988;&#21512;&#25104;&#31232;&#30095;&#22855;&#20598;&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#30495;&#23454;&#38382;&#39064;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#35745;&#31639;&#32479;&#35745;&#24046;&#36317;&#23384;&#22312;&#30340;&#24773;&#20917;&#19979;&#65292;&#28145;&#24230;&#23398;&#20064;&#20013;&#24494;&#22937;&#30340;&#31639;&#27861;&#35774;&#35745;&#36873;&#25321;&#12290;&#25105;&#20204;&#39318;&#20808;&#32771;&#34385;&#20102;&#31163;&#32447;&#31232;&#30095;&#22855;&#20598;&#23398;&#20064;&#65292;&#36825;&#26159;&#19968;&#20010;&#26377;&#20851;&#22810;&#23618;&#24863;&#30693;&#22120;&#26799;&#24230;&#35757;&#32451;&#30340;&#30417;&#30563;&#20998;&#31867;&#38382;&#39064;&#65292;&#20854;&#20855;&#26377;&#32479;&#35745;&#26597;&#35810;&#19979;&#30028;&#12290;&#36825;&#20010;&#19979;&#30028;&#21487;&#20197;&#35299;&#37322;&#20026;&#22810;&#36164;&#28304;&#30340;&#26435;&#34913;&#21069;&#27839;&#65306;&#25104;&#21151;&#23398;&#20064;&#21482;&#26377;&#22312;&#19968;&#20010;&#36275;&#22815;&#20016;&#23500;&#65288;&#22823;&#22411;&#27169;&#22411;&#65289;&#12289;&#30693;&#35782;&#28170;&#21338;&#65288;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65289;&#12289;&#32784;&#24515;&#65288;&#35757;&#32451;&#36845;&#20195;&#27425;&#25968;&#22810;&#65289;&#25110;&#24184;&#36816;&#65288;&#38543;&#26426;&#29468;&#27979;&#27425;&#25968;&#22810;&#65289;&#30340;&#24773;&#20917;&#19979;&#25165;&#33021;&#21457;&#29983;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#31232;&#30095;&#21021;&#22987;&#21270;&#21644;&#22686;&#21152;&#32593;&#32476;&#23485;&#24230;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#26679;&#26412;&#25928;&#29575;&#12290;&#22312;&#36825;&#37324;&#65292;&#23485;&#24230;&#36215;&#21040;&#20102;&#24182;&#34892;&#25628;&#32034;&#30340;&#20316;&#29992;&#65306;&#23427;&#22686;&#21152;&#20102;&#25214;&#21040;&#8220;&#24184;&#36816;&#31070;&#32463;&#20803;&#8221;&#30340;&#27010;&#29575;&#65292;&#36825;&#20123;&#31070;&#32463;&#20803;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#31232;&#30095;&#29305;&#24449;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#34920;&#26126;&#21512;&#25104;&#31232;&#30095;&#22855;&#20598;&#20219;&#21153;&#21487;&#20197;&#20316;&#20026;&#30495;&#23454;&#38382;&#39064;&#30340;&#20195;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work investigates the nuanced algorithm design choices for deep learning in the presence of computational-statistical gaps. We begin by considering offline sparse parity learning, a supervised classification problem which admits a statistical query lower bound for gradient-based training of a multilayer perceptron. This lower bound can be interpreted as a multi-resource tradeoff frontier: successful learning can only occur if one is sufficiently rich (large model), knowledgeable (large dataset), patient (many training iterations), or lucky (many random guesses). We show, theoretically and experimentally, that sparse initialization and increasing network width yield significant improvements in sample efficiency in this setting. Here, width plays the role of parallel search: it amplifies the probability of finding "lottery ticket" neurons, which learn sparse features more sample-efficiently. Finally, we show that the synthetic sparse parity task can be useful as a proxy for real pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DIAM&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#26816;&#27979;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#24182;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#65292;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2309.02460</link><description>&lt;p&gt;
&#26377;&#25928;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Effective Multi-Graph Neural Networks for Illicit Account Detection on Cryptocurrency Transaction Networks. (arXiv:2309.02460v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.02460
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;DIAM&#65292;&#29992;&#20110;&#26377;&#25928;&#22320;&#26816;&#27979;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#12290;&#35813;&#27169;&#22411;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#33410;&#28857;&#34920;&#31034;&#24182;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#65292;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#32447;&#37329;&#34701;&#24066;&#22330;&#20013;&#26085;&#30410;&#37325;&#35201;&#30340;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#32593;&#32476;&#19978;&#30340;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;&#12290;&#22312;&#21152;&#23494;&#36135;&#24065;&#19978;&#30340;&#38750;&#27861;&#27963;&#21160;&#28608;&#22686;&#23548;&#33268;&#20102;&#26222;&#36890;&#29992;&#25143;&#25968;&#21313;&#20159;&#30340;&#25439;&#22833;&#12290;&#29616;&#26377;&#30340;&#35299;&#20915;&#26041;&#26696;&#35201;&#20040;&#20381;&#36182;&#20110;&#32321;&#29712;&#30340;&#29305;&#24449;&#24037;&#31243;&#26469;&#33719;&#24471;&#25163;&#24037;&#29305;&#24449;&#65292;&#35201;&#20040;&#19981;&#33021;&#20805;&#20998;&#21033;&#29992;&#21152;&#23494;&#36135;&#24065;&#20132;&#26131;&#25968;&#25454;&#20013;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#20174;&#32780;&#23548;&#33268;&#20122;&#20248;&#21270;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#38750;&#27861;&#36134;&#25143;&#26816;&#27979;&#38382;&#39064;&#23450;&#20041;&#20026;&#24102;&#26377;&#36793;&#23646;&#24615;&#30340;&#26377;&#21521;&#22810;&#22270;&#19978;&#30340;&#20998;&#31867;&#20219;&#21153;&#65292;&#24182;&#25552;&#20986;&#20102;DIAM&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#22823;&#22411;&#20132;&#26131;&#32593;&#32476;&#19978;&#26377;&#25928;&#22320;&#26816;&#27979;&#38750;&#27861;&#36134;&#25143;&#12290;&#39318;&#20808;&#65292;DIAM&#21253;&#21547;&#19968;&#20010;Edge2Seq&#27169;&#22359;&#65292;&#36890;&#36807;&#21516;&#26102;&#32771;&#34385;&#36793;&#23646;&#24615;&#21644;&#26377;&#21521;&#36793;&#24207;&#21015;&#20381;&#36182;&#20851;&#31995;&#65292;&#33258;&#21160;&#23398;&#20064;&#26377;&#25928;&#30340;&#33410;&#28857;&#34920;&#31034;&#65292;&#20445;&#30041;&#24179;&#34892;&#36793;&#30340;&#20869;&#22312;&#20132;&#26131;&#27169;&#24335;&#12290;&#28982;&#21518;&#21033;&#29992;t
&lt;/p&gt;
&lt;p&gt;
We study illicit account detection on transaction networks of cryptocurrencies that are increasi_testngly important in online financial markets. The surge of illicit activities on cryptocurrencies has resulted in billions of losses from normal users. Existing solutions either rely on tedious feature engineering to get handcrafted features, or are inadequate to fully utilize the rich semantics of cryptocurrency transaction data, and consequently, yield sub-optimal performance. In this paper, we formulate the illicit account detection problem as a classification task over directed multigraphs with edge attributes, and present DIAM, a novel multi-graph neural network model to effectively detect illicit accounts on large transaction networks. First, DIAM includes an Edge2Seq module that automatically learns effective node representations preserving intrinsic transaction patterns of parallel edges, by considering both edge attributes and directed edge sequence dependencies. Then utilizing t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20511;&#37492;&#20154;&#31867;&#30340;&#35282;&#33394;&#20998;&#37197;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#34892;&#21160;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#19968;&#20010;&#25163;&#33218;&#29992;&#20110;&#22266;&#23450;&#29289;&#20307;&#65292;&#21478;&#19968;&#20010;&#25163;&#33218;&#29992;&#20110;&#25191;&#34892;&#20219;&#21153;&#12290;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#31283;&#23450;&#20998;&#31867;&#22120;&#21644;&#34892;&#21160;&#31574;&#30053;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#19968;&#26694;&#26550;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;</title><link>http://arxiv.org/abs/2309.01087</link><description>&lt;p&gt;
&#31283;&#23450;&#34892;&#21160;&#65306;&#23398;&#20064;&#21327;&#35843;&#21452;&#25163;&#25805;&#20316;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stabilize to Act: Learning to Coordinate for Bimanual Manipulation. (arXiv:2309.01087v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01087
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20511;&#37492;&#20154;&#31867;&#30340;&#35282;&#33394;&#20998;&#37197;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31283;&#23450;&#34892;&#21160;&#30340;&#26694;&#26550;&#65292;&#20854;&#20013;&#19968;&#20010;&#25163;&#33218;&#29992;&#20110;&#22266;&#23450;&#29289;&#20307;&#65292;&#21478;&#19968;&#20010;&#25163;&#33218;&#29992;&#20110;&#25191;&#34892;&#20219;&#21153;&#12290;&#36890;&#36807;&#23398;&#20064;&#21040;&#30340;&#31283;&#23450;&#20998;&#31867;&#22120;&#21644;&#34892;&#21160;&#31574;&#30053;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;&#36825;&#19968;&#26694;&#26550;&#24182;&#23545;&#20854;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#36827;&#34892;&#20016;&#23500;&#21644;&#28789;&#24039;&#30340;&#25805;&#20316;&#30340;&#20851;&#38190;&#26159;&#33021;&#22815;&#21327;&#35843;&#25511;&#21046;&#20004;&#21482;&#25163;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#21452;&#25163;&#26426;&#22120;&#20154;&#31995;&#32479;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#21069;&#26223;&#65292;&#20294;&#26500;&#24314;&#21452;&#33218;&#33258;&#20027;&#31995;&#32479;&#30340;&#25511;&#21046;&#31574;&#30053;&#21364;&#38754;&#20020;&#38590;&#39064;&#12290;&#20854;&#20013;&#19968;&#20010;&#22256;&#38590;&#26159;&#21452;&#25163;&#21160;&#20316;&#31354;&#38388;&#30340;&#39640;&#32500;&#24230;&#65292;&#36825;&#32473;&#22522;&#20110;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#22686;&#21152;&#20102;&#22797;&#26434;&#24615;&#12290;&#25105;&#20204;&#20174;&#20154;&#31867;&#36523;&#19978;&#24471;&#21040;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35282;&#33394;&#20998;&#37197;&#26694;&#26550;&#26469;&#24212;&#23545;&#36825;&#19968;&#25361;&#25112;&#65306;&#19968;&#20010;&#31283;&#23450;&#30340;&#33218;&#23558;&#29289;&#20307;&#22266;&#23450;&#22312;&#19968;&#20010;&#20301;&#32622;&#19978;&#65292;&#31616;&#21270;&#29615;&#22659;&#65292;&#32780;&#19968;&#20010;&#27963;&#21160;&#30340;&#33218;&#21017;&#25191;&#34892;&#20219;&#21153;&#12290;&#25105;&#20204;&#21033;&#29992;&#23398;&#20064;&#21040;&#30340;&#31283;&#23450;&#20998;&#31867;&#22120;&#26469;&#23454;&#26045; BimanUal Dexterity from Stabilization (BUDS) &#26694;&#26550;&#65292;&#35813;&#20998;&#31867;&#22120;&#20132;&#26367;&#26356;&#26032;&#23398;&#20064;&#21040;&#30340;&#31283;&#23450;&#20301;&#32622;&#20197;&#20445;&#25345;&#29615;&#22659;&#31283;&#23450;&#65292;&#24182;&#21033;&#29992;&#31034;&#33539;&#23398;&#20064;&#21040;&#30340;&#34892;&#21160;&#31574;&#30053;&#23436;&#25104;&#20219;&#21153;&#12290;&#25105;&#20204;&#23545;BUDS&#36827;&#34892;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#21452;&#25163;&#20219;&#21153;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Key to rich, dexterous manipulation in the real world is the ability to coordinate control across two hands. However, while the promise afforded by bimanual robotic systems is immense, constructing control policies for dual arm autonomous systems brings inherent difficulties. One such difficulty is the high-dimensionality of the bimanual action space, which adds complexity to both model-based and data-driven methods. We counteract this challenge by drawing inspiration from humans to propose a novel role assignment framework: a stabilizing arm holds an object in place to simplify the environment while an acting arm executes the task. We instantiate this framework with BimanUal Dexterity from Stabilization (BUDS), which uses a learned restabilizing classifier to alternate between updating a learned stabilization position to keep the environment unchanged, and accomplishing the task with an acting policy learned from demonstrations. We evaluate BUDS on four bimanual tasks of varying compl
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMILIE&#30340;&#36845;&#20195;&#22810;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#36845;&#20195;&#32534;&#36753;&#65292;&#24182;&#24341;&#20837;&#26799;&#24230;&#25511;&#21046;&#25805;&#20316;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#32534;&#36753;&#30340;&#31890;&#24230;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2309.00613</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#30340;&#22810;&#31890;&#24230;&#36845;&#20195;&#22270;&#20687;&#32534;&#36753;
&lt;/p&gt;
&lt;p&gt;
Iterative Multi-granular Image Editing using Diffusion Models. (arXiv:2309.00613v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.00613
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;EMILIE&#30340;&#36845;&#20195;&#22810;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#22120;&#65292;&#36890;&#36807;&#37325;&#26032;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#23454;&#29616;&#36845;&#20195;&#32534;&#36753;&#65292;&#24182;&#24341;&#20837;&#26799;&#24230;&#25511;&#21046;&#25805;&#20316;&#26469;&#23454;&#29616;&#23545;&#22270;&#20687;&#32534;&#36753;&#30340;&#31890;&#24230;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25991;&#26412;&#24341;&#23548;&#30340;&#22270;&#20687;&#21512;&#25104;&#30340;&#36827;&#23637;&#26497;&#22823;&#22320;&#25913;&#21464;&#20102;&#21019;&#24847;&#19987;&#19994;&#20154;&#21592;&#29983;&#25104;&#33402;&#26415;&#21644;&#23457;&#32654;&#19978;&#20196;&#20154;&#24841;&#24742;&#30340;&#35270;&#35273;&#36164;&#20135;&#30340;&#26041;&#24335;&#12290;&#20026;&#20102;&#20805;&#20998;&#25903;&#25345;&#36825;&#26679;&#30340;&#21019;&#24847;&#21162;&#21147;&#65292;&#35813;&#36807;&#31243;&#24212;&#20855;&#22791;&#20197;&#19979;&#33021;&#21147;&#65306;1&#65289;&#36845;&#20195;&#22320;&#32534;&#36753;&#29983;&#25104;&#30340;&#22270;&#20687;&#65292;2&#65289;&#25511;&#21046;&#25152;&#38656;&#21464;&#21270;&#30340;&#31354;&#38388;&#33539;&#22260;&#65288;&#20840;&#23616;&#12289;&#23616;&#37096;&#25110;&#20171;&#20110;&#20004;&#32773;&#20043;&#38388;&#65289;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#23454;&#29992;&#30340;&#38382;&#39064;&#35774;&#23450;&#27491;&#24335;&#21270;&#20026;&#36845;&#20195;&#22810;&#31890;&#24230;&#32534;&#36753;&#12290;&#34429;&#28982;&#22312;&#22270;&#20687;&#21512;&#25104;&#21644;&#32534;&#36753;&#26041;&#38754;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#37117;&#26159;&#19968;&#27425;&#24615;&#25805;&#20316;&#65288;&#21363;&#27809;&#26377;&#36845;&#20195;&#32534;&#36753;&#33021;&#21147;&#65289;&#65292;&#24182;&#19988;&#19981;&#33021;&#33258;&#28982;&#20135;&#29983;&#22810;&#31890;&#24230;&#25511;&#21046;&#65288;&#21363;&#28085;&#30422;&#20174;&#23616;&#37096;&#21040;&#20840;&#23616;&#32534;&#36753;&#30340;&#20840;&#35889;&#65289;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#32570;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;EMILIE&#65306;&#36845;&#20195;&#22810;&#31890;&#24230;&#22270;&#20687;&#32534;&#36753;&#22120;&#12290;EMILIE&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28508;&#22312;&#36845;&#20195;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#20419;&#36827;&#36845;&#20195;&#32534;&#36753;&#12290;&#21516;&#26102;&#65292;&#36824;&#24341;&#20837;&#20102;&#26799;&#24230;&#25511;&#21046;&#25805;&#20316;&#26469;&#23454;&#29616;&#23545;&#25152;&#38656;&#21464;&#21270;&#30340;&#31890;&#24230;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in text-guided image synthesis has dramatically changed how creative professionals generate artistic and aesthetically pleasing visual assets. To fully support such creative endeavors, the process should possess the ability to: 1) iteratively edit the generations and 2) control the spatial reach of desired changes (global, local or anything in between). We formalize this pragmatic problem setting as Iterative Multi-granular Editing. While there has been substantial progress with diffusion-based models for image synthesis and editing, they are all one shot (i.e., no iterative editing capabilities) and do not naturally yield multi-granular control (i.e., covering the full spectrum of local-to-global edits). To overcome these drawbacks, we propose EMILIE: Iterative Multi-granular Image Editor. EMILIE introduces a novel latent iteration strategy, which re-purposes a pre-trained diffusion model to facilitate iterative editing. This is complemented by a gradient control opera
&lt;/p&gt;</description></item><item><title>NAS-X&#26159;&#19968;&#31181;&#22522;&#20110;&#25197;&#26354;&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;&#21796;&#37266;-&#30561;&#30496;&#31639;&#27861;&#26469;&#23398;&#20064;&#21644;&#25512;&#26029;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#25512;&#26029;&#21644;&#21442;&#25968;&#24674;&#22797;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.14864</link><description>&lt;p&gt;
NAS-X: &#22522;&#20110;&#25197;&#26354;&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
NAS-X: Neural Adaptive Smoothing via Twisting. (arXiv:2308.14864v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.14864
&lt;/p&gt;
&lt;p&gt;
NAS-X&#26159;&#19968;&#31181;&#22522;&#20110;&#25197;&#26354;&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#30340;&#21796;&#37266;-&#30561;&#30496;&#31639;&#27861;&#26469;&#23398;&#20064;&#21644;&#25512;&#26029;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#65292;&#24182;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#30340;&#25512;&#26029;&#21644;&#21442;&#25968;&#24674;&#22797;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;NAS-X&#30340;&#31070;&#32463;&#33258;&#36866;&#24212;&#24179;&#28369;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#37325;&#26032;&#21152;&#26435;&#30340;&#21796;&#37266;-&#30561;&#30496;&#31639;&#27861;&#36827;&#34892;&#39034;&#24207;&#28508;&#21464;&#37327;&#27169;&#22411;&#30340;&#23398;&#20064;&#21644;&#25512;&#26029;&#12290;NAS-X&#36866;&#29992;&#20110;&#31163;&#25955;&#21644;&#36830;&#32493;&#28508;&#21464;&#37327;&#65292;&#24182;&#21033;&#29992;&#24179;&#28369;SMC&#26041;&#27861;&#26469;&#25311;&#21512;&#27604;&#20256;&#32479;&#30340;&#37325;&#26032;&#21152;&#26435;&#21796;&#37266;-&#30561;&#30496;&#26041;&#27861;&#26356;&#24191;&#27867;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#31163;&#25955;&#21644;&#36830;&#32493;&#20219;&#21153;&#19978;&#27979;&#35797;&#20102;NAS-X&#65292;&#24182;&#21457;&#29616;&#22312;&#25512;&#26029;&#21644;&#21442;&#25968;&#24674;&#22797;&#26041;&#38754;&#65292;&#23427;&#26126;&#26174;&#20248;&#20110;&#20808;&#21069;&#30340;&#21464;&#20998;&#21644;&#22522;&#20110;&#37325;&#26032;&#21152;&#26435;&#21796;&#37266;-&#30561;&#30496;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Neural Adaptive Smoothing via Twisting (NAS-X), a method for learning and inference in sequential latent variable models based on reweighted wake-sleep (RWS). NAS-X works with both discrete and continuous latent variables, and leverages smoothing SMC to fit a broader range of models than traditional RWS methods. We test NAS-X on discrete and continuous tasks and find that it substantially outperforms previous variational and RWS-based methods in inference and parameter recovery.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#24605;&#32500;&#38142;&#65288;KD-CoT&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#20462;&#25913;LLMs&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#36890;&#36807;&#19982;&#22806;&#37096;&#30693;&#35782;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#24187;&#35273;&#21644;&#38169;&#35823;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2308.13259</link><description>&lt;p&gt;
&#22522;&#20110;&#30693;&#35782;&#39537;&#21160;&#30340;CoT&#65306;&#25506;&#32034;LLMs&#20013;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#38382;&#31572;&#36827;&#34892;&#24544;&#23454;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Driven CoT: Exploring Faithful Reasoning in LLMs for Knowledge-intensive Question Answering. (arXiv:2308.13259v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.13259
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#24605;&#32500;&#38142;&#65288;KD-CoT&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#39564;&#35777;&#21644;&#20462;&#25913;LLMs&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#36890;&#36807;&#19982;&#22806;&#37096;&#30693;&#35782;&#30340;&#20132;&#20114;&#26469;&#35299;&#20915;&#24187;&#35273;&#21644;&#38169;&#35823;&#20256;&#25773;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#37197;&#22791;&#20102;&#24605;&#32500;&#38142;&#65288;CoT&#65289;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#24187;&#35273;&#21644;&#26080;&#27861;&#35775;&#38382;&#22806;&#37096;&#30693;&#35782;&#65292;LLMs&#22312;&#23545;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#65288;&#22914;&#30693;&#35782;&#24211;&#38382;&#31572;&#65289;&#36827;&#34892;&#25512;&#29702;&#26102;&#24120;&#24120;&#20250;&#20135;&#29983;&#19981;&#27491;&#30830;&#25110;&#19981;&#24544;&#23454;&#30340;&#20013;&#38388;&#25512;&#29702;&#27493;&#39588;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;&#30693;&#35782;&#39537;&#21160;&#30340;&#24605;&#32500;&#38142;&#65288;KD-CoT&#65289;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#19982;&#22806;&#37096;&#30693;&#35782;&#30340;&#20132;&#20114;&#26469;&#39564;&#35777;&#21644;&#20462;&#25913;CoT&#20013;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#20174;&#32780;&#20811;&#26381;&#24187;&#35273;&#21644;&#38169;&#35823;&#20256;&#25773;&#12290;&#20855;&#20307;&#22320;&#65292;&#25105;&#20204;&#23558;LLMs&#30340;CoT&#25512;&#29702;&#36807;&#31243;&#35268;&#33539;&#21270;&#20026;&#32467;&#26500;&#21270;&#30340;&#22810;&#36718;&#38382;&#31572;&#26684;&#24335;&#12290;&#22312;&#27599;&#19968;&#36718;&#20013;&#65292;LLMs&#19982;&#19968;&#20010;&#38382;&#31572;&#31995;&#32479;&#36827;&#34892;&#20132;&#20114;&#65292;&#35813;&#31995;&#32479;&#26816;&#32034;&#22806;&#37096;&#30693;&#35782;&#24182;&#22522;&#20110;&#26816;&#32034;&#21040;&#30340;&#20934;&#30830;&#31572;&#26696;&#20135;&#29983;&#24544;&#23454;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#25105;&#20204;&#24320;&#21457;&#30340;KBQA CoT&#38598;&#21512;&#20419;&#36827;&#20102;LLMs&#30340;&#32467;&#26500;&#21270;CoT&#25512;&#29702;&#65292;&#23427;&#20316;&#20026;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#19968;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
Equipped with Chain-of-Thought (CoT), Large language models (LLMs) have shown impressive reasoning ability in various downstream tasks. Even so, suffering from hallucinations and the inability to access external knowledge, LLMs often come with incorrect or unfaithful intermediate reasoning steps, especially in the context of answering knowledge-intensive tasks such as KBQA. To alleviate this issue, we propose a framework called Knowledge-Driven Chain-of-Thought (KD-CoT) to verify and modify reasoning traces in CoT via interaction with external knowledge, and thus overcome the hallucinations and error propagation. Concretely, we formulate the CoT rationale process of LLMs into a structured multi-round QA format. In each round, LLMs interact with a QA system that retrieves external knowledge and produce faithful reasoning traces based on retrieved precise answers. The structured CoT reasoning of LLMs is facilitated by our developed KBQA CoT collection, which serves as in-context learning
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;Electra Transformer&#12289;GloVe&#21644;LSTM&#27169;&#22411;&#30340;&#21019;&#26032;&#38382;&#39064;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;TREC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#34701;&#21512;&#19981;&#21516;&#25216;&#26415;&#21487;&#20197;&#33719;&#24471;&#26356;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.06828</link><description>&lt;p&gt;
&#38382;&#39064;&#20998;&#31867;&#30340;&#38598;&#25104;&#26041;&#27861;&#65306;&#34701;&#21512;Electra Transformer&#12289;GloVe&#21644;LSTM
&lt;/p&gt;
&lt;p&gt;
An Ensemble Approach to Question Classification: Integrating Electra Transformer, GloVe, and LSTM. (arXiv:2308.06828v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06828
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;Electra Transformer&#12289;GloVe&#21644;LSTM&#27169;&#22411;&#30340;&#21019;&#26032;&#38382;&#39064;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;TREC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20005;&#26684;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#34701;&#21512;&#19981;&#21516;&#25216;&#26415;&#21487;&#20197;&#33719;&#24471;&#26356;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24050;&#32463;&#25104;&#20026;&#29702;&#35299;&#21644;&#29983;&#25104;&#20154;&#31867;&#35821;&#35328;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#23427;&#22312;&#26426;&#22120;&#32763;&#35793;&#12289;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#23588;&#20854;&#26159;&#22312;&#38382;&#39064;&#20998;&#31867;&#26041;&#38754;&#12290;&#20316;&#20026;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#23376;&#39046;&#22495;&#65292;&#38382;&#39064;&#20998;&#31867;&#19987;&#27880;&#20110;&#30830;&#23450;&#25152;&#38656;&#20449;&#24687;&#30340;&#31867;&#22411;&#65292;&#36825;&#26159;&#38382;&#39064;&#22238;&#31572;&#31995;&#32479;&#31561;&#19979;&#28216;&#24212;&#29992;&#30340;&#22522;&#26412;&#27493;&#39588;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#38382;&#39064;&#20998;&#31867;&#38598;&#25104;&#26041;&#27861;&#65292;&#23558;Electra&#12289;GloVe&#21644;LSTM&#27169;&#22411;&#30340;&#20248;&#21183;&#30456;&#32467;&#21512;&#12290;&#35813;&#27169;&#22411;&#22312;&#33879;&#21517;&#30340;TREC&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#22914;&#20309;&#25972;&#21512;&#36825;&#20123;&#19981;&#21516;&#25216;&#26415;&#21487;&#20197;&#24471;&#21040;&#26356;&#20248;&#36234;&#30340;&#32467;&#26524;&#12290;Electra&#25552;&#20379;&#20102;&#22522;&#20110;transformer&#30340;&#22797;&#26434;&#35821;&#35328;&#29702;&#35299;&#33021;&#21147;&#65292;GloVe&#25552;&#20379;&#20102;&#20840;&#23616;&#21521;&#37327;&#34920;&#31034;&#20197;&#25429;&#25417;&#35789;&#32423;&#35821;&#20041;&#65292;LSTM&#21017;&#36129;&#29486;&#20102;&#24207;&#21015;&#23398;&#20064;&#33021;&#21147;&#20197;&#24314;&#27169;&#38271;&#26399;&#20381;&#36182;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural Language Processing (NLP) has emerged as a crucial technology for understanding and generating human language, playing an essential role in tasks such as machine translation, sentiment analysis, and more pertinently, question classification. As a subfield within NLP, question classification focuses on determining the type of information being sought, a fundamental step for downstream applications like question answering systems. This study presents an innovative ensemble approach for question classification, combining the strengths of Electra, GloVe, and LSTM models. Rigorously tested on the well-regarded TREC dataset, the model demonstrates how the integration of these disparate technologies can lead to superior results. Electra brings in its transformer-based capabilities for complex language understanding, GloVe offers global vector representations for capturing word-level semantics, and LSTM contributes its sequence learning abilities to model long-term dependencies. By fus
&lt;/p&gt;</description></item><item><title>&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#36890;&#36807;&#25913;&#36827;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;MAWU&#65292;&#23427;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;</title><link>http://arxiv.org/abs/2308.06091</link><description>&lt;p&gt;
&#23545;&#21327;&#21516;&#36807;&#28388;&#20002;&#22833;&#20989;&#25968;&#30340;&#26356;&#22909;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Toward a Better Understanding of Loss Functions for Collaborative Filtering. (arXiv:2308.06091v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.06091
&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#34920;&#26126;&#65292;&#36890;&#36807;&#25913;&#36827;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;MAWU&#65292;&#23427;&#32771;&#34385;&#20102;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21327;&#21516;&#36807;&#28388;&#65288;CF&#65289;&#26159;&#29616;&#20195;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;CF&#27169;&#22411;&#30340;&#23398;&#20064;&#36807;&#31243;&#36890;&#24120;&#30001;&#19977;&#20010;&#32452;&#20214;&#32452;&#25104;&#65306;&#20132;&#20114;&#32534;&#30721;&#22120;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#36127;&#37319;&#26679;&#12290;&#23613;&#31649;&#35768;&#22810;&#29616;&#26377;&#30740;&#31350;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;CF&#27169;&#22411;&#26469;&#35774;&#35745;&#22797;&#26434;&#30340;&#20132;&#20114;&#32534;&#30721;&#22120;&#65292;&#20294;&#26368;&#36817;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#31616;&#21333;&#22320;&#37325;&#26032;&#21046;&#23450;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#23454;&#29616;&#26174;&#33879;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#26412;&#25991;&#28145;&#20837;&#20998;&#26512;&#20102;&#29616;&#26377;&#25439;&#22833;&#20989;&#25968;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#25968;&#23398;&#20998;&#26512;&#25581;&#31034;&#20102;&#20808;&#21069;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#35299;&#37322;&#20026;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#20989;&#25968;&#65306;&#65288;i&#65289;&#23545;&#40784;&#21305;&#37197;&#29992;&#25143;&#21644;&#29289;&#21697;&#34920;&#31034;&#65292;&#65288;ii&#65289;&#22343;&#21248;&#24615;&#20998;&#25955;&#29992;&#25143;&#21644;&#29289;&#21697;&#20998;&#24067;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#23545;&#40784;&#21644;&#22343;&#21248;&#24615;&#35774;&#35745;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#32771;&#34385;&#21040;&#25968;&#25454;&#38598;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#31216;&#20026;Margin-aware Alignment and Weighted Uniformity&#65288;MAWU&#65289;&#12290;MAWU&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;
&lt;/p&gt;
&lt;p&gt;
Collaborative filtering (CF) is a pivotal technique in modern recommender systems. The learning process of CF models typically consists of three components: interaction encoder, loss function, and negative sampling. Although many existing studies have proposed various CF models to design sophisticated interaction encoders, recent work shows that simply reformulating the loss functions can achieve significant performance gains. This paper delves into analyzing the relationship among existing loss functions. Our mathematical analysis reveals that the previous loss functions can be interpreted as alignment and uniformity functions: (i) the alignment matches user and item representations, and (ii) the uniformity disperses user and item distributions. Inspired by this analysis, we propose a novel loss function that improves the design of alignment and uniformity considering the unique patterns of datasets called Margin-aware Alignment and Weighted Uniformity (MAWU). The key novelty of MAWU 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HoLe&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#20013;&#22686;&#24378;&#21516;&#31867;&#24615;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22270;&#32858;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2308.05309</link><description>&lt;p&gt;
&#22270;&#32858;&#31867;&#30340;&#21516;&#31867;&#24615;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Homophily-enhanced Structure Learning for Graph Clustering. (arXiv:2308.05309v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.05309
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;HoLe&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#22270;&#32467;&#26500;&#20013;&#22686;&#24378;&#21516;&#31867;&#24615;&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#22270;&#32858;&#31867;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32858;&#31867;&#26159;&#22270;&#20998;&#26512;&#20013;&#30340;&#19968;&#20010;&#22522;&#26412;&#20219;&#21153;&#65292;&#22312;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25104;&#26524;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#22522;&#20110;GNN&#30340;&#22270;&#32858;&#31867;&#26041;&#27861;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#24573;&#35270;&#20102;&#22270;&#32467;&#26500;&#30340;&#36136;&#37327;&#65292;&#36825;&#26159;&#30001;&#20110;&#29616;&#23454;&#19990;&#30028;&#22270;&#30340;&#31232;&#30095;&#24615;&#21644;&#22810;&#26679;&#24615;&#25152;&#22266;&#26377;&#30340;&#65292;&#20174;&#32780;&#23548;&#33268;&#20102;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22270;&#32467;&#26500;&#23398;&#20064;&#21487;&#20197;&#36890;&#36807;&#28155;&#21152;&#32570;&#22833;&#30340;&#36830;&#25509;&#21644;&#21024;&#38500;&#38169;&#35823;&#30340;&#36830;&#25509;&#26469;&#20248;&#21270;&#36755;&#20837;&#22270;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#22270;&#32467;&#26500;&#23398;&#20064;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#26377;&#30417;&#30563;&#30340;&#35774;&#32622;&#19978;&#65292;&#24182;&#19988;&#30001;&#20110;&#32570;&#20047;&#30495;&#23454;&#26631;&#31614;&#65292;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#25105;&#20204;&#30340;&#29305;&#23450;&#32858;&#31867;&#20219;&#21153;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;&#21516;&#31867;&#24615;&#22686;&#24378;&#32467;&#26500;&#23398;&#20064;&#22270;&#32858;&#31867;&#65288;HoLe&#65289;&#12290;&#25105;&#20204;&#30340;&#21160;&#26426;&#28304;&#20110;&#35266;&#23519;&#21040;&#65292;&#24494;&#22937;&#22320;&#22686;&#24378;&#22270;&#32467;&#26500;&#20013;&#30340;&#21516;&#31867;&#24615;&#31243;&#24230;&#21487;&#20197;&#26174;&#33879;&#25552;&#21319;GNNs&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph clustering is a fundamental task in graph analysis, and recent advances in utilizing graph neural networks (GNNs) have shown impressive results. Despite the success of existing GNN-based graph clustering methods, they often overlook the quality of graph structure, which is inherent in real-world graphs due to their sparse and multifarious nature, leading to subpar performance. Graph structure learning allows refining the input graph by adding missing links and removing spurious connections. However, previous endeavors in graph structure learning have predominantly centered around supervised settings, and cannot be directly applied to our specific clustering tasks due to the absence of ground-truth labels. To bridge the gap, we propose a novel method called \textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering (HoLe). Our motivation stems from the observation that subtly enhancing the degree of homophily within the graph structure can significantly improve G
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AI&#20215;&#20540;&#38142;&#30340;&#27010;&#24565;&#65292;&#20197;&#28385;&#36275;AI&#20262;&#29702;&#30740;&#31350;&#21644;&#24178;&#39044;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;AI&#20215;&#20540;&#38142;&#28041;&#21450;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#26041;&#21521;&#65292;&#26088;&#22312;&#25512;&#21160;&#26356;&#20855;&#20262;&#29702;&#30340;AI&#21457;&#23637;&#21644;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.16787</link><description>&lt;p&gt;
AI&#20215;&#20540;&#38142;&#30340;&#20262;&#29702;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
The Ethics of AI Value Chains. (arXiv:2307.16787v2 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.16787
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AI&#20215;&#20540;&#38142;&#30340;&#27010;&#24565;&#65292;&#20197;&#28385;&#36275;AI&#20262;&#29702;&#30740;&#31350;&#21644;&#24178;&#39044;&#30340;&#38656;&#27714;&#12290;&#36890;&#36807;&#32508;&#21512;&#35780;&#20272;AI&#20215;&#20540;&#38142;&#28041;&#21450;&#30340;&#20262;&#29702;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#26041;&#21521;&#65292;&#26088;&#22312;&#25512;&#21160;&#26356;&#20855;&#20262;&#29702;&#30340;AI&#21457;&#23637;&#21644;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#23545;&#20154;&#24037;&#26234;&#33021;&#20262;&#29702;&#24863;&#20852;&#36259;&#30340;&#30740;&#31350;&#20154;&#21592;&#12289;&#20174;&#19994;&#20154;&#21592;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#65292;&#20182;&#20204;&#38656;&#35201;&#26356;&#22810;&#32508;&#21512;&#30340;&#26041;&#27861;&#26469;&#30740;&#31350;&#21644;&#24178;&#39044;&#22312;&#22810;&#31181;&#32972;&#26223;&#21644;&#27963;&#21160;&#35268;&#27169;&#19979;&#30340;AI&#31995;&#32479;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;AI&#20215;&#20540;&#38142;&#20316;&#20026;&#19968;&#20010;&#28385;&#36275;&#36825;&#19968;&#38656;&#27714;&#30340;&#32508;&#21512;&#27010;&#24565;&#12290;&#20026;&#20102;&#26356;&#28165;&#26224;&#22320;&#29702;&#35770;&#21270;AI&#20215;&#20540;&#38142;&#65292;&#24182;&#22312;&#27010;&#24565;&#19978;&#23558;&#20854;&#19982;&#20379;&#24212;&#38142;&#21306;&#20998;&#24320;&#26469;&#65292;&#25105;&#20204;&#22238;&#39038;&#20102;&#25112;&#30053;&#31649;&#29702;&#12289;&#26381;&#21153;&#31185;&#23398;&#12289;&#32463;&#27982;&#22320;&#29702;&#23398;&#12289;&#34892;&#19994;&#12289;&#25919;&#24220;&#21644;&#24212;&#29992;&#30740;&#31350;&#25991;&#29486;&#20013;&#20851;&#20110;&#20215;&#20540;&#38142;&#21644;AI&#20215;&#20540;&#38142;&#30340;&#29702;&#35770;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23545;&#28085;&#30422;AI&#20215;&#20540;&#38142;&#28041;&#21450;&#30340;&#20262;&#29702;&#38382;&#39064;&#30340;67&#20010;&#26469;&#28304;&#36827;&#34892;&#32508;&#21512;&#35780;&#20272;&#12290;&#26681;&#25454;&#25105;&#20204;&#32508;&#21512;&#35780;&#20272;&#30340;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#20010;&#26410;&#26469;&#26041;&#21521;&#65292;&#30740;&#31350;&#20154;&#21592;&#12289;&#20174;&#19994;&#20154;&#21592;&#21644;&#25919;&#31574;&#21046;&#23450;&#32773;&#21487;&#20197;&#37319;&#21462;&#36825;&#20123;&#26041;&#21521;&#26469;&#25512;&#21160;&#22312;AI&#20215;&#20540;&#38142;&#19978;&#23454;&#29616;&#26356;&#20855;&#20262;&#29702;&#30340;&#21457;&#23637;&#21644;&#20351;&#29992;&#12290;&#25105;&#20204;&#30340;&#32508;&#36848;&#21644;&#24314;&#35758;&#26377;&#21161;&#20110;&#25512;&#36827;&#30740;&#31350;&#35758;&#31243;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
Researchers, practitioners, and policymakers with an interest in AI ethics need more integrative approaches for studying and intervening in AI systems across many contexts and scales of activity. This paper presents AI value chains as an integrative concept that satisfies that need. To more clearly theorize AI value chains and conceptually distinguish them from supply chains, we review theories of value chains and AI value chains from the strategic management, service science, economic geography, industry, government, and applied research literature. We then conduct an integrative review of a sample of 67 sources that cover the ethical concerns implicated in AI value chains. Building upon the findings of our integrative review, we recommend four future directions that researchers, practitioners, and policymakers can take to advance more ethical practices of AI development and use across AI value chains. Our review and recommendations contribute to the advancement of research agendas, i
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;UGAT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#21160;&#24577;&#36716;&#25442;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#65292;&#23454;&#29616;&#20102;&#20174;&#27169;&#25311;&#29615;&#22659;&#21040;&#30495;&#23454;&#29615;&#22659;&#30340;&#31574;&#30053;&#36716;&#31227;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.12388</link><description>&lt;p&gt;
&#38754;&#21521;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#22522;&#20110;&#23454;&#20363;&#30340;&#34892;&#21160;&#36716;&#25442;&#30340;&#27169;&#25311;&#21040;&#23454;&#38469;&#36716;&#31227;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-aware Grounded Action Transformation towards Sim-to-Real Transfer for Traffic Signal Control. (arXiv:2307.12388v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.12388
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;UGAT&#26041;&#27861;&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#21160;&#24577;&#36716;&#25442;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#65292;&#23454;&#29616;&#20102;&#20174;&#27169;&#25311;&#29615;&#22659;&#21040;&#30495;&#23454;&#29615;&#22659;&#30340;&#31574;&#30053;&#36716;&#31227;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#65288;TSC&#65289;&#26159;&#19968;&#20010;&#24433;&#21709;&#25968;&#30334;&#19975;&#20154;&#26085;&#24120;&#29983;&#27963;&#30340;&#22797;&#26434;&#32780;&#37325;&#35201;&#30340;&#20219;&#21153;&#12290;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#20248;&#21270;&#20132;&#36890;&#20449;&#21495;&#25511;&#21046;&#26041;&#38754;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#24403;&#21069;&#22522;&#20110;RL&#30340;TSC&#26041;&#27861;&#20027;&#35201;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#65292;&#23384;&#22312;&#27169;&#25311;&#21644;&#30495;&#23454;&#19990;&#30028;&#20043;&#38388;&#24615;&#33021;&#24046;&#36317;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#27169;&#25311;&#21040;&#23454;&#38469;&#29615;&#22659;&#36716;&#31227;&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;UGAT&#65292;&#36890;&#36807;&#22312;&#27169;&#25311;&#20013;&#21160;&#24577;&#36716;&#25442;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#34892;&#21160;&#65292;&#20197;&#20943;&#36731;&#36716;&#31227;&#21160;&#24577;&#30340;&#39046;&#22495;&#24046;&#36317;&#65292;&#23558;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#23398;&#20064;&#31574;&#30053;&#36716;&#31227;&#21040;&#30495;&#23454;&#29615;&#22659;&#20013;&#12290;&#25105;&#20204;&#22312;&#27169;&#25311;&#20132;&#36890;&#29615;&#22659;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#34920;&#26126;&#23427;&#26174;&#33879;&#25552;&#39640;&#20102;&#36716;&#31227;&#21518;&#30340;RL&#31574;&#30053;&#22312;&#30495;&#23454;&#19990;&#30028;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic signal control (TSC) is a complex and important task that affects the daily lives of millions of people. Reinforcement Learning (RL) has shown promising results in optimizing traffic signal control, but current RL-based TSC methods are mainly trained in simulation and suffer from the performance gap between simulation and the real world. In this paper, we propose a simulation-to-real-world (sim-to-real) transfer approach called UGAT, which transfers a learned policy trained from a simulated environment to a real-world environment by dynamically transforming actions in the simulation with uncertainty to mitigate the domain gap of transition dynamics. We evaluate our method on a simulated traffic environment and show that it significantly improves the performance of the transferred RL policy in the real world.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2307.11949</link><description>&lt;p&gt;
HIQL: &#20197;&#28508;&#22312;&#29366;&#24577;&#20316;&#20026;&#21160;&#20316;&#30340;&#31163;&#32447;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
HIQL: Offline Goal-Conditioned RL with Latent States as Actions. (arXiv:2307.11949v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.11949
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#30340;&#32467;&#26500;&#65292;&#20351;&#29992;&#19968;&#20010;&#26080;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20174;&#32780;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#26356;&#26377;&#25928;&#22320;&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#39044;&#35757;&#32451;&#26368;&#36817;&#24050;&#25104;&#20026;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#22522;&#30707;&#12290;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#21487;&#20197;&#28508;&#22312;&#22320;&#21033;&#29992;&#22823;&#37327;&#26410;&#26631;&#35760;&#30340;&#65288;&#26080;&#22870;&#21169;&#65289;&#25968;&#25454;&#65292;&#25552;&#20379;&#31867;&#20284;&#20110;&#33258;&#25105;&#30417;&#30563;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26500;&#24314;&#26377;&#25928;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24182;&#30452;&#25509;&#20174;&#22810;&#26679;&#21270;&#30340;&#31163;&#32447;&#25968;&#25454;&#20013;&#36827;&#34892;&#23398;&#20064;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#20934;&#30830;&#20272;&#35745;&#36828;&#26399;&#30446;&#26631;&#30340;&#20215;&#20540;&#20989;&#25968;&#24456;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#30446;&#26631;&#36798;&#25104;&#38382;&#39064;&#34920;&#29616;&#20986;&#19968;&#23450;&#30340;&#32467;&#26500;&#65292;&#21363;&#36798;&#21040;&#36828;&#26399;&#30446;&#26631;&#38656;&#35201;&#39318;&#20808;&#36890;&#36807;&#36739;&#36817;&#23376;&#30446;&#26631;&#12290;&#36825;&#31181;&#32467;&#26500;&#38750;&#24120;&#26377;&#29992;&#65292;&#22240;&#20026;&#35780;&#20272;&#37051;&#36817;&#30446;&#26631;&#30340;&#21160;&#20316;&#36136;&#37327;&#36890;&#24120;&#27604;&#26356;&#36828;&#30446;&#26631;&#23481;&#26131;&#12290;&#22522;&#20110;&#36825;&#19968;&#24605;&#24819;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#31163;&#32447;&#25968;&#25454;&#30340;&#30446;&#26631;&#23548;&#21521;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#23618;&#31639;&#27861;&#12290;&#21033;&#29992;&#19968;&#20010;&#27809;&#26377;&#21160;&#20316;&#30340;&#20215;&#20540;&#20989;&#25968;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#20004;&#20010;&#31574;&#30053;&#65292;&#20801;&#35768;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#32467;&#26500;&#65306;&#19968;&#20010;&#39640;&#23618;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Unsupervised pre-training has recently become the bedrock for computer vision and natural language processing. In reinforcement learning (RL), goal-conditioned RL can potentially provide an analogous self-supervised approach for making use of large quantities of unlabeled (reward-free) data. However, building effective algorithms for goal-conditioned RL that can learn directly from diverse offline data is challenging, because it is hard to accurately estimate the exact value function for faraway goals. Nonetheless, goal-reaching problems exhibit structure, such that reaching distant goals entails first passing through closer subgoals. This structure can be very useful, as assessing the quality of actions for nearby goals is typically easier than for more distant goals. Based on this idea, we propose a hierarchical algorithm for goal-conditioned RL from offline data. Using one action-free value function, we learn two policies that allow us to exploit this structure: a high-level policy 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;</title><link>http://arxiv.org/abs/2307.06775</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#22411;&#30340;&#19982;&#24179;&#21488;&#26080;&#20851;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
A Novel Site-Agnostic Multimodal Deep Learning Model to Identify Pro-Eating Disorder Content on Social Media. (arXiv:2307.06775v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06775
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23558;&#25991;&#26412;&#21644;&#35270;&#35273;&#25968;&#25454;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#20934;&#30830;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#39278;&#39135;&#32010;&#20081;&#30340;&#35786;&#26029;&#21644;&#19982;&#20043;&#30456;&#20851;&#30340;&#27515;&#20129;&#25968;&#37327;&#22823;&#24133;&#22686;&#21152;&#65292;&#23588;&#20854;&#26159;&#22312;&#26032;&#20896;&#30123;&#24773;&#26399;&#38388;&#12290;&#36825;&#31181;&#24040;&#22823;&#22686;&#38271;&#37096;&#20998;&#26469;&#28304;&#20110;&#30123;&#24773;&#30340;&#21387;&#21147;&#65292;&#20294;&#20063;&#19982;&#31038;&#20132;&#23186;&#20307;&#30340;&#26292;&#38706;&#22686;&#21152;&#26377;&#20851;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#20805;&#26021;&#30528;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;&#36825;&#20123;&#20869;&#23481;&#21487;&#20197;&#35825;&#21457;&#35266;&#30475;&#32773;&#30340;&#39278;&#39135;&#32010;&#20081;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#21019;&#24314;&#19968;&#20010;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#33021;&#22815;&#22522;&#20110;&#35270;&#35273;&#21644;&#25991;&#26412;&#25968;&#25454;&#30340;&#32452;&#21512;&#21028;&#26029;&#32473;&#23450;&#30340;&#31038;&#20132;&#23186;&#20307;&#24086;&#23376;&#26159;&#21542;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#12290;&#20174;Twitter&#25910;&#38598;&#20102;&#19968;&#20010;&#24102;&#26377;&#26631;&#31614;&#30340;&#25512;&#25991;&#25968;&#25454;&#38598;&#65292;&#23545;&#20854;&#36827;&#34892;&#20102;&#21313;&#20108;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#26681;&#25454;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#26368;&#26377;&#25928;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26159;RoBERTa&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#21644;MaxViT&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22411;&#65292;&#20934;&#30830;&#29575;&#21644;F1&#20998;&#25968;&#20998;&#21035;&#36798;&#21040;95.9%&#21644;0.959&#12290;RoBERTa&#21644;MaxViT&#34701;&#21512;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35782;&#21035;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#20419;&#36827;&#39278;&#39135;&#32010;&#20081;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, there has been a vast increase in eating disorder diagnoses and eating disorder-attributed deaths, reaching their zenith during the Covid-19 pandemic. This immense growth derived in part from the stressors of the pandemic but also from increased exposure to social media, which is rife with content that promotes eating disorders. Such content can induce eating disorders in viewers. This study aimed to create a multimodal deep learning model capable of determining whether a given social media post promotes eating disorders based on a combination of visual and textual data. A labeled dataset of Tweets was collected from Twitter, upon which twelve deep learning models were trained and tested. Based on model performance, the most effective deep learning model was the multimodal fusion of the RoBERTa natural language processing model and the MaxViT image classification model, attaining accuracy and F1 scores of 95.9% and 0.959 respectively. The RoBERTa and MaxViT fusion
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;</title><link>http://arxiv.org/abs/2307.06290</link><description>&lt;p&gt;
&#25351;&#20196;&#25366;&#25496;&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Instruction Mining: High-Quality Instruction Data Selection for Large Language Models. (arXiv:2307.06290v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.06290
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#31181;&#29992;&#20110;&#36873;&#25321;&#39640;&#36136;&#37327;&#25351;&#20196;&#25968;&#25454;&#30340;&#32447;&#24615;&#35268;&#21017;&#65292;&#20197;&#22686;&#24378;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#33021;&#21147;&#12290;&#36890;&#36807;&#29305;&#23450;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#24314;&#27169;&#65292;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#32463;&#21382;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#20004;&#20010;&#35757;&#32451;&#38454;&#27573;&#12290;&#23613;&#31649;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#36171;&#20104;&#27169;&#22411;&#24378;&#22823;&#30340;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#22238;&#24212;&#30340;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#39044;&#35757;&#32451;&#27169;&#22411;&#26377;&#26102;&#20173;&#28982;&#26080;&#27861;&#29702;&#35299;&#20154;&#31867;&#25351;&#20196;&#12290;&#20026;&#20102;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#35299;&#37322;&#21644;&#21709;&#24212;&#25351;&#20196;&#30340;&#33021;&#21147;&#65292;&#25351;&#20196;&#24494;&#35843;&#24050;&#25104;&#20026;&#35813;&#39046;&#22495;&#30340;&#20851;&#38190;&#26041;&#27861;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#21363;&#20351;&#21482;&#26377;&#23569;&#37327;&#39640;&#36136;&#37327;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20063;&#21487;&#20197;&#36827;&#34892;&#33391;&#22909;&#30340;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#36873;&#25321;&#29992;&#20110;&#24494;&#35843;&#35821;&#35328;&#27169;&#22411;&#30340;&#39640;&#36136;&#37327;&#25968;&#25454;&#38598;&#20173;&#32570;&#20047;&#26126;&#30830;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;InstructMining&#65292;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#36136;&#37327;&#30340;&#32447;&#24615;&#35268;&#21017;&#12290;&#25105;&#20204;&#20351;&#29992;&#20855;&#20307;&#30340;&#33258;&#28982;&#35821;&#35328;&#25351;&#26631;&#26469;&#36827;&#34892;InstructMining&#30340;&#24314;&#27169;&#12290;&#20026;&#20102;&#30740;&#31350;&#25968;&#25454;&#36136;&#37327;&#19982;&#36825;&#20123;&#25351;&#26631;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#32454;&#33268;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models typically undergo two training stages, pretraining and finetuning. Despite that large-scale pretraining endows the model with strong capabilities to generate natural language responses, these pretrained models can still fail to understand human instructions at times. To enhance language models' ability of interpreting and responding to instructions, instruction finetuning has emerged as a critical method in this area. Recent studies found that large language models can be finetuned to perform well even with a small amount of high-quality instruction-following data. However, the selection of high-quality datasets for finetuning language models still lacks clear guidelines to follow. In this paper, we propose InstructMining, a linear rule for evaluating instruction-following data quality. We formulate InstructMining using specific natural language indicators. To investigate the relationship between data quality and these indicators, we further conduct extensive fine
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;$\beta$-&#20998;&#35299;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\beta$D-Bayes&#65292;&#19968;&#31181;&#33021;&#22815;&#23454;&#29616;&#24046;&#20998;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.05194</link><description>&lt;p&gt;
&#36890;&#36807;$\beta$-&#20998;&#35299;&#19968;&#21518;&#39564;&#37319;&#26679;&#23454;&#29616;&#24046;&#20998;&#35745;&#31639;&#26426;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling. (arXiv:2307.05194v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.05194
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#21644;&#27169;&#22411;&#20043;&#38388;&#30340;$\beta$-&#20998;&#35299;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\beta$D-Bayes&#65292;&#19968;&#31181;&#33021;&#22815;&#23454;&#29616;&#24046;&#20998;&#26426;&#22120;&#23398;&#20064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24046;&#20998;&#31169;&#23494;&#24615;&#30830;&#20445;&#20102;&#21253;&#21547;&#25935;&#24863;&#25968;&#25454;&#30340;&#32479;&#35745;&#20998;&#26512;&#32467;&#26524;&#21487;&#20197;&#22312;&#19981;&#25439;&#23475;&#20219;&#20309;&#20010;&#20307;&#38544;&#31169;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#21457;&#24067;&#12290;&#23454;&#29616;&#36825;&#31181;&#20445;&#35777;&#36890;&#24120;&#38656;&#35201;&#22312;&#21442;&#25968;&#20272;&#35745;&#25110;&#20272;&#35745;&#36807;&#31243;&#20013;&#30452;&#25509;&#27880;&#20837;&#22122;&#38899;&#12290;&#32780;&#37319;&#26679;&#26469;&#33258;&#36125;&#21494;&#26031;&#21518;&#39564;&#20998;&#24067;&#24050;&#34987;&#35777;&#26126;&#26159;&#25351;&#25968;&#26426;&#21046;&#30340;&#19968;&#31181;&#29305;&#27530;&#24773;&#20917;&#65292;&#21487;&#20197;&#20135;&#29983;&#19968;&#33268;&#19988;&#39640;&#25928;&#30340;&#31169;&#23494;&#20272;&#35745;&#65292;&#32780;&#19981;&#20250;&#25913;&#21464;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#26041;&#27861;&#30340;&#24212;&#29992;&#21463;&#21040;&#36739;&#24378;&#30340;&#36793;&#30028;&#20551;&#35774;&#30340;&#38480;&#21046;&#65292;&#36825;&#20123;&#20551;&#35774;&#23545;&#20110;&#22522;&#26412;&#27169;&#22411;&#65288;&#22914;&#31616;&#21333;&#30340;&#32447;&#24615;&#22238;&#24402;&#22120;&#65289;&#24182;&#19981;&#25104;&#31435;&#12290;&#20026;&#20102;&#25913;&#21892;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;$\beta$D-Bayes&#65292;&#19968;&#31181;&#20174;&#24191;&#20041;&#21518;&#39564;&#20013;&#36827;&#34892;&#21518;&#39564;&#37319;&#26679;&#30340;&#26041;&#26696;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#27169;&#22411;&#19982;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#20043;&#38388;&#30340;$\beta$-&#20998;&#35299;&#12290;&#36825;&#25552;&#20379;&#20102;&#31169;&#23494;&#20272;&#35745;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differential privacy guarantees allow the results of a statistical analysis involving sensitive data to be released without compromising the privacy of any individual taking part. Achieving such guarantees generally requires the injection of noise, either directly into parameter estimates or into the estimation process. Instead of artificially introducing perturbations, sampling from Bayesian posterior distributions has been shown to be a special case of the exponential mechanism, producing consistent, and efficient private estimates without altering the data generative process. The application of current approaches has, however, been limited by their strong bounding assumptions which do not hold for basic models, such as simple linear regressors. To ameliorate this, we propose $\beta$D-Bayes, a posterior sampling scheme from a generalised posterior targeting the minimisation of the $\beta$-divergence between the model and the data generating process. This provides private estimation t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ScoreOpt&#30340;&#26032;&#22411;&#23545;&#25239;&#38450;&#24481;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#20248;&#21270;&#23545;&#25239;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.04333</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#20998;&#25968;&#30340;&#20248;&#21270;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Adversarial Robustness via Score-Based Optimization. (arXiv:2307.04333v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.04333
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ScoreOpt&#30340;&#26032;&#22411;&#23545;&#25239;&#38450;&#24481;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#36890;&#36807;&#20248;&#21270;&#23545;&#25239;&#26679;&#26412;&#26469;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#20248;&#20110;&#29616;&#26377;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#25915;&#20987;&#26377;&#21487;&#33021;&#36890;&#36807;&#24341;&#20837;&#24494;&#23567;&#25200;&#21160;&#26469;&#35823;&#23548;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20998;&#31867;&#22120;&#12290;&#24320;&#21457;&#33021;&#22815;&#20943;&#36731;&#36825;&#20123;&#25915;&#20987;&#24433;&#21709;&#30340;&#31639;&#27861;&#23545;&#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#30340;&#23433;&#20840;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22522;&#20110;&#20998;&#25968;&#30340;&#25193;&#25955;&#27169;&#22411;&#22312;&#23545;&#25239;&#38450;&#24481;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#38450;&#24481;&#20381;&#36182;&#20110;&#39034;&#24207;&#27169;&#25311;&#25193;&#25955;&#27169;&#22411;&#30340;&#21453;&#21521;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#65292;&#36825;&#22312;&#35745;&#31639;&#25928;&#29575;&#19978;&#26159;&#20302;&#25928;&#30340;&#65292;&#24182;&#19988;&#20135;&#29983;&#27425;&#20248;&#32467;&#26524;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;ScoreOpt&#30340;&#26032;&#22411;&#23545;&#25239;&#38450;&#24481;&#26041;&#26696;&#65292;&#35813;&#26041;&#26696;&#22312;&#27979;&#35797;&#26102;&#36890;&#36807;&#22312;&#30001;&#22522;&#20110;&#20998;&#25968;&#20808;&#39564;&#25351;&#23548;&#30340;&#26041;&#21521;&#19978;&#23545;&#21407;&#22987;&#24178;&#20928;&#25968;&#25454;&#36827;&#34892;&#20248;&#21270;&#26469;&#20248;&#21270;&#23545;&#25239;&#26679;&#26412;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#65292;&#21253;&#25324;CIFAR10&#12289;CIFAR100&#21644;ImageNet&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#40065;&#26834;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#23545;&#25239;&#38450;&#24481;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adversarial attacks have the potential to mislead deep neural network classifiers by introducing slight perturbations. Developing algorithms that can mitigate the effects of these attacks is crucial for ensuring the safe use of artificial intelligence. Recent studies have suggested that score-based diffusion models are effective in adversarial defenses. However, existing diffusion-based defenses rely on the sequential simulation of the reversed stochastic differential equations of diffusion models, which are computationally inefficient and yield suboptimal results. In this paper, we introduce a novel adversarial defense scheme named ScoreOpt, which optimizes adversarial samples at test-time, towards original clean data in the direction guided by score-based priors. We conduct comprehensive experiments on multiple datasets, including CIFAR10, CIFAR100 and ImageNet. Our experimental results demonstrate that our approach outperforms existing adversarial defenses in terms of both robustnes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21457;&#23637;&#33539;&#24335;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#21452;&#26041;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#20811;&#26381;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#25552;&#39640;&#32852;&#21512;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#25351;&#20986;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#26410;&#32771;&#34385;&#21040;&#21160;&#24577;&#12289;&#36866;&#24212;&#24615;&#21644;&#21327;&#20316;&#22242;&#38431;&#29615;&#22659;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#21151;&#33021;&#65292;&#21628;&#21505;&#21152;&#24378;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#30340;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.03913</link><description>&lt;p&gt;
&#22312;&#21457;&#23637;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20013;&#24212;&#29992;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#20197;&#20154;&#24037;&#26234;&#33021;-&#20154;&#31867;&#20849;&#21516;&#35748;&#30693;&#31995;&#32479;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Applying human-centered AI in developing effective human-AI teaming: A perspective of human-AI joint cognitive systems. (arXiv:2307.03913v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03913
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#21457;&#23637;&#33539;&#24335;&#30340;&#26041;&#27861;&#65292;&#24378;&#35843;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#38656;&#35201;&#20805;&#20998;&#21033;&#29992;&#21452;&#26041;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#20811;&#26381;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#25552;&#39640;&#32852;&#21512;&#34920;&#29616;&#12290;&#21516;&#26102;&#65292;&#35813;&#30740;&#31350;&#25351;&#20986;&#29616;&#26377;&#30740;&#31350;&#24448;&#24448;&#26410;&#32771;&#34385;&#21040;&#21160;&#24577;&#12289;&#36866;&#24212;&#24615;&#21644;&#21327;&#20316;&#22242;&#38431;&#29615;&#22659;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#21151;&#33021;&#65292;&#21628;&#21505;&#21152;&#24378;&#20851;&#20110;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21644;&#24212;&#29992;&#24050;&#23558;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#20316;&#20026;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#26469;&#21457;&#23637;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#12290;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#35748;&#35782;&#21040;&#20154;&#24037;&#26234;&#33021;&#23558;&#20316;&#20026;&#19968;&#21517;&#38431;&#21451;&#32780;&#19981;&#20165;&#20165;&#26159;&#24037;&#20855;&#19982;&#20154;&#31867;&#21327;&#20316;&#12290;&#26377;&#25928;&#30340;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#38656;&#35201;&#33021;&#22815;&#20805;&#20998;&#21033;&#29992;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#29420;&#29305;&#33021;&#21147;&#65292;&#21516;&#26102;&#20811;&#26381;&#27599;&#20010;&#25104;&#21592;&#30340;&#24050;&#30693;&#25361;&#25112;&#21644;&#38480;&#21046;&#65292;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#65292;&#24182;&#23558;&#32852;&#21512;&#24615;&#33021;&#25552;&#39640;&#21040;&#20219;&#20309;&#23454;&#20307;&#20043;&#19978;&#12290;2023&#24180;&#20840;&#22269;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#25112;&#30053;&#35745;&#21010;&#26356;&#26032;&#35748;&#35782;&#21040;&#65292;&#20027;&#35201;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#29420;&#31435;&#24615;&#33021;&#30340;&#30740;&#31350;&#35745;&#21010;&#24448;&#24448;&#26410;&#32771;&#34385;&#21040;&#20154;&#24037;&#26234;&#33021;&#22312;&#21160;&#24577;&#12289;&#36866;&#24212;&#24615;&#21644;&#21327;&#20316;&#22242;&#38431;&#29615;&#22659;&#20013;&#24517;&#39035;&#25552;&#20379;&#30340;&#21151;&#33021;&#65292;&#24182;&#21628;&#21505;&#36827;&#19968;&#27493;&#30740;&#31350;&#20154;&#24037;&#26234;&#33021;&#19982;&#20154;&#31867;&#22242;&#38431;&#21327;&#20316;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#23545;&#20110;&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#33021;&#20316;&#20026;&#20154;&#31867;&#30340;&#38431;&#21451;&#23384;&#22312;&#20105;&#35758;&#12290;&#20027;&#35201;&#30340;&#20851;&#27880;&#28857;&#22312;&#20110;&#37319;&#29992;"&#21327;&#20316;"&#33539;&#24335;&#26159;&#21542;&#19982;&#20154;&#31867;&#30340;&#35748;&#30693;&#36807;&#31243;&#30456;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;
Research and application have used human-AI teaming (HAT) as a new paradigm to develop AI systems. HAT recognizes that AI will function as a teammate instead of simply a tool in collaboration with humans. Effective human-AI teams need to be capable of taking advantage of the unique abilities of both humans and AI while overcoming the known challenges and limitations of each member, augmenting human capabilities, and raising joint performance beyond that of either entity. The National AI Research and Strategic Plan 2023 update has recognized that research programs focusing primarily on the independent performance of AI systems generally fail to consider the functionality that AI must provide within the context of dynamic, adaptive, and collaborative teams and calls for further research on human-AI teaming and collaboration. However, there has been debate about whether AI can work as a teammate with humans. The primary concern is that adopting the "teaming" paradigm contradicts the human
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#23454;&#29616;&#22238;&#24402;&#38382;&#39064;&#30340;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2307.03848</link><description>&lt;p&gt;
&#21487;&#23454;&#29616;&#22238;&#24402;&#30340;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#65306;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Optimal Learners for Realizable Regression: PAC Learning and Online Learning. (arXiv:2307.03848v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#21487;&#23454;&#29616;&#22238;&#24402;&#38382;&#39064;&#30340;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#20110;&#21487;&#23398;&#20064;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#21644;&#20805;&#20998;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#23545;&#21487;&#23454;&#29616;&#22238;&#24402;&#22312;PAC&#23398;&#20064;&#21644;&#22312;&#32447;&#23398;&#20064;&#30340;&#32479;&#35745;&#22797;&#26434;&#24230;&#36827;&#34892;&#21051;&#30011;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#26377;&#38480;&#30340;fat shattering&#32500;&#24230;&#23545;&#20110;PAC&#23398;&#20064;&#30340;&#20805;&#20998;&#24615;&#20197;&#21450;&#26377;&#38480;&#30340;scaled Natarajan&#32500;&#24230;&#23545;&#20110;&#24517;&#35201;&#24615;&#30340;&#23384;&#22312;&#65292;&#20294;&#33258;&#20174;Simon 1997&#65288;SICOMP '97&#65289;&#30340;&#24037;&#20316;&#20197;&#26469;&#65292;&#23545;&#20110;&#26356;&#23436;&#25972;&#30340;&#21051;&#30011;&#30340;&#36827;&#23637;&#29978;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#26368;&#23567;&#21270;&#23454;&#20363;&#26368;&#20248;&#23398;&#20064;&#31639;&#27861;&#26469;&#23545;&#21487;&#23454;&#29616;&#22238;&#24402;&#36827;&#34892;&#23398;&#20064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26082;&#23450;&#24615;&#21448;&#23450;&#37327;&#22320;&#21051;&#30011;&#20102;&#21738;&#20123;&#31867;&#30340;&#23454;&#25968;&#39044;&#27979;&#22120;&#21487;&#20197;&#34987;&#23398;&#20064;&#30340;&#26032;&#39062;&#32500;&#24230;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#20010;&#19982;&#22270;&#32500;&#24230;&#30456;&#20851;&#30340;&#32452;&#21512;&#32500;&#24230;&#65292;&#35813;&#32500;&#24230;&#21051;&#30011;&#20102;&#22312;&#21487;&#23454;&#29616;&#35774;&#32622;&#20013;&#30340;ERM&#21487;&#23398;&#20064;&#24615;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#19982;DS&#32500;&#24230;&#30456;&#20851;&#30340;&#32452;&#21512;&#32500;&#24230;&#24314;&#31435;&#20102;&#23398;&#20064;&#21487;&#34892;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#65292;&#24182;&#29468;&#27979;&#23427;&#20063;&#21487;&#33021;&#26159;&#20805;&#20998;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we aim to characterize the statistical complexity of realizable regression both in the PAC learning setting and the online learning setting.  Previous work had established the sufficiency of finiteness of the fat shattering dimension for PAC learnability and the necessity of finiteness of the scaled Natarajan dimension, but little progress had been made towards a more complete characterization since the work of Simon 1997 (SICOMP '97). To this end, we first introduce a minimax instance optimal learner for realizable regression and propose a novel dimension that both qualitatively and quantitatively characterizes which classes of real-valued predictors are learnable. We then identify a combinatorial dimension related to the Graph dimension that characterizes ERM learnability in the realizable setting. Finally, we establish a necessary condition for learnability based on a combinatorial dimension related to the DS dimension, and conjecture that it may also be sufficient in 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20915;&#31574;&#21046;&#23450;&#35270;&#20026;&#31163;&#32447;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25506;&#31350;&#20102;&#24207;&#21015;&#24314;&#27169;&#22312;&#36712;&#36857;&#21387;&#32553;&#21644;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#65288;GCPC&#65289;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20135;&#29983;&#24378;&#22823;&#36712;&#36857;&#34920;&#31034;&#24182;&#23454;&#29616;&#39640;&#24615;&#33021;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.03406</link><description>&lt;p&gt;
&#20351;&#29992;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#20316;&#20026;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#38544;&#24335;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Goal-Conditioned Predictive Coding as an Implicit Planner for Offline Reinforcement Learning. (arXiv:2307.03406v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.03406
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#23558;&#20915;&#31574;&#21046;&#23450;&#35270;&#20026;&#31163;&#32447;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#24182;&#25506;&#31350;&#20102;&#24207;&#21015;&#24314;&#27169;&#22312;&#36712;&#36857;&#21387;&#32553;&#21644;&#31574;&#30053;&#23398;&#20064;&#20013;&#30340;&#20316;&#29992;&#12290;&#36890;&#36807;&#24341;&#20837;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#65288;GCPC&#65289;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#20135;&#29983;&#24378;&#22823;&#36712;&#36857;&#34920;&#31034;&#24182;&#23454;&#29616;&#39640;&#24615;&#33021;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24050;&#32463;&#35777;&#26126;&#20102;&#23558;&#20915;&#31574;&#21046;&#23450;&#35270;&#20026;&#31163;&#32447;&#25910;&#38598;&#30340;&#36712;&#36857;&#30340;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#36712;&#36857;&#25968;&#25454;&#19978;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#30340;&#22909;&#22788;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#24207;&#21015;&#24314;&#27169;&#26159;&#21542;&#20855;&#22791;&#23558;&#36712;&#36857;&#21387;&#32553;&#20026;&#26377;&#29992;&#34920;&#31034;&#24182;&#23545;&#31574;&#30053;&#23398;&#20064;&#26377;&#25152;&#36129;&#29486;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#30340;&#26694;&#26550;&#65292;&#39318;&#20808;&#20351;&#29992;&#24207;&#21015;&#24314;&#27169;&#25216;&#26415;&#24635;&#32467;&#36712;&#36857;&#65292;&#28982;&#21518;&#21033;&#29992;&#36825;&#20123;&#34920;&#31034;&#23398;&#20064;&#31574;&#30053;&#20197;&#21450;&#19968;&#20010;&#26399;&#26395;&#30340;&#30446;&#26631;&#12290;&#36825;&#20010;&#35774;&#35745;&#20351;&#24471;&#35768;&#22810;&#29616;&#26377;&#30340;&#30417;&#30563;&#24335;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;&#25105;&#20204;&#26694;&#26550;&#30340;&#29305;&#20363;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20869;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30446;&#26631;&#26465;&#20214;&#39044;&#27979;&#32534;&#30721;&#65288;GCPC&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#24102;&#26469;&#24378;&#22823;&#36712;&#36857;&#34920;&#31034;&#24182;&#23548;&#33268;&#39640;&#24615;&#33021;&#31574;&#30053;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;AntMaze&#65292;FrankaKitchen&#21644;Locomotion&#29615;&#22659;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#35777;&#35780;&#20272;&#65292;&#24182;&#35266;&#23519;&#21040;...
&lt;/p&gt;
&lt;p&gt;
Recent work has demonstrated the effectiveness of formulating decision making as a supervised learning problem on offline-collected trajectories. However, the benefits of performing sequence modeling on trajectory data is not yet clear. In this work we investigate if sequence modeling has the capability to condense trajectories into useful representations that can contribute to policy learning. To achieve this, we adopt a two-stage framework that first summarizes trajectories with sequence modeling techniques, and then employs these representations to learn a policy along with a desired goal. This design allows many existing supervised offline RL methods to be considered as specific instances of our framework. Within this framework, we introduce Goal-Conditioned Predicitve Coding (GCPC), an approach that brings powerful trajectory representations and leads to performant policies. We conduct extensive empirical evaluations on AntMaze, FrankaKitchen and Locomotion environments, and obser
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCP&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#26465;&#20214;&#31574;&#30053;&#20998;&#31163;&#29305;&#24449;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#24182;&#20998;&#21035;&#36827;&#34892;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;6.69%&#12290;</title><link>http://arxiv.org/abs/2307.01217</link><description>&lt;p&gt;
FedCP:&#36890;&#36807;&#26465;&#20214;&#31574;&#30053;&#23545;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#20449;&#24687;&#36827;&#34892;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy. (arXiv:2307.01217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01217
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCP&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#26465;&#20214;&#31574;&#30053;&#20998;&#31163;&#29305;&#24449;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#24182;&#20998;&#21035;&#36827;&#34892;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;6.69%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFL&#65289;&#22312;&#38544;&#31169;&#20445;&#25252;&#12289;&#21327;&#20316;&#23398;&#20064;&#20197;&#21450;&#35299;&#20915;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20363;&#22914;&#21307;&#38498;&#12289;&#31227;&#21160;&#26234;&#33021;&#25163;&#26426;&#31561;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;pFL&#26041;&#27861;&#20391;&#37325;&#20110;&#21033;&#29992;&#23458;&#25143;&#31471;&#32423;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#20294;&#24573;&#30053;&#20102;&#25968;&#25454;&#26159;&#36825;&#20004;&#31181;&#20449;&#24687;&#30340;&#28304;&#22836;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#26465;&#20214;&#31574;&#30053;&#65288;FedCP&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#27599;&#20010;&#26679;&#26412;&#29983;&#25104;&#19968;&#20010;&#26465;&#20214;&#31574;&#30053;&#65292;&#20197;&#20998;&#31163;&#20854;&#29305;&#24449;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#28982;&#21518;&#20998;&#21035;&#36890;&#36807;&#20840;&#23616;&#22836;&#21644;&#20010;&#24615;&#21270;&#22836;&#36827;&#34892;&#22788;&#29702;&#12290;&#19982;&#29616;&#26377;&#30340;pFL&#26041;&#27861;&#30456;&#27604;&#65292;FedCP&#26356;&#21152;&#32454;&#31890;&#24230;&#22320;&#32771;&#34385;&#20010;&#24615;&#21270;&#30340;&#26679;&#26412;&#29305;&#23450;&#26041;&#24335;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;FedCP&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;6.69%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, personalized federated learning (pFL) has attracted increasing attention in privacy protection, collaborative learning, and tackling statistical heterogeneity among clients, e.g., hospitals, mobile smartphones, etc. Most existing pFL methods focus on exploiting the global information and personalized information in the client-level model parameters while neglecting that data is the source of these two kinds of information. To address this, we propose the Federated Conditional Policy (FedCP) method, which generates a conditional policy for each sample to separate the global information and personalized information in its features and then processes them by a global head and a personalized head, respectively. FedCP is more fine-grained to consider personalization in a sample-specific manner than existing pFL methods. Extensive experiments in computer vision and natural language processing domains show that FedCP outperforms eleven state-of-the-art methods by up to 6.69%. Furthe
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;DeepGPET&#30340;&#24320;&#28304;&#20840;&#33258;&#21160;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#20013;&#33033;&#32476;&#33180;&#21306;&#22495;&#20998;&#21106;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#20934;&#30830;&#24615;&#21644;&#22788;&#29702;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.00904</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#20013;&#33033;&#32476;&#33180;&#30340;&#39640;&#25928;&#20840;&#33258;&#21160;&#20998;&#26512;&#30340;&#24320;&#28304;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
An open-source deep learning algorithm for efficient and fully-automatic analysis of the choroid in optical coherence tomography. (arXiv:2307.00904v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24320;&#21457;&#20102;&#19968;&#20010;&#21517;&#20026;DeepGPET&#30340;&#24320;&#28304;&#20840;&#33258;&#21160;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#20013;&#33033;&#32476;&#33180;&#21306;&#22495;&#20998;&#21106;&#65292;&#24182;&#19988;&#22312;&#25968;&#25454;&#20934;&#30830;&#24615;&#21644;&#22788;&#29702;&#36895;&#24230;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#30340;&#65306;&#24320;&#21457;&#19968;&#20010;&#24320;&#28304;&#20840;&#33258;&#21160;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;DeepGPET&#65292;&#29992;&#20110;&#20809;&#23398;&#30456;&#24178;&#26029;&#23618;&#25195;&#25551;&#65288;OCT&#65289;&#25968;&#25454;&#20013;&#33033;&#32476;&#33180;&#21306;&#22495;&#20998;&#21106;&#12290;&#26041;&#27861;&#65306;&#25105;&#20204;&#20351;&#29992;&#20102;&#26469;&#33258;3&#20010;&#19982;&#31995;&#32479;&#24615;&#30142;&#30149;&#30456;&#20851;&#30340;&#20020;&#24202;&#30740;&#31350;&#30340;715&#20010;OCT B-&#25195;&#25551;&#65288;82&#21517;&#21463;&#35797;&#32773;&#65292;115&#21482;&#30524;&#30555;&#65289;&#30340;&#25968;&#25454;&#38598;&#12290;&#20351;&#29992;&#20020;&#24202;&#39564;&#35777;&#30340;&#21322;&#33258;&#21160;&#33033;&#32476;&#33180;&#20998;&#21106;&#26041;&#27861;&#39640;&#26031;&#36807;&#31243;&#36793;&#32536;&#36861;&#36394;&#65288;GPET&#65289;&#29983;&#25104;&#20102;&#22320;&#38754;&#30495;&#23454;&#20998;&#21106;&#12290;&#25105;&#20204;&#23545;&#22312;ImageNet&#19978;&#39044;&#35757;&#32451;&#20102;MobileNetV3&#39592;&#24178;&#30340;UNet&#36827;&#34892;&#20102;&#24494;&#35843;&#12290;&#26631;&#20934;&#20998;&#21106;&#19968;&#33268;&#24615;&#25351;&#26631;&#20197;&#21450;&#33033;&#32476;&#33180;&#21402;&#24230;&#21644;&#38754;&#31215;&#30340;&#34893;&#29983;&#24230;&#37327;&#34987;&#29992;&#20110;&#35780;&#20272;DeepGPET&#65292;&#21516;&#26102;&#36824;&#36827;&#34892;&#20102;&#20020;&#24202;&#30524;&#31185;&#21307;&#29983;&#30340;&#23450;&#24615;&#35780;&#20272;&#12290;&#32467;&#26524;&#65306;DeepGPET&#22312;&#26469;&#33258;3&#20010;&#20020;&#24202;&#30740;&#31350;&#30340;&#25968;&#25454;&#19978;&#19982;GPET&#36798;&#21040;&#20102;&#24456;&#22909;&#30340;&#19968;&#33268;&#24615;&#65288;AUC = 0.9994&#65292;Dice = 0.9664&#65307;&#33033;&#32476;&#33180;&#21402;&#24230;&#30340;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;&#20026;0.8908&#65292;&#33033;&#32476;&#33180;&#38754;&#31215;&#30340;&#30382;&#23572;&#36874;&#30456;&#20851;&#31995;&#25968;&#20026;0.9082&#65289;&#65292;&#21516;&#26102;&#23558;&#22312;&#26631;&#20934;&#31508;&#35760;&#26412;&#30005;&#33041;CPU&#19978;&#27599;&#24352;&#22270;&#20687;&#30340;&#24179;&#22343;&#22788;&#29702;&#26102;&#38388;&#32553;&#30701;&#33267;34.49&#31186;
&lt;/p&gt;
&lt;p&gt;
Purpose: To develop an open-source, fully-automatic deep learning algorithm, DeepGPET, for choroid region segmentation in optical coherence tomography (OCT) data. Methods: We used a dataset of 715 OCT B-scans (82 subjects, 115 eyes) from 3 clinical studies related to systemic disease. Ground truth segmentations were generated using a clinically validated, semi-automatic choroid segmentation method, Gaussian Process Edge Tracing (GPET). We finetuned a UNet with MobileNetV3 backbone pre-trained on ImageNet. Standard segmentation agreement metrics, as well as derived measures of choroidal thickness and area, were used to evaluate DeepGPET, alongside qualitative evaluation from a clinical ophthalmologist. Results: DeepGPET achieves excellent agreement with GPET on data from 3 clinical studies (AUC=0.9994, Dice=0.9664; Pearson correlation of 0.8908 for choroidal thickness and 0.9082 for choroidal area), while reducing the mean processing time per image on a standard laptop CPU from 34.49s (
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#21106;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31934;&#30830;&#20998;&#21106;&#38382;&#39064;&#30340;&#35299;&#65292;&#36890;&#36807;&#23884;&#20837;&#20999;&#24179;&#38754;&#27861;&#25214;&#21040;&#20102;&#23481;&#37327;VRP&#30340;&#19979;&#38480;&#12290;</title><link>http://arxiv.org/abs/2306.17283</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;Rounded Capacity Inequalities&#30340;&#31070;&#32463;&#20998;&#21106;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Neural Separation Algorithm for the Rounded Capacity Inequalities. (arXiv:2306.17283v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#20998;&#21106;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#31934;&#30830;&#20998;&#21106;&#38382;&#39064;&#30340;&#35299;&#65292;&#36890;&#36807;&#23884;&#20837;&#20999;&#24179;&#38754;&#27861;&#25214;&#21040;&#20102;&#23481;&#37327;VRP&#30340;&#19979;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20999;&#24179;&#38754;&#27861;&#26159;&#25104;&#21151;&#30340;&#20998;&#25903;&#23450;&#20215;&#27861;&#21644;&#20998;&#25903;&#20999;&#21106;&#27861;&#31639;&#27861;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#36710;&#36742;&#36335;&#24452;&#38382;&#39064;&#65288;VRPs&#65289;&#30340;&#30830;&#20999;&#26368;&#20248;&#35299;&#12290;&#22312;&#21508;&#31181;&#20999;&#21106;&#20013;&#65292;&#22278;&#35282;&#23481;&#37327;&#19981;&#31561;&#24335;&#65288;RCIs&#65289;&#26159;&#26368;&#22522;&#26412;&#30340;&#12290;&#29983;&#25104;RCIs&#38656;&#35201;&#35299;&#20915;&#20998;&#21106;&#38382;&#39064;&#65292;&#20854;&#31934;&#30830;&#35299;&#38656;&#35201;&#36739;&#38271;&#30340;&#26102;&#38388;&#33719;&#21462;&#65292;&#22240;&#27492;&#24191;&#27867;&#20351;&#29992;&#21551;&#21457;&#24335;&#26041;&#27861;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#24102;&#26377;&#22270;&#31895;&#21270;&#30340;&#20998;&#21106;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#23398;&#20064;&#31934;&#30830;&#20998;&#21106;&#38382;&#39064;&#30340;&#35299;&#65292;&#32463;&#36807;50&#21040;100&#20010;&#23458;&#25143;&#30340;&#23567;&#23454;&#20363;&#35757;&#32451;&#12290;&#25105;&#20204;&#23558;&#20998;&#21106;&#31639;&#27861;&#23884;&#20837;&#20999;&#24179;&#38754;&#27861;&#20013;&#65292;&#20197;&#25214;&#21040;&#23481;&#37327;VRP&#65288;CVRP&#65289;&#30340;&#19979;&#38480;&#65292;&#20854;&#20013;&#21253;&#25324;&#39640;&#36798;1,000&#20010;&#23458;&#25143;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;CVRPSEP&#36827;&#34892;&#20102;&#24615;&#33021;&#27604;&#36739;&#65292;CVRPSEP&#26159;&#29992;&#20110;&#35299;&#20915;VRP&#20013;&#21508;&#31181;&#20999;&#21106;&#38382;&#39064;&#30340;&#27969;&#34892;&#20998;&#21106;&#36719;&#20214;&#21253;&#12290;&#25105;&#20204;&#30340;&#35745;&#31639;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#20248;&#20110;CVRPSEP&#12290;
&lt;/p&gt;
&lt;p&gt;
The cutting plane method is a key technique for successful branch-and-cut and branch-price-and-cut algorithms that find the exact optimal solutions for various vehicle routing problems (VRPs). Among various cuts, the rounded capacity inequalities (RCIs) are the most fundamental. To generate RCIs, we need to solve the separation problem, whose exact solution takes a long time to obtain; therefore, heuristic methods are widely used. We design a learning-based separation heuristic algorithm with graph coarsening that learns the solutions of the exact separation problem with a graph neural network (GNN), which is trained with small instances of 50 to 100 customers. We embed our separation algorithm within the cutting plane method to find a lower bound for the capacitated VRP (CVRP) with up to 1,000 customers. We compare the performance of our approach with CVRPSEP, a popular separation software package for various cuts used in solving VRPs. Our computational results show that our approach 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#35757;&#32451;&#27169;&#22411;&#21644;&#25512;&#36827;&#39046;&#22495;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.16424</link><description>&lt;p&gt;
&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#29992;&#20110;&#21453;&#27927;&#38065;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Realistic Synthetic Financial Transactions for Anti-Money Laundering Models. (arXiv:2306.16424v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16424
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#36924;&#30495;&#30340;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#65292;&#20197;&#28385;&#36275;&#35757;&#32451;&#27169;&#22411;&#21644;&#25512;&#36827;&#39046;&#22495;&#21457;&#23637;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#37329;&#34701;&#30340;&#24191;&#27867;&#25968;&#23383;&#21270;&#21644;&#21152;&#23494;&#36135;&#24065;&#30340;&#26085;&#30410;&#27969;&#34892;&#65292;&#32593;&#32476;&#29359;&#32618;&#20998;&#23376;&#35774;&#35745;&#30340;&#27450;&#35784;&#26041;&#26696;&#36234;&#26469;&#36234;&#22797;&#26434;&#12290;&#27927;&#38065;&#8212;&#8212;&#23558;&#38750;&#27861;&#36164;&#37329;&#31227;&#21160;&#20197;&#25513;&#30422;&#20854;&#26469;&#28304;&#8212;&#8212;&#21487;&#20197;&#36328;&#36234;&#38134;&#34892;&#21644;&#22269;&#30028;&#65292;&#20135;&#29983;&#22797;&#26434;&#30340;&#20132;&#26131;&#27169;&#24335;&#12290;&#32852;&#21512;&#22269;&#20272;&#35745;&#27599;&#24180;&#20840;&#29699;&#27927;&#38065;&#37329;&#39069;&#21344;&#20840;&#29699;GDP&#30340;2-5%&#65292;&#32422;&#20026;0.8-2.0&#19975;&#20159;&#32654;&#20803;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36890;&#24120;&#26080;&#27861;&#33719;&#24471;&#29992;&#20110;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#27927;&#38065;&#30340;&#30495;&#23454;&#25968;&#25454;&#65292;&#19988;&#20043;&#21069;&#30340;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#23384;&#22312;&#26174;&#33879;&#32570;&#38519;&#12290;&#20026;&#20102;&#27604;&#36739;&#27169;&#22411;&#24182;&#25512;&#36827;&#35813;&#39046;&#22495;&#30340;&#21457;&#23637;&#65292;&#38656;&#35201;&#19968;&#20010;&#36924;&#30495;&#12289;&#26631;&#20934;&#21270;&#12289;&#20844;&#24320;&#21487;&#29992;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21512;&#25104;&#37329;&#34701;&#20132;&#26131;&#25968;&#25454;&#38598;&#29983;&#25104;&#22120;&#21644;&#19968;&#32452;&#21512;&#25104;&#30340;&#21453;&#27927;&#38065;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#26681;&#25454;&#23454;&#38469;&#20132;&#26131;&#23613;&#21487;&#33021;&#22320;&#26657;&#20934;&#20102;&#36825;&#20010;&#22522;&#20110;&#20195;&#29702;&#30340;&#29983;&#25104;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the widespread digitization of finance and the increasing popularity of cryptocurrencies, the sophistication of fraud schemes devised by cybercriminals is growing. Money laundering -- the movement of illicit funds to conceal their origins -- can cross bank and national boundaries, producing complex transaction patterns. The UN estimates 2-5\% of global GDP or \$0.8 - \$2.0 trillion dollars are laundered globally each year. Unfortunately, real data to train machine learning models to detect laundering is generally not available, and previous synthetic data generators have had significant shortcomings. A realistic, standardized, publicly-available benchmark is needed for comparing models and for the advancement of the area.  To this end, this paper contributes a synthetic financial transaction dataset generator and a set of synthetically generated AML (Anti-Money Laundering) datasets. We have calibrated this agent-based generator to match real transactions as closely as possible and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#24182;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.15156</link><description>&lt;p&gt;
&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Learning non-Markovian Decision-Making from State-only Sequences. (arXiv:2306.15156v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.15156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20174;&#20165;&#29366;&#24577;&#24207;&#21015;&#23398;&#20064;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#21644;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#33021;&#22815;&#23454;&#29616;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#24182;&#22312;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#23637;&#31034;&#20102;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#27169;&#20223;&#23398;&#20064;&#20551;&#35774;&#33021;&#22815;&#33719;&#24471;&#23637;&#31034;&#32773;&#30340;&#21160;&#20316;&#65292;&#20294;&#26159;&#22312;&#33258;&#28982;&#29615;&#22659;&#20013;&#36825;&#20123;&#21160;&#20316;&#36890;&#24120;&#26080;&#27861;&#35266;&#27979;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#30340;&#24207;&#21015;&#20915;&#31574;&#34892;&#20026;&#21487;&#33021;&#20559;&#31163;&#26631;&#20934;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#30340;&#20551;&#35774;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#38750;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;nMDP&#65289;&#20013;&#20165;&#29366;&#24577;&#24207;&#21015;&#30340;&#28145;&#24230;&#29983;&#25104;&#24314;&#27169;&#65292;&#20854;&#20013;&#31574;&#30053;&#26159;&#28508;&#22312;&#29366;&#24577;&#36716;&#31227;&#29983;&#25104;&#22120;&#30340;&#33021;&#37327;&#20808;&#39564;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#26368;&#22823;&#20284;&#28982;&#20272;&#35745;&#26469;&#23454;&#29616;&#22522;&#20110;&#27169;&#22411;&#30340;&#27169;&#20223;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#20808;&#39564;&#36827;&#34892;&#30701;&#26399;MCMC&#37319;&#26679;&#21644;&#23545;&#21518;&#39564;&#36827;&#34892;&#37325;&#35201;&#24615;&#37319;&#26679;&#12290;&#23398;&#20064;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#8220;&#25512;&#29702;&#24335;&#20915;&#31574;&#8221;&#65292;&#21363;&#26080;&#27169;&#22411;&#31574;&#30053;&#25191;&#34892;&#31561;&#20215;&#20110;&#20808;&#39564;&#37319;&#26679;&#65292;&#22522;&#20110;&#27169;&#22411;&#30340;&#35268;&#21010;&#21017;&#26159;&#20174;&#31574;&#30053;&#21021;&#22987;&#21270;&#30340;&#21518;&#39564;&#37319;&#26679;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20855;&#26377;&#38750;&#39532;&#23572;&#31185;&#22827;&#29305;&#24449;&#30340;&#21407;&#22411;&#36335;&#24452;&#35268;&#21010;&#20219;&#21153;&#20013;&#35777;&#26126;&#20102;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Conventional imitation learning assumes access to the actions of demonstrators, but these motor signals are often non-observable in naturalistic settings. Additionally, sequential decision-making behaviors in these settings can deviate from the assumptions of a standard Markov Decision Process (MDP). To address these challenges, we explore deep generative modeling of state-only sequences with non-Markov Decision Process (nMDP), where the policy is an energy-based prior in the latent space of the state transition generator. We develop maximum likelihood estimation to achieve model-based imitation, which involves short-run MCMC sampling from the prior and importance sampling for the posterior. The learned model enables \textit{decision-making as inference}: model-free policy execution is equivalent to prior sampling, model-based planning is posterior sampling initialized from the policy. We demonstrate the efficacy of the proposed method in a prototypical path planning task with non-Mark
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#20248;&#21270;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#28436;&#21069;&#21521;&#27169;&#22411;&#12289;&#23454;&#26102;&#20248;&#21270;&#32534;&#30721;&#21442;&#25968;&#31561;&#25163;&#27573;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24863;&#30693;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2306.13104</link><description>&lt;p&gt;
&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#30340;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Human-in-the-Loop Optimization for Deep Stimulus Encoding in Visual Prostheses. (arXiv:2306.13104v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#20248;&#21270;&#8221;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21453;&#28436;&#21069;&#21521;&#27169;&#22411;&#12289;&#23454;&#26102;&#20248;&#21270;&#32534;&#30721;&#21442;&#25968;&#31561;&#25163;&#27573;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#21069;&#21015;&#33146;&#22312;&#24674;&#22797;&#22833;&#21435;&#30340;&#24863;&#23448;&#21151;&#33021;&#21644;&#22686;&#24378;&#20154;&#31867;&#33021;&#21147;&#26041;&#38754;&#20855;&#26377;&#28508;&#21147;&#65292;&#20294;&#24403;&#21069;&#35774;&#22791;&#20135;&#29983;&#30340;&#24863;&#35273;&#36890;&#24120;&#20284;&#20046;&#19981;&#33258;&#28982;&#25110;&#25197;&#26354;&#12290;&#26893;&#20837;&#22120;&#30340;&#30830;&#20999;&#20301;&#32622;&#21644;&#20010;&#20307;&#24863;&#30693;&#30340;&#24046;&#24322;&#23548;&#33268;&#21050;&#28608;&#21709;&#24212;&#23384;&#22312;&#26174;&#30528;&#24046;&#24322;&#65292;&#20351;&#20010;&#24615;&#21270;&#21050;&#28608;&#20248;&#21270;&#25104;&#20026;&#20851;&#38190;&#25361;&#25112;&#12290;&#36125;&#21494;&#26031;&#20248;&#21270;&#21487;&#29992;&#20110;&#20248;&#21270;&#20855;&#26377;&#26377;&#38480;&#22122;&#22768;&#35266;&#23519;&#25968;&#25454;&#30340;&#24739;&#32773;&#19987;&#23646;&#21050;&#28608;&#21442;&#25968;&#65292;&#20294;&#23545;&#20110;&#39640;&#32500;&#21050;&#28608;&#19981;&#21487;&#34892;&#12290;&#32780;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#20248;&#21270;&#21050;&#28608;&#32534;&#30721;&#31574;&#30053;&#65292;&#20294;&#36890;&#24120;&#20551;&#35774;&#26377;&#20851;&#24739;&#32773;&#29305;&#23450;&#21464;&#21270;&#30340;&#23436;&#32654;&#30693;&#35782;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#12289;&#23454;&#38469;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#20811;&#26381;&#20102;&#36825;&#20004;&#20010;&#22522;&#26412;&#23616;&#38480;&#24615;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#21453;&#28436;&#23558;&#30005;&#21050;&#28608;&#26144;&#23556;&#21040;&#35270;&#35273;&#24863;&#30693;&#30340;&#21069;&#21521;&#27169;&#22411;&#65292;&#35757;&#32451;&#28145;&#24230;&#32534;&#30721;&#22120;&#32593;&#32476;&#20197;&#20026;&#20219;&#20309;&#20010;&#20307;&#24739;&#32773;&#20135;&#29983;&#26368;&#20339;&#21050;&#28608;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#36873;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#20197;&#23454;&#26102;&#20248;&#21270;&#32534;&#30721;&#21442;&#25968;&#65292;&#25104;&#21151;&#20351;&#30693;&#35273;&#21050;&#28608;&#26356;&#21152;&#36924;&#30495;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#8220;&#20154;&#31867;&#20171;&#20837;&#30340;&#35270;&#35273;&#21069;&#21015;&#33146;&#28145;&#24230;&#21050;&#28608;&#32534;&#30721;&#20248;&#21270;&#8221;&#26041;&#27861;&#22312;&#21160;&#29289;&#27169;&#22411;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#30456;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#24863;&#30693;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neuroprostheses show potential in restoring lost sensory function and enhancing human capabilities, but the sensations produced by current devices often seem unnatural or distorted. Exact placement of implants and differences in individual perception lead to significant variations in stimulus response, making personalized stimulus optimization a key challenge. Bayesian optimization could be used to optimize patient-specific stimulation parameters with limited noisy observations, but is not feasible for high-dimensional stimuli. Alternatively, deep learning models can optimize stimulus encoding strategies, but typically assume perfect knowledge of patient-specific variations. Here we propose a novel, practically feasible approach that overcomes both of these fundamental limitations. First, a deep encoder network is trained to produce optimal stimuli for any individual patient by inverting a forward model mapping electrical stimuli to visual percepts. Second, a preferential Bayesian opti
&lt;/p&gt;</description></item><item><title>SRe$^2$L&#26159;&#19968;&#31181;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2306.13092</link><description>&lt;p&gt;
&#20174;&#26032;&#30340;&#35282;&#24230;&#21387;&#32553;ImageNet&#35268;&#27169;&#25968;&#25454;&#38598;&#65306;SRe$^2$L
&lt;/p&gt;
&lt;p&gt;
Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective. (arXiv:2306.13092v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13092
&lt;/p&gt;
&lt;p&gt;
SRe$^2$L&#26159;&#19968;&#31181;&#25968;&#25454;&#38598;&#21387;&#32553;&#26041;&#27861;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#65292;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#26368;&#20339;&#24615;&#33021;&#65292;&#24182;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#26694;&#26550;&#65292;&#31216;&#20026;Squeeze&#12289;Recover&#21644;Relabel&#65288;SRe$^2$L&#65289;&#65292;&#23427;&#22312;&#35757;&#32451;&#26399;&#38388;&#20998;&#31163;&#20102;&#27169;&#22411;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#21452;&#23618;&#20248;&#21270;&#65292;&#20197;&#22788;&#29702;&#19981;&#21516;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#20307;&#31995;&#32467;&#26500;&#21644;&#22270;&#20687;&#20998;&#36776;&#29575;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#30340;&#25968;&#25454;&#38598;&#21387;&#32553;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#23637;&#31034;&#20102;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#35268;&#27169;&#19978;&#30340;&#28789;&#27963;&#24615;&#65292;&#24182;&#22312;&#21512;&#25104;&#22270;&#20687;&#20219;&#24847;&#20998;&#36776;&#29575;&#12289;&#39640;&#20998;&#36776;&#29575;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#20302;&#35757;&#32451;&#25104;&#26412;&#21644;&#20869;&#23384;&#28040;&#32791;&#20197;&#21450;&#25193;&#23637;&#21040;&#20219;&#24847;&#35780;&#20272;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;Tiny-ImageNet&#21644;&#23436;&#25972;&#30340;ImageNet-1K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#22312;50IPC&#19979;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Tiny-ImageNet&#21644;ImageNet-1K&#19978;&#20998;&#21035;&#23454;&#29616;&#20102;42.5&#65285;&#21644;60.8&#65285;&#30340;&#26368;&#39640;&#39564;&#35777;&#31934;&#24230;&#65292;&#36739;&#20043;&#21069;&#25152;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#25552;&#39640;&#20102;14.5&#65285;&#21644;32.9&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;Res&#19978;&#20063;&#27604;MTT&#24555;&#32422;52&#20493;(ConvNet-4)&#21644;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for effective dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution training, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also outperforms MTT by approximately 52$\times$ (ConvNet-4) and 16$\times$ (Res
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.10759</link><description>&lt;p&gt;
&#20026;&#22823;&#22411;&#22270;&#34920;&#31034;&#31616;&#21270;&#21644;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
Simplifying and Empowering Transformers for Large-Graph Representations. (arXiv:2306.10759v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.10759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#22823;&#22411;&#22270;&#19978;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#21363;&#21487;&#33719;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#65292;&#25361;&#25112;&#20102;&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#22797;&#26434;&#27169;&#22411;&#30340;&#24212;&#29992;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20197;&#25552;&#39640;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#22270;&#19978;&#23398;&#20064;&#34920;&#31034;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#65292;&#22240;&#20026;&#20854;&#20013;&#28041;&#21450;&#20102;&#22823;&#37327;&#25968;&#25454;&#28857;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;Transformer&#20316;&#20026;&#19968;&#31181;&#26032;&#20852;&#30340;&#29992;&#20110;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#22522;&#26412;&#32534;&#30721;&#22120;&#31867;&#21035;&#65292;&#30001;&#20110;&#20854;&#20840;&#23616;&#27880;&#24847;&#21147;&#21487;&#20197;&#25429;&#25417;&#21040;&#37051;&#33410;&#28857;&#20043;&#22806;&#30340;&#25152;&#26377;&#23545;&#24433;&#21709;&#65292;&#22240;&#27492;&#22312;&#23567;&#22411;&#22270;&#19978;&#34920;&#29616;&#20986;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#29616;&#26377;&#26041;&#27861;&#24448;&#24448;&#32487;&#25215;&#20102;Transformer&#22312;&#35821;&#35328;&#21644;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#24605;&#24819;&#65292;&#24182;&#36890;&#36807;&#22534;&#21472;&#28145;&#23618;&#22810;&#22836;&#27880;&#24847;&#21147;&#26469;&#37319;&#29992;&#22797;&#26434;&#30340;&#27169;&#22411;&#12290;&#26412;&#25991;&#36890;&#36807;&#20851;&#20110;&#33410;&#28857;&#23646;&#24615;&#39044;&#27979;&#22522;&#20934;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#21363;&#20351;&#21482;&#20351;&#29992;&#19968;&#23618;&#27880;&#24847;&#21147;&#20063;&#33021;&#22312;&#33410;&#28857;&#25968;&#37327;&#20174;&#21315;&#32423;&#21040;&#21313;&#20159;&#32423;&#30340;&#33539;&#22260;&#20869;&#24102;&#26469;&#20196;&#20154;&#24778;&#35766;&#30340;&#31454;&#20105;&#24615;&#33021;&#12290;&#36825;&#40723;&#21169;&#25105;&#20204;&#37325;&#26032;&#24605;&#32771;&#22312;&#22823;&#22411;&#22270;&#19978;&#35774;&#35745;Transformer&#30340;&#29702;&#24565;&#65292;&#20854;&#20013;&#20840;&#23616;&#27880;&#24847;&#21147;&#26159;&#19968;&#20010;&#38459;&#30861;&#21487;&#25193;&#23637;&#24615;&#30340;&#35745;&#31639;&#24320;&#38144;&#12290;&#25105;&#20204;&#23558;&#25552;&#20986;&#30340;&#26041;&#26696;&#31216;&#20026;&#31616;&#21270;&#22270;Transformer&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning representations on large-sized graphs is a long-standing challenge due to the inter-dependence nature involved in massive data points. Transformers, as an emerging class of foundation encoders for graph-structured data, have shown promising performance on small graphs due to its global attention capable of capturing all-pair influence beyond neighboring nodes. Even so, existing approaches tend to inherit the spirit of Transformers in language and vision tasks, and embrace complicated models by stacking deep multi-head attentions. In this paper, we critically demonstrate that even using a one-layer attention can bring up surprisingly competitive performance across node property prediction benchmarks where node numbers range from thousand-level to billion-level. This encourages us to rethink the design philosophy for Transformers on large graphs, where the global attention is a computation overhead hindering the scalability. We frame the proposed scheme as Simplified Graph Trans
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26469;&#36827;&#34892;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#31574;&#30053;&#23450;&#21046;&#65292;&#26080;&#38656;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2306.09526</link><description>&lt;p&gt;
&#27531;&#24046; Q &#23398;&#20064;&#65306;&#26080;&#38656;&#20215;&#20540;&#30340;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
Residual Q-Learning: Offline and Online Policy Customization without Value. (arXiv:2306.09526v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09526
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26469;&#36827;&#34892;&#31163;&#32447;&#21644;&#22312;&#32447;&#30340;&#31574;&#30053;&#23450;&#21046;&#65292;&#26080;&#38656;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27169;&#20223;&#23398;&#20064;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#26694;&#26550;&#65292;&#36866;&#29992;&#20110;&#20174;&#28436;&#31034;&#20013;&#23398;&#20064;&#27169;&#20223;&#34892;&#20026;&#12290;&#24403;&#25163;&#24037;&#21046;&#20316;&#22870;&#21169;&#20989;&#25968;&#22256;&#38590;&#25110;&#30446;&#26631;&#26159;&#27169;&#20223;&#20154;&#31867;&#19987;&#23478;&#34892;&#20026;&#26102;&#65292;&#36825;&#31181;&#26041;&#27861;&#29305;&#21035;&#26377;&#21560;&#24341;&#21147;&#12290;&#20294;&#26159;&#65292;&#23398;&#20064;&#30340;&#27169;&#20223;&#31574;&#30053;&#21482;&#33021;&#36981;&#24490;&#28436;&#31034;&#20013;&#30340;&#34892;&#20026;&#12290;&#22312;&#24212;&#29992;&#27169;&#20223;&#31574;&#30053;&#26102;&#65292;&#25105;&#20204;&#21487;&#33021;&#38656;&#35201;&#26681;&#25454;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#35201;&#27714;&#23450;&#21046;&#31574;&#30053;&#34892;&#20026;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#20173;&#24076;&#26395;&#23450;&#21046;&#30340;&#31574;&#30053;&#20445;&#25345;&#20854;&#27169;&#20223;&#24615;&#36136;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#38382;&#39064;&#35774;&#32622;&#65292;&#31216;&#20026;&#31574;&#30053;&#23450;&#21046;&#12290;&#23427;&#23558;&#23398;&#20064;&#20219;&#21153;&#23450;&#20041;&#20026;&#35757;&#32451;&#19968;&#31181;&#31574;&#30053;&#65292;&#35813;&#31574;&#30053;&#32487;&#25215;&#20808;&#21069;&#31574;&#30053;&#30340;&#29305;&#24615;&#65292;&#21516;&#26102;&#28385;&#36275;&#30446;&#26631;&#19979;&#28216;&#20219;&#21153;&#24378;&#21152;&#30340;&#19968;&#20123;&#38468;&#21152;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#21644;&#26377;&#21407;&#21017;&#30340;&#26041;&#27861;&#26469;&#35299;&#37322;&#21644;&#30830;&#23450;&#20004;&#20010;&#20219;&#21153;&#30446;&#26631;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#31181;&#21160;&#24577;&#25511;&#21046;&#27531;&#24046;&#30340; Q &#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#20351;&#29992;&#20215;&#20540;&#20989;&#25968;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#21644;&#31163;&#32447;&#31574;&#30053;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Imitation Learning (IL) is a widely used framework for learning imitative behavior from demonstrations. It is especially appealing for solving complex real-world tasks where handcrafting reward function is difficult, or when the goal is to mimic human expert behavior. However, the learned imitative policy can only follow the behavior in the demonstration. When applying the imitative policy, we may need to customize the policy behavior to meet different requirements coming from diverse downstream tasks. Meanwhile, we still want the customized policy to maintain its imitative nature. To this end, we formulate a new problem setting called policy customization. It defines the learning task as training a policy that inherits the characteristics of the prior policy while satisfying some additional requirements imposed by a target downstream task. We propose a novel and principled approach to interpret and determine the trade-off between the two task objectives. Specifically, we formulate the
&lt;/p&gt;</description></item><item><title>&#26681;&#25454;&#20154;&#31867;&#35760;&#24518;&#21644;&#25512;&#29702;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28436;&#21270;LLM&#26234;&#33021;&#20307;&#26694;&#26550;REMEMBERER&#65292;&#36890;&#36807;&#20026;LLM&#35013;&#22791;&#38271;&#26399;&#32463;&#39564;&#35760;&#24518;&#65292;&#21487;&#20197;&#20026;&#19981;&#21516;&#20219;&#21153;&#25552;&#20379;&#20248;&#24322;&#30340;&#26234;&#33021;&#20307;&#65292;&#20854;&#26500;&#25104;&#20102;&#21322;&#21442;&#25968;RL&#20195;&#29702;&#12290;&#25104;&#21151;&#29575;&#36229;&#36807;&#20808;&#21069;SOTA 4&#65285;&#21644;2&#65285;&#12290;</title><link>http://arxiv.org/abs/2306.07929</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26159;&#21322;&#21442;&#25968;&#24378;&#21270;&#23398;&#20064;&#26234;&#33021;&#20307;
&lt;/p&gt;
&lt;p&gt;
Large Language Model Is Semi-Parametric Reinforcement Learning Agent. (arXiv:2306.07929v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07929
&lt;/p&gt;
&lt;p&gt;
&#26681;&#25454;&#20154;&#31867;&#35760;&#24518;&#21644;&#25512;&#29702;&#26426;&#21046;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28436;&#21270;LLM&#26234;&#33021;&#20307;&#26694;&#26550;REMEMBERER&#65292;&#36890;&#36807;&#20026;LLM&#35013;&#22791;&#38271;&#26399;&#32463;&#39564;&#35760;&#24518;&#65292;&#21487;&#20197;&#20026;&#19981;&#21516;&#20219;&#21153;&#25552;&#20379;&#20248;&#24322;&#30340;&#26234;&#33021;&#20307;&#65292;&#20854;&#26500;&#25104;&#20102;&#21322;&#21442;&#25968;RL&#20195;&#29702;&#12290;&#25104;&#21151;&#29575;&#36229;&#36807;&#20808;&#21069;SOTA 4&#65285;&#21644;2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#35748;&#30693;&#31185;&#23398;&#23545;&#20154;&#31867;&#35760;&#24518;&#21644;&#25512;&#29702;&#26426;&#21046;&#30340;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21487;&#28436;&#21270;LLM&#65288;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65289;&#26234;&#33021;&#20307;&#26694;&#26550;REMEMBERER&#12290;&#36890;&#36807;&#20026;LLM&#35013;&#22791;&#38271;&#26399;&#32463;&#39564;&#35760;&#24518;&#65292;REMEMBERER&#33021;&#22815;&#21033;&#29992;&#36807;&#21435;&#21095;&#38598;&#30340;&#32463;&#39564;&#65292;&#29978;&#33267;&#21487;&#20197;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#30446;&#26631;&#25552;&#20379;&#20248;&#24322;&#30340;LLM&#26234;&#33021;&#20307;&#65292;&#36825;&#20248;&#20110;&#20855;&#26377;&#22266;&#23450;&#23454;&#20363;&#25110;&#20855;&#26377;&#30701;&#26242;&#24037;&#20316;&#35760;&#24518;&#30340;LLM&#26234;&#33021;&#20307;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20171;&#32461;&#20102;&#32463;&#39564;&#35760;&#24518;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLEM&#65289;&#26469;&#26356;&#26032;&#35760;&#24518;&#12290;&#22240;&#27492;&#65292;&#25972;&#20010;&#31995;&#32479;&#21487;&#20197;&#20174;&#25104;&#21151;&#21644;&#22833;&#36133;&#30340;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#24182;&#22312;&#19981;&#24494;&#35843;LLM&#21442;&#25968;&#30340;&#24773;&#20917;&#19979;&#21457;&#23637;&#20854;&#33021;&#21147;&#12290;&#20197;&#27492;&#26041;&#24335;&#65292;&#25152;&#25552;&#20986;&#30340;REMEMBERER&#26500;&#25104;&#20102;&#21322;&#21442;&#25968;RL&#20195;&#29702;&#12290;&#22312;&#20004;&#20010;RL&#20219;&#21153;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#20197;&#35780;&#20272;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#12290;&#19981;&#21516;&#21021;&#22987;&#21270;&#21644;&#35757;&#32451;&#38598;&#30340;&#24179;&#22343;&#32467;&#26524;&#23545;&#20110;&#25104;&#21151;&#29575;&#36229;&#36807;&#20808;&#21069;SOTA 4&#65285;&#21644;2&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the insights in cognitive science with respect to human memory and reasoning mechanism, a novel evolvable LLM-based (Large Language Model) agent framework is proposed as REMEMBERER. By equipping the LLM with a long-term experience memory, REMEMBERER is capable of exploiting the experiences from the past episodes even for different task goals, which excels an LLM-based agent with fixed exemplars or equipped with a transient working memory. We further introduce Reinforcement Learning with Experience Memory (RLEM) to update the memory. Thus, the whole system can learn from the experiences of both success and failure, and evolve its capability without fine-tuning the parameters of the LLM. In this way, the proposed REMEMBERER constitutes a semi-parametric RL agent. Extensive experiments are conducted on two RL task sets to evaluate the proposed framework. The average results with different initialization and training sets exceed the prior SOTA by 4% and 2% for the success rate 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#26469;&#32479;&#19968;&#23450;&#20041;&#21644;&#28548;&#28165;&#33258;&#21160;&#27010;&#24565;&#25552;&#21462;&#21644;&#27010;&#24565;&#37325;&#35201;&#24615;&#35780;&#20272;&#65292;&#36827;&#32780;&#25552;&#20379;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#23454;&#29616;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#27604;&#36739;&#20197;&#21450;&#25512;&#23548;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;</title><link>http://arxiv.org/abs/2306.07304</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#33258;&#21160;&#27010;&#24565;&#25552;&#21462;&#21644;&#27010;&#24565;&#37325;&#35201;&#24615;&#35780;&#20272;&#30340;&#20840;&#38754;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Holistic Approach to Unifying Automatic Concept Extraction and Concept Importance Estimation. (arXiv:2306.07304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.07304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#26469;&#32479;&#19968;&#23450;&#20041;&#21644;&#28548;&#28165;&#33258;&#21160;&#27010;&#24565;&#25552;&#21462;&#21644;&#27010;&#24565;&#37325;&#35201;&#24615;&#35780;&#20272;&#65292;&#36827;&#32780;&#25552;&#20379;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#20197;&#23454;&#29616;&#23545;&#36825;&#20123;&#26041;&#27861;&#30340;&#27604;&#36739;&#20197;&#21450;&#25512;&#23548;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#25104;&#20026;&#20102;&#19968;&#20123;&#26368;&#26377;&#21069;&#36884;&#30340;&#21487;&#35299;&#37322;&#26041;&#27861;&#65292;&#24110;&#21161;&#25105;&#20204;&#35299;&#37322;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30340;&#20915;&#31574;&#12290;&#36825;&#20123;&#26041;&#27861;&#35797;&#22270;&#22312;&#20004;&#20010;&#20851;&#38190;&#27493;&#39588;&#20013;&#21457;&#29616;&#34987;&#38544;&#34255;&#22312;ANN&#28608;&#27963;&#30340;&#22797;&#26434;&#27169;&#24335;&#20013;&#30340;&#21487;&#29702;&#35299;&#30340;&#35270;&#35273;&#8220;&#27010;&#24565;&#8221;&#65306;&#65288;1&#65289;&#27010;&#24565;&#25552;&#21462;&#65292;&#65288;2&#65289;&#37325;&#35201;&#24615;&#35780;&#20272;&#12290;&#34429;&#28982;&#36825;&#20004;&#20010;&#27493;&#39588;&#26159;&#21508;&#31181;&#26041;&#27861;&#20043;&#38388;&#20849;&#21516;&#30340;&#65292;&#20294;&#23427;&#20204;&#30340;&#20855;&#20307;&#23454;&#29616;&#37117;&#26377;&#25152;&#19981;&#21516;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#29702;&#35770;&#26694;&#26550;&#65292;&#20840;&#38754;&#23450;&#20041;&#21644;&#28548;&#28165;&#20102;&#36825;&#20004;&#20010;&#27493;&#39588;&#12290;&#35813;&#26694;&#26550;&#20855;&#26377;&#20960;&#20010;&#20248;&#28857;&#65292;&#23427;&#20801;&#35768;&#25105;&#20204;&#65306;&#65288;i&#65289;&#25552;&#20986;&#26032;&#30340;&#35780;&#20272;&#25351;&#26631;&#26469;&#27604;&#36739;&#19981;&#21516;&#30340;&#27010;&#24565;&#25552;&#21462;&#26041;&#27861;&#65307;&#65288;ii&#65289;&#21033;&#29992;&#29616;&#20195;&#24402;&#22240;&#26041;&#27861;&#21644;&#35780;&#20272;&#25351;&#26631;&#26469;&#25193;&#23637;&#21644;&#31995;&#32479;&#22320;&#35780;&#20272;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#21644;&#37325;&#35201;&#24615;&#35780;&#20272;&#25216;&#26415;&#65307;&#65288;iii&#65289;&#25512;&#23548;&#20851;&#20110;&#36825;&#31181;&#26041;&#27861;&#30340;&#26368;&#20248;&#24615;&#30340;&#29702;&#35770;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, concept-based approaches have emerged as some of the most promising explainability methods to help us interpret the decisions of Artificial Neural Networks (ANNs). These methods seek to discover intelligible visual 'concepts' buried within the complex patterns of ANN activations in two key steps: (1) concept extraction followed by (2) importance estimation. While these two steps are shared across methods, they all differ in their specific implementations. Here, we introduce a unifying theoretical framework that comprehensively defines and clarifies these two steps. This framework offers several advantages as it allows us: (i) to propose new evaluation metrics for comparing different concept extraction approaches; (ii) to leverage modern attribution methods and evaluation metrics to extend and systematically evaluate state-of-the-art concept-based approaches and importance estimation techniques; (iii) to derive theoretical guarantees regarding the optimality of such met
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACO&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#30456;&#20301;&#35889;&#29983;&#25104;&#22270;&#20687;&#65292;&#21516;&#26102;&#20445;&#25345;&#24133;&#24230;&#24658;&#23450;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.06805</link><description>&lt;p&gt;
&#29992;&#24133;&#24230;&#21463;&#38480;&#21046;&#20248;&#21270;&#35299;&#38145;&#26356;&#28145;&#23618;&#32593;&#32476;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Unlocking Feature Visualization for Deeper Networks with MAgnitude Constrained Optimization. (arXiv:2306.06805v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06805
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MACO&#30340;&#31616;&#21333;&#26041;&#27861;&#65292;&#36890;&#36807;&#20248;&#21270;&#30456;&#20301;&#35889;&#29983;&#25104;&#22270;&#20687;&#65292;&#21516;&#26102;&#20445;&#25345;&#24133;&#24230;&#24658;&#23450;&#65292;&#35299;&#20915;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19978;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#20102;&#39640;&#36136;&#37327;&#21644;&#39640;&#25928;&#30340;&#21487;&#35299;&#37322;&#29305;&#24449;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#21487;&#35270;&#21270;&#22312;Olah&#31561;&#20154;2017&#24180;&#30340;&#26377;&#24433;&#21709;&#21147;&#30340;&#24037;&#20316;&#20043;&#21518;&#33719;&#24471;&#20102;&#24456;&#22823;&#30340; popularity&#65292;&#23558;&#20854;&#30830;&#31435;&#20026;&#21487;&#35299;&#37322;&#24615;&#30340;&#37325;&#35201;&#24037;&#20855;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20381;&#36182;&#20110;&#29983;&#25104;&#21487;&#35299;&#37322;&#22270;&#20687;&#30340;&#25216;&#24039;&#20197;&#21450;&#22312;&#23558;&#20854;&#25193;&#23637;&#21040;&#26356;&#28145;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#20854;&#24191;&#27867;&#24212;&#29992;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;MACO&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#20027;&#35201;&#24605;&#24819;&#26159;&#36890;&#36807;&#20248;&#21270;&#30456;&#20301;&#35889;&#29983;&#25104;&#22270;&#20687;&#65292;&#21516;&#26102;&#20445;&#25345;&#24133;&#24230;&#24658;&#23450;&#65292;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#35299;&#37322;&#20301;&#20110;&#33258;&#28982;&#22270;&#20687;&#31354;&#38388;&#20013;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23450;&#24615;&#21644;&#23450;&#37327;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#30528;&#30340;&#25913;&#36827;&#65292;&#24182;&#20026;&#22823;&#22411;&#26368;&#20808;&#36827;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#39640;&#25928;&#19988;&#21487;&#35299;&#37322;&#30340;&#29305;&#24449;&#21487;&#35270;&#21270;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#19968;&#20010;&#23646;&#24615;&#26426;&#21046;&#65292;&#21487;&#20197;&#22686;&#24378;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#31354;&#38388;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature visualization has gained substantial popularity, particularly after the influential work by Olah et al. in 2017, which established it as a crucial tool for explainability. However, its widespread adoption has been limited due to a reliance on tricks to generate interpretable images, and corresponding challenges in scaling it to deeper neural networks. Here, we describe MACO, a simple approach to address these shortcomings. The main idea is to generate images by optimizing the phase spectrum while keeping the magnitude constant to ensure that generated explanations lie in the space of natural images. Our approach yields significantly better results (both qualitatively and quantitatively) and unlocks efficient and interpretable feature visualizations for large state-of-the-art neural networks. We also show that our approach exhibits an attribution mechanism allowing us to augment feature visualizations with spatial importance. We validate our method on a novel benchmark for compa
&lt;/p&gt;</description></item><item><title>ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2306.06446</link><description>&lt;p&gt;
ShiftAddViT&#65306;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#28151;&#21512;&#23454;&#29616;&#39640;&#25928;&#30340;&#35270;&#35273;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. (arXiv:2306.06446v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06446
&lt;/p&gt;
&lt;p&gt;
ShiftAddViT&#36890;&#36807;&#20351;&#29992;&#20301;&#31227;&#21644;&#21152;&#27861;&#31561;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#23545;ViT&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#65292;&#23454;&#29616;&#20102;&#20943;&#23569;&#20056;&#27861;&#25805;&#20316;&#30340;&#39640;&#25928;&#35270;&#35273;&#21464;&#25442;&#22120;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;GPU&#19978;&#23454;&#29616;&#31471;&#21040;&#31471;&#30340;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;&#65288;ViT&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#22810;&#20010;&#35270;&#35273;&#20219;&#21153;&#30340;&#32479;&#19968;&#39592;&#24178;&#12290;&#20294;&#26159;&#65292;ViTs&#20013;&#30340;&#27880;&#24847;&#21147;&#21644;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLPs&#65289;&#30001;&#20110;&#23494;&#38598;&#30340;&#20056;&#27861;&#32780;&#19981;&#22815;&#39640;&#25928;&#65292;&#23548;&#33268;&#35757;&#32451;&#21644;&#25512;&#29702;&#20195;&#20215;&#39640;&#26114;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#39044;&#35757;&#32451;&#30340;ViT&#20197;&#22810;&#31181;&#20056;&#27861;&#21407;&#35821;&#65288;&#20363;&#22914;&#20301;&#31227;&#21644;&#21152;&#27861;&#65289;&#37325;&#26032;&#21442;&#25968;&#21270;&#30340;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#20840;&#26032;&#31867;&#22411;&#30340;&#20943;&#23569;&#20056;&#27861;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;ShiftAddViT&#65292;&#26088;&#22312;&#23454;&#29616;GPU&#19978;&#30340;&#31471;&#21040;&#31471;&#25512;&#29702;&#21152;&#36895;&#65292;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#26597;&#35810;&#21644;&#38190;&#26144;&#23556;&#20026;&#27721;&#26126;&#31354;&#38388;&#20013;&#30340;&#20108;&#36827;&#21046;&#30721;&#20043;&#21518;&#65292;&#37319;&#29992;&#21152;&#27861;&#26680;&#23545;&#26597;&#35810;&#12289;&#38190;&#21644;&#20540;&#20043;&#38388;&#30340;MatMul&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#21097;&#20313;&#30340;MLPs&#25110;&#32447;&#24615;&#23618;&#21017;&#37319;&#29992;&#20301;&#31227;&#26680;&#36827;&#34892;&#37325;&#26032;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#21033;&#29992;TVM&#22312;GPU&#19978;&#23454;&#26045;&#24182;&#20248;&#21270;&#36825;&#20123;&#23450;&#21046;&#26680;&#65292;&#20197;&#23454;&#29616;&#23454;&#38469;&#30828;&#20214;&#37096;&#32626;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#36825;&#31181;&#37325;&#26032;&#21442;&#25968;&#21270;&#26041;&#27861;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#25512;&#29702;&#36895;&#24230;&#65292;&#32780;&#26080;&#38656;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. But both attention and multi-layer perceptions (MLPs) in ViTs are not efficient enough due to dense multiplications, resulting in costly training and inference. To this end, we propose to reparameterize the pre-trained ViT with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\textbf{ShiftAddViT}$, which aims for end-to-end inference speedups on GPUs without the need of training from scratch. Specifically, all $\texttt{MatMuls}$ among queries, keys, and values are reparameterized by additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized by shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameter
&lt;/p&gt;</description></item><item><title>&#20915;&#31574;&#22534;&#21472;&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;&#30446;&#26631;&#26465;&#20214;&#21270;&#31574;&#30053;&#20195;&#29702;&#20998;&#35299;&#20026;3&#20010;&#29983;&#25104;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#25945;&#24072;&#24378;&#21046;&#24182;&#34892;&#23398;&#20064;&#65292;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#21333;&#20010;&#27169;&#22359;&#20197;&#32771;&#34385;&#26550;&#26500;&#20559;&#24046;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#21160;&#24577;&#12289;&#36328;&#39046;&#22495;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.06253</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22359;&#21270;&#29983;&#25104;&#27169;&#22411;&#23454;&#29616;&#28789;&#27963;&#30340;&#24378;&#21270;&#23398;&#20064;&#20915;&#31574;&#22534;&#21472;
&lt;/p&gt;
&lt;p&gt;
Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models. (arXiv:2306.06253v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06253
&lt;/p&gt;
&lt;p&gt;
&#20915;&#31574;&#22534;&#21472;&#26159;&#19968;&#31181;&#28789;&#27963;&#30340;&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;&#30446;&#26631;&#26465;&#20214;&#21270;&#31574;&#30053;&#20195;&#29702;&#20998;&#35299;&#20026;3&#20010;&#29983;&#25104;&#27169;&#22359;&#65292;&#24182;&#36890;&#36807;&#25945;&#24072;&#24378;&#21046;&#24182;&#34892;&#23398;&#20064;&#65292;&#21487;&#20197;&#29992;&#20110;&#35774;&#35745;&#21333;&#20010;&#27169;&#22359;&#20197;&#32771;&#34385;&#26550;&#26500;&#20559;&#24046;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#21160;&#24577;&#12289;&#36328;&#39046;&#22495;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#12290;&#22312;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#28789;&#27963;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#21560;&#24341;&#21147;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#25512;&#29702;&#39034;&#24207;&#20915;&#31574;&#21046;&#23450;&#30340;&#20960;&#20010;&#19981;&#21516;&#26041;&#38754;&#65292;&#22914;&#25351;&#23450;&#22797;&#26434;&#30446;&#26631;&#12289;&#35268;&#21010;&#26410;&#26469;&#35266;&#23519;&#21644;&#34892;&#21160;&#65292;&#20197;&#21450;&#25209;&#35780;&#20854;&#23454;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#33021;&#21147;&#30340;&#32508;&#21512;&#38598;&#25104;&#22312;&#20445;&#25345;&#26368;&#22823;&#34920;&#36798;&#33021;&#21147;&#30340;&#21516;&#26102;&#20801;&#35768;&#36827;&#34892;&#27169;&#22411;&#36873;&#25321;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#23398;&#20064;&#21644;&#25512;&#29702;&#65292;&#36825;&#26500;&#25104;&#20102;&#31454;&#20105;&#24615;&#30340;&#31639;&#27861;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20915;&#31574;&#22534;&#21472;&#65288;Decision Stacks&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29983;&#25104;&#26694;&#26550;&#65292;&#23558;&#30446;&#26631;&#26465;&#20214;&#21270;&#31574;&#30053;&#20195;&#29702;&#20998;&#35299;&#20026;3&#20010;&#29983;&#25104;&#27169;&#22359;&#12290;&#36825;&#20123;&#27169;&#22359;&#36890;&#36807;&#29420;&#31435;&#30340;&#29983;&#25104;&#27169;&#22411;&#27169;&#25311;&#20102;&#35266;&#27979;&#12289;&#22870;&#21169;&#21644;&#34892;&#21160;&#30340;&#26102;&#38388;&#28436;&#21464;&#65292;&#21487;&#20197;&#36890;&#36807;&#25945;&#24072;&#24378;&#21046;&#24182;&#34892;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20445;&#35777;&#20102;&#34920;&#36798;&#33021;&#21147;&#21644;&#28789;&#27963;&#24615;&#65292;&#22312;&#35774;&#35745;&#21333;&#20010;&#27169;&#22359;&#20197;&#32771;&#34385;&#20851;&#38190;&#22240;&#32032;&#65288;&#22914;&#26550;&#26500;&#20559;&#24046;&#12289;&#20248;&#21270;&#30446;&#26631;&#21644;&#21160;&#24577;&#12289;&#36328;&#39046;&#22495;&#30340;&#21487;&#36716;&#31227;&#24615;&#21644;&#25512;&#29702;&#36895;&#24230;&#65289;&#26041;&#38754;&#20855;&#26377;&#20248;&#21183;&#12290;&#25105;&#20204;&#23545;&#19968;&#31995;&#21015;&#36830;&#32493;&#25511;&#21046;&#22522;&#20934;&#36827;&#34892;&#30340;&#23454;&#35777;&#32467;&#26524;&#34920;&#26126;&#65292;&#20915;&#31574;&#22534;&#21472;&#25552;&#20379;&#20102;&#19968;&#31181;&#28789;&#27963;&#19988;&#21487;&#25193;&#23637;&#30340;&#26367;&#20195;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning presents an attractive paradigm to reason about several distinct aspects of sequential decision making, such as specifying complex goals, planning future observations and actions, and critiquing their utilities. However, the combined integration of these capabilities poses competing algorithmic challenges in retaining maximal expressivity while allowing for flexibility in modeling choices for efficient learning and inference. We present Decision Stacks, a generative framework that decomposes goal-conditioned policy agents into 3 generative modules. These modules simulate the temporal evolution of observations, rewards, and actions via independent generative models that can be learned in parallel via teacher forcing. Our framework guarantees both expressivity and flexibility in designing individual modules to account for key factors such as architectural bias, optimization objective and dynamics, transferrability across domains, and inference speed. Our empirical 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;</title><link>http://arxiv.org/abs/2306.06101</link><description>&lt;p&gt;
Prodigy: &#19968;&#31181;&#24555;&#36895;&#33258;&#36866;&#24212;&#38646;&#21442;&#25968;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Prodigy: An Expeditiously Adaptive Parameter-Free Learner. (arXiv:2306.06101v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06101
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#26041;&#27861;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#24555;&#36895;&#19988;&#27491;&#30830;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#24182;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#21644;&#27169;&#22411;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;D-Adaptation&#24182;&#21487;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#33258;&#36866;&#24212;&#31639;&#27861;(&#22914;Adagrad&#21644;Adam)&#20013;&#30340;&#23398;&#20064;&#29575;&#20272;&#35745;&#38382;&#39064;&#65292;&#25551;&#36848;&#20102;&#20004;&#31181;&#25216;&#26415;Prodigy&#21644;Resetting&#65292;&#21487;&#20197;&#35777;&#26126;&#22320;&#20272;&#35745;&#21040;&#36798;&#35299;&#20915;&#26041;&#26696;&#25152;&#38656;&#30340;&#36317;&#31163;D&#65292;&#20197;&#20415;&#26368;&#20248;&#35774;&#32622;&#23398;&#20064;&#29575;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#26159;&#22522;&#20110;&#23398;&#20064;&#29575;&#33258;&#30001;&#30340;D-Adaptation&#26041;&#27861;&#30340;&#20462;&#25913;&#65292;&#24182;&#36890;&#36807;$O(\sqrt{\log(D/d_0)})$&#30340;&#22240;&#23376;&#25552;&#39640;&#20102;D-Adaptation&#30340;&#25910;&#25947;&#36895;&#24230;&#65292;&#20854;&#20013;$d_0$&#26159;$D$&#30340;&#21021;&#22987;&#20272;&#35745;&#20540;&#12290;&#25105;&#20204;&#22312;12&#20010;&#24120;&#35265;&#30340;&#36923;&#36753;&#22238;&#24402;&#22522;&#20934;&#25968;&#25454;&#38598;&#12289;&#22312;CIFAR10&#19978;&#35757;&#32451;&#30340;VGG11&#21644;ResNet-50&#12289;&#22312;Imagenet&#19978;&#35757;&#32451;&#30340;ViT&#12289;&#22312;IWSLT14&#19978;&#35757;&#32451;&#30340;LSTM&#12289;&#22312;Criteo&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;DLRM&#12289;&#22312;Knee MRI&#25968;&#25454;&#38598;&#19978;&#30340;VarNet&#65292;&#20197;&#21450;&#22312;BookWiki&#19978;&#35757;&#32451;&#30340;RoBERTa&#21644;GPT transformer&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22987;&#32456;&#20248;&#20110;D-Adaptation&#65292;&#24182;&#36798;&#21040;&#25163;&#21160;&#35843;&#25972;Adam&#30340;&#27979;&#35797;&#20934;&#30830;&#24230;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of estimating the learning rate in adaptive methods, such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to provably estimate the distance to the solution $D$, which is needed to set the learning rate optimally. Our techniques are modifications of the D-Adaptation method for learning-rate-free learning. Our methods improve upon the convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where $d_0$ is the initial estimate of $D$. We test our methods on 12 common logistic-regression benchmark datasets, VGG11 and ResNet-50 training on CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT transformer training on BookWiki. Our experimental results show that our approaches consistently outperform D-Adaptation and reach test accuracy values close to that of hand-tuned Adam.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36890;&#29992;&#30340;&#22823;&#35268;&#27169;&#21644;&#28508;&#22312;&#26410;&#30693;&#22270;&#19978;&#23450;&#20041;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#22270;&#20869;&#26680;&#65292;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2306.05304</link><description>&lt;p&gt;
&#22270;&#19978;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bayesian Optimisation of Functions on Graphs. (arXiv:2306.05304v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05304
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#36890;&#29992;&#30340;&#22823;&#35268;&#27169;&#21644;&#28508;&#22312;&#26410;&#30693;&#22270;&#19978;&#23450;&#20041;&#20989;&#25968;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#31639;&#27861;&#65292;&#24182;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#22270;&#20869;&#26680;&#65292;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#19981;&#26029;&#28044;&#29616;&#25512;&#21160;&#20102;&#22312;&#22270;&#33410;&#28857;&#38598;&#19978;&#23450;&#20041;&#20989;&#25968;&#30340;&#20248;&#21270;&#20219;&#21153;&#12290;&#20256;&#32479;&#30340;&#22270;&#25628;&#32034;&#31639;&#27861;&#21487;&#29992;&#20110;&#27492;&#65292;&#20294;&#23427;&#20204;&#21487;&#33021;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#65292;&#24182;&#19988;&#19981;&#21033;&#29992;&#20851;&#20110;&#20989;&#25968;&#20540;&#30340;&#20449;&#24687;&#65307;&#21478;&#19968;&#26041;&#38754;&#65292;&#36125;&#21494;&#26031;&#20248;&#21270;&#26159;&#19968;&#31867;&#26377;&#21069;&#36884;&#30340;&#40657;&#30418;&#27714;&#35299;&#22120;&#65292;&#20855;&#26377;&#26356;&#39640;&#30340;&#26679;&#26412;&#25928;&#29575;&#65292;&#20294;&#23427;&#24456;&#23569;&#34987;&#24212;&#29992;&#20110;&#36825;&#26679;&#30340;&#26032;&#39062;&#35774;&#32622;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36125;&#21494;&#26031;&#20248;&#21270;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#20248;&#21270;&#22312;&#36890;&#29992;&#65292;&#22823;&#35268;&#27169;&#21644;&#28508;&#22312;&#30340;&#26410;&#30693;&#22270;&#19978;&#23450;&#20041;&#30340;&#20989;&#25968;&#12290;&#36890;&#36807;&#23398;&#20064;&#36866;&#24403;&#30340;&#22270;&#20869;&#26680;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#20855;&#26377;&#36866;&#24212;&#30446;&#26631;&#20989;&#25968;&#34892;&#20026;&#30340;&#20248;&#28857;&#12290;&#23616;&#37096;&#24314;&#27169;&#26041;&#27861;&#36827;&#19968;&#27493;&#20445;&#35777;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#25928;&#29575;&#12290;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#19990;&#30028;&#22270;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;&#20248;&#21270;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The increasing availability of graph-structured data motivates the task of optimising over functions defined on the node set of graphs. Traditional graph search algorithms can be applied in this case, but they may be sample-inefficient and do not make use of information about the function values; on the other hand, Bayesian optimisation is a class of promising black-box solvers with superior sample efficiency, but it has been scarcely been applied to such novel setups. To fill this gap, we propose a novel Bayesian optimisation framework that optimises over functions defined on generic, large-scale and potentially unknown graphs. Through the learning of suitable kernels on graphs, our framework has the advantage of adapting to the behaviour of the target function. The local modelling approach further guarantees the efficiency of our method. Extensive experiments on both synthetic and real-world graphs demonstrate the effectiveness of the proposed optimisation framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FactorCL&#65292;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20165;&#32771;&#34385;&#36328;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#65292;&#36824;&#33021;&#25429;&#25417;&#36328;&#27169;&#24577;&#21807;&#19968;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2306.05268</link><description>&lt;p&gt;
&#20998;&#35299;&#23545;&#27604;&#23398;&#20064;&#65306;&#36229;&#36234;&#22810;&#35270;&#35282;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
Factorized Contrastive Learning: Going Beyond Multi-view Redundancy. (arXiv:2306.05268v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.05268
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;FactorCL&#65292;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#19981;&#20165;&#32771;&#34385;&#36328;&#27169;&#24577;&#20849;&#20139;&#20449;&#24687;&#65292;&#36824;&#33021;&#25429;&#25417;&#36328;&#27169;&#24577;&#21807;&#19968;&#30340;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24191;&#27867;&#30340;&#22810;&#27169;&#24577;&#20219;&#21153;&#20013;&#65292;&#23545;&#27604;&#23398;&#20064;&#24050;&#25104;&#20026;&#19968;&#31181;&#29305;&#21035;&#21560;&#24341;&#20154;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#25104;&#21151;&#22320;&#23398;&#20064;&#20855;&#26377;&#20016;&#23500;&#26410;&#26631;&#35760;&#25968;&#25454;&#30340;&#34920;&#31034;&#65292;&#21482;&#38656;&#37197;&#23545;&#20449;&#24687;&#65288;&#20363;&#22914;&#65292;&#22270;&#20687;&#26631;&#39064;&#25110;&#35270;&#39057;&#38899;&#39057;&#23545;&#65289;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#22522;&#30784;&#26159;&#22810;&#35270;&#35282;&#20887;&#20313;&#30340;&#20551;&#35774;&#8212;&#8212;&#36328;&#27169;&#24577;&#38388;&#20849;&#20139;&#20449;&#24687;&#23545;&#20110;&#19979;&#28216;&#20219;&#21153;&#26159;&#24517;&#35201;&#19988;&#36275;&#22815;&#30340;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24773;&#20917;&#19979;&#65292;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#20063;&#21253;&#21547;&#22312;&#36328;&#27169;&#24577;&#21807;&#19968;&#21306;&#22495;&#20013;&#65306;&#19968;&#31181;&#20165;&#23384;&#22312;&#20110;&#19968;&#20010;&#27169;&#24577;&#20013;&#20294;&#19982;&#20219;&#21153;&#20173;&#28982;&#30456;&#20851;&#30340;&#20449;&#24687;&#12290;&#22914;&#20309;&#23398;&#20064;&#33258;&#25105;&#30417;&#30563;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#20197;&#25429;&#33719;&#19982;&#19979;&#28216;&#20219;&#21153;&#30456;&#20851;&#30340;&#20849;&#20139;&#21644;&#21807;&#19968;&#20449;&#24687;&#65311;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#27169;&#24577;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;FactorCL&#65292;&#20197;&#36229;&#36234;&#22810;&#35270;&#35282;&#20887;&#20313;&#12290;FactorCL&#30340;&#22522;&#30784;&#26159;&#19977;&#20010;&#26032;&#30340;&#36129;&#29486;&#65306;&#65288;1&#65289;&#23558;&#20219;&#21153;&#30456;&#20851;&#20449;&#24687;&#20998;&#35299;&#20026;&#20849;&#20139;&#21644;&#21807;&#19968;&#34920;&#31034;&#65292;&#65288;2&#65289;&#38480;&#21046;&#20849;&#20139;&#21644;&#21807;&#19968;&#25104;&#20998;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#65288;3&#65289;&#20351;&#29992;&#22240;&#23376;&#27491;&#21017;&#21270;&#20419;&#36827;&#34920;&#31034;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) cap
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#40657;&#30418;SDM&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#24314;&#31435;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#25551;&#36848;&#20854;&#33021;&#21147;&#21644;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#25191;&#34892;&#36825;&#20123;&#33021;&#21147;&#30340;&#21487;&#33021;&#25928;&#26524;&#21644;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2306.04806</link><description>&lt;p&gt;
&#40657;&#30418;&#24207;&#36143;&#20915;&#31574;&#31995;&#32479;&#30340;&#33258;&#20027;&#33021;&#21147;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Autonomous Capability Assessment of Black-Box Sequential Decision-Making Systems. (arXiv:2306.04806v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04806
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#35780;&#20272;&#40657;&#30418;SDM&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#20027;&#21160;&#23398;&#20064;&#26469;&#24314;&#31435;&#19968;&#20010;&#21487;&#35299;&#37322;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#22320;&#25551;&#36848;&#20854;&#33021;&#21147;&#21644;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#25191;&#34892;&#36825;&#20123;&#33021;&#21147;&#30340;&#21487;&#33021;&#25928;&#26524;&#21644;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#23616;&#38480;&#24615;&#23545;&#20110;&#23433;&#20840;&#22320;&#20351;&#29992;&#23427;&#20204;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#35753;&#29992;&#25143;&#35780;&#20272;&#20855;&#26377;&#19981;&#26029;&#21457;&#23637;&#30340;&#24207;&#36143;&#20915;&#31574;&#33021;&#21147;&#30340;AI&#31995;&#32479;&#30340;&#38382;&#39064;&#30456;&#23545;&#23569;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#24314;&#27169;&#40657;&#30418;AI&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#22312;&#38543;&#26426;&#29615;&#22659;&#20013;&#25191;&#34892;&#36825;&#20123;&#33021;&#21147;&#30340;&#21487;&#33021;&#25928;&#26524;&#21644;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20027;&#21160;&#23398;&#20064;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#19982;&#40657;&#30418;SDM&#31995;&#32479;&#20132;&#20114;&#65292;&#24182;&#23398;&#20064;&#25551;&#36848;&#20854;&#33021;&#21147;&#30340;&#21487;&#35299;&#37322;&#27010;&#29575;&#27169;&#22411;&#12290;&#23545;&#35813;&#26041;&#27861;&#30340;&#29702;&#35770;&#20998;&#26512;&#30830;&#23450;&#20102;&#23398;&#20064;&#36807;&#31243;&#25910;&#25947;&#21040;&#20195;&#29702;&#27491;&#30830;&#27169;&#22411;&#30340;&#26465;&#20214;&#65307;&#23545;&#19981;&#21516;&#20195;&#29702;&#21644;&#27169;&#25311;&#22330;&#26223;&#30340;&#23454;&#35777;&#35780;&#20272;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#25551;&#36848;&#20219;&#24847;&#40657;&#30418;SDM&#20195;&#29702;&#30340;&#33021;&#21147;&#65292;&#24182;&#33021;&#36827;&#34892;&#23569;&#27425;&#36890;&#29992;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
It is essential for users to understand what their AI systems can and can't do in order to use them safely. However, the problem of enabling users to assess AI systems with evolving sequential decision making (SDM) capabilities is relatively understudied. This paper presents a new approach for modeling the capabilities of black-box AI systems that can plan and act, along with the possible effects and requirements for executing those capabilities in stochastic settings. We present an active-learning approach that can effectively interact with a black-box SDM system and learn an interpretable probabilistic model describing its capabilities. Theoretical analysis of the approach identifies the conditions under which the learning process is guaranteed to converge to the correct model of the agent; empirical evaluations on different agents and simulated scenarios show that this approach is few-shot generalizable and can effectively describe the capabilities of arbitrary black-box SDM agents 
&lt;/p&gt;</description></item><item><title>SGD&#22312;&#35757;&#32451;&#36807;&#24230;&#34920;&#36798;&#30340;&#32593;&#32476;&#26102;&#65292;&#20250;&#38543;&#26426;&#22320;&#23558;&#21160;&#24577;&#21560;&#24341;&#21040;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#65292;&#36825;&#31181;&#38543;&#26426;&#21560;&#24341;&#24615;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2306.04251</link><description>&lt;p&gt;
&#38543;&#26426;&#22349;&#32553;&#65306;&#22914;&#20309;&#21033;&#29992;&#26799;&#24230;&#22122;&#22768;&#20351;SGD&#21160;&#24577;&#36235;&#21521;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards Simpler Subnetworks. (arXiv:2306.04251v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04251
&lt;/p&gt;
&lt;p&gt;
SGD&#22312;&#35757;&#32451;&#36807;&#24230;&#34920;&#36798;&#30340;&#32593;&#32476;&#26102;&#65292;&#20250;&#38543;&#26426;&#22320;&#23558;&#21160;&#24577;&#21560;&#24341;&#21040;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#65292;&#36825;&#31181;&#38543;&#26426;&#21560;&#24341;&#24615;&#33021;&#22815;&#25552;&#39640;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25581;&#31034;&#20102;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#30340;&#19968;&#20010;&#24378;&#28872;&#38544;&#24335;&#20559;&#22909;&#65292;&#23427;&#23558;&#36807;&#24230;&#34920;&#36798;&#30340;&#32593;&#32476;&#39537;&#21160;&#21040;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#65292;&#20174;&#32780;&#22823;&#22823;&#20943;&#23569;&#20102;&#29420;&#31435;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#25581;&#31034;&#36825;&#20010;&#20559;&#22909;&#65292;&#25105;&#20204;&#35782;&#21035;&#20102;&#19981;&#21464;&#38598;&#65292;&#25110;&#32773;&#35828;&#26159;SGD&#26410;&#20462;&#25913;&#30340;&#21442;&#25968;&#31354;&#38388;&#30340;&#23376;&#38598;&#12290;&#25105;&#20204;&#19987;&#27880;&#20110;&#20004;&#31867;&#19981;&#21464;&#38598;&#65292;&#23427;&#20204;&#23545;&#24212;&#20110;&#29616;&#20195;&#26550;&#26500;&#20013;&#24120;&#35265;&#30340;&#26356;&#31616;&#21333;&#30340;&#23376;&#32593;&#32476;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;SGD&#22312;&#36825;&#20123;&#31616;&#21333;&#19981;&#21464;&#38598;&#26041;&#38754;&#20855;&#26377;&#38543;&#26426;&#21560;&#24341;&#24615;&#30340;&#29305;&#24615;&#12290;&#25105;&#20204;&#26681;&#25454;&#25439;&#22833;&#26223;&#35266;&#22312;&#19981;&#21464;&#38598;&#21608;&#22260;&#30340;&#26354;&#29575;&#21644;&#38543;&#26426;&#26799;&#24230;&#24341;&#20837;&#30340;&#22122;&#22768;&#20043;&#38388;&#30340;&#31454;&#20105;&#24314;&#31435;&#20102;&#19968;&#31181;&#38543;&#26426;&#21560;&#24341;&#24615;&#30340;&#20805;&#20998;&#26465;&#20214;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#22686;&#21152;&#22122;&#22768;&#27700;&#24179;&#20250;&#22686;&#24378;&#21560;&#24341;&#21147;&#65292;&#23548;&#33268;&#19982;&#38797;&#28857;&#25110;&#35757;&#32451;&#25439;&#22833;&#30340;&#23616;&#37096;&#26497;&#22823;&#20540;&#30456;&#20851;&#30340;&#21560;&#24341;&#19981;&#21464;&#38598;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we reveal a strong implicit bias of stochastic gradient descent (SGD) that drives overly expressive networks to much simpler subnetworks, thereby dramatically reducing the number of independent parameters, and improving generalization. To reveal this bias, we identify invariant sets, or subsets of parameter space that remain unmodified by SGD. We focus on two classes of invariant sets that correspond to simpler subnetworks and commonly appear in modern architectures. Our analysis uncovers that SGD exhibits a property of stochastic attractivity towards these simpler invariant sets. We establish a sufficient condition for stochastic attractivity based on a competition between the loss landscape's curvature around the invariant set and the noise introduced by stochastic gradients. Remarkably, we find that an increased level of noise strengthens attractivity, leading to the emergence of attractive invariant sets associated with saddle-points or local maxima of the train loss.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2306.03552</link><description>&lt;p&gt;
&#25968;&#25454;&#20013;&#21160;&#24577;&#20559;&#31227;&#30340;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
State Regularized Policy Optimization on Data with Dynamics Shift. (arXiv:2306.03552v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570; SRPO (&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;) &#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#31283;&#24577;&#20998;&#24067;&#26469;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#30340;&#31574;&#30053;&#65292;&#22312;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#22810;&#20010;&#29615;&#22659;&#26102;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20351;&#29992;&#30340;&#25968;&#25454;&#21463;&#21040;&#21160;&#24577;&#20559;&#31227;&#30340;&#24433;&#21709;&#65292;&#21363;&#20855;&#26377;&#19981;&#21516;&#30340;&#29615;&#22659;&#21160;&#24577;&#12290;&#30446;&#21069;&#30340;&#22823;&#22810;&#25968;&#26041;&#27861;&#36890;&#36807;&#35757;&#32451;&#19978;&#19979;&#25991;&#32534;&#30721;&#22120;&#26469;&#35782;&#21035;&#29615;&#22659;&#21442;&#25968;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#26681;&#25454;&#20854;&#29615;&#22659;&#21442;&#25968;&#23558;&#24102;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20998;&#24320;&#20197;&#35757;&#32451;&#30456;&#24212;&#30340;&#31574;&#30053;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#33021;&#20250;&#20986;&#29616;&#26679;&#26412;&#25928;&#29575;&#20302;&#19979;&#30340;&#38382;&#39064;&#65292;&#22240;&#20026;&#25968;&#25454;&#26159;&#8220;&#29305;&#23450;&#22330;&#26223;&#8221;&#20351;&#29992;&#30340;&#65292;&#38024;&#23545;&#26576;&#20010;&#29615;&#22659;&#35757;&#32451;&#30340;&#31574;&#30053;&#19981;&#33021;&#20174;&#25910;&#38598;&#22312;&#20854;&#20182;&#20855;&#26377;&#19981;&#21516;&#21160;&#24577;&#30340;&#25152;&#26377;&#20854;&#20182;&#29615;&#22659;&#20013;&#30340;&#25968;&#25454;&#20013;&#21463;&#30410;&#12290;&#26412;&#25991;&#21457;&#29616;&#65292;&#22312;&#35768;&#22810;&#20855;&#26377;&#30456;&#20284;&#32467;&#26500;&#21644;&#19981;&#21516;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#65292;&#26368;&#20248;&#31574;&#30053;&#20855;&#26377;&#31867;&#20284;&#30340;&#31283;&#24577;&#20998;&#24067;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#29305;&#24615;&#65292;&#24182;&#20174;&#20855;&#26377;&#21160;&#24577;&#28418;&#31227;&#30340;&#25968;&#25454;&#20013;&#23398;&#20064;&#31283;&#24577;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#25968;&#25454;&#37325;&#29992;&#12290;&#36825;&#31181;&#20998;&#24067;&#29992;&#20110;&#35268;&#33539;&#26032;&#29615;&#22659;&#20013;&#35757;&#32451;&#30340;&#31574;&#30053;&#65292;&#23548;&#33268;&#20102; SRPO&#65288;&#29366;&#24577;&#35268;&#33539;&#21270;&#31574;&#30053;&#20248;&#21270;&#65289;&#31639;&#27861;&#30340;&#20986;&#29616;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;SRPO &#22312;&#20855;&#26377;&#21160;&#24577;&#20559;&#31227;&#30340;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In many real-world scenarios, Reinforcement Learning (RL) algorithms are trained on data with dynamics shift, i.e., with different underlying environment dynamics. A majority of current methods address such issue by training context encoders to identify environment parameters. Data with dynamics shift are separated according to their environment parameters to train the corresponding policy. However, these methods can be sample inefficient as data are used \textit{ad hoc}, and policies trained for one dynamics cannot benefit from data collected in all other environments with different dynamics. In this paper, we find that in many environments with similar structures and different dynamics, optimal policies have similar stationary state distributions. We exploit such property and learn the stationary state distribution from data with dynamics shift for efficient data reuse. Such distribution is used to regularize the policy trained in a new environment, leading to the SRPO (\textbf{S}tat
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Early-Exit&#32593;&#32476;&#20013;&#23454;&#29616;&#26465;&#20214;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#27169;&#22411;&#36716;&#21270;&#20026;&#30495;&#27491;&#30340;&#38543;&#26102;&#20998;&#31867;&#22120;&#12290;</title><link>http://arxiv.org/abs/2306.02652</link><description>&lt;p&gt;
&#36890;&#36807;&#24378;&#21046;&#26465;&#20214;&#21333;&#35843;&#24615;&#22312;Early-Exit&#32467;&#26500;&#20013;&#23454;&#29616;&#38543;&#26102;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Towards Anytime Classification in Early-Exit Architectures by Enforcing Conditional Monotonicity. (arXiv:2306.02652v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.02652
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;Early-Exit&#32593;&#32476;&#20013;&#23454;&#29616;&#26465;&#20214;&#21333;&#35843;&#24615;&#30340;&#26041;&#27861;&#65292;&#23558;&#28145;&#24230;&#27169;&#22411;&#36716;&#21270;&#20026;&#30495;&#27491;&#30340;&#38543;&#26102;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#37096;&#32626;&#22312;&#35745;&#31639;&#39044;&#31639;&#21160;&#24577;&#30340;&#29615;&#22659;&#20013;&#12290;&#38543;&#26102;&#31639;&#27861;&#38750;&#24120;&#36866;&#29992;&#20110;&#36825;&#31181;&#29615;&#22659;&#65292;&#22240;&#20026;&#23427;&#20204;&#22312;&#35745;&#31639;&#30340;&#20219;&#20309;&#26102;&#20505;&#37117;&#21487;&#20197;&#36755;&#20986;&#39044;&#27979;&#20540;&#65292;&#20854;&#36136;&#37327;&#26159;&#35745;&#31639;&#26102;&#38388;&#30340;&#20989;&#25968;&#12290;&#30001;&#20110;&#20854;&#33021;&#22815;&#22312;&#32593;&#32476;&#21508;&#20010;&#38454;&#27573;&#25552;&#20379;&#20013;&#38388;&#39044;&#27979;&#32467;&#26524;&#30340;&#33021;&#21147;&#65292;Early-Exit&#31070;&#32463;&#32593;&#32476;&#22312;&#38543;&#26102;&#35745;&#31639;&#30340;&#32972;&#26223;&#19979;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#35777;&#26126;&#24403;&#21069;&#30340;Early-Exit&#32593;&#32476;&#24182;&#19981;&#30452;&#25509;&#36866;&#29992;&#20110;&#20219;&#20309;&#26102;&#20505;&#30340;&#35774;&#32622;&#65292;&#22240;&#20026;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#39044;&#27979;&#36136;&#37327;&#19981;&#33021;&#20445;&#35777;&#38543;&#30528;&#35745;&#31639;&#26102;&#38388;&#30340;&#22686;&#21152;&#32780;&#25552;&#39640;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#32570;&#38519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20248;&#38597;&#30340;&#20107;&#21518;&#20462;&#25913;&#65292;&#22522;&#20110;&#19987;&#23478;&#20056;&#31215;&#65292;&#40723;&#21169;Early-Exit&#32593;&#32476;&#36880;&#28176;&#21464;&#24471;&#33258;&#20449;&#12290;&#36825;&#36171;&#20104;&#20102;&#25105;&#20204;&#30340;&#28145;&#24230;&#27169;&#22411;&#26465;&#20214;&#21333;&#35843;&#24615;&#30340;&#29305;&#24615;&#8212;&#8212;&#36825;&#26159;&#23454;&#29616;&#30495;&#27491;&#38543;&#26102;&#20998;&#31867;&#30340;&#37325;&#35201;&#22522;&#30707;&#12290;
&lt;/p&gt;
&lt;p&gt;
Modern predictive models are often deployed to environments in which computational budgets are dynamic. Anytime algorithms are well-suited to such environments as, at any point during computation, they can output a prediction whose quality is a function of computation time. Early-exit neural networks have garnered attention in the context of anytime computation due to their capability to provide intermediate predictions at various stages throughout the network. However, we demonstrate that current early-exit networks are not directly applicable to anytime settings, as the quality of predictions for individual data points is not guaranteed to improve with longer computation. To address this shortcoming, we propose an elegant post-hoc modification, based on the Product-of-Experts, that encourages an early-exit network to become gradually confident. This gives our deep models the property of conditional monotonicity in the prediction quality -- an essential stepping stone towards truly an
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20174;&#26410;&#30693;&#24178;&#39044;&#25968;&#25454;&#20013;&#25512;&#26029;&#38750;&#21442;&#25968;&#22240;&#26524;&#34920;&#36798;&#24335;&#23398;&#20064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#20004;&#20010;&#22240;&#26524;&#21464;&#37327;&#30340;&#22522;&#26412;&#35774;&#32622;&#20013;&#65292;&#26080;&#27861;&#28040;&#38500;&#19968;&#20123;&#30001;&#24178;&#39044;&#25968;&#25454;&#24341;&#36215;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2306.00542</link><description>&lt;p&gt;
&#26410;&#30693;&#24178;&#39044;&#30340;&#22240;&#26524;&#34920;&#36798;&#24335;&#30340;&#38750;&#21442;&#25968;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Nonparametric Identifiability of Causal Representations from Unknown Interventions. (arXiv:2306.00542v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#29992;&#20110;&#20174;&#26410;&#30693;&#24178;&#39044;&#25968;&#25454;&#20013;&#25512;&#26029;&#38750;&#21442;&#25968;&#22240;&#26524;&#34920;&#36798;&#24335;&#23398;&#20064;&#65292;&#24182;&#19988;&#35777;&#26126;&#20102;&#22312;&#20004;&#20010;&#22240;&#26524;&#21464;&#37327;&#30340;&#22522;&#26412;&#35774;&#32622;&#20013;&#65292;&#26080;&#27861;&#28040;&#38500;&#19968;&#20123;&#30001;&#24178;&#39044;&#25968;&#25454;&#24341;&#36215;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#22240;&#26524;&#34920;&#36798;&#24335;&#23398;&#20064;&#65292;&#21363;&#20174;&#21464;&#37327;&#30340;&#39640;&#32500;&#20989;&#25968;&#65288;&#8220;&#28151;&#21512;&#29289;&#8221;&#65289;&#20013;&#25512;&#26029;&#28508;&#22312;&#30340;&#22240;&#26524;&#21464;&#37327;&#21450;&#20854;&#22240;&#26524;&#20851;&#31995;&#30340;&#20219;&#21153;&#12290;&#20197;&#21069;&#30340;&#24037;&#20316;&#20381;&#36182;&#20110;&#24369;&#30417;&#30563;&#65292;&#22914;&#21453;&#20107;&#23454;&#30340;&#24178;&#39044;&#35266;&#23519;&#25110;&#26102;&#38388;&#32467;&#26500;&#65307;&#23545;&#28151;&#21512;&#20989;&#25968;&#25110;&#28508;&#22312;&#22240;&#26524;&#27169;&#22411;&#26045;&#21152;&#38480;&#21046;&#65292;&#22914;&#32447;&#24615;&#65307;&#25110;&#38656;&#35201;&#37096;&#20998;&#20102;&#35299;&#29983;&#25104;&#36807;&#31243;&#65292;&#22914;&#22240;&#26524;&#22270;&#25110;&#24178;&#39044;&#30446;&#26631;&#12290;&#25105;&#20204;&#32771;&#34385;&#21040;&#22240;&#26524;&#27169;&#22411;&#21644;&#28151;&#21512;&#20989;&#25968;&#37117;&#26159;&#38750;&#21442;&#25968;&#30340;&#19968;&#33324;&#24773;&#20917;&#12290;&#23398;&#20064;&#20449;&#21495;&#37319;&#29992;&#26469;&#33258;&#22522;&#30784;&#22240;&#26524;&#27169;&#22411;&#20013;&#26410;&#30693;&#24178;&#39044;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#25110;&#29615;&#22659;&#30340;&#24418;&#24335;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#22320;&#38754;&#30495;&#23454;&#28508;&#21464;&#37327;&#21450;&#20854;&#22240;&#26524;&#22270;&#37492;&#23450;&#20986;&#26469;&#65292;&#21516;&#26102;&#35299;&#20915;&#19968;&#32452;&#20174;&#24178;&#39044;&#25968;&#25454;&#26080;&#27861;&#28040;&#38500;&#30340;&#27495;&#20041;&#38382;&#39064;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#20010;&#22240;&#26524;&#21464;&#37327;&#30340;&#22522;&#26412;&#35774;&#32622;&#65292;&#24182;&#35777;&#26126;&#20102;...
&lt;/p&gt;
&lt;p&gt;
We study causal representation learning, the task of inferring latent causal variables and their causal relations from high-dimensional functions ("mixtures") of the variables. Prior work relies on weak supervision, in the form of counterfactual pre- and post-intervention views or temporal structure; places restrictive assumptions, such as linearity, on the mixing function or latent causal model; or requires partial knowledge of the generative process, such as the causal graph or the intervention targets. We instead consider the general setting in which both the causal model and the mixing function are nonparametric. The learning signal takes the form of multiple datasets, or environments, arising from unknown interventions in the underlying causal model. Our goal is to identify both the ground truth latents and their causal graph up to a set of ambiguities which we show to be irresolvable from interventional data. We study the fundamental setting of two causal variables and prove that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#35282;&#24230;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#25193;&#25955;&#35757;&#32451;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36127;&#36801;&#31227;&#24182;&#25552;&#39640;&#21435;&#22122;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.00354</link><description>&lt;p&gt;
&#35299;&#20915;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#36127;&#36801;&#31227;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Addressing Negative Transfer in Diffusion Models. (arXiv:2306.00354v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22810;&#20219;&#21153;&#23398;&#20064;&#35282;&#24230;&#20986;&#21457;&#65292;&#30740;&#31350;&#20102;&#25193;&#25955;&#35757;&#32451;&#20013;&#30340;&#36127;&#36801;&#31227;&#29616;&#35937;&#65292;&#25552;&#20986;&#20102;&#21033;&#29992;&#27491;&#21017;&#21270;&#25216;&#26415;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#30340;&#26041;&#27861;&#65292;&#20197;&#20943;&#36731;&#36127;&#36801;&#31227;&#24182;&#25552;&#39640;&#21435;&#22122;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#21508;&#20010;&#39046;&#22495;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23427;&#22312;&#21516;&#26102;&#28085;&#30422;&#19981;&#21516;&#22122;&#22768;&#27700;&#24179;&#30340;&#21435;&#22122;&#20219;&#21153;&#19978;&#35757;&#32451;&#27169;&#22411;&#65292;&#20195;&#34920;&#20102;&#19968;&#31181;&#22810;&#20219;&#21153;&#23398;&#20064;&#65288;MTL&#65289;&#30340;&#24418;&#24335;&#12290;&#28982;&#32780;&#65292;&#20174;MTL&#30340;&#35282;&#24230;&#20998;&#26512;&#21644;&#25913;&#21892;&#25193;&#25955;&#27169;&#22411;&#20173;&#28982;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#29305;&#21035;&#22320;&#65292;MTL&#26377;&#26102;&#20250;&#23548;&#33268;&#20247;&#25152;&#21608;&#30693;&#30340;$\textit{&#36127;&#36801;&#31227;}$&#29616;&#35937;&#65292;&#36825;&#31181;&#29616;&#35937;&#26159;&#30001;&#20110;&#20219;&#21153;&#20043;&#38388;&#23384;&#22312;&#20914;&#31361;&#32780;&#23548;&#33268;&#26576;&#20123;&#20219;&#21153;&#30340;&#24615;&#33021;&#38477;&#20302;&#12290;&#26412;&#25991;&#26088;&#22312;&#20174;MTL&#30340;&#35282;&#24230;&#20998;&#26512;&#25193;&#25955;&#35757;&#32451;&#65292;&#25552;&#20986;&#20102;&#20004;&#20010;&#20851;&#38190;&#35266;&#23519;&#65306;$\textbf{(O1)}$ &#38543;&#30528;&#22122;&#22768;&#27700;&#24179;&#20043;&#38388;&#30340;&#24046;&#36317;&#21152;&#22823;&#65292;&#21435;&#22122;&#20219;&#21153;&#20043;&#38388;&#30340;&#20219;&#21153;&#20146;&#21644;&#21147;&#20943;&#24369;&#65292; $\textbf{(O2)}$ &#22312;&#25193;&#25955;&#35757;&#32451;&#30340;&#32972;&#26223;&#19979;&#65292;&#36127;&#36801;&#31227;&#20063;&#21487;&#33021;&#20250;&#20986;&#29616;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20943;&#36731;&#36127;&#36801;&#31227;&#26469;&#22686;&#24378;&#25193;&#25955;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21033;&#29992;&#29616;&#26377;&#30340;MTL&#26041;&#27861;&#12289;&#20855;&#20307;&#26159;&#27491;&#21017;&#21270;&#25216;&#26415;&#65292;&#26469;&#40723;&#21169;&#20219;&#21153;&#29305;&#23450;&#30340;&#29305;&#24449;&#25552;&#21462;&#24182;&#20943;&#23569;&#20219;&#21153;&#24178;&#25200;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20943;&#36731;&#20102;&#36127;&#36801;&#31227;&#65292;&#25552;&#39640;&#20102;&#25193;&#25955;&#27169;&#22411;&#22312;&#19968;&#31995;&#21015;&#21435;&#22122;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based generative models have achieved remarkable success in various domains. It trains a model on denoising tasks that encompass different noise levels simultaneously, representing a form of multi-task learning (MTL). However, analyzing and improving diffusion models from an MTL perspective remains under-explored. In particular, MTL can sometimes lead to the well-known phenomenon of $\textit{negative transfer}$, which results in the performance degradation of certain tasks due to conflicts between tasks. In this paper, we aim to analyze diffusion training from an MTL standpoint, presenting two key observations: $\textbf{(O1)}$ the task affinity between denoising tasks diminishes as the gap between noise levels widens, and $\textbf{(O2)}$ negative transfer can arise even in the context of diffusion training. Building upon these observations, our objective is to enhance diffusion training by mitigating negative transfer. To achieve this, we propose leveraging existing MTL metho
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;IBIA&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;PGM&#36716;&#21270;&#20026;&#26377;&#30028;&#22242;&#22823;&#23567;&#30340;SLCTF&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#21551;&#21457;&#24335;&#32622;&#20449;&#24230;&#26356;&#26032;&#31639;&#27861;&#26469;&#25512;&#23548;&#36793;&#32536;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2306.00335</link><description>&lt;p&gt;
&#20351;&#29992;IBIA&#26694;&#26550;&#30340;&#36793;&#32536;&#36817;&#20284;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Approximate inference of marginals using the IBIA framework. (arXiv:2306.00335v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00335
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;IBIA&#26694;&#26550;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#23558;PGM&#36716;&#21270;&#20026;&#26377;&#30028;&#22242;&#22823;&#23567;&#30340;SLCTF&#24207;&#21015;&#65292;&#24182;&#20351;&#29992;&#21551;&#21457;&#24335;&#32622;&#20449;&#24230;&#26356;&#26032;&#31639;&#27861;&#26469;&#25512;&#23548;&#36793;&#32536;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#31639;&#27861;&#27604;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#26356;&#21152;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#22270;&#27169;&#22411;&#65288;PGM&#65289;&#20013;&#36793;&#32536;&#30340;&#31934;&#30830;&#25512;&#26029;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#35745;&#31639;&#30340;&#65292;&#22240;&#27492;&#38656;&#35201;&#20351;&#29992;&#36817;&#20284;&#26041;&#27861;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#21464;&#20998;&#25216;&#26415;&#22312;&#29615;&#36335;&#22270;&#20013;&#25191;&#34892;&#36845;&#20195;&#20449;&#24687;&#20256;&#36882;&#65292;&#36825;&#23545;&#20110;&#35768;&#22810;&#22522;&#20934;&#27979;&#35797;&#26469;&#35828;&#25910;&#25947;&#36895;&#24230;&#24930;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22686;&#37327;&#26500;&#24314;-&#25512;&#29702;-&#36817;&#20284;&#65288;IBIA&#65289;&#33539;&#20363;&#30340;&#26032;&#22411;&#36793;&#32536;&#25512;&#26029;&#31639;&#27861;&#12290;&#25105;&#20204;&#30340;&#31639;&#27861;&#23558;PGM&#36716;&#21270;&#20026;&#20855;&#26377;&#26377;&#30028;&#22242;&#22823;&#23567;&#30340;&#19968;&#31995;&#21015;&#38142;&#25509;&#30340;&#22242;&#26641;&#26862;&#26519;&#65288;SLCTF&#65289;&#65292;&#28982;&#21518;&#20351;&#29992;&#21551;&#21457;&#24335;&#32622;&#20449;&#24230;&#26356;&#26032;&#31639;&#27861;&#26469;&#25512;&#23548;&#36793;&#32536;&#12290;&#23545;&#20110;&#36125;&#21494;&#26031;&#32593;&#32476;&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#26174;&#31034;&#22914;&#26524;IBIA&#20013;&#22686;&#37327;&#26500;&#24314;&#27493;&#39588;&#20351;&#29992;&#21464;&#37327;&#30340;&#25299;&#25169;&#39034;&#24207;&#65292;&#21017;&#65288;a&#65289;&#25152;&#26377;SLCTF&#20013;&#30340;CTF&#30340;&#20808;&#39564;&#36793;&#32536;&#19968;&#33268;&#65292;&#65288;b&#65289;&#19968;&#26086;&#23558;&#25152;&#26377;&#35777;&#25454;&#21464;&#37327;&#28155;&#21152;&#21040;SLCTF&#20013;&#65292;&#21518;&#39564;&#36793;&#32536;&#23601;&#26159;&#19968;&#33268;&#30340;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;&#32622;&#20449;&#20256;&#36882;&#27493;&#39588;&#26159;&#38750;&#36845;&#20195;&#30340;&#65292;&#20934;&#30830;&#24230;&#21644;&#22797;&#26434;&#24230;&#20043;&#38388;&#30340;&#26435;&#34913;&#21487;&#20197;&#36890;&#36807;&#35843;&#25972;&#22242;&#30340;&#22823;&#23567;&#26469;&#25511;&#21046;&#12290;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#25105;&#20204;&#31639;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Exact inference of marginals in probabilistic graphical models (PGM) is known to be intractable, necessitating the use of approximate methods. Most of the existing variational techniques perform iterative message passing in loopy graphs which is slow to converge for many benchmarks. In this paper, we propose a new algorithm for marginal inference that is based on the incremental build-infer-approximate (IBIA) paradigm. Our algorithm converts the PGM into a sequence of linked clique tree forests (SLCTF) with bounded clique sizes, and then uses a heuristic belief update algorithm to infer the marginals. For the special case of Bayesian networks, we show that if the incremental build step in IBIA uses the topological order of variables then (a) the prior marginals are consistent in all CTFs in the SLCTF and (b) the posterior marginals are consistent once all evidence variables are added to the SLCTF. In our approach, the belief propagation step is non-iterative and the accuracy-complexity
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#30446;&#26631;&#21306;&#22495;&#20462;&#25913;AI&#21512;&#25104;&#22270;&#20687;&#24182;&#20445;&#30041;&#21407;&#22987;&#19978;&#19979;&#25991;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#26041;&#27861;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.00219</link><description>&lt;p&gt;
&#25193;&#25955;&#30011;&#31508;&#65306;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#29983;&#25104;&#22270;&#20687;&#32534;&#36753;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Diffusion Brush: A Latent Diffusion Model-based Editing Tool for AI-generated Images. (arXiv:2306.00219v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00219
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#26681;&#25454;&#30446;&#26631;&#21306;&#22495;&#20462;&#25913;AI&#21512;&#25104;&#22270;&#20687;&#24182;&#20445;&#30041;&#21407;&#22987;&#19978;&#19979;&#25991;&#12290;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#27604;&#36739;&#65292;&#35813;&#26041;&#27861;&#22312;&#29992;&#25143;&#30740;&#31350;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#30340;&#29983;&#25104;&#27169;&#22411;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#38480;&#21046;&#65292;&#29983;&#25104;&#30340;&#22270;&#20687;&#32463;&#24120;&#21253;&#21547;&#19981;&#33391;&#30340;&#20266;&#24433;&#25110;&#20854;&#20182;&#38169;&#35823;&#12290;&#29616;&#26377;&#30340;&#24494;&#35843;&#29983;&#25104;&#22270;&#20687;&#30340;&#25216;&#26415;&#35201;&#20040;&#32791;&#26102;&#65288;&#25163;&#21160;&#32534;&#36753;&#65289;&#65292;&#35201;&#20040;&#20135;&#29983;&#19981;&#22815;&#23436;&#32654;&#30340;&#32467;&#26524;&#65288;&#20462;&#34917;&#65289;&#65292;&#35201;&#20040;&#20250;&#23548;&#33268;&#25972;&#20307;&#22270;&#20687;&#20135;&#29983;&#24847;&#24819;&#19981;&#21040;&#30340;&#21464;&#21270;&#65288;&#21464;&#20307;&#36873;&#25321;&#21644;&#25552;&#31034;&#24494;&#35843;&#65289;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25193;&#25955;&#30011;&#31508;&#30340;&#22522;&#20110;&#28508;&#22312;&#25193;&#25955;&#27169;&#22411;&#30340;AI&#22270;&#20687;&#24494;&#35843;&#24037;&#20855;&#65292;&#21487;&#20197;&#26377;&#25928;&#22320;&#24494;&#35843;AI&#21512;&#25104;&#22270;&#20687;&#20013;&#25152;&#38656;&#30340;&#21306;&#22495;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21453;&#21521;&#25193;&#25955;&#36807;&#31243;&#20013;&#22312;&#30446;&#26631;&#21306;&#22495;&#24341;&#20837;&#20102;&#26032;&#30340;&#38543;&#26426;&#22122;&#22768;&#27169;&#24335;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#20445;&#30041;&#20854;&#20182;&#21306;&#22495;&#21407;&#22987;&#19978;&#19979;&#25991;&#30340;&#21516;&#26102;&#65292;&#39640;&#25928;&#22320;&#23545;&#25351;&#23450;&#21306;&#22495;&#36827;&#34892;&#26356;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#33402;&#26415;&#23478;&#36827;&#34892;&#30340;&#29992;&#25143;&#30740;&#31350;&#35780;&#20272;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#29992;&#24615;&#21644;&#26377;&#25928;&#24615;&#65292;&#23558;&#25105;&#20204;&#30340;&#25216;&#26415;&#19982;&#20854;&#20182;&#26368;&#20808;&#36827;&#30340;&#22270;&#20687;&#20462;&#22797;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-to-image generative models have made remarkable advancements in generating high-quality images. However, generated images often contain undesirable artifacts or other errors due to model limitations. Existing techniques to fine-tune generated images are time-consuming (manual editing), produce poorly-integrated results (inpainting), or result in unexpected changes across the entire image (variation selection and prompt fine-tuning). In this work, we present Diffusion Brush, a Latent Diffusion Model-based (LDM) tool to efficiently fine-tune desired regions within an AI-synthesized image. Our method introduces new random noise patterns at targeted regions during the reverse diffusion process, enabling the model to efficiently make changes to the specified regions while preserving the original context for the rest of the image. We evaluate our method's usability and effectiveness through a user study with artists, comparing our technique against other state-of-the-art image inpaintin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#23384;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#38543;&#26426;&#36873;&#25321;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#19982;&#20840;&#37096;&#31070;&#32463;&#20803;&#30456;&#20284;&#24230;&#39640;&#19988;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#29992;&#20110;&#38477;&#20302;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2306.00183</link><description>&lt;p&gt;
&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#25193;&#25955;&#20887;&#20313;
&lt;/p&gt;
&lt;p&gt;
Diffused Redundancy in Pre-trained Representations. (arXiv:2306.00183v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.00183
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#23384;&#22312;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#38543;&#26426;&#36873;&#25321;&#19968;&#37096;&#20998;&#31070;&#32463;&#20803;&#19982;&#20840;&#37096;&#31070;&#32463;&#20803;&#30456;&#20284;&#24230;&#39640;&#19988;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#29992;&#20110;&#38477;&#20302;&#23454;&#38469;&#37096;&#32626;&#20013;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#33719;&#24471;&#30340;&#34920;&#31034;&#24050;&#34987;&#36234;&#26469;&#36234;&#22810;&#22320;&#25104;&#21151;&#24212;&#29992;&#20110;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#26356;&#21152;&#28145;&#20837;&#22320;&#30740;&#31350;&#20102;&#36825;&#31181;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#29305;&#24449;&#26159;&#22914;&#20309;&#34987;&#32534;&#30721;&#30340;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#32473;&#23450;&#23618;&#20013;&#23398;&#21040;&#30340;&#34920;&#31034;&#23637;&#29616;&#20986;&#19968;&#23450;&#31243;&#24230;&#30340;&#25193;&#25955;&#20887;&#20313;&#65292;&#21363;&#23545;&#20110;&#36229;&#36807;&#19968;&#20010;&#38408;&#20540;&#22823;&#23567;&#30340;&#20219;&#20309;&#38543;&#26426;&#23376;&#38598;&#31070;&#32463;&#20803;&#65292;&#37117;&#19982;&#23436;&#25972;&#23618;&#20855;&#26377;&#24456;&#39640;&#30340;&#30456;&#20284;&#24230;&#65292;&#24182;&#19988;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#33021;&#22815;&#34920;&#29616;&#20986;&#19982;&#25972;&#20010;&#23618;&#30456;&#20284;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#19981;&#21516;&#30340;&#31070;&#32463;&#26550;&#26500;&#65288;&#21253;&#25324;CNN&#21644;Transformer&#65289;&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#20351;&#29992;&#20102;ImageNet1k&#21644;ImageNet21k&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#35780;&#20272;&#20102;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#65292;&#22914;&#22270;&#20687;&#20998;&#31867;&#12289;&#30446;&#26631;&#26816;&#27979;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#21033;&#29992;&#39044;&#35757;&#32451;&#34920;&#31034;&#20013;&#30340;&#20887;&#20313;&#26469;&#38477;&#20302;&#22312;&#23454;&#38469;&#37096;&#32626;&#20013;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#25104;&#26412;&#65292;&#21516;&#26102;&#20173;&#28982;&#20445;&#25345;&#30456;&#24403;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Representations learned by pre-training a neural network on a large dataset are increasingly used successfully to perform a variety of downstream tasks. In this work, we take a closer look at how features are encoded in such pre-trained representations. We find that learned representations in a given layer exhibit a degree of diffuse redundancy, i.e., any randomly chosen subset of neurons in the layer that is larger than a threshold size shares a large degree of similarity with the full layer and is able to perform similarly as the whole layer on a variety of downstream tasks. For example, a linear probe trained on $20\%$ of randomly picked neurons from a ResNet50 pre-trained on ImageNet1k achieves an accuracy within $5\%$ of a linear probe trained on the full layer of neurons for downstream CIFAR10 classification. We conduct experiments on different neural architectures (including CNNs and Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a variety of downstrea
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lattice&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#31574;&#30053;&#32593;&#32476;&#30340;&#28508;&#22312;&#29366;&#24577;&#20013;&#27880;&#20837;&#26102;&#38388;&#30456;&#20851;&#24615;&#22122;&#22768;&#65292;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#22810;&#20316;&#29992;&#22120;&#31995;&#32479;&#23384;&#22312;&#30340;&#25506;&#32034;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.20065</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#28508;&#22312;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Latent Exploration for Reinforcement Learning. (arXiv:2305.20065v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.20065
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Lattice&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21521;&#31574;&#30053;&#32593;&#32476;&#30340;&#28508;&#22312;&#29366;&#24577;&#20013;&#27880;&#20837;&#26102;&#38388;&#30456;&#20851;&#24615;&#22122;&#22768;&#65292;&#26469;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#22810;&#20316;&#29992;&#22120;&#31995;&#32479;&#23384;&#22312;&#30340;&#25506;&#32034;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#65292;&#26234;&#33021;&#20307;&#36890;&#36807;&#25506;&#32034;&#21644;&#19982;&#29615;&#22659;&#20114;&#21160;&#26469;&#23398;&#20064;&#31574;&#30053;&#12290;&#30001;&#20110;&#32500;&#24230;&#28798;&#38590;&#65292;&#23398;&#20064;&#23558;&#39640;&#32500;&#24863;&#30693;&#36755;&#20837;&#26144;&#23556;&#21040;&#36816;&#21160;&#36755;&#20986;&#30340;&#31574;&#30053;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65288;&#22914;SAC&#65292;PPO&#31561;&#65289;&#36890;&#36807;&#23545;&#20316;&#29992;&#21147;&#26045;&#21152;&#29420;&#31435;&#30340;&#39640;&#26031;&#22122;&#22768;&#26469;&#25506;&#32034;&#29615;&#22659;&#12290;&#23613;&#31649;&#36825;&#31181;&#38750;&#32467;&#26500;&#21270;&#30340;&#25506;&#32034;&#26041;&#27861;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#35777;&#26126;&#25104;&#21151;&#65292;&#20294;&#23545;&#20110;&#36807;&#21160;&#20316;&#31995;&#32479;&#26469;&#35828;&#21487;&#33021;&#19981;&#22815;&#20248;&#21270;&#12290;&#24403;&#22810;&#20010;&#20316;&#29992;&#22120;&#65288;&#22914;&#39532;&#36798;&#25110;&#32908;&#32905;&#65289;&#39537;&#21160;&#34892;&#20026;&#26102;&#65292;&#19981;&#30456;&#20851;&#30340;&#25200;&#21160;&#21487;&#33021;&#20250;&#20943;&#23569;&#24444;&#27492;&#30340;&#24433;&#21709;&#65292;&#25110;&#20197;&#19982;&#20219;&#21153;&#26080;&#20851;&#30340;&#26041;&#24335;&#20462;&#25913;&#34892;&#20026;&#12290;&#34429;&#28982;&#24050;&#32463;&#23384;&#22312;&#36890;&#36807;&#24341;&#20837;&#21160;&#20316;&#25200;&#21160;&#20043;&#38388;&#30340;&#26102;&#38388;&#30456;&#20851;&#24615;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#20294;&#24573;&#35270;&#20102;&#36328;&#20316;&#29992;&#22120;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28508;&#22312;&#26102;&#38388;&#30456;&#20851;&#25506;&#32034;&#65288;Lattice&#65289;&#65292;&#19968;&#31181;&#23558;&#26102;&#38388;&#30456;&#20851;&#22122;&#22768;&#27880;&#20837;&#21040;&#31574;&#30053;&#32593;&#32476;&#30340;&#28508;&#22312;&#29366;&#24577;&#20013;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In Reinforcement Learning, agents learn policies by exploring and interacting with the environment. Due to the curse of dimensionality, learning policies that map high-dimensional sensory input to motor output is particularly challenging. During training, state of the art methods (SAC, PPO, etc.) explore the environment by perturbing the actuation with independent Gaussian noise. While this unstructured exploration has proven successful in numerous tasks, it can be suboptimal for overactuated systems. When multiple actuators, such as motors or muscles, drive behavior, uncorrelated perturbations risk diminishing each other's effect, or modifying the behavior in a task-irrelevant way. While solutions to introduce time correlation across action perturbations exist, introducing correlation across actuators has been largely ignored. Here, we propose LATent TIme-Correlated Exploration (Lattice), a method to inject temporally-correlated noise into the latent state of the policy network, which
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#21644;&#26494;&#24347;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#36825;&#23545;&#20110;RL&#31639;&#27861;&#35774;&#35745;&#20197;&#21450;&#26410;&#26469;&#30340;&#21487;&#22797;&#21046;&#24615;&#30740;&#31350;&#20855;&#26377;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.19562</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#29616;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Replicability in Reinforcement Learning. (arXiv:2305.19562v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19562
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#25552;&#20986;&#20102;&#21487;&#22797;&#21046;&#31639;&#27861;&#21644;&#26494;&#24347;&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#24182;&#32473;&#20986;&#20102;&#30456;&#24212;&#30340;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#65292;&#36825;&#23545;&#20110;RL&#31639;&#27861;&#35774;&#35745;&#20197;&#21450;&#26410;&#26469;&#30340;&#21487;&#22797;&#21046;&#24615;&#30740;&#31350;&#20855;&#26377;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22312;&#24378;&#21270;&#23398;&#20064; (RL) &#30340;&#32972;&#26223;&#19979;&#65292;&#23558;&#21487;&#22797;&#29616;&#24615;&#20316;&#20026;&#31639;&#27861;&#23646;&#24615;&#36827;&#34892;&#20102;&#25968;&#23398;&#30740;&#31350;&#12290;&#25105;&#20204;&#20851;&#27880;&#30340;&#26159;&#20855;&#26377;&#29983;&#25104;&#27169;&#22411;&#35775;&#38382;&#26435;&#30340;&#24102;&#25240;&#25187;&#34920;&#26684;MDP&#30340;&#22522;&#26412;&#35774;&#32622;&#12290;&#21463;Impagliazzo&#31561;&#20154; [2022]&#30340;&#21551;&#21457;&#65292;&#22914;&#26524;&#22312;&#20869;&#37096;&#38543;&#26426;&#24615;&#30456;&#21516;&#26102;&#65292;RL&#31639;&#27861;&#22312;&#20174;&#29983;&#25104;&#22120;&#25277;&#21462;&#30340;&#20004;&#20010;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#30340;&#26679;&#26412;&#19978;&#25191;&#34892;&#20004;&#27425;&#24182;&#36755;&#20986;&#23436;&#20840;&#30456;&#21516;&#30340;&#31574;&#30053;&#65292;&#21017;&#34920;&#31034;&#35813;RL&#31639;&#27861;&#26159;&#21487;&#22797;&#21046;&#30340;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20379;&#19968;&#20010;&#26377;&#25928;&#30340;$\rho$-&#21487;&#22797;&#21046;&#31639;&#27861;&#65292;&#29992;&#20110;$(\varepsilon,\delta)$-&#26368;&#20248;&#31574;&#30053;&#20272;&#35745;&#65292;&#20854;&#26679;&#26412;&#21644;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026; $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$&#65292;&#20854;&#20013;$N$&#26159;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30340;&#25968;&#37327;&#12290;&#28982;&#21518;&#65292;&#23545;&#20110;&#30830;&#23450;&#24615;&#31639;&#27861;&#30340;&#23376;&#31867;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102; $ \Omega\left(\frac {N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right) $ &#38454;&#30340;&#19979;&#38480;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;Kalavasis&#31561;&#20154;[2019]&#25552;&#20986;&#30340;&#21487;&#22797;&#21046;&#24615;&#30340;&#26494;&#24347;&#29256;&#26412;&#65292;&#20854;&#20013;&#20165;&#35201;&#27714;&#31639;&#27861;&#30340;&#36755;&#20986;&#25509;&#36817;&#22797;&#21046;&#31639;&#27861;&#30340;&#36755;&#20986;&#65292;&#32780;&#19981;&#26159;&#30456;&#21516;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#31639;&#27861;&#65292;&#20854;&#26102;&#38388;&#21644;&#26679;&#26412;&#22797;&#26434;&#24230;&#20026; $\widetilde O\left(\frac{N^5\cdot\log(1/\delta)}{(1-\gamma)^9\cdot\varepsilon^4\cdot\rho^2}\right)$&#65292;&#29992;&#20110;$(\varepsilon,\delta)$&#24847;&#20041;&#19979;&#30340;&#21487;&#22797;&#21046;&#24615;&#65292;&#36825;&#27604;&#20808;&#21069;&#19982;&#30456;&#20851;&#38382;&#39064;&#30340;&#30028;&#38480;&#26356;&#22909;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;RL&#31639;&#27861;&#35774;&#35745;&#21644;&#21487;&#37325;&#22797;&#24615;&#30740;&#31350;&#30340;&#26410;&#26469;&#26041;&#21521;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We initiate the mathematical study of replicability as an algorithmic property in the context of reinforcement learning (RL). We focus on the fundamental setting of discounted tabular MDPs with access to a generative model. Inspired by Impagliazzo et al. [2022], we say that an RL algorithm is replicable if, with high probability, it outputs the exact same policy after two executions on i.i.d. samples drawn from the generator when its internal randomness is the same. We first provide an efficient $\rho$-replicable algorithm for $(\varepsilon, \delta)$-optimal policy estimation with sample and time complexity $\widetilde O\left(\frac{N^3\cdot\log(1/\delta)}{(1-\gamma)^5\cdot\varepsilon^2\cdot\rho^2}\right)$, where $N$ is the number of state-action pairs. Next, for the subclass of deterministic algorithms, we provide a lower bound of order $\Omega\left(\frac{N^3}{(1-\gamma)^3\cdot\varepsilon^2\cdot\rho^2}\right)$. Then, we study a relaxed version of replicability proposed by Kalavasis et 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20195;&#29702;&#31243;&#24207;SheetCopilot&#65292;&#35813;&#31243;&#24207;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#36719;&#20214;&#25191;&#34892;&#30005;&#23376;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#31561;&#20219;&#21153;&#12290;&#35813;&#31243;&#24207;&#35774;&#35745;&#20102;&#19968;&#32452;&#25277;&#35937;&#30340;&#30005;&#23376;&#34920;&#26684;&#36719;&#20214;&#21151;&#33021;&#21407;&#23376;&#21160;&#20316;&#20197;&#21450;&#22522;&#20110;&#29366;&#24577;&#26426;&#30340;&#20219;&#21153;&#35268;&#21010;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;LLMs&#19982;&#30005;&#23376;&#34920;&#26684;&#30340;&#40065;&#26834;&#20132;&#20114;&#65292;&#21487;&#20197;&#21333;&#27425;&#27491;&#30830;&#23436;&#25104;44.3&#65285;&#30340;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.19308</link><description>&lt;p&gt;
SheetCopilot: &#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23558;&#36719;&#20214;&#29983;&#20135;&#21147;&#25552;&#21319;&#21040;&#26032;&#30340;&#27700;&#24179;
&lt;/p&gt;
&lt;p&gt;
SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models. (arXiv:2305.19308v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19308
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20195;&#29702;&#31243;&#24207;SheetCopilot&#65292;&#35813;&#31243;&#24207;&#21487;&#20197;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25351;&#23548;&#36719;&#20214;&#25191;&#34892;&#30005;&#23376;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#31561;&#20219;&#21153;&#12290;&#35813;&#31243;&#24207;&#35774;&#35745;&#20102;&#19968;&#32452;&#25277;&#35937;&#30340;&#30005;&#23376;&#34920;&#26684;&#36719;&#20214;&#21151;&#33021;&#21407;&#23376;&#21160;&#20316;&#20197;&#21450;&#22522;&#20110;&#29366;&#24577;&#26426;&#30340;&#20219;&#21153;&#35268;&#21010;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;LLMs&#19982;&#30005;&#23376;&#34920;&#26684;&#30340;&#40065;&#26834;&#20132;&#20114;&#65292;&#21487;&#20197;&#21333;&#27425;&#27491;&#30830;&#23436;&#25104;44.3&#65285;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#26426;&#32456;&#31471;&#29992;&#25143;&#33457;&#36153;&#20102;&#25968;&#21313;&#20159;&#23567;&#26102;&#23436;&#25104;&#35832;&#22914;&#34920;&#26684;&#25968;&#25454;&#22788;&#29702;&#21644;&#39033;&#30446;&#26102;&#38388;&#36724;&#35843;&#24230;&#31561;&#26085;&#24120;&#20219;&#21153;&#12290;&#36825;&#20123;&#20219;&#21153;&#22823;&#22810;&#26159;&#37325;&#22797;&#24615;&#30340;&#21644;&#23481;&#26131;&#20986;&#38169;&#30340;&#65292;&#28982;&#32780;&#22823;&#22810;&#25968;&#32456;&#31471;&#29992;&#25143;&#32570;&#20047;&#33258;&#21160;&#21270;&#36825;&#20123;&#32321;&#29712;&#24037;&#20316;&#30340;&#25216;&#33021;&#12290;&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#65292;&#29992;&#33258;&#28982;&#35821;&#35328;&#29992;&#25143;&#35831;&#27714;&#25351;&#23548;&#36719;&#20214;&#25104;&#20026;&#20102;&#19968;&#20010;&#21487;&#36798;&#25104;&#30340;&#30446;&#26631;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;SheetCopilot&#20195;&#29702;&#65292;&#35813;&#20195;&#29702;&#25509;&#21463;&#33258;&#28982;&#35821;&#35328;&#20219;&#21153;&#21644;&#25511;&#21046;&#30005;&#23376;&#34920;&#26684;&#20197;&#28385;&#36275;&#35201;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#21407;&#23376;&#21160;&#20316;&#20316;&#20026;&#30005;&#23376;&#34920;&#26684;&#36719;&#20214;&#21151;&#33021;&#30340;&#25277;&#35937;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#22522;&#20110;&#29366;&#24577;&#26426;&#30340;&#20219;&#21153;&#35268;&#21010;&#26694;&#26550;&#65292;&#20197;&#20415;LLMs&#19982;&#30005;&#23376;&#34920;&#26684;&#36827;&#34892;&#40065;&#26834;&#30340;&#20132;&#20114;&#12290;&#25105;&#20204;&#31574;&#21010;&#20102;&#19968;&#20010;&#21253;&#21547;221&#31181;&#30005;&#23376;&#34920;&#26684;&#25511;&#21046;&#20219;&#21153;&#30340;&#20195;&#34920;&#24615;&#25968;&#25454;&#38598;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#23436;&#20840;&#33258;&#21160;&#21270;&#30340;&#35780;&#20272;&#31649;&#36947;&#65292;&#20197;&#20005;&#26684;&#35780;&#20272;LLMs&#22312;&#36719;&#20214;&#25511;&#21046;&#20219;&#21153;&#20013;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;SheetCopilot&#21333;&#27425;&#27491;&#30830;&#23436;&#25104;44.3&#65285;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill of automating away these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent which takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\% of tasks for a single 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#31574;&#30053;&#23398;&#20064;&#22312;NetHack&#20013;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#20998;&#26512;&#20102;&#33719;&#32988;&#30340;&#31526;&#21495;&#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#21160;&#20316;&#23618;&#32423;&#30340;&#20248;&#21183;&#12289;&#31070;&#32463;&#26550;&#26500;&#30340;&#25913;&#36827;&#20197;&#21450;&#24378;&#21270;&#23398;&#20064;&#19982;&#27169;&#20223;&#23398;&#20064;&#30340;&#25972;&#21512;&#31561;&#26041;&#38754;&#21487;&#33021;&#25552;&#21319;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.19240</link><description>&lt;p&gt;
NetHack&#24456;&#38590;&#34987;&#40657;&#23458;&#20837;&#20405;&#12290;
&lt;/p&gt;
&lt;p&gt;
NetHack is Hard to Hack. (arXiv:2305.19240v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.19240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31070;&#32463;&#31574;&#30053;&#23398;&#20064;&#22312;NetHack&#20013;&#30340;&#24615;&#33021;&#24046;&#36317;&#65292;&#24182;&#20998;&#26512;&#20102;&#33719;&#32988;&#30340;&#31526;&#21495;&#20195;&#29702;&#65292;&#25552;&#20986;&#20102;&#21160;&#20316;&#23618;&#32423;&#30340;&#20248;&#21183;&#12289;&#31070;&#32463;&#26550;&#26500;&#30340;&#25913;&#36827;&#20197;&#21450;&#24378;&#21270;&#23398;&#20064;&#19982;&#27169;&#20223;&#23398;&#20064;&#30340;&#25972;&#21512;&#31561;&#26041;&#38754;&#21487;&#33021;&#25552;&#21319;&#24615;&#33021;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#31574;&#30053;&#23398;&#20064;&#26041;&#27861;&#22312;&#21508;&#31181;&#25511;&#21046;&#38382;&#39064;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20174;Atari&#28216;&#25103;&#21040;&#27169;&#25311;&#36816;&#21160;&#12290;&#28982;&#32780;&#65292;&#22312;&#38271;&#35270;&#37326;&#20219;&#21153;&#20013;&#65292;&#23588;&#20854;&#26159;&#22312;&#20855;&#26377;&#22810;&#27169;&#24577;&#35266;&#27979;&#30340;&#24320;&#25918;&#24335;&#29615;&#22659;&#20013;&#65292;&#27604;&#22914;&#27969;&#34892;&#30340;&#22320;&#29282;&#25506;&#38505;&#28216;&#25103;NetHack&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#37117;&#38754;&#20020;&#22256;&#38590;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;NeurIPS 2021 NetHack&#25361;&#25112;&#36187;&#34920;&#26126;&#65292;&#31526;&#21495;&#20195;&#29702;&#22312;&#20013;&#20301;&#28216;&#25103;&#24471;&#20998;&#19978;&#36229;&#36807;&#31070;&#32463;&#26041;&#27861;&#22235;&#20493;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#28145;&#20837;&#25506;&#35752;&#20102;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#24182;&#23545;NetHack&#30340;&#31070;&#32463;&#31574;&#30053;&#23398;&#20064;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#20026;&#20102;&#36827;&#34892;&#36825;&#39033;&#30740;&#31350;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#33719;&#32988;&#30340;&#31526;&#21495;&#20195;&#29702;&#65292;&#24182;&#25193;&#23637;&#20102;&#20854;&#20195;&#30721;&#24211;&#20197;&#36319;&#36394;&#20869;&#37096;&#31574;&#30053;&#36873;&#25321;&#65292;&#20197;&#29983;&#25104;&#20854;&#20013;&#19968;&#20010;&#26368;&#22823;&#30340;&#21487;&#29992;&#28436;&#31034;&#25968;&#25454;&#38598;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20197;&#19979;&#20960;&#20010;&#26041;&#38754;&#65306;(i) &#21160;&#20316;&#23618;&#32423;&#30340;&#20248;&#21183;&#65307;(ii) &#31070;&#32463;&#26550;&#26500;&#30340;&#25913;&#36827;&#65307;&#20197;&#21450; (iii) &#24378;&#21270;&#23398;&#20064;&#19982;&#27169;&#20223;&#23398;&#20064;&#30340;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; Gram &#30697;&#38453;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#21644;&#23618;&#35268;&#33539;&#21270;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2305.18399</link><description>&lt;p&gt;
&#20851;&#20110;&#28608;&#27963;&#20989;&#25968;&#21644;&#35268;&#33539;&#21270;&#23545;&#21021;&#22987;&#21270;&#31561;&#36317;&#23884;&#20837;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
On the impact of activation and normalization in obtaining isometric embeddings at initialization. (arXiv:2305.18399v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18399
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340; Gram &#30697;&#38453;&#32467;&#26500;&#65292;&#35777;&#26126;&#20102;&#28608;&#27963;&#20989;&#25968;&#21644;&#23618;&#35268;&#33539;&#21270;&#32467;&#21512;&#20351;&#29992;&#21487;&#20197;&#22312;&#21021;&#22987;&#21270;&#26102;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#20174;&#32780;&#24357;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#20498;&#25968;&#31532;&#20108;&#20010; Gram &#30697;&#38453;&#30340;&#32467;&#26500;&#65292;&#35813;&#30697;&#38453;&#21253;&#21547;&#19982;&#19968;&#25209;&#36755;&#20837;&#23545;&#24212;&#30340;&#36755;&#20986;&#20043;&#38388;&#30340;&#25104;&#23545;&#20869;&#31215;&#12290;&#22312;&#20960;&#31181;&#26550;&#26500;&#20013;&#65292;&#35266;&#23519;&#21040;&#22312;&#21021;&#22987;&#21270;&#26102;&#35813; Gram &#30697;&#38453;&#20250;&#38543;&#30528;&#28145;&#24230;&#21464;&#24471;&#36864;&#21270;&#65292;&#20174;&#32780;&#20005;&#37325;&#20943;&#32531;&#35757;&#32451;&#36895;&#24230;&#12290;&#35268;&#33539;&#21270;&#23618;&#22914;&#25209;&#22788;&#29702;&#35268;&#33539;&#21270;&#25110;&#23618;&#35268;&#33539;&#21270;&#65292;&#22312;&#38450;&#27490;&#31209;&#23849;&#28291;&#38382;&#39064;&#26041;&#38754;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#29616;&#26377;&#30340;&#29702;&#35770;&#32467;&#26524;&#26080;&#27861;&#20840;&#38754;&#35206;&#30422;&#24191;&#27867;&#29992;&#20110; transformer &#20013;&#30340;&#23618;&#35268;&#33539;&#21270;&#21644;&#26377;&#38480;&#28145;&#24230;&#19979;&#35268;&#33539;&#21270;&#30340;&#37327;&#21270;&#20559;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#21021;&#22987;&#21270;&#26102;&#65292;&#32467;&#21512;&#28608;&#27963;&#20989;&#25968;&#23618;&#20351;&#29992;&#30340;&#23618;&#35268;&#33539;&#21270;&#21487;&#20197;&#20351;&#22810;&#23618;&#24863;&#30693;&#26426;&#30340; Gram &#30697;&#38453;&#20559;&#21521;&#25351;&#25968;&#32423;&#28145;&#24230;&#31561;&#36317;&#65292;&#24182;&#20351;&#29992;&#28608;&#27963;&#20989;&#25968;&#30340; Hermite &#23637;&#24320;&#26469;&#37327;&#21270;&#36825;&#20010;&#36895;&#24230;&#65292;&#20174;&#32780;&#22635;&#34917;&#20102;&#29616;&#26377;&#29702;&#35770;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we explore the structure of the penultimate Gram matrix in deep neural networks, which contains the pairwise inner products of outputs corresponding to a batch of inputs. In several architectures it has been observed that this Gram matrix becomes degenerate with depth at initialization, which dramatically slows training. Normalization layers, such as batch or layer normalization, play a pivotal role in preventing the rank collapse issue. Despite promising advances, the existing theoretical results (i) do not extend to layer normalization, which is widely used in transformers, (ii) can not characterize the bias of normalization quantitatively at finite depth.  To bridge this gap, we provide a proof that layer normalization, in conjunction with activation layers, biases the Gram matrix of a multilayer perceptron towards isometry at an exponential rate with depth at initialization. We quantify this rate using the Hermite expansion of the activation function, highlighting th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KARD&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21521;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#20837;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#26469;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.18395</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#30340;&#25512;&#29702;&#33976;&#39311;&#65306;&#38754;&#21521;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Knowledge-Augmented Reasoning Distillation for Small Language Models in Knowledge-Intensive Tasks. (arXiv:2305.18395v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18395
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;KARD&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#21521;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21152;&#20837;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#26469;&#35299;&#20915;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#35760;&#24518;&#33021;&#21147;&#26377;&#38480;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#38656;&#35201;&#22797;&#21512;&#30693;&#35782;&#29702;&#35299;&#30340;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#24615;&#33021;&#12290;&#20294;&#26159;&#65292;&#30001;&#20110;&#35745;&#31639;&#35201;&#27714;&#39640;&#19988;&#28041;&#21450;&#25968;&#25454;&#38544;&#31169;&#65292;&#23558;&#27492;&#31867;&#27169;&#22411;&#37096;&#32626;&#21040;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#20013;&#21487;&#33021;&#20250;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#19987;&#27880;&#20110;&#36890;&#36807;&#24494;&#35843;&#20855;&#26377;&#26631;&#35760;&#25968;&#25454;&#25110;&#33976;&#39311;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#26500;&#24314;&#20219;&#21153;&#29305;&#23450;&#30340;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#20294;&#26159;&#30001;&#20110;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#35760;&#24518;&#25152;&#38656;&#30693;&#35782;&#26041;&#38754;&#30340;&#33021;&#21147;&#26377;&#38480;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#36866;&#29992;&#20110;&#30693;&#35782;&#23494;&#38598;&#22411;&#25512;&#29702;&#20219;&#21153;&#12290;&#22312;&#29702;&#35770;&#20998;&#26512;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#30693;&#35782;&#22686;&#24378;&#30340;&#25512;&#29702;&#33976;&#39311; (KARD) &#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24494;&#35843;&#23567;&#22411;&#35821;&#35328;&#27169;&#22411;&#20197;&#29983;&#25104;&#20174;&#22806;&#37096;&#30693;&#35782;&#24211;&#26816;&#32034;&#21040;&#30340;&#22686;&#24378;&#30693;&#35782;&#30340;&#20381;&#25454;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31070;&#32463;&#37325;&#25490;&#22120;&#65292;&#29992;&#20110;&#33719;&#24471;&#19982;&#20381;&#25454;&#29983;&#25104;&#30456;&#20851;&#30340;&#25991;&#26723;&#12290;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#65292;KARD&#22312;&#19977;&#39033;&#30693;&#35782;&#23494;&#38598;&#22411;&#20219;&#21153;&#19978;&#26174;&#30528;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#27169;&#22411;&#23610;&#23544;&#30456;&#21516;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#36798;&#21040;&#19982;LLMs&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs) have shown promising performance in knowledge-intensive reasoning tasks that require a compound understanding of knowledge. However, deployment of the LLMs in real-world applications can be challenging due to their high computational requirements and concerns on data privacy. Previous studies have focused on building task-specific small language models (LMs) by fine-tuning them with labeled data or distilling LLMs. However, these approaches are ill-suited for knowledge-intensive reasoning tasks due to the limited capacity of small LMs in memorizing the knowledge required. Motivated by our theoretical analysis on memorization, we propose Knowledge-Augmented Reasoning Distillation (KARD), a novel method that fine-tunes small LMs to generate rationales with augmented knowledge retrieved from an external knowledge base. Moreover, we further propose a neural reranker to obtain documents relevant to rationale generation. We empirically show that KARD significantl
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#19981;&#33021;&#20165;&#20165;&#20381;&#38752;&#32622;&#20449;&#24230;&#65292;&#36824;&#24212;&#32771;&#34385;&#39044;&#27979;&#26679;&#26412;&#25110;&#31867;&#21035;&#30340;&#38750;&#20856;&#22411;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#20856;&#22411;&#36755;&#20837;&#25110;&#31867;&#21035;&#65292;&#27169;&#22411;&#39044;&#27979;&#26356;&#36807;&#20110;&#33258;&#20449;&#19988;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#65292;&#23558;&#38750;&#20856;&#22411;&#24615;&#32435;&#20837;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.18262</link><description>&lt;p&gt;
&#36229;&#36234;&#32622;&#20449;&#24230;&#65306;&#21487;&#38752;&#27169;&#22411;&#36824;&#24212;&#32771;&#34385;&#38750;&#20856;&#22411;&#24615;
&lt;/p&gt;
&lt;p&gt;
Beyond Confidence: Reliable Models Should Also Consider Atypicality. (arXiv:2305.18262v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.18262
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#65292;&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#19981;&#33021;&#20165;&#20165;&#20381;&#38752;&#32622;&#20449;&#24230;&#65292;&#36824;&#24212;&#32771;&#34385;&#39044;&#27979;&#26679;&#26412;&#25110;&#31867;&#21035;&#30340;&#38750;&#20856;&#22411;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38750;&#20856;&#22411;&#36755;&#20837;&#25110;&#31867;&#21035;&#65292;&#27169;&#22411;&#39044;&#27979;&#26356;&#36807;&#20110;&#33258;&#20449;&#19988;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#65292;&#23558;&#38750;&#20856;&#22411;&#24615;&#32435;&#20837;&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33021;&#22815;&#25552;&#20379;&#32622;&#20449;&#24230;&#20197;&#39044;&#27979;&#32467;&#26524;&#65292;&#28982;&#32780;&#65292;&#32622;&#20449;&#24230;&#26080;&#27861;&#23436;&#20840;&#29702;&#35299;&#39044;&#27979;&#30340;&#21487;&#38752;&#24615;&#12290;&#20363;&#22914;&#65292;&#24403;&#36755;&#20837;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#27809;&#26377;&#24456;&#22909;&#30340;&#34920;&#31034;&#25110;&#32773;&#36755;&#20837; inherently &#26131;&#28151;&#28102;&#26102;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#32473;&#20986;&#36739;&#20302;&#30340;&#32622;&#20449;&#24230;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26679;&#26412;&#25110;&#31867;&#21035;&#30340;&#38750;&#20856;&#22411;&#24615;&#19982;&#27169;&#22411;&#39044;&#27979;&#21487;&#38752;&#24615;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#39318;&#20808;&#35777;&#26126;&#20102;&#38750;&#20856;&#22411;&#24615;&#19982;&#35823;&#26657;&#20934;&#21644;&#20934;&#30830;&#24615;&#20043;&#38388;&#30340;&#24378;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23454;&#35777;&#34920;&#26126;&#23545;&#20110;&#38750;&#20856;&#22411;&#30340;&#36755;&#20837;&#25110;&#38750;&#20856;&#22411;&#30340;&#31867;&#21035;&#65292;&#39044;&#27979;&#26356;&#21152;&#36807;&#20110;&#33258;&#20449;&#19988;&#20934;&#30830;&#24615;&#36739;&#20302;&#12290;&#21033;&#29992;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#38750;&#20856;&#22411;&#24615;&#32435;&#20837;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#21644;&#37492;&#21035;&#24615;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21033;&#29992;&#38750;&#20856;&#22411;&#24615;&#22914;&#20309;&#25552;&#39640;&#19981;&#21516;&#32932;&#33394;&#32676;&#20307;&#30340;&#30382;&#32932;&#30149;&#21464;&#20998;&#31867;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While most machine learning models can provide confidence in their predictions, confidence is insufficient to understand a prediction's reliability. For instance, the model may have a low confidence prediction if the input is not well-represented in the training dataset or if the input is inherently ambiguous. In this work, we investigate the relationship between how atypical(rare) a sample or a class is and the reliability of a model's predictions. We first demonstrate that atypicality is strongly related to miscalibration and accuracy. In particular, we empirically show that predictions for atypical inputs or atypical classes are more overconfident and have lower accuracy. Using these insights, we show incorporating atypicality improves uncertainty quantification and model performance for discriminative neural networks and large language models. In a case study, we show that using atypicality improves the performance of a skin lesion classifier across different skin tone groups witho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#31181;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.17588</link><description>&lt;p&gt;
&#35786;&#26029;&#21464;&#21387;&#22120;&#65306;&#25581;&#31034;&#20020;&#24202;&#20915;&#31574;&#20013;&#30340;&#29305;&#24449;&#31354;&#38388;&#12290; (arXiv:2305.17588v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Diagnosing Transformers: Illuminating Feature Spaces for Clinical Decision-Making. (arXiv:2305.17588v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17588
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#22686;&#24378;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#35813;&#26694;&#26550;&#21033;&#29992;&#22810;&#31181;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#35299;&#20915;&#20102;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#23398;&#31561;&#39640;&#39118;&#38505;&#39046;&#22495;&#65292;&#20026;&#20102;&#24314;&#31435;&#20449;&#20219;&#21644;&#30830;&#20445;&#23433;&#20840;&#65292;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#20351;&#29992;&#26377;&#38480;&#30340;&#20020;&#24202;&#35760;&#24405;&#23545;&#39044;&#35757;&#32451;&#30340;&#21464;&#21387;&#22120;&#36827;&#34892;&#24494;&#35843;&#20197;&#36741;&#21161;&#20020;&#24202;&#20915;&#31574;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;SUFO&#30340;&#31995;&#32479;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22686;&#24378;&#20102;&#24494;&#35843;&#30340;&#21464;&#21387;&#22120;&#29305;&#24449;&#31354;&#38388;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;SUFO&#21033;&#29992;&#19968;&#31995;&#21015;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#25216;&#26415;&#65292;&#21253;&#25324;&#30417;&#30563;&#25506;&#32034;&#12289;&#26080;&#30417;&#30563;&#30456;&#20284;&#24615;&#20998;&#26512;&#12289;&#29305;&#24449;&#21160;&#24577;&#21644;&#24322;&#24120;&#20540;&#20998;&#26512;&#65292;&#26469;&#35299;&#20915;&#20851;&#20110;&#27169;&#22411;&#20449;&#20219;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#30740;&#31350;&#20102;&#39044;&#35757;&#32451;&#25968;&#25454;&#23545;&#30495;&#23454;&#19990;&#30028;&#30149;&#29702;&#20998;&#31867;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#24182;&#22312;MedNLI&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#21457;&#29616;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#20116;&#20010;110M&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#27169;&#22411;&#65292;&#20998;&#20026;&#36890;&#29992;&#39046;&#22495;&#65288;BERT, TNLR&#65289;&#12289;&#28151;&#21512;&#39046;&#22495;&#65288;BioBERT, Clinical BioBERT&#65289;&#21644;&#39046;&#22495;&#29305;&#23450;&#65288;PubMedBERT&#65289;&#32452;&#12290;&#25105;&#20204;&#30340;SUFO&#20998;&#26512;&#25581;&#31034;&#20102;&#65306;(1)
&lt;/p&gt;
&lt;p&gt;
Pre-trained transformers are often fine-tuned to aid clinical decision-making using limited clinical notes. Model interpretability is crucial, especially in high-stakes domains like medicine, to establish trust and ensure safety, which requires human engagement. We introduce SUFO, a systematic framework that enhances interpretability of fine-tuned transformer feature spaces. SUFO utilizes a range of analytic and visualization techniques, including Supervised probing, Unsupervised similarity analysis, Feature dynamics, and Outlier analysis to address key questions about model trust and interpretability. We conduct a case study investigating the impact of pre-training data where we focus on real-world pathology classification tasks, and validate our findings on MedNLI. We evaluate five 110M-sized pre-trained transformer models, categorized into general-domain (BERT, TNLR), mixed-domain (BioBERT, Clinical BioBERT), and domain-specific (PubMedBERT) groups. Our SUFO analyses reveal that: (1
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#36172;&#21338;&#30340;&#26041;&#24335;&#36827;&#34892;&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#65292;&#33021;&#22815;&#23545;&#19981;&#26029;&#20135;&#29983;&#30340;&#25968;&#25454;&#36827;&#34892;&#36830;&#32493;&#30340;&#30417;&#25511;&#65292;&#24182;&#22788;&#29702;&#22240;&#20998;&#24067;&#28418;&#31227;&#23548;&#33268;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17570</link><description>&lt;p&gt;
&#36890;&#36807;&#36172;&#21338;&#36827;&#34892;&#20844;&#24179;&#24615;&#23457;&#35745;
&lt;/p&gt;
&lt;p&gt;
Auditing Fairness by Betting. (arXiv:2305.17570v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#36172;&#21338;&#30340;&#26041;&#24335;&#36827;&#34892;&#20844;&#24179;&#24615;&#23457;&#35745;&#30340;&#26041;&#27861;&#65292;&#30456;&#27604;&#20043;&#21069;&#30340;&#26041;&#27861;&#65292;&#36825;&#31181;&#26041;&#27861;&#20855;&#26377;&#26356;&#39640;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#65292;&#33021;&#22815;&#23545;&#19981;&#26029;&#20135;&#29983;&#30340;&#25968;&#25454;&#36827;&#34892;&#36830;&#32493;&#30340;&#30417;&#25511;&#65292;&#24182;&#22788;&#29702;&#22240;&#20998;&#24067;&#28418;&#31227;&#23548;&#33268;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20379;&#20102;&#23454;&#29992;&#12289;&#39640;&#25928;&#12289;&#38750;&#21442;&#25968;&#26041;&#27861;&#65292;&#29992;&#20110;&#23457;&#35745;&#24050;&#37096;&#32626;&#30340;&#20998;&#31867;&#21644;&#22238;&#24402;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#12290;&#30456;&#27604;&#20043;&#21069;&#20381;&#36182;&#20110;&#22266;&#23450;&#26679;&#26412;&#37327;&#30340;&#26041;&#27861;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#24207;&#36143;&#30340;&#65292;&#24182;&#20801;&#35768;&#23545;&#19981;&#26029;&#20135;&#29983;&#30340;&#25968;&#25454;&#36827;&#34892;&#36830;&#32493;&#30340;&#30417;&#25511;&#65292;&#22240;&#27492;&#38750;&#24120;&#36866;&#29992;&#20110;&#36319;&#36394;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#20063;&#20801;&#35768;&#25968;&#25454;&#36890;&#36807;&#27010;&#29575;&#31574;&#30053;&#36827;&#34892;&#25910;&#38598;&#65292;&#32780;&#19981;&#26159;&#20174;&#20154;&#21475;&#20013;&#22343;&#21248;&#37319;&#26679;&#12290;&#36825;&#20351;&#24471;&#23457;&#35745;&#21487;&#20197;&#22312;&#20026;&#20854;&#20182;&#30446;&#30340;&#25910;&#38598;&#30340;&#25968;&#25454;&#19978;&#36827;&#34892;&#12290;&#27492;&#22806;&#65292;&#35813;&#31574;&#30053;&#21487;&#20197;&#38543;&#26102;&#38388;&#25913;&#21464;&#65292;&#24182;&#19988;&#19981;&#21516;&#30340;&#23376;&#20154;&#32676;&#21487;&#20197;&#20351;&#29992;&#19981;&#21516;&#30340;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22788;&#29702;&#22240;&#27169;&#22411;&#21464;&#26356;&#25110;&#22522;&#30784;&#20154;&#32676;&#21464;&#26356;&#23548;&#33268;&#30340;&#20998;&#24067;&#28418;&#31227;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26368;&#36817;&#20851;&#20110; anytime-valid &#25512;&#26029;&#21644;&#21338;&#24328;&#32479;&#35745;&#23398;&#30340;&#36827;&#23637;&#65292;&#23588;&#20854;&#26159;"&#36890;&#36807;&#36172;&#21338;&#36827;&#34892;&#27979;&#35797;"&#26694;&#26550;&#12290;&#36825;&#20123;&#32852;&#31995;&#30830;&#20445;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#12289;&#24555;&#36895;&#21644;&#25552;&#20379;&#32479;&#35745;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
We provide practical, efficient, and nonparametric methods for auditing the fairness of deployed classification and regression models. Whereas previous work relies on a fixed-sample size, our methods are sequential and allow for the continuous monitoring of incoming data, making them highly amenable to tracking the fairness of real-world systems. We also allow the data to be collected by a probabilistic policy as opposed to sampled uniformly from the population. This enables auditing to be conducted on data gathered for another purpose. Moreover, this policy may change over time and different policies may be used on different subpopulations. Finally, our methods can handle distribution shift resulting from either changes to the model or changes in the underlying population. Our approach is based on recent progress in anytime-valid inference and game-theoretic statistics-the "testing by betting" framework in particular. These connections ensure that our methods are interpretable, fast, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65306;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#65292;&#23427;&#26159;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#21644;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#27867;&#21270;&#21644;&#29305;&#20363;&#65292;&#20854;&#30446;&#26631;&#26159;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#65292;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2305.17225</link><description>&lt;p&gt;
&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Causal Component Analysis. (arXiv:2305.17225v1 [stat.ML])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17225
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65306;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#65292;&#23427;&#26159;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#21644;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#27867;&#21270;&#21644;&#29305;&#20363;&#65292;&#20854;&#30446;&#26631;&#26159;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#65292;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;(ICA)&#30340;&#30446;&#26631;&#26159;&#20174;&#28151;&#21512;&#35266;&#27979;&#21040;&#30340;&#21464;&#37327;&#20013;&#24674;&#22797;&#29420;&#31435;&#30340;&#28508;&#22312;&#21464;&#37327;&#12290;&#32780;&#22240;&#26524;&#34920;&#31034;&#23398;&#20064;(CRL)&#30340;&#30446;&#26631;&#26159;&#25512;&#26029;&#22240;&#26524;&#20851;&#31995;&#24378;&#30456;&#20851;&#24615;&#30340;&#28508;&#22312;&#21464;&#37327;&#65292;&#20197;&#21450;&#32534;&#30721;&#23427;&#20204;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#26410;&#30693;&#22270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20013;&#38388;&#38382;&#39064;&#65292;&#31216;&#20026;&#22240;&#26524;&#25104;&#20998;&#20998;&#26512;(CauCA)&#12290;CauCA&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;ICA&#30340;&#19968;&#31181;&#25512;&#24191;&#65292;&#23545;&#28508;&#22312;&#25104;&#20998;&#20043;&#38388;&#30340;&#22240;&#26524;&#20381;&#36182;&#24314;&#27169;&#65292;&#20063;&#26159;CRL&#30340;&#19968;&#20010;&#29305;&#20363;&#12290;&#19982;CRL&#19981;&#21516;&#30340;&#26159;&#65292;&#23427;&#39044;&#35774;&#20102;&#22240;&#26524;&#22270;&#30340;&#30693;&#35782;&#65292;&#20165;&#20851;&#27880;&#20110;&#23398;&#20064;&#35299;&#28151;&#20989;&#25968;&#21644;&#22240;&#26524;&#26426;&#21046;&#12290;&#25152;&#26377;&#20851;&#20110;CauCA&#22238;&#25910;&#22522;&#30784;&#30495;&#30456;&#30340;&#19981;&#21487;&#33021;&#32467;&#26524;&#20063;&#36866;&#29992;&#20110;CRL&#65292;&#32780;&#21487;&#33021;&#24615;&#32467;&#26524;&#21487;&#20197;&#20316;&#20026;&#25193;&#23637;CRL&#30340;&#22522;&#30784;&#12290;&#25105;&#20204;&#23558;&#20174;&#23545;&#28508;&#22312;&#22240;&#26524;&#21464;&#37327;&#23454;&#26045;&#19981;&#21516;&#31867;&#22411;&#24178;&#39044;&#30340;&#22810;&#20010;&#25968;&#25454;&#38598;&#20013;&#34920;&#24449;CauCA&#30340;&#21487;&#35782;&#21035;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Independent Component Analysis (ICA) aims to recover independent latent variables from observed mixtures thereof. Causal Representation Learning (CRL) aims instead to infer causally related (thus often statistically dependent) latent variables, together with the unknown graph encoding their causal relationships. We introduce an intermediate problem termed Causal Component Analysis (CauCA). CauCA can be viewed as a generalization of ICA, modelling the causal dependence among the latent components, and as a special case of CRL. In contrast to CRL, it presupposes knowledge of the causal graph, focusing solely on learning the unmixing function and the causal mechanisms. Any impossibility results regarding the recovery of the ground truth in CauCA also apply for CRL, while possibility results may serve as a stepping stone for extensions to CRL. We characterize CauCA identifiability from multiple datasets generated through different types of interventions on the latent causal variables. As a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#29992;&#20110;&#20174;&#24773;&#25253;&#20027;&#20307;&#30340;&#21333;&#24352;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#19968;&#36830;&#20018;&#22797;&#26434;&#30340;&#21382;&#21490;&#21644;&#26410;&#26469;&#20107;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#21482;&#26377;&#23569;&#25968;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#21040;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.17195</link><description>&lt;p&gt;
&#36890;&#36807;&#24819;&#35937;&#36807;&#21435;&#26469;&#25512;&#26029;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Inferring the Future by Imagining the Past. (arXiv:2305.17195v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17195
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#29992;&#20110;&#20174;&#24773;&#25253;&#20027;&#20307;&#30340;&#21333;&#24352;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#19968;&#36830;&#20018;&#22797;&#26434;&#30340;&#21382;&#21490;&#21644;&#26410;&#26469;&#20107;&#20214;&#65292;&#24182;&#19988;&#33021;&#22815;&#22312;&#21482;&#26377;&#23569;&#25968;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#21040;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28459;&#30011;&#20070;&#20013;&#30340;&#21333;&#19968;&#30011;&#38754;&#33021;&#22815;&#23637;&#29616;&#20154;&#29289;&#30340;&#24403;&#21069;&#29366;&#24577;&#12289;&#26469;&#33258;&#20309;&#22788;&#12289;&#21160;&#26426;&#20197;&#21450;&#21487;&#33021;&#21457;&#29983;&#30340;&#20107;&#24773;&#65292;&#36825;&#21551;&#31034;&#20102;&#25105;&#20204;&#21487;&#20197;&#20174;&#24773;&#25253;&#20027;&#20307;&#30340;&#21333;&#24352;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#19968;&#36830;&#20018;&#22797;&#26434;&#30340;&#21382;&#21490;&#21644;&#26410;&#26469;&#20107;&#20214;&#12290;&#26412;&#25991;&#22522;&#20110;&#26368;&#36817;&#30340;&#35748;&#30693;&#31185;&#23398;&#30740;&#31350;&#65292;&#25552;&#20379;&#19968;&#31181;&#33945;&#29305;&#21345;&#32599;&#31639;&#27861;&#29992;&#20110;&#36827;&#34892;&#27492;&#31867;&#25512;&#26029;&#12290;&#24314;&#31435;&#22312;&#35745;&#31639;&#26426;&#22270;&#24418;&#23398;&#20013;&#30340;&#33945;&#29305;&#21345;&#32599;&#36335;&#24452;&#36861;&#36394;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#20511;&#37492;&#20102;&#35768;&#22810;&#29702;&#24565;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#20808;&#21069;&#24037;&#20316;&#20013;&#30340;&#26679;&#26412;&#25928;&#29575;&#12290;&#36825;&#20351;&#24471;&#25105;&#20204;&#33021;&#22815;&#22312;&#21482;&#26377;&#23569;&#25968;&#26679;&#26412;&#30340;&#24773;&#20917;&#19979;&#25193;&#23637;&#21040;&#21508;&#31181;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25512;&#26029;&#38382;&#39064;&#12290;&#23427;&#20063;&#34920;&#26126;&#20102;&#19968;&#23450;&#30340;&#35748;&#30693;&#21512;&#29702;&#24615;&#65292;&#20107;&#23454;&#19978;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#22312;&#21508;&#31181;&#39046;&#22495;&#20013;&#65292;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#21305;&#37197;&#20808;&#21069;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#30340;&#20154;&#31867;&#30452;&#35273;&#30340;&#20154;&#31867;&#21463;&#35797;&#32773;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
A single panel of a comic book can say a lot: it shows not only where characters currently are, but also where they came from, what their motivations are, and what might happen next. More generally, humans can often infer a complex sequence of past and future events from a *single snapshot image* of an intelligent agent.  Building on recent work in cognitive science, we offer a Monte Carlo algorithm for making such inferences. Drawing a connection to Monte Carlo path tracing in computer graphics, we borrow ideas that help us dramatically improve upon prior work in sample efficiency. This allows us to scale to a wide variety of challenging inference problems with only a handful of samples. It also suggests some degree of cognitive plausibility, and indeed we present human subject studies showing that our algorithm matches human intuitions in a variety of domains that previous methods could not scale to.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; 3T &#26041;&#27861;&#65292;&#21363;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;3T &#21516;&#26102;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#23545;&#27604;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#23545;&#26816;&#32034;&#20219;&#21153;&#21644;&#20998;&#31867;&#38382;&#39064;&#22343;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16999</link><description>&lt;p&gt;
&#19977;&#22612;&#65306;&#21033;&#29992;&#39044;&#35757;&#32451;&#22270;&#20687;&#27169;&#22411;&#36827;&#34892;&#28789;&#27963;&#30340;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Three Towers: Flexible Contrastive Learning with Pretrained Image Models. (arXiv:2305.16999v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16999
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; 3T &#26041;&#27861;&#65292;&#21363;&#22312;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#24341;&#20837;&#39044;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#65292;&#20174;&#32780;&#25552;&#39640;&#23545;&#27604;&#23398;&#20064;&#30340;&#28789;&#27963;&#24615;&#12290;3T &#21516;&#26102;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#23545;&#27604;&#35757;&#32451;&#65292;&#24182;&#22312;&#23454;&#39564;&#35777;&#26126;&#23545;&#26816;&#32034;&#20219;&#21153;&#21644;&#20998;&#31867;&#38382;&#39064;&#22343;&#21462;&#24471;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;&#19977;&#22612;&#65288;3T&#65289;&#8221;&#30340;&#28789;&#27963;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#39044;&#20808;&#35757;&#32451;&#30340;&#22270;&#20687;&#20998;&#31867;&#22120;&#32435;&#20837;&#23545;&#27604;&#23398;&#20064;&#65292;&#25913;&#36827;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#27604;&#23398;&#20064;&#12290;&#19982;&#36890;&#24120;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#23545;&#27604;&#27169;&#22411;&#19981;&#21516;&#65292;&#26368;&#36817;&#30340; LiT&#65288;Zhai &#31561;&#20154;&#65292;2022&#65289;&#34920;&#26126;&#20102;&#20351;&#29992;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#23884;&#20837;&#30340;&#24615;&#33021;&#25552;&#21319;&#12290;&#20294;&#26159;&#65292;LiT &#30452;&#25509;&#29992;&#20923;&#32467;&#30340;&#23884;&#20837;&#26367;&#25442;&#22270;&#20687;&#22612;&#65292;&#25490;&#38500;&#20102;&#23545;&#22270;&#20687;&#22612;&#36827;&#34892;&#23545;&#27604;&#35757;&#32451;&#30340;&#20219;&#20309;&#28508;&#22312;&#22909;&#22788;&#12290;&#36890;&#36807; 3T&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#28789;&#27963;&#30340;&#31574;&#30053;&#65292;&#20801;&#35768;&#22270;&#20687;&#22612;&#21516;&#26102;&#21463;&#30410;&#20110;&#39044;&#35757;&#32451;&#23884;&#20837;&#21644;&#23545;&#27604;&#35757;&#32451;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31532;&#19977;&#20010;&#22612;&#65292;&#20854;&#20013;&#21253;&#21547;&#20923;&#32467;&#30340;&#39044;&#35757;&#32451;&#23884;&#20837;&#65292;&#24182;&#40723;&#21169;&#35813;&#31532;&#19977;&#20010;&#22612;&#19982;&#20027;&#35201;&#30340;&#22270;&#20687;-&#25991;&#26412;&#22612;&#20043;&#38388;&#30340;&#23545;&#40784;&#12290;&#22312;&#23454;&#39564;&#35777;&#26126;&#65292;3T &#22312;&#26816;&#32034;&#20219;&#21153;&#19978;&#22987;&#32456;&#20248;&#20110; LiT &#21644; CLIP &#39118;&#26684;&#30340;&#20174;&#22836;&#24320;&#22987;&#23545;&#27604;&#23398;&#20064;&#22522;&#32447;&#12290;&#23545;&#20110;&#20998;&#31867;&#38382;&#39064;&#65292;3T &#22312;&#20174;&#22836;&#24320;&#22987;&#22522;&#32447;&#30340;&#22522;&#30784;&#19978;&#21487;&#38752;&#22320;&#25913;&#21892;&#65292;&#34429;&#28982;&#22312;&#26576;&#20123;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#21450; LiT&#65292;&#20294;&#20173;&#28982;&#23454;&#29616;&#20102;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#26412;&#26041;&#27861;&#20984;&#26174;&#20102;&#23558;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#27880;&#20837;&#21040;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#28789;&#27963;&#30340;&#21033;&#29992;&#23427;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce Three Towers (3T), a flexible method to improve the contrastive learning of vision-language models by incorporating pretrained image classifiers. While contrastive models are usually trained from scratch, LiT (Zhai et al., 2022) has recently shown performance gains from using pretrained classifier embeddings. However, LiT directly replaces the image tower with the frozen embeddings, excluding any potential benefits of contrastively training the image tower. With 3T, we propose a more flexible strategy that allows the image tower to benefit from both pretrained embeddings and contrastive training. To achieve this, we introduce a third tower that contains the frozen pretrained embeddings, and we encourage alignment between this third tower and the main image-text towers. Empirically, 3T consistently improves over LiT and the CLIP-style from-scratch baseline for retrieval tasks. For classification, 3T reliably improves over the from-scratch baseline, and while it underperform
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#24182;&#22312;&#23545;&#40784;&#22522;&#20934;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.16960</link><description>&lt;p&gt;
&#22312;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#35757;&#32451;&#31038;&#20250;&#23545;&#40784;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Training Socially Aligned Language Models in Simulated Human Society. (arXiv:2305.16960v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16960
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#27169;&#25311;&#20154;&#31867;&#31038;&#20250;&#20013;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#30456;&#27604;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#24182;&#22312;&#23545;&#40784;&#22522;&#20934;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#20013;&#30340;&#31038;&#20250;&#23545;&#40784;&#26088;&#22312;&#30830;&#20445;&#36825;&#20123;&#27169;&#22411;&#25353;&#29031;&#26082;&#23450;&#30340;&#31038;&#20250;&#20215;&#20540;&#34892;&#20107;&#12290;&#28982;&#32780;&#65292;&#19982;&#20154;&#31867;&#19981;&#21516;&#65292;&#20154;&#20204;&#36890;&#36807;&#31038;&#20132;&#20114;&#21160;&#24471;&#20986;&#23545;&#20215;&#20540;&#21028;&#26029;&#30340;&#20849;&#35782;&#65292;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#21017;&#22312;&#23396;&#31435;&#22320;&#22797;&#21046;&#20854;&#35757;&#32451;&#35821;&#26009;&#24211;&#26102;&#34987;&#35757;&#32451;&#20986;&#26469;&#65292;&#23548;&#33268;&#22312;&#38476;&#29983;&#22330;&#26223;&#20013;&#34920;&#29616;&#19981;&#20339;&#65292;&#24182;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#35757;&#32451;&#33539;&#24335;&#65292;&#20801;&#35768;LMs&#20174;&#27169;&#25311;&#30340;&#31038;&#20132;&#20114;&#21160;&#20013;&#23398;&#20064;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#26356;&#22823;&#30340;&#21487;&#25193;&#23637;&#24615;&#21644;&#39640;&#25928;&#24615;&#65292;&#22312;&#23545;&#40784;&#22522;&#20934;&#21644;&#20154;&#31867;&#35780;&#20272;&#20013;&#23637;&#31034;&#20986;&#26356;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;&#36825;&#31181;LMs&#35757;&#32451;&#20013;&#30340;&#33539;&#24335;&#36716;&#21464;&#20351;&#25105;&#20204;&#31163;&#24320;&#21457;&#33021;&#22815;&#24378;&#26377;&#21147;&#19988;&#20934;&#30830;&#21453;&#26144;&#31038;&#20250;&#35268;&#33539;&#21644;&#20215;&#20540;&#30340;AI&#31995;&#32479;&#26356;&#36817;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
Social alignment in AI systems aims to ensure that these models behave according to established societal values. However, unlike humans, who derive consensus on value judgments through social interaction, current language models (LMs) are trained to rigidly replicate their training corpus in isolation, leading to subpar generalization in unfamiliar scenarios and vulnerability to adversarial attacks. This work presents a novel training paradigm that permits LMs to learn from simulated social interactions. In comparison to existing methodologies, our approach is considerably more scalable and efficient, demonstrating superior performance in alignment benchmarks and human evaluations. This paradigm shift in the training of LMs brings us a step closer to developing AI systems that can robustly and accurately reflect societal norms and values.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.16380</link><description>&lt;p&gt;
&#25195;&#25551;&#19982;&#25293;&#29031;&#65306;&#29702;&#35299;1&#23618;Transformer&#20013;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26631;&#35760;&#32452;&#25104;
&lt;/p&gt;
&lt;p&gt;
Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;1&#23618;Transformer&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#65292;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65292;&#20174;&#32780;&#36880;&#27493;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#24182;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#24635;&#32467;&#30456;&#20851;&#20449;&#24687;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#21516;&#26102;&#30740;&#31350;&#20102;&#26631;&#35760;&#39057;&#29575;&#12289;&#19978;&#19979;&#25991;&#21644;&#21021;&#22987;&#21270;&#33258;&#25105;&#20851;&#27880;&#23618;&#31561;&#23545;Transformer&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#22810;&#20010;&#30740;&#31350;&#39046;&#22495;&#34920;&#29616;&#20986;&#20102;&#24778;&#20154;&#30340;&#24615;&#33021;&#65292;&#24182;&#25104;&#20026;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22522;&#30784;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#23545;&#20854;&#22914;&#20309;&#24037;&#20316;&#30340;&#29702;&#35299;&#20173;&#28982;&#26377;&#38480;&#12290;&#29305;&#21035;&#26159;&#65292;&#36890;&#36807;&#31616;&#21333;&#30340;&#39044;&#27979;&#24615;&#25439;&#22833;&#65292;&#34920;&#31034;&#22914;&#20309;&#20174;&#26799;&#24230;&#35757;&#32451;&#21160;&#24577;&#20013;&#20986;&#29616;&#20173;&#28982;&#26159;&#19968;&#20010;&#35868;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#38024;&#23545;&#20855;&#26377;&#19968;&#20010;&#33258;&#25105;&#20851;&#27880;&#23618;&#21644;&#19968;&#20010;&#35299;&#30721;&#22120;&#23618;&#30340;1&#23618;Transformer&#65292;&#25105;&#20204;&#20197;&#25968;&#23398;&#20005;&#35880;&#30340;&#26041;&#24335;&#20998;&#26512;&#20854;&#22312;&#19979;&#19968;&#20010;&#26631;&#35760;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;SGD&#35757;&#32451;&#21160;&#24577;&#12290;&#25105;&#20204;&#25171;&#24320;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#32452;&#21512;&#36755;&#20837;&#26631;&#35760;&#30340;&#21160;&#24577;&#36807;&#31243;&#30340;&#40657;&#30418;&#23376;&#65292;&#24182;&#25581;&#31034;&#20102;&#24213;&#23618;&#24402;&#32435;&#20559;&#24046;&#30340;&#26412;&#36136;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27809;&#26377;&#20301;&#32622;&#32534;&#30721;&#12289;&#38271;&#36755;&#20837;&#24207;&#21015;&#21644;&#35299;&#30721;&#22120;&#23618;&#23398;&#20064;&#36895;&#24230;&#24555;&#20110;&#33258;&#25105;&#20851;&#27880;&#23618;&#30340;&#20551;&#35774;&#19979;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#33258;&#25105;&#20851;&#27880;&#23618;&#20805;&#24403;&#20102;&#8220;&#21306;&#20998;&#24615;&#25195;&#25551;&#31639;&#27861;&#8221;&#65306;&#20174;&#22343;&#21248;&#27880;&#24847;&#21147;&#24320;&#22987;&#65292;&#23427;&#36880;&#28176;&#20851;&#27880;&#21040;&#30456;&#20851;&#26631;&#35760;&#65292;&#25490;&#38500;&#19981;&#30456;&#20851;&#30340;&#26631;&#35760;&#65292;&#30452;&#21040;&#25152;&#26377;&#30456;&#20851;&#20449;&#24687;&#34987;&#25195;&#25551;&#24182;&#24635;&#32467;&#22312;&#32534;&#30721;&#34920;&#31034;&#20013;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#36824;&#26174;&#31034;&#20102;&#26631;&#35760;&#39057;&#29575;&#21644;&#19978;&#19979;&#25991;&#22914;&#20309;&#24433;&#21709;&#27880;&#24847;&#26435;&#37325;&#65292;&#20197;&#21450;&#33258;&#25105;&#20851;&#27880;&#23618;&#21021;&#22987;&#21270;&#22914;&#20309;&#24433;&#21709;&#25910;&#25947;&#36895;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer architecture has shown impressive performance in multiple research domains and has become the backbone of many neural network models. However, there is limited understanding on how it works. In particular, with a simple predictive loss, how the representation emerges from the gradient \emph{training dynamics} remains a mystery. In this paper, for 1-layer transformer with one self-attention layer plus one decoder layer, we analyze its SGD training dynamics for the task of next token prediction in a mathematically rigorous manner. We open the black box of the dynamic process of how the self-attention layer combines input tokens, and reveal the nature of underlying inductive bias. More specifically, with the assumption (a) no positional encoding, (b) long input sequence, and (c) the decoder layer learns faster than the self-attention layer, we prove that self-attention acts as a \emph{discriminative scanning algorithm}: starting from uniform attention, it gradually attends mor
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#33258;&#21160;&#35821;&#35328;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#25193;&#20805;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#20013;&#38754;&#23545;&#39046;&#22495;&#21464;&#21270;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.16289</link><description>&lt;p&gt;
&#29992;&#33258;&#21160;&#25193;&#20805;&#22522;&#20110;&#25193;&#25955;&#30340;&#26041;&#27861;&#20016;&#23500;&#35270;&#35273;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Diversify Your Vision Datasets with Automatic Diffusion-Based Augmentation. (arXiv:2305.16289v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.16289
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#33258;&#21160;&#35821;&#35328;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#65292;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#25193;&#20805;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#20013;&#38754;&#23545;&#39046;&#22495;&#21464;&#21270;&#30340;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32454;&#31890;&#24230;&#20998;&#31867;&#20219;&#21153;&#65292;&#22914;&#32597;&#35265;&#21160;&#29289;&#35782;&#21035;&#65292;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#65292;&#22240;&#27492;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#20998;&#31867;&#22120;&#36890;&#24120;&#26080;&#27861;&#25512;&#24191;&#21040;&#39046;&#22495;&#20013;&#30340;&#21464;&#21270;&#65292;&#22914;&#22825;&#27668;&#25110;&#20301;&#32622;&#30340;&#21464;&#21270;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#22914;&#20309;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#20013;&#25152;&#35265;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#19982;&#22312;&#22810;&#26679;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#22823;&#22411;&#35270;&#35273;&#27169;&#22411;&#32467;&#21512;&#65292;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#26377;&#29992;&#21464;&#21270;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;ALIA&#65288;&#33258;&#21160;&#35821;&#35328;&#24341;&#23548;&#22270;&#20687;&#22686;&#24378;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#29983;&#25104;&#25968;&#25454;&#38598;&#39046;&#22495;&#30340;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#65292;&#24182;&#36890;&#36807;&#35821;&#35328;&#24341;&#23548;&#22270;&#20687;&#32534;&#36753;&#26469;&#25193;&#20805;&#35757;&#32451;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#20445;&#25345;&#25968;&#25454;&#30340;&#23436;&#25972;&#24615;&#65292;&#21407;&#22987;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#20250;&#28388;&#38500;&#26368;&#23567;&#30340;&#22270;&#20687;&#32534;&#36753;&#21644;&#37027;&#20123;&#25439;&#22351;&#31867;&#30456;&#20851;&#20449;&#24687;&#30340;&#32534;&#36753;&#12290;&#26368;&#32456;&#30340;&#25968;&#25454;&#38598;&#22312;&#35270;&#35273;&#19978;&#19982;&#21407;&#22987;&#35757;&#32451;&#25968;&#25454;&#19968;&#33268;&#65292;&#24182;&#25552;&#20379;&#20102;&#26174;&#33879;&#22686;&#24378;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many fine-grained classification tasks, like rare animal identification, have limited training data and consequently classifiers trained on these datasets often fail to generalize to variations in the domain like changes in weather or location. As such, we explore how natural language descriptions of the domains seen in training data can be used with large vision models trained on diverse pretraining datasets to generate useful variations of the training data. We introduce ALIA (Automated Language-guided Image Augmentation), a method which utilizes large vision and language models to automatically generate natural language descriptions of a dataset's domains and augment the training data via language-guided image editing. To maintain data integrity, a model trained on the original dataset filters out minimal image edits and those which corrupt class-relevant information. The resulting dataset is visually consistent with the original training data and offers significantly enhanced diver
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#31934;&#30830;MLE&#23398;&#20064;&#12289;&#26377;&#25928;&#25277;&#26679;&#26032;&#30340;&#19977;&#20803;&#32452;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#65292;&#33719;&#24471;&#20102;&#27604;&#21407;&#22987;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15944</link><description>&lt;p&gt;
&#22914;&#20309;&#36890;&#36807;&#27010;&#29575;&#30005;&#36335;&#23558;&#24744;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
How to Turn Your Knowledge Graph Embeddings into Generative Models via Probabilistic Circuits. (arXiv:2305.15944v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15944
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#23558;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#36716;&#21270;&#20026;&#29983;&#25104;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#20801;&#35768;&#31934;&#30830;MLE&#23398;&#20064;&#12289;&#26377;&#25928;&#25277;&#26679;&#26032;&#30340;&#19977;&#20803;&#32452;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#65292;&#33719;&#24471;&#20102;&#27604;&#21407;&#22987;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#20123;&#25104;&#21151;&#30340;&#30693;&#35782;&#22270;&#23884;&#20837;&#27169;&#22411;&#21487;&#29992;&#20316;&#22522;&#20110;&#33021;&#37327;&#30340;&#27169;&#22411;&#65292;&#32780;&#36825;&#31687;&#35770;&#25991;&#35299;&#37322;&#20102;&#36825;&#20123;&#27169;&#22411;&#30340;&#24471;&#20998;&#20989;&#25968;&#65292;&#23558;&#20854;&#37325;&#26032;&#35299;&#37322;&#25104;&#20026;&#30005;&#36335;&#24418;&#24335;--&#36825;&#26159;&#19968;&#31181;&#20801;&#35768;&#26377;&#25928;&#36793;&#38469;&#21270;&#30340;&#32422;&#26463;&#35745;&#31639;&#22270;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#26041;&#27861;&#26469;&#33719;&#24471;&#26377;&#25928;&#30340;&#29983;&#25104;&#30005;&#36335;&#27169;&#22411;&#65292;&#20854;&#20013;&#19968;&#20010;&#26041;&#27861;&#26159;&#23558;&#20854;&#28608;&#27963;&#38480;&#21046;&#20026;&#38750;&#36127;&#25968;&#65292;&#21478;&#19968;&#20010;&#26041;&#27861;&#26159;&#23558;&#20854;&#36755;&#20986;&#24179;&#26041;&#12290;&#25105;&#20204;&#30340;&#35299;&#37322;&#19981;&#20250;&#24433;&#21709;&#21040;&#39044;&#27979;&#33410;&#28857;&#36830;&#36793;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#20294;&#30005;&#36335;&#26694;&#26550;&#20351;&#24471;MLE&#30340;&#31934;&#30830;&#23398;&#20064;&#12289;&#26032;&#19977;&#20803;&#32452;&#30340;&#26377;&#25928;&#25277;&#26679;&#20197;&#21450;&#20445;&#35777;&#36923;&#36753;&#32422;&#26463;&#24471;&#20197;&#28385;&#36275;&#25104;&#20026;&#21487;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#25317;&#26377;&#25968;&#30334;&#19975;&#20010;&#23454;&#20307;&#30340;&#22270;&#19978;&#27604;&#21407;&#22987;&#30340;KGEs&#26356;&#20855;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Some of the most successful knowledge graph embedding (KGE) models for link prediction -- CP, RESCAL, TuckER, ComplEx -- can be interpreted as energy-based models. Under this perspective they are not amenable for exact maximum-likelihood estimation (MLE), sampling and struggle to integrate logical constraints. This work re-interprets the score functions of these KGEs as circuits -- constrained computational graphs allowing efficient marginalisation. Then, we design two recipes to obtain efficient generative circuit models by either restricting their activations to be non-negative or squaring their outputs. Our interpretation comes with little or no loss of performance for link prediction, while the circuits framework unlocks exact learning by MLE, efficient sampling of new triples, and guarantee that logical constraints are satisfied by design. Furthermore, our models scale more gracefully than the original KGEs on graphs with millions of entities.
&lt;/p&gt;</description></item><item><title>LayoutGPT&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#35268;&#21010;&#21644;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#26465;&#20214;&#29983;&#25104;&#24067;&#23616;&#26469;&#25552;&#39640;&#29992;&#25143;&#23545;&#29983;&#25104;&#32467;&#26524;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;LayoutGPT&#22312;&#22810;&#20010;&#39046;&#22495;&#29983;&#25104;&#21512;&#29702;&#30340;&#24067;&#23616;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;/&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.15393</link><description>&lt;p&gt;
LayoutGPT: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#32452;&#21512;&#24335;&#35270;&#35273;&#35268;&#21010;&#21644;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
LayoutGPT: Compositional Visual Planning and Generation with Large Language Models. (arXiv:2305.15393v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15393
&lt;/p&gt;
&lt;p&gt;
LayoutGPT&#26159;&#19968;&#31181;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35273;&#35268;&#21010;&#21644;&#29983;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20174;&#25991;&#26412;&#26465;&#20214;&#29983;&#25104;&#24067;&#23616;&#26469;&#25552;&#39640;&#29992;&#25143;&#23545;&#29983;&#25104;&#32467;&#26524;&#30340;&#25511;&#21046;&#33021;&#21147;&#12290;LayoutGPT&#22312;&#22810;&#20010;&#39046;&#22495;&#29983;&#25104;&#21512;&#29702;&#30340;&#24067;&#23616;&#65292;&#24182;&#20248;&#20110;&#20854;&#20182;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;/&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35270;&#35273;&#29983;&#25104;&#20013;&#65292;&#23454;&#29616;&#39640;&#24230;&#30340;&#29992;&#25143;&#21487;&#25511;&#24615;&#36890;&#24120;&#38656;&#35201;&#31934;&#32454;&#30340;&#36755;&#20837;&#65292;&#22914;&#24067;&#23616;&#12290;&#28982;&#32780;&#65292;&#19982;&#31616;&#21333;&#30340;&#25991;&#26412;&#36755;&#20837;&#30456;&#27604;&#65292;&#36825;&#26679;&#30340;&#36755;&#20837;&#32473;&#29992;&#25143;&#24102;&#26469;&#20102;&#24456;&#22823;&#30340;&#36127;&#25285;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;&#20309;&#36890;&#36807;&#20174;&#25991;&#26412;&#26465;&#20214;&#20013;&#29983;&#25104;&#24067;&#23616;&#26469;&#20805;&#24403;&#35270;&#35273;&#35268;&#21010;&#32773;&#65292;&#24182;&#19982;&#35270;&#35273;&#29983;&#25104;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LayoutGPT&#65292;&#19968;&#31181;&#20351;&#29992;&#26679;&#24335;&#34920;&#35821;&#35328;&#26469;&#32452;&#25104;&#19978;&#19979;&#25991;&#20013;&#30340;&#35270;&#35273;&#31034;&#20363;&#65292;&#20197;&#22686;&#24378;LLMs&#30340;&#35270;&#35273;&#35268;&#21010;&#33021;&#21147;&#30340;&#26041;&#27861;&#12290;LayoutGPT&#21487;&#20197;&#22312;&#22810;&#20010;&#39046;&#22495;&#29983;&#25104;&#21512;&#29702;&#30340;&#24067;&#23616;&#65292;&#20174;2D&#22270;&#20687;&#21040;3D&#23460;&#20869;&#22330;&#26223;&#12290;LayoutGPT&#36824;&#22312;&#23558;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35821;&#35328;&#27010;&#24565;&#65288;&#22914;&#25968;&#23383;&#21644;&#31354;&#38388;&#20851;&#31995;&#65289;&#36716;&#21270;&#20026;&#24067;&#23616;&#23433;&#25490;&#20197;&#36827;&#34892;&#24544;&#23454;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;&#24403;&#19982;&#19979;&#28216;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#32467;&#21512;&#26102;&#65292;LayoutGPT&#30340;&#24615;&#33021;&#20248;&#20110;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;/&#31995;&#32479;20-40%&#65292;&#24182;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
Attaining a high degree of user controllability in visual generation often requires intricate, fine-grained inputs like layouts. However, such inputs impose a substantial burden on users when compared to simple text inputs. To address the issue, we study how Large Language Models (LLMs) can serve as visual planners by generating layouts from text conditions, and thus collaborate with visual generative models. We propose LayoutGPT, a method to compose in-context visual demonstrations in style sheet language to enhance the visual planning skills of LLMs. LayoutGPT can generate plausible layouts in multiple domains, ranging from 2D images to 3D indoor scenes. LayoutGPT also shows superior performance in converting challenging language concepts like numerical and spatial relations to layout arrangements for faithful text-to-image generation. When combined with a downstream image generation model, LayoutGPT outperforms text-to-image models/systems by 20-40% and achieves comparable performan
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;OPC UA&#30340;&#25216;&#26415;&#27010;&#36848;&#21644;&#32508;&#36848;&#65292;&#26469;&#35299;&#20915;&#23558;&#24378;&#21270;&#23398;&#20064;&#26080;&#32541;&#38598;&#25104;&#21040;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.15113</link><description>&lt;p&gt;
&#20851;&#20110;&#21033;&#29992;OPC UA&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#30340;&#36855;&#20320;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Mini Review on the utilization of Reinforcement Learning with OPC UA. (arXiv:2305.15113v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.15113
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#26088;&#22312;&#36890;&#36807;&#25552;&#20379;&#20851;&#20110;&#24378;&#21270;&#23398;&#20064;&#21644;OPC UA&#30340;&#25216;&#26415;&#27010;&#36848;&#21644;&#32508;&#36848;&#65292;&#26469;&#35299;&#20915;&#23558;&#24378;&#21270;&#23398;&#20064;&#26080;&#32541;&#38598;&#25104;&#21040;&#24037;&#19994;&#31995;&#32479;&#20013;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#26159;&#19968;&#31181;&#24378;&#22823;&#30340;&#26426;&#22120;&#23398;&#20064;&#33539; paradigm&#65292;&#24050;&#24212;&#29992;&#20110;&#26426;&#22120;&#20154;&#25216;&#26415;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#28216;&#25103;&#20013;&#65292;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#25104;&#26524;&#12290;&#38024;&#23545;&#35299;&#20915;&#39034;&#24207;&#20915;&#31574;&#38382;&#39064;&#65292;&#23427;&#36890;&#36807;&#35774;&#35745;&#21487;&#20197;&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;&#65292;&#22240;&#27492;&#36866;&#24212;&#19981;&#26029;&#21464;&#21270;&#30340;&#21160;&#24577;&#29615;&#22659;&#12290;&#36825;&#20123;&#33021;&#21147;&#20351;&#20854;&#25104;&#20026;&#25511;&#21046;&#21644;&#20248;&#21270;&#24037;&#19994;&#22797;&#26434;&#36807;&#31243;&#30340;&#29702;&#24819;&#36873;&#25321;&#12290;&#20805;&#20998;&#21033;&#29992;&#36825;&#19968;&#28508;&#21147;&#30340;&#20851;&#38190;&#26159;&#23558;RL&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#24037;&#19994;&#31995;&#32479;&#20013;&#12290;&#24037;&#19994;&#36890;&#20449;&#26631;&#20934;Open Platform Communications Unified Architecture&#65288;OPC UA&#65289;&#21487;&#33021;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;RL&#21644;OPC UA&#26469;&#33258;&#19981;&#21516;&#30340;&#39046;&#22495;&#65292;&#38656;&#35201;&#30740;&#31350;&#20154;&#21592;&#26469;&#24357;&#21512;&#20004;&#31181;&#25216;&#26415;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#25552;&#20379;&#31616;&#35201;&#30340;&#25216;&#26415;&#27010;&#36848;&#24182;&#36827;&#34892;&#21322;&#35814;&#23613;&#30340;&#25991;&#29486;&#32508;&#36848;&#26469;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#20197;&#33719;&#24471;&#28145;&#20837;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning (RL) is a powerful machine learning paradigm that has been applied in various fields such as robotics, natural language processing and game playing achieving state-of-the-art results. Targeted to solve sequential decision making problems, it is by design able to learn from experience and therefore adapt to changing dynamic environments. These capabilities make it a prime candidate for controlling and optimizing complex processes in industry. The key to fully exploiting this potential is the seamless integration of RL into existing industrial systems. The industrial communication standard Open Platform Communications UnifiedArchitecture (OPC UA) could bridge this gap. However, since RL and OPC UA are from different fields,there is a need for researchers to bridge the gap between the two technologies. This work serves to bridge this gap by providing a brief technical overview of both technologies and carrying out a semi-exhaustive literature review to gain insights
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;JTFT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25552;&#21462;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#20943;&#23567;&#35745;&#31639;&#38656;&#27714;&#65292;&#36866;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20302;&#31209;&#27880;&#24847;&#23618;&#20197;&#26377;&#25928;&#25429;&#33719;&#36328;&#32500;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.14649</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#32852;&#21512;&#26102;&#39057;&#22495;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
A Joint Time-frequency Domain Transformer for Multivariate Time Series Forecasting. (arXiv:2305.14649v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14649
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;JTFT&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#25552;&#21462;&#26102;&#38388;&#20381;&#36182;&#24615;&#30340;&#21516;&#26102;&#26377;&#25928;&#22320;&#20943;&#23567;&#35745;&#31639;&#38656;&#27714;&#65292;&#36866;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#24182;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#20302;&#31209;&#27880;&#24847;&#23618;&#20197;&#26377;&#25928;&#25429;&#33719;&#36328;&#32500;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#22312;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31216;&#20026;&#32852;&#21512;&#26102;&#39057;&#22495;&#21464;&#25442;&#22120;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#21516;&#26102;&#26368;&#23567;&#21270;&#35745;&#31639;&#38656;&#27714;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#39057;&#29575;&#30340;&#31232;&#30095;&#24615;&#65292;&#22312;&#39057;&#22495;&#26377;&#25928;&#22320;&#25552;&#21462;&#26102;&#38388;&#20381;&#36182;&#24615;&#12290;&#38500;&#20102;&#39057;&#29575;&#22495;&#34920;&#31034;&#22806;&#65292;&#26368;&#36817;&#30340;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#28857;&#20063;&#34987;&#30452;&#25509;&#32534;&#30721;&#22312;&#26102;&#38388;&#22495;&#20013;&#65292;&#20197;&#22686;&#24378;&#23398;&#20064;&#23616;&#37096;&#20851;&#31995;&#24182;&#20943;&#36731;&#38750;&#24179;&#31283;&#24615;&#30340;&#19981;&#21033;&#24433;&#21709;&#12290;JTFT&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#22240;&#20026;&#20869;&#37096;&#34920;&#31034;&#30340;&#38271;&#24230;&#20445;&#25345;&#29420;&#31435;&#20110;&#36755;&#20837;&#24207;&#21015;&#38271;&#24230;&#12290;&#27492;&#22806;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20302;&#31209;&#27880;&#24847;&#23618;&#65292;&#20197;&#26377;&#25928;&#25429;&#33719;&#36328;&#32500;&#24230;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#24182;&#38450;&#27490;&#30001;&#20110;&#26102;&#38388;&#21644;&#36890;&#36947;&#24314;&#27169;&#30340;&#32416;&#32544;&#32780;&#23548;&#33268;&#24615;&#33021;&#38477;&#32423;&#12290; &#23545;&#20845;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#36827;&#34892;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;JTFT&#20248;&#20110;&#29616;&#26377;&#30340;&#29366;&#24577;-of-the-art&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
To enhance predicting performance while minimizing computational demands, this paper introduces a joint time-frequency domain Transformer (JTFT) for multivariate forecasting. The method exploits the sparsity of time series in the frequency domain using a small number of learnable frequencies to extract temporal dependencies effectively. Alongside the frequency domain representation, a fixed number of the most recent data points are directly encoded in the time domain, bolstering the learning of local relationships and mitigating the adverse effects of non-stationarity. JTFT achieves linear complexity since the length of the internal representation remains independent of the input sequence length. Additionally, a low-rank attention layer is proposed to efficiently capture cross-dimensional dependencies and prevent performance degradation due to the entanglement of temporal and channel-wise modeling. Experiments conducted on six real-world datasets demonstrate that JTFT outperforms state
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;EPNS&#30340;&#31561;&#21464;&#27010;&#29575;&#31070;&#32463;&#27169;&#25311;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#31995;&#32479;&#28436;&#21270;&#20013;&#29983;&#25104;&#31561;&#21464;&#20998;&#24067;&#65292;&#24182;&#22312;&#38543;&#26426;&#26102;&#31354;&#21160;&#24577;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>http://arxiv.org/abs/2305.14286</link><description>&lt;p&gt;
&#31561;&#21464;&#31070;&#32463;&#27169;&#25311;&#22120;&#29992;&#20110;&#38543;&#26426;&#26102;&#31354;&#21160;&#24577;
&lt;/p&gt;
&lt;p&gt;
Equivariant Neural Simulators for Stochastic Spatiotemporal Dynamics. (arXiv:2305.14286v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14286
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;EPNS&#30340;&#31561;&#21464;&#27010;&#29575;&#31070;&#32463;&#27169;&#25311;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#31995;&#32479;&#28436;&#21270;&#20013;&#29983;&#25104;&#31561;&#21464;&#20998;&#24067;&#65292;&#24182;&#22312;&#38543;&#26426;&#26102;&#31354;&#21160;&#24577;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#27491;&#22312;&#25104;&#20026;&#21487;&#25193;&#23637;&#30340;&#25968;&#25454;&#39537;&#21160;&#39640;&#32500;&#21160;&#24577;&#31995;&#32479;&#27169;&#25311;&#24037;&#20855;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#20540;&#26041;&#27861;&#19981;&#21487;&#34892;&#25110;&#35745;&#31639;&#26114;&#36149;&#30340;&#24773;&#20917;&#19979;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24050;&#32463;&#35777;&#26126;&#22312;&#30830;&#23450;&#24615;&#31070;&#32463;&#27169;&#25311;&#22120;&#20013;&#24341;&#20837;&#22495;&#23545;&#31216;&#24615;&#21487;&#20197;&#22823;&#22823;&#25552;&#39640;&#20854;&#31934;&#30830;&#24615;&#12289;&#26679;&#26412;&#25928;&#29575;&#21644;&#21442;&#25968;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23558;&#23545;&#31216;&#24615;&#32435;&#20837;&#21487;&#20197;&#27169;&#25311;&#38543;&#26426;&#29616;&#35937;&#30340;&#27010;&#29575;&#31070;&#32463;&#27169;&#25311;&#22120;&#20013;&#65292;&#25105;&#20204;&#38656;&#35201;&#19968;&#20010;&#33021;&#22815;&#29983;&#25104;&#31561;&#21464;&#36712;&#36857;&#20998;&#24067;&#32780;&#19981;&#26159;&#31561;&#21464;&#20989;&#25968;&#36924;&#36817;&#30340;&#27169;&#22411;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31561;&#21464;&#27010;&#29575;&#31070;&#32463;&#27169;&#25311;&#65288;EPNS&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#31561;&#21464;&#20998;&#24067;&#31995;&#32479;&#28436;&#21270;&#30340;&#33258;&#22238;&#24402;&#27010;&#29575;&#24314;&#27169;&#26694;&#26550;&#12290;&#25105;&#20204;&#20351;&#29992;EPNS&#35774;&#35745;&#20102;&#19968;&#20010;&#29992;&#20110;&#38543;&#26426;N&#20307;&#31995;&#32479;&#21644;&#38543;&#26426;&#32454;&#32990;&#21160;&#21147;&#23398;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;EPNS&#22312;p&#26041;&#38754;&#27604;&#29616;&#26377;&#30340;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks are emerging as a tool for scalable data-driven simulation of high-dimensional dynamical systems, especially in settings where numerical methods are infeasible or computationally expensive. Notably, it has been shown that incorporating domain symmetries in deterministic neural simulators can substantially improve their accuracy, sample efficiency, and parameter efficiency. However, to incorporate symmetries in probabilistic neural simulators that can simulate stochastic phenomena, we need a model that produces equivariant distributions over trajectories, rather than equivariant function approximations. In this paper, we propose Equivariant Probabilistic Neural Simulation (EPNS), a framework for autoregressive probabilistic modeling of equivariant distributions over system evolutions. We use EPNS to design models for a stochastic n-body system and stochastic cellular dynamics. Our results show that EPNS considerably outperforms existing neural network-based methods for p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35266;&#23519;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.14257</link><description>&lt;p&gt;
&#20998;&#23618;&#25552;&#31034;&#25552;&#21319;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Prompting Assists Large Language Model on Web Navigation. (arXiv:2305.14257v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14257
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#26469;&#35299;&#20915;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#22312;&#22788;&#29702;&#22797;&#26434;&#35266;&#23519;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22256;&#38590;&#12290;&#30740;&#31350;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#32593;&#32476;&#23548;&#33322;&#20013;&#30340;&#25928;&#26524;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;&#65292;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#22788;&#29702;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#22797;&#26434;&#35266;&#23519;&#26102;&#36935;&#21040;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#20998;&#23618;&#25552;&#31034;&#26041;&#27861;&#12290;&#19981;&#21516;&#20110;&#20197;&#24448;&#24635;&#26159;&#25226;\emph{&#23436;&#25972;}&#35266;&#23519;&#65288;&#20363;&#22914;&#32593;&#39029;&#65289;&#25918;&#21040;&#25552;&#31034;&#20013;&#30340;&#25552;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#39318;&#20808;&#26500;&#24314;&#19968;&#20010;&#19982;&#21160;&#20316;&#30456;&#20851;&#30340;\emph{&#21387;&#32553;}&#21644;\emph{&#30456;&#20851;}&#30340;&#35266;&#23519;&#65292;&#24182;&#20351;&#29992;&#19987;&#38376;&#30340;\summ&#25552;&#31034;&#12290;&#28982;&#21518;&#65292;\actor&#25552;&#31034;&#26681;&#25454;&#24635;&#32467;&#30340;&#35266;&#23519;&#39044;&#27979;&#19979;&#19968;&#20010;&#21160;&#20316;&#12290;&#23613;&#31649;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#24191;&#27867;&#30340;&#36866;&#29992;&#24615;&#65292;&#20294;&#25105;&#20204;&#23588;&#20854;&#23637;&#31034;&#20102;&#23427;&#22312;&#22797;&#26434;&#30340;&#32593;&#32476;&#23548;&#33322;&#39046;&#22495;&#30340;&#26377;&#25928;&#24615;&#65292;&#20854;&#20013;&#23436;&#25972;&#30340;&#35266;&#23519;&#36890;&#24120;&#21253;&#21547;&#20887;&#20313;&#21644;&#26080;&#20851;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20219;&#21153;&#25104;&#21151;&#29575;&#19978;&#20248;&#20110;&#20808;&#21069;&#26368;&#20808;&#36827;&#30340;&#25552;&#31034;&#26426;&#21046;6.2\%&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#20855;&#26377;&#38271;&#26102;&#38388;&#35266;&#23519;&#36712;&#36857;&#30340;&#20132;&#20114;&#20915;&#31574;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) struggle on processing complicated observations in interactive decision making tasks. To alleviate this issue, we propose a simple hierarchical prompting approach. Diverging from previous prompting approaches that always put the \emph{full} observation~(\eg a web page) to the prompt, we propose to first construct an action-aware observation which is more \emph{condensed} and \emph{relevant} with a dedicated \summ prompt. The \actor prompt then predicts the next action based on the summarized observation. While our method has broad applicability, we particularly demonstrate its efficacy in the complex domain of web navigation where a full observation often contains redundant and irrelevant information. Our approach outperforms the previous state-of-the-art prompting mechanis by 6.2\% on task success rate, demonstrating its potential on interactive decision making tasks with long observation traces.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REFLEX&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#28155;&#21152;&#20102;&#21512;&#29702;&#24615;&#21644;&#33258;&#21453;&#24615;&#23618;&#65292;&#20197;&#20351;&#27169;&#22411;&#30340;&#31572;&#26696;&#24471;&#20197;&#35299;&#37322;&#24182;&#28040;&#38500;&#28508;&#22312;&#30340;&#30683;&#30462;&#12290;</title><link>http://arxiv.org/abs/2305.14250</link><description>&lt;p&gt;
&#20855;&#26377;&#21512;&#29702;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language Models with Rationality. (arXiv:2305.14250v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14250
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;REFLEX&#30340;&#26041;&#27861;&#65292;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19978;&#28155;&#21152;&#20102;&#21512;&#29702;&#24615;&#21644;&#33258;&#21453;&#24615;&#23618;&#65292;&#20197;&#20351;&#27169;&#22411;&#30340;&#31572;&#26696;&#24471;&#20197;&#35299;&#37322;&#24182;&#28040;&#38500;&#28508;&#22312;&#30340;&#30683;&#30462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Large Language Models, LLMs)&#22312;&#38382;&#31572;&#20013;&#38750;&#24120;&#25797;&#38271;&#65292;&#20294;&#23427;&#20204;&#30340;&#31572;&#26696;&#19982;&#20854;&#20869;&#22312;&#30340;&#8220;&#20449;&#24565;&#8221;&#20043;&#38388;&#30340;&#20851;&#31995;&#24448;&#24448;&#19981;&#26126;&#30830;&#12290;&#36825;&#31181;&#32570;&#20047;&#35299;&#37322;&#24615;&#38459;&#30861;&#20102;LLMs&#30340;&#24191;&#27867;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#27169;&#22411;&#30340;&#20449;&#24565;&#20197;&#21450;&#23427;&#20204;&#30340;&#25512;&#29702;&#20851;&#31995;&#21464;&#24471;&#26126;&#30830;&#65292;&#24182;&#28040;&#38500;&#21487;&#33021;&#23384;&#22312;&#30340;&#30683;&#30462;&#65292;&#20197;&#20415;&#31572;&#26696;&#33021;&#22815;&#36890;&#36807;&#20174;&#19968;&#33268;&#30340;&#20449;&#24565;&#32593;&#32476;&#20013;&#24471;&#20986;&#30340;&#21487;&#35299;&#37322;&#30340;&#25512;&#29702;&#38142;&#26469;&#25903;&#25345;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21517;&#20026;REFLEX&#65292;&#22312;LLM&#20043;&#19978;&#28155;&#21152;&#20102;&#19968;&#20010;&#20855;&#26377;&#21512;&#29702;&#24615;&#21644;&#33258;&#21453;&#24615;&#30340;&#23618;&#12290;&#39318;&#20808;&#65292;&#32473;&#23450;&#19968;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;&#21453;&#21521;&#38142;&#25509;&#36807;&#31243;&#26500;&#24314;&#19968;&#20010;&#20449;&#24565;&#22270;&#65292;&#20197;&#23454;&#29616;&#30456;&#20851;&#27169;&#22411;&#20449;&#24565;(&#21253;&#25324;&#23545;&#31572;&#26696;&#20505;&#36873;&#32773;&#30340;&#20449;&#24565;)&#21450;&#20854;&#25512;&#29702;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#20351;&#29992;&#24418;&#24335;&#32422;&#26463;&#25512;&#29702;&#22120;&#35782;&#21035;&#21644;&#26368;&#23567;&#21270;&#35813;&#22270;&#20013;&#30340;&#30683;&#30462;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;REFLEX&#26174;&#33879;&#25552;&#39640;&#20102;&#19968;&#33268;&#24615;(&#32477;&#23545;&#20540;&#25552;&#21319;&#20102;8%-11%)&#65292;&#32780;&#19981;&#25439;&#23475;&#24050;&#26377;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LLMs) are proficient at question-answering (QA), it is not always clear how (or even if) an answer follows from their latent "beliefs". This lack of interpretability is a growing impediment to widespread use of LLMs. To address this, our goals are to make model beliefs and their inferential relationships explicit, and to resolve inconsistencies that may exist, so that answers are supported by interpretable chains of reasoning drawn from a consistent network of beliefs. Our approach, which we call REFLEX, is to add a rational, self-reflecting layer on top of the LLM. First, given a question, we construct a belief graph using a backward-chaining process to materialize relevant model beliefs (including beliefs about answer candidates) and their inferential relationships. Second, we identify and minimize contradictions in that graph using a formal constraint reasoner. We find that REFLEX significantly improves consistency (by 8%-11% absolute) without harming ov
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PEQA&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#37327;&#21270;LLM&#30340;&#20248;&#28857;&#12290;&#36890;&#36807;&#20165;&#26356;&#26032;&#37327;&#21270;&#23610;&#24230;&#65292;PEQA&#21487;&#20197;&#39640;&#25928;&#22320;&#24494;&#35843;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20943;&#23569;&#20869;&#23384;&#24320;&#38144;&#12290;</title><link>http://arxiv.org/abs/2305.14152</link><description>&lt;p&gt;
&#36890;&#36807;&#23376;4&#20301;&#25972;&#25968;&#37327;&#21270;&#23454;&#29616;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20869;&#23384;&#39640;&#25928;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization. (arXiv:2305.14152v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.14152
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;PEQA&#30340;&#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#21644;&#37327;&#21270;LLM&#30340;&#20248;&#28857;&#12290;&#36890;&#36807;&#20165;&#26356;&#26032;&#37327;&#21270;&#23610;&#24230;&#65292;PEQA&#21487;&#20197;&#39640;&#25928;&#22320;&#24494;&#35843;&#21387;&#32553;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#20943;&#23569;&#20869;&#23384;&#24320;&#38144;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38754;&#20020;&#30528;&#22312;&#24494;&#35843;&#21644;&#37096;&#32626;&#36807;&#31243;&#20013;&#30001;&#20110;&#20854;&#39640;&#20869;&#23384;&#38656;&#27714;&#21644;&#35745;&#31639;&#25104;&#26412;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#34429;&#28982;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#65288;PEFT&#65289;&#26041;&#27861;&#26088;&#22312;&#20943;&#23569;&#24494;&#35843;&#36807;&#31243;&#20013;&#20248;&#21270;&#22120;&#29366;&#24577;&#30340;&#20869;&#23384;&#20351;&#29992;&#37327;&#65292;&#20294;&#39044;&#35757;&#32451;LLM&#26435;&#37325;&#26412;&#36523;&#30340;&#22823;&#23567;&#20173;&#28982;&#26159;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#23613;&#31649;&#37327;&#21270;&#25216;&#26415;&#34987;&#24191;&#27867;&#25552;&#20986;&#26469;&#32531;&#35299;&#20869;&#23384;&#38656;&#27714;&#21644;&#21152;&#24555;LLM&#25512;&#29702;&#36895;&#24230;&#65292;&#20294;&#22823;&#22810;&#25968;&#36825;&#20123;&#25216;&#26415;&#37117;&#26159;&#38024;&#23545;&#37096;&#32626;&#38454;&#27573;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#21442;&#25968;&#39640;&#25928;&#21644;&#37327;&#21270;&#24863;&#30693;&#36866;&#24212;&#65288;PEQA&#65289;-&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#23558;PEFT&#30340;&#20248;&#28857;&#19982;&#37327;&#21270;LLM&#32467;&#21512;&#36215;&#26469;&#12290;&#36890;&#36807;&#20165;&#26356;&#26032;&#37327;&#21270;&#23610;&#24230;&#65292;PEQA&#21487;&#20197;&#30452;&#25509;&#24212;&#29992;&#20110;&#37327;&#21270;LLM&#65292;&#30830;&#20445;&#24179;&#31283;&#30340;&#20219;&#21153;&#36716;&#25442;&#12290;&#19982;&#29616;&#26377;&#30340;PEFT&#26041;&#27861;&#24182;&#34892;&#65292;PEQA&#26174;&#30528;&#20943;&#23569;&#20102;&#19982;&#20248;&#21270;&#22120;&#29366;&#24577;&#30456;&#20851;&#30340;&#20869;&#23384;&#24320;&#38144;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#20805;&#20998;&#21033;&#29992;&#20102;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages o
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LogiCoT, &#19968;&#20010;&#22522;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25945;&#25480;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#21644;&#24341;&#20986;&#19968;&#33324;&#25512;&#29702;&#25216;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.12147</link><description>&lt;p&gt;
LogiCoT&#65306;&#22522;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#25910;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
LogiCoT: Logical Chain-of-Thought Instruction-Tuning Data Collection with GPT-4. (arXiv:2305.12147v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.12147
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;LogiCoT, &#19968;&#20010;&#22522;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#25945;&#25480;&#27169;&#22411;&#36923;&#36753;&#25512;&#29702;&#21644;&#24341;&#20986;&#19968;&#33324;&#25512;&#29702;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;4&#65288;GPT-4&#65289;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24605;&#32500;&#38142;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;&#33258;&#25105;&#25351;&#23548;&#35843;&#25972;&#30740;&#31350;&#65288;&#22914;Alpaca&#65289;&#20391;&#37325;&#20110;&#22686;&#24378;&#27169;&#22411;&#30340;&#36890;&#29992;&#33021;&#21147;&#12290;&#36825;&#20123;&#25351;&#20196;&#20351;&#27169;&#22411;&#22312;&#19968;&#33324;&#20219;&#21153;&#65288;&#22914;&#24320;&#25918;&#39046;&#22495;&#25991;&#26412;&#29983;&#25104;&#21644;&#37322;&#20041;&#65289;&#19978;&#33021;&#22815;&#36798;&#21040;&#19982;GPT-3.5&#30456;&#24403;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#19981;&#33021;&#24110;&#21161;&#27169;&#22411;&#22788;&#29702;&#22797;&#26434;&#30340;&#25512;&#29702;&#20219;&#21153;&#12290;&#20026;&#22635;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;LogiCoT&#65292;&#19968;&#31181;&#26032;&#30340;&#36923;&#36753;&#24605;&#32500;&#25351;&#20196;&#35843;&#25972;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;GPT-4&#30340;&#36923;&#36753;&#24605;&#32500;&#38142;&#25512;&#29702;&#12290;&#25105;&#20204;&#35814;&#32454;&#38416;&#36848;&#20102;&#25910;&#38598;&#25351;&#20196;&#20197;&#25552;&#31034;GPT-4&#29983;&#25104;&#24605;&#32500;&#38142;&#25512;&#29702;&#30340;&#36807;&#31243;&#12290;LogiCoT&#20316;&#20026;&#25945;&#25480;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#25351;&#20196;&#38598;&#65292;&#24182;&#24341;&#20986;&#20102;&#19968;&#33324;&#25512;&#29702;&#25216;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Generative Pre-trained Transformer 4 (GPT-4) demonstrates impressive chain-of-thought reasoning ability. Recent work on self-instruction tuning, such as Alpaca, has focused on enhancing the general proficiency of models. These instructions enable the model to achieve performance comparable to GPT-3.5 on general tasks like open-domain text generation and paraphrasing. However, they fall short of helping the model handle complex reasoning tasks. To bridge the gap, this paper presents LogiCoT, a new instruction-tuning dataset for Logical Chain-of-Thought reasoning with GPT-4. We elaborate on the process of harvesting instructions for prompting GPT-4 to generate chain-of-thought rationales. LogiCoT serves as an instruction set for teaching models of logical reasoning and elicits general reasoning skills.
&lt;/p&gt;</description></item><item><title>STOAT&#27169;&#22411;&#26159;&#34920;&#26684;&#21644;&#25512;&#29702;&#24847;&#35782;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#25968;&#23383;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#26102;&#38388;&#25512;&#29702;&#12289;&#34920;&#26684;&#30693;&#35782;&#21644;&#23454;&#20307;&#30693;&#35782;&#26041;&#38754;&#26377;&#36739;&#22909;&#30340;&#25511;&#21046;&#65292;&#25552;&#39640;&#20102;&#20998;&#26512;&#21477;&#23376;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.11826</link><description>&lt;p&gt;
STOAT: &#32467;&#26500;&#21270;&#25968;&#25454;&#25511;&#21046;&#24615;&#20998;&#26512;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
STOAT: Structured Data to Analytical Text With Controls. (arXiv:2305.11826v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11826
&lt;/p&gt;
&lt;p&gt;
STOAT&#27169;&#22411;&#26159;&#34920;&#26684;&#21644;&#25512;&#29702;&#24847;&#35782;&#30340;&#29983;&#25104;&#27169;&#22411;&#65292;&#22312;&#25968;&#23383;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#26102;&#38388;&#25512;&#29702;&#12289;&#34920;&#26684;&#30693;&#35782;&#21644;&#23454;&#20307;&#30693;&#35782;&#26041;&#38754;&#26377;&#36739;&#22909;&#30340;&#25511;&#21046;&#65292;&#25552;&#39640;&#20102;&#20998;&#26512;&#21477;&#23376;&#29983;&#25104;&#30340;&#36136;&#37327;&#21644;&#20934;&#30830;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#35821;&#35328;&#27169;&#22411;&#22312;&#32467;&#26500;&#21270;&#25968;&#25454;&#21040;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#24403;&#38656;&#35201;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#20197;&#29983;&#25104;&#25551;&#36848;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#20174;&#32467;&#26500;&#21270;&#25968;&#25454;&#65288;&#20363;&#22914;&#34920;&#26684;&#65289;&#29983;&#25104;&#20998;&#26512;&#25991;&#26412;&#12290;&#22312;&#65288;Gupta et al.,2020&#65289;&#25552;&#20986;&#30340;&#20998;&#31867;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#20197;&#19979;&#25512;&#29702;&#31867;&#21035;&#30340;&#21487;&#25511;&#21046;&#34920;&#26684;&#21040;&#25991;&#26412;&#29983;&#25104;&#65306;&#25968;&#23383;&#25512;&#29702;&#12289;&#24120;&#35782;&#25512;&#29702;&#12289;&#26102;&#38388;&#25512;&#29702;&#12289;&#34920;&#26684;&#30693;&#35782;&#21644;&#23454;&#20307;&#30693;&#35782;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;STOAT&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#34920;&#26684;&#21644;&#25512;&#29702;&#24847;&#35782;&#65292;&#24182;&#36890;&#36807;&#30690;&#37327;&#37327;&#21270;&#23558;&#32473;&#23450;&#30340;&#25512;&#29702;&#31867;&#21035;&#27880;&#20837;&#36755;&#20986;&#20013;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#22312;&#20998;&#26512;&#21477;&#23376;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;iToTTo&#21644;Infotabs&#30340;PARENT&#25351;&#26631;&#19978;&#20998;&#21035;&#25552;&#20379;&#20102;10.19&#65285;&#21644;1.13&#65285;&#30340;&#20248;&#21270;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#19982;&#22522;&#32447;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#30340;&#25551;&#36848;&#26356;&#21152;&#20934;&#30830;&#21644;&#20998;&#26512;&#65292;&#20154;&#31867;&#35780;&#20272;&#20013;&#22686;&#21152;&#20102;15.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent language models have made tremendous progress in the structured data to text generation task. However, these models still give sub-optimal performance where logical inference is required to generate the descriptions. In this work, we specifically focus on analytical text generation from structured data such as tables. Building on the taxonomy proposed in (Gupta et al., 2020) we focus on controllable table to text generation for the following reasoning categories: numerical reasoning, commonsense reasoning, temporal reasoning, table knowledge, and entity knowledge. We propose STOAT model, which is table and reasoning aware, with vector-quantization to infuse the given reasoning categories in the output. We observe that our model provides 10.19%, 1.13% improvement on the PARENT metric in iToTTo and Infotabs for the analytical sentence task. We also found that our model generates 15.3% more faithful and analytical descriptions as compared to the baseline models in human evaluation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.10626</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#36935;&#35265;&#19990;&#30028;&#27169;&#22411;&#65306;&#23454;&#20307;&#32463;&#39564;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Language Models Meet World Models: Embodied Experiences Enhance Language Models. (arXiv:2305.10626v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10626
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#8212;&#8212;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#20016;&#23500;&#30340;&#23454;&#20307;&#32463;&#39564;&#36827;&#34892;&#24494;&#35843;, &#20197;&#25552;&#39640;&#20854;&#22312;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#24182;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#20445;&#25345;&#25110;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411; (LMs) &#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#38750;&#20961;&#30340;&#33021;&#21147;&#65292;&#20294;&#23427;&#20204;&#24120;&#24120;&#22312;&#22788;&#29702;&#29289;&#29702;&#29615;&#22659;&#19979;&#30340;&#31616;&#21333;&#25512;&#29702;&#21644;&#35268;&#21010;&#38382;&#39064;&#26102;&#36935;&#21040;&#22256;&#38590;&#65292;&#20363;&#22914;&#29702;&#35299;&#29289;&#20307;&#27704;&#24658;&#25110;&#35268;&#21010;&#23478;&#24237;&#27963;&#21160;&#12290;&#36825;&#31181;&#38480;&#21046;&#28304;&#20110; LM &#20165;&#21463;&#20070;&#38754;&#35821;&#35328;&#35757;&#32451;&#65292;&#32570;&#23569;&#24517;&#35201;&#30340;&#23454;&#20307;&#30693;&#35782;&#21644;&#25216;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378; LM &#30340;&#26041;&#27861;&#65292;&#21363;&#23558;&#20854;&#19982;&#19990;&#30028;&#27169;&#22411;&#30456;&#32467;&#21512;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#30693;&#35782;&#65292;&#21516;&#26102;&#20445;&#25345;&#20854;&#19968;&#33324;&#35821;&#35328;&#33021;&#21147;&#12290;&#26412;&#26041;&#27861;&#22312;&#19990;&#30028;&#27169;&#22411;&#20013;&#37096;&#32626;&#19968;&#20010;&#34701;&#20837;&#23454;&#20307;&#32463;&#39564;&#30340;&#20195;&#29702;&#65292;&#29305;&#21035;&#26159;&#19968;&#20010;&#27169;&#25311;&#29289;&#29702;&#19990;&#30028;&#30340;&#20223;&#30495;&#22120;(VirtualHome)&#65292;&#36890;&#36807;&#26377;&#30446;&#30340;&#30340;&#35268;&#21010;&#21644;&#38543;&#26426;&#25506;&#32034;&#33719;&#24471;&#22810;&#26679;&#21270;&#30340;&#23454;&#20307;&#32463;&#39564;&#12290;&#28982;&#21518;&#65292;&#21033;&#29992;&#36825;&#20123;&#32463;&#39564;&#24494;&#35843; LM &#65292;&#20197;&#25945;&#25480;&#22312;&#29289;&#29702;&#19990;&#30028;&#20013;&#30340;&#21508;&#31181;&#25512;&#29702;&#21644;&#34892;&#20026;&#33021;&#21147;&#65292;&#20363;&#22914;&#35268;&#21010;&#21644;&#23436;&#25104;&#30446;&#26631;&#12289;&#29289;&#20307;&#27704;&#24658;&#21644;&#36319;&#36394;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#36731;&#26494;&#25193;&#23637;&#20197;&#21033;&#29992;&#20854;&#20182;&#27169;&#25311;&#22120;&#65292;&#21253;&#25324;&#26426;&#22120;&#20154;&#25110;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#30528;&#25552;&#39640;&#20102; LM &#22312;&#19968;&#31995;&#21015;&#29289;&#29702;&#25512;&#29702;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#30041;&#24182;&#32463;&#24120;&#25552;&#39640;&#20102;&#23427;&#20204;&#22312;&#35821;&#35328;&#22522;&#20934;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
While large language models (LMs) have shown remarkable capabilities across numerous tasks, they often struggle with simple reasoning and planning in physical environments, such as understanding object permanence or planning household activities. The limitation arises from the fact that LMs are trained only on written text and miss essential embodied knowledge and skills. In this paper, we propose a new paradigm of enhancing LMs by finetuning them with world models, to gain diverse embodied knowledge while retaining their general language capabilities. Our approach deploys an embodied agent in a world model, particularly a simulator of the physical world (VirtualHome), and acquires a diverse set of embodied experiences through both goal-oriented planning and random exploration. These experiences are then used to finetune LMs to teach diverse abilities of reasoning and acting in the physical world, e.g., planning and completing goals, object permanence and tracking, etc. Moreover, it is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#39046;&#22495;&#20869;&#23578;&#26410;&#30740;&#31350;&#30340;&#38382;&#39064;&#65306;&#24773;&#26223;&#21644;&#32454;&#33268;&#22320;&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.10156</link><description>&lt;p&gt;
&#38405;&#35835;&#36807;&#31243;&#20013;&#23545;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#30340;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
Personality Understanding of Fictional Characters during Book Reading. (arXiv:2305.10156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10156
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;NLP&#39046;&#22495;&#20869;&#23578;&#26410;&#30740;&#31350;&#30340;&#38382;&#39064;&#65306;&#24773;&#26223;&#21644;&#32454;&#33268;&#22320;&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#65292;&#24182;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#23567;&#35828;&#20154;&#29289;&#20010;&#24615;&#26159;&#38405;&#35835;&#25925;&#20107;&#30340;&#20851;&#38190;&#12290;&#38543;&#30528;&#35835;&#32773;&#19982;&#25925;&#20107;&#30340;&#20114;&#21160;&#65292;&#20182;&#20204;&#23545;&#19968;&#20010;&#20154;&#29289;&#30340;&#29702;&#35299;&#20250;&#26681;&#25454;&#26032;&#30340;&#20107;&#20214;&#21644;&#20449;&#24687;&#32780;&#28436;&#21464;&#65307;&#24182;&#19988;&#21487;&#20197;&#24863;&#30693;&#21040;&#22810;&#20010;&#31934;&#32454;&#30340;&#20010;&#24615;&#26041;&#38754;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#33258;&#28982;&#30340;&#38382;&#39064;&#65306;&#24773;&#22659;&#21644;&#31934;&#32454;&#30340;&#20010;&#24615;&#29702;&#35299;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;NLP&#39046;&#22495;&#20013;&#27809;&#26377;&#24471;&#21040;&#30740;&#31350;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#32570;&#20047;&#27169;&#20223;&#38405;&#35835;&#36807;&#31243;&#30340;&#36866;&#24403;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#26631;&#35760;&#25968;&#25454;&#38598;PersoNet&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26032;&#22411;&#27880;&#37322;&#31574;&#30053;&#28041;&#21450;&#29992;&#22312;&#32447;&#38405;&#35835;&#24212;&#29992;&#31243;&#24207;&#30340;&#29992;&#25143;&#31508;&#35760;&#20316;&#20026;&#21407;&#22987;&#20070;&#31821;&#30340;&#20195;&#29702;&#36827;&#34892;&#27880;&#37322;&#12290;&#23454;&#39564;&#21644;&#20154;&#20307;&#30740;&#31350;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#26082;&#26377;&#25928;&#21448;&#20934;&#30830;&#65307;&#25105;&#20204;&#30340;&#20219;&#21153;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20381;&#36182;&#20110;&#38271;&#26399;&#30340;&#19978;&#19979;&#25991;&#20197;&#23454;&#29616;&#23545;&#26426;&#22120;&#21644;&#20154;&#31867;&#30340;&#20934;&#30830;&#39044;&#27979;&#12290;&#25968;&#25454;&#38598;&#21487;&#22312;https://github.com/Gorov/personet_acl23&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comprehending characters' personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situated and fine-grained personality understanding. The problem has not been studied in the NLP field, primarily due to the lack of appropriate datasets mimicking the process of book reading. We present the first labeled dataset PersoNet for this problem. Our novel annotation strategy involves annotating user notes from online reading apps as a proxy for the original books. Experiments and human studies indicate that our dataset construction is both efficient and accurate; and our task heavily relies on long-term context to achieve accurate predictions for both machines and humans. The dataset is available at https://github.com/Gorov/personet_acl23.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#22270;&#24418;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM(GPT-3/4)&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.10037</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#21542;&#29992;&#33258;&#28982;&#35821;&#35328;&#35299;&#20915;&#22270;&#38382;&#39064;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Solve Graph Problems in Natural Language?. (arXiv:2305.10037v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.10037
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#36825;&#26159;&#19968;&#20010;&#20840;&#38754;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#27979;&#35797;&#65292;&#26088;&#22312;&#35780;&#20272;LLM&#22312;&#25991;&#26412;&#25551;&#36848;&#30340;&#22270;&#24418;&#32467;&#26500;&#21644;&#22270;&#24418;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#30340;&#22788;&#29702;&#33021;&#21147;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;LLM(GPT-3/4)&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#19968;&#20123;&#20855;&#26377;&#38544;&#24335;&#22270;&#24418;&#32467;&#26500;&#30340;&#20219;&#21153;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#35268;&#21010;&#12289;&#22810;&#36339;&#38382;&#39064;&#22238;&#31572;&#25110;&#30693;&#35782;&#25506;&#32034;&#12289;&#32467;&#26500;&#21270;&#24120;&#35782;&#25512;&#29702;&#31561;&#31561;&#12290;&#34429;&#28982;LLM&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#23450;&#30340;&#36827;&#23637;&#65292;&#20294;&#26159;LLM&#26159;&#21542;&#33021;&#22815;&#26174;&#24335;&#22788;&#29702;&#22270;&#24418;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#23558;&#23427;&#20204;&#26144;&#23556;&#21040;&#22522;&#20110;&#27010;&#24565;&#30340;&#31354;&#38388;&#20013;&#65292;&#24182;&#25191;&#34892;&#32467;&#26500;&#21270;&#25805;&#20316;&#20173;&#28982;&#23578;&#26410;&#24471;&#21040;&#36275;&#22815;&#30340;&#30740;&#31350;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#33258;&#28982;&#35821;&#35328;&#22270;&#24418;(NLGraph)&#65292;&#23427;&#26159;&#19968;&#20010;&#35774;&#35745;&#29992;&#20110;&#33258;&#28982;&#35821;&#35328;&#30340;&#22522;&#20110;&#22270;&#24418;&#38382;&#39064;&#35299;&#20915;&#20840;&#38754;&#27979;&#35797;&#12290;NLGraph&#21253;&#21547;29,370&#20010;&#38382;&#39064;&#65292;&#28085;&#30422;&#20102;&#20843;&#20010;&#22270;&#24418;&#25512;&#29702;&#20219;&#21153;&#65292;&#20174;&#31616;&#21333;&#30340;&#36830;&#25509;&#21644;&#26368;&#30701;&#36335;&#24452;&#21040;&#22797;&#26434;&#30340;&#26368;&#22823;&#27969;&#21644;&#27169;&#25311;&#22270;&#31070;&#32463;&#32593;&#32476;&#31561;&#20219;&#21153;&#19981;&#31561;&#12290;&#25105;&#20204;&#22312;NLGraph&#22522;&#20934;&#27979;&#35797;&#19978;&#35780;&#20272;&#20102;LLM(GPT-3/4)&#65292;&#24182;&#21457;&#29616;1)&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#30456;&#24212;&#30340;&#22270;&#24418;&#25512;&#29702;&#33021;&#21147;&#65307;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35270;&#39057;&#30340;&#21160;&#20316;&#35782;&#21035;&#20013;&#23569;&#26679;&#26412;&#33258;&#36866;&#24212;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;FSDA-AR&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#30446;&#26631;&#35270;&#39057;&#23454;&#29616;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#12290;&#23454;&#39564;&#35777;&#26126;FSDA-AR&#19982;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;</title><link>http://arxiv.org/abs/2305.08420</link><description>&lt;p&gt;
RelaMiX: &#25506;&#32034;&#35270;&#39057;&#20013;&#22522;&#20110;&#21160;&#20316;&#35782;&#21035;&#30340;&#23569;&#26679;&#26412;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
RelaMiX: Exploring Few-Shot Adaptation in Video-based Action Recognition. (arXiv:2305.08420v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.08420
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;&#35270;&#39057;&#30340;&#21160;&#20316;&#35782;&#21035;&#20013;&#23569;&#26679;&#26412;&#33258;&#36866;&#24212;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;FSDA-AR&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#23569;&#37327;&#26631;&#35760;&#30340;&#30446;&#26631;&#35270;&#39057;&#23454;&#29616;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#12290;&#23454;&#39564;&#35777;&#26126;FSDA-AR&#19982;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#33258;&#36866;&#24212;&#23545;&#20110;&#27963;&#21160;&#35782;&#21035;&#33267;&#20851;&#37325;&#35201;&#65292;&#20197;&#30830;&#20445;&#22312;&#19981;&#21516;&#30340;&#29615;&#22659;&#12289;&#20256;&#24863;&#22120;&#31867;&#22411;&#21644;&#25968;&#25454;&#26469;&#28304;&#19979;&#33021;&#22815;&#20934;&#30830;&#21644;&#31283;&#20581;&#22320;&#36827;&#34892;&#24615;&#33021;&#34920;&#29616;&#12290;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#26469;&#33258;&#30446;&#26631;&#39046;&#22495;&#30340;&#22823;&#35268;&#27169;&#26080;&#26631;&#31614;&#25968;&#25454;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#22522;&#20110;&#35270;&#39057;&#30340;&#27963;&#21160;&#35782;&#21035;&#30340;&#23569;&#26679;&#26412;&#39046;&#22495;&#33258;&#36866;&#24212;&#38382;&#39064;&#65288;FSDA-AR&#65289;&#65292;&#23427;&#21033;&#29992;&#20102;&#23569;&#37327;&#30340;&#26631;&#35760;&#30446;&#26631;&#35270;&#39057;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#33258;&#36866;&#24212;&#12290;&#36825;&#31181;&#35774;&#32622;&#23545;&#20110;&#24212;&#29992;&#31243;&#24207;&#26469;&#35828;&#26159;&#26377;&#21560;&#24341;&#21147;&#21644;&#26377;&#21069;&#26223;&#30340;&#65292;&#22240;&#20026;&#23427;&#21482;&#38656;&#35201;&#22312;&#30446;&#26631;&#39046;&#22495;&#20013;&#35760;&#24405;&#21644;&#26631;&#35760;&#24456;&#23569;&#29978;&#33267;&#19968;&#20010;&#26679;&#26412;&#30340;&#31867;&#21035;&#65292;&#21253;&#25324;&#37027;&#20123;&#32597;&#35265;&#20294;&#33267;&#20851;&#37325;&#35201;&#30340;&#27963;&#21160;&#12290;&#25105;&#20204;&#20351;&#29992;&#20116;&#20010;&#24050;&#24314;&#31435;&#30340;&#25968;&#25454;&#38598;&#26500;&#24314;&#20102;FSDA-AR&#22522;&#20934;&#65292;&#32771;&#34385;&#20102;&#19981;&#21516;&#30340;&#39046;&#22495;&#31867;&#22411;&#65306;UCF101&#65292;HMDB51&#65292;EPIC-KITCHEN&#65292;Sims4Action&#21644;ToyotaSmartHome&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;FSDA-AR&#19982;&#26080;&#30417;&#30563;&#39046;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#30456;&#24403;&#12290;
&lt;/p&gt;
&lt;p&gt;
Domain adaptation is essential for activity recognition to ensure accurate and robust performance across diverse environments, sensor types, and data sources. Unsupervised domain adaptation methods have been extensively studied, yet, they require large-scale unlabeled data from the target domain. In this work, we address Few-Shot Domain Adaptation for video-based Activity Recognition (FSDA-AR), which leverages a very small amount of labeled target videos to achieve effective adaptation. This setting is attractive and promising for applications, as it requires recording and labeling only a few, or even a single example per class in the target domain, which often includes activities that are rare yet crucial to recognize. We construct FSDA-AR benchmarks using five established datasets considering diverse domain types: UCF101, HMDB51, EPIC-KITCHEN, Sims4Action, and ToyotaSmartHome. Our results demonstrate that FSDA-AR performs comparably to unsupervised domain adaptation with significantl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#33021;&#37327;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;k-&#21311;&#21517;&#21270;&#35757;&#32451;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#20294;&#33021;&#28304;&#28040;&#32791;&#36739;&#23569;&#65292;&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#26377;&#28508;&#21147;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#33021;&#28304;&#28040;&#32791;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.07116</link><description>&lt;p&gt;
k-&#21311;&#21517;&#21644;&#21512;&#25104;&#25968;&#25454;&#25216;&#26415;&#30340;&#33021;&#37327;&#25104;&#26412;&#21644;&#26426;&#22120;&#23398;&#20064;&#20934;&#30830;&#24615;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
Energy cost and machine learning accuracy impact of k-anonymisation and synthetic data techniques. (arXiv:2305.07116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.07116
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20934;&#30830;&#24615;&#21644;&#33021;&#37327;&#28040;&#32791;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;k-&#21311;&#21517;&#21270;&#35757;&#32451;&#30340;&#27169;&#22411;&#20934;&#30830;&#24615;&#36739;&#20302;&#65292;&#20294;&#33021;&#28304;&#28040;&#32791;&#36739;&#23569;&#65292;&#21512;&#25104;&#25968;&#25454;&#26159;&#19968;&#31181;&#26377;&#28508;&#21147;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#22312;&#33021;&#28304;&#28040;&#32791;&#26356;&#23569;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#19982;&#38544;&#31169;&#21644;&#27668;&#20505;&#21464;&#21270;&#26377;&#20851;&#30340;&#24840;&#21457;&#22686;&#38271;&#30340;&#31038;&#20250;&#20851;&#20999;&#65292;&#27431;&#30431;&#39041;&#24067;&#20102;&#12298;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#12299;(GDPR)&#24182;&#25215;&#35834;&#20102;&#32511;&#33394;&#21327;&#35758;&#12290;&#22823;&#37327;&#30740;&#31350;&#25506;&#31350;&#20102;&#36816;&#29992;&#21311;&#21517;&#25968;&#25454;&#38598;&#35757;&#32451;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#25928;&#21644;&#20934;&#30830;&#24615;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#22987;&#25506;&#31350;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65288;PET&#65289;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;k-&#21311;&#21517;&#12290;&#30001;&#20110;&#21512;&#25104;&#25968;&#25454;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#65292;&#22240;&#27492;&#26412;&#25991;&#20998;&#26512;&#20102;&#20004;&#20010;&#38454;&#27573;&#30340;&#33021;&#37327;&#28040;&#32791;&#21644;&#20934;&#30830;&#24615;&#65306;a&#65289;&#23558;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#24212;&#29992;&#20110;&#30456;&#20851;&#25968;&#25454;&#38598;&#65292;b&#65289;&#22312;&#30456;&#20851;&#38544;&#31169;&#22686;&#24378;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#27169;&#22411;&#12290;&#25105;&#20204;&#20351;&#29992;&#20004;&#31181;&#38544;&#31169;&#22686;&#24378;&#25216;&#26415;&#65306;k-&#21311;&#21517;&#21270;&#65288;&#20351;&#29992;&#27867;&#21270;&#21644;&#25233;&#21046;&#65289;&#21644;&#21512;&#25104;&#25968;&#25454;&#65292;&#20197;&#21450;&#19977;&#31181;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#27599;&#20010;&#27169;&#22411;&#37117;&#22312;&#27599;&#20010;&#38544;&#31169;&#22686;&#24378;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#22312;&#32463;&#36807;k-&#21311;&#21517;&#21270;&#30340;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#20855;&#26377;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#65292;&#24182;&#19988;&#28040;&#32791;&#30340;&#33021;&#37327;&#36739;&#23569;&#65292;&#19982;&#22312;&#38750;&#21311;&#21517;&#21270;&#25968;&#25454;&#19978;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#27604;&#12290;&#28982;&#32780;&#65292;k-&#21311;&#21517;&#21270;&#36807;&#31243;&#20013;&#28040;&#32791;&#30340;&#33021;&#37327;&#38750;&#24120;&#21487;&#35266;&#65292;&#22312;&#35780;&#20272;&#20854;&#26377;&#29992;&#24615;&#26102;&#24517;&#39035;&#23558;&#20854;&#32771;&#34385;&#22312;&#20869;&#12290;&#21512;&#25104;&#25968;&#25454;&#35777;&#26126;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#36873;&#25321;&#65292;&#22240;&#20026;&#23427;&#22312;&#28040;&#32791;&#26356;&#23569;&#33021;&#28304;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#19982;&#38750;&#21311;&#21517;&#21270;&#25968;&#25454;&#21487;&#27604;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
To address increasing societal concerns regarding privacy and climate, the EU adopted the General Data Protection Regulation (GDPR) and committed to the Green Deal. Considerable research studied the energy efficiency of software and the accuracy of machine learning models trained on anonymised data sets. Recent work began exploring the impact of privacy-enhancing techniques (PET) on both the energy consumption and accuracy of the machine learning models, focusing on k-anonymity. As synthetic data is becoming an increasingly popular PET, this paper analyses the energy consumption and accuracy of two phases: a) applying privacy-enhancing techniques to the concerned data set, b) training the models on the concerned privacy-enhanced data set. We use two privacy-enhancing techniques: k-anonymisation (using generalisation and suppression) and synthetic data, and three machine-learning models. Each model is trained on each privacy-enhanced data set. Our results show that models trained on k-a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#35774;&#35745;&#38382;&#39064;&#21450;&#20854;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#8220;&#39532;&#23572;&#31185;&#22827;&#20449;&#20196;&#21338;&#24328;&#8221;&#30340;&#27010;&#24565;&#12290;</title><link>http://arxiv.org/abs/2305.06807</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Information Design in Multi-Agent Reinforcement Learning. (arXiv:2305.06807v1 [cs.GT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#31350;&#20102;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20449;&#24687;&#35774;&#35745;&#38382;&#39064;&#21450;&#20854;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#8220;&#39532;&#23572;&#31185;&#22827;&#20449;&#20196;&#21338;&#24328;&#8221;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#27169;&#20223;&#20154;&#31867;&#21644;&#21160;&#29289;&#19982;&#29615;&#22659;&#20132;&#20114;&#30340;&#26041;&#24335;&#12290;&#28982;&#32780;&#23454;&#38469;&#29615;&#22659;&#20013;&#23384;&#22312;&#20854;&#20182;&#26377;&#33258;&#24049;&#30446;&#26631;&#30340;&#26234;&#33021;&#20307;&#65292;&#23427;&#20204;&#20250;&#36866;&#24212;&#22320;&#19982;&#33258;&#24049;&#30456;&#20114;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#36825;&#20123;&#29615;&#22659;&#20013;&#25104;&#21151;&#65292;&#33258;&#20027;&#26234;&#33021;&#20307;&#38656;&#35201;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#20197;&#20351;&#23427;&#20204;&#30340;&#34892;&#20026;&#26356;&#26377;&#30410;&#12290;&#20449;&#24687;&#35774;&#35745;&#26159;&#24433;&#21709;&#20854;&#20182;&#26234;&#33021;&#20307;&#34892;&#20026;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#38024;&#23545;&#19968;&#32452;RL&#20195;&#29702;&#30340;&#20449;&#24687;&#35774;&#35745;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#8220;&#39532;&#23572;&#31185;&#22827;&#20449;&#20196;&#21338;&#24328;&#8221;&#30340;&#27010;&#24565;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) mimics how humans and animals interact with the environment. The setting is somewhat idealized because, in actual tasks, other agents in the environment have their own goals and behave adaptively to the ego agent. To thrive in those environments, the agent needs to influence other agents so their actions become more helpful and less harmful. Research in computational economics distills two ways to influence others directly: by providing tangible goods (mechanism design) and by providing information (information design). This work investigates information design problems for a group of RL agents. The main challenges are two-fold. One is the information provided will immediately affect the transition of the agent trajectories, which introduces additional non-stationarity. The other is the information can be ignored, so the sender must provide information that the receivers are willing to respect. We formulate the Markov signaling game, and develop the notions 
&lt;/p&gt;</description></item><item><title>The Vault&#26159;&#19968;&#20010;&#25552;&#20379;&#20102;10&#31181;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#30340;40&#30334;&#19975;&#34892;&#20195;&#30721;-&#25991;&#26412;&#23545;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#38754;&#21521;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#65292;&#26377;&#26395;&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.06156</link><description>&lt;p&gt;
The Vault&#65306;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20026;&#20419;&#36827;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#32780;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation. (arXiv:2305.06156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06156
&lt;/p&gt;
&lt;p&gt;
The Vault&#26159;&#19968;&#20010;&#25552;&#20379;&#20102;10&#31181;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#30340;40&#30334;&#19975;&#34892;&#20195;&#30721;-&#25991;&#26412;&#23545;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#38754;&#21521;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#65292;&#26377;&#26395;&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; The Vault&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#20195;&#30721;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#38754;&#21521;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#35757;&#32451;&#22522;&#20110;&#20195;&#30721;&#30340;LLM&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#22312;&#22823;&#23567;&#12289;&#36136;&#37327;(&#30001;&#20110;&#22122;&#22768;&#20449;&#21495;)&#21644;&#26684;&#24335;&#65288;&#20165;&#21253;&#21547;&#20195;&#30721;&#20989;&#25968;&#21644;&#25991;&#26412;&#35828;&#26126;&#37197;&#23545;&#65289;&#26041;&#38754;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;The Vault&#36890;&#36807;&#25552;&#20379;10&#31181;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#30340;40&#30334;&#19975;&#34892;&#20195;&#30721;-&#25991;&#26412;&#23545;&#65292;&#24443;&#24213;&#28165;&#38500;10&#31181;&#22810;&#26679;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#21508;&#31181;&#32423;&#21035;&#30340;&#20195;&#30721;-&#25991;&#26412;&#23545;&#65292;&#21253;&#25324;&#31867;&#12289;&#20989;&#25968;&#21644;&#20195;&#30721;&#34892;&#31561;&#32423;&#21035;&#65292;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#21487;&#20197;&#21033;&#29992;The Vault&#26469;&#35757;&#32451;&#19981;&#21516;&#30340;&#38754;&#21521;&#20195;&#30721;&#30340;LLM&#65292;&#25110;&#32773;&#23558;&#25552;&#20379;&#30340;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#21644;&#33050;&#26412;&#21512;&#24182;&#21040;&#33258;&#24049;&#30340;&#25968;&#25454;&#38598;&#20013;&#26469;&#25913;&#36827;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#23558;The Vault&#20316;&#20026;&#38754;&#21521;&#20195;&#30721;&#30340;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#39044;&#35745;&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#23454;&#36341;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present The Vault, an open-source, large-scale code-text dataset designed to enhance the training of code-focused large language models (LLMs). Existing open-source datasets for training code-based LLMs often face challenges in terms of size, quality (due to noisy signals), and format (only containing code function and text explanation pairings). The Vault overcomes these limitations by providing 40 million code-text pairs across 10 popular programming languages, thorough cleaning for 10+ prevalent issues, and various levels of code-text pairings, including class, function, and line levels. Researchers and practitioners can utilize The Vault for training diverse code-focused LLMs or incorporate the provided data cleaning methods and scripts to improve their datasets. By employing The Vault as the training dataset for code-centric LLMs, we anticipate significant advancements in code understanding and generation tasks, fostering progress in both artificial intelligence research and so
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05803</link><description>&lt;p&gt;
&#22522;&#20110;Segment Anything Model (SAM)&#22686;&#24378;&#20266;&#26631;&#31614;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#22270;&#20687;&#32423;&#21035;&#30340;&#30417;&#30563;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;(WSSS)&#30001;&#20110;&#20854;&#19982;&#20687;&#32032;&#32423;&#27880;&#37322;&#30456;&#27604;&#30340;&#20302;&#25104;&#26412;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#31867;&#28608;&#27963;&#22270;(CAM)&#29983;&#25104;&#20687;&#32032;&#32423;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;CAM&#32463;&#24120;&#36973;&#21463;&#23616;&#37096;&#28608;&#27963;&#30340;&#38480;&#21046;-&#21482;&#28608;&#27963;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#37096;&#20998;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#23545;&#35937;&#21306;&#22495;&#21644;&#34394;&#20551;&#30340;&#28608;&#27963;-&#19981;&#24517;&#35201;&#22320;&#28608;&#27963;&#29289;&#20307;&#21608;&#22260;&#30340;&#32972;&#26223;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#21363;&#21033;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;Segment Anything Model (SAM)&#22686;&#24378;CAM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#12290;SAM&#26159;&#19968;&#20010;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#27573;&#33853;&#30340;&#24378;&#38646;-shot&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#36825;&#20123;&#21306;&#22495;&#30340;&#35821;&#20041;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#29305;&#23450;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#20316;&#20026;&#20449;&#21495;&#26469;&#36873;&#25321;m&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision has garnered increasing attention due to its low annotation cost compared to pixel-level annotation. Most existing methods rely on Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, it is well known that CAM often suffers from partial activation -- activating the most discriminative part instead of the entire object area, and false activation -- unnecessarily activating the background around the object. In this study, we introduce a simple yet effective approach to address these limitations by harnessing the recently released Segment Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is a segmentation foundation model that demonstrates strong zero-shot ability in partitioning images into segments but lacks semantic labels for these regions. To circumvent this, we employ pseudo labels for a specific class as the signal to select the m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.03598</link><description>&lt;p&gt;
NLI4CT&#65306;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#22810;&#35777;&#25454;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
NLI4CT: Multi-Evidence Natural Language Inference for Clinical Trial Reports. (arXiv:2305.03598v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03598
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#30340;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#27169;&#22411;&#65292;&#36890;&#36807;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#26469;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22914;&#20309;&#35299;&#37322;&#21644;&#26816;&#32034;&#29992;&#20110;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#30340;&#21307;&#23398;&#35777;&#25454;&#65311;&#22810;&#24180;&#26469;&#65292;&#31215;&#32047;&#19979;&#26469;&#30340;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#21253;&#21547;&#20102;&#21457;&#23637;&#20010;&#24615;&#21270;&#21307;&#23398;&#25152;&#24517;&#38656;&#30340;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#25214;&#21040;&#26368;&#20339;&#30340;&#23454;&#39564;&#27835;&#30103;&#35777;&#25454;&#65292;&#25163;&#21160;&#26816;&#26597;&#36229;&#36807;400,000&#20010;&#20020;&#24202;&#35797;&#39564;&#25253;&#21578;&#26159;&#23454;&#38469;&#19978;&#19981;&#21487;&#34892;&#30340;&#12290;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#65288;NLI&#65289;&#25552;&#20379;&#20102;&#19968;&#20010;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#20801;&#35768;&#21487;&#25193;&#23637;&#35745;&#31639;&#25991;&#26412;&#34164;&#21547;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;NLI&#27169;&#22411;&#22312;&#29983;&#29289;&#21307;&#23398;&#35821;&#26009;&#24211;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#20043;&#21069;&#21457;&#24067;&#30340;&#25968;&#25454;&#38598;&#26080;&#27861;&#25429;&#25417;CTR&#25512;&#29702;&#30340;&#20840;&#37096;&#22797;&#26434;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#65292;&#20197;&#25512;&#36827;&#20851;&#20110;CTR&#25512;&#29702;&#30340;NLI&#30740;&#31350;&#12290;&#35813;&#36164;&#28304;&#21253;&#25324;&#20004;&#20010;&#20027;&#35201;&#20219;&#21153;&#12290;&#39318;&#20808;&#65292;&#30830;&#23450;&#33258;&#28982;&#35821;&#35328;&#38472;&#36848;&#21644;CTR&#20043;&#38388;&#30340;&#25512;&#29702;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#26816;&#32034;&#25903;&#25345;&#20107;&#23454;&#20197;&#35777;&#26126;&#39044;&#27979;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;NLI4CT&#65292;&#19968;&#20010;&#22522;&#20110;CTR&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
How can we interpret and retrieve medical evidence to support clinical decisions? Clinical trial reports (CTR) amassed over the years contain indispensable information for the development of personalized medicine. However, it is practically infeasible to manually inspect over 400,000+ clinical trial reports in order to find the best evidence for experimental treatments. Natural Language Inference (NLI) offers a potential solution to this problem, by allowing the scalable computation of textual entailment. However, existing NLI models perform poorly on biomedical corpora, and previously published datasets fail to capture the full complexity of inference over CTRs. In this work, we present a novel resource to advance research on NLI for reasoning on CTRs. The resource includes two main tasks. Firstly, to determine the inference relation between a natural language statement, and a CTR. Secondly, to retrieve supporting facts to justify the predicted relation. We provide NLI4CT, a corpus of
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32763;&#35793;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#12289;&#36328;&#35821;&#35328;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26694;&#26550;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#22810;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;</title><link>http://arxiv.org/abs/2305.03510</link><description>&lt;p&gt;
&#22522;&#20110;&#32763;&#35793;&#23545;&#40784;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#36328;&#35821;&#35328;&#36801;&#31227;&#30340;&#21442;&#25968;&#39640;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Parameter-Efficient Cross-lingual Transfer of Vision and Language Models via Translation-based Alignment. (arXiv:2305.03510v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.03510
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#32763;&#35793;&#23545;&#40784;&#30340;&#26041;&#24335;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#12289;&#36328;&#35821;&#35328;&#30340;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#35813;&#26694;&#26550;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#22810;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#22312;&#36830;&#25509;&#22270;&#20687;&#21644;&#33521;&#35821;&#25991;&#26412;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#21151;&#12290;&#23613;&#31649;&#26368;&#36817;&#35797;&#22270;&#25193;&#23637;CLIP&#20197;&#25903;&#25345;&#20854;&#20182;&#35821;&#35328;&#65292;&#20294;&#30001;&#20110;&#36164;&#28304;&#19981;&#24179;&#34913;&#65292;&#35266;&#23519;&#21040;&#20102;&#19981;&#21516;&#35821;&#35328;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#26041;&#27861;&#20250;&#28040;&#32791;&#22823;&#37327;&#36164;&#28304;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21442;&#25968;&#39640;&#25928;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#23398;&#20064;&#26694;&#26550;&#65292;&#21033;&#29992;&#22522;&#20110;&#32763;&#35793;&#30340;&#23545;&#40784;&#26041;&#27861;&#26469;&#20943;&#36731;&#22810;&#35821;&#35328;&#24046;&#24322;&#65292;&#24182;&#25506;&#32034;&#21442;&#25968;&#39640;&#25928;&#30340;&#24494;&#35843;&#26041;&#27861;&#26469;&#23454;&#29616;&#21442;&#25968;&#39640;&#25928;&#30340;&#36328;&#35821;&#35328;&#36801;&#31227;&#12290;&#22312;XTD&#21644;Multi30K&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#28085;&#30422;&#20102;&#38646;-shot&#12289;few-shot&#21644;&#20840;&#25968;&#25454;&#38598;&#23398;&#20064;&#22330;&#26223;&#19979;&#30340;11&#31181;&#35821;&#35328;&#65292;&#32467;&#26524;&#26174;&#31034;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#20943;&#23569;&#20102;&#35821;&#35328;&#20043;&#38388;&#30340;&#22810;&#35821;&#35328;&#24046;&#24322;&#65292;&#24182;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained vision and language models such as CLIP have witnessed remarkable success in connecting images and texts with a primary focus on English texts. Despite recent efforts to extend CLIP to support other languages, disparities in performance among different languages have been observed due to uneven resource availability. Additionally, current cross-lingual transfer methods of those pre-trained models would consume excessive resources for a large number of languages. Therefore, we propose a new parameter-efficient cross-lingual transfer learning framework that utilizes a translation-based alignment method to mitigate multilingual disparities and explores parameter-efficient fine-tuning methods for parameter-efficient cross-lingual transfer. Extensive experiments on XTD and Multi30K datasets, covering 11 languages under zero-shot, few-shot, and full-dataset learning scenarios, show that our framework significantly reduces the multilingual disparities among languages and improves 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2305.02109</link><description>&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#19982;O-RAN&#30340;&#21327;&#21516;&#65306;&#38754;&#21521;&#22810;&#20010;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26381;&#21153;&#30340;&#24377;&#24615;&#34394;&#25311;&#21270;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02109
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32852;&#37030;&#23398;&#20064;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#19979;&#30340;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#31216;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#65292;&#26469;&#25903;&#25345;DMS-FL&#20013;&#30340;&#35774;&#35745;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#26159;&#26368;&#27969;&#34892;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65292;&#20294;&#26159;&#22312;&#29616;&#20195;&#26080;&#32447;&#32593;&#32476;&#20013;&#23454;&#29616;&#32852;&#37030;&#23398;&#20064;&#38754;&#20020;&#30528;&#35768;&#22810;&#25361;&#25112;&#65292;&#20027;&#35201;&#21253;&#25324;&#32593;&#32476;&#26465;&#20214;&#30340;&#21160;&#24577;&#24615;&#12289;&#31995;&#32479;&#20013;&#22810;&#20010;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;/&#20219;&#21153;&#30340;&#24182;&#23384;&#20197;&#21450;&#32852;&#37030;&#23398;&#20064;&#26381;&#21153;&#19982;&#20854;&#20182;&#32593;&#32476;&#26381;&#21153;&#30340;&#24182;&#34892;&#25191;&#34892;&#31561;&#12290;&#38024;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21160;&#24577;&#22810;&#26381;&#21153;&#32852;&#37030;&#23398;&#20064;&#65288;DMS-FL&#65289;&#30340;&#32852;&#37030;&#23398;&#20064;&#27867;&#22411;&#26550;&#26500;&#65292;&#24182;&#36890;&#36807;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26550;&#26500;&#8212;&#8212;&#24377;&#24615;&#34394;&#25311;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;EV-FL&#65289;&#26469;&#35299;&#20915;DMS-FL&#20013;&#30340;&#19977;&#20010;&#26410;&#25506;&#32034;&#30340;&#35774;&#35745;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) is the most popular distributed machine learning technique. However, implementation of FL over modern wireless networks faces key challenges caused by (i) dynamics of the network conditions, (ii) coexistence of multiple FL services/tasks in the system, and (iii) concurrent execution of FL services with other network services, which are not jointly considered in prior works. Motivated by these challenges, we introduce a generic FL paradigm over next-generation (NextG) networks, called dynamic multi-service FL (DMS-FL). We identify three unexplored design considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless resource fragmentation, and (iii) signal strength fluctuations. We take the first steps towards addressing these design considerations through proposing a novel distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL unleashes the full potential of Open RAN (O-RAN) systems and introduces an elastic resource provisioning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21452;&#37325;&#20351;&#29992;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20221;&#23450;&#21046;&#30340;&#21452;&#37325;&#20351;&#29992;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#29366;&#20917;&#21644;&#21487;&#33021;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.08315</link><description>&lt;p&gt;
&#33606;&#26840;&#29611;&#29808;&#65306;&#25506;&#31350;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#30340;&#21452;&#37325;&#20351;&#29992;&#22256;&#22659;
&lt;/p&gt;
&lt;p&gt;
Thorny Roses: Investigating the Dual Use Dilemma in Natural Language Processing. (arXiv:2304.08315v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.08315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35843;&#26597;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#30340;&#21452;&#37325;&#20351;&#29992;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20221;&#23450;&#21046;&#30340;&#21452;&#37325;&#20351;&#29992;&#23450;&#20041;&#65292;&#24182;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#29366;&#20917;&#21644;&#21487;&#33021;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21452;&#37325;&#20351;&#29992;&#26159;&#25351;&#26377;&#24847;&#23558;&#25216;&#26415;&#21644;&#31185;&#23398;&#25104;&#26524;&#29992;&#20110;&#26377;&#23475;&#30446;&#30340;&#30340;&#38382;&#39064;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#23578;&#26410;&#26126;&#30830;&#23450;&#20041;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;NLP&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#21644;&#22312;&#31038;&#20250;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#20854;&#20869;&#37096;&#36816;&#34892;&#26041;&#24335;&#21464;&#24471;&#36234;&#26469;&#36234;&#19981;&#36879;&#26126;&#12290;&#22240;&#27492;&#65292;&#29702;&#35299;&#21452;&#37325;&#20351;&#29992;&#30340;&#38382;&#39064;&#20197;&#21450;&#38480;&#21046;&#21452;&#37325;&#20351;&#29992;&#30340;&#28508;&#22312;&#26041;&#27861;&#23545;&#20110;&#20943;&#23569;&#30740;&#31350;&#21644;&#24320;&#21457;&#30340;&#28508;&#22312;&#21361;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;NLP&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#32773;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#23545;&#35813;&#38382;&#39064;&#30340;&#28145;&#24230;&#29702;&#35299;&#21644;&#35266;&#28857;&#65292;&#24182;&#35780;&#20272;&#29616;&#26377;&#30340;&#25903;&#25345;&#24773;&#20917;&#12290;&#26681;&#25454;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#20026;NLP&#31038;&#21306;&#25552;&#20379;&#20102;&#19968;&#20221;&#23450;&#21046;&#30340;&#21452;&#37325;&#20351;&#29992;&#23450;&#20041;&#12290;&#35843;&#26597;&#32467;&#26524;&#26174;&#31034;&#65292;&#22823;&#22810;&#25968;&#30740;&#31350;&#20154;&#21592;&#23545;&#20182;&#20204;&#30340;&#30740;&#31350;&#30340;&#28508;&#22312;&#21452;&#37325;&#20351;&#29992;&#38382;&#39064;&#34920;&#31034;&#20851;&#20999;&#65292;&#20294;&#21482;&#37319;&#21462;&#26377;&#38480;&#30340;&#34892;&#21160;&#12290;&#22522;&#20110;&#35843;&#26597;&#32467;&#26524;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#24403;&#21069;&#30340;&#29366;&#20917;&#21644;&#21487;&#33021;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dual use, the intentional, harmful reuse of technology and scientific artefacts, is a problem yet to be well-defined within the context of Natural Language Processing (NLP). However, as NLP technologies continue to advance and become increasingly widespread in society, their inner workings have become increasingly opaque. Therefore, understanding dual use concerns and potential ways of limiting them is critical to minimising the potential harms of research and development. In this paper, we conduct a survey of NLP researchers and practitioners to understand the depth and their perspective of the problem as well as to assess existing available support. Based on the results of our survey, we offer a definition of dual use that is tailored to the needs of the NLP community. The survey revealed that a majority of researchers are concerned about the potential dual use of their research but only take limited action toward it. In light of the survey results, we discuss the current state and p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;</title><link>http://arxiv.org/abs/2304.07056</link><description>&lt;p&gt;
&#38754;&#37096;&#35270;&#39057;&#21387;&#32553;&#30340;&#24863;&#30693;&#36136;&#37327;&#35780;&#20272;&#65306;&#22522;&#20934;&#21644;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Perceptual Quality Assessment of Face Video Compression: A Benchmark and An Effective Method. (arXiv:2304.07056v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.07056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#29992;&#20110;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#38754;&#37096;&#35270;&#39057;&#21387;&#32553;&#30340;&#38656;&#27714;&#21576;&#25351;&#25968;&#32423;&#22686;&#38271;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#25104;&#21151;&#20351;&#24471;&#36229;&#20986;&#20102;&#20256;&#32479;&#30340;&#28151;&#21512;&#35270;&#39057;&#32534;&#30721;&#33539;&#22260;&#12290;&#29983;&#25104;&#24335;&#32534;&#30721;&#26041;&#27861;&#34987;&#30830;&#23450;&#20026;&#20855;&#26377;&#21512;&#29702;&#30340;&#24863;&#30693;&#30721;&#29575;&#22833;&#30495;&#25240;&#34935;&#30340;&#26377;&#21069;&#36884;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#21033;&#29992;&#38754;&#37096;&#35270;&#39057;&#30340;&#32479;&#35745;&#20808;&#39564;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#31354;&#38388;&#21644;&#26102;&#38388;&#22495;&#20013;&#25197;&#26354;&#31867;&#22411;&#30340;&#26497;&#22823;&#22810;&#26679;&#24615;&#65292;&#20174;&#20256;&#32479;&#30340;&#28151;&#21512;&#32534;&#30721;&#26694;&#26550;&#21040;&#29983;&#25104;&#27169;&#22411;&#65292;&#32473;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;VQA&#65289;&#24102;&#26469;&#20102;&#24040;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22823;&#35268;&#27169;&#21387;&#32553;&#38754;&#37096;&#35270;&#39057;&#36136;&#37327;&#35780;&#20272;&#65288;CFVQA&#65289;&#25968;&#25454;&#24211;&#65292;&#36825;&#26159;&#31995;&#32479;&#22320;&#20102;&#35299;&#38754;&#37096;&#35270;&#39057;&#24863;&#30693;&#36136;&#37327;&#21644;&#22810;&#26679;&#21270;&#21387;&#32553;&#22833;&#30495;&#30340;&#31532;&#19968;&#27425;&#23581;&#35797;&#12290;&#35813;&#25968;&#25454;&#24211;&#21253;&#21547; 3,240 &#20010;&#21387;&#32553;&#30340;&#38754;&#37096;&#35270;&#39057;&#29255;&#27573;&#65292;&#28085;&#30422;&#22810;&#20010;&#21387;&#32553;&#32423;&#21035;&#65292;&#36825;&#20123;&#29255;&#27573;&#26469;&#33258; 135 &#20010;&#28304;&#35270;&#39057;&#65292;&#20855;&#26377;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed an exponential increase in the demand for face video compression, and the success of artificial intelligence has expanded the boundaries beyond traditional hybrid video coding. Generative coding approaches have been identified as promising alternatives with reasonable perceptual rate-distortion trade-offs, leveraging the statistical priors of face videos. However, the great diversity of distortion types in spatial and temporal domains, ranging from the traditional hybrid coding frameworks to generative models, present grand challenges in compressed face video quality assessment (VQA). In this paper, we introduce the large-scale Compressed Face Video Quality Assessment (CFVQA) database, which is the first attempt to systematically understand the perceptual quality and diversified compression distortions in face videos. The database contains 3,240 compressed face video clips in multiple compression levels, which are derived from 135 source videos with diversif
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.03271</link><description>&lt;p&gt;
&#20351;AI&#8220;&#21475;&#28212;&#8221;&#20943;&#23569;&#30340;&#26041;&#27861;&#65306;&#25581;&#31034;&#21644;&#35299;&#20915;AI&#27169;&#22411;&#30340;&#31192;&#23494;&#27700;&#28040;&#32791;
&lt;/p&gt;
&lt;p&gt;
Making AI Less "Thirsty": Uncovering and Addressing the Secret Water Footprint of AI Models. (arXiv:2304.03271v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.03271
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25581;&#31034;&#20197;&#21450;&#25552;&#20986;&#20102;&#35299;&#20915;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#24040;&#22823;&#27700;&#36275;&#36857;&#30340;&#26041;&#27861;&#65292;&#22240;&#20026;&#20854;&#28129;&#27700;&#28040;&#32791;&#24050;&#32463;&#24341;&#36215;&#22269;&#38469;&#31038;&#20250;&#30340;&#37325;&#35270;&#65292;&#24182;&#19988;AI&#27169;&#22411;&#24212;&#35813;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20570;&#20986;&#38754;&#23545;&#27700;&#21361;&#26426;&#30340;&#34920;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#27169;&#22411;&#30340;&#30899;&#36275;&#36857;&#19981;&#26029;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#20687;GPT-3&#21644;GPT-4&#36825;&#26679;&#30340;&#22823;&#22411;&#27169;&#22411;&#65292;&#24050;&#32463;&#21463;&#21040;&#20844;&#20247;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#21516;&#31561;&#37325;&#35201;&#19988;&#24040;&#22823;&#30340;AI&#27169;&#22411;&#27700;&#21360;&#23578;&#26410;&#24341;&#36215;&#20154;&#20204;&#30340;&#27880;&#24847;&#12290;&#20363;&#22914;&#65292;&#22312;&#24494;&#36719;&#26368;&#20808;&#36827;&#30340;&#32654;&#22269;&#25968;&#25454;&#20013;&#24515;&#20013;&#35757;&#32451;GPT-3&#21487;&#20197;&#30452;&#25509;&#28040;&#32791;70&#19975;&#21319;&#28165;&#27905;&#28129;&#27700;&#65288;&#30456;&#24403;&#20110;&#29983;&#20135;370&#36742;&#23453;&#39532;&#27773;&#36710;&#25110;320&#36742;&#29305;&#26031;&#25289;&#30005;&#21160;&#27773;&#36710;&#65289;&#65292;&#22914;&#26524;&#22312;&#24494;&#36719;&#30340;&#20122;&#27954;&#25968;&#25454;&#20013;&#24515;&#36827;&#34892;&#35757;&#32451;&#65292;&#36825;&#20010;&#27700;&#28040;&#32791;&#37327;&#23558;&#22686;&#21152;&#19977;&#20493;&#65292;&#20294;&#36825;&#26679;&#30340;&#20449;&#24687;&#19968;&#30452;&#34987;&#20445;&#23494;&#12290;&#36825;&#26497;&#20854;&#20196;&#20154;&#25285;&#24551;&#65292;&#22240;&#20026;&#28129;&#27700;&#30701;&#32570;&#24050;&#25104;&#20026;&#22312;&#20154;&#21475;&#36805;&#36895;&#22686;&#38271;&#12289;&#27700;&#36164;&#28304;&#20943;&#23569;&#21644;&#32769;&#21270;&#30340;&#27700;&#22522;&#30784;&#35774;&#26045;&#30340;&#32972;&#26223;&#19979;&#65292;&#25105;&#20204;&#25152;&#26377;&#20154;&#38754;&#20020;&#30340;&#26368;&#32039;&#36843;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#20026;&#20102;&#24212;&#23545;&#20840;&#29699;&#27700;&#36164;&#28304;&#30340;&#25361;&#25112;&#65292;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#21487;&#20197;&#65292;&#32780;&#19988;&#24212;&#35813;&#65292;&#25215;&#25285;&#31038;&#20250;&#36131;&#20219;&#65292;&#20197;&#36523;&#20316;&#21017;&#35299;&#20915;&#33258;&#24049;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing carbon footprint of artificial intelligence (AI) models, especially large ones such as GPT-3 and GPT-4, has been undergoing public scrutiny. Unfortunately, however, the equally important and enormous water footprint of AI models has remained under the radar. For example, training GPT-3 in Microsoft's state-of-the-art U.S. data centers can directly consume 700,000 liters of clean freshwater (enough for producing 370 BMW cars or 320 Tesla electric vehicles) and the water consumption would have been tripled if training were done in Microsoft's Asian data centers, but such information has been kept as a secret. This is extremely concerning, as freshwater scarcity has become one of the most pressing challenges shared by all of us in the wake of the rapidly growing population, depleting water resources, and aging water infrastructures. To respond to the global water challenges, AI models can, and also should, take social responsibility and lead by example by addressing their own 
&lt;/p&gt;</description></item><item><title>MEGClass&#26159;&#19968;&#31181;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#23454;&#29616;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#23618;&#20851;&#27880;&#26426;&#21046;&#20174;&#25991;&#26723;&#12289;&#21477;&#23376;&#21644;&#21333;&#35789;&#20013;&#25552;&#21462;&#36830;&#36143;&#19988;&#22810;&#26679;&#30340;&#23376;&#25991;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#30340;&#25991;&#26723;&#65292;&#24182;&#19988;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01969</link><description>&lt;p&gt;
MEGClass: &#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#23454;&#29616;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
MEGClass: Text Classification with Extremely Weak Supervision via Mutually-Enhancing Text Granularities. (arXiv:2304.01969v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01969
&lt;/p&gt;
&lt;p&gt;
MEGClass&#26159;&#19968;&#31181;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#23454;&#29616;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#23618;&#20851;&#27880;&#26426;&#21046;&#20174;&#25991;&#26723;&#12289;&#21477;&#23376;&#21644;&#21333;&#35789;&#20013;&#25552;&#21462;&#36830;&#36143;&#19988;&#22810;&#26679;&#30340;&#23376;&#25991;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#30340;&#25991;&#26723;&#65292;&#24182;&#19988;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#36825;&#22312;&#21160;&#24577;&#26032;&#20852;&#39046;&#22495;&#20013;&#26159;&#26114;&#36149;&#30340;&#12290;&#26576;&#20123;&#26041;&#27861;&#36890;&#36807;&#20165;&#20381;&#36182;&#31867;&#21517;&#34920;&#38754;&#25991;&#26412;&#20316;&#20026;&#26497;&#24369;&#30417;&#30563;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#32771;&#34385;&#21040;&#21333;&#19968;&#31867;&#21035;&#25991;&#26723;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#30340;&#24773;&#20917;&#12290;&#20027;&#39064;&#22810;&#26679;&#24615;&#21644;&#27169;&#31946;&#30340;&#21477;&#23376;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#21040;&#25991;&#26723;&#30340;&#24213;&#23618;&#34920;&#31034;&#65292;&#20174;&#32780;&#24433;&#21709;&#39044;&#27979;&#31867;&#21035;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#29420;&#31435;&#22320;&#20851;&#27880;&#25991;&#26723;&#12289;&#21477;&#23376;&#25110;&#21333;&#35789;&#30340;&#25991;&#26412;&#31890;&#24230;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#25105;&#20204;&#32852;&#21512;&#20174;&#25152;&#26377;&#19977;&#32773;&#20013;&#25552;&#21462;&#31895;&#31890;&#24230;&#25110;&#32454;&#31890;&#24230;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#26469;&#35782;&#21035;&#20998;&#31867;&#30340;&#37325;&#35201;&#23376;&#25991;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MEGClass&#65292;&#19968;&#31181;&#21033;&#29992;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#36827;&#34892;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MEGClass&#36890;&#36807;&#20998;&#23618;&#20851;&#27880;&#26426;&#21046;&#20174;&#25991;&#26723;&#12289;&#21477;&#23376;&#21644;&#21333;&#35789;&#20013;&#25552;&#21462;&#36830;&#36143;&#19988;&#22810;&#26679;&#30340;&#23376;&#25991;&#26412;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#21644;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#31890;&#24230;&#30340;&#24369;&#20449;&#21495;&#65292;&#20197;&#20934;&#30830;&#20998;&#31867;&#25991;&#26723;&#65292;&#21363;&#20351;&#23427;&#20204;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#12290;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#26497;&#24369;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26377;&#30528;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification typically requires a substantial amount of human-annotated data to serve as supervision, which is costly to obtain in dynamic emerging domains. Certain methods seek to address this problem by solely relying on the surface text of class names to serve as extremely weak supervision. However, existing methods fail to account for single-class documents discussing multiple topics. Both topic diversity and vague sentences may introduce noise into the document's underlying representation and consequently the precision of the predicted class. Furthermore, current work focuses on text granularities (documents, sentences, or words) independently, which limits the degree of coarse- or fine-grained context that we can jointly extract from all three to identify significant subtext for classification. In order to address this problem, we propose MEGClass, an extremely weakly-supervised text classification method to exploit Mutually-Enhancing Text Granularities. Specifically, MEGC
&lt;/p&gt;</description></item><item><title>Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;</title><link>http://arxiv.org/abs/2303.17503</link><description>&lt;p&gt;
Pgx:&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#30340;&#24182;&#34892;&#28216;&#25103;&#27169;&#25311;&#22120;
&lt;/p&gt;
&lt;p&gt;
Pgx: Hardware-accelerated parallel game simulation for reinforcement learning. (arXiv:2303.17503v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17503
&lt;/p&gt;
&lt;p&gt;
Pgx&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#65292;&#20855;&#26377;&#24378;&#21270;&#23398;&#20064;&#30828;&#20214;&#21152;&#36895;&#33021;&#21147;&#65292;&#25903;&#25345;&#24182;&#34892;&#25191;&#34892;&#65292;&#36895;&#24230;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290; &#23427;&#23454;&#29616;&#20102;Backgammon&#65292;Shogi&#21644;Go&#31561;&#22522;&#20934;&#27979;&#35797;&#28216;&#25103;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Pgx&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;JAX&#32534;&#20889;&#30340;&#26827;&#30424;&#28216;&#25103;&#27169;&#25311;&#22120;&#38598;&#21512;&#12290;&#30001;&#20110;JAX&#30340;&#33258;&#21160;&#21521;&#37327;&#21270;&#21644;&#21363;&#26102;&#32534;&#35793;&#21151;&#33021;&#65292;Pgx&#26131;&#20110;&#22312;GPU/TPU&#21152;&#36895;&#22120;&#19978;&#36827;&#34892;&#22823;&#35268;&#27169;&#24182;&#34892;&#25191;&#34892;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#22312;&#21333;&#20010;A100 GPU&#19978;&#30340;Pgx&#27169;&#25311;&#27604;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#24211;&#24555;10&#20493;&#12290;Pgx&#23454;&#29616;&#20102;&#34987;&#35748;&#20026;&#26159;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20013;&#33267;&#20851;&#37325;&#35201;&#30340;&#22522;&#20934;&#27979;&#35797;&#30340;&#28216;&#25103;&#65292;&#22914;Backgammon&#65292;Shogi&#21644;Go&#12290; Pgx&#21487;&#22312;https://github.com/sotetsuk/pgx&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose Pgx, a collection of board game simulators written in JAX. Thanks to auto-vectorization and Just-In-Time compilation of JAX, Pgx scales easily to thousands of parallel execution on GPU/TPU accelerators. We found that the simulation of Pgx on a single A100 GPU is 10x faster than that of existing reinforcement learning libraries. Pgx implements games considered vital benchmarks in artificial intelligence research, such as Backgammon, Shogi, and Go. Pgx is available at https://github.com/sotetsuk/pgx.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#30418;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#21033;&#29992;&#38646;&#26679;&#26412;&#22270;&#20687;&#20928;&#21270;&#26377;&#25928;&#22320;&#38450;&#24481;&#21508;&#31181;&#25915;&#20987;&#65292;&#26080;&#38656;&#20219;&#20309;&#20869;&#37096;&#20449;&#24687;&#25110;&#20808;&#21069;&#30693;&#35782;&#65292;&#36890;&#36807;&#23545;&#21463;&#27745;&#26579;&#30340;&#22270;&#20687;&#36827;&#34892;&#32447;&#24615;&#21464;&#25442;&#21644;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#24674;&#22797;&#32570;&#22833;&#35821;&#20041;&#20449;&#24687;&#23454;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.12175</link><description>&lt;p&gt;
&#38646;&#26679;&#26412;&#22270;&#20687;&#20928;&#21270;&#30340;&#40657;&#30418;&#21518;&#38376;&#38450;&#24481;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Black-box Backdoor Defense via Zero-shot Image Purification. (arXiv:2303.12175v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.12175
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40657;&#30418;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#21033;&#29992;&#38646;&#26679;&#26412;&#22270;&#20687;&#20928;&#21270;&#26377;&#25928;&#22320;&#38450;&#24481;&#21508;&#31181;&#25915;&#20987;&#65292;&#26080;&#38656;&#20219;&#20309;&#20869;&#37096;&#20449;&#24687;&#25110;&#20808;&#21069;&#30693;&#35782;&#65292;&#36890;&#36807;&#23545;&#21463;&#27745;&#26579;&#30340;&#22270;&#20687;&#36827;&#34892;&#32447;&#24615;&#21464;&#25442;&#21644;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#24674;&#22797;&#32570;&#22833;&#35821;&#20041;&#20449;&#24687;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21518;&#38376;&#25915;&#20987;&#20250;&#23558;&#27602;&#25968;&#25454;&#27880;&#20837;&#35757;&#32451;&#38598;&#65292;&#23548;&#33268;&#27169;&#22411;&#25512;&#29702;&#26102;&#26679;&#26412;&#38169;&#35823;&#20998;&#31867;&#12290;&#22312;&#20165;&#26377;&#27169;&#22411;&#39044;&#27979;&#21487;&#29992;&#30340;&#23454;&#38469;&#40657;&#30418;&#29615;&#22659;&#20013;&#65292;&#38450;&#24481;&#27492;&#31867;&#25915;&#20987;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21518;&#38376;&#38450;&#24481;&#26694;&#26550;&#65292;&#21487;&#20197;&#36890;&#36807;&#38646;&#26679;&#26412;&#22270;&#20687;&#20928;&#21270;&#65288;ZIP&#65289;&#26377;&#25928;&#22320;&#25269;&#24481;&#21508;&#31181;&#25915;&#20987;&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#26694;&#26550;&#21487;&#20197;&#24212;&#29992;&#20110;&#40657;&#30418;&#27169;&#22411;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#20851;&#20110;&#21463;&#27745;&#26579;&#27169;&#22411;&#30340;&#20869;&#37096;&#20449;&#24687;&#25110;&#20219;&#20309;&#20851;&#20110;&#24178;&#20928;/&#21463;&#27745;&#26579;&#26679;&#26412;&#30340;&#20808;&#21069;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#38450;&#24481;&#26694;&#26550;&#21253;&#25324;&#20004;&#20010;&#27493;&#39588;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23545;&#21463;&#27745;&#26579;&#30340;&#22270;&#20687;&#24212;&#29992;&#32447;&#24615;&#21464;&#25442;&#20197;&#30772;&#22351;&#35302;&#21457;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#24674;&#22797;&#30001;&#21464;&#25442;&#21435;&#38500;&#30340;&#32570;&#22833;&#35821;&#20041;&#20449;&#24687;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#26032;&#30340;&#21453;&#21521;&#36807;&#31243;&#65292;&#20351;&#29992;&#21464;&#25442;&#21518;&#30340;&#22270;&#20687;&#26469;&#24341;&#23548;&#39640;&#20445;&#30495;&#24230;&#20928;&#21270;&#22270;&#20687;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Backdoor attacks inject poisoned data into the training set, resulting in misclassification of the poisoned samples during model inference. Defending against such attacks is challenging, especially in real-world black-box settings where only model predictions are available. In this paper, we propose a novel backdoor defense framework that can effectively defend against various attacks through zero-shot image purification (ZIP). Our proposed framework can be applied to black-box models without requiring any internal information about the poisoned model or any prior knowledge of the clean/poisoned samples. Our defense framework involves a two-step process. First, we apply a linear transformation on the poisoned image to destroy the trigger pattern. Then, we use a pre-trained diffusion model to recover the missing semantic information removed by the transformation. In particular, we design a new reverse process using the transformed image to guide the generation of high-fidelity purified 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09716</link><description>&lt;p&gt;
&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#20013;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#25919;&#31574;&#36845;&#20195;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
A New Policy Iteration Algorithm For Reinforcement Learning in Zero-Sum Markov Games. (arXiv:2303.09716v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#22522;&#20110;&#27169;&#22411;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#21487;&#20197;&#34987;&#35270;&#20026;&#20855;&#26377;&#20004;&#20010;&#38454;&#27573;: &#23398;&#20064;&#38454;&#27573;&#21644;&#35268;&#21010;&#38454;&#27573;&#12290;&#22312;&#26631;&#20934;MDPs&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#20351;&#29992;&#20215;&#20540;&#36845;&#20195;&#25110;&#31574;&#30053;&#36845;&#20195;&#26469;&#35299;&#20915;&#23398;&#20064;&#38382;&#39064;&#12290;&#20294;&#22312;&#38646;&#21644;&#39532;&#23572;&#21487;&#22827;&#21338;&#24328;&#30340;&#24773;&#20917;&#19979;&#65292;&#27809;&#26377;&#26377;&#25928;&#30340;&#31574;&#30053;&#36845;&#20195;&#31639;&#27861;&#65292;&#20197;&#21069;&#30340;&#23581;&#35797;&#37117;&#26377;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#31574;&#30053;&#36845;&#20195;&#21464;&#20307;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many model-based reinforcement learning (RL) algorithms can be viewed as having two phases that are iteratively implemented: a learning phase where the model is approximately learned and a planning phase where the learned model is used to derive a policy. In the case of standard MDPs, the learning problem can be solved using either value iteration or policy iteration. However, in the case of zero-sum Markov games, there is no efficient policy iteration algorithm; e.g., it has been shown in Hansen et al. (2013) that one has to solve Omega(1/(1-alpha)) MDPs, where alpha is the discount factor, to implement the only known convergent version of policy iteration. Another algorithm for Markov zero-sum games, called naive policy iteration, is easy to implement but is only provably convergent under very restrictive assumptions. Prior attempts to fix naive policy iteration algorithm have several limitations. Here, we show that a simple variant of naive policy iteration for games converges, and 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.06833</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Planning for Symbolic Regression. (arXiv:2303.06833v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06833
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#21644;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;TPSR&#65292;&#21487;&#20197;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#25552;&#39640;&#26041;&#31243;&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31526;&#21495;&#22238;&#24402;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#23427;&#28041;&#21450;&#22522;&#20110;&#20989;&#25968;&#20540;&#26597;&#25214;&#20854;&#25968;&#23398;&#34920;&#36798;&#24335;&#12290;&#26368;&#36817;&#65292;&#31526;&#21495;&#22238;&#24402;&#30340;&#19968;&#20123;&#36827;&#23637;&#34920;&#26126;&#65292;&#39044;&#35757;&#32451;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#23545;&#20110;&#29983;&#25104;&#26041;&#31243;&#24207;&#21015;&#26159;&#26377;&#25928;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#20174;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#33719;&#30410;&#65292;&#24182;&#22312;&#25512;&#29702;&#26102;&#38388;&#26041;&#38754;&#27604;&#22522;&#20110;GP&#30340;&#26041;&#27861;&#20855;&#26377;&#26174;&#33879;&#20248;&#21183;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#20851;&#27880;&#30340;&#26159;&#20511;&#37492;&#25991;&#26412;&#29983;&#25104;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#30446;&#26631;&#65292;&#32780;&#24573;&#30053;&#20102;&#26041;&#31243;&#30340;&#29305;&#23450;&#30446;&#26631;&#65292;&#22914;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TPSR&#65292;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#31526;&#21495;&#22238;&#24402;&#35268;&#21010;&#31574;&#30053;&#65292;&#23558;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#34701;&#20837;&#21040;Transformer&#35299;&#30721;&#36807;&#31243;&#20013;&#12290;&#19982;&#20256;&#32479;&#30340;&#35299;&#30721;&#31574;&#30053;&#19981;&#21516;&#65292;TPSR&#20801;&#35768;&#23558;&#38750;&#21487;&#24494;&#30340;&#21453;&#39304;&#65288;&#22914;&#25311;&#21512;&#20934;&#30830;&#24615;&#21644;&#22797;&#26434;&#24615;&#65289;&#20316;&#20026;&#30693;&#35782;&#30340;&#22806;&#37096;&#26469;&#28304;&#34701;&#20837;&#21040;&#26041;&#31243;&#29983;&#25104;&#36807;&#31243;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symbolic regression (SR) is a challenging task in machine learning that involves finding a mathematical expression for a function based on its values. Recent advancements in SR have demonstrated the efficacy of pretrained transformer-based models for generating equations as sequences, which benefit from large-scale pretraining on synthetic datasets and offer considerable advantages over GP-based methods in terms of inference time. However, these models focus on supervised pretraining goals borrowed from text generation and ignore equation-specific objectives like accuracy and complexity. To address this, we propose TPSR, a Transformer-based Planning strategy for Symbolic Regression that incorporates Monte Carlo Tree Search into the transformer decoding process. TPSR, as opposed to conventional decoding strategies, allows for the integration of non-differentiable feedback, such as fitting accuracy and complexity, as external sources of knowledge into the equation generation process. Ext
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Cal-QL&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#24494;&#35843;&#26102;&#21516;&#26102;&#20445;&#38556;&#20102;&#24555;&#36895;&#21644;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.05479</link><description>&lt;p&gt;
Cal-QL: &#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#39044;&#20808;&#35757;&#32451;&#29992;&#20110;&#39640;&#25928;&#22312;&#32447;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Cal-QL: Calibrated Offline RL Pre-Training for Efficient Online Fine-Tuning. (arXiv:2303.05479v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.05479
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#35745;&#31639;&#26426;&#36741;&#21161;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;Cal-QL&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20351;&#24471;&#22312;&#22312;&#32447;&#24494;&#35843;&#26102;&#21516;&#26102;&#20445;&#38556;&#20102;&#24555;&#36895;&#21644;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#21487;&#20197;&#29992;&#26469;&#20174;&#29616;&#26377;&#25968;&#25454;&#38598;&#20013;&#33719;&#21462;&#31574;&#30053;&#21021;&#22987;&#21270;&#24182;&#36890;&#36807;&#26377;&#38480;&#20114;&#21160;&#36827;&#34892;&#24555;&#36895;&#22312;&#32447;&#24494;&#35843;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#22312;&#32447;&#24494;&#35843;&#20013;&#34920;&#29616;&#36739;&#24046;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20445;&#23432;&#33073;&#26426;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#20013;&#30340;&#24494;&#35843;&#38382;&#39064;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#20064;&#21040;&#26377;&#25928;&#30340;&#21021;&#22987;&#21270;&#65292;&#24182;&#20351;&#20854;&#20855;&#22791;&#24555;&#36895;&#30340;&#22312;&#32447;&#24494;&#35843;&#21151;&#33021;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#21363;Cal-QL&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#20010;&#20445;&#23432;&#30340;&#20540;&#20989;&#25968;&#21021;&#22987;&#21270;&#65292;&#20302;&#20272;&#20174;&#33073;&#26426;&#25968;&#25454;&#20013;&#23398;&#21040;&#30340;&#31574;&#30053;&#30340;&#20215;&#20540;&#65292;&#21516;&#26102;&#30830;&#20445;&#23398;&#20064;&#21040;&#30340;Q&#20540;&#22312;&#21512;&#29702;&#30340;&#33539;&#22260;&#20869;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20010;&#22522;&#20934;&#29615;&#22659;&#20013;&#20855;&#26377;&#26174;&#33879;&#30340;&#24615;&#33021;&#20248;&#21183;&#65292;&#24182;&#19988;&#20063;&#33021;&#22312;&#30495;&#23454;&#26426;&#22120;&#20154;&#38382;&#39064;&#20013;&#36827;&#34892;&#26377;&#25928;&#30340;&#22312;&#32447;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
A compelling use case of offline reinforcement learning (RL) is to obtain a policy initialization from existing datasets followed by fast online fine-tuning with limited interaction. However, existing offline RL methods tend to behave poorly during fine-tuning. In this paper, we study the fine-tuning problem in the context of conservative offline RL methods and we devise an approach for learning an effective initialization from offline data that also enables fast online fine-tuning capabilities. Our approach, calibrated Q-learning (Cal-QL), accomplishes this by learning a conservative value function initialization that underestimates the value of the learned policy from offline data, while also ensuring that the learned Q-values are at a reasonable scale. We refer to this property as calibration, and define it formally as providing a lower bound on the true value function of the learned policy and an upper bound on the value of some other (suboptimal) reference policy, which may simply
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23637;&#31034;&#20102;&#21033;&#29992;&#19981;&#23545;&#31216;&#24615;&#36827;&#34892;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#32467;&#26500;&#21270;&#36755;&#20986;&#38382;&#39064;&#20135;&#29983;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#22312;&#23553;&#38381;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21512;&#25104;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;180&#19975;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#27492;&#25968;&#25454;&#38598;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#36828;&#36229;&#20808;&#21069;&#39046;&#20808;&#25216;&#26415;&#30340;&#25104;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.04132</link><description>&lt;p&gt;
&#21033;&#29992;&#19981;&#23545;&#31216;&#24615;&#36827;&#34892;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#65306;SynthIE&#21644;&#20449;&#24687;&#25552;&#21462;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Exploiting Asymmetry for Synthetic Training Data Generation: SynthIE and the Case of Information Extraction. (arXiv:2303.04132v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23637;&#31034;&#20102;&#21033;&#29992;&#19981;&#23545;&#31216;&#24615;&#36827;&#34892;&#21512;&#25104;&#35757;&#32451;&#25968;&#25454;&#29983;&#25104;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#38024;&#23545;&#32467;&#26500;&#21270;&#36755;&#20986;&#38382;&#39064;&#20135;&#29983;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#12290;&#22312;&#23553;&#38381;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#21512;&#25104;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;180&#19975;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#21033;&#29992;&#27492;&#25968;&#25454;&#38598;&#23545;&#23567;&#22411;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#21462;&#24471;&#20102;&#36828;&#36229;&#20808;&#21069;&#39046;&#20808;&#25216;&#26415;&#30340;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26041;&#38754;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#36825;&#39033;&#24037;&#20316;&#34920;&#26126;&#65292;&#21363;&#20351;&#23545;&#20110;LLM&#26080;&#27861;&#30452;&#25509;&#35299;&#20915;&#30340;&#20219;&#21153;&#65292;&#20063;&#21487;&#20197;&#21512;&#25104;&#29983;&#25104;&#26377;&#29992;&#30340;&#25968;&#25454;&#65306;&#23545;&#20110;&#20855;&#26377;&#32467;&#26500;&#21270;&#36755;&#20986;&#30340;&#38382;&#39064;&#65292;&#21487;&#20197;&#25552;&#31034;LLM&#22312;&#21453;&#21521;&#26041;&#21521;&#19978;&#25191;&#34892;&#20219;&#21153;&#65292;&#36890;&#36807;&#20026;&#30446;&#26631;&#36755;&#20986;&#32467;&#26500;&#29983;&#25104;&#21512;&#29702;&#30340;&#36755;&#20837;&#25991;&#26412;&#12290;&#21033;&#29992;&#20219;&#21153;&#22256;&#38590;&#24230;&#30340;&#19981;&#23545;&#31216;&#24615;&#65292;&#21487;&#20197;&#29983;&#25104;&#22823;&#35268;&#27169;&#12289;&#39640;&#36136;&#37327;&#30340;&#22797;&#26434;&#20219;&#21153;&#25968;&#25454;&#12290;&#25105;&#20204;&#22312;&#23553;&#38381;&#20449;&#24687;&#25552;&#21462;&#26041;&#38754;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#35813;&#39046;&#22495;&#38590;&#20197;&#25910;&#38598;&#21040;&#30495;&#23454;&#25968;&#25454;&#65292;&#33267;&#20170;&#27809;&#26377;&#20196;&#20154;&#28385;&#24847;&#30340;&#25968;&#25454;&#38598;&#23384;&#22312;&#12290;&#25105;&#20204;&#21512;&#25104;&#29983;&#25104;&#20102;&#19968;&#20010;&#21253;&#21547;180&#19975;&#25968;&#25454;&#28857;&#30340;&#25968;&#25454;&#38598;&#65292;&#24182;&#22312;&#20154;&#24037;&#35780;&#20272;&#20013;&#35777;&#26126;&#20854;&#19982;&#29616;&#26377;&#25968;&#25454;&#38598;&#30456;&#27604;&#20855;&#26377;&#26356;&#22909;&#30340;&#36136;&#37327;&#65292;&#24182;&#21033;&#29992;&#35813;&#25968;&#25454;&#38598;&#23545;&#23567;&#22411;&#27169;&#22411;&#65288;220M&#21644;770M&#21442;&#25968;&#65289;&#36827;&#34892;&#24494;&#35843;&#65292;&#36825;&#20123;&#27169;&#22411;&#34987;&#31216;&#20026;SynthIE&#65292;&#20197;&#36828;&#36828;&#36229;&#36807;&#20808;&#21069;&#39046;&#20808;&#25216;&#26415;&#30340;&#27700;&#24179;&#65288;&#20855;&#26377;&#30456;&#21516;&#30340;&#27169;&#22411;&#22823;&#23567;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have great potential for synthetic data generation. This work shows that useful data can be synthetically generated even for tasks that cannot be solved directly by LLMs: for problems with structured outputs, it is possible to prompt an LLM to perform the task in the reverse direction, by generating plausible input text for a target output structure. Leveraging this asymmetry in task difficulty makes it possible to produce large-scale, high-quality data for complex tasks. We demonstrate the effectiveness of this approach on closed information extraction, where collecting ground-truth data is challenging, and no satisfactory dataset exists to date. We synthetically generate a dataset of 1.8M data points, establish its superior quality compared to existing datasets in a human evaluation, and use it to finetune small models (220M and 770M parameters), termed SynthIE, that outperform the prior state of the art (with equal model size) by a substantial margin of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.02265</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learning to Influence Human Behavior with Offline Reinforcement Learning. (arXiv:2303.02265v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02265
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#23398;&#20064;&#24433;&#21709;&#20154;&#31867;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a method of learning to influence human behavior through offline reinforcement learning, which can improve human performance in collaborative tasks.
&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#19982;&#20154;&#31867;&#20114;&#21160;&#26159;&#26368;&#22797;&#26434;&#30340;&#35774;&#32622;&#20043;&#19968;&#65292;&#22240;&#20026;&#20154;&#31867;&#24448;&#24448;&#30001;&#20110;&#22797;&#26434;&#30340;&#20559;&#35265;&#32780;&#34920;&#29616;&#20986;&#27425;&#20248;&#30340;&#12289;&#19981;&#21487;&#39044;&#27979;&#30340;&#34892;&#20026;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#19982;&#20154;&#31867;&#20114;&#21160;&#30340;&#20195;&#29702;&#26368;&#32456;&#20250;&#24433;&#21709;&#36825;&#20123;&#20154;&#25152;&#37319;&#21462;&#30340;&#34892;&#21160;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20351;&#20195;&#29702;&#33021;&#22815;&#21033;&#29992;&#36825;&#31181;&#24433;&#21709;&#26469;&#25552;&#39640;&#20154;&#31867;&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#38543;&#30528;&#20219;&#21153;&#30340;&#23637;&#24320;&#12290;&#19982;&#20197;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;&#25105;&#20204;&#19981;&#20551;&#35774;&#19982;&#20154;&#21592;&#36827;&#34892;&#22312;&#32447;&#22521;&#35757;&#65288;&#36825;&#24448;&#24448;&#22826;&#26114;&#36149;&#21644;&#19981;&#23433;&#20840;&#65289;&#65292;&#20063;&#19981;&#20551;&#35774;&#26377;&#39640;&#20445;&#30495;&#24230;&#29615;&#22659;&#27169;&#25311;&#22120;&#30340;&#35775;&#38382;&#26435;&#38480;&#12290;&#25105;&#20204;&#30340;&#24819;&#27861;&#26159;&#65292;&#36890;&#36807;&#37319;&#29992;&#21508;&#31181;&#20808;&#21069;&#35266;&#23519;&#21040;&#30340;&#20154;&#31867;-&#20154;&#31867;&#20132;&#20114;&#25968;&#25454;&#24182;&#23558;&#20854;&#26631;&#35760;&#20026;&#20219;&#21153;&#22870;&#21169;&#65292;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#21487;&#20197;&#23398;&#20064;&#32452;&#21512;&#34892;&#20026;&#30340;&#32452;&#20214;&#65292;&#24182;&#21457;&#29616;&#23548;&#33268;&#26356;&#29702;&#24819;&#30340;&#20154;&#31867;&#34892;&#20026;&#30340;&#34892;&#21160;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31163;&#32447;RL&#21487;&#20197;&#23398;&#20064;&#31574;&#30053;&#26469;&#24433;&#21709;&#21644;&#25913;&#21892;&#20154;&#31867;&#34892;&#20026;&#65292;&#23613;&#31649;&#36825;&#20123;&#31574;&#30053;&#21487;&#33021;&#19982;&#20154;&#31867;&#30340;&#26399;&#26395;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the real world, some of the most complex settings for learned agents involve interaction with humans, who often exhibit suboptimal, unpredictable behavior due to sophisticated biases. Agents that interact with people in such settings end up influencing the actions that these people take. Our goal in this work is to enable agents to leverage that influence to improve the human's performance in collaborative tasks, as the task unfolds. Unlike prior work, we do not assume online training with people (which tends to be too expensive and unsafe), nor access to a high fidelity simulator of the environment. Our idea is that by taking a variety of previously observed human-human interaction data and labeling it with the task reward, offline reinforcement learning (RL) can learn to combine components of behavior, and uncover actions that lead to more desirable human actions. First, we show that offline RL can learn strategies to influence and improve human behavior, despite those strategies 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHAP-IQ&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20219;&#24847;&#38454;Shapley&#20114;&#21160;&#65292;&#24182;&#25552;&#20379;&#20102;&#36924;&#36817;&#36136;&#37327;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#26041;&#24046;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#36924;&#36817;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.01179</link><description>&lt;p&gt;
SHAP-IQ: &#20219;&#24847;&#38454;Shapley interaction&#30340;&#32479;&#19968;&#36924;&#36817;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
SHAP-IQ: Unified Approximation of any-order Shapley Interactions. (arXiv:2303.01179v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.01179
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SHAP-IQ&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#35745;&#31639;&#20219;&#24847;&#38454;Shapley&#20114;&#21160;&#65292;&#24182;&#25552;&#20379;&#20102;&#36924;&#36817;&#36136;&#37327;&#30340;&#29702;&#35770;&#20445;&#35777;&#21644;&#26041;&#24046;&#20272;&#35745;&#12290;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#36924;&#36817;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30740;&#31350;&#20013;&#65292;Shapley&#20540;&#65288;SV&#65289;&#36890;&#24120;&#34987;&#24212;&#29992;&#20110;&#30830;&#23450;&#20219;&#20309;&#40657;&#30418;&#27169;&#22411;&#30340;&#29305;&#24449;&#37325;&#35201;&#24615;&#24471;&#20998;&#12290; Shapley interaction indices&#23558;SV&#25193;&#23637;&#20026;&#23450;&#20041;&#20219;&#24847;&#38454;&#29305;&#24449;&#30456;&#20114;&#20316;&#29992;&#24471;&#20998;&#12290;&#23450;&#20041;&#29420;&#29305;&#30340;Shapley interaction index&#26159;&#19968;&#20010;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#65292;&#36804;&#20170;&#20026;&#27490;&#24050;&#32463;&#25552;&#20986;&#20102;&#19977;&#20010;&#23450;&#20041;&#65292;&#20854;&#19981;&#21516;&#20043;&#22788;&#22312;&#20110;&#25152;&#36873;&#25321;&#30340;&#20844;&#29702;&#12290;&#27492;&#22806;&#65292;&#27599;&#20010;&#23450;&#20041;&#37117;&#38656;&#35201;&#29305;&#23450;&#30340;&#36924;&#36817;&#25216;&#26415;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#37319;&#26679;&#30340;&#26377;&#25928;&#36924;&#36817;&#26041;&#27861;SHAPley Interaction Quantification&#65288;SHAP-IQ&#65289;&#65292;&#20197;&#35745;&#31639;&#20219;&#24847;&#22522;&#25968;&#20132;&#20114;&#25351;&#25968;&#65288;CII&#65289;&#30340;Shapley&#20114;&#21160;&#12290;&#21363;&#28385;&#36275;&#32447;&#24615;&#12289;&#23545;&#31216;&#21644;&#34394;&#25311;&#20844;&#29702;&#30340;&#20132;&#20114;&#25351;&#25968;&#12290;SHAP-IQ&#22522;&#20110;&#19968;&#31181;&#26032;&#39062;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#20026;&#20854;&#36924;&#36817;&#36136;&#37327;&#25552;&#20379;&#20102;&#29702;&#35770;&#20445;&#35777;&#65292;&#20197;&#21450;&#28857;&#20272;&#35745;&#30340;&#26041;&#24046;&#20272;&#35745;&#12290;&#23545;&#20110;SV&#30340;&#29305;&#27530;&#24773;&#20917;&#65292;&#25105;&#20204;&#30340;&#36924;&#36817;&#26041;&#27861;&#19982;&#31934;&#30830;&#35745;&#31639;&#19968;&#33268;&#12290;&#36827;&#34892;&#20102;&#25968;&#20540;&#23454;&#39564;&#65292;&#20197;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#26174;&#31034;SHAP-IQ&#22312;&#35745;&#31639;&#25104;&#26412;&#21644;&#36924;&#36817;&#36136;&#37327;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predominately in explainable artificial intelligence (XAI) research, the Shapley value (SV) is applied to determine feature importance scores for any black box model. Shapley interaction indices extend the SV to define any-order feature interaction scores. Defining a unique Shapley interaction index is an open research question and, so far, three definitions have been proposed, which differ by their choice of axioms. Moreover, each definition requires a specific approximation technique. Here, we propose SHAPley Interaction Quantification (SHAP-IQ), an efficient sampling-based approximator to compute Shapley interactions for arbitrary cardinal interaction indices (CII), i.e. interaction indices that satisfy the linearity, symmetry and dummy axiom. SHAP-IQ is based on a novel representation and, in contrast to existing methods, we provide theoretical guarantees for its approximation quality, as well as estimates for the variance of the point estimates. For the special case of SV, our app
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;</title><link>http://arxiv.org/abs/2302.12247</link><description>&lt;p&gt;
&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#65306;&#19968;&#31181;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.12247
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#20449;&#24687;&#20998;&#35299;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#37327;&#21270;&#21644;&#24314;&#27169;&#22810;&#27169;&#24577;&#20132;&#20114;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;PID&#32479;&#35745;&#37327;&#26469;&#24230;&#37327;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35299;&#20915;&#22810;&#27169;&#24577;&#20219;&#21153;&#25152;&#38656;&#30340;&#20132;&#20114;&#22914;&#20309;&#36827;&#34892;&#37327;&#21270;&#65311;&#26368;&#36866;&#21512;&#25429;&#25417;&#36825;&#20123;&#20132;&#20114;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#26159;&#20160;&#20040;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20449;&#24687;&#35770;&#26041;&#27861;&#26469;&#37327;&#21270;&#36755;&#20837;&#27169;&#24577;&#19982;&#36755;&#20986;&#20219;&#21153;&#20043;&#38388;&#30340;&#20887;&#20313;&#24230;&#12289;&#29420;&#29305;&#24615;&#21644;&#21327;&#21516;&#24615;&#12290;&#25105;&#20204;&#23558;&#36825;&#19977;&#20010;&#34913;&#37327;&#26631;&#20934;&#31216;&#20026;&#22810;&#27169;&#24577;&#20998;&#24067;&#65288;&#25110;&#31616;&#31216;PID&#65289;&#30340;PID&#32479;&#35745;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#20010;&#26032;&#30340;PID&#32479;&#35745;&#20272;&#35745;&#22120;&#65292;&#36866;&#29992;&#20110;&#39640;&#32500;&#20998;&#24067;&#12290;&#20026;&#20102;&#39564;&#35777;PID&#20272;&#35745;&#65292;&#25105;&#20204;&#22312;&#24050;&#30693;PID&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#21644;&#22823;&#35268;&#27169;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#28151;&#21512;&#19987;&#23478;&#23545;&#35805;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#26032;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65288;MoE-LMs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#38024;&#23545;&#23545;&#35805;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#35805;&#31649;&#29702;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10850</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#29992;&#20110;&#28151;&#21512;&#19987;&#23478;&#23545;&#35805;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
Offline Reinforcement Learning for Mixture-of-Expert Dialogue Management. (arXiv:2302.10850v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10850
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#22312;&#28151;&#21512;&#19987;&#23478;&#23545;&#35805;&#31649;&#29702;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#21033;&#29992;&#26368;&#26032;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65288;MoE-LMs&#65289;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#38024;&#23545;&#23545;&#35805;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;&#24182;&#25552;&#39640;&#20102;&#23545;&#35805;&#31649;&#29702;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#22312;&#24320;&#21457;&#23545;&#35805;&#31649;&#29702;&#65288;DM&#65289;&#20195;&#29702;&#65292;&#23454;&#29616;&#38750;&#30446;&#26631;&#23548;&#21521;&#65292;&#36827;&#34892;&#23500;&#26377;&#20869;&#23481;&#30340;&#23545;&#35805;&#65292;&#26368;&#22823;&#21270;&#29992;&#25143;&#28385;&#24847;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#24378;&#21270;&#23398;&#20064;&#21644;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#26368;&#36817;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#39537;&#21160;&#30340;&#23545;&#35805;&#32842;&#22825;&#26426;&#22120;&#20154;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#37096;&#20998;&#21407;&#22240;&#26159;&#24378;&#21270;&#23398;&#20064;&#38656;&#35201;&#22312;&#32447;&#25506;&#32034;&#20197;&#26377;&#25928;&#23398;&#20064;&#65292;&#32780;&#25910;&#38598;&#26032;&#39062;&#30340;&#20154;&#26426;&#20132;&#20114;&#21487;&#33021;&#26082;&#26114;&#36149;&#21448;&#19981;&#23433;&#20840;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#38754;&#23545;&#36825;&#20123;&#31639;&#27861;&#30340;&#32452;&#21512;&#21160;&#20316;&#31354;&#38388;&#26102;&#21464;&#24471;&#26356;&#20026;&#20005;&#37325;&#65292;&#22240;&#20026;&#22823;&#22810;&#25968;&#35821;&#35328;&#27169;&#22411;&#20195;&#29702;&#20197;&#35789;&#32423;&#21035;&#29983;&#25104;&#21709;&#24212;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#22810;&#31181;&#38024;&#23545;&#23545;&#35805;&#35268;&#21010;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#21033;&#29992;&#26368;&#26032;&#30340;&#28151;&#21512;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65288;MoE-LMs&#65289; - &#19968;&#31181;&#25429;&#25417;&#22810;&#26679;&#35821;&#20041;&#65292;&#29983;&#25104;&#21453;&#26144;&#19981;&#21516;&#24847;&#22270;&#30340;&#35805;&#35821;&#30340;&#27169;&#22411;&#65292;&#36866;&#29992;&#20110;&#22810;&#36718;&#23545;&#35805;&#31649;&#29702;&#12290;&#36890;&#36807;&#21033;&#29992;MoE-LM&#32467;&#26500;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#20943;&#23569;&#20102;&#34892;&#21160;&#31354;&#38388;&#30340;&#22823;&#23567;&#65292;&#24182;&#25552;&#39640;&#20102;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#30340;&#23545;&#35805;&#31649;&#29702;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning (RL) has shown great promise for developing dialogue management (DM) agents that are non-myopic, conduct rich conversations, and maximize overall user satisfaction. Despite recent developments in RL and language models (LMs), using RL to power conversational chatbots remains challenging, in part because RL requires online exploration to learn effectively, whereas collecting novel human-bot interactions can be expensive and unsafe. This issue is exacerbated by the combinatorial action spaces facing these algorithms, as most LM agents generate responses at the word level. We develop a variety of RL algorithms, specialized to dialogue planning, that leverage recent Mixture-of-Expert Language Models (MoE-LMs) -- models that capture diverse semantics, generate utterances reflecting different intents, and are amenable for multi-turn DM. By exploiting MoE-LM structure, our methods significantly reduce the size of the action space and improve the efficacy of RL-based DM.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#26469;&#20248;&#21270;Jaccard&#25351;&#25968;&#65292;&#35813;&#25439;&#22833;&#22312;&#36719;&#26631;&#31614;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2302.05666</link><description>&lt;p&gt;
Jaccard&#24230;&#37327;&#25439;&#22833;&#65306;&#20351;&#29992;&#36719;&#26631;&#31614;&#20248;&#21270;Jaccard&#25351;&#25968;
&lt;/p&gt;
&lt;p&gt;
Jaccard Metric Losses: Optimizing the Jaccard Index with Soft Labels. (arXiv:2302.05666v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05666
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#26469;&#20248;&#21270;Jaccard&#25351;&#25968;&#65292;&#35813;&#25439;&#22833;&#22312;&#36719;&#26631;&#31614;&#19979;&#20173;&#28982;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
IoU&#25439;&#22833;&#26159;&#30452;&#25509;&#20248;&#21270;Jaccard&#25351;&#25968;&#30340;&#26367;&#20195;&#21697;&#12290;&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#65292;&#23558;IoU&#25439;&#22833;&#20316;&#20026;&#25439;&#22833;&#20989;&#25968;&#30340;&#19968;&#37096;&#20998;&#65292;&#19982;&#20165;&#20248;&#21270;&#20687;&#32032;&#25439;&#22833;&#65288;&#22914;&#20132;&#21449;&#29109;&#25439;&#22833;&#65289;&#30456;&#27604;&#65292;&#23545;&#20110;Jaccard&#25351;&#25968;&#27979;&#37327;&#34920;&#29616;&#26356;&#22909;&#12290;&#26368;&#26174;&#30528;&#30340;IoU&#25439;&#22833;&#26159;&#36719;Jaccard&#25439;&#22833;&#21644;Lovasz-Softmax&#25439;&#22833;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25439;&#22833;&#19982;&#26426;&#22120;&#23398;&#20064;&#20013;&#26222;&#36941;&#23384;&#22312;&#30340;&#36719;&#26631;&#31614;&#19981;&#20860;&#23481;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Jaccard&#24230;&#37327;&#25439;&#22833;&#65288;JMLs&#65289;&#65292;&#23427;&#20204;&#22312;&#26631;&#20934;&#35774;&#32622;&#19979;&#19982;&#36719;&#26631;&#31614;&#20860;&#23481;&#65292;&#19982;&#36719;Jaccard&#25439;&#22833;&#30456;&#21516;&#12290;&#20351;&#29992;JMLs&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#26368;&#27969;&#34892;&#30340;&#36719;&#26631;&#31614;&#29992;&#20363;&#65306;&#26631;&#31614;&#24179;&#28369;&#21644;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#19977;&#20010;&#35821;&#20041;&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;Cityscapes&#12289;PASCAL VOC&#21644;DeepGlobe Land&#65289;&#19978;&#65292;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#20132;&#21449;&#29109;&#25439;&#22833;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#31616;&#21333;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#22312;DeepGlobe Land&#25968;&#25454;&#38598;&#19978;&#36229;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
IoU losses are surrogates that directly optimize the Jaccard index. In semantic segmentation, leveraging IoU losses as part of the loss function is shown to perform better with respect to the Jaccard index measure than optimizing pixel-wise losses such as the cross-entropy loss alone. The most notable IoU losses are the soft Jaccard loss and the Lovasz-Softmax loss. However, these losses are incompatible with soft labels which are ubiquitous in machine learning. In this paper, we propose Jaccard metric losses (JMLs), which are identical to the soft Jaccard loss in a standard setting with hard labels, but are compatible with soft labels. With JMLs, we study two of the most popular use cases of soft labels: label smoothing and knowledge distillation. With a variety of architectures, our experiments show significant improvements over the cross-entropy loss on three semantic segmentation datasets (Cityscapes, PASCAL VOC and DeepGlobe Land), and our simple approach outperforms state-of-the-
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#22810;&#20219;&#21153;&#20855;&#36523;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#35268;&#21010;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#35268;&#21010;&#26041;&#27861;(DEPS)&#26469;&#35299;&#20915;&#35745;&#21010;&#25191;&#34892;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.01560</link><description>&lt;p&gt;
&#25551;&#36848;&#12289;&#35299;&#37322;&#12289;&#35268;&#21010;&#21644;&#36873;&#25321;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21551;&#29992;&#24320;&#25918;&#19990;&#30028;&#22810;&#20219;&#21153;&#26234;&#33021;&#20307;&#30340;&#20132;&#20114;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Describe, Explain, Plan and Select: Interactive Planning with Large Language Models Enables Open-World Multi-Task Agents. (arXiv:2302.01560v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01560
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#22810;&#20219;&#21153;&#20855;&#36523;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#35268;&#21010;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20132;&#20114;&#24335;&#35268;&#21010;&#26041;&#27861;(DEPS)&#26469;&#35299;&#20915;&#35745;&#21010;&#25191;&#34892;&#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#20013;&#22810;&#20219;&#21153;&#20855;&#36523;&#26234;&#33021;&#20307;&#30340;&#20219;&#21153;&#35268;&#21010;&#25361;&#25112;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#20010;&#20027;&#35201;&#22256;&#38590;&#65306;1&#65289;&#22312;&#24320;&#25918;&#19990;&#30028;&#29615;&#22659;&#65288;&#22914;Minecraft&#65289;&#20013;&#25191;&#34892;&#35745;&#21010;&#38656;&#35201;&#20934;&#30830;&#30340;&#22810;&#27493;&#25512;&#29702;&#65292;&#22240;&#20026;&#20219;&#21153;&#26159;&#38271;&#26399;&#24615;&#30340;&#65307;2&#65289;&#30001;&#20110;&#20256;&#32479;&#35268;&#21010;&#22120;&#19981;&#32771;&#34385;&#24403;&#21069;&#26234;&#33021;&#20307;&#23436;&#25104;&#32473;&#23450;&#23376;&#20219;&#21153;&#30340;&#38590;&#24230;&#65292;&#22312;&#22797;&#26434;&#35745;&#21010;&#20013;&#23545;&#24182;&#34892;&#23376;&#30446;&#26631;&#36827;&#34892;&#25490;&#24207;&#21487;&#33021;&#23548;&#33268;&#35745;&#21010;&#20302;&#25928;&#29978;&#33267;&#19981;&#21487;&#34892;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#8220;&#25551;&#36848;&#12289;&#35299;&#37322;&#12289;&#35268;&#21010;&#21644;&#36873;&#25321;&#8221;&#65288;DEPS&#65289;&#20132;&#20114;&#24335;&#35268;&#21010;&#26041;&#27861;&#12290;DEPS&#36890;&#36807;&#25972;&#21512;&#35745;&#21010;&#25191;&#34892;&#36807;&#31243;&#30340;&#25551;&#36848;&#21644;&#22312;&#35268;&#21010;&#38454;&#27573;&#36935;&#21040;&#22833;&#36133;&#26102;&#25552;&#20379;&#33258;&#25105;&#35299;&#37322;&#30340;&#21453;&#39304;&#65292;&#23454;&#29616;&#20102;&#23545;&#21021;&#22987;LLM&#29983;&#25104;&#30340;&#35745;&#21010;&#30340;&#26356;&#22909;&#30340;&#38169;&#35823;&#20462;&#27491;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the challenge of task planning for multi-task embodied agents in open-world environments. Two main difficulties are identified: 1) executing plans in an open-world environment (e.g., Minecraft) necessitates accurate and multi-step reasoning due to the long-term nature of tasks, and 2) as vanilla planners do not consider how easy the current agent can achieve a given sub-task when ordering parallel sub-goals within a complicated plan, the resulting plan could be inefficient or even infeasible. To this end, we propose "$\underline{D}$escribe, $\underline{E}$xplain, $\underline{P}$lan and $\underline{S}$elect" ($\textbf{DEPS}$), an interactive planning approach based on Large Language Models (LLMs). DEPS facilitates better error correction on initial LLM-generated $\textit{plan}$ by integrating $\textit{description}$ of the plan execution process and providing self-$\textit{explanation}$ of feedback when encountering failures during the extended planning phases. Furthermore
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#34920;&#31034;&#20855;&#26377;&#31867;&#20284;&#30340;&#20960;&#20309;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#38543;&#30528;&#23618;&#32423;&#30340;&#31227;&#21160;&#65292;&#23427;&#20204;&#22312;&#26368;&#21021;&#30340;&#20960;&#23618;&#20013;&#21464;&#24471;&#39640;&#32500;&#65292;&#28982;&#21518;&#22312;&#20013;&#38388;&#23618;&#20013;&#26174;&#33879;&#25910;&#32553;&#65292;&#22312;&#27169;&#22411;&#30340;&#26368;&#21518;&#37096;&#20998;&#65292;&#20445;&#25345;&#24658;&#23450;&#25110;&#24418;&#25104;&#31532;&#20108;&#20010;&#27973;&#23792;&#12290;&#22312;&#31532;&#19968;&#20010;&#23792;&#20540;&#32467;&#26463;&#26102;&#65292;&#25968;&#25454;&#38598;&#30340;&#35821;&#20041;&#20449;&#24687;&#34987;&#26356;&#22909;&#22320;&#34920;&#36798;&#12290;</title><link>http://arxiv.org/abs/2302.00294</link><description>&lt;p&gt;
&#22823;&#22411;Transformer&#27169;&#22411;&#30340;&#38544;&#34255;&#34920;&#31034;&#30340;&#20960;&#20309;&#23398;
&lt;/p&gt;
&lt;p&gt;
The geometry of hidden representations of large transformer models. (arXiv:2302.00294v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00294
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;Transformer&#27169;&#22411;&#20013;&#30340;&#38544;&#34255;&#34920;&#31034;&#20855;&#26377;&#31867;&#20284;&#30340;&#20960;&#20309;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#38543;&#30528;&#23618;&#32423;&#30340;&#31227;&#21160;&#65292;&#23427;&#20204;&#22312;&#26368;&#21021;&#30340;&#20960;&#23618;&#20013;&#21464;&#24471;&#39640;&#32500;&#65292;&#28982;&#21518;&#22312;&#20013;&#38388;&#23618;&#20013;&#26174;&#33879;&#25910;&#32553;&#65292;&#22312;&#27169;&#22411;&#30340;&#26368;&#21518;&#37096;&#20998;&#65292;&#20445;&#25345;&#24658;&#23450;&#25110;&#24418;&#25104;&#31532;&#20108;&#20010;&#27973;&#23792;&#12290;&#22312;&#31532;&#19968;&#20010;&#23792;&#20540;&#32467;&#26463;&#26102;&#65292;&#25968;&#25454;&#38598;&#30340;&#35821;&#20041;&#20449;&#24687;&#34987;&#26356;&#22909;&#22320;&#34920;&#36798;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;Transformer&#27169;&#22411;&#26159;&#29992;&#20110;&#33258;&#30417;&#30563;&#25968;&#25454;&#20998;&#26512;&#30340;&#24378;&#22823;&#26550;&#26500;&#65292;&#21487;&#20197;&#22788;&#29702;&#21253;&#25324;&#34507;&#30333;&#36136;&#24207;&#21015;&#12289;&#22270;&#20687;&#21644;&#25991;&#26412;&#22312;&#20869;&#30340;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#12290;&#22312;&#36825;&#20123;&#27169;&#22411;&#20013;&#65292;&#25968;&#25454;&#38598;&#30340;&#35821;&#20041;&#32467;&#26500;&#36890;&#36807;&#19968;&#20010;&#34920;&#31034;&#19982;&#19979;&#19968;&#20010;&#34920;&#31034;&#20043;&#38388;&#30340;&#19968;&#31995;&#21015;&#21464;&#25442;&#32780;&#20986;&#29616;&#12290;&#25105;&#20204;&#34920;&#24449;&#20102;&#36825;&#20123;&#34920;&#31034;&#30340;&#20960;&#20309;&#21644;&#32479;&#35745;&#29305;&#24615;&#65292;&#20197;&#21450;&#23427;&#20204;&#22312;&#23618;&#32423;&#31227;&#21160;&#26102;&#30340;&#21464;&#21270;&#12290;&#36890;&#36807;&#20998;&#26512;&#20869;&#22312;&#32500;&#24230;&#65288;ID&#65289;&#21644;&#37051;&#23621;&#32452;&#25104;&#65292;&#25105;&#20204;&#21457;&#29616;&#22312;&#35757;&#32451;&#22312;&#34507;&#30333;&#36136;&#35821;&#35328;&#20219;&#21153;&#21644;&#22270;&#20687;&#37325;&#24314;&#20219;&#21153;&#19978;&#30340;Transformer&#27169;&#22411;&#20013;&#65292;&#34920;&#31034;&#20197;&#30456;&#20284;&#30340;&#26041;&#24335;&#28436;&#21270;&#12290;&#22312;&#26368;&#21021;&#30340;&#20960;&#23618;&#20013;&#65292;&#25968;&#25454;&#27969;&#24418;&#25193;&#23637;&#65292;&#21464;&#24471;&#39640;&#32500;&#65292;&#28982;&#21518;&#22312;&#20013;&#38388;&#23618;&#20013;&#26174;&#33879;&#25910;&#32553;&#12290;&#22312;&#27169;&#22411;&#30340;&#26368;&#21518;&#37096;&#20998;&#65292;ID&#20445;&#25345;&#22823;&#33268;&#24658;&#23450;&#25110;&#24418;&#25104;&#31532;&#20108;&#20010;&#27973;&#23792;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25968;&#25454;&#38598;&#30340;&#35821;&#20041;&#20449;&#24687;&#22312;&#31532;&#19968;&#20010;&#23792;&#20540;&#32467;&#26463;&#26102;&#26356;&#22909;&#22320;&#34920;&#36798;&#65292;&#36825;&#19968;&#29616;&#35937;&#21487;&#20197;&#34987;&#35266;&#23519;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large transformers are powerful architectures used for self-supervised data analysis across various data types, including protein sequences, images, and text. In these models, the semantic structure of the dataset emerges from a sequence of transformations between one representation and the next. We characterize the geometric and statistical properties of these representations and how they change as we move through the layers. By analyzing the intrinsic dimension (ID) and neighbor composition, we find that the representations evolve similarly in transformers trained on protein language tasks and image reconstruction tasks. In the first layers, the data manifold expands, becoming high-dimensional, and then contracts significantly in the intermediate layers. In the last part of the model, the ID remains approximately constant or forms a second shallow peak. We show that the semantic information of the dataset is better expressed at the end of the first peak, and this phenomenon can be ob
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;U&#24418;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#39640;&#24230;&#23545;&#40784;&#30340;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#23545;&#25968;&#25454;&#30340;&#21033;&#29992;&#26356;&#21152;&#26377;&#25928;&#65292;&#20294;&#19982;&#20154;&#31867;&#23545;&#40784;&#24182;&#38750;&#24517;&#35201;&#26465;&#20214;&#12290;</title><link>http://arxiv.org/abs/2301.11990</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#25903;&#25345;&#40065;&#26834;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Alignment with human representations supports robust few-shot learning. (arXiv:2301.11990v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11990
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#34920;&#29616;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#23384;&#22312;U&#24418;&#20851;&#31995;&#65292;&#24182;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#23454;&#39564;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;&#39640;&#24230;&#23545;&#40784;&#30340;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#65292;&#23545;&#25968;&#25454;&#30340;&#21033;&#29992;&#26356;&#21152;&#26377;&#25928;&#65292;&#20294;&#19982;&#20154;&#31867;&#23545;&#40784;&#24182;&#38750;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#26159;&#21542;&#24212;&#35813;&#20851;&#24515;AI&#31995;&#32479;&#26159;&#21542;&#20855;&#26377;&#19982;&#20154;&#31867;&#30456;&#20284;&#30340;&#19990;&#30028;&#34920;&#24449;&#65311;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#20449;&#24687;&#35770;&#20998;&#26512;&#65292;&#24314;&#35758;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#30340;&#34920;&#29616;&#24230;&#19982;&#20154;&#31867;&#34920;&#24449;&#30340;&#19968;&#33268;&#24615;&#20043;&#38388;&#24212;&#35813;&#23384;&#22312;&#19968;&#20010;U&#24418;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#23545;491&#20010;&#35745;&#31639;&#26426;&#35270;&#35273;&#27169;&#22411;&#30340;&#24615;&#33021;&#20998;&#26512;&#39564;&#35777;&#20102;&#36825;&#20010;&#39044;&#27979;&#30340;&#21487;&#34892;&#24615;&#65292;&#24182;&#19988;&#34920;&#26126;&#39640;&#24230;&#23545;&#40784;&#30340;&#27169;&#22411;&#26356;&#21152;&#40065;&#26834;&#20110;&#23545;&#25239;&#25915;&#20987;&#21644;&#22495;&#20559;&#31227;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#20154;&#31867;&#23545;&#40784;&#24448;&#24448;&#26159;&#27169;&#22411;&#26377;&#25928;&#21033;&#29992;&#26377;&#38480;&#25968;&#25454;&#12289;&#40065;&#26834;&#24615; &#20197;&#21450;&#27867;&#21270;&#33021;&#21147;&#30340;&#20805;&#20998;&#20294;&#19981;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Should we care whether AI systems have representations of the world that are similar to those of humans? We provide an information-theoretic analysis that suggests that there should be a U-shaped relationship between the degree of representational alignment with humans and performance on few-shot learning tasks. We confirm this prediction empirically, finding such a relationship in an analysis of the performance of 491 computer vision models. We also show that highly-aligned models are more robust to both adversarial attacks and domain shifts. Our results suggest that human-alignment is often a sufficient, but not necessary, condition for models to make effective use of limited data, be robust, and generalize well.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#30340;&#26465;&#20214;&#38598;&#26469;&#34920;&#24449;&#21644;&#23398;&#20064;&#22240;&#26524;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32422;&#26463;&#24615;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#21644;&#26465;&#20214;&#38598;&#36739;&#22823;&#26102;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;k-&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#22312;&#19981;&#33021;&#21033;&#29992;&#25152;&#26377;&#26465;&#20214;&#29420;&#31435;&#24615;&#35821;&#21477;&#26102;&#20173;&#28982;&#36866;&#29992;&#12290;</title><link>http://arxiv.org/abs/2301.09028</link><description>&lt;p&gt;
&#29992;&#23567;&#30340;&#26465;&#20214;&#38598;&#34920;&#24449;&#21644;&#23398;&#20064;&#22240;&#26524;&#22270;
&lt;/p&gt;
&lt;p&gt;
Characterization and Learning of Causal Graphs with Small Conditioning Sets. (arXiv:2301.09028v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09028
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23567;&#30340;&#26465;&#20214;&#38598;&#26469;&#34920;&#24449;&#21644;&#23398;&#20064;&#22240;&#26524;&#22270;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;&#32422;&#26463;&#24615;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#22312;&#25968;&#25454;&#26377;&#38480;&#21644;&#26465;&#20214;&#38598;&#36739;&#22823;&#26102;&#30340;&#22256;&#38590;&#12290;&#25105;&#20204;&#23450;&#20041;&#20102;k-&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#27010;&#24565;&#65292;&#35813;&#27010;&#24565;&#22312;&#19981;&#33021;&#21033;&#29992;&#25152;&#26377;&#26465;&#20214;&#29420;&#31435;&#24615;&#35821;&#21477;&#26102;&#20173;&#28982;&#36866;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32422;&#26463;&#24615;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#36890;&#36807;&#31995;&#32479;&#22320;&#27979;&#35797;&#25968;&#25454;&#20013;&#35266;&#23519;&#21040;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#26469;&#23398;&#20064;&#22240;&#26524;&#22270;&#30340;&#19968;&#37096;&#20998;&#32467;&#26500;&#12290;&#36825;&#20123;&#31639;&#27861;&#65292;&#22914;PC&#31639;&#27861;&#21450;&#20854;&#21464;&#20307;&#65292;&#20381;&#36182;&#20110;&#30001;Pearl&#25552;&#20986;&#30340;&#25152;&#35859;&#22240;&#26524;&#22270;&#31561;&#20215;&#31867;&#30340;&#22270;&#24418;&#34920;&#24449;&#12290;&#28982;&#32780;&#65292;&#24403;&#25968;&#25454;&#26377;&#38480;&#26102;&#65292;&#32422;&#26463;&#24615;&#22240;&#26524;&#21457;&#29616;&#31639;&#27861;&#24448;&#24448;&#38754;&#20020;&#22256;&#38590;&#65292;&#22240;&#20026;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#24456;&#24555;&#22833;&#21435;&#32479;&#35745;&#33021;&#21147;&#65292;&#23588;&#20854;&#26159;&#24403;&#26465;&#20214;&#38598;&#24456;&#22823;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26465;&#20214;&#29420;&#31435;&#24615;&#26816;&#39564;&#65292;&#22312;&#40065;&#26834;&#30340;&#22240;&#26524;&#21457;&#29616;&#20013;&#23558;&#26465;&#20214;&#38598;&#30340;&#22823;&#23567;&#19978;&#38480;&#35774;&#32622;&#20026;&#26576;&#20010;&#25972;&#25968; k&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22240;&#26524;&#22270;&#31561;&#20215;&#31867;&#30340;&#22270;&#24418;&#34920;&#24449;&#22312;&#25105;&#20204;&#19981;&#33021;&#21033;&#29992;&#25152;&#26377;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#35821;&#21477;&#26102;&#19981;&#36866;&#29992;&#12290;&#25105;&#20204;&#39318;&#20808;&#23450;&#20041;&#20102; k-&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#27010;&#24565;&#65306;&#22914;&#26524;&#20004;&#20010;&#22240;&#26524;&#22270;&#24471;&#21040;&#30456;&#21516;&#30340;&#26465;&#20214;&#29420;&#31435;&#24615;&#35821;&#21477;&#65292;&#23427;&#20204;&#26159; k-&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Constraint-based causal discovery algorithms learn part of the causal graph structure by systematically testing conditional independences observed in the data. These algorithms, such as the PC algorithm and its variants, rely on graphical characterizations of the so-called equivalence class of causal graphs proposed by Pearl. However, constraint-based causal discovery algorithms struggle when data is limited since conditional independence tests quickly lose their statistical power, especially when the conditioning set is large. To address this, we propose using conditional independence tests where the size of the conditioning set is upper bounded by some integer $k$ for robust causal discovery. The existing graphical characterizations of the equivalence classes of causal graphs are not applicable when we cannot leverage all the conditional independence statements. We first define the notion of $k$-Markov equivalence: Two causal graphs are $k$-Markov equivalent if they entail the same c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#35282;&#24230;&#30740;&#31350;&#20102;&#22870;&#21169;&#25512;&#26029;&#23545;&#38169;&#35823;&#20154;&#31867;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#21487;&#33021;&#26500;&#24314;&#23567;&#30340;&#23545;&#25239;&#24615;&#20559;&#24046;&#30340;&#24773;&#20917;&#12290;</title><link>http://arxiv.org/abs/2212.04717</link><description>&lt;p&gt;
&#20851;&#20110;&#22870;&#21169;&#25512;&#26029;&#23545;&#38169;&#35823;&#20154;&#31867;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;
&lt;/p&gt;
&lt;p&gt;
On the Sensitivity of Reward Inference to Misspecified Human Models. (arXiv:2212.04717v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04717
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20174;&#29702;&#35770;&#21644;&#23454;&#35777;&#35282;&#24230;&#30740;&#31350;&#20102;&#22870;&#21169;&#25512;&#26029;&#23545;&#38169;&#35823;&#20154;&#31867;&#27169;&#22411;&#30340;&#25935;&#24863;&#24615;&#65292;&#24182;&#21457;&#29616;&#23384;&#22312;&#21487;&#33021;&#26500;&#24314;&#23567;&#30340;&#23545;&#25239;&#24615;&#20559;&#24046;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#20154;&#31867;&#34892;&#20026;&#20013;&#25512;&#26029;&#22870;&#21169;&#20989;&#25968;&#26159;&#19982;&#20215;&#20540;&#23545;&#40784;&#23494;&#20999;&#30456;&#20851;&#30340;&#20869;&#23481; - &#30830;&#20445;&#20154;&#24037;&#26234;&#33021;&#30340;&#30446;&#26631;&#19982;&#25105;&#20204;&#20154;&#31867;&#23454;&#38469;&#24819;&#35201;&#30340;&#19968;&#33268;&#12290;&#20294;&#36825;&#38656;&#35201;&#24314;&#31435;&#20154;&#31867;&#30340;&#34892;&#20026;&#27169;&#22411;&#12290;&#32463;&#36807;&#20960;&#21313;&#24180;&#30340;&#35748;&#30693;&#31185;&#23398;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#34892;&#20026;&#32463;&#27982;&#23398;&#30740;&#31350;&#65292;&#33719;&#24471;&#20934;&#30830;&#30340;&#20154;&#31867;&#27169;&#22411;&#20173;&#28982;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#23545;&#20110;&#22870;&#21169;&#25512;&#26029;&#30340;&#20934;&#30830;&#24615;&#26377;&#22810;&#37325;&#35201;&#65311;&#19968;&#26041;&#38754;&#65292;&#22914;&#26524;&#27169;&#22411;&#20013;&#23384;&#22312;&#23567;&#38169;&#35823;&#23601;&#20250;&#23548;&#33268;&#25512;&#26029;&#30340;&#28798;&#38590;&#24615;&#38169;&#35823;&#65292;&#37027;&#20040;&#22870;&#21169;&#23398;&#20064;&#30340;&#25972;&#20010;&#26694;&#26550;&#20284;&#20046;&#27880;&#23450;&#22833;&#36133;&#65292;&#22240;&#20026;&#25105;&#20204;&#27704;&#36828;&#26080;&#27861;&#25317;&#26377;&#23436;&#32654;&#30340;&#20154;&#31867;&#34892;&#20026;&#27169;&#22411;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22914;&#26524;&#38543;&#30528;&#27169;&#22411;&#30340;&#25913;&#36827;&#65292;&#21487;&#20197;&#20445;&#35777;&#22870;&#21169;&#30340;&#20934;&#30830;&#24615;&#20063;&#20250;&#25552;&#39640;&#65292;&#36825;&#23558;&#35777;&#26126;&#22312;&#27169;&#22411;&#26041;&#38754;&#20570;&#26356;&#22810;&#24037;&#20316;&#30340;&#30410;&#22788;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#21644;&#23454;&#35777;&#26041;&#38754;&#30740;&#31350;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30830;&#23454;&#23637;&#31034;&#20102;&#26500;&#24314;&#23567;&#30340;&#23545;&#25239;&#24615;&#20559;&#24046;&#26159;&#21487;&#33021;&#30340;
&lt;/p&gt;
&lt;p&gt;
Inferring reward functions from human behavior is at the center of value alignment - aligning AI objectives with what we, humans, actually want. But doing so relies on models of how humans behave given their objectives. After decades of research in cognitive science, neuroscience, and behavioral economics, obtaining accurate human models remains an open research topic. This begs the question: how accurate do these models need to be in order for the reward inference to be accurate? On the one hand, if small errors in the model can lead to catastrophic error in inference, the entire framework of reward learning seems ill-fated, as we will never have perfect models of human behavior. On the other hand, if as our models improve, we can have a guarantee that reward accuracy also improves, this would show the benefit of more work on the modeling side. We study this question both theoretically and empirically. We do show that it is unfortunately possible to construct small adversarial biases 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#20540;&#20989;&#25968;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#22788;&#29702;&#20102;&#25968;&#25454;&#38598;&#21644;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#26681;&#25454;&#20445;&#23432;&#31243;&#24230;&#21160;&#24577;&#36873;&#25321;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2212.04607</link><description>&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#32622;&#20449;&#24230;&#26465;&#20214;&#20540;&#20989;&#25968;
&lt;/p&gt;
&lt;p&gt;
Confidence-Conditioned Value Functions for Offline Reinforcement Learning. (arXiv:2212.04607v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04607
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26465;&#20214;&#20540;&#20989;&#25968;&#30340;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20013;&#22788;&#29702;&#20102;&#25968;&#25454;&#38598;&#21644;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#24182;&#21487;&#20197;&#22312;&#35757;&#32451;&#21644;&#35780;&#20272;&#26102;&#26681;&#25454;&#20445;&#23432;&#31243;&#24230;&#21160;&#24577;&#36873;&#25321;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#25215;&#35834;&#33021;&#22815;&#20165;&#20351;&#29992;&#29616;&#26377;&#30340;&#38745;&#24577;&#25968;&#25454;&#38598;&#23398;&#20064;&#26377;&#25928;&#30340;&#31574;&#30053;&#65292;&#32780;&#19981;&#38656;&#35201;&#20219;&#20309;&#26114;&#36149;&#30340;&#22312;&#32447;&#20132;&#20114;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#31163;&#32447;RL&#26041;&#27861;&#24517;&#39035;&#22788;&#29702;&#25968;&#25454;&#38598;&#21644;&#23398;&#20064;&#31574;&#30053;&#20043;&#38388;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#26368;&#24120;&#35265;&#30340;&#26041;&#27861;&#26159;&#23398;&#20064;&#20445;&#23432;&#25110;&#19979;&#38480;&#20540;&#20989;&#25968;&#65292;&#23427;&#20204;&#20302;&#20272;&#20102;&#36229;&#20986;&#20998;&#24067;&#30340;&#34892;&#20026;&#30340;&#22238;&#25253;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#23384;&#22312;&#19968;&#20010;&#26174;&#33879;&#32570;&#28857;&#65306;&#22312;&#36825;&#20123;&#20540;&#20989;&#25968;&#19978;&#20248;&#21270;&#30340;&#31574;&#30053;&#21482;&#33021;&#26681;&#25454;&#22266;&#23450;&#30340;&#12289;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#20445;&#23432;&#31243;&#24230;&#26469;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#33021;&#22815;&#22312;&#35757;&#32451;&#26102;&#23398;&#20064;&#19981;&#21516;&#20445;&#23432;&#31243;&#24230;&#30340;&#31574;&#30053;&#65292;&#24182;&#35774;&#35745;&#19968;&#31181;&#26041;&#27861;&#22312;&#35780;&#20272;&#26102;&#21160;&#24577;&#36873;&#25321;&#20854;&#20013;&#20043;&#19968;&#65292;&#36825;&#20010;&#38382;&#39064;&#23601;&#21487;&#20197;&#24471;&#21040;&#32531;&#35299;&#12290;&#20026;&#27492;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#23398;&#20064;&#38468;&#21152;&#26465;&#20214;&#30340;&#20540;&#20989;&#25968;&#65292;&#36825;&#20123;&#20540;&#20989;&#25968;&#20381;&#36182;&#20110;&#20445;&#23432;&#31243;&#24230;&#65292;&#25105;&#20204;&#23558;&#20854;&#31216;&#20026;&#32622;&#20449;&#24230;&#26465;&#20214;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Offline reinforcement learning (RL) promises the ability to learn effective policies solely using existing, static datasets, without any costly online interaction. To do so, offline RL methods must handle distributional shift between the dataset and the learned policy. The most common approach is to learn conservative, or lower-bound, value functions, which underestimate the return of out-of-distribution (OOD) actions. However, such methods exhibit one notable drawback: policies optimized on such value functions can only behave according to a fixed, possibly suboptimal, degree of conservatism. However, this can be alleviated if we instead are able to learn policies for varying degrees of conservatism at training time and devise a method to dynamically choose one of them during evaluation. To do so, in this work, we propose learning value functions that additionally condition on the degree of conservatism, which we dub confidence-conditioned value functions. We derive a new form of a Be
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#20027;&#21160;&#25628;&#32034;&#26694;&#26550;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21327;&#21161;&#22320;&#29702;&#31354;&#38388;&#25506;&#32034;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20197;&#24191;&#38420;&#21306;&#22495;&#30340;&#22270;&#20687;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#26597;&#35810;&#26469;&#39564;&#35777;&#30446;&#26631;&#23545;&#35937;&#31034;&#20363;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#20851;&#20110;&#30446;&#26631;&#23545;&#35937;&#31354;&#38388;&#20998;&#24067;&#30340;&#20449;&#24687;&#12290;</title><link>http://arxiv.org/abs/2211.15788</link><description>&lt;p&gt;
&#29992;&#20110;&#22320;&#29702;&#31354;&#38388;&#25506;&#32034;&#30340;&#35270;&#35273;&#20027;&#21160;&#25628;&#32034;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Visual Active Search Framework for Geospatial Exploration. (arXiv:2211.15788v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#35270;&#35273;&#20027;&#21160;&#25628;&#32034;&#26694;&#26550;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#21327;&#21161;&#22320;&#29702;&#31354;&#38388;&#25506;&#32034;&#38382;&#39064;&#12290;&#35813;&#26694;&#26550;&#20197;&#24191;&#38420;&#21306;&#22495;&#30340;&#22270;&#20687;&#20026;&#36755;&#20837;&#65292;&#36890;&#36807;&#26377;&#38480;&#30340;&#26597;&#35810;&#26469;&#39564;&#35777;&#30446;&#26631;&#23545;&#35937;&#31034;&#20363;&#30340;&#23384;&#22312;&#65292;&#24182;&#25552;&#20379;&#20851;&#20110;&#30446;&#26631;&#23545;&#35937;&#31354;&#38388;&#20998;&#24067;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#38382;&#39064;&#21487;&#20197;&#34987;&#35270;&#20026;&#21033;&#29992;&#33322;&#31354;&#24433;&#20687;&#21327;&#21161;&#30340;&#22320;&#29702;&#31354;&#38388;&#25628;&#32034;&#30340;&#24418;&#24335;&#65292;&#20363;&#22914;&#26816;&#27979;&#30423;&#29454;&#27963;&#21160;&#21644;&#20154;&#21475;&#36137;&#21334;&#31561;&#12290;&#26412;&#35770;&#25991;&#22312;&#35270;&#35273;&#20027;&#21160;&#25628;&#32034;&#65288;VAS&#65289;&#26694;&#26550;&#20013;&#24314;&#31435;&#20102;&#36825;&#31867;&#38382;&#39064;&#30340;&#27169;&#22411;&#65292;&#35813;&#26694;&#26550;&#20197;&#24191;&#38420;&#21306;&#22495;&#30340;&#22270;&#20687;&#20026;&#36755;&#20837;&#65292;&#24182;&#26088;&#22312;&#23613;&#21487;&#33021;&#22810;&#22320;&#35782;&#21035;&#30446;&#26631;&#23545;&#35937;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#26377;&#38480;&#30340;&#26597;&#35810;&#65292;VAS&#20250;&#39564;&#35777;&#22312;&#32473;&#23450;&#21306;&#22495;&#20869;&#26159;&#21542;&#23384;&#22312;&#31034;&#20363;&#12290;VAS&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24449;&#26159;&#65292;&#27599;&#20010;&#36825;&#26679;&#30340;&#26597;&#35810;&#37117;&#20250;&#25552;&#20379;&#26377;&#20851;&#30446;&#26631;&#23545;&#35937;&#31354;&#38388;&#20998;&#24067;&#30340;&#20449;&#24687;&#65292;&#36229;&#20986;&#20102;&#21487;&#35270;&#21270;&#25152;&#25429;&#25417;&#30340;&#20869;&#23481;&#65288;&#20363;&#22914;&#65292;&#30001;&#20110;&#31354;&#38388;&#30456;&#20851;&#24615;&#65289;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#38024;&#23545;VAS&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#21033;&#29992;&#23436;&#20840;&#27880;&#37322;&#30340;&#25628;&#32034;&#20219;&#21153;&#38598;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#26469;&#23398;&#20064;&#25628;&#32034;&#31574;&#30053;&#65292;&#24182;&#23558;&#36755;&#20837;&#22270;&#20687;&#30340;&#29305;&#24449;&#19982;&#20027;&#21160;&#25628;&#32034;&#29366;&#24577;&#30340;&#33258;&#28982;&#34920;&#31034;&#30456;&#32467;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22495;&#33258;&#36866;&#24212;&#25216;&#26415;&#65292;&#20197;&#25913;&#21892;&#20915;&#31574;&#26102;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Many problems can be viewed as forms of geospatial search aided by aerial imagery, with examples ranging from detecting poaching activity to human trafficking. We model this class of problems in a visual active search (VAS) framework, which takes as input an image of a broad area, and aims to identify as many examples of a target object as possible. It does this through a limited sequence of queries, each of which verifies whether an example is present in a given region. A crucial feature of VAS is that each such query is informative about the spatial distribution of target objects beyond what is captured visually (for example, due to spatial correlation). We propose a reinforcement learning approach for VAS that leverages a collection of fully annotated search tasks as training data to learn a search policy, and combines features of the input image with a natural representation of active search state. Additionally, we propose domain adaptation techniques to improve the policy at decis
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36974;&#32617;&#26041;&#27861;&#65292;&#36890;&#36807;&#23618;&#36974;&#32617;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#36974;&#32617;&#24341;&#36215;&#30340;&#32570;&#22833;&#20559;&#24046;&#65292;&#24182;&#28040;&#38500;&#25110;&#26368;&#23567;&#21270;&#20102;&#36974;&#32617;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2211.14646</link><description>&lt;p&gt;
&#25913;&#36827;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36974;&#32617;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Improved Input Masking for Convolutional Neural Networks. (arXiv:2211.14646v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25913;&#36827;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36974;&#32617;&#26041;&#27861;&#65292;&#36890;&#36807;&#23618;&#36974;&#32617;&#33021;&#22815;&#26377;&#25928;&#20943;&#23569;&#36974;&#32617;&#24341;&#36215;&#30340;&#32570;&#22833;&#20559;&#24046;&#65292;&#24182;&#28040;&#38500;&#25110;&#26368;&#23567;&#21270;&#20102;&#36974;&#32617;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#29702;&#35299;&#21644;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#26469;&#35828;&#65292;&#20174;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20837;&#20013;&#31227;&#38500;&#29305;&#24449;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#35270;&#35273;&#27169;&#22411;&#26469;&#35828;&#65292;&#36825;&#26159;&#38750;&#24120;&#22256;&#38590;&#30340;&#65292;&#22240;&#20026;&#36974;&#32617;&#22270;&#20687;&#30340;&#19968;&#37096;&#20998;&#36890;&#24120;&#20250;&#24341;&#36215;&#24456;&#22823;&#30340;&#20998;&#24067;&#20559;&#31227;&#12290;&#36825;&#26159;&#22240;&#20026;&#29992;&#20110;&#36974;&#32617;&#30340;&#22522;&#20934;&#39068;&#33394;&#65288;&#36890;&#24120;&#26159;&#28784;&#33394;&#25110;&#40657;&#33394;&#65289;&#26159;&#22788;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#12290;&#27492;&#22806;&#65292;&#36974;&#32617;&#26412;&#36523;&#30340;&#24418;&#29366;&#21487;&#20197;&#21253;&#21547;&#19981;&#38656;&#35201;&#30340;&#20449;&#21495;&#65292;&#27169;&#22411;&#21487;&#33021;&#20250;&#21033;&#29992;&#36825;&#20123;&#20449;&#21495;&#36827;&#34892;&#39044;&#27979;&#12290;&#26368;&#36817;&#65292;&#22312;&#35270;&#35273;&#21464;&#25442;&#22120;&#20013;&#23545;&#22270;&#20687;&#36974;&#32617;&#30340;&#32570;&#22833;&#20559;&#24046;&#38382;&#39064;&#26041;&#38754;&#24050;&#32463;&#21462;&#24471;&#20102;&#19968;&#20123;&#36827;&#23637;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;CNN&#36974;&#32617;&#26041;&#27861;&#65292;&#31216;&#20043;&#20026;&#23618;&#36974;&#32617;&#65292;&#21487;&#20197;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20943;&#23569;&#36974;&#32617;&#24341;&#36215;&#30340;&#32570;&#22833;&#20559;&#24046;&#12290;&#30452;&#35266;&#19978;&#65292;&#23618;&#36974;&#32617;&#23558;&#19968;&#20010;&#36974;&#32617;&#24212;&#29992;&#20110;&#20013;&#38388;&#28608;&#27963;&#22270;&#65292;&#20351;&#24471;&#27169;&#22411;&#21482;&#22788;&#29702;&#27809;&#26377;&#36974;&#32617;&#30340;&#36755;&#20837;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65288;i&#65289;&#33021;&#22815;&#28040;&#38500;&#25110;&#26368;&#23567;&#21270;&#36974;&#32617;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
The ability to remove features from the input of machine learning models is very important to understand and interpret model predictions. However, this is non-trivial for vision models since masking out parts of the input image typically causes large distribution shifts. This is because the baseline color used for masking (typically grey or black) is out of distribution. Furthermore, the shape of the mask itself can contain unwanted signals which can be used by the model for its predictions. Recently, there has been some progress in mitigating this issue (called missingness bias) in image masking for vision transformers. In this work, we propose a new masking method for CNNs we call layer masking in which the missingness bias caused by masking is reduced to a large extent. Intuitively, layer masking applies a mask to intermediate activation maps so that the model only processes the unmasked input. We show that our method (i) is able to eliminate or minimize the influence of the mask sh
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#23436;&#20840;&#23433;&#20840;&#30340;&#38544;&#20889;&#26415;&#65292;&#24182;&#21457;&#29616;&#21482;&#26377;&#26368;&#23567;&#29109;&#32806;&#21512;&#36807;&#31243;&#25165;&#26159;&#26368;&#39640;&#25928;&#30340;&#65292;&#25552;&#20986;&#20102;&#39640;&#21487;&#20280;&#32553;&#24615;&#30340;&#23436;&#32654;&#23433;&#20840;&#38544;&#20889;&#31639;&#27861;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>http://arxiv.org/abs/2210.14889</link><description>&lt;p&gt;
&#20351;&#29992;&#26368;&#23567;&#29109;&#32806;&#21512;&#30340;&#23436;&#32654;&#23433;&#20840;&#38544;&#20889;&#26415;
&lt;/p&gt;
&lt;p&gt;
Perfectly Secure Steganography Using Minimum Entropy Coupling. (arXiv:2210.14889v3 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14889
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#23436;&#20840;&#23433;&#20840;&#30340;&#38544;&#20889;&#26415;&#65292;&#24182;&#21457;&#29616;&#21482;&#26377;&#26368;&#23567;&#29109;&#32806;&#21512;&#36807;&#31243;&#25165;&#26159;&#26368;&#39640;&#25928;&#30340;&#65292;&#25552;&#20986;&#20102;&#39640;&#21487;&#20280;&#32553;&#24615;&#30340;&#23436;&#32654;&#23433;&#20840;&#38544;&#20889;&#31639;&#27861;&#65292;&#20855;&#26377;&#23454;&#38469;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38544;&#20889;&#26415;&#26159;&#23558;&#31192;&#23494;&#20449;&#24687;&#32534;&#30721;&#21040;&#26080;&#23475;&#30340;&#20869;&#23481;&#20013;&#65292;&#20197;&#20415;&#23545;&#25163;&#26080;&#27861;&#24847;&#35782;&#21040;&#38544;&#34255;&#30340;&#21547;&#20041;&#30340;&#23454;&#36341;&#12290;&#26412;&#25991;&#38024;&#23545;&#38544;&#20889;&#26415;&#30340;&#23433;&#20840;&#24615;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#26368;&#23567;&#29109;&#32806;&#21512;&#23454;&#29616;&#23436;&#20840;&#23433;&#20840;&#30340;&#38544;&#20889;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35777;&#26126;&#20102;&#22312;&#25152;&#26377;&#23436;&#20840;&#23433;&#20840;&#30340;&#38544;&#20889;&#36807;&#31243;&#20013;&#65292;&#20165;&#20165;&#26159;&#26368;&#23567;&#29109;&#32806;&#21512;&#30340;&#36807;&#31243;&#26159;&#26368;&#39640;&#25928;&#30340;&#12290;&#36825;&#20123;&#32467;&#26524;&#20026;&#23454;&#29616;&#20855;&#26377;&#23454;&#29992;&#24615;&#21644;&#39640;&#21487;&#20280;&#32553;&#24615;&#30340;&#23436;&#32654;&#23433;&#20840;&#20445;&#38556;&#30340;&#38544;&#20889;&#31639;&#27861;&#25552;&#20379;&#20102;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Steganography is the practice of encoding secret information into innocuous content in such a manner that an adversarial third party would not realize that there is hidden meaning. While this problem has classically been studied in security literature, recent advances in generative models have led to a shared interest among security and machine learning researchers in developing scalable steganography techniques. In this work, we show that a steganography procedure is perfectly secure under Cachin (1998)'s information theoretic-model of steganography if and only if it is induced by a coupling. Furthermore, we show that, among perfectly secure procedures, a procedure is maximally efficient if and only if it is induced by a minimum entropy coupling. These insights yield what are, to the best of our knowledge, the first steganography algorithms to achieve perfect security guarantees with non-trivial efficiency; additionally, these algorithms are highly scalable. To provide empirical valid
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#19978;&#30340;Transformer&#12290;&#36890;&#36807;&#25913;&#36827;&#27880;&#24847;&#26426;&#21046;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2210.13148</link><description>&lt;p&gt;
&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#19978;&#30340;Transformer
&lt;/p&gt;
&lt;p&gt;
Transformers over Directed Acyclic Graphs. (arXiv:2210.13148v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.13148
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#21521;&#26080;&#29615;&#22270;&#19978;&#30340;Transformer&#12290;&#36890;&#36807;&#25913;&#36827;&#27880;&#24847;&#26426;&#21046;&#21644;&#20301;&#32622;&#32534;&#30721;&#65292;&#26412;&#26041;&#27861;&#22312;&#22810;&#31181;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;Transformer&#27169;&#22411;&#22312;&#22270;&#34920;&#31034;&#23398;&#20064;&#20013;&#21464;&#24471;&#27969;&#34892;&#36215;&#26469;&#65292;&#22240;&#20026;&#23427;&#20204;&#26377;&#33021;&#21147;&#23398;&#20064;&#36229;&#20986;&#24120;&#35268;&#22270;&#31070;&#32463;&#32593;&#32476;&#25429;&#25417;&#21040;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#20027;&#35201;&#30340;&#30740;&#31350;&#38382;&#39064;&#26159;&#22914;&#20309;&#23558;&#22270;&#30340;&#32467;&#26500;&#20559;&#24046;&#27880;&#20837;&#21040;Transformer&#30340;&#26550;&#26500;&#20013;&#65292;&#24182;&#38024;&#23545;&#26377;&#21521;&#26080;&#29615;&#22270;&#65288;DAGs&#65289;&#25552;&#20986;&#20102;&#19968;&#20123;&#36866;&#24212;&#24615;&#30340;&#26550;&#26500;&#25913;&#36827;&#65306;&#65288;1&#65289;&#19968;&#20010;&#27604;&#24120;&#35268;Transformer&#30340;&#20108;&#27425;&#22797;&#26434;&#24230;&#26356;&#39640;&#25928;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#21516;&#26102;&#24544;&#23454;&#22320;&#25429;&#25417;&#20102;DAGs&#30340;&#32467;&#26500;&#65292;&#65288;2&#65289;&#19968;&#20010;&#23545;DAG&#30340;&#20559;&#24207;&#36827;&#34892;&#20301;&#32622;&#32534;&#30721;&#65292;&#34917;&#20805;&#20102;&#21069;&#32773;&#12290;&#25105;&#20204;&#23545;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21508;&#31181;&#31867;&#22411;&#30340;&#20219;&#21153;&#19978;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#35780;&#20272;&#65292;&#20174;&#23545;&#28304;&#20195;&#30721;&#22270;&#30340;&#20998;&#31867;&#21040;&#23545;&#24341;&#29992;&#32593;&#32476;&#20013;&#30340;&#33410;&#28857;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#22312;&#20004;&#20010;&#37325;&#35201;&#30340;&#20219;&#21153;&#19978;&#26159;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer models have recently gained popularity in graph representation learning as they have the potential to learn complex relationships beyond the ones captured by regular graph neural networks. The main research question is how to inject the structural bias of graphs into the transformer architecture, and several proposals have been made for undirected molecular graphs and, recently, also for larger network graphs. In this paper, we study transformers over directed acyclic graphs (DAGs) and propose architecture adaptations tailored to DAGs: (1) An attention mechanism that is considerably more efficient than the regular quadratic complexity of transformers and at the same time faithfully captures the DAG structure, and (2) a positional encoding of the DAG's partial order, complementing the former. We rigorously evaluate our approach over various types of tasks, ranging from classifying source code graphs to nodes in citation networks, and show that it is effective in two importan
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30740;&#31350;&#24322;&#26500;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#24178;&#25200;&#29616;&#35937;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#26412;&#22320;&#27169;&#22411;&#26469;&#25191;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#26041;&#38754;&#26356;&#21152;&#40065;&#26834;&#12290;</title><link>http://arxiv.org/abs/2210.12974</link><description>&lt;p&gt;
&#30740;&#31350;&#24322;&#26500;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#31070;&#32463;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
Investigating Neuron Disturbing in Fusing Heterogeneous Neural Networks. (arXiv:2210.12974v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12974
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#30740;&#31350;&#24322;&#26500;&#31070;&#32463;&#32593;&#32476;&#20013;&#31070;&#32463;&#24178;&#25200;&#29616;&#35937;&#30340;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#39564;&#39564;&#35777;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#26412;&#22320;&#27169;&#22411;&#26469;&#25191;&#34892;&#39044;&#27979;&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#26041;&#38754;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#22312;&#19981;&#21516;&#20301;&#32622;&#19978;&#35757;&#32451;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#34701;&#21512;&#25104;&#19968;&#20010;&#20840;&#23616;&#27169;&#22411;&#26159;&#32852;&#37030;&#23398;&#20064;&#30340;&#30452;&#25509;&#23454;&#29616;&#12290;&#23613;&#31649;&#24403;&#21069;&#30340;&#27169;&#22411;&#34701;&#21512;&#26041;&#27861;&#22312;&#34701;&#21512;&#20960;&#20046;&#30456;&#21516;&#32467;&#26500;&#30340;&#31070;&#32463;&#32593;&#32476;&#26102;&#22312;&#23454;&#39564;&#19978;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23427;&#20204;&#24456;&#23569;&#34987;&#36827;&#34892;&#29702;&#35770;&#20998;&#26512;&#12290;&#26412;&#25991;&#20174;&#36125;&#21494;&#26031;&#35266;&#28857;&#32467;&#21512;&#26469;&#33258;&#23458;&#25143;&#31471;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#21644;&#31070;&#32463;&#32593;&#32476;&#30340;&#29305;&#24615;&#65292;&#25581;&#31034;&#20102;&#31070;&#32463;&#20803;&#24178;&#25200;&#30340;&#29616;&#35937;&#65292;&#21363;&#24322;&#26500;&#26412;&#22320;&#27169;&#22411;&#30340;&#31070;&#32463;&#20803;&#30456;&#20114;&#24178;&#25200;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#33258;&#36866;&#24212;&#36873;&#25321;&#26412;&#22320;&#27169;&#22411;&#26469;&#25191;&#34892;&#39044;&#27979;&#30340;&#23454;&#39564;&#26041;&#27861;&#65292;&#31216;&#20026;AMS&#65292;&#20197;&#25490;&#38500;&#31070;&#32463;&#20803;&#24178;&#25200;&#24182;&#34701;&#21512;&#31070;&#32463;&#32593;&#32476;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;AMS&#22312;&#25968;&#25454;&#24322;&#36136;&#24615;&#26041;&#38754;&#27604;&#19968;&#33324;&#30340;&#27169;&#22411;&#34701;&#21512;&#21644;&#38598;&#25104;&#26041;&#27861;&#26356;&#21152;&#40065;&#26834;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fusing deep learning models trained on separately located clients into a global model in a one-shot communication round is a straightforward implementation of Federated Learning. Although current model fusion methods are shown experimentally valid in fusing neural networks with almost identical architectures, they are rarely theoretically analyzed. In this paper, we reveal the phenomenon of neuron disturbing, where neurons from heterogeneous local models interfere with each other mutually. We give detailed explanations from a Bayesian viewpoint combining the data heterogeneity among clients and properties of neural networks. Furthermore, to validate our findings, we propose an experimental method that excludes neuron disturbing and fuses neural networks via adaptively selecting a local model, called AMS, to execute the prediction according to the input. The experiments demonstrate that AMS is more robust in data heterogeneity than general model fusion and ensemble methods. This implies
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#20174;&#19968;&#33324;&#39046;&#22495;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#21040;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2210.12770</link><description>&lt;p&gt;
&#20851;&#20110;&#20020;&#24202;&#25991;&#26412;&#25366;&#25496;&#30340;&#36328;&#39046;&#22495;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65306;&#22312;&#25968;&#25454;&#21463;&#38480;&#24494;&#35843;&#20013;&#23427;&#20204;&#34920;&#29616;&#22914;&#20309;&#65311;
&lt;/p&gt;
&lt;p&gt;
On Cross-Domain Pre-Trained Language Models for Clinical Text Mining: How Do They Perform on Data-Constrained Fine-Tuning?. (arXiv:2210.12770v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.12770
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#65292;&#23558;&#20174;&#19968;&#33324;&#39046;&#22495;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#24494;&#35843;&#21040;&#29305;&#23450;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#65292;&#24494;&#35843;&#30340;&#35821;&#35328;&#27169;&#22411;&#30456;&#23545;&#20110;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;&#27169;&#22411;&#22312;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#65292;&#20351;&#29992;&#20174;&#19968;&#33324;&#25110;&#30456;&#20851;&#39046;&#22495;&#25968;&#25454;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#23558;&#20854;&#24494;&#35843;&#21040;&#29305;&#23450;&#39046;&#22495;&#21644;&#20219;&#21153;&#19978;&#65292;&#24182;&#20351;&#29992;&#26032;&#20219;&#21153;&#20013;&#21487;&#29992;&#30340;&#26377;&#38480;&#36164;&#28304;&#36827;&#34892;&#24494;&#35843;&#65292;&#19968;&#30452;&#20197;&#26469;&#37117;&#26159;&#19968;&#20010;&#27969;&#34892;&#30340;&#23454;&#36341;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#37325;&#26032;&#32771;&#34385;&#20102;&#36825;&#31181;&#20551;&#35774;&#65292;&#24182;&#22312;&#20020;&#24202;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#20855;&#20307;&#26159;&#22312;&#33647;&#29289;&#21450;&#20854;&#30456;&#20851;&#23646;&#24615;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#20219;&#21153;&#19978;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;Transformer&#27169;&#22411;&#21644;&#36890;&#36807;&#24494;&#35843;BERT-based LLMs&#65288;&#21253;&#25324;BERT-base&#12289;BioBERT&#21644;ClinicalBERT&#65289;&#36827;&#34892;&#24494;&#35843;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#23545;&#36825;&#20123;&#27169;&#22411;&#21450;&#20854;&#25193;&#23637;&#27169;&#22411;&#19982;&#24102;&#26377;CRF&#23618;&#30340;&#36830;&#32493;&#23398;&#20064;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#20351;&#29992;n2c2-2018&#20849;&#20139;&#20219;&#21153;&#25968;&#25454;&#36827;&#34892;&#27169;&#22411;&#24320;&#21457;&#21644;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65306;1&#65289;CRF&#23618;&#23545;&#25152;&#26377;&#31070;&#32463;&#27169;&#22411;&#37117;&#36215;&#21040;&#20102;&#31215;&#26497;&#30340;&#24433;&#21709;&#65307;2&#65289;&#22312;&#20351;&#29992;&#23439;&#24179;&#22343;F1&#23545;BIO-strict&#36328;&#24230;&#32423;&#21035;&#36827;&#34892;&#35780;&#20272;&#26102;&#65292;&#24494;&#35843;&#30340;LLMs&#33719;&#24471;&#20102;0.83+&#30340;&#24471;&#20998;&#65292;&#32780;&#20174;&#22836;&#24320;&#22987;&#23398;&#20064;&#30340;TransformerCRF&#27169;&#22411;&#24471;&#20998;&#20026;0.78+&#65292;&#35777;&#26126;&#20102;&#24494;&#35843;&#27169;&#22411;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Large Language Models (LLMs) pre-trained from general or related domain data to a specific domain and task using a limited amount of resources available in the new task has been a popular practice in NLP fields. In this work, we re-visit this assumption, and carry out investigation in clinical NLP, specifically named-entity recognition on Drugs and their related Attributes. We compare Transformer models that are learned from scratch to fine-tuning BERT-based LLMs including BERT-base, BioBERT, and ClinicalBERT. We also investigate the comparison of such models and their extended models with a CRF layer for continuous learning. We use n2c2-2018 shared task data for model development and evaluations. The experimental outcomes show that 1) the CRF layer makes a difference for all neural models; 2) on BIO-strict span level evaluation using macro-average F1, while the fine-tuned LLMs achieved scores 0.83+, the TransformerCRF model learned from scratch achieved 0.78+ demonstrating
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#36827;&#34892;&#20844;&#24179;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20559;&#35265;&#26159;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#22266;&#26377;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#33080;&#35782;&#21035;&#31561;&#38590;&#39064;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;</title><link>http://arxiv.org/abs/2210.09943</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#20559;&#35265;&#32531;&#35299;&#65306;&#26356;&#20844;&#24179;&#30340;&#26550;&#26500;&#23454;&#29616;&#26356;&#20844;&#24179;&#30340;&#20154;&#33080;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Rethinking Bias Mitigation: Fairer Architectures Make for Fairer Face Recognition. (arXiv:2210.09943v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.09943
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#36827;&#34892;&#20844;&#24179;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#25105;&#20204;&#21457;&#29616;&#20559;&#35265;&#26159;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#22266;&#26377;&#30340;&#65292;&#32780;&#19981;&#20165;&#20165;&#26159;&#35757;&#32451;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20154;&#33080;&#35782;&#21035;&#31561;&#38590;&#39064;&#19978;&#21462;&#24471;&#20102;&#27604;&#20854;&#20182;&#26041;&#27861;&#26356;&#22909;&#30340;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33080;&#35782;&#21035;&#31995;&#32479;&#24191;&#27867;&#24212;&#29992;&#20110;&#25191;&#27861;&#31561;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#65292;&#20294;&#23427;&#20204;&#22312;&#24615;&#21035;&#21644;&#31181;&#26063;&#31561;&#31038;&#20250;&#20154;&#21475;&#32479;&#35745;&#32500;&#24230;&#19978;&#23384;&#22312;&#20559;&#35265;&#12290;&#20256;&#32479;&#35266;&#28857;&#35748;&#20026;&#65292;&#27169;&#22411;&#20559;&#35265;&#28304;&#20110;&#26377;&#20559;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#22240;&#27492;&#65292;&#20197;&#24448;&#20851;&#20110;&#20559;&#35265;&#32531;&#35299;&#30340;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#22312;&#39044;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#12289;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#28155;&#21152;&#24809;&#32602;&#39033;&#20197;&#38450;&#27490;&#20559;&#35265;&#24433;&#21709;&#27169;&#22411;&#65292;&#25110;&#23545;&#39044;&#27979;&#32467;&#26524;&#36827;&#34892;&#21518;&#22788;&#29702;&#20197;&#28040;&#38500;&#20559;&#35265;&#65292;&#20294;&#36825;&#20123;&#26041;&#27861;&#22312;&#20154;&#33080;&#35782;&#21035;&#31561;&#38590;&#39064;&#19978;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20559;&#35265;&#23454;&#38469;&#19978;&#26681;&#28304;&#20110;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26412;&#36523;&#12290;&#22522;&#20110;&#36825;&#19968;&#37325;&#26032;&#23450;&#20041;&#65292;&#25105;&#20204;&#39318;&#27425;&#36827;&#34892;&#20102;&#20844;&#24179;&#24615;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#21516;&#26102;&#36827;&#34892;&#20102;&#36229;&#21442;&#25968;&#25628;&#32034;&#12290;&#25105;&#20204;&#30340;&#25628;&#32034;&#36755;&#20986;&#20102;&#19968;&#31995;&#21015;&#22312;&#20934;&#30830;&#24615;&#21644;&#20844;&#24179;&#24615;&#26041;&#38754;&#22343;&#20248;&#20110;&#20854;&#20182;&#39640;&#24615;&#33021;&#26550;&#26500;&#21644;&#29616;&#26377;&#20559;&#35265;&#32531;&#35299;&#26041;&#27861;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Face recognition systems are widely deployed in safety-critical applications, including law enforcement, yet they exhibit bias across a range of socio-demographic dimensions, such as gender and race. Conventional wisdom dictates that model biases arise from biased training data. As a consequence, previous works on bias mitigation largely focused on pre-processing the training data, adding penalties to prevent bias from effecting the model during training, or post-processing predictions to debias them, yet these approaches have shown limited success on hard problems such as face recognition. In our work, we discover that biases are actually inherent to neural network architectures themselves. Following this reframing, we conduct the first neural architecture search for fairness, jointly with a search for hyperparameters. Our search outputs a suite of models which Pareto-dominate all other high-performance architectures and existing bias mitigation methods in terms of accuracy and fairne
&lt;/p&gt;</description></item><item><title>ConSpec&#26159;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#65292;&#20855;&#26377;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#27493;&#39588;&#21644;&#21487;&#35299;&#37322;&#30340;&#20449;&#29992;&#20998;&#37197;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2210.05845</link><description>&lt;p&gt;
ConSpec: &#31361;&#20986;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
ConSpec: honing in on critical steps for rapid learning and generalization in RL. (arXiv:2210.05845v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05845
&lt;/p&gt;
&lt;p&gt;
ConSpec&#26159;&#19968;&#20010;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#23454;&#29616;&#24555;&#36895;&#23398;&#20064;&#21644;&#27867;&#21270;&#12290;&#35813;&#31639;&#27861;&#36890;&#36807;&#23398;&#20064;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#65292;&#20855;&#26377;&#24555;&#36895;&#35782;&#21035;&#20851;&#38190;&#27493;&#39588;&#21644;&#21487;&#35299;&#37322;&#30340;&#20449;&#29992;&#20998;&#37197;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#29616;&#23454;&#29983;&#27963;&#20013;&#65292;&#25104;&#21151;&#24448;&#24448;&#21462;&#20915;&#20110;&#22810;&#20010;&#20851;&#38190;&#27493;&#39588;&#65292;&#36825;&#20123;&#27493;&#39588;&#22312;&#26102;&#38388;&#19978;&#30456;&#36317;&#36739;&#36828;&#65292;&#19982;&#26368;&#32456;&#22870;&#21169;&#20063;&#30456;&#36317;&#29978;&#36828;&#12290;&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#20381;&#36182;Bellman&#26041;&#31243;&#65292;&#24456;&#38590;&#35782;&#21035;&#36825;&#20123;&#20851;&#38190;&#27493;&#39588;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#65292;&#20351;&#29992;&#31163;&#32447;&#23545;&#27604;&#23398;&#20064;&#26469;&#30830;&#23450;&#20851;&#38190;&#27493;&#39588;&#12290;&#36825;&#20010;&#31639;&#27861;&#34987;&#31216;&#20026;&#23545;&#27604;&#20869;&#30465;&#65288;ConSpec&#65289;&#65292;&#21487;&#20197;&#28155;&#21152;&#21040;&#20219;&#20309;&#29616;&#26377;&#30340;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#20013;&#12290;ConSpec&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#27604;&#25439;&#22833;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#30340;&#21407;&#22411;&#65292;&#24182;&#22312;&#24403;&#21069;&#29366;&#24577;&#19982;&#36825;&#20123;&#21407;&#22411;&#20043;&#19968;&#21305;&#37197;&#26102;&#25552;&#20379;&#20869;&#22312;&#22870;&#21169;&#12290;ConSpec&#20013;&#30340;&#21407;&#22411;&#22312;&#20449;&#29992;&#20998;&#37197;&#26041;&#38754;&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#20248;&#21183;&#65306;&#65288;1&#65289;&#23427;&#20204;&#20351;&#24471;&#33021;&#22815;&#36805;&#36895;&#35782;&#21035;&#25152;&#26377;&#20851;&#38190;&#27493;&#39588;&#65307;&#65288;2&#65289;&#23427;&#20204;&#20197;&#23481;&#26131;&#35299;&#37322;&#30340;&#26041;&#24335;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20351;&#24471;&#22312;&#24863;&#35273;&#29305;&#24449;&#25913;&#21464;&#26102;&#21487;&#20197;&#36827;&#34892;&#36229;&#20986;&#20998;&#24067;&#30340;&#27867;&#21270;&#12290;&#19982;&#20854;&#20182;&#24403;&#20195;&#30340;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#19981;&#21516;&#65292;
&lt;/p&gt;
&lt;p&gt;
In real life, success is often contingent upon multiple critical steps that are distant in time from each other and from the final reward. These critical steps are challenging to identify with traditional reinforcement learning (RL) methods that rely on the Bellman equation for credit assignment. Here, we present a new RL algorithm that uses offline contrastive learning to hone in on critical steps. This algorithm, which we call contrastive introspection (ConSpec), can be added to any existing RL algorithm. ConSpec learns a set of prototypes for the critical steps in a task by a novel contrastive loss and delivers an intrinsic reward when the current state matches one of these prototypes. The prototypes in ConSpec provide two key benefits for credit assignment: (1) They enable rapid identification of all the critical steps. (2) They do so in a readily interpretable manner, enabling out-of-distribution generalization when sensory features are altered. Distinct from other contemporary RL
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#24335;&#21363;&#24109;&#22242;&#38431;&#21512;&#20316;&#38382;&#39064;&#65292;&#36890;&#36807;&#22522;&#20110;&#22270;&#30340;&#31574;&#30053;&#23398;&#20064;&#33719;&#24471;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#23436;&#20840;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#30340;&#22242;&#38431;&#21512;&#20316;&#12290;</title><link>http://arxiv.org/abs/2210.05448</link><description>&lt;p&gt;
&#20351;&#29992;&#22522;&#20110;&#22270;&#30340;&#31574;&#30053;&#23398;&#20064;&#30340;&#36890;&#29992;&#23398;&#20064;&#26694;&#26550;&#36827;&#34892;&#24320;&#25918;&#24335;&#21363;&#24109;&#22242;&#38431;&#21512;&#20316;
&lt;/p&gt;
&lt;p&gt;
A General Learning Framework for Open Ad Hoc Teamwork Using Graph-based Policy Learning. (arXiv:2210.05448v2 [cs.MA] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.05448
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#29992;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#24320;&#25918;&#24335;&#21363;&#24109;&#22242;&#38431;&#21512;&#20316;&#38382;&#39064;&#65292;&#36890;&#36807;&#22522;&#20110;&#22270;&#30340;&#31574;&#30053;&#23398;&#20064;&#33719;&#24471;&#26368;&#20248;&#31574;&#30053;&#65292;&#24182;&#19988;&#33021;&#22815;&#36866;&#24212;&#19981;&#23436;&#20840;&#21487;&#35266;&#27979;&#24773;&#20917;&#19979;&#30340;&#22242;&#38431;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24320;&#25918;&#24335;&#21363;&#24109;&#22242;&#38431;&#21512;&#20316;&#26159;&#25351;&#35757;&#32451;&#19968;&#20010;&#21333;&#19968;&#20195;&#29702;&#19982;&#19968;&#20010;&#26410;&#30693;&#30340;&#21487;&#33021;&#38543;&#26102;&#38388;&#21464;&#21270;&#30340;&#22242;&#38431;&#39640;&#25928;&#21512;&#20316;&#30340;&#38382;&#39064;&#12290;&#23545;&#20110;&#20195;&#29702;&#26469;&#35828;&#65292;&#21487;&#21464;&#30340;&#22242;&#38431;&#32452;&#25104;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22914;&#38656;&#35201;&#36866;&#24212;&#26032;&#30340;&#22242;&#38431;&#21160;&#24577;&#21644;&#22788;&#29702;&#19981;&#26029;&#21464;&#21270;&#30340;&#29366;&#24577;&#21521;&#37327;&#22823;&#23567;&#12290;&#36825;&#20123;&#25361;&#25112;&#22312;&#21463;&#25511;&#29615;&#22659;&#20165;&#20855;&#26377;&#37096;&#20998;&#35270;&#22270;&#30340;&#30495;&#23454;&#24212;&#29992;&#20013;&#26356;&#21152;&#20005;&#23803;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#38024;&#23545;&#23436;&#20840;&#21644;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#24320;&#21457;&#20102;&#19968;&#31867;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#39318;&#20808;&#38024;&#23545;&#23436;&#20840;&#21487;&#35266;&#27979;&#30340;&#24773;&#20917;&#24320;&#21457;&#20102;&#19968;&#31181;&#35299;&#20915;&#26041;&#26696;&#65292;&#35813;&#35299;&#20915;&#26041;&#26696;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#22522;&#20110;&#24378;&#21270;&#23398;&#20064;&#33719;&#24471;&#26368;&#20248;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#23558;&#36825;&#31181;&#35299;&#20915;&#26041;&#26696;&#25193;&#23637;&#21040;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#22330;&#26223;&#20013;&#65292;&#36825;&#20123;&#26041;&#27861;&#21487;&#20197;&#22312;&#28508;&#22312;&#30340;&#29615;&#22659;&#29366;&#24577;&#21644;&#22242;&#38431;&#32452;&#25104;&#19978;&#20445;&#25345;&#20449;&#24565;&#20272;&#35745;&#12290;&#36825;&#20123;&#20449;&#24565;&#20272;&#35745;&#19982;&#25105;&#20204;&#23545;&#23436;&#20840;&#21487;&#35266;&#27979;&#24773;&#20917;&#30340;&#35299;&#20915;&#26041;&#26696;&#30456;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Open ad hoc teamwork is the problem of training a single agent to efficiently collaborate with an unknown group of teammates whose composition may change over time. A variable team composition creates challenges for the agent, such as the requirement to adapt to new team dynamics and dealing with changing state vector sizes. These challenges are aggravated in real-world applications in which the controlled agent only has a partial view of the environment. In this work, we develop a class of solutions for open ad hoc teamwork under full and partial observability. We start by developing a solution for the fully observable case that leverages graph neural network architectures to obtain an optimal policy based on reinforcement learning. We then extend this solution to partially observable scenarios by proposing different methodologies that maintain belief estimates over the latent environment states and team composition. These belief estimates are combined with our solution for the fully 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#31532;&#19968;&#20010;&#31995;&#32479;&#30740;&#31350;&#36229;&#20998;&#24067;&#27867;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#19981;&#21516;&#32500;&#24230;&#30340;&#28304;&#20195;&#30721;&#25968;&#25454;&#23646;&#24615;&#21644;&#24494;&#35843;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30740;&#31350;&#20102;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;</title><link>http://arxiv.org/abs/2210.04802</link><description>&lt;p&gt;
SimSCOOD: Fine-tuned&#28304;&#20195;&#30721;&#27169;&#22411;&#30340;&#36229;&#20998;&#24067;&#27867;&#21270;&#30340;&#31995;&#32479;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
SimSCOOD: Systematic Analysis of Out-of-Distribution Generalization in Fine-tuned Source Code Models. (arXiv:2210.04802v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04802
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#31532;&#19968;&#20010;&#31995;&#32479;&#30740;&#31350;&#36229;&#20998;&#24067;&#27867;&#21270;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#27169;&#25311;&#19981;&#21516;&#32500;&#24230;&#30340;&#28304;&#20195;&#30721;&#25968;&#25454;&#23646;&#24615;&#21644;&#24494;&#35843;&#26041;&#27861;&#65292;&#22312;&#19981;&#21516;&#22330;&#26223;&#20013;&#30740;&#31350;&#20102;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#20195;&#30721;&#25968;&#25454;&#38598;&#24050;&#32463;&#36234;&#26469;&#36234;&#23481;&#26131;&#22320;&#29992;&#20110;&#39044;&#35757;&#32451;&#28304;&#20195;&#30721;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24494;&#35843;&#38454;&#27573;&#26469;&#35828;&#65292;&#33719;&#21462;&#20195;&#34920;&#24615;&#30340;&#35757;&#32451;&#25968;&#25454;&#20197;&#20805;&#20998;&#35206;&#30422;&#29305;&#23450;&#19979;&#28216;&#20219;&#21153;&#30340;&#20195;&#30721;&#20998;&#24067;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21407;&#22240;&#26159;&#20219;&#21153;&#29305;&#23450;&#24615;&#21644;&#26377;&#38480;&#30340;&#26631;&#27880;&#36164;&#28304;&#12290;&#27492;&#22806;&#65292;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36951;&#24536;&#20197;&#21069;&#33719;&#24471;&#30340;&#39044;&#35757;&#32451;&#30693;&#35782;&#12290;&#36825;&#20123;&#38382;&#39064;&#23548;&#33268;&#20102;&#36229;&#20998;&#24067;&#27867;&#21270;&#38382;&#39064;&#65292;&#21363;&#27169;&#22411;&#30340;&#25512;&#29702;&#34892;&#20026;&#20986;&#29616;&#24847;&#22806;&#24773;&#20917;&#65292;&#36825;&#23578;&#26410;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#31995;&#32479;&#26041;&#27861;&#65292;&#27169;&#25311;&#20102;&#19981;&#21516;&#32500;&#24230;&#28304;&#20195;&#30721;&#25968;&#25454;&#23646;&#24615;&#30340;&#21508;&#31181;&#36229;&#20998;&#24067;&#22330;&#26223;&#65292;&#24182;&#30740;&#31350;&#20102;&#36825;&#20123;&#22330;&#26223;&#20013;&#24494;&#35843;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#24494;&#35843;&#26041;&#27861;&#65288;&#21253;&#25324;&#20840;&#24494;&#35843;&#21644;&#20302;&#31209;&#36866;&#24212;&#24494;&#35843;&#26041;&#27861;&#65289;&#19979;&#27169;&#22411;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#22312;&#21508;&#20010;&#31995;&#32479;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large code datasets have become increasingly accessible for pre-training source code models. However, for the fine-tuning phase, obtaining representative training data that fully covers the code distribution for specific downstream tasks remains challenging due to the task-specific nature and limited labeling resources. Moreover, fine-tuning pretrained models can result in forgetting previously acquired pre-training knowledge. These lead to out-of-distribution (OOD) generalization issues with unexpected model inference behaviors that have not been systematically studied yet. In this paper, we contribute the first systematic approach that simulates various OOD scenarios along different dimensions of source code data properties and study the fine-tuned model behaviors in such scenarios. We investigate the behaviors of models under different fine-tuning methodologies, including full fine-tuning and Low-Rank Adaptation (LoRA) fine-tuning methods. Our comprehensive analysis, conducted on fo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#29702;&#35299;&#19982;&#30446;&#26631;&#20107;&#20214;&#26377;&#20851;&#30340;&#27493;&#39588;&#20107;&#20214;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#24182;&#36129;&#29486;&#20102;&#19968;&#20010;&#30001;&#19987;&#23478;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#37325;&#35201;&#24615;&#30340;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2210.04074</link><description>&lt;p&gt;
&#25152;&#26377;&#27493;&#39588;&#37117;&#21516;&#31561;&#37325;&#35201;&#21527;&#65311;&#22522;&#20110;&#20107;&#20214;&#30340;&#37325;&#35201;&#24615;&#26816;&#27979;&#30340;&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are All Steps Equally Important? Benchmarking Essentiality Detection of Events. (arXiv:2210.04074v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.04074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#24403;&#21069;&#27169;&#22411;&#22312;&#29702;&#35299;&#19982;&#30446;&#26631;&#20107;&#20214;&#26377;&#20851;&#30340;&#27493;&#39588;&#20107;&#20214;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#30340;&#22256;&#38590;&#65292;&#24182;&#36129;&#29486;&#20102;&#19968;&#20010;&#30001;&#19987;&#23478;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#37325;&#35201;&#24615;&#30340;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#35821;&#35328;&#20197;&#19981;&#21516;&#30340;&#32454;&#31890;&#24230;&#34920;&#36798;&#20107;&#20214;&#65292;&#20854;&#20013;&#31895;&#31890;&#24230;&#20107;&#20214;&#65288;&#30446;&#26631;&#65289;&#21487;&#20197;&#32454;&#20998;&#20026;&#26356;&#32454;&#31890;&#24230;&#30340;&#20107;&#20214;&#24207;&#21015;&#65288;&#27493;&#39588;&#65289;&#12290;&#29702;&#35299;&#20107;&#20214;&#36807;&#31243;&#30340;&#19968;&#20010;&#20851;&#38190;&#20294;&#24120;&#34987;&#24573;&#35270;&#30340;&#26041;&#38754;&#26159;&#35748;&#35782;&#21040;&#24182;&#38750;&#25152;&#26377;&#27493;&#39588;&#20107;&#20214;&#23545;&#20110;&#23436;&#25104;&#30446;&#26631;&#20855;&#26377;&#30456;&#21516;&#30340;&#37325;&#35201;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#30740;&#31350;&#24403;&#21069;&#27169;&#22411;&#29702;&#35299;&#19982;&#30446;&#26631;&#20107;&#20214;&#26377;&#20851;&#30340;&#27493;&#39588;&#20107;&#20214;&#30340;&#37325;&#35201;&#24615;&#30340;&#31243;&#24230;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#35748;&#30693;&#30740;&#31350;&#34920;&#26126;&#65292;&#36825;&#31181;&#33021;&#21147;&#20351;&#26426;&#22120;&#33021;&#22815;&#27169;&#25311;&#20154;&#31867;&#23545;&#26085;&#24120;&#20219;&#21153;&#30340;&#20808;&#20915;&#26465;&#20214;&#21644;&#24517;&#35201;&#21162;&#21147;&#30340;&#24120;&#35782;&#25512;&#29702;&#12290;&#25105;&#20204;&#36129;&#29486;&#20102;&#19968;&#20010;&#20248;&#36136;&#30340;&#35821;&#26009;&#24211;&#65288;&#30446;&#26631;&#65292;&#27493;&#39588;&#65289;&#23545;&#65292;&#35813;&#35821;&#26009;&#24211;&#20174;&#31038;&#21306;&#25351;&#21335;&#32593;&#31449;WikiHow&#25910;&#38598;&#65292;&#36890;&#36807;&#19987;&#23478;&#25163;&#21160;&#27880;&#37322;&#27493;&#39588;&#30340;&#37325;&#35201;&#24615;&#12290;&#39640;&#19968;&#33268;&#24615;&#30340;&#26631;&#27880;&#32773;&#38388;&#19968;&#33268;&#24615;&#34920;&#26126;&#20154;&#31867;&#23545;&#20107;&#20214;&#37325;&#35201;&#24615;&#20855;&#26377;&#19968;&#33268;&#30340;&#29702;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#35780;&#20272;&#22810;&#20010;&#32479;&#35745;&#27169;&#22411;&#20043;&#21518;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#21069;&#30340;&#27169;&#22411;&#22312;&#29702;&#35299;&#27493;&#39588;&#30340;&#37325;&#35201;&#24615;&#26041;&#38754;&#36824;&#23384;&#22312;&#22256;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;
Natural language expresses events with varying granularities, where coarse-grained events (goals) can be broken down into finer-grained event sequences (steps). A critical yet overlooked aspect of understanding event processes is recognizing that not all step events hold equal importance toward the completion of a goal. In this paper, we address this gap by examining the extent to which current models comprehend the essentiality of step events in relation to a goal event. Cognitive studies suggest that such capability enables machines to emulate human commonsense reasoning about preconditions and necessary efforts of everyday tasks. We contribute a high-quality corpus of (goal, step) pairs gathered from the community guideline website WikiHow, with steps manually annotated for their essentiality concerning the goal by experts. The high inter-annotator agreement demonstrates that humans possess a consistent understanding of event essentiality. However, after evaluating multiple statisti
&lt;/p&gt;</description></item><item><title>Learnware&#33539;&#24335;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20805;&#20998;&#21033;&#29992;&#23567;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#36229;&#36234;&#21407;&#22987;&#30446;&#30340;&#30340;&#20107;&#24773;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38754;&#20020;&#30340;&#25968;&#25454;&#37327;&#22823;&#12289;&#35757;&#32451;&#38590;&#24230;&#39640;&#12289;&#25968;&#25454;&#23433;&#20840;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.03647</link><description>&lt;p&gt;
Learnware: &#23567;&#27169;&#22411;&#23454;&#29616;&#22823;&#20316;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learnware: Small Models Do Big. (arXiv:2210.03647v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03647
&lt;/p&gt;
&lt;p&gt;
Learnware&#33539;&#24335;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20805;&#20998;&#21033;&#29992;&#23567;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#36229;&#36234;&#21407;&#22987;&#30446;&#30340;&#30340;&#20107;&#24773;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38754;&#20020;&#30340;&#25968;&#25454;&#37327;&#22823;&#12289;&#35757;&#32451;&#38590;&#24230;&#39640;&#12289;&#25968;&#25454;&#23433;&#20840;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23384;&#22312;&#35757;&#32451;&#25968;&#25454;&#37327;&#22823;&#12289;&#35757;&#32451;&#25216;&#33021;&#39640;&#12289;&#36830;&#32493;&#23398;&#20064;&#38590;&#12289;&#36951;&#24536;&#39118;&#38505;&#22823;&#12289;&#25968;&#25454;&#38544;&#31169;&#21644;&#19987;&#26377;&#20449;&#24687;&#27844;&#38706;&#31561;&#38382;&#39064;&#65292;&#32780;&#36807;&#21435;&#30340;&#22823;&#27169;&#22411;&#33539;&#24335;&#34429;&#28982;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#65292;&#20294;&#24182;&#26410;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21453;&#32780;&#25104;&#20026;&#20005;&#37325;&#30340;&#30899;&#25490;&#25918;&#28304;&#12290;&#35813;&#25991;&#27010;&#36848;&#20102;Learnware&#33539;&#24335;&#65292;&#35753;&#29992;&#25143;&#19981;&#38656;&#35201;&#20174;&#22836;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24076;&#26395;&#21033;&#29992;&#23567;&#22411;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#21407;&#22987;&#30446;&#30340;&#30340;&#20107;&#24773;&#65292;&#20854;&#20013;&#20851;&#38190;&#26159;&#35268;&#33539;&#65292;&#21487;&#20197;&#20351;&#35757;&#32451;&#30340;&#27169;&#22411;&#24471;&#21040;&#20805;&#20998;&#37492;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are complaints about current machine learning techniques such as the requirement of a huge amount of training data and proficient training skills, the difficulty of continual learning, the risk of catastrophic forgetting, the leaking of data privacy/proprietary, etc. Most research efforts have been focusing on one of those concerned issues separately, paying less attention to the fact that most issues are entangled in practice. The prevailing big model paradigm, which has achieved impressive results in natural language processing and computer vision applications, has not yet addressed those issues, whereas becoming a serious source of carbon emissions. This article offers an overview of the learnware paradigm, which attempts to enable users not need to build machine learning models from scratch, with the hope of reusing small models to do things even beyond their original purposes, where the key ingredient is the specification which enables a trained model to be adequately identi
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#27969;&#24418;&#30340;&#28508;&#31354;&#38388;&#26469;&#25913;&#36827;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;GM-VAE&#26041;&#27861;&#22312;&#23494;&#24230;&#20272;&#35745;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2209.15217</link><description>&lt;p&gt;
&#36890;&#36807;&#38544;&#21464;&#37327;&#39640;&#26031;&#20998;&#24067;&#30340;&#21452;&#26354; VAE
&lt;/p&gt;
&lt;p&gt;
Hyperbolic VAE via Latent Gaussian Distributions. (arXiv:2209.15217v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15217
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20351;&#29992;&#39640;&#26031;&#27969;&#24418;&#30340;&#28508;&#31354;&#38388;&#26469;&#25913;&#36827;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(VAE)&#30340;&#26041;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#36825;&#31181;GM-VAE&#26041;&#27861;&#22312;&#23494;&#24230;&#20272;&#35745;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20855;&#26377;&#36739;&#24378;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#26031;&#27969;&#24418;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;(GM-VAE)&#65292;&#20854;&#28508;&#31354;&#38388;&#30001;&#19968;&#32452;&#39640;&#26031;&#20998;&#24067;&#32452;&#25104;&#12290;&#24050;&#30693;&#19968;&#32500;&#39640;&#26031;&#20998;&#24067;&#38598;&#21512;&#22312; Fisher &#20449;&#24687;&#24230;&#37327;&#19979;&#24418;&#25104;&#20102;&#19968;&#20010;&#21452;&#26354;&#31354;&#38388;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#39640;&#26031;&#27969;&#24418;&#12290;&#20026;&#20102;&#23398;&#20064;&#20855;&#26377;&#39640;&#26031;&#27969;&#24418;&#30340; VAE&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110; Kullback-Leibler &#25955;&#24230;&#30340;&#20266;&#39640;&#26031;&#27969;&#24418;&#27491;&#24577;&#20998;&#24067;&#65292;&#23427;&#26159;&#23545;&#24179;&#26041; Fisher-Rao &#36317;&#31163;&#30340;&#23616;&#37096;&#36817;&#20284;&#65292;&#29992;&#20110;&#23450;&#20041;&#28508;&#31354;&#38388;&#19978;&#30340;&#23494;&#24230;&#12290;&#22312;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102; GM-VAE &#22312;&#20004;&#20010;&#19981;&#21516;&#20219;&#21153;&#19978;&#30340;&#26377;&#25928;&#24615;&#65306;&#22270;&#20687;&#25968;&#25454;&#38598;&#23494;&#24230;&#20272;&#35745;&#21644;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#29615;&#22659;&#24314;&#27169;&#12290;GM-VAE &#22312;&#23494;&#24230;&#20272;&#35745;&#20219;&#21153;&#19978;&#36229;&#36234;&#20102;&#20854;&#20182;&#21452;&#26354;&#21644;&#27431;&#20960;&#37324;&#24471; VAE &#30340;&#21464;&#20307;&#65292;&#24182;&#22312;&#27169;&#22411;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#20013;&#23637;&#29616;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#25105;&#20204;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#25968;&#20540;&#31283;&#23450;&#24615;&#65292;&#35299;&#20915;&#20102;&#19968;&#20010;&#24120;&#35265;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a Gaussian manifold variational auto-encoder (GM-VAE) whose latent space consists of a set of Gaussian distributions. It is known that the set of the univariate Gaussian distributions with the Fisher information metric form a hyperbolic space, which we call a Gaussian manifold. To learn the VAE endowed with the Gaussian manifolds, we propose a pseudo-Gaussian manifold normal distribution based on the Kullback-Leibler divergence, a local approximation of the squared Fisher-Rao distance, to define a density over the latent space. In experiments, we demonstrate the efficacy of GM-VAE on two different tasks: density estimation of image datasets and environment modeling in model-based reinforcement learning. GM-VAE outperforms the other variants of hyperbolicand Euclidean-VAEs on density estimation tasks and shows competitive performance in model-based reinforcement learning. We observe that our model provides strong numerical stability, addressing a common limitation reported 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2207.07051</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#26174;&#31034;&#23545;&#25512;&#29702;&#20219;&#21153;&#20855;&#26377;&#31867;&#20284;&#20154;&#31867;&#30340;&#20869;&#23481;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Language models show human-like content effects on reasoning tasks. (arXiv:2207.07051v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.07051
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#20013;&#26159;&#21542;&#20687;&#20154;&#31867;&#19968;&#26679;&#36890;&#36807;&#28151;&#20837;&#20869;&#23481;&#26469;&#24433;&#21709;&#31572;&#26696;&#65292;&#32467;&#26524;&#21457;&#29616;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#33021;&#22815;&#25429;&#25417;&#21040;&#36825;&#31181;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#35937;&#25512;&#29702;&#26159;&#26234;&#33021;&#31995;&#32479;&#30340;&#20851;&#38190;&#33021;&#21147;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#25277;&#35937;&#25512;&#29702;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#39640;&#20110;&#38543;&#26426;&#30340;&#24615;&#33021;&#65292;&#20294;&#23384;&#22312;&#35768;&#22810;&#19981;&#23436;&#21892;&#20043;&#22788;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#30340;&#25277;&#35937;&#25512;&#29702;&#20063;&#26159;&#19981;&#23436;&#32654;&#30340;&#12290;&#20363;&#22914;&#65292;&#20154;&#31867;&#25512;&#29702;&#21463;&#21040;&#25105;&#20204;&#23545;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#21644;&#20449;&#24565;&#30340;&#24433;&#21709;&#65292;&#24182;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#8220;&#20869;&#23481;&#25928;&#24212;&#8221;&#65307;&#24403;&#38382;&#39064;&#30340;&#35821;&#20041;&#20869;&#23481;&#25903;&#25345;&#27491;&#30830;&#30340;&#36923;&#36753;&#25512;&#29702;&#26102;&#65292;&#20154;&#31867;&#26356;&#21487;&#38752;&#22320;&#36827;&#34892;&#25512;&#29702;&#12290;&#36825;&#20123;&#20869;&#23481;&#32416;&#32544;&#30340;&#25512;&#29702;&#27169;&#24335;&#22312;&#20851;&#20110;&#20154;&#31867;&#26234;&#33021;&#22522;&#26412;&#24615;&#36136;&#30340;&#20105;&#35770;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#35821;&#35328;&#27169;&#22411;&#26159;&#21542;&#20197;&#31867;&#20284;&#30340;&#26041;&#24335;&#28151;&#20837;&#20869;&#23481;&#26469;&#22238;&#31572;&#36923;&#36753;&#38382;&#39064;&#65292;&#36825;&#20123;&#35821;&#35328;&#27169;&#22411;&#30340;&#20808;&#39564;&#26399;&#26395;&#25429;&#25417;&#20102;&#19968;&#20123;&#20154;&#31867;&#30693;&#35782;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#36923;&#36753;&#25512;&#29702;&#20219;&#21153;&#19978;&#25506;&#32034;&#20102;&#36825;&#20010;&#38382;&#39064;&#65306;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#12289;&#21028;&#26029;&#19977;&#27573;&#35770;&#30340;&#36923;&#36753;&#26377;&#25928;&#24615;&#21644;Wason&#36873;&#25321;&#20219;&#21153;&#12290;&#25105;&#20204;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks, but exhibit many imperfections. However, human abstract reasoning is also imperfect. For example, human reasoning is affected by our real-world knowledge and beliefs, and shows notable "content effects"; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns play a central role in debates about the fundamental nature of human intelligence. Here, we investigate whether language models $\unicode{x2014}$ whose prior expectations capture some aspects of human knowledge $\unicode{x2014}$ similarly mix content into their answers to logical problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art large 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#26631;&#35760;&#30340;&#19981;&#30830;&#23450;&#31034;&#20363;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#24341;&#20837;&#28151;&#28102;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#23558;&#31034;&#20363;&#20998;&#20026;&#20998;&#24067;&#22806;&#12289;&#36793;&#30028;&#21644;&#39640;&#20998;&#24067;&#35823;&#20998;&#31867;&#21306;&#22495;&#30340;&#19977;&#31867;&#65292;&#26412;&#30740;&#31350;&#20026;&#35780;&#20272;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#35780;&#20272;&#22522;&#20934;&#12290;</title><link>http://arxiv.org/abs/2207.05161</link><description>&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#20013;&#30340;&#26631;&#35760;&#26159;&#20160;&#20040;&#65311;&#28508;&#22312;&#23494;&#24230;&#27169;&#22411;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
What is Flagged in Uncertainty Quantification? Latent Density Models for Uncertainty Categorization. (arXiv:2207.05161v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31867;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#26631;&#35760;&#30340;&#19981;&#30830;&#23450;&#31034;&#20363;&#36827;&#34892;&#20998;&#31867;&#12290;&#36890;&#36807;&#24341;&#20837;&#28151;&#28102;&#23494;&#24230;&#30697;&#38453;&#65292;&#24182;&#23558;&#31034;&#20363;&#20998;&#20026;&#20998;&#24067;&#22806;&#12289;&#36793;&#30028;&#21644;&#39640;&#20998;&#24067;&#35823;&#20998;&#31867;&#21306;&#22495;&#30340;&#19977;&#31867;&#65292;&#26412;&#30740;&#31350;&#20026;&#35780;&#20272;&#19981;&#21516;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#25552;&#20379;&#20102;&#26032;&#30340;&#35270;&#35282;&#21644;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#65288;UQ&#65289;&#23545;&#20110;&#21019;&#24314;&#21487;&#20449;&#36182;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#36817;&#24180;&#26469;&#65292;&#20986;&#29616;&#20102;&#35768;&#22810;&#33021;&#22815;&#26631;&#35760;&#21487;&#30097;&#26679;&#26412;&#30340;UQ&#26041;&#27861;&#65292;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#31350;&#31455;&#35782;&#21035;&#20102;&#20160;&#20040;&#20869;&#23481;&#24448;&#24448;&#19981;&#28165;&#26970;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#29992;&#20110;&#23545;&#20998;&#31867;&#20219;&#21153;&#20013;&#34987;UQ&#26041;&#27861;&#26631;&#35760;&#20026;&#19981;&#30830;&#23450;&#30340;&#31034;&#20363;&#36827;&#34892;&#20998;&#31867;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#28151;&#28102;&#23494;&#24230;&#30697;&#38453;&#8212;&#8212;&#22522;&#20110;&#26680;&#30340;&#35823;&#20998;&#31867;&#23494;&#24230;&#30340;&#36817;&#20284;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#23558;&#34987;&#32473;&#23450;&#19981;&#30830;&#23450;&#24615;&#26041;&#27861;&#35782;&#21035;&#20026;&#21487;&#30097;&#26679;&#26412;&#30340;&#31034;&#20363;&#20998;&#20026;&#19977;&#31867;&#65306;&#20998;&#24067;&#22806;&#65288;OOD&#65289;&#26679;&#26412;&#12289;&#36793;&#30028;&#65288;Bnd&#65289;&#26679;&#26412;&#21644;&#22788;&#20110;&#39640;&#20998;&#24067;&#35823;&#20998;&#31867;&#21306;&#22495;&#65288;IDM&#65289;&#30340;&#31034;&#20363;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#35780;&#20272;&#19981;&#21516;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#26041;&#27861;&#20043;&#38388;&#30340;&#24046;&#24322;&#25552;&#20379;&#20102;&#19968;&#31181;&#26032;&#30340;&#29420;&#29305;&#35270;&#35282;&#65292;&#20174;&#32780;&#24418;&#25104;&#20102;&#19968;&#20010;&#26377;&#20215;&#20540;&#30340;&#35780;&#20272;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Uncertainty Quantification (UQ) is essential for creating trustworthy machine learning models. Recent years have seen a steep rise in UQ methods that can flag suspicious examples, however, it is often unclear what exactly these methods identify. In this work, we propose a framework for categorizing uncertain examples flagged by UQ methods in classification tasks. We introduce the confusion density matrix -- a kernel-based approximation of the misclassification density -- and use this to categorize suspicious examples identified by a given uncertainty method into three classes: out-of-distribution (OOD) examples, boundary (Bnd) examples, and examples in regions of high in-distribution misclassification (IDM). Through extensive experiments, we show that our framework provides a new and distinct perspective for assessing differences between uncertainty quantification methods, thereby forming a valuable assessment benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25216;&#26415;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#35282;&#24230;&#20986;&#21457;&#65292;&#39318;&#27425;&#35843;&#26597;&#20102;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#20559;&#35265;&#26469;&#28304;&#30340;&#20998;&#31867;&#27861;&#21644;&#30001;&#20854;&#24341;&#36215;&#30340;&#27495;&#35270;&#31867;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#26426;&#22120;&#20154;&#23398;&#20064;&#39046;&#22495;&#20013;&#19981;&#20844;&#24179;&#32467;&#26524;&#30340;&#22330;&#26223;&#21644;&#32531;&#35299;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2207.03444</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#21644;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Fairness and Bias in Robot Learning. (arXiv:2207.03444v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25216;&#26415;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#35282;&#24230;&#20986;&#21457;&#65292;&#39318;&#27425;&#35843;&#26597;&#20102;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25552;&#20986;&#20102;&#20559;&#35265;&#26469;&#28304;&#30340;&#20998;&#31867;&#27861;&#21644;&#30001;&#20854;&#24341;&#36215;&#30340;&#27495;&#35270;&#31867;&#22411;&#65292;&#24182;&#25506;&#35752;&#20102;&#19981;&#21516;&#26426;&#22120;&#20154;&#23398;&#20064;&#39046;&#22495;&#20013;&#19981;&#20844;&#24179;&#32467;&#26524;&#30340;&#22330;&#26223;&#21644;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#26174;&#33879;&#22686;&#24378;&#20102;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#65292;&#20351;&#23427;&#20204;&#33021;&#22815;&#22312;&#20154;&#31867;&#29615;&#22659;&#20013;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#24182;&#36866;&#24212;&#25105;&#20204;&#19981;&#30830;&#23450;&#30340;&#30495;&#23454;&#19990;&#30028;&#12290;&#26368;&#36817;&#21508;&#20010;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#32771;&#34385;&#20844;&#24179;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#36825;&#20123;&#31639;&#27861;&#19981;&#20250;&#37325;&#22797;&#20154;&#31867;&#30340;&#20559;&#35265;&#65292;&#24182;&#22240;&#27492;&#23548;&#33268;&#20855;&#26377;&#27495;&#35270;&#24615;&#30340;&#32467;&#26524;&#12290;&#38543;&#30528;&#26426;&#22120;&#20154;&#23398;&#20064;&#31995;&#32479;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#20013;&#25191;&#34892;&#36234;&#26469;&#36234;&#22810;&#30340;&#20219;&#21153;&#65292;&#20102;&#35299;&#36825;&#31181;&#20559;&#35265;&#30340;&#24433;&#21709;&#20197;&#38450;&#27490;&#23545;&#26576;&#20123;&#20154;&#32676;&#20135;&#29983;&#24847;&#22806;&#34892;&#20026;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#25216;&#26415;&#12289;&#20262;&#29702;&#21644;&#27861;&#24459;&#25361;&#25112;&#30340;&#36328;&#23398;&#31185;&#35282;&#24230;&#25552;&#20986;&#20102;&#26426;&#22120;&#20154;&#23398;&#20064;&#20013;&#20844;&#24179;&#24615;&#30340;&#39318;&#27425;&#35843;&#26597;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20559;&#35265;&#26469;&#28304;&#30340;&#20998;&#31867;&#27861;&#21644;&#30001;&#23427;&#20204;&#24341;&#36215;&#30340;&#27495;&#35270;&#31867;&#22411;&#12290;&#36890;&#36807;&#19981;&#21516;&#26426;&#22120;&#20154;&#23398;&#20064;&#39046;&#22495;&#30340;&#31034;&#20363;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19981;&#20844;&#24179;&#32467;&#26524;&#30340;&#22330;&#26223;&#21644;&#32531;&#35299;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning has significantly enhanced the abilities of robots, enabling them to perform a wide range of tasks in human environments and adapt to our uncertain real world. Recent works in various machine learning domains have highlighted the importance of accounting for fairness to ensure that these algorithms do not reproduce human biases and consequently lead to discriminatory outcomes. With robot learning systems increasingly performing more and more tasks in our everyday lives, it is crucial to understand the influence of such biases to prevent unintended behavior toward certain groups of people. In this work, we present the first survey on fairness in robot learning from an interdisciplinary perspective spanning technical, ethical, and legal challenges. We propose a taxonomy for sources of bias and the resulting types of discrimination due to them. Using examples from different robot learning domains, we examine scenarios of unfair outcomes and strategies to mitigate them. We
&lt;/p&gt;</description></item><item><title>SCOPE &#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21322;&#30417;&#30563;&#23545;&#27604;&#24322;&#24120;&#20540;&#21076;&#38500;&#30340;&#20266;&#26368;&#22823;&#26399;&#26395;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#28151;&#28102;&#38169;&#35823;&#26469;&#25552;&#39640;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2206.14261</link><description>&lt;p&gt;
&#21322;&#30417;&#30563;&#23545;&#27604;&#24322;&#24120;&#20540;&#21076;&#38500;&#30340;&#20266;&#26368;&#22823;&#26399;&#26395;&#21270;&#26041;&#27861;&#65288;SCOPE&#65289;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised Contrastive Outlier removal for Pseudo Expectation Maximization (SCOPE). (arXiv:2206.14261v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.14261
&lt;/p&gt;
&lt;p&gt;
SCOPE &#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#21322;&#30417;&#30563;&#23545;&#27604;&#24322;&#24120;&#20540;&#21076;&#38500;&#30340;&#20266;&#26368;&#22823;&#26399;&#26395;&#21270;&#26041;&#27861;&#65292;&#36890;&#36807;&#25233;&#21046;&#28151;&#28102;&#38169;&#35823;&#26469;&#25552;&#39640;&#21322;&#30417;&#30563;&#23398;&#20064;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21322;&#30417;&#30563;&#23398;&#20064;&#26159;&#23558;&#23569;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#19982;&#36739;&#22823;&#30340;&#26410;&#26631;&#35760;&#25968;&#25454;&#38598;&#30456;&#32467;&#21512;&#65292;&#35757;&#32451;&#20986;&#20934;&#30830;&#30340;&#39044;&#27979;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#21322;&#30417;&#30563;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#20266;&#26631;&#31614;&#12289;&#19968;&#33268;&#24615;&#27491;&#21017;&#21270;&#21644;&#23545;&#27604;&#23398;&#20064;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#20266;&#26631;&#31614;&#26041;&#27861;&#23481;&#26131;&#21463;&#21040;&#28151;&#28102;&#30340;&#24433;&#21709;&#65292;&#22312;&#26089;&#26399;&#36845;&#20195;&#20013;&#65292;&#38169;&#35823;&#30340;&#20266;&#26631;&#31614;&#34987;&#35748;&#20026;&#26159;&#30495;&#23454;&#26631;&#31614;&#65292;&#23548;&#33268;&#27169;&#22411;&#21152;&#24378;&#20854;&#20808;&#21069;&#30340;&#20559;&#35265;&#65292;&#36827;&#32780;&#26080;&#27861;&#24456;&#22909;&#22320;&#25512;&#24191;&#21040;&#24378;&#22823;&#30340;&#39044;&#27979;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#20010;&#25105;&#20204;&#31216;&#20043;&#20026;&#21322;&#30417;&#30563;&#23545;&#27604;&#24322;&#24120;&#20540;&#21076;&#38500;&#30340;&#20266;&#26368;&#22823;&#26399;&#26395;&#21270;&#26041;&#27861;&#65288;SCOPE&#65289;&#26469;&#25233;&#21046;&#28151;&#28102;&#38169;&#35823;&#12290;&#20687;&#22522;&#26412;&#30340;&#20266;&#26631;&#31614;&#26041;&#27861;&#19968;&#26679;&#65292;SCOPE&#19982;&#26368;&#22823;&#26399;&#26395;&#21270;&#65288;EM&#65289;&#30456;&#20851;&#65292;EM&#26159;&#19968;&#31181;&#28508;&#21464;&#37327;&#26694;&#26550;&#65292;&#21487;&#20197;&#25193;&#23637;&#21040;&#29702;&#35299;&#32858;&#31867;&#20551;&#35774;&#30340;&#28145;&#24230;&#21322;&#30417;&#30563;&#31639;&#27861;&#12290;&#20294;&#26159;&#65292;&#19982;&#22522;&#26412;&#30340;&#26041;&#27861;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning is the problem of training an accurate predictive model by combining a small labeled dataset with a presumably much larger unlabeled dataset. Many methods for semi-supervised deep learning have been developed, including pseudolabeling, consistency regularization, and contrastive learning techniques. Pseudolabeling methods however are highly susceptible to confounding, in which erroneous pseudolabels are assumed to be true labels in early iterations, thereby causing the model to reinforce its prior biases and thereby fail to generalize to strong predictive performance. We present a new approach to suppress confounding errors through a method we describe as Semi-supervised Contrastive Outlier removal for Pseudo Expectation Maximization (SCOPE). Like basic pseudolabeling, SCOPE is related to Expectation Maximization (EM), a latent variable framework which can be extended toward understanding cluster-assumption deep semi-supervised algorithms. However, unlike basic
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;IVRE&#30340;&#20132;&#20114;&#24335;&#29615;&#22659;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#22312;IVRE&#20013;&#35774;&#32622;&#19981;&#30830;&#23450;&#30340;&#21160;&#20316;-&#25928;&#26524;&#23545;&#65292;&#35201;&#27714;&#20195;&#29702;&#30830;&#23450;&#23545;&#35937;&#30340;&#35282;&#33394;&#65292;&#24182;&#40723;&#21169;&#20195;&#29702;&#22522;&#20110;&#35266;&#23519;&#25552;&#20986;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#23454;&#39564;&#26469;&#39564;&#35777;&#20551;&#35774;&#12290;</title><link>http://arxiv.org/abs/2206.09203</link><description>&lt;p&gt;
&#12298;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#20132;&#20114;&#24335;&#35270;&#35273;&#25512;&#29702;&#12299;
&lt;/p&gt;
&lt;p&gt;
Interactive Visual Reasoning under Uncertainty. (arXiv:2206.09203v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.09203
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;IVRE&#30340;&#20132;&#20114;&#24335;&#29615;&#22659;&#65292;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#22312;IVRE&#20013;&#35774;&#32622;&#19981;&#30830;&#23450;&#30340;&#21160;&#20316;-&#25928;&#26524;&#23545;&#65292;&#35201;&#27714;&#20195;&#29702;&#30830;&#23450;&#23545;&#35937;&#30340;&#35282;&#33394;&#65292;&#24182;&#40723;&#21169;&#20195;&#29702;&#22522;&#20110;&#35266;&#23519;&#25552;&#20986;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#23454;&#39564;&#26469;&#39564;&#35777;&#20551;&#35774;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#30340;&#22522;&#26412;&#35748;&#30693;&#33021;&#21147;&#20043;&#19968;&#26159;&#36890;&#36807;&#29983;&#25104;&#20551;&#35774;&#24182;&#36890;&#36807;&#31215;&#26497;&#35797;&#39564;&#26469;&#36805;&#36895;&#35299;&#20915;&#19981;&#30830;&#23450;&#24615;&#12290;&#24403;&#36935;&#21040;&#20276;&#38543;&#30528;&#27169;&#31946;&#30340;&#22240;&#26524;&#20851;&#31995;&#30340;&#26032;&#29616;&#35937;&#26102;&#65292;&#20154;&#31867;&#23545;&#25968;&#25454;&#25552;&#20986;&#20551;&#35774;&#65292;&#36890;&#36807;&#35266;&#23519;&#36827;&#34892;&#25512;&#29702;&#65292;&#36890;&#36807;&#23454;&#39564;&#26469;&#27979;&#35797;&#20182;&#20204;&#30340;&#29702;&#35770;&#65292;&#24182;&#22312;&#19981;&#19968;&#33268;&#20986;&#29616;&#26102;&#20462;&#27491;&#21629;&#39064;&#12290;&#36825;&#20123;&#36845;&#20195;&#36807;&#31243;&#25345;&#32493;&#21040;&#24213;&#23618;&#26426;&#21046;&#21464;&#24471;&#28165;&#26224;&#20026;&#27490;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#21517;&#20026;IVRE&#65288;&#35835;&#20316;"ivory"&#65289;&#30340;&#29615;&#22659;&#65292;&#29992;&#20110;&#35780;&#20272;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#22312;&#19981;&#30830;&#23450;&#24615;&#19979;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;IVRE&#26159;&#19968;&#20010;&#20132;&#20114;&#24335;&#29615;&#22659;&#65292;&#22260;&#32469;Blicket&#26816;&#27979;&#30340;&#20016;&#23500;&#22330;&#26223;&#23637;&#24320;&#12290;IVRE&#20013;&#30340;&#20195;&#29702;&#34987;&#25918;&#32622;&#22312;&#20855;&#26377;&#21508;&#31181;&#27169;&#31946;&#30340;&#21160;&#20316;-&#25928;&#26524;&#23545;&#30340;&#29615;&#22659;&#20013;&#65292;&#24182;&#34987;&#35201;&#27714;&#30830;&#23450;&#27599;&#20010;&#23545;&#35937;&#30340;&#35282;&#33394;&#12290;&#20182;&#20204;&#34987;&#40723;&#21169;&#22522;&#20110;&#35266;&#23519;&#25552;&#20986;&#26377;&#25928;&#19988;&#39640;&#25928;&#30340;&#23454;&#39564;&#26469;&#39564;&#35777;&#20182;&#20204;&#30340;&#20551;&#35774;&#65292;&#24182;&#31215;&#26497;&#25910;&#38598;&#26032;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the fundamental cognitive abilities of humans is to quickly resolve uncertainty by generating hypotheses and testing them via active trials. Encountering a novel phenomenon accompanied by ambiguous cause-effect relationships, humans make hypotheses against data, conduct inferences from observation, test their theory via experimentation, and correct the proposition if inconsistency arises. These iterative processes persist until the underlying mechanism becomes clear. In this work, we devise the IVRE (pronounced as "ivory") environment for evaluating artificial agents' reasoning ability under uncertainty. IVRE is an interactive environment featuring rich scenarios centered around Blicket detection. Agents in IVRE are placed into environments with various ambiguous action-effect pairs and asked to determine each object's role. They are encouraged to propose effective and efficient experiments to validate their hypotheses based on observations and actively gather new information. T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Machine Personality Inventory (MPI)&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20010;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLM)&#20855;&#26377;&#20010;&#24615;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Personality Prompting (P^2)&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLM&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#12290;</title><link>http://arxiv.org/abs/2206.07550</link><description>&lt;p&gt;
&#35780;&#20272;&#21644;&#35825;&#23548;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20010;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating and Inducing Personality in Pre-trained Language Models. (arXiv:2206.07550v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.07550
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Machine Personality Inventory (MPI)&#25968;&#25454;&#38598;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#20010;&#24615;&#65292;&#36890;&#36807;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(LLM)&#20855;&#26377;&#20010;&#24615;&#12290;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;Personality Prompting (P^2)&#26041;&#27861;&#65292;&#29992;&#20110;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLM&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#36215;&#28304;&#20110;&#21746;&#23398;&#25506;&#32034;&#65292;&#20851;&#27880;&#20010;&#20307;&#22312;&#24605;&#32771;&#12289;&#24773;&#24863;&#21644;&#34892;&#20026;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#20026;&#20102;&#26500;&#24314;&#33021;&#22815;&#19982;&#20154;&#31867;&#26085;&#24120;&#21512;&#20316;&#30340;&#31038;&#20132;&#26426;&#22120;&#65292;&#25105;&#20204;&#24819;&#30693;&#36947;&#65306;&#29616;&#26377;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#21542;&#25317;&#26377;&#19982;&#20154;&#31867;&#31867;&#20284;&#30340;&#20010;&#24615;&#65311;&#22914;&#26524;&#26159;&#65292;&#25105;&#20204;&#22914;&#20309;&#35780;&#20272;&#23427;&#20204;&#65311;&#36827;&#19968;&#27493;&#22320;&#65292;&#22312;&#27492;&#35780;&#20272;&#26694;&#26550;&#30340;&#22522;&#30784;&#19978;&#65292;&#22914;&#20309;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;&#20855;&#26377;&#29305;&#23450;&#20010;&#24615;&#30340;&#35821;&#35328;&#27169;&#22411;&#65311;&#20026;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26426;&#22120;&#20010;&#24615;&#24211;(Machine Personality Inventory, MPI)&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#35780;&#20272;&#26426;&#22120;&#30340;&#20010;&#24615;&#12290;MPI&#36981;&#24490;&#26631;&#20934;&#21270;&#30340;&#20010;&#24615;&#27979;&#35797;&#65292;&#22522;&#20110;&#20116;&#22240;&#32032;&#20154;&#26684;&#29702;&#35770;&#21644;&#20154;&#26684;&#35780;&#20272;&#24211;&#24314;&#31435;&#12290;&#36890;&#36807;&#29992;MPI&#31995;&#32479;&#22320;&#35780;&#20272;LLM&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#31532;&#19968;&#20010;&#35777;&#25454;&#65292;&#35777;&#26126;&#20102;LLM&#30340;&#20010;&#24615;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#31181;&#20010;&#24615;&#25552;&#31034;(Personality Prompting, P^2)&#26041;&#27861;&#65292;&#20197;&#21487;&#25511;&#30340;&#26041;&#24335;&#35825;&#23548;LLMs&#20855;&#26377;&#29305;&#23450;&#30340;&#20010;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Originating as a philosophical quest, the study of personality concerns how individuals differ in thinking, feeling, and behaving. Towards building social machines that work with humans on a daily basis, we are motivated to ask: Do existing Large Language Models (LLMs) possess personalities akin to their human counterparts? If so, how can we evaluate them? Further, given this evaluation framework, how can we induce a particular personality in a controllable fashion? To answer these three questions, we propose the Machine Personality Inventory (MPI) dataset for evaluating the machine personality; MPI follows standardized personality tests, built upon the Big Five Personality Factors (Big Five) theory and personality assessment inventories. By systematically evaluating LLMs with MPI, we provide the first piece of evidence showing the existence of personality in LLMs. We further devise a Personality Prompting (P^2) method to induce LLMs with a specific personality in a controllable manner
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#65292;&#20351;&#20854;&#20855;&#22791;&#26356;&#39640;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#30340;&#23398;&#20064;&#25928;&#26524;&#26356;&#22909;&#19988;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2206.02341</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#36827;&#34892;&#22797;&#26434;&#36816;&#21160;&#25216;&#33021;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Complex Locomotion Skill Learning via Differentiable Physics. (arXiv:2206.02341v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.02341
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#21487;&#24494;&#20998;&#29289;&#29702;&#23454;&#29616;&#20102;&#32479;&#19968;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#65292;&#20351;&#20854;&#20855;&#22791;&#26356;&#39640;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;&#19982;&#24378;&#21270;&#23398;&#20064;&#30456;&#27604;&#65292;&#35813;&#26694;&#26550;&#30340;&#23398;&#20064;&#25928;&#26524;&#26356;&#22909;&#19988;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#24494;&#20998;&#29289;&#29702;&#33021;&#22815;&#36890;&#36807;&#26377;&#25928;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#20248;&#21270;&#26469;&#33719;&#24471;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#21482;&#33021;&#25552;&#20379;&#20855;&#26377;&#26377;&#38480;&#33021;&#21147;&#21644;&#27867;&#21270;&#33021;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23454;&#29992;&#30340;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#36755;&#20986;&#32479;&#19968;&#30340;&#20855;&#26377;&#26356;&#39640;&#22797;&#26434;&#24615;&#21644;&#22810;&#26679;&#24615;&#30340;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#22120;&#12290;&#20026;&#20102;&#31995;&#32479;&#22320;&#25552;&#39640;&#35757;&#32451;&#30340;&#40065;&#26834;&#24615;&#21644;&#25928;&#29575;&#65292;&#25105;&#20204;&#23545;&#22522;&#20934;&#26041;&#27861;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#25913;&#36827;&#65292;&#21253;&#25324;&#21608;&#26399;&#28608;&#27963;&#20989;&#25968;&#21644;&#23450;&#21046;&#25439;&#22833;&#20989;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#25209;&#22788;&#29702;&#21644;Adam&#20248;&#21270;&#22120;&#22312;&#35757;&#32451;&#22797;&#26434;&#36816;&#21160;&#20219;&#21153;&#26102;&#38750;&#24120;&#26377;&#25928;&#12290;&#25105;&#20204;&#22312;&#21487;&#24494;&#20998;&#30340;&#36136;&#28857;&#24377;&#31783;&#21644;&#26448;&#26009;&#28857;&#27861;&#65288;MPM&#65289;&#27169;&#25311;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26694;&#26550;&#65292;&#36827;&#34892;&#20102;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36816;&#21160;&#20219;&#21153;&#21644;&#22810;&#20010;&#26426;&#22120;&#20154;&#35774;&#35745;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22522;&#20110;&#21487;&#24494;&#20998;&#29289;&#29702;&#30340;&#23398;&#20064;&#26694;&#26550;&#27604;&#24378;&#21270;&#23398;&#20064;&#20855;&#26377;&#26356;&#22909;&#30340;&#32467;&#26524;&#65292;&#24182;&#19988;&#25910;&#25947;&#36895;&#24230;&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;
Differentiable physics enables efficient gradient-based optimizations of neural network (NN) controllers. However, existing work typically only delivers NN controllers with limited capability and generalizability. We present a practical learning framework that outputs unified NN controllers capable of tasks with significantly improved complexity and diversity. To systematically improve training robustness and efficiency, we investigated a suite of improvements over the baseline approach, including periodic activation functions, and tailored loss functions. In addition, we find our adoption of batching and an Adam optimizer effective in training complex locomotion tasks. We evaluate our framework on differentiable mass-spring and material point method (MPM) simulations, with challenging locomotion tasks and multiple robot designs. Experiments show that our learning framework, based on differentiable physics, delivers better results than reinforcement learning and converges much faster. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2205.15677</link><description>&lt;p&gt;
&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#30340;&#33258;&#25105;&#30417;&#30563;&#22686;&#24378;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.15677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#29992;&#20110;&#23545;&#29983;&#25104;&#25968;&#25454;&#21450;&#20854;&#22686;&#24378;&#21442;&#25968;&#30340;&#39044;&#27979;&#65292;&#20174;&#32780;&#25552;&#39640;&#21028;&#21035;&#22120;&#30340;&#34920;&#29616;&#21644;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#23454;&#29616;&#20102;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#25968;&#25454;&#24773;&#20917;&#19979;&#35757;&#32451;&#29983;&#25104;&#24335;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#21028;&#21035;&#22120;&#23481;&#26131;&#36807;&#25311;&#21512;&#12290;&#20808;&#21069;&#25552;&#20986;&#30340;&#21487;&#24494;&#22686;&#24378;&#25216;&#26415;&#25913;&#21892;&#20102;GAN&#35757;&#32451;&#30340;&#25968;&#25454;&#25928;&#29575;&#12290;&#20294;&#26159;&#65292;&#22686;&#24378;&#25216;&#26415;&#38544;&#24335;&#22320;&#24341;&#20837;&#20102;&#19981;&#33391;&#19981;&#21464;&#24615;&#22240;&#32032;&#65292;&#22240;&#20026;&#23427;&#24573;&#30053;&#20102;&#30001;&#25968;&#25454;&#36716;&#25442;&#24341;&#36215;&#30340;&#26631;&#31614;&#31354;&#38388;&#35821;&#20041;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#38480;&#21046;&#20102;&#21028;&#21035;&#22120;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#65292;&#24182;&#26368;&#32456;&#24433;&#21709;&#29983;&#25104;&#27169;&#22411;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#20943;&#36731;&#19981;&#21464;&#24615;&#30340;&#36127;&#38754;&#24433;&#21709;&#65292;&#21516;&#26102;&#32487;&#25215;&#25968;&#25454;&#22686;&#24378;&#30340;&#22909;&#22788;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22686;&#24378;&#24863;&#30693;&#30340;&#33258;&#30417;&#30563;&#21028;&#21035;&#22120;&#65292;&#35813;&#21028;&#21035;&#22120;&#21487;&#20197;&#39044;&#27979;&#22686;&#24378;&#25968;&#25454;&#30340;&#21442;&#25968;&#12290;&#29305;&#21035;&#22320;&#65292;&#30495;&#23454;&#25968;&#25454;&#21644;&#29983;&#25104;&#25968;&#25454;&#30340;&#39044;&#27979;&#30446;&#26631;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38656;&#35201;&#21306;&#21035;&#24320;&#26469;&#12290;&#25105;&#20204;&#36824;&#40723;&#21169;&#29983;&#25104;&#22120;&#23545;&#25239;&#22320;&#29983;&#25104;&#20854;&#22686;&#24378;&#21442;&#25968;&#21487;&#20197;&#34987;&#21028;&#21035;&#22120;&#20934;&#30830;&#39044;&#27979;&#30340;&#25968;&#25454;&#65292;&#20174;&#32780;&#33719;&#24471;&#26356;&#22810;&#20449;&#24687;&#37327;&#21644;&#26356;&#39640;&#25928;&#30340;&#21028;&#21035;&#22120;&#65292;&#25552;&#39640;&#29983;&#25104;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#25968;&#25454;&#26377;&#25928;&#30340; GAN &#35757;&#32451;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training generative adversarial networks (GANs) with limited data is challenging because discriminator is prone to overfitting. Previously proposed differentiable augmentation demonstrates improved data efficiency of training GANs. However, the augmentation implicitly introduces undesired invariance to augmentation for the discriminator since it ignores the change of semantics in the label space caused by data transformation, which may limit the representation learning ability of the discriminator and ultimately affect the generative modeling performance of the generator. To mitigate the negative impact of invariance while inheriting the benefits of data augmentation, we propose a novel augmentation-aware self-supervised discriminator that predicts the augmentation parameter of the augmented data. Particularly, the prediction targets of real data and generated data are required to be distinguished since they are different during training. We further encourage the generator to adversari
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#23398;&#20064;&#22240;&#26524;Transformer&#65288;TLT&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#22122;&#22768;&#22270;&#20687;&#20998;&#31867;&#12290;&#36890;&#36807;&#23558;"&#23384;&#22312;&#22122;&#22768;"&#30340;&#20449;&#24687;&#20316;&#20026;&#22788;&#29702;&#36755;&#20837;&#65292;&#24182;&#32852;&#21512;&#20272;&#35745;&#22788;&#29702;&#25928;&#26524;&#65292;TLT&#33021;&#22815;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#29992;&#28508;&#22312;&#29983;&#25104;&#27169;&#22411;&#20272;&#35745;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2203.15529</link><description>&lt;p&gt;
&#22788;&#29702;&#23398;&#20064;&#22240;&#26524;Transformer&#29992;&#20110;&#22122;&#22768;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Treatment Learning Causal Transformer for Noisy Image Classification. (arXiv:2203.15529v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.15529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#23398;&#20064;&#22240;&#26524;Transformer&#65288;TLT&#65289;&#26550;&#26500;&#65292;&#29992;&#20110;&#22122;&#22768;&#22270;&#20687;&#20998;&#31867;&#12290;&#36890;&#36807;&#23558;"&#23384;&#22312;&#22122;&#22768;"&#30340;&#20449;&#24687;&#20316;&#20026;&#22788;&#29702;&#36755;&#20837;&#65292;&#24182;&#32852;&#21512;&#20272;&#35745;&#22788;&#29702;&#25928;&#26524;&#65292;TLT&#33021;&#22815;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#65292;&#24182;&#20351;&#29992;&#28508;&#22312;&#29983;&#25104;&#27169;&#22411;&#20272;&#35745;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#39030;&#32423;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#35270;&#35273;&#27169;&#22411;&#20027;&#35201;&#22522;&#20110;&#25506;&#32034;&#21644;&#21033;&#29992;&#35757;&#32451;&#25968;&#25454;&#26679;&#26412;&#21644;&#20854;&#20851;&#32852;&#26631;&#31614;&#20043;&#38388;&#30340;&#20869;&#22312;&#30456;&#20851;&#24615;&#12290;&#28982;&#32780;&#65292;&#24050;&#30693;&#30340;&#19968;&#20010;&#23454;&#38469;&#25361;&#25112;&#26159;&#23427;&#20204;&#22312;"&#22122;&#22768;"&#25968;&#25454;&#23545;&#25239;&#19979;&#24615;&#33021;&#19979;&#38477;&#65292;&#22122;&#22768;&#25968;&#25454;&#30001;&#20110;&#19981;&#21516;&#24773;&#20917;&#24341;&#36215;&#65292;&#27604;&#22914;&#34394;&#20551;&#30456;&#20851;&#24615;&#12289;&#26080;&#20851;&#32972;&#26223;&#12289;&#39046;&#22495;&#36716;&#31227;&#21644;&#23545;&#25239;&#25915;&#20987;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558;"&#23384;&#22312;&#22122;&#22768;"&#30340;&#20108;&#36827;&#21046;&#20449;&#24687;&#20316;&#20026;&#22788;&#29702;&#36755;&#20837;&#21040;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#65292;&#36890;&#36807;&#32852;&#21512;&#20272;&#35745;&#22788;&#29702;&#25928;&#26524;&#26469;&#25552;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#21463;&#22240;&#26524;&#21464;&#20998;&#25512;&#26029;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#22788;&#29702;&#23398;&#20064;&#22240;&#26524;Transformer&#65288;TLT&#65289;&#65292;&#23427;&#20351;&#29992;&#28508;&#22312;&#30340;&#29983;&#25104;&#27169;&#22411;&#20174;&#24403;&#21069;&#35266;&#27979;&#36755;&#20837;&#20013;&#20272;&#35745;&#40065;&#26834;&#30340;&#29305;&#24449;&#34920;&#31034;&#20197;&#36827;&#34892;&#22122;&#22768;&#22270;&#20687;&#20998;&#31867;&#12290;&#26681;&#25454;&#20272;&#35745;&#30340;&#22122;&#22768;&#27700;&#24179;&#65288;&#24314;&#27169;&#20026;&#20108;&#36827;&#21046;&#22788;&#29702;&#22240;&#23376;&#65289;&#65292;TLT&#20998;&#37197;&#30456;&#24212;&#30340;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Current top-notch deep learning (DL) based vision models are primarily based on exploring and exploiting the inherent correlations between training data samples and their associated labels. However, a known practical challenge is their degraded performance against "noisy" data, induced by different circumstances such as spurious correlations, irrelevant contexts, domain shift, and adversarial attacks. In this work, we incorporate this binary information of "existence of noise" as treatment into image classification tasks to improve prediction accuracy by jointly estimating their treatment effects. Motivated from causal variational inference, we propose a transformer-based architecture, Treatment Learning Causal Transformer (TLT), that uses a latent generative model to estimate robust feature representations from current observational input for noise image classification. Depending on the estimated noise level (modeled as a binary treatment factor), TLT assigns the corresponding inferen
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#31639;&#27861;MROT&#65292;&#29992;&#20110;&#25913;&#36827;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#21644;&#22686;&#24378;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MROT&#22312;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#21644;&#26448;&#26009;&#21560;&#38468;&#36873;&#25321;&#31561;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#20855;&#26377;&#21152;&#36895;&#21457;&#29616;&#20855;&#26377;&#26399;&#26395;&#24615;&#36136;&#30340;&#26032;&#29289;&#36136;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2202.06208</link><description>&lt;p&gt;
&#29992;&#24230;&#37327;&#23398;&#20064;&#22686;&#24378;&#30340;&#26368;&#20248;&#20256;&#36755;&#25913;&#36827;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Improving Molecular Representation Learning with Metric Learning-enhanced Optimal Transport. (arXiv:2202.06208v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.06208
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#31639;&#27861;MROT&#65292;&#29992;&#20110;&#25913;&#36827;&#20998;&#23376;&#34920;&#31034;&#23398;&#20064;&#21644;&#22686;&#24378;&#20854;&#27867;&#21270;&#33021;&#21147;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;MROT&#22312;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#21644;&#26448;&#26009;&#21560;&#38468;&#36873;&#25321;&#31561;&#20219;&#21153;&#20013;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#20855;&#26377;&#21152;&#36895;&#21457;&#29616;&#20855;&#26377;&#26399;&#26395;&#24615;&#36136;&#30340;&#26032;&#29289;&#36136;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#21270;&#23398;&#21644;&#29983;&#29289;&#24212;&#29992;&#20013;&#65292;&#35757;&#32451;&#25968;&#25454;&#36890;&#24120;&#26377;&#38480;&#25110;&#24322;&#26500;&#12290;&#29616;&#26377;&#30340;&#21270;&#23398;&#21644;&#26448;&#26009;&#31185;&#23398;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26410;&#33021;&#32771;&#34385;&#35757;&#32451;&#39046;&#22495;&#20043;&#22806;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#26368;&#20248;&#20256;&#36755;&#30340;&#26032;&#31639;&#27861;MROT&#65292;&#29992;&#20110;&#22686;&#24378;&#20998;&#23376;&#22238;&#24402;&#38382;&#39064;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;MROT&#36890;&#36807;&#27979;&#37327;&#19968;&#31181;&#26032;&#30340;&#39046;&#22495;&#36317;&#31163;&#24230;&#37327;&#21644;&#20256;&#36755;&#35745;&#21010;&#19978;&#30340;&#21518;&#39564;&#26041;&#24046;&#27491;&#21017;&#21270;&#65292;&#26469;&#23398;&#20064;&#25968;&#25454;&#30340;&#36830;&#32493;&#26631;&#31614;&#65292;&#20197;&#24357;&#21512;&#21270;&#23398;&#39046;&#22495;&#30340;&#24046;&#36317;&#12290;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#26080;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#30340;&#22522;&#26412;&#21270;&#23398;&#22238;&#24402;&#20219;&#21153;&#65292;&#21253;&#25324;&#21270;&#23398;&#24615;&#36136;&#39044;&#27979;&#21644;&#26448;&#26009;&#21560;&#38468;&#36873;&#25321;&#12290;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;MROT&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#65292;&#22312;&#21152;&#36895;&#21457;&#29616;&#20855;&#26377;&#26399;&#26395;&#24615;&#36136;&#30340;&#26032;&#29289;&#36136;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Training data are usually limited or heterogeneous in many chemical and biological applications. Existing machine learning models for chemistry and materials science fail to consider generalizing beyond training domains. In this article, we develop a novel optimal transport-based algorithm termed MROT to enhance their generalization capability for molecular regression problems. MROT learns a continuous label of the data by measuring a new metric of domain distances and a posterior variance regularization over the transport plan to bridge the chemical domain gap. Among downstream tasks, we consider basic chemical regression tasks in unsupervised and semi-supervised settings, including chemical property prediction and materials adsorption selection. Extensive experiments show that MROT significantly outperforms state-of-the-art models, showing promising potential in accelerating the discovery of new substances with desired properties.
&lt;/p&gt;</description></item><item><title>CubeTR&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;&#38271;&#24207;&#21015;&#21160;&#20316;&#21644;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20174;&#20219;&#24847;&#36215;&#22987;&#29366;&#24577;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#25509;&#36817;&#19987;&#19994;&#20154;&#21592;&#35299;&#20915;&#26041;&#26696;&#38271;&#24230;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2111.06036</link><description>&lt;p&gt;
CubeTR: &#20351;&#29992;Transformer&#23398;&#20064;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
CubeTR: Learning to Solve The Rubiks Cube Using Transformers. (arXiv:2111.06036v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2111.06036
&lt;/p&gt;
&lt;p&gt;
CubeTR&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#20851;&#27880;&#38271;&#24207;&#21015;&#21160;&#20316;&#21644;&#35299;&#20915;&#31232;&#30095;&#22870;&#21169;&#30340;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#20174;&#20219;&#24847;&#36215;&#22987;&#29366;&#24577;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#65292;&#24182;&#33021;&#22815;&#29983;&#25104;&#25509;&#36817;&#19987;&#19994;&#20154;&#21592;&#35299;&#20915;&#26041;&#26696;&#38271;&#24230;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;Transformer&#39318;&#27425;&#20986;&#29616;&#20197;&#26469;&#65292;&#24050;&#32463;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#21040;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#21508;&#31181;&#39046;&#22495;&#12290;&#26368;&#36817;&#25552;&#20986;&#23558;Transformer&#24212;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#65292;&#23558;&#20854;&#37325;&#26032;&#23450;&#20041;&#20026;&#24207;&#21015;&#24314;&#27169;&#38382;&#39064;&#12290;&#19982;&#20854;&#20182;&#24120;&#35265;&#30340;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#30456;&#27604;&#65292;&#39764;&#26041;&#38382;&#39064;&#20855;&#26377;&#29420;&#29305;&#30340;&#25361;&#25112;&#12290;&#39764;&#26041;&#26377;&#30528;&#25968;&#20197;&#21315;&#19975;&#35745;&#30340;&#21487;&#33021;&#32452;&#21512;&#65292;&#20294;&#21482;&#26377;&#19968;&#20010;&#35299;&#20915;&#30340;&#29366;&#24577;&#65292;&#36825;&#23548;&#33268;&#20102;&#26497;&#24230;&#31232;&#30095;&#30340;&#22870;&#21169;&#20449;&#21495;&#12290;&#25552;&#20986;&#30340;&#27169;&#22411;CubeTR&#20851;&#27880;&#20110;&#38271;&#24207;&#21015;&#30340;&#21160;&#20316;&#65292;&#24182;&#35299;&#20915;&#20102;&#31232;&#30095;&#22870;&#21169;&#30340;&#38382;&#39064;&#12290;CubeTR&#33021;&#22815;&#20174;&#20219;&#24847;&#36215;&#22987;&#29366;&#24577;&#23398;&#20064;&#22914;&#20309;&#35299;&#20915;&#39764;&#26041;&#38382;&#39064;&#65292;&#32463;&#36807;&#31227;&#21160;&#35268;&#33539;&#21270;&#21518;&#65292;&#29983;&#25104;&#30340;&#35299;&#20915;&#26041;&#26696;&#38271;&#24230;&#39044;&#26399;&#23558;&#38750;&#24120;&#25509;&#36817;&#19987;&#19994;&#20154;&#21592;&#20351;&#29992;&#30340;&#31639;&#27861;&#32473;&#20986;&#30340;&#35299;&#20915;&#26041;&#26696;&#38271;&#24230;&#12290;CubeTR&#20026;&#23398;&#20064;&#31639;&#27861;&#22312;&#26356;&#39640;&#32500;&#24230;&#39764;&#26041;&#19978;&#30340;&#27867;&#21270;&#33021;&#21147;&#25552;&#20379;&#20102;&#28145;&#20837;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since its first appearance, transformers have been successfully used in wide ranging domains from computer vision to natural language processing. Application of transformers in Reinforcement Learning by reformulating it as a sequence modelling problem was proposed only recently. Compared to other commonly explored reinforcement learning problems, the Rubiks cube poses a unique set of challenges. The Rubiks cube has a single solved state for quintillions of possible configurations which leads to extremely sparse rewards. The proposed model CubeTR attends to longer sequences of actions and addresses the problem of sparse rewards. CubeTR learns how to solve the Rubiks cube from arbitrary starting states without any human prior, and after move regularisation, the lengths of solutions generated by it are expected to be very close to those given by algorithms used by expert human solvers. CubeTR provides insights to the generalisability of learning algorithms to higher dimensional cubes and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#26144;&#23556;&#30340;&#31070;&#32463;&#27169;&#22411;&#37325;&#32534;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#35813;&#26041;&#27861;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#31435;&#38518;&#23451;&#35821;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2110.03894</link><description>&lt;p&gt;
&#22522;&#20110;&#30456;&#20284;&#24615;&#26144;&#23556;&#30340;&#31070;&#32463;&#27169;&#22411;&#37325;&#32534;&#31243;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Classification. (arXiv:2110.03894v4 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.03894
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30456;&#20284;&#24615;&#26144;&#23556;&#30340;&#31070;&#32463;&#27169;&#22411;&#37325;&#32534;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#26377;&#38480;&#30340;&#25968;&#25454;&#26465;&#20214;&#19979;&#65292;&#35813;&#26041;&#27861;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#31435;&#38518;&#23451;&#35821;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#23545;&#25239;&#37325;&#32534;&#31243;&#65288;AR&#65289;&#26041;&#27861;&#65292;&#29992;&#20110;&#20302;&#36164;&#28304;&#21475;&#35821;&#21629;&#20196;&#35782;&#21035;&#65288;SCR&#65289;&#65292;&#24182;&#26500;&#24314;&#20102;&#19968;&#20010;AR-SCR&#31995;&#32479;&#12290;AR&#36807;&#31243;&#26088;&#22312;&#20462;&#25913;&#26469;&#33258;&#30446;&#26631;&#39046;&#22495;&#30340;&#22768;&#23398;&#20449;&#21495;&#65292;&#20197;&#37325;&#26032;&#35843;&#25972;&#39044;&#35757;&#32451;&#30340;SCR&#27169;&#22411;&#65292;&#20174;&#32780;&#23454;&#29616;&#37325;&#32534;&#31243;&#12290;&#20026;&#20102;&#35299;&#20915;&#28304;&#22495;&#21644;&#30446;&#26631;&#22495;&#20043;&#38388;&#30340;&#26631;&#31614;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#36827;&#19968;&#27493;&#25552;&#39640;AR&#30340;&#31283;&#23450;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#30456;&#20284;&#24615;&#30340;&#26631;&#31614;&#26144;&#23556;&#25216;&#26415;&#26469;&#23545;&#40784;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#23558;&#36801;&#31227;&#23398;&#20064;&#65288;TL&#65289;&#25216;&#26415;&#19982;&#21407;&#22987;AR&#36807;&#31243;&#30456;&#32467;&#21512;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#30340;&#36866;&#24212;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#19977;&#20010;&#20302;&#36164;&#28304;SCR&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#25552;&#20986;&#30340;AR-SCR&#31995;&#32479;&#65292;&#21253;&#25324;&#38463;&#25289;&#20271;&#35821;&#12289;&#31435;&#38518;&#23451;&#35821;&#21644;&#35328;&#35821;&#38556;&#30861;&#24615;&#26222;&#36890;&#35805;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#22823;&#35268;&#27169;&#33521;&#35821;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#30340;&#39044;&#35757;&#32451;AM&#30340;&#22522;&#30784;&#19978;&#65292;&#25552;&#20986;&#30340;AR-SCR&#31995;&#32479;&#22312;&#38463;&#25289;&#20271;&#35821;&#21644;&#31435;&#38518;&#23451;&#35821;&#21629;&#20196;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#19988;&#20165;&#20351;&#29992;&#26377;&#38480;&#25968;&#37327;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we propose a novel adversarial reprogramming (AR) approach for low-resource spoken command recognition (SCR), and build an AR-SCR system. The AR procedure aims to modify the acoustic signals (from the target domain) to repurpose a pretrained SCR model (from the source domain). To solve the label mismatches between source and target domains, and further improve the stability of AR, we propose a novel similarity-based label mapping technique to align classes. In addition, the transfer learning (TL) technique is combined with the original AR process to improve the model adaptation capability. We evaluate the proposed AR-SCR system on three low-resource SCR datasets, including Arabic, Lithuanian, and dysarthric Mandarin speech. Experimental results show that with a pretrained AM trained on a large-scale English dataset, the proposed AR-SCR system outperforms the current state-of-the-art results on Arabic and Lithuanian speech commands datasets, with only a limited amount of 
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;NLP&#20013;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#24182;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#23545;&#29616;&#26377;&#30340;KEPLMs&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#26368;&#21518;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#20013;KEPLMs&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2110.00269</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#27169;&#22411;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Survey of Knowledge Enhanced Pre-trained Models. (arXiv:2110.00269v4 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.00269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#25552;&#20379;&#20102;&#20851;&#20110;NLP&#20013;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#32508;&#21512;&#27010;&#36848;&#65292;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#65292;&#24182;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#23545;&#29616;&#26377;&#30340;KEPLMs&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#26368;&#21518;&#27010;&#36848;&#20102;&#26410;&#26469;&#30740;&#31350;&#20013;KEPLMs&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36890;&#36807;&#33258;&#30417;&#30563;&#23398;&#20064;&#22312;&#22823;&#35268;&#27169;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#23398;&#20064;&#20102;&#20449;&#24687;&#20016;&#23500;&#30340;&#35789;&#34920;&#31034;&#65292;&#22312;&#32454;&#35843;&#20043;&#21518;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#27169;&#22411;&#23384;&#22312;&#40065;&#26834;&#24615;&#24046;&#21644;&#21487;&#35299;&#37322;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23558;&#27880;&#20837;&#30693;&#35782;&#30340;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#31216;&#20026;&#30693;&#35782;&#22686;&#24378;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(KEPLMs)&#12290;&#36825;&#20123;&#27169;&#22411;&#34920;&#29616;&#20986;&#28145;&#20837;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#65292;&#24182;&#24341;&#20837;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;NLP&#20013;KEPLMs&#30340;&#32508;&#21512;&#27010;&#36848;&#12290;&#25105;&#20204;&#39318;&#20808;&#35752;&#35770;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#21644;&#30693;&#35782;&#34920;&#31034;&#23398;&#20064;&#30340;&#36827;&#23637;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20174;&#19977;&#20010;&#19981;&#21516;&#30340;&#35282;&#24230;&#31995;&#32479;&#22320;&#20998;&#31867;&#20102;&#29616;&#26377;&#30340;KEPLMs&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#19968;&#20123;&#26410;&#26469;&#30740;&#31350;&#20013;KEPLMs&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models learn informative word representations on a large-scale text corpus through self-supervised learning, which has achieved promising performance in fields of natural language processing (NLP) after fine-tuning. These models, however, suffer from poor robustness and lack of interpretability. We refer to pre-trained language models with knowledge injection as knowledge-enhanced pre-trained language models (KEPLMs). These models demonstrate deep understanding and logical reasoning and introduce interpretability. In this survey, we provide a comprehensive overview of KEPLMs in NLP. We first discuss the advancements in pre-trained language models and knowledge representation learning. Then we systematically categorize existing KEPLMs from three different perspectives. Finally, we outline some potential directions of KEPLMs for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#20174;&#32463;&#39564;&#12289;&#20849;&#35782;&#21644;&#26368;&#20339;&#23454;&#36341;&#20013;&#25552;&#28860;&#20986;&#30340;&#25351;&#23548;&#21407;&#21017;&#65292;&#26088;&#22312;&#24341;&#39046;&#21307;&#23398;&#24433;&#20687;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;&#20449;&#20219;&#12289;&#23433;&#20840;&#24615;&#21644;&#24212;&#29992;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2109.09658</link><description>&lt;p&gt;
FUTURE-AI:&#21307;&#23398;&#24433;&#20687;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#20849;&#35782;&#24314;&#35758;
&lt;/p&gt;
&lt;p&gt;
FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging. (arXiv:2109.09658v4 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2109.09658
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31995;&#21015;&#20174;&#32463;&#39564;&#12289;&#20849;&#35782;&#21644;&#26368;&#20339;&#23454;&#36341;&#20013;&#25552;&#28860;&#20986;&#30340;&#25351;&#23548;&#21407;&#21017;&#65292;&#26088;&#22312;&#24341;&#39046;&#21307;&#23398;&#24433;&#20687;&#20013;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#65292;&#25552;&#39640;&#20449;&#20219;&#12289;&#23433;&#20840;&#24615;&#21644;&#24212;&#29992;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#21644;&#20020;&#24202;&#31995;&#32479;&#29983;&#25104;&#30340;&#22823;&#37327;&#25968;&#25454;&#30340;&#32467;&#21512;&#65292;&#25512;&#21160;&#20102;&#21307;&#23398;&#24433;&#20687;&#39046;&#22495;&#25972;&#20010;&#20215;&#20540;&#38142;&#19978;&#30340;&#25104;&#20687;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#21457;&#23637;&#65292;&#21253;&#25324;&#22270;&#20687;&#37325;&#24314;&#12289;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#12289;&#22522;&#20110;&#22270;&#20687;&#30340;&#35786;&#26029;&#21644;&#27835;&#30103;&#35268;&#21010;&#12290;&#23613;&#31649;&#21307;&#23398;&#24433;&#20687;&#20013;&#30340;&#20154;&#24037;&#26234;&#33021;&#21462;&#24471;&#20102;&#25104;&#21151;&#24182;&#26377;&#30528;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#35768;&#22810;&#21033;&#30410;&#30456;&#20851;&#32773;&#25285;&#24515;&#25104;&#20687;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#28508;&#22312;&#39118;&#38505;&#21644;&#20262;&#29702;&#38382;&#39064;&#65292;&#35748;&#20026;&#20854;&#22797;&#26434;&#12289;&#19981;&#36879;&#26126;&#12289;&#38590;&#20197;&#29702;&#35299;&#12289;&#38590;&#20197;&#24212;&#29992;&#21644;&#38590;&#20197;&#22312;&#20851;&#38190;&#20020;&#24202;&#24212;&#29992;&#20013;&#24314;&#31435;&#20449;&#20219;&#12290;&#23613;&#31649;&#23384;&#22312;&#36825;&#20123;&#25285;&#24551;&#21644;&#39118;&#38505;&#65292;&#20294;&#30446;&#21069;&#23578;&#27809;&#26377;&#20855;&#20307;&#30340;&#25351;&#23548;&#21407;&#21017;&#21644;&#26368;&#20339;&#23454;&#36341;&#26469;&#24341;&#23548;&#26410;&#26469;&#21307;&#23398;&#24433;&#20687;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#20197;&#22686;&#21152;&#20449;&#20219;&#12289;&#23433;&#20840;&#24615;&#21644;&#37319;&#29992;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#20174;&#31215;&#32047;&#30340;&#32463;&#39564;&#12289;&#20849;&#35782;&#21644;&#26368;&#20339;&#23454;&#36341;&#20013;&#31934;&#36873;&#20986;&#30340;&#25351;&#23548;&#21407;&#21017;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advancements in artificial intelligence (AI) combined with the extensive amount of data generated by today's clinical systems, has led to the development of imaging AI solutions across the whole value chain of medical imaging, including image reconstruction, medical image segmentation, image-based diagnosis and treatment planning. Notwithstanding the successes and future potential of AI in medical imaging, many stakeholders are concerned of the potential risks and ethical implications of imaging AI solutions, which are perceived as complex, opaque, and difficult to comprehend, utilise, and trust in critical clinical applications. Despite these concerns and risks, there are currently no concrete guidelines and best practices for guiding future AI developments in medical imaging towards increased trust, safety and adoption. To bridge this gap, this paper introduces a careful selection of guiding principles drawn from the accumulated experiences, consensus, and best practices f
&lt;/p&gt;</description></item><item><title>&#21457;&#29616;&#22240;&#26524;&#32467;&#26500;&#30340;&#20219;&#21153;&#26159;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22312;&#36830;&#32493;&#20248;&#21270;&#20013;&#20351;&#29992;&#30340;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#20989;&#25968;&#21463;&#38480;&#20110;&#39640;&#26031;&#22122;&#22768;&#20551;&#35774;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#19982;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#19968;&#33268;&#30340;&#22522;&#20110;&#29109;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2106.02835</link><description>&lt;p&gt;
&#35770;&#36830;&#32493;&#20248;&#21270;&#19979;&#22522;&#20110;&#29109;&#25439;&#22833;&#20989;&#25968;&#22312;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
On the Role of Entropy-based Loss for Learning Causal Structures with Continuous Optimization. (arXiv:2106.02835v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.02835
&lt;/p&gt;
&lt;p&gt;
&#21457;&#29616;&#22240;&#26524;&#32467;&#26500;&#30340;&#20219;&#21153;&#26159;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22312;&#36830;&#32493;&#20248;&#21270;&#20013;&#20351;&#29992;&#30340;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#20989;&#25968;&#21463;&#38480;&#20110;&#39640;&#26031;&#22122;&#22768;&#20551;&#35774;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29702;&#35770;&#19978;&#19982;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#19968;&#33268;&#30340;&#22522;&#20110;&#29109;&#30340;&#25439;&#22833;&#20989;&#25968;&#26469;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#21457;&#29616;&#26159;&#35768;&#22810;&#31185;&#23398;&#39046;&#22495;&#20013;&#37325;&#35201;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#26368;&#36817;&#65292;&#19968;&#31181;&#21517;&#20026;NOTEARS&#30340;&#38750;&#32452;&#21512;&#26377;&#21521;&#26080;&#29615;&#32422;&#26463;&#26041;&#27861;&#23558;&#22240;&#26524;&#32467;&#26500;&#23398;&#20064;&#38382;&#39064;&#36716;&#21270;&#20026;&#20351;&#29992;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#30340;&#36830;&#32493;&#20248;&#21270;&#38382;&#39064;&#12290;&#34429;&#28982;&#26368;&#23567;&#20108;&#20056;&#25439;&#22833;&#20989;&#25968;&#22312;&#26631;&#20934;&#39640;&#26031;&#22122;&#22768;&#20551;&#35774;&#19979;&#26159;&#21512;&#29702;&#30340;&#65292;&#20294;&#22914;&#26524;&#35813;&#20551;&#35774;&#19981;&#25104;&#31435;&#65292;&#23427;&#20250;&#21463;&#21040;&#38480;&#21046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#39640;&#26031;&#22122;&#22768;&#20551;&#35774;&#30340;&#36829;&#21453;&#23558;&#38459;&#30861;&#22240;&#26524;&#26041;&#21521;&#30340;&#35782;&#21035;&#65292;&#20351;&#22240;&#26524;&#26041;&#21521;&#23436;&#20840;&#30001;&#22240;&#26524;&#24378;&#24230;&#20197;&#21450;&#32447;&#24615;&#24773;&#20917;&#19979;&#22122;&#22768;&#26041;&#24046;&#21644;&#38750;&#32447;&#24615;&#24773;&#20917;&#19979;&#24378;&#38750;&#39640;&#26031;&#22122;&#22768;&#30830;&#23450;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26356;&#19968;&#33324;&#30340;&#22522;&#20110;&#29109;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#22312;&#20219;&#20309;&#22122;&#22768;&#20998;&#24067;&#19979;&#65292;&#22312;&#29702;&#35770;&#19978;&#19982;&#20284;&#28982;&#20998;&#25968;&#19968;&#33268;&#12290;&#25105;&#20204;&#23545;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal discovery from observational data is an important but challenging task in many scientific fields. Recently, a method with non-combinatorial directed acyclic constraint, called NOTEARS, formulates the causal structure learning problem as a continuous optimization problem using least-square loss. Though the least-square loss function is well justified under the standard Gaussian noise assumption, it is limited if the assumption does not hold. In this work, we theoretically show that the violation of the Gaussian noise assumption will hinder the causal direction identification, making the causal orientation fully determined by the causal strength as well as the variances of noises in the linear case and by the strong non-Gaussian noises in the nonlinear case. Consequently, we propose a more general entropy-based loss that is theoretically consistent with the likelihood score under any noise distribution. We run extensive empirical evaluations on both synthetic data and real-world d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#30340;&#26041;&#24335;&#36827;&#21270;&#21442;&#25968;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2103.08249</link><description>&lt;p&gt;
&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#31867;&#20013;&#36827;&#21270;&#21442;&#25968;&#21270;&#25439;&#22833;&#20989;&#25968;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Evolving parametrized Loss for Image Classification Learning on Small Datasets. (arXiv:2103.08249v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.08249
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20803;&#23398;&#20064;&#30340;&#26041;&#24335;&#36827;&#21270;&#21442;&#25968;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#25552;&#39640;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26041;&#27861;&#26469;&#36827;&#21270;&#21442;&#25968;&#21270;&#25439;&#22833;&#20989;&#25968;&#65292;&#31216;&#20026;&#20803;&#25439;&#22833;&#32593;&#32476;&#65288;MLN&#65289;&#65292;&#29992;&#20110;&#22312;&#23567;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22270;&#20687;&#20998;&#31867;&#23398;&#20064;&#12290;&#22312;&#25105;&#20204;&#30340;&#26041;&#27861;&#20013;&#65292;MLN&#34987;&#23884;&#20837;&#21040;&#20998;&#31867;&#23398;&#20064;&#30340;&#26694;&#26550;&#20013;&#20316;&#20026;&#19968;&#20010;&#21487;&#24494;&#30340;&#30446;&#26631;&#20989;&#25968;&#12290;&#36890;&#36807;&#29992;&#36827;&#21270;&#31574;&#30053;&#31639;&#27861;&#65288;ES&#65289;&#26469;&#20248;&#21270;&#25439;&#22833;&#20989;&#25968;&#28436;&#21464;MLN&#65292;&#20351;&#24471;&#36890;&#36807;&#26368;&#23567;&#21270;&#35813;&#25439;&#22833;&#20248;&#21270;&#30340;&#20998;&#31867;&#22120;&#33021;&#22815;&#36798;&#21040;&#24456;&#22909;&#30340;&#27867;&#21270;&#25928;&#26524;&#12290;&#20998;&#31867;&#22120;&#22312;&#23567;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#20351;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#65288;SGD&#65289;&#26469;&#26368;&#23567;&#21270;MLN&#65292;&#28982;&#21518;MLN&#26681;&#25454;&#23567;&#25968;&#25454;&#38598;&#26356;&#26032;&#30340;&#20998;&#31867;&#22120;&#22312;&#22823;&#39564;&#35777;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#28436;&#21270;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;FashionMNIST&#37319;&#26679;&#20102;&#22823;&#37327;&#23567;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#26469;&#35757;&#32451;MLN&#65292;&#24182;&#22312;FashionMNIST&#21644;CIFAR10&#37319;&#26679;&#30340;&#39564;&#35777;&#20219;&#21153;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;MLN&#33021;&#22815;&#26377;&#25928;&#22320;&#25913;&#21892;&#20998;&#31867;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a meta-learning approach to evolving a parametrized loss function, which is called Meta-Loss Network (MLN), for training the image classification learning on small datasets. In our approach, the MLN is embedded in the framework of classification learning as a differentiable objective function. The MLN is evolved with the Evolutionary Strategy algorithm (ES) to an optimized loss function, such that a classifier, which optimized to minimize this loss, will achieve a good generalization effect. A classifier learns on a small training dataset to minimize MLN with Stochastic Gradient Descent (SGD), and then the MLN is evolved with the precision of the small-dataset-updated classifier on a large validation dataset. In order to evaluate our approach, the MLN is trained with a large number of small sample learning tasks sampled from FashionMNIST and tested on validation tasks sampled from FashionMNIST and CIFAR10. Experiment results demonstrate that the MLN effectively impr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#21457;&#32946;Braitenberg Vehicles&#20195;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21457;&#32946;&#21551;&#21457;&#30340;&#23398;&#20064;&#20195;&#29702;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#33258;&#20027;&#24615;&#30340;&#20855;&#36523;&#32463;&#39564;&#21644;&#24418;&#24577;&#21457;&#29983;&#22686;&#38271;&#27169;&#25311;&#65292;&#24182;&#32771;&#34385;&#20102;&#21457;&#32946;&#36712;&#36857;&#22312;&#31070;&#32463;&#31995;&#32479;&#24418;&#24577;&#29983;&#25104;&#12289;&#21457;&#32946;&#23398;&#20064;&#21644;&#21487;&#22609;&#24615;&#31561;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2103.05753</link><description>&lt;p&gt;
&#25345;&#32493;&#21457;&#23637;&#30340;&#31070;&#32463;&#20223;&#30495;&#65306;&#22522;&#20110;&#20855;&#36523;&#35745;&#31639;&#20195;&#29702;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Continual Developmental Neurosimulation Using Embodied Computational Agents. (arXiv:2103.05753v2 [q-bio.NC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.05753
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#21457;&#32946;Braitenberg Vehicles&#20195;&#29702;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20855;&#26377;&#21457;&#32946;&#21551;&#21457;&#30340;&#23398;&#20064;&#20195;&#29702;&#35774;&#35745;&#65292;&#23454;&#29616;&#20102;&#35745;&#31639;&#33258;&#20027;&#24615;&#30340;&#20855;&#36523;&#32463;&#39564;&#21644;&#24418;&#24577;&#21457;&#29983;&#22686;&#38271;&#27169;&#25311;&#65292;&#24182;&#32771;&#34385;&#20102;&#21457;&#32946;&#36712;&#36857;&#22312;&#31070;&#32463;&#31995;&#32479;&#24418;&#24577;&#29983;&#25104;&#12289;&#21457;&#32946;&#23398;&#20064;&#21644;&#21487;&#22609;&#24615;&#31561;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32508;&#21512;&#21457;&#32946;&#29983;&#29289;&#23398;&#12289;&#35748;&#30693;&#31185;&#23398;&#21644;&#35745;&#31639;&#24314;&#27169;&#65292;&#25105;&#20204;&#21487;&#20197;&#33719;&#24471;&#24456;&#22810;&#30693;&#35782;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#30446;&#26631;&#26159;&#22522;&#20110;Braitenberg Vehicles&#35774;&#35745;&#24320;&#21457;&#21463;&#21457;&#32946;&#21551;&#21457;&#30340;&#23398;&#20064;&#20195;&#29702;&#12290;&#21033;&#29992;&#36825;&#20123;&#20195;&#29702;&#20307;&#29616;&#20102;&#35745;&#31639;&#33258;&#20027;&#24615;&#30340;&#20855;&#36523;&#29305;&#24615;&#65292;&#19981;&#26029;&#38752;&#36817;&#23545;&#20855;&#36523;&#32463;&#39564;&#21644;&#24418;&#24577;&#21457;&#29983;&#22686;&#38271;&#20316;&#20026;&#35748;&#30693;&#21457;&#23637;&#33021;&#21147;&#32452;&#25104;&#37096;&#20998;&#30340;&#24314;&#27169;&#12290;&#25105;&#20204;&#32771;&#34385;&#29983;&#29289;&#21644;&#35748;&#30693;&#21457;&#23637;&#23545;&#25104;&#24180;&#34920;&#22411;&#29983;&#25104;&#21644;&#21487;&#29992;&#21457;&#23637;&#36335;&#24452;&#30340;&#24433;&#21709;&#12290;&#25345;&#32493;&#21457;&#23637;&#31070;&#32463;&#20223;&#30495;&#20351;&#25105;&#20204;&#33021;&#22815;&#32771;&#34385;&#21457;&#32946;&#36712;&#36857;&#22312;&#36830;&#25509;&#31070;&#32463;&#31995;&#32479;&#24418;&#24577;&#21457;&#29983;&#12289;&#21457;&#32946;&#23398;&#20064;&#21644;&#21487;&#22609;&#24615;&#31561;&#30456;&#20851;&#29616;&#35937;&#20013;&#25152;&#36215;&#30340;&#20316;&#29992;&#12290;&#30001;&#20110;&#19982;&#25345;&#32493;&#23398;&#20064;&#32039;&#23494;&#30456;&#20851;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#21457;&#32946;&#20855;&#36523;&#32039;&#23494;&#38598;&#25104;&#65292;&#21487;&#20197;&#20351;&#29992;&#19968;&#31181;&#31216;&#20026;&#21457;&#32946;Braitenberg Vehicles (dBVs)&#30340;&#20195;&#29702;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is much to learn through synthesis of Developmental Biology, Cognitive Science and Computational Modeling. Our path forward is to present a design for developmentally-inspired learning agents based on Braitenberg Vehicles. Using these agents to exemplify the embodied nature of computational autonomy, we move closer to modeling embodied experience and morphogenetic growth as components of cognitive developmental capacity. We consider biological and cognitive development which influence the generation of adult phenotypes and the contingency of available developmental pathways. Continual developmental neurosimulation allows us to consider the role of developmental trajectories in bridging the related phenomena of nervous system morphogenesis, developmental learning, and plasticity. Being closely tied to continual learning, our approach is tightly integrated with developmental embodiment, and can be implemented using a type of agent called developmental Braitenberg Vehicles (dBVs). T
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#23398;&#20064;&#40065;&#26834;&#22235;&#36275;&#36339;&#36291;&#25511;&#21046;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#32771;&#34385;&#25509;&#35302;&#28857;&#21644;&#38454;&#27573;&#30340;&#22870;&#21169;&#20989;&#25968;&#20197;&#25552;&#21319;&#36339;&#36291;&#24615;&#33021;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#30495;&#23454;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#12290;</title><link>http://arxiv.org/abs/2011.00446</link><description>&lt;p&gt;
&#39640;&#25928;&#23398;&#20064;&#20351;&#29992;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#40065;&#26834;&#22235;&#36275;&#36339;&#36291;&#30340;&#25511;&#21046;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Efficient Learning of Control Policies for Robust Quadruped Bounding using Pretrained Neural Networks. (arXiv:2011.00446v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.00446
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#23398;&#20064;&#40065;&#26834;&#22235;&#36275;&#36339;&#36291;&#25511;&#21046;&#31574;&#30053;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65292;&#32771;&#34385;&#25509;&#35302;&#28857;&#21644;&#38454;&#27573;&#30340;&#22870;&#21169;&#20989;&#25968;&#20197;&#25552;&#21319;&#36339;&#36291;&#24615;&#33021;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#30495;&#23454;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36339;&#36291;&#26159;&#22235;&#36275;&#21160;&#29289;&#22312;&#20811;&#26381;&#38556;&#30861;&#29289;&#26102;&#38750;&#24120;&#37325;&#35201;&#30340;&#27493;&#24577;&#20043;&#19968;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#39640;&#25928;&#22320;&#23398;&#20064;&#40065;&#26834;&#30340;&#36339;&#36291;&#27493;&#24577;&#65292;&#23613;&#31649;&#20854;&#21160;&#24577;&#36523;&#20307;&#36816;&#21160;&#21464;&#21270;&#36739;&#22823;&#12290;&#20316;&#32773;&#39318;&#20808;&#21033;&#29992;&#20256;&#32479;&#30340;&#22522;&#20110;&#27169;&#22411;&#30340;&#25511;&#21046;&#22120;&#20174;&#26426;&#22120;&#20154;&#30340;&#25968;&#25454;&#20013;&#39044;&#35757;&#32451;&#31070;&#32463;&#32593;&#32476;&#65292;&#28982;&#21518;&#36890;&#36807;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#19968;&#27493;&#20248;&#21270;&#39044;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#12290;&#29305;&#21035;&#22320;&#65292;&#20316;&#32773;&#35774;&#35745;&#20102;&#19968;&#20010;&#32771;&#34385;&#25509;&#35302;&#28857;&#21644;&#38454;&#27573;&#30340;&#22870;&#21169;&#20989;&#25968;&#65292;&#20197;&#24378;&#21046;&#36339;&#36291;&#27493;&#24577;&#30340;&#23545;&#31216;&#24615;&#21644;&#21608;&#26399;&#24615;&#65292;&#20174;&#32780;&#25913;&#21892;&#36339;&#36291;&#24615;&#33021;&#12290;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#22312;&#20223;&#30495;&#20013;&#23398;&#20064;&#65292;&#24182;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#30495;&#23454;&#30340;&#22235;&#36275;&#26426;&#22120;&#20154;Jueying Mini&#19978;&#12290;&#20316;&#32773;&#30340;&#26041;&#27861;&#22312;&#23460;&#20869;&#22806;&#29615;&#22659;&#20013;&#23637;&#31034;&#20102;&#22810;&#26679;&#21270;&#30340;&#24212;&#29992;&#12290;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#39640;&#25928;&#30340;&#35745;&#31639;&#21644;Jueying Mini&#22235;&#36275;&#26426;&#22120;&#20154;&#36339;&#36291;&#36229;&#36234;&#38556;&#30861;&#29289;&#30340;&#33391;&#22909;&#34892;&#21160;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bounding is one of the important gaits in quadrupedal locomotion for negotiating obstacles. The authors proposed an effective approach that can learn robust bounding gaits more efficiently despite its large variation in dynamic body movements. The authors first pretrained the neural network (NN) based on data from a robot operated by conventional model based controllers, and then further optimised the pretrained NN via deep reinforcement learning (DRL). In particular, the authors designed a reward function considering contact points and phases to enforce the gait symmetry and periodicity, which improved the bounding performance. The NN based feedback controller was learned in the simulation and directly deployed on the real quadruped robot Jueying Mini successfully. A variety of environments are presented both indoors and outdoors with the authors approach. The authors approach shows efficient computing and good locomotion results by the Jueying Mini quadrupedal robot bounding over une
&lt;/p&gt;</description></item></channel></rss>