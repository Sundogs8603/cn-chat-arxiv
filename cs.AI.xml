<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;</title><link>https://rss.arxiv.org/abs/2402.01295</link><description>&lt;p&gt;
ExtremeCast: &#25552;&#21319;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#30340;&#26497;&#20540;&#39044;&#27979;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
ExtremeCast: Boosting Extreme Value Prediction for Global Weather Forecast
&lt;/p&gt;
&lt;p&gt;
https://rss.arxiv.org/abs/2402.01295
&lt;/p&gt;
&lt;p&gt;
ExtremeCast&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;Exloss&#65292;&#23454;&#29616;&#20102;&#38024;&#23545;&#26497;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#21516;&#26102;&#24341;&#20837;&#20102;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#25454;&#39537;&#21160;&#22825;&#27668;&#39044;&#25253;&#22312;&#20840;&#29699;&#20013;&#26399;&#39044;&#25253;&#20013;&#24050;&#32463;&#24471;&#21040;&#20102;&#24555;&#36895;&#21457;&#23637;&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#20934;&#30830;&#39044;&#27979;&#26497;&#31471;&#22825;&#27668;&#26041;&#38754;&#23384;&#22312;&#22256;&#38590;&#65292;&#32780;&#26497;&#31471;&#20540;&#39044;&#27979;&#19982;&#27492;&#23494;&#20999;&#30456;&#20851;&#12290;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#65292;&#25105;&#20204;&#35777;&#26126;&#20351;&#29992;&#23545;&#31216;&#25439;&#22833;&#65292;&#22914;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#65292;&#20250;&#23548;&#33268;&#39044;&#27979;&#26377;&#20559;&#24046;&#24182;&#20302;&#20272;&#26497;&#20540;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Exloss&#65292;&#19968;&#31181;&#26032;&#30340;&#25439;&#22833;&#20989;&#25968;&#65292;&#36890;&#36807;&#38750;&#23545;&#31216;&#20248;&#21270;&#31361;&#20986;&#26497;&#20540;&#65292;&#20197;&#33719;&#24471;&#20934;&#30830;&#30340;&#26497;&#31471;&#22825;&#27668;&#39044;&#25253;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26497;&#20540;&#22686;&#24378;&#31574;&#30053;ExEnsemble&#65292;&#23427;&#22686;&#21152;&#20102;&#20687;&#32032;&#20540;&#30340;&#26041;&#24046;&#65292;&#24182;&#25552;&#39640;&#20102;&#39044;&#25253;&#30340;&#31283;&#20581;&#24615;&#12290;&#32467;&#21512;&#20808;&#36827;&#30340;&#20840;&#29699;&#22825;&#27668;&#39044;&#25253;&#27169;&#22411;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Data-driven weather forecast based on machine learning (ML) has experienced rapid development and demonstrated superior performance in the global medium-range forecast compared to traditional physics-based dynamical models. However, most of these ML models struggle with accurately predicting extreme weather, which is closely related to the extreme value prediction. Through mathematical analysis, we prove that the use of symmetric losses, such as the Mean Squared Error (MSE), leads to biased predictions and underestimation of extreme values. To address this issue, we introduce Exloss, a novel loss function that performs asymmetric optimization and highlights extreme values to obtain accurate extreme weather forecast. Furthermore, we introduce a training-free extreme value enhancement strategy named ExEnsemble, which increases the variance of pixel values and improves the forecast robustness. Combined with an advanced global weather forecast model, extensive experiments show that our sol
&lt;/p&gt;</description></item><item><title>&#35821;&#35328;&#26657;&#27491;&#27969;&#26159;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;&#27010;&#29575;&#27969;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#20256;&#36755;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#21644;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#39046;&#22495;&#36716;&#31227;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.16995</link><description>&lt;p&gt;
&#35821;&#35328;&#26657;&#27491;&#27969;&#65306;&#36890;&#36807;&#27010;&#29575;&#27969;&#25512;&#21160;&#25193;&#25955;&#35821;&#35328;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Language Rectified Flow: Advancing Diffusion Language Generation with Probabilistic Flows
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16995
&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#26657;&#27491;&#27969;&#26159;&#19968;&#31181;&#22522;&#20110;&#26631;&#20934;&#27010;&#29575;&#27969;&#27169;&#22411;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#20256;&#36755;&#65292;&#25552;&#20379;&#20102;&#32479;&#19968;&#21644;&#26377;&#25928;&#30340;&#29983;&#25104;&#27169;&#22411;&#21644;&#39046;&#22495;&#36716;&#31227;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#25193;&#25955;&#35821;&#35328;&#27169;&#22411;&#22522;&#30784;&#19978;&#25511;&#21046;&#21477;&#23376;&#23646;&#24615;&#65288;&#20363;&#22914;&#24773;&#24863;&#65289;&#21644;&#32467;&#26500;&#65288;&#20363;&#22914;&#21477;&#27861;&#32467;&#26500;&#65289;&#21462;&#24471;&#20102;&#25104;&#21151;&#12290;&#19968;&#20010;&#25512;&#21160;&#39640;&#36136;&#37327;&#26679;&#26412;&#29983;&#25104;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#36845;&#20195;&#21435;&#22122;&#25968;&#21315;&#27493;&#12290;&#23613;&#31649;&#26377;&#30410;&#65292;&#20294;&#20174;&#22122;&#22768;&#24320;&#22987;&#30340;&#22797;&#26434;&#24615;&#21644;&#23398;&#20064;&#27493;&#39588;&#38480;&#21046;&#20102;&#20854;&#22312;&#35768;&#22810;NLP&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#23454;&#29616;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Language Rectified Flow&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26631;&#20934;&#27010;&#29575;&#27969;&#27169;&#22411;&#30340;&#37325;&#26500;&#12290;&#35821;&#35328;&#26657;&#27491;&#27969;&#23398;&#20064;&#65288;&#31070;&#32463;&#65289;&#24120;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;&#22312;&#28304;&#20998;&#24067;&#21644;&#30446;&#26631;&#20998;&#24067;&#20043;&#38388;&#20256;&#36755;&#65292;&#20026;&#29983;&#25104;&#24314;&#27169;&#21644;&#22495;&#36716;&#31227;&#25552;&#20379;&#20102;&#32479;&#19968;&#21644;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20174;&#28304;&#20998;&#24067;&#24320;&#22987;&#65292;&#25105;&#20204;&#30340;&#35821;&#35328;&#26657;&#27491;&#27969;&#20135;&#29983;&#24555;&#36895;&#20223;&#30495;&#21644;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16995v1 Announce Type: cross  Abstract: Recent works have demonstrated success in controlling sentence attributes ($e.g.$, sentiment) and structure ($e.g.$, syntactic structure) based on the diffusion language model. A key component that drives theimpressive performance for generating high-quality samples from noise is iteratively denoise for thousands of steps. While beneficial, the complexity of starting from the noise and the learning steps has limited its implementation to many NLP real-world applications. This paper proposes Language Rectified Flow ({\ours}). Our method is based on the reformulation of the standard probabilistic flow models. Language rectified flow learns (neural) ordinary differential equation models to transport between the source distribution and the target distribution, hence providing a unified and effective solution to generative modeling and domain transfer. From the source distribution, our language rectified flow yields fast simulation and effe
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20027;&#20307;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#26377;&#30028;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#27880;&#24847;&#21147;&#23618;&#28151;&#21512;&#19981;&#21516;&#20027;&#20307;&#35270;&#35273;&#29305;&#24449;&#23548;&#33268;&#30340;&#35821;&#20041;&#27844;&#28431;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16990</link><description>&lt;p&gt;
&#20570;&#33258;&#24049;&#65306;&#22810;&#20027;&#20307;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#26377;&#30028;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Be Yourself: Bounded Attention for Multi-Subject Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16990
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22810;&#20027;&#20307;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#30340;&#26377;&#30028;&#27880;&#24847;&#21147;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#25193;&#25955;&#27169;&#22411;&#27880;&#24847;&#21147;&#23618;&#28151;&#21512;&#19981;&#21516;&#20027;&#20307;&#35270;&#35273;&#29305;&#24449;&#23548;&#33268;&#30340;&#35821;&#20041;&#27844;&#28431;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16990v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#20027;&#20307;&#25991;&#26412;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#20855;&#26377;&#29983;&#25104;&#22810;&#26679;&#19988;&#39640;&#36136;&#37327;&#22270;&#20687;&#30340;&#21069;&#25152;&#26410;&#26377;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24120;&#24120;&#38590;&#20197;&#24544;&#23454;&#22320;&#25429;&#25417;&#21253;&#21547;&#22810;&#20010;&#20027;&#39064;&#30340;&#22797;&#26434;&#36755;&#20837;&#25552;&#31034;&#30340;&#39044;&#26399;&#35821;&#20041;&#12290;&#26368;&#36817;&#65292;&#35768;&#22810;&#24067;&#23616;&#21040;&#22270;&#20687;&#30340;&#25193;&#23637;&#24050;&#34987;&#24341;&#20837;&#20197;&#25552;&#39640;&#29992;&#25143;&#25511;&#21046;&#65292;&#26088;&#22312;&#23450;&#20301;&#30001;&#29305;&#23450;&#20196;&#29260;&#34920;&#31034;&#30340;&#20027;&#39064;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20250;&#20135;&#29983;&#35821;&#20041;&#19981;&#20934;&#30830;&#30340;&#22270;&#20687;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#22810;&#20010;&#22312;&#35821;&#20041;&#19978;&#25110;&#35270;&#35273;&#19978;&#30456;&#20284;&#30340;&#20027;&#39064;&#26102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#24182;&#20998;&#26512;&#20102;&#36825;&#20123;&#38480;&#21046;&#30340;&#21407;&#22240;&#12290;&#25105;&#20204;&#30340;&#25506;&#32034;&#25581;&#31034;&#20102;&#36825;&#20010;&#20027;&#35201;&#38382;&#39064;&#36215;&#28304;&#20110;&#21435;&#22122;&#36807;&#31243;&#20013;&#20027;&#39064;&#20043;&#38388;&#30340;&#26080;&#24847;&#20041;&#35821;&#20041;&#27844;&#28431;&#12290;&#36825;&#31181;&#27844;&#28431;&#24402;&#22240;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#23618;&#65292;&#36825;&#20123;&#23618;&#20542;&#21521;&#20110;&#28151;&#21512;&#19981;&#21516;&#20027;&#20307;&#30340;&#35270;&#35273;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26377;&#30028;&#27880;&#24847;&#21147;&#65292;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16990v1 Announce Type: cross  Abstract: Text-to-image diffusion models have an unprecedented ability to generate diverse and high-quality images. However, they often struggle to faithfully capture the intended semantics of complex input prompts that include multiple subjects. Recently, numerous layout-to-image extensions have been introduced to improve user control, aiming to localize subjects represented by specific tokens. Yet, these methods often produce semantically inaccurate images, especially when dealing with multiple semantically or visually similar subjects. In this work, we study and analyze the causes of these limitations. Our exploration reveals that the primary issue stems from inadvertent semantic leakage between subjects in the denoising process. This leakage is attributed to the diffusion model's attention layers, which tend to blend the visual features of different subjects. To address these issues, we introduce Bounded Attention, a training-free method for
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#19981;&#21516;&#30340;&#24863;&#20852;&#36259;&#26041;&#38754;&#26469;&#25913;&#36827;&#27010;&#24565;&#23884;&#20837;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#25417;&#26356;&#24191;&#27867;&#30340;&#24120;&#35782;&#23646;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16984</link><description>&lt;p&gt;
&#29992;&#22810;&#26041;&#38754;&#27010;&#24565;&#23884;&#20837;&#27169;&#22411;&#24314;&#27169;&#24120;&#35782;&#20849;&#24615;
&lt;/p&gt;
&lt;p&gt;
Modelling Commonsense Commonalities with Multi-Facet Concept Embeddings
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16984
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#26126;&#30830;&#24314;&#27169;&#19981;&#21516;&#30340;&#24863;&#20852;&#36259;&#26041;&#38754;&#26469;&#25913;&#36827;&#27010;&#24565;&#23884;&#20837;&#65292;&#20351;&#20854;&#33021;&#22815;&#25429;&#25417;&#26356;&#24191;&#27867;&#30340;&#24120;&#35782;&#23646;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#23884;&#20837;&#25552;&#20379;&#20102;&#19968;&#31181;&#23454;&#29992;&#19988;&#39640;&#25928;&#30340;&#26426;&#21046;&#65292;&#23558;&#24120;&#35782;&#30693;&#35782;&#27880;&#20837;&#21040;&#19979;&#28216;&#20219;&#21153;&#20013;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#26631;&#20934;&#23884;&#20837;&#20027;&#35201;&#21453;&#26144;&#22522;&#26412;&#20998;&#31867;&#31867;&#21035;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#22312;&#23398;&#20064;&#27010;&#24565;&#23884;&#20837;&#26102;&#26126;&#30830;&#24314;&#27169;&#24863;&#20852;&#36259;&#30340;&#19981;&#21516;&#26041;&#38754;&#65292;&#24471;&#21040;&#20102;&#33021;&#22815;&#25429;&#25417;&#26356;&#24191;&#27867;&#24120;&#35782;&#23646;&#24615;&#30340;&#23884;&#20837;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16984v1 Announce Type: new  Abstract: Concept embeddings offer a practical and efficient mechanism for injecting commonsense knowledge into downstream tasks. Their core purpose is often not to predict the commonsense properties of concepts themselves, but rather to identify commonalities, i.e.\ sets of concepts which share some property of interest. Such commonalities are the basis for inductive generalisation, hence high-quality concept embeddings can make learning easier and more robust. Unfortunately, standard embeddings primarily reflect basic taxonomic categories, making them unsuitable for finding commonalities that refer to more specific aspects (e.g.\ the colour of objects or the materials they are made of). In this paper, we address this limitation by explicitly modelling the different facets of interest when learning concept embeddings. We show that this leads to embeddings which capture a more diverse range of commonsense properties, and consistently improves resu
&lt;/p&gt;</description></item><item><title>VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16973</link><description>&lt;p&gt;
VoiceCraft&#65306;&#37326;&#22806;&#38646;-shot&#35821;&#38899;&#32534;&#36753;&#21644;&#25991;&#26412;&#21040;&#35821;&#38899;
&lt;/p&gt;
&lt;p&gt;
VoiceCraft: Zero-Shot Speech Editing and Text-to-Speech in the Wild
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16973
&lt;/p&gt;
&lt;p&gt;
VoiceCraft&#26159;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#22312;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#23454;&#29616;&#20102;&#22312;&#22810;&#26679;&#24615;&#25968;&#25454;&#38598;&#19978;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;VoiceCraft&#65292;&#19968;&#20010;&#22522;&#20110;&#26631;&#35760;&#22635;&#20805;&#30340;&#31070;&#32463;&#32534;&#35299;&#30721;&#22120;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22312;&#26377;&#22768;&#20070;&#12289;&#20114;&#32852;&#32593;&#35270;&#39057;&#21644;&#25773;&#23458;&#19978;&#35821;&#38899;&#32534;&#36753;&#21644;&#38646;-shot&#25991;&#26412;&#21040;&#35821;&#38899;&#65288;TTS&#65289;&#26041;&#38754;&#30340;&#26368;&#26032;&#24615;&#33021;&#12290;VoiceCraft&#37319;&#29992;Transformer&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#26631;&#35760;&#37325;&#25490;&#36807;&#31243;&#65292;&#32467;&#21512;&#20102;&#22240;&#26524;&#25513;&#30721;&#21644;&#24310;&#36831;&#22534;&#21472;&#65292;&#20197;&#23454;&#29616;&#22312;&#29616;&#26377;&#24207;&#21015;&#20869;&#30340;&#29983;&#25104;&#12290;&#22312;&#35821;&#38899;&#32534;&#36753;&#20219;&#21153;&#19978;&#65292;VoiceCraft&#29983;&#25104;&#30340;&#32534;&#36753;&#35821;&#38899;&#22312;&#33258;&#28982;&#24230;&#26041;&#38754;&#20960;&#20046;&#19982;&#26410;&#32534;&#36753;&#30340;&#24405;&#38899;&#38590;&#20197;&#21306;&#20998;&#65292;&#32463;&#20154;&#31867;&#35780;&#20272;&#65307;&#23545;&#20110;&#38646;-shot TTS&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#27169;&#22411;&#65292;&#21253;&#25324;VALLE&#21644;&#27969;&#34892;&#30340;&#21830;&#19994;&#27169;&#22411;XTTS-v2&#12290;&#20851;&#38190;&#30340;&#26159;&#65292;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#22810;&#26679;&#21475;&#38899;&#12289;&#35821;&#38899;&#39118;&#26684;&#12289;&#24405;&#21046;&#26465;&#20214;&#12289;&#32972;&#26223;&#22122;&#38899;&#21644;&#38899;&#20048;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#21644;&#30495;&#23454;&#24615;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19982;&#20854;&#20182;&#27169;&#22411;&#30456;&#27604;&#34920;&#29616;&#22987;&#32456;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16973v1 Announce Type: cross  Abstract: We introduce VoiceCraft, a token infilling neural codec language model, that achieves state-of-the-art performance on both speech editing and zero-shot text-to-speech (TTS) on audiobooks, internet videos, and podcasts. VoiceCraft employs a Transformer decoder architecture and introduces a token rearrangement procedure that combines causal masking and delayed stacking to enable generation within an existing sequence. On speech editing tasks, VoiceCraft produces edited speech that is nearly indistinguishable from unedited recordings in terms of naturalness, as evaluated by humans; for zero-shot TTS, our model outperforms prior SotA models including VALLE and the popular commercial model XTTS-v2. Crucially, the models are evaluated on challenging and realistic datasets, that consist of diverse accents, speaking styles, recording conditions, and background noise and music, and our model performs consistently well compared to other models a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;LLM&#20195;&#29702;&#25805;&#20316;&#31995;&#32479;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12289;&#20419;&#36827;&#20195;&#29702;&#38388;&#19978;&#19979;&#25991;&#20999;&#25442;&#12289;&#23454;&#29616;&#24182;&#21457;&#25191;&#34892;&#20197;&#21450;&#20026;&#20195;&#29702;&#25552;&#20379;&#24037;&#20855;&#26381;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.16971</link><description>&lt;p&gt;
LLM Agent Operating System
&lt;/p&gt;
&lt;p&gt;
LLM Agent Operating System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16971
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25805;&#20316;&#31995;&#32479;&#20013;&#30340;LLM&#20195;&#29702;&#25805;&#20316;&#31995;&#32479;&#65292;&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#12289;&#20419;&#36827;&#20195;&#29702;&#38388;&#19978;&#19979;&#25991;&#20999;&#25442;&#12289;&#23454;&#29616;&#24182;&#21457;&#25191;&#34892;&#20197;&#21450;&#20026;&#20195;&#29702;&#25552;&#20379;&#24037;&#20855;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16971v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26234;&#33021;&#20195;&#29702;&#23384;&#22312;&#35832;&#22810;&#25361;&#25112;&#65292;&#20250;&#25439;&#23475;&#23427;&#20204;&#30340;&#25928;&#29575;&#21644;&#21151;&#25928;&#12290;&#20854;&#20013;&#21253;&#25324;&#20195;&#29702;&#35831;&#27714;&#22312;LLM&#19978;&#30340;&#27425;&#20248;&#35843;&#24230;&#21644;&#36164;&#28304;&#20998;&#37197;&#12289;&#22312;&#20195;&#29702;&#21644;LLM&#20043;&#38388;&#20132;&#20114;&#26102;&#20445;&#25345;&#19978;&#19979;&#25991;&#30340;&#22256;&#38590;&#65292;&#20197;&#21450;&#23558;&#20855;&#26377;&#19981;&#21516;&#33021;&#21147;&#21644;&#19987;&#19994;&#21270;&#30340;&#24322;&#26500;&#20195;&#29702;&#38598;&#25104;&#22312;&#19968;&#36215;&#30340;&#22797;&#26434;&#24615;&#12290;&#20195;&#29702;&#25968;&#37327;&#21644;&#22797;&#26434;&#24615;&#30340;&#24555;&#36895;&#22686;&#21152;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#36890;&#24120;&#20250;&#23548;&#33268;&#36164;&#28304;&#29942;&#39048;&#21644;&#27425;&#20248;&#36164;&#28304;&#21033;&#29992;&#12290;&#21463;&#21040;&#36825;&#20123;&#25361;&#25112;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;AIOS&#65292;&#19968;&#31181;LLM&#20195;&#29702;&#25805;&#20316;&#31995;&#32479;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23884;&#20837;&#25805;&#20316;&#31995;&#32479;&#65288;OS&#65289;&#20013;&#12290;&#20855;&#20307;&#22320;&#65292;AIOS&#26088;&#22312;&#20248;&#21270;&#36164;&#28304;&#20998;&#37197;&#65292;&#20419;&#36827;&#20195;&#29702;&#20043;&#38388;&#30340;&#19978;&#19979;&#25991;&#20999;&#25442;&#65292;&#23454;&#29616;&#20195;&#29702;&#30340;&#24182;&#21457;&#25191;&#34892;&#65292;&#20026;&#20195;&#29702;&#25552;&#20379;&#24037;&#20855;&#26381;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16971v1 Announce Type: cross  Abstract: The integration and deployment of large language model (LLM)-based intelligent agents have been fraught with challenges that compromise their efficiency and efficacy. Among these issues are sub-optimal scheduling and resource allocation of agent requests over the LLM, the difficulties in maintaining context during interactions between agent and LLM, and the complexities inherent in integrating heterogeneous agents with different capabilities and specializations. The rapid increase of agent quantity and complexity further exacerbates these issues, often leading to bottlenecks and sub-optimal utilization of resources. Inspired by these challenges, this paper presents AIOS, an LLM agent operating system, which embeds large language model into operating systems (OS). Specifically, AIOS is designed to optimize resource allocation, facilitate context switch across agents, enable concurrent execution of agents, provide tool service for agents
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#65292;&#21487;&#20197;&#37327;&#21270;&#22320;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#19982;&#25968;&#25454;&#28151;&#21512;&#27604;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#25311;&#21512;&#20989;&#25968;&#24418;&#24335;&#26469;&#24341;&#23548;&#29702;&#24819;&#30340;&#25968;&#25454;&#28151;&#21512;&#36873;&#25321;&#65292;&#20174;&#32780;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#28151;&#21512;&#12290;</title><link>https://arxiv.org/abs/2403.16952</link><description>&lt;p&gt;
&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#65306;&#36890;&#36807;&#39044;&#27979;&#35821;&#35328;&#24314;&#27169;&#24615;&#33021;&#26469;&#20248;&#21270;&#25968;&#25454;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Data Mixing Laws: Optimizing Data Mixtures by Predicting Language Modeling Performance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16952
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#20102;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#65292;&#21487;&#20197;&#37327;&#21270;&#22320;&#39044;&#27979;&#27169;&#22411;&#24615;&#33021;&#19982;&#25968;&#25454;&#28151;&#21512;&#27604;&#20363;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36890;&#36807;&#25311;&#21512;&#20989;&#25968;&#24418;&#24335;&#26469;&#24341;&#23548;&#29702;&#24819;&#30340;&#25968;&#25454;&#28151;&#21512;&#36873;&#25321;&#65292;&#20174;&#32780;&#20248;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35757;&#32451;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#21253;&#25324;&#22810;&#20010;&#39046;&#22495;&#65288;&#20363;&#22914;&#32593;&#32476;&#25991;&#26412;&#12289;&#23398;&#26415;&#35770;&#25991;&#12289;&#20195;&#30721;&#65289;&#65292;&#20854;&#28151;&#21512;&#27604;&#20363;&#23545;&#32467;&#26524;&#27169;&#22411;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#24037;&#20316;&#36890;&#24120;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#25110;&#23450;&#24615;&#31574;&#30053;&#26469;&#35843;&#25972;&#27604;&#20363;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#27169;&#22411;&#24615;&#33021;&#19982;&#28151;&#21512;&#27604;&#20363;&#20043;&#38388;&#30340;&#20989;&#25968;&#24418;&#24335;&#30340;&#23450;&#37327;&#21487;&#39044;&#27979;&#24615;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#12290;&#22312;&#26679;&#26412;&#28151;&#21512;&#19978;&#25311;&#21512;&#36825;&#31181;&#20989;&#25968;&#25581;&#31034;&#20102;&#26410;&#35265;&#28151;&#21512;&#30340;&#27169;&#22411;&#24615;&#33021;&#65292;&#20174;&#32780;&#24341;&#23548;&#36873;&#25321;&#29702;&#24819;&#30340;&#25968;&#25454;&#28151;&#21512;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35757;&#32451;&#27493;&#39588;&#12289;&#27169;&#22411;&#22823;&#23567;&#21644;&#25105;&#20204;&#30340;&#25968;&#25454;&#28151;&#21512;&#35268;&#24459;&#30340;&#32553;&#25918;&#35268;&#24459;&#30340;&#23884;&#22871;&#20351;&#29992;&#65292;&#20197;&#20351;&#24471;&#20165;&#36890;&#36807;&#23567;&#35268;&#27169;&#35757;&#32451;&#23601;&#33021;&#22815;&#39044;&#27979;&#22312;&#21508;&#31181;&#28151;&#21512;&#25968;&#25454;&#19979;&#35757;&#32451;&#30340;&#22823;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#23454;&#39564;&#32467;&#26524;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#25928;&#22320;&#20248;&#21270;&#20102;&#35757;&#32451;&#28151;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16952v1 Announce Type: cross  Abstract: Pretraining data of large language models composes multiple domains (e.g., web texts, academic papers, codes), whose mixture proportions crucially impact the competence of outcome models. While existing endeavors rely on heuristics or qualitative strategies to tune the proportions, we discover the quantitative predictability of model performance regarding the mixture proportions in function forms, which we refer to as the data mixing laws. Fitting such functions on sample mixtures unveils model performance on unseen mixtures before actual runs, thus guiding the selection of an ideal data mixture. Furthermore, we propose nested use of the scaling laws of training steps, model sizes, and our data mixing law to enable predicting the performance of large models trained on massive data under various mixtures with only small-scale training. Moreover, experimental results verify that our method effectively optimizes the training mixture of a 
&lt;/p&gt;</description></item><item><title>&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16950</link><description>&lt;p&gt;
&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#19968;&#33268;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#25104;&#23545;&#20559;&#22909;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Aligning with Human Judgement: The Role of Pairwise Preference in Large Language Model Evaluators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16950
&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35780;&#20272;&#20013;&#65292;&#36890;&#36807;&#24341;&#20837;&#25104;&#23545;&#20559;&#22909;&#25628;&#32034;&#26041;&#27861;PAIRS&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;LLMs&#19982;&#20154;&#31867;&#21028;&#26029;&#19981;&#19968;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#21462;&#24471;&#20102;&#20248;&#20110;&#30452;&#25509;&#25171;&#20998;&#30340;&#26368;&#20808;&#36827;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#33258;&#21160;&#35780;&#20272;&#22120;&#22312;&#35780;&#20272;&#29983;&#25104;&#30340;&#33258;&#28982;&#35821;&#35328;&#36136;&#37327;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;LLMs&#22312;&#35780;&#20272;&#20013;&#20173;&#23384;&#22312;&#20559;&#35265;&#65292;&#24120;&#24120;&#38590;&#20197;&#29983;&#25104;&#19982;&#20154;&#31867;&#35780;&#20272;&#19968;&#33268;&#30340;&#36830;&#36143;&#35780;&#20272;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;LLM&#35780;&#20272;&#22120;&#19982;&#20154;&#31867;&#21028;&#26029;&#20043;&#38388;&#30340;&#19981;&#19968;&#33268;&#36827;&#34892;&#31995;&#32479;&#30740;&#31350;&#65292;&#25581;&#31034;&#29616;&#26377;&#26088;&#22312;&#20943;&#36731;&#20559;&#35265;&#30340;&#26657;&#20934;&#26041;&#27861;&#19981;&#36275;&#20197;&#26377;&#25928;&#23558;LLM&#35780;&#20272;&#22120;&#23545;&#40784;&#12290;&#21463;&#21040;RLHF&#20013;&#23545;&#20559;&#22909;&#25968;&#25454;&#30340;&#20351;&#29992;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23558;&#35780;&#20272;&#24418;&#24335;&#21270;&#20026;&#19968;&#20010;&#25490;&#24207;&#38382;&#39064;&#65292;&#24182;&#24341;&#20837;Pairwise-preference Search&#65288;PAIRS&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#20197;LLMs&#36827;&#34892;&#25104;&#23545;&#27604;&#36739;&#24182;&#26377;&#25928;&#23545;&#20505;&#36873;&#25991;&#26412;&#36827;&#34892;&#25490;&#24207;&#30340;&#22522;&#20110;&#19981;&#30830;&#23450;&#24615;&#24341;&#23548;&#30340;&#25628;&#32034;&#26041;&#27861;&#12290;PAIRS&#22312;&#20195;&#34920;&#24615;&#35780;&#20272;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#24182;&#19988;&#26174;&#31034;&#20986;&#27604;&#30452;&#25509;&#25171;&#20998;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16950v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have demonstrated promising capabilities as automatic evaluators in assessing the quality of generated natural language. However, LLMs still exhibit biases in evaluation and often struggle to generate coherent evaluations that align with human assessments. In this work, we first conduct a systematic study of the misalignment between LLM evaluators and human judgement, revealing that existing calibration methods aimed at mitigating biases are insufficient for effectively aligning LLM evaluators. Inspired by the use of preference data in RLHF, we formulate the evaluation as a ranking problem and introduce Pairwise-preference Search (PAIRS), an uncertainty-guided search method that employs LLMs to conduct pairwise comparisons and efficiently ranks candidate texts. PAIRS achieves state-of-the-art performance on representative evaluation tasks and demonstrates significant improvements over direct scoring. Furthe
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;SPACE-IDEAS&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;&#19982;&#31354;&#38388;&#21019;&#26032;&#30456;&#20851;&#30340;&#26174;&#33879;&#20449;&#24687;&#65292;&#21253;&#25324;&#22810;&#31181;&#25991;&#26412;&#39118;&#26684;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#35757;&#32451;&#26356;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.16941</link><description>&lt;p&gt;
SPACE-IDEAS&#65306;&#29992;&#20110;&#31354;&#38388;&#21019;&#26032;&#20013;&#26174;&#33879;&#20449;&#24687;&#26816;&#27979;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SPACE-IDEAS: A Dataset for Salient Information Detection in Space Innovation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16941
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#20171;&#32461;&#20102;SPACE-IDEAS&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#26816;&#27979;&#19982;&#31354;&#38388;&#21019;&#26032;&#30456;&#20851;&#30340;&#26174;&#33879;&#20449;&#24687;&#65292;&#21253;&#25324;&#22810;&#31181;&#25991;&#26412;&#39118;&#26684;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#35757;&#32451;&#26356;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16941v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#36328;&#39046;&#22495; &#25688;&#35201;&#65306;&#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22312;&#25991;&#26412;&#20013;&#26816;&#27979;&#26174;&#33879;&#37096;&#20998;&#24050;&#34987;&#24191;&#27867;&#24212;&#29992;&#20197;&#32531;&#35299;&#20449;&#24687;&#36807;&#36733;&#30340;&#24433;&#21709;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21487;&#29992;&#20110;&#27492;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#20027;&#35201;&#28304;&#33258;&#23398;&#26415;&#20986;&#29256;&#29289;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;SPACE-IDEAS&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#20174;&#19982;&#31354;&#38388;&#39046;&#22495;&#30456;&#20851;&#30340;&#21019;&#26032;&#29702;&#24565;&#20013;&#26816;&#27979;&#26174;&#33879;&#20449;&#24687;&#30340;&#25968;&#25454;&#38598;&#12290;SPACE-IDEAS&#20013;&#30340;&#25991;&#26412;&#39118;&#26684;&#22810;&#26679;&#65292;&#21253;&#25324;&#38750;&#27491;&#24335;&#12289;&#25216;&#26415;&#12289;&#23398;&#26415;&#21644;&#21830;&#19994;&#20889;&#20316;&#39118;&#26684;&#12290;&#38500;&#20102;&#25163;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#22806;&#65292;&#25105;&#20204;&#36824;&#21457;&#24067;&#20102;&#19968;&#20010;&#20351;&#29992;&#22823;&#22411;&#29983;&#25104;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#30340;&#25193;&#23637;&#29256;&#26412;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#19981;&#21516;&#30340;&#21477;&#23376;&#21644;&#24207;&#21015;&#21477;&#20998;&#31867;&#22120;&#65292;&#24182;&#34920;&#26126;&#33258;&#21160;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21487;&#20197;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#26469;&#35757;&#32451;&#26356;&#22909;&#30340;&#20998;&#31867;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16941v1 Announce Type: cross  Abstract: Detecting salient parts in text using natural language processing has been widely used to mitigate the effects of information overflow. Nevertheless, most of the datasets available for this task are derived mainly from academic publications. We introduce SPACE-IDEAS, a dataset for salient information detection from innovation ideas related to the Space domain. The text in SPACE-IDEAS varies greatly and includes informal, technical, academic and business-oriented writing styles. In addition to a manually annotated dataset we release an extended version that is annotated using a large generative language model. We train different sentence and sequential sentence classifiers, and show that the automatically annotated dataset can be leveraged using multitask learning to train better classifiers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102; Generalized Latent Equilibrium (GLE)&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#29289;&#29702;&#21160;&#24577;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.16933</link><description>&lt;p&gt;
&#36890;&#36807;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#22823;&#33041;&#36827;&#34892;&#21453;&#21521;&#20256;&#25773;
&lt;/p&gt;
&lt;p&gt;
Backpropagation through space, time, and the brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16933
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102; Generalized Latent Equilibrium (GLE)&#65292;&#23427;&#26159;&#19968;&#31181;&#38024;&#23545;&#31070;&#32463;&#20803;&#32593;&#32476;&#30340;&#29289;&#29702;&#21160;&#24577;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#38656;&#35201;&#26681;&#25454;&#23427;&#20204;&#23545;&#35299;&#20915;&#20219;&#21153;&#30340;&#30456;&#23545;&#36129;&#29486;&#26469;&#35843;&#25972;&#21333;&#20010;&#31361;&#35302;&#12290;&#28982;&#32780;&#65292;&#26080;&#35770;&#26159;&#29983;&#29289;&#36824;&#26159;&#20154;&#24037;&#30340;&#29289;&#29702;&#31070;&#32463;&#31995;&#32479;&#37117;&#21463;&#21040;&#26102;&#31354;&#23616;&#38480;&#12290;&#36825;&#26679;&#30340;&#32593;&#32476;&#22914;&#20309;&#25191;&#34892;&#39640;&#25928;&#30340;&#20449;&#29992;&#20998;&#37197;&#65292;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#20173;&#26159;&#19968;&#20010;&#24748;&#32780;&#26410;&#20915;&#30340;&#38382;&#39064;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#38169;&#35823;&#30340;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#20960;&#20046;&#26222;&#36941;&#34987;&#31354;&#38388;&#65288;BP&#65289;&#21644;&#26102;&#38388;&#65288;BPTT&#65289;&#20004;&#31181;&#26041;&#24335;&#32473;&#20986;&#31572;&#26696;&#12290;&#28982;&#32780;&#65292;BP(TT)&#34987;&#24191;&#27867;&#35748;&#20026;&#20381;&#36182;&#20110;&#19981;&#20855;&#29983;&#29289;&#23398;&#24847;&#20041;&#30340;&#20551;&#35774;&#65292;&#29305;&#21035;&#26159;&#20851;&#20110;&#26102;&#31354;&#23616;&#38480;&#24615;&#65292;&#32780;&#27491;&#21521;&#20256;&#25773;&#27169;&#22411;&#65292;&#22914;&#23454;&#26102;&#36882;&#24402;&#23398;&#20064;&#65288;RTRL&#65289;&#65292;&#21017;&#21463;&#21040;&#20869;&#23384;&#32422;&#26463;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#24191;&#20041;&#28508;&#22312;&#24179;&#34913;&#65288;GLE&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#38024;&#23545;&#31070;&#32463;&#20803;&#29289;&#29702;&#21160;&#24577;&#32593;&#32476;&#23436;&#20840;&#23616;&#37096;&#26102;&#31354;&#20449;&#29992;&#20998;&#37197;&#30340;&#35745;&#31639;&#26694;&#26550;&#12290;&#25105;&#20204;&#20174;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16933v1 Announce Type: cross  Abstract: Effective learning in neuronal networks requires the adaptation of individual synapses given their relative contribution to solving a task. However, physical neuronal systems -- whether biological or artificial -- are constrained by spatio-temporal locality. How such networks can perform efficient credit assignment, remains, to a large extent, an open question. In Machine Learning, the answer is almost universally given by the error backpropagation algorithm, through both space (BP) and time (BPTT). However, BP(TT) is well-known to rely on biologically implausible assumptions, in particular with respect to spatiotemporal (non-)locality, while forward-propagation models such as real-time recurrent learning (RTRL) suffer from prohibitive memory constraints. We introduce Generalized Latent Equilibrium (GLE), a computational framework for fully local spatio-temporal credit assignment in physical, dynamical networks of neurons. We start by 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16915</link><description>&lt;p&gt;
&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31895;&#35843;&#20248;&#30340;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Coarse-Tuning for Ad-hoc Document Retrieval Using Pre-trained Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#65292;&#22312;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#20013;&#26174;&#33879;&#25913;&#21892;&#20102;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#31995;&#32479;&#20013;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM-based IR&#65289;&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#38500;&#20102;&#19979;&#28216;&#20219;&#21153;&#29305;&#23450;&#30340;&#23398;&#20064;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#31895;&#35843;&#20248;&#20316;&#20026;&#19968;&#20010;&#20013;&#38388;&#23398;&#20064;&#38454;&#27573;&#65292;&#36830;&#25509;&#20102;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#12290;&#36890;&#36807;&#22312;&#31895;&#35843;&#20248;&#23398;&#20064;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#65292;&#25105;&#20204;&#26088;&#22312;&#20943;&#23569;&#24494;&#35843;&#30340;&#36127;&#25285;&#65292;&#25552;&#39640;&#19979;&#28216;IR&#20219;&#21153;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#29992;&#20110;&#31895;&#35843;&#20248;&#30340;&#26597;&#35810;-&#25991;&#26723;&#23545;&#39044;&#27979;&#65288;QDPP&#65289;&#65292;&#20854;&#39044;&#27979;&#26597;&#35810;-&#25991;&#26723;&#23545;&#30340;&#36866;&#24403;&#24615;&#12290;&#35780;&#20272;&#23454;&#39564;&#26174;&#31034;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25913;&#21892;&#20102;&#22235;&#20010;&#19987;&#39064;&#25991;&#26723;&#26816;&#32034;&#25968;&#25454;&#38598;&#20013;&#30340;MRR&#21644;/&#25110;nDCG@5&#12290;&#27492;&#22806;&#65292;&#26597;&#35810;&#39044;&#27979;&#20219;&#21153;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#31895;&#35843;&#20248;&#20419;&#36827;&#20102;&#26597;&#35810;&#34920;&#31034;&#21644;&#26597;&#35810;-&#25991;&#26723;&#20851;&#31995;&#30340;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16915v1 Announce Type: cross  Abstract: Fine-tuning in information retrieval systems using pre-trained language models (PLM-based IR) requires learning query representations and query-document relations, in addition to downstream task-specific learning. This study introduces coarse-tuning as an intermediate learning stage that bridges pre-training and fine-tuning. By learning query representations and query-document relations in coarse-tuning, we aim to reduce the load of fine-tuning and improve the learning effect of downstream IR tasks. We propose Query-Document Pair Prediction (QDPP) for coarse-tuning, which predicts the appropriateness of query-document pairs. Evaluation experiments show that the proposed method significantly improves MRR and/or nDCG@5 in four ad-hoc document retrieval datasets. Furthermore, the results of the query prediction task suggested that coarse-tuning facilitated learning of query representation and query-document relations.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;GPT-3&#20998;&#26512;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#22312;&#21512;&#25104;&#25968;&#25454;&#20013;&#30340;&#21387;&#21147;&#22240;&#32032;&#34920;&#31034;&#65292;&#21019;&#24314;&#20986;&#21253;&#21547;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#21387;&#21147;&#22240;&#32032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20026;&#20197;&#21518;&#20351;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#29983;&#25104;&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16909</link><description>&lt;p&gt;
&#26397;&#21521;&#31639;&#27861;&#24544;&#23454;&#24615;&#65306;&#21512;&#25104;&#25968;&#25454;&#19982;&#20154;&#31867;&#29983;&#25104;&#25968;&#25454;&#20013;&#36328;&#20154;&#21475;&#32479;&#35745;&#30340;&#24515;&#29702;&#20581;&#24247;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Towards Algorithmic Fidelity: Mental Health Representation across Demographics in Synthetic vs. Human-generated Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16909
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;GPT-3&#20998;&#26512;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#22312;&#21512;&#25104;&#25968;&#25454;&#20013;&#30340;&#21387;&#21147;&#22240;&#32032;&#34920;&#31034;&#65292;&#21019;&#24314;&#20986;&#21253;&#21547;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#21387;&#21147;&#22240;&#32032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#20026;&#20197;&#21518;&#20351;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#29983;&#25104;&#30740;&#31350;&#25552;&#20379;&#20102;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#26377;&#28508;&#21147;&#24433;&#21709;&#24212;&#29992;&#21644;&#39046;&#22495;&#65292;&#23588;&#20854;&#26159;&#22312;&#25968;&#25454;&#31232;&#32570;&#30340;&#24773;&#20917;&#19979;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;&#36825;&#20123;&#25968;&#25454;&#29992;&#20110;&#35832;&#22914;&#24515;&#29702;&#20581;&#24247;&#36825;&#26679;&#30340;&#25935;&#24863;&#20219;&#21153;&#20043;&#21069;&#65292;&#25105;&#20204;&#38656;&#35201;&#20102;&#35299;&#19981;&#21516;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#22312;&#20854;&#20013;&#30340;&#34920;&#31034;&#26041;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#25506;&#32034;GPT-3&#30340;&#28508;&#21147;&#26469;&#20998;&#26512;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#30340;&#21487;&#33021;&#24615;&#65292;&#20174;&#32780;&#25506;&#35752;&#20854;&#23545;&#19981;&#21516;&#31181;&#26063;&#21644;&#24615;&#21035;&#32452;&#21512;&#25152;&#20135;&#29983;&#30340;&#21508;&#31181;&#21387;&#21147;&#22240;&#32032;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#21442;&#32771;&#65292;&#36825;&#20123;&#30740;&#31350;&#20154;&#21592;&#27491;&#22312;&#30740;&#31350;&#20351;&#29992;LLMs&#36827;&#34892;&#25968;&#25454;&#29983;&#25104;&#12290;&#25105;&#20204;&#21033;&#29992;GPT-3&#24320;&#21457;&#20102;HEADROOM&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#25324;3120&#31687;&#20851;&#20110;&#23548;&#33268;&#25233;&#37057;&#30151;&#21387;&#21147;&#22240;&#32032;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#25511;&#21046;&#31181;&#26063;&#12289;&#24615;&#21035;&#21644;&#26102;&#38388;&#33539;&#22260;&#65288;COVID-19&#20043;&#21069;&#21644;&#20043;&#21518;&#65289;&#12290;&#21033;&#29992;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36827;&#34892;&#35821;&#20041;&#21644;&#35789;&#27719;&#20998;&#26512;&#26469;&#65288;1&#65289;&#30830;&#23450;&#27599;&#20010;&#20154;&#21475;&#32479;&#35745;&#32676;&#20307;&#30340;&#20027;&#35201;&#21387;&#21147;&#22240;&#32032;&#65307;&#65288;2&#65289;&#23558;&#25105;&#20204;&#30340;&#21512;&#25104;&#25968;&#25454;&#19982;&#20154;&#31867;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29983;&#25104;&#26597;&#35810;&#20197;&#21457;&#23637;&#20381;&#36182;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16909v1 Announce Type: new  Abstract: Synthetic data generation has the potential to impact applications and domains with scarce data. However, before such data is used for sensitive tasks such as mental health, we need an understanding of how different demographics are represented in it. In our paper, we analyze the potential of producing synthetic data using GPT-3 by exploring the various stressors it attributes to different race and gender combinations, to provide insight for future researchers looking into using LLMs for data generation. Using GPT-3, we develop HEADROOM, a synthetic dataset of 3,120 posts about depression-triggering stressors, by controlling for race, gender, and time frame (before and after COVID-19). Using this dataset, we conduct semantic and lexical analyses to (1) identify the predominant stressors for each demographic group; and (2) compare our synthetic data to a human-generated dataset. We present the procedures to generate queries to develop dep
&lt;/p&gt;</description></item><item><title>QXG&#26159;&#19968;&#31181;&#32479;&#19968;&#31526;&#21495;&#21644;&#23450;&#24615;&#34920;&#31034;&#65292;&#21033;&#29992;&#26102;&#31354;&#22270;&#21644;&#23450;&#24615;&#32422;&#26463;&#20174;&#21407;&#22987;&#20256;&#24863;&#22120;&#36755;&#20837;&#20013;&#25552;&#21462;&#22330;&#26223;&#35821;&#20041;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#25552;&#20379;&#21487;&#20449;&#30340;&#22330;&#26223;&#29702;&#35299;&#21644;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.16908</link><description>&lt;p&gt;
&#36890;&#36807;&#23450;&#24615;&#22330;&#26223;&#29702;&#35299;&#21644;&#35299;&#37322;&#23454;&#29616;&#21487;&#20449;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Towards Trustworthy Automated Driving through Qualitative Scene Understanding and Explanations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16908
&lt;/p&gt;
&lt;p&gt;
QXG&#26159;&#19968;&#31181;&#32479;&#19968;&#31526;&#21495;&#21644;&#23450;&#24615;&#34920;&#31034;&#65292;&#21033;&#29992;&#26102;&#31354;&#22270;&#21644;&#23450;&#24615;&#32422;&#26463;&#20174;&#21407;&#22987;&#20256;&#24863;&#22120;&#36755;&#20837;&#20013;&#25552;&#21462;&#22330;&#26223;&#35821;&#20041;&#65292;&#20026;&#33258;&#21160;&#39550;&#39542;&#25552;&#20379;&#21487;&#20449;&#30340;&#22330;&#26223;&#29702;&#35299;&#21644;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#39550;&#39542;&#22330;&#26223;&#24182;&#20256;&#36798;&#33258;&#21160;&#39550;&#39542;&#36710;&#36742;&#20915;&#31574;&#26159;&#21487;&#20449;&#33258;&#21160;&#39550;&#39542;&#30340;&#20851;&#38190;&#35201;&#27714;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#23450;&#24615;&#21487;&#35299;&#37322;&#22270;&#65288;QXG&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;&#22478;&#24066;&#31227;&#21160;&#20013;&#22330;&#26223;&#29702;&#35299;&#30340;&#32479;&#19968;&#31526;&#21495;&#21644;&#23450;&#24615;&#34920;&#31034;&#12290;QXG&#21033;&#29992;&#26102;&#31354;&#22270;&#21644;&#23450;&#24615;&#32422;&#26463;&#20174;&#21407;&#22987;&#20256;&#24863;&#22120;&#36755;&#20837;&#65288;&#22914;LiDAR&#21644;&#25668;&#20687;&#22836;&#25968;&#25454;&#65289;&#20013;&#25552;&#21462;&#22330;&#26223;&#35821;&#20041;&#65292;&#25552;&#20379;&#21487;&#35299;&#37322;&#30340;&#22330;&#26223;&#27169;&#22411;&#12290;QXG&#21487;&#20197;&#23454;&#26102;&#22686;&#37327;&#26500;&#24314;&#65292;&#25104;&#20026;&#19968;&#31181;&#36866;&#29992;&#20110;&#36710;&#36733;&#35299;&#37322;&#30340;&#22810;&#31181;&#20256;&#24863;&#22120;&#31867;&#22411;&#30340;&#22810;&#21151;&#33021;&#24037;&#20855;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#23637;&#31034;&#20102;QXG&#22312;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#30340;&#28508;&#21147;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#23558;&#22270;&#24418;&#19982;&#35266;&#23519;&#21040;&#30340;&#34892;&#20026;&#32852;&#31995;&#36215;&#26469;&#26469;&#29702;&#35299;&#20915;&#31574;&#12290;&#36825;&#20123;&#35299;&#37322;&#21487;&#20197;&#29992;&#20110;&#22810;&#31181;&#30446;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16908v1 Announce Type: new  Abstract: Understanding driving scenes and communicating automated vehicle decisions are key requirements for trustworthy automated driving. In this article, we introduce the Qualitative Explainable Graph (QXG), which is a unified symbolic and qualitative representation for scene understanding in urban mobility. The QXG enables interpreting an automated vehicle's environment using sensor data and machine learning models. It utilizes spatio-temporal graphs and qualitative constraints to extract scene semantics from raw sensor inputs, such as LiDAR and camera data, offering an interpretable scene model. A QXG can be incrementally constructed in real-time, making it a versatile tool for in-vehicle explanations across various sensor types. Our research showcases the potential of QXG, particularly in the context of automated driving, where it can rationalize decisions by linking the graph with observed actions. These explanations can serve diverse purp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#37319;&#29992;&#20248;&#21270;&#25216;&#26415;&#33258;&#21160;&#21270;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#20998;&#26512;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#20248;&#21270;&#26041;&#27861;&#25193;&#23637;&#20102;&#20256;&#32479;FMECA&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#31995;&#32479;&#37325;&#35201;&#24615;&#21644;&#24320;&#21457;&#32422;&#26463;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.16904</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#20248;&#21270;&#29992;&#20110;&#23545;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#23433;&#20840;&#20998;&#26512;&#30340;&#20301;&#32622;&#35770;&#25991;
&lt;/p&gt;
&lt;p&gt;
Multi-Agent Optimization for Safety Analysis of Cyber-Physical Systems: Position Paper
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16904
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#37319;&#29992;&#20248;&#21270;&#25216;&#26415;&#33258;&#21160;&#21270;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#30340;&#23433;&#20840;&#20998;&#26512;&#20013;&#30340;&#20915;&#31574;&#36807;&#31243;&#65292;&#36890;&#36807;&#22810;&#26234;&#33021;&#20307;&#20248;&#21270;&#26041;&#27861;&#25193;&#23637;&#20102;&#20256;&#32479;FMECA&#65292;&#25552;&#20379;&#20102;&#20851;&#20110;&#31995;&#32479;&#37325;&#35201;&#24615;&#21644;&#24320;&#21457;&#32422;&#26463;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22833;&#25928;&#27169;&#24335;&#12289;&#24433;&#21709;&#21644;&#37325;&#35201;&#24615;&#20998;&#26512;&#65288;FMECA&#65289;&#26159;&#22823;&#22810;&#25968;&#22269;&#38469;&#26631;&#20934;&#25512;&#33616;&#30340;&#19968;&#31181;&#23433;&#20840;&#20998;&#26512;&#26041;&#27861;&#12290;&#32463;&#20856;&#30340;FMECA&#36890;&#24120;&#20197;&#25163;&#24037;&#22635;&#20889;&#34920;&#26684;&#25110;&#20351;&#29992;&#23433;&#20840;&#20998;&#26512;&#24037;&#20855;&#30340;&#24418;&#24335;&#36827;&#34892;&#12290;&#22312;&#36825;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#35774;&#35745;&#24037;&#31243;&#24072;&#24517;&#39035;&#22312;&#23433;&#20840;&#24615;&#21644;&#20854;&#20182;&#24320;&#21457;&#32422;&#26463;&#20043;&#38388;&#36827;&#34892;&#26435;&#34913;&#12290;&#23545;&#20110;&#20855;&#26377;&#25968;&#21315;&#20010;&#25351;&#23450;&#32422;&#26463;&#26465;&#20214;&#30340;&#22797;&#26434;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#65288;CPS&#65289;&#65292;&#36825;&#21487;&#33021;&#23548;&#33268;&#20005;&#37325;&#38382;&#39064;&#65292;&#26174;&#33879;&#24433;&#21709;&#25972;&#20010;CPS&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#37319;&#29992;&#20248;&#21270;&#25216;&#26415;&#26469;&#33258;&#21160;&#21270;&#22312;CPS&#30340;FMECA&#20043;&#21518;&#36827;&#34892;&#30340;&#20915;&#31574;&#36807;&#31243;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#26234;&#33021;&#20307;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#25193;&#23637;&#20102;&#32463;&#20856;&#30340;FMECA&#65292;&#20197;&#25552;&#20379;&#20851;&#20110;CPS&#30340;&#37325;&#35201;&#24615;&#21644;&#24320;&#21457;&#32422;&#26463;&#30340;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16904v1 Announce Type: new  Abstract: Failure Mode, Effects and Criticality Analysis (FMECA) is one of the safety analysis methods recommended by most of the international standards. The classical FMECA is made in a form of a table filled in either manually or by using safety analysis tools. In both cases, the design engineers have to choose the trade-offs between safety and other development constraints. In the case of complex cyber-physical systems (CPS) with thousands of specified constraints, this may lead to severe problems and significantly impact the overall criticality of CPS. In this paper, we propose to adopt optimization techniques to automate the decision making process conducted after FMECA of CPS. We describe a multi-agent based optimization method which extends classical FMECA for offering optimal solutions in terms of criticality and development constraints of CPS.
&lt;/p&gt;</description></item><item><title>&#21306;&#22359;&#38142;&#25216;&#26415;&#20351;&#24471;&#26234;&#33021;&#21512;&#32422;&#33021;&#22815;&#23433;&#20840;&#22320;&#25968;&#23383;&#21270;&#20304;&#35777;&#20132;&#26131;&#65292;&#20026;&#20102;&#20445;&#35777;&#20854;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#38656;&#35201;&#26397;&#21521;&#23433;&#20840;&#21487;&#20449;&#30340;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.16903</link><description>&lt;p&gt;
&#26397;&#21521;&#23433;&#20840;&#21487;&#20449;&#35774;&#35745;&#30340;&#26234;&#33021;&#21512;&#32422;
&lt;/p&gt;
&lt;p&gt;
Towards Secure and Trusted-by-Design Smart Contracts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16903
&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#25216;&#26415;&#20351;&#24471;&#26234;&#33021;&#21512;&#32422;&#33021;&#22815;&#23433;&#20840;&#22320;&#25968;&#23383;&#21270;&#20304;&#35777;&#20132;&#26131;&#65292;&#20026;&#20102;&#20445;&#35777;&#20854;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#65292;&#38656;&#35201;&#26397;&#21521;&#23433;&#20840;&#21487;&#20449;&#30340;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#24335;&#19981;&#21487;&#21464;&#36134;&#26412;&#25110;&#21306;&#22359;&#38142;&#20801;&#35768;&#23558;&#20304;&#35777;&#20132;&#26131;&#23433;&#20840;&#22320;&#25968;&#23383;&#21270;&#65292;&#26080;&#38656;&#20381;&#36182;&#21487;&#20449;&#31532;&#19977;&#26041;&#12290;&#20304;&#35777;&#20132;&#26131;&#28041;&#21450;&#20219;&#20309;&#24418;&#24335;&#30340;&#29289;&#35777;&#20132;&#25442;&#65292;&#22914;&#36135;&#24065;&#12289;&#20986;&#29983;&#35777;&#26126;&#12289;&#31614;&#35777;&#12289;&#38376;&#31080;&#31561;&#12290;&#21306;&#22359;&#38142;&#25552;&#20379;&#20102;&#20256;&#36755;&#35777;&#25454;&#30340;&#26426;&#21046;&#65292;&#32780;&#26234;&#33021;&#21512;&#32422;&#8212;&#8212;&#20197;&#20998;&#25955;&#21644;&#22797;&#21046;&#30340;&#26041;&#24335;&#22312;&#21306;&#22359;&#38142;&#20869;&#25191;&#34892;&#30340;&#31243;&#24207;&#8212;&#8212;&#20801;&#35768;&#22312;&#21306;&#22359;&#38142;&#20043;&#19978;&#23545;&#20304;&#35777;&#21327;&#35758;&#36827;&#34892;&#32534;&#30721;&#12290;&#30001;&#20110;&#26234;&#33021;&#21512;&#32422;&#25918;&#24323;&#20102;&#21487;&#20449;&#31532;&#19977;&#26041;&#24182;&#22312;&#22810;&#21488;&#21311;&#21517;&#26426;&#22120;&#19978;&#36816;&#34892;&#65292;&#22240;&#27492;&#23427;&#26500;&#25104;&#19968;&#20010;&#39640;&#24230;&#20851;&#38190;&#30340;&#31243;&#24207;&#65292;&#24517;&#39035;&#20855;&#26377;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16903v1 Announce Type: cross  Abstract: Distributed immutable ledgers, or blockchains, allow the secure digitization of evidential transactions without relying on a trusted third-party. Evidential transactions involve the exchange of any form of physical evidence, such as money, birth certificate, visas, tickets, etc. Most of the time, evidential transactions occur in the context of complex procedures, called evidential protocols, among physical agents. The blockchain provides the mechanisms to transfer evidence, while smart contracts - programs executing within the blockchain in a decentralized and replicated fashion - allow encoding evidential protocols on top of a blockchain.   As a smart contract foregoes trusted third-parties and runs on several machines anonymously, it constitutes a highly critical program that has to be secure and trusted-by-design. While most of the current smart contract languages focus on easy programmability, they do not directly address the need 
&lt;/p&gt;</description></item><item><title>&#39046;&#22495;&#19987;&#23478;&#22312;&#33258;&#28982;&#31185;&#23398;&#30740;&#31350;&#20013;&#23545;AI&#31995;&#32479;&#30340;&#20302;&#37319;&#29992;&#38459;&#30861;&#20102;&#20154;&#26426;&#21512;&#20316;&#36827;&#23637;&#65292;&#30740;&#31350;&#21457;&#29616;&#25552;&#20986;&#20102;&#26356;&#22909;&#30340;AI&#37319;&#29992;&#24314;&#35758;&#12290;</title><link>https://arxiv.org/abs/2403.16895</link><description>&lt;p&gt;
&#8220;&#23427;&#23384;&#22312;&#65292;&#20320;&#38656;&#35201;&#23427;&#65292;&#37027;&#20320;&#20026;&#20160;&#20040;&#19981;&#20351;&#29992;&#23427;&#65311;&#8221;&#22312;&#33258;&#28982;&#31185;&#23398;&#30740;&#31350;&#26696;&#20363;&#20013;&#23454;&#29616;AI&#31995;&#32479;&#34987;&#39046;&#22495;&#19987;&#23478;&#26356;&#22909;&#22320;&#37319;&#32435;
&lt;/p&gt;
&lt;p&gt;
"It is there, and you need it, so why do you not use it?" Achieving better adoption of AI systems by domain experts, in the case study of natural science research
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16895
&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#19987;&#23478;&#22312;&#33258;&#28982;&#31185;&#23398;&#30740;&#31350;&#20013;&#23545;AI&#31995;&#32479;&#30340;&#20302;&#37319;&#29992;&#38459;&#30861;&#20102;&#20154;&#26426;&#21512;&#20316;&#36827;&#23637;&#65292;&#30740;&#31350;&#21457;&#29616;&#25552;&#20986;&#20102;&#26356;&#22909;&#30340;AI&#37319;&#29992;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22312;&#21307;&#23398;&#21644;&#33258;&#28982;&#31185;&#23398;&#30740;&#31350;&#31561;&#39046;&#22495;&#21464;&#24471;&#26080;&#22788;&#19981;&#22312;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#33853;&#22320;&#26102;&#65292;&#39046;&#22495;&#19987;&#23478;&#32463;&#24120;&#25298;&#32477;&#20351;&#29992;AI&#31995;&#32479;&#12290;&#20302;&#25509;&#21463;&#24230;&#38459;&#30861;&#20102;&#26377;&#25928;&#30340;&#20154;&#26426;&#21512;&#20316;&#65292;&#21363;&#20351;&#36825;&#23545;&#20110;&#36827;&#27493;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#33258;&#28982;&#31185;&#23398;&#30740;&#31350;&#20013;&#65292;&#31185;&#23398;&#23478;&#23545;&#21551;&#29992;AI&#31995;&#32479;&#30340;&#20351;&#29992;&#25928;&#26524;&#19981;&#20339;&#20250;&#22952;&#30861;&#20182;&#20204;&#20998;&#26512;&#25968;&#25454;&#21644;&#25512;&#21160;&#30740;&#31350;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#20197;&#22303;&#33879;&#25991;&#21270;&#20026;&#22522;&#30784;&#30340;&#30740;&#31350;&#65292;&#23545;&#19968;&#20010;&#38754;&#20020;&#31639;&#27861;&#31995;&#32479;&#20302;&#37319;&#29992;&#29575;&#30340;&#32452;&#32455;&#30340;AI&#20174;&#19994;&#32773;&#21644;&#33258;&#28982;&#31185;&#23398;&#23478;&#36827;&#34892;&#20102;10&#27425;&#28145;&#24230;&#35775;&#35848;&#12290;&#30740;&#31350;&#32467;&#26524;&#34987;&#25972;&#21512;&#25104;&#20026;&#26356;&#22909;&#30340;AI&#37319;&#29992;&#24314;&#35758;&#65306;i) &#22312;&#31995;&#32479;&#20351;&#29992;&#30340;&#21021;&#22987;&#38454;&#27573;&#31215;&#26497;&#25903;&#25345;&#19987;&#23478;&#65292;ii) &#20197;&#29992;&#25143;&#30456;&#20851;&#30340;&#26041;&#24335;&#20256;&#36798;&#31995;&#32479;&#30340;&#33021;&#21147;&#65292;&#20197;&#21450; iii) &#36981;&#24490;&#39044;&#23450;&#20041;&#30340;&#21512;&#20316;&#35268;&#21017;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#25105;&#20204;&#30740;&#31350;&#32467;&#26524;&#30340;&#26356;&#24191;&#27867;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16895v1 Announce Type: cross  Abstract: Artificial Intelligence (AI) is becoming ubiquitous in domains such as medicine and natural science research. However, when AI systems are implemented in practice, domain experts often refuse them. Low acceptance hinders effective human-AI collaboration, even when it is essential for progress. In natural science research, scientists' ineffective use of AI-enabled systems can impede them from analysing their data and advancing their research. We conducted an ethnographically informed study of 10 in-depth interviews with AI practitioners and natural scientists at the organisation facing low adoption of algorithmic systems. Results were consolidated into recommendations for better AI adoption: i) actively supporting experts during the initial stages of system use, ii) communicating the capabilities of a system in a user-relevant way, and iii) following predefined collaboration rules. We discuss the broader implications of our findings and
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837; BorealTC &#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#26032;&#39062;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;-Mamba&#20307;&#31995;&#32467;&#26500;&#22312;&#21271;&#26041;&#26862;&#26519;&#22320;&#24418;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.16877</link><description>&lt;p&gt;
&#24863;&#30693;&#21147;&#23601;&#26159;&#20320;&#25152;&#38656;&#35201;&#30340;&#65306;&#21271;&#26041;&#26862;&#26519;&#30340;&#22320;&#24418;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Proprioception Is All You Need: Terrain Classification for Boreal Forests
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16877
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837; BorealTC &#25968;&#25454;&#38598;&#65292;&#32467;&#21512;&#29616;&#26377;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#26032;&#39062;&#30340;&#29366;&#24577;&#31354;&#38388;&#27169;&#22411;&#65288;SSM&#65289;-Mamba&#20307;&#31995;&#32467;&#26500;&#22312;&#21271;&#26041;&#26862;&#26519;&#22320;&#24418;&#20998;&#31867;&#19978;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#39046;&#22495;&#26426;&#22120;&#20154;&#23398;&#30740;&#31350;&#24378;&#35843;&#20102;&#25269;&#24481;&#19981;&#21516;&#31867;&#22411;&#22320;&#24418;&#30340;&#37325;&#35201;&#24615;&#12290;&#21271;&#26041;&#26862;&#26519;&#29305;&#21035;&#21463;&#21040;&#35768;&#22810;&#38480;&#21046;&#26426;&#21160;&#24615;&#30340;&#22320;&#24418;&#30340;&#24433;&#21709;&#65292;&#36825;&#20123;&#22320;&#24418;&#24212;&#35813;&#22312;&#36234;&#37326;&#33258;&#20027;&#23548;&#33322;&#20013;&#21152;&#20197;&#32771;&#34385;&#12290;&#27492;&#22806;&#65292;&#20316;&#20026;&#22320;&#29699;&#19978;&#26368;&#22823;&#30340;&#38470;&#22320;&#29983;&#29289;&#32676;&#33853;&#20043;&#19968;&#65292;&#21271;&#26041;&#26862;&#26519;&#26159;&#39044;&#35745;&#33258;&#20027;&#36710;&#36742;&#23558;&#26085;&#30410;&#26222;&#21450;&#30340;&#22320;&#21306;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#24341;&#20837;BorealTC&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#22522;&#20110;&#24863;&#30693;&#21147;&#30340;&#22320;&#24418;&#20998;&#31867;&#65288;TC&#65289;&#30340;&#20844;&#24320;&#21487;&#29992;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#35760;&#24405;&#20102;Husky A200&#30340;116&#20998;&#38047;&#30340;&#24815;&#24615;&#27979;&#37327;&#21333;&#20803;&#65288;IMU&#65289;&#12289;&#30005;&#26426;&#30005;&#27969;&#21644;&#36718;&#32974;&#37324;&#31243;&#25968;&#25454;&#65292;&#37325;&#28857;&#20851;&#27880;&#20856;&#22411;&#30340;&#21271;&#26041;&#26862;&#26519;&#22320;&#24418;&#65292;&#29305;&#21035;&#26159;&#38634;&#12289;&#20912;&#21644;&#28132;&#27877;&#22756;&#12290;&#32467;&#21512;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19982;&#21478;&#19968;&#20010;&#26469;&#33258;&#26368;&#26032;&#25216;&#26415;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#22312;TC t
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16877v1 Announce Type: cross  Abstract: Recent works in field robotics highlighted the importance of resiliency against different types of terrains. Boreal forests, in particular, are home to many mobility-impeding terrains that should be considered for off-road autonomous navigation. Also, being one of the largest land biomes on Earth, boreal forests are an area where autonomous vehicles are expected to become increasingly common. In this paper, we address this issue by introducing BorealTC, a publicly available dataset for proprioceptive-based terrain classification (TC). Recorded with a Husky A200, our dataset contains 116 min of Inertial Measurement Unit (IMU), motor current, and wheel odometry data, focusing on typical boreal forest terrains, notably snow, ice, and silty loam. Combining our dataset with another dataset from the state-of-the-art, we evaluate both a Convolutional Neural Network (CNN) and the novel state space model (SSM)-based Mamba architecture on a TC t
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#38543;&#26426;&#25628;&#32034;&#33258;&#21160;&#21457;&#29616;&#26356;&#22909;&#30340;GPU&#26412;&#26426;&#25351;&#20196;&#35843;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;CUDA&#20869;&#26680;&#30340;&#21534;&#21520;&#37327;</title><link>https://arxiv.org/abs/2403.16863</link><description>&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#25351;&#20196;&#25200;&#21160;&#23454;&#29616;GPU&#26412;&#26426;&#35843;&#24230;&#30340;&#33258;&#21160;&#35843;&#20248;
&lt;/p&gt;
&lt;p&gt;
SIP: Autotuning GPU Native Schedules via Stochastic Instruction Perturbation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16863
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#38543;&#26426;&#25628;&#32034;&#33258;&#21160;&#21457;&#29616;&#26356;&#22909;&#30340;GPU&#26412;&#26426;&#25351;&#20196;&#35843;&#24230;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;CUDA&#20869;&#26680;&#30340;&#21534;&#21520;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLMs)&#33258;&#20986;&#29616;&#20197;&#26469;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#24037;&#20316;&#36127;&#36733;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20854;&#25317;&#26377;&#25968;&#21313;&#20159;&#21442;&#25968;&#24182;&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#23427;&#20204;&#30340;&#35745;&#31639;&#25104;&#26412;&#20063;&#24456;&#39640;&#12290;&#22240;&#27492;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#24320;&#21457;&#20102;&#19987;&#38376;&#29992;&#20110;LLM&#35757;&#32451;&#21644;&#25512;&#26029;&#30340;CUDA&#20869;&#26680;&#65292;&#32780;&#19981;&#26159;&#20381;&#36182;&#20110;&#32534;&#35793;&#22120;&#29983;&#25104;&#30340;&#20869;&#26680;&#65292;&#20197;&#20415;&#23613;&#21487;&#33021;&#20805;&#20998;&#21033;&#29992;&#30828;&#20214;&#36164;&#28304;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#36890;&#36807;GPU&#26412;&#26426;&#25351;&#20196;&#20248;&#21270;&#36827;&#19968;&#27493;&#25512;&#21160;CUDA&#20869;&#26680;&#36798;&#21040;&#26497;&#33268;&#24615;&#33021;&#30340;&#21487;&#33021;&#24615;&#12290;&#19982;&#20197;&#24448;&#30340;&#24037;&#20316;&#30456;&#21453;&#65292;&#25105;&#20204;&#37319;&#29992;&#33258;&#21160;&#20248;&#21270;&#26041;&#27861;&#65292;&#23450;&#20041;&#20102;&#21487;&#33021;&#30340;GPU&#26412;&#26426;&#25351;&#20196;&#35843;&#24230;&#25628;&#32034;&#31354;&#38388;&#65292;&#28982;&#21518;&#37319;&#29992;&#38543;&#26426;&#25628;&#32034;&#36827;&#34892;&#20248;&#21270;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#26356;&#22909;&#30340;GPU&#26412;&#26426;&#25351;&#20196;&#35843;&#24230;&#65292;SIP&#33021;&#22815;&#36827;&#19968;&#27493;&#25552;&#39640;CUDA&#20869;&#26680;&#30340;&#21534;&#21520;&#37327;&#65292;&#24182;&#23545;&#20248;&#21270;&#21518;&#30340;&#35843;&#24230;&#36827;&#34892;&#20102;1000&#19975;&#27425;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16863v1 Announce Type: cross  Abstract: Large language models (LLMs) have become a significant workload since their appearance. However, they are also computationally expensive as they have billions of parameters and are trained with massive amounts of data. Thus, recent works have developed dedicated CUDA kernels for LLM training and inference instead of relying on compilergenerated ones, so that hardware resources are as fully utilized as possible. In this work, we explore the possibility of GPU native instruction optimization to further push the CUDA kernels to extreme performance. Contrary to prior works, we adopt an automatic optimization approach by defining a search space of possible GPU native instruction schedules, and then we apply stochastic search to perform optimization. Experiments show that SIP can further improve CUDA kernel throughput by automatically discovering better GPU native instruction schedules and the optimized schedules are tested by 10 million tes
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26089;&#26399;&#37319;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26381;&#21153;&#26694;&#26550;XAIport&#65292;&#20855;&#26377;&#35299;&#37322;&#36136;&#37327;&#12289;&#26550;&#26500;&#20860;&#23481;&#24615;&#21644;&#21487;&#37197;&#32622;&#25805;&#20316;&#19977;&#20010;&#20851;&#38190;&#23646;&#24615;&#65292;&#20026;AI&#27169;&#22411;&#30340;&#24320;&#21457;&#25552;&#20379;&#21487;&#20449;&#30340;&#26089;&#26399;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.16858</link><description>&lt;p&gt;
XAIport&#65306;&#29992;&#20110;&#22312;AI&#27169;&#22411;&#24320;&#21457;&#20013;&#26089;&#26399;&#37319;&#29992;XAI&#30340;&#26381;&#21153;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
XAIport: A Service Framework for the Early Adoption of XAI in AI Model Development
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16858
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26089;&#26399;&#37319;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26381;&#21153;&#26694;&#26550;XAIport&#65292;&#20855;&#26377;&#35299;&#37322;&#36136;&#37327;&#12289;&#26550;&#26500;&#20860;&#23481;&#24615;&#21644;&#21487;&#37197;&#32622;&#25805;&#20316;&#19977;&#20010;&#20851;&#38190;&#23646;&#24615;&#65292;&#20026;AI&#27169;&#22411;&#30340;&#24320;&#21457;&#25552;&#20379;&#21487;&#20449;&#30340;&#26089;&#26399;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26089;&#26399;&#37319;&#29992;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#65292;&#37325;&#28857;&#20851;&#27880;&#19977;&#20010;&#23646;&#24615;&#65306;&#35299;&#37322;&#30340;&#36136;&#37327;&#65292;&#35299;&#37322;&#25688;&#35201;&#24212;&#35813;&#22312;&#22810;&#31181;XAI&#26041;&#27861;&#20013;&#20445;&#25345;&#19968;&#33268;&#65307;&#26550;&#26500;&#20860;&#23481;&#24615;&#65292;&#20026;&#20102;&#26377;&#25928;&#22320;&#23558;XAI&#38598;&#25104;&#21040;XAI&#20013;&#65292;XAI&#26041;&#27861;&#21644;&#35201;&#35299;&#37322;&#30340;&#27169;&#22411;&#30340;&#26550;&#26500;&#39118;&#26684;&#24517;&#39035;&#19982;&#26694;&#26550;&#20860;&#23481;&#65307;&#21487;&#37197;&#32622;&#25805;&#20316;&#65292;XAI&#35299;&#37322;&#26159;&#21487;&#25805;&#20316;&#30340;&#65292;&#31867;&#20284;&#20110;&#26426;&#22120;&#23398;&#20064;&#25805;&#20316;&#12290;&#22240;&#27492;&#65292;AI&#27169;&#22411;&#30340;&#35299;&#37322;&#24212;&#35813;&#26159;&#21487;&#37325;&#29616;&#30340;&#21644;&#26131;&#22788;&#29702;&#30340;&#65292;&#25165;&#33021;&#33719;&#24471;&#20449;&#20219;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;XAIport&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;XAI&#24494;&#26381;&#21153;&#23553;&#35013;&#25104;&#24320;&#25918;API&#30340;&#26694;&#26550;&#65292;&#20026;&#23398;&#20064;&#27169;&#22411;&#36136;&#37327;&#20445;&#38556;&#25552;&#20379;&#26089;&#26399;&#35299;&#37322;&#35266;&#23519;&#12290;XAIport&#20351;&#24471;&#21487;&#37197;&#32622;&#30340;XAI&#25805;&#20316;&#19982;&#26426;&#22120;&#23398;&#20064;&#24320;&#21457;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#37327;&#21270;&#20102;&#22312;Microsoft Azure Cognitive Services&#19978;&#23558;XAI&#19982;&#19977;&#31181;&#20113;&#35745;&#31639;&#26426;&#35270;&#35273;&#26381;&#21153;&#25972;&#21512;&#30340;&#36816;&#33829;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16858v1 Announce Type: new  Abstract: In this study, we propose the early adoption of Explainable AI (XAI) with a focus on three properties: Quality of explanation, the explanation summaries should be consistent across multiple XAI methods; Architectural Compatibility, for effective integration in XAI, the architecture styles of both the XAI methods and the models to be explained must be compatible with the framework; Configurable operations, XAI explanations are operable, akin to machine learning operations. Thus, an explanation for AI models should be reproducible and tractable to be trustworthy. We present XAIport, a framework of XAI microservices encapsulated into Open APIs to deliver early explanations as observation for learning model quality assurance. XAIport enables configurable XAI operations along with machine learning development. We quantify the operational costs of incorporating XAI with three cloud computer vision services on Microsoft Azure Cognitive Services
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLMs&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#25903;&#25345;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#30340;&#23398;&#20064;&#21644;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#38544;&#34255;&#21327;&#20316;&#32454;&#33410;&#65292;&#23637;&#29616;&#20986;&#27604;&#29616;&#26377;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16854</link><description>&lt;p&gt;
&#19968;&#20010;&#19987;&#23478;&#20215;&#20540;&#19968;&#20010;&#20195;&#24065;&#65306;&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;
&lt;/p&gt;
&lt;p&gt;
An Expert is Worth One Token: Synergizing Multiple Expert LLMs as Generalist via Expert Token Routing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16854
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#23558;&#22810;&#20010;&#19987;&#23478;LLM&#21327;&#21516;&#20316;&#20026;&#36890;&#29992;&#22411;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLMs&#30340;&#26080;&#32541;&#38598;&#25104;&#65292;&#25903;&#25345;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#30340;&#23398;&#20064;&#21644;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#65292;&#21516;&#26102;&#26356;&#22909;&#22320;&#38544;&#34255;&#21327;&#20316;&#32454;&#33410;&#65292;&#23637;&#29616;&#20986;&#27604;&#29616;&#26377;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#26356;&#22909;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#65288;Expert-Token-Routing&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#32479;&#19968;&#30340;&#36890;&#29992;&#22411;&#26694;&#26550;&#65292;&#21487;&#20197;&#23454;&#29616;&#22810;&#20010;&#19987;&#23478;LLM&#30340;&#26080;&#32541;&#38598;&#25104;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#23558;&#19987;&#23478;LLMs&#34920;&#31034;&#20026;&#20803;LLM&#35789;&#27719;&#20013;&#30340;&#29305;&#27530;&#19987;&#23478;&#20195;&#24065;&#12290;&#20803;LLM&#21487;&#20197;&#36335;&#30001;&#21040;&#19987;&#23478;LLM&#65292;&#23601;&#20687;&#29983;&#25104;&#26032;&#20195;&#24065;&#19968;&#26679;&#12290;&#19987;&#23478;&#20195;&#24065;&#36335;&#30001;&#19981;&#20165;&#21487;&#20197;&#20174;&#29616;&#26377;&#30340;&#25351;&#23548;&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#19987;&#23478;LLMs&#30340;&#38544;&#24335;&#19987;&#19994;&#30693;&#35782;&#65292;&#36824;&#21487;&#20197;&#20197;&#21363;&#25554;&#21363;&#29992;&#30340;&#26041;&#24335;&#21160;&#24577;&#25193;&#23637;&#26032;&#30340;&#19987;&#23478;LLMs&#12290;&#23427;&#36824;&#21487;&#20197;&#38544;&#34255;&#29992;&#25143;&#35270;&#35282;&#20013;&#30340;&#35814;&#32454;&#21327;&#20316;&#36807;&#31243;&#65292;&#20419;&#36827;&#20132;&#20114;&#23601;&#20687;&#26159;&#19968;&#20010;&#21333;&#19968;&#30340;LLM&#19968;&#26679;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#28085;&#30422;&#20845;&#20010;&#19981;&#21516;&#19987;&#23478;&#39046;&#22495;&#30340;&#22522;&#20934;&#27979;&#35797;&#20013;&#32988;&#36807;&#20102;&#21508;&#31181;&#29616;&#26377;&#30340;&#22810;LLM&#21327;&#20316;&#33539;&#24335;&#65292;&#23637;&#29616;&#20102;&#36890;&#36807;&#21327;&#21516;&#22810;&#20010;&#19987;&#23478;LLM&#26469;&#26500;&#24314;&#36890;&#29992;&#22411;LLM&#31995;&#32479;&#30340;&#25928;&#26524;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16854v1 Announce Type: cross  Abstract: We present Expert-Token-Routing, a unified generalist framework that facilitates seamless integration of multiple expert LLMs. Our framework represents expert LLMs as special expert tokens within the vocabulary of a meta LLM. The meta LLM can route to an expert LLM like generating new tokens. Expert-Token-Routing not only supports learning the implicit expertise of expert LLMs from existing instruction dataset but also allows for dynamic extension of new expert LLMs in a plug-and-play manner. It also conceals the detailed collaboration process from the user's perspective, facilitating interaction as though it were a singular LLM. Our framework outperforms various existing multi-LLM collaboration paradigms across benchmarks that incorporate six diverse expert domains, demonstrating effectiveness and robustness in building generalist LLM system via synergizing multiple expert LLMs.
&lt;/p&gt;</description></item><item><title>&#20808;&#20363;&#26159;&#20419;&#36827;&#27861;&#24459;NLP&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#30340;&#20808;&#20363;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#30340;&#33021;&#21147;&#19981;&#38169;&#65292;&#20294;&#20854;&#20351;&#29992;&#20808;&#20363;&#30340;&#26041;&#24335;&#19982;&#20154;&#31867;&#27861;&#23448;&#19981;&#21516;&#12290;</title><link>https://arxiv.org/abs/2403.16852</link><description>&lt;p&gt;
&#22312;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20013;&#36808;&#21521;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Towards Explainability in Legal Outcome Prediction Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16852
&lt;/p&gt;
&lt;p&gt;
&#20808;&#20363;&#26159;&#20419;&#36827;&#27861;&#24459;NLP&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#30340;&#20808;&#20363;&#65292;&#24182;&#21457;&#29616;&#27169;&#22411;&#39044;&#27979;&#32467;&#26524;&#30340;&#33021;&#21147;&#19981;&#38169;&#65292;&#20294;&#20854;&#20351;&#29992;&#20808;&#20363;&#30340;&#26041;&#24335;&#19982;&#20154;&#31867;&#27861;&#23448;&#19981;&#21516;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411; - &#27861;&#24459;NLP&#30340;&#22522;&#26412;&#32452;&#25104;&#37096;&#20998; - &#19981;&#33021;&#35299;&#37322;&#20854;&#25512;&#29702;&#36807;&#31243;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#24212;&#29992;&#36825;&#20123;&#27169;&#22411;&#65292;&#20154;&#31867;&#27861;&#24459;&#20027;&#20307;&#38656;&#35201;&#33021;&#22815;&#29702;&#35299;&#23427;&#20204;&#30340;&#20915;&#31574;&#12290;&#22312;&#26222;&#36890;&#27861;&#26696;&#20363;&#20013;&#65292;&#27861;&#24459;&#20174;&#19994;&#32773;&#36890;&#36807;&#21442;&#32771;&#34987;&#31216;&#20026;&#20808;&#20363;&#30340;&#36807;&#21435;&#26696;&#20363;&#27861;&#24459;&#25512;&#29702;&#21040;&#26696;&#20214;&#32467;&#26524;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#20808;&#20363;&#22240;&#27492;&#25104;&#20026;&#20419;&#36827;&#27861;&#24459;NLP&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#30340;&#19968;&#31181;&#33258;&#28982;&#26041;&#24335;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#35782;&#21035;&#27861;&#24459;&#32467;&#26524;&#39044;&#27979;&#27169;&#22411;&#20351;&#29992;&#30340;&#20808;&#20363;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#21046;&#23450;&#27861;&#24459;&#20808;&#20363;&#30340;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#27604;&#36739;&#20154;&#31867;&#27861;&#23448;&#21644;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#20182;&#20204;&#20381;&#36182;&#30340;&#19981;&#21516;&#31867;&#22411;&#20808;&#20363;&#26041;&#38754;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#34429;&#28982;&#27169;&#22411;&#23398;&#20250;&#20102;&#21512;&#29702;&#22320;&#39044;&#27979;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20351;&#29992;&#30340;&#20808;&#20363;&#26041;&#24335;&#19981;&#21516;&#20110;&#20154;&#31867;&#27861;&#23448;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16852v1 Announce Type: cross  Abstract: Current legal outcome prediction models - a staple of legal NLP - do not explain their reasoning. However, to employ these models in the real world, human legal actors need to be able to understand their decisions. In the case of common law, legal practitioners reason towards the outcome of a case by referring to past case law, known as precedent. We contend that precedent is, therefore, a natural way of facilitating explainability for legal NLP models. In this paper, we contribute a novel method for identifying the precedent employed by legal outcome prediction models. Furthermore, by developing a taxonomy of legal precedent, we are able to compare human judges and our models with respect to the different types of precedent they rely on. We find that while the models learn to predict outcomes reasonably well, their use of precedent is unlike that of human judges.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#26159;&#21542;&#33021;&#22815;&#22522;&#20110;Twitter&#25552;&#21450;&#26469;&#39044;&#27979;&#25991;&#31456;&#30340;&#25764;&#22238;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#39044;&#27979;&#26410;&#26469;&#34987;&#25764;&#22238;&#30340;&#26377;&#38382;&#39064;&#25991;&#31456;&#26041;&#38754;&#26159;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.16851</link><description>&lt;p&gt;
ChatGPT&#26159;&#21542;&#33021;&#22815;&#22522;&#20110;Twitter&#25552;&#21450;&#26469;&#39044;&#27979;&#25991;&#31456;&#30340;&#25764;&#22238;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can ChatGPT predict article retraction based on Twitter mentions?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16851
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#26159;&#21542;&#33021;&#22815;&#22522;&#20110;Twitter&#25552;&#21450;&#26469;&#39044;&#27979;&#25991;&#31456;&#30340;&#25764;&#22238;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#39044;&#27979;&#26410;&#26469;&#34987;&#25764;&#22238;&#30340;&#26377;&#38382;&#39064;&#25991;&#31456;&#26041;&#38754;&#26159;&#20855;&#26377;&#19968;&#23450;&#28508;&#21147;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#27979;&#26377;&#38382;&#39064;&#30340;&#30740;&#31350;&#25991;&#31456;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#65292;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#26681;&#25454;&#34987;&#25764;&#22238;&#25991;&#31456;&#22312;Twitter&#19978;&#30340;&#25552;&#21450;&#26159;&#21542;&#33021;&#22815;&#22312;&#25991;&#31456;&#34987;&#25764;&#22238;&#21069;&#21457;&#20986;&#20449;&#21495;&#65292;&#20174;&#32780;&#22312;&#39044;&#27979;&#26410;&#26469;&#34987;&#25764;&#22238;&#30340;&#26377;&#38382;&#39064;&#25991;&#31456;&#26041;&#38754;&#21457;&#25381;&#20316;&#29992;&#12290;&#20998;&#26512;&#20102;&#21253;&#25324;3,505&#31687;&#24050;&#25764;&#22238;&#25991;&#31456;&#21450;&#20854;&#30456;&#20851;Twitter&#25552;&#21450;&#22312;&#20869;&#30340;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#20351;&#29992;&#31895;&#31961;&#31934;&#30830;&#21305;&#37197;&#26041;&#27861;&#33719;&#21462;&#30340;&#20855;&#26377;&#31867;&#20284;&#29305;&#24449;&#30340;3,505&#31687;&#26410;&#25764;&#22238;&#25991;&#31456;&#12290;&#36890;&#36807;&#22235;&#31181;&#39044;&#27979;&#26041;&#27861;&#35780;&#20272;&#20102;Twitter&#25552;&#21450;&#22312;&#39044;&#27979;&#25991;&#31456;&#25764;&#22238;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#21253;&#25324;&#25163;&#21160;&#26631;&#27880;&#12289;&#20851;&#38190;&#35789;&#35782;&#21035;&#12289;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21644;ChatGPT&#12290;&#25163;&#21160;&#26631;&#27880;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#30340;&#30830;&#26377;&#34987;&#25764;&#22238;&#30340;&#25991;&#31456;&#65292;&#20854;Twitter&#25552;&#21450;&#21253;&#21547;&#22312;&#25764;&#22238;&#21069;&#21457;&#20986;&#20449;&#21495;&#30340;&#21487;&#35782;&#21035;&#35777;&#25454;&#65292;&#23613;&#31649;&#23427;&#20204;&#21482;&#21344;&#25152;&#26377;&#34987;&#25764;&#22238;&#25991;&#31456;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16851v1 Announce Type: cross  Abstract: Detecting problematic research articles timely is a vital task. This study explores whether Twitter mentions of retracted articles can signal potential problems with the articles prior to retraction, thereby playing a role in predicting future retraction of problematic articles. A dataset comprising 3,505 retracted articles and their associated Twitter mentions is analyzed, alongside 3,505 non-retracted articles with similar characteristics obtained using the Coarsened Exact Matching method. The effectiveness of Twitter mentions in predicting article retraction is evaluated by four prediction methods, including manual labelling, keyword identification, machine learning models, and ChatGPT. Manual labelling results indicate that there are indeed retracted articles with their Twitter mentions containing recognizable evidence signaling problems before retraction, although they represent only a limited share of all retracted articles with 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#38024;&#23545;&#21160;&#24577;&#22270;&#30340;&#26032;&#39062;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65306;GreeDy&#21644;CoDy&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoDy&#22312;&#23547;&#25214;&#37325;&#35201;&#21453;&#20107;&#23454;&#36755;&#20837;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;59%&#12290;</title><link>https://arxiv.org/abs/2403.16846</link><description>&lt;p&gt;
GreeDy&#21644;CoDy&#65306;&#21160;&#24577;&#22270;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;
&lt;/p&gt;
&lt;p&gt;
GreeDy and CoDy: Counterfactual Explainers for Dynamic Graphs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16846
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#38024;&#23545;&#21160;&#24577;&#22270;&#30340;&#26032;&#39062;&#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65306;GreeDy&#21644;CoDy&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;CoDy&#22312;&#23547;&#25214;&#37325;&#35201;&#21453;&#20107;&#23454;&#36755;&#20837;&#26041;&#38754;&#34920;&#29616;&#20248;&#24322;&#65292;&#25104;&#21151;&#29575;&#39640;&#36798;59%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;TGNNs&#65289;&#23545;&#20110;&#24314;&#27169;&#20855;&#26377;&#26102;&#38388;&#21464;&#21270;&#20132;&#20114;&#30340;&#21160;&#24577;&#22270;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#30001;&#20110;&#20854;&#22797;&#26434;&#30340;&#27169;&#22411;&#32467;&#26500;&#65292;&#22312;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#21453;&#20107;&#23454;&#35299;&#37322;&#23545;&#20110;&#29702;&#35299;&#27169;&#22411;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#23427;&#30740;&#31350;&#36755;&#20837;&#22270;&#30340;&#21464;&#21270;&#22914;&#20309;&#24433;&#21709;&#32467;&#26524;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#20004;&#31181;&#26032;&#39062;&#30340; TGNNs &#21453;&#20107;&#23454;&#35299;&#37322;&#26041;&#27861;&#65306;GreeDy&#65288;&#21160;&#24577;&#22270;&#30340;&#36138;&#24515;&#35299;&#37322;&#22120;&#65289;&#21644; CoDy&#65288;&#21160;&#24577;&#22270;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;&#22120;&#65289;&#12290;&#23427;&#20204;&#23558;&#35299;&#37322;&#35270;&#20026;&#19968;&#20010;&#25628;&#32034;&#38382;&#39064;&#65292;&#23547;&#25214;&#25913;&#21464;&#27169;&#22411;&#39044;&#27979;&#30340;&#36755;&#20837;&#22270;&#20462;&#25913;&#12290;GreeDy &#20351;&#29992;&#31616;&#21333;&#30340;&#36138;&#24515;&#26041;&#27861;&#65292;&#32780; CoDy &#20351;&#29992;&#22797;&#26434;&#30340;&#33945;&#29305;&#21345;&#27931;&#26641;&#25628;&#32034;&#31639;&#27861;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#20004;&#31181;&#26041;&#27861;&#37117;&#33021;&#26377;&#25928;&#29983;&#25104;&#28165;&#26224;&#30340;&#35299;&#37322;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;CoDy &#30340;&#24615;&#33021;&#20248;&#20110; GreeDy &#21644;&#29616;&#26377;&#30340;&#20107;&#23454;&#26041;&#27861;&#65292;&#23547;&#25214;&#21040;&#37325;&#35201;&#30340;&#21453;&#20107;&#23454;&#36755;&#20837;&#30340;&#25104;&#21151;&#29575;&#25552;&#39640;&#20102;&#39640;&#36798; 59\%&#12290;&#36825;&#31361;&#20986;&#20102; CoDy &#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16846v1 Announce Type: cross  Abstract: Temporal Graph Neural Networks (TGNNs), crucial for modeling dynamic graphs with time-varying interactions, face a significant challenge in explainability due to their complex model structure. Counterfactual explanations, crucial for understanding model decisions, examine how input graph changes affect outcomes. This paper introduces two novel counterfactual explanation methods for TGNNs: GreeDy (Greedy Explainer for Dynamic Graphs) and CoDy (Counterfactual Explainer for Dynamic Graphs). They treat explanations as a search problem, seeking input graph alterations that alter model predictions. GreeDy uses a simple, greedy approach, while CoDy employs a sophisticated Monte Carlo Tree Search algorithm. Experiments show both methods effectively generate clear explanations. Notably, CoDy outperforms GreeDy and existing factual methods, with up to 59\% higher success rate in finding significant counterfactual inputs. This highlights CoDy's p
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#20013;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#65292;&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#20132;&#20114;&#34892;&#20026;&#21644;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.16843</link><description>&lt;p&gt;
LLM&#20195;&#29702;&#26159;&#21542;&#20250;&#24863;&#21040;&#21518;&#24724;&#65311;&#22312;&#32447;&#23398;&#20064;&#21644;&#28216;&#25103;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Do LLM Agents Have Regret? A Case Study in Online Learning and Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16843
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#20013;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#65292;&#35780;&#20272;LLM&#20195;&#29702;&#30340;&#20132;&#20114;&#34892;&#20026;&#21644;&#24615;&#33021;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#22810;&#20195;&#29702;&#29615;&#22659;&#20013;&#30340;&#28508;&#21147;&#21644;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#36234;&#26469;&#36234;&#22810;&#22320;&#34987;&#29992;&#20110;(&#20132;&#20114;&#24335;)&#20915;&#31574;&#21046;&#23450;&#65292;&#36890;&#36807;&#24320;&#21457;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#12290;&#23613;&#31649;&#23427;&#20204;&#21462;&#24471;&#20102;&#19981;&#26029;&#30340;&#25104;&#21151;&#65292;&#20294;LLM&#20195;&#29702;&#22312;&#20915;&#31574;&#21046;&#23450;&#20013;&#30340;&#34920;&#29616;&#23578;&#26410;&#36890;&#36807;&#23450;&#37327;&#25351;&#26631;&#36827;&#34892;&#20805;&#20998;&#35843;&#26597;&#65292;&#29305;&#21035;&#26159;&#22312;&#23427;&#20204;&#30456;&#20114;&#20316;&#29992;&#26102;&#30340;&#22810;&#20195;&#29702;&#35774;&#32622;&#20013;&#65292;&#36825;&#26159;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#20856;&#22411;&#22330;&#26223;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;LLM&#20195;&#29702;&#22312;&#36825;&#20123;&#20132;&#20114;&#29615;&#22659;&#20013;&#30340;&#38480;&#21046;&#65292;&#25105;&#20204;&#24314;&#35758;&#30740;&#31350;&#23427;&#20204;&#22312;&#22312;&#32447;&#23398;&#20064;&#21644;&#21338;&#24328;&#35770;&#30340;&#22522;&#20934;&#20915;&#31574;&#35774;&#32622;&#20013;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#24182;&#36890;&#36807;\emph{&#21518;&#24724;}&#24615;&#33021;&#25351;&#26631;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#32463;&#20856;(&#38750;&#24179;&#31283;)&#22312;&#32447;&#23398;&#20064;&#38382;&#39064;&#20013;&#32463;&#39564;&#24615;&#22320;&#30740;&#31350;LLMs&#30340;&#26080;&#21518;&#24724;&#34892;&#20026;&#65292;&#20197;&#21450;&#24403;LLM&#20195;&#29702;&#36890;&#36807;&#36827;&#34892;&#37325;&#22797;&#28216;&#25103;&#36827;&#34892;&#20132;&#20114;&#26102;&#22343;&#34913;&#30340;&#20986;&#29616;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#26080;&#21518;&#24724;&#34892;&#20026;&#25552;&#20379;&#19968;&#20123;&#29702;&#35770;&#27934;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16843v1 Announce Type: cross  Abstract: Large language models (LLMs) have been increasingly employed for (interactive) decision-making, via the development of LLM-based autonomous agents. Despite their emerging successes, the performance of LLM agents in decision-making has not been fully investigated through quantitative metrics, especially in the multi-agent setting when they interact with each other, a typical scenario in real-world LLM-agent applications. To better understand the limits of LLM agents in these interactive environments, we propose to study their interactions in benchmark decision-making settings in online learning and game theory, through the performance metric of \emph{regret}. We first empirically study the {no-regret} behaviors of LLMs in canonical (non-stationary) online learning problems, as well as the emergence of equilibria when LLM agents interact through playing repeated games. We then provide some theoretical insights into the no-regret behavior
&lt;/p&gt;</description></item><item><title>UrbanVLP&#26159;&#19968;&#31181;&#22810;&#31890;&#24230;&#20449;&#24687;&#38598;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#20811;&#26381;&#30446;&#21069;&#22478;&#24066;&#25351;&#26631;&#39044;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#31934;&#24230;</title><link>https://arxiv.org/abs/2403.16831</link><description>&lt;p&gt;
UrbanVLP&#65306;&#29992;&#20110;&#22478;&#24066;&#25351;&#26631;&#39044;&#27979;&#30340;&#22810;&#31890;&#24230;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#22522;&#30784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
UrbanVLP: A Multi-Granularity Vision-Language Pre-Trained Foundation Model for Urban Indicator Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16831
&lt;/p&gt;
&lt;p&gt;
UrbanVLP&#26159;&#19968;&#31181;&#22810;&#31890;&#24230;&#20449;&#24687;&#38598;&#25104;&#30340;&#35270;&#35273;&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#26088;&#22312;&#20811;&#26381;&#30446;&#21069;&#22478;&#24066;&#25351;&#26631;&#39044;&#27979;&#20013;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#65292;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#21644;&#31934;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#25351;&#26631;&#39044;&#27979;&#26088;&#22312;&#21033;&#29992;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#25512;&#26029;&#19981;&#21516;&#22478;&#24066;&#26223;&#35266;&#20013;&#30340;&#31038;&#20250;&#32463;&#27982;&#25351;&#26631;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#27969;&#34892;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#20381;&#36182;&#21355;&#26143;&#22270;&#20687;&#30340;&#27169;&#22411;&#65292;&#38754;&#20020;&#30528;&#21452;&#37325;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#20165;&#38598;&#20013;&#22312;&#21355;&#26143;&#25968;&#25454;&#20013;&#30340;&#23439;&#35266;&#32423;&#21035;&#27169;&#24335;&#21487;&#33021;&#24341;&#20837;&#20559;&#35265;&#65292;&#22312;&#24494;&#35266;&#32423;&#21035;&#32570;&#20047;&#32454;&#33268;&#30340;&#32454;&#33410;&#65292;&#20363;&#22914;&#26576;&#22320;&#30340;&#24314;&#31569;&#32454;&#33410;&#12290;&#20854;&#27425;&#65292;&#39044;&#35757;&#32451;&#27169;&#22411;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#25552;&#20379;&#22478;&#24066;&#35268;&#21010;&#36879;&#26126;&#35777;&#25454;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#38024;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Vision-Language Pre-Trained Model&#65288;UrbanVLP&#65289;&#12290;&#25105;&#20204;&#30340;UrbanVLP&#26080;&#32541;&#25972;&#21512;&#26469;&#33258;&#23439;&#35266;&#65288;&#21355;&#26143;&#65289;&#21644;&#24494;&#35266;&#65288;&#34903;&#26223;&#65289;&#32423;&#21035;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#65292;&#20811;&#26381;&#20102;&#20808;&#21069;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#24341;&#20837;&#20102;&#33258;&#21160;&#29983;&#25104;&#25991;&#26412;&#21644;&#26657;&#20934;&#65292;&#25552;&#39640;&#20102;&#22312;&#19979;&#28216;&#24212;&#29992;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16831v1 Announce Type: cross  Abstract: Urban indicator prediction aims to infer socio-economic metrics in diverse urban landscapes using data-driven methods. However, prevalent pre-trained models, particularly those reliant on satellite imagery, face dual challenges. Firstly, concentrating solely on macro-level patterns from satellite data may introduce bias, lacking nuanced details at micro levels, such as architectural details at a place. Secondly, the lack of interpretability in pre-trained models limits their utility in providing transparent evidence for urban planning. In response to these issues, we devise a novel Vision-Language Pre-Trained Model (UrbanVLP) in this paper. Our UrbanVLP seamlessly integrates multi-granularity information from both macro (satellite) and micro (street-view) levels, overcoming the limitations of prior pre-trained models. Moreover, it introduces automatic text generation and calibration, elevating interpretability in downstream application
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20351;&#29992;&#26377;&#38480;&#26679;&#26412;&#24674;&#22797;&#20986;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#26368;&#32456;&#24471;&#21040;&#30340;&#26368;&#20248;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#38750;&#24120;&#25509;&#36817;&#12290;</title><link>https://arxiv.org/abs/2403.16829</link><description>&lt;p&gt;
&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#29109;&#27491;&#21017;&#21270;&#36870;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#30340;&#25910;&#25947;&#24615;
&lt;/p&gt;
&lt;p&gt;
Convergence of a model-free entropy-regularized inverse reinforcement learning algorithm
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16829
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#20351;&#29992;&#26377;&#38480;&#26679;&#26412;&#24674;&#22797;&#20986;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#65292;&#24182;&#19988;&#26368;&#32456;&#24471;&#21040;&#30340;&#26368;&#20248;&#31574;&#30053;&#19982;&#19987;&#23478;&#31574;&#30053;&#38750;&#24120;&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32473;&#23450;&#19968;&#32452;&#19987;&#23478;&#28436;&#31034;&#25968;&#25454;&#38598;&#30340;&#24773;&#20917;&#19979;&#65292;&#36870;&#24378;&#21270;&#23398;&#20064;&#26088;&#22312;&#24674;&#22797;&#19968;&#20010;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#27169;&#22411;&#30340;&#31639;&#27861;&#26469;&#35299;&#20915;&#29109;&#27491;&#21017;&#21270;&#36870;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#38543;&#26426;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#22870;&#21169;&#65292;&#37319;&#29992;&#38543;&#26426;&#36719;&#31574;&#30053;&#36845;&#20195;&#26356;&#26032;&#31574;&#30053;&#12290;&#20551;&#35774;&#21487;&#20197;&#35775;&#38382;&#19968;&#20010;&#29983;&#25104;&#27169;&#22411;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#31639;&#27861;&#33021;&#22815;&#20445;&#35777;&#20351;&#29992;$\mathcal{O}(1/\varepsilon^{2})$&#20010;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#26679;&#26412;&#24674;&#22797;&#20986;&#19968;&#20010;&#20351;&#19987;&#23478;&#34920;&#29616;&#26368;&#20339;&#30340;&#22870;&#21169;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;$\mathcal{O}(1/\varepsilon^{4})$&#20010;&#26679;&#26412;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#19982;&#24674;&#22797;&#22870;&#21169;&#23545;&#24212;&#30340;&#26368;&#20248;&#31574;&#30053;&#22312;&#24635;&#21464;&#24046;&#36317;&#31163;&#19978;&#19982;&#19987;&#23478;&#31574;&#30053;$\varepsilon$-&#25509;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16829v1 Announce Type: cross  Abstract: Given a dataset of expert demonstrations, inverse reinforcement learning (IRL) aims to recover a reward for which the expert is optimal. This work proposes a model-free algorithm to solve entropy-regularized IRL problem. In particular, we employ a stochastic gradient descent update for the reward and a stochastic soft policy iteration update for the policy. Assuming access to a generative model, we prove that our algorithm is guaranteed to recover a reward for which the expert is $\varepsilon$-optimal using $\mathcal{O}(1/\varepsilon^{2})$ samples of the Markov decision process (MDP). Furthermore, with $\mathcal{O}(1/\varepsilon^{4})$ samples we prove that the optimal policy corresponding to the recovered reward is $\varepsilon$-close to the expert policy in total variation distance.
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#34920;&#36798;&#36890;&#29992;&#31574;&#30053;&#21644;&#38382;&#39064;&#20998;&#35299;&#30340;&#35821;&#35328;&#24341;&#20837;&#20102;&#20869;&#37096;&#23384;&#20648;&#29366;&#24577;&#12289;&#32034;&#24341;&#29305;&#24449;&#21644;&#27169;&#22359;&#25193;&#23637;&#65292;&#20351;&#24471;&#31574;&#30053;&#21644;&#33609;&#22270;&#26356;&#21152;&#28789;&#27963;&#21644;&#21487;&#37325;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.16824</link><description>&lt;p&gt;
&#20851;&#20110;&#31574;&#30053;&#37325;&#29992;&#65306;&#19968;&#31181;&#29992;&#20110;&#34920;&#31034;&#21644;&#25191;&#34892;&#35843;&#29992;&#20854;&#20182;&#31574;&#30053;&#30340;&#36890;&#29992;&#31574;&#30053;&#30340;&#34920;&#36798;&#35821;&#35328;
&lt;/p&gt;
&lt;p&gt;
On Policy Reuse: An Expressive Language for Representing and Executing General Policies that Call Other Policies
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16824
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#34920;&#36798;&#36890;&#29992;&#31574;&#30053;&#21644;&#38382;&#39064;&#20998;&#35299;&#30340;&#35821;&#35328;&#24341;&#20837;&#20102;&#20869;&#37096;&#23384;&#20648;&#29366;&#24577;&#12289;&#32034;&#24341;&#29305;&#24449;&#21644;&#27169;&#22359;&#25193;&#23637;&#65292;&#20351;&#24471;&#31574;&#30053;&#21644;&#33609;&#22270;&#26356;&#21152;&#28789;&#27963;&#21644;&#21487;&#37325;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#19968;&#31181;&#26088;&#22312;&#36890;&#36807;&#22312;&#19968;&#32452;&#24067;&#23572;&#21644;&#25968;&#20540;&#29305;&#24449;&#19978;&#23450;&#20041;&#30340;&#35268;&#21017;&#26469;&#34920;&#36798;&#21644;&#23398;&#20064;&#36890;&#29992;&#31574;&#30053;&#21644;&#38382;&#39064;&#20998;&#35299;&#65288;&#33609;&#22270;&#65289;&#30340;&#31616;&#21333;&#32780;&#24378;&#22823;&#30340;&#35821;&#35328;&#24050;&#32463;&#34987;&#24341;&#20837;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#27492;&#35821;&#35328;&#30340;&#19977;&#20010;&#25193;&#23637;&#65292;&#26088;&#22312;&#20351;&#31574;&#30053;&#21644;&#33609;&#22270;&#26356;&#21152;&#28789;&#27963;&#21644;&#21487;&#37325;&#29992;&#65306;&#20869;&#37096;&#23384;&#20648;&#29366;&#24577;&#65292;&#31867;&#20284;&#20110;&#26377;&#38480;&#29366;&#24577;&#25511;&#21046;&#22120;; &#32034;&#24341;&#29305;&#24449;&#65292;&#20854;&#20540;&#26159;&#29366;&#24577;&#21644;&#19968;&#20123;&#21487;&#35013;&#20837;&#23545;&#35937;&#30340;&#20869;&#37096;&#23492;&#23384;&#22120;&#25968;&#37327;&#30340;&#20989;&#25968;; &#20197;&#21450;&#23553;&#35013;&#31574;&#30053;&#21644;&#33609;&#22270;&#30340;&#27169;&#22359;&#65292;&#24182;&#20801;&#35768;&#23427;&#20204;&#36890;&#36807;&#20256;&#36882;&#21442;&#25968;&#30456;&#20114;&#35843;&#29992;&#12290;&#27492;&#22806;&#65292;&#19982;&#36873;&#25321;&#29366;&#24577;&#36716;&#25442;&#32780;&#19981;&#26159;&#22522;&#30784;&#21160;&#20316;&#30340;&#36890;&#29992;&#31574;&#30053;&#19981;&#21516;&#65292;&#26032;&#35821;&#35328;&#20801;&#35768;&#36873;&#25321;&#36825;&#20123;&#21160;&#20316;&#12290;&#36890;&#36807;&#19968;&#20123;&#31034;&#20363;&#23637;&#31034;&#20102;&#25152;&#24471;&#21040;&#30340;&#29992;&#20110;&#31574;&#30053;&#21644;&#33609;&#22270;&#30340;&#35821;&#35328;&#30340;&#34920;&#29616;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16824v1 Announce Type: new  Abstract: Recently, a simple but powerful language for expressing and learning general policies and problem decompositions (sketches) has been introduced in terms of rules defined over a set of Boolean and numerical features. In this work, we consider three extensions of this language aimed at making policies and sketches more flexible and reusable: internal memory states, as in finite state controllers; indexical features, whose values are a function of the state and a number of internal registers that can be loaded with objects; and modules that wrap up policies and sketches and allow them to call each other by passing parameters. In addition, unlike general policies that select state transitions rather than ground actions, the new language allows for the selection of such actions. The expressive power of the resulting language for policies and sketches is illustrated through a number of examples.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#36777;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;LLM&#22686;&#24378;&#30340;&#36777;&#35770;&#20154;&#24037;&#26234;&#33021;&#20419;&#36827;&#20154;&#31867;&#21453;&#24605;&#21644;&#35752;&#35770;&#20915;&#31574;&#20013;&#30340;&#24847;&#35265;&#20998;&#27495;&#12290;</title><link>https://arxiv.org/abs/2403.16812</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#36777;&#35770;&#65306;LLM&#22686;&#24378;&#30340;&#36777;&#35770;&#20154;&#24037;&#26234;&#33021;&#35774;&#35745;&#19982;&#35780;&#20272;&#65292;&#29992;&#20110;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#20915;&#31574;
&lt;/p&gt;
&lt;p&gt;
Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16812
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#36777;&#35770;&#26694;&#26550;&#65292;&#36890;&#36807;LLM&#22686;&#24378;&#30340;&#36777;&#35770;&#20154;&#24037;&#26234;&#33021;&#20419;&#36827;&#20154;&#31867;&#21453;&#24605;&#21644;&#35752;&#35770;&#20915;&#31574;&#20013;&#30340;&#24847;&#35265;&#20998;&#27495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#36741;&#21161;&#20915;&#31574;&#20013;&#65292;&#20154;&#31867;&#36890;&#24120;&#34987;&#21160;&#22320;&#23457;&#26597;&#20154;&#24037;&#26234;&#33021;&#30340;&#24314;&#35758;&#65292;&#28982;&#21518;&#20915;&#23450;&#26159;&#21542;&#20840;&#30424;&#25509;&#21463;&#25110;&#25298;&#32477;&#12290;&#22312;&#36825;&#26679;&#30340;&#33539;&#24335;&#20013;&#65292;&#21457;&#29616;&#20154;&#31867;&#24456;&#23569;&#28608;&#21457;&#20998;&#26512;&#24605;&#32500;&#65292;&#19988;&#22312;&#21457;&#29983;&#20998;&#27495;&#26102;&#38590;&#20197;&#23558;&#30683;&#30462;&#24847;&#35265;&#30340;&#32454;&#24494;&#24046;&#21035;&#20256;&#36798;&#32473;&#20154;&#24037;&#26234;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#20154;&#31867;&#30340;&#36777;&#35770;&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#26088;&#22312;&#20419;&#36827;&#20154;&#31867;&#22312;&#20915;&#31574;&#36807;&#31243;&#20013;&#23545;&#20154;&#24037;&#26234;&#33021;&#24847;&#35265;&#20998;&#27495;&#36827;&#34892;&#21453;&#24605;&#21644;&#35752;&#35770;&#12290;&#22522;&#20110;&#20154;&#31867;&#36777;&#35770;&#29702;&#35770;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#32500;&#24230;&#32423;&#24847;&#35265;&#24449;&#38598;&#12289;&#36777;&#35770;&#35752;&#35770;&#21644;&#20915;&#31574;&#26356;&#26032;&#65292;&#23558;&#20154;&#31867;&#21644;&#20154;&#24037;&#26234;&#33021;&#36827;&#34892;&#20114;&#21160;&#12290;&#20026;&#20102;&#36171;&#20104;&#20154;&#24037;&#26234;&#33021;&#36777;&#35770;&#33021;&#21147;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#36777;&#35770;&#20154;&#24037;&#26234;&#33021;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20316;&#20026;&#20154;&#31867;&#21644;&#39046;&#22495;&#29305;&#23450;&#27169;&#22411;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#23454;&#29616;&#28789;&#27963;&#30340;&#23545;&#35805;&#20132;&#20114;&#21644;&#24544;&#23454;&#30340;&#20449;&#24687;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16812v1 Announce Type: cross  Abstract: In AI-assisted decision-making, humans often passively review AI's suggestion and decide whether to accept or reject it as a whole. In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur. To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making. Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion elicitation, deliberative discussion, and decision updates. To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages large language models (LLMs) as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision. An exploratory evaluation on a grad
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#20248;&#21270;&#20154;&#22312;&#22238;&#36335;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#36141;&#29289;&#20013;&#24515;&#20013;&#27169;&#25311;&#21508;&#31181;&#20154;&#32676;&#28909;&#37327;&#20559;&#22909;&#30340;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.16809</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#30340;&#25968;&#23383;&#23402;&#29983;&#20307;&#29992;&#20110;&#20248;&#21270;&#20154;&#22312;&#22238;&#36335;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16809
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#20248;&#21270;&#20154;&#22312;&#22238;&#36335;&#31995;&#32479;&#30340;&#26041;&#27861;&#65292;&#24182;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#22312;&#36141;&#29289;&#20013;&#24515;&#20013;&#27169;&#25311;&#21508;&#31181;&#20154;&#32676;&#28909;&#37327;&#20559;&#22909;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#29289;&#32852;&#32593;&#21644;&#29289;&#29702;&#31995;&#32479;(CPS-IoT)&#24212;&#29992;&#21644;&#22522;&#30784;&#27169;&#22411;&#30340;&#26085;&#30410;&#26222;&#21450;&#65292;&#26032;&#30340;&#24212;&#29992;&#27491;&#22312;&#20852;&#36215;&#65292;&#21033;&#29992;&#29615;&#22659;&#30340;&#23454;&#26102;&#25511;&#21046;&#12290;&#20363;&#22914;&#65292;&#23545;&#20110;&#20379;&#26262;&#12289;&#36890;&#39118;&#21644;&#31354;&#35843;(HVAC)&#31995;&#32479;&#30340;&#23454;&#26102;&#25511;&#21046;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#20026;&#20154;&#21592;&#33298;&#36866;&#32780;&#36816;&#34892;&#26102;&#20943;&#23569;&#20854;&#20351;&#29992;&#65292;&#20174;&#32780;&#38477;&#20302;&#33021;&#28304;&#28040;&#32791;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#23545;&#36825;&#31181;&#20154;&#22312;&#22238;&#36335;(HITL)&#31995;&#32479;&#20013;&#20154;&#31867;&#20559;&#22909;&#30340;&#23454;&#26102;&#21453;&#39304;&#25910;&#38598;&#26159;&#22256;&#38590;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26469;&#24212;&#23545;CPS&#20248;&#21270;&#20013;&#21160;&#24577;&#29615;&#22659;&#21644;&#38590;&#20197;&#33719;&#21462;&#25968;&#25454;&#30340;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#65292;&#21033;&#29992;LLM&#20195;&#29702;&#27169;&#20223;&#36141;&#29289;&#20013;&#24515;&#20013;&#21508;&#31181;&#20154;&#32676;&#65288;&#22914;&#24180;&#36731;&#23478;&#24237;&#12289;&#32769;&#24180;&#20154;&#65289;&#30340;&#34892;&#20026;&#21644;&#28909;&#37327;&#20559;&#22909;&#12290;&#32858;&#21512;&#30340;&#28909;&#37327;&#20559;&#22909;&#34987;&#25972;&#21512;&#21040;&#19968;&#20010;&#20195;&#29702;&#20307;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16809v1 Announce Type: cross  Abstract: The increasing prevalence of Cyber-Physical Systems and the Internet of Things (CPS-IoT) applications and Foundation Models are enabling new applications that leverage real-time control of the environment. For example, real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems can reduce its usage when not needed for the comfort of human occupants, hence reducing energy consumption. Collecting real-time feedback on human preferences in such human-in-the-loop (HITL) systems, however, is difficult in practice. We propose the use of large language models (LLMs) to deal with the challenges of dynamic environments and difficult-to-obtain data in CPS optimization. In this paper, we present a case study that employs LLM agents to mimic the behaviors and thermal preferences of various population groups (e.g. young families, the elderly) in a shopping mall. The aggregated thermal preferences are integrated into an agent-in-th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#36136;&#37327;&#27169;&#22411;&#21644;&#21512;&#21516;&#27861;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#27431;&#30431;AI&#27861;&#26696;&#23545;&#39640;&#39118;&#38505;AI&#31995;&#32479;&#35201;&#27714;&#30340;&#26041;&#27861;&#35770;&#36884;&#24452;&#12290;</title><link>https://arxiv.org/abs/2403.16808</link><description>&lt;p&gt;
&#29702;&#35299;&#27431;&#30431;AI&#27861;&#26696;: &#21512;&#35268;&#23433;&#20840;&#20851;&#38190;&#20135;&#21697;&#30340;&#26041;&#27861;&#35770;&#36884;&#24452;
&lt;/p&gt;
&lt;p&gt;
Navigating the EU AI Act: A Methodological Approach to Compliance for Safety-critical Products
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16808
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#36136;&#37327;&#27169;&#22411;&#21644;&#21512;&#21516;&#27861;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#35299;&#37322;&#27431;&#30431;AI&#27861;&#26696;&#23545;&#39640;&#39118;&#38505;AI&#31995;&#32479;&#35201;&#27714;&#30340;&#26041;&#27861;&#35770;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
2023&#24180;12&#26376;&#65292;&#27431;&#27954;&#35758;&#20250;&#26242;&#26102;&#21516;&#24847;&#20102;&#27431;&#30431;AI&#27861;&#26696;&#12290;&#36825;&#19968;&#21069;&#25152;&#26410;&#26377;&#30340;AI&#31995;&#32479;&#30417;&#31649;&#26694;&#26550;&#21046;&#23450;&#20102;&#30830;&#20445;AI&#20135;&#21697;&#23433;&#20840;&#12289;&#21512;&#27861;&#21644;&#20540;&#24471;&#20449;&#36182;&#30340;&#25351;&#23548;&#26041;&#38024;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#20135;&#21697;&#36136;&#37327;&#27169;&#22411;&#26469;&#35299;&#37322;&#39640;&#39118;&#38505;AI&#31995;&#32479;&#22312;&#27431;&#30431;AI&#27861;&#26696;&#19979;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#39318;&#20808;&#25552;&#20986;&#20102;&#19968;&#20010;&#25193;&#23637;&#30340;AI&#31995;&#32479;&#20135;&#21697;&#36136;&#37327;&#27169;&#22411;&#65292;&#23558;&#27861;&#26696;&#20013;&#26410;&#28085;&#30422;&#30340;&#30456;&#20851;&#23646;&#24615;&#32435;&#20837;&#32771;&#34385;&#12290;&#25105;&#20204;&#23558;&#27861;&#26696;&#35201;&#27714;&#19982;&#30456;&#20851;&#36136;&#37327;&#23646;&#24615;&#36827;&#34892;&#26144;&#23556;&#65292;&#30446;&#30340;&#26159;&#23558;&#20854;&#32454;&#21270;&#20026;&#21487;&#34913;&#37327;&#30340;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21512;&#21516;&#30340;&#26041;&#27861;&#65292;&#20174;&#21033;&#30410;&#30456;&#20851;&#32773;&#23618;&#38754;&#25512;&#23548;&#25216;&#26415;&#35201;&#27714;&#12290;&#36825;&#26377;&#21161;&#20110;&#24320;&#21457;&#21644;&#35780;&#20272;&#19981;&#20165;&#31526;&#21512;&#24050;&#24314;&#31435;&#36136;&#37327;&#26631;&#20934;&#65292;&#32780;&#19988;&#31526;&#21512;&#27861;&#26696;&#20013;&#38024;&#23545;&#39640;&#39118;&#38505;&#65288;&#21253;&#25324;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16808v1 Announce Type: new  Abstract: In December 2023, the European Parliament provisionally agreed on the EU AI Act. This unprecedented regulatory framework for AI systems lays out guidelines to ensure the safety, legality, and trustworthiness of AI products. This paper presents a methodology for interpreting the EU AI Act requirements for high-risk AI systems by leveraging product quality models. We first propose an extended product quality model for AI systems, incorporating attributes relevant to the Act not covered by current quality models. We map the Act requirements to relevant quality attributes with the goal of refining them into measurable characteristics. We then propose a contract-based approach to derive technical requirements at the stakeholder level. This facilitates the development and assessment of AI systems that not only adhere to established quality standards, but also comply with the regulatory requirements outlined in the Act for high-risk (including 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#26041;&#27861;CB-Norm&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.16798</link><description>&lt;p&gt;
&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#23618;
&lt;/p&gt;
&lt;p&gt;
Cluster-Based Normalization Layer for Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16798
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#32858;&#31867;&#30340;&#31070;&#32463;&#32593;&#32476;&#35268;&#33539;&#21270;&#26041;&#27861;CB-Norm&#65292;&#36890;&#36807;&#24341;&#20837;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#65292;&#35299;&#20915;&#20102;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22312;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#36807;&#31243;&#20013;&#38754;&#20020;&#37325;&#35201;&#25361;&#25112;&#65292;&#21253;&#25324;&#20869;&#37096;&#21327;&#21464;&#37327;&#28418;&#31227;&#12289;&#26631;&#31614;&#28418;&#31227;&#12289;&#26799;&#24230;&#28040;&#22833;/&#29190;&#28856;&#12289;&#36807;&#25311;&#21512;&#21644;&#35745;&#31639;&#22797;&#26434;&#24615;&#12290;&#20256;&#32479;&#30340;&#35268;&#33539;&#21270;&#26041;&#27861;&#65292;&#22914;&#25209;&#26631;&#20934;&#21270;&#65292;&#26088;&#22312;&#35299;&#20915;&#20854;&#20013;&#19968;&#20123;&#38382;&#39064;&#65292;&#20294;&#36890;&#24120;&#20381;&#36182;&#20110;&#38480;&#21046;&#20854;&#36866;&#24212;&#24615;&#30340;&#20551;&#35774;&#12290;&#28151;&#21512;&#35268;&#33539;&#21270;&#22312;&#22788;&#29702;&#22810;&#20010;&#39640;&#26031;&#20998;&#24067;&#26102;&#38754;&#20020;&#35745;&#31639;&#38556;&#30861;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;CB-Norm&#65289;&#30340;&#20004;&#20010;&#21464;&#20307;&#8212;&#8212;&#30417;&#30563;&#24335;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;SCB-Norm&#65289;&#21644;&#26080;&#30417;&#30563;&#24335;&#22522;&#20110;&#32858;&#31867;&#30340;&#35268;&#33539;&#21270;&#65288;UCB-Norm&#65289;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#24320;&#21019;&#24615;&#30340;&#19968;&#27493;&#35268;&#33539;&#21270;&#26041;&#27861;&#12290;CB-Norm&#21033;&#29992;&#39640;&#26031;&#28151;&#21512;&#27169;&#22411;&#26469;&#19987;&#38376;&#35299;&#20915;&#19982;&#26799;&#24230;&#31283;&#23450;&#24615;&#21644;&#23398;&#20064;&#21152;&#36895;&#26377;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16798v1 Announce Type: cross  Abstract: Deep learning faces significant challenges during the training of neural networks, including internal covariate shift, label shift, vanishing/exploding gradients, overfitting, and computational complexity. While conventional normalization methods, such as Batch Normalization, aim to tackle some of these issues, they often depend on assumptions that constrain their adaptability. Mixture Normalization faces computational hurdles in its pursuit of handling multiple Gaussian distributions.   This paper introduces Cluster-Based Normalization (CB-Norm) in two variants - Supervised Cluster-Based Normalization (SCB-Norm) and Unsupervised Cluster-Based Normalization (UCB-Norm) - proposing a groundbreaking one-step normalization approach. CB-Norm leverages a Gaussian mixture model to specifically address challenges related to gradient stability and learning acceleration.   For SCB-Norm, a supervised variant, the novel mechanism involves introduc
&lt;/p&gt;</description></item><item><title>&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#21040;&#30340;&#27010;&#24565;&#20135;&#29983;&#20102;&#23454;&#36136;&#24615;&#30340;&#24433;&#21709;&#65292;&#24341;&#20837;&#26032;&#27010;&#24565;&#25110;&#20462;&#25913;&#29616;&#26377;&#27010;&#24565;&#65292;&#24182;&#19988;&#36825;&#31181;&#24433;&#21709;&#21487;&#20197;&#36890;&#36807;&#32447;&#24615;&#20998;&#35299;&#23545;&#25200;&#21160;&#36827;&#34892;&#35299;&#37322;&#12290;</title><link>https://arxiv.org/abs/2403.16782</link><description>&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#35299;&#21078;&#23398;&#65306;&#22522;&#20110;&#27010;&#24565;&#30340;XAI&#35299;&#21078;
&lt;/p&gt;
&lt;p&gt;
The Anatomy of Adversarial Attacks: Concept-based XAI Dissection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16782
&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#23398;&#21040;&#30340;&#27010;&#24565;&#20135;&#29983;&#20102;&#23454;&#36136;&#24615;&#30340;&#24433;&#21709;&#65292;&#24341;&#20837;&#26032;&#27010;&#24565;&#25110;&#20462;&#25913;&#29616;&#26377;&#27010;&#24565;&#65292;&#24182;&#19988;&#36825;&#31181;&#24433;&#21709;&#21487;&#20197;&#36890;&#36807;&#32447;&#24615;&#20998;&#35299;&#23545;&#25200;&#21160;&#36827;&#34892;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#25239;&#24615;&#25915;&#20987;(AAs)&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#26500;&#25104;&#37325;&#22823;&#23041;&#32961;&#12290;&#34429;&#28982;&#36825;&#20123;&#25915;&#20987;&#23545;&#27169;&#22411;&#39044;&#27979;&#30340;&#24433;&#21709;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#23427;&#20204;&#23545;&#36825;&#20123;&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#30340;&#34920;&#31034;&#21644;&#27010;&#24565;&#30340;&#24433;&#21709;&#20173;&#28982;&#22823;&#22810;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;(XAI)&#25216;&#26415;&#65292;&#23545;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;(CNNs)&#20013;&#23545;&#25239;&#24615;&#25915;&#20987;&#23545;&#23398;&#20064;&#21040;&#30340;&#27010;&#24565;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#32593;&#32476;&#26550;&#26500;&#21644;&#26377;&#38024;&#23545;&#24615;&#30340;AA&#25216;&#26415;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20960;&#20010;&#20851;&#38190;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;AAs&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36896;&#25104;&#27010;&#24565;&#32452;&#25104;&#30340;&#23454;&#36136;&#24615;&#21464;&#21270;&#65292;&#24341;&#20837;&#26032;&#27010;&#24565;&#25110;&#20462;&#25913;&#29616;&#26377;&#27010;&#24565;&#12290;&#20854;&#27425;&#65292;&#23545;&#25239;&#24615;&#25200;&#21160;&#26412;&#36523;&#21487;&#20197;&#34987;&#32447;&#24615;&#20998;&#35299;&#20026;&#19968;&#32452;&#28508;&#22312;&#30690;&#37327;&#20998;&#37327;&#65292;&#20854;&#20013;&#37096;&#20998;&#20998;&#37327;&#36127;&#36131;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16782v1 Announce Type: cross  Abstract: Adversarial attacks (AAs) pose a significant threat to the reliability and robustness of deep neural networks. While the impact of these attacks on model predictions has been extensively studied, their effect on the learned representations and concepts within these models remains largely unexplored. In this work, we perform an in-depth analysis of the influence of AAs on the concepts learned by convolutional neural networks (CNNs) using eXplainable artificial intelligence (XAI) techniques. Through an extensive set of experiments across various network architectures and targeted AA techniques, we unveil several key findings. First, AAs induce substantial alterations in the concept composition within the feature space, introducing new concepts or modifying existing ones. Second, the adversarial perturbation itself can be linearly decomposed into a set of latent vector components, with a subset of these being responsible for the attack's 
&lt;/p&gt;</description></item><item><title>DeepKnowledge&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#27867;&#21270;&#29702;&#35770;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#27979;&#35797;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;DNN&#30340;&#31283;&#20581;&#24615;&#24182;&#20943;&#23569;&#40657;&#21283;&#23376;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;</title><link>https://arxiv.org/abs/2403.16768</link><description>&lt;p&gt;
DeepKnowledge: &#22522;&#20110;&#27867;&#21270;&#39537;&#21160;&#30340;&#28145;&#24230;&#23398;&#20064;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
DeepKnowledge: Generalisation-Driven Deep Learning Testing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16768
&lt;/p&gt;
&lt;p&gt;
DeepKnowledge&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#27867;&#21270;&#29702;&#35770;&#30340;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#27979;&#35797;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;DNN&#30340;&#31283;&#20581;&#24615;&#24182;&#20943;&#23569;&#40657;&#21283;&#23376;&#27169;&#22411;&#30340;&#39118;&#38505;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#24494;&#23567;&#21464;&#21270;&#26497;&#20026;&#33030;&#24369;&#65292;&#36825;&#35201;&#27714;&#26377;&#25928;&#30340;&#27979;&#35797;&#25216;&#26415;&#26469;&#35780;&#20272;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#12290;&#23613;&#31649;&#36817;&#24180;&#26469;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27979;&#35797;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#32570;&#20047;&#31995;&#32479;&#21270;&#30340;&#27979;&#35797;&#26041;&#27861;&#26469;&#35780;&#20272;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#35757;&#32451;&#20998;&#24067;&#20043;&#22806;&#30340;&#25968;&#25454;&#19978;&#27867;&#21270;&#21644;&#36816;&#34892;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36890;&#36807;DeepKnowledge&#26469;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#30693;&#35782;&#27867;&#21270;&#29702;&#35770;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#31995;&#32479;&#27979;&#35797;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#31283;&#20581;&#24615;&#65292;&#24182;&#20943;&#23569;&#8220;&#40657;&#21283;&#23376;&#8221;&#27169;&#22411;&#30340;&#21097;&#20313;&#39118;&#38505;&#12290;&#26681;&#25454;&#36825;&#19968;&#29702;&#35770;&#65292;DeepKnowledge&#35748;&#20026;&#26680;&#24515;&#35745;&#31639;DNN&#21333;&#20803;&#65292;&#31216;&#20026;&#36716;&#31227;&#30693;&#35782;&#31070;&#32463;&#20803;&#65292;&#22312;&#22495;&#21464;&#21270;&#19979;&#21487;&#20197;&#27867;&#21270;&#12290;DeepKnowledge&#25552;&#20379;&#20102;&#19968;&#31181;&#23458;&#35266;&#30340;&#20449;&#24515;&#24230;&#37327;&#65292;&#29992;&#20110;&#35780;&#20272;&#32473;&#23450;&#25968;&#25454;&#20998;&#24067;&#21464;&#21270;&#30340;DNN&#27979;&#35797;&#27963;&#21160;&#65292;&#24182;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#26469;&#25512;&#21160;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16768v1 Announce Type: cross  Abstract: Despite their unprecedented success, DNNs are notoriously fragile to small shifts in data distribution, demanding effective testing techniques that can assess their dependability. Despite recent advances in DNN testing, there is a lack of systematic testing approaches that assess the DNN's capability to generalise and operate comparably beyond data in their training distribution. We address this gap with DeepKnowledge, a systematic testing methodology for DNN-based systems founded on the theory of knowledge generalisation, which aims to enhance DNN robustness and reduce the residual risk of 'black box' models. Conforming to this theory, DeepKnowledge posits that core computational DNN units, termed Transfer Knowledge neurons, can generalise under domain shift. DeepKnowledge provides an objective confidence measurement on testing activities of DNN given data distribution shifts and uses this information to instrument a generalisation-in
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#39033;&#24863;&#30693;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#20154;&#20204;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#23545;&#21512;&#25104;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#19982;&#30495;&#23454;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#20197;&#25506;&#35752;&#20154;&#31867;&#23545;&#27450;&#39575;&#24615;&#21512;&#25104;&#23186;&#20307;&#30340;&#26131;&#21463;&#31243;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.16760</link><description>&lt;p&gt;
&#21644;&#25243;&#30828;&#24065;&#19968;&#26679;&#22909;&#65306;&#20154;&#31867;&#23545;AI&#29983;&#25104;&#30340;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#38899;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#30340;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
As Good As A Coin Toss Human detection of AI-generated images, videos, audio, and audiovisual stimuli
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16760
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#39033;&#24863;&#30693;&#30740;&#31350;&#65292;&#35780;&#20272;&#20102;&#20154;&#20204;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#23545;&#21512;&#25104;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#19982;&#30495;&#23454;&#30340;&#21306;&#20998;&#33021;&#21147;&#65292;&#20197;&#25506;&#35752;&#20154;&#31867;&#23545;&#27450;&#39575;&#24615;&#21512;&#25104;&#23186;&#20307;&#30340;&#26131;&#21463;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21512;&#25104;&#23186;&#20307;&#21464;&#24471;&#36234;&#26469;&#36234;&#36924;&#30495;&#65292;&#20351;&#29992;&#23427;&#30340;&#38556;&#30861;&#19981;&#26029;&#38477;&#20302;&#65292;&#36825;&#39033;&#25216;&#26415;&#36234;&#26469;&#36234;&#34987;&#24694;&#24847;&#21033;&#29992;&#65292;&#20174;&#37329;&#34701;&#27450;&#35784;&#21040;&#38750;&#33258;&#24895;&#33394;&#24773;&#12290;&#20170;&#22825;&#65292;&#23545;&#25239;&#34987;&#21512;&#25104;&#23186;&#20307;&#35823;&#23548;&#30340;&#20027;&#35201;&#38450;&#24481;&#20381;&#36182;&#20110;&#20154;&#31867;&#35266;&#23519;&#32773;&#22312;&#35270;&#35273;&#21644;&#21548;&#35273;&#19978;&#21306;&#20998;&#30495;&#20551;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#20154;&#20204;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#23454;&#38469;&#19978;&#23545;&#27450;&#39575;&#24615;&#21512;&#25104;&#23186;&#20307;&#26377;&#22810;&#33030;&#24369;&#20173;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#20010;&#21253;&#21547;1276&#21517;&#21442;&#19982;&#32773;&#30340;&#24863;&#30693;&#30740;&#31350;&#65292;&#35780;&#20272;&#20154;&#20204;&#22312;&#21306;&#20998;&#21512;&#25104;&#22270;&#20687;&#12289;&#20165;&#38899;&#39057;&#12289;&#20165;&#35270;&#39057;&#21644;&#38899;&#35270;&#39057;&#21050;&#28608;&#19982;&#30495;&#23454;&#30340;&#20934;&#30830;&#24615;&#22914;&#20309;&#12290;&#20026;&#20102;&#21453;&#26144;&#20154;&#20204;&#22312;&#37326;&#22806;&#21487;&#33021;&#36935;&#21040;&#21512;&#25104;&#23186;&#20307;&#30340;&#24773;&#20917;&#65292;&#27979;&#35797;&#26465;&#20214;&#21644;&#21050;&#28608;&#27169;&#25311;&#20102;&#20856;&#22411;&#30340;&#22312;&#32447;&#24179;&#21488;&#65292;&#32780;&#35843;&#26597;&#20013;&#20351;&#29992;&#30340;&#25152;&#26377;&#21512;&#25104;&#23186;&#20307;&#22343;&#26469;&#33258;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16760v1 Announce Type: cross  Abstract: As synthetic media becomes progressively more realistic and barriers to using it continue to lower, the technology has been increasingly utilized for malicious purposes, from financial fraud to nonconsensual pornography. Today, the principal defense against being misled by synthetic media relies on the ability of the human observer to visually and auditorily discern between real and fake. However, it remains unclear just how vulnerable people actually are to deceptive synthetic media in the course of their day to day lives. We conducted a perceptual study with 1276 participants to assess how accurate people were at distinguishing synthetic images, audio only, video only, and audiovisual stimuli from authentic. To reflect the circumstances under which people would likely encounter synthetic media in the wild, testing conditions and stimuli emulated a typical online platform, while all synthetic media used in the survey was sourced from 
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35282;&#33394;&#25366;&#25496;&#20013;&#30340;&#21452;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#24191;&#20041;&#22122;&#22768;&#35282;&#33394;&#25366;&#25496;&#38382;&#39064;&#65288;GNRM&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#22266;&#23450;&#21442;&#25968;&#21487;&#35299;&#24615;&#65292;&#20026;&#35299;&#20915;&#35282;&#33394;&#25366;&#25496;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#25552;&#20379;&#20102;&#37325;&#35201;&#22522;&#30784;&#12290;</title><link>https://arxiv.org/abs/2403.16757</link><description>&lt;p&gt;
&#35282;&#33394;&#25366;&#25496;&#20013;&#30340;&#21452;&#30446;&#26631;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Bi-objective Optimization in Role Mining
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16757
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#35282;&#33394;&#25366;&#25496;&#20013;&#30340;&#21452;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24341;&#20837;&#20102;&#24191;&#20041;&#22122;&#22768;&#35282;&#33394;&#25366;&#25496;&#38382;&#39064;&#65288;GNRM&#65289;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20855;&#26377;&#22266;&#23450;&#21442;&#25968;&#21487;&#35299;&#24615;&#65292;&#20026;&#35299;&#20915;&#35282;&#33394;&#25366;&#25496;&#20013;&#30340;&#23454;&#38469;&#38382;&#39064;&#25552;&#20379;&#20102;&#37325;&#35201;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35282;&#33394;&#25366;&#25496;&#26159;&#19968;&#31181;&#20174;&#29616;&#26377;&#31574;&#30053;&#20013;&#23548;&#20986;&#22522;&#20110;&#35282;&#33394;&#30340;&#25480;&#26435;&#31574;&#30053;&#30340;&#25216;&#26415;&#12290;&#32473;&#23450;&#19968;&#32452;&#29992;&#25143;$U$&#65292;&#19968;&#32452;&#26435;&#38480;$P$&#21644;&#19968;&#20010;&#29992;&#25143;-&#26435;&#38480;&#25480;&#26435;&#20851;&#31995;$\mathit{UPA}\subseteq U\times P$&#65292;&#35282;&#33394;&#25366;&#25496;&#31639;&#27861;&#26088;&#22312;&#35745;&#31639;&#19968;&#32452;&#35282;&#33394;$R$&#65292;&#19968;&#20010;&#29992;&#25143;-&#35282;&#33394;&#25480;&#26435;&#20851;&#31995;$\mathit{UA}\subseteq U\times R$&#21644;&#19968;&#20010;&#26435;&#38480;-&#35282;&#33394;&#25480;&#26435;&#20851;&#31995;$\mathit{PA}\subseteq R\times P$&#65292;&#20197;&#20415;$\mathit{UA}$&#21644;$\mathit{PA}$&#30340;&#32452;&#21512;&#22312;&#26576;&#31181;&#36866;&#24403;&#30340;&#24847;&#20041;&#19979;&#25509;&#36817;&#20110;$\mathit{UPA}$&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#24191;&#20041;&#22122;&#22768;&#35282;&#33394;&#25366;&#25496;&#38382;&#39064;&#65288;GNRM&#65289;--&#26368;&#23567;&#22122;&#22768;&#35282;&#33394;&#25366;&#25496;&#38382;&#39064;&#30340;&#19968;&#33324;&#21270;&#65292;&#25105;&#20204;&#35748;&#20026;&#36825;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#23454;&#38469;&#30456;&#20851;&#24615;&#12290;&#24310;&#32493;Fomin&#31561;&#20154;&#30340;&#24037;&#20316;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GNRM&#26159;&#20855;&#26377;&#22266;&#23450;&#21442;&#25968;&#21487;&#35299;&#24615;&#30340;&#65292;&#21442;&#25968;&#20026;$r+k$&#65292;&#20854;&#20013;$r$&#26159;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#35282;&#33394;&#25968;&#37327;&#65292;$k$&#26159;&#19981;&#19968;&#33268;&#24615;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16757v1 Announce Type: cross  Abstract: Role mining is a technique used to derive a role-based authorization policy from an existing policy. Given a set of users $U$, a set of permissions $P$ and a user-permission authorization relation $\mahtit{UPA}\subseteq U\times P$, a role mining algorithm seeks to compute a set of roles $R$, a user-role authorization relation $\mathit{UA}\subseteq U\times R$ and a permission-role authorization relation $\mathit{PA}\subseteq R\times P$, such that the composition of $\mathit{UA}$ and $\mathit{PA}$ is close (in some appropriate sense) to $\mathit{UPA}$.   In this paper, we first introduce the Generalized Noise Role Mining problem (GNRM) -- a generalization of the MinNoise Role Mining problem -- which we believe has considerable practical relevance. Extending work of Fomin et al., we show that GNRM is fixed parameter tractable, with parameter $r + k$, where $r$ is the number of roles in the solution and $k$ is the number of discrepancies b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20877;&#29983;&#20154;&#24037;&#26234;&#33021;&#20013;&#30828;&#20214;&#35774;&#35745;&#20013;CWEs&#30340;&#24418;&#24335;&#39564;&#35777;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#30828;&#20214;&#20195;&#30721;&#26102;&#24182;&#26410;&#32771;&#34385;&#30828;&#20214;CWEs&#65292;&#23548;&#33268;&#22823;&#32422;60%&#30340;&#30828;&#20214;&#35774;&#35745;&#23384;&#22312;&#28431;&#27934;&#12290;</title><link>https://arxiv.org/abs/2403.16750</link><description>&lt;p&gt;
&#20154;&#24037;&#24635;&#31639;&#19981;&#37027;&#20040;&#26234;&#33021;&#65306;&#20174;&#24418;&#24335;&#39564;&#35777;&#30340;&#35282;&#24230;&#30475;GenAI
&lt;/p&gt;
&lt;p&gt;
All Artificial, Less Intelligence: GenAI through the Lens of Formal Verification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20877;&#29983;&#20154;&#24037;&#26234;&#33021;&#20013;&#30828;&#20214;&#35774;&#35745;&#20013;CWEs&#30340;&#24418;&#24335;&#39564;&#35777;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#29983;&#25104;&#30828;&#20214;&#20195;&#30721;&#26102;&#24182;&#26410;&#32771;&#34385;&#30828;&#20214;CWEs&#65292;&#23548;&#33268;&#22823;&#32422;60%&#30340;&#30828;&#20214;&#35774;&#35745;&#23384;&#22312;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#30828;&#20214;&#35774;&#35745;&#21464;&#24471;&#36234;&#26469;&#36234;&#39640;&#25928;&#21644;&#22797;&#26434;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24120;&#24120;&#23481;&#26131;&#21463;&#21040;&#24120;&#35265;&#24369;&#28857;&#26522;&#20030;&#65288;CWEs&#65289;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#20851;&#27880;&#30340;&#26159;&#22312;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36171;&#33021;&#30340;&#20877;&#29983;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20013;&#65292;&#23545;&#19968;&#32452;&#29992;SystemVerilog&#32534;&#20889;&#30340;&#30828;&#20214;&#35774;&#35745;&#20013;CWEs&#30340;&#24418;&#24335;&#39564;&#35777;&#12290;&#25105;&#20204;&#24212;&#29992;&#24418;&#24335;&#39564;&#35777;&#26469;&#23558;&#27599;&#20010;&#30828;&#20214;&#35774;&#35745;&#20998;&#31867;&#20026;&#26131;&#21463;&#25915;&#20987;&#25110;&#26080;CWE&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#26159;&#30001;4&#20010;&#19981;&#21516;&#30340;LLMs&#29983;&#25104;&#30340;&#65292;&#20026;&#25105;&#20204;&#35770;&#25991;&#20013;&#38024;&#23545;&#30340;10&#31181;CWE&#20013;&#30340;&#27599;&#19968;&#31181;&#29305;&#24615;&#35774;&#35745;&#20102;&#19968;&#32452;&#29420;&#29305;&#30340;&#35774;&#35745;&#12290;&#25105;&#20204;&#23558;&#35782;&#21035;&#20986;&#30340;&#28431;&#27934;&#19982;CWE&#32534;&#21495;&#20851;&#32852;&#65292;&#29992;&#20110;60,000&#20010;&#29983;&#25104;&#30340;SystemVerilog&#23492;&#23384;&#22120;&#20256;&#36755;&#32423;&#65288;RTL&#65289;&#20195;&#30721;&#30340;&#25968;&#25454;&#38598;&#12290;&#30740;&#31350;&#36824;&#21457;&#29616;&#65292;&#22823;&#22810;&#25968;LLMs&#24182;&#19981;&#30693;&#36947;&#20219;&#20309;&#30828;&#20214;CWEs&#65307;&#22240;&#27492;&#65292;&#23427;&#20204;&#36890;&#24120;&#22312;&#29983;&#25104;&#30828;&#20214;&#20195;&#30721;&#26102;&#19981;&#20104;&#32771;&#34385;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#26174;&#31034;&#65292;&#22823;&#32422;60%&#30001;LLMs&#29983;&#25104;&#30340;&#30828;&#20214;&#35774;&#35745;&#23384;&#22312;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16750v1 Announce Type: new  Abstract: Modern hardware designs have grown increasingly efficient and complex. However, they are often susceptible to Common Weakness Enumerations (CWEs). This paper is focused on the formal verification of CWEs in a dataset of hardware designs written in SystemVerilog from Regenerative Artificial Intelligence (AI) powered by Large Language Models (LLMs). We applied formal verification to categorize each hardware design as vulnerable or CWE-free. This dataset was generated by 4 different LLMs and features a unique set of designs for each of the 10 CWEs we target in our paper. We have associated the identified vulnerabilities with CWE numbers for a dataset of 60,000 generated SystemVerilog Register Transfer Level (RTL) code. It was also found that most LLMs are not aware of any hardware CWEs; hence they are usually not considered when generating the hardware code. Our study reveals that approximately 60% of the hardware designs generated by LLMs 
&lt;/p&gt;</description></item><item><title>&#36845;&#20195;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26032;&#26041;&#27861;&#21033;&#29992;&#36830;&#32493;&#36755;&#20986;&#30340;&#25910;&#25947;&#36895;&#29575;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#27604;&#38598;&#25104;&#26041;&#27861;&#26356;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#20808;&#36827;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.16732</link><description>&lt;p&gt;
&#22312;&#36845;&#20195;&#31070;&#32463;&#32593;&#32476;&#20013;&#23454;&#29616;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Enabling Uncertainty Estimation in Iterative Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16732
&lt;/p&gt;
&lt;p&gt;
&#36845;&#20195;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#26032;&#26041;&#27861;&#21033;&#29992;&#36830;&#32493;&#36755;&#20986;&#30340;&#25910;&#25947;&#36895;&#29575;&#20316;&#20026;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#29992;&#20195;&#29702;&#65292;&#25552;&#20379;&#20102;&#27604;&#38598;&#25104;&#26041;&#27861;&#26356;&#20302;&#35745;&#31639;&#25104;&#26412;&#30340;&#20808;&#36827;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#20256;&#36882;&#32593;&#32476;&#26550;&#26500;&#36716;&#21464;&#20026;&#36845;&#20195;&#32593;&#32476;&#26550;&#26500;&#65292;&#36845;&#20195;&#32593;&#32476;&#20351;&#29992;&#33258;&#36523;&#30340;&#36755;&#20986;&#20316;&#20026;&#36755;&#20837;&#65292;&#36825;&#26159;&#19968;&#31181;&#25552;&#21319;&#24615;&#33021;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#35748;&#20026;&#36825;&#31181;&#26550;&#26500;&#36824;&#25552;&#20379;&#20102;&#39069;&#22806;&#30340;&#22909;&#22788;&#65306;&#36830;&#32493;&#36755;&#20986;&#30340;&#25910;&#25947;&#36895;&#29575;&#19982;&#20854;&#25910;&#25947;&#20540;&#30340;&#20934;&#30830;&#24615;&#39640;&#24230;&#30456;&#20851;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#25910;&#25947;&#36895;&#29575;&#29992;&#20316;&#19981;&#30830;&#23450;&#24615;&#30340;&#26377;&#29992;&#20195;&#29702;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#31181;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#27861;&#65292;&#20197;&#27604;&#35832;&#22914;&#38598;&#25104;&#26041;&#27861;&#26356;&#20302;&#30340;&#35745;&#31639;&#25104;&#26412;&#25552;&#20379;&#20102;&#26368;&#20808;&#36827;&#30340;&#20272;&#35745;&#65292;&#32780;&#19988;&#19981;&#38656;&#35201;&#23545;&#21407;&#22987;&#36845;&#20195;&#27169;&#22411;&#36827;&#34892;&#20219;&#20309;&#20462;&#25913;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#23884;&#20837;&#21040;&#20004;&#20010;&#24212;&#29992;&#39046;&#22495;&#20013;&#26469;&#23637;&#31034;&#20854;&#23454;&#29992;&#20215;&#20540;&#65306;&#33322;&#31354;&#22270;&#20687;&#20013;&#30340;&#36947;&#36335;&#26816;&#27979;&#21644;&#20108;&#32500;&#21644;&#19977;&#32500;&#24418;&#29366;&#30340;&#31354;&#27668;&#21160;&#21147;&#29305;&#24615;&#20272;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16732v1 Announce Type: new  Abstract: Turning pass-through network architectures into iterative ones, which use their own output as input, is a well-known approach for boosting performance. In this paper, we argue that such architectures offer an additional benefit: The convergence rate of their successive outputs is highly correlated with the accuracy of the value to which they converge. Thus, we can use the convergence rate as a useful proxy for uncertainty. This results in an approach to uncertainty estimation that provides state-of-the-art estimates at a much lower computational cost than techniques like Ensembles, and without requiring any modifications to the original iterative model. We demonstrate its practical value by embedding it in two application domains: road detection in aerial images and the estimation of aerodynamic properties of 2D and 3D shapes.
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#23450;&#26102;&#20266;Huber&#25439;&#22833;&#20989;&#25968;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#30772;&#22351;&#25269;&#25239;&#21147;&#65292;&#21487;&#22312;&#20445;&#30041;&#39640;&#36136;&#37327;&#29983;&#25104;&#25968;&#25454;&#30340;&#21516;&#26102;&#25552;&#20379;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#25439;&#22351;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#26356;&#22909;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16728</link><description>&lt;p&gt;
&#20351;&#29992;&#23450;&#26102;&#20266;Huber&#25439;&#22833;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#30772;&#22351;&#25269;&#25239;&#21147;
&lt;/p&gt;
&lt;p&gt;
Improving Diffusion Models's Data-Corruption Resistance using Scheduled Pseudo-Huber Loss
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16728
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23450;&#26102;&#20266;Huber&#25439;&#22833;&#20989;&#25968;&#25913;&#36827;&#25193;&#25955;&#27169;&#22411;&#30340;&#25968;&#25454;&#30772;&#22351;&#25269;&#25239;&#21147;&#65292;&#21487;&#22312;&#20445;&#30041;&#39640;&#36136;&#37327;&#29983;&#25104;&#25968;&#25454;&#30340;&#21516;&#26102;&#25552;&#20379;&#40065;&#26834;&#24615;&#65292;&#24182;&#22312;&#25439;&#22351;&#25968;&#25454;&#38598;&#19978;&#23637;&#29616;&#26356;&#22909;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#22240;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#24322;&#24120;&#20540;&#32780;&#33030;&#24369;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#26367;&#20195;&#25193;&#25955;&#25439;&#22833;&#20989;&#25968;&#65292;&#35813;&#20989;&#25968;&#21487;&#20197;&#22312;&#20445;&#30041;&#39640;&#36136;&#37327;&#29983;&#25104;&#25968;&#25454;&#30340;&#21516;&#26102;&#65292;&#20855;&#26377;&#40065;&#26834;&#24615;&#20197;&#25269;&#25239;&#24322;&#24120;&#20540;&#12290;&#25105;&#20204;&#24314;&#35758;&#20351;&#29992;&#24102;&#26377;&#26102;&#38388;&#30456;&#20851;&#21442;&#25968;&#30340;&#20266;Huber&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#22312;&#26368;&#33030;&#24369;&#30340;&#26089;&#26399;&#36870;&#25193;&#25955;&#27493;&#39588;&#20013;&#23454;&#29616;&#40065;&#26834;&#24615;&#21644;&#26368;&#32456;&#27493;&#39588;&#20013;&#32454;&#33410;&#24674;&#22797;&#20043;&#38388;&#30340;&#26435;&#34913;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#26102;&#38388;&#30456;&#20851;&#21442;&#25968;&#30340;&#20266;Huber&#25439;&#22833;&#22312;&#22270;&#20687;&#21644;&#38899;&#39057;&#39046;&#22495;&#30340;&#25439;&#22351;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#26356;&#22909;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#25439;&#22833;&#20989;&#25968;&#21487;&#20197;&#24110;&#21161;&#25193;&#25955;&#27169;&#22411;&#25269;&#25239;&#25968;&#25454;&#38598;&#30772;&#22351;&#65292;&#32780;&#19982;&#20256;&#32479;&#35757;&#32451;&#31639;&#27861;&#30456;&#27604;&#65292;&#19981;&#38656;&#35201;&#25968;&#25454;&#36807;&#28388;&#25110;&#20928;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16728v1 Announce Type: new  Abstract: Diffusion models are known to be vulnerable to outliers in training data. In this paper we study an alternative diffusion loss function, which can preserve the high quality of generated data like the original squared $L_{2}$ loss while at the same time being robust to outliers. We propose to use pseudo-Huber loss function with a time-dependent parameter to allow for the trade-off between robustness on the most vulnerable early reverse-diffusion steps and fine details restoration on the final steps. We show that pseudo-Huber loss with the time-dependent parameter exhibits better performance on corrupted datasets in both image and audio domains. In addition, the loss function we propose can potentially help diffusion models to resist dataset corruption while not requiring data filtering or purification compared to conventional training algorithms.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#20154;&#30340;&#20215;&#20540;&#37197;&#32622;&#21644;&#35780;&#20272;&#30340;&#34892;&#21160;&#26694;&#26550;&#65292;&#20026;&#28385;&#24847;&#30340;&#12289;&#20247;&#20803;&#30340;&#12289;&#20197;&#34892;&#21160;&#20026;&#22522;&#30784;&#30340;&#21644;&#39318;&#36873;&#30340;&#21151;&#21033;&#20262;&#29702;&#25552;&#20379;&#20102;&#35745;&#31639;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2403.16719</link><description>&lt;p&gt;
&#36808;&#21521;&#22522;&#20110;&#20215;&#20540;&#30340;&#34892;&#21160;&#21644;&#21151;&#21033;&#20262;&#29702;&#30340;&#24418;&#24335;&#21270;
&lt;/p&gt;
&lt;p&gt;
Towards a Formalisation of Value-based Actions and Consequentialist Ethics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16719
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20195;&#29702;&#20154;&#30340;&#20215;&#20540;&#37197;&#32622;&#21644;&#35780;&#20272;&#30340;&#34892;&#21160;&#26694;&#26550;&#65292;&#20026;&#28385;&#24847;&#30340;&#12289;&#20247;&#20803;&#30340;&#12289;&#20197;&#34892;&#21160;&#20026;&#22522;&#30784;&#30340;&#21644;&#39318;&#36873;&#30340;&#21151;&#21033;&#20262;&#29702;&#25552;&#20379;&#20102;&#35745;&#31639;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#29702;&#20154;&#30340;&#34892;&#21160;&#26088;&#22312;&#23454;&#29616;&#19982;&#20854;&#20010;&#20154;&#25110;&#26426;&#26500;&#20215;&#20540;&#26356;&#30456;&#23481;&#30340;&#19990;&#30028;&#29366;&#24577;&#12290;&#20026;&#20102;&#24418;&#24335;&#21270;&#36825;&#19968;&#30452;&#35273;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;STRIPS&#24418;&#24335;&#21270;&#30340;&#34892;&#21160;&#26694;&#26550;&#12290;&#25216;&#26415;&#19978;&#65292;&#26412;&#25991;&#30340;&#36129;&#29486;&#26159;&#36890;&#36807;&#22522;&#20110;&#20215;&#20540;&#30340;&#24418;&#24335;&#25512;&#29702;&#65288;VFR&#65289;&#26469;&#34920;&#36798;&#34892;&#21160;&#65292;VFR&#25552;&#20379;&#20102;&#20174;&#20195;&#29702;&#20154;&#30340;&#20215;&#20540;&#37197;&#32622;&#25991;&#20214;&#21644;&#20195;&#29702;&#20154;&#23545;&#21629;&#39064;&#19982;&#37197;&#32622;&#25991;&#20214;&#30340;&#35780;&#20272;&#20013;&#25512;&#23548;&#20986;&#30340;&#21629;&#39064;&#38598;&#21512;&#12290;&#20174;&#27010;&#24565;&#19978;&#35762;&#65292;&#26412;&#25991;&#20026;&#19968;&#31181;&#28385;&#24847;&#30340;&#12289;&#20247;&#20803;&#30340;&#12289;&#20197;&#34892;&#21160;&#20026;&#22522;&#30784;&#30340;&#21644;&#39318;&#36873;&#30340;&#21151;&#21033;&#20262;&#29702;&#25552;&#20379;&#20102;&#19968;&#20010;&#35745;&#31639;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16719v1 Announce Type: cross  Abstract: Agents act to bring about a state of the world that is more compatible with their personal or institutional values. To formalise this intuition, the paper proposes an action framework based on the STRIPS formalisation. Technically, the contribution expresses actions in terms of Value-based Formal Reasoning (VFR), which provides a set of propositions derived from an Agent's value profile and the Agent's assessment of propositions with respect to the profile. Conceptually, the contribution provides a computational framework for a form of consequentialist ethics which is satisficing, luralistic, act-based, and preferential.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21333;&#27425;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#20013;&#25209;&#24402;&#19968;&#21270;&#23618;&#32479;&#35745;&#25968;&#25454;&#22256;&#38590;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16707</link><description>&lt;p&gt;
&#21333;&#27425;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
One-Shot Domain Incremental Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16707
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22788;&#29702;&#21333;&#27425;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#20013;&#25209;&#24402;&#19968;&#21270;&#23618;&#32479;&#35745;&#25968;&#25454;&#22256;&#38590;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20197;&#21069;&#20851;&#20110;&#29992;&#20110;&#20998;&#31867;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#30740;&#31350;&#20013;&#24050;&#32463;&#35752;&#35770;&#20102;&#39046;&#22495;&#22686;&#37327;&#23398;&#20064;&#65288;DIL&#65289;&#12290;&#22312;DIL&#20013;&#65292;&#25105;&#20204;&#20551;&#35774;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#35266;&#23519;&#26032;&#39046;&#22495;&#19978;&#30340;&#26679;&#26412;&#12290;&#27169;&#22411;&#24517;&#39035;&#23545;&#25152;&#26377;&#39046;&#22495;&#19978;&#30340;&#36755;&#20837;&#36827;&#34892;&#20998;&#31867;&#12290;&#28982;&#32780;&#65292;&#22312;&#23454;&#36341;&#20013;&#65292;&#25105;&#20204;&#21487;&#33021;&#20250;&#36935;&#21040;&#36825;&#26679;&#19968;&#31181;&#24773;&#20917;&#65292;&#21363;&#25105;&#20204;&#38656;&#35201;&#22312;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#20165;&#38388;&#27463;&#24615;&#22320;&#34987;&#35266;&#23519;&#30340;&#32422;&#26463;&#19979;&#25191;&#34892;DIL&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#26497;&#31471;&#24773;&#20917;&#65292;&#21363;&#25105;&#20204;&#21482;&#26377;&#19968;&#20221;&#26469;&#33258;&#26032;&#39046;&#22495;&#30340;&#26679;&#26412;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#21333;&#27425;DIL&#12290;&#25105;&#20204;&#39318;&#20808;&#32463;&#39564;&#24615;&#22320;&#34920;&#26126;&#29616;&#26377;&#30340;DIL&#26041;&#27861;&#22312;&#21333;&#27425;DIL&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#36890;&#36807;&#21508;&#31181;&#35843;&#26597;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#31181;&#22833;&#36133;&#30340;&#21407;&#22240;&#12290;&#26681;&#25454;&#25105;&#20204;&#30340;&#20998;&#26512;&#65292;&#25105;&#20204;&#26126;&#30830;&#20102;&#21333;&#27425;DIL&#30340;&#22256;&#38590;&#26159;&#30001;&#25209;&#24402;&#19968;&#21270;&#23618;&#20013;&#30340;&#32479;&#35745;&#25968;&#25454;&#24341;&#36215;&#30340;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#20110;&#36825;&#20123;&#32479;&#35745;&#25968;&#25454;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16707v1 Announce Type: cross  Abstract: Domain incremental learning (DIL) has been discussed in previous studies on deep neural network models for classification. In DIL, we assume that samples on new domains are observed over time. The models must classify inputs on all domains. In practice, however, we may encounter a situation where we need to perform DIL under the constraint that the samples on the new domain are observed only infrequently. Therefore, in this study, we consider the extreme case where we have only one sample from the new domain, which we call one-shot DIL. We first empirically show that existing DIL methods do not work well in one-shot DIL. We have analyzed the reason for this failure through various investigations. According to our analysis, we clarify that the difficulty of one-shot DIL is caused by the statistics in the batch normalization layers. Therefore, we propose a technique regarding these statistics and demonstrate the effectiveness of our tech
&lt;/p&gt;</description></item><item><title>&#25506;&#31350;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35805;&#25945;&#23398;&#22312;&#33041;&#30005;&#22270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#23545;&#35805;&#24335;&#25945;&#23398;&#22330;&#26223;&#20013;&#65292;ChatGPT&#33021;&#22815;&#26377;&#25928;&#23653;&#34892;&#25945;&#23398;&#35282;&#33394;&#65292;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.16687</link><description>&lt;p&gt;
&#25506;&#31350;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35805;&#25945;&#23398;&#22312;&#33041;&#30005;&#22270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Investigation of the effectiveness of applying ChatGPT in Dialogic Teaching Using Electroencephalography
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16687
&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#23558;ChatGPT&#24212;&#29992;&#20110;&#23545;&#35805;&#25945;&#23398;&#22312;&#33041;&#30005;&#22270;&#23398;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#21457;&#29616;&#22312;&#23545;&#35805;&#24335;&#25945;&#23398;&#22330;&#26223;&#20013;&#65292;ChatGPT&#33021;&#22815;&#26377;&#25928;&#23653;&#34892;&#25945;&#23398;&#35282;&#33394;&#65292;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#23588;&#20854;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;ChatGPT&#30340;&#20986;&#29616;&#65292;&#20026;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#24102;&#26469;&#20102;&#26174;&#33879;&#30340;&#21069;&#26223;&#12290; LLM&#20855;&#26377;&#35299;&#37322;&#30693;&#35782;&#12289;&#22238;&#31572;&#38382;&#39064;&#21644;&#32771;&#34385;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#65292;&#22240;&#27492;&#20026;&#23398;&#29983;&#25552;&#20379;&#23545;&#35805;&#24335;&#25945;&#23398;&#25903;&#25345;&#12290;&#22240;&#27492;&#65292;&#26816;&#39564;LLM&#26377;&#25928;&#23653;&#34892;&#25945;&#23398;&#35282;&#33394;&#30340;&#33021;&#21147;&#65292;&#20174;&#32780;&#22312;&#23545;&#35805;&#25945;&#23398;&#22330;&#26223;&#20013;&#20419;&#36827;&#23398;&#29983;&#23398;&#20064;&#30340;&#33021;&#21147;&#65292;&#31867;&#20284;&#20110;&#20154;&#31867;&#25945;&#32946;&#32773;&#65292;&#26159;&#19968;&#20010;&#38750;&#24120;&#26377;&#20215;&#20540;&#30340;&#30740;&#31350;&#35838;&#39064;&#12290; &#26412;&#30740;&#31350;&#25307;&#21215;&#20102;34&#21517;&#26412;&#31185;&#29983;&#20316;&#20026;&#21442;&#19982;&#32773;&#65292;&#38543;&#26426;&#20998;&#20026;&#20004;&#32452;&#12290; &#23454;&#39564;&#32452;&#20351;&#29992;ChatGPT&#36827;&#34892;&#23545;&#35805;&#24335;&#25945;&#23398;&#65292;&#32780;&#25511;&#21046;&#32452;&#19982;&#20154;&#31867;&#25945;&#24072;&#20114;&#21160;&#12290; &#20004;&#32452;&#37117;&#23398;&#20064;&#20102;&#20449;&#24687;&#30456;&#20851;&#35838;&#31243;&#8220;&#25968;&#23383;&#22270;&#20687;&#8221;&#30340;&#30452;&#26041;&#22270;&#22343;&#34913;&#21333;&#20803;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16687v1 Announce Type: cross  Abstract: In recent years, the rapid development of artificial intelligence technology, especially the emergence of large language models (LLMs) such as ChatGPT, has presented significant prospects for application in the field of education. LLMs possess the capability to interpret knowledge, answer questions, and consider context, thus providing support for dialogic teaching to students. Therefore, an examination of the capacity of LLMs to effectively fulfill instructional roles, thereby facilitating student learning akin to human educators within dialogic teaching scenarios, is an exceptionally valuable research topic. This research recruited 34 undergraduate students as participants, who were randomly divided into two groups. The experimental group engaged in dialogic teaching using ChatGPT, while the control group interacted with human teachers. Both groups learned the histogram equalization unit in the information-related course "Digital Ima
&lt;/p&gt;</description></item><item><title>&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#28388;&#27844;&#12289;&#37325;&#32622;&#21644;&#24490;&#29615;&#31561;&#24314;&#27169;&#32452;&#20214;&#22312;&#24179;&#34913;&#35760;&#24518;&#20445;&#30041;&#12289;&#26102;&#38388;&#22788;&#29702;&#21644;&#21160;&#24577;&#24314;&#27169;&#26041;&#38754;&#30340;&#21151;&#33021;&#35282;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.16674</link><description>&lt;p&gt;
&#29702;&#35299;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#24314;&#27169;&#32452;&#20214;&#30340;&#21151;&#33021;&#35282;&#33394;
&lt;/p&gt;
&lt;p&gt;
Understanding the Functional Roles of Modelling Components in Spiking Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16674
&lt;/p&gt;
&lt;p&gt;
&#31995;&#32479;&#30740;&#31350;&#25581;&#31034;&#20102;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#20013;&#28388;&#27844;&#12289;&#37325;&#32622;&#21644;&#24490;&#29615;&#31561;&#24314;&#27169;&#32452;&#20214;&#22312;&#24179;&#34913;&#35760;&#24518;&#20445;&#30041;&#12289;&#26102;&#38388;&#22788;&#29702;&#21644;&#21160;&#24577;&#24314;&#27169;&#26041;&#38754;&#30340;&#21151;&#33021;&#35282;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;&#22823;&#33041;&#31070;&#32463;&#22238;&#36335;&#21551;&#21457;&#65292;&#23574;&#23792;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#22312;&#23454;&#29616;&#39640;&#35745;&#31639;&#25928;&#29575;&#21644;&#29983;&#29289;&#20445;&#30495;&#24230;&#26041;&#38754;&#24456;&#26377;&#21069;&#26223;&#12290;&#28982;&#32780;&#65292;&#20248;&#21270;SNNs&#30456;&#24403;&#22256;&#38590;&#65292;&#22240;&#20026;&#20854;&#24314;&#27169;&#32452;&#20214;&#30340;&#21151;&#33021;&#35282;&#33394;&#20173;&#19981;&#28165;&#26970;&#12290;&#36890;&#36807;&#35774;&#35745;&#21644;&#35780;&#20272;&#32463;&#20856;&#27169;&#22411;&#30340;&#20960;&#20010;&#21464;&#20307;&#65292;&#25105;&#20204;&#31995;&#32479;&#30740;&#31350;&#20102;&#28388;&#27844;&#12289;&#37325;&#32622;&#21644;&#24490;&#29615;&#36825;&#20123;&#20851;&#38190;&#24314;&#27169;&#32452;&#20214;&#22312;&#22522;&#20110;&#28431;&#31215;&#20998;&#25918;&#30005;&#65288;LIF&#65289;&#30340;SNNs&#20013;&#30340;&#21151;&#33021;&#35282;&#33394;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#36825;&#20123;&#32452;&#20214;&#22914;&#20309;&#24433;&#21709;SNNs&#30340;&#20934;&#30830;&#24615;&#12289;&#27867;&#21270;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21457;&#29616;&#28388;&#27844;&#22312;&#24179;&#34913;&#35760;&#24518;&#20445;&#30041;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#37325;&#32622;&#26426;&#21046;&#23545;&#20110;&#19981;&#38388;&#26029;&#30340;&#26102;&#38388;&#22788;&#29702;&#21644;&#35745;&#31639;&#25928;&#29575;&#33267;&#20851;&#37325;&#35201;&#65292;&#32780;&#24490;&#29615;&#21017;&#20016;&#23500;&#20102;&#27169;&#22411;&#22797;&#26434;&#21160;&#24577;&#30340;&#33021;&#21147;&#65292;&#20294;&#20250;&#25439;&#23475;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16674v1 Announce Type: cross  Abstract: Spiking neural networks (SNNs), inspired by the neural circuits of the brain, are promising in achieving high computational efficiency with biological fidelity. Nevertheless, it is quite difficult to optimize SNNs because the functional roles of their modelling components remain unclear. By designing and evaluating several variants of the classic model, we systematically investigate the functional roles of key modelling components, leakage, reset, and recurrence, in leaky integrate-and-fire (LIF) based SNNs. Through extensive experiments, we demonstrate how these components influence the accuracy, generalization, and robustness of SNNs. Specifically, we find that the leakage plays a crucial role in balancing memory retention and robustness, the reset mechanism is essential for uninterrupted temporal processing and computational efficiency, and the recurrence enriches the capability to model complex dynamics at a cost of robustness degr
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#36127;&#36131;&#20219;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#65292;&#24182;&#32435;&#20837;ESG&#29366;&#24577;&#21644;&#30446;&#26631;&#65292;&#19982;&#20462;&#25913;&#21518;&#30340;&#22343;&#20540;-&#26041;&#24046;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#22312;&#36127;&#36131;&#20219;&#25237;&#36164;&#32452;&#21512;&#20998;&#37197;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#24615;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.16667</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#22343;&#20540;-&#26041;&#24046;&#31574;&#30053;&#29992;&#20110;&#36127;&#36131;&#20219;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Reinforcement Learning and Mean-Variance Strategies for Responsible Portfolio Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16667
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#36127;&#36131;&#20219;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#65292;&#24182;&#32435;&#20837;ESG&#29366;&#24577;&#21644;&#30446;&#26631;&#65292;&#19982;&#20462;&#25913;&#21518;&#30340;&#22343;&#20540;-&#26041;&#24046;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#65292;&#32467;&#26524;&#26174;&#31034;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#22312;&#36127;&#36131;&#20219;&#25237;&#36164;&#32452;&#21512;&#20998;&#37197;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#24615;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#28041;&#21450;&#30830;&#23450;&#25237;&#36164;&#32452;&#21512;&#36164;&#20135;&#30340;&#26368;&#20339;&#37197;&#32622;&#65292;&#20197;&#23454;&#29616;&#32473;&#23450;&#30340;&#25237;&#36164;&#30446;&#26631;&#12290;&#20256;&#32479;&#19978;&#65292;&#36890;&#24120;&#20351;&#29992;&#26576;&#31181;&#24418;&#24335;&#30340;&#22343;&#20540;-&#26041;&#24046;&#20248;&#21270;&#65292;&#26088;&#22312;&#26368;&#22823;&#21270;&#22238;&#25253;&#21516;&#26102;&#26368;&#23567;&#21270;&#39118;&#38505;&#65292;&#28982;&#32780;&#65292;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#24320;&#22987;&#25506;&#32034;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#24418;&#24335;&#12290;&#25237;&#36164;&#32773;&#36234;&#26469;&#36234;&#20542;&#21521;&#20110;&#22312;&#25237;&#36164;&#20915;&#31574;&#20013;&#32435;&#20837;ESG&#30446;&#26631;&#65292;&#22240;&#27492;&#23545;&#32463;&#20856;&#22343;&#20540;-&#26041;&#24046;&#20248;&#21270;&#26694;&#26550;&#36827;&#34892;&#20102;&#20462;&#25913;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#36127;&#36131;&#20219;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#65292;&#36890;&#36807;&#32435;&#20837;ESG&#29366;&#24577;&#21644;&#30446;&#26631;&#65292;&#24182;&#38024;&#23545;&#20462;&#25913;&#21518;&#30340;&#22343;&#20540;-&#26041;&#24046;&#26041;&#27861;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31574;&#30053;&#22312;&#36127;&#36131;&#20219;&#25237;&#36164;&#32452;&#21512;&#20998;&#37197;&#26041;&#38754;&#21487;&#20197;&#25552;&#20379;&#19982;&#22343;&#20540;-&#26041;&#24046;&#26041;&#27861;&#31454;&#20105;&#24615;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16667v1 Announce Type: new  Abstract: Portfolio optimization involves determining the optimal allocation of portfolio assets in order to maximize a given investment objective. Traditionally, some form of mean-variance optimization is used with the aim of maximizing returns while minimizing risk, however, more recently, deep reinforcement learning formulations have been explored. Increasingly, investors have demonstrated an interest in incorporating ESG objectives when making investment decisions, and modifications to the classical mean-variance optimization framework have been developed. In this work, we study the use of deep reinforcement learning for responsible portfolio optimization, by incorporating ESG states and objectives, and provide comparisons against modified mean-variance approaches. Our results show that deep reinforcement learning policies can provide competitive performance against mean-variance approaches for responsible portfolio allocation across additive 
&lt;/p&gt;</description></item><item><title>&#12298;&#30561;&#32654;&#20154;&#38382;&#39064;&#12299;&#20174;&#25968;&#23398;&#35282;&#24230;&#37325;&#26032;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#21487;&#33021;&#30340;&#27010;&#29575;&#31354;&#38388;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30561;&#32654;&#20154;&#21487;&#33719;&#21462;&#20449;&#24687;&#30340;&#26631;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.16666</link><description>&lt;p&gt;
&#37325;&#26032;&#23457;&#35270;&#12298;&#30561;&#32654;&#20154;&#38382;&#39064;&#12299;
&lt;/p&gt;
&lt;p&gt;
Revisiting the Sleeping Beauty problem
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16666
&lt;/p&gt;
&lt;p&gt;
&#12298;&#30561;&#32654;&#20154;&#38382;&#39064;&#12299;&#20174;&#25968;&#23398;&#35282;&#24230;&#37325;&#26032;&#20998;&#26512;&#65292;&#30830;&#23450;&#20102;&#21487;&#33021;&#30340;&#27010;&#29575;&#31354;&#38388;&#36873;&#25321;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#30561;&#32654;&#20154;&#21487;&#33719;&#21462;&#20449;&#24687;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16666v1 &#20844;&#21578;&#31867;&#22411;:&#20132;&#21449;&#25688;&#35201;:&#12298;&#30561;&#32654;&#20154;&#38382;&#39064;&#12299;&#26159;&#19968;&#20010;&#27010;&#29575;&#35868;&#39064;&#65292;&#20108;&#21313;&#22810;&#24180;&#26469;&#27809;&#26377;&#26126;&#30830;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#35299;&#20915;&#26041;&#26696;&#22312;&#35768;&#22810;&#39046;&#22495;&#37117;&#20855;&#26377;&#26497;&#22823;&#30340;&#20852;&#36259;&#12290;&#35813;&#38382;&#39064;&#26377;&#20004;&#31181;&#20027;&#35201;&#30340;&#31454;&#20105;&#24615;&#35299;&#20915;&#26041;&#26696;&#65306;&#19968;&#21322;&#32773;&#26041;&#27861;&#21644;&#19977;&#20998;&#20043;&#19968;&#32773;&#26041;&#27861;&#12290;&#25991;&#29486;&#20013;&#24847;&#35265;&#20998;&#27495;&#30340;&#20027;&#35201;&#21407;&#22240;&#19982;&#20351;&#29992;&#19981;&#21516;&#30340;&#27010;&#29575;&#31354;&#38388;&#26469;&#34920;&#31034;&#30456;&#21516;&#30340;&#27010;&#29575;&#35868;&#39064;&#26377;&#20851;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20174;&#25968;&#23398;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#35782;&#21035;&#20102;&#30452;&#25509;&#20174;&#24605;&#32500;&#23454;&#39564;&#35268;&#21017;&#24341;&#21457;&#30340;&#27010;&#29575;&#20998;&#24067;&#12290;&#30830;&#23450;&#27010;&#29575;&#31354;&#38388;&#30340;&#31934;&#30830;&#36873;&#25321;&#20026;&#36825;&#20010;&#38382;&#39064;&#25552;&#20379;&#20102;&#19968;&#21322;&#32773;&#21644;&#19977;&#20998;&#20043;&#19968;&#32773;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#20026;&#20102;&#23581;&#35797;&#20915;&#23450;&#24212;&#35813;&#36873;&#25321;&#21738;&#31181;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#28041;&#21450;&#21040;&#30561;&#32654;&#20154;&#21487;&#33719;&#24471;&#30340;&#20449;&#24687;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16666v1 Announce Type: cross  Abstract: The Sleeping Beauty problem is a probability riddle with no definite solution for more than two decades and its solution is of great interest in many fields of knowledge. There are two main competing solutions to the problem: the halfer approach, and the thirder approach. The main reason for disagreement in the literature is connected to the use of different probability spaces to represent the same probabilistic riddle. In this work, we analyse the problem from a mathematical perspective, identifying probability distributions induced directly from the thought experiment's rules. The precise choices of probability spaces provide both halfer and thirder solutions to the problem. To try and decide on which approach to follow, a criterion involving the information available to Sleeping Beauty is proposed.
&lt;/p&gt;</description></item><item><title>CLHA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#37325;&#35780;&#20998;&#31574;&#30053;&#21644;&#25439;&#22833;&#20989;&#25968;&#35843;&#25972;&#65292;&#22312;&#25552;&#21319;&#23545;&#40784;&#25928;&#26524;&#30340;&#21516;&#26102;&#31616;&#21270;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2403.16649</link><description>&lt;p&gt;
CLHA: &#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29992;&#20110;&#20154;&#31867;&#23545;&#40784;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
CLHA: A Simple yet Effective Contrastive Learning Framework for Human Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16649
&lt;/p&gt;
&lt;p&gt;
CLHA&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#24110;&#21161;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#36890;&#36807;&#26032;&#39062;&#30340;&#37325;&#35780;&#20998;&#31574;&#30053;&#21644;&#25439;&#22833;&#20989;&#25968;&#35843;&#25972;&#65292;&#22312;&#25552;&#21319;&#23545;&#40784;&#25928;&#26524;&#30340;&#21516;&#26102;&#31616;&#21270;&#20102;&#35757;&#32451;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#30340;&#24378;&#21270;&#23398;&#20064;&#26159;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#30340;&#20851;&#38190;&#25216;&#26415;&#65292;&#30830;&#20445;&#36825;&#20123;LLMs&#20197;&#23545;&#29992;&#25143;&#26377;&#30410;&#19988;&#26131;&#20110;&#29702;&#35299;&#30340;&#26041;&#24335;&#34892;&#20026;&#12290;&#28982;&#32780;&#65292;&#22522;&#20110;RL&#30340;&#20154;&#31867;&#23545;&#40784;&#25216;&#26415;&#20013;&#23384;&#22312;&#30340;&#38271;&#26399;&#25361;&#25112;&#22312;&#20110;&#20854;&#22266;&#26377;&#30340;&#22797;&#26434;&#24615;&#21644;&#35757;&#32451;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#29992;&#20110;&#20154;&#31867;&#23545;&#40784;&#30340;&#23545;&#27604;&#23398;&#20064;&#26694;&#26550;&#65288;CLHA&#65289;&#65292;&#30452;&#25509;&#23558;LLMs&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#12290;CLHA&#37319;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#37325;&#35780;&#20998;&#31574;&#30053;&#26469;&#35780;&#20272;&#25968;&#25454;&#20869;&#30340;&#22122;&#22768;&#65292;&#32771;&#34385;&#20854;&#22266;&#26377;&#36136;&#37327;&#24182;&#21160;&#24577;&#35843;&#25972;&#35757;&#32451;&#36807;&#31243;&#12290;&#21516;&#26102;&#65292;CLHA&#21033;&#29992;&#25104;&#23545;&#23545;&#27604;&#25439;&#22833;&#21644;&#33258;&#36866;&#24212;&#30417;&#30563;&#24494;&#35843;&#25439;&#22833;&#26469;&#33258;&#36866;&#24212;&#22320;&#20462;&#25913;&#29983;&#25104;&#21709;&#24212;&#30340;&#21487;&#33021;&#24615;&#65292;&#30830;&#20445;&#19982;&#20154;&#31867;&#20559;&#22909;&#30340;&#22686;&#24378;&#23545;&#40784;&#12290;&#20351;&#29992;&#20808;&#36827;&#26041;&#27861;&#65292;CLHA&#36229;&#36234;&#20102;&#20854;&#20182;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16649v1 Announce Type: new  Abstract: Reinforcement learning from human feedback (RLHF) is a crucial technique in aligning large language models (LLMs) with human preferences, ensuring these LLMs behave in beneficial and comprehensible ways to users. However, a longstanding challenge in human alignment techniques based on reinforcement learning lies in their inherent complexity and difficulty in training. To address this challenge, we present a simple yet effective Contrastive Learning Framework for Human Alignment (CLHA) to align LLMs with human preferences directly. CLHA employs a novel rescoring strategy to evaluate the noise within the data by considering its inherent quality and dynamically adjusting the training process. Simultaneously, CLHA utilizes pairwise contrastive loss and adaptive supervised fine-tuning loss to adaptively modify the likelihood of generating responses, ensuring enhanced alignment with human preferences. Using advanced methods, CLHA surpasses oth
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16591</link><description>&lt;p&gt;
&#25581;&#31034;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Deciphering the Interplay between Local Differential Privacy, Average Bayesian Privacy, and Maximum Bayesian Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16591
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25506;&#35752;&#20102;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#12289;&#36125;&#21494;&#26031;&#38544;&#31169;&#21450;&#20854;&#20043;&#38388;&#30340;&#30456;&#20114;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#31361;&#20986;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#36805;&#36895;&#21457;&#23637;&#23548;&#33268;&#20102;&#38544;&#31169;&#23450;&#20041;&#30340;&#22810;&#26679;&#21270;&#65292;&#30001;&#20110;&#23545;&#38544;&#31169;&#26500;&#25104;&#30340;&#23041;&#32961;&#65292;&#21253;&#25324;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#65288;LDP&#65289;&#30340;&#27010;&#24565;&#12290;&#34429;&#28982;&#34987;&#24191;&#27867;&#25509;&#21463;&#24182;&#22312;&#35768;&#22810;&#39046;&#22495;&#20013;&#34987;&#21033;&#29992;&#65292;&#20294;&#36825;&#31181;&#20256;&#32479;&#30340;&#38544;&#31169;&#27979;&#37327;&#26041;&#27861;&#20173;&#28982;&#23384;&#22312;&#19968;&#23450;&#38480;&#21046;&#65292;&#20174;&#26080;&#27861;&#38450;&#27490;&#25512;&#26029;&#25259;&#38706;&#21040;&#32570;&#20047;&#23545;&#23545;&#25163;&#32972;&#26223;&#30693;&#35782;&#30340;&#32771;&#34385;&#12290;&#22312;&#36825;&#39033;&#20840;&#38754;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#36125;&#21494;&#26031;&#38544;&#31169;&#24182;&#28145;&#20837;&#25506;&#35752;&#26412;&#22320;&#24046;&#20998;&#38544;&#31169;&#21644;&#20854;&#36125;&#21494;&#26031;&#23545;&#24212;&#29289;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#65292;&#25581;&#31034;&#20102;&#20851;&#20110;&#25928;&#29992;-&#38544;&#31169;&#26435;&#34913;&#30340;&#26032;&#35265;&#35299;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#27010;&#25324;&#20102;&#25915;&#20987;&#21644;&#38450;&#24481;&#31574;&#30053;&#65292;&#31361;&#20986;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#21644;&#25928;&#26524;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#36129;&#29486;&#22522;&#20110;&#24179;&#22343;&#36125;&#21494;&#26031;&#38544;&#31169;&#65288;ABP&#65289;&#21644;&#26368;&#22823;&#36125;&#21494;&#26031;&#38544;&#31169;&#20043;&#38388;&#30340;&#20005;&#26684;&#23450;&#20041;&#21644;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16591v1 Announce Type: cross  Abstract: The swift evolution of machine learning has led to emergence of various definitions of privacy due to the threats it poses to privacy, including the concept of local differential privacy (LDP). Although widely embraced and utilized across numerous domains, this conventional approach to measure privacy still exhibits certain limitations, spanning from failure to prevent inferential disclosure to lack of consideration for the adversary's background knowledge. In this comprehensive study, we introduce Bayesian privacy and delve into the intricate relationship between local differential privacy and its Bayesian counterparts, unveiling novel insights into utility-privacy trade-offs. We introduce a framework that encapsulates both attack and defense strategies, highlighting their interplay and effectiveness. Our theoretical contributions are anchored in the rigorous definitions and relationships between Average Bayesian Privacy (ABP) and Max
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#21516;&#26102;&#36873;&#25321;&#34701;&#21512;&#31574;&#30053;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#23545;&#20316;&#29289;&#20998;&#31867;&#20855;&#26377;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.16582</link><description>&lt;p&gt;
&#22312;&#21033;&#29992;&#20840;&#29699;&#36965;&#24863;&#25968;&#25454;&#36827;&#34892;&#20316;&#29289;&#20998;&#31867;&#30340;&#22810;&#35270;&#22270;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#20339;&#36873;&#25321;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
In the Search for Optimal Multi-view Learning Models for Crop Classification with Global Remote Sensing Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16582
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#21516;&#26102;&#36873;&#25321;&#34701;&#21512;&#31574;&#30053;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#23545;&#20316;&#29289;&#20998;&#31867;&#20855;&#26377;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#29289;&#20998;&#31867;&#22312;&#30740;&#31350;&#20316;&#29289;&#27169;&#24335;&#21464;&#21270;&#12289;&#36164;&#28304;&#31649;&#29702;&#21644;&#30899;&#22266;&#23384;&#20013;&#20855;&#26377;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#37319;&#29992;&#25968;&#25454;&#39537;&#21160;&#25216;&#26415;&#36827;&#34892;&#39044;&#27979;&#26102;&#65292;&#21033;&#29992;&#21508;&#31181;&#26102;&#38388;&#25968;&#25454;&#28304;&#26159;&#24517;&#35201;&#30340;&#12290;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#24050;&#34987;&#35777;&#26126;&#23545;&#23558;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#26144;&#23556;&#21040;&#39640;&#32423;&#34920;&#31034;&#20197;&#36827;&#34892;&#39044;&#27979;&#20219;&#21153;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#24403;&#22788;&#29702;&#22810;&#20010;&#36755;&#20837;&#27169;&#24335;&#26102;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#25991;&#29486;&#23545;&#22810;&#35270;&#22270;&#23398;&#20064;&#65288;MVL&#65289;&#22330;&#26223;&#25552;&#20379;&#20102;&#26377;&#38480;&#30340;&#25351;&#23548;&#65292;&#20027;&#35201;&#38598;&#20013;&#22312;&#25506;&#32034;&#20855;&#26377;&#29305;&#23450;&#32534;&#30721;&#22120;&#30340;&#34701;&#21512;&#31574;&#30053;&#65292;&#24182;&#22312;&#23616;&#37096;&#22320;&#21306;&#23545;&#20854;&#36827;&#34892;&#39564;&#35777;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#23545;&#20892;&#30000;&#22303;&#22320;&#21644;&#20316;&#29289;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#26102;&#21516;&#26102;&#36873;&#25321;&#34701;&#21512;&#31574;&#30053;&#21644;&#32534;&#30721;&#22120;&#26550;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16582v1 Announce Type: cross  Abstract: Crop classification is of critical importance due to its role in studying crop pattern changes, resource management, and carbon sequestration. When employing data-driven techniques for its prediction, utilizing various temporal data sources is necessary. Deep learning models have proven to be effective for this task by mapping time series data to high-level representation for prediction. However, they face substantial challenges when dealing with multiple input patterns. The literature offers limited guidance for Multi-View Learning (MVL) scenarios, as it has primarily focused on exploring fusion strategies with specific encoders and validating them in local regions. In contrast, we investigate the impact of simultaneous selection of the fusion strategy and the encoder architecture evaluated on a global-scale cropland and crop-type classifications. We use a range of five fusion strategies (Input, Feature, Decision, Ensemble, Hybrid) an
&lt;/p&gt;</description></item><item><title>SegICL&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#21106;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26032;&#20219;&#21153;&#20013;&#36866;&#24212;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#25110;&#36827;&#34892;&#22797;&#26434;&#24494;&#35843;&#12290;</title><link>https://arxiv.org/abs/2403.16578</link><description>&lt;p&gt;
SegICL&#65306;&#19968;&#31181;&#29992;&#20110;&#22686;&#24378;&#21307;&#23398;&#25104;&#20687;&#20998;&#21106;&#30340;&#36890;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
SegICL: A Universal In-context Learning Framework for Enhanced Segmentation in Medical Imaging
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16578
&lt;/p&gt;
&lt;p&gt;
SegICL&#24341;&#20837;&#20102;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22270;&#20687;&#20998;&#21106;&#26032;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26032;&#20219;&#21153;&#20013;&#36866;&#24212;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26080;&#38656;&#20174;&#22836;&#35757;&#32451;&#27169;&#22411;&#25110;&#36827;&#34892;&#22797;&#26434;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19978;&#19979;&#25991;&#23398;&#20064;&#20197;&#26032;&#20219;&#21153;&#20013;&#36866;&#24212;&#30340;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#27169;&#22411;&#26159;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#36827;&#23637;&#12290;&#36890;&#29992;&#20998;&#21106;&#27169;&#22411;&#26088;&#22312;&#27178;&#36328;&#21307;&#23398;&#22270;&#20687;&#30340;&#19981;&#21516;&#27169;&#24577;&#36827;&#34892;&#27010;&#25324;&#65292;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25928;&#26524;&#22312;&#24212;&#29992;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#65288;OOD&#65289;&#25968;&#25454;&#27169;&#24577;&#21644;&#20219;&#21153;&#26102;&#36890;&#24120;&#20250;&#20943;&#24369;&#65292;&#38656;&#35201;&#23545;&#27169;&#22411;&#36827;&#34892;&#22797;&#26434;&#24494;&#35843;&#20197;&#33719;&#24471;&#26368;&#20339;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;SegICL&#65292;&#19968;&#31181;&#21033;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#36827;&#34892;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#19981;&#21516;&#65292;SegICL&#33021;&#22815;&#21033;&#29992;&#25991;&#26412;&#24341;&#23548;&#20998;&#21106;&#24182;&#20351;&#29992;&#19968;&#23567;&#32452;&#22270;&#20687;-&#25513;&#30721;&#23545;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#65292;&#28040;&#38500;&#20102;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#27169;&#22411;&#25110;&#20026;OOD&#20219;&#21153;&#65288;&#21253;&#25324;OOD&#27169;&#24577;&#21644;&#25968;&#25454;&#38598;&#65289;&#36827;&#34892;&#24494;&#35843;&#30340;&#38656;&#35201;&#12290;SegICL&#30340;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#34920;&#26126;&#65292;&#25552;&#31034;&#31034;&#20363;&#25968;&#37327;&#19982;&#20998;&#21106;&#20043;&#38388;&#23384;&#22312;&#27491;&#30456;&#20851;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16578v1 Announce Type: cross  Abstract: Medical image segmentation models adapting to new tasks in a training-free manner through in-context learning is an exciting advancement. Universal segmentation models aim to generalize across the diverse modality of medical images, yet their effectiveness often diminishes when applied to out-of-distribution (OOD) data modalities and tasks, requiring intricate fine-tuning of model for optimal performance. For addressing this challenge, we introduce SegICL, a novel approach leveraging In-Context Learning (ICL) for image segmentation. Unlike existing methods, SegICL has the capability to employ text-guided segmentation and conduct in-context learning with a small set of image-mask pairs, eliminating the need for training the model from scratch or fine-tuning for OOD tasks (including OOD modality and dataset). Extensive experimental validation of SegICL demonstrates a positive correlation between the number of prompt samples and segmentat
&lt;/p&gt;</description></item><item><title>NSINA&#26159;&#20026;&#35299;&#20915;&#20711;&#20285;&#32599;&#35821;&#20013;LLMs&#36866;&#24212;&#24615;&#25361;&#25112;&#32780;&#24341;&#20837;&#30340;&#26368;&#22823;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#20026;&#25913;&#36827;&#35813;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25552;&#20379;&#20102;&#23453;&#36149;&#36164;&#28304;&#21644;&#22522;&#20934;&#12290;</title><link>https://arxiv.org/abs/2403.16571</link><description>&lt;p&gt;
NSINA&#65306;&#29992;&#20110;&#20711;&#20285;&#32599;&#35821;&#30340;&#26032;&#38395;&#35821;&#26009;&#24211;
&lt;/p&gt;
&lt;p&gt;
NSINA: A News Corpus for Sinhala
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16571
&lt;/p&gt;
&lt;p&gt;
NSINA&#26159;&#20026;&#35299;&#20915;&#20711;&#20285;&#32599;&#35821;&#20013;LLMs&#36866;&#24212;&#24615;&#25361;&#25112;&#32780;&#24341;&#20837;&#30340;&#26368;&#22823;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#20026;&#25913;&#36827;&#35813;&#35821;&#35328;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25552;&#20379;&#20102;&#23453;&#36149;&#36164;&#28304;&#21644;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24341;&#20837;&#25512;&#21160;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#21457;&#23637;&#65292;&#20294;&#23427;&#20204;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#21462;&#20915;&#20110;&#39044;&#35757;&#32451;&#36164;&#28304;&#12290;&#23588;&#20854;&#26159;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#65288;&#22914;&#20711;&#20285;&#32599;&#35821;&#65289;&#20013;&#65292;&#36825;&#19968;&#28857;&#23588;&#20026;&#26126;&#26174;&#65292;&#22240;&#20026;&#23427;&#20204;&#38754;&#20020;&#30528;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#32570;&#20047;&#20805;&#36275;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26377;&#38480;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;NSINA&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#25324;&#26469;&#33258;&#28909;&#38376;&#20711;&#20285;&#32599;&#35821;&#26032;&#38395;&#32593;&#31449;&#30340;50&#19975;&#22810;&#31687;&#25991;&#31456;&#30340;&#20840;&#38754;&#26032;&#38395;&#35821;&#26009;&#24211;&#65292;&#20197;&#21450;&#19977;&#39033;NLP&#20219;&#21153;&#65306;&#26032;&#38395;&#23186;&#20307;&#35782;&#21035;&#12289;&#26032;&#38395;&#31867;&#21035;&#39044;&#27979;&#21644;&#26032;&#38395;&#26631;&#39064;&#29983;&#25104;&#12290;NSINA&#30340;&#21457;&#24067;&#26088;&#22312;&#20026;&#36866;&#24212;&#20711;&#20285;&#32599;&#35821;&#30340;LLMs&#24102;&#26469;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#21644;&#29992;&#20110;&#25913;&#36827;&#20711;&#20285;&#32599;&#35821;NLP&#30340;&#22522;&#20934;&#12290;NSINA&#26159;&#36804;&#20170;&#20026;&#27490;&#26368;&#22823;&#30340;&#20711;&#20285;&#32599;&#35821;&#26032;&#38395;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16571v1 Announce Type: cross  Abstract: The introduction of large language models (LLMs) has advanced natural language processing (NLP), but their effectiveness is largely dependent on pre-training resources. This is especially evident in low-resource languages, such as Sinhala, which face two primary challenges: the lack of substantial training data and limited benchmarking datasets. In response, this study introduces NSINA, a comprehensive news corpus of over 500,000 articles from popular Sinhala news websites, along with three NLP tasks: news media identification, news category prediction, and news headline generation. The release of NSINA aims to provide a solution to challenges in adapting LLMs to Sinhala, offering valuable resources and benchmarks for improving NLP in the Sinhala language. NSINA is the largest news corpus for Sinhala, available up to date.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FedFixer&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#26500;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20010;&#24615;&#21270;&#27169;&#22411;&#19982;&#20840;&#23616;&#27169;&#22411;&#21512;&#20316;&#26469;&#26377;&#25928;&#36873;&#25321;&#24178;&#20928;&#30340;&#23458;&#25143;&#31471;&#29305;&#23450;&#26679;&#26412;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#27491;&#21017;&#21270;&#22120;&#21644;&#22522;&#20110;&#26679;&#26412;&#20849;&#20139;&#30340;&#21452;&#27169;&#22411;&#26356;&#26032;&#31574;&#30053;&#26469;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.16561</link><description>&lt;p&gt;
FedFixer&#65306;&#22312;&#32852;&#37030;&#23398;&#20064;&#20013;&#32531;&#35299;&#24322;&#26500;&#26631;&#31614;&#22122;&#22768;
&lt;/p&gt;
&lt;p&gt;
FedFixer: Mitigating Heterogeneous Label Noise in Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16561
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FedFixer&#26469;&#35299;&#20915;&#32852;&#37030;&#23398;&#20064;&#20013;&#24322;&#26500;&#26631;&#31614;&#22122;&#22768;&#30340;&#38382;&#39064;&#65292;&#24341;&#20837;&#20010;&#24615;&#21270;&#27169;&#22411;&#19982;&#20840;&#23616;&#27169;&#22411;&#21512;&#20316;&#26469;&#26377;&#25928;&#36873;&#25321;&#24178;&#20928;&#30340;&#23458;&#25143;&#31471;&#29305;&#23450;&#26679;&#26412;&#65292;&#36890;&#36807;&#32622;&#20449;&#24230;&#27491;&#21017;&#21270;&#22120;&#21644;&#22522;&#20110;&#26679;&#26412;&#20849;&#20139;&#30340;&#21452;&#27169;&#22411;&#26356;&#26032;&#31574;&#30053;&#26469;&#20943;&#36731;&#36807;&#25311;&#21512;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;(FL)&#22312;&#24615;&#33021;&#19978;&#20005;&#37325;&#20381;&#36182;&#26631;&#31614;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#20010;&#20307;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#26631;&#31614;&#20998;&#24067;&#36890;&#24120;&#21516;&#26102;&#23384;&#22312;&#22122;&#22768;&#21644;&#24322;&#26500;&#24615;&#12290;&#22312;&#24322;&#26500;&#26631;&#31614;&#22122;&#22768;&#20013;&#30001;&#23458;&#25143;&#31471;&#29305;&#23450;&#26679;&#26412;&#24341;&#36215;&#30340;&#39640;&#25439;&#22833;&#23545;&#21306;&#20998;&#23458;&#25143;&#31471;&#29305;&#23450;&#21644;&#22024;&#26434;&#26631;&#31614;&#26679;&#26412;&#26500;&#25104;&#20102;&#25361;&#25112;&#65292;&#24433;&#21709;&#20102;&#29616;&#26377;&#26631;&#31614;&#22122;&#22768;&#23398;&#20064;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedFixer&#65292;&#20854;&#20013;&#24341;&#20837;&#20102;&#20010;&#24615;&#21270;&#27169;&#22411;&#19982;&#20840;&#23616;&#27169;&#22411;&#21512;&#20316;&#65292;&#20197;&#26377;&#25928;&#36873;&#25321;&#24178;&#20928;&#30340;&#23458;&#25143;&#31471;&#29305;&#23450;&#26679;&#26412;&#12290;&#22312;&#21452;&#27169;&#22411;&#20013;&#65292;&#20165;&#22312;&#26412;&#22320;&#32423;&#21035;&#26356;&#26032;&#20010;&#24615;&#21270;&#27169;&#22411;&#21487;&#33021;&#23548;&#33268;&#30001;&#20110;&#26679;&#26412;&#26377;&#38480;&#32780;&#23545;&#22122;&#22768;&#25968;&#25454;&#36807;&#24230;&#25311;&#21512;&#65292;&#36827;&#32780;&#24433;&#21709;&#23616;&#37096;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#20026;&#20943;&#36731;&#36807;&#25311;&#21512;&#65292;&#25105;&#20204;&#20174;&#20004;&#20010;&#35282;&#24230;&#35299;&#20915;&#20102;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16561v1 Announce Type: cross  Abstract: Federated Learning (FL) heavily depends on label quality for its performance. However, the label distribution among individual clients is always both noisy and heterogeneous. The high loss incurred by client-specific samples in heterogeneous label noise poses challenges for distinguishing between client-specific and noisy label samples, impacting the effectiveness of existing label noise learning approaches. To tackle this issue, we propose FedFixer, where the personalized model is introduced to cooperate with the global model to effectively select clean client-specific samples. In the dual models, updating the personalized model solely at a local level can lead to overfitting on noisy data due to limited samples, consequently affecting both the local and global models' performance. To mitigate overfitting, we address this concern from two perspectives. Firstly, we employ a confidence regularizer to alleviate the impact of unconfident 
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Poincar&#233;&#35299;&#37322;&#26041;&#27861;&#22312;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^2logn)&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#25237;&#24433;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#23618;&#27425;&#32858;&#31867;&#36807;&#31243;&#21487;&#20197;&#35270;&#20026;&#26500;&#24314;&#26368;&#23567;&#29983;&#25104;&#26641;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#26377;&#25928;&#30340;&#31639;&#27861;</title><link>https://arxiv.org/abs/2403.16554</link><description>&lt;p&gt;
PE&#65306;&#19968;&#31181;&#29992;&#20110;&#24555;&#36895;&#25991;&#26412;&#23618;&#27425;&#29983;&#25104;&#30340;Poincar&#233;&#35299;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PE: A Poincare Explanation Method for Fast Text Hierarchy Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16554
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;Poincar&#233;&#35299;&#37322;&#26041;&#27861;&#22312;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#30340;&#26032;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(n^2logn)&#30340;&#26694;&#26550;&#65292;&#35777;&#26126;&#20102;&#22312;&#25237;&#24433;&#31354;&#38388;&#20013;&#36827;&#34892;&#30340;&#23618;&#27425;&#32858;&#31867;&#36807;&#31243;&#21487;&#20197;&#35270;&#20026;&#26500;&#24314;&#26368;&#23567;&#29983;&#25104;&#26641;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#26377;&#25928;&#30340;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16554v1 &#20844;&#21578;&#31867;&#22411;: cross &#25688;&#35201;: NLP&#20013;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#29305;&#24615;&#38459;&#30861;&#20102;&#23427;&#20204;&#30340;&#24191;&#27867;&#24212;&#29992;&#12290;&#30740;&#31350;&#37325;&#28857;&#24050;&#32463;&#36716;&#31227;&#21040;&#23618;&#27425;&#23646;&#24615;&#65288;HA&#65289;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#20351;&#29992;&#27431;&#20960;&#37324;&#24471;&#31354;&#38388;&#20013;&#32791;&#26102;&#30340;&#36138;&#23146;&#25628;&#32034;&#26469;&#24314;&#27169;&#38750;&#36830;&#32493;&#32452;&#21512;&#65292;&#24573;&#30053;&#20102;&#29305;&#24449;&#34920;&#31034;&#20013;&#28508;&#22312;&#30340;&#35821;&#35328;&#20449;&#24687;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#21363;Poincar&#233;&#35299;&#37322;&#65288;PE&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;&#36229;&#20960;&#20309;&#31354;&#38388;&#24314;&#27169;&#29305;&#24449;&#20132;&#20114;&#20316;&#29992;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$O(n^2logn)$&#12290;&#21463;Poincar&#233;&#27169;&#22411;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#23884;&#20837;&#25237;&#24433;&#21040;&#36229;&#20960;&#20309;&#31354;&#38388;&#20013;&#65292;&#36825;&#23637;&#31034;&#20986;&#26356;&#22909;&#30340;&#23545;&#21477;&#27861;&#21644;&#35821;&#20041;&#23618;&#27425;&#32467;&#26500;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25237;&#24433;&#31354;&#38388;&#20013;&#30340;&#23618;&#27425;&#32858;&#31867;&#36807;&#31243;&#21487;&#20197;&#34987;&#35270;&#20026;&#26500;&#24314;&#26368;&#23567;&#29983;&#25104;&#26641;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26102;&#38388;&#26377;&#25928;&#30340;&#31639;&#27861;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16554v1 Announce Type: cross  Abstract: The black-box nature of deep learning models in NLP hinders their widespread application. The research focus has shifted to Hierarchical Attribution (HA) for its ability to model feature interactions. Recent works model non-contiguous combinations with a time-costly greedy search in Eculidean spaces, neglecting underlying linguistic information in feature representations. In this work, we introduce a novel method, namely Poincar\'e Explanation (PE), for modeling feature interactions using hyperbolic spaces in an $O(n^2logn)$ time complexity. Inspired by Poincar\'e model, we propose a framework to project the embeddings into hyperbolic spaces, which exhibit better inductive biases for syntax and semantic hierarchical structures. Eventually, we prove that the hierarchical clustering process in the projected space could be viewed as building a minimum spanning tree and propose a time efficient algorithm. Experimental results demonstrate t
&lt;/p&gt;</description></item><item><title>QKFormer&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#33033;&#20914;&#24418;&#24335;Q-K&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#20998;&#23618;&#32467;&#26500;&#21644;&#34917;&#19969;&#23884;&#20837;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#33033;&#20914;&#21464;&#21387;&#22120;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16552</link><description>&lt;p&gt;
QKFormer: &#20351;&#29992;Q-K&#27880;&#24847;&#21147;&#30340;&#20998;&#23618;&#33033;&#20914;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
QKFormer: Hierarchical Spiking Transformer using Q-K Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16552
&lt;/p&gt;
&lt;p&gt;
QKFormer&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#33033;&#20914;&#24418;&#24335;Q-K&#27880;&#24847;&#21147;&#26426;&#21046;&#12289;&#20998;&#23618;&#32467;&#26500;&#21644;&#34917;&#19969;&#23884;&#20837;&#27169;&#22359;&#65292;&#20197;&#25552;&#39640;&#33033;&#20914;&#21464;&#21387;&#22120;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#21464;&#21387;&#22120;&#23558;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNNs&#65289;&#19982;&#21464;&#21387;&#22120;&#26550;&#26500;&#30456;&#32467;&#21512;&#65292;&#30001;&#20110;&#20854;&#33410;&#33021;&#39640;&#24615;&#33021;&#30340;&#28508;&#21147;&#65292;&#21560;&#24341;&#20102;&#24456;&#22810;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#35813;&#39046;&#22495;&#29616;&#26377;&#27169;&#22411;&#20173;&#28982;&#23384;&#22312;&#24615;&#33021;&#19981;&#20339;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#24615;&#33021;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#20960;&#39033;&#21019;&#26032;&#65306;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;SNNs&#37327;&#36523;&#23450;&#21046;&#30340;&#26032;&#22411;&#33033;&#20914;&#24418;&#24335;Q-K&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#36890;&#36807;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24615;&#30340;&#20108;&#36827;&#21046;&#21521;&#37327;&#26377;&#25928;&#22320;&#24314;&#27169;&#20196;&#29260;&#25110;&#36890;&#36947;&#32500;&#24230;&#30340;&#37325;&#35201;&#24615;&#12290;ii&#65289;&#25105;&#20204;&#23558;&#20855;&#26377;&#26174;&#33879;&#24615;&#33021;&#20248;&#21183;&#30340;&#20998;&#23618;&#32467;&#26500;&#24341;&#20837;&#33033;&#20914;&#21464;&#21387;&#22120;&#65292;&#20174;&#32780;&#33719;&#24471;&#22810;&#23610;&#24230;&#33033;&#20914;&#34920;&#31034;&#65292;&#36825;&#23545;&#22823;&#33041;&#21644;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#37117;&#26377;&#26174;&#30528;&#22909;&#22788;&#12290;iii&#65289;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#36890;&#29992;&#19988;&#24378;&#22823;&#30340;&#34917;&#19969;&#23884;&#20837;&#27169;&#22359;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#19968;&#20010;&#19987;&#38376;&#20026;&#33033;&#20914;&#21464;&#21387;&#22120;&#35774;&#35745;&#30340;&#21464;&#24418;&#24555;&#25463;&#26041;&#24335;&#12290;&#24635;&#20043;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;QKFormer&#65292;&#19968;&#31181;&#20998;&#23618;&#33033;&#20914;&#21464;&#21387;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16552v1 Announce Type: cross  Abstract: Spiking Transformers, which integrate Spiking Neural Networks (SNNs) with Transformer architectures, have attracted significant attention due to their potential for energy efficiency and high performance. However, existing models in this domain still suffer from suboptimal performance. We introduce several innovations to improve the performance: i) We propose a novel spike-form Q-K attention mechanism, tailored for SNNs, which efficiently models the importance of token or channel dimensions through binary vectors with linear complexity. ii) We incorporate the hierarchical structure, which significantly benefits the performance of both the brain and artificial neural networks, into spiking transformers to obtain multi-scale spiking representation. iii) We design a versatile and powerful patch embedding module with a deformed shortcut specifically for spiking transformers. Together, we develop QKFormer, a hierarchical spiking transformer
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20174;&#22810;&#20010;&#21477;&#23376;&#34920;&#31034;&#20013;&#25552;&#21462;&#20114;&#34917;&#30340;&#21028;&#21035;&#20449;&#24687;&#65292;&#25552;&#39640;&#23569;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;&#20013;&#30340;&#20449;&#24687;&#25552;&#21462;&#25928;&#29575;</title><link>https://arxiv.org/abs/2403.16543</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#34920;&#31034;&#23398;&#20064;&#25552;&#39640;&#23569;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;&#20013;&#30340;&#39640;&#25928;&#20449;&#24687;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Efficient Information Extraction in Few-Shot Relation Classification through Contrastive Representation Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16543
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#23398;&#20064;&#20174;&#22810;&#20010;&#21477;&#23376;&#34920;&#31034;&#20013;&#25552;&#21462;&#20114;&#34917;&#30340;&#21028;&#21035;&#20449;&#24687;&#65292;&#25552;&#39640;&#23569;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;&#20013;&#30340;&#20449;&#24687;&#25552;&#21462;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26377;&#38480;&#26631;&#35760;&#23454;&#20363;&#19979;&#30340;&#20851;&#31995;&#20998;&#31867;&#20013;&#21306;&#20998;&#23454;&#20307;&#23545;&#20043;&#38388;&#30340;&#20851;&#31995;&#26500;&#25104;&#23569;&#26679;&#26412;&#20851;&#31995;&#20998;&#31867;&#20013;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#25991;&#26412;&#25968;&#25454;&#30340;&#34920;&#31034;&#25552;&#21462;&#20102;&#36328;&#39046;&#22495;&#12289;&#23454;&#20307;&#21644;&#20851;&#31995;&#30340;&#20016;&#23500;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#32467;&#21512;&#22810;&#20010;&#21477;&#23376;&#34920;&#31034;&#21644;&#23545;&#27604;&#23398;&#20064;&#20197;&#22686;&#24378;&#20449;&#24687;&#25552;&#21462;&#30340;&#26032;&#26041;&#27861;&#12290;&#23613;&#31649;&#20851;&#31995;&#20998;&#31867;&#20013;&#36890;&#24120;&#20351;&#29992;&#23454;&#20307;&#26631;&#35760;&#20196;&#29260;&#25552;&#21462;&#34920;&#31034;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#27169;&#22411;&#20869;&#37096;&#34920;&#31034;&#20013;&#23384;&#22312;&#22823;&#37327;&#26410;&#34987;&#21033;&#29992;&#30340;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#40784;&#22810;&#20010;&#21477;&#23376;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#22914;[CLS]&#20196;&#29260;&#12289;&#25552;&#31034;&#20013;&#20351;&#29992;&#30340;[MASK]&#20196;&#29260;&#21644;&#23454;&#20307;&#26631;&#35760;&#20196;&#29260;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#23545;&#27604;&#23398;&#20064;&#20174;&#36825;&#20123;&#20010;&#20307;&#34920;&#31034;&#20013;&#25552;&#21462;&#20114;&#34917;&#30340;&#21028;&#21035;&#20449;&#24687;&#12290;&#36825;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#29305;&#21035;&#30456;&#20851;&#65292;&#20854;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16543v1 Announce Type: cross  Abstract: Differentiating relationships between entity pairs with limited labeled instances poses a significant challenge in few-shot relation classification. Representations of textual data extract rich information spanning the domain, entities, and relations. In this paper, we introduce a novel approach to enhance information extraction combining multiple sentence representations and contrastive learning. While representations in relation classification are commonly extracted using entity marker tokens, we argue that substantial information within the internal model representations remains untapped. To address this, we propose aligning multiple sentence representations, such as the [CLS] token, the [MASK] token used in prompting, and entity marker tokens. Our method employs contrastive learning to extract complementary discriminative information from these individual representations. This is particularly relevant in low-resource settings where
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#19968;&#31181;&#20013;&#38388;&#34701;&#21512;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#20302;&#31209;&#25991;&#26412;&#21040;&#22270;&#20687;&#27880;&#24847;&#21147;&#35745;&#31639;&#26469;&#25552;&#39640;&#35757;&#32451;&#21644;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.16530</link><description>&lt;p&gt;
&#19968;&#31181;&#20013;&#38388;&#34701;&#21512;&#30340;ViT&#22312;&#25193;&#25955;&#27169;&#22411;&#20013;&#23454;&#29616;&#20102;&#25991;&#26412;-&#22270;&#20687;&#30340;&#39640;&#25928;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
An Intermediate Fusion ViT Enables Efficient Text-Image Alignment in Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16530
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#19968;&#31181;&#20013;&#38388;&#34701;&#21512;&#30340;&#31574;&#30053;&#65292;&#21487;&#20197;&#25552;&#39640;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#30340;&#29983;&#25104;&#36136;&#37327;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#20302;&#31209;&#25991;&#26412;&#21040;&#22270;&#20687;&#27880;&#24847;&#21147;&#35745;&#31639;&#26469;&#25552;&#39640;&#35757;&#32451;&#21644;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25193;&#25955;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#26465;&#20214;&#25968;&#25454;&#36328;&#27169;&#24577;&#29983;&#25104;&#20219;&#21153;&#65292;&#22914;&#25991;&#26412;&#21040;&#22270;&#20687;&#21644;&#25991;&#26412;&#21040;&#35270;&#39057;&#12290;&#28982;&#32780;&#65292;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#20173;&#28982;&#26410;&#33021;&#23558;&#29983;&#25104;&#30340;&#35270;&#35273;&#27010;&#24565;&#19982;&#39640;&#32423;&#35821;&#20041;&#65288;&#22914;&#29289;&#20307;&#25968;&#37327;&#12289;&#31354;&#38388;&#20851;&#31995;&#31561;&#65289;&#36827;&#34892;&#23545;&#40784;&#12290;&#25105;&#20204;&#20174;&#22810;&#27169;&#24577;&#25968;&#25454;&#34701;&#21512;&#30340;&#35282;&#24230;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#19981;&#21516;&#34701;&#21512;&#31574;&#30053;&#22914;&#20309;&#24433;&#21709;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;&#22312;&#39044;&#35757;&#32451;&#22270;&#20687;&#29305;&#24449;&#31354;&#38388;&#20013;&#24191;&#27867;&#20351;&#29992;&#30340;&#26089;&#26399;&#34701;&#21512;&#30456;&#27604;&#65292;&#19968;&#31181;&#29305;&#21035;&#35774;&#35745;&#30340;&#20013;&#38388;&#34701;&#21512;&#21487;&#20197;&#65306;(i) &#25552;&#39640;&#25991;&#26412;&#21040;&#22270;&#20687;&#23545;&#40784;&#30340;&#29983;&#25104;&#36136;&#37327;&#65307;(ii) &#36890;&#36807;&#20943;&#23569;&#20302;&#31209;&#25991;&#26412;&#21040;&#22270;&#20687;&#27880;&#24847;&#21147;&#35745;&#31639;&#26469;&#25552;&#39640;&#35757;&#32451;&#21644;&#25512;&#29702;&#25928;&#29575;&#12290;&#25105;&#20204;&#22312;MS-COCO&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#20013;&#38388;&#34701;&#21512;&#26426;&#21046;&#19982;&#32463;&#20856;&#30340;&#26089;&#26399;&#34701;&#21512;&#26426;&#21046;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16530v1 Announce Type: cross  Abstract: Diffusion models have been widely used for conditional data cross-modal generation tasks such as text-to-image and text-to-video. However, state-of-the-art models still fail to align the generated visual concepts with high-level semantics in a language such as object count, spatial relationship, etc. We approach this problem from a multimodal data fusion perspective and investigate how different fusion strategies can affect vision-language alignment. We discover that compared to the widely used early fusion of conditioning text in a pretrained image feature space, a specially designed intermediate fusion can: (i) boost text-to-image alignment with improved generation quality and (ii) improve training and inference efficiency by reducing low-rank text-to-image attention calculations. We perform experiments using a text-to-image generation task on the MS-COCO dataset. We compare our intermediate fusion mechanism with the classic early fu
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#35268;&#21010;&#32773;&#32570;&#23569;&#30340;&#24120;&#35782;&#25512;&#29702;&#65292;&#20197;&#36866;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#30340;&#22330;&#26223;&#12290;</title><link>https://arxiv.org/abs/2403.16527</link><description>&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#65306;&#28789;&#27963;&#23450;&#20041;&#19982;&#29616;&#26377;&#25216;&#26415;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16527
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22522;&#30784;&#27169;&#22411;&#30340;&#24187;&#35273;&#26816;&#27979;&#26088;&#22312;&#22635;&#34917;&#29616;&#26377;&#35268;&#21010;&#32773;&#32570;&#23569;&#30340;&#24120;&#35782;&#25512;&#29702;&#65292;&#20197;&#36866;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#30340;&#22330;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#31995;&#32479;&#21363;&#23558;&#26080;&#22788;&#19981;&#22312;&#65292;&#20174;&#21046;&#36896;&#19994;&#30340;&#33258;&#20027;&#24615;&#21040;&#20892;&#19994;&#39046;&#22495;&#30340;&#26426;&#22120;&#20154;&#65292;&#20174;&#21307;&#30103;&#21161;&#29702;&#21040;&#23089;&#20048;&#20135;&#19994;&#12290;&#22823;&#22810;&#25968;&#31995;&#32479;&#26159;&#36890;&#36807;&#27169;&#22359;&#21270;&#30340;&#23376;&#32452;&#20214;&#24320;&#21457;&#30340;&#65292;&#29992;&#20110;&#20915;&#31574;&#12289;&#35268;&#21010;&#21644;&#25511;&#21046;&#65292;&#36825;&#20123;&#32452;&#20214;&#21487;&#33021;&#26159;&#25163;&#24037;&#35774;&#35745;&#30340;&#65292;&#20063;&#21487;&#33021;&#26159;&#22522;&#20110;&#23398;&#20064;&#30340;&#12290;&#34429;&#28982;&#29616;&#26377;&#26041;&#27861;&#22312;&#23427;&#20204;&#19987;&#38376;&#35774;&#35745;&#30340;&#24773;&#22659;&#19979;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#27979;&#35797;&#26102;&#19981;&#21487;&#36991;&#20813;&#22320;&#20250;&#22312;&#32597;&#35265;&#30340;&#12289;&#36229;&#20986;&#20998;&#24067;&#33539;&#22260;&#30340;&#22330;&#26223;&#19979;&#34920;&#29616;&#29305;&#21035;&#24046;&#12290;&#22522;&#20110;&#22810;&#20010;&#20219;&#21153;&#35757;&#32451;&#30340;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#20197;&#21450;&#20174;&#21508;&#20010;&#39046;&#22495;&#37319;&#38598;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#65292;&#20351;&#30740;&#31350;&#20154;&#21592;&#30456;&#20449;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#25552;&#20379;&#29616;&#26377;&#35268;&#21010;&#32773;&#25152;&#32570;&#20047;&#30340;&#24120;&#35782;&#25512;&#29702;&#12290;&#30740;&#31350;&#20154;&#21592;&#35748;&#20026;&#65292;&#36825;&#31181;&#24120;&#35782;&#25512;&#29702;&#23558;&#24357;&#21512;&#31639;&#27861;&#24320;&#21457;&#21644;&#37096;&#32626;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#36866;&#29992;&#20110;&#36229;&#20986;&#20998;&#24067;&#20219;&#21153;&#30340;&#24773;&#20917;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16527v1 Announce Type: new  Abstract: Autonomous systems are soon to be ubiquitous, from manufacturing autonomy to agricultural field robots, and from health care assistants to the entertainment industry. The majority of these systems are developed with modular sub-components for decision-making, planning, and control that may be hand-engineered or learning-based. While these existing approaches have been shown to perform well under the situations they were specifically designed for, they can perform especially poorly in rare, out-of-distribution scenarios that will undoubtedly arise at test-time. The rise of foundation models trained on multiple tasks with impressively large datasets from a variety of fields has led researchers to believe that these models may provide common sense reasoning that existing planners are missing. Researchers posit that this common sense reasoning will bridge the gap between algorithm development and deployment to out-of-distribution tasks, like
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;LLMs&#20026;MAS&#20013;&#30340;agent&#36171;&#20104;&#35268;&#33539;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#24314;&#20855;&#26377;&#35268;&#33539;&#21151;&#33021;&#30340;LLM agent&#30340;&#24895;&#26223;&#12290;</title><link>https://arxiv.org/abs/2403.16524</link><description>&lt;p&gt;
&#21033;&#29992;LLMs&#30340;&#21147;&#37327;&#36827;&#34892;MAS&#20013;&#30340;&#35268;&#33539;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Harnessing the power of LLMs for normative reasoning in MASs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16524
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;LLMs&#20026;MAS&#20013;&#30340;agent&#36171;&#20104;&#35268;&#33539;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#24182;&#25552;&#20986;&#20102;&#21019;&#24314;&#20855;&#26377;&#35268;&#33539;&#21151;&#33021;&#30340;LLM agent&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;agent&#65292;&#26080;&#35770;&#26159;&#20154;&#31867;&#36824;&#26159;&#35745;&#31639;&#26426;&#65292;&#37117;&#19981;&#26159;&#29420;&#31435;&#23384;&#22312;&#30340;&#65292;&#36890;&#24120;&#38656;&#35201;&#19982;&#20182;&#20154;&#21327;&#20316;&#25110;&#21327;&#35843;&#20197;&#23454;&#29616;&#20182;&#20204;&#30340;&#30446;&#26631;&#12290;&#22312;&#20154;&#31867;&#31038;&#20250;&#20013;&#65292;&#35268;&#33539;&#31561;&#31038;&#20250;&#26426;&#21046;&#30830;&#20445;&#20102;&#26377;&#25928;&#30340;&#36816;&#34892;&#65292;&#30740;&#31350;&#20154;&#21592;&#22312;&#22810;Agent&#31995;&#32479;&#65288;MAS&#65289;&#20013;&#37319;&#29992;&#36825;&#20123;&#25216;&#26415;&#26469;&#21019;&#24314;&#20855;&#26377;&#31038;&#20250;&#24847;&#35782;&#30340;agent&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#25216;&#26415;&#23384;&#22312;&#19968;&#20123;&#23616;&#38480;&#24615;&#65292;&#27604;&#22914;&#22312;&#26377;&#38480;&#29615;&#22659;&#20013;&#36816;&#20316;&#65292;&#36890;&#24120;&#20351;&#29992;&#33030;&#24369;&#30340;&#31526;&#21495;&#25512;&#29702;&#12290;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#20986;&#29616;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#20016;&#23500;&#21644;&#23500;&#26377;&#34920;&#29616;&#21147;&#30340;&#35789;&#27719;&#34920;&#36798;&#35268;&#33539;&#65292;&#20351;&#33021;&#22815;&#25191;&#34892;&#19968;&#31995;&#21015;&#20219;&#21153;&#30340;&#20855;&#26377;&#35268;&#33539;&#21151;&#33021;&#30340;agent&#65292;&#22914;&#35268;&#33539;&#21457;&#29616;&#12289;&#35268;&#33539;&#25512;&#29702;&#21644;&#20915;&#31574;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22522;&#20110;LLM&#30340;agent&#33719;&#24471;&#35268;&#33539;&#33021;&#21147;&#30340;&#28508;&#21147;&#65292;&#20511;&#37492;&#20102;&#26368;&#36817;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#21644;LLM&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#21019;&#24314;&#35268;&#33539;LLM agent&#30340;&#24895;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16524v1 Announce Type: new  Abstract: Software agents, both human and computational, do not exist in isolation and often need to collaborate or coordinate with others to achieve their goals. In human society, social mechanisms such as norms ensure efficient functioning, and these techniques have been adopted by researchers in multi-agent systems (MAS) to create socially aware agents. However, traditional techniques have limitations, such as operating in limited environments often using brittle symbolic reasoning. The advent of Large Language Models (LLMs) offers a promising solution, providing a rich and expressive vocabulary for norms and enabling norm-capable agents that can perform a range of tasks such as norm discovery, normative reasoning and decision-making. This paper examines the potential of LLM-based agents to acquire normative capabilities, drawing on recent Natural Language Processing (NLP) and LLM research. We present our vision for creating normative LLM agent
&lt;/p&gt;</description></item><item><title>&#20174;&#35745;&#25968;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#32467;&#26500;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#38750;&#21487;&#36776;&#35782;&#24615;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#27850;&#26494;&#20998;&#25903;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#22914;&#26524;&#26681;&#39030;&#28857;$X$&#26159;&#24050;&#30693;&#30340;&#65292;&#21017;&#21487;&#20197;&#30830;&#23450;&#20174;$X$&#21040;&#20854;&#23376;&#33410;&#28857;$Y$&#30340;&#22240;&#26524;&#39034;&#24207;&#12290;</title><link>https://arxiv.org/abs/2403.16523</link><description>&lt;p&gt;
&#21033;&#29992;&#39640;&#38454;&#32047;&#31215;&#37327;&#21644;&#36335;&#24452;&#20998;&#26512;&#20174;&#27850;&#26494;&#20998;&#25903;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Causal Discovery from Poisson Branching Structural Causal Model Using High-Order Cumulant with Path Analysis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16523
&lt;/p&gt;
&lt;p&gt;
&#20174;&#35745;&#25968;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#32467;&#26500;&#30340;&#20851;&#38190;&#25361;&#25112;&#22312;&#20110;&#38750;&#21487;&#36776;&#35782;&#24615;&#38382;&#39064;&#65292;&#26412;&#30740;&#31350;&#21457;&#29616;&#22312;&#27850;&#26494;&#20998;&#25903;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#20013;&#65292;&#22914;&#26524;&#26681;&#39030;&#28857;$X$&#26159;&#24050;&#30693;&#30340;&#65292;&#21017;&#21487;&#20197;&#30830;&#23450;&#20174;$X$&#21040;&#20854;&#23376;&#33410;&#28857;$Y$&#30340;&#22240;&#26524;&#39034;&#24207;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#25968;&#25968;&#25454;&#22312;&#37329;&#34701;&#12289;&#31070;&#32463;&#31185;&#23398;&#21644;&#27969;&#34892;&#30149;&#23398;&#31561;&#39046;&#22495;&#20013;&#33258;&#28982;&#20135;&#29983;&#65292;&#22312;&#21508;&#31181;&#31185;&#23398;&#21644;&#24037;&#19994;&#22330;&#26223;&#20013;&#21457;&#29616;&#35745;&#25968;&#25968;&#25454;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#26159;&#19968;&#39033;&#20851;&#38190;&#20219;&#21153;&#12290;&#35745;&#25968;&#25968;&#25454;&#30340;&#19968;&#20010;&#26368;&#24120;&#35265;&#29305;&#24449;&#26159;&#30001;&#20108;&#39033;&#24335;&#31232;&#30095;&#36816;&#31639;&#31526;&#21644;&#29420;&#31435;&#30340;&#27850;&#26494;&#20998;&#24067;&#25551;&#36848;&#30340;&#22266;&#26377;&#20998;&#25903;&#32467;&#26500;&#65292;&#35813;&#32467;&#26500;&#25429;&#25417;&#20102;&#20998;&#25903;&#21644;&#22122;&#22768;&#12290;&#20363;&#22914;&#65292;&#22312;&#20154;&#21475;&#35745;&#25968;&#24773;&#26223;&#20013;&#65292;&#27515;&#20129;&#21644;&#31227;&#27665;&#23545;&#35745;&#25968;&#26377;&#36129;&#29486;&#65292;&#20854;&#20013;&#29983;&#23384;&#36981;&#24490;&#20271;&#21162;&#21033;&#20998;&#24067;&#65292;&#31227;&#27665;&#36981;&#24490;&#27850;&#26494;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#19981;&#21487;&#36776;&#35782;&#24615;&#38382;&#39064;&#65292;&#20174;&#36825;&#20123;&#25968;&#25454;&#20013;&#21457;&#29616;&#22240;&#26524;&#20851;&#31995;&#20855;&#26377;&#25361;&#25112;&#24615;&#65306;&#21333;&#19968;&#22240;&#26524;&#23545;&#26159;&#39532;&#23572;&#21487;&#22827;&#31561;&#20215;&#30340;&#65292;&#21363;$X\rightarrow Y$&#21644;$Y\rightarrow X$&#22312;&#20998;&#24067;&#19978;&#26159;&#31561;&#20215;&#30340;&#12290;&#24184;&#36816;&#30340;&#26159;&#65292;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22914;&#26524;$X$&#26159;&#19968;&#20010;&#26681;&#39030;&#28857;&#65292;&#37027;&#20040;&#20174;$X$&#21040;&#20854;&#23376;&#33410;&#28857;$Y$&#30340;&#22240;&#26524;&#39034;&#24207;&#26159;&#21487;&#35782;&#21035;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16523v1 Announce Type: cross  Abstract: Count data naturally arise in many fields, such as finance, neuroscience, and epidemiology, and discovering causal structure among count data is a crucial task in various scientific and industrial scenarios. One of the most common characteristics of count data is the inherent branching structure described by a binomial thinning operator and an independent Poisson distribution that captures both branching and noise. For instance, in a population count scenario, mortality and immigration contribute to the count, where survival follows a Bernoulli distribution, and immigration follows a Poisson distribution. However, causal discovery from such data is challenging due to the non-identifiability issue: a single causal pair is Markov equivalent, i.e., $X\rightarrow Y$ and $Y\rightarrow X$ are distributed equivalent. Fortunately, in this work, we found that the causal order from $X$ to its child $Y$ is identifiable if $X$ is a root vertex and
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16512</link><description>&lt;p&gt;
LLMs&#26159;&#23569;&#26679;&#26412;&#24773;&#22659;&#20302;&#36164;&#28304;&#35821;&#35328;&#23398;&#20064;&#22120;
&lt;/p&gt;
&lt;p&gt;
LLMs Are Few-Shot In-Context Low-Resource Language Learners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16512
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#21457;&#29616;&#20102;&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;LLMs&#36827;&#34892;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#25552;&#20986;&#20102;&#26367;&#20195;&#26041;&#27861;&#26597;&#35810;&#23545;&#40784;&#65292;&#24182;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#25552;&#20379;&#20102;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#30340;&#25903;&#25345;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#21033;&#29992;&#30701;&#26102;&#30340;&#24773;&#22659;&#20449;&#24687;&#25191;&#34892;&#21508;&#31181;&#20219;&#21153;&#65292;&#36825;&#20026;&#32553;&#23567;&#39640;&#36164;&#28304;&#35821;&#35328;&#21644;&#20302;&#36164;&#28304;&#35821;&#35328;&#20043;&#38388;&#30340;&#24046;&#36317;&#25552;&#20379;&#20102;&#37325;&#35201;&#36884;&#24452;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#21482;&#26377;&#23569;&#25968;&#30740;&#31350;&#25506;&#35752;&#20102;&#38024;&#23545;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#38598;&#20013;&#22312;&#30456;&#23545;&#39640;&#36164;&#28304;&#30340;&#35821;&#35328;&#65292;&#27604;&#22914;&#27861;&#35821;&#21644;&#35199;&#29677;&#29273;&#35821;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23545;25&#31181;&#20302;&#36164;&#28304;&#35821;&#35328;&#21644;7&#31181;&#30456;&#23545;&#36739;&#39640;&#36164;&#28304;&#35821;&#35328;&#19978;&#30340;ICL&#21450;&#20854;&#36328;&#35821;&#35328;&#21464;&#20307;&#65288;X-ICL&#65289;&#36827;&#34892;&#20102;&#24191;&#27867;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#19981;&#20165;&#35780;&#20272;&#20102;LLMs&#22312;&#20302;&#36164;&#28304;&#35821;&#35328;&#20013;&#20351;&#29992;ICL&#30340;&#26377;&#25928;&#24615;&#65292;&#36824;&#21457;&#29616;&#20102;&#24773;&#22659;&#26631;&#31614;&#23545;&#40784;&#30340;&#32570;&#38519;&#65292;&#24182;&#24341;&#20837;&#20102;&#26356;&#26377;&#25928;&#30340;&#26367;&#20195;&#26041;&#27861;&#65306;&#26597;&#35810;&#23545;&#40784;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20302;&#36164;&#28304;&#35821;&#35328;&#30340;ICL&#30340;&#21508;&#20010;&#26041;&#38754;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24635;&#32467;&#20102;&#23569;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16512v1 Announce Type: cross  Abstract: In-context learning (ICL) empowers large language models (LLMs) to perform diverse tasks in underrepresented languages using only short in-context information, offering a crucial avenue for narrowing the gap between high-resource and low-resource languages. Nonetheless, there is only a handful of works explored ICL for low-resource languages with most of them focusing on relatively high-resource languages, such as French and Spanish. In this work, we extensively study ICL and its cross-lingual variation (X-ICL) on 25 low-resource and 7 relatively higher-resource languages. Our study not only assesses the effectiveness of ICL with LLMs in low-resource languages but also identifies the shortcomings of in-context label alignment, and introduces a more effective alternative: query alignment. Moreover, we provide valuable insights into various facets of ICL for low-resource languages. Our study concludes the significance of few-shot in-cont
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;WL&#31639;&#27861;&#29983;&#25104;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#30340;WL-GOOSE&#26041;&#27861;&#21487;&#38752;&#22320;&#23398;&#20064;&#35268;&#21010;&#21551;&#21457;&#24335;&#65292;&#24615;&#33021;&#20248;&#20110;$h^{\text{FF}}$&#21551;&#21457;&#24335;&#21644;LAMA&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;</title><link>https://arxiv.org/abs/2403.16508</link><description>&lt;p&gt;
&#37325;&#36820;&#20256;&#32479;&#65306;&#29992;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#23398;&#20064;&#21487;&#38752;&#30340;&#21551;&#21457;&#24335;
&lt;/p&gt;
&lt;p&gt;
Return to Tradition: Learning Reliable Heuristics with Classical Machine Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16508
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;WL&#31639;&#27861;&#29983;&#25104;&#29305;&#24449;&#65292;&#24182;&#32467;&#21512;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#20986;&#30340;WL-GOOSE&#26041;&#27861;&#21487;&#38752;&#22320;&#23398;&#20064;&#35268;&#21010;&#21551;&#21457;&#24335;&#65292;&#24615;&#33021;&#20248;&#20110;$h^{\text{FF}}$&#21551;&#21457;&#24335;&#21644;LAMA&#65292;&#22312;&#29305;&#23450;&#39046;&#22495;&#21462;&#24471;&#20102;&#20248;&#24322;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#23398;&#20064;&#35268;&#21010;&#30340;&#26041;&#27861;&#22312;&#20960;&#20010;&#39046;&#22495;&#20173;&#26410;&#33021;&#19982;&#32463;&#20856;&#35268;&#21010;&#22120;&#31454;&#20105;&#65292;&#24182;&#19988;&#24635;&#20307;&#24615;&#33021;&#36739;&#24046;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26500;&#24314;&#20102;&#25260;&#21319;&#35268;&#21010;&#20219;&#21153;&#30340;&#26032;&#39062;&#22270;&#34920;&#31034;&#65292;&#24182;&#20351;&#29992;WL&#31639;&#27861;&#20174;&#20013;&#29983;&#25104;&#29305;&#24449;&#12290;&#36825;&#20123;&#29305;&#24449;&#19982;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#19968;&#36215;&#20351;&#29992;&#65292;&#20854;&#21442;&#25968;&#25968;&#37327;&#27604;&#26368;&#20808;&#36827;&#30340;&#35268;&#21010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23569;2&#20010;&#25968;&#37327;&#32423;&#65292;&#24182;&#19988;&#35757;&#32451;&#36895;&#24230;&#27604;&#20854;&#24555;3&#20010;&#25968;&#37327;&#32423;&#12290;&#25105;&#20204;&#30340;&#26032;&#26041;&#27861;WL-GOOSE&#21487;&#21487;&#38752;&#22320;&#20174;&#38646;&#24320;&#22987;&#23398;&#20064;&#21551;&#21457;&#24335;&#65292;&#24182;&#22312;&#20844;&#24179;&#31454;&#20105;&#29615;&#22659;&#20013;&#32988;&#36807;&#20102;$h^{\text{FF}}$&#21551;&#21457;&#24335;&#12290;&#23427;&#22312;&#35206;&#30422;&#29575;&#19978;&#36229;&#36807;&#25110;&#24182;&#21015;&#20110;LAMA&#30340;10&#20010;&#39046;&#22495;&#20013;&#30340;4&#20010;&#39046;&#22495;&#65292;&#24182;&#19988;&#22312;&#35745;&#21010;&#36136;&#37327;&#19978;&#36229;&#36807;&#25110;&#24182;&#21015;&#20110;7&#20010;&#39046;&#22495;&#20013;&#30340;LAMA&#12290;WL-GOOSE&#26159;&#31532;&#19968;&#20010;&#23454;&#29616;&#36825;&#20123;&#25104;&#23601;&#30340;&#23398;&#20064;&#35268;&#21010;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#26032;&#22411;WL&#29305;&#24449;&#29983;&#25104;&#26041;&#27861;&#19982;&#20854;&#20182;&#26041;&#27861;&#20043;&#38388;&#30340;&#32852;&#31995;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16508v1 Announce Type: new  Abstract: Current approaches for learning for planning have yet to achieve competitive performance against classical planners in several domains, and have poor overall performance. In this work, we construct novel graph representations of lifted planning tasks and use the WL algorithm to generate features from them. These features are used with classical machine learning methods which have up to 2 orders of magnitude fewer parameters and train up to 3 orders of magnitude faster than the state-of-the-art deep learning for planning models. Our novel approach, WL-GOOSE, reliably learns heuristics from scratch and outperforms the $h^{\text{FF}}$ heuristic in a fair competition setting. It also outperforms or ties with LAMA on 4 out of 10 domains on coverage and 7 out of 10 domains on plan quality. WL-GOOSE is the first learning for planning model which achieves these feats. Furthermore, we study the connections between our novel WL feature generation 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#8220;&#23398;&#20064;&#25351;&#23548;&#8221;&#65288;LTG&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#19987;&#23478;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#26426;&#22120;&#20915;&#31574;&#21644;&#38754;&#20020;&#26080;&#21161;&#20110;&#27169;&#22411;&#25918;&#24323;&#30340;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16501</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#25351;&#23548;&#20154;&#31867;&#20915;&#31574;&#32773;
&lt;/p&gt;
&lt;p&gt;
Learning To Guide Human Decision Makers With Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16501
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#8220;&#23398;&#20064;&#25351;&#23548;&#8221;&#65288;LTG&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#19987;&#23478;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#26426;&#22120;&#20915;&#31574;&#21644;&#38754;&#20020;&#26080;&#21161;&#20110;&#27169;&#22411;&#25918;&#24323;&#30340;&#20915;&#31574;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36234;&#26469;&#36234;&#22810;&#30340;&#20154;&#23545;&#24320;&#21457;&#20154;&#24037;&#26234;&#33021;&#20197;&#21327;&#21161;&#20154;&#31867;&#36827;&#34892;&#39640;&#39118;&#38505;&#20219;&#21153;&#20013;&#30340;&#20915;&#31574;&#34920;&#29616;&#20986;&#20852;&#36259;&#65292;&#27604;&#22914;&#21307;&#23398;&#35786;&#26029;&#65292;&#26088;&#22312;&#25552;&#39640;&#20915;&#31574;&#36136;&#37327;&#21644;&#20943;&#23569;&#35748;&#30693;&#36127;&#25285;&#12290;&#20027;&#27969;&#26041;&#27861;&#26159;&#23558;&#19987;&#23478;&#19982;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21512;&#20316;&#65292;&#23558;&#26356;&#23433;&#20840;&#30340;&#20915;&#31574;&#19979;&#25918;&#65292;&#35753;&#21069;&#32773;&#19987;&#27880;&#20110;&#38656;&#35201;&#20182;&#20204;&#20851;&#27880;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#22312;&#39640;&#39118;&#38505;&#22330;&#26223;&#20013;&#65292;&#36825;&#31181;&#8220;&#36131;&#20219;&#20998;&#24037;&#8221;&#35774;&#32622;&#26159;&#19981;&#22815;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16501v1 Announce Type: new  Abstract: There is increasing interest in developing AIs for assisting human decision making in \textit{high-stakes} tasks, such as medical diagnosis, for the purpose of improving decision quality and reducing cognitive strain.   %   Mainstream approaches team up an expert with a machine learning model to which safer decisions are offloaded, thus letting the former focus on cases that demand their attention.   %   This \textit{separation of responsibilities} setup, however, is inadequate for high-stakes scenarios. On the one hand, the expert may end up over-relying on the machine's decisions due to \textit{anchoring bias}, thus losing the human oversight that is increasingly being required by regulatory agencies to ensure trustworthy AI. On the other hand, the expert is left entirely unassisted on the (typically hardest) decisions on which the model abstained.   %   As a remedy, we introduce \textit{learning to guide} (LTG), an alternative framewo
&lt;/p&gt;</description></item><item><title>LSTTN&#26694;&#26550;&#32508;&#21512;&#32771;&#34385;&#21382;&#21490;&#20132;&#36890;&#27969;&#37327;&#20013;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#29305;&#24449;&#65292;&#36890;&#36807;&#25513;&#30721;&#23376;&#24207;&#21015;Transformer&#35299;&#20915;&#20102;&#29616;&#26377;STGNNs&#27169;&#22411;&#21482;&#33021;&#21033;&#29992;&#30701;&#31243;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#27969;&#37327;&#22797;&#26434;&#36235;&#21183;&#21644;&#21608;&#26399;&#29305;&#24449;&#30340;&#20805;&#20998;&#23398;&#20064;</title><link>https://arxiv.org/abs/2403.16495</link><description>&lt;p&gt;
LSTTN&#65306;&#22522;&#20110;&#38271;&#30701;&#26399;Transformer&#30340;&#26102;&#31354;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
LSTTN: A Long-Short Term Transformer-based Spatio-temporal Neural Network for Traffic Flow Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16495
&lt;/p&gt;
&lt;p&gt;
LSTTN&#26694;&#26550;&#32508;&#21512;&#32771;&#34385;&#21382;&#21490;&#20132;&#36890;&#27969;&#37327;&#20013;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#29305;&#24449;&#65292;&#36890;&#36807;&#25513;&#30721;&#23376;&#24207;&#21015;Transformer&#35299;&#20915;&#20102;&#29616;&#26377;STGNNs&#27169;&#22411;&#21482;&#33021;&#21033;&#29992;&#30701;&#31243;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#30340;&#38480;&#21046;&#65292;&#23454;&#29616;&#20102;&#23545;&#20132;&#36890;&#27969;&#37327;&#22797;&#26434;&#36235;&#21183;&#21644;&#21608;&#26399;&#29305;&#24449;&#30340;&#20805;&#20998;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#20132;&#36890;&#39044;&#27979;&#26159;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#20013;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#36890;&#36807;&#26102;&#31354;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;STGNNs&#65289;&#23398;&#20064;&#24102;&#20851;&#38190;&#20449;&#24687;&#30340;&#38271;&#31243;&#20132;&#36890;&#34920;&#31034;&#26159;&#24403;&#21069;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#27169;&#22411;&#30340;&#22522;&#26412;&#20551;&#35774;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#32467;&#26500;&#38480;&#21046;&#65292;&#29616;&#26377;&#30340;STGNNs&#21482;&#33021;&#21033;&#29992;&#30701;&#31243;&#20132;&#36890;&#27969;&#37327;&#25968;&#25454;&#65307;&#22240;&#27492;&#65292;&#36825;&#20123;&#27169;&#22411;&#26080;&#27861;&#20805;&#20998;&#23398;&#20064;&#20132;&#36890;&#27969;&#37327;&#20013;&#30340;&#22797;&#26434;&#36235;&#21183;&#21644;&#21608;&#26399;&#29305;&#24449;&#12290;&#27492;&#22806;&#65292;&#20174;&#38271;&#26399;&#21382;&#21490;&#20132;&#36890;&#31995;&#21015;&#20013;&#25552;&#21462;&#20851;&#38190;&#26102;&#38388;&#20449;&#24687;&#24182;&#33719;&#24471;&#32039;&#20945;&#34920;&#31034;&#20063;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;LSTTN&#65288;Long-Short Term Transformer-based Network&#65289;&#26694;&#26550;&#65292;&#20840;&#38754;&#32771;&#34385;&#21382;&#21490;&#20132;&#36890;&#27969;&#37327;&#20013;&#30340;&#38271;&#26399;&#21644;&#30701;&#26399;&#29305;&#24449;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21033;&#29992;&#19968;&#20010;&#25513;&#30721;&#23376;&#24207;&#21015;Transformer&#26469;&#25512;&#26029;&#26469;&#33258;&#23569;&#37327;&#26410;&#34987;&#25513;&#30721;&#30340;&#23376;&#24207;&#21015;&#30340;&#20869;&#23481;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16495v1 Announce Type: cross  Abstract: Accurate traffic forecasting is a fundamental problem in intelligent transportation systems and learning long-range traffic representations with key information through spatiotemporal graph neural networks (STGNNs) is a basic assumption of current traffic flow prediction models. However, due to structural limitations, existing STGNNs can only utilize short-range traffic flow data; therefore, the models cannot adequately learn the complex trends and periodic features in traffic flow. Besides, it is challenging to extract the key temporal information from the long historical traffic series and obtain a compact representation. To solve the above problems, we propose a novel LSTTN (Long-Short Term Transformer-based Network) framework comprehensively considering the long- and short-term features in historical traffic flow. First, we employ a masked subseries Transformer to infer the content of masked subseries from a small portion of unmask
&lt;/p&gt;</description></item><item><title>FedAC&#26694;&#26550;&#36890;&#36807;&#35299;&#32806;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#19981;&#21516;&#32858;&#21512;&#26041;&#27861;&#20026;&#27599;&#20010;&#23376;&#27169;&#22359;&#25552;&#20379;&#20840;&#23616;&#30693;&#35782;&#65292;&#24341;&#20837;&#32463;&#27982;&#39640;&#25928;&#30340;&#22312;&#32447;&#27169;&#22411;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21450;&#38598;&#32676;&#25968;&#37327;&#24494;&#35843;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16460</link><description>&lt;p&gt;
FedAC&#65306;&#19968;&#31181;&#29992;&#20110;&#24322;&#26500;&#25968;&#25454;&#30340;&#33258;&#36866;&#24212;&#20998;&#31751;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
FedAC: A Adaptive Clustered Federated Learning Framework for Heterogeneous Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16460
&lt;/p&gt;
&lt;p&gt;
FedAC&#26694;&#26550;&#36890;&#36807;&#35299;&#32806;&#31070;&#32463;&#32593;&#32476;&#65292;&#20351;&#29992;&#19981;&#21516;&#32858;&#21512;&#26041;&#27861;&#20026;&#27599;&#20010;&#23376;&#27169;&#22359;&#25552;&#20379;&#20840;&#23616;&#30693;&#35782;&#65292;&#24341;&#20837;&#32463;&#27982;&#39640;&#25928;&#30340;&#22312;&#32447;&#27169;&#22411;&#30456;&#20284;&#24230;&#24230;&#37327;&#65292;&#21450;&#38598;&#32676;&#25968;&#37327;&#24494;&#35843;&#27169;&#22359;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#20998;&#31751;&#32852;&#37030;&#23398;&#20064;&#26694;&#26550;FedAC&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#32806;&#31070;&#32463;&#32593;&#32476;&#24182;&#21033;&#29992;&#19981;&#21516;&#30340;&#32858;&#21512;&#26041;&#27861;&#20026;&#27599;&#20010;&#23376;&#27169;&#22359;&#26377;&#25928;&#22320;&#23558;&#20840;&#23616;&#30693;&#35782;&#25972;&#21512;&#21040;&#31751;&#20869;&#23398;&#20064;&#20013;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#24615;&#33021;&#65307;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#38477;&#32500;&#30340;&#32463;&#27982;&#39640;&#25928;&#30340;&#22312;&#32447;&#27169;&#22411;&#30456;&#20284;&#24230;&#24230;&#37327;&#65307;&#24182;&#19988;&#32467;&#21512;&#20102;&#19968;&#20010;&#29992;&#20110;&#25913;&#36827;&#22797;&#26434;&#24615;&#30340;&#38598;&#32676;&#25968;&#37327;&#24494;&#35843;&#27169;&#22359;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#36866;&#24212;&#24615;&#21644;&#21487;&#20280;&#32553;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16460v1 Announce Type: cross  Abstract: Clustered federated learning (CFL) is proposed to mitigate the performance deterioration stemming from data heterogeneity in federated learning (FL) by grouping similar clients for cluster-wise model training. However, current CFL methods struggle due to inadequate integration of global and intra-cluster knowledge and the absence of an efficient online model similarity metric, while treating the cluster count as a fixed hyperparameter limits flexibility and robustness. In this paper, we propose an adaptive CFL framework, named FedAC, which (1) efficiently integrates global knowledge into intra-cluster learning by decoupling neural networks and utilizing distinct aggregation methods for each submodule, significantly enhancing performance; (2) includes a costeffective online model similarity metric based on dimensionality reduction; (3) incorporates a cluster number fine-tuning module for improved adaptability and scalability in complex,
&lt;/p&gt;</description></item><item><title>DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;</title><link>https://arxiv.org/abs/2403.16451</link><description>&lt;p&gt;
DeepMachining: &#38115;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#22312;&#32447;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
DeepMachining: Online Prediction of Machining Errors of Lathe Machines
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16451
&lt;/p&gt;
&lt;p&gt;
DeepMachining&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;AI&#31995;&#32479;&#65292;&#21487;&#20197;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#35823;&#24046;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#39640;&#20934;&#30830;&#24615;&#39044;&#27979;&#65292;&#26159;&#39318;&#25209;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25551;&#36848;&#20102;DeepMachining&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#65292;&#29992;&#20110;&#22312;&#32447;&#39044;&#27979;&#36710;&#24202;&#21152;&#24037;&#25805;&#20316;&#30340;&#21152;&#24037;&#35823;&#24046;&#12290;&#25105;&#20204;&#22522;&#20110;&#24037;&#21378;&#30340;&#21046;&#36896;&#25968;&#25454;&#26500;&#24314;&#24182;&#35780;&#20272;&#20102;DeepMachining&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#23545;&#29305;&#23450;&#36710;&#24202;&#26426;&#24202;&#25805;&#20316;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20197;&#23398;&#20064;&#21152;&#24037;&#29366;&#24577;&#30340;&#26174;&#33879;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24494;&#35843;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#36866;&#24212;&#29305;&#23450;&#21152;&#24037;&#20219;&#21153;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;DeepMachining&#22312;&#28041;&#21450;&#19981;&#21516;&#24037;&#20214;&#21644;&#20992;&#20855;&#30340;&#22810;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#39640;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#39033;&#24037;&#20316;&#26159;&#20351;&#29992;&#39044;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#39044;&#27979;&#36710;&#24202;&#26426;&#24202;&#21152;&#24037;&#35823;&#24046;&#30340;&#39318;&#25209;&#24037;&#21378;&#23454;&#39564;&#20043;&#19968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16451v1 Announce Type: cross  Abstract: We describe DeepMachining, a deep learning-based AI system for online prediction of machining errors of lathe machine operations. We have built and evaluated DeepMachining based on manufacturing data from factories. Specifically, we first pretrain a deep learning model for a given lathe machine's operations to learn the salient features of machining states. Then, we fine-tune the pretrained model to adapt to specific machining tasks. We demonstrate that DeepMachining achieves high prediction accuracy for multiple tasks that involve different workpieces and cutting tools. To the best of our knowledge, this work is one of the first factory experiments using pre-trained deep-learning models to predict machining errors of lathe machines.
&lt;/p&gt;</description></item><item><title>CodeS&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;NL2Repo&#65292;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#20013;&#29983;&#25104;&#25972;&#20010;&#20195;&#30721;&#20179;&#24211;&#65292;&#36890;&#36807;&#22810;&#23618;&#33609;&#22270;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.16443</link><description>&lt;p&gt;
CodeS: &#36890;&#36807;&#22810;&#23618;&#33609;&#22270;&#23454;&#29616;&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#20179;&#24211;&#30340;&#36716;&#25442;
&lt;/p&gt;
&lt;p&gt;
CodeS: Natural Language to Code Repository via Multi-Layer Sketch
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16443
&lt;/p&gt;
&lt;p&gt;
CodeS&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;NL2Repo&#65292;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#20013;&#29983;&#25104;&#25972;&#20010;&#20195;&#30721;&#20179;&#24211;&#65292;&#36890;&#36807;&#22810;&#23618;&#33609;&#22270;&#30340;&#26041;&#24335;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#30456;&#20851;&#20219;&#21153;&#19978;&#30340;&#20986;&#33394;&#24615;&#33021;&#23637;&#31034;&#20102;&#23436;&#20840;&#33258;&#21160;&#21270;&#36719;&#20214;&#24320;&#21457;&#30340;&#28508;&#21147;&#12290;&#37492;&#20110;&#27492;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#36719;&#20214;&#24037;&#31243;&#20219;&#21153;&#65292;&#21363;&#33258;&#28982;&#35821;&#35328;&#21040;&#20195;&#30721;&#20179;&#24211;&#65288;NL2Repo&#65289;&#12290;&#35813;&#20219;&#21153;&#26088;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#38656;&#27714;&#20013;&#29983;&#25104;&#25972;&#20010;&#20195;&#30721;&#20179;&#24211;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20219;&#21153;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26694;&#26550; CodeS&#65292;&#36890;&#36807;&#22810;&#23618;&#33609;&#22270;&#23558;NL2Repo&#20998;&#35299;&#20026;&#22810;&#20010;&#23376;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;CodeS&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;RepoSketcher&#65292;FileSketcher&#21644;SketchFiller&#12290;RepoSketcher&#39318;&#20808;&#20026;&#32473;&#23450;&#30340;&#38656;&#27714;&#29983;&#25104;&#20195;&#30721;&#20179;&#24211;&#30340;&#30446;&#24405;&#32467;&#26500;&#65307;FileSketcher&#28982;&#21518;&#20026;&#29983;&#25104;&#30340;&#32467;&#26500;&#20013;&#30340;&#27599;&#20010;&#25991;&#20214;&#29983;&#25104;&#19968;&#20010;&#25991;&#20214;&#33609;&#22270;&#65307;SketchFiller&#26368;&#32456;&#20026;&#29983;&#25104;&#30340;&#25991;&#20214;&#33609;&#22270;&#20013;&#30340;&#27599;&#20010;&#20989;&#25968;&#22635;&#20805;&#32454;&#33410;&#12290;&#20026;&#20102;&#20005;&#26684;&#35780;&#20272;CodeS&#22312;NL2Repo&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#36890;&#36807;&#33258;&#21160;&#21644;&#25163;&#21160;&#30340;&#26041;&#24335;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16443v1 Announce Type: cross  Abstract: The impressive performance of large language models (LLMs) on code-related tasks has shown the potential of fully automated software development. In light of this, we introduce a new software engineering task, namely Natural Language to code Repository (NL2Repo). This task aims to generate an entire code repository from its natural language requirements. To address this task, we propose a simple yet effective framework CodeS, which decomposes NL2Repo into multiple sub-tasks by a multi-layer sketch. Specifically, CodeS includes three modules: RepoSketcher, FileSketcher, and SketchFiller. RepoSketcher first generates a repository's directory structure for given requirements; FileSketcher then generates a file sketch for each file in the generated structure; SketchFiller finally fills in the details for each function in the generated file sketch. To rigorously assess CodeS on the NL2Repo task, we carry out evaluations through both automat
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#25581;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#25552;&#31034;&#20197;&#35823;&#23548;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#23545;&#35813;&#33539;&#24335;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#25285;&#24551;&#12290;</title><link>https://arxiv.org/abs/2403.16432</link><description>&lt;p&gt;
$\textit{LinkPrompt}$: &#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#28982;&#21644;&#36890;&#29992;&#23545;&#25239;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
$\textit{LinkPrompt}$: Natural and Universal Adversarial Attacks on Prompt-based Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16432
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25552;&#31034;&#30340;&#35821;&#35328;&#27169;&#22411;&#30340;&#20248;&#21270;&#36807;&#31243;&#25581;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#25552;&#31034;&#20197;&#35823;&#23548;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#23545;&#35813;&#33539;&#24335;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#25285;&#24551;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Prompt-based learning &#26159;&#19968;&#31181;&#26032;&#30340;&#35821;&#35328;&#27169;&#22411;&#35757;&#32451;&#33539;&#24335;&#65292;&#23427;&#23558;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#35843;&#25972;&#21040;&#19979;&#28216;&#20219;&#21153;&#65292;&#20174;&#32780;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#25552;&#21319;&#20102;&#24615;&#33021;&#22522;&#20934;&#12290;&#19968;&#20123;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#20248;&#21270;&#25628;&#32034;&#25552;&#31034;&#30340;&#26377;&#25928;&#24615;&#65292;&#32780;&#19981;&#26159;&#20351;&#29992;&#22266;&#23450;&#30340;&#25552;&#31034;&#27169;&#26495;&#26469;&#24494;&#35843;&#27169;&#22411;&#12290;&#36825;&#31181;&#22522;&#20110;&#25552;&#31034;&#20248;&#21270;&#36807;&#31243;&#23545;PLMs&#30340;&#23398;&#20064;&#20063;&#25581;&#31034;&#20102;&#29983;&#25104;&#23545;&#25239;&#25552;&#31034;&#20197;&#35823;&#23548;&#27169;&#22411;&#30340;&#35265;&#35299;&#65292;&#24341;&#21457;&#20102;&#23545;&#36825;&#19968;&#33539;&#24335;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#25285;&#24551;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#29983;&#25104;&#36890;&#29992;&#23545;&#25239;&#35302;&#21457;&#22120;&#65288;UATs&#65289;&#26469;&#25913;&#21464;&#19981;&#20165;&#30446;&#26631;PLMs&#30340;&#39044;&#27979;&#65292;&#36824;&#26377;&#23545;&#24212;Prompt-based Fine-tuning Models&#65288;PFMs&#65289;&#30340;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#20316;&#21697;&#20013;&#21457;&#29616;&#30340;UATs&#36890;&#24120;&#26159;&#26080;&#27861;&#38405;&#35835;&#30340;&#20196;&#29260;&#25110;&#23383;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16432v1 Announce Type: cross  Abstract: Prompt-based learning is a new language model training paradigm that adapts the Pre-trained Language Models (PLMs) to downstream tasks, which revitalizes the performance benchmarks across various natural language processing (NLP) tasks. Instead of using a fixed prompt template to fine-tune the model, some research demonstrates the effectiveness of searching for the prompt via optimization. Such prompt optimization process of prompt-based learning on PLMs also gives insight into generating adversarial prompts to mislead the model, raising concerns about the adversarial vulnerability of this paradigm. Recent studies have shown that universal adversarial triggers (UATs) can be generated to alter not only the predictions of the target PLMs but also the prediction of corresponding Prompt-based Fine-tuning Models (PFMs) under the prompt-based learning paradigm. However, UATs found in previous works are often unreadable tokens or characters a
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Disentangled Object-Centric Transformer (DOCTR)&#65292;&#26088;&#22312;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#20415;&#21033;&#22810;&#20010;&#23545;&#35937;&#23398;&#20064;&#28857;&#22330;&#26223;&#29702;&#35299;&#30340;&#22810;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#35821;&#20041;-&#20960;&#20309;&#35299;&#32806;&#26597;&#35810;&#35774;&#35745;&#26469;&#20248;&#21270;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>https://arxiv.org/abs/2403.16431</link><description>&lt;p&gt;
DOCTR&#65306;&#28857;&#22330;&#26223;&#29702;&#35299;&#30340;&#35299;&#32806;&#23545;&#35937;&#20013;&#24515;Transformer
&lt;/p&gt;
&lt;p&gt;
DOCTR: Disentangled Object-Centric Transformer for Point Scene Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16431
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Disentangled Object-Centric Transformer (DOCTR)&#65292;&#26088;&#22312;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#20415;&#21033;&#22810;&#20010;&#23545;&#35937;&#23398;&#20064;&#28857;&#22330;&#26223;&#29702;&#35299;&#30340;&#22810;&#20010;&#23376;&#20219;&#21153;&#65292;&#24182;&#24341;&#20837;&#20102;&#35821;&#20041;-&#20960;&#20309;&#35299;&#32806;&#26597;&#35810;&#35774;&#35745;&#26469;&#20248;&#21270;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28857;&#22330;&#26223;&#29702;&#35299;&#26159;&#22788;&#29702;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#28857;&#20113;&#30340;&#19968;&#39033;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#26088;&#22312;&#21516;&#26102;&#23545;&#27599;&#20010;&#23545;&#35937;&#36827;&#34892;&#20998;&#21106;&#65292;&#20272;&#35745;&#20854;&#23039;&#24577;&#24182;&#37325;&#26032;&#26500;&#24314;&#20854;&#32593;&#26684;&#12290;&#26368;&#36817;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#39318;&#20808;&#20998;&#21106;&#27599;&#20010;&#23545;&#35937;&#65292;&#28982;&#21518;&#20351;&#29992;&#22810;&#20010;&#38454;&#27573;&#20998;&#21035;&#22788;&#29702;&#19981;&#21516;&#30340;&#23376;&#20219;&#21153;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#22797;&#26434;&#30340;&#27969;&#27700;&#32447;&#20248;&#21270;&#65292;&#24182;&#20351;&#24471;&#38590;&#20197;&#21033;&#29992;&#22810;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#32422;&#26463;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;Disentangled Object-Centric TRansformer (DOCTR)&#65292;&#23427;&#25506;&#32034;&#20102;&#23545;&#35937;&#20013;&#24515;&#34920;&#31034;&#65292;&#20197;&#20415;&#20197;&#32479;&#19968;&#30340;&#26041;&#24335;&#20415;&#21033;&#22810;&#20010;&#23545;&#35937;&#23398;&#20064;&#22810;&#20010;&#23376;&#20219;&#21153;&#12290;&#27599;&#20010;&#23545;&#35937;&#34987;&#34920;&#31034;&#20026;&#19968;&#20010;&#26597;&#35810;&#65292;&#19968;&#20010;Transformer&#35299;&#30721;&#22120;&#34987;&#35843;&#25972;&#20026;&#36845;&#20195;&#22320;&#20248;&#21270;&#28041;&#21450;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#25152;&#26377;&#26597;&#35810;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#35821;&#20041;&#20960;&#20309;&#35299;&#32806;&#26597;&#35810;&#65288;SGDQ&#65289;&#35774;&#35745;&#65292;&#20351;&#26597;&#35810;&#29305;&#24449;&#33021;&#22815;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16431v1 Announce Type: cross  Abstract: Point scene understanding is a challenging task to process real-world scene point cloud, which aims at segmenting each object, estimating its pose, and reconstructing its mesh simultaneously. Recent state-of-the-art method first segments each object and then processes them independently with multiple stages for the different sub-tasks. This leads to a complex pipeline to optimize and makes it hard to leverage the relationship constraints between multiple objects. In this work, we propose a novel Disentangled Object-Centric TRansformer (DOCTR) that explores object-centric representation to facilitate learning with multiple objects for the multiple sub-tasks in a unified manner. Each object is represented as a query, and a Transformer decoder is adapted to iteratively optimize all the queries involving their relationship. In particular, we introduce a semantic-geometry disentangled query (SGDQ) design that enables the query features to a
&lt;/p&gt;</description></item><item><title>Re2LLM&#26159;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#25552;&#20986;&#30340;&#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#23548;LLMs&#19987;&#27880;&#20110;&#26356;&#20934;&#30830;&#25512;&#33616;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.16427</link><description>&lt;p&gt;
Re2LLM: &#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Re2LLM: Reflective Reinforcement Large Language Model for Session-based Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16427
&lt;/p&gt;
&lt;p&gt;
Re2LLM&#26159;&#20026;&#22522;&#20110;&#20250;&#35805;&#30340;&#25512;&#33616;&#25552;&#20986;&#30340;&#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24341;&#23548;LLMs&#19987;&#27880;&#20110;&#26356;&#20934;&#30830;&#25512;&#33616;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#27491;&#26085;&#30410;&#34987;&#30475;&#20316;&#26159;&#22686;&#24378;&#22522;&#20110;&#20250;&#35805;&#25512;&#33616;(SBR)&#30340;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#24050;&#24191;&#27867;&#30740;&#31350;&#20102;&#22522;&#20110;&#25552;&#31034;&#21644;&#24494;&#35843;&#30340;&#26041;&#27861;&#65292;&#20197;&#20351;LLMs&#19982;SBR&#23545;&#40784;&#12290;&#28982;&#32780;&#65292;&#21069;&#32773;&#22240;&#32570;&#20047;&#20219;&#21153;&#29305;&#23450;&#21453;&#39304;&#32780;&#38590;&#20197;&#25214;&#21040;&#24341;&#23548;LLMs&#27491;&#30830;&#25512;&#29702;&#30340;&#26368;&#20339;&#25552;&#31034;&#65292;&#23548;&#33268;&#25512;&#33616;&#32467;&#26524;&#19981;&#20339;&#12290;&#23613;&#31649;&#21518;&#32773;&#35797;&#22270;&#29992;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#24494;&#35843;LLMs&#65292;&#20294;&#23427;&#20204;&#38754;&#20020;&#35832;&#22914;&#39640;&#35745;&#31639;&#25104;&#26412;&#21644;&#20381;&#36182;&#24320;&#28304;&#39592;&#24178;&#30340;&#38480;&#21046;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;SBR&#30340;&#21453;&#23556;&#24335;&#24378;&#21270;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(Re2LLM)&#65292;&#24341;&#23548;LLMs&#19987;&#27880;&#20110;&#26356;&#20934;&#30830;&#25512;&#33616;&#25152;&#38656;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#23454;&#29616;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#35774;&#35745;&#20102;&#21453;&#23556;&#24335;&#25506;&#32034;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16427v1 Announce Type: new  Abstract: Large Language Models (LLMs) are emerging as promising approaches to enhance session-based recommendation (SBR), where both prompt-based and fine-tuning-based methods have been widely investigated to align LLMs with SBR.   However, the former methods struggle with optimal prompts to elicit the correct reasoning of LLMs due to the lack of task-specific feedback, leading to unsatisfactory recommendations.   Although the latter methods attempt to fine-tune LLMs with domain-specific knowledge, they face limitations such as high computational costs and reliance on open-source backbones.   To address such issues, we propose a \underline{Re}flective \underline{Re}inforcement \underline{L}arge \underline{L}anguage \underline{M}odel (Re2LLM) for SBR, guiding LLMs to focus on specialized knowledge essential for more accurate recommendations effectively and efficiently.   In particular, we first design the Reflective Exploration Module to effective
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20027;&#39064;&#26631;&#22836;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#35299;&#20915;&#23398;&#26415;&#22270;&#20070;&#39302;&#24453;&#32534;&#30446;&#39033;&#30446;&#31215;&#21387;&#38382;&#39064;&#20855;&#26377;&#25112;&#30053;&#24212;&#23545;&#24847;&#20041;&#65292;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#20154;&#31867;&#32534;&#30446;&#21592;&#20173;&#28982;&#22312;&#39564;&#35777;&#21644;&#22686;&#24378;&#29983;&#25104;&#20027;&#39064;&#26631;&#22836;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16424</link><description>&lt;p&gt;
&#20351;&#29992;ChatGPT&#20026;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#25351;&#23450;LCSH&#20027;&#39064;&#30340;&#23454;&#39564;
&lt;/p&gt;
&lt;p&gt;
An Experiment with the Use of ChatGPT for LCSH Subject Assignment on Electronic Theses and Dissertations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20027;&#39064;&#26631;&#22836;&#30340;&#28508;&#21147;&#65292;&#23637;&#31034;&#20102;&#20854;&#23545;&#20110;&#35299;&#20915;&#23398;&#26415;&#22270;&#20070;&#39302;&#24453;&#32534;&#30446;&#39033;&#30446;&#31215;&#21387;&#38382;&#39064;&#20855;&#26377;&#25112;&#30053;&#24212;&#23545;&#24847;&#20041;&#65292;&#21516;&#26102;&#20063;&#24378;&#35843;&#20102;&#20154;&#31867;&#32534;&#30446;&#21592;&#20173;&#28982;&#22312;&#39564;&#35777;&#21644;&#22686;&#24378;&#29983;&#25104;&#20027;&#39064;&#26631;&#22836;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#32654;&#22269;&#22269;&#20250;&#22270;&#20070;&#39302;&#20027;&#39064;&#26631;&#22836;&#65288;LCSH&#65289;&#30340;&#28508;&#21147;&#12290;&#20316;&#32773;&#20351;&#29992;ChatGPT&#26681;&#25454;&#30005;&#23376;&#23398;&#20301;&#35770;&#25991;&#30340;&#26631;&#39064;&#21644;&#25688;&#35201;&#29983;&#25104;&#20027;&#39064;&#26631;&#22836;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23613;&#31649;&#19968;&#20123;&#29983;&#25104;&#30340;&#20027;&#39064;&#26631;&#22836;&#26159;&#26377;&#25928;&#30340;&#65292;&#20294;&#23384;&#22312;&#29305;&#23450;&#24615;&#21644;&#35814;&#23613;&#24615;&#26041;&#38754;&#30340;&#38382;&#39064;&#12290;&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;LLMs&#21487;&#20197;&#20316;&#20026;&#23398;&#26415;&#22270;&#20070;&#39302;&#24453;&#32534;&#30446;&#39033;&#30446;&#30340;&#25112;&#30053;&#24615;&#24212;&#23545;&#25514;&#26045;&#65292;&#21516;&#26102;&#20063;&#25552;&#20379;&#20102;&#19968;&#31181;&#25104;&#26412;&#25928;&#30410;&#39640;&#19988;&#24555;&#36895;&#29983;&#25104;LCSH&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20154;&#31867;&#32534;&#30446;&#21592;&#20173;&#28982;&#26159;&#39564;&#35777;&#21644;&#22686;&#24378;LLMs&#29983;&#25104;&#30340;LCSH&#30340;&#26377;&#25928;&#24615;&#12289;&#35814;&#23613;&#24615;&#21644;&#29305;&#23450;&#24615;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16424v1 Announce Type: new  Abstract: This study delves into the potential use of Large Language Models (LLMs) for generating Library of Congress Subject Headings (LCSH). The authors employed ChatGPT to generate subject headings for electronic theses and dissertations (ETDs) based on their titles and summaries. The results revealed that although some generated subject headings were valid, there were issues regarding specificity and exhaustiveness. The study showcases that LLMs can serve as a strategic response to the backlog of items awaiting cataloging in academic libraries, while also offering a cost-effective approach for promptly generating LCSH. Nonetheless, human catalogers remain essential for verifying and enhancing the validity, exhaustiveness, and specificity of LCSH generated by LLMs.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;LenCom-Eval&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#20173;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;</title><link>https://arxiv.org/abs/2403.16422</link><description>&lt;p&gt;
&#20248;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#65306;&#21521;&#20934;&#30830;&#30340;&#26080;&#38656;&#35757;&#32451;&#30340;&#23383;&#24418;&#22686;&#24378;&#22270;&#20687;&#29983;&#25104;&#36808;&#36827;
&lt;/p&gt;
&lt;p&gt;
Refining Text-to-Image Generation: Towards Accurate Training-Free Glyph-Enhanced Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16422
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;LenCom-Eval&#22522;&#20934;&#27979;&#35797;&#65292;&#30740;&#31350;&#32773;&#21457;&#29616;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#20173;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#24182;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#24180;&#65292;&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#29983;&#25104;&#26041;&#27861;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#26222;&#36890;&#25193;&#25955;&#27169;&#22411;&#36890;&#24120;&#22312;&#29983;&#25104;&#22270;&#20687;&#20013;&#26174;&#31034;&#30340;&#25991;&#26412;&#20013;&#23384;&#22312;&#25340;&#20889;&#19981;&#20934;&#30830;&#30340;&#38382;&#39064;&#12290;&#29983;&#25104;&#35270;&#35273;&#25991;&#26412;&#30340;&#33021;&#21147;&#33267;&#20851;&#37325;&#35201;&#65292;&#19981;&#20165;&#20855;&#26377;&#23398;&#26415;&#20215;&#20540;&#65292;&#36824;&#26377;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#29983;&#25104;&#20934;&#30830;&#30340;&#35270;&#35273;&#25991;&#26412;&#22270;&#20687;&#65292;&#26368;&#20808;&#36827;&#30340;&#25216;&#26415;&#37319;&#29992;&#20102;&#19968;&#31181;&#23383;&#24418;&#25511;&#21046;&#30340;&#22270;&#20687;&#29983;&#25104;&#26041;&#27861;&#65292;&#21253;&#25324;&#25991;&#26412;&#24067;&#23616;&#29983;&#25104;&#22120;&#65292;&#28982;&#21518;&#26159;&#19968;&#20010;&#22312;&#29983;&#25104;&#30340;&#25991;&#26412;&#24067;&#23616;&#30340;&#26465;&#20214;&#19979;&#29983;&#25104;&#22270;&#20687;&#30340;&#22270;&#20687;&#29983;&#25104;&#22120;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20173;&#28982;&#38754;&#20020;&#19977;&#20010;&#20027;&#35201;&#25361;&#25112;&#65292;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#27979;&#35797;&#24179;&#21488;&#26469;&#20419;&#36827;&#26410;&#26469;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;LenCom-Eval&#30340;&#22522;&#20934;&#27979;&#35797;&#65292;&#19987;&#38376;&#29992;&#20110;&#27979;&#35797;&#27169;&#22411;&#22312;&#29983;&#25104;&#20855;&#26377;&#22797;&#26434;&#35270;&#35273;&#25991;&#26412;&#30340;&#22270;&#20687;&#26041;&#38754;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16422v1 Announce Type: cross  Abstract: Over the past few years, Text-to-Image (T2I) generation approaches based on diffusion models have gained significant attention. However, vanilla diffusion models often suffer from spelling inaccuracies in the text displayed within the generated images. The capability to generate visual text is crucial, offering both academic interest and a wide range of practical applications. To produce accurate visual text images, state-of-the-art techniques adopt a glyph-controlled image generation approach, consisting of a text layout generator followed by an image generator that is conditioned on the generated text layout. Nevertheless, our study reveals that these models still face three primary challenges, prompting us to develop a testbed to facilitate future research. We introduce a benchmark, LenCom-Eval, specifically designed for testing models' capability in generating images with Lengthy and Complex visual text. Subsequently, we introduce 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#22686;&#24335;MaxSAT&#30340;&#23398;&#20064;&#24179;&#34913;&#35268;&#21017;&#27169;&#22411;IMLIB&#65292;&#32467;&#21512;&#20102;SAT&#21644;MaxSAT&#26041;&#27861;&#65292;&#38480;&#21046;&#35268;&#21017;&#22823;&#23567;&#20197;&#23454;&#29616;&#24179;&#34913;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.16418</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#36882;&#22686;&#24335;MaxSAT&#30340;&#23398;&#20064;&#24179;&#34913;&#35268;&#21017;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
An incremental MaxSAT-based model to learn balanced rules
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16418
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36882;&#22686;&#24335;MaxSAT&#30340;&#23398;&#20064;&#24179;&#34913;&#35268;&#21017;&#27169;&#22411;IMLIB&#65292;&#32467;&#21512;&#20102;SAT&#21644;MaxSAT&#26041;&#27861;&#65292;&#38480;&#21046;&#35268;&#21017;&#22823;&#23567;&#20197;&#23454;&#29616;&#24179;&#34913;&#65292;&#24182;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#19981;&#26029;&#21457;&#23637;&#23548;&#33268;&#20102;&#20247;&#22810;&#24212;&#29992;&#31243;&#24207;&#30340;&#24320;&#21457;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#24182;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#20165;&#20934;&#30830;&#24615;&#21487;&#33021;&#19981;&#36275;&#22815;&#12290;&#35768;&#22810;&#23454;&#38469;&#38382;&#39064;&#36824;&#38656;&#35201;&#35299;&#37322;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#26088;&#22312;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;MaxSAT&#30340;&#22686;&#37327;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#21487;&#35299;&#37322;&#19988;&#24179;&#34913;&#30340;&#35268;&#21017;&#65292;&#31216;&#20026;IMLIB&#12290;&#36825;&#20010;&#26032;&#27169;&#22411;&#22522;&#20110;&#21478;&#22806;&#20004;&#31181;&#26041;&#27861;&#65292;&#19968;&#31181;&#26159;&#22522;&#20110;SAT&#30340;&#65292;&#21478;&#19968;&#31181;&#26159;&#22522;&#20110;MaxSAT&#30340;&#12290;&#22522;&#20110;SAT&#30340;&#26041;&#27861;&#38480;&#21046;&#20102;&#27599;&#20010;&#29983;&#25104;&#35268;&#21017;&#30340;&#22823;&#23567;&#65292;&#20351;&#24471;&#21487;&#20197;&#24179;&#34913;&#23427;&#20204;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26679;&#19968;&#32452;&#35268;&#21017;&#27604;&#19968;&#20010;&#28151;&#21512;&#20102;&#22823;&#35268;&#21017;&#21644;&#23567;&#35268;&#21017;&#26356;&#23481;&#26131;&#29702;&#35299;&#12290;&#22522;&#20110;MaxSAT&#30340;&#26041;&#27861;&#65292;&#31216;&#20026;IMLI&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#25552;&#39640;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16418v1 Announce Type: cross  Abstract: The increasing advancements in the field of machine learning have led to the development of numerous applications that effectively address a wide range of problems with accurate predictions. However, in certain cases, accuracy alone may not be sufficient. Many real-world problems also demand explanations and interpretability behind the predictions. One of the most popular interpretable models that are classification rules. This work aims to propose an incremental model for learning interpretable and balanced rules based on MaxSAT, called IMLIB. This new model was based on two other approaches, one based on SAT and the other on MaxSAT. The one based on SAT limits the size of each generated rule, making it possible to balance them. We suggest that such a set of rules seem more natural to be understood compared to a mixture of large and small rules. The approach based on MaxSAT, called IMLI, presents a technique to increase performance th
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23545;LLMs&#26500;&#24314;CRS&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20998;&#26512;&#65292;&#20026;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16416</link><description>&lt;p&gt;
&#24744;&#30340;&#27169;&#25311;&#22120;&#26377;&#22810;&#21487;&#38752;? &#23545;&#24403;&#21069;&#22522;&#20110;LLM&#30340;&#29992;&#25143;&#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#27169;&#25311;&#22120;&#23616;&#38480;&#24615;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
How Reliable is Your Simulator? Analysis on the Limitations of Current LLM-based User Simulators for Conversational Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16416
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;LLMs&#26500;&#24314;CRS&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#23616;&#38480;&#24615;&#36827;&#34892;&#20998;&#26512;&#65292;&#20026;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#20102;&#37325;&#35201;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16416v1 &#20844;&#21578;&#31867;&#22411;: &#26032; &#25277;&#35937;: &#23545;&#35805;&#25512;&#33616;&#31995;&#32479;&#65288;CRS&#65289;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#19982;&#29992;&#25143;&#20132;&#20114;&#65292;&#20102;&#35299;&#20182;&#20204;&#30340;&#20559;&#22909;&#24182;&#23454;&#26102;&#25552;&#20379;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;CRS&#24050;&#23637;&#31034;&#20986;&#26174;&#33879;&#28508;&#21147;&#65292;&#20419;&#20351;&#30740;&#31350;&#20154;&#21592;&#23558;&#21457;&#23637;&#26356;&#29616;&#23454;&#21644;&#21487;&#38752;&#30340;&#29992;&#25143;&#27169;&#25311;&#22120;&#20316;&#20026;&#37325;&#28857;&#12290;&#26368;&#36817;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#33021;&#21147;&#22312;&#21508;&#20010;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#27491;&#22312;&#21162;&#21147;&#22522;&#20110;LLMs&#26500;&#24314;&#29992;&#25143;&#27169;&#25311;&#22120;&#12290;&#34429;&#28982;&#36825;&#20123;&#24037;&#20316;&#23637;&#29616;&#20102;&#21019;&#26032;&#65292;&#20294;&#20063;&#23384;&#22312;&#19968;&#23450;&#30340;&#23616;&#38480;&#24615;&#38656;&#35201;&#20851;&#27880;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#20998;&#26512;&#20351;&#29992;LLMs&#26500;&#24314;CRS&#29992;&#25143;&#27169;&#25311;&#22120;&#30340;&#23616;&#38480;&#24615;&#65292;&#20197;&#25351;&#23548;&#26410;&#26469;&#30740;&#31350;&#12290;&#20026;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#23545;&#30693;&#21517;&#24037;&#20316;iEvaLM&#36827;&#34892;&#20102;&#20998;&#26512;&#39564;&#35777;&#12290;&#36890;&#36807;&#22312;&#23545;&#35805;&#25512;&#33616;&#39046;&#22495;&#20013;&#20004;&#20010;&#24191;&#27867;&#20351;&#29992;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#22810;&#27425;&#23454;&#39564;&#65292;&#25105;&#20204;&#31361;&#26174;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16416v1 Announce Type: new  Abstract: Conversational Recommender System (CRS) interacts with users through natural language to understand their preferences and provide personalized recommendations in real-time. CRS has demonstrated significant potential, prompting researchers to address the development of more realistic and reliable user simulators as a key focus. Recently, the capabilities of Large Language Models (LLMs) have attracted a lot of attention in various fields. Simultaneously, efforts are underway to construct user simulators based on LLMs. While these works showcase innovation, they also come with certain limitations that require attention. In this work, we aim to analyze the limitations of using LLMs in constructing user simulators for CRS, to guide future research. To achieve this goal, we conduct analytical validation on the notable work, iEvaLM. Through multiple experiments on two widely-used datasets in the field of conversational recommendation, we highli
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;FedU2&#26041;&#27861;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#32479;&#19968;&#27491;&#21017;&#21270;&#22120;&#65288;FUR&#65289;&#21644;&#39640;&#25928;&#30340;&#32479;&#19968;&#32858;&#21512;&#22120;&#65288;EUA&#65289;&#65292;&#22686;&#24378;&#20102;&#22312;&#20855;&#26377;&#38750;IID&#25968;&#25454;&#30340;FUSL&#20013;&#29983;&#25104;&#32479;&#19968;&#21644;&#32479;&#19968;&#34920;&#31034;&#12290;</title><link>https://arxiv.org/abs/2403.16398</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#32852;&#37030;&#38750;IID&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#20013;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Representation in Federated Unsupervised Learning with Non-IID Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16398
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;FedU2&#26041;&#27861;&#65292;&#36890;&#36807;&#28789;&#27963;&#30340;&#32479;&#19968;&#27491;&#21017;&#21270;&#22120;&#65288;FUR&#65289;&#21644;&#39640;&#25928;&#30340;&#32479;&#19968;&#32858;&#21512;&#22120;&#65288;EUA&#65289;&#65292;&#22686;&#24378;&#20102;&#22312;&#20855;&#26377;&#38750;IID&#25968;&#25454;&#30340;FUSL&#20013;&#29983;&#25104;&#32479;&#19968;&#21644;&#32479;&#19968;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16398v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#32852;&#37030;&#23398;&#20064;&#22312;&#24314;&#27169;&#20998;&#24067;&#24335;&#25968;&#25454;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#25928;&#24615;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#23458;&#25143;&#31471;&#25968;&#25454;&#26631;&#31614;&#19981;&#23436;&#21892;&#65292;&#36825;&#20351;&#24471;&#32852;&#37030;&#38750;IID&#25968;&#25454;&#30340;&#26080;&#30417;&#30563;&#23398;&#20064;&#65288;FUSL&#65289;&#20855;&#26377;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;FUSL&#26041;&#27861;&#30340;&#24615;&#33021;&#21463;&#21040;&#34920;&#31034;&#19981;&#36275;&#30340;&#24433;&#21709;&#65292;&#21363;&#65288;1&#65289;&#23616;&#37096;&#21644;&#20840;&#23616;&#27169;&#22411;&#20043;&#38388;&#30340;&#34920;&#31034;&#23849;&#28291;&#32416;&#32544;&#65292;&#20197;&#21450;&#65288;2&#65289;&#23616;&#37096;&#27169;&#22411;&#20043;&#38388;&#34920;&#31034;&#31354;&#38388;&#30340;&#19981;&#19968;&#33268;&#12290;&#21069;&#32773;&#34920;&#31034;&#23616;&#37096;&#27169;&#22411;&#20013;&#30340;&#34920;&#31034;&#23849;&#28291;&#23558;&#38543;&#21518;&#24433;&#21709;&#20840;&#23616;&#27169;&#22411;&#21644;&#20854;&#20182;&#23616;&#37096;&#27169;&#22411;&#12290;&#21518;&#32773;&#24847;&#21619;&#30528;&#30001;&#20110;&#32570;&#20047;&#30417;&#30563;&#20449;&#21495;&#65292;&#23458;&#25143;&#31471;&#27169;&#22411;&#25968;&#25454;&#34920;&#31034;&#20855;&#26377;&#19981;&#19968;&#33268;&#30340;&#21442;&#25968;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;FedU2&#65292;&#35813;&#26041;&#27861;&#22686;&#24378;&#20102;&#22312;&#20855;&#26377;&#38750;IID&#25968;&#25454;&#30340;FUSL&#20013;&#29983;&#25104;&#32479;&#19968;&#21644;&#32479;&#19968;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;FedU2&#30001;&#28789;&#27963;&#30340;&#32479;&#19968;&#27491;&#21017;&#21270;&#22120;&#65288;FUR&#65289;&#21644;&#39640;&#25928;&#30340;&#32479;&#19968;&#32858;&#21512;&#22120;&#65288;EUA&#65289;&#32452;&#25104;&#12290;&#27599;&#20010;FUR
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16398v1 Announce Type: cross  Abstract: Federated learning achieves effective performance in modeling decentralized data. In practice, client data are not well-labeled, which makes it potential for federated unsupervised learning (FUSL) with non-IID data. However, the performance of existing FUSL methods suffers from insufficient representations, i.e., (1) representation collapse entanglement among local and global models, and (2) inconsistent representation spaces among local models. The former indicates that representation collapse in local model will subsequently impact the global model and other local models. The latter means that clients model data representation with inconsistent parameters due to the deficiency of supervision signals. In this work, we propose FedU2 which enhances generating uniform and unified representation in FUSL with non-IID data. Specifically, FedU2 consists of flexible uniform regularizer (FUR) and efficient unified aggregator (EUA). FUR in each
&lt;/p&gt;</description></item><item><title>RadioGAT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#39057;&#24102;&#23556;&#39057;&#22320;&#22270;&#37325;&#24314;&#20013;&#30340;&#25361;&#25112;&#65292;&#21019;&#26032;&#22320;&#23558;&#27169;&#22411;&#24314;&#27169;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#28040;&#38500;&#20102;&#23545;&#22810;&#21306;&#22495;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;</title><link>https://arxiv.org/abs/2403.16397</link><description>&lt;p&gt;
RadioGAT&#65306;&#22522;&#20110;&#32852;&#21512;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#26694;&#26550;&#29992;&#20110;&#22810;&#39057;&#24102;&#26080;&#32447;&#23556;&#39057;&#22320;&#22270;&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
RadioGAT: A Joint Model-based and Data-driven Framework for Multi-band Radiomap Reconstruction via Graph Attention Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16397
&lt;/p&gt;
&lt;p&gt;
RadioGAT&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#39057;&#24102;&#23556;&#39057;&#22320;&#22270;&#37325;&#24314;&#20013;&#30340;&#25361;&#25112;&#65292;&#21019;&#26032;&#22320;&#23558;&#27169;&#22411;&#24314;&#27169;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#27861;&#30456;&#32467;&#21512;&#65292;&#28040;&#38500;&#20102;&#23545;&#22810;&#21306;&#22495;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#39057;&#24102;&#23556;&#39057;&#22320;&#22270;&#37325;&#24314;&#65288;MB-RMR&#65289;&#26159;&#26080;&#32447;&#36890;&#20449;&#20013;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#65292;&#29992;&#20110;&#20219;&#21153;&#22914;&#39057;&#35889;&#31649;&#29702;&#21644;&#32593;&#32476;&#35268;&#21010;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;MB-RMR&#26041;&#27861;&#22312;&#37096;&#32626;&#20013;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#27169;&#25311;&#25968;&#25454;&#25110;&#23436;&#25972;&#32467;&#26500;&#21270;&#22320;&#38754;&#23454;&#20917;&#65292;&#36825;&#20123;&#25361;&#25112;&#28304;&#33258;&#27169;&#25311;&#25968;&#25454;&#19982;&#23454;&#38469;&#25968;&#25454;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#29616;&#23454;&#19990;&#30028;&#27979;&#37327;&#25968;&#25454;&#30340;&#31232;&#32570;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;RadioGAT&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#22270;&#27880;&#24847;&#21147;&#32593;&#32476;&#65288;GAT&#65289;&#30340;&#26032;&#26694;&#26550;&#65292;&#19987;&#38376;&#29992;&#20110;MB-RMR&#65292;&#22312;&#21333;&#20010;&#21306;&#22495;&#20869;&#28040;&#38500;&#20102;&#23545;&#22810;&#21306;&#22495;&#25968;&#25454;&#38598;&#30340;&#38656;&#27714;&#12290;RadioGAT&#21019;&#26032;&#22320;&#23558;&#22522;&#20110;&#27169;&#22411;&#30340;&#31354;&#38388;-&#39057;&#35889;&#30456;&#20851;&#32534;&#30721;&#19982;&#25968;&#25454;&#39537;&#21160;&#30340;&#23556;&#39057;&#22320;&#22270;&#27867;&#21270;&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#26368;&#22823;&#31243;&#24230;&#22320;&#20943;&#23569;&#23545;&#22823;&#37327;&#25968;&#25454;&#28304;&#30340;&#20381;&#36182;&#12290;&#35813;&#26694;&#26550;&#36890;&#36807;&#21019;&#26032;&#30340;&#32534;&#30721;&#23558;&#31232;&#30095;&#30340;&#22810;&#39057;&#24102;&#25968;&#25454;&#36716;&#25442;&#20026;&#22270;&#32467;&#26500;&#24320;&#22987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16397v1 Announce Type: cross  Abstract: Multi-band radiomap reconstruction (MB-RMR) is a key component in wireless communications for tasks such as spectrum management and network planning. However, traditional machine-learning-based MB-RMR methods, which rely heavily on simulated data or complete structured ground truth, face significant deployment challenges. These challenges stem from the differences between simulated and actual data, as well as the scarcity of real-world measurements. To address these challenges, our study presents RadioGAT, a novel framework based on Graph Attention Network (GAT) tailored for MB-RMR within a single area, eliminating the need for multi-region datasets. RadioGAT innovatively merges model-based spatial-spectral correlation encoding with data-driven radiomap generalization, thus minimizing the reliance on extensive data sources. The framework begins by transforming sparse multi-band data into a graph structure through an innovative encoding
&lt;/p&gt;</description></item><item><title>&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#30340;&#27867;&#21270;&#38382;&#39064;&#28304;&#20110;&#29616;&#35937;&#31354;&#38388;&#20013;&#30340;&#20559;&#24046;&#65292;&#38656;&#35201;&#37327;&#21270;&#21644;&#35299;&#20915;&#35821;&#35328;&#21644;&#35270;&#35273;&#20559;&#24046;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;</title><link>https://arxiv.org/abs/2403.16394</link><description>&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#20013;&#29616;&#35937;&#31354;&#38388;&#20013;&#30340;&#20559;&#24046;&#38459;&#30861;&#20102;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Skews in the Phenomenon Space Hinder Generalization in Text-to-Image Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16394
&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#30340;&#27867;&#21270;&#38382;&#39064;&#28304;&#20110;&#29616;&#35937;&#31354;&#38388;&#20013;&#30340;&#20559;&#24046;&#65292;&#38656;&#35201;&#37327;&#21270;&#21644;&#35299;&#20915;&#35821;&#35328;&#21644;&#35270;&#35273;&#20559;&#24046;&#65292;&#20197;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#39046;&#22495;&#30340;&#25991;&#29486;&#23384;&#22312;&#30528;&#20851;&#20110;&#22914;&#20309;&#24544;&#23454;&#22320;&#32452;&#21512;&#23454;&#20307;&#19982;&#20851;&#31995;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#23545;&#23454;&#20307;-&#20851;&#31995;&#32452;&#21512;&#22914;&#20309;&#26377;&#25928;&#23398;&#20064;&#30340;&#24418;&#24335;&#21270;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#21453;&#26144;&#38382;&#39064;&#32467;&#26500;&#30340;&#22522;&#30784;&#29616;&#35937;&#31354;&#38388;&#24182;&#19981;&#26126;&#30830;&#23450;&#20041;&#65292;&#23548;&#33268;&#20026;&#20102;&#24076;&#26395;&#27867;&#21270;&#22312;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#20013;&#24471;&#20197;&#23637;&#29616;&#32780;&#19981;&#26029;&#36861;&#27714;&#26356;&#22810;&#25968;&#25454;&#12290;&#25105;&#20204;&#29468;&#27979;&#22522;&#30784;&#29616;&#35937;&#23398;&#35206;&#30422;&#33539;&#22260;&#24182;&#26410;&#25353;&#27604;&#20363;&#25193;&#23637;&#65292;&#23548;&#33268;&#25152;&#21576;&#29616;&#29616;&#35937;&#30340;&#20559;&#24046;&#23545;&#27867;&#21270;&#36896;&#25104;&#20102;&#20260;&#23475;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#32479;&#35745;&#24230;&#37327;&#26631;&#20934;&#26469;&#37327;&#21270;&#25968;&#25454;&#38598;&#20013;&#30340;&#35821;&#35328;&#21644;&#35270;&#35273;&#20559;&#24046;&#65292;&#29992;&#20110;&#20851;&#31995;&#23398;&#20064;&#65292;&#24182;&#34920;&#26126;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#30340;&#27867;&#21270;&#22833;&#36133;&#30452;&#25509;&#28304;&#20110;&#29616;&#35937;&#23398;&#35206;&#30422;&#19981;&#23436;&#25972;&#25110;&#19981;&#24179;&#34913;&#12290;&#25105;&#20204;&#39318;&#20808;&#22312;&#21512;&#25104;&#39046;&#22495;&#36827;&#34892;&#23454;&#39564;&#21644;&#28436;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16394v1 Announce Type: cross  Abstract: The literature on text-to-image generation is plagued by issues of faithfully composing entities with relations. But there lacks a formal understanding of how entity-relation compositions can be effectively learned. Moreover, the underlying phenomenon space that meaningfully reflects the problem structure is not well-defined, leading to an arms race for larger quantities of data in the hope that generalization emerges out of large-scale pretraining. We hypothesize that the underlying phenomenological coverage has not been proportionally scaled up, leading to a skew of the presented phenomenon which harms generalization. We introduce statistical metrics that quantify both the linguistic and visual skew of a dataset for relational learning, and show that generalization failures of text-to-image generation are a direct result of incomplete or unbalanced phenomenological coverage. We first perform experiments in a synthetic domain and demo
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#21462;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#24182;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#38169;&#35823;&#26816;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.16393</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#65288;CLED&#65289;
&lt;/p&gt;
&lt;p&gt;
Concurrent Linguistic Error Detection (CLED) for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16393
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#26041;&#26696;&#65292;&#36890;&#36807;&#25552;&#21462;&#25991;&#26412;&#30340;&#35821;&#35328;&#29305;&#24449;&#24182;&#20351;&#29992;&#20998;&#31867;&#22120;&#36827;&#34892;&#38169;&#35823;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24191;&#27867;&#37319;&#29992;&#20351;&#24471;&#23427;&#20204;&#30340;&#21487;&#38752;&#24615;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#38382;&#39064;&#12290;&#38169;&#35823;&#30340;&#26816;&#27979;&#26159;&#20943;&#36731;&#20854;&#23545;&#31995;&#32479;&#24433;&#21709;&#30340;&#31532;&#19968;&#27493;&#65292;&#22240;&#27492;&#65292;LLMs&#30340;&#39640;&#25928;&#38169;&#35823;&#26816;&#27979;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22522;&#20110;&#23545;LLMs&#36755;&#20986;&#36827;&#34892;&#30340;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#36827;&#34892;&#24182;&#21457;&#35821;&#35328;&#38169;&#35823;&#26816;&#27979;&#65288;CLED&#65289;&#65307;&#35813;&#26041;&#26696;&#25552;&#21462;LLMs&#29983;&#25104;&#25991;&#26412;&#30340;&#19968;&#20123;&#35821;&#35328;&#29305;&#24449;&#65292;&#24182;&#23558;&#23427;&#20204;&#36755;&#20837;&#21040;&#19968;&#20010;&#24182;&#21457;&#20998;&#31867;&#22120;&#20013;&#36827;&#34892;&#38169;&#35823;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16393v1 Announce Type: new  Abstract: The wide adoption of Large language models (LLMs) makes their dependability a pressing concern. Detection of errors is the first step to mitigating their impact on a system and thus, efficient error detection for LLMs is an important issue. In many settings, the LLM is considered as a black box with no access to the internal nodes; this prevents the use of many error detection schemes that need access to the model's internal nodes. An interesting observation is that the output of LLMs in error-free operation should be valid and normal text. Therefore, when the text is not valid or differs significantly from normal text, it is likely that there is an error. Based on this observation we propose to perform Concurrent Linguistic Error Detection (CLED); this scheme extracts some linguistic features of the text generated by the LLM and feeds them to a concurrent classifier that detects errors. Since the proposed error detection mechanism only 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#35786;&#26029;&#20449;&#24687;&#20316;&#20026;&#24341;&#23548;&#25552;&#31034;&#65292;Dia-LLaMA&#26694;&#26550;&#23558;LLaMA2-7B&#36866;&#24212;&#20110;CT&#25253;&#21578;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#27491;&#24120;&#21644;&#24322;&#24120;&#26696;&#20363;&#20998;&#24067;&#19981;&#24179;&#34913;&#20197;&#21450;&#24120;&#35265;&#27169;&#26495;&#21477;&#23376;&#28153;&#27809;&#20851;&#38190;&#24322;&#24120;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.16386</link><description>&lt;p&gt;
Dia-LLaMA: &#36808;&#21521;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;CT&#25253;&#21578;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Dia-LLaMA: Towards Large Language Model-driven CT Report Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16386
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#35786;&#26029;&#20449;&#24687;&#20316;&#20026;&#24341;&#23548;&#25552;&#31034;&#65292;Dia-LLaMA&#26694;&#26550;&#23558;LLaMA2-7B&#36866;&#24212;&#20110;CT&#25253;&#21578;&#29983;&#25104;&#65292;&#35299;&#20915;&#20102;&#27491;&#24120;&#21644;&#24322;&#24120;&#26696;&#20363;&#20998;&#24067;&#19981;&#24179;&#34913;&#20197;&#21450;&#24120;&#35265;&#27169;&#26495;&#21477;&#23376;&#28153;&#27809;&#20851;&#38190;&#24322;&#24120;&#20449;&#24687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21307;&#23398;&#25253;&#21578;&#29983;&#25104;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#20173;&#38754;&#20020;&#19968;&#20123;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#22312;&#27491;&#24120;&#21644;&#24322;&#24120;&#26696;&#20363;&#30340;&#20998;&#24067;&#19978;&#23384;&#22312;&#22266;&#26377;&#30340;&#19981;&#24179;&#34913;&#65292;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#23545;&#27491;&#24120;&#26679;&#26412;&#23637;&#29616;&#20986;&#20559;&#35265;&#65292;&#20174;&#32780;&#23548;&#33268;&#19981;&#21487;&#38752;&#30340;&#35786;&#26029;&#12290;&#20854;&#27425;&#65292;&#25253;&#21578;&#20013;&#24120;&#35265;&#27169;&#26495;&#21477;&#23376;&#30340;&#39057;&#32321;&#20986;&#29616;&#21487;&#33021;&#28153;&#27809;&#20102;&#20851;&#38190;&#30340;&#24322;&#24120;&#20449;&#24687;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#24037;&#20316;&#20391;&#37325;&#20110;2D&#33016;&#37096;X&#23556;&#32447;&#65292;&#30001;&#20110;CT&#22270;&#20687;&#30340;&#39640;&#32500;&#29305;&#24615;&#21644;CT-&#25253;&#21578;&#23545;&#30340;&#26377;&#38480;&#24615;&#65292;CT&#25253;&#21578;&#29983;&#25104;&#39046;&#22495;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#26368;&#36817;&#65292;LLM&#24050;&#32463;&#23637;&#31034;&#20102;&#36890;&#36807;&#36866;&#24403;&#30340;&#25552;&#31034;&#29983;&#25104;&#21487;&#38752;&#31572;&#26696;&#30340;&#33021;&#21147;&#65292;&#36825;&#20026;&#35299;&#20915;&#19978;&#36848;&#25361;&#25112;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Dia-LLaMA&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;LLaMA2-7B&#35843;&#25972;&#20026;CT&#25253;&#21578;&#29983;&#25104;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#24341;&#20837;&#35786;&#26029;&#20449;&#24687;&#20316;&#20026;&#24341;&#23548;&#25552;&#31034;&#12290;&#32771;&#34385;&#21040;&#39640;&#32500;&#24230;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16386v1 Announce Type: cross  Abstract: Medical report generation has achieved remarkable advancements yet has still been faced with several challenges. First, the inherent imbalance in the distribution of normal and abnormal cases may lead models to exhibit a biased focus on normal samples, resulting in unreliable diagnoses. Second, the frequent occurrence of common template sentences in the reports may overwhelm the critical abnormal information. Moreover, existing works focus on 2D chest X-rays, leaving CT report generation underexplored due to the high-dimensional nature of CT images and the limited availability of CT-report pairs. Recently, LLM has shown a great ability to generate reliable answers with appropriate prompts, which shed light on addressing the aforementioned challenges. In this paper, we propose Dia-LLaMA, a framework to adapt the LLaMA2-7B for CT report generation by incorporating diagnostic information as guidance prompts. Considering the high dimension
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;</title><link>https://arxiv.org/abs/2403.16369</link><description>&lt;p&gt;
&#20351;&#29992;&#19981;&#21464;&#24615;&#23398;&#20064;&#22522;&#20110;&#21160;&#20316;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Learning Action-based Representations Using Invariance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16369
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#65292;&#23398;&#20064;&#20102;&#19968;&#20010;&#21487;&#20197;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#20351;&#29992;&#39640;&#32500;&#24230;&#35266;&#27979;&#24517;&#39035;&#33021;&#22815;&#22312;&#35768;&#22810;&#22806;&#28304;&#24615;&#24178;&#25200;&#20013;&#35782;&#21035;&#30456;&#20851;&#29366;&#24577;&#29305;&#24449;&#12290;&#19968;&#20010;&#33021;&#22815;&#25429;&#25417;&#21487;&#25511;&#24615;&#30340;&#34920;&#31034;&#36890;&#36807;&#30830;&#23450;&#24433;&#21709;&#20195;&#29702;&#25511;&#21046;&#30340;&#22240;&#32032;&#26469;&#35782;&#21035;&#36825;&#20123;&#29366;&#24577;&#20803;&#32032;&#12290;&#34429;&#28982;&#35832;&#22914;&#36870;&#21160;&#21147;&#23398;&#21644;&#20114;&#20449;&#24687;&#31561;&#26041;&#27861;&#21487;&#20197;&#25429;&#25417;&#26377;&#38480;&#25968;&#37327;&#30340;&#26102;&#38388;&#27493;&#30340;&#21487;&#25511;&#24615;&#65292;&#20294;&#25429;&#33719;&#38271;&#26102;&#38388;&#20803;&#32032;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#30701;&#35270;&#30340;&#21487;&#25511;&#24615;&#21487;&#20197;&#25429;&#25417;&#20195;&#29702;&#21363;&#23558;&#25758;&#21521;&#22681;&#22721;&#30340;&#30636;&#38388;&#65292;&#20294;&#19981;&#33021;&#22312;&#20195;&#29702;&#36824;&#26377;&#19968;&#23450;&#36317;&#31163;&#20043;&#26102;&#25429;&#25417;&#22681;&#22721;&#30340;&#25511;&#21046;&#30456;&#20851;&#24615;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21160;&#20316;&#21452;&#27169;&#25311;&#32534;&#30721;&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21040;&#21452;&#27169;&#25311;&#19981;&#21464;&#37327;&#20551;&#24230;&#37327;&#21551;&#21457;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#36882;&#24402;&#19981;&#21464;&#24615;&#32422;&#26463;&#25193;&#23637;&#20102;&#21333;&#27493;&#25511;&#21046;&#24615;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;&#21160;&#20316;&#21452;&#27169;&#25311;&#23398;&#20064;&#20102;&#19968;&#20010;&#24179;&#28369;&#25240;&#25187;&#36828;&#26399;&#20803;&#32032;&#30340;&#22810;&#27493;&#25511;&#21046;&#24230;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16369v1 Announce Type: cross  Abstract: Robust reinforcement learning agents using high-dimensional observations must be able to identify relevant state features amidst many exogeneous distractors. A representation that captures controllability identifies these state elements by determining what affects agent control. While methods such as inverse dynamics and mutual information capture controllability for a limited number of timesteps, capturing long-horizon elements remains a challenging problem. Myopic controllability can capture the moment right before an agent crashes into a wall, but not the control-relevance of the wall while the agent is still some distance away. To address this we introduce action-bisimulation encoding, a method inspired by the bisimulation invariance pseudometric, that extends single-step controllability with a recursive invariance constraint. By doing this, action-bisimulation learns a multi-step controllability metric that smoothly discounts dist
&lt;/p&gt;</description></item><item><title>ChatDBG&#26159;&#31532;&#19968;&#20010;AI-Powered&#35843;&#35797;&#21161;&#25163;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20256;&#32479;&#35843;&#35797;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#23545;&#35805;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12289;&#25191;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;</title><link>https://arxiv.org/abs/2403.16354</link><description>&lt;p&gt;
ChatDBG: &#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35843;&#35797;&#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
ChatDBG: An AI-Powered Debugging Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16354
&lt;/p&gt;
&lt;p&gt;
ChatDBG&#26159;&#31532;&#19968;&#20010;AI-Powered&#35843;&#35797;&#21161;&#25163;&#65292;&#36890;&#36807;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#38598;&#25104;&#21040;&#20256;&#32479;&#35843;&#35797;&#22120;&#20013;&#65292;&#23454;&#29616;&#20102;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#20043;&#38388;&#30340;&#21327;&#20316;&#23545;&#35805;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#38382;&#39064;&#12289;&#25191;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;ChatDBG&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#35843;&#35797;&#21161;&#25163;&#12290;ChatDBG&#38598;&#25104;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#26174;&#33879;&#22686;&#24378;&#20102;&#20256;&#32479;&#35843;&#35797;&#22120;&#30340;&#21151;&#33021;&#21644;&#29992;&#25143;&#21451;&#22909;&#24615;&#12290;ChatDBG&#20801;&#35768;&#31243;&#24207;&#21592;&#19982;&#35843;&#35797;&#22120;&#36827;&#34892;&#21327;&#20316;&#23545;&#35805;&#65292;&#20351;&#20182;&#20204;&#33021;&#22815;&#25552;&#20986;&#20851;&#20110;&#31243;&#24207;&#29366;&#24577;&#30340;&#22797;&#26434;&#38382;&#39064;&#65292;&#23545;&#23849;&#28291;&#25110;&#26029;&#35328;&#22833;&#36133;&#36827;&#34892;&#26681;&#26412;&#21407;&#22240;&#20998;&#26512;&#65292;&#24182;&#25506;&#32034;&#35832;&#22914;&#8220;&#20026;&#20160;&#20040;x&#20026;&#31354;&#65311;&#8221;&#20043;&#31867;&#30340;&#24320;&#25918;&#24615;&#26597;&#35810;&#12290;&#20026;&#20102;&#22788;&#29702;&#36825;&#20123;&#26597;&#35810;&#65292;ChatDBG&#25480;&#20104;LLM&#33258;&#20027;&#26435;&#65292;&#36890;&#36807;&#21457;&#20986;&#21629;&#20196;&#26469;&#27983;&#35272;&#22534;&#26632;&#21644;&#26816;&#26597;&#31243;&#24207;&#29366;&#24577;&#36827;&#34892;&#35843;&#35797;&#65307;&#28982;&#21518;&#25253;&#21578;&#20854;&#21457;&#29616;&#24182;&#23558;&#25511;&#21046;&#26435;&#20132;&#36824;&#32473;&#31243;&#24207;&#21592;&#12290;&#25105;&#20204;&#30340;ChatDBG&#21407;&#22411;&#19982;&#26631;&#20934;&#35843;&#35797;&#22120;&#38598;&#25104;&#65292;&#21253;&#25324;LLDB&#12289;GDB&#21644;WinDBG&#29992;&#20110;&#26412;&#22320;&#20195;&#30721;&#20197;&#21450;&#29992;&#20110;Python&#30340;Pdb&#12290;&#25105;&#20204;&#22312;&#21508;&#31181;&#20195;&#30721;&#38598;&#21512;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#21253;&#25324;&#20855;&#26377;&#24050;&#30693;&#38169;&#35823;&#30340;C/C++&#20195;&#30721;&#21644;&#19968;&#22871;Python&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16354v1 Announce Type: cross  Abstract: This paper presents ChatDBG, the first AI-powered debugging assistant. ChatDBG integrates large language models (LLMs) to significantly enhance the capabilities and user-friendliness of conventional debuggers. ChatDBG lets programmers engage in a collaborative dialogue with the debugger, allowing them to pose complex questions about program state, perform root cause analysis for crashes or assertion failures, and explore open-ended queries like "why is x null?". To handle these queries, ChatDBG grants the LLM autonomy to take the wheel and drive debugging by issuing commands to navigate through stacks and inspect program state; it then reports its findings and yields back control to the programmer. Our ChatDBG prototype integrates with standard debuggers including LLDB, GDB, and WinDBG for native code and Pdb for Python. Our evaluation across a diverse set of code, including C/C++ code with known bugs and a suite of Python code includi
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;CID&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;ChatGPT&#24182;&#35201;&#27714;&#25552;&#20986;&#24773;&#22659;&#30456;&#20284;&#20294;&#25991;&#26412;&#26377;&#20998;&#27495;&#30340;&#38382;&#39064;&#65292;&#26469;&#33258;&#21160;&#27979;&#35797;&#21644;&#26816;&#27979;ChatGPT&#21709;&#24212;&#20013;&#30340;&#19981;&#27491;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16347</link><description>&lt;p&gt;
ChatGPT&#22312;&#36719;&#20214;&#35780;&#35770;&#20013;&#30340;&#19981;&#27491;&#30830;&#24615;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
ChatGPT Incorrectness Detection in Software Reviews
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16347
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#21517;&#20026;CID&#30340;&#24037;&#20855;&#65292;&#36890;&#36807;&#36845;&#20195;&#25552;&#31034;ChatGPT&#24182;&#35201;&#27714;&#25552;&#20986;&#24773;&#22659;&#30456;&#20284;&#20294;&#25991;&#26412;&#26377;&#20998;&#27495;&#30340;&#38382;&#39064;&#65292;&#26469;&#33258;&#21160;&#27979;&#35797;&#21644;&#26816;&#27979;ChatGPT&#21709;&#24212;&#20013;&#30340;&#19981;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23545;135&#21517;&#36719;&#20214;&#24037;&#31243;&#65288;SE&#65289;&#20174;&#19994;&#32773;&#36827;&#34892;&#20102;&#35843;&#26597;&#65292;&#20197;&#20102;&#35299;&#20182;&#20204;&#22914;&#20309;&#20351;&#29992;&#29983;&#25104;&#24335;AI&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;&#22914;ChatGPT&#65289;&#36827;&#34892;SE&#20219;&#21153;&#12290;&#25105;&#20204;&#21457;&#29616;&#20182;&#20204;&#24076;&#26395;&#23558;ChatGPT&#29992;&#20110;&#36719;&#20214;&#24211;&#36873;&#25321;&#31561;SE&#20219;&#21153;&#65292;&#20294;&#32463;&#24120;&#25285;&#24515;ChatGPT&#21709;&#24212;&#30340;&#30495;&#23454;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#22871;&#25216;&#26415;&#21644;&#19968;&#31181;&#21517;&#20026;CID&#65288;ChatGPT&#19981;&#27491;&#30830;&#24615;&#26816;&#27979;&#22120;&#65289;&#30340;&#24037;&#20855;&#65292;&#29992;&#20110;&#33258;&#21160;&#27979;&#35797;&#21644;&#26816;&#27979;ChatGPT&#21709;&#24212;&#20013;&#30340;&#19981;&#27491;&#30830;&#24615;&#12290;CID&#22522;&#20110;&#36890;&#36807;&#35831;&#27714;&#23545;ChatGPT&#36827;&#34892;&#36845;&#20195;&#25552;&#31034;&#26469;&#25552;&#38382;&#20855;&#26377;&#24773;&#22659;&#30456;&#20284;&#20294;&#25991;&#26412;&#26377;&#20998;&#27495;&#30340;&#38382;&#39064;&#65288;&#20351;&#29992;&#19968;&#31181;&#21033;&#29992;&#25991;&#26412;&#20013;&#30340;&#21464;&#24577;&#20851;&#31995;&#30340;&#26041;&#27861;&#65289;&#12290;CID&#30340;&#22522;&#26412;&#21407;&#21017;&#26159;&#65292;&#23545;&#20110;&#32473;&#23450;&#30340;&#38382;&#39064;&#65292;&#19982;&#20854;&#20182;&#21709;&#24212;&#19981;&#21516;&#65288;&#36328;&#38382;&#39064;&#30340;&#22810;&#20010;&#21270;&#36523;&#65289;&#30340;&#21709;&#24212;&#21487;&#33021;&#26159;&#19981;&#27491;&#30830;&#30340;&#21709;&#24212;&#12290; &#22312;&#19968;&#20010;&#20851;&#20110;&#24211;&#36873;&#25321;&#30340;&#22522;&#20934;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;CID&#33021;&#22815;&#26816;&#27979;ChatGPT&#30340;&#19981;&#27491;&#30830;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16347v1 Announce Type: cross  Abstract: We conducted a survey of 135 software engineering (SE) practitioners to understand how they use Generative AI-based chatbots like ChatGPT for SE tasks. We find that they want to use ChatGPT for SE tasks like software library selection but often worry about the truthfulness of ChatGPT responses. We developed a suite of techniques and a tool called CID (ChatGPT Incorrectness Detector) to automatically test and detect the incorrectness in ChatGPT responses. CID is based on the iterative prompting to ChatGPT by asking it contextually similar but textually divergent questions (using an approach that utilizes metamorphic relationships in texts). The underlying principle in CID is that for a given question, a response that is different from other responses (across multiple incarnations of the question) is likely an incorrect response. In a benchmark study of library selection, we show that CID can detect incorrect responses from ChatGPT with 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#33719;&#21462;&#30340;&#25991;&#26723;&#21644;&#30456;&#20851;&#26597;&#35810;&#26469;&#22686;&#24378;&#20998;&#38754;&#39044;&#27979;&#30340;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19987;&#27880;&#20110;&#20165;&#20351;&#29992;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#26469;&#39044;&#27979;&#20998;&#38754;&#30340;&#26694;&#26550;</title><link>https://arxiv.org/abs/2403.16345</link><description>&lt;p&gt;
&#22522;&#20110;LLM&#32534;&#36753;&#30340;&#22686;&#24378;&#24335;&#20998;&#38754;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Enhanced Facet Generation with LLM Editing
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16345
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#33719;&#21462;&#30340;&#25991;&#26723;&#21644;&#30456;&#20851;&#26597;&#35810;&#26469;&#22686;&#24378;&#20998;&#38754;&#39044;&#27979;&#30340;&#31574;&#30053;&#65292;&#24182;&#25552;&#20986;&#20102;&#19987;&#27880;&#20110;&#20165;&#20351;&#29992;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#26469;&#39044;&#27979;&#20998;&#38754;&#30340;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20449;&#24687;&#26816;&#32034;&#20013;&#65292;&#29992;&#25143;&#26597;&#35810;&#30340;&#20998;&#38754;&#35782;&#21035;&#26159;&#19968;&#39033;&#37325;&#35201;&#20219;&#21153;&#12290;&#22914;&#26524;&#25628;&#32034;&#26381;&#21153;&#33021;&#22815;&#35782;&#21035;&#29992;&#25143;&#26597;&#35810;&#30340;&#20998;&#38754;&#65292;&#23601;&#26377;&#28508;&#21147;&#20026;&#29992;&#25143;&#25552;&#20379;&#26356;&#24191;&#27867;&#30340;&#25628;&#32034;&#32467;&#26524;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#21487;&#20197;&#36890;&#36807;&#21033;&#29992;&#25628;&#32034;&#24341;&#25806;&#33719;&#21462;&#30340;&#26816;&#32034;&#25991;&#26723;&#21644;&#30456;&#20851;&#26597;&#35810;&#26469;&#22686;&#24378;&#20998;&#38754;&#39044;&#27979;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;&#25628;&#32034;&#24341;&#25806;&#20316;&#20026;&#27169;&#22411;&#30340;&#19968;&#37096;&#20998;&#36827;&#34892;&#25193;&#23637;&#21040;&#20854;&#20182;&#24212;&#29992;&#26102;&#23384;&#22312;&#25361;&#25112;&#12290;&#31532;&#19968;&#65292;&#25628;&#32034;&#24341;&#25806;&#19981;&#26029;&#26356;&#26032;&#12290;&#22240;&#27492;&#65292;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#26399;&#38388;&#38468;&#21152;&#20449;&#24687;&#21487;&#33021;&#20250;&#26377;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#20250;&#38477;&#20302;&#24615;&#33021;&#12290;&#31532;&#20108;&#20010;&#25361;&#25112;&#26159;&#20844;&#20849;&#25628;&#32034;&#24341;&#25806;&#26080;&#27861;&#25628;&#32034;&#20869;&#37096;&#25991;&#20214;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#26500;&#24314;&#19968;&#20010;&#21333;&#29420;&#30340;&#25628;&#32034;&#31995;&#32479;&#26469;&#23558;&#20844;&#21496;&#20869;&#37096;&#25991;&#26723;&#32435;&#20837;&#20854;&#20013;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#31574;&#30053;&#65292;&#19987;&#27880;&#20110;&#19968;&#20010;&#21487;&#20197;&#20165;&#36890;&#36807;&#26597;&#35810;&#20316;&#20026;&#36755;&#20837;&#26469;&#39044;&#27979;&#20998;&#38754;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16345v1 Announce Type: cross  Abstract: In information retrieval, facet identification of a user query is an important task. If a search service can recognize the facets of a user's query, it has the potential to offer users a much broader range of search results. Previous studies can enhance facet prediction by leveraging retrieved documents and related queries obtained through a search engine. However, there are challenges in extending it to other applications when a search engine operates as part of the model. First, search engines are constantly updated. Therefore, additional information may change during training and test, which may reduce performance. The second challenge is that public search engines cannot search for internal documents. Therefore, a separate search system needs to be built to incorporate documents from private domains within the company. We propose two strategies that focus on a framework that can predict facets by taking only queries as input withou
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;&#26631;&#20934;&#35270;&#39057;&#21387;&#32553;&#32534;&#35299;&#30721;&#22120;&#23545;&#24191;&#35282;&#40060;&#30524;&#30456;&#26426;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#39318;&#27425;&#20998;&#26512;&#65292;&#20197;&#35777;&#26126;&#26377;&#25439;&#35270;&#39057;&#21387;&#32553;&#30072;&#21464;&#19981;&#20250;&#23545;&#24863;&#30693;&#31639;&#27861;&#30340;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.16338</link><description>&lt;p&gt;
&#35270;&#39057;&#21387;&#32553;&#30072;&#21464;&#23545;&#40060;&#30524;&#30456;&#26426;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Impact of Video Compression Artifacts on Fisheye Camera Visual Perception Tasks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16338
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;&#26631;&#20934;&#35270;&#39057;&#21387;&#32553;&#32534;&#35299;&#30721;&#22120;&#23545;&#24191;&#35282;&#40060;&#30524;&#30456;&#26426;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#39318;&#27425;&#20998;&#26512;&#65292;&#20197;&#35777;&#26126;&#26377;&#25439;&#35270;&#39057;&#21387;&#32553;&#30072;&#21464;&#19981;&#20250;&#23545;&#24863;&#30693;&#31639;&#27861;&#30340;&#24615;&#33021;&#20135;&#29983;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#38656;&#35201;&#24191;&#27867;&#30340;&#25968;&#25454;&#25910;&#38598;&#26041;&#26696;&#26469;&#35206;&#30422;&#26500;&#24314;&#24378;&#22823;&#21644;&#23433;&#20840;&#31995;&#32479;&#25152;&#38656;&#30340;&#21508;&#31181;&#22330;&#26223;&#12290;&#25968;&#25454;&#37327;&#36798;&#21040;&#30334;&#20159;&#23383;&#33410;&#30340;&#25968;&#37327;&#32423;&#65292;&#24517;&#39035;&#38271;&#26102;&#38388;&#23384;&#20648;&#65288;&#21363;&#36710;&#36742;&#29983;&#21629;&#21608;&#26399;&#36229;&#36807;10&#24180;&#65289;&#12290;&#26080;&#25439;&#21387;&#32553;&#25552;&#20379;&#30340;&#21387;&#32553;&#27604;&#19981;&#22815;&#65292;&#22240;&#27492;&#36827;&#34892;&#20102;&#26377;&#25439;&#35270;&#39057;&#21387;&#32553;&#30340;&#30740;&#31350;&#12290;&#24517;&#39035;&#35777;&#26126;&#26377;&#25439;&#35270;&#39057;&#21387;&#32553;&#30072;&#21464;&#19981;&#20250;&#24433;&#21709;&#24863;&#30693;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#24037;&#20316;&#26377;&#38480;&#65292;&#38590;&#20197;&#24471;&#20986;&#22362;&#23454;&#30340;&#32467;&#35770;&#12290;&#29305;&#21035;&#26159;&#65292;&#22312;&#40060;&#30524;&#30456;&#26426;&#39046;&#22495;&#36824;&#27809;&#26377;&#36825;&#26679;&#30340;&#30740;&#31350;&#20869;&#23481;&#65292;&#40060;&#30524;&#30456;&#26426;&#20855;&#26377;&#24456;&#39640;&#30340;&#24452;&#21521;&#30072;&#21464;&#65292;&#21387;&#32553;&#21487;&#33021;&#20250;&#26377;&#26356;&#39640;&#30340;&#30072;&#21464;&#12290;&#40060;&#30524;&#30456;&#26426;&#24120;&#29992;&#20110;&#27773;&#36710;&#31995;&#32479;&#30340;&#19977;&#32500;&#29289;&#20307;&#26816;&#27979;&#20219;&#21153;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23545;&#26631;&#20934;&#35270;&#39057;&#21387;&#32553;&#32534;&#35299;&#30721;&#22120;&#23545;&#23485;&#35270;&#22330;&#35282;&#40060;&#30524;&#30456;&#26426;&#30340;&#24433;&#21709;&#36827;&#34892;&#20102;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16338v1 Announce Type: cross  Abstract: Autonomous driving systems require extensive data collection schemes to cover the diverse scenarios needed for building a robust and safe system. The data volumes are in the order of Exabytes and have to be stored for a long period of time (i.e., more than 10 years of the vehicle's life cycle). Lossless compression doesn't provide sufficient compression ratios, hence, lossy video compression has been explored. It is essential to prove that lossy video compression artifacts do not impact the performance of the perception algorithms. However, there is limited work in this area to provide a solid conclusion. In particular, there is no such work for fisheye cameras, which have high radial distortion and where compression may have higher artifacts. Fisheye cameras are commonly used in automotive systems for 3D object detection task. In this work, we provide the first analysis of the impact of standard video compression codecs on wide FOV fi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GLIDER&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#20998;&#24067;&#36716;&#31227;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#27867;&#21270;&#24615;&#33021;&#20248;&#36234;&#12290;</title><link>https://arxiv.org/abs/2403.16334</link><description>&lt;p&gt;
&#22270;&#22312;&#20998;&#24067;&#36716;&#31227;&#19979;&#30340;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Graphs Generalization under Distribution Shifts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16334
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;GLIDER&#30340;&#26032;&#26694;&#26550;&#65292;&#26088;&#22312;&#35299;&#20915;&#22270;&#32467;&#26500;&#25968;&#25454;&#20013;&#20998;&#24067;&#36716;&#31227;&#24102;&#26469;&#30340;&#25361;&#25112;&#65292;&#24182;&#23454;&#29616;&#27867;&#21270;&#24615;&#33021;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#20005;&#37325;&#20381;&#36182;&#20110;&#29420;&#31435;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#65292;&#24403;&#27979;&#35797;&#20998;&#24067;&#19982;&#35757;&#32451;&#20998;&#24067;&#26377;&#25152;&#20559;&#31163;&#26102;&#20250;&#21463;&#21040;&#38480;&#21046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#20851;&#38190;&#38382;&#39064;&#65292;&#38024;&#23545;&#24050;&#30693;&#20998;&#24067;&#36716;&#31227;&#32780;&#36798;&#21040;&#20196;&#20154;&#28385;&#24847;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#38024;&#23545;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#20998;&#24067;&#22806;&#27867;&#21270;&#26041;&#27861;&#23384;&#22312;&#32570;&#20047;&#26126;&#26224;&#24615;&#19988;&#23578;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#65292;&#20027;&#35201;&#30001;&#20110;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#12290;&#39318;&#20808;&#65292;&#22270;&#20013;&#30340;&#20998;&#24067;&#36716;&#31227;&#36890;&#24120;&#21516;&#26102;&#21457;&#29983;&#22312;&#33410;&#28857;&#23646;&#24615;&#21644;&#22270;&#25299;&#25169;&#19978;&#12290;&#20854;&#27425;&#65292;&#22312;&#21508;&#31181;&#20998;&#24067;&#36716;&#31227;&#20013;&#25429;&#33719;&#19981;&#21464;&#20449;&#24687;&#34987;&#35777;&#26126;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20840;&#26032;&#30340;&#26694;&#26550;&#65292;&#21363;&#22270;&#23398;&#20064;&#19981;&#21464;&#22495;&#29983;&#25104;&#65288;GLIDER&#65289;&#12290;&#20854;&#30446;&#26631;&#26159;(1)&#22810;&#26679;&#21270;&#21464;&#21270;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16334v1 Announce Type: cross  Abstract: Traditional machine learning methods heavily rely on the independent and identically distribution assumption, which imposes limitations when the test distribution deviates from the training distribution. To address this crucial issue, out-of-distribution (OOD) generalization, which aims to achieve satisfactory generalization performance when faced with unknown distribution shifts, has made a significant process. However, the OOD method for graph-structured data currently lacks clarity and remains relatively unexplored due to two primary challenges. Firstly, distribution shifts on graphs often occur simultaneously on node attributes and graph topology. Secondly, capturing invariant information amidst diverse distribution shifts proves to be a formidable challenge. To overcome these obstacles, in this paper, we introduce a novel framework, namely Graph Learning Invariant Domain genERation (GLIDER). The goal is to (1) diversify variations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#31070;&#32463;&#24494;&#30005;&#36335;&#30340;&#21551;&#21457;&#19979;&#65292;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#24494;&#30005;&#36335;&#20316;&#20026;&#32452;&#35013;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#65292;&#36991;&#20813;&#20102;&#32467;&#26500;&#19978;&#30340;&#40784;&#36136;&#21270;&#25152;&#24102;&#26469;&#30340;&#22797;&#26434;&#35757;&#32451;&#21644;&#23398;&#20064;&#24037;&#20855;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16327</link><description>&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#24494;&#30005;&#36335;&#20316;&#20026;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#65306;&#27010;&#24565;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Artificial Neural Microcircuits as Building Blocks: Concept and Challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16327
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#22312;&#29983;&#29289;&#23398;&#20013;&#31070;&#32463;&#24494;&#30005;&#36335;&#30340;&#21551;&#21457;&#19979;&#65292;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#24494;&#30005;&#36335;&#20316;&#20026;&#32452;&#35013;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#30340;&#22522;&#26412;&#26500;&#24314;&#27169;&#22359;&#65292;&#36991;&#20813;&#20102;&#32467;&#26500;&#19978;&#30340;&#40784;&#36136;&#21270;&#25152;&#24102;&#26469;&#30340;&#22797;&#26434;&#35757;&#32451;&#21644;&#23398;&#20064;&#24037;&#20855;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;(ANNs)&#26159;&#26368;&#24191;&#27867;&#24212;&#29992;&#30340;&#29983;&#29289;&#21551;&#21457;&#24335;&#35745;&#31639;&#24418;&#24335;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#30340;&#36235;&#21183;&#26159;&#23558;ANNs&#32467;&#26500;&#19978;&#20445;&#25345;&#40784;&#36136;&#12290;&#27492;&#22806;&#65292;&#36825;&#31181;&#32467;&#26500;&#19978;&#30340;&#40784;&#36136;&#24615;&#38656;&#35201;&#24212;&#29992;&#22797;&#26434;&#30340;&#35757;&#32451;&#21644;&#23398;&#20064;&#24037;&#20855;&#65292;&#20197;&#20135;&#29983;&#29305;&#23450;&#24212;&#29992;&#30340;ANNs&#65292;&#23481;&#26131;&#36935;&#21040;&#36807;&#25311;&#21512;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#31070;&#32463;&#24494;&#30005;&#36335;&#22312;&#29983;&#29289;&#23398;&#20013;&#25152;&#25198;&#28436;&#30340;&#35282;&#33394;&#65292;&#21363;&#26377;&#26426;&#31070;&#32463;&#31995;&#32479;&#30340;&#8220;&#22522;&#26412;&#22788;&#29702;&#20803;&#20214;&#8221;&#12290;&#22914;&#20309;&#20351;&#29992;&#20154;&#24037;&#31070;&#32463;&#24494;&#30005;&#36335;(ANMs)&#32452;&#35013;&#22823;&#22411;&#31070;&#32463;&#32593;&#32476;&#65292;&#29305;&#21035;&#26159;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;(SNNs)&#65292;&#26088;&#22312;&#20316;&#20026;&#29616;&#25104;&#38646;&#20214;&#65292;&#34249;&#30001;&#21033;&#29992;&#26032;&#39062;&#24615;&#25628;&#32034;&#26469;&#20135;&#29983;&#36825;&#31181;&#24494;&#30005;&#36335;&#30446;&#24405;&#30340;&#21021;&#27493;&#24037;&#20316;&#32467;&#26524;&#65307;&#25509;&#30528;&#26159;&#25193;&#23637;&#36825;&#39033;&#21021;&#27493;&#24037;&#20316;&#30340;&#21162;&#21147;&#65292;&#21253;&#25324;&#35752;&#35770;&#30456;&#24212;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16327v1 Announce Type: cross  Abstract: Artificial Neural Networks (ANNs) are one of the most widely employed forms of bio-inspired computation. However the current trend is for ANNs to be structurally homogeneous. Furthermore, this structural homogeneity requires the application of complex training and learning tools that produce application specific ANNs, susceptible to pitfalls such as overfitting. In this paper, an new approach is explored, inspired by the role played in biology by Neural Microcircuits, the so called ``fundamental processing elements'' of organic nervous systems. How large neural networks, particularly Spiking Neural Networks (SNNs) can be assembled using Artificial Neural Microcircuits (ANMs), intended as off-the-shelf components, is articulated; the results of initial work to produce a catalogue of such Microcircuits though the use of Novelty Search is shown; followed by efforts to expand upon this initial work, including a discussion of challenges unc
&lt;/p&gt;</description></item><item><title>LLMs&#24050;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#20840;&#38754;&#23637;&#31034;&#20102;LLMs&#22312;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20854;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#25913;&#36827;&#65292;&#25581;&#31034;&#20102;&#20027;&#35201;&#21457;&#23637;&#36235;&#21183;&#21644;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#35752;&#35770;&#20102;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.16303</link><description>&lt;p&gt;
&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Large Language Models in Biomedical and Health Informatics: A Bibliometric Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16303
&lt;/p&gt;
&lt;p&gt;
LLMs&#24050;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#20013;&#37325;&#35201;&#30340;&#24037;&#20855;&#65292;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#20840;&#38754;&#23637;&#31034;&#20102;LLMs&#22312;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#65292;&#25552;&#20986;&#20102;&#20854;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#24212;&#29992;&#30340;&#25913;&#36827;&#65292;&#25581;&#31034;&#20102;&#20027;&#35201;&#21457;&#23637;&#36235;&#21183;&#21644;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#35752;&#35770;&#20102;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36805;&#36895;&#25104;&#20026;&#29983;&#29289;&#21307;&#23398;&#19982;&#20581;&#24247;&#20449;&#24687;&#23398;&#65288;BHI&#65289;&#20013;&#30340;&#37325;&#35201;&#24037;&#20855;&#65292;&#20026;&#20998;&#26512;&#25968;&#25454;&#12289;&#27835;&#30103;&#24739;&#32773;&#21644;&#24320;&#23637;&#30740;&#31350;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#24335;&#12290;&#26412;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#26088;&#22312;&#36890;&#36807;&#26816;&#26597;&#33258;2022&#24180;&#33267;2023&#24180;&#30340;&#30740;&#31350;&#25991;&#31456;&#21644;&#21512;&#20316;&#32593;&#32476;&#65292;&#20840;&#38754;&#23637;&#31034;LLMs&#22312;BHI&#20013;&#30340;&#24212;&#29992;&#24773;&#20917;&#12290;&#23427;&#36827;&#19968;&#27493;&#25506;&#35752;&#20102;LLMs&#22914;&#20309;&#21487;&#20197;&#25913;&#36827;&#21508;&#31181;BHI&#39046;&#22495;&#20013;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#24212;&#29992;&#65292;&#22914;&#21307;&#23398;&#35786;&#26029;&#12289;&#24739;&#32773;&#21442;&#19982;&#12289;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31649;&#29702;&#21644;&#20010;&#24615;&#21270;&#21307;&#23398;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#25991;&#29486;&#35745;&#37327;&#23398;&#32508;&#36848;&#30830;&#23450;&#20102;&#20851;&#38190;&#36235;&#21183;&#65292;&#32472;&#21046;&#20102;&#30740;&#31350;&#32593;&#32476;&#65292;&#24182;&#31361;&#20986;&#20102;&#36825;&#20010;&#24555;&#36895;&#21457;&#23637;&#39046;&#22495;&#30340;&#20027;&#35201;&#36827;&#23637;&#12290;&#26368;&#21518;&#65292;&#23427;&#35752;&#35770;&#20102;&#22312;BHI&#20013;&#20351;&#29992;LLMs&#30340;&#20262;&#29702;&#20851;&#20999;&#21644;&#23454;&#38469;&#25361;&#25112;&#65292;&#22914;&#25968;&#25454;&#38544;&#31169;&#21644;&#21487;&#38752;&#30340;&#21307;&#30103;&#24314;&#35758;&#12290;&#23637;&#26395;&#26410;&#26469;&#65292;&#25105;&#20204;&#32771;&#34385;LLMs&#22914;&#20309;&#36827;&#19968;&#27493;&#25913;&#21464;&#29983;&#29289;&#21307;&#23398;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16303v1 Announce Type: cross  Abstract: Large Language Models (LLMs) have rapidly become important tools in Biomedical and Health Informatics (BHI), enabling new ways to analyze data, treat patients, and conduct research. This bibliometric review aims to provide a panoramic view of how LLMs have been used in BHI by examining research articles and collaboration networks from 2022 to 2023. It further explores how LLMs can improve Natural Language Processing (NLP) applications in various BHI areas like medical diagnosis, patient engagement, electronic health record management, and personalized medicine. To do this, our bibliometric review identifies key trends, maps out research networks, and highlights major developments in this fast-moving field. Lastly, it discusses the ethical concerns and practical challenges of using LLMs in BHI, such as data privacy and reliable medical recommendations. Looking ahead, we consider how LLMs could further transform biomedical research as we
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#29031;&#26009;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#20154;&#24037;&#24515;&#26234;&#29702;&#35770;&#26469;&#29468;&#27979;&#20154;&#31867;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#21361;&#38505;&#24773;&#20917;&#24182;&#23454;&#26102;&#28040;&#38500;&#21361;&#38505;&#30340;&#31639;&#27861;&#65292;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39640;&#25104;&#21151;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.16291</link><description>&lt;p&gt;
&#22312;&#29031;&#26009;&#26426;&#22120;&#20154;&#20013;&#29468;&#27979;&#20154;&#31867;&#24847;&#22270;&#20197;&#36991;&#20813;&#21361;&#38505;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Guessing human intentions to avoid dangerous situations in caregiving robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16291
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#29031;&#26009;&#26426;&#22120;&#20154;&#20013;&#20351;&#29992;&#20154;&#24037;&#24515;&#26234;&#29702;&#35770;&#26469;&#29468;&#27979;&#20154;&#31867;&#24847;&#22270;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#21361;&#38505;&#24773;&#20917;&#24182;&#23454;&#26102;&#28040;&#38500;&#21361;&#38505;&#30340;&#31639;&#27861;&#65292;&#22312;&#27169;&#25311;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39640;&#25104;&#21151;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35201;&#27714;&#26426;&#22120;&#20154;&#36827;&#34892;&#31038;&#20132;&#20114;&#21160;&#65292;&#23427;&#20204;&#24517;&#39035;&#20934;&#30830;&#35299;&#37322;&#20154;&#31867;&#24847;&#22270;&#24182;&#39044;&#27979;&#28508;&#22312;&#32467;&#26524;&#12290;&#23545;&#20110;&#20026;&#20154;&#31867;&#25252;&#29702;&#35774;&#35745;&#30340;&#31038;&#20132;&#26426;&#22120;&#20154;&#32780;&#35328;&#23588;&#20026;&#37325;&#35201;&#65292;&#21487;&#33021;&#20250;&#38754;&#20020;&#20154;&#31867;&#30340;&#21361;&#38505;&#24773;&#20917;&#65292;&#27604;&#22914;&#26410;&#35265;&#38556;&#30861;&#29289;&#65292;&#24212;&#35813;&#20104;&#20197;&#36991;&#20813;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#20154;&#24037;&#24515;&#26234;&#29702;&#35770;&#65288;ATM&#65289;&#26041;&#27861;&#26469;&#25512;&#26029;&#21644;&#35299;&#37322;&#20154;&#31867;&#24847;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26816;&#27979;&#20154;&#31867;&#39118;&#38505;&#24773;&#20917;&#30340;&#31639;&#27861;&#65292;&#36873;&#25321;&#23454;&#26102;&#28040;&#38500;&#21361;&#38505;&#30340;&#26426;&#22120;&#20154;&#21160;&#20316;&#12290;&#25105;&#20204;&#37319;&#29992;&#22522;&#20110;&#27169;&#25311;&#30340;ATM&#26041;&#27861;&#65292;&#24182;&#37319;&#29992;&#8220;&#20687;&#25105;&#19968;&#26679;&#8221;&#30340;&#31574;&#30053;&#23558;&#24847;&#22270;&#21644;&#21160;&#20316;&#20998;&#37197;&#32473;&#20154;&#31867;&#12290;&#36890;&#36807;&#36825;&#31181;&#31574;&#30053;&#65292;&#26426;&#22120;&#20154;&#22312;&#26377;&#38480;&#26102;&#38388;&#20869;&#21487;&#20197;&#39640;&#25104;&#21151;&#29575;&#22320;&#26816;&#27979;&#21644;&#34892;&#21160;&#12290;&#35813;&#31639;&#27861;&#24050;&#32463;&#20316;&#20026;&#29616;&#26377;&#26426;&#22120;&#20154;&#35748;&#30693;&#26550;&#26500;&#30340;&#19968;&#37096;&#20998;&#23454;&#26045;&#65292;&#24182;&#22312;&#27169;&#25311;&#22330;&#26223;&#20013;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;&#36827;&#34892;&#20102;&#19977;&#20010;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16291v1 Announce Type: cross  Abstract: For robots to interact socially, they must interpret human intentions and anticipate their potential outcomes accurately. This is particularly important for social robots designed for human care, which may face potentially dangerous situations for people, such as unseen obstacles in their way, that should be avoided. This paper explores the Artificial Theory of Mind (ATM) approach to inferring and interpreting human intentions. We propose an algorithm that detects risky situations for humans, selecting a robot action that removes the danger in real time. We use the simulation-based approach to ATM and adopt the 'like-me' policy to assign intentions and actions to people. Using this strategy, the robot can detect and act with a high rate of success under time-constrained situations. The algorithm has been implemented as part of an existing robotics cognitive architecture and tested in simulation scenarios. Three experiments have been co
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20248;&#21270;&#21644;&#25286;&#20998;&#23433;&#20840;&#38656;&#27714;&#30340;&#31649;&#36947;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#33021;&#21147;&#65292;&#24182;&#35782;&#21035;&#22810;&#20313;&#25110;&#30683;&#30462;&#30340;&#38656;&#27714;&#65292;&#20026;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#38656;&#27714;&#21464;&#26356;&#39057;&#32321;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>https://arxiv.org/abs/2403.16289</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24037;&#31243;&#21270;&#33258;&#21160;&#39550;&#39542;&#23433;&#20840;&#38656;&#27714;
&lt;/p&gt;
&lt;p&gt;
Engineering Safety Requirements for Autonomous Driving with Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16289
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33258;&#21160;&#20248;&#21270;&#21644;&#25286;&#20998;&#23433;&#20840;&#38656;&#27714;&#30340;&#31649;&#36947;&#65292;&#36890;&#36807;&#35780;&#20272;LLMs&#30340;&#33021;&#21147;&#65292;&#24182;&#35782;&#21035;&#22810;&#20313;&#25110;&#30683;&#30462;&#30340;&#38656;&#27714;&#65292;&#20026;&#35299;&#20915;&#33258;&#21160;&#39550;&#39542;&#39046;&#22495;&#20013;&#38656;&#27714;&#21464;&#26356;&#39057;&#32321;&#30340;&#25361;&#25112;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27773;&#36710;&#39046;&#22495;&#65292;&#38656;&#27714;&#25991;&#26723;&#30340;&#21464;&#26356;&#21644;&#26356;&#26032;&#39057;&#32321;&#65292;&#36825;&#23545;&#23433;&#20840;&#36816;&#33829;&#26500;&#25104;&#25361;&#25112;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20197;&#20854;&#20986;&#33394;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#21644;&#29983;&#25104;&#33021;&#21147;&#65292;&#22312;&#27599;&#27425;&#26356;&#26032;&#21518;&#33258;&#21160;&#23436;&#21892;&#21644;&#25286;&#20998;&#38656;&#27714;&#26041;&#38754;&#33021;&#21457;&#25381;&#20851;&#38190;&#20316;&#29992;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25552;&#31034;&#21644;LLMs&#31649;&#36947;&#30340;&#21407;&#22411;&#65292;&#25509;&#25910;&#39033;&#30446;&#23450;&#20041;&#24182;&#20197;&#23433;&#20840;&#38656;&#27714;&#30340;&#24418;&#24335;&#36755;&#20986;&#35299;&#20915;&#26041;&#26696;&#12290;&#35813;&#31649;&#36947;&#36824;&#23545;&#38656;&#27714;&#25968;&#25454;&#38598;&#36827;&#34892;&#23457;&#26680;&#65292;&#24182;&#35782;&#21035;&#22810;&#20313;&#25110;&#30683;&#30462;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#39318;&#20808;&#30830;&#23450;&#20102;&#25191;&#34892;HARA&#25152;&#38656;&#30340;&#29305;&#24449;&#65292;&#28982;&#21518;&#23450;&#20041;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#26159;&#21542;&#31526;&#21512;&#36825;&#20123;&#26631;&#20934;&#30340;&#27979;&#35797;&#12290;&#25105;&#20204;&#37319;&#29992;&#35774;&#35745;&#31185;&#23398;&#26041;&#27861;&#36827;&#34892;&#22810;&#27425;&#36845;&#20195;&#65292;&#24182;&#36992;&#35831;&#26469;&#33258;&#19981;&#21516;&#20844;&#21496;&#30340;&#19987;&#23478;&#23450;&#37327;&#21644;&#23450;&#24615;&#35780;&#20272;&#27599;&#20010;&#21608;&#26399;&#12290;&#26368;&#21518;&#65292;&#23454;&#29616;&#20102;&#35813;&#21407;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16289v1 Announce Type: new  Abstract: Changes and updates in the requirement artifacts, which can be frequent in the automotive domain, are a challenge for SafetyOps. Large Language Models (LLMs), with their impressive natural language understanding and generating capabilities, can play a key role in automatically refining and decomposing requirements after each update. In this study, we propose a prototype of a pipeline of prompts and LLMs that receives an item definition and outputs solutions in the form of safety requirements. This pipeline also performs a review of the requirement dataset and identifies redundant or contradictory requirements. We first identified the necessary characteristics for performing HARA and then defined tests to assess an LLM's capability in meeting these criteria. We used design science with multiple iterations and let experts from different companies evaluate each cycle quantitatively and qualitatively. Finally, the prototype was implemented a
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;AVicuna&#65292;&#29983;&#25104;&#20102;PU-VALOR&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#38899;&#39057;-&#35270;&#35273;&#26102;&#38388;&#25351;&#20195;&#23545;&#35805;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#32570;&#20047;&#20934;&#30830;&#26102;&#38388;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#22797;&#26434;&#26102;&#38388;&#32447;&#32034;&#30340;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.16276</link><description>&lt;p&gt;
AVicuna&#65306;&#20855;&#26377;&#20132;&#38169;&#22120;&#21644;&#19978;&#19979;&#25991;&#36793;&#30028;&#23545;&#40784;&#30340;&#38899;&#39057;-&#35270;&#35273;LLM&#29992;&#20110;&#26102;&#38388;&#25351;&#20195;&#23545;&#35805;
&lt;/p&gt;
&lt;p&gt;
AVicuna: Audio-Visual LLM with Interleaver and Context-Boundary Alignment for Temporal Referential Dialogue
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16276
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;AVicuna&#65292;&#29983;&#25104;&#20102;PU-VALOR&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102;&#38899;&#39057;-&#35270;&#35273;&#26102;&#38388;&#25351;&#20195;&#23545;&#35805;&#20013;&#30340;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#32570;&#20047;&#20934;&#30830;&#26102;&#38388;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#21644;&#25972;&#21512;&#22797;&#26434;&#26102;&#38388;&#32447;&#32034;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#20132;&#27969;&#20013;&#65292;&#20154;&#31867;&#32463;&#24120;&#20351;&#29992;&#35821;&#38899;&#21644;&#25163;&#21183;&#26469;&#25351;&#20195;&#29305;&#23450;&#21306;&#22495;&#25110;&#23545;&#35937;&#65292;&#36825;&#20010;&#36807;&#31243;&#31216;&#20026;&#25351;&#20195;&#23545;&#35805;&#65288;RD&#65289;&#12290;&#23613;&#31649;&#20808;&#21069;&#30340;&#30740;&#31350;&#24050;&#32463;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25110;&#22823;&#22411;&#22810;&#27169;&#22411;&#27169;&#22411;&#65288;LMMs&#65289;&#22312;&#38745;&#24577;&#29615;&#22659;&#20013;&#35843;&#26597;&#20102;RD&#65292;&#20294;&#22312;&#38899;&#39057;-&#35270;&#35273;&#23186;&#20307;&#20013;&#25506;&#32034;&#26102;&#38388;&#25351;&#20195;&#23545;&#35805;&#65288;TRD&#65289;&#20173;&#28982;&#26377;&#38480;&#12290;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#38459;&#30861;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#36827;&#23637;&#65306;&#65288;1&#65289;&#32570;&#20047;&#20855;&#26377;&#31934;&#30830;&#26102;&#38388;&#27880;&#37322;&#30340;&#20840;&#38754;&#26410;&#20462;&#21098;&#38899;&#39057;-&#35270;&#35273;&#35270;&#39057;&#25968;&#25454;&#38598;&#65292;&#20197;&#21450;&#65288;2&#65289;&#38656;&#35201;&#26377;&#25928;&#25972;&#21512;&#22797;&#26434;&#30340;&#26102;&#38388;&#21548;&#35273;&#21644;&#35270;&#35273;&#32447;&#32034;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#29983;&#25104;PU-VALOR&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#36229;&#36807;114,000&#20010;&#26410;&#20462;&#21098;&#35270;&#39057;&#30340;&#24191;&#27867;&#38899;&#39057;-&#35270;&#35273;&#25968;&#25454;&#38598;&#65292;&#24182;&#20171;&#32461;&#20102;AVicuna&#65292;&#20855;&#26377;&#38899;&#39057;-&#35270;&#35273;&#20196;&#29260;&#20132;&#38169;&#22120;&#65288;AVTI&#65289;&#65292;&#30830;&#20445;&#20102;&#26102;&#38388;&#23545;&#40784;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16276v1 Announce Type: cross  Abstract: In everyday communication, humans frequently use speech and gestures to refer to specific areas or objects, a process known as Referential Dialogue (RD). While prior studies have investigated RD through Large Language Models (LLMs) or Large Multimodal Models (LMMs) in static contexts, the exploration of Temporal Referential Dialogue (TRD) within audio-visual media remains limited. Two primary challenges hinder progress in this field: (1) the absence of comprehensive, untrimmed audio-visual video datasets with precise temporal annotations, and (2) the need for methods to integrate complex temporal auditory and visual cues effectively. To address these challenges, we introduce a novel framework to generate PU-VALOR, an extensive audio-visual dataset comprising over 114,000 untrimmed videos with accurate temporal demarcations. We also present AVicuna, featuring an Audio-Visual Tokens Interleaver (AVTI) that ensures the temporal alignment 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32437;&#21521;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#65292;&#37319;&#29992;&#26102;&#38388;&#24863;&#30693;&#20301;&#32622;&#23884;&#20837;&#21644;&#30142;&#30149;&#36827;&#23637;&#24863;&#30693;&#25513;&#34109;&#65292;&#29992;&#20110;&#39044;&#27979;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#36827;&#23637;&#65292;&#26088;&#22312;&#26356;&#20934;&#30830;&#35780;&#20272;&#30142;&#30149;&#36827;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.16272</link><description>&lt;p&gt;
L-MAE: &#20855;&#26377;&#26102;&#38388;&#21644;&#20005;&#37325;&#24615;&#24863;&#30693;&#32534;&#30721;&#30340;&#32437;&#21521;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#29992;&#20110;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#36827;&#23637;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
L-MAE: Longitudinal masked auto-encoder with time and severity-aware encoding for diabetic retinopathy progression prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16272
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#32437;&#21521;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#65292;&#37319;&#29992;&#26102;&#38388;&#24863;&#30693;&#20301;&#32622;&#23884;&#20837;&#21644;&#30142;&#30149;&#36827;&#23637;&#24863;&#30693;&#25513;&#34109;&#65292;&#29992;&#20110;&#39044;&#27979;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#36827;&#23637;&#65292;&#26088;&#22312;&#26356;&#20934;&#30830;&#35780;&#20272;&#30142;&#30149;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#35768;&#22810;&#19979;&#28216;&#20219;&#21153;&#26159;&#26377;&#25928;&#30340;&#20266;&#20219;&#21153;&#12290;&#30001;&#20110;&#21307;&#23398;&#21644;&#33258;&#28982;&#22270;&#20687;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#20856;&#22411;SSL&#22312;&#21307;&#23398;&#25104;&#20687;&#20013;&#30340;&#24212;&#29992;&#24182;&#19981;&#30452;&#25509;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#20266;&#20219;&#21153;&#36890;&#24120;&#32570;&#20047;&#23545;&#20110;&#35745;&#31639;&#26426;&#36741;&#21161;&#20020;&#24202;&#20915;&#31574;&#25903;&#25345;&#33267;&#20851;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22522;&#20110;&#33879;&#21517;&#30340;&#22522;&#20110;Transformer&#30340;MAE&#24320;&#21457;&#20102;&#19968;&#31181;&#32437;&#21521;&#36974;&#32617;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;MAE&#65289;&#12290;&#29305;&#21035;&#22320;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26102;&#38388;&#24863;&#30693;&#20301;&#32622;&#23884;&#20837;&#20197;&#21450;&#30142;&#30149;&#36827;&#23637;&#24863;&#30693;&#25513;&#34109;&#30340;&#37325;&#35201;&#24615;&#12290;&#32771;&#34385;&#21040;&#26816;&#26597;&#20043;&#38388;&#30340;&#26102;&#38388;&#32780;&#19981;&#20165;&#20165;&#26159;&#25490;&#23450;&#23427;&#20204;&#65292;&#33021;&#25429;&#25417;&#21040;&#26102;&#38388;&#21464;&#21270;&#21644;&#36235;&#21183;&#30340;&#22909;&#22788;&#12290;&#25513;&#34109;&#31574;&#30053;&#22312;&#21518;&#32493;&#36807;&#31243;&#20013;&#21457;&#23637;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#30149;&#29702;&#21464;&#21270;&#65292;&#30830;&#20445;&#26356;&#20934;&#30830;&#22320;&#35780;&#20272;&#30142;&#30149;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16272v1 Announce Type: cross  Abstract: Pre-training strategies based on self-supervised learning (SSL) have proven to be effective pretext tasks for many downstream tasks in computer vision. Due to the significant disparity between medical and natural images, the application of typical SSL is not straightforward in medical imaging. Additionally, those pretext tasks often lack context, which is critical for computer-aided clinical decision support. In this paper, we developed a longitudinal masked auto-encoder (MAE) based on the well-known Transformer-based MAE. In particular, we explored the importance of time-aware position embedding as well as disease progression-aware masking. Taking into account the time between examinations instead of just scheduling them offers the benefit of capturing temporal changes and trends. The masking strategy, for its part, evolves during follow-up to better capture pathological changes, ensuring a more accurate assessment of disease progress
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#20316;&#32773;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#20256;&#32479;&#27169;&#22411;&#38598;&#25104;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#22810;&#26679;&#24615;&#38480;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.16260</link><description>&lt;p&gt;
&#36890;&#36807;&#28145;&#24230;&#22810;&#29702;&#35299;&#38598;&#25104;&#23454;&#29616;&#36234;&#30028;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Out-of-Distribution Detection via Deep Multi-Comprehension Ensemble
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16260
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#20316;&#32773;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#20851;&#38190;&#32570;&#38519;&#65292;&#25552;&#20986;&#20102;&#25552;&#39640;&#20256;&#32479;&#27169;&#22411;&#38598;&#25104;&#32500;&#24230;&#30340;&#26041;&#27861;&#65292;&#20197;&#20811;&#26381;&#29305;&#24449;&#34920;&#31034;&#20013;&#30340;&#22810;&#26679;&#24615;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#36234;&#30028;&#65288;OOD&#65289;&#29305;&#24449;&#34920;&#31034;&#39046;&#22495;&#35268;&#27169;&#23545;&#27169;&#22411;&#22312;OOD&#26816;&#27979;&#20013;&#25928;&#26524;&#30340;&#37325;&#35201;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#37319;&#29992;&#27169;&#22411;&#38598;&#25104;&#20316;&#20026;&#22686;&#24378;&#36825;&#19968;&#29305;&#24449;&#34920;&#31034;&#39046;&#22495;&#30340;&#31361;&#20986;&#31574;&#30053;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#31361;&#20986;&#30340;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#26399;&#30340;&#27169;&#22411;&#22810;&#26679;&#24615;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26032;&#39062;&#30340;&#23450;&#24615;&#21644;&#23450;&#37327;&#27169;&#22411;&#38598;&#25104;&#35780;&#20272;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#25439;&#22833;&#30406;/&#38556;&#30861;&#21487;&#35270;&#21270;&#21644;&#33258;&#32806;&#21512;&#25351;&#25968;&#65292;&#25581;&#31034;&#20102;&#29616;&#26377;&#38598;&#25104;&#26041;&#27861;&#30340;&#19968;&#20010;&#20851;&#38190;&#32570;&#38519;&#12290;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#21253;&#21547;&#21487;&#36827;&#34892;&#20223;&#23556;&#21464;&#25442;&#30340;&#26435;&#37325;&#65292;&#34920;&#29616;&#20986;&#26377;&#38480;&#30340;&#21487;&#21464;&#24615;&#65292;&#20174;&#32780;&#26410;&#33021;&#23454;&#29616;&#29305;&#24449;&#34920;&#31034;&#20013;&#25152;&#38656;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16260v1 Announce Type: cross  Abstract: Recent research underscores the pivotal role of the Out-of-Distribution (OOD) feature representation field scale in determining the efficacy of models in OOD detection. Consequently, the adoption of model ensembles has emerged as a prominent strategy to augment this feature representation field, capitalizing on anticipated model diversity.   However, our introduction of novel qualitative and quantitative model ensemble evaluation methods, specifically Loss Basin/Barrier Visualization and the Self-Coupling Index, reveals a critical drawback in existing ensemble methods. We find that these methods incorporate weights that are affine-transformable, exhibiting limited variability and thus failing to achieve the desired diversity in feature representation.   To address this limitation, we elevate the dimensions of traditional model ensembles, incorporating various factors such as different weight initializations, data holdout, etc., into di
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#21407;&#23376;&#21147;&#26174;&#24494;&#38236;&#22270;&#20687;&#21644;&#23567;&#25968;&#25454;&#24211;&#26102;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#35782;&#21035;/&#20998;&#31867;&#65292;&#35752;&#35770;&#20102;&#38500;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20043;&#22806;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.16230</link><description>&lt;p&gt;
&#20851;&#20110;&#21407;&#23376;&#21147;&#26174;&#24494;&#38236;&#22270;&#20687;&#30340;&#26426;&#22120;&#23398;&#20064;&#20998;&#26512;&#65292;&#29992;&#20110;&#22270;&#20687;&#20998;&#31867;&#21644;&#26679;&#21697;&#34920;&#38754;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
On machine learning analysis of atomic force microscopy images for image classification, sample surface recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16230
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25506;&#35752;&#20102;&#22312;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;&#21407;&#23376;&#21147;&#26174;&#24494;&#38236;&#22270;&#20687;&#21644;&#23567;&#25968;&#25454;&#24211;&#26102;&#65292;&#24212;&#29992;&#26426;&#22120;&#23398;&#20064;&#36827;&#34892;&#35782;&#21035;/&#20998;&#31867;&#65292;&#35752;&#35770;&#20102;&#38500;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20043;&#22806;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21407;&#23376;&#21147;&#26174;&#24494;&#38236;&#65288;AFM&#25110;SPM&#65289;&#25104;&#20687;&#26159;&#26174;&#24494;&#25216;&#26415;&#20013;&#19982;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#20998;&#26512;&#26368;&#21305;&#37197;&#30340;&#20043;&#19968;&#12290;AFM&#22270;&#20687;&#30340;&#25968;&#23383;&#26684;&#24335;&#20801;&#35768;&#30452;&#25509;&#22312;ML&#31639;&#27861;&#20013;&#20351;&#29992;&#65292;&#26080;&#38656;&#39069;&#22806;&#22788;&#29702;&#12290;&#27492;&#22806;&#65292;AFM&#20351;&#24471;&#33021;&#22815;&#21516;&#26102;&#25104;&#20687;&#26679;&#21697;&#34920;&#38754;&#21313;&#20960;&#31181;&#19981;&#21516;&#29289;&#29702;&#21270;&#23398;&#24615;&#36136;&#30340;&#20998;&#24067;&#65292;&#36825;&#20010;&#36807;&#31243;&#34987;&#31216;&#20026;&#22810;&#32500;&#25104;&#20687;&#12290;&#34429;&#28982;&#36825;&#20123;&#20016;&#23500;&#30340;&#20449;&#24687;&#21487;&#33021;&#38590;&#20197;&#20351;&#29992;&#20256;&#32479;&#26041;&#27861;&#36827;&#34892;&#20998;&#26512;&#65292;&#20294;ML&#20026;&#27492;&#20219;&#21153;&#25552;&#20379;&#20102;&#19968;&#31181;&#26080;&#32541;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;AFM&#25104;&#20687;&#30456;&#23545;&#36739;&#24930;&#30340;&#36895;&#24230;&#22312;&#24212;&#29992;&#24191;&#27867;&#29992;&#20110;&#22270;&#20687;&#35782;&#21035;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26102;&#25552;&#20986;&#20102;&#25361;&#25112;&#12290;&#36825;&#20010;&#21069;&#26223;&#19987;&#27880;&#20110;&#20351;&#29992;&#30456;&#23545;&#36739;&#23569;&#30340;AFM&#22270;&#20687;&#12289;&#23567;&#25968;&#25454;&#24211;&#26102;&#30340;ML&#35782;&#21035;/&#20998;&#31867;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#38500;&#27969;&#34892;&#30340;&#28145;&#24230;&#23398;&#20064;&#31070;&#32463;&#32593;&#32476;&#20043;&#22806;&#30340;ML&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16230v1 Announce Type: cross  Abstract: Atomic force microscopy (AFM or SPM) imaging is one of the best matches with machine learning (ML) analysis among microscopy techniques. The digital format of AFM images allows for direct utilization in ML algorithms without the need for additional processing. Additionally, AFM enables the simultaneous imaging of distributions of over a dozen different physicochemical properties of sample surfaces, a process known as multidimensional imaging. While this wealth of information can be challenging to analyze using traditional methods, ML provides a seamless approach to this task. However, the relatively slow speed of AFM imaging poses a challenge in applying deep learning methods broadly used in image recognition. This Prospective is focused on ML recognition/classification when using a relatively small number of AFM images, small database. We discuss ML methods other than popular deep-learning neural networks. The described approach has a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#26412;&#20307;&#26469;&#26500;&#24314;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.16222</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#32593;&#32476;&#23433;&#20840;&#30693;&#35782;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cyber-Security Knowledge Graph Generation by Hierarchical Nonnegative Matrix Factorization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16222
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#26412;&#20307;&#26469;&#26500;&#24314;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35768;&#22810;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#30340;&#20154;&#31867;&#30693;&#35782;&#37117;&#34987;&#23553;&#35013;&#22312;&#19981;&#26029;&#22686;&#38271;&#30340;&#31185;&#23398;&#35770;&#25991;&#20013;&#12290;&#38543;&#30528;&#36825;&#20123;&#25991;&#26412;&#25968;&#25454;&#30340;&#19981;&#26029;&#25193;&#22823;&#65292;&#25991;&#26723;&#32452;&#32455;&#26041;&#27861;&#30340;&#37325;&#35201;&#24615;&#21464;&#24471;&#26085;&#30410;&#20851;&#38190;&#65292;&#29992;&#20110;&#20174;&#22823;&#22411;&#25991;&#26412;&#25968;&#25454;&#38598;&#20013;&#25552;&#21462;&#38544;&#34255;&#30340;&#21487;&#34892;&#35265;&#35299;&#12290;&#30693;&#35782;&#22270;&#65288;KGs&#65289;&#20316;&#20026;&#19968;&#31181;&#20197;&#32467;&#26500;&#21270;&#26041;&#24335;&#23384;&#20648;&#23454;&#38469;&#20449;&#24687;&#30340;&#25163;&#27573;&#65292;&#25552;&#20379;&#21253;&#25324;&#26469;&#33258;&#32593;&#32476;&#23433;&#20840;&#31185;&#23398;&#25991;&#29486;&#30340;&#39046;&#22495;&#29305;&#23450;&#20449;&#24687;&#22312;&#20869;&#30340;&#26126;&#30830;&#12289;&#21487;&#35299;&#37322;&#30340;&#30693;&#35782;&#12290;&#26500;&#24314;&#20174;&#31185;&#23398;&#25991;&#29486;&#20013;&#30340;&#30693;&#35782;&#22270;&#26159;&#25552;&#21462;&#26412;&#20307;&#30340;&#19968;&#20010;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#36825;&#20010;&#20027;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#20174;&#31185;&#23398;&#35770;&#25991;&#20013;&#25552;&#21462;&#32467;&#26500;&#21270;&#26412;&#20307;&#26469;&#26500;&#24314;&#22810;&#27169;&#24577;KG&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#22312;&#32593;&#32476;&#23433;&#20840;&#39046;&#22495;&#23637;&#31034;&#20102;&#36825;&#19968;&#27010;&#24565;&#12290;KG&#30340;&#19968;&#31181;&#27169;&#24577;&#20195;&#34920;&#20102;&#35770;&#25991;&#20013;&#30340;&#21487;&#35266;&#23519;&#20449;&#24687;&#65292;&#22914;&#30446;&#24405;&#31561;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16222v1 Announce Type: new  Abstract: Much of human knowledge in cybersecurity is encapsulated within the ever-growing volume of scientific papers. As this textual data continues to expand, the importance of document organization methods becomes increasingly crucial for extracting actionable insights hidden within large text datasets. Knowledge Graphs (KGs) serve as a means to store factual information in a structured manner, providing explicit, interpretable knowledge that includes domain-specific information from the cybersecurity scientific literature. One of the challenges in constructing a KG from scientific literature is the extraction of ontology from unstructured text. In this paper, we address this topic and introduce a method for building a multi-modal KG by extracting structured ontology from scientific papers. We demonstrate this concept in the cybersecurity domain. One modality of the KG represents observable information from the papers, such as the categories i
&lt;/p&gt;</description></item><item><title>CoverUp&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#22312;&#25913;&#36827;&#35206;&#30422;&#29575;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#12290;</title><link>https://arxiv.org/abs/2403.16218</link><description>&lt;p&gt;
CoverUp&#65306;&#22522;&#20110;&#35206;&#30422;&#29575;&#24341;&#23548;&#30340;LLM&#27979;&#35797;&#29983;&#25104;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
CoverUp: Coverage-Guided LLM-Based Test Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16218
&lt;/p&gt;
&lt;p&gt;
CoverUp&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30456;&#32467;&#21512;&#30340;&#26041;&#24335;&#65292;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#65292;&#24182;&#22312;&#25913;&#36827;&#35206;&#30422;&#29575;&#26041;&#38754;&#21462;&#24471;&#26174;&#33879;&#25104;&#23601;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;CoverUp&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#22411;&#31995;&#32479;&#65292;&#36890;&#36807;&#35206;&#30422;&#29575;&#20998;&#26512;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32467;&#21512;&#39537;&#21160;&#29983;&#25104;&#39640;&#35206;&#30422;&#29575;&#30340;Python&#22238;&#24402;&#27979;&#35797;&#12290;CoverUp&#36890;&#36807;&#36845;&#20195;&#25913;&#21892;&#35206;&#30422;&#29575;&#65292;&#23558;&#35206;&#30422;&#29575;&#20998;&#26512;&#19982;LLM&#23545;&#35805;&#20132;&#26367;&#36827;&#34892;&#65292;&#20197;&#20415;&#23558;&#27880;&#24847;&#21147;&#38598;&#20013;&#22312;&#23578;&#26410;&#28085;&#30422;&#30340;&#20195;&#30721;&#34892;&#21644;&#20998;&#25903;&#19978;&#12290;&#26368;&#32456;&#30340;&#27979;&#35797;&#22871;&#20214;&#30456;&#27604;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#26174;&#33879;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#65306;&#19982;CodaMosa&#30456;&#27604;&#65292;&#19968;&#31181;&#28151;&#21512;LLM / &#22522;&#20110;&#25628;&#32034;&#30340;&#36719;&#20214;&#27979;&#35797;&#31995;&#32479;&#65292;CoverUp&#22312;&#21508;&#26041;&#38754;&#37117;&#22823;&#24133;&#25552;&#39640;&#20102;&#35206;&#30422;&#29575;&#12290;&#20197;&#27169;&#22359;&#20026;&#22522;&#30784;&#65292;CoverUp&#23454;&#29616;&#20102;81%&#30340;&#20013;&#20301;&#32447;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;62%&#65289;&#12289;53%&#30340;&#20998;&#25903;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;35%&#65289;&#21644;78%&#30340;&#32447;+&#20998;&#25903;&#35206;&#30422;&#29575;&#65288;&#23545;&#27604;55%&#65289;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CoverUp&#30340;&#36845;&#20195;&#12289;&#35206;&#30422;&#29575;&#24341;&#23548;&#26041;&#27861;&#23545;&#20854;&#26377;&#25928;&#24615;&#33267;&#20851;&#37325;&#35201;&#65292;&#20026;&#20854;&#25104;&#21151;&#30340;&#36817;&#19968;&#21322;&#20316;&#20986;&#20102;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16218v1 Announce Type: cross  Abstract: This paper presents CoverUp, a novel system that drives the generation of high-coverage Python regression tests via a combination of coverage analysis and large-language models (LLMs). CoverUp iteratively improves coverage, interleaving coverage analysis with dialogs with the LLM to focus its attention on as yet uncovered lines and branches. The resulting test suites significantly improve coverage over the current state of the art: compared to CodaMosa, a hybrid LLM / search-based software testing system, CoverUp substantially improves coverage across the board. On a per-module basis, CoverUp achieves median line coverage of 81% (vs. 62%), branch coverage of 53% (vs. 35%) and line+branch coverage of 78% (vs. 55%). We show that CoverUp's iterative, coverage-guided approach is crucial to its effectiveness, contributing to nearly half of its successes.
&lt;/p&gt;</description></item><item><title>Frankenstein&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#36947;&#20013;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#35821;&#20041;&#30456;&#20851;&#30340;3D&#24418;&#29366;&#65292;&#20026;&#29983;&#25104;&#25151;&#38388;&#20869;&#37096;&#21644;&#20154;&#31867;&#21270;&#36523;&#31561;&#22330;&#26223;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.16210</link><description>&lt;p&gt;
Frankenstein: &#22312;&#19968;&#20010;&#19977;&#38754;&#20301;&#24179;&#38754;&#20013;&#29983;&#25104;&#35821;&#20041;-&#32452;&#21512;&#24335;3D&#22330;&#26223;
&lt;/p&gt;
&lt;p&gt;
Frankenstein: Generating Semantic-Compositional 3D Scenes in One Tri-Plane
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16210
&lt;/p&gt;
&lt;p&gt;
Frankenstein&#26159;&#19968;&#20010;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#36947;&#20013;&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#35821;&#20041;&#30456;&#20851;&#30340;3D&#24418;&#29366;&#65292;&#20026;&#29983;&#25104;&#25151;&#38388;&#20869;&#37096;&#21644;&#20154;&#31867;&#21270;&#36523;&#31561;&#22330;&#26223;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;Frankenstein&#65292;&#36825;&#26159;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#22312;&#21333;&#20010;&#36890;&#36947;&#20013;&#29983;&#25104;&#35821;&#20041;-&#32452;&#21512;&#24335;3D&#22330;&#26223;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#36755;&#20986;&#21333;&#20010;&#32479;&#19968;&#30340;3D&#24418;&#29366;&#19981;&#21516;&#65292;Frankenstein&#21516;&#26102;&#29983;&#25104;&#22810;&#20010;&#29420;&#31435;&#30340;&#24418;&#29366;&#65292;&#27599;&#20010;&#23545;&#24212;&#19968;&#20010;&#35821;&#20041;&#19978;&#26377;&#24847;&#20041;&#30340;&#37096;&#20998;&#12290;3D&#22330;&#26223;&#20449;&#24687;&#32534;&#30721;&#22312;&#19968;&#20010;&#19977;&#38754;&#20301;&#24179;&#38754;&#24352;&#37327;&#20013;&#65292;&#20174;&#20013;&#21487;&#20197;&#35299;&#30721;&#22810;&#20010;&#31526;&#21495;&#36317;&#31163;&#20989;&#25968;&#65288;SDF&#65289;&#22330;&#20197;&#34920;&#31034;&#32452;&#21512;&#24418;&#29366;&#12290;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#19968;&#20010;&#33258;&#32534;&#30721;&#22120;&#23558;&#19977;&#38754;&#20301;&#24179;&#38754;&#21387;&#32553;&#21040;&#28508;&#22312;&#31354;&#38388;&#65292;&#28982;&#21518;&#20351;&#29992;&#21435;&#22122;&#25193;&#25955;&#36807;&#31243;&#26469;&#36924;&#36817;&#32452;&#21512;&#22330;&#26223;&#30340;&#20998;&#24067;&#12290;Frankenstein&#22312;&#29983;&#25104;&#25151;&#38388;&#20869;&#37096;&#21644;&#20855;&#26377;&#33258;&#21160;&#20998;&#31163;&#37096;&#20998;&#30340;&#20154;&#31867;&#21270;&#36523;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;&#29983;&#25104;&#30340;&#22330;&#26223;&#26377;&#21161;&#20110;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#65292;&#20363;&#22914;&#37096;&#20998;&#37325;&#36148;&#22270;&#12289;&#25151;&#38388;&#25110;&#21270;&#36523;&#34915;&#26381;&#30340;&#23545;&#35937;&#37325;&#26032;&#25490;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16210v1 Announce Type: cross  Abstract: We present Frankenstein, a diffusion-based framework that can generate semantic-compositional 3D scenes in a single pass. Unlike existing methods that output a single, unified 3D shape, Frankenstein simultaneously generates multiple separated shapes, each corresponding to a semantically meaningful part. The 3D scene information is encoded in one single tri-plane tensor, from which multiple Singed Distance Function (SDF) fields can be decoded to represent the compositional shapes. During training, an auto-encoder compresses tri-planes into a latent space, and then the denoising diffusion process is employed to approximate the distribution of the compositional scenes. Frankenstein demonstrates promising results in generating room interiors as well as human avatars with automatically separated parts. The generated scenes facilitate many downstream applications, such as part-wise re-texturing, object rearrangement in the room or avatar clo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#19987;&#38376;&#38024;&#23545;&#21517;&#20154;&#29031;&#29255;&#30340;&#22270;&#20687;&#25551;&#36848;&#65292;&#26088;&#22312;&#22686;&#24378;&#26032;&#38395;&#34892;&#19994;&#23454;&#36341;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#33258;&#21160;&#26032;&#38395;&#20869;&#23481;&#29983;&#25104;&#30340;&#25913;&#36827;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.16209</link><description>&lt;p&gt;
&#26032;&#38395;&#25253;&#36947;&#22330;&#26223;&#20013;&#30340;&#22270;&#20687;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
Image Captioning in news report scenario
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25506;&#32034;&#20102;&#19987;&#38376;&#38024;&#23545;&#21517;&#20154;&#29031;&#29255;&#30340;&#22270;&#20687;&#25551;&#36848;&#65292;&#26088;&#22312;&#22686;&#24378;&#26032;&#38395;&#34892;&#19994;&#23454;&#36341;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;&#33258;&#21160;&#26032;&#38395;&#20869;&#23481;&#29983;&#25104;&#30340;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16209v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#22270;&#20687;&#25551;&#36848;&#26088;&#22312;&#20026;&#25351;&#23450;&#30340;&#22270;&#20687;&#29983;&#25104;&#30456;&#20851;&#30340;&#25551;&#36848;&#65292;&#20351;&#20854;&#22788;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30340;&#20132;&#21449;&#28857;&#12290;&#36825;&#39033;&#21162;&#21147;&#22312;&#25512;&#33616;&#31995;&#32479;&#12289;&#26032;&#38395;&#23186;&#20307;&#12289;&#31038;&#20132;&#23186;&#20307;&#31561;&#39046;&#22495;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;&#29305;&#21035;&#26159;&#22312;&#26032;&#38395;&#25253;&#36947;&#39046;&#22495;&#65292;&#26631;&#39064;&#24212;&#28085;&#30422;&#35814;&#32454;&#20449;&#24687;&#65292;&#22914;&#22270;&#20687;&#20013;&#25429;&#25417;&#21040;&#30340;&#21517;&#20154;&#30340;&#36523;&#20221;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#22823;&#37096;&#20998;&#24037;&#20316;&#20027;&#35201;&#38598;&#20013;&#20110;&#29702;&#35299;&#22330;&#26223;&#21644;&#21160;&#20316;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#19987;&#38376;&#38024;&#23545;&#21517;&#20154;&#29031;&#29255;&#30340;&#22270;&#20687;&#25551;&#36848;&#39046;&#22495;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#22686;&#24378;&#26032;&#38395;&#34892;&#19994;&#23454;&#36341;&#26041;&#38754;&#30340;&#24191;&#27867;&#28508;&#21147;&#12290;&#36825;&#19968;&#25506;&#32034;&#26088;&#22312;&#22686;&#24378;&#33258;&#21160;&#21270;&#26032;&#38395;&#20869;&#23481;&#29983;&#25104;&#65292;&#20174;&#32780;&#20419;&#36827;&#26356;&#21152;&#32454;&#33268;&#22320;&#20256;&#25773;&#20449;&#24687;&#12290;&#25105;&#20204;&#30340;&#21162;&#21147;&#23637;&#31034;&#20102;&#19968;&#20010;&#26356;&#24191;&#38420;&#30340;&#35270;&#37326;&#65292;&#20016;&#23500;&#20102;n
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16209v1 Announce Type: cross  Abstract: Image captioning strives to generate pertinent captions for specified images, situating itself at the crossroads of Computer Vision (CV) and Natural Language Processing (NLP). This endeavor is of paramount importance with far-reaching applications in recommendation systems, news outlets, social media, and beyond. Particularly within the realm of news reporting, captions are expected to encompass detailed information, such as the identities of celebrities captured in the images. However, much of the existing body of work primarily centers around understanding scenes and actions. In this paper, we explore the realm of image captioning specifically tailored for celebrity photographs, illustrating its broad potential for enhancing news industry practices. This exploration aims to augment automated news content generation, thereby facilitating a more nuanced dissemination of information. Our endeavor shows a broader horizon, enriching the n
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#30456;&#20851;&#24615;&#21644;&#20449;&#24687;&#20256;&#25773;&#30340;&#34920;&#31034;&#65292;&#20197;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;</title><link>https://arxiv.org/abs/2403.16206</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#29992;&#20110;&#35875;&#35328;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Rumor Detection with a novel graph neural network approach
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16206
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#30456;&#20851;&#24615;&#21644;&#20449;&#24687;&#20256;&#25773;&#30340;&#34920;&#31034;&#65292;&#20197;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#19978;&#35875;&#35328;&#30340;&#24191;&#27867;&#20256;&#25773;&#23545;&#20154;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#36896;&#25104;&#20102;&#36127;&#38754;&#24433;&#21709;&#65292;&#23548;&#33268;&#20844;&#20247;&#20135;&#29983;&#28508;&#22312;&#30340;&#24656;&#24908;&#12289;&#24656;&#24807;&#21644;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#12290;&#22914;&#20309;&#23613;&#26089;&#25581;&#31359;&#35875;&#35328;&#20173;&#28982;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#21033;&#29992;&#20449;&#24687;&#20256;&#25773;&#32467;&#26500;&#26469;&#26816;&#27979;&#35875;&#35328;&#65292;&#32780;&#24456;&#23569;&#26377;&#30740;&#31350;&#20851;&#27880;&#29992;&#25143;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#21363;&#20182;&#20204;&#21487;&#33021;&#21327;&#35843;&#20256;&#25773;&#35875;&#35328;&#20197;&#33719;&#24471;&#36739;&#22823;&#30340;&#27969;&#34892;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26816;&#27979;&#27169;&#22411;&#65292;&#21516;&#26102;&#23398;&#20064;&#29992;&#25143;&#30456;&#20851;&#24615;&#21644;&#20449;&#24687;&#20256;&#25773;&#30340;&#34920;&#31034;&#65292;&#20197;&#20415;&#26816;&#27979;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#35875;&#35328;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#20174;&#25551;&#36848;&#29992;&#25143;&#21644;&#26469;&#28304;&#25512;&#25991;&#20043;&#38388;&#30456;&#20851;&#24615;&#30340;&#20108;&#37096;&#22270;&#20013;&#23398;&#20064;&#29992;&#25143;&#30456;&#20851;&#24615;&#30340;&#34920;&#31034;&#65292;&#20197;&#21450;&#20351;&#29992;&#26641;&#32467;&#26500;&#23398;&#20064;&#20449;&#24687;&#20256;&#25773;&#30340;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#32467;&#21512;&#24471;&#21040;&#30340;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16206v1 Announce Type: new  Abstract: The wide spread of rumors on social media has caused a negative impact on people's daily life, leading to potential panic, fear, and mental health problems for the public. How to debunk rumors as early as possible remains a challenging problem. Existing studies mainly leverage information propagation structure to detect rumors, while very few works focus on correlation among users that they may coordinate to spread rumors in order to gain large popularity. In this paper, we propose a new detection model, that jointly learns both the representations of user correlation and information propagation to detect rumors on social media. Specifically, we leverage graph neural networks to learn the representations of user correlation from a bipartite graph that describes the correlations between users and source tweets, and the representations of information propagation with a tree structure. Then we combine the learned representations from these 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#20855;&#26377;&#25298;&#32477;&#36873;&#39033;&#30340;&#32447;&#24615;SVC&#65292;&#25552;&#20379;&#20102;&#27491;&#30830;&#24615;&#21644;&#26497;&#23567;&#24615;&#20445;&#35777;&#30340;&#35299;&#37322;&#65292;&#30456;&#27604;&#21551;&#21457;&#24335;&#31639;&#27861;Anchors&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32473;&#20986;&#20102;&#26356;&#30701;&#30340;&#35299;&#37322;</title><link>https://arxiv.org/abs/2403.16190</link><description>&lt;p&gt;
&#22522;&#20110;&#36923;&#36753;&#30340;&#25903;&#25345;&#21521;&#37327;&#26426;&#32447;&#24615;&#20998;&#31867;&#22120;&#25298;&#32477;&#36873;&#39033;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Logic-based Explanations for Linear Support Vector Classifiers with Reject Option
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16190
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#20855;&#26377;&#25298;&#32477;&#36873;&#39033;&#30340;&#32447;&#24615;SVC&#65292;&#25552;&#20379;&#20102;&#27491;&#30830;&#24615;&#21644;&#26497;&#23567;&#24615;&#20445;&#35777;&#30340;&#35299;&#37322;&#65292;&#30456;&#27604;&#21551;&#21457;&#24335;&#31639;&#27861;Anchors&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#32473;&#20986;&#20102;&#26356;&#30701;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25903;&#25345;&#21521;&#37327;&#20998;&#31867;&#22120;&#65288;SVC&#65289;&#26159;&#19968;&#31181;&#29992;&#20110;&#32447;&#24615;&#20998;&#31867;&#38382;&#39064;&#30340;&#20247;&#25152;&#21608;&#30693;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#12290;&#23427;&#21487;&#20197;&#19982;&#25298;&#32477;&#36873;&#39033;&#31574;&#30053;&#32467;&#21512;&#20351;&#29992;&#65292;&#25298;&#32477;&#38590;&#20197;&#27491;&#30830;&#20998;&#31867;&#30340;&#23454;&#20363;&#65292;&#24182;&#23558;&#23427;&#20204;&#22996;&#25176;&#32473;&#19987;&#23478;&#12290;&#36825;&#36827;&#19968;&#27493;&#22686;&#21152;&#20102;&#27169;&#22411;&#30340;&#20449;&#24515;&#12290;&#22240;&#27492;&#65292;&#33719;&#24471;&#25298;&#32477;&#21407;&#22240;&#30340;&#35299;&#37322;&#23545;&#20110;&#19981;&#30450;&#30446;&#20449;&#20219;&#25152;&#33719;&#32467;&#26524;&#26159;&#37325;&#35201;&#30340;&#12290;&#23613;&#31649;&#22823;&#22810;&#25968;&#30456;&#20851;&#24037;&#20316;&#24050;&#32463;&#24320;&#21457;&#20986;&#20102;&#38024;&#23545;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#36825;&#31181;&#35299;&#37322;&#30340;&#25163;&#27573;&#65292;&#20294;&#23601;&#25105;&#20204;&#25152;&#30693;&#65292;&#23578;&#26410;&#26377;&#20154;&#20026;&#23384;&#22312;&#25298;&#32477;&#36873;&#39033;&#26102;&#25552;&#20379;&#36825;&#31181;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36923;&#36753;&#30340;&#26041;&#27861;&#65292;&#23545;&#20855;&#26377;&#25298;&#32477;&#36873;&#39033;&#30340;&#32447;&#24615;SVC&#30340;&#35299;&#37322;&#36827;&#34892;&#27491;&#30830;&#24615;&#21644;&#26497;&#23567;&#24615;&#30340;&#24418;&#24335;&#20445;&#35777;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#20854;&#19982;&#29983;&#25104;&#35299;&#37322;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;Anchors&#36827;&#34892;&#27604;&#36739;&#26469;&#35780;&#20272;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;&#25152;&#33719;&#32467;&#26524;&#26174;&#31034;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#32473;&#20986;&#20102;&#26356;&#30701;&#30340;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16190v1 Announce Type: new  Abstract: Support Vector Classifier (SVC) is a well-known Machine Learning (ML) model for linear classification problems. It can be used in conjunction with a reject option strategy to reject instances that are hard to correctly classify and delegate them to a specialist. This further increases the confidence of the model. Given this, obtaining an explanation of the cause of rejection is important to not blindly trust the obtained results. While most of the related work has developed means to give such explanations for machine learning models, to the best of our knowledge none have done so for when reject option is present. We propose a logic-based approach with formal guarantees on the correctness and minimality of explanations for linear SVCs with reject option. We evaluate our approach by comparing it to Anchors, which is a heuristic algorithm for generating explanations. Obtained results show that our proposed method gives shorter explanations
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27425;&#20248;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#36125;&#21494;&#26031;&#36866;&#24212;&#30340;&#28151;&#21512;&#20513;&#35758;&#20154;&#26426;&#21327;&#20316;&#30340;&#35745;&#31639;&#24314;&#27169;&#21644;&#20248;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#25512;&#26029;&#20154;&#20204;&#22312;&#21327;&#20316;&#20013;&#26159;&#21542;&#24895;&#24847;&#36981;&#20174;&#26426;&#22120;&#20154;&#30340;&#24110;&#21161;&#12290;</title><link>https://arxiv.org/abs/2403.16178</link><description>&lt;p&gt;
&#22312;&#27425;&#20248;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#36125;&#21494;&#26031;&#36866;&#24212;&#30340;&#28151;&#21512;&#20513;&#35758;&#20154;&#26426;&#21327;&#20316;
&lt;/p&gt;
&lt;p&gt;
Mixed-Initiative Human-Robot Teaming under Suboptimality with Online Bayesian Adaptation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16178
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#22312;&#27425;&#20248;&#24773;&#20917;&#19979;&#36827;&#34892;&#22312;&#32447;&#36125;&#21494;&#26031;&#36866;&#24212;&#30340;&#28151;&#21512;&#20513;&#35758;&#20154;&#26426;&#21327;&#20316;&#30340;&#35745;&#31639;&#24314;&#27169;&#21644;&#20248;&#21270;&#25216;&#26415;&#65292;&#33021;&#22815;&#25512;&#26029;&#20154;&#20204;&#22312;&#21327;&#20316;&#20013;&#26159;&#21542;&#24895;&#24847;&#36981;&#20174;&#26426;&#22120;&#20154;&#30340;&#24110;&#21161;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#23454;&#29616;&#26377;&#25928;&#30340;&#20154;-&#26234;&#33021;&#20307;&#21327;&#20316;&#65292;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#20195;&#29702;&#24517;&#39035;&#25512;&#26029;&#20182;&#20204;&#30340;&#20154;&#31867;&#20249;&#20276;&#30340;&#33021;&#21147;&#21644;&#34892;&#20026;&#21709;&#24212;&#27169;&#24335;&#65292;&#24182;&#30456;&#24212;&#22320;&#36827;&#34892;&#35843;&#25972;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#24037;&#20316;&#37117;&#20316;&#20986;&#20102;&#19968;&#20010;&#19981;&#20999;&#23454;&#38469;&#30340;&#20551;&#35774;&#65292;&#21363;&#19968;&#20010;&#25110;&#22810;&#20010;&#38431;&#21451;&#21487;&#20197;&#22312;&#25509;&#36817;&#26368;&#20248;&#30340;&#24773;&#20917;&#19979;&#34892;&#21160;&#12290;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#21327;&#20316;&#20013;&#65292;&#20154;&#31867;&#21644;&#33258;&#20027;&#20195;&#29702;&#21487;&#33021;&#26159;&#27425;&#20248;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#27599;&#20010;&#20154;&#21482;&#25317;&#26377;&#37096;&#20998;&#39046;&#22495;&#30693;&#35782;&#26102;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#29992;&#20110;&#22686;&#24378;&#27425;&#20248;&#20154;-&#26234;&#33021;&#20307;&#22242;&#38431;&#34920;&#29616;&#30340;&#35745;&#31639;&#24314;&#27169;&#21644;&#20248;&#21270;&#25216;&#26415;&#65292;&#20854;&#20013;&#20154;&#31867;&#21644;&#20195;&#29702;&#20855;&#26377;&#19981;&#23545;&#31216;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#30001;&#20110;&#29615;&#22659;&#30693;&#35782;&#19981;&#23436;&#25972;&#32780;&#34920;&#29616;&#27425;&#20248;&#12290;&#25105;&#20204;&#37319;&#29992;&#19968;&#31181;&#22312;&#32447;&#36125;&#21494;&#26031;&#26041;&#27861;&#65292;&#20351;&#26426;&#22120;&#20154;&#33021;&#22815;&#25512;&#26029;&#20154;&#20204;&#22312;&#39034;&#24207;&#20915;&#31574;&#28216;&#25103;&#20013;&#26159;&#21542;&#24895;&#24847;&#36981;&#20174;&#20854;&#24110;&#21161;&#12290;&#25105;&#20204;&#30340;&#29992;&#25143;&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#25143;&#20559;&#22909;&#21644;&#22242;&#38431;&#34920;&#29616;&#30830;&#23454;&#38543;&#30528;&#26426;&#22120;&#20154;&#20171;&#20837;&#39118;&#26684;&#32780;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16178v1 Announce Type: cross  Abstract: For effective human-agent teaming, robots and other artificial intelligence (AI) agents must infer their human partner's abilities and behavioral response patterns and adapt accordingly. Most prior works make the unrealistic assumption that one or more teammates can act near-optimally. In real-world collaboration, humans and autonomous agents can be suboptimal, especially when each only has partial domain knowledge. In this work, we develop computational modeling and optimization techniques for enhancing the performance of suboptimal human-agent teams, where the human and the agent have asymmetric capabilities and act suboptimally due to incomplete environmental knowledge. We adopt an online Bayesian approach that enables a robot to infer people's willingness to comply with its assistance in a sequential decision-making game. Our user studies show that user preferences and team performance indeed vary with robot intervention styles, an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26679;&#26412;&#30340;&#30697;&#20256;&#25773;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#34920;&#24449;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#20854;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#25552;&#20379;&#20102;&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#20256;&#36882;&#30340;&#38543;&#26426;&#21464;&#37327;&#21327;&#26041;&#24046;&#30340;&#35299;&#26512;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.16163</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#20013;&#21327;&#26041;&#24046;&#20256;&#25773;&#30340;&#35299;&#26512;&#35299;
&lt;/p&gt;
&lt;p&gt;
An Analytic Solution to Covariance Propagation in Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16163
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26679;&#26412;&#30340;&#30697;&#20256;&#25773;&#25216;&#26415;&#65292;&#33021;&#22815;&#20934;&#30830;&#34920;&#24449;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#65292;&#20854;&#20851;&#38190;&#21019;&#26032;&#22312;&#20110;&#25552;&#20379;&#20102;&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#20256;&#36882;&#30340;&#38543;&#26426;&#21464;&#37327;&#21327;&#26041;&#24046;&#30340;&#35299;&#26512;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#30340;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#23545;&#20110;&#34913;&#37327;&#28145;&#24230;&#23398;&#20064;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#40065;&#26834;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#28041;&#21450;&#26114;&#36149;&#25110;&#19981;&#20934;&#30830;&#30340;&#37319;&#26679;&#26041;&#27861;&#21644;&#36817;&#20284;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#26679;&#26412;&#30340;&#30697;&#20256;&#25773;&#25216;&#26415;&#65292;&#36890;&#36807;&#32593;&#32476;&#20256;&#25773;&#22343;&#20540;&#21521;&#37327;&#21644;&#21327;&#26041;&#24046;&#30697;&#38453;&#65292;&#20934;&#30830;&#34920;&#24449;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#30340;&#19968;&#20010;&#20851;&#38190;&#20248;&#21183;&#26159;&#20026;&#36890;&#36807;&#38750;&#32447;&#24615;&#28608;&#27963;&#20989;&#25968;&#65288;&#22914;Heaviside&#12289;ReLU&#21644;GELU&#65289;&#20256;&#36882;&#30340;&#38543;&#26426;&#21464;&#37327;&#30340;&#21327;&#26041;&#24046;&#25552;&#20379;&#20102;&#35299;&#26512;&#35299;&#12290;&#36890;&#36807;&#20998;&#26512;&#32463;&#36807;&#35757;&#32451;&#30340;&#31070;&#32463;&#32593;&#32476;&#30340;&#36755;&#20837;&#36755;&#20986;&#20998;&#24067;&#20197;&#21450;&#35757;&#32451;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#23454;&#39564;&#65292;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#25216;&#26415;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#21644;&#20248;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16163v1 Announce Type: cross  Abstract: Uncertainty quantification of neural networks is critical to measuring the reliability and robustness of deep learning systems. However, this often involves costly or inaccurate sampling methods and approximations. This paper presents a sample-free moment propagation technique that propagates mean vectors and covariance matrices across a network to accurately characterize the input-output distributions of neural networks. A key enabler of our technique is an analytic solution for the covariance of random variables passed through nonlinear activation functions, such as Heaviside, ReLU, and GELU. The wide applicability and merits of the proposed technique are shown in experiments analyzing the input-output distributions of trained neural networks and training Bayesian neural networks.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#20248;&#21270;&#35270;&#35282;&#30475;&#24453;&#24085;&#32047;&#25176;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#36716;&#21270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#22810;&#20219;&#21153;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#32852;&#21512;&#35299;&#20915;&#22810;&#20010;&#23376;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#19968;&#32452;&#20248;&#21270;&#19988;&#20998;&#24067;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.16162</link><description>&lt;p&gt;
&#22522;&#20110;&#22810;&#20219;&#21153;&#20248;&#21270;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Task Learning with Multi-Task Optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16162
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22810;&#20219;&#21153;&#20248;&#21270;&#35270;&#35282;&#30475;&#24453;&#24085;&#32047;&#25176;&#22810;&#20219;&#21153;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#36716;&#21270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#29420;&#29305;&#30340;&#22810;&#20219;&#21153;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#32852;&#21512;&#35299;&#20915;&#22810;&#20010;&#23376;&#38382;&#39064;&#65292;&#20174;&#32780;&#23454;&#29616;&#19968;&#32452;&#20248;&#21270;&#19988;&#20998;&#24067;&#33391;&#22909;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#20219;&#21153;&#23398;&#20064;&#35299;&#20915;&#20102;&#22810;&#20010;&#30456;&#20851;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20043;&#38388;&#21487;&#33021;&#23384;&#22312;&#20914;&#31361;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#21333;&#20010;&#35299;&#20915;&#26041;&#26696;&#24456;&#23569;&#33021;&#22815;&#20248;&#21270;&#25152;&#26377;&#20219;&#21153;&#65292;&#23548;&#33268;&#24615;&#33021;&#25240;&#34935;&#12290;&#20026;&#20102;&#22312;&#19968;&#20010;&#31639;&#27861;&#36890;&#36807;&#20013;&#33719;&#24471;&#19968;&#32452;&#20248;&#21270;&#19988;&#20998;&#24067;&#33391;&#22909;&#30340;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#38598;&#20307;&#20307;&#29616;&#20102;&#19981;&#21516;&#26435;&#34913;&#65292;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#22810;&#20219;&#21153;&#20248;&#21270;&#30340;&#35270;&#35282;&#30475;&#24453;&#24085;&#32047;&#25176;&#22810;&#20219;&#21153;&#23398;&#20064;&#12290;&#39318;&#20808;&#23558;&#22810;&#20219;&#21153;&#23398;&#20064;&#35270;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#28982;&#21518;&#23558;&#20854;&#20998;&#35299;&#20026;&#19968;&#32452;&#19981;&#21463;&#32422;&#26463;&#30340;&#26631;&#37327;&#20215;&#20540;&#23376;&#38382;&#39064;&#12290;&#20351;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20219;&#21153;&#26799;&#24230;&#19979;&#38477;&#26041;&#27861;&#20849;&#21516;&#35299;&#20915;&#36825;&#20123;&#23376;&#38382;&#39064;&#65292;&#20854;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#22312;&#20248;&#21270;&#36807;&#31243;&#20013;&#22312;&#23376;&#38382;&#39064;&#20043;&#38388;&#36845;&#20195;&#20256;&#36755;&#27169;&#22411;&#21442;&#25968;&#12290;&#25552;&#20986;&#20102;&#19968;&#20010;&#23450;&#29702;&#65292;&#35777;&#26126;&#36890;&#36807;&#21253;&#21547;&#36825;&#26679;&#30340;&#20256;&#36755;&#21487;&#20197;&#23454;&#29616;&#26356;&#24555;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#25552;&#20986;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16162v1 Announce Type: new  Abstract: Multi-task learning solves multiple correlated tasks. However, conflicts may exist between them. In such circumstances, a single solution can rarely optimize all the tasks, leading to performance trade-offs. To arrive at a set of optimized yet well-distributed models that collectively embody different trade-offs in one algorithmic pass, this paper proposes to view Pareto multi-task learning through the lens of multi-task optimization. Multi-task learning is first cast as a multi-objective optimization problem, which is then decomposed into a diverse set of unconstrained scalar-valued subproblems. These subproblems are solved jointly using a novel multi-task gradient descent method, whose uniqueness lies in the iterative transfer of model parameters among the subproblems during the course of optimization. A theorem proving faster convergence through the inclusion of such transfers is presented. We investigate the proposed multi-task learn
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25513;&#30422;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20256;&#24863;&#22120;&#25925;&#38556;&#26816;&#27979;&#12289;&#38548;&#31163;&#21644;&#23481;&#38169;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#21019;&#24314;&#38543;&#26426;&#25513;&#30721;&#26469;&#32479;&#19968;&#25214;&#21040;&#24182;&#32416;&#27491;&#25925;&#38556;&#20256;&#24863;&#22120;&#65292;&#26377;&#25928;&#24615;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#39118;&#21147;&#28065;&#36718;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.16153</link><description>&lt;p&gt;
&#19968;&#20010;&#25513;&#30422;&#27169;&#22411;&#23601;&#36275;&#22815;&#23454;&#29616;&#20256;&#24863;&#22120;&#25925;&#38556;&#26816;&#27979;&#12289;&#38548;&#31163;&#21644;&#23481;&#38169;
&lt;/p&gt;
&lt;p&gt;
One Masked Model is All You Need for Sensor Fault Detection, Isolation and Accommodation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16153
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25513;&#30422;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20256;&#24863;&#22120;&#25925;&#38556;&#26816;&#27979;&#12289;&#38548;&#31163;&#21644;&#23481;&#38169;&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#35757;&#32451;&#36807;&#31243;&#20013;&#21019;&#24314;&#38543;&#26426;&#25513;&#30721;&#26469;&#32479;&#19968;&#25214;&#21040;&#24182;&#32416;&#27491;&#25925;&#38556;&#20256;&#24863;&#22120;&#65292;&#26377;&#25928;&#24615;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;&#23454;&#38469;&#39118;&#21147;&#28065;&#36718;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#30830;&#21487;&#38752;&#30340;&#20256;&#24863;&#22120;&#27979;&#37327;&#23545;&#20110;&#30830;&#20445;&#39118;&#21147;&#28065;&#36718;&#31561;&#22797;&#26434;&#24037;&#31243;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#21644;&#38271;&#26399;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#25513;&#30422;&#27169;&#22411;&#21644;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#20256;&#24863;&#22120;&#25925;&#38556;&#26816;&#27979;&#12289;&#38548;&#31163;&#21644;&#23481;&#38169;&#65288;FDIA&#65289;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26159;&#19968;&#31181;&#36890;&#29992;&#30340;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#26041;&#27861;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#20219;&#20309;&#33021;&#22815;&#36827;&#34892;&#24207;&#21015;&#24314;&#27169;&#30340;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#27169;&#22411;&#65292;&#24182;&#25429;&#25417;&#19981;&#21516;&#20256;&#24863;&#22120;&#20043;&#38388;&#22797;&#26434;&#30340;&#26102;&#31354;&#20851;&#31995;&#12290;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#25552;&#20986;&#30340;&#25513;&#30422;&#26041;&#27861;&#21019;&#24314;&#38543;&#26426;&#25513;&#30721;&#65292;&#23427;&#23601;&#20687;&#19968;&#20010;&#25925;&#38556;&#65292;&#38024;&#23545;&#19968;&#20010;&#25110;&#22810;&#20010;&#20256;&#24863;&#22120;&#65292;&#20351;&#35757;&#32451;&#21644;&#25512;&#26029;&#20219;&#21153;&#32479;&#19968;&#65306;&#25214;&#21040;&#26377;&#25925;&#38556;&#30340;&#20256;&#24863;&#22120;&#24182;&#36827;&#34892;&#32416;&#27491;&#12290;&#25105;&#20204;&#22312;&#19968;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#21644;GE&#36817;&#28023;&#39118;&#21147;&#28065;&#36718;&#30340;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25216;&#26415;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22312;&#26816;&#27979;&#12289;&#35786;&#26029;&#21644;&#32416;&#27491;&#20256;&#24863;&#22120;&#25925;&#38556;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16153v1 Announce Type: cross  Abstract: Accurate and reliable sensor measurements are critical for ensuring the safety and longevity of complex engineering systems such as wind turbines. In this paper, we propose a novel framework for sensor fault detection, isolation, and accommodation (FDIA) using masked models and self-supervised learning. Our proposed approach is a general time series modeling approach that can be applied to any neural network (NN) model capable of sequence modeling, and captures the complex spatio-temporal relationships among different sensors. During training, the proposed masked approach creates a random mask, which acts like a fault, for one or more sensors, making the training and inference task unified: finding the faulty sensors and correcting them. We validate our proposed technique on both a public dataset and a real-world dataset from GE offshore wind turbines, and demonstrate its effectiveness in detecting, diagnosing and correcting sensor fau
&lt;/p&gt;</description></item><item><title>&#26412;&#35843;&#26597;&#38024;&#23545;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#27969;&#37327;&#20998;&#26512;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;CIoT&#39046;&#22495;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16149</link><description>&lt;p&gt;
&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#27969;&#37327;&#30340;&#35843;&#26597;&#65306;&#23433;&#20840;&#19982;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
A Survey on Consumer IoT Traffic: Security and Privacy
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35843;&#26597;&#38024;&#23545;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#27969;&#37327;&#20998;&#26512;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#24635;&#32467;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26032;&#29305;&#24449;&#12289;&#26368;&#26032;&#36827;&#23637;&#21644;&#25361;&#25112;&#65292;&#35748;&#20026;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#21487;&#20197;&#25581;&#31034;CIoT&#39046;&#22495;&#20013;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#28040;&#36153;&#32773;&#29289;&#32852;&#32593;&#65288;CIoT&#65289;&#24050;&#32463;&#36827;&#20837;&#20102;&#20844;&#20247;&#29983;&#27963;&#12290;&#23613;&#31649;CIoT&#25552;&#39640;&#20102;&#20154;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#20415;&#21033;&#24615;&#65292;&#20294;&#20063;&#24102;&#26469;&#20102;&#26032;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#38382;&#39064;&#12290;&#25105;&#20204;&#23581;&#35797;&#36890;&#36807;&#27969;&#37327;&#20998;&#26512;&#36825;&#19968;&#23433;&#20840;&#39046;&#22495;&#20013;&#30340;&#27969;&#34892;&#26041;&#27861;&#65292;&#25214;&#20986;&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#20174;&#27969;&#37327;&#20998;&#26512;&#20013;&#20102;&#35299;CIoT&#23433;&#20840;&#21644;&#38544;&#31169;&#26041;&#38754;&#30340;&#20869;&#23481;&#12290;&#26412;&#35843;&#26597;&#20174;&#23433;&#20840;&#21644;&#38544;&#31169;&#35282;&#24230;&#25506;&#35752;&#20102;CIoT&#27969;&#37327;&#20998;&#26512;&#20013;&#30340;&#26032;&#29305;&#24449;&#12289;CIoT&#27969;&#37327;&#20998;&#26512;&#30340;&#26368;&#26032;&#36827;&#23637;&#20197;&#21450;&#23578;&#26410;&#35299;&#20915;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#20174;2018&#24180;1&#26376;&#33267;2023&#24180;12&#26376;&#25910;&#38598;&#20102;310&#31687;&#19982;CIoT&#27969;&#37327;&#20998;&#26512;&#26377;&#20851;&#30340;&#23433;&#20840;&#21644;&#38544;&#31169;&#35282;&#24230;&#30340;&#35770;&#25991;&#65292;&#24635;&#32467;&#20102;&#35782;&#21035;&#20102;CIoT&#26032;&#29305;&#24449;&#30340;CIoT&#27969;&#37327;&#20998;&#26512;&#36807;&#31243;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#26681;&#25454;&#20116;&#20010;&#24212;&#29992;&#30446;&#26631;&#35814;&#32454;&#20171;&#32461;&#20102;&#29616;&#26377;&#30340;&#30740;&#31350;&#24037;&#20316;&#65306;&#35774;&#22791;&#25351;&#32441;&#35782;&#21035;&#12289;&#29992;&#25143;&#27963;&#21160;&#25512;&#26029;&#12289;&#24694;&#24847;&#34892;&#20026;&#26816;&#27979;&#12289;&#38544;&#31169;&#27844;&#38706;&#20197;&#21450;&#36890;&#20449;&#27169;&#24335;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16149v1 Announce Type: cross  Abstract: For the past few years, the Consumer Internet of Things (CIoT) has entered public lives. While CIoT has improved the convenience of people's daily lives, it has also brought new security and privacy concerns. In this survey, we try to figure out what researchers can learn about the security and privacy of CIoT by traffic analysis, a popular method in the security community. From the security and privacy perspective, this survey seeks out the new characteristics in CIoT traffic analysis, the state-of-the-art progress in CIoT traffic analysis, and the challenges yet to be solved. We collected 310 papers from January 2018 to December 2023 related to CIoT traffic analysis from the security and privacy perspective and summarized the process of CIoT traffic analysis in which the new characteristics of CIoT are identified. Then, we detail existing works based on five application goals: device fingerprinting, user activity inference, malicious
&lt;/p&gt;</description></item><item><title>&#19968;&#31181;&#22522;&#20110;&#25237;&#24433;&#30340;&#27010;&#24565;&#21435;&#38500;&#26041;&#27861;&#20250;&#22312;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#38598;&#20013;&#27880;&#20837;&#24378;&#22823;&#30340;&#32479;&#35745;&#20381;&#36182;&#24615;&#65292;&#24182;&#23548;&#33268;&#34920;&#31034;&#31354;&#38388;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;&#21453;&#32858;&#31867;&#26041;&#27861;&#37325;&#24314;&#21407;&#22987;&#26631;&#35760;&#12290;</title><link>https://arxiv.org/abs/2403.16142</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25237;&#24433;&#30340;&#27010;&#24565;&#21435;&#38500;&#26041;&#27861;&#23545;&#25968;&#25454;&#38598;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
What Happens to a Dataset Transformed by a Projection-based Concept Removal Method?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16142
&lt;/p&gt;
&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#25237;&#24433;&#30340;&#27010;&#24565;&#21435;&#38500;&#26041;&#27861;&#20250;&#22312;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#38598;&#20013;&#27880;&#20837;&#24378;&#22823;&#30340;&#32479;&#35745;&#20381;&#36182;&#24615;&#65292;&#24182;&#23548;&#33268;&#34920;&#31034;&#31354;&#38388;&#39640;&#24230;&#32467;&#26500;&#21270;&#65292;&#20351;&#24471;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;&#21453;&#32858;&#31867;&#26041;&#27861;&#37325;&#24314;&#21407;&#22987;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#20351;&#29992;&#32447;&#24615;&#25237;&#24433;&#26041;&#27861;&#20174;&#35821;&#35328;&#34920;&#31034;&#20013;&#21435;&#38500;&#27010;&#24565;&#20449;&#24687;&#30340;&#26041;&#27861;&#30340;&#34892;&#20026;&#65292;&#24182;&#32771;&#34385;&#20102;&#32463;&#36807;&#36825;&#31181;&#26041;&#27861;&#36716;&#25442;&#30340;&#25968;&#25454;&#38598;&#20250;&#21457;&#29983;&#20160;&#20040;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23545;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#36825;&#20123;&#26041;&#27861;&#20250;&#21521;&#36716;&#25442;&#21518;&#30340;&#25968;&#25454;&#38598;&#20013;&#27880;&#20837;&#24378;&#22823;&#30340;&#32479;&#35745;&#20381;&#36182;&#24615;&#12290;&#24212;&#29992;&#27492;&#31867;&#26041;&#27861;&#21518;&#65292;&#34920;&#31034;&#31354;&#38388;&#20855;&#26377;&#39640;&#24230;&#32467;&#26500;&#21270;&#65306;&#22312;&#36716;&#25442;&#21518;&#30340;&#31354;&#38388;&#20013;&#65292;&#19968;&#20010;&#23454;&#20363;&#20542;&#21521;&#20110;&#20301;&#20110;&#30456;&#21453;&#26631;&#31614;&#30340;&#23454;&#20363;&#38468;&#36817;&#12290;&#22240;&#27492;&#65292;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#65292;&#21487;&#20197;&#36890;&#36807;&#24212;&#29992;&#21453;&#32858;&#31867;&#26041;&#27861;&#26469;&#37325;&#24314;&#21407;&#22987;&#26631;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16142v1 Announce Type: cross  Abstract: We investigate the behavior of methods that use linear projections to remove information about a concept from a language representation, and we consider the question of what happens to a dataset transformed by such a method. A theoretical analysis and experiments on real-world and synthetic data show that these methods inject strong statistical dependencies into the transformed datasets. After applying such a method, the representation space is highly structured: in the transformed space, an instance tends to be located near instances of the opposite label. As a consequence, the original labeling can in some cases be reconstructed by applying an anti-clustering method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#21644;&#27604;&#36739;&#20102;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;34&#39033;&#20195;&#34920;&#24615;&#20114;&#34917;&#25512;&#33616;&#30740;&#31350;&#65292;&#21253;&#25324;&#24314;&#27169;&#20135;&#21697;&#20043;&#38388;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#19981;&#21516;&#30740;&#31350;&#38382;&#39064;&#19979;&#30340;&#27169;&#22411;&#20998;&#31867;&#19982;&#27604;&#36739;&#65292;&#20197;&#21450;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.16135</link><description>&lt;p&gt;
&#30005;&#23376;&#21830;&#21153;&#20013;&#30340;&#20114;&#34917;&#25512;&#33616;&#65306;&#23450;&#20041;&#12289;&#26041;&#27861;&#19982;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Complementary Recommendation in E-commerce: Definition, Approaches, and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20840;&#38754;&#24635;&#32467;&#21644;&#27604;&#36739;&#20102;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#20013;34&#39033;&#20195;&#34920;&#24615;&#20114;&#34917;&#25512;&#33616;&#30740;&#31350;&#65292;&#21253;&#25324;&#24314;&#27169;&#20135;&#21697;&#20043;&#38388;&#30340;&#20114;&#34917;&#20851;&#31995;&#65292;&#19981;&#21516;&#30740;&#31350;&#38382;&#39064;&#19979;&#30340;&#27169;&#22411;&#20998;&#31867;&#19982;&#27604;&#36739;&#65292;&#20197;&#21450;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20114;&#34917;&#25512;&#33616;&#22312;&#30005;&#23376;&#21830;&#21153;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26412;&#25991;&#23545;2009&#24180;&#33267;2024&#24180;&#38388;&#36827;&#34892;&#30340;34&#39033;&#20195;&#34920;&#24615;&#30740;&#31350;&#36827;&#34892;&#20102;&#20840;&#38754;&#24635;&#32467;&#21644;&#27604;&#36739;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#29992;&#20110;&#24314;&#27169;&#20135;&#21697;&#20043;&#38388;&#20114;&#34917;&#20851;&#31995;&#30340;&#25968;&#25454;&#21644;&#26041;&#27861;&#65292;&#21253;&#25324;&#31616;&#21333;&#30340;&#20114;&#34917;&#24615;&#20197;&#21450;&#26356;&#22797;&#26434;&#30340;&#24773;&#26223;&#65292;&#20363;&#22914;&#38750;&#23545;&#31216;&#20114;&#34917;&#24615;&#12289;&#20135;&#21697;&#20043;&#38388;&#26367;&#20195;&#21644;&#20114;&#34917;&#20851;&#31995;&#20849;&#23384;&#65292;&#20197;&#21450;&#19981;&#21516;&#20135;&#21697;&#23545;&#20043;&#38388;&#30340;&#20114;&#34917;&#31243;&#24230;&#19981;&#21516;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#26681;&#25454;&#20114;&#34917;&#25512;&#33616;&#30340;&#30740;&#31350;&#38382;&#39064;&#23545;&#27169;&#22411;&#36827;&#34892;&#20998;&#31867;&#24182;&#36827;&#34892;&#27604;&#36739;&#65292;&#22914;&#22810;&#26679;&#24615;&#12289;&#20010;&#24615;&#21270;&#21644;&#20919;&#21551;&#21160;&#31561;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#19981;&#21516;&#30740;&#31350;&#22312;&#21516;&#19968;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#32467;&#26524;&#36827;&#34892;&#27604;&#36739;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#30830;&#23450;&#30740;&#31350;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16135v1 Announce Type: cross  Abstract: In recent years, complementary recommendation has received extensive attention in the e-commerce domain. In this paper, we comprehensively summarize and compare 34 representative studies conducted between 2009 and 2024. Firstly, we compare the data and methods used for modeling complementary relationships between products, including simple complementarity and more complex scenarios such as asymmetric complementarity, the coexistence of substitution and complementarity relationships between products, and varying degrees of complementarity between different pairs of products. Next, we classify and compare the models based on the research problems of complementary recommendation, such as diversity, personalization, and cold-start. Furthermore, we provide a comparative analysis of experimental results from different studies conducted on the same dataset, which helps identify the strengths and weaknesses of the research. Compared to previou
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;&#65288;SSHPool&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#33410;&#28857;&#20998;&#37197;&#21040;&#19981;&#21516;&#30340;&#31751;&#24182;&#21033;&#29992;&#23616;&#37096;&#22270;&#21367;&#31215;&#21333;&#20803;&#36827;&#34892;&#21387;&#32553;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#21462;&#20102;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20998;&#23618;&#20840;&#23616;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2403.16133</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;SSHPool
&lt;/p&gt;
&lt;p&gt;
SSHPool: The Separated Subgraph-based Hierarchical Pooling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16133
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;&#65288;SSHPool&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#33410;&#28857;&#20998;&#37197;&#21040;&#19981;&#21516;&#30340;&#31751;&#24182;&#21033;&#29992;&#23616;&#37096;&#22270;&#21367;&#31215;&#21333;&#20803;&#36827;&#34892;&#21387;&#32553;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#65292;&#26377;&#25928;&#25552;&#21462;&#20102;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20998;&#23618;&#20840;&#23616;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26412;&#22320;&#22270;&#27744;&#21270;&#26041;&#27861;&#65292;&#31216;&#20026;&#22522;&#20110;&#20998;&#38548;&#23376;&#22270;&#30340;&#20998;&#23618;&#27744;&#21270;&#65288;SSHPool&#65289;&#65292;&#29992;&#20110;&#22270;&#20998;&#31867;&#12290;&#36890;&#36807;&#23558;&#19968;&#20010;&#26679;&#26412;&#22270;&#30340;&#33410;&#28857;&#20998;&#37197;&#21040;&#19981;&#21516;&#30340;&#31751;&#20013;&#65292;&#20174;&#32780;&#20135;&#29983;&#19968;&#31995;&#21015;&#20998;&#38548;&#30340;&#23376;&#22270;&#12290;&#25105;&#20204;&#20998;&#21035;&#20351;&#29992;&#26412;&#22320;&#22270;&#21367;&#31215;&#21333;&#20803;&#20316;&#20026;&#23616;&#37096;&#32467;&#26500;&#65292;&#36827;&#19968;&#27493;&#23558;&#27599;&#20010;&#23376;&#22270;&#21387;&#32553;&#25104;&#19968;&#20010;&#31895;&#31961;&#33410;&#28857;&#65292;&#23558;&#21407;&#22987;&#22270;&#36716;&#21270;&#20026;&#31895;&#31961;&#22270;&#12290;&#30001;&#20110;&#36825;&#20123;&#23376;&#22270;&#30001;&#19981;&#21516;&#30340;&#31751;&#20998;&#38548;&#24320;&#65292;&#32467;&#26500;&#20449;&#24687;&#26080;&#27861;&#22312;&#23427;&#20204;&#20043;&#38388;&#20256;&#25773;&#65292;&#23616;&#37096;&#21367;&#31215;&#25805;&#20316;&#21487;&#20197;&#26174;&#33879;&#36991;&#20813;&#22823;&#22810;&#25968;&#29616;&#26377;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#20986;&#29616;&#30340;&#36807;&#24230;&#24179;&#28369;&#38382;&#39064;&#12290;&#36890;&#36807;&#22312;&#32467;&#26524;&#31895;&#31961;&#22270;&#19978;&#23618;&#27425;&#22320;&#25191;&#34892;&#25152;&#25552;&#35758;&#30340;&#31243;&#24207;&#65292;SSHPool&#21487;&#20197;&#26377;&#25928;&#22320;&#25552;&#21462;&#21407;&#22987;&#22270;&#32467;&#26500;&#30340;&#20998;&#23618;&#20840;&#23616;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16133v1 Announce Type: new  Abstract: In this paper, we develop a novel local graph pooling method, namely the Separated Subgraph-based Hierarchical Pooling (SSHPool), for graph classification. To this end, we commence by assigning the nodes of a sample graph into different clusters, resulting in a family of separated subgraphs. We individually employ a local graph convolution units as the local structure to further compress each subgraph into a coarsened node, transforming the original graph into a coarsened graph. Since these subgraphs are separated by different clusters and the structural information cannot be propagated between them, the local convolution operation can significantly avoid the over-smoothing problem arising in most existing Graph Neural Networks (GNNs). By hierarchically performing the proposed procedures on the resulting coarsened graph, the proposed SSHPool can effectively extract the hierarchical global feature of the original graph structure, encapsul
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20998;&#31867;&#30340;&#33258;&#36866;&#24212;&#26680;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#29305;&#24449;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#19981;&#21516;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#26377;&#25928;&#35782;&#21035;&#32467;&#26500;&#37325;&#35201;&#24615;&#65292;&#24182;&#35745;&#31639;&#19982;&#37325;&#35201;&#23376;&#32467;&#26500;&#30456;&#20851;&#30340;&#22270;&#23545;&#20043;&#38388;&#30340;R-&#21367;&#31215;&#26680;&#12290;</title><link>https://arxiv.org/abs/2403.16130</link><description>&lt;p&gt;
AKBR: &#23398;&#20064;&#33258;&#36866;&#24212;&#22522;&#20110;&#26680;&#30340;&#22270;&#20998;&#31867;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
AKBR: Learning Adaptive Kernel-based Representations for Graph Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16130
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#22270;&#20998;&#31867;&#30340;&#33258;&#36866;&#24212;&#26680;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#29305;&#24449;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#25429;&#25417;&#19981;&#21516;&#23376;&#32467;&#26500;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#65292;&#26377;&#25928;&#35782;&#21035;&#32467;&#26500;&#37325;&#35201;&#24615;&#65292;&#24182;&#35745;&#31639;&#19982;&#37325;&#35201;&#23376;&#32467;&#26500;&#30456;&#20851;&#30340;&#22270;&#23545;&#20043;&#38388;&#30340;R-&#21367;&#31215;&#26680;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#27169;&#22411;&#65292;&#29992;&#20110;&#23398;&#20064;&#33258;&#36866;&#24212;&#22522;&#20110;&#26680;&#30340;&#22270;&#20998;&#31867;&#34920;&#31034;&#65288;AKBR&#65289;&#12290;&#19982;&#20165;&#36890;&#36807;&#35745;&#31639;&#22270;&#20043;&#38388;&#21516;&#26500;&#23376;&#32467;&#26500;&#23545;&#30340;&#25968;&#37327;&#26469;&#23450;&#20041;&#30340;&#26368;&#20808;&#36827;&#30340; R-&#21367;&#31215;&#22270;&#26680;&#19981;&#21516;&#65292;&#26080;&#27861;&#20026;&#20998;&#31867;&#22120;&#25552;&#20379;&#31471;&#21040;&#31471;&#23398;&#20064;&#26426;&#21046;&#65292;&#25152;&#25552;&#20986;&#30340;AKBR&#26041;&#27861;&#26088;&#22312;&#23450;&#20041;&#19968;&#20010;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#27169;&#22411;&#65292;&#20026;&#22270;&#26500;&#24314;&#33258;&#36866;&#24212;&#26680;&#30697;&#38453;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#19968;&#31181;&#26032;&#39062;&#30340;&#29305;&#24449;&#36890;&#36947;&#27880;&#24847;&#26426;&#21046;&#26469;&#25429;&#25417;&#21407;&#22987;&#22270;&#20013;&#19981;&#21516;&#23376;&#32467;&#26500;&#19981;&#21464;&#24615;&#20043;&#38388;&#30340;&#30456;&#20114;&#20381;&#36182;&#20851;&#31995;&#12290;&#25152;&#25552;&#20986;&#30340;AKBR&#27169;&#22411;&#22240;&#27492;&#21487;&#20197;&#26377;&#25928;&#22320;&#30830;&#23450;&#19981;&#21516;&#23376;&#32467;&#26500;&#30340;&#32467;&#26500;&#37325;&#35201;&#24615;&#65292;&#24182;&#35745;&#31639;&#19982;&#30001;&#20854;&#32467;&#26500;&#27880;&#24847;&#21147;&#25351;&#23450;&#30340;&#26356;&#37325;&#35201;&#23376;&#32467;&#26500;&#30456;&#20851;&#30340;&#25104;&#23545;&#22270;&#20043;&#38388;&#30340;R-&#21367;&#31215;&#26680;&#12290;&#30001;&#20110;&#32467;&#26524;&#26680;&#30697;&#38453;&#30340;&#27599;&#19968;&#34892;...&#65288;&#27492;&#22788;&#34987;&#25130;&#26029;&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16130v1 Announce Type: cross  Abstract: In this paper, we propose a new model to learn Adaptive Kernel-based Representations (AKBR) for graph classification. Unlike state-of-the-art R-convolution graph kernels that are defined by merely counting any pair of isomorphic substructures between graphs and cannot provide an end-to-end learning mechanism for the classifier, the proposed AKBR approach aims to define an end-to-end representation learning model to construct an adaptive kernel matrix for graphs. To this end, we commence by leveraging a novel feature-channel attention mechanism to capture the interdependencies between different substructure invariants of original graphs. The proposed AKBR model can thus effectively identify the structural importance of different substructures, and compute the R-convolution kernel between pairwise graphs associated with the more significant substructures specified by their structural attentions. Since each row of the resulting kernel mat
&lt;/p&gt;</description></item><item><title>WangchanLion&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#27888;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;0-shot&#21644;1-shot&#35774;&#32622;&#19979;&#33021;&#22815;&#29702;&#35299;&#19978;&#19979;&#25991;&#24182;&#20135;&#29983;&#19982;&#21442;&#32771;&#31572;&#26696;&#19968;&#33268;&#30340;&#22238;&#31572;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.16127</link><description>&lt;p&gt;
WangchanLion&#19982;WangchanX MRC&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
WangchanLion and WangchanX MRC Eval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16127
&lt;/p&gt;
&lt;p&gt;
WangchanLion&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#27888;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#30340;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#65292;&#22312;0-shot&#21644;1-shot&#35774;&#32622;&#19979;&#33021;&#22815;&#29702;&#35299;&#19978;&#19979;&#25991;&#24182;&#20135;&#29983;&#19982;&#21442;&#32771;&#31572;&#26696;&#19968;&#33268;&#30340;&#22238;&#31572;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25216;&#26415;&#25253;&#21578;&#25551;&#36848;&#20102;WangchanLion&#30340;&#24320;&#21457;&#36807;&#31243;&#65292;&#36825;&#26159;&#19968;&#20010;&#19987;&#27880;&#20110;&#27888;&#35821;&#26426;&#22120;&#38405;&#35835;&#29702;&#35299;&#65288;MRC&#65289;&#30340;&#25351;&#20196;&#24494;&#35843;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22522;&#20110;SEA-LION&#21644;&#19968;&#31995;&#21015;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#20419;&#36827;&#24320;&#25918;&#30740;&#31350;&#21644;&#21487;&#37325;&#22797;&#24615;&#65292;&#25105;&#20204;&#20844;&#24320;&#21457;&#24067;&#20102;&#25152;&#26377;&#35757;&#32451;&#25968;&#25454;&#12289;&#20195;&#30721;&#21644;&#26368;&#32456;&#27169;&#22411;&#26435;&#37325;&#65292;&#37319;&#29992;Apache-2&#35768;&#21487;&#35777;&#12290;&#20026;&#20102;&#35780;&#20272;&#19978;&#19979;&#25991;&#29702;&#35299;&#33021;&#21147;&#65292;&#25105;&#20204;&#20351;&#29992;&#20004;&#20010;&#27888;&#35821;MRC&#25968;&#25454;&#38598;XQuAD&#21644;Iapp_wiki_qa_squad&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#30740;&#31350;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;0-shot&#21644;1-shot&#35774;&#32622;&#19979;&#65292;&#27169;&#22411;&#33021;&#22815;&#29702;&#35299;&#19978;&#19979;&#25991;&#24182;&#20135;&#29983;&#19982;&#21442;&#32771;&#31572;&#26696;&#19968;&#33268;&#30340;&#22238;&#31572;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#35780;&#20272;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;MRC&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#35780;&#20272;&#26041;&#26696;&#65292;&#35780;&#20272;&#31572;&#26696;&#30340;&#27491;&#30830;&#24615;&#12289;&#24110;&#21161;&#24615;&#12289;&#31616;&#27905;&#24615;&#21644;&#19978;&#19979;&#25991;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#25581;&#31034;&#20102;&#25105;&#20204;&#22914;&#20309;&#25913;&#36827;&#27169;&#22411;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16127v1 Announce Type: cross  Abstract: This technical report describes the development of WangchanLion, an instruction fine-tuned model focusing on Machine Reading Comprehension (MRC) in the Thai language. Our model is based on SEA-LION and a collection of instruction following datasets. To promote open research and reproducibility, we publically release all training data, code, and the final model weights under the Apache-2 license. To assess the contextual understanding capability, we conducted extensive experimental studies using two Thai MRC datasets, XQuAD and Iapp_wiki_qa_squad. Experimental results demonstrate the model's ability to comprehend the context and produce an answer faithful to the reference one in 0-shot and 1-shot settings. In addition, our evaluation goes beyond the traditional MRC. We propose a new evaluation scheme assessing the answer's correctness, helpfulness, conciseness, and contextuality. Evaluation results provide insight into how we can improv
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#34920;&#26126;&#65292;Neural Scene Flow Prior (NSFP)&#30340;&#24615;&#33021;&#19982;&#36755;&#20837;&#28857;&#20113;&#30340;&#25968;&#37327;&#21576;&#21453;&#27604;&#20851;&#31995;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#24103;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.16116</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#22810;&#24103;&#31070;&#32463;&#22330;&#26223;&#27969;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Multi-Frame Neural Scene Flow
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16116
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#34920;&#26126;&#65292;Neural Scene Flow Prior (NSFP)&#30340;&#24615;&#33021;&#19982;&#36755;&#20837;&#28857;&#20113;&#30340;&#25968;&#37327;&#21576;&#21453;&#27604;&#20851;&#31995;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#24103;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Neural Scene Flow Prior (NSFP)&#21644;Fast Neural Scene Flow (FNSF)&#22312;&#22823;&#35268;&#27169;&#22330;&#26223;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#33258;&#36866;&#24212;&#24615;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#22522;&#30784;&#21407;&#22240;&#23578;&#19981;&#28165;&#26970;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#32479;&#19968;&#31283;&#23450;&#24615;&#30340;&#35270;&#35282;&#26469;&#23457;&#35270;NSFP&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25581;&#31034;&#20854;&#24615;&#33021;&#19982;&#36755;&#20837;&#28857;&#20113;&#25968;&#37327;&#21576;&#21453;&#27604;&#20851;&#31995;&#12290;&#36825;&#19968;&#21457;&#29616;&#25581;&#31034;&#20102;NSFP&#22312;&#22788;&#29702;&#22823;&#35268;&#27169;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;&#20219;&#21153;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;&#21463;&#36825;&#20123;&#29702;&#35770;&#27934;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#21033;&#29992;&#22810;&#24103;&#21382;&#21490;&#28857;&#20113;&#26469;&#25913;&#36827;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#28857;&#20113;&#30340;&#25968;&#37327;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#22810;&#24103;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16116v1 Announce Type: cross  Abstract: Neural Scene Flow Prior (NSFP) and Fast Neural Scene Flow (FNSF) have shown remarkable adaptability in the context of large out-of-distribution autonomous driving. Despite their success, the underlying reasons for their astonishing generalization capabilities remain unclear. Our research addresses this gap by examining the generalization capabilities of NSFP through the lens of uniform stability, revealing that its performance is inversely proportional to the number of input point clouds. This finding sheds light on NSFP's effectiveness in handling large-scale point cloud scene flow estimation tasks. Motivated by such theoretical insights, we further explore the improvement of scene flow estimation by leveraging historical point clouds across multiple frames, which inherently increases the number of point clouds. Consequently, we propose a simple and effective method for multi-frame point cloud scene flow estimation, along with a theor
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#25918;&#23556;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#20854;&#22312;&#25918;&#23556;&#23398;&#25945;&#32946;&#12289;&#25253;&#21578;&#29983;&#25104;&#21644;&#24433;&#20687;&#24212;&#29992;&#26041;&#38754;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;AI&#27169;&#22411;&#22312;&#25918;&#23556;&#23398;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#24555;&#36895;&#38761;&#21629;&#12290;</title><link>https://arxiv.org/abs/2403.16112</link><description>&lt;p&gt;
&#22312;&#25918;&#23556;&#23398;&#20013;&#24212;&#29992;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Opportunities and challenges in the application of large artificial intelligence models in radiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16112
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#22823;&#22411;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22312;&#25918;&#23556;&#23398;&#20013;&#30340;&#24212;&#29992;&#65292;&#24635;&#32467;&#20102;&#20854;&#22312;&#25918;&#23556;&#23398;&#25945;&#32946;&#12289;&#25253;&#21578;&#29983;&#25104;&#21644;&#24433;&#20687;&#24212;&#29992;&#26041;&#38754;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#65292;&#24182;&#25351;&#20986;&#20102;&#22823;&#22411;AI&#27169;&#22411;&#22312;&#25918;&#23556;&#23398;&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#26088;&#22312;&#25512;&#21160;&#35813;&#39046;&#22495;&#30340;&#24555;&#36895;&#38761;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21463;ChatGPT&#30340;&#24433;&#21709;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#22823;&#22411;&#27169;&#22411;&#22312;&#20840;&#29699;&#33539;&#22260;&#20869;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#36814;&#26469;&#20102;&#39640;&#28526;&#12290;&#38543;&#30528;&#20154;&#20204;&#20139;&#21463;&#30528;&#36825;&#31181;AI&#22823;&#22411;&#27169;&#22411;&#24102;&#26469;&#30340;&#20415;&#21033;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#32454;&#20998;&#39046;&#22495;&#20013;&#25552;&#20986;&#20102;&#22823;&#22411;&#27169;&#22411;&#65292;&#23588;&#20854;&#26159;&#25918;&#23556;&#23398;&#25104;&#20687;&#39046;&#22495;&#20013;&#30340;&#22823;&#22411;&#27169;&#22411;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#22823;&#22411;&#27169;&#22411;&#30340;&#21457;&#23637;&#21382;&#21490;&#12289;&#25216;&#26415;&#32454;&#33410;&#12289;&#24037;&#20316;&#27969;&#31243;&#12289;&#22810;&#27169;&#24335;&#22823;&#22411;&#27169;&#22411;&#24037;&#20316;&#21407;&#29702;&#20197;&#21450;&#35270;&#39057;&#29983;&#25104;&#22823;&#22411;&#27169;&#22411;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24635;&#32467;&#20102;AI&#22823;&#22411;&#27169;&#22411;&#22312;&#25918;&#23556;&#23398;&#25945;&#32946;&#12289;&#25918;&#23556;&#23398;&#25253;&#21578;&#29983;&#25104;&#12289;&#21333;&#27169;&#24335;&#21644;&#22810;&#27169;&#24335;&#25918;&#23556;&#23398;&#24212;&#29992;&#26041;&#38754;&#30340;&#26368;&#26032;&#30740;&#31350;&#36827;&#23637;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#36824;&#24635;&#32467;&#20102;&#25918;&#23556;&#23398;&#20013;&#22823;&#22411;AI&#27169;&#22411;&#38754;&#20020;&#30340;&#19968;&#20123;&#25361;&#25112;&#65292;&#26088;&#22312;&#26356;&#22909;&#22320;&#25512;&#21160;&#25918;&#23556;&#23398;&#39046;&#22495;&#30340;&#24555;&#36895;&#38761;&#21629;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16112v1 Announce Type: cross  Abstract: Influenced by ChatGPT, artificial intelligence (AI) large models have witnessed a global upsurge in large model research and development. As people enjoy the convenience by this AI large model, more and more large models in subdivided fields are gradually being proposed, especially large models in radiology imaging field. This article first introduces the development history of large models, technical details, workflow, working principles of multimodal large models and working principles of video generation large models. Secondly, we summarize the latest research progress of AI large models in radiology education, radiology report generation, applications of unimodal and multimodal radiology. Finally, this paper also summarizes some of the challenges of large AI models in radiology, with the aim of better promoting the rapid revolution in the field of radiography.
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#29420;&#29305;&#30340;Transformer&#27169;&#22411;&#22312;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#20026;&#21487;&#38752;&#21644;&#21487;&#25345;&#32493;&#30340;&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.16108</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#30340;Transformer&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Transformer approach for Electricity Price Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16108
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#29420;&#29305;&#30340;Transformer&#27169;&#22411;&#22312;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26356;&#22909;&#30340;&#34920;&#29616;&#65292;&#20026;&#21487;&#38752;&#21644;&#21487;&#25345;&#32493;&#30340;&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#25552;&#20379;&#20102;&#26377;&#21069;&#26223;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#32431;Transformer&#27169;&#22411;&#36827;&#34892;&#30005;&#21147;&#20215;&#26684;&#39044;&#27979;&#65288;EPF&#65289;&#30340;&#26032;&#26041;&#27861;&#12290;&#19982;&#20854;&#20182;&#26041;&#27861;&#19981;&#21516;&#65292;&#27809;&#26377;&#20351;&#29992;&#20854;&#20182;&#36882;&#24402;&#32593;&#32476;&#32467;&#21512;&#27880;&#24847;&#21147;&#26426;&#21046;&#12290;&#22240;&#27492;&#65292;&#34920;&#26126;&#27880;&#24847;&#21147;&#23618;&#36275;&#20197;&#25429;&#25417;&#26102;&#38388;&#27169;&#24335;&#12290;&#35813;&#35770;&#25991;&#36824;&#36890;&#36807;&#20351;&#29992;&#24320;&#28304;EPF&#24037;&#20855;&#36827;&#34892;&#20102;&#23545;&#27169;&#22411;&#30340;&#20844;&#24179;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#20195;&#30721;&#20197;&#22686;&#24378;EPF&#30740;&#31350;&#30340;&#21487;&#20877;&#29616;&#24615;&#21644;&#36879;&#26126;&#24230;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;Transformer&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;&#65292;&#20026;&#21487;&#38752;&#21644;&#21487;&#25345;&#32493;&#30340;&#30005;&#21147;&#31995;&#32479;&#36816;&#34892;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16108v1 Announce Type: cross  Abstract: This paper presents a novel approach to electricity price forecasting (EPF) using a pure Transformer model. As opposed to other alternatives, no other recurrent network is used in combination to the attention mechanism. Hence, showing that the attention layer is enough for capturing the temporal patterns. The paper also provides fair comparison of the models using the open-source EPF toolbox and provide the code to enhance reproducibility and transparency in EPF research. The results show that the Transformer model outperforms traditional methods, offering a promising solution for reliable and sustainable power system operation.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#36890;&#36807;&#22269;&#38469;&#35843;&#26597;&#35780;&#20272;&#20102;&#19981;&#21516;&#22269;&#23478;&#23545;&#20854;&#20915;&#31574;&#24773;&#26223;&#20013;&#21508;&#31181;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#30340;&#36866;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16101</link><description>&lt;p&gt;
&#36328;&#22269;&#30028;&#35780;&#20272;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#65306;&#26469;&#33258;&#20154;&#31867;&#24863;&#30693;&#30340;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Evaluating Fairness Metrics Across Borders from Human Perceptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16101
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#36890;&#36807;&#22269;&#38469;&#35843;&#26597;&#35780;&#20272;&#20102;&#19981;&#21516;&#22269;&#23478;&#23545;&#20854;&#20915;&#31574;&#24773;&#26223;&#20013;&#21508;&#31181;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#30340;&#36866;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21738;&#20123;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#36866;&#29992;&#20110;&#24744;&#30340;&#22330;&#26223;&#65311;&#21363;&#20351;&#32467;&#26524;&#31526;&#21512;&#24050;&#24314;&#31435;&#30340;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#65292;&#20063;&#21487;&#33021;&#23384;&#22312;&#20851;&#20110;&#20844;&#24179;&#24863;&#30693;&#30340;&#19981;&#19968;&#33268;&#24773;&#20917;&#12290;&#24050;&#36827;&#34892;&#20102;&#22810;&#39033;&#35843;&#26597;&#65292;&#35780;&#20272;&#20102;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#19982;&#20154;&#20204;&#23545;&#20844;&#24179;&#30340;&#24863;&#30693;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#35843;&#26597;&#33539;&#22260;&#26377;&#38480;&#65292;&#20165;&#21253;&#25324;&#21333;&#20010;&#22269;&#23478;&#20013;&#25968;&#30334;&#21517;&#21442;&#19982;&#32773;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#22269;&#38469;&#35843;&#26597;&#65292;&#20197;&#35780;&#20272;&#21508;&#31181;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#22312;&#20915;&#31574;&#22330;&#26223;&#20013;&#30340;&#36866;&#29992;&#24615;&#12290;&#25105;&#20204;&#20998;&#21035;&#20174;&#20013;&#22269;&#12289;&#27861;&#22269;&#12289;&#26085;&#26412;&#21644;&#32654;&#22269;&#30340;&#27599;&#20010;&#22269;&#23478;&#25910;&#38598;&#20102;1,000&#21517;&#21442;&#19982;&#32773;&#30340;&#22238;&#24212;&#65292;&#24635;&#35745;&#24471;&#21040;&#20102;4,000&#20010;&#22238;&#24212;&#65292;&#20197;&#20998;&#26512;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#35843;&#26597;&#21253;&#25324;&#19977;&#20010;&#19981;&#21516;&#22330;&#26223;&#65292;&#37197;&#22791;&#20102;&#22235;&#31181;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#65292;&#27599;&#20010;&#21442;&#19982;&#32773;&#22312;&#27599;&#31181;&#24773;&#20917;&#19979;&#36873;&#25321;&#20854;&#21916;&#22909;&#30340;&#20844;&#24179;&#24230;&#37327;&#26631;&#20934;&#12290;&#35813;&#30740;&#31350;&#25506;&#35752;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16101v1 Announce Type: new  Abstract: Which fairness metrics are appropriately applicable in your contexts? There may be instances of discordance regarding the perception of fairness, even when the outcomes comply with established fairness metrics. Several surveys have been conducted to evaluate fairness metrics with human perceptions of fairness. However, these surveys were limited in scope, including only a few hundred participants within a single country. In this study, we conduct an international survey to evaluate the appropriateness of various fairness metrics in decision-making scenarios. We collected responses from 1,000 participants in each of China, France, Japan, and the United States, amassing a total of 4,000 responses, to analyze the preferences of fairness metrics. Our survey consists of three distinct scenarios paired with four fairness metrics, and each participant answers their preference for the fairness metric in each case. This investigation explores the
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#20262;&#29702;&#31995;&#32479;&#24212;&#35813;&#20855;&#22791;&#30340;&#23646;&#24615;&#65292;&#24182;&#25361;&#25112;&#31038;&#21306;&#20197;&#26356;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16100</link><description>&lt;p&gt;
&#25351;&#23450;&#20195;&#29702;&#20154;&#20262;&#29702;&#65288;&#34013;&#22825;&#24819;&#27861;&#65289;
&lt;/p&gt;
&lt;p&gt;
Specifying Agent Ethics (Blue Sky Ideas)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16100
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35752;&#35770;&#20102;&#26426;&#22120;&#20262;&#29702;&#31995;&#32479;&#24212;&#35813;&#20855;&#22791;&#30340;&#23646;&#24615;&#65292;&#24182;&#25361;&#25112;&#31038;&#21306;&#20197;&#26356;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#26426;&#22120;&#20262;&#29702;&#31995;&#32479;&#24212;&#35813;&#20855;&#22791;&#20160;&#20040;&#23646;&#24615;&#12290;&#36825;&#20010;&#38382;&#39064;&#30001;&#20110;&#23384;&#22312;&#26080;&#27861;&#36798;&#25104;&#19968;&#33268;&#35299;&#20915;&#26041;&#26696;&#30340;&#20262;&#29702;&#22256;&#22659;&#32780;&#21464;&#24471;&#22797;&#26434;&#12290;&#25105;&#20204;&#25552;&#20379;&#19968;&#20010;&#20363;&#23376;&#26469;&#28608;&#21169;&#25105;&#20204;&#20026;&#20160;&#20040;&#35748;&#20026;&#20165;&#20381;&#36182;&#21033;&#30410;&#30456;&#20851;&#32773;&#30340;&#20215;&#20540;&#35266;&#34920;&#36798;&#24182;&#19981;&#36275;&#20197;&#20445;&#35777;&#36825;&#31867;&#31995;&#32479;&#30340;&#27491;&#30830;&#24615;&#12290;&#25105;&#20204;&#32487;&#32493;&#23450;&#20041;&#20102;&#25105;&#20204;&#33258;&#24049;&#24037;&#20316;&#20013;&#20986;&#29616;&#30340;&#20004;&#31867;&#20262;&#29702;&#23646;&#24615;&#65292;&#24182;&#21521;&#31038;&#21306;&#25552;&#20986;&#19968;&#20010;&#25361;&#25112;&#65292;&#20197;&#26356;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#25506;&#35752;&#36825;&#19968;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16100v1 Announce Type: new  Abstract: We consider the question of what properties a Machine Ethics system should have. This question is complicated by the existence of ethical dilemmas with no agreed upon solution. We provide an example to motivate why we do not believe falling back on the elicitation of values from stakeholders is sufficient to guarantee correctness of such systems. We go on to define two broad categories of ethical property that have arisen in our own work and present a challenge to the community to approach this question in a more systematic way.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21363;&#36923;&#36753;&#20195;&#30721;&#27169;&#25311;&#65292;&#36843;&#20351;LLMs&#22312;&#39044;&#27979;&#36923;&#36753;&#31243;&#24207;&#30340;&#32467;&#26524;&#26102;&#27169;&#25311;&#36923;&#36753;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#20197;&#28145;&#20837;&#35843;&#26597;&#36825;&#19968;&#20219;&#21153;&#23545;LLMs&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.16097</link><description>&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#27169;&#25311;&#27714;&#35299;&#22120;&#21527;&#65311;LLMs&#30340;&#36923;&#36753;&#20195;&#30721;&#27169;&#25311;
&lt;/p&gt;
&lt;p&gt;
Can Language Models Pretend Solvers? Logic Code Simulation with LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16097
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20219;&#21153;&#65292;&#21363;&#36923;&#36753;&#20195;&#30721;&#27169;&#25311;&#65292;&#36843;&#20351;LLMs&#22312;&#39044;&#27979;&#36923;&#36753;&#31243;&#24207;&#30340;&#32467;&#26524;&#26102;&#27169;&#25311;&#36923;&#36753;&#27714;&#35299;&#22120;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#20197;&#28145;&#20837;&#35843;&#26597;&#36825;&#19968;&#20219;&#21153;&#23545;LLMs&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35299;&#20915;&#36923;&#36753;&#38382;&#39064;&#26041;&#38754;&#23637;&#31034;&#20102;&#37325;&#35201;&#28508;&#21147;&#12290;&#21033;&#29992;LLMs&#22312;&#20195;&#30721;&#30456;&#20851;&#27963;&#21160;&#20013;&#30340;&#21331;&#36234;&#33021;&#21147;&#65292;&#26368;&#36817;&#25552;&#20986;&#20102;&#20960;&#31181;&#21033;&#29992;&#36923;&#36753;&#27714;&#35299;&#22120;&#36827;&#34892;&#36923;&#36753;&#25512;&#29702;&#30340;&#26694;&#26550;&#12290;&#34429;&#28982;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#23558;LLMs&#35270;&#20026;&#33258;&#28982;&#35821;&#35328;&#36923;&#36753;&#27714;&#35299;&#22120;&#25110;&#32763;&#35793;&#22120;&#65292;&#20294;&#23427;&#20204;&#20316;&#20026;&#36923;&#36753;&#20195;&#30721;&#35299;&#37322;&#22120;&#21644;&#25191;&#34892;&#22120;&#30340;&#35282;&#33394;&#21463;&#21040;&#20102;&#36739;&#23569;&#20851;&#27880;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#26041;&#38754;&#65292;&#21363;&#36923;&#36753;&#20195;&#30721;&#27169;&#25311;&#65292;&#23427;&#36843;&#20351;LLMs&#22312;&#39044;&#27979;&#36923;&#36753;&#31243;&#24207;&#30340;&#32467;&#26524;&#26102;&#27169;&#25311;&#36923;&#36753;&#27714;&#35299;&#22120;&#12290;&#20026;&#36827;&#19968;&#27493;&#30740;&#31350;&#36825;&#19968;&#26032;&#39062;&#20219;&#21153;&#65292;&#25105;&#20204;&#21046;&#23450;&#20102;&#19977;&#20010;&#30740;&#31350;&#38382;&#39064;&#65306;LLMs&#33021;&#21542;&#26377;&#25928;&#22320;&#27169;&#25311;&#36923;&#36753;&#20195;&#30721;&#30340;&#36755;&#20986;&#65311;&#36923;&#36753;&#20195;&#30721;&#27169;&#25311;&#20276;&#38543;&#30528;&#21738;&#20123;&#20248;&#21183;&#65311;&#20197;&#21450;&#23384;&#22312;&#21738;&#20123;&#32570;&#38519;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25972;&#29702;&#20102;&#19977;&#20010;&#38024;&#23545;&#36923;&#36753;&#20195;&#30721;&#27169;&#25311;&#30340;&#26032;&#39062;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16097v1 Announce Type: new  Abstract: Transformer-based large language models (LLMs) have demonstrated significant potential in addressing logic problems. capitalizing on the great capabilities of LLMs for code-related activities, several frameworks leveraging logical solvers for logic reasoning have been proposed recently. While existing research predominantly focuses on viewing LLMs as natural language logic solvers or translators, their roles as logic code interpreters and executors have received limited attention. This study delves into a novel aspect, namely logic code simulation, which forces LLMs to emulate logical solvers in predicting the results of logical programs. To further investigate this novel task, we formulate our three research questions: Can LLMs efficiently simulate the outputs of logic codes? What strength arises along with logic code simulation? And what pitfalls? To address these inquiries, we curate three novel datasets tailored for the logic code si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; AI &#22312;&#23398;&#20064;&#21644;&#25945;&#32946;&#20013;&#30340;&#22810;&#32500;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102; AI&#12289;&#20998;&#26512;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25361;&#25112;&#20102;&#23558; AI &#35270;&#20026;&#38543;&#26426;&#24037;&#20855;&#30340;&#35266;&#24565;&#65292;&#24378;&#35843;&#20102; AI &#20316;&#20026;&#29702;&#35299;&#20154;&#31867;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#29420;&#29305;&#30340;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.16081</link><description>&lt;p&gt;
&#25945;&#32946;&#20013;&#23398;&#20064;&#12289;&#20998;&#26512;&#21644;&#20154;&#24037;&#26234;&#33021;&#30340;&#30456;&#20114;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
The Interplay of Learning, Analytics, and Artificial Intelligence in Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16081
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; AI &#22312;&#23398;&#20064;&#21644;&#25945;&#32946;&#20013;&#30340;&#22810;&#32500;&#35270;&#35282;&#65292;&#24378;&#35843;&#20102; AI&#12289;&#20998;&#26512;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25361;&#25112;&#20102;&#23558; AI &#35270;&#20026;&#38543;&#26426;&#24037;&#20855;&#30340;&#35266;&#24565;&#65292;&#24378;&#35843;&#20102; AI &#20316;&#20026;&#29702;&#35299;&#20154;&#31867;&#23398;&#20064;&#30340;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19977;&#31181;&#29420;&#29305;&#30340;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20154;&#24037;&#26234;&#33021;&#22312;&#23398;&#20064;&#21644;&#25945;&#32946;&#20013;&#30340;&#22810;&#32500;&#35270;&#35282;&#65292;&#24378;&#35843;&#20154;&#24037;&#26234;&#33021;&#12289;&#20998;&#26512;&#21644;&#23398;&#20064;&#36807;&#31243;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#31508;&#32773;&#25361;&#25112;&#20102;&#23558;&#20154;&#24037;&#26234;&#33021;&#20165;&#35270;&#20026;&#38543;&#26426;&#24037;&#20855;&#30340;&#26222;&#36941;&#35266;&#24565;&#65292;&#20363;&#22914;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65292;&#20027;&#24352;&#37325;&#35270;&#23545;&#20154;&#24037;&#26234;&#33021;&#30340;&#26367;&#20195;&#27010;&#24565;&#12290;&#25991;&#31456;&#31361;&#20986;&#20102;&#20154;&#31867;&#26234;&#33021;&#19982;&#20154;&#24037;&#20449;&#24687;&#22788;&#29702;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;AI&#31639;&#27861;&#20013;&#22266;&#26377;&#30340;&#35748;&#30693;&#22810;&#26679;&#24615;&#65292;&#24182;&#25552;&#20986;AI&#20063;&#21487;&#20197;&#20316;&#20026;&#29702;&#35299;&#20154;&#31867;&#23398;&#20064;&#30340;&#24037;&#20855;&#12290;&#20174;&#23558;AI&#35270;&#20026;&#20154;&#31867;&#26234;&#33021;&#30340;&#31867;&#27604;&#30340;&#26089;&#26399;&#23398;&#20064;&#31185;&#23398;&#21644;&#25945;&#32946;&#20013;&#30340;AI&#30740;&#31350;&#24050;&#32463;&#20559;&#31163;&#36825;&#19968;&#35266;&#28857;&#65292;&#20419;&#20351;&#26377;&#24517;&#35201;&#37325;&#26032;&#28857;&#29123;&#36825;&#31181;&#32852;&#31995;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#31181;&#29420;&#29305;&#30340;&#25945;&#32946;&#20013;&#20154;&#24037;&#26234;&#33021;&#30340;&#27010;&#24565;&#21270;&#65306;&#20154;&#31867;&#35748;&#30693;&#30340;&#22806;&#37096;&#21270;&#12289;&#20869;&#21270;AI&#27169;&#22411;&#20197;&#24433;&#21709;&#20154;&#31867;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16081v1 Announce Type: cross  Abstract: This paper presents a multi dimensional view of AI's role in learning and education, emphasizing the intricate interplay between AI, analytics, and the learning processes. Here, I challenge the prevalent narrow conceptualization of AI as stochastic tools, as exemplified in generative AI, and argue for the importance of alternative conceptualisations of AI. I highlight the differences between human intelligence and artificial information processing, the cognitive diversity inherent in AI algorithms, and posit that AI can also serve as an instrument for understanding human learning. Early learning sciences and AI in Education research, which saw AI as an analogy for human intelligence, have diverged from this perspective, prompting a need to rekindle this connection. The paper presents three unique conceptualizations of AI in education: the externalization of human cognition, the internalization of AI models to influence human thought pr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#21767;&#37096;&#22320;&#26631;&#24341;&#23548;&#30340;&#32454;&#31890;&#24230;&#35270;&#35273;&#32447;&#32034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#35828;&#35805;&#20154;&#30340;&#21767;&#35835;&#27169;&#22411;&#65292;&#26377;&#25928;&#38477;&#20302;&#35828;&#35805;&#20154;&#20043;&#38388;&#30340;&#35270;&#35273;&#21464;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.16071</link><description>&lt;p&gt;
&#36890;&#36807;&#20114;&#20449;&#24687;&#27491;&#21017;&#21270;&#36827;&#34892;&#22522;&#20934;&#24341;&#23548;&#30340;&#36328;&#35828;&#35805;&#20154;&#21767;&#35835;
&lt;/p&gt;
&lt;p&gt;
Landmark-Guided Cross-Speaker Lip Reading with Mutual Information Regularization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16071
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#21767;&#37096;&#22320;&#26631;&#24341;&#23548;&#30340;&#32454;&#31890;&#24230;&#35270;&#35273;&#32447;&#32034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#24212;&#35828;&#35805;&#20154;&#30340;&#21767;&#35835;&#27169;&#22411;&#65292;&#26377;&#25928;&#38477;&#20302;&#35828;&#35805;&#20154;&#20043;&#38388;&#30340;&#35270;&#35273;&#21464;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Lip reading&#65292;&#21363;&#36890;&#36807;&#35270;&#35273;&#21767;&#37096;&#36816;&#21160;&#35299;&#37322;&#26080;&#22768;&#35821;&#38899;&#30340;&#36807;&#31243;&#65292;&#30001;&#20110;&#20854;&#24191;&#27867;&#30340;&#23454;&#38469;&#24212;&#29992;&#32780;&#24341;&#36215;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#26497;&#22823;&#22320;&#25913;&#36827;&#20102;&#24403;&#21069;&#30340;&#21767;&#35835;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#22312;&#35828;&#35805;&#20154;&#21464;&#21270;&#30340;&#20132;&#21449;&#35828;&#35805;&#20154;&#22330;&#26223;&#20013;&#36827;&#34892;&#21767;&#35835;&#65292;&#30001;&#20110;&#35828;&#35805;&#20154;&#20043;&#38388;&#30340;&#21464;&#24322;&#24615;&#65292;&#23384;&#22312;&#25361;&#25112;&#24615;&#38382;&#39064;&#12290;&#19968;&#20010;&#35757;&#32451;&#33391;&#22909;&#30340;&#21767;&#35835;&#31995;&#32479;&#22312;&#22788;&#29702;&#20840;&#26032;&#30340;&#35828;&#35805;&#20154;&#26102;&#21487;&#33021;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#23398;&#20064;&#19968;&#20010;&#36866;&#24212;&#35828;&#35805;&#20154;&#30340;&#21767;&#35835;&#27169;&#22411;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#35265;&#35299;&#26159;&#20943;&#23569;&#35828;&#35805;&#20154;&#20043;&#38388;&#30340;&#35270;&#35273;&#21464;&#21270;&#65292;&#36991;&#20813;&#27169;&#22411;&#36807;&#24230;&#25311;&#21512;&#29305;&#23450;&#35828;&#35805;&#20154;&#12290;&#26412;&#30740;&#31350;&#38024;&#23545;&#22522;&#20110;&#28151;&#21512;CTC/attention&#26550;&#26500;&#30340;&#36755;&#20837;&#35270;&#35273;&#32447;&#32034;&#21644;&#22522;&#20110;&#38544;&#21464;&#37327;&#34920;&#31034;&#65292;&#25552;&#20986;&#21033;&#29992;&#21767;&#37096;&#22320;&#26631;&#24341;&#23548;&#30340;&#32454;&#31890;&#24230;&#35270;&#35273;&#32447;&#32034;&#65292;&#32780;&#19981;&#26159;&#39057;&#32321;&#20351;&#29992;&#30340;&#35009;&#21098;&#22068;&#24052;&#22270;&#29255;&#20316;&#20026;&#36755;&#20837;&#29305;&#24449;&#65292;&#20943;&#23569;&#35828;&#35805;&#20154;&#29305;&#23450;&#30340;&#22806;&#35266;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16071v1 Announce Type: new  Abstract: Lip reading, the process of interpreting silent speech from visual lip movements, has gained rising attention for its wide range of realistic applications. Deep learning approaches greatly improve current lip reading systems. However, lip reading in cross-speaker scenarios where the speaker identity changes, poses a challenging problem due to inter-speaker variability. A well-trained lip reading system may perform poorly when handling a brand new speaker. To learn a speaker-robust lip reading model, a key insight is to reduce visual variations across speakers, avoiding the model overfitting to specific speakers. In this work, in view of both input visual clues and latent representations based on a hybrid CTC/attention architecture, we propose to exploit the lip landmark-guided fine-grained visual clues instead of frequently-used mouth-cropped images as input features, diminishing speaker-specific appearance characteristics. Furthermore, 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#29420;&#31435;&#20110;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#31283;&#20581;&#21453;&#21521;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#26377;&#25928;&#22788;&#29702;&#23545;&#25239;&#20928;&#21270;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#25439;&#22833;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.16067</link><description>&lt;p&gt;
&#38024;&#23545;&#23545;&#25239;&#20928;&#21270;&#30340;&#24378;&#22823;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Robust Diffusion Models for Adversarial Purification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16067
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#29420;&#31435;&#20110;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#31283;&#20581;&#21453;&#21521;&#36807;&#31243;&#65292;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#65292;&#26377;&#25928;&#22788;&#29702;&#23545;&#25239;&#20928;&#21270;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#25439;&#22833;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#27169;&#22411;&#65288;DM&#65289;&#30340;&#23545;&#25239;&#20928;&#21270;&#65288;AP&#65289;&#24050;&#34987;&#35777;&#26126;&#26159;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#26368;&#26377;&#21147;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24573;&#30053;&#20102;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26412;&#36523;&#23545;&#23545;&#25239;&#25915;&#20987;&#24182;&#19981;&#31283;&#20581;&#36825;&#19968;&#20107;&#23454;&#12290;&#27492;&#22806;&#65292;&#25193;&#25955;&#36807;&#31243;&#24456;&#23481;&#26131;&#30772;&#22351;&#35821;&#20041;&#20449;&#24687;&#65292;&#22312;&#21453;&#21521;&#36807;&#31243;&#21518;&#29983;&#25104;&#39640;&#36136;&#37327;&#22270;&#20687;&#20294;&#19982;&#21407;&#22987;&#36755;&#20837;&#22270;&#20687;&#23436;&#20840;&#19981;&#21516;&#65292;&#23548;&#33268;&#26631;&#20934;&#31934;&#24230;&#19979;&#38477;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#19968;&#20010;&#33258;&#28982;&#30340;&#24819;&#27861;&#26159;&#21033;&#29992;&#23545;&#25239;&#35757;&#32451;&#31574;&#30053;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#28982;&#32780;&#36825;&#22312;&#35745;&#31639;&#19978;&#26159;&#31105;&#27490;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20855;&#26377;&#23545;&#25239;&#24341;&#23548;&#30340;&#31283;&#20581;&#21453;&#21521;&#36807;&#31243;&#65292;&#23427;&#29420;&#31435;&#20110;&#32473;&#23450;&#30340;&#39044;&#35757;&#32451;DMs&#65292;&#24182;&#19988;&#36991;&#20813;&#20102;&#37325;&#26032;&#35757;&#32451;&#25110;&#24494;&#35843;DMs&#12290;&#36825;&#31181;&#24378;&#22823;&#30340;&#24341;&#23548;&#19981;&#20165;&#21487;&#20197;&#30830;&#20445;&#29983;&#25104;&#30340;&#20928;&#21270;&#31034;&#20363;&#20445;&#30041;&#26356;&#22810;&#30340;&#35821;&#20041;&#20869;&#23481;&#65292;&#36824;&#21487;&#20197;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16067v1 Announce Type: cross  Abstract: Diffusion models (DMs) based adversarial purification (AP) has shown to be the most powerful alternative to adversarial training (AT). However, these methods neglect the fact that pre-trained diffusion models themselves are not robust to adversarial attacks as well. Additionally, the diffusion process can easily destroy semantic information and generate a high quality image but totally different from the original input image after the reverse process, leading to degraded standard accuracy. To overcome these issues, a natural idea is to harness adversarial training strategy to retrain or fine-tune the pre-trained diffusion model, which is computationally prohibitive. We propose a novel robust reverse process with adversarial guidance, which is independent of given pre-trained DMs and avoids retraining or fine-tuning the DMs. This robust guidance can not only ensure to generate purified examples retaining more semantic content but also m
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#39318;&#27425;&#23558;&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#30452;&#25509;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21160;&#24577;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.16066</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#21160;&#24577;&#25512;&#33616;&#30340;&#26102;&#38388;&#22270;&#32593;&#32476;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Temporal Graph Network Framework for Dynamic Recommendation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16066
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#39318;&#27425;&#23558;&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#30452;&#25509;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#21160;&#24577;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#23545;&#20110;&#29992;&#25143;&#22312;&#30005;&#23376;&#21830;&#21153;&#21644;&#27969;&#23186;&#20307;&#26381;&#21153;&#31561;&#24179;&#21488;&#19978;&#30340;&#21442;&#19982;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;&#30001;&#20110;&#38745;&#24577;&#25968;&#25454;&#20381;&#36182;&#65292;&#25512;&#33616;&#31995;&#32479;&#24120;&#24120;&#33853;&#21518;&#20110;&#29992;&#25143;&#19981;&#26029;&#21464;&#21270;&#30340;&#20559;&#22909;&#12290;&#22312;&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#34987;&#25552;&#20986;&#21518;&#65292;&#21508;&#31181;&#30740;&#31350;&#34920;&#26126;TGN&#21487;&#20197;&#26174;&#33879;&#25913;&#21892;&#33410;&#28857;&#21644;&#36793;&#30340;&#29305;&#24449;&#38543;&#26102;&#38388;&#21160;&#24577;&#21464;&#21270;&#30340;&#24773;&#20917;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20854;&#26377;&#30528;&#33391;&#22909;&#30340;&#28508;&#21147;&#65292;&#20294;&#36804;&#20170;&#20026;&#27490;&#23578;&#26410;&#30452;&#25509;&#24212;&#29992;&#20110;&#25512;&#33616;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#36890;&#36807;&#30452;&#25509;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23454;&#29616;&#26102;&#38388;&#22270;&#32593;&#32476;&#65288;TGN&#65289;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#36825;&#22312;&#35813;&#39046;&#22495;&#23578;&#23646;&#39318;&#27425;&#12290;&#36890;&#36807;&#20351;&#29992;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#21644;&#19968;&#31995;&#21015;&#22270;&#24418;&#21644;&#21382;&#21490;&#23884;&#20837;&#26041;&#27861;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;TGN&#30340;&#36866;&#24212;&#24615;&#65292;&#35777;&#23454;&#20102;&#20854;&#22312;&#21160;&#24577;&#25512;&#33616;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16066v1 Announce Type: new  Abstract: Recommender systems, crucial for user engagement on platforms like e-commerce and streaming services, often lag behind users' evolving preferences due to static data reliance. After Temporal Graph Networks (TGNs) were proposed, various studies have shown that TGN can significantly improve situations where the features of nodes and edges dynamically change over time. However, despite its promising capabilities, it has not been directly applied in recommender systems to date. Our study bridges this gap by directly implementing Temporal Graph Networks (TGN) in recommender systems, a first in this field. Using real-world datasets and a range of graph and history embedding methods, we show TGN's adaptability, confirming its effectiveness in dynamic recommendation scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#20013;&#21307;&#39046;&#22495;&#26500;&#24314;&#20102;&#19987;&#19994;&#35821;&#26009;&#24211;&#65292;&#22522;&#20110;LLaMA&#25104;&#21151;&#24320;&#21457;&#20102;&#39318;&#20010;&#32463;&#36807;&#23436;&#25972;&#35757;&#32451;&#30340;Qibo&#27169;&#22411;&#65292;&#24182;&#25512;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#24615;&#33021;&#30340;Qibo&#22522;&#20934;&#27979;&#35797;&#12290;</title><link>https://arxiv.org/abs/2403.16056</link><description>&lt;p&gt;
Qibo: &#19968;&#31181;&#29992;&#20110;&#20013;&#21307;&#39046;&#22495;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Qibo: A Large Language Model for Traditional Chinese Medicine
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16056
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#20013;&#21307;&#39046;&#22495;&#26500;&#24314;&#20102;&#19987;&#19994;&#35821;&#26009;&#24211;&#65292;&#22522;&#20110;LLaMA&#25104;&#21151;&#24320;&#21457;&#20102;&#39318;&#20010;&#32463;&#36807;&#23436;&#25972;&#35757;&#32451;&#30340;Qibo&#27169;&#22411;&#65292;&#24182;&#25512;&#20986;&#20102;&#29992;&#20110;&#35780;&#20272;LLMs&#24615;&#33021;&#30340;Qibo&#22522;&#20934;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#22312;&#29992;&#25143;&#24847;&#22270;&#29702;&#35299;&#21644;&#21709;&#24212;&#26041;&#38754;&#21462;&#24471;&#30340;&#26174;&#33879;&#36827;&#23637;&#65292;&#22312;&#35768;&#22810;&#19987;&#19994;&#39046;&#22495;&#65292;&#21253;&#25324;&#21307;&#23398;&#12289;&#27861;&#24459;&#21644;&#37329;&#34701;&#12290;&#28982;&#32780;&#65292;&#22312;&#20013;&#21307;&#39046;&#22495;&#65292;LLMs&#30340;&#24615;&#33021;&#25552;&#21319;&#21463;&#21040;&#25361;&#25112;&#65292;&#20854;&#21407;&#22240;&#22312;&#20110;&#20013;&#21307;&#29702;&#35770;&#19982;&#29616;&#20195;&#21307;&#23398;&#20043;&#38388;&#30340;&#26681;&#26412;&#24046;&#24322;&#65292;&#20197;&#21450;&#32570;&#20047;&#19987;&#19994;&#35821;&#26009;&#24211;&#36164;&#28304;&#12290;&#26412;&#25991;&#26088;&#22312;&#26500;&#24314;&#21644;&#25972;&#29702;&#20013;&#21307;&#39046;&#22495;&#30340;&#19987;&#19994;&#35821;&#26009;&#24211;&#65292;&#36171;&#20104;&#22823;&#22411;&#27169;&#22411;&#20855;&#26377;&#20013;&#21307;&#29702;&#35770;&#29305;&#33394;&#30340;&#19987;&#19994;&#30693;&#35782;&#65292;&#24182;&#25104;&#21151;&#22522;&#20110;LLaMA&#24320;&#21457;&#20102;Qibo&#27169;&#22411;&#65292;&#36825;&#26159;&#20013;&#21307;&#39046;&#22495;&#31532;&#19968;&#20010;&#32463;&#36807;&#23436;&#25972;&#35757;&#32451;&#36807;&#31243;&#65288;&#20174;&#39044;&#35757;&#32451;&#21040;&#30417;&#30563;&#24494;&#35843;&#65289;&#30340;LLM&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;Qibo&#22522;&#20934;&#27979;&#35797;&#65292;&#36825;&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;LLMs&#24615;&#33021;&#30340;&#19987;&#38376;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16056v1 Announce Type: cross  Abstract: In the field of Artificial Intelligence, Large Language Models (LLMs) have demonstrated significant advances in user intent understanding and response in a number of specialized domains, including medicine, law, and finance. However, in the unique domain of traditional Chinese medicine (TCM), the performance enhancement of LLMs is challenged by the essential differences between its theories and modern medicine, as well as the lack of specialized corpus resources. In this paper, we aim to construct and organize a professional corpus in the field of TCM, to endow the large model with professional knowledge that is characteristic of TCM theory, and to successfully develop the Qibo model based on LLaMA, which is the first LLM in the field of TCM to undergo a complete training process from pre-training to Supervised Fine-Tuning (SFT). Furthermore, we develop the Qibo-benchmark, a specialized tool for evaluating the performance of LLMs, whic
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;NeRF&#27169;&#22411;&#22312;&#37325;&#24314;3D&#32467;&#26500;&#20013;&#20165;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#36890;&#36807;&#21435;&#38500;RGB&#36755;&#20986;&#32452;&#20214;&#65292;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#37325;&#28857;&#20851;&#27880;&#35821;&#20041;&#36755;&#20986;&#19982;&#22320;&#38754;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#12290;</title><link>https://arxiv.org/abs/2403.16043</link><description>&lt;p&gt;
&#21482;&#38656;&#35821;&#20041;&#20449;&#24687;&#65306;NeRF&#37325;&#24314;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#36275;&#22815;
&lt;/p&gt;
&lt;p&gt;
Semantic Is Enough: Only Semantic Information For NeRF Reconstruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16043
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23637;&#31034;&#20102;NeRF&#27169;&#22411;&#22312;&#37325;&#24314;3D&#32467;&#26500;&#20013;&#20165;&#21033;&#29992;&#35821;&#20041;&#20449;&#24687;&#30340;&#20248;&#31168;&#34920;&#29616;&#65292;&#36890;&#36807;&#21435;&#38500;RGB&#36755;&#20986;&#32452;&#20214;&#65292;&#27169;&#22411;&#35757;&#32451;&#36807;&#31243;&#37325;&#28857;&#20851;&#27880;&#35821;&#20041;&#36755;&#20986;&#19982;&#22320;&#38754;&#30495;&#23454;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#32467;&#21512;&#38544;&#24335;3D&#34920;&#31034;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#22914;Semantic-NeRF&#65292;&#35777;&#26126;NeRF&#27169;&#22411;&#22312;&#21576;&#29616;&#20855;&#26377;&#35821;&#20041;&#26631;&#31614;&#30340;3D&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#20165;&#20851;&#27880;&#35821;&#20041;&#36755;&#20986;&#24182;&#31227;&#38500;RGB&#36755;&#20986;&#32452;&#20214;&#26469;&#25193;&#23637;Semantic Neural Radiance Fields&#65288;Semantic-NeRF&#65289;&#27169;&#22411;&#12290;&#25105;&#20204;&#37325;&#26032;&#26500;&#24314;&#27169;&#22411;&#21450;&#20854;&#35757;&#32451;&#36807;&#31243;&#65292;&#20165;&#21033;&#29992;&#27169;&#22411;&#35821;&#20041;&#36755;&#20986;&#19982;&#22320;&#38754;&#30495;&#23454;&#35821;&#20041;&#22270;&#20687;&#20043;&#38388;&#30340;&#20132;&#21449;&#29109;&#25439;&#22833;&#65292;&#31227;&#38500;&#20102;&#21407;&#22987;Semantic-NeRF&#26041;&#27861;&#20013;&#20256;&#32479;&#20351;&#29992;&#30340;&#39068;&#33394;&#25968;&#25454;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#20351;&#29992;&#21407;&#22987;&#21644;&#20462;&#25913;&#21518;&#30340;Semantic-NeRF&#27169;&#22411;&#36827;&#34892;&#20102;&#19968;&#31995;&#21015;&#30456;&#21516;&#30340;&#23454;&#39564;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#35266;&#23519;&#36825;&#31181;&#20462;&#25913;&#23545;Semantic-NeRF&#27169;&#22411;&#24615;&#33021;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#22330;&#26223;&#29702;&#35299;&#12289;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;&#20219;&#21153;&#12290;&#32467;&#26524;&#20026;&#22330;&#26223;&#29702;&#35299;&#12289;&#29289;&#20307;&#26816;&#27979;&#21644;&#20998;&#21106;&#31561;&#20219;&#21153;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16043v1 Announce Type: cross  Abstract: Recent research that combines implicit 3D representation with semantic information, like Semantic-NeRF, has proven that NeRF model could perform excellently in rendering 3D structures with semantic labels. This research aims to extend the Semantic Neural Radiance Fields (Semantic-NeRF) model by focusing solely on semantic output and removing the RGB output component. We reformulate the model and its training procedure to leverage only the cross-entropy loss between the model semantic output and the ground truth semantic images, removing the colour data traditionally used in the original Semantic-NeRF approach. We then conduct a series of identical experiments using the original and the modified Semantic-NeRF model. Our primary objective is to obverse the impact of this modification on the model performance by Semantic-NeRF, focusing on tasks such as scene understanding, object detection, and segmentation. The results offer valuable ins
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38754;&#21521;&#20851;&#33410;&#23545;&#35937;&#30340;&#20581;&#22766;&#24863;&#30693;&#21644;&#25805;&#20316;&#26694;&#26550;RPMArt&#65292;&#20027;&#35201;&#36129;&#29486;&#26159;&#33021;&#22815;&#31283;&#20581;&#22320;&#39044;&#27979;&#20851;&#33410;&#21442;&#25968;&#21644;&#21487;&#20449;&#28857;&#30340;RoArtNet&#12290;</title><link>https://arxiv.org/abs/2403.16023</link><description>&lt;p&gt;
RPMArt&#65306;&#38754;&#21521;&#20851;&#33410;&#23545;&#35937;&#30340;&#20581;&#22766;&#24863;&#30693;&#21644;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
RPMArt: Towards Robust Perception and Manipulation for Articulated Objects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16023
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38754;&#21521;&#20851;&#33410;&#23545;&#35937;&#30340;&#20581;&#22766;&#24863;&#30693;&#21644;&#25805;&#20316;&#26694;&#26550;RPMArt&#65292;&#20027;&#35201;&#36129;&#29486;&#26159;&#33021;&#22815;&#31283;&#20581;&#22320;&#39044;&#27979;&#20851;&#33410;&#21442;&#25968;&#21644;&#21487;&#20449;&#28857;&#30340;RoArtNet&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#33410;&#23545;&#35937;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#24456;&#24120;&#35265;&#12290;&#23545;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#26469;&#35828;&#65292;&#26426;&#22120;&#20154;&#33021;&#22815;&#34920;&#29616;&#20986;&#23545;&#20851;&#33410;&#23545;&#35937;&#30340;&#20581;&#22766;&#24863;&#30693;&#21644;&#25805;&#20316;&#25216;&#33021;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20851;&#33410;&#23545;&#35937;&#26041;&#27861;&#19981;&#22815;&#35299;&#20915;&#28857;&#20113;&#20013;&#30340;&#22122;&#22768;&#38382;&#39064;&#65292;&#38590;&#20197;&#24357;&#21512;&#27169;&#25311;&#19982;&#29616;&#23454;&#20043;&#38388;&#30340;&#24046;&#36317;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#37096;&#32626;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38754;&#21521;&#20851;&#33410;&#23545;&#35937;&#30340;&#20581;&#22766;&#24863;&#30693;&#21644;&#25805;&#20316;&#30340;&#26694;&#26550;&#65288;RPMArt&#65289;&#65292;&#35813;&#26694;&#26550;&#23398;&#20064;&#22914;&#20309;&#20174;&#22024;&#26434;&#30340;&#28857;&#20113;&#20013;&#20272;&#35745;&#20851;&#33410;&#21442;&#25968;&#24182;&#25805;&#20316;&#20851;&#33410;&#37096;&#20998;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#19968;&#20010;&#20581;&#22766;&#20851;&#33410;&#32593;&#32476;&#65288;RoArtNet&#65289;&#65292;&#36890;&#36807;&#23616;&#37096;&#29305;&#24449;&#23398;&#20064;&#21644;&#28857;&#20803;&#32452;&#25237;&#31080;&#33021;&#22815;&#31283;&#20581;&#22320;&#39044;&#27979;&#20851;&#33410;&#21442;&#25968;&#21644;&#21487;&#20449;&#28857;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#20851;&#33410;&#24863;&#30693;&#20998;&#31867;&#26041;&#26696;&#26469;&#22686;&#24378;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16023v1 Announce Type: cross  Abstract: Articulated objects are commonly found in daily life. It is essential that robots can exhibit robust perception and manipulation skills for articulated objects in real-world robotic applications. However, existing methods for articulated objects insufficiently address noise in point clouds and struggle to bridge the gap between simulation and reality, thus limiting the practical deployment in real-world scenarios. To tackle these challenges, we propose a framework towards Robust Perception and Manipulation for Articulated Objects (RPMArt), which learns to estimate the articulation parameters and manipulate the articulation part from the noisy point cloud. Our primary contribution is a Robust Articulation Network (RoArtNet) that is able to predict both joint parameters and affordable points robustly by local feature learning and point tuple voting. Moreover, we introduce an articulation-aware classification scheme to enhance its ability
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#20462;&#22797;&#27969;&#31243;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#20419;&#36827;&#21644;&#25511;&#21046;&#29983;&#25104;&#20869;&#23481;&#26041;&#38754;&#30340;&#20851;&#38190;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2403.16016</link><description>&lt;p&gt;
&#22635;&#34917;&#31354;&#30333;(&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#20462;&#22797;&#27969;&#31243;)
&lt;/p&gt;
&lt;p&gt;
Fill in the ____ (a Diffusion-based Image Inpainting Pipeline)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16016
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#20462;&#22797;&#27969;&#31243;&#65292;&#24357;&#34917;&#20102;&#29616;&#26377;&#27169;&#22411;&#22312;&#20419;&#36827;&#21644;&#25511;&#21046;&#29983;&#25104;&#20869;&#23481;&#26041;&#38754;&#30340;&#20851;&#38190;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#20462;&#22797;&#26159;&#23558;&#22270;&#20687;&#36755;&#20837;&#24182;&#29983;&#25104;&#20002;&#22833;&#25110;&#25925;&#24847;&#36974;&#25377;&#37096;&#20998;&#30340;&#36807;&#31243;&#12290;&#20462;&#22797;&#26377;&#26080;&#25968;&#24212;&#29992;&#65292;&#21253;&#25324;&#24674;&#22797;&#20808;&#21069;&#25439;&#22351;&#30340;&#22270;&#29255;&#12289;&#24674;&#22797;&#30001;&#20110;&#21387;&#32553;&#32780;&#38477;&#20302;&#36136;&#37327;&#30340;&#22270;&#20687;&#65292;&#20197;&#21450;&#21024;&#38500;&#19981;&#38656;&#35201;&#30340;&#29289;&#20307;/&#25991;&#26412;&#12290;&#29616;&#20195;&#20462;&#22797;&#25216;&#26415;&#24050;&#32463;&#26174;&#31034;&#20986;&#22312;&#20026;&#20855;&#26377;&#36974;&#32617;&#36974;&#25377;&#30340;&#22270;&#20687;&#29983;&#25104;&#21512;&#29702;&#23436;&#25104;&#26041;&#38754;&#30340;&#26174;&#30528;&#33021;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#25552;&#20379;&#20462;&#22797;&#25216;&#26415;&#36827;&#23637;&#30340;&#27010;&#36848;&#65292;&#24182;&#30830;&#23450;&#24403;&#21069;&#39046;&#20808;&#26041;&#27861;&#65292;&#37325;&#28857;&#25918;&#22312;&#23427;&#20204;&#30340;&#20248;&#21183;&#21644;&#21155;&#21183;&#19978;&#12290;&#25105;&#20204;&#23558;&#35299;&#20915;&#36825;&#20123;&#29616;&#26377;&#27169;&#22411;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#24046;&#36317;&#65292;&#37325;&#28857;&#26159;&#20419;&#36827;&#21644;&#25511;&#21046;&#29983;&#25104;&#30340;&#30830;&#20999;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#36824;&#23558;&#35777;&#26126;&#20026;&#20160;&#20040;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#20462;&#22797;&#27169;&#22411;&#24517;&#39035;&#37319;&#21462;&#30340;&#33258;&#28982;&#30340;&#19979;&#19968;&#20010;&#36827;&#27493;&#27493;&#39588;&#65292;&#24182;&#25552;&#20379;&#23454;&#29616;&#36825;&#19968;&#21151;&#33021;&#30340;&#22810;&#31181;&#26041;&#27861;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16016v1 Announce Type: cross  Abstract: Image inpainting is the process of taking an image and generating lost or intentionally occluded portions. Inpainting has countless applications including restoring previously damaged pictures, restoring the quality of images that have been degraded due to compression, and removing unwanted objects/text. Modern inpainting techniques have shown remarkable ability in generating sensible completions for images with mask occlusions. In our paper, an overview of the progress of inpainting techniques will be provided, along with identifying current leading approaches, focusing on their strengths and weaknesses. A critical gap in these existing models will be addressed, focusing on the ability to prompt and control what exactly is generated. We will additionally justify why we think this is the natural next progressive step that inpainting models must take, and provide multiple approaches to implementing this functionality. Finally, we will e
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#22270;&#32593;&#32476;&#32467;&#26500;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#32852;&#37030;&#21442;&#25968;&#32858;&#21512;&#26041;&#27861;FLGNN&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#35774;&#35745;&#20102;&#38544;&#31169;&#23433;&#20840;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#23454;&#39564;&#21644;&#24046;&#20998;&#38544;&#31169;&#38450;&#24481;&#23454;&#39564;&#12290;</title><link>https://arxiv.org/abs/2403.16004</link><description>&lt;p&gt;
&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#22270;&#32593;&#32476;&#32467;&#26500;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#32852;&#37030;&#21442;&#25968;&#32858;&#21512;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Federated Parameter Aggregation Method for Node Classification Tasks with Different Graph Network Structures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16004
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#20855;&#26377;&#19981;&#21516;&#22270;&#32593;&#32476;&#32467;&#26500;&#30340;&#33410;&#28857;&#20998;&#31867;&#20219;&#21153;&#30340;&#32852;&#37030;&#21442;&#25968;&#32858;&#21512;&#26041;&#27861;FLGNN&#65292;&#24182;&#39564;&#35777;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;&#21516;&#26102;&#65292;&#35774;&#35745;&#20102;&#38544;&#31169;&#23433;&#20840;&#30340;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#23454;&#39564;&#21644;&#24046;&#20998;&#38544;&#31169;&#38450;&#24481;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#20960;&#24180;&#37324;&#65292;&#30001;&#20110;&#20854;&#21327;&#20316;&#35757;&#32451;&#22810;&#20010;&#26469;&#28304;&#25968;&#25454;&#32780;&#19981;&#20250;&#27844;&#38706;&#38544;&#31169;&#30340;&#33021;&#21147;&#65292;&#32852;&#37030;&#23398;&#20064;&#24050;&#32463;&#24191;&#27867;&#24212;&#29992;&#20110;&#21508;&#31181;&#32463;&#20856;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#22312;&#22270;&#31070;&#32463;&#32593;&#32476;&#39046;&#22495;&#65292;&#30001;&#23458;&#25143;&#31471;&#25345;&#26377;&#30340;&#22270;&#30340;&#33410;&#28857;&#21644;&#32593;&#32476;&#32467;&#26500;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#26159;&#19981;&#21516;&#30340;&#65292;&#30452;&#25509;&#20849;&#20139;&#27169;&#22411;&#26799;&#24230;&#30340;&#32858;&#21512;&#26041;&#27861;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#36825;&#31181;&#24773;&#20917;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#21508;&#31181;&#22270;&#32852;&#37030;&#22330;&#26223;&#30340;&#32852;&#37030;&#32858;&#21512;&#26041;&#27861;FLGNN&#65292;&#24182;&#25506;&#35752;&#20102;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#27599;&#19968;&#23618;&#21442;&#25968;&#20849;&#20139;&#30340;&#32858;&#21512;&#25928;&#26524;&#12290;&#36890;&#36807;&#22312;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#39564;&#35777;&#20102;&#32852;&#37030;&#32858;&#21512;&#26041;&#27861;FLGNN&#30340;&#26377;&#25928;&#24615;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20445;&#25252;FLGNN&#30340;&#38544;&#31169;&#23433;&#20840;&#65292;&#26412;&#25991;&#35774;&#35745;&#20102;&#25104;&#21592;&#25512;&#29702;&#25915;&#20987;&#23454;&#39564;&#21644;&#24046;&#20998;&#38544;&#31169;&#38450;&#24481;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16004v1 Announce Type: cross  Abstract: Over the past few years, federated learning has become widely used in various classical machine learning fields because of its collaborative ability to train data from multiple sources without compromising privacy. However, in the area of graph neural networks, the nodes and network structures of graphs held by clients are different in many practical applications, and the aggregation method that directly shares model gradients cannot be directly applied to this scenario. Therefore, this work proposes a federated aggregation method FLGNN applied to various graph federation scenarios and investigates the aggregation effect of parameter sharing at each layer of the graph neural network model. The effectiveness of the federated aggregation method FLGNN is verified by experiments on real datasets. Additionally, for the privacy security of FLGNN, this paper designs membership inference attack experiments and differential privacy defense expe
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20803;&#34920;&#31034;&#23884;&#20837;(DRE)&#26694;&#26550;&#65292;&#29992;&#20110;&#32456;&#36523;&#20154;&#21592;&#20877;&#35782;&#21035;(LReID)&#65292;&#21487;&#20197;&#22312;&#23398;&#20064;&#26032;&#20449;&#24687;&#30340;&#21516;&#26102;&#26377;&#25928;&#20445;&#30041;&#26087;&#30693;&#35782;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#32422;&#26463;&#27169;&#22359;(ACM)&#23454;&#29616;&#22810;&#20010;&#34920;&#31034;&#20043;&#38388;&#30340;&#25972;&#21512;&#21644;&#25512;&#24320;&#25805;&#20316;&#65292;&#20026;&#27599;&#20010;&#23454;&#20363;&#33719;&#21462;&#23494;&#38598;&#23884;&#20837;&#23376;&#31354;&#38388;&#65292;&#25552;&#39640;&#26377;&#38480;&#26087;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#30340;&#21305;&#37197;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.16003</link><description>&lt;p&gt;
&#22810;&#20803;&#34920;&#31034;&#23884;&#20837;&#29992;&#20110;&#32456;&#36523;&#20154;&#21592;&#20877;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Diverse Representation Embedding for Lifelong Person Re-Identification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.16003
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#20803;&#34920;&#31034;&#23884;&#20837;(DRE)&#26694;&#26550;&#65292;&#29992;&#20110;&#32456;&#36523;&#20154;&#21592;&#20877;&#35782;&#21035;(LReID)&#65292;&#21487;&#20197;&#22312;&#23398;&#20064;&#26032;&#20449;&#24687;&#30340;&#21516;&#26102;&#26377;&#25928;&#20445;&#30041;&#26087;&#30693;&#35782;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#32422;&#26463;&#27169;&#22359;(ACM)&#23454;&#29616;&#22810;&#20010;&#34920;&#31034;&#20043;&#38388;&#30340;&#25972;&#21512;&#21644;&#25512;&#24320;&#25805;&#20316;&#65292;&#20026;&#27599;&#20010;&#23454;&#20363;&#33719;&#21462;&#23494;&#38598;&#23884;&#20837;&#23376;&#31354;&#38388;&#65292;&#25552;&#39640;&#26377;&#38480;&#26087;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#30340;&#21305;&#37197;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32456;&#36523;&#20154;&#21592;&#20877;&#35782;&#21035;(LReID)&#26088;&#22312;&#19981;&#26029;&#23398;&#20064;&#36830;&#32493;&#30340;&#25968;&#25454;&#27969;&#65292;&#36328;&#22810;&#20010;&#25668;&#20687;&#22836;&#21305;&#37197;&#20010;&#20154;&#12290;LReID&#30340;&#20851;&#38190;&#25361;&#25112;&#26159;&#22312;&#22686;&#37327;&#23398;&#20064;&#26032;&#20449;&#24687;&#30340;&#21516;&#26102;&#26377;&#25928;&#20445;&#30041;&#26087;&#30693;&#35782;&#12290;&#20219;&#21153;&#32423;&#22495;&#24046;&#36317;&#21644;&#26377;&#38480;&#30340;&#26087;&#20219;&#21153;&#25968;&#25454;&#38598;&#26159;&#23548;&#33268;ReID&#20013;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#20851;&#38190;&#22240;&#32032;&#65292;&#36825;&#20123;&#22240;&#32032;&#22312;&#29616;&#26377;&#26041;&#27861;&#20013;&#34987;&#24573;&#35270;&#12290;&#20026;&#20102;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#20803;&#34920;&#31034;&#23884;&#20837;(DRE)&#26694;&#26550;&#29992;&#20110;LReID&#12290;&#25152;&#25552;&#20986;&#30340;DRE&#22522;&#20110;&#23454;&#20363;&#32423;&#21644;&#20219;&#21153;&#32423;&#24067;&#23616;&#65292;&#20445;&#30041;&#26087;&#30693;&#35782;&#21516;&#26102;&#36866;&#24212;&#26032;&#20449;&#24687;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#32422;&#26463;&#27169;&#22359;(ACM)&#26469;&#23454;&#29616;&#22810;&#20010;&#34920;&#31034;&#20043;&#38388;&#30340;&#25972;&#21512;&#21644;&#25512;&#24320;&#25805;&#20316;&#65292;&#20026;&#27599;&#20010;&#23454;&#20363;&#33719;&#21462;&#23494;&#38598;&#23884;&#20837;&#23376;&#31354;&#38388;&#65292;&#20197;&#25552;&#39640;&#26377;&#38480;&#26087;&#20219;&#21153;&#25968;&#25454;&#38598;&#19978;&#30340;&#21305;&#37197;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.16003v1 Announce Type: cross  Abstract: Lifelong Person Re-Identification (LReID) aims to continuously learn from successive data streams, matching individuals across multiple cameras. The key challenge for LReID is how to effectively preserve old knowledge while learning new information incrementally. Task-level domain gaps and limited old task datasets are key factors leading to catastrophic forgetting in ReLD, which are overlooked in existing methods. To alleviate this problem, we propose a novel Diverse Representation Embedding (DRE) framework for LReID. The proposed DRE preserves old knowledge while adapting to new information based on instance-level and task-level layout. Concretely, an Adaptive Constraint Module (ACM) is proposed to implement integration and push away operations between multiple representations, obtaining dense embedding subspace for each instance to improve matching ability on limited old task datasets. Based on the processed diverse representation, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SpoT-GCN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#24863;&#21463;&#37326;&#33258;&#36866;&#24212;&#28369;&#21160;&#31383;&#21475;&#31574;&#30053;&#26469;&#26377;&#25928;&#35782;&#21035;&#38754;&#37096;&#34920;&#24773;&#65292;&#23398;&#20064;&#38754;&#37096;&#22270;&#27169;&#24335;&#20197;&#22686;&#24378;&#24494;&#23567;&#36816;&#21160;&#29305;&#24449;&#30340;&#25552;&#21462;&#12290;</title><link>https://arxiv.org/abs/2403.15994</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Multi-Scale Spatio-Temporal Graph Convolutional Network for Facial Expression Spotting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SpoT-GCN&#65289;&#65292;&#36890;&#36807;&#24341;&#20837;&#24863;&#21463;&#37326;&#33258;&#36866;&#24212;&#28369;&#21160;&#31383;&#21475;&#31574;&#30053;&#26469;&#26377;&#25928;&#35782;&#21035;&#38754;&#37096;&#34920;&#24773;&#65292;&#23398;&#20064;&#38754;&#37096;&#22270;&#27169;&#24335;&#20197;&#22686;&#24378;&#24494;&#23567;&#36816;&#21160;&#29305;&#24449;&#30340;&#25552;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#26159;&#38754;&#37096;&#34920;&#24773;&#20998;&#26512;&#20013;&#30340;&#20851;&#38190;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#34920;&#24773;&#35782;&#21035;&#30340;&#20934;&#30830;&#24615;&#21463;&#21040;&#19981;&#30456;&#20851;&#38754;&#37096;&#36816;&#21160;&#20197;&#21450;&#38590;&#20197;&#23519;&#35273;&#24494;&#34920;&#24773;&#32454;&#24494;&#36816;&#21160;&#30340;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#30340;&#22810;&#23610;&#24230;&#26102;&#31354;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;SpoT-GCN&#65289;&#12290;&#20026;&#20102;&#25552;&#21462;&#26356;&#21152;&#31283;&#20581;&#30340;&#36816;&#21160;&#29305;&#24449;&#65292;&#25105;&#20204;&#36319;&#36394;&#38754;&#37096;&#32908;&#32905;&#30340;&#30701;&#26399;&#21644;&#38271;&#26399;&#36816;&#21160;&#65292;&#22312;&#32039;&#20945;&#30340;&#28369;&#21160;&#31383;&#21475;&#20013;&#65292;&#31383;&#21475;&#38271;&#24230;&#36866;&#24212;&#20110;&#32593;&#32476;&#30340;&#26102;&#38388;&#24863;&#21463;&#37326;&#12290;&#36825;&#31181;&#31574;&#30053;&#34987;&#31216;&#20026;&#24863;&#21463;&#37326;&#33258;&#36866;&#24212;&#28369;&#21160;&#31383;&#21475;&#31574;&#30053;&#65292;&#26377;&#25928;&#25918;&#22823;&#20102;&#36816;&#21160;&#29305;&#24449;&#65292;&#21516;&#26102;&#32531;&#35299;&#20102;&#20005;&#37325;&#22836;&#37096;&#36816;&#21160;&#30340;&#38382;&#39064;&#12290;&#24494;&#23567;&#30340;&#36816;&#21160;&#29305;&#24449;&#28982;&#21518;&#36716;&#25442;&#20026;&#38754;&#37096;&#22270;&#34920;&#31034;&#65292;&#20854;&#26102;&#31354;&#22270;&#27169;&#24335;&#30001;&#22270;&#21367;&#31215;&#32593;&#32476;&#23398;&#20064;&#12290;&#36825;&#20010;&#32593;&#32476;&#23398;&#20064;&#20102;bo&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15994v1 Announce Type: cross  Abstract: Facial expression spotting is a significant but challenging task in facial expression analysis. The accuracy of expression spotting is affected not only by irrelevant facial movements but also by the difficulty of perceiving subtle motions in micro-expressions. In this paper, we propose a Multi-Scale Spatio-Temporal Graph Convolutional Network (SpoT-GCN) for facial expression spotting. To extract more robust motion features, we track both short- and long-term motion of facial muscles in compact sliding windows whose window length adapts to the temporal receptive field of the network. This strategy, termed the receptive field adaptive sliding window strategy, effectively magnifies the motion features while alleviating the problem of severe head movement. The subtle motion features are then converted to a facial graph representation, whose spatio-temporal graph patterns are learned by a graph convolutional network. This network learns bo
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;KGML&#65289;&#32467;&#21512;&#31185;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#31185;&#23398;&#19968;&#33268;&#24615;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15989</link><description>&lt;p&gt;
&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65306;&#24403;&#21069;&#36235;&#21183;&#19982;&#26410;&#26469;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Knowledge-guided Machine Learning: Current Trends and Future Prospects
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15989
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;KGML&#65289;&#32467;&#21512;&#31185;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#31185;&#23398;&#19968;&#33268;&#24615;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#27010;&#36848;&#20102;&#31185;&#23398;&#24314;&#27169;&#65292;&#24182;&#35752;&#35770;&#20102;&#19982;&#22522;&#20110;&#36807;&#31243;&#30340;&#27169;&#22411;&#30456;&#27604;&#65292;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#22312;&#31185;&#23398;&#24314;&#27169;&#20013;&#30340;&#20114;&#34917;&#20248;&#21183;&#21644;&#21155;&#21183;&#12290;&#25991;&#31456;&#36824;&#20171;&#32461;&#20102;&#26032;&#20852;&#39046;&#22495;&#31185;&#23398;&#30693;&#35782;&#24341;&#23548;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;KGML&#65289;&#30740;&#31350;&#30340;&#24403;&#21069;&#29366;&#24577;&#65292;&#26088;&#22312;&#21033;&#29992;&#31185;&#23398;&#30693;&#35782;&#21644;&#25968;&#25454;&#22312;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#20013;&#23454;&#29616;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12289;&#31185;&#23398;&#19968;&#33268;&#24615;&#21644;&#32467;&#26524;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#20174;&#20351;&#29992;&#30340;&#31185;&#23398;&#30693;&#35782;&#31867;&#22411;&#12289;&#25506;&#35752;&#30340;&#30693;&#35782;-ML&#38598;&#25104;&#24418;&#24335;&#20197;&#21450;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#25972;&#21512;&#31185;&#23398;&#30693;&#35782;&#30340;&#26041;&#27861;&#31561;&#26041;&#38754;&#35752;&#35770;&#20102;KGML&#30740;&#31350;&#30340;&#19981;&#21516;&#26041;&#38754;&#12290;&#25105;&#20204;&#36824;&#35752;&#35770;&#20102;&#22312;&#29615;&#22659;&#31185;&#23398;&#20013;&#21457;&#23637;&#30340;KGML&#26041;&#27861;&#30340;&#19968;&#20123;&#24120;&#35265;&#29992;&#20363;&#31867;&#21035;&#65292;&#20197;&#27599;&#20010;&#31867;&#21035;&#20013;&#30340;&#23454;&#20363;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15989v1 Announce Type: cross  Abstract: This paper presents an overview of scientific modeling and discusses the complementary strengths and weaknesses of ML methods for scientific modeling in comparison to process-based models. It also provides an introduction to the current state of research in the emerging field of scientific knowledge-guided machine learning (KGML) that aims to use both scientific knowledge and data in ML frameworks to achieve better generalizability, scientific consistency, and explainability of results. We discuss different facets of KGML research in terms of the type of scientific knowledge used, the form of knowledge-ML integration explored, and the method for incorporating scientific knowledge in ML. We also discuss some of the common categories of use cases in environmental sciences where KGML methods are being developed, using illustrative examples in each category.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#8220;&#21452;&#27969;&#20551;&#35774;&#8221;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#20998;&#20026;&#33145;&#20391;&#27969;&#21644;&#32972;&#20391;&#27969;&#20004;&#37096;&#20998;&#65292;&#20197;&#23454;&#29616;&#23545;&#28966;&#21644;&#22788;&#29702;&#22270;&#20687;patch&#30340;&#24207;&#21015;&#12290;</title><link>https://arxiv.org/abs/2403.15977</link><description>&lt;p&gt;
&#26397;&#21521;&#22522;&#20110;&#21452;&#27969;&#30524;&#24213;&#32858;&#28966;&#30340;&#20027;&#21160;&#35270;&#35273;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Two-Stream Foveation-based Active Vision Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15977
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#8220;&#21452;&#27969;&#20551;&#35774;&#8221;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#27169;&#25311;&#20154;&#31867;&#35270;&#35273;&#31995;&#32479;&#30340;&#24037;&#20316;&#21407;&#29702;&#65292;&#20998;&#20026;&#33145;&#20391;&#27969;&#21644;&#32972;&#20391;&#27969;&#20004;&#37096;&#20998;&#65292;&#20197;&#23454;&#29616;&#23545;&#28966;&#21644;&#22788;&#29702;&#22270;&#20687;patch&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#26426;&#22120;&#24863;&#30693;&#26694;&#26550;&#20197;&#19968;&#27425;&#24615;&#26041;&#24335;&#22788;&#29702;&#25972;&#20010;&#36755;&#20837;&#65292;&#20197;&#25552;&#20379;&#8220;&#34987;&#35266;&#23519;&#21040;&#30340;&#29289;&#20307;&#26159;&#20160;&#20040;&#8221;&#21644;&#8220;&#23427;&#20301;&#20110;&#21738;&#37324;&#8221;&#36825;&#20004;&#20010;&#38382;&#39064;&#30340;&#31572;&#26696;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#31070;&#32463;&#31185;&#23398;&#20013;&#30340;&#8220;&#21452;&#27969;&#20551;&#35774;&#8221;&#35299;&#37322;&#20102;&#20154;&#31867;&#35270;&#35273;&#30382;&#23618;&#20013;&#30340;&#31070;&#32463;&#22788;&#29702;&#65292;&#34920;&#26126;&#20854;&#20316;&#20026;&#19968;&#20010;&#21033;&#29992;&#22823;&#33041;&#30340;&#20004;&#20010;&#19981;&#21516;&#21306;&#22495;&#26469;&#22238;&#31572;&#8220;&#26159;&#20160;&#20040;&#8221;&#21644;&#8220;&#22312;&#21738;&#37324;&#8221;&#30340;&#35270;&#35273;&#31995;&#32479;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21463;&#8220;&#21452;&#27969;&#20551;&#35774;&#8221;&#21551;&#21457;&#30340;&#26426;&#22120;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#25506;&#35752;&#20102;&#23427;&#25152;&#24102;&#26469;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#27169;&#25311;&#20102;&#20197;&#19979;&#26426;&#21046;&#65306;1&#65289;&#33145;&#20391;&#65288;&#26159;&#20160;&#20040;&#65289;&#27969;&#32858;&#28966;&#20110;&#30524;&#29699;&#65288;&#30524;&#24213;&#65289;&#30340;&#35270;&#37326;&#37096;&#20998;&#65292;2&#65289;&#32972;&#20391;&#65288;&#22312;&#21738;&#37324;&#65289;&#27969;&#25552;&#20379;&#35270;&#35273;&#24341;&#23548;&#65292;3&#65289;&#20004;&#20010;&#27969;&#30340;&#36845;&#20195;&#22788;&#29702;&#20197;&#26657;&#20934;&#35270;&#35273;&#28966;&#28857;&#24182;&#22788;&#29702;&#19968;&#31995;&#21015;&#32858;&#28966;&#30340;&#22270;&#20687;&#22359;&#12290;&#35813;&#26694;&#26550;&#30340;&#35757;&#32451;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15977v1 Announce Type: cross  Abstract: Deep neural network (DNN) based machine perception frameworks process the entire input in a one-shot manner to provide answers to both "what object is being observed" and "where it is located". In contrast, the "two-stream hypothesis" from neuroscience explains the neural processing in the human visual cortex as an active vision system that utilizes two separate regions of the brain to answer the what and the where questions. In this work, we propose a machine learning framework inspired by the "two-stream hypothesis" and explore the potential benefits that it offers. Specifically, the proposed framework models the following mechanisms: 1) ventral (what) stream focusing on the input regions perceived by the fovea part of an eye (foveation), 2) dorsal (where) stream providing visual guidance, and 3) iterative processing of the two streams to calibrate visual focus and process the sequence of focused image patches. The training of the pr
&lt;/p&gt;</description></item><item><title>CBGT-Net&#26159;&#19968;&#31181;&#21463;CBGT&#22238;&#36335;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#31215;&#32047;&#36275;&#22815;&#30340;&#35777;&#25454;&#21518;&#25165;&#23545;&#27969;&#25968;&#25454;&#20135;&#29983;&#20998;&#31867;&#20915;&#31574;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15974</link><description>&lt;p&gt;
CBGT-Net: &#19968;&#31181;&#29992;&#20110;&#23545;&#27969;&#25968;&#25454;&#36827;&#34892;&#31283;&#20581;&#20998;&#31867;&#30340;&#31867;&#33041;&#27169;&#20223;&#26550;&#26500;
&lt;/p&gt;
&lt;p&gt;
CBGT-Net: A Neuromimetic Architecture for Robust Classification of Streaming Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15974
&lt;/p&gt;
&lt;p&gt;
CBGT-Net&#26159;&#19968;&#31181;&#21463;CBGT&#22238;&#36335;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#36890;&#36807;&#31215;&#32047;&#36275;&#22815;&#30340;&#35777;&#25454;&#21518;&#25165;&#23545;&#27969;&#25968;&#25454;&#20135;&#29983;&#20998;&#31867;&#20915;&#31574;&#65292;&#22312;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25551;&#36848;&#20102;CBGT-Net&#65292;&#36825;&#26159;&#19968;&#31181;&#21463;&#21754;&#20083;&#21160;&#29289;&#22823;&#33041;&#20013;&#30382;&#36136;-&#22522;&#24213;&#33410;-&#19992;&#33041;&#65288;CBGT&#65289;&#22238;&#36335;&#21551;&#21457;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;&#19982;&#20256;&#32479;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#19981;&#21516;&#65292;CBGT-Net&#23398;&#20064;&#22312;&#35266;&#23519;&#21040;&#30340;&#25968;&#25454;&#27969;&#20013;&#36798;&#21040;&#36275;&#22815;&#35777;&#25454;&#26631;&#20934;&#21518;&#20135;&#29983;&#36755;&#20986;&#12290;&#23545;&#20110;&#27599;&#27425;&#35266;&#23519;&#65292;CBGT-Net&#29983;&#25104;&#19968;&#20010;&#30690;&#37327;&#65292;&#26126;&#30830;&#34920;&#31034;&#35266;&#23519;&#20026;&#27599;&#20010;&#28508;&#22312;&#20915;&#23450;&#25552;&#20379;&#30340;&#35777;&#25454;&#37327;&#65292;&#38543;&#26102;&#38388;&#32047;&#31215;&#35777;&#25454;&#65292;&#24182;&#22312;&#32047;&#31215;&#35777;&#25454;&#36229;&#36807;&#39044;&#23450;&#20041;&#38408;&#20540;&#26102;&#29983;&#25104;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#22270;&#20687;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#65292;&#22312;&#36825;&#20123;&#20219;&#21153;&#20013;&#27169;&#22411;&#38656;&#35201;&#26681;&#25454;&#20174;&#22270;&#20687;&#20013;&#25552;&#21462;&#30340;&#23567;&#34917;&#19969;&#27969;&#26469;&#39044;&#27979;&#22270;&#20687;&#31867;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;CBGT-Net&#30456;&#27604;&#20256;&#32479;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#26041;&#38754;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15974v1 Announce Type: cross  Abstract: This paper describes CBGT-Net, a neural network model inspired by the cortico-basal ganglia-thalamic (CBGT) circuits found in mammalian brains. Unlike traditional neural network models, which either generate an output for each provided input, or an output after a fixed sequence of inputs, the CBGT-Net learns to produce an output after a sufficient criteria for evidence is achieved from a stream of observed data. For each observation, the CBGT-Net generates a vector that explicitly represents the amount of evidence the observation provides for each potential decision, accumulates the evidence over time, and generates a decision when the accumulated evidence exceeds a pre-defined threshold. We evaluate the proposed model on two image classification tasks, where models need to predict image categories based on a stream of small patches extracted from the image. We show that the CBGT-Net provides improved accuracy and robustness compared t
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26356;&#23569;&#30340;&#29305;&#24449;&#36827;&#34892;&#38382;&#39064;&#36172;&#21338;&#26816;&#27979;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23558;&#29305;&#24449;&#20174;102&#20010;&#20943;&#23569;&#21040;5&#20010;&#12290;</title><link>https://arxiv.org/abs/2403.15962</link><description>&lt;p&gt;
&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#26816;&#27979;&#38382;&#39064;&#36172;&#21338;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Detection of Problem Gambling with Less Features Using Machine Learning Methods
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15962
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26356;&#23569;&#30340;&#29305;&#24449;&#36827;&#34892;&#38382;&#39064;&#36172;&#21338;&#26816;&#27979;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#22312;&#19981;&#24433;&#21709;&#24615;&#33021;&#30340;&#24773;&#20917;&#19979;&#21487;&#20197;&#23558;&#29305;&#24449;&#20174;102&#20010;&#20943;&#23569;&#21040;5&#20010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36172;&#21338;&#30740;&#31350;&#20013;&#65292;&#22522;&#20110;&#23545;&#29992;&#25143;&#27599;&#26085;&#34892;&#20026;&#25968;&#25454;&#30340;&#30417;&#25511;&#32780;&#25191;&#34892;&#20998;&#26512;&#29305;&#24449;&#12290;&#22312;&#25191;&#34892;&#38382;&#39064;&#36172;&#21338;&#26816;&#27979;&#26102;&#65292;&#29616;&#26377;&#25968;&#25454;&#38598;&#20026;&#24314;&#31435;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#27169;&#22411;&#25552;&#20379;&#20102;&#30456;&#23545;&#20016;&#23500;&#30340;&#20998;&#26512;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#32771;&#34385;&#21040;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25910;&#38598;&#20998;&#26512;&#29305;&#24449;&#30340;&#22797;&#26434;&#24615;&#21644;&#25104;&#26412;&#65292;&#20351;&#29992;&#26356;&#23569;&#30340;&#29305;&#24449;&#36827;&#34892;&#31934;&#30830;&#26816;&#27979;&#23558;&#26497;&#22823;&#20943;&#23569;&#25968;&#25454;&#25910;&#38598;&#25104;&#26412;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476; PGN4&#65292;&#22312;&#20351;&#29992;&#26377;&#38480;&#30340;&#20998;&#26512;&#29305;&#24449;&#26102;&#34920;&#29616;&#33391;&#22909;&#12290;&#36890;&#36807;&#23545;&#20004;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#23454;&#39564;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#23558;102&#20010;&#29305;&#24449;&#20943;&#23569;&#21040;5&#20010;&#29305;&#24449;&#26102;&#65292;PGN4&#20165;&#36973;&#36935;&#20102;&#36731;&#24494;&#30340;&#24615;&#33021;&#19979;&#38477;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#20004;&#20010;&#25968;&#25454;&#38598;&#20013;&#25490;&#21517;&#21069;5&#30340;&#29305;&#24449;&#30340;&#20849;&#21516;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15962v1 Announce Type: cross  Abstract: Analytic features in gambling study are performed based on the amount of data monitoring on user daily actions. While performing the detection of problem gambling, existing datasets provide relatively rich analytic features for building machine learning based model. However, considering the complexity and cost of collecting the analytic features in real applications, conducting precise detection with less features will tremendously reduce the cost of data collection. In this study, we propose a deep neural networks PGN4 that performs well when using limited analytic features. Through the experiment on two datasets, we discover that PGN4 only experiences a mere performance drop when cutting 102 features to 5 features. Besides, we find the commonality within the top 5 features from two datasets.
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;SAT&#32534;&#30721;&#30340;&#20559;&#24207;&#27169;&#22411;&#29992;&#20110;&#22270;&#30528;&#33394;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#23545;&#24102;&#23485;&#30528;&#33394;&#38382;&#39064;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;</title><link>https://arxiv.org/abs/2403.15961</link><description>&lt;p&gt;
SAT&#32534;&#30721;&#30340;&#20559;&#24207;&#27169;&#22411;&#29992;&#20110;&#22270;&#30528;&#33394;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
SAT Encoding of Partial Ordering Models for Graph Coloring Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15961
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#26032;&#30340;SAT&#32534;&#30721;&#30340;&#20559;&#24207;&#27169;&#22411;&#29992;&#20110;&#22270;&#30528;&#33394;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;&#22312;&#19968;&#20123;&#24773;&#20917;&#19979;&#36229;&#36234;&#20102;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#23545;&#24102;&#23485;&#30528;&#33394;&#38382;&#39064;&#36827;&#34892;&#20102;&#29702;&#35770;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#20559;&#24207;&#30340;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#65288;ILP&#65289;&#27169;&#22411;&#30340;&#22270;&#30528;&#33394;&#38382;&#39064;&#65288;GCP&#65289;&#21644;&#24102;&#23485;&#30528;&#33394;&#38382;&#39064;&#65288;BCP&#65289;&#30340;&#26032;SAT&#32534;&#30721;&#12290; GCP&#35201;&#27714;&#32473;&#23450;&#22270;&#30340;&#39030;&#28857;&#20998;&#37197;&#26368;&#23569;&#25968;&#37327;&#30340;&#39068;&#33394;&#65292;&#20197;&#20415;&#27599;&#20004;&#20010;&#30456;&#37051;&#30340;&#39030;&#28857;&#24471;&#21040;&#19981;&#21516;&#30340;&#39068;&#33394;&#12290; BCP&#26159;&#19968;&#20010;&#27867;&#21270;&#38382;&#39064;&#65292;&#20854;&#20013;&#27599;&#26465;&#36793;&#37117;&#26377;&#19968;&#20010;&#26435;&#37325;&#65292;&#35201;&#27714;&#20998;&#37197;&#30340;&#39068;&#33394;&#20043;&#38388;&#26377;&#26368;&#23567;&#30340;&#8220;&#36317;&#31163;&#8221;&#65292;&#30446;&#26631;&#26159;&#26368;&#23567;&#21270;&#20351;&#29992;&#30340;&#8220;&#26368;&#22823;&#8221;&#39068;&#33394;&#12290; &#23545;&#20110;&#34987;&#24191;&#27867;&#30740;&#31350;&#30340;GCP&#65292;&#25105;&#20204;&#22312;DIMACS&#22522;&#20934;&#38598;&#19978;&#23454;&#39564;&#27604;&#36739;&#20102;&#25105;&#20204;&#26032;&#30340;SAT&#32534;&#30721;&#19982;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290; &#25105;&#20204;&#30340;&#35780;&#20272;&#35777;&#23454;&#65292;&#36825;&#31181;SAT&#32534;&#30721;&#23545;&#20110;&#31232;&#30095;&#22270;&#26159;&#26377;&#25928;&#30340;&#65292;&#24182;&#19988;&#29978;&#33267;&#22312;&#19968;&#20123;DIMACS&#31034;&#20363;&#19978;&#32988;&#36807;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290; &#23545;&#20110;BCP&#65292;&#25105;&#20204;&#30340;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#22522;&#20110;&#20559;&#24207;&#30340;SAT&#21644;ILP&#20844;&#24335;&#30340;&#22823;&#23567;&#22312;&#28176;&#36817;&#24847;&#20041;&#19979;&#23567;&#20110;&#32463;&#20856;&#30340;&#35299;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15961v1 Announce Type: new  Abstract: In this paper, we suggest new SAT encodings of the partial-ordering based ILP model for the graph coloring problem (GCP) and the bandwidth coloring problem (BCP). The GCP asks for the minimum number of colors that can be assigned to the vertices of a given graph such that each two adjacent vertices get different colors. The BCP is a generalization, where each edge has a weight that enforces a minimal "distance" between the assigned colors, and the goal is to minimize the "largest" color used. For the widely studied GCP, we experimentally compare our new SAT encoding to the state-of-the-art approaches on the DIMACS benchmark set. Our evaluation confirms that this SAT encoding is effective for sparse graphs and even outperforms the state-of-the-art on some DIMACS instances. For the BCP, our theoretical analysis shows that the partial-ordering based SAT and ILP formulations have an asymptotically smaller size than that of the classical assi
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36879;&#26126;&#27700;&#21360;&#26816;&#27979;&#30340;&#40657;&#30418;&#26041;&#27861;WMD&#65292;&#22312;&#26080;&#27880;&#37322;&#35774;&#32622;&#19979;&#65292;&#21033;&#29992;&#24178;&#20928;&#26080;&#27700;&#21360;&#25968;&#25454;&#38598;&#26816;&#27979;&#20219;&#24847;&#27700;&#21360;&#65292;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.15955</link><description>&lt;p&gt;
&#22312;&#24178;&#33609;&#22534;&#20013;&#23547;&#25214;&#38024;: &#19968;&#31181;&#36879;&#26126;&#27700;&#21360;&#26816;&#27979;&#30340;&#40657;&#30418;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Finding needles in a haystack: A Black-Box Approach to Invisible Watermark Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15955
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36879;&#26126;&#27700;&#21360;&#26816;&#27979;&#30340;&#40657;&#30418;&#26041;&#27861;WMD&#65292;&#22312;&#26080;&#27880;&#37322;&#35774;&#32622;&#19979;&#65292;&#21033;&#29992;&#24178;&#20928;&#26080;&#27700;&#21360;&#25968;&#25454;&#38598;&#26816;&#27979;&#20219;&#24847;&#27700;&#21360;&#65292;&#25928;&#26524;&#26174;&#33879;&#20248;&#20110;&#20256;&#32479;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;WaterMark Detection&#65288;WMD&#65289;&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#22312;&#40657;&#30418;&#21644;&#26080;&#27880;&#37322;&#35774;&#32622;&#19979;&#36827;&#34892;&#36879;&#26126;&#27700;&#21360;&#26816;&#27979;&#30340;&#26041;&#27861;&#12290;WMD&#33021;&#22815;&#21033;&#29992;&#19968;&#20010;&#24178;&#20928;&#30340;&#26080;&#27700;&#21360;&#25968;&#25454;&#38598;&#20316;&#20026;&#21442;&#32771;&#65292;&#22312;&#19981;&#20381;&#36182;&#29305;&#23450;&#35299;&#30721;&#26041;&#27861;&#25110;&#23545;&#27700;&#21360;&#25216;&#26415;&#30340;&#20107;&#20808;&#20102;&#35299;&#30340;&#24773;&#20917;&#19979;&#65292;&#26816;&#27979;&#32473;&#23450;&#21442;&#32771;&#25968;&#25454;&#38598;&#20013;&#30340;&#20219;&#24847;&#27700;&#21360;&#12290;&#25105;&#20204;&#20351;&#29992;&#20559;&#31227;&#23398;&#20064;&#30340;&#22522;&#30784;&#24320;&#21457;&#20102;WMD&#65292;&#24178;&#20928;&#30340;&#26080;&#27700;&#21360;&#25968;&#25454;&#38598;&#20351;&#25105;&#20204;&#33021;&#22815;&#20165;&#20998;&#31163;&#20986;&#21442;&#32771;&#25968;&#25454;&#38598;&#20013;&#24102;&#27700;&#21360;&#26679;&#26412;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#35777;&#26126;&#20102;WMD&#30340;&#26377;&#25928;&#24615;&#65292;&#26126;&#26174;&#20248;&#20110;&#20165;&#20135;&#29983;&#32422;0.5&#30340;AUC&#24471;&#20998;&#30340;&#31616;&#21333;&#26816;&#27979;&#26041;&#27861;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;WMD&#22312;&#22823;&#22810;&#25968;&#21333;&#27700;&#21360;&#25968;&#25454;&#38598;&#20013;&#25345;&#32493;&#33719;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26816;&#27979;AUC&#24471;&#20998;&#65292;&#36229;&#36807;0.9&#65292;&#24182;&#22312;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#22810;&#27700;&#21360;&#22330;&#26223;&#20013;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#21644;&#27700;&#21360;&#26041;&#27861;&#20013;&#36229;&#36807;0.7&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15955v1 Announce Type: cross  Abstract: In this paper, we propose WaterMark Detection (WMD), the first invisible watermark detection method under a black-box and annotation-free setting. WMD is capable of detecting arbitrary watermarks within a given reference dataset using a clean non-watermarked dataset as a reference, without relying on specific decoding methods or prior knowledge of the watermarking techniques. We develop WMD using foundations of offset learning, where a clean non-watermarked dataset enables us to isolate the influence of only watermarked samples in the reference dataset. Our comprehensive evaluations demonstrate the effectiveness of WMD, significantly outperforming naive detection methods, which only yield AUC scores around 0.5. In contrast, WMD consistently achieves impressive detection AUC scores, surpassing 0.9 in most single-watermark datasets and exceeding 0.7 in more challenging multi-watermark scenarios across diverse datasets and watermarking me
&lt;/p&gt;</description></item><item><title>&#28145;&#20837;&#25506;&#31350;&#25439;&#22833;&#21387;&#32553;&#23545;&#27169;&#22411;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#29616;&#20195;&#25439;&#22833;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#35777;&#36136;&#37327;&#25439;&#22833;&#22312;1%&#20197;&#19979;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;50-100&#20493;&#30340;&#21387;&#32553;&#27604;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2403.15953</link><description>&lt;p&gt;
&#20102;&#35299;&#25439;&#22833;&#21387;&#32553;&#22312;&#26426;&#22120;&#23398;&#20064;&#35757;&#32451;&#38598;&#20013;&#30340;&#26377;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Understanding The Effectiveness of Lossy Compression in Machine Learning Training Sets
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15953
&lt;/p&gt;
&lt;p&gt;
&#28145;&#20837;&#25506;&#31350;&#25439;&#22833;&#21387;&#32553;&#23545;&#27169;&#22411;&#36136;&#37327;&#30340;&#24433;&#21709;&#65292;&#21457;&#29616;&#29616;&#20195;&#25439;&#22833;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#22312;&#20445;&#35777;&#36136;&#37327;&#25439;&#22833;&#22312;1%&#20197;&#19979;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;50-100&#20493;&#30340;&#21387;&#32553;&#27604;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#21644;&#20154;&#24037;&#26234;&#33021;&#65288;ML/AI&#65289;&#25216;&#26415;&#22312;&#39640;&#24615;&#33021;&#35745;&#31639;&#65288;HPC&#65289;&#20013;&#21464;&#24471;&#26085;&#30410;&#26222;&#21450;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#28014;&#28857;&#25968;&#25454;&#29992;&#20110;&#35757;&#32451;&#21644;&#39564;&#35777;&#65292;&#38656;&#35201;&#26041;&#27861;&#22312;&#24191;&#22495;&#32593;&#32476;&#65288;WAN&#65289;&#19978;&#20849;&#20139;&#25968;&#25454;&#25110;&#23558;&#20854;&#20174;&#36793;&#32536;&#35774;&#22791;&#20256;&#36755;&#21040;&#25968;&#25454;&#20013;&#24515;&#12290;&#25968;&#25454;&#21387;&#32553;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20294;&#38656;&#35201;&#28145;&#20837;&#20102;&#35299;&#25439;&#22833;&#21387;&#32553;&#22914;&#20309;&#24433;&#21709;&#27169;&#22411;&#36136;&#37327;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#35780;&#20272;ML/AI&#25968;&#25454;&#20943;&#23569;&#25216;&#26415;&#30340;&#31995;&#32479;&#26041;&#27861;&#65292;&#24182;&#20351;&#29992;&#23427;&#23545;17&#31181;&#25968;&#25454;&#20943;&#23569;&#26041;&#27861;&#22312;7&#20010;ML/AI&#24212;&#29992;&#31243;&#24207;&#19978;&#36827;&#34892;&#20102;&#38750;&#24120;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#34920;&#26126;&#29616;&#20195;&#30340;&#25439;&#22833;&#21387;&#32553;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;50-100&#20493;&#30340;&#21387;&#32553;&#27604;&#25913;&#21892;&#65292;&#36136;&#37327;&#25439;&#22833;&#22312;1%&#20197;&#19979;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20851;&#38190;&#35265;&#35299;&#65292;&#25351;&#23548;&#26410;&#26469;&#30340;&#20351;&#29992;&#21644;de
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15953v1 Announce Type: cross  Abstract: Learning and Artificial Intelligence (ML/AI) techniques have become increasingly prevalent in high performance computing (HPC). However, these methods depend on vast volumes of floating point data for training and validation which need methods to share the data on a wide area network (WAN) or to transfer it from edge devices to data centers. Data compression can be a solution to these problems, but an in-depth understanding of how lossy compression affects model quality is needed. Prior work largely considers a single application or compression method. We designed a systematic methodology for evaluating data reduction techniques for ML/AI, and we use it to perform a very comprehensive evaluation with 17 data reduction methods on 7 ML/AI applications to show modern lossy compression methods can achieve a 50-100x compression ratio improvement for a 1% or less loss in quality. We identify critical insights that guide the future use and de
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#39640;&#36136;&#37327;&#35828;&#21809;&#22836;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#39044;&#35757;&#32451;&#27169;&#22359;&#12290;</title><link>https://arxiv.org/abs/2403.15944</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#36229;&#20998;&#36776;&#29575;&#29992;&#20110;&#19968;&#25293;&#21363;&#21512;&#30340;&#35828;&#21809;&#22836;&#35270;&#39057;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Adaptive Super Resolution For One-Shot Talking-Head Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15944
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#39640;&#36136;&#37327;&#35828;&#21809;&#22836;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#65292;&#26080;&#38656;&#39069;&#22806;&#30340;&#39044;&#35757;&#32451;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15944v1 &#31867;&#22411;&#65306;&#20132;&#21449; &#25688;&#35201;&#65306;&#19968;&#25293;&#21363;&#21512;&#30340;&#35828;&#21809;&#22836;&#35270;&#39057;&#29983;&#25104;&#23398;&#20064;&#22914;&#20309;&#22312;&#30456;&#21516;&#25110;&#19981;&#21516;&#36523;&#20221;&#35270;&#39057;&#30340;&#25805;&#25511;&#19979;&#21512;&#25104;&#19968;&#20010;&#20197;&#28304;&#32918;&#20687;&#22270;&#20687;&#20026;&#22522;&#30784;&#30340;&#35828;&#21809;&#22836;&#35270;&#39057;&#12290;&#36890;&#24120;&#36825;&#20123;&#26041;&#27861;&#36890;&#36807;&#38597;&#21487;&#27604;&#30697;&#38453;&#25110;&#33080;&#37096;&#22270;&#20687;&#21464;&#24418;&#26469;&#36827;&#34892;&#22522;&#20110;&#24179;&#38754;&#30340;&#20687;&#32032;&#21464;&#25442;&#65292;&#20197;&#29983;&#25104;&#26032;&#30340;&#23039;&#21183;&#12290;&#20351;&#29992;&#21333;&#19968;&#22270;&#20687;&#28304;&#21644;&#20687;&#32032;&#20301;&#31227;&#30340;&#32422;&#26463;&#36890;&#24120;&#20250;&#25439;&#23475;&#21512;&#25104;&#22270;&#20687;&#30340;&#28165;&#26224;&#24230;&#12290;&#19968;&#20123;&#26041;&#27861;&#23581;&#35797;&#36890;&#36807;&#24341;&#20837;&#39069;&#22806;&#30340;&#36229;&#20998;&#36776;&#29575;&#27169;&#22359;&#26469;&#25552;&#39640;&#21512;&#25104;&#35270;&#39057;&#30340;&#36136;&#37327;&#65292;&#20294;&#36825;&#26080;&#30097;&#20250;&#22686;&#21152;&#35745;&#31639;&#24320;&#38144;&#24182;&#30772;&#22351;&#21407;&#22987;&#25968;&#25454;&#20998;&#24067;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#39640;&#36136;&#37327;&#35828;&#21809;&#22836;&#35270;&#39057;&#29983;&#25104;&#26041;&#27861;&#65292;&#21487;&#20197;&#21512;&#25104;&#39640;&#20998;&#36776;&#29575;&#35270;&#39057;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#39044;&#35757;&#32451;&#27169;&#22359;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#21463;&#29616;&#26377;&#36229;&#20998;&#36776;&#29575;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#23545;&#19968;&#25293;&#21363;&#21512;&#30340;&#28304;&#22270;&#20687;&#36827;&#34892;&#19979;&#37319;&#26679;&#65292;&#28982;&#21518;&#33258;&#36866;&#24212;&#37325;&#24314;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15944v1 Announce Type: cross  Abstract: The one-shot talking-head generation learns to synthesize a talking-head video with one source portrait image under the driving of same or different identity video. Usually these methods require plane-based pixel transformations via Jacobin matrices or facial image warps for novel poses generation. The constraints of using a single image source and pixel displacements often compromise the clarity of the synthesized images. Some methods try to improve the quality of synthesized videos by introducing additional super-resolution modules, but this will undoubtedly increase computational consumption and destroy the original data distribution. In this work, we propose an adaptive high-quality talking-head video generation method, which synthesizes high-resolution video without additional pre-trained modules. Specifically, inspired by existing super-resolution methods, we down-sample the one-shot source image, and then adaptively reconstruct 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2403.15941</link><description>&lt;p&gt;
&#25506;&#32034;&#30452;&#21040;&#33258;&#20449;: &#38754;&#21521;&#20855;&#36523;&#38382;&#31572;&#30340;&#39640;&#25928;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Explore until Confident: Efficient Exploration for Embodied Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15941
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#65292;&#32467;&#21512;&#28145;&#24230;&#20449;&#24687;&#21644;&#35270;&#35273;&#25552;&#31034;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#35299;&#20915;&#20855;&#36523;&#38382;&#31572;&#20013;&#30340;&#26377;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#38382;&#39064;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#20855;&#36523;&#38382;&#31572;&#65288;EQA&#65289;&#30340;&#38382;&#39064;&#65292;&#36825;&#25351;&#30340;&#26159;&#22312;&#38656;&#35201;&#20027;&#21160;&#25506;&#32034;&#29615;&#22659;&#20197;&#25910;&#38598;&#20449;&#24687;&#30452;&#21040;&#23545;&#38382;&#39064;&#30340;&#31572;&#26696;&#26377;&#33258;&#20449;&#30340;&#20855;&#36523;&#20195;&#29702;&#65292;&#20363;&#22914;&#26426;&#22120;&#20154;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#30340;&#24378;&#22823;&#35821;&#20041;&#25512;&#29702;&#33021;&#21147;&#26469;&#39640;&#25928;&#25506;&#32034;&#21644;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#22312;EQA&#20013;&#20351;&#29992;VLMs&#26102;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#25361;&#25112;&#65306;&#23427;&#20204;&#27809;&#26377;&#20869;&#37096;&#35760;&#24518;&#23558;&#22330;&#26223;&#26144;&#23556;&#20197;&#20415;&#35268;&#21010;&#22914;&#20309;&#38543;&#26102;&#38388;&#25506;&#32034;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#32622;&#20449;&#24230;&#21487;&#33021;&#34987;&#38169;&#35823;&#26657;&#20934;&#24182;&#21487;&#33021;&#23548;&#33268;&#26426;&#22120;&#20154;&#36807;&#26089;&#20572;&#27490;&#25506;&#32034;&#25110;&#36807;&#24230;&#25506;&#32034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#39318;&#20808;&#22522;&#20110;&#28145;&#24230;&#20449;&#24687;&#21644;&#36890;&#36807;&#35270;&#35273;&#25552;&#31034;VLM&#26469;&#26500;&#24314;&#22330;&#26223;&#30340;&#35821;&#20041;&#22320;&#22270;-&#21033;&#29992;&#20854;&#23545;&#22330;&#26223;&#30456;&#20851;&#21306;&#22495;&#30340;&#24191;&#27867;&#30693;&#35782;&#26469;&#36827;&#34892;&#25506;&#32034;&#12290;&#25509;&#19979;&#26469;&#65292;&#25105;&#20204;&#20351;&#29992;&#31526;&#21512;&#39044;&#27979;&#26469;&#26657;&#20934;VLM&#30340;&#32622;&#20449;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15941v1 Announce Type: cross  Abstract: We consider the problem of Embodied Question Answering (EQA), which refers to settings where an embodied agent such as a robot needs to actively explore an environment to gather information until it is confident about the answer to a question. In this work, we leverage the strong semantic reasoning capabilities of large vision-language models (VLMs) to efficiently explore and answer such questions. However, there are two main challenges when using VLMs in EQA: they do not have an internal memory for mapping the scene to be able to plan how to explore over time, and their confidence can be miscalibrated and can cause the robot to prematurely stop exploration or over-explore. We propose a method that first builds a semantic map of the scene based on depth information and via visual prompting of a VLM - leveraging its vast knowledge of relevant regions of the scene for exploration. Next, we use conformal prediction to calibrate the VLM's 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#29702;&#20195;&#24065;&#30340;&#27010;&#24565;&#65292;&#23558;&#20854;&#20316;&#20026;&#21464;&#21387;&#22120;&#30340;&#36755;&#20837;&#32452;&#20214;&#19982;&#20855;&#20307;&#22320;&#29702;&#20301;&#32622;&#32852;&#31995;&#36215;&#26469;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;&#29699;&#38754;&#22352;&#26631;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.15940</link><description>&lt;p&gt;
&#22320;&#29702;&#20195;&#24065;&#19982;&#22320;&#29702;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Geotokens and Geotransformers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15940
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#29702;&#20195;&#24065;&#30340;&#27010;&#24565;&#65292;&#23558;&#20854;&#20316;&#20026;&#21464;&#21387;&#22120;&#30340;&#36755;&#20837;&#32452;&#20214;&#19982;&#20855;&#20307;&#22320;&#29702;&#20301;&#32622;&#32852;&#31995;&#36215;&#26469;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#38024;&#23545;&#29699;&#38754;&#22352;&#26631;&#30340;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21464;&#21387;&#22120;&#26550;&#26500;&#20013;&#65292;&#20301;&#32622;&#32534;&#30721;&#20027;&#35201;&#20026;&#36755;&#20837;&#20195;&#24065;&#25552;&#20379;&#20102;&#24207;&#21015;&#30340;&#24847;&#20041;&#12290;&#21407;&#22987;&#21464;&#21387;&#22120;&#35770;&#25991;&#30340;&#26041;&#27861;&#22312;&#19968;&#33324;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#32467;&#26524;&#65292;&#20294;&#20063;&#26377;&#20102;&#26032;&#30340;&#25552;&#35758;&#65292;&#22914;&#26059;&#36716;&#20301;&#32622;&#23884;&#20837;&#65288;RoPE&#65289;&#65292;&#20197;&#36827;&#19968;&#27493;&#25913;&#36827;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#22320;&#29702;&#20195;&#24065;&#65292;&#21464;&#21387;&#22120;&#30340;&#36755;&#20837;&#32452;&#20214;&#65292;&#27599;&#19968;&#20010;&#20195;&#24065;&#19982;&#29305;&#23450;&#22320;&#36136;&#20301;&#32622;&#30456;&#36830;&#12290;&#19982;&#20856;&#22411;&#30340;&#35821;&#35328;&#24207;&#21015;&#19981;&#21516;&#65292;&#23545;&#20110;&#36825;&#20123;&#20195;&#24065;&#65292;&#39034;&#24207;&#24182;&#19981;&#20687;&#22320;&#29702;&#22352;&#26631;&#26412;&#36523;&#37027;&#26679;&#37325;&#35201;&#12290;&#20026;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#34920;&#31034;&#30456;&#23545;&#20301;&#32622;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#36317;&#31163;&#21644;&#23884;&#20837;&#31354;&#38388;&#20013;&#30340;&#36317;&#31163;&#20043;&#38388;&#20445;&#25345;&#24179;&#34913;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#20301;&#32622;&#32534;&#30721;&#26041;&#27861;&#65292;&#20511;&#37492;&#20110;RoPE&#32467;&#26500;&#20294;&#19987;&#20026;&#29699;&#38754;&#22352;&#26631;&#23450;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15940v1 Announce Type: cross  Abstract: In transformer architectures, position encoding primarily provides a sense of sequence for input tokens. While the original transformer paper's method has shown satisfactory results in general language processing tasks, there have been new proposals, such as Rotary Position Embedding (RoPE), for further improvement. This paper presents geotokens, input components for transformers, each linked to a specific geological location. Unlike typical language sequences, for these tokens, the order is not as vital as the geographical coordinates themselves. To represent the relative position in this context and to keep a balance between the real world distance and the distance in the embedding space, we design a position encoding approach drawing from the RoPE structure but tailored for spherical coordinates.
&lt;/p&gt;</description></item><item><title>LlamBERT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#26410;&#26631;&#35760;&#25968;&#25454;&#24211;&#24182;&#29992;&#20110;&#24494;&#35843;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#25104;&#26412;&#30340;&#21516;&#26102;&#30053;&#24494;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15938</link><description>&lt;p&gt;
LlamBERT&#65306;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20013;&#22823;&#35268;&#27169;&#12289;&#20302;&#25104;&#26412;&#30340;&#25968;&#25454;&#27880;&#37322;
&lt;/p&gt;
&lt;p&gt;
LlamBERT: Large-scale low-cost data annotation in NLP
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15938
&lt;/p&gt;
&lt;p&gt;
LlamBERT&#26159;&#19968;&#31181;&#21033;&#29992;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#27880;&#37322;&#26410;&#26631;&#35760;&#25968;&#25454;&#24211;&#24182;&#29992;&#20110;&#24494;&#35843;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#22312;&#38477;&#20302;&#25104;&#26412;&#30340;&#21516;&#26102;&#30053;&#24494;&#29306;&#29298;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#22914;GPT-4&#21644;Llama 2&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#12290;&#23613;&#31649;&#23427;&#20204;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#19982;&#23427;&#20204;&#30340;&#20351;&#29992;&#30456;&#20851;&#30340;&#39640;&#25104;&#26412;&#24102;&#26469;&#20102;&#25361;&#25112;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;LlamBERT&#65292;&#36825;&#26159;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#23545;&#22823;&#37327;&#26410;&#26631;&#35760;&#25968;&#25454;&#24211;&#30340;&#23567;&#23376;&#38598;&#36827;&#34892;&#27880;&#37322;&#65292;&#24182;&#23558;&#32467;&#26524;&#29992;&#20110;&#24494;&#35843;&#31867;&#20284;BERT&#21644;RoBERTa&#30340;&#21464;&#21387;&#22120;&#32534;&#30721;&#22120;&#12290;&#36825;&#19968;&#31574;&#30053;&#22312;&#20004;&#20010;&#19981;&#21516;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#65306;IMDb&#24433;&#35780;&#25968;&#25454;&#38598;&#21644;UMLS Meta-Thesaurus&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;LlamBERT&#26041;&#27861;&#22312;&#31245;&#24494;&#29306;&#29298;&#20934;&#30830;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#20379;&#20102;&#26356;&#39640;&#30340;&#25104;&#26412;&#25928;&#30410;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15938v1 Announce Type: cross  Abstract: Large Language Models (LLMs), such as GPT-4 and Llama 2, show remarkable proficiency in a wide range of natural language processing (NLP) tasks. Despite their effectiveness, the high costs associated with their use pose a challenge. We present LlamBERT, a hybrid approach that leverages LLMs to annotate a small subset of large, unlabeled databases and uses the results for fine-tuning transformer encoders like BERT and RoBERTa. This strategy is evaluated on two diverse datasets: the IMDb review dataset and the UMLS Meta-Thesaurus. Our results indicate that the LlamBERT approach slightly compromises on accuracy while offering much greater cost-effectiveness.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#37327;&#21270;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#22312;&#19981;&#21516;&#22823;&#23567;&#39046;&#22495;&#38388;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#39046;&#22495;&#22823;&#23567;&#27867;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.15933</link><description>&lt;p&gt;
&#29702;&#35299;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#20013;&#30340;&#22495;&#22823;&#23567;&#27867;&#21270;
&lt;/p&gt;
&lt;p&gt;
Understanding Domain-Size Generalization in Markov Logic Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15933
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#37327;&#21270;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#22312;&#19981;&#21516;&#22823;&#23567;&#39046;&#22495;&#38388;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#30340;&#26041;&#24335;&#26469;&#20248;&#21270;&#39046;&#22495;&#22823;&#23567;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#39532;&#23572;&#31185;&#22827;&#36923;&#36753;&#32593;&#32476;&#65288;MLNs&#65289;&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#20851;&#31995;&#32467;&#26500;&#20043;&#38388;&#30340;&#27867;&#21270;&#34892;&#20026;&#12290;&#22810;&#20010;&#30740;&#31350;&#27880;&#24847;&#21040;&#65292;&#22312;&#32473;&#23450;&#22495;&#19978;&#23398;&#20064;&#30340;MLNs&#22312;&#19981;&#21516;&#22823;&#23567;&#30340;&#22495;&#19978;&#27867;&#21270;&#24456;&#24046;&#12290;&#36825;&#31181;&#34892;&#20026;&#28304;&#20110;MLN&#22312;&#19981;&#21516;&#22495;&#22823;&#23567;&#19978;&#20351;&#29992;&#26102;&#30340;&#20869;&#37096;&#19968;&#33268;&#24615;&#32570;&#22833;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37327;&#21270;&#20102;&#36825;&#31181;&#19981;&#19968;&#33268;&#24615;&#65292;&#24182;&#23558;&#20854;&#38480;&#21046;&#22312;MLN&#21442;&#25968;&#30340;&#26041;&#24046;&#33539;&#22260;&#20869;&#12290;&#21442;&#25968;&#26041;&#24046;&#36824;&#38480;&#21046;&#20102;&#20174;&#19981;&#21516;&#22495;&#22823;&#23567;&#20013;&#21462;&#20986;&#30340;MLN&#36793;&#32536;&#20998;&#24067;&#20043;&#38388;&#30340;KL&#25955;&#24230;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#20123;&#30028;&#38480;&#23637;&#31034;&#65292;&#26368;&#22823;&#21270;&#25968;&#25454;&#23545;&#25968;&#20284;&#28982;&#21516;&#26102;&#26368;&#23567;&#21270;&#21442;&#25968;&#26041;&#24046;&#65292;&#23545;&#24212;&#20110;&#22495;&#22823;&#23567;&#27867;&#21270;&#30340;&#20004;&#20010;&#33258;&#28982;&#27010;&#24565;&#12290;&#25105;&#20204;&#30340;&#29702;&#35770;&#32467;&#26524;&#36866;&#29992;&#20110;&#25351;&#25968;&#38543;&#26426;&#22270;&#21644;&#20854;&#20182;&#22522;&#20110;&#39532;&#23572;&#31185;&#22827;&#32593;&#32476;&#30340;&#20851;&#31995;&#27169;&#22411;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#24050;&#30693;&#30340;&#35299;&#20915;&#26041;&#26696;&#20250;&#20943;&#23569;&#26041;&#24046;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15933v1 Announce Type: new  Abstract: We study the generalization behavior of Markov Logic Networks (MLNs) across relational structures of different sizes. Multiple works have noticed that MLNs learned on a given domain generalize poorly across domains of different sizes. This behavior emerges from a lack of internal consistency within an MLN when used across different domain sizes. In this paper, we quantify this inconsistency and bound it in terms of the variance of the MLN parameters. The parameter variance also bounds the KL divergence between an MLN's marginal distributions taken from different domain sizes. We use these bounds to show that maximizing the data log-likelihood while simultaneously minimizing the parameter variance corresponds to two natural notions of generalization across domain sizes. Our theoretical results apply to Exponential Random Graphs and other Markov network based relational models. Finally, we observe that solutions known to decrease the varia
&lt;/p&gt;</description></item><item><title>&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;X-Portrait&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#26102;&#38388;&#36830;&#36143;&#24615;&#30340;&#32918;&#20687;&#21160;&#30011;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#25511;&#21046;&#20449;&#21495;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#36816;&#21160;&#31934;&#24230;&#12290;</title><link>https://arxiv.org/abs/2403.15931</link><description>&lt;p&gt;
X-Portrait: &#20855;&#26377;&#20998;&#23618;&#21160;&#20316;&#27880;&#24847;&#21147;&#30340;&#34920;&#29616;&#24615;&#32918;&#20687;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
X-Portrait: Expressive Portrait Animation with Hierarchical Motion Attention
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15931
&lt;/p&gt;
&lt;p&gt;
&#36825;&#37324;&#26159;&#20013;&#25991;&#24635;&#32467;&#20986;&#30340;&#19968;&#21477;&#35805;&#35201;&#28857;: &#35813;&#35770;&#25991;&#25552;&#20986;&#20102;X-Portrait&#65292;&#19968;&#31181;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#26102;&#38388;&#36830;&#36143;&#24615;&#30340;&#32918;&#20687;&#21160;&#30011;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#21033;&#29992;&#25511;&#21046;&#20449;&#21495;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#25511;&#21046;&#65292;&#20197;&#25552;&#39640;&#36816;&#21160;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;X-Portrait&#65292;&#36825;&#26159;&#19968;&#31181;&#21019;&#26032;&#30340;&#26465;&#20214;&#25193;&#25955;&#27169;&#22411;&#65292;&#19987;&#38376;&#29992;&#20110;&#29983;&#25104;&#20855;&#26377;&#34920;&#29616;&#21147;&#21644;&#26102;&#38388;&#36830;&#36143;&#24615;&#30340;&#32918;&#20687;&#21160;&#30011;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26088;&#22312;&#22522;&#20110;&#21333;&#20010;&#32918;&#20687;&#20316;&#20026;&#22806;&#35266;&#21442;&#32771;&#65292;&#24182;&#21033;&#29992;&#26469;&#33258;&#39537;&#21160;&#35270;&#39057;&#30340;&#36816;&#21160;&#26469;&#20026;&#20854;&#28155;&#21152;&#21160;&#30011;&#65292;&#25429;&#25417;&#20855;&#26377;&#39640;&#24230;&#21160;&#24577;&#24615;&#21644;&#24494;&#22937;&#38754;&#37096;&#34920;&#24773;&#20197;&#21450;&#24191;&#27867;&#33539;&#22260;&#22836;&#37096;&#36816;&#21160;&#12290;&#22312;&#20854;&#26680;&#24515;&#37096;&#20998;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#39044;&#20808;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#20808;&#39564;&#20316;&#20026;&#28210;&#26579;&#39592;&#26550;&#65292;&#21516;&#26102;&#22312;ControlNet&#26694;&#26550;&#20869;&#36890;&#36807;&#26032;&#39062;&#30340;&#25511;&#21046;&#20449;&#21495;&#23454;&#29616;&#20102;&#32454;&#31890;&#24230;&#22836;&#37096;&#23039;&#21183;&#21644;&#34920;&#24773;&#25511;&#21046;&#12290;&#19982;&#20256;&#32479;&#30340;&#31895;&#31961;&#26174;&#24335;&#25511;&#21046;&#65288;&#22914;&#38754;&#37096;&#26631;&#24535;&#28857;&#65289;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#36816;&#21160;&#25511;&#21046;&#27169;&#22359;&#23398;&#20250;&#30452;&#25509;&#20174;&#21407;&#22987;&#39537;&#21160;RGB&#36755;&#20837;&#20013;&#35299;&#35835;&#21160;&#24577;&#12290;&#36890;&#36807;&#26377;&#25928;&#22686;&#24378;&#23545;&#30524;&#31070;&#31561;&#23567;&#23610;&#24230;&#32454;&#24494;&#24046;&#24322;&#30340;&#36816;&#21160;&#20851;&#27880;&#30340;&#22522;&#20110;&#34917;&#19969;&#30340;&#23616;&#37096;&#25511;&#21046;&#27169;&#22359;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#20102;&#36816;&#21160;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15931v1 Announce Type: cross  Abstract: We propose X-Portrait, an innovative conditional diffusion model tailored for generating expressive and temporally coherent portrait animation. Specifically, given a single portrait as appearance reference, we aim to animate it with motion derived from a driving video, capturing both highly dynamic and subtle facial expressions along with wide-range head movements. As its core, we leverage the generative prior of a pre-trained diffusion model as the rendering backbone, while achieve fine-grained head pose and expression control with novel controlling signals within the framework of ControlNet. In contrast to conventional coarse explicit controls such as facial landmarks, our motion control module is learned to interpret the dynamics directly from the original driving RGB inputs. The motion accuracy is further enhanced with a patch-based local control module that effectively enhance the motion attention to small-scale nuances like eyeba
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#30456;&#20851;&#22810;&#26234;&#33021;&#20307;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#38598;&#20013;&#24335;&#26041;&#27861;&#39640;&#25928;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#22312;&#20004;&#20010;&#38382;&#39064;&#19978;&#23637;&#29616;&#20986;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15916</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#21464;&#21387;&#22120;&#21152;&#36895;RL&#20197;&#28385;&#36275;STL&#35268;&#33539;
&lt;/p&gt;
&lt;p&gt;
Multi-agent transformer-accelerated RL for satisfaction of STL specifications
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15916
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#26102;&#38388;&#30456;&#20851;&#22810;&#26234;&#33021;&#20307;&#21464;&#21387;&#22120;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#36890;&#36807;&#38598;&#20013;&#24335;&#26041;&#27861;&#39640;&#25928;&#35299;&#20915;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#65292;&#22312;&#20004;&#20010;&#38382;&#39064;&#19978;&#23637;&#29616;&#20986;&#26126;&#26174;&#20248;&#20110;&#22522;&#32447;&#31639;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20027;&#35201;&#25361;&#25112;&#20043;&#19968;&#26159;&#38543;&#30528;&#26234;&#33021;&#20307;&#25968;&#37327;&#22686;&#21152;&#65292;&#21487;&#25193;&#23637;&#24615;&#21464;&#24046;&#12290;&#22914;&#26524;&#32771;&#34385;&#30340;&#38382;&#39064;&#26159;&#26102;&#38388;&#30456;&#20851;&#30340;&#65292;&#21017;&#36825;&#20010;&#38382;&#39064;&#20250;&#36827;&#19968;&#27493;&#24694;&#21270;&#12290;&#24403;&#20170;&#26368;&#20808;&#36827;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#36981;&#24490;&#38598;&#20013;&#24335;&#35757;&#32451;&#19982;&#20998;&#24067;&#24335;&#25191;&#34892;&#30340;&#33539;&#24335;&#65292;&#20197;&#22788;&#29702;&#21487;&#25193;&#23637;&#24615;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#36890;&#36807;&#39640;&#25928;&#22788;&#29702;&#22823;&#36755;&#20837;&#30340;&#21464;&#21387;&#22120;&#26469;&#26377;&#25928;&#35299;&#20915;&#26102;&#38388;&#30456;&#20851;&#22810;&#26234;&#33021;&#20307;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#38382;&#39064;&#19978;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20351;&#29992;&#32479;&#35745;&#24037;&#20855;&#39564;&#35777;&#20102;&#22312;&#31574;&#30053;&#19979;&#29983;&#25104;&#30340;&#36712;&#36857;&#28385;&#36275;&#20219;&#21153;&#30340;&#27010;&#29575;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#22343;&#20248;&#20110;&#25991;&#29486;&#22522;&#32447;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15916v1 Announce Type: new  Abstract: One of the main challenges in multi-agent reinforcement learning is scalability as the number of agents increases. This issue is further exacerbated if the problem considered is temporally dependent. State-of-the-art solutions today mainly follow centralized training with decentralized execution paradigm in order to handle the scalability concerns. In this paper, we propose time-dependent multi-agent transformers which can solve the temporally dependent multi-agent problem efficiently with a centralized approach via the use of transformers that proficiently handle the large input. We highlight the efficacy of this method on two problems and use tools from statistics to verify the probability that the trajectories generated under the policy satisfy the task. The experiments show that our approach has superior performance against the literature baseline algorithms in both cases.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24341;&#20837;MatchSeg&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#21644;&#32852;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#22686;&#24378;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#25903;&#25345;&#38598;&#21644;&#26597;&#35810;&#38598;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;</title><link>https://arxiv.org/abs/2403.15901</link><description>&lt;p&gt;
&#36890;&#36807;&#21442;&#32771;&#22270;&#20687;&#21305;&#37197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20998;&#21106;&#65306;MatchSeg
&lt;/p&gt;
&lt;p&gt;
MatchSeg: Towards Better Segmentation via Reference Image Matching
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15901
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24341;&#20837;MatchSeg&#26694;&#26550;&#65292;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#21644;&#32852;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#22686;&#24378;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#65292;&#26377;&#25928;&#23454;&#29616;&#20102;&#25903;&#25345;&#38598;&#21644;&#26597;&#35810;&#38598;&#20043;&#38388;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33258;&#21160;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#26041;&#27861;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#22823;&#37327;&#30340;&#26631;&#27880;&#25968;&#25454;&#38598;&#65292;&#32780;&#33719;&#21462;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#39640;&#26114;&#19988;&#32791;&#26102;&#12290;Few-shot learning&#26088;&#22312;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#23567;&#22411;&#26631;&#35760;&#25968;&#25454;&#38598;&#65288;&#31216;&#20026;&#25903;&#25345;&#38598;&#65289;&#26469;&#25351;&#23548;&#39044;&#27979;&#26032;&#30340;&#12289;&#26410;&#26631;&#35760;&#22270;&#20687;&#65288;&#31216;&#20026;&#26597;&#35810;&#38598;&#65289;&#30340;&#26631;&#31614;&#65292;&#20174;&#32780;&#20811;&#26381;&#23545;&#26631;&#27880;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#21463;&#21040;&#36825;&#19968;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;MatchSeg&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#36807;&#25112;&#30053;&#24615;&#21442;&#32771;&#22270;&#20687;&#21305;&#37197;&#22686;&#24378;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#30340;&#26032;&#26694;&#26550;&#12290;&#25105;&#20204;&#21033;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#65288;CLIP&#65289;&#22312;&#23450;&#20041;&#25903;&#25345;&#38598;&#26102;&#36873;&#25321;&#39640;&#24230;&#30456;&#20851;&#30340;&#26679;&#26412;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#32852;&#21512;&#27880;&#24847;&#21147;&#27169;&#22359;&#26469;&#21152;&#24378;&#25903;&#25345;&#21644;&#26597;&#35810;&#29305;&#24449;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#20419;&#36827;&#26356;&#26377;&#25928;&#30340;&#30693;&#35782;&#36716;&#31227;&#12290;&#25105;&#20204;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15901v1 Announce Type: new  Abstract: Recently, automated medical image segmentation methods based on deep learning have achieved great success. However, they heavily rely on large annotated datasets, which are costly and time-consuming to acquire. Few-shot learning aims to overcome the need for annotated data by using a small labeled dataset, known as a support set, to guide predicting labels for new, unlabeled images, known as the query set. Inspired by this paradigm, we introduce MatchSeg, a novel framework that enhances medical image segmentation through strategic reference image matching. We leverage contrastive language-image pre-training (CLIP) to select highly relevant samples when defining the support set. Additionally, we design a joint attention module to strengthen the interaction between support and query features, facilitating a more effective knowledge transfer between support and query sets. We validated our method across four public datasets. Experimental re
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21033;&#29992;&#38646;-shot&#25552;&#31034;&#26469;&#24341;&#20986;&#25945;&#24072;&#27169;&#22411;&#30340;&#29702;&#30001;&#65292;&#20943;&#23569;&#25163;&#24037;&#21046;&#20316;&#30340;&#23569;-shot&#31034;&#20363;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#38477;&#20302;&#25152;&#38656;&#30340;&#24635;&#35760;&#21495;&#25968;&#65292;&#36825;&#30452;&#25509;&#36716;&#21270;&#20026;&#25104;&#26412;&#33410;&#32422;&#12290;</title><link>https://arxiv.org/abs/2403.15886</link><description>&lt;p&gt;
&#21033;&#29992;&#38646;-shot&#25552;&#31034;&#23454;&#29616;&#39640;&#25928;&#35821;&#35328;&#27169;&#22411;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Leveraging Zero-Shot Prompting for Efficient Language Model Distillation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15886
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21033;&#29992;&#38646;-shot&#25552;&#31034;&#26469;&#24341;&#20986;&#25945;&#24072;&#27169;&#22411;&#30340;&#29702;&#30001;&#65292;&#20943;&#23569;&#25163;&#24037;&#21046;&#20316;&#30340;&#23569;-shot&#31034;&#20363;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#38477;&#20302;&#25152;&#38656;&#30340;&#24635;&#35760;&#21495;&#25968;&#65292;&#36825;&#30452;&#25509;&#36716;&#21270;&#20026;&#25104;&#26412;&#33410;&#32422;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23558;LLMs&#39640;&#25928;&#22320;&#33976;&#39311;&#20026;&#26356;&#23567;&#12289;&#29305;&#23450;&#20110;&#24212;&#29992;&#30340;&#27169;&#22411;&#65292;&#26174;&#33879;&#38477;&#20302;&#36816;&#33829;&#25104;&#26412;&#21644;&#20154;&#24037;&#21171;&#21160;&#12290;&#35813;&#25216;&#26415;&#21033;&#29992;LLMs&#30340;&#25512;&#29702;&#33021;&#21147;&#20026;&#26410;&#26631;&#35760;&#25968;&#25454;&#29983;&#25104;&#26631;&#31614;&#21644;&#33258;&#28982;&#35821;&#35328;&#29702;&#30001;&#65292;&#20197;&#35299;&#20915;&#23558;&#35745;&#31639;&#23494;&#38598;&#22411;LLMs&#37096;&#32626;&#21040;&#29305;&#23450;&#24212;&#29992;&#25110;&#36793;&#32536;&#35774;&#22791;&#30340;&#25361;&#25112;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#37319;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#26694;&#26550;&#65292;&#20854;&#20013;&#23398;&#29983;&#27169;&#22411;&#27169;&#20223;&#36825;&#20123;&#29702;&#30001;&#20197;&#21450;&#25945;&#24072;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#26469;&#22686;&#24378;&#24494;&#35843;&#21644;&#33976;&#39311;&#12290;&#20851;&#38190;&#36129;&#29486;&#21253;&#25324;&#21033;&#29992;&#38646;-shot&#25552;&#31034;&#26469;&#24341;&#20986;&#25945;&#24072;&#27169;&#22411;&#30340;&#29702;&#30001;&#65292;&#20943;&#23569;&#25163;&#24037;&#21046;&#20316;&#30340;&#23569;-shot&#31034;&#20363;&#30340;&#24517;&#35201;&#24615;&#65292;&#24182;&#38477;&#20302;&#25152;&#38656;&#30340;&#24635;&#35760;&#21495;&#25968;&#65292;&#36825;&#30452;&#25509;&#36716;&#21270;&#20026;&#25104;&#26412;&#33410;&#32422;&#65292;&#32771;&#34385;&#21040;&#20027;&#35201;&#25216;&#26415;&#20844;&#21496;LLM APIs&#30340;&#25353;&#35760;&#21495;&#35745;&#36153;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35843;&#26597;&#20102;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15886v1 Announce Type: cross  Abstract: This paper introduces a novel approach for efficiently distilling LLMs into smaller, application-specific models, significantly reducing operational costs and manual labor. Addressing the challenge of deploying computationally intensive LLMs in specific applications or edge devices, this technique utilizes LLMs' reasoning capabilities to generate labels and natural language rationales for unlabeled data. Our approach enhances both finetuning and distillation by employing a multi-task training framework where student models mimic these rationales alongside teacher predictions. Key contributions include the employment of zero-shot prompting to elicit teacher model rationales, reducing the necessity for handcrafted few-shot examples and lowering the overall token count required, which directly translates to cost savings given the pay-per-token billing model of major tech companies' LLM APIs. Additionally, the paper investigates the impact
&lt;/p&gt;</description></item><item><title>TrustSQL&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#38382;&#39064;&#26102;&#30340;&#21487;&#38752;&#24615;&#30340;&#26032;&#22522;&#20934;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;SQL&#39044;&#27979;&#21644;&#25918;&#24323;&#39044;&#27979;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.15879</link><description>&lt;p&gt;
TrustSQL: &#29992;&#20110;&#20855;&#26377;&#22810;&#26679;&#26080;&#27861;&#22238;&#31572;&#38382;&#39064;&#30340;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#30340;&#21487;&#38752;&#24615;&#22522;&#20934;
&lt;/p&gt;
&lt;p&gt;
TrustSQL: A Reliability Benchmark for Text-to-SQL Models with Diverse Unanswerable Questions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15879
&lt;/p&gt;
&lt;p&gt;
TrustSQL&#26159;&#19968;&#20010;&#26088;&#22312;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#38382;&#39064;&#26102;&#30340;&#21487;&#38752;&#24615;&#30340;&#26032;&#22522;&#20934;&#65292;&#35201;&#27714;&#27169;&#22411;&#22312;SQL&#39044;&#27979;&#21644;&#25918;&#24323;&#39044;&#27979;&#20004;&#31181;&#24773;&#20917;&#19979;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;&#23558;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32763;&#35793;&#25104;SQL&#26597;&#35810;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;SQL&#29983;&#25104;&#30340;&#20934;&#30830;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#21516;&#26102;&#65292;&#24456;&#23569;&#26377;&#20154;&#20102;&#35299;&#36825;&#20123;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#30495;&#23454;&#19990;&#30028;&#37096;&#32626;&#36807;&#31243;&#20013;&#33021;&#21542;&#21487;&#38752;&#22788;&#29702;&#21508;&#31181;&#31867;&#22411;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#25506;&#35752;&#36825;&#19968;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TrustSQL&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#65292;&#26088;&#22312;&#35780;&#20272;&#25991;&#26412;&#21040;SQL&#27169;&#22411;&#22312;&#21333;&#19968;&#25968;&#25454;&#24211;&#21644;&#36328;&#25968;&#25454;&#24211;&#35774;&#32622;&#20013;&#30340;&#21487;&#38752;&#24615;&#12290;&#22522;&#20934;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#25552;&#20379;&#20004;&#31181;&#32467;&#26524;&#20043;&#19968;&#65306;1&#65289;SQL&#39044;&#27979;&#65307;&#25110;2&#65289;&#22312;&#29983;&#25104;&#30340;SQL&#20013;&#21487;&#33021;&#23384;&#22312;&#38169;&#35823;&#25110;&#38754;&#20020;&#26080;&#27861;&#22238;&#31572;&#30340;&#38382;&#39064;&#26102;&#25918;&#24323;&#39044;&#27979;&#12290;&#20026;&#20102;&#35780;&#20272;&#27169;&#22411;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#19987;&#38376;&#20026;&#36825;&#19968;&#20219;&#21153;&#35774;&#35745;&#30340;&#21508;&#31181;&#24314;&#27169;&#26041;&#27861;&#65292;&#21253;&#25324;&#65306;1&#65289;&#20026;&#21487;&#22238;&#31572;&#24615;&#20248;&#21270;&#21333;&#29420;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15879v1 Announce Type: new  Abstract: Recent advances in large language models (LLMs) have led to significant improvements in translating natural language questions into SQL queries. While achieving high accuracy in SQL generation is crucial, little is known about the extent to which these text-to-SQL models can reliably handle diverse types of questions encountered during real-world deployment, including unanswerable ones. To explore this aspect, we present TrustSQL, a new benchmark designed to assess the reliability of text-to-SQL models in both single-database and cross-database settings. The benchmark tasks models with providing one of two outcomes: 1) SQL prediction; or 2) abstention from making a prediction, either when there is a potential error in the generated SQL or when faced with unanswerable questions. For model evaluation, we explore various modeling approaches specifically designed for this task. These include: 1) optimizing separate models for answerability d
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21457;&#29616;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#22312;&#35299;&#37322;&#36974;&#34109;&#30340;&#35270;&#35273;&#20869;&#23481;&#26102;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#37096;&#20998;&#21306;&#22495;&#34987;&#36974;&#34109;&#65292;&#27169;&#22411;&#20173;&#33021;&#20934;&#30830;&#29983;&#25104;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;</title><link>https://arxiv.org/abs/2403.15876</link><description>&lt;p&gt;
&#35748;&#30693;&#38887;&#24615;&#65306;&#25581;&#31034;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#35299;&#37322;&#36974;&#34109;&#35270;&#35273;&#20869;&#23481;&#30340;&#29087;&#32451;&#24230;
&lt;/p&gt;
&lt;p&gt;
Cognitive resilience: Unraveling the proficiency of image-captioning models to interpret masked visual content
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21457;&#29616;&#22270;&#20687;&#23383;&#24149;&#27169;&#22411;&#22312;&#35299;&#37322;&#36974;&#34109;&#30340;&#35270;&#35273;&#20869;&#23481;&#26102;&#34920;&#29616;&#20986;&#24456;&#24378;&#30340;&#33021;&#21147;&#65292;&#21363;&#20351;&#37096;&#20998;&#21306;&#22495;&#34987;&#36974;&#34109;&#65292;&#27169;&#22411;&#20173;&#33021;&#20934;&#30830;&#29983;&#25104;&#25551;&#36848;&#24615;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;&#22270;&#20687;&#23383;&#24149;&#65288;IC&#65289;&#27169;&#22411;&#35299;&#30721;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#36974;&#34109;&#35270;&#35273;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#25581;&#31034;&#20102;IC&#27169;&#22411;&#33021;&#22815;&#20174;&#36974;&#34109;&#22270;&#20687;&#29983;&#25104;&#23383;&#24149;&#65292;&#36825;&#20123;&#23383;&#24149;&#19982;&#21407;&#22987;&#20869;&#23481;&#38750;&#24120;&#30456;&#20284;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#21363;&#20351;&#23384;&#22312;&#36974;&#32617;&#65292;&#35813;&#27169;&#22411;&#20173;&#33021;&#29087;&#32451;&#22320;&#29983;&#25104;&#36229;&#36234;&#21407;&#22987;&#22270;&#20687;&#21487;&#35266;&#23519;&#33539;&#22260;&#30340;&#25551;&#36848;&#24615;&#25991;&#26412;&#20449;&#24687;&#12290;&#34429;&#28982;IC&#27169;&#22411;&#30340;&#35299;&#30721;&#24615;&#33021;&#38543;&#30528;&#36974;&#34109;&#21306;&#22495;&#38754;&#31215;&#22686;&#21152;&#32780;&#19979;&#38477;&#65292;&#20294;&#22312;&#37325;&#35201;&#21306;&#22495;&#26410;&#34987;&#39640;&#24230;&#36974;&#34109;&#26102;&#65292;&#27169;&#22411;&#20173;&#33021;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15876v1 Announce Type: cross  Abstract: This study explores the ability of Image Captioning (IC) models to decode masked visual content sourced from diverse datasets. Our findings reveal the IC model's capability to generate captions from masked images, closely resembling the original content. Notably, even in the presence of masks, the model adeptly crafts descriptive textual information that goes beyond what is observable in the original image-generated captions. While the decoding performance of the IC model experiences a decline with an increase in the masked region's area, the model still performs well when important regions of the image are not masked at high coverage.
&lt;/p&gt;</description></item><item><title>LAMPER&#26694;&#26550;&#26088;&#22312;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#21463;&#21040;PLMs&#26368;&#22823;&#36755;&#20837;&#26631;&#35760;&#38408;&#20540;&#30340;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2403.15875</link><description>&lt;p&gt;
LAMPER&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#25552;&#31034;&#24037;&#31243;
&lt;/p&gt;
&lt;p&gt;
LAMPER: LanguAge Model and Prompt EngineeRing for zero-shot time series classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15875
&lt;/p&gt;
&lt;p&gt;
LAMPER&#26694;&#26550;&#26088;&#22312;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#38646;&#26679;&#26412;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#20013;&#30340;&#36866;&#24212;&#33021;&#21147;&#65292;&#30740;&#31350;&#21457;&#29616;&#20854;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#21463;&#21040;PLMs&#26368;&#22823;&#36755;&#20837;&#26631;&#35760;&#38408;&#20540;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#26500;&#24314;&#20102;LanguAge&#27169;&#22411;&#21644;Prompt EngineeRing&#65288;LAMPER&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#31995;&#32479;&#35780;&#20272;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#22312;&#23481;&#32435;&#22810;&#26679;&#25552;&#31034;&#21450;&#20854;&#22312;&#38646;&#26679;&#26412;&#26102;&#38388;&#24207;&#21015;&#65288;TS&#65289;&#20998;&#31867;&#20013;&#30340;&#25972;&#21512;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#23454;&#39564;&#35780;&#20272;&#20013;&#37096;&#32626;LAMPER&#65292;&#20351;&#29992;&#20102;&#26469;&#28304;&#20110;UCR&#23384;&#26723;&#30340;128&#20010;&#21333;&#21464;&#37327;TS&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#34920;&#26126;&#65292;LAMPER&#30340;&#29305;&#24449;&#34920;&#31034;&#33021;&#21147;&#21463;&#21040;PLMs&#24378;&#21152;&#30340;&#26368;&#22823;&#36755;&#20837;&#26631;&#35760;&#38408;&#20540;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15875v1 Announce Type: new  Abstract: This study constructs the LanguAge Model with Prompt EngineeRing (LAMPER) framework, designed to systematically evaluate the adaptability of pre-trained language models (PLMs) in accommodating diverse prompts and their integration in zero-shot time series (TS) classification. We deploy LAMPER in experimental assessments using 128 univariate TS datasets sourced from the UCR archive. Our findings indicate that the feature representation capacity of LAMPER is influenced by the maximum input token threshold imposed by PLMs.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3.5&#21644;GPT-4&#26469;&#22686;&#24378;OntoClean&#26041;&#27861;&#35770;&#20013;&#30340;&#26412;&#20307;&#37325;&#26500;&#36807;&#31243;&#65292;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#31574;&#30053;&#23637;&#31034;&#20102;&#39640;&#26631;&#35760;&#20934;&#30830;&#24615;&#65292;&#25552;&#20986;&#20102;&#20026;&#26412;&#20307;&#24037;&#20855;&#24320;&#21457;&#25554;&#20214;&#36719;&#20214;&#20197;&#20419;&#36827;&#25972;&#21512;&#30340;&#28508;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15864</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23545;&#22522;&#20110;OntoClean&#30340;&#26412;&#20307;&#37325;&#26500;&#36827;&#34892;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Using Large Language Models for OntoClean-based Ontology Refinement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15864
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22914;GPT-3.5&#21644;GPT-4&#26469;&#22686;&#24378;OntoClean&#26041;&#27861;&#35770;&#20013;&#30340;&#26412;&#20307;&#37325;&#26500;&#36807;&#31243;&#65292;&#36890;&#36807;&#20004;&#31181;&#25552;&#31034;&#31574;&#30053;&#23637;&#31034;&#20102;&#39640;&#26631;&#35760;&#20934;&#30830;&#24615;&#65292;&#25552;&#20986;&#20102;&#20026;&#26412;&#20307;&#24037;&#20855;&#24320;&#21457;&#25554;&#20214;&#36719;&#20214;&#20197;&#20419;&#36827;&#25972;&#21512;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#23558;GPT-3.5&#21644;GPT-4&#31561;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#25972;&#21512;&#21040;&#26412;&#20307;&#37325;&#26500;&#36807;&#31243;&#20013;&#30340;&#26041;&#27861;&#65292;&#37325;&#28857;&#20851;&#27880;OntoClean&#26041;&#27861;&#35770;&#12290;OntoClean&#23545;&#35780;&#20272;&#26412;&#20307;&#30340;&#24418;&#32780;&#19978;&#36136;&#37327;&#33267;&#20851;&#37325;&#35201;&#65292;&#28041;&#21450;&#23558;&#20803;&#23646;&#24615;&#20998;&#37197;&#32473;&#31867;&#21035;&#30340;&#20004;&#27493;&#36807;&#31243;&#21644;&#39564;&#35777;&#19968;&#32452;&#32422;&#26463;&#26465;&#20214;&#12290;&#25163;&#21160;&#23436;&#25104;&#31532;&#19968;&#27493;&#22312;&#23454;&#36341;&#20013;&#24456;&#22256;&#38590;&#65292;&#22240;&#20026;&#38656;&#35201;&#21746;&#23398;&#19987;&#19994;&#30693;&#35782;&#21644;&#26412;&#20307;&#35770;&#32773;&#20043;&#38388;&#32570;&#20047;&#20849;&#35782;&#12290;&#36890;&#36807;&#37319;&#29992;&#20004;&#31181;&#25552;&#31034;&#31574;&#30053;&#30340;LLMs&#65292;&#30740;&#31350;&#34920;&#26126;&#21487;&#20197;&#23454;&#29616;&#39640;&#20934;&#30830;&#29575;&#30340;&#26631;&#35760;&#36807;&#31243;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;LLMs&#26377;&#25552;&#21319;&#26412;&#20307;&#37325;&#26500;&#30340;&#28508;&#21147;&#65292;&#25552;&#20986;&#24320;&#21457;&#26412;&#20307;&#24037;&#20855;&#30340;&#25554;&#20214;&#36719;&#20214;&#20197;&#20419;&#36827;&#36825;&#31181;&#25972;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15864v1 Announce Type: new  Abstract: This paper explores the integration of Large Language Models (LLMs) such as GPT-3.5 and GPT-4 into the ontology refinement process, specifically focusing on the OntoClean methodology. OntoClean, critical for assessing the metaphysical quality of ontologies, involves a two-step process of assigning meta-properties to classes and verifying a set of constraints. Manually conducting the first step proves difficult in practice, due to the need for philosophical expertise and lack of consensus among ontologists. By employing LLMs with two prompting strategies, the study demonstrates that high accuracy in the labelling process can be achieved. The findings suggest the potential for LLMs to enhance ontology refinement, proposing the development of plugin software for ontology tools to facilitate this integration.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27169;&#22411;&#27979;&#35797;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#33258;&#21160;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#35780;&#20272;&#26080;&#20154;&#26426;&#31995;&#32479;&#32423;&#27979;&#35797;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.15857</link><description>&lt;p&gt;
&#26080;&#20154;&#26426;&#31995;&#32479;&#32423;&#27979;&#35797;&#30340;&#33258;&#21160;&#21270;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Automated System-level Testing of Unmanned Aerial Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15857
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#27169;&#22411;&#27979;&#35797;&#21644;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#33258;&#21160;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#35780;&#20272;&#26080;&#20154;&#26426;&#31995;&#32479;&#32423;&#27979;&#35797;&#30340;&#26032;&#39062;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#20154;&#26426;&#31995;&#32479;&#20381;&#36182;&#20110;&#21508;&#31181;&#23433;&#20840;&#20851;&#38190;&#21644;&#20219;&#21153;&#20851;&#38190;&#30340;&#33322;&#31354;&#30005;&#23376;&#31995;&#32479;&#12290;&#22269;&#38469;&#23433;&#20840;&#26631;&#20934;&#30340;&#20027;&#35201;&#35201;&#27714;&#20043;&#19968;&#26159;&#23545;&#33322;&#31354;&#30005;&#23376;&#36719;&#20214;&#31995;&#32479;&#36827;&#34892;&#20005;&#26684;&#30340;&#31995;&#32479;&#32423;&#27979;&#35797;&#12290;&#24403;&#21069;&#24037;&#19994;&#23454;&#36341;&#26159;&#25163;&#21160;&#21019;&#24314;&#27979;&#35797;&#26041;&#26696;&#65292;&#20351;&#29992;&#27169;&#25311;&#22120;&#25163;&#21160;/&#33258;&#21160;&#25191;&#34892;&#36825;&#20123;&#26041;&#26696;&#65292;&#24182;&#25163;&#21160;&#35780;&#20272;&#32467;&#26524;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#33258;&#21160;&#21270;&#26080;&#20154;&#26426;&#31995;&#32479;&#32423;&#27979;&#35797;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;(AITester)&#21033;&#29992;&#22522;&#20110;&#27169;&#22411;&#30340;&#27979;&#35797;&#21644;&#20154;&#24037;&#26234;&#33021;(AI)&#25216;&#26415;&#65292;&#33258;&#21160;&#29983;&#25104;&#12289;&#25191;&#34892;&#21644;&#35780;&#20272;&#21508;&#31181;&#27979;&#35797;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15857v1 Announce Type: cross  Abstract: Unmanned aerial systems (UAS) rely on various avionics systems that are safety-critical and mission-critical. A major requirement of international safety standards is to perform rigorous system-level testing of avionics software systems. The current industrial practice is to manually create test scenarios, manually/automatically execute these scenarios using simulators, and manually evaluate outcomes. The test scenarios typically consist of setting certain flight or environment conditions and testing the system under test in these settings. The state-of-the-art approaches for this purpose also require manual test scenario development and evaluation. In this paper, we propose a novel approach to automate the system-level testing of the UAS. The proposed approach (AITester) utilizes model-based testing and artificial intelligence (AI) techniques to automatically generate, execute, and evaluate various test scenarios. The test scenarios a
&lt;/p&gt;</description></item><item><title>&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2403.15855</link><description>&lt;p&gt;
&#21021;&#22987;&#20540;&#21644;&#25299;&#25169;&#32467;&#26500;&#22312;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Initialisation and Topology Effects in Decentralised Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15855
&lt;/p&gt;
&lt;p&gt;
&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#20110;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#21270;&#31574;&#30053;&#65292;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20855;&#26377;&#23436;&#20840;&#20998;&#25955;&#24335;&#29305;&#24449;&#30340;&#32852;&#37030;&#23398;&#20064;&#20351;&#24471;&#22312;&#32593;&#32476;&#19978;&#20998;&#24067;&#24335;&#35774;&#22791;&#19978;&#23545;&#20010;&#20307;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#36827;&#34892;&#21327;&#20316;&#35757;&#32451;&#65292;&#21516;&#26102;&#20445;&#25345;&#35757;&#32451;&#25968;&#25454;&#26412;&#22320;&#21270;&#12290;&#36825;&#31181;&#26041;&#27861;&#22686;&#24378;&#20102;&#25968;&#25454;&#38544;&#31169;&#24615;&#65292;&#28040;&#38500;&#20102;&#21333;&#28857;&#25925;&#38556;&#21644;&#20013;&#22830;&#21327;&#35843;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#20998;&#25955;&#24335;&#32852;&#37030;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#21463;&#21040;&#36830;&#25509;&#35774;&#22791;&#30340;&#32593;&#32476;&#25299;&#25169;&#32467;&#26500;&#30340;&#26174;&#33879;&#24433;&#21709;&#12290;&#19968;&#20010;&#31616;&#21270;&#30340;&#25968;&#20540;&#27169;&#22411;&#29992;&#20110;&#30740;&#31350;&#36825;&#20123;&#31995;&#32479;&#30340;&#26089;&#26399;&#34892;&#20026;&#65292;&#20351;&#25105;&#20204;&#24471;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#24213;&#23618;&#32593;&#32476;&#33410;&#28857;&#30340;&#29305;&#24449;&#21521;&#37327;&#20013;&#24515;&#24615;&#20998;&#24067;&#30340;&#25913;&#36827;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21021;&#22987;&#20540;&#31574;&#30053;&#65292;&#20174;&#32780;&#22823;&#22823;&#25552;&#39640;&#20102;&#35757;&#32451;&#25928;&#29575;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#25105;&#20204;&#25552;&#20986;&#30340;&#21021;&#22987;&#21270;&#31574;&#30053;&#19979;&#30340;&#27604;&#20363;&#34892;&#20026;&#21644;&#29615;&#22659;&#21442;&#25968;&#30340;&#36873;&#25321;&#12290;&#36825;&#39033;&#24037;&#20316;&#20026;&#26356;&#22810;&#30740;&#31350;&#25171;&#24320;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15855v1 Announce Type: cross  Abstract: Fully decentralised federated learning enables collaborative training of individual machine learning models on distributed devices on a network while keeping the training data localised. This approach enhances data privacy and eliminates both the single point of failure and the necessity for central coordination. Our research highlights that the effectiveness of decentralised federated learning is significantly influenced by the network topology of connected devices. A simplified numerical model for studying the early behaviour of these systems leads us to an improved artificial neural network initialisation strategy, which leverages the distribution of eigenvector centralities of the nodes of the underlying network, leading to a radically improved training efficiency. Additionally, our study explores the scaling behaviour and choice of environmental parameters under our proposed initialisation strategy. This work paves the way for mor
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;LCG&#65292;&#36890;&#36807;&#27169;&#25311;&#21508;&#31181;&#36719;&#20214;&#36807;&#31243;&#27169;&#22411;&#20197;&#21450;&#21033;&#29992;&#21327;&#20316;&#21644;&#25216;&#26415;&#25552;&#39640;&#20195;&#30721;&#36136;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15852</link><description>&lt;p&gt;
&#24403;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#36935;&#19978;&#36719;&#20214;&#24320;&#21457;&#27969;&#31243;
&lt;/p&gt;
&lt;p&gt;
When LLM-based Code Generation Meets the Software Development Process
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15852
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#24341;&#20837;&#20102;&#22522;&#20110;LLM&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;LCG&#65292;&#36890;&#36807;&#27169;&#25311;&#21508;&#31181;&#36719;&#20214;&#36807;&#31243;&#27169;&#22411;&#20197;&#21450;&#21033;&#29992;&#21327;&#20316;&#21644;&#25216;&#26415;&#25552;&#39640;&#20195;&#30721;&#36136;&#37327;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#20854;&#22312;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#19978;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#36807;&#31243;&#27169;&#22411;&#22312;&#20419;&#36827;&#36719;&#20214;&#22242;&#38431;&#20869;&#21327;&#20316;&#19982;&#27807;&#36890;&#65292;&#20351;&#20854;&#33021;&#22815;&#26377;&#25928;&#24212;&#23545;&#22797;&#26434;&#30340;&#24320;&#21457;&#20219;&#21153;&#26041;&#38754;&#25285;&#24403;&#30528;&#20851;&#38190;&#35282;&#33394;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;LCG&#65292;&#36825;&#26159;&#19968;&#20010;&#21463;&#21040;&#25104;&#29087;&#36719;&#20214;&#24037;&#31243;&#23454;&#36341;&#21551;&#21457;&#30340;&#20195;&#30721;&#29983;&#25104;&#26694;&#26550;&#12290;LCG&#21033;&#29992;&#22810;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#20195;&#29702;&#26469;&#27169;&#25311;&#21508;&#31181;&#36719;&#20214;&#36807;&#31243;&#27169;&#22411;&#65292;&#21363;LCGWaterfall&#12289;LCGTDD&#21644;LCGScrum&#12290;&#27599;&#20010;&#27169;&#22411;&#20026;LLM&#20195;&#29702;&#20998;&#37197;&#29305;&#23450;&#35282;&#33394;&#65292;&#22914;&#38656;&#27714;&#24037;&#31243;&#24072;&#12289;&#26550;&#26500;&#24072;&#12289;&#24320;&#21457;&#20154;&#21592;&#12289;&#27979;&#35797;&#20154;&#21592;&#21644;Scrum Master&#65292;&#21453;&#26144;&#20102;&#20856;&#22411;&#30340;&#24320;&#21457;&#27963;&#21160;&#21644;&#27807;&#36890;&#27169;&#24335;&#12290;&#36890;&#36807;&#21033;&#29992;&#24605;&#32500;&#38142;&#21644;&#25552;&#31034;&#32452;&#21512;&#25216;&#26415;&#36827;&#34892;&#21327;&#20316;&#65292;&#20195;&#29702;&#19981;&#26029;&#23436;&#21892;&#33258;&#36523;&#20197;&#25552;&#39640;&#20195;&#30721;&#36136;&#37327;&#12290;&#22312;GPT3.5&#20316;&#20026;&#22522;&#30784;LLM&#21644;&#22522;&#20934;(GPT)&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;LCG&#22312;&#22235;&#20010;&#20195;&#30721;&#29983;&#25104;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#34920;&#29616;&#65306;HumanEval&#12289;HumanEval-ET&#12289;MBPP&#21644;MBPP-ET&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15852v1 Announce Type: cross  Abstract: Software process models play a pivotal role in fostering collaboration and communication within software teams, enabling them to tackle intricate development tasks effectively. This paper introduces LCG, a code generation framework inspired by established software engineering practices. LCG leverages multiple Large Language Model (LLM) agents to emulate various software process models, namely LCGWaterfall, LCGTDD, and LCGScrum. Each model assigns LLM agents specific roles such as requirement engineer, architect, developer, tester, and scrum master, mirroring typical development activities and communication patterns. Through collaborative efforts utilizing chain-of-thought and prompt composition techniques, the agents continuously refine themselves to enhance code quality. Utilizing GPT3.5 as the underlying LLM and baseline (GPT), we evaluate LCG across four code generation benchmarks: HumanEval, HumanEval-ET, MBPP, and MBPP-ET. Results
&lt;/p&gt;</description></item><item><title>ARO&#26694;&#26550;&#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26426;&#22120;&#20154;&#25216;&#33021;&#30340;&#33258;&#20027;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#37096;&#20998;&#20219;&#21153;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15834</link><description>&lt;p&gt;
ARO&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30417;&#30563;&#26426;&#22120;&#20154;&#25991;&#26412;&#21040;&#25216;&#33021;&#33258;&#20027;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
ARO: Large Language Model Supervised Robotics Text2Skill Autonomous Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15834
&lt;/p&gt;
&lt;p&gt;
ARO&#26694;&#26550;&#26088;&#22312;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23454;&#29616;&#26426;&#22120;&#20154;&#25216;&#33021;&#30340;&#33258;&#20027;&#23398;&#20064;&#65292;&#20351;&#20854;&#33021;&#22815;&#22312;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#37096;&#20998;&#20219;&#21153;&#65292;&#21516;&#26102;&#36824;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#23398;&#20064;&#39640;&#24230;&#20381;&#36182;&#20110;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;&#21162;&#21147;&#65292;&#22914;&#28436;&#31034;&#65292;&#24378;&#21270;&#23398;&#20064;&#20013;&#22870;&#21169;&#20989;&#25968;&#30340;&#35774;&#35745;&#65292;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#36827;&#34892;&#24615;&#33021;&#35780;&#20272;&#31561;&#12290;&#28982;&#32780;&#65292;&#20381;&#36182;&#20154;&#31867;&#21327;&#21161;&#21487;&#33021;&#23548;&#33268;&#26114;&#36149;&#30340;&#23398;&#20064;&#25104;&#26412;&#65292;&#24182;&#20351;&#25216;&#33021;&#23398;&#20064;&#38590;&#20197;&#25193;&#23637;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30417;&#30563;&#26426;&#22120;&#20154;&#25991;&#26412;&#21040;&#25216;&#33021;&#33258;&#20027;&#23398;&#20064;&#65288;ARO&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#29992;&#21253;&#21547;&#22870;&#21169;&#20989;&#25968;&#35774;&#35745;&#21644;&#24615;&#33021;&#35780;&#20272;&#30340;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#21462;&#20195;&#26426;&#22120;&#20154;&#25216;&#33021;&#23398;&#20064;&#36807;&#31243;&#20013;&#30340;&#20154;&#31867;&#21442;&#19982;&#12290;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#23436;&#20840;&#33258;&#20027;&#30340;&#26426;&#22120;&#20154;&#25216;&#33021;&#23398;&#20064;&#65292;&#33021;&#22815;&#22312;&#26080;&#38656;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#23436;&#25104;&#37096;&#20998;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#35813;&#26041;&#27861;&#22312;&#20219;&#21153;&#29702;&#35299;&#21644;&#20248;&#21270;&#31283;&#23450;&#24615;&#26041;&#38754;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15834v1 Announce Type: cross  Abstract: Robotics learning highly relies on human expertise and efforts, such as demonstrations, design of reward functions in reinforcement learning, performance evaluation using human feedback, etc. However, reliance on human assistance can lead to expensive learning costs and make skill learning difficult to scale. In this work, we introduce the Large Language Model Supervised Robotics Text2Skill Autonomous Learning (ARO) framework, which aims to replace human participation in the robot skill learning process with large-scale language models that incorporate reward function design and performance evaluation. We provide evidence that our approach enables fully autonomous robot skill learning, capable of completing partial tasks without human intervention. Furthermore, we also analyze the limitations of this approach in task understanding and optimization stability.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#36827;&#34892;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;&#23454;&#29616;&#23545;&#29305;&#23450;&#20219;&#21153;&#30446;&#26631;&#30340;&#23450;&#37327;&#28385;&#36275;&#35821;&#20041;&#65292;&#24182;&#21033;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.15826</link><description>&lt;p&gt;
&#36890;&#36807;Dropout&#23545;&#26102;&#38388;&#20219;&#21153;&#36827;&#34892;&#27604;&#20363;&#23398;&#20064;&#30340;&#31574;&#30053;&#20248;&#21270;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Scaling Learning based Policy Optimization for Temporal Tasks via Dropout
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#29992;&#20110;&#35757;&#32451;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#36890;&#36807;&#23545;&#20219;&#21153;&#36827;&#34892;&#24418;&#24335;&#21270;&#34920;&#36848;&#65292;&#23454;&#29616;&#23545;&#29305;&#23450;&#20219;&#21153;&#30446;&#26631;&#30340;&#23450;&#37327;&#28385;&#36275;&#35821;&#20041;&#65292;&#24182;&#21033;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35757;&#32451;&#22312;&#39640;&#24230;&#38750;&#32447;&#24615;&#29615;&#22659;&#20013;&#36816;&#34892;&#30340;&#33258;&#20027;&#26234;&#33021;&#20307;&#30340;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#24076;&#26395;&#32463;&#36807;&#35757;&#32451;&#30340;&#31574;&#30053;&#33021;&#22815;&#30830;&#20445;&#35813;&#26234;&#33021;&#20307;&#28385;&#36275;&#29305;&#23450;&#30340;&#20219;&#21153;&#30446;&#26631;&#65292;&#36825;&#20123;&#30446;&#26631;&#20197;&#31163;&#25955;&#26102;&#38388;&#20449;&#21495;&#26102;&#38388;&#36923;&#36753;&#65288;DT-STL&#65289;&#34920;&#31034;&#12290;&#36890;&#36807;&#23558;&#20219;&#21153;&#37325;&#26032;&#34920;&#36848;&#20026;&#24418;&#24335;&#21270;&#26694;&#26550;&#65288;&#22914;DT-STL&#65289;&#65292;&#19968;&#20010;&#20248;&#21183;&#26159;&#20801;&#35768;&#23450;&#37327;&#28385;&#36275;&#35821;&#20041;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#32473;&#23450;&#19968;&#20010;&#36712;&#36857;&#21644;&#19968;&#20010;DT-STL&#20844;&#24335;&#65292;&#25105;&#20204;&#21487;&#20197;&#35745;&#31639;&#40065;&#26834;&#24615;&#65292;&#36825;&#21487;&#20197;&#35299;&#37322;&#20026;&#36712;&#36857;&#19982;&#28385;&#36275;&#35813;&#20844;&#24335;&#30340;&#36712;&#36857;&#38598;&#20043;&#38388;&#30340;&#36817;&#20284;&#26377;&#31526;&#21495;&#36317;&#31163;&#12290;&#25105;&#20204;&#21033;&#29992;&#21453;&#39304;&#25511;&#21046;&#22120;&#65292;&#24182;&#20551;&#35774;&#20351;&#29992;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#26469;&#23398;&#20064;&#36825;&#20123;&#21453;&#39304;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#20010;&#23398;&#20064;&#38382;&#39064;&#19982;&#35757;&#32451;&#36882;&#24402;&#31070;&#32463;&#32593;&#32476;&#65288;RNNs&#65289;&#31867;&#20284;&#30340;&#22320;&#26041;&#65292;&#20854;&#20013;&#36882;&#24402;&#21333;&#20803;&#30340;&#25968;&#37327;&#19982;&#26234;&#33021;&#20307;&#30340;&#26102;&#38388;&#35270;&#37326;&#25104;&#27604;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15826v1 Announce Type: cross  Abstract: This paper introduces a model-based approach for training feedback controllers for an autonomous agent operating in a highly nonlinear environment. We desire the trained policy to ensure that the agent satisfies specific task objectives, expressed in discrete-time Signal Temporal Logic (DT-STL). One advantage for reformulation of a task via formal frameworks, like DT-STL, is that it permits quantitative satisfaction semantics. In other words, given a trajectory and a DT-STL formula, we can compute the robustness, which can be interpreted as an approximate signed distance between the trajectory and the set of trajectories satisfying the formula. We utilize feedback controllers, and we assume a feed forward neural network for learning these feedback controllers. We show how this learning problem is similar to training recurrent neural networks (RNNs), where the number of recurrent units is proportional to the temporal horizon of the agen
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22522;&#20110;&#30899;&#25490;&#25918;&#24378;&#24230;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#36873;&#25321;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20302;&#30899;&#24378;&#24230;&#26102;&#27573;&#20351;&#29992;&#26356;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#22312;&#39640;&#30899;&#24378;&#24230;&#26102;&#27573;&#20351;&#29992;&#26356;&#20302;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#26377;&#25928;&#25913;&#21892;&#35270;&#35273;&#35782;&#21035;&#26381;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#22810;&#25552;&#39640;&#30899;&#25490;&#25918;&#25928;&#29575;&#36798;80%&#12290;</title><link>https://arxiv.org/abs/2403.15824</link><description>&lt;p&gt;
&#22522;&#20110;&#30899;&#25490;&#25918;&#24378;&#24230;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#33258;&#36866;&#24212;&#25512;&#26029;
&lt;/p&gt;
&lt;p&gt;
Carbon Intensity-Aware Adaptive Inference of DNNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15824
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#30899;&#25490;&#25918;&#24378;&#24230;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#36873;&#25321;&#65292;&#26412;&#30740;&#31350;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#20302;&#30899;&#24378;&#24230;&#26102;&#27573;&#20351;&#29992;&#26356;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#22312;&#39640;&#30899;&#24378;&#24230;&#26102;&#27573;&#20351;&#29992;&#26356;&#20302;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#26377;&#25928;&#25913;&#21892;&#35270;&#35273;&#35782;&#21035;&#26381;&#21153;&#30340;&#20934;&#30830;&#24615;&#65292;&#26368;&#22810;&#25552;&#39640;&#30899;&#25490;&#25918;&#25928;&#29575;&#36798;80%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
DNN&#25512;&#26029;&#20197;&#20854;&#24040;&#22823;&#30340;&#33021;&#32791;&#21450;&#30001;&#27492;&#20135;&#29983;&#30340;&#39640;&#30899;&#36275;&#36857;&#32780;&#38395;&#21517;&#65292;&#36890;&#36807;&#26681;&#25454;&#19968;&#22825;&#20013;&#30899;&#25490;&#25918;&#24378;&#24230;&#30340;&#21464;&#21270;&#35843;&#25972;&#27169;&#22411;&#22823;&#23567;&#21644;&#20934;&#30830;&#24615;&#65292;&#21487;&#20197;&#20351;&#20854;&#26356;&#21152;&#21487;&#25345;&#32493;&#12290;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#22312;&#20302;&#24378;&#24230;&#26102;&#27573;&#20351;&#29992;&#26356;&#22823;&#12289;&#26356;&#39640;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#65292;&#22312;&#39640;&#24378;&#24230;&#26102;&#27573;&#20351;&#29992;&#26356;&#23567;&#12289;&#26356;&#20302;&#20934;&#30830;&#24615;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#25351;&#26631;&#65292;&#21363;&#30899;&#25490;&#25918;&#25928;&#29575;&#65292;&#20197;&#37327;&#21270;&#33258;&#36866;&#24212;&#27169;&#22411;&#36873;&#25321;&#22312;&#30899;&#36275;&#36857;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#33021;&#22815;&#23558;&#35270;&#35273;&#35782;&#21035;&#26381;&#21153;&#30340;&#20934;&#30830;&#24615;&#25552;&#21319;&#39640;&#36798;80%&#20197;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15824v1 Announce Type: cross  Abstract: DNN inference, known for its significant energy consumption and the resulting high carbon footprint, can be made more sustainable by adapting model size and accuracy to the varying carbon intensity throughout the day. Our heuristic algorithm uses larger, high-accuracy models during low-intensity periods and smaller, lower-accuracy ones during high-intensity periods. We also introduce a metric, carbon-emission efficiency, which quantitatively measures the efficacy of adaptive model selection in terms of carbon footprint. The evaluation showed that the proposed approach could improve the carbon emission efficiency in improving the accuracy of vision recognition services by up to 80%.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#35774;&#35745;&#20248;&#21270;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#27424;&#39537;&#21160;&#25163;&#22806;&#39592;&#39612;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#65292;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#30456;&#36739;&#20110;&#34542;&#21147;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26356;&#30701;&#26102;&#38388;&#20869;&#33719;&#24471;&#26356;&#31934;&#30830;&#21644;&#26356;&#20248;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#35774;&#35745;&#30340;&#20248;&#21270;&#12290;</title><link>https://arxiv.org/abs/2403.15812</link><description>&lt;p&gt;
&#36827;&#21270;&#35745;&#31639;&#23545;&#26426;&#22120;&#20154;&#35774;&#35745;&#30340;&#24433;&#21709;&#65306;&#19968;&#20010;&#22522;&#20110;&#27424;&#39537;&#21160;&#25163;&#22806;&#39592;&#39612;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
The Impact of Evolutionary Computation on Robotic Design: A Case Study with an Underactuated Hand Exoskeleton
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15812
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#35774;&#35745;&#20248;&#21270;&#20013;&#30340;&#28508;&#21147;&#65292;&#24182;&#36890;&#36807;&#27424;&#39537;&#21160;&#25163;&#22806;&#39592;&#39612;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#65292;&#36827;&#21270;&#35745;&#31639;&#26041;&#27861;&#30456;&#36739;&#20110;&#34542;&#21147;&#26041;&#27861;&#65292;&#33021;&#22815;&#22312;&#26356;&#30701;&#26102;&#38388;&#20869;&#33719;&#24471;&#26356;&#31934;&#30830;&#21644;&#26356;&#20248;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20174;&#32780;&#25552;&#39640;&#35774;&#35745;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#22806;&#39592;&#39612;&#21487;&#20197;&#22686;&#24378;&#20154;&#20307;&#21147;&#37327;&#65292;&#24110;&#21161;&#24739;&#26377;&#32930;&#20307;&#27531;&#30142;&#30340;&#20154;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#36825;&#20123;&#22806;&#39592;&#39612;&#20197;&#30830;&#20445;&#23433;&#20840;&#21644;&#26368;&#20339;&#24615;&#33021;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#12290;&#24320;&#21457;&#22806;&#39592;&#39612;&#24212;&#35813;&#25972;&#21512;&#29305;&#23450;&#30340;&#20248;&#21270;&#31639;&#27861;&#26469;&#25214;&#21040;&#26368;&#20339;&#35774;&#35745;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36827;&#21270;&#35745;&#31639;&#65288;EC&#65289;&#26041;&#27861;&#22312;&#26426;&#22120;&#20154;&#35774;&#35745;&#20248;&#21270;&#20013;&#30340;&#28508;&#21147;&#65292;&#20197;&#27424;&#39537;&#21160;&#25163;&#22806;&#39592;&#39612;&#65288;U-HEx&#65289;&#20316;&#20026;&#26696;&#20363;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#36890;&#36807;&#38598;&#25104;&#36951;&#20256;&#31639;&#27861;&#21644;&#22823;&#29190;&#28856;-&#22823;&#20860;&#24182;&#31639;&#27861;&#31561;EC&#25216;&#26415;&#26469;&#25913;&#36827;U-HEx&#35774;&#35745;&#30340;&#24615;&#33021;&#21644;&#21487;&#29992;&#24615;&#65292;&#35813;&#35774;&#35745;&#26368;&#21021;&#20351;&#29992;&#22825;&#30495;&#30340;&#34542;&#21147;&#26041;&#27861;&#36827;&#34892;&#20248;&#21270;&#12290;&#27604;&#36739;&#20998;&#26512;&#26174;&#31034;&#65292;EC&#26041;&#27861;&#22987;&#32456;&#27604;&#34542;&#21147;&#26041;&#27861;&#22312;&#26126;&#26174;&#26356;&#30701;&#30340;&#26102;&#38388;&#20869;&#20135;&#29983;&#26356;&#31934;&#30830;&#21644;&#26356;&#20248;&#21270;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#36890;&#36807;&#22686;&#21152;&#35774;&#35745;&#20013;&#30340;&#21464;&#37327;&#25968;&#37327;&#26469;&#25913;&#36827;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15812v1 Announce Type: cross  Abstract: Robotic exoskeletons can enhance human strength and aid people with physical disabilities. However, designing them to ensure safety and optimal performance presents significant challenges. Developing exoskeletons should incorporate specific optimization algorithms to find the best design. This study investigates the potential of Evolutionary Computation (EC) methods in robotic design optimization, with an underactuated hand exoskeleton (U-HEx) used as a case study. We propose improving the performance and usability of the U-HEx design, which was initially optimized using a naive brute-force approach, by integrating EC techniques such as Genetic Algorithm and Big Bang-Big Crunch Algorithm. Comparative analysis revealed that EC methods consistently yield more precise and optimal solutions than brute force in a significantly shorter time. This allowed us to improve the optimization by increasing the number of variables in the design, whic
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#25928;&#28151;&#21512;&#21521;&#37327;-&#20851;&#31995;&#25628;&#32034;&#30340;&#26367;&#20195;&#25968;&#25454;&#35775;&#38382;&#36335;&#24452;&#35774;&#35745;&#21644;&#20248;&#21270;&#26041;&#27861;&#12290;</title><link>https://arxiv.org/abs/2403.15807</link><description>&lt;p&gt;
&#39640;&#25928;&#30340;&#28151;&#21512;&#21521;&#37327;-&#20851;&#31995;&#25628;&#32034;&#25968;&#25454;&#35775;&#38382;&#36335;&#24452;
&lt;/p&gt;
&lt;p&gt;
Efficient Data Access Paths for Mixed Vector-Relational Search
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15807
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#38024;&#23545;&#39640;&#25928;&#28151;&#21512;&#21521;&#37327;-&#20851;&#31995;&#25628;&#32034;&#30340;&#26367;&#20195;&#25968;&#25454;&#35775;&#38382;&#36335;&#24452;&#35774;&#35745;&#21644;&#20248;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#33021;&#21147;&#30340;&#24555;&#36895;&#22686;&#38271;&#20197;&#21450;&#20351;&#29992;&#21521;&#37327;&#23884;&#20837;&#36827;&#34892;&#25968;&#25454;&#22788;&#29702;&#26041;&#27861;&#30340;&#37319;&#29992;&#24341;&#21457;&#20102;&#23545;&#21019;&#24314;&#21521;&#37327;&#25968;&#25454;&#31649;&#29702;&#31995;&#32479;&#30340;&#26497;&#22823;&#20852;&#36259;&#12290;&#26412;&#25991;&#37325;&#26032;&#35780;&#20272;&#31934;&#30830;&#20294;&#31351;&#23613;&#30340;&#25195;&#25551;&#24335;&#25628;&#32034;&#65292;&#24182;&#25552;&#20986;&#30828;&#20214;&#20248;&#21270;&#21644;&#26367;&#20195;&#24352;&#37327;&#24335;&#20844;&#24335;&#21270;&#21644;&#25209;&#22788;&#29702;&#20197;&#25269;&#28040;&#25104;&#26412;&#12290;&#25105;&#20204;&#27010;&#36848;&#22797;&#26434;&#30340;&#35775;&#38382;&#36335;&#24452;&#35774;&#35745;&#31354;&#38388;&#65292;&#20027;&#35201;&#30001;&#20851;&#31995;&#36873;&#25321;&#24615;&#39537;&#21160;&#65292;&#20197;&#21450;&#32771;&#34385;&#30340;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15807v1 Announce Type: cross  Abstract: The rapid growth of machine learning capabilities and the adoption of data processing methods using vector embeddings sparked a great interest in creating systems for vector data management. While the predominant approach of vector data management is to use specialized index structures for fast search over the entirety of the vector embeddings, once combined with other (meta)data, the search queries can also become selective on relational attributes - typical for analytical queries. As using vector indexes differs from traditional relational data access, we revisit and analyze alternative access paths for efficient mixed vector-relational search.   We first evaluate the accurate but exhaustive scan-based search and propose hardware optimizations and alternative tensor-based formulation and batching to offset the cost. We outline the complex access-path design space, primarily driven by relational selectivity, and the decisions to consi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.15796</link><description>&lt;p&gt;
&#20174;&#25439;&#22833;&#35282;&#24230;&#29702;&#35299;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Understanding Emergent Abilities of Language Models from the Loss Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15796
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#25439;&#22833;&#35282;&#24230;&#37325;&#26032;&#23450;&#20041;&#20102;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#65292;&#21457;&#29616;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#27169;&#22411;&#22312;&#19981;&#21516;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#20284;&#65292;&#32780;&#24403;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#27169;&#22411;&#23558;&#23637;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#30740;&#31350;&#36136;&#30097;&#20102;&#20256;&#32479;&#35748;&#20026;&#35821;&#35328;&#27169;&#22411;&#30340;&#31361;&#29616;&#33021;&#21147;&#20165;&#23384;&#22312;&#20110;&#22823;&#27169;&#22411;&#20013;&#30340;&#35266;&#28857;&#12290;&#36825;&#31181;&#24576;&#30097;&#28304;&#33258;&#20004;&#28857;&#35266;&#23519;&#65306;1&#65289;&#36739;&#23567;&#30340;&#27169;&#22411;&#20063;&#33021;&#23637;&#29616;&#20986;&#23545;&#31361;&#29616;&#33021;&#21147;&#30340;&#39640;&#24615;&#33021;&#65307;2&#65289;&#36136;&#30097;&#29992;&#20110;&#27979;&#37327;&#36825;&#20123;&#33021;&#21147;&#30340;&#19981;&#36830;&#32493;&#24615;&#25351;&#26631;&#12290;&#26412;&#25991;&#25552;&#35758;&#20174;&#39044;&#35757;&#32451;&#25439;&#22833;&#30340;&#35282;&#24230;&#30740;&#31350;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#38750;&#27169;&#22411;&#22823;&#23567;&#25110;&#35757;&#32451;&#35745;&#31639;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#30456;&#21516;&#39044;&#35757;&#32451;&#25439;&#22833;&#20294;&#19981;&#21516;&#27169;&#22411;&#21644;&#25968;&#25454;&#22823;&#23567;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#19978;&#34920;&#29616;&#30456;&#21516;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#24403;&#26576;&#19968;&#27169;&#22411;&#30340;&#39044;&#35757;&#32451;&#25439;&#22833;&#20302;&#20110;&#29305;&#23450;&#38408;&#20540;&#26102;&#65292;&#22312;&#26576;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#31361;&#29616;&#33021;&#21147;&#65292;&#32780;&#19981;&#35770;&#25351;&#26631;&#30340;&#36830;&#32493;&#24615;&#22914;&#20309;&#65307;&#32780;&#22312;&#36798;&#21040;&#35813;&#38408;&#20540;&#20043;&#21069;&#65292;&#20854;&#24615;&#33021;&#20173;&#20445;&#25345;&#22312;&#38543;&#26426;&#29468;&#27979;&#27700;&#24179;&#12290;&#36825;&#21551;&#21457;&#25105;&#20204;&#37325;&#26032;&#23450;&#20041;&#31361;&#29616;&#33021;&#21147;&#20026;&#37027;&#20123;......
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15796v1 Announce Type: cross  Abstract: Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#38544;&#31169;&#12289;&#36947;&#24503;&#21644;&#27861;&#24459;&#25361;&#25112;&#30340;&#38024;&#23545;&#24615;&#36951;&#24536;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#23436;&#25972;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20445;&#25345;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#21516;&#26102;&#21024;&#38500;&#29305;&#23450;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15779</link><description>&lt;p&gt;
&#25968;&#25454;&#28040;&#38500;&#30340;&#21069;&#27839;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;
&lt;/p&gt;
&lt;p&gt;
The Frontier of Data Erasure: Machine Unlearning for Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15779
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26426;&#22120;&#36951;&#24536;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#35299;&#20915;&#38544;&#31169;&#12289;&#36947;&#24503;&#21644;&#27861;&#24459;&#25361;&#25112;&#30340;&#38024;&#23545;&#24615;&#36951;&#24536;&#20449;&#24687;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#23436;&#25972;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#20445;&#25345;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#21516;&#26102;&#21024;&#38500;&#29305;&#23450;&#25968;&#25454;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#26159;&#20154;&#24037;&#26234;&#33021;&#36827;&#27493;&#30340;&#22522;&#30784;&#65292;&#25512;&#21160;&#20102;&#35832;&#22914;&#39044;&#27979;&#25991;&#26412;&#29983;&#25104;&#20043;&#31867;&#30340;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#21487;&#33021;&#20250;&#36890;&#36807;&#28508;&#22312;&#22320;&#35760;&#24518;&#21644;&#20256;&#25773;&#26469;&#33258;&#24222;&#22823;&#25968;&#25454;&#38598;&#30340;&#25935;&#24863;&#12289;&#20559;&#35265;&#25110;&#21463;&#29256;&#26435;&#20445;&#25252;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#23384;&#22312;&#39118;&#38505;&#12290;&#26426;&#22120;&#36951;&#24536;&#20316;&#20026;&#19968;&#20010;&#21069;&#27839;&#35299;&#20915;&#26041;&#26696;&#24212;&#36816;&#32780;&#29983;&#65292;&#25552;&#20379;&#20102;&#20379;LLMs&#26377;&#36873;&#25321;&#24615;&#22320;&#20002;&#24323;&#26576;&#20123;&#25968;&#25454;&#30340;&#25216;&#26415;&#12290;&#26412;&#25991;&#22238;&#39038;&#20102;&#38024;&#23545;LLMs&#30340;&#26426;&#22120;&#36951;&#24536;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#20171;&#32461;&#20102;&#29992;&#20110;&#26377;&#38024;&#23545;&#24615;&#22320;&#36951;&#24536;&#20449;&#24687;&#20197;&#35299;&#20915;&#38544;&#31169;&#12289;&#36947;&#24503;&#21644;&#27861;&#24459;&#25361;&#25112;&#30340;&#26041;&#27861;&#65292;&#32780;&#19981;&#38656;&#35201;&#36827;&#34892;&#23436;&#25972;&#27169;&#22411;&#37325;&#26032;&#35757;&#32451;&#12290;&#23427;&#23558;&#29616;&#26377;&#30740;&#31350;&#20998;&#20026;&#26469;&#33258;&#38750;&#32467;&#26500;&#21270;/&#25991;&#26412;&#25968;&#25454;&#21644;&#32467;&#26500;&#21270;/&#20998;&#31867;&#25968;&#25454;&#30340;&#36951;&#24536;&#65292;&#23637;&#31034;&#20102;&#36825;&#20123;&#26041;&#27861;&#22312;&#21024;&#38500;&#29305;&#23450;&#25968;&#25454;&#30340;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#26377;&#25928;&#24615;&#12290;&#24378;&#35843;&#26426;&#22120;&#36951;&#24536;&#30340;&#23454;&#29992;&#24615;&#65292;&#26412;&#20998;&#26512;&#36824;&#25351;&#20986;&#20102;&#20854;&#20013;&#30340;&#38556;&#30861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15779v1 Announce Type: new  Abstract: Large Language Models (LLMs) are foundational to AI advancements, facilitating applications like predictive text generation. Nonetheless, they pose risks by potentially memorizing and disseminating sensitive, biased, or copyrighted information from their vast datasets. Machine unlearning emerges as a cutting-edge solution to mitigate these concerns, offering techniques for LLMs to selectively discard certain data. This paper reviews the latest in machine unlearning for LLMs, introducing methods for the targeted forgetting of information to address privacy, ethical, and legal challenges without necessitating full model retraining. It divides existing research into unlearning from unstructured/textual data and structured/classification data, showcasing the effectiveness of these approaches in removing specific data while maintaining model efficacy. Highlighting the practicality of machine unlearning, this analysis also points out the hurdl
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#25991;&#26723;&#32423;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#26641;&#19982;&#21477;&#32423;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#22270;&#32467;&#21512;&#36215;&#26469;&#26500;&#24314;S3&#22270;&#65292;&#24418;&#25104;&#32479;&#19968;&#30340;&#35821;&#20041;&#35805;&#35821;&#32467;&#26500;&#65292;&#29992;&#20110;&#26631;&#39064;&#29983;&#25104;&#26694;&#26550;&#20013;&#65292;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#20998;&#23618;&#32467;&#26500;&#20462;&#21098;&#26426;&#21046;&#65292;&#25552;&#39640;&#26631;&#39064;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15776</link><description>&lt;p&gt;
&#20026;&#39640;&#36136;&#37327;&#26631;&#39064;&#29983;&#25104;&#24314;&#27169;&#32479;&#19968;&#35821;&#20041;&#35805;&#35821;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
Modeling Unified Semantic Discourse Structure for High-quality Headline Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15776
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#25991;&#26723;&#32423;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#26641;&#19982;&#21477;&#32423;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#22270;&#32467;&#21512;&#36215;&#26469;&#26500;&#24314;S3&#22270;&#65292;&#24418;&#25104;&#32479;&#19968;&#30340;&#35821;&#20041;&#35805;&#35821;&#32467;&#26500;&#65292;&#29992;&#20110;&#26631;&#39064;&#29983;&#25104;&#26694;&#26550;&#20013;&#65292;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#20998;&#23618;&#32467;&#26500;&#20462;&#21098;&#26426;&#21046;&#65292;&#25552;&#39640;&#26631;&#39064;&#29983;&#25104;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39064;&#20026;&#29983;&#25104;&#26088;&#22312;&#29992;&#31616;&#30701;&#12289;&#21560;&#24341;&#20154;&#30340;&#26631;&#39064;&#24635;&#32467;&#38271;&#31687;&#25991;&#26723;&#65292;&#21453;&#26144;&#20027;&#35201;&#24605;&#24819;&#12290;&#36825;&#38656;&#35201;&#20934;&#30830;&#25429;&#25417;&#26680;&#24515;&#25991;&#26723;&#35821;&#20041;&#65292;&#30001;&#20110;&#25991;&#26412;&#30340;&#38271;&#24230;&#21644;&#32972;&#26223;&#20449;&#24687;&#20016;&#23500;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#32479;&#19968;&#30340;&#35821;&#20041;&#35805;&#35821;&#32467;&#26500;(S3)&#26469;&#34920;&#31034;&#25991;&#26723;&#35821;&#20041;&#65292;&#36890;&#36807;&#23558;&#25991;&#26723;&#32423;&#20462;&#36766;&#32467;&#26500;&#29702;&#35770;&#65288;RST&#65289;&#26641;&#19982;&#21477;&#32423;&#25277;&#35937;&#24847;&#20041;&#34920;&#31034;&#65288;AMR&#65289;&#22270;&#32467;&#21512;&#36215;&#26469;&#26500;&#24314;S3&#22270;&#12290;&#21477;&#23376;&#12289;&#20174;&#21477;&#21644;&#35789;&#27719;&#30340;&#20998;&#23618;&#32452;&#21512;&#22266;&#26377;&#22320;&#34920;&#36798;&#20102;&#25972;&#20010;&#25991;&#26723;&#30340;&#35821;&#20041;&#21547;&#20041;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#26631;&#39064;&#29983;&#25104;&#26694;&#26550;&#65292;&#22312;&#20854;&#20013;&#23558;S3&#22270;&#32534;&#30721;&#20026;&#19978;&#19979;&#25991;&#29305;&#24449;&#12290;&#20026;&#20102;&#24041;&#22266;S3&#22270;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#35774;&#35745;&#20102;&#19968;&#20010;&#20998;&#23618;&#32467;&#26500;&#20462;&#21098;&#26426;&#21046;&#65292;&#21160;&#24577;&#31579;&#36873;&#22810;&#20313;&#21644;&#38750;&#24517;&#35201;&#33410;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15776v1 Announce Type: cross  Abstract: Headline generation aims to summarize a long document with a short, catchy title that reflects the main idea. This requires accurately capturing the core document semantics, which is challenging due to the lengthy and background information-rich na ture of the texts. In this work, We propose using a unified semantic discourse structure (S3) to represent document semantics, achieved by combining document-level rhetorical structure theory (RST) trees with sentence-level abstract meaning representation (AMR) graphs to construct S3 graphs. The hierarchical composition of sentence, clause, and word intrinsically characterizes the semantic meaning of the overall document. We then develop a headline generation framework, in which the S3 graphs are encoded as contextual features. To consolidate the efficacy of S3 graphs, we further devise a hierarchical structure pruning mechanism to dynamically screen the redundant and nonessential nodes with
&lt;/p&gt;</description></item><item><title>FusionINN&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#35299;&#24320;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#21521;&#20998;&#35299;&#65292;&#20445;&#35777;&#26080;&#25439;&#30340;&#20687;&#32032;&#26144;&#23556;&#12290;</title><link>https://arxiv.org/abs/2403.15769</link><description>&lt;p&gt;
FusionINN&#65306;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#29992;&#20110;&#33041;&#32959;&#30244;&#30417;&#27979;
&lt;/p&gt;
&lt;p&gt;
FusionINN: Invertible Image Fusion for Brain Tumor Monitoring
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15769
&lt;/p&gt;
&lt;p&gt;
FusionINN&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#21487;&#20197;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#35299;&#24320;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#21521;&#20998;&#35299;&#65292;&#20445;&#35777;&#26080;&#25439;&#30340;&#20687;&#32032;&#26144;&#23556;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#20687;&#34701;&#21512;&#36890;&#24120;&#20351;&#29992;&#19981;&#21487;&#36870;&#31070;&#32463;&#32593;&#32476;&#23558;&#22810;&#20010;&#28304;&#22270;&#20687;&#21512;&#24182;&#20026;&#21333;&#20010;&#34701;&#21512;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#20020;&#24202;&#19987;&#23478;&#65292;&#20165;&#20381;&#36182;&#34701;&#21512;&#22270;&#20687;&#21487;&#33021;&#19981;&#36275;&#20197;&#20570;&#20986;&#35786;&#26029;&#20915;&#31574;&#65292;&#22240;&#20026;&#34701;&#21512;&#26426;&#21046;&#28151;&#21512;&#20102;&#26469;&#33258;&#28304;&#22270;&#20687;&#30340;&#29305;&#24449;&#65292;&#20174;&#32780;&#38590;&#20197;&#35299;&#37322;&#28508;&#22312;&#30340;&#32959;&#30244;&#30149;&#29702;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;FusionINN&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#36870;&#22270;&#20687;&#34701;&#21512;&#26694;&#26550;&#65292;&#33021;&#22815;&#39640;&#25928;&#29983;&#25104;&#34701;&#21512;&#22270;&#20687;&#65292;&#24182;&#36890;&#36807;&#27714;&#35299;&#34701;&#21512;&#36807;&#31243;&#30340;&#36870;&#36807;&#31243;&#23558;&#20854;&#20998;&#35299;&#22238;&#28304;&#22270;&#20687;&#12290;FusionINN&#36890;&#36807;&#25972;&#21512;&#19968;&#20010;&#27491;&#24577;&#20998;&#24067;&#30340;&#28508;&#22312;&#22270;&#20687;&#19982;&#34701;&#21512;&#22270;&#20687;&#19968;&#36215;&#65292;&#20197;&#20419;&#36827;&#20998;&#35299;&#36807;&#31243;&#30340;&#29983;&#25104;&#24314;&#27169;&#65292;&#20174;&#32780;&#20445;&#35777;&#26080;&#25439;&#30340;&#19968;&#23545;&#19968;&#20687;&#32032;&#26144;&#23556;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#25105;&#20204;&#26159;&#39318;&#27425;&#30740;&#31350;&#34701;&#21512;&#22270;&#20687;&#30340;&#21487;&#20998;&#35299;&#24615;&#65292;&#36825;&#23545;&#20110;&#29983;&#21629;&#25935;&#24863;&#24212;&#29992;&#31243;&#24207;&#23588;&#20026;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15769v1 Announce Type: cross  Abstract: Image fusion typically employs non-invertible neural networks to merge multiple source images into a single fused image. However, for clinical experts, solely relying on fused images may be insufficient for making diagnostic decisions, as the fusion mechanism blends features from source images, thereby making it difficult to interpret the underlying tumor pathology. We introduce FusionINN, a novel invertible image fusion framework, capable of efficiently generating fused images and also decomposing them back to the source images by solving the inverse of the fusion process. FusionINN guarantees lossless one-to-one pixel mapping by integrating a normally distributed latent image alongside the fused image to facilitate the generative modeling of the decomposition process. To the best of our knowledge, we are the first to investigate the decomposability of fused images, which is particularly crucial for life-sensitive applications such as
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#25193;&#25955;&#27169;&#22411;&#30340;Bagging&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65288;BEND&#65289;&#65292;&#26377;&#25928;&#26500;&#24314;&#20102;&#22810;&#20010;&#22522;&#26412;&#20998;&#31867;&#22120;&#65292;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;</title><link>https://arxiv.org/abs/2403.15766</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#25193;&#25955;&#30340;Bagging&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#65288;BEND&#65289;
&lt;/p&gt;
&lt;p&gt;
BEND: Bagging Deep Learning Training Based on Efficient Neural Network Diffusion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15766
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#25193;&#25955;&#27169;&#22411;&#30340;Bagging&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65288;BEND&#65289;&#65292;&#26377;&#25928;&#26500;&#24314;&#20102;&#22810;&#20010;&#22522;&#26412;&#20998;&#31867;&#22120;&#65292;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Bagging&#36890;&#36807;&#25972;&#21512;&#22810;&#20010;&#22522;&#26412;&#20998;&#31867;&#22120;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#21333;&#19968;&#20998;&#31867;&#22120;&#26469;&#38477;&#20302;&#27169;&#22411;&#26041;&#24046;&#65292;&#22312;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39640;&#25928;&#31070;&#32463;&#32593;&#32476;&#25193;&#25955;&#30340;Bagging&#28145;&#24230;&#23398;&#20064;&#35757;&#32451;&#31639;&#27861;&#65288;BEND&#65289;&#65292;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#25193;&#25955;&#27169;&#22411;&#39640;&#25928;&#26500;&#24314;&#22522;&#26412;&#20998;&#31867;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#31616;&#21333;&#32780;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15766v1 Announce Type: cross  Abstract: Bagging has achieved great success in the field of machine learning by integrating multiple base classifiers to build a single strong classifier to reduce model variance. The performance improvement of bagging mainly relies on the number and diversity of base classifiers. However, traditional deep learning model training methods are expensive to train individually and difficult to train multiple models with low similarity in a restricted dataset. Recently, diffusion models, which have been tremendously successful in the fields of imaging and vision, have been found to be effective in generating neural network model weights and biases with diversity. We creatively propose a Bagging deep learning training algorithm based on Efficient Neural network Diffusion (BEND). The originality of BEND comes from the first use of a neural network diffusion model to efficiently build base classifiers for bagging. Our approach is simple but effective, 
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#22312;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#29616;&#26377;&#30417;&#30563;&#22522;&#20934;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#20004;&#20010;&#26032;&#30340;&#23569;&#26679;&#26412;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#20851;&#31995;&#20108;&#32500;&#31354;&#38388;&#20808;&#39564;&#21644;&#26679;&#26412;&#30699;&#27491;&#30340;&#21464;&#20998;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.15765</link><description>&lt;p&gt;
&#26397;&#21521;&#31867;&#20154;&#26426;&#29702;&#35299;&#30340;&#26041;&#21521;&#65306;&#22312;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Human-Like Machine Comprehension: Few-Shot Relational Learning in Visually-Rich Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15765
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#32858;&#28966;&#20110;&#22312;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#20013;&#36827;&#34892;&#23569;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#29616;&#26377;&#30417;&#30563;&#22522;&#20934;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#20004;&#20010;&#26032;&#30340;&#23569;&#26679;&#26412;&#22522;&#20934;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#20851;&#31995;&#20108;&#32500;&#31354;&#38388;&#20808;&#39564;&#21644;&#26679;&#26412;&#30699;&#27491;&#30340;&#21464;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20851;&#38190;-&#20540;&#20851;&#31995;&#22312;&#35270;&#35273;&#20016;&#23500;&#25991;&#26723;&#65288;VRDs&#65289;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#36890;&#24120;&#22312;&#19981;&#21516;&#30340;&#31354;&#38388;&#21306;&#22495;&#20013;&#21576;&#29616;&#65292;&#20276;&#38543;&#29305;&#23450;&#30340;&#39068;&#33394;&#21644;&#23383;&#20307;&#39118;&#26684;&#12290;&#36825;&#20123;&#38750;&#25991;&#26412;&#32447;&#32034;&#20316;&#20026;&#37325;&#35201;&#25351;&#31034;&#22120;&#65292;&#26497;&#22823;&#22686;&#24378;&#20102;&#20154;&#31867;&#23545;&#36825;&#31181;&#20851;&#31995;&#19977;&#20803;&#32452;&#30340;&#29702;&#35299;&#21644;&#33719;&#21462;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#25991;&#26723;AI&#26041;&#27861;&#24448;&#24448;&#26410;&#32771;&#34385;&#19982;&#35270;&#35273;&#21644;&#31354;&#38388;&#29305;&#24449;&#30456;&#20851;&#30340;&#36825;&#20123;&#26377;&#20215;&#20540;&#30340;&#20808;&#39564;&#20449;&#24687;&#65292;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#29305;&#21035;&#26159;&#22312;&#22788;&#29702;&#26377;&#38480;&#31034;&#20363;&#26102;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32858;&#28966;&#20110;&#23569;&#26679;&#26412;&#20851;&#31995;&#23398;&#20064;&#65292;&#20855;&#20307;&#38024;&#23545;&#22312;VRDs&#20013;&#25552;&#21462;&#20851;&#38190;-&#20540;&#20851;&#31995;&#19977;&#20803;&#32452;&#12290;&#37492;&#20110;&#32570;&#20047;&#36866;&#29992;&#20110;&#36825;&#19968;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#22522;&#20110;&#29616;&#26377;&#30417;&#30563;&#22522;&#20934;&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#20004;&#20010;&#26032;&#30340;&#23569;&#26679;&#26412;&#22522;&#20934;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21253;&#21547;&#20851;&#31995;&#20108;&#32500;&#31354;&#38388;&#20808;&#39564;&#21644;&#26679;&#26412;&#30699;&#27491;&#30340;&#21464;&#20998;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15765v1 Announce Type: cross  Abstract: Key-value relations are prevalent in Visually-Rich Documents (VRDs), often depicted in distinct spatial regions accompanied by specific color and font styles. These non-textual cues serve as important indicators that greatly enhance human comprehension and acquisition of such relation triplets. However, current document AI approaches often fail to consider this valuable prior information related to visual and spatial features, resulting in suboptimal performance, particularly when dealing with limited examples. To address this limitation, our research focuses on few-shot relational learning, specifically targeting the extraction of key-value relation triplets in VRDs. Given the absence of a suitable dataset for this task, we introduce two new few-shot benchmarks built upon existing supervised benchmark datasets. Furthermore, we propose a variational approach that incorporates relational 2D-spatial priors and prototypical rectification 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#30340;&#30693;&#35782;&#20256;&#36755;&#32473;&#23458;&#25143;&#31471;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#20256;&#39640;&#25928;&#30340;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#26041;&#26696;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15760</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#23558;&#26381;&#21153;&#22120;&#31471;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#20013;&#30340;&#30693;&#35782;&#20256;&#36755;&#32473;&#24322;&#26500;&#32852;&#21512;&#23398;&#20064;&#23458;&#25143;&#31471;&#30340;&#19978;&#20256;&#39640;&#25928;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
An Upload-Efficient Scheme for Transferring Knowledge From a Server-Side Pre-trained Generator to Clients in Heterogeneous Federated Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15760
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#30340;&#30693;&#35782;&#20256;&#36755;&#32473;&#23458;&#25143;&#31471;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#19978;&#20256;&#39640;&#25928;&#30340;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#26041;&#26696;&#65292;&#25104;&#21151;&#35299;&#20915;&#20102;&#24322;&#26500;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#25968;&#25454;&#21644;&#27169;&#22411;&#24322;&#26500;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#26500;&#32852;&#21512;&#23398;&#20064;&#65288;HtFL&#65289;&#23454;&#29616;&#20102;&#22312;&#20855;&#26377;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#30340;&#22810;&#20010;&#23458;&#25143;&#31471;&#19978;&#36827;&#34892;&#21327;&#20316;&#23398;&#20064;&#65292;&#21516;&#26102;&#20445;&#25252;&#38544;&#31169;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#19978;&#20256;&#39640;&#25928;&#30340;&#30693;&#35782;&#20256;&#36755;&#26041;&#26696;&#65292;&#31216;&#20026;&#32852;&#21512;&#30693;&#35782;&#20256;&#36755;&#24490;&#29615;&#65288;FedKTL&#65289;&#65292;&#20197;&#22788;&#29702;&#24322;&#26500;&#32852;&#21512;&#23398;&#20064;&#20013;&#30340;&#30693;&#35782;&#20849;&#20139;&#38382;&#39064;&#12290;FedKTL&#21487;&#20197;&#36890;&#36807;&#26381;&#21153;&#22120;&#19978;&#39044;&#35757;&#32451;&#29983;&#25104;&#22120;&#30340;&#25512;&#29702;&#20135;&#29983;&#19982;&#23458;&#25143;&#31471;&#20219;&#21153;&#30456;&#20851;&#30340;&#21407;&#22411;&#22270;&#20687;-&#21521;&#37327;&#23545;&#12290;&#20511;&#21161;&#36825;&#20123;&#23545;&#65292;&#27599;&#20010;&#23458;&#25143;&#31471;&#37117;&#21487;&#20197;&#36890;&#36807;&#38468;&#21152;&#30340;&#30417;&#30563;&#26412;&#22320;&#20219;&#21153;&#23558;&#26469;&#33258;&#29983;&#25104;&#22120;&#30340;&#39044;&#20808;&#23384;&#22312;&#30340;&#30693;&#35782;&#20256;&#36755;&#21040;&#20854;&#26412;&#22320;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#21253;&#25324;CNN&#21644;ViT&#22312;&#20869;&#30340;14&#31181;&#27169;&#22411;&#19979;&#65292;&#23545;&#22235;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#24191;&#27867;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#19978;&#20256;&#39640;&#25928;&#30340;FedKTL&#36229;&#36234;&#20102;&#19971;&#31181;&#26368;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15760v1 Announce Type: new  Abstract: Heterogeneous Federated Learning (HtFL) enables collaborative learning on multiple clients with different model architectures while preserving privacy. Despite recent research progress, knowledge sharing in HtFL is still difficult due to data and model heterogeneity. To tackle this issue, we leverage the knowledge stored in pre-trained generators and propose a new upload-efficient knowledge transfer scheme called Federated Knowledge-Transfer Loop (FedKTL). Our FedKTL can produce client-task-related prototypical image-vector pairs via the generator's inference on the server. With these pairs, each client can transfer pre-existing knowledge from the generator to its local model through an additional supervised local task. We conduct extensive experiments on four datasets under two types of data heterogeneity with 14 kinds of models including CNNs and ViTs. Results show that our upload-efficient FedKTL surpasses seven state-of-the-art metho
&lt;/p&gt;</description></item><item><title>&#29992;&#25143;&#31471;&#23454;&#29616;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#31215;&#26497;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#31471;&#36816;&#34892;&#36890;&#29992;&#31639;&#27861;&#26469;&#35299;&#20915;&#24120;&#35265;&#38382;&#39064;&#65292;&#26080;&#38656;&#26381;&#21153;&#25552;&#20379;&#21830;&#25913;&#21464;&#26381;&#21153;&#26412;&#36523;&#12290;</title><link>https://arxiv.org/abs/2403.15757</link><description>&lt;p&gt;
&#29992;&#25143;&#31471;&#23454;&#29616;
&lt;/p&gt;
&lt;p&gt;
User-Side Realization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15757
&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#31471;&#23454;&#29616;&#20026;&#29992;&#25143;&#25552;&#20379;&#20102;&#31215;&#26497;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#36890;&#36807;&#22312;&#29992;&#25143;&#31471;&#36816;&#34892;&#36890;&#29992;&#31639;&#27861;&#26469;&#35299;&#20915;&#24120;&#35265;&#38382;&#39064;&#65292;&#26080;&#38656;&#26381;&#21153;&#25552;&#20379;&#21830;&#25913;&#21464;&#26381;&#21153;&#26412;&#36523;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29992;&#25143;&#23545;&#26381;&#21153;&#24863;&#21040;&#19981;&#28385;&#24847;&#12290;&#30001;&#20110;&#26381;&#21153;&#24182;&#38750;&#37327;&#36523;&#23450;&#21046;&#32473;&#29992;&#25143;&#65292;&#22240;&#27492;&#19981;&#28385;&#24847;&#26159;&#33258;&#28982;&#32780;&#28982;&#30340;&#12290;&#38382;&#39064;&#22312;&#20110;&#65292;&#21363;&#20351;&#29992;&#25143;&#24863;&#21040;&#19981;&#28385;&#24847;&#65292;&#20182;&#20204;&#36890;&#24120;&#20063;&#27809;&#26377;&#35299;&#20915;&#19981;&#28385;&#30340;&#25163;&#27573;&#12290;&#29992;&#25143;&#26080;&#27861;&#20462;&#25913;&#26381;&#21153;&#30340;&#28304;&#20195;&#30721;&#65292;&#20063;&#26080;&#27861;&#24378;&#36843;&#26381;&#21153;&#25552;&#20379;&#21830;&#36827;&#34892;&#26356;&#25913;&#12290;&#29992;&#25143;&#21035;&#26080;&#36873;&#25321;&#65292;&#21482;&#33021;&#20445;&#25345;&#19981;&#28385;&#24847;&#25110;&#36864;&#20986;&#26381;&#21153;&#12290;&#29992;&#25143;&#31471;&#23454;&#29616;&#36890;&#36807;&#25552;&#20379;&#36890;&#29992;&#31639;&#27861;&#26469;&#22788;&#29702;&#29992;&#25143;&#31471;&#30340;&#24120;&#35265;&#38382;&#39064;&#65292;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#25552;&#20379;&#20102;&#31215;&#26497;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#36825;&#20123;&#31639;&#27861;&#22312;&#29992;&#25143;&#31471;&#36816;&#34892;&#65292;&#24182;&#22312;&#19981;&#38656;&#35201;&#26381;&#21153;&#25552;&#20379;&#21830;&#25913;&#21464;&#26381;&#21153;&#26412;&#36523;&#30340;&#24773;&#20917;&#19979;&#35299;&#20915;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15757v1 Announce Type: cross  Abstract: Users are dissatisfied with services. Since the service is not tailor-made for a user, it is natural for dissatisfaction to arise. The problem is, that even if users are dissatisfied, they often do not have the means to resolve their dissatisfaction. The user cannot alter the source code of the service, nor can they force the service provider to change. The user has no choice but to remain dissatisfied or quit the service. User-side realization offers proactive solutions to this problem by providing general algorithms to deal with common problems on the user's side. These algorithms run on the user's side and solve the problems without having the service provider change the service itself.
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21021;&#27493;&#23433;&#20840;&#39118;&#38505;&#20998;&#26512;&#20013;&#23637;&#29616;&#20102;&#27604;&#20154;&#31867;&#26356;&#24555;&#36895;&#30340;&#20449;&#24687;&#24635;&#32467;&#33021;&#21147;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#25506;&#35752;&#20102;&#24494;&#35843;&#27169;&#22411;&#22312;&#21327;&#21161;&#20174;&#19994;&#32773;&#36827;&#34892;PSRA&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15756</link><description>&lt;p&gt;
&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#21021;&#27493;&#23433;&#20840;&#39118;&#38505;&#20998;&#26512;&#65306;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Leveraging Large Language Models for Preliminary Security Risk Analysis: A Mission-Critical Case Study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15756
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#21021;&#27493;&#23433;&#20840;&#39118;&#38505;&#20998;&#26512;&#20013;&#23637;&#29616;&#20102;&#27604;&#20154;&#31867;&#26356;&#24555;&#36895;&#30340;&#20449;&#24687;&#24635;&#32467;&#33021;&#21147;&#65292;&#26412;&#30740;&#31350;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#25506;&#35752;&#20102;&#24494;&#35843;&#27169;&#22411;&#22312;&#21327;&#21161;&#20174;&#19994;&#32773;&#36827;&#34892;PSRA&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21021;&#27493;&#23433;&#20840;&#39118;&#38505;&#20998;&#26512;&#65288;PSRA&#65289;&#25552;&#20379;&#20102;&#19968;&#31181;&#24555;&#36895;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;&#12289;&#35780;&#20272;&#21644;&#25552;&#20986;&#28508;&#22312;&#39118;&#38505;&#22312;&#20855;&#20307;&#24773;&#22659;&#20013;&#30340;&#24212;&#23545;&#25514;&#26045;&#12290;&#22312;&#20851;&#38190;&#20219;&#21153;&#24773;&#22659;&#20013;&#65292;&#21450;&#26102;&#21644;&#36805;&#36895;&#30340;&#34892;&#21160;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#25152;&#20197;&#23545;&#26377;&#25928;PSRA&#25152;&#38656;&#30340;&#24191;&#27867;&#19987;&#19994;&#30693;&#35782;&#21644;&#22823;&#37327;&#19982;&#25991;&#26412;&#30456;&#20851;&#30340;&#20219;&#21153;&#22312;&#38459;&#30861;&#24555;&#36895;&#35780;&#20272;&#12290;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#27604;&#20154;&#31867;&#26356;&#24555;&#36895;&#22320;&#24635;&#32467;&#20449;&#24687;&#65292;&#21033;&#29992;&#24494;&#35843;&#27169;&#22411;&#65288;FTM&#65289;&#22312;PSRA&#20013;&#30340;&#33021;&#21147;&#23578;&#26410;&#34987;&#20808;&#21069;&#30340;&#30740;&#31350;&#25152;&#25506;&#35752;&#12290;&#26412;&#25991;&#36890;&#36807;&#26696;&#20363;&#30740;&#31350;&#35843;&#26597;FTM&#22312;&#21327;&#21161;&#20174;&#19994;&#32773;&#36827;&#34892;PSRA&#20013;&#30340;&#29087;&#32451;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15756v1 Announce Type: cross  Abstract: Preliminary security risk analysis (PSRA) provides a quick approach to identify, evaluate and propose remeditation to potential risks in specific scenarios. The extensive expertise required for an effective PSRA and the substantial ammount of textual-related tasks hinder quick assessments in mission-critical contexts, where timely and prompt actions are essential. The speed and accuracy of human experts in PSRA significantly impact response time. A large language model can quickly summarise information in less time than a human. To our knowledge, no prior study has explored the capabilities of fine-tuned models (FTM) in PSRA. Our case study investigates the proficiency of FTM to assist practitioners in PSRA. We manually curated 141 representative samples from over 50 mission-critical analyses archived by the industrial context team in the last five years.We compared the proficiency of the FTM versus seven human experts. Within the indu
&lt;/p&gt;</description></item><item><title>CodeShell-Base&#26159;&#19968;&#20010;70&#20159;&#21442;&#25968;&#35268;&#27169;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20195;&#30721;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;Grouped-Query Attention&#21644;Rotary Positional Embedding&#31561;&#25216;&#26415;&#24418;&#25104;&#29420;&#29305;&#30340;&#26550;&#26500;&#35774;&#35745;&#12290;</title><link>https://arxiv.org/abs/2403.15747</link><description>&lt;p&gt;
CodeShell&#25216;&#26415;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
CodeShell Technical Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15747
&lt;/p&gt;
&lt;p&gt;
CodeShell-Base&#26159;&#19968;&#20010;70&#20159;&#21442;&#25968;&#35268;&#27169;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#22312;&#20195;&#30721;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;Grouped-Query Attention&#21644;Rotary Positional Embedding&#31561;&#25216;&#26415;&#24418;&#25104;&#29420;&#29305;&#30340;&#26550;&#26500;&#35774;&#35745;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#21462;&#24471;&#20102;&#37325;&#35201;&#31361;&#30772;&#12290;&#23427;&#20204;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#29702;&#35299;&#21644;&#29983;&#25104;&#32534;&#31243;&#35821;&#35328;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#32534;&#30721;&#24320;&#21457;&#24037;&#20316;&#27969;&#30340;&#25928;&#29575;&#12290;&#26412;&#25216;&#26415;&#25253;&#21578;&#20171;&#32461;&#20102;CodeShell-Base&#65292;&#36825;&#26159;&#19968;&#20010;70&#20159;&#21442;&#25968;&#35268;&#27169;&#30340;&#22522;&#30784;&#27169;&#22411;&#65292;&#20855;&#26377;8K&#19978;&#19979;&#25991;&#38271;&#24230;&#65292;&#22312;&#20195;&#30721;&#29702;&#35299;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#12290;&#36890;&#36807;&#23558;Grouped-Query Attention&#21644;Rotary Positional Embedding&#25972;&#21512;&#21040;GPT-2&#20013;&#65292;CodeShell-Base&#34701;&#21512;&#20102;StarCoder&#21644;CodeLlama&#30340;&#32467;&#26500;&#20248;&#28857;&#65292;&#24182;&#24418;&#25104;&#20102;&#20854;&#29420;&#29305;&#30340;&#26550;&#26500;&#35774;&#35745;&#12290;&#25105;&#20204;&#36824;&#31934;&#24515;&#26500;&#24314;&#20102;&#21253;&#25324;&#31867;&#20284;&#25968;&#25454;&#21435;&#37325;&#12289;&#22522;&#20110;&#22256;&#24785;&#24230;&#30340;&#25968;&#25454;&#36807;&#28388;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#25968;&#25454;&#36807;&#28388;&#22312;&#20869;&#30340;&#20840;&#38754;&#25968;&#25454;&#39044;&#22788;&#29702;&#27969;&#31243;&#12290;&#36890;&#36807;&#36825;&#19968;&#36807;&#31243;&#65292;&#25105;&#20204;&#20174;GitHub&#20013;&#31579;&#36873;&#20986;&#20102;1000&#20159;&#26465;&#39640;&#36136;&#37327;&#30340;&#39044;&#35757;&#32451;&#25968;&#25454;&#12290;&#20973;&#20511;&#36825;&#20123;&#39640;&#36136;&#37327;&#25968;&#25454;&#65292;CodeShell-Base&#32988;&#36807;&#20102;Co
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15747v1 Announce Type: cross  Abstract: Code large language models mark a pivotal breakthrough in artificial intelligence. They are specifically crafted to understand and generate programming languages, significantly boosting the efficiency of coding development workflows. In this technical report, we present CodeShell-Base, a seven billion-parameter foundation model with 8K context length, showcasing exceptional proficiency in code comprehension. By incorporating Grouped-Query Attention and Rotary Positional Embedding into GPT-2, CodeShell-Base integrates the structural merits of StarCoder and CodeLlama and forms its unique architectural design. We then carefully built a comprehensive data pre-processing process, including similar data deduplication, perplexity-based data filtering, and model-based data filtering. Through this process, We have curated 100 billion high-quality pre-training data from GitHub. Benefiting from the high-quality data, CodeShell-Base outperforms Co
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#23558;&#20154;&#24037;&#21183;&#22330;&#20449;&#24687;&#25972;&#21512;&#21040;CBF-QP&#26694;&#26550;&#20013;&#65292;&#24314;&#31435;&#20102;&#20154;&#24037;&#21183;&#22330;&#19982;&#23433;&#20840;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#24182;&#25193;&#23637;&#20102;CBF-QP&#23433;&#20840;&#28388;&#27874;&#22120;&#30340;&#35774;&#35745;&#20197;&#36866;&#24212;&#26356;&#19968;&#33324;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25511;&#21046;&#20223;&#23556;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#19968;&#33324;APF&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.15743</link><description>&lt;p&gt;
&#20154;&#24037;&#21183;&#22330;&#19982;&#23433;&#20840;&#28388;&#27874;&#22120;&#30340;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Comparative Study of Artificial Potential Fields and Safety Filters
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15743
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#20154;&#24037;&#21183;&#22330;&#20449;&#24687;&#25972;&#21512;&#21040;CBF-QP&#26694;&#26550;&#20013;&#65292;&#24314;&#31435;&#20102;&#20154;&#24037;&#21183;&#22330;&#19982;&#23433;&#20840;&#28388;&#27874;&#22120;&#20043;&#38388;&#30340;&#36830;&#25509;&#65292;&#24182;&#25193;&#23637;&#20102;CBF-QP&#23433;&#20840;&#28388;&#27874;&#22120;&#30340;&#35774;&#35745;&#20197;&#36866;&#24212;&#26356;&#19968;&#33324;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#65292;&#20174;&#32780;&#25552;&#20379;&#20102;&#19968;&#31181;&#36866;&#29992;&#20110;&#25511;&#21046;&#20223;&#23556;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#19968;&#33324;APF&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#30001;&#32463;&#20856;&#36816;&#21160;&#35268;&#21010;&#24037;&#20855;&#35774;&#35745;&#30340;&#25511;&#21046;&#22120;&#65292;&#21363;&#20154;&#24037;&#21183;&#22330;&#65288;APFs&#65289;&#65292;&#21487;&#20197;&#20174;&#26368;&#36817;&#26222;&#21450;&#30340;&#26041;&#27861;&#20013;&#24471;&#21040;&#65306;&#25511;&#21046;&#23631;&#38556;&#20989;&#25968;&#20108;&#27425;&#35268;&#21010;&#65288;CBF-QP&#65289;&#23433;&#20840;&#28388;&#27874;&#22120;&#12290;&#36890;&#36807;&#23558;APF&#20449;&#24687;&#25972;&#21512;&#21040;CBF-QP&#26694;&#26550;&#20013;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#36825;&#20004;&#31181;&#26041;&#27861;&#20043;&#38388;&#30340;&#26725;&#26753;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36825;&#26159;&#36890;&#36807;&#23558;&#26377;&#21560;&#24341;&#21147;&#30340;&#21183;&#22330;&#20316;&#20026;&#25511;&#21046;&#26446;&#38597;&#26222;&#35834;&#22827;&#20989;&#25968;&#65288;CLF&#65289;&#26469;&#24341;&#23548;&#21517;&#20041;&#25511;&#21046;&#22120;&#30340;&#35774;&#35745;&#65292;&#28982;&#21518;&#23558;&#25490;&#26021;&#24615;&#21183;&#22330;&#20316;&#20026;&#30456;&#20114;&#20316;&#29992;CBF&#65288;RCBF&#65289;&#26469;&#23450;&#20041;&#19968;&#20010;CBF-QP&#23433;&#20840;&#28388;&#27874;&#22120;&#12290;&#22522;&#20110;&#36825;&#31181;&#25972;&#21512;&#65292;&#25105;&#20204;&#23558;CBF-QP&#23433;&#20840;&#28388;&#27874;&#22120;&#30340;&#35774;&#35745;&#25193;&#23637;&#21040;&#36866;&#24212;&#26356;&#19968;&#33324;&#30340;&#21253;&#21547;&#25511;&#21046;&#20223;&#23556;&#32467;&#26500;&#30340;&#21160;&#21147;&#23398;&#27169;&#22411;&#31867;&#12290;&#36825;&#31181;&#25193;&#23637;&#20135;&#29983;&#20102;&#19968;&#31181;&#29305;&#27530;&#30340;CBF-QP&#23433;&#20840;&#28388;&#27874;&#22120;&#21644;&#36866;&#29992;&#20110;&#25511;&#21046;&#20223;&#23556;&#21160;&#21147;&#23398;&#27169;&#22411;&#30340;&#19968;&#33324;APF&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15743v1 Announce Type: cross  Abstract: In this paper, we have demonstrated that the controllers designed by a classical motion planning tool, namely artificial potential fields (APFs), can be derived from a recently prevalent approach: control barrier function quadratic program (CBF-QP) safety filters. By integrating APF information into the CBF-QP framework, we establish a bridge between these two methodologies. Specifically, this is achieved by employing the attractive potential field as a control Lyapunov function (CLF) to guide the design of the nominal controller, and then the repulsive potential field serves as a reciprocal CBF (RCBF) to define a CBF-QP safety filter. Building on this integration, we extend the design of the CBF-QP safety filter to accommodate a more general class of dynamical models featuring a control-affine structure. This extension yields a special CBF-QP safety filter and a general APF solution suitable for control-affine dynamical models. Throug
&lt;/p&gt;</description></item><item><title>&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;&#65292;&#33021;&#22815;&#21387;&#32553;&#20449;&#24687;&#24182;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#37325;&#22823;&#20248;&#21183;</title><link>https://arxiv.org/abs/2403.15729</link><description>&lt;p&gt;
&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards a \textbf{RAG}-based Summarization Agent for the Electron-Ion Collider
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15729
&lt;/p&gt;
&lt;p&gt;
&#24320;&#21457;&#20102;&#19968;&#31181;&#38754;&#21521;&#30005;&#23376;&#31163;&#23376;&#23545;&#25758;&#26426;&#30340;&#22522;&#20110;RAG&#30340;&#25688;&#35201;&#29983;&#25104;&#20195;&#29702;&#65292;&#33021;&#22815;&#21387;&#32553;&#20449;&#24687;&#24182;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#37325;&#22823;&#20248;&#21183;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#24615;&#21644;&#24222;&#22823;&#30340;&#20449;&#24687;&#37327;&#28085;&#30422;&#20102;&#22823;&#35268;&#27169;&#23454;&#39564;&#30340;&#25991;&#20214;&#12289;&#35770;&#25991;&#12289;&#25968;&#25454;&#21644;&#20854;&#20182;&#36164;&#28304;&#65292;&#23548;&#33268;&#23548;&#33322;&#36825;&#20123;&#22810;&#26679;&#24418;&#24335;&#20449;&#24687;&#30340;&#20219;&#21153;&#38656;&#35201;&#22823;&#37327;&#26102;&#38388;&#21644;&#31934;&#21147;&#65292;&#23545;&#20110;&#26032;&#21512;&#20316;&#32773;&#21644;&#26089;&#26399;&#31185;&#23398;&#23478;&#26469;&#35828;&#23588;&#20026;&#33392;&#24040;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27491;&#22312;&#24320;&#21457;&#19968;&#31181;&#22522;&#20110;&#26816;&#32034;&#22686;&#24378;&#29983;&#25104;&#65288;RAG&#65289;&#30340;EIC&#25688;&#35201;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#65288;RAGS4EIC&#65289;&#12290;&#35813;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#19981;&#20165;&#21387;&#32553;&#20449;&#24687;&#65292;&#36824;&#26377;&#25928;&#24341;&#29992;&#30456;&#20851;&#22238;&#22797;&#65292;&#20026;&#21512;&#20316;&#32773;&#25552;&#20379;&#20102;&#37325;&#22823;&#20248;&#21183;&#12290;&#25105;&#20204;&#30340;&#39033;&#30446;&#37319;&#21462;&#20102;&#20004;&#27493;&#26041;&#27861;&#65306;&#39318;&#20808;&#65292;&#26597;&#35810;&#21253;&#21547;&#25152;&#26377;&#30456;&#20851;&#23454;&#39564;&#20449;&#24687;&#30340;&#32508;&#21512;&#21521;&#37327;&#25968;&#25454;&#24211;&#65307;&#20854;&#27425;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26681;&#25454;&#29992;&#25143;&#26597;&#35810;&#21644;&#26816;&#32034;&#25968;&#25454;&#29983;&#25104;&#21253;&#21547;&#24341;&#29992;&#30340;&#31616;&#27905;&#25688;&#35201;&#12290;&#25105;&#20204;&#25551;&#36848;&#20102;&#20351;&#29992;RAG&#35780;&#20272;&#30340;&#35780;&#20272;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15729v1 Announce Type: cross  Abstract: The complexity and sheer volume of information encompassing documents, papers, data, and other resources from large-scale experiments demand significant time and effort to navigate, making the task of accessing and utilizing these varied forms of information daunting, particularly for new collaborators and early-career scientists. To tackle this issue, a Retrieval Augmented Generation (RAG)--based Summarization AI for EIC (RAGS4EIC) is under development. This AI-Agent not only condenses information but also effectively references relevant responses, offering substantial advantages for collaborators. Our project involves a two-step approach: first, querying a comprehensive vector database containing all pertinent experiment information; second, utilizing a Large Language Model (LLM) to generate concise summaries enriched with citations based on user queries and retrieved data. We describe the evaluation methods that use RAG assessments 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21327;&#21516;&#24863;&#30693;&#27169;&#22411;&#21644;&#35777;&#25454;&#29702;&#35770;&#26694;&#26550;&#19979;&#30340;&#32452;&#21512;&#35268;&#21017;&#65292;&#26469;&#25552;&#39640;WSNs&#26816;&#27979;&#33021;&#21147;&#30340;&#23398;&#20064;&#22411;&#20256;&#24863;&#22120;&#37096;&#32626;&#32593;&#32476;&#65288;LSDNet&#65289;&#12290;</title><link>https://arxiv.org/abs/2403.15728</link><description>&lt;p&gt;
&#21487;&#23398;&#20064;&#30340;&#35777;&#25454;&#21327;&#21516;&#24863;&#30693;&#27169;&#22411;&#30340;WSN&#37096;&#32626;
&lt;/p&gt;
&lt;p&gt;
Learnable WSN Deployment of Evidential Collaborative Sensing Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21327;&#21516;&#24863;&#30693;&#27169;&#22411;&#21644;&#35777;&#25454;&#29702;&#35770;&#26694;&#26550;&#19979;&#30340;&#32452;&#21512;&#35268;&#21017;&#65292;&#26469;&#25552;&#39640;WSNs&#26816;&#27979;&#33021;&#21147;&#30340;&#23398;&#20064;&#22411;&#20256;&#24863;&#22120;&#37096;&#32626;&#32593;&#32476;&#65288;LSDNet&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26080;&#32447;&#20256;&#24863;&#22120;&#32593;&#32476;&#65288;WSNs&#65289;&#20013;&#65292;&#35206;&#30422;&#21644;&#37096;&#32626;&#26159;&#36827;&#34892;&#26816;&#27979;&#20219;&#21153;&#26102;&#26368;&#20851;&#38190;&#30340;&#20004;&#20010;&#38382;&#39064;&#12290;&#20294;&#36890;&#24120;&#26469;&#33258;&#20256;&#24863;&#22120;&#30340;&#26816;&#27979;&#20449;&#24687;&#24182;&#27809;&#26377;&#34987;&#20805;&#20998;&#21033;&#29992;&#21644;&#39640;&#25928;&#25972;&#21512;&#12290;&#26412;&#25991;&#26088;&#22312;&#23454;&#29616;WSN&#37096;&#32626;&#30340;&#26368;&#20339;&#35206;&#30422;&#36136;&#37327;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#20256;&#24863;&#22120;&#30340;&#21327;&#21516;&#24863;&#30693;&#27169;&#22411;&#65292;&#21033;&#29992;&#35777;&#25454;&#29702;&#35770;&#26694;&#26550;&#19979;&#30340;&#32452;&#21512;&#35268;&#21017;&#24471;&#21040;&#30340;&#21327;&#21516;&#20449;&#24687;&#26469;&#22686;&#24378;WSNs&#30340;&#26816;&#27979;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15728v1 Announce Type: new  Abstract: In wireless sensor networks (WSNs), coverage and deployment are two most crucial issues when conducting detection tasks. However, the detection information collected from sensors is oftentimes not fully utilized and efficiently integrated. Such sensing model and deployment strategy, thereby, cannot reach the maximum quality of coverage, particularly when the amount of sensors within WSNs expands significantly. In this article, we aim at achieving the optimal coverage quality of WSN deployment. We develop a collaborative sensing model of sensors to enhance detection capabilities of WSNs, by leveraging the collaborative information derived from the combination rule under the framework of evidence theory. In this model, the performance evaluation of evidential fusion systems is adopted as the criterion of the sensor selection. A learnable sensor deployment network (LSDNet) considering both sensor contribution and detection capability, is pr
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;PEaCE&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#20854;&#20013;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#35760;&#24405;&#35780;&#20272;&#20102;&#22522;&#20110;transformer&#30340;OCR&#27169;&#22411;&#22312;&#21270;&#23398;&#25991;&#29486;&#20013;&#30340;&#35782;&#21035;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#21487;&#20197;&#27169;&#25311;&#30495;&#23454;&#35760;&#24405;&#29305;&#24449;&#30340;&#36716;&#25442;&#12290;</title><link>https://arxiv.org/abs/2403.15724</link><description>&lt;p&gt;
PEaCE&#65306;&#29992;&#20110;&#31185;&#23398;&#25991;&#26723;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#30340;&#21270;&#23398;&#23548;&#21521;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
PEaCE: A Chemistry-Oriented Dataset for Optical Character Recognition on Scientific Documents
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15724
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;PEaCE&#25968;&#25454;&#38598;&#65292;&#21033;&#29992;&#20854;&#20013;&#30340;&#21512;&#25104;&#21644;&#30495;&#23454;&#35760;&#24405;&#35780;&#20272;&#20102;&#22522;&#20110;transformer&#30340;OCR&#27169;&#22411;&#22312;&#21270;&#23398;&#25991;&#29486;&#20013;&#30340;&#35782;&#21035;&#25928;&#26524;&#65292;&#24182;&#25552;&#20986;&#21487;&#20197;&#27169;&#25311;&#30495;&#23454;&#35760;&#24405;&#29305;&#24449;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Optical Character Recognition&#65288;OCR&#65289;&#26159;&#19968;&#20010;&#26088;&#22312;&#35782;&#21035;&#22270;&#20687;&#20013;&#23384;&#22312;&#30340;&#25991;&#26412;&#30340;&#26082;&#23450;&#20219;&#21153;&#12290;&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#29616;&#25104;&#30340;OCR&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26159;&#38024;&#23545;&#31185;&#23398;&#65288;&#20363;&#22914;&#65292;&#20844;&#24335;&#65289;&#25110;&#36890;&#29992;&#21360;&#21047;&#33521;&#25991;&#25991;&#26412;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20174;&#21270;&#23398;&#20986;&#29256;&#29289;&#20013;&#25552;&#21462;&#25991;&#26412;&#38656;&#35201;&#19968;&#31181;&#33021;&#22815;&#22312;&#36825;&#20004;&#20010;&#39046;&#22495;&#20013;&#36827;&#34892;&#25805;&#20316;&#30340;OCR&#27169;&#22411;&#12290;&#26368;&#36817;&#30340;&#24037;&#20855;Nougat&#34920;&#29616;&#20986;&#35299;&#26512;&#23398;&#26415;&#25991;&#26723;&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#26080;&#27861;&#35299;&#26512;PubMed&#25991;&#31456;&#20013;&#30340;&#34920;&#26684;&#65292;&#36825;&#26500;&#25104;&#20102;&#23398;&#26415;&#30028;&#19968;&#20010;&#37325;&#35201;&#37096;&#20998;&#65292;&#24182;&#19988;&#20063;&#26159;&#26412;&#27425;&#24037;&#20316;&#30340;&#37325;&#28857;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21253;&#21547;&#21512;&#25104;&#21644;&#30495;&#23454;&#35760;&#24405;&#30340;Printed English and Chemical Equations&#65288;PEaCE&#65289;&#25968;&#25454;&#38598;&#65292;&#24182;&#35780;&#20272;&#20102;&#24403;&#36825;&#19968;&#36164;&#28304;&#36827;&#34892;&#35757;&#32451;&#26102;&#65292;&#22522;&#20110;transformer&#30340;OCR&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;&#37492;&#20110;&#30495;&#23454;&#35760;&#24405;&#21253;&#21547;&#21512;&#25104;&#35760;&#24405;&#20013;&#19981;&#23384;&#22312;&#30340;&#20154;&#24037;&#21046;&#21697;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#27169;&#20223;&#36825;&#20123;&#29305;&#36136;&#30340;&#36716;&#25442;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15724v1 Announce Type: cross  Abstract: Optical Character Recognition (OCR) is an established task with the objective of identifying the text present in an image. While many off-the-shelf OCR models exist, they are often trained for either scientific (e.g., formulae) or generic printed English text. Extracting text from chemistry publications requires an OCR model that is capable in both realms. Nougat, a recent tool, exhibits strong ability to parse academic documents, but is unable to parse tables in PubMed articles, which comprises a significant part of the academic community and is the focus of this work. To mitigate this gap, we present the Printed English and Chemical Equations (PEaCE) dataset, containing both synthetic and real-world records, and evaluate the efficacy of transformer-based OCR models when trained on this resource. Given that real-world records contain artifacts not present in synthetic records, we propose transformations that mimic such qualities. We p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#21551;&#21457;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#24335;&#20272;&#35745;&#22120;&#12289;&#36816;&#21160;&#23398;&#36319;&#36394;&#25511;&#21046;&#21644;&#23398;&#20064;&#40065;&#26834;&#21160;&#24577;&#25511;&#21046;&#22120;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;&#20998;&#24067;&#24335;&#32534;&#38431;&#25511;&#21046;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15716</link><description>&lt;p&gt;
&#22522;&#20110;&#29983;&#29289;&#21551;&#21457;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#32534;&#38431;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Distributed Robust Learning based Formation Control of Mobile Robots based on Bioinspired Neural Dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15716
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29983;&#29289;&#21551;&#21457;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#24335;&#20272;&#35745;&#22120;&#12289;&#36816;&#21160;&#23398;&#36319;&#36394;&#25511;&#21046;&#21644;&#23398;&#20064;&#40065;&#26834;&#21160;&#24577;&#25511;&#21046;&#22120;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;&#20998;&#24067;&#24335;&#32534;&#38431;&#25511;&#21046;&#30340;&#25361;&#25112;&#65292;&#24182;&#36890;&#36807;&#25968;&#23398;&#20998;&#26512;&#35777;&#26126;&#20102;&#26041;&#27861;&#30340;&#31283;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#22810;&#20010;&#31227;&#21160;&#26426;&#22120;&#20154;&#20998;&#24067;&#24335;&#32534;&#38431;&#25511;&#21046;&#30340;&#25361;&#25112;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22686;&#24378;&#23454;&#38469;&#21487;&#34892;&#24615;&#30340;&#26032;&#26041;&#27861;&#12290;&#39318;&#20808;&#65292;&#37319;&#29992;&#21464;&#32467;&#26500;&#21644;&#32423;&#32852;&#35774;&#35745;&#25216;&#26415;&#24341;&#20837;&#20102;&#19968;&#20010;&#20998;&#24067;&#24335;&#20272;&#35745;&#22120;&#65292;&#28040;&#38500;&#20102;&#23545;&#23548;&#25968;&#20449;&#24687;&#30340;&#38656;&#27714;&#20197;&#25552;&#39640;&#23454;&#26102;&#24615;&#33021;&#12290;&#28982;&#21518;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#36816;&#21160;&#23398;&#36319;&#36394;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#20102;&#22522;&#20110;&#29983;&#29289;&#21551;&#21457;&#31070;&#32463;&#21160;&#21147;&#23398;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#20379;&#24179;&#28369;&#30340;&#25511;&#21046;&#36755;&#20837;&#24182;&#26377;&#25928;&#35299;&#20915;&#36895;&#24230;&#36339;&#36291;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#35299;&#20915;&#23436;&#20840;&#26410;&#30693;&#21160;&#24577;&#21644;&#25200;&#21160;&#26465;&#20214;&#19979;&#36816;&#34892;&#30340;&#26426;&#22120;&#20154;&#30340;&#25361;&#25112;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#40065;&#26834;&#21160;&#24577;&#25511;&#21046;&#22120;&#12290;&#35813;&#25511;&#21046;&#22120;&#22312;&#20445;&#25345;&#20854;&#23545;&#25239;&#25200;&#21160;&#24615;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#23454;&#26102;&#21442;&#25968;&#20272;&#35745;&#12290;&#25152;&#25552;&#20986;&#26041;&#27861;&#30340;&#25972;&#20307;&#31283;&#23450;&#24615;&#36890;&#36807;&#20005;&#26684;&#30340;&#25968;&#23398;&#20998;&#26512;&#24471;&#21040;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15716v1 Announce Type: cross  Abstract: This paper addresses the challenges of distributed formation control in multiple mobile robots, introducing a novel approach that enhances real-world practicability. We first introduce a distributed estimator using a variable structure and cascaded design technique, eliminating the need for derivative information to improve the real time performance. Then, a kinematic tracking control method is developed utilizing a bioinspired neural dynamic-based approach aimed at providing smooth control inputs and effectively resolving the speed jump issue. Furthermore, to address the challenges for robots operating with completely unknown dynamics and disturbances, a learning-based robust dynamic controller is developed. This controller provides real time parameter estimates while maintaining its robustness against disturbances. The overall stability of the proposed method is proved with rigorous mathematical analysis. At last, multiple comprehens
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CATMO&#65292;&#36890;&#36807;&#25972;&#21512;&#29289;&#29702;&#25509;&#35302;&#20449;&#24687;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#35270;&#35273;&#33258;&#28982;&#19988;&#29289;&#29702;&#21512;&#29702;&#30340;3D&#20154;&#20307;&#21160;&#20316;&#12290;</title><link>https://arxiv.org/abs/2403.15709</link><description>&lt;p&gt;
&#20174;&#25991;&#26412;&#25551;&#36848;&#29983;&#25104;&#32771;&#34385;&#25509;&#35302;&#30340;&#20154;&#20307;&#21160;&#20316;
&lt;/p&gt;
&lt;p&gt;
Contact-aware Human Motion Generation from Textual Descriptions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15709
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;CATMO&#65292;&#36890;&#36807;&#25972;&#21512;&#29289;&#29702;&#25509;&#35302;&#20449;&#24687;&#65292;&#20174;&#25991;&#26412;&#25551;&#36848;&#20013;&#29983;&#25104;&#35270;&#35273;&#33258;&#28982;&#19988;&#29289;&#29702;&#21512;&#29702;&#30340;3D&#20154;&#20307;&#21160;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#20174;&#25991;&#26412;&#29983;&#25104;3D&#20132;&#20114;&#24335;&#20154;&#20307;&#21160;&#20316;&#30340;&#38382;&#39064;&#12290;&#32473;&#23450;&#25551;&#36848;&#20102;&#19981;&#21516;&#36523;&#20307;&#37096;&#20301;&#25509;&#35302;&#29289;&#20307;&#21160;&#20316;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#25105;&#20204;&#32508;&#21512;&#29983;&#25104;&#35270;&#35273;&#33258;&#28982;&#19988;&#29289;&#29702;&#21512;&#29702;&#30340;3D&#20154;&#20307;&#23039;&#21183;&#24207;&#21015;&#12290;&#28982;&#32780;&#65292;&#36825;&#20010;&#20219;&#21153;&#23384;&#22312;&#19968;&#20010;&#37325;&#35201;&#25361;&#25112;&#65292;&#21363;&#22312;&#21160;&#20316;&#21644;&#25991;&#26412;&#25551;&#36848;&#20013;&#23545;&#29289;&#29702;&#25509;&#35302;&#30340;&#20114;&#21160;&#32771;&#34385;&#19981;&#36275;&#65292;&#23548;&#33268;&#24207;&#21015;&#19981;&#33258;&#28982;&#19988;&#19981;&#21512;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#21517;&#20026;RICH-CAT&#30340;&#26032;&#25968;&#25454;&#38598;&#65292;&#34920;&#31034;&#20174;RICH&#25968;&#25454;&#38598;&#26500;&#24314;&#30340;&#8220;&#32771;&#34385;&#25509;&#35302;&#8221;&#30340;&#25991;&#26412;&#12290;RICH-CAT&#21253;&#25324;&#39640;&#36136;&#37327;&#21160;&#20316;&#12289;&#20934;&#30830;&#30340;&#20154;-&#29289;&#25509;&#35302;&#26631;&#31614;&#21644;&#35814;&#32454;&#30340;&#25991;&#26412;&#25551;&#36848;&#65292;&#28085;&#30422;&#20102;26&#31181;&#23460;&#20869;/&#23460;&#22806;&#21160;&#20316;&#30340;8500&#22810;&#23545;&#21160;&#20316;-&#25991;&#26412;&#37197;&#23545;&#12290;&#21033;&#29992;RICH-CAT&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CATMO&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#25991;&#26412;&#39537;&#21160;&#30340;&#20132;&#20114;&#24335;&#20154;&#20307;&#21160;&#20316;&#21512;&#25104;&#65292;&#26126;&#30830;&#25972;&#21512;&#20102;&#29289;&#29702;&#25509;&#35302;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15709v1 Announce Type: cross  Abstract: This paper addresses the problem of generating 3D interactive human motion from text. Given a textual description depicting the actions of different body parts in contact with objects, we synthesize sequences of 3D body poses that are visually natural and physically plausible. Yet, this task poses a significant challenge due to the inadequate consideration of interactions by physical contacts in both motion and textual descriptions, leading to unnatural and implausible sequences. To tackle this challenge, we create a novel dataset named RICH-CAT, representing ``Contact-Aware Texts'' constructed from the RICH dataset. RICH-CAT comprises high-quality motion, accurate human-object contact labels, and detailed textual descriptions, encompassing over 8,500 motion-text pairs across 26 indoor/outdoor actions. Leveraging RICH-CAT, we propose a novel approach named CATMO for text-driven interactive human motion synthesis that explicitly integra
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#26032;&#30340;Dynamic Signal Distribution (DSD)&#20998;&#31867;&#20219;&#21153;&#65292;&#27169;&#25311;&#22270;&#20687;&#30001;$k$&#20010;&#32500;&#24230;&#20026;$d$&#30340;&#34917;&#19969;&#32452;&#25104;&#65292;&#20197;&#35299;&#20915;CNNs&#30456;&#23545;&#20110;LCNs&#21644;FCNs&#30340;&#32479;&#35745;&#20248;&#21183;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.15707</link><description>&lt;p&gt;
&#22320;&#22495;&#24615;&#21644;&#26435;&#37325;&#20849;&#20139;&#22312;&#22522;&#20110;&#22270;&#20687;&#30340;&#20219;&#21153;&#20013;&#30340;&#20316;&#29992;&#65306;CNN&#12289;LCN&#21644;FCN&#20043;&#38388;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
Role of Locality and Weight Sharing in Image-Based Tasks: A Sample Complexity Separation between CNNs, LCNs, and FCNs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15707
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#26032;&#30340;Dynamic Signal Distribution (DSD)&#20998;&#31867;&#20219;&#21153;&#65292;&#27169;&#25311;&#22270;&#20687;&#30001;$k$&#20010;&#32500;&#24230;&#20026;$d$&#30340;&#34917;&#19969;&#32452;&#25104;&#65292;&#20197;&#35299;&#20915;CNNs&#30456;&#23545;&#20110;LCNs&#21644;FCNs&#30340;&#32479;&#35745;&#20248;&#21183;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#20219;&#21153;&#30340;&#29305;&#28857;&#26159;&#22320;&#22495;&#24615;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#12290;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#22312;&#36825;&#20123;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36825;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#24402;&#22240;&#20110;&#20854;&#26550;&#26500;&#20013;&#22266;&#26377;&#30340;&#22320;&#22495;&#24615;&#21644;&#26435;&#37325;&#20849;&#20139;&#30340;&#24402;&#32435;&#20559;&#24046;&#12290;&#29616;&#26377;&#30340;&#35797;&#22270;&#37327;&#21270;&#36825;&#20123;&#20559;&#24046;&#22312;CNNs&#19978;&#30456;&#23545;&#20110;&#23616;&#37096;&#36830;&#25509;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;LCNs&#65289;&#21644;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FCNs&#65289;&#30340;&#32479;&#35745;&#20248;&#21183;&#30340;&#23581;&#35797;&#21487;&#20197;&#24402;&#20026;&#20197;&#19979;&#20960;&#31867;&#65306;&#35201;&#20040;&#23427;&#20204;&#24573;&#35270;&#20248;&#21270;&#22120;&#65292;&#20165;&#25552;&#20379;&#20855;&#26377;&#32479;&#19968;&#25910;&#25947;&#19978;&#30028;&#20294;&#27809;&#26377;&#20998;&#38548;&#19979;&#30028;&#30340;&#32479;&#35745;&#25910;&#25947;&#24615;&#65292;&#35201;&#20040;&#32771;&#34385;&#21040;&#19981;&#30495;&#23454;&#22320;&#21453;&#26144;&#29616;&#23454;&#19990;&#30028;&#35270;&#35273;&#20219;&#21153;&#20013;&#30340;&#22320;&#22495;&#24615;&#21644;&#24179;&#31227;&#19981;&#21464;&#24615;&#30340;&#31616;&#21333;&#20219;&#21153;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#19981;&#36275;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#21160;&#24577;&#20449;&#21495;&#20998;&#24067;&#65288;DSD&#65289;&#20998;&#31867;&#20219;&#21153;&#65292;&#23427;&#23558;&#22270;&#20687;&#24314;&#27169;&#20026;&#21253;&#21547;$k$&#20010;&#23610;&#23544;&#20026;$d$&#30340;&#34917;&#19969;&#65292;&#26631;&#31614;&#26159;de
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15707v1 Announce Type: cross  Abstract: Vision tasks are characterized by the properties of locality and translation invariance. The superior performance of convolutional neural networks (CNNs) on these tasks is widely attributed to the inductive bias of locality and weight sharing baked into their architecture. Existing attempts to quantify the statistical benefits of these biases in CNNs over locally connected convolutional neural networks (LCNs) and fully connected neural networks (FCNs) fall into one of the following categories: either they disregard the optimizer and only provide uniform convergence upper bounds with no separating lower bounds, or they consider simplistic tasks that do not truly mirror the locality and translation invariance as found in real-world vision tasks. To address these deficiencies, we introduce the Dynamic Signal Distribution (DSD) classification task that models an image as consisting of $k$ patches, each of dimension $d$, and the label is de
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#31243;&#24207;&#21270;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;SceneX&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;</title><link>https://arxiv.org/abs/2403.15698</link><description>&lt;p&gt;
SceneX&#65306;&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#31243;&#24207;&#21270;&#21487;&#25511;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SceneX:Procedural Controllable Large-scale Scene Generation via Large-language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15698
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#31243;&#24207;&#21270;&#24314;&#27169;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;SceneX&#65292;&#21487;&#20197;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#24040;&#22823;&#30340;&#24212;&#29992;&#28508;&#21147;&#65292;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#24378;&#22823;&#30340;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#25152;&#38656;&#30340;&#22330;&#26223;&#65292;&#24182;&#21462;&#24471;&#20102;&#20196;&#20154;&#26399;&#24453;&#30340;&#32467;&#26524;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#36825;&#20123;&#26041;&#27861;&#20351;&#29992;&#19981;&#20860;&#23481;&#24037;&#19994;&#27969;&#31243;&#30340;3D&#22522;&#20803;&#65288;&#22914;&#28857;&#20113;&#25110;&#36752;&#23556;&#22330;&#65289;&#26469;&#34920;&#31034;&#22330;&#26223;&#65292;&#36825;&#23548;&#33268;&#23398;&#26415;&#30740;&#31350;&#19982;&#24037;&#19994;&#37096;&#32626;&#20043;&#38388;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#12290;&#31243;&#24207;&#21270;&#21487;&#25511;&#29983;&#25104;&#65288;PCG&#65289;&#26159;&#19968;&#31181;&#39640;&#25928;&#30340;&#25216;&#26415;&#65292;&#21487;&#21019;&#24314;&#21487;&#25193;&#23637;&#21644;&#39640;&#36136;&#37327;&#30340;&#36164;&#20135;&#65292;&#20294;&#23545;&#26222;&#36890;&#29992;&#25143;&#19981;&#21451;&#22909;&#65292;&#22240;&#20026;&#23427;&#38656;&#35201;&#28145;&#20837;&#30340;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#39537;&#21160;&#31243;&#24207;&#21270;&#24314;&#27169;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#22330;&#26223;&#29983;&#25104;&#26694;&#26550;SceneX&#65292;&#21487;&#20197;&#26681;&#25454;&#35774;&#35745;&#24072;&#30340;&#25991;&#26412;&#25551;&#36848;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#31243;&#24207;&#21270;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15698v1 Announce Type: cross  Abstract: Due to its great application potential, large-scale scene generation has drawn extensive attention in academia and industry. Recent research employs powerful generative models to create desired scenes and achieves promising results. However, most of these methods represent the scene using 3D primitives (e.g. point cloud or radiance field) incompatible with the industrial pipeline, which leads to a substantial gap between academic research and industrial deployment. Procedural Controllable Generation (PCG) is an efficient technique for creating scalable and high-quality assets, but it is unfriendly for ordinary users as it demands profound domain expertise. To address these issues, we resort to using the large language model (LLM) to drive the procedural modeling. In this paper, we introduce a large-scale scene generation framework, SceneX, which can automatically produce high-quality procedural models according to designers' textual de
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;MixRE&#65292;&#24182;&#26500;&#24314;&#20102;&#25903;&#25345;&#35813;&#20219;&#21153;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;MixRED&#65292;&#22635;&#34917;&#20102;&#22810;&#35821;&#35328;&#24773;&#26223;&#19979;&#20851;&#31995;&#25277;&#21462;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;</title><link>https://arxiv.org/abs/2403.15696</link><description>&lt;p&gt;
MixRED: &#19968;&#20010;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
MixRED: A Mix-lingual Relation Extraction Dataset
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15696
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#35821;&#35328;&#20851;&#31995;&#25277;&#21462;&#20219;&#21153;MixRE&#65292;&#24182;&#26500;&#24314;&#20102;&#25903;&#25345;&#35813;&#20219;&#21153;&#30340;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;MixRED&#65292;&#22635;&#34917;&#20102;&#22810;&#35821;&#35328;&#24773;&#26223;&#19979;&#20851;&#31995;&#25277;&#21462;&#30740;&#31350;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15696v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25688;&#35201;&#65306;&#20851;&#31995;&#25277;&#21462;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#20219;&#21153;&#65292;&#20855;&#26377;&#35768;&#22810;&#29616;&#23454;&#19990;&#30028;&#30340;&#24212;&#29992;&#12290;&#29616;&#26377;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#21333;&#35821;&#20851;&#31995;&#25277;&#21462;&#25110;&#29992;&#20110;&#20851;&#31995;&#25277;&#21462;&#30340;&#36328;&#35821;&#35328;&#22686;&#24378;&#12290;&#28982;&#32780;&#65292;&#22312;&#22810;&#35821;&#35328;&#65288;&#25110;&#20195;&#30721;&#28151;&#21512;&#65289;&#22330;&#26223;&#20013;&#65292;&#20173;&#23384;&#22312;&#23545;&#20851;&#31995;&#25277;&#21462;&#30340;&#29702;&#35299;&#23384;&#22312;&#37325;&#22823;&#24046;&#36317;&#65292;&#22312;&#35813;&#22330;&#26223;&#20013;&#65292;&#20010;&#20307;&#22312;&#21477;&#23376;&#20013;&#28151;&#21512;&#26469;&#33258;&#19981;&#21516;&#35821;&#35328;&#30340;&#20869;&#23481;&#65292;&#29983;&#25104;&#22810;&#35821;&#20869;&#23481;&#12290;&#30001;&#20110;&#32570;&#20047;&#19987;&#38376;&#30340;&#25968;&#25454;&#38598;&#65292;&#29616;&#26377;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23578;&#26410;&#25506;&#35752;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#39033;&#26032;&#30340;&#20219;&#21153;&#65292;&#21363;&#32771;&#34385;&#22810;&#35821;&#35328;&#24773;&#26223;&#20013;&#30340;&#20851;&#31995;&#25277;&#21462;&#65292;&#31216;&#20026;MixRE&#65292;&#24182;&#26500;&#24314;&#20102;&#20154;&#24037;&#27880;&#37322;&#25968;&#25454;&#38598;MixRED&#20197;&#25903;&#25345;&#27492;&#20219;&#21153;&#12290;&#38500;&#20102;&#26500;&#24314;MixRED&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;&#30417;&#30563;&#27169;&#22411;&#21644;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15696v1 Announce Type: new  Abstract: Relation extraction is a critical task in the field of natural language processing with numerous real-world applications. Existing research primarily focuses on monolingual relation extraction or cross-lingual enhancement for relation extraction. Yet, there remains a significant gap in understanding relation extraction in the mix-lingual (or code-switching) scenario, where individuals intermix contents from different languages within sentences, generating mix-lingual content. Due to the lack of a dedicated dataset, the effectiveness of existing relation extraction models in such a scenario is largely unexplored. To address this issue, we introduce a novel task of considering relation extraction in the mix-lingual scenario called MixRE and constructing the human-annotated dataset MixRED to support this task. In addition to constructing the MixRED dataset, we evaluate both state-of-the-art supervised models and large language models (LLMs)
&lt;/p&gt;</description></item><item><title>EAGLE&#25552;&#20986;&#20102;&#19968;&#20010;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#20174;&#26087;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#23398;&#20064;&#29305;&#24449;&#30340;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#26816;&#27979;&#20986;&#26410;&#30693;&#30446;&#26631;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;</title><link>https://arxiv.org/abs/2403.15690</link><description>&lt;p&gt;
EAGLE&#65306;&#38754;&#21521;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#30340;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
EAGLE: A Domain Generalization Framework for AI-generated Text Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15690
&lt;/p&gt;
&lt;p&gt;
EAGLE&#25552;&#20986;&#20102;&#19968;&#20010;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#65292;&#33021;&#22815;&#21033;&#29992;&#20174;&#26087;&#35821;&#35328;&#27169;&#22411;&#20013;&#33719;&#24471;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#23398;&#20064;&#29305;&#24449;&#30340;&#19981;&#21464;&#24615;&#65292;&#20174;&#32780;&#26816;&#27979;&#20986;&#26410;&#30693;&#30446;&#26631;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33021;&#21147;&#30340;&#25552;&#21319;&#65292;&#36127;&#36131;&#20219;&#21644;&#23433;&#20840;&#20351;&#29992;&#36825;&#20123;LLMs&#30340;&#19968;&#20010;&#37325;&#35201;&#27493;&#39588;&#26159;&#33021;&#22815;&#26816;&#27979;&#36825;&#20123;&#27169;&#22411;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;&#23613;&#31649;&#30417;&#30563;&#24335;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#26816;&#27979;&#22120;&#22312;&#26087;LLMs&#29983;&#25104;&#30340;&#25991;&#26412;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#38543;&#30528;&#26032;LLMs&#30340;&#39057;&#32321;&#21457;&#24067;&#65292;&#26500;&#24314;&#29992;&#20110;&#35782;&#21035;&#36825;&#20123;&#26032;&#27169;&#22411;&#25991;&#26412;&#30340;&#30417;&#30563;&#26816;&#27979;&#22120;&#23558;&#38656;&#35201;&#26032;&#30340;&#26631;&#35760;&#35757;&#32451;&#25968;&#25454;&#65292;&#22312;&#23454;&#36341;&#20013;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#26816;&#27979;&#26469;&#33258;&#26410;&#30693;&#30446;&#26631;&#29983;&#25104;&#22120;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#39046;&#22495;&#27867;&#21270;&#26694;&#26550;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;EAGLE&#21033;&#29992;&#36804;&#20170;&#20026;&#27490;&#20174;&#26087;&#35821;&#35328;&#27169;&#22411;&#33719;&#24471;&#30340;&#26631;&#35760;&#25968;&#25454;&#65292;&#24182;&#23398;&#20064;&#36328;&#36825;&#20123;&#29983;&#25104;&#22120;&#19981;&#21464;&#30340;&#29305;&#24449;&#65292;&#20197;&#20415;&#26816;&#27979;&#30001;&#26410;&#30693;&#30446;&#26631;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#25991;&#26412;&#12290;EAGLE&#36890;&#36807;&#32467;&#21512;&#33258;&#30417;&#30563;&#30340;&#34920;&#24449;&#33021;&#21147;&#26469;&#23398;&#20064;&#36825;&#31181;&#39046;&#22495;&#19981;&#21464;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15690v1 Announce Type: cross  Abstract: With the advancement in capabilities of Large Language Models (LLMs), one major step in the responsible and safe use of such LLMs is to be able to detect text generated by these models. While supervised AI-generated text detectors perform well on text generated by older LLMs, with the frequent release of new LLMs, building supervised detectors for identifying text from such new models would require new labeled training data, which is infeasible in practice. In this work, we tackle this problem and propose a domain generalization framework for the detection of AI-generated text from unseen target generators. Our proposed framework, EAGLE, leverages the labeled data that is available so far from older language models and learns features invariant across these generators, in order to detect text generated by an unknown target generator. EAGLE learns such domain-invariant features by combining the representational power of self-supervised 
&lt;/p&gt;</description></item><item><title>SRLM &#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#24335;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#65292;&#36890;&#36807;&#23454;&#26102;&#30340;&#20154;&#31867;&#35821;&#35328;&#25351;&#20196;&#25512;&#26029;&#20840;&#23616;&#35268;&#21010;&#65292;&#24182;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#25552;&#20379;&#22810;&#31181;&#31038;&#20132;&#26381;&#21153;&#65292;&#34920;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15648</link><description>&lt;p&gt;
SRLM: &#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#20154;&#26426;&#20132;&#20114;&#24335;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
SRLM: Human-in-Loop Interactive Social Robot Navigation with Large Language Model and Deep Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15648
&lt;/p&gt;
&lt;p&gt;
SRLM &#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#21512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#20154;&#26426;&#20132;&#20114;&#24335;&#31038;&#20132;&#26426;&#22120;&#20154;&#23548;&#33322;&#65292;&#36890;&#36807;&#23454;&#26102;&#30340;&#20154;&#31867;&#35821;&#35328;&#25351;&#20196;&#25512;&#26029;&#20840;&#23616;&#35268;&#21010;&#65292;&#24182;&#22312;&#20844;&#20849;&#31354;&#38388;&#20013;&#25552;&#20379;&#22810;&#31181;&#31038;&#20132;&#26381;&#21153;&#65292;&#34920;&#29616;&#20986;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19968;&#21517;&#20132;&#20114;&#24335;&#31038;&#20132;&#26426;&#22120;&#20154;&#21161;&#25163;&#24517;&#39035;&#22312;&#22797;&#26434;&#25317;&#25380;&#30340;&#31354;&#38388;&#20013;&#25552;&#20379;&#26381;&#21153;&#65292;&#26681;&#25454;&#23454;&#26102;&#30340;&#20154;&#31867;&#35821;&#35328;&#25351;&#20196;&#25110;&#21453;&#39304;&#35843;&#25972;&#20854;&#34892;&#20026;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Social Robot Planner (SRLM) &#30340;&#26032;&#22411;&#28151;&#21512;&#26041;&#27861;&#65292;&#23427;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#25972;&#21512;&#36215;&#26469;&#65292;&#20197;&#22312;&#20805;&#26021;&#30528;&#20154;&#32676;&#30340;&#20844;&#20849;&#31354;&#38388;&#20013;&#23548;&#33322;&#65292;&#24182;&#25552;&#20379;&#22810;&#31181;&#31038;&#20132;&#26381;&#21153;&#12290;SRLM &#36890;&#36807;&#23454;&#26102;&#30340;&#20154;&#26426;&#20132;&#20114;&#25351;&#20196;&#25512;&#26029;&#20840;&#23616;&#35268;&#21010;&#65292;&#24182;&#23558;&#31038;&#20132;&#20449;&#24687;&#32534;&#30721;&#21040;&#22522;&#20110;LLM&#30340;&#22823;&#22411;&#23548;&#33322;&#27169;&#22411;&#65288;LNM&#65289;&#20013;&#65292;&#29992;&#20110;&#20302;&#23618;&#27425;&#30340;&#36816;&#21160;&#25191;&#34892;&#12290;&#27492;&#22806;&#65292;&#35774;&#35745;&#20102;&#19968;&#20010;&#22522;&#20110;DRL&#30340;&#35268;&#21010;&#22120;&#26469;&#20445;&#25345;&#22522;&#20934;&#24615;&#33021;&#65292;&#36890;&#36807;&#22823;&#22411;&#21453;&#39304;&#27169;&#22411;&#65288;LFM&#65289;&#19982;LNM&#34701;&#21512;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#25991;&#26412;&#21644;LLM&#39537;&#21160;&#30340;LNM&#30340;&#19981;&#31283;&#23450;&#24615;&#12290;&#26368;&#21518;&#65292;SRLM &#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#20013;&#23637;&#31034;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#26377;&#20851;&#27492;&#24037;&#20316;&#30340;&#26356;&#22810;&#35814;&#32454;&#20449;&#24687;&#65292;&#35831;&#35775;&#38382;&#65306;https://sites.g
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15648v1 Announce Type: cross  Abstract: An interactive social robotic assistant must provide services in complex and crowded spaces while adapting its behavior based on real-time human language commands or feedback. In this paper, we propose a novel hybrid approach called Social Robot Planner (SRLM), which integrates Large Language Models (LLM) and Deep Reinforcement Learning (DRL) to navigate through human-filled public spaces and provide multiple social services. SRLM infers global planning from human-in-loop commands in real-time, and encodes social information into a LLM-based large navigation model (LNM) for low-level motion execution. Moreover, a DRL-based planner is designed to maintain benchmarking performance, which is blended with LNM by a large feedback model (LFM) to address the instability of current text and LLM-driven LNM. Finally, SRLM demonstrates outstanding performance in extensive experiments. More details about this work are available at: https://sites.g
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#23558;&#22269;&#23478;&#26631;&#20934;&#19982;&#25216;&#26415;&#30740;&#31350;&#25152;&#30340;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;&#31649;&#29702;&#26694;&#26550;&#65288;NIST AI RMF&#65289;&#24212;&#29992;&#20110;&#30417;&#25511;&#25216;&#26415;&#39046;&#22495;&#30340;&#24847;&#20041;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#30340;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#65292;&#26088;&#22312;&#25512;&#21160;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#23454;&#36341;&#12290;</title><link>https://arxiv.org/abs/2403.15646</link><description>&lt;p&gt;
&#23558;NIST&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;&#31649;&#29702;&#26694;&#26550;&#24212;&#29992;&#20110;&#30417;&#25511;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Application of the NIST AI Risk Management Framework to Surveillance Technology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#20102;&#23558;&#22269;&#23478;&#26631;&#20934;&#19982;&#25216;&#26415;&#30740;&#31350;&#25152;&#30340;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;&#31649;&#29702;&#26694;&#26550;&#65288;NIST AI RMF&#65289;&#24212;&#29992;&#20110;&#30417;&#25511;&#25216;&#26415;&#39046;&#22495;&#30340;&#24847;&#20041;&#65292;&#25552;&#20986;&#20102;&#38024;&#23545;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#30340;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#65292;&#26088;&#22312;&#25512;&#21160;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#23454;&#36341;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#28145;&#20837;&#20998;&#26512;&#20102;&#22269;&#23478;&#26631;&#20934;&#19982;&#25216;&#26415;&#30740;&#31350;&#25152;&#30340;&#20154;&#24037;&#26234;&#33021;&#39118;&#38505;&#31649;&#29702;&#26694;&#26550;&#65288;NIST AI RMF&#65289;&#22312;&#30417;&#25511;&#25216;&#26415;&#39046;&#22495;&#65292;&#29305;&#21035;&#26159;&#38754;&#37096;&#35782;&#21035;&#25216;&#26415;&#20013;&#30340;&#24212;&#29992;&#21644;&#24433;&#21709;&#12290;&#37492;&#20110;&#38754;&#37096;&#35782;&#21035;&#31995;&#32479;&#30340;&#22266;&#26377;&#39640;&#39118;&#38505;&#21644;&#37325;&#35201;&#24615;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24378;&#35843;&#20102;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#36827;&#34892;&#39118;&#38505;&#31649;&#29702;&#30340;&#32467;&#26500;&#21270;&#26041;&#27861;&#30340;&#36843;&#20999;&#38656;&#35201;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#35814;&#32454;&#30340;&#26696;&#20363;&#30740;&#31350;&#65292;&#23637;&#31034;&#20102;NIST AI RMF&#22312;&#35782;&#21035;&#21644;&#32531;&#35299;&#36825;&#20123;&#25216;&#26415;&#20013;&#21487;&#33021;&#34987;&#24573;&#35270;&#30340;&#39118;&#38505;&#26041;&#38754;&#30340;&#23454;&#29992;&#24615;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#21046;&#23450;&#19968;&#20010;&#20840;&#38754;&#30340;&#39118;&#38505;&#31649;&#29702;&#31574;&#30053;&#65292;&#25512;&#21160;&#36127;&#36131;&#20219;&#30340;&#20154;&#24037;&#26234;&#33021;&#21033;&#29992;&#23454;&#36341;&#20197;&#21487;&#34892;&#12289;&#21487;&#25193;&#23637;&#30340;&#26041;&#24335;&#36827;&#34892;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;&#30417;&#25511;&#25216;&#26415;&#29305;&#23450;&#25361;&#25112;&#30340;&#20845;&#27493;&#27969;&#31243;&#65292;&#26088;&#22312;&#20135;&#29983;&#26356;&#31995;&#32479;&#21270;&#12289;&#26356;&#26377;&#25928;&#30340;&#39118;&#38505;&#31649;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15646v1 Announce Type: cross  Abstract: This study offers an in-depth analysis of the application and implications of the National Institute of Standards and Technology's AI Risk Management Framework (NIST AI RMF) within the domain of surveillance technologies, particularly facial recognition technology. Given the inherently high-risk and consequential nature of facial recognition systems, our research emphasizes the critical need for a structured approach to risk management in this sector. The paper presents a detailed case study demonstrating the utility of the NIST AI RMF in identifying and mitigating risks that might otherwise remain unnoticed in these technologies. Our primary objective is to develop a comprehensive risk management strategy that advances the practice of responsible AI utilization in feasible, scalable ways. We propose a six-step process tailored to the specific challenges of surveillance technology that aims to produce a more systematic and effective ri
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;Contextual Restless Bandits (CRB)&#26694;&#26550;&#65292;&#33021;&#22815;&#21516;&#26102;&#24314;&#27169;&#27599;&#20010;&#33218;&#30340;&#20869;&#37096;&#29366;&#24577;&#36716;&#25442;&#21644;&#22806;&#37096;&#20840;&#23616;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#30340;&#25351;&#25968;&#31574;&#30053;&#31639;&#27861;&#35299;&#20915;CRB&#38382;&#39064;&#65292;&#24182;&#22312;&#38656;&#27714;&#21709;&#24212;&#20915;&#31574;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;</title><link>https://arxiv.org/abs/2403.15640</link><description>&lt;p&gt;
&#20855;&#26377;&#24212;&#29992;&#20110;&#38656;&#27714;&#21709;&#24212;&#20915;&#31574;&#30340;&#32972;&#26223;&#19979;&#22810;&#33218;&#32769;&#34382;&#26426;
&lt;/p&gt;
&lt;p&gt;
Contextual Restless Multi-Armed Bandits with Application to Demand Response Decision-Making
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15640
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;Contextual Restless Bandits (CRB)&#26694;&#26550;&#65292;&#33021;&#22815;&#21516;&#26102;&#24314;&#27169;&#27599;&#20010;&#33218;&#30340;&#20869;&#37096;&#29366;&#24577;&#36716;&#25442;&#21644;&#22806;&#37096;&#20840;&#23616;&#29615;&#22659;&#30340;&#24433;&#21709;&#65292;&#25552;&#20986;&#20102;&#21487;&#25193;&#23637;&#30340;&#25351;&#25968;&#31574;&#30053;&#31639;&#27861;&#35299;&#20915;CRB&#38382;&#39064;&#65292;&#24182;&#22312;&#38656;&#27714;&#21709;&#24212;&#20915;&#31574;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#33218;&#32769;&#34382;&#26426;&#26694;&#26550;&#65292;&#31216;&#20026;&#32972;&#26223;&#19979;&#19981;&#23433;&#38745;&#30340;&#32769;&#34382;&#26426;&#65288;CRB&#65289;&#65292;&#29992;&#20110;&#22797;&#26434;&#30340;&#22312;&#32447;&#20915;&#31574;&#12290;&#36825;&#31181;CRB&#26694;&#26550;&#34701;&#21512;&#20102;&#19978;&#19979;&#25991;&#32769;&#34382;&#26426;&#21644;&#19981;&#23433;&#38745;&#32769;&#34382;&#26426;&#30340;&#26680;&#24515;&#29305;&#24449;&#65292;&#22240;&#27492;&#21487;&#20197;&#27169;&#25311;&#27599;&#21482;&#33218;&#30340;&#20869;&#37096;&#29366;&#24577;&#36716;&#25442;&#21644;&#22806;&#37096;&#20840;&#23616;&#29615;&#22659;&#19978;&#19979;&#25991;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#21452;&#37325;&#20998;&#35299;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#25351;&#25968;&#31574;&#30053;&#31639;&#27861;&#65292;&#29992;&#20110;&#35299;&#20915;CRB&#38382;&#39064;&#65292;&#24182;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;&#35813;&#31639;&#27861;&#30340;&#28176;&#36817;&#26368;&#20248;&#24615;&#12290;&#22312;&#33218;&#27169;&#22411;&#26410;&#30693;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25351;&#25968;&#31574;&#30053;&#30340;&#27169;&#22411;&#20026;&#22522;&#30784;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#29992;&#20110;&#21516;&#26102;&#23398;&#20064;&#33218;&#27169;&#22411;&#21644;&#20570;&#20986;&#20915;&#31574;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#29305;&#21035;&#23558;&#25152;&#25552;&#20986;&#30340;CRB&#26694;&#26550;&#21644;&#25351;&#25968;&#31574;&#30053;&#31639;&#27861;&#24212;&#29992;&#20110;&#26234;&#33021;&#30005;&#32593;&#20013;&#30340;&#38656;&#27714;&#21709;&#24212;&#20915;&#31574;&#38382;&#39064;&#12290;&#25968;&#20540;&#27169;&#25311;&#35777;&#26126;&#20102;&#35813;&#31639;&#27861;&#30340;&#24615;&#33021;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15640v1 Announce Type: new  Abstract: This paper introduces a novel multi-armed bandits framework, termed Contextual Restless Bandits (CRB), for complex online decision-making. This CRB framework incorporates the core features of contextual bandits and restless bandits, so that it can model both the internal state transitions of each arm and the influence of external global environmental contexts. Using the dual decomposition method, we develop a scalable index policy algorithm for solving the CRB problem, and theoretically analyze the asymptotical optimality of this algorithm. In the case when the arm models are unknown, we further propose a model-based online learning algorithm based on the index policy to learn the arm models and make decisions simultaneously. Furthermore, we apply the proposed CRB framework and the index policy algorithm specifically to the demand response decision-making problem in smart grids. The numerical simulations demonstrate the performance and e
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;AI&#26234;&#33021;&#22330;&#26223;&#25551;&#36848;&#24212;&#29992;&#22312;&#30450;&#20154;&#21644;&#20302;&#35270;&#21147;&#20154;&#32676;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#21457;&#29616;&#29992;&#25143;&#20027;&#35201;&#29992;&#20110;&#35782;&#21035;&#24050;&#30693;&#23545;&#35937;&#30340;&#35270;&#35273;&#29305;&#24449;&#20197;&#21450;&#36991;&#20813;&#19982;&#21361;&#38505;&#29289;&#20307;&#25509;&#35302;&#65292;&#24182;&#19988;&#29992;&#25143;&#23545;&#25551;&#36848;&#30340;&#28385;&#24847;&#24230;&#35780;&#20998;&#30456;&#23545;&#36739;&#20302;&#12290;</title><link>https://arxiv.org/abs/2403.15604</link><description>&lt;p&gt;
&#30740;&#31350;AI&#26234;&#33021;&#22330;&#26223;&#25551;&#36848;&#24212;&#29992;&#22312;&#30450;&#20154;&#21644;&#20302;&#35270;&#21147;&#20154;&#32676;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;
&lt;/p&gt;
&lt;p&gt;
Investigating Use Cases of AI-Powered Scene Description Applications for Blind and Low Vision People
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15604
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;AI&#26234;&#33021;&#22330;&#26223;&#25551;&#36848;&#24212;&#29992;&#22312;&#30450;&#20154;&#21644;&#20302;&#35270;&#21147;&#20154;&#32676;&#20013;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#21457;&#29616;&#29992;&#25143;&#20027;&#35201;&#29992;&#20110;&#35782;&#21035;&#24050;&#30693;&#23545;&#35937;&#30340;&#35270;&#35273;&#29305;&#24449;&#20197;&#21450;&#36991;&#20813;&#19982;&#21361;&#38505;&#29289;&#20307;&#25509;&#35302;&#65292;&#24182;&#19988;&#29992;&#25143;&#23545;&#25551;&#36848;&#30340;&#28385;&#24847;&#24230;&#35780;&#20998;&#30456;&#23545;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#8220;&#22330;&#26223;&#25551;&#36848;&#8221;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#24110;&#21161;&#30450;&#20154;&#21644;&#20302;&#35270;&#21147;&#20154;&#22763;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#29702;&#35299;&#29031;&#29255;&#20013;&#30340;&#35270;&#35273;&#20869;&#23481;&#12290;&#30740;&#31350;&#20154;&#21592;&#24050;&#32463;&#30740;&#31350;&#20102;&#36825;&#20123;&#24212;&#29992;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#20294;&#20182;&#20204;&#21482;&#30740;&#31350;&#20102;&#21033;&#29992;&#36828;&#31243;&#26377;&#35270;&#21147;&#21161;&#25163;&#30340;&#24212;&#29992;&#65292;&#23545;&#20110;&#21033;&#29992;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#25551;&#36848;&#30340;&#24212;&#29992;&#30693;&#20043;&#29978;&#23569;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#35843;&#26597;&#20854;&#20351;&#29992;&#24773;&#20917;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#20026;&#26399;&#20004;&#21608;&#30340;&#26085;&#35760;&#30740;&#31350;&#65292;&#22312;&#27492;&#26399;&#38388;&#65292;16&#21517;&#30450;&#20154;&#21644;&#20302;&#35270;&#21147;&#21442;&#19982;&#32773;&#20351;&#29992;&#20102;&#25105;&#20204;&#35774;&#35745;&#30340;AI&#26234;&#33021;&#22330;&#26223;&#25551;&#36848;&#24212;&#29992;&#12290;&#36890;&#36807;&#20182;&#20204;&#30340;&#26085;&#35760;&#35760;&#24405;&#21644;&#21518;&#32493;&#35775;&#35848;&#65292;&#29992;&#25143;&#20998;&#20139;&#20102;&#20182;&#20204;&#30340;&#20449;&#24687;&#30446;&#26631;&#20197;&#21450;&#20182;&#20204;&#25910;&#21040;&#30340;&#35270;&#35273;&#25551;&#36848;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#20998;&#26512;&#20102;&#36825;&#20123;&#35760;&#24405;&#65292;&#24182;&#21457;&#29616;&#20102;&#24120;&#35265;&#30340;&#20351;&#29992;&#24773;&#20917;&#65292;&#27604;&#22914;&#35782;&#21035;&#24050;&#30693;&#23545;&#35937;&#30340;&#35270;&#35273;&#29305;&#24449;&#65292;&#20197;&#21450;&#19968;&#20123;&#20196;&#20154;&#24778;&#35766;&#30340;&#24773;&#20917;&#65292;&#27604;&#22914;&#36991;&#20813;&#25509;&#35302;&#21361;&#38505;&#29289;&#20307;&#12290;&#25105;&#20204;&#36824;&#21457;&#29616;&#65292;&#29992;&#25143;&#23545;&#36825;&#20123;&#25551;&#36848;&#30340;&#28385;&#24847;&#24230;&#35780;&#20998;&#30456;&#23545;&#36739;&#20302;&#65292;&#24179;&#22343;&#20026;2.76&#65288;&#26631;&#20934;&#24046;=1.49&#65289;&#65292;&#23545;&#28385;&#24847;&#24230;&#30340;&#35780;&#20998;&#20026;2.43&#65288;&#26631;&#20934;&#24046;=1&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15604v1 Announce Type: cross  Abstract: "Scene description" applications that describe visual content in a photo are useful daily tools for blind and low vision (BLV) people. Researchers have studied their use, but they have only explored those that leverage remote sighted assistants; little is known about applications that use AI to generate their descriptions. Thus, to investigate their use cases, we conducted a two-week diary study where 16 BLV participants used an AI-powered scene description application we designed. Through their diary entries and follow-up interviews, users shared their information goals and assessments of the visual descriptions they received. We analyzed the entries and found frequent use cases, such as identifying visual features of known objects, and surprising ones, such as avoiding contact with dangerous objects. We also found users scored the descriptions relatively low on average, 2.76 out of 5 (SD=1.49) for satisfaction and 2.43 out of 4 (SD=1
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#20272;&#35745;&#26799;&#24230;&#24182;&#29983;&#25104;&#26174;&#33879;&#22270;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#65292;&#36890;&#36807;Likelihood Ratio&#26041;&#27861;&#20272;&#35745;&#36755;&#20986;&#21040;&#36755;&#20837;&#30340;&#26799;&#24230;&#65292;&#24182;&#24212;&#29992;&#20998;&#22359;&#35745;&#31639;&#25216;&#26415;&#25552;&#39640;&#20272;&#35745;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#35777;&#23454;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15603</link><description>&lt;p&gt;
&#22522;&#20110;&#21069;&#21521;&#23398;&#20064;&#30340;&#22522;&#20110;&#26799;&#24230;&#30340;&#40657;&#30418;&#26174;&#33879;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Forward Learning for Gradient-based Black-box Saliency Map Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15603
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#20272;&#35745;&#26799;&#24230;&#24182;&#29983;&#25104;&#26174;&#33879;&#22270;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#65292;&#36890;&#36807;Likelihood Ratio&#26041;&#27861;&#20272;&#35745;&#36755;&#20986;&#21040;&#36755;&#20837;&#30340;&#26799;&#24230;&#65292;&#24182;&#24212;&#29992;&#20998;&#22359;&#35745;&#31639;&#25216;&#26415;&#25552;&#39640;&#20272;&#35745;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#35777;&#23454;&#26377;&#25928;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;-based&#26174;&#33879;&#22270;&#34987;&#24191;&#27867;&#29992;&#20110;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20915;&#31574;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#21464;&#24471;&#26356;&#28145;&#21644;&#26356;&#40657;&#30418;&#65292;&#22914;&#22312;&#38381;&#28304;API&#65288;&#22914;ChatGPT&#65289;&#20013;&#65292;&#35745;&#31639;&#26799;&#24230;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#38459;&#30861;&#20256;&#32479;&#35299;&#37322;&#26041;&#27861;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#32479;&#19968;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#20272;&#35745;&#26799;&#24230;&#24182;&#29983;&#25104;&#26174;&#33879;&#22270;&#26469;&#35299;&#37322;&#27169;&#22411;&#20915;&#31574;&#12290;&#25105;&#20204;&#37319;&#29992;&#20284;&#28982;&#27604;&#26041;&#27861;&#26469;&#20272;&#35745;&#36755;&#20986;&#21040;&#36755;&#20837;&#30340;&#26799;&#24230;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#26174;&#33879;&#22270;&#29983;&#25104;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#22359;&#35745;&#31639;&#25216;&#26415;&#26469;&#22686;&#24378;&#20272;&#35745;&#20934;&#30830;&#24615;&#12290;&#22312;&#40657;&#30418;&#35774;&#32622;&#20013;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20934;&#30830;&#30340;&#26799;&#24230;&#20272;&#35745;&#21644;&#29983;&#25104;&#26174;&#33879;&#22270;&#30340;&#35299;&#37322;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#24212;&#29992;&#23427;&#26469;&#35299;&#37322;GPT-Vision&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#25581;&#31034;&#20102;&#26799;&#24230;&#30456;&#20851;&#24615;&#30340;&#25345;&#32493;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15603v1 Announce Type: cross  Abstract: Gradient-based saliency maps are widely used to explain deep neural network decisions. However, as models become deeper and more black-box, such as in closed-source APIs like ChatGPT, computing gradients become challenging, hindering conventional explanation methods. In this work, we introduce a novel unified framework for estimating gradients in black-box settings and generating saliency maps to interpret model decisions. We employ the likelihood ratio method to estimate output-to-input gradients and utilize them for saliency map generation. Additionally, we propose blockwise computation techniques to enhance estimation accuracy. Extensive experiments in black-box settings validate the effectiveness of our method, demonstrating accurate gradient estimation and explainability of generated saliency maps. Furthermore, we showcase the scalability of our approach by applying it to explain GPT-Vision, revealing the continued relevance of gr
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#22823;&#22810;&#25968;&#25945;&#32946;&#26426;&#26500;&#32570;&#20047;&#19987;&#38376;&#25351;&#23548;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#22914;ChatGPT&#30340;&#36947;&#24503;&#24212;&#29992;&#30340;&#25919;&#31574;&#65292;&#24182;&#19988;&#39640;&#20013;&#30456;&#23545;&#20110;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#22312;&#21046;&#23450;&#25919;&#31574;&#19978;&#26356;&#20026;&#29369;&#35947;&#65292;&#24050;&#26377;&#30340;&#25919;&#31574;&#32463;&#24120;&#24573;&#35270;&#23398;&#29983;&#38544;&#31169;&#21644;&#31639;&#27861;&#36879;&#26126;&#24230;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15601</link><description>&lt;p&gt;
&#20174;&#25351;&#23548;&#26041;&#38024;&#21040;&#27835;&#29702;&#65306;&#25945;&#32946;AI&#25919;&#31574;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
From Guidelines to Governance: A Study of AI Policies in Education
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15601
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#22823;&#22810;&#25968;&#25945;&#32946;&#26426;&#26500;&#32570;&#20047;&#19987;&#38376;&#25351;&#23548;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#22914;ChatGPT&#30340;&#36947;&#24503;&#24212;&#29992;&#30340;&#25919;&#31574;&#65292;&#24182;&#19988;&#39640;&#20013;&#30456;&#23545;&#20110;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#22312;&#21046;&#23450;&#25919;&#31574;&#19978;&#26356;&#20026;&#29369;&#35947;&#65292;&#24050;&#26377;&#30340;&#25919;&#31574;&#32463;&#24120;&#24573;&#35270;&#23398;&#29983;&#38544;&#31169;&#21644;&#31639;&#27861;&#36879;&#26126;&#24230;&#31561;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15601v1 &#20844;&#24067;&#31867;&#22411;&#65306;&#20132;&#21449;&#25688;&#35201;&#65306;&#20687;ChatGPT&#36825;&#26679;&#30340;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#31561;&#26032;&#20852;&#25216;&#26415;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#25945;&#32946;&#29615;&#22659;&#65292;&#20026;&#23398;&#20064;&#25552;&#20379;&#21019;&#26032;&#26041;&#27861;&#30340;&#21516;&#26102;&#20063;&#24102;&#26469;&#26032;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#35843;&#26597;&#26041;&#27861;&#26816;&#35270;&#28041;&#21450;&#36825;&#20123;&#25216;&#26415;&#30340;&#25919;&#31574;&#26684;&#23616;&#65292;&#24471;&#20986;102&#20301;&#39640;&#20013;&#26657;&#38271;&#21644;&#39640;&#31561;&#25945;&#32946;&#26657;&#38271;&#30340;&#35265;&#35299;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#19968;&#20010;&#31361;&#20986;&#30340;&#25919;&#31574;&#31354;&#30333;&#65306;&#22823;&#22810;&#25968;&#26426;&#26500;&#32570;&#20047;&#19987;&#38376;&#25351;&#23548;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#22914;ChatGPT&#30340;&#36947;&#24503;&#24212;&#29992;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#39640;&#20013;&#19981;&#22826;&#20542;&#21521;&#20110;&#21046;&#23450;&#25919;&#31574;&#65292;&#30456;&#23545;&#20110;&#39640;&#31561;&#25945;&#32946;&#26426;&#26500;&#32780;&#35328;&#12290;&#23384;&#22312;&#30340;&#36825;&#20123;&#25919;&#31574;&#24448;&#24448;&#24573;&#35270;&#20851;&#38190;&#38382;&#39064;&#65292;&#21253;&#25324;&#23398;&#29983;&#38544;&#31169;&#21644;&#31639;&#27861;&#36879;&#26126;&#24230;&#12290;&#34892;&#25919;&#20154;&#21592;&#26222;&#36941;&#35748;&#35782;&#21040;&#36825;&#20123;&#25919;&#31574;&#30340;&#24517;&#35201;&#24615;&#65292;&#20027;&#35201;&#20026;&#20102;&#20445;&#38556;&#23398;&#29983;&#23433;&#20840;&#21644;&#20943;&#36731;&#25220;&#34989;&#39118;&#38505;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#24378;&#35843;&#20102;&#12290;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15601v1 Announce Type: cross  Abstract: Emerging technologies like generative AI tools, including ChatGPT, are increasingly utilized in educational settings, offering innovative approaches to learning while simultaneously posing new challenges. This study employs a survey methodology to examine the policy landscape concerning these technologies, drawing insights from 102 high school principals and higher education provosts. Our results reveal a prominent policy gap: the majority of institutions lack specialized guide-lines for the ethical deployment of AI tools such as ChatGPT. Moreover,we observed that high schools are less inclined to work on policies than higher educational institutions. Where such policies do exist, they often overlook crucial issues, including student privacy and algorithmic transparency. Administrators overwhelmingly recognize the necessity of these policies, primarily to safeguard student safety and mitigate plagiarism risks. Our findings underscore t
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;ChatGPT&#29983;&#25104;&#30340;&#20195;&#30721;&#21644;StackOverflow&#31572;&#26696;&#65292;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#22312;&#36873;&#25321;&#20195;&#30721;&#29255;&#27573;&#26102;&#23545;&#23433;&#20840;&#28431;&#27934;&#30340;&#35748;&#35782;&#12290;</title><link>https://arxiv.org/abs/2403.15600</link><description>&lt;p&gt;
&#20165;&#20165;&#21448;&#26159;&#22797;&#21046;&#31896;&#36148;&#21527;&#65311;&#27604;&#36739;ChatGPT&#29983;&#25104;&#20195;&#30721;&#21644;StackOverflow&#31572;&#26696;&#30340;&#23433;&#20840;&#28431;&#27934;
&lt;/p&gt;
&lt;p&gt;
Just another copy and paste? Comparing the security vulnerabilities of ChatGPT generated code and StackOverflow answers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15600
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;ChatGPT&#29983;&#25104;&#30340;&#20195;&#30721;&#21644;StackOverflow&#31572;&#26696;&#65292;&#25552;&#39640;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#22312;&#36873;&#25321;&#20195;&#30721;&#29255;&#27573;&#26102;&#23545;&#23433;&#20840;&#28431;&#27934;&#30340;&#35748;&#35782;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv&#65306;2403.15600v1 &#21457;&#24067;&#31867;&#22411;&#65306;&#36328;  &#25688;&#35201;&#65306;Sonatype&#30340;2023&#24180;&#25253;&#21578;&#21457;&#29616;&#65292;97%&#30340;&#24320;&#21457;&#20154;&#21592;&#21644;&#23433;&#20840;&#36127;&#36131;&#20154;&#23558;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#65292;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#38598;&#25104;&#21040;&#20182;&#20204;&#30340;&#24320;&#21457;&#27969;&#31243;&#20013;&#12290;&#23545;&#36825;&#19968;&#36235;&#21183;&#30340;&#23433;&#20840;&#24433;&#21709;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#25285;&#24551;&#12290;&#24320;&#21457;&#20154;&#21592;&#29616;&#22312;&#27491;&#22312;&#26435;&#34913;LLMs&#30456;&#23545;&#20110;&#20854;&#20182;&#21487;&#38752;&#20449;&#24687;&#26469;&#28304;&#65288;&#22914;StackOverflow&#65288;SO&#65289;&#65289;&#30340;&#22909;&#22788;&#21644;&#39118;&#38505;&#65292;&#38656;&#35201;&#23454;&#35777;&#25968;&#25454;&#26469;&#25351;&#23548;&#20182;&#20204;&#30340;&#36873;&#25321;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#23454;&#35777;&#27604;&#36739;ChatGPT&#21644;StackOverflow&#30340;&#28431;&#27934;&#65292;&#24341;&#36215;&#36719;&#20214;&#24320;&#21457;&#20154;&#21592;&#36873;&#25321;&#20195;&#30721;&#29255;&#27573;&#26102;&#30340;&#23433;&#20840;&#24433;&#21709;&#24847;&#35782;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#26469;&#33258;SO&#30340;&#19982;&#23433;&#20840;&#30456;&#20851;&#38382;&#39064;&#21644;&#31572;&#26696;&#30340;&#29616;&#26377;Java&#25968;&#25454;&#38598;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35810;&#38382;ChatGPT&#30456;&#21516;&#30340;SO&#38382;&#39064;&#65292;&#25910;&#38598;&#29983;&#25104;&#30340;&#20195;&#30721;&#36827;&#34892;&#27604;&#36739;&#12290;&#22312;&#25972;&#29702;&#25968;&#25454;&#38598;&#21518;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;Common Weakness Enumeration (CWE) &#28431;&#27934;&#30340;&#25968;&#37327;&#21644;&#31867;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15600v1 Announce Type: cross  Abstract: Sonatype's 2023 report found that 97% of developers and security leads integrate generative Artificial Intelligence (AI), particularly Large Language Models (LLMs), into their development process. Concerns about the security implications of this trend have been raised. Developers are now weighing the benefits and risks of LLMs against other relied-upon information sources, such as StackOverflow (SO), requiring empirical data to inform their choice. In this work, our goal is to raise software developers awareness of the security implications when selecting code snippets by empirically comparing the vulnerabilities of ChatGPT and StackOverflow. To achieve this, we used an existing Java dataset from SO with security-related questions and answers. Then, we asked ChatGPT the same SO questions, gathering the generated code for comparison. After curating the dataset, we analyzed the number and types of Common Weakness Enumeration (CWE) vulner
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#30340;ChatGPT&#22312;&#32676;&#20307;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#20026;&#25552;&#21462;&#24847;&#35265;&#21644;&#20570;&#20986;&#20915;&#31574;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15587</link><description>&lt;p&gt;
&#22522;&#20110;ChatGPT&#30340;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#32676;&#20307;&#20915;&#31574;&#65306;&#27169;&#22411;&#12289;&#20998;&#26512;&#21644;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Large language models for crowd decision making based on prompt design strategies using ChatGPT: models, analysis and challenges
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15587
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#22522;&#20110;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#30340;ChatGPT&#22312;&#32676;&#20307;&#20915;&#31574;&#36807;&#31243;&#20013;&#30340;&#24212;&#29992;&#65292;&#20026;&#25552;&#21462;&#24847;&#35265;&#21644;&#20570;&#20986;&#20915;&#31574;&#25552;&#20379;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#21644;&#20114;&#32852;&#32593;&#26377;&#28508;&#21147;&#34987;&#21033;&#29992;&#20316;&#20026;&#20016;&#23500;&#20915;&#31574;&#35299;&#20915;&#26041;&#26696;&#24847;&#35265;&#30340;&#26469;&#28304;&#12290;&#32676;&#20307;&#20915;&#31574;&#65288;CDM&#65289;&#26159;&#19968;&#31181;&#33021;&#22815;&#36890;&#36807;&#24773;&#24863;&#20998;&#26512;&#20174;&#32431;&#25991;&#26412;&#65288;&#22914;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#21457;&#24067;&#30340;&#35780;&#35770;&#65289;&#20013;&#25512;&#26029;&#24847;&#35265;&#21644;&#20915;&#31574;&#30340;&#26041;&#27861;&#12290;&#26412;&#25991;&#20998;&#26512;&#20102;&#21033;&#29992;&#22522;&#20110;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#30340;ChatGPT&#26469;&#36741;&#21161;CDM&#36807;&#31243;&#65292;&#20197;&#25552;&#21462;&#24847;&#35265;&#21644;&#20570;&#20986;&#20915;&#31574;&#12290;&#25105;&#20204;&#23558;ChatGPT&#25972;&#21512;&#21040;CDM&#36807;&#31243;&#20013;&#20316;&#20026;&#19968;&#31181;&#28789;&#27963;&#30340;&#24037;&#20855;&#65292;&#25512;&#26029;&#20986;&#25991;&#26412;&#20013;&#34920;&#36798;&#30340;&#24847;&#35265;&#65292;&#24182;&#26681;&#25454;&#25552;&#31034;&#35774;&#35745;&#31574;&#30053;&#21046;&#23450;&#20915;&#31574;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15587v1 Announce Type: new  Abstract: Social Media and Internet have the potential to be exploited as a source of opinion to enrich Decision Making solutions. Crowd Decision Making (CDM) is a methodology able to infer opinions and decisions from plain texts, such as reviews published in social media platforms, by means of Sentiment Analysis. Currently, the emergence and potential of Large Language Models (LLMs) lead us to explore new scenarios of automatically understand written texts, also known as natural language processing. This paper analyzes the use of ChatGPT based on prompt design strategies to assist in CDM processes to extract opinions and make decisions. We integrate ChatGPT in CDM processes as a flexible tool that infer the opinions expressed in texts, providing numerical or linguistic evaluations where the decision making models are based on the prompt design strategies. We include a multi-criteria decision making scenario with a category ontology for criteria. 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#35748;&#30693;&#27700;&#24179;&#12289;&#24635;&#20307;&#24773;&#32490;&#21644;&#24433;&#21709;&#22240;&#32032;&#65292;&#32467;&#26524;&#26174;&#31034;&#25945;&#32946;&#32773;&#23545;&#36825;&#20123;&#24037;&#20855;&#30340;&#24577;&#24230;&#36880;&#28176;&#21464;&#24471;&#31215;&#26497;&#12290;</title><link>https://arxiv.org/abs/2403.15586</link><description>&lt;p&gt;
&#25945;&#32946;&#39046;&#22495;&#20013;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#65306;&#25945;&#32946;&#32773;&#24847;&#35782;&#12289;&#24773;&#32490;&#21644;&#24433;&#21709;&#22240;&#32032;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Generative AI in Education: A Study of Educators' Awareness, Sentiments, and Influencing Factors
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15586
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35843;&#26597;&#20102;&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#24037;&#20855;&#30340;&#35748;&#30693;&#27700;&#24179;&#12289;&#24635;&#20307;&#24773;&#32490;&#21644;&#24433;&#21709;&#22240;&#32032;&#65292;&#32467;&#26524;&#26174;&#31034;&#25945;&#32946;&#32773;&#23545;&#36825;&#20123;&#24037;&#20855;&#30340;&#24577;&#24230;&#36880;&#28176;&#21464;&#24471;&#31215;&#26497;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#24555;&#36895;&#21457;&#23637;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#19981;&#26029;&#25972;&#21512;&#24341;&#21457;&#20102;&#20851;&#20110;&#23427;&#20204;&#22312;&#25945;&#32946;&#20013;&#24212;&#29992;&#30340;&#35752;&#35770;&#12290;&#26412;&#30740;&#31350;&#28145;&#20837;&#25506;&#35752;&#22823;&#23398;&#25945;&#24072;&#23545;AI&#35821;&#35328;&#27169;&#22411;&#30340;&#32463;&#39564;&#21644;&#24577;&#24230;&#65292;&#36890;&#36807;&#20998;&#26512;&#25945;&#32946;&#32773;&#23545;AI&#22312;&#35838;&#22530;&#20013;&#30340;&#20316;&#29992;&#20197;&#21450;&#23545;&#25945;&#23398;&#21644;&#23398;&#20064;&#28508;&#22312;&#24433;&#21709;&#30340;&#35266;&#28857;&#65292;&#22635;&#34917;&#20102;&#25991;&#29486;&#20013;&#30340;&#31354;&#30333;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#35843;&#26597;&#39640;&#31561;&#25945;&#32946;&#20013;LLMs&#21644;&#29983;&#25104;&#24335;AI&#24037;&#20855;&#30340;&#35748;&#30693;&#27700;&#24179;&#12289;&#37319;&#32435;&#30340;&#24635;&#20307;&#24773;&#32490;&#20197;&#21450;&#24433;&#21709;&#36825;&#20123;&#24577;&#24230;&#30340;&#22240;&#32032;&#12290;&#25968;&#25454;&#36890;&#36807;&#21033;&#29992;&#26446;&#20811;&#29305;&#37327;&#34920;&#36827;&#34892;&#30340;&#35843;&#26597;&#25910;&#38598;&#65292;&#21516;&#26102;&#36741;&#20197;&#36319;&#36827;&#35775;&#35848;&#20197;&#26356;&#32454;&#33268;&#22320;&#20102;&#35299;&#25945;&#24072;&#30340;&#35266;&#28857;&#12290;&#25910;&#38598;&#21040;&#30340;&#25968;&#25454;&#20351;&#29992;&#32479;&#35745;&#21644;&#20027;&#39064;&#20998;&#26512;&#25216;&#26415;&#36827;&#34892;&#22788;&#29702;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;&#65292;&#25945;&#32946;&#24037;&#20316;&#32773;&#23545;&#29983;&#25104;&#24335;AI&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#24037;&#20855;&#30340;&#24577;&#24230;&#36880;&#28176;&#21464;&#24471;&#31215;&#26497;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15586v1 Announce Type: new  Abstract: The rapid advancement of artificial intelligence (AI) and the expanding integration of large language models (LLMs) have ignited a debate about their application in education. This study delves into university instructors' experiences and attitudes toward AI language models, filling a gap in the literature by analyzing educators' perspectives on AI's role in the classroom and its potential impacts on teaching and learning. The objective of this research is to investigate the level of awareness, overall sentiment towardsadoption, and the factors influencing these attitudes for LLMs and generative AI-based tools in higher education. Data was collected through a survey using a Likert scale, which was complemented by follow-up interviews to gain a more nuanced understanding of the instructors' viewpoints. The collected data was processed using statistical and thematic analysis techniques. Our findings reveal that educators are increasingly a
&lt;/p&gt;</description></item><item><title>MedPromptX&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#22522;&#30784;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#34917;&#20805;&#32570;&#22833;&#30340;EHR&#20449;&#24687;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#20294;&#36873;&#25321;&#26368;&#20339;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#39640;&#36136;&#37327;&#20505;&#36873;&#32773;&#20173;&#26377;&#24453;&#35299;&#20915;&#12290;</title><link>https://arxiv.org/abs/2403.15585</link><description>&lt;p&gt;
MedPromptX&#65306;&#22522;&#20110;&#29616;&#23454;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;
&lt;/p&gt;
&lt;p&gt;
MedPromptX: Grounded Multimodal Prompting for Chest X-ray Diagnosis
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15585
&lt;/p&gt;
&lt;p&gt;
MedPromptX&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#21644;&#35270;&#35273;&#22522;&#30784;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#34917;&#20805;&#32570;&#22833;&#30340;EHR&#20449;&#24687;&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24187;&#35273;&#38382;&#39064;&#65292;&#20294;&#36873;&#25321;&#26368;&#20339;&#23569;&#26679;&#26412;&#31034;&#20363;&#21644;&#39640;&#36136;&#37327;&#20505;&#36873;&#32773;&#20173;&#26377;&#24453;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33016;&#37096;X&#32447;&#22270;&#20687;&#36890;&#24120;&#29992;&#20110;&#39044;&#27979;&#24613;&#24615;&#21644;&#24930;&#24615;&#24515;&#32954;&#30142;&#30149;&#65292;&#20294;&#26159;&#23558;&#23427;&#20204;&#19982;&#32467;&#26500;&#21270;&#20020;&#24202;&#25968;&#25454;&#25972;&#21512;&#30340;&#21162;&#21147;&#38754;&#20020;&#30528;&#22240;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;EHR&#65289;&#19981;&#23436;&#25972;&#32780;&#24102;&#26469;&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;MedPromptX&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#23558;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#12289;&#23569;&#26679;&#26412;&#25552;&#31034;&#65288;FP&#65289;&#21644;&#35270;&#35273;&#22522;&#30784;&#65288;VG&#65289;&#30456;&#32467;&#21512;&#65292;&#23558;&#22270;&#20687;&#19982;EHR&#25968;&#25454;&#29992;&#20110;&#33016;&#37096;X&#32447;&#35786;&#26029;&#30340;&#27169;&#22411;&#12290;&#39044;&#35757;&#32451;&#30340;MLLM&#34987;&#29992;&#26469;&#34917;&#20805;&#32570;&#22833;&#30340;EHR&#20449;&#24687;&#65292;&#25552;&#20379;&#23545;&#24739;&#32773;&#30149;&#21490;&#30340;&#20840;&#38754;&#29702;&#35299;&#12290;&#27492;&#22806;&#65292;&#23569;&#26679;&#26412;&#25552;&#31034;&#20943;&#23569;&#20102;&#23545;MLLM&#30340;&#22823;&#37327;&#35757;&#32451;&#30340;&#24517;&#35201;&#24615;&#65292;&#21516;&#26102;&#26377;&#25928;&#35299;&#20915;&#20102;&#24187;&#35273;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#30830;&#23450;&#26368;&#20339;&#23569;&#26679;&#26412;&#31034;&#20363;&#30340;&#36807;&#31243;&#21644;&#36873;&#25321;&#39640;&#36136;&#37327;&#20505;&#36873;&#32773;&#21487;&#33021;&#36807;&#20110;&#32321;&#29712;&#65292;&#20294;&#23427;&#23545;&#27169;&#22411;&#24615;&#33021;&#20135;&#29983;&#30528;&#28145;&#36828;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#26469;&#21160;&#24577;&#22320;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15585v1 Announce Type: cross  Abstract: Chest X-ray images are commonly used for predicting acute and chronic cardiopulmonary conditions, but efforts to integrate them with structured clinical data face challenges due to incomplete electronic health records (EHR). This paper introduces \textbf{MedPromptX}, the first model to integrate multimodal large language models (MLLMs), few-shot prompting (FP) and visual grounding (VG) to combine imagery with EHR data for chest X-ray diagnosis. A pre-trained MLLM is utilized to complement the missing EHR information, providing a comprehensive understanding of patients' medical history. Additionally, FP reduces the necessity for extensive training of MLLMs while effectively tackling the issue of hallucination. Nevertheless, the process of determining the optimal number of few-shot examples and selecting high-quality candidates can be burdensome, yet it profoundly influences model performance. Hence, we propose a new technique that dynam
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#38598;&#25104;&#30340;DNN&#22238;&#24402;&#22120;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#22788;&#29702;&#65292;&#22312;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#22330;&#26223;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#36710;&#22836;&#38388;&#38548;&#30340;&#39044;&#27979;&#24182;&#25552;&#20379;&#20102;&#27010;&#29575;&#23433;&#20840;&#20445;&#35777;&#12290;</title><link>https://arxiv.org/abs/2403.15577</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#38598;&#25104;&#30340;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#30340;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#33258;&#21160;&#39550;&#39542;
&lt;/p&gt;
&lt;p&gt;
Autonomous Driving With Perception Uncertainties: Deep-Ensemble Based Adaptive Cruise Control
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#38598;&#25104;&#30340;DNN&#22238;&#24402;&#22120;&#29992;&#20110;&#33258;&#21160;&#39550;&#39542;&#20013;&#24863;&#30693;&#19981;&#30830;&#23450;&#24615;&#30340;&#22788;&#29702;&#65292;&#22312;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#22330;&#26223;&#20013;&#65292;&#23454;&#29616;&#20102;&#23545;&#36710;&#22836;&#38388;&#38548;&#30340;&#39044;&#27979;&#24182;&#25552;&#20379;&#20102;&#27010;&#29575;&#23433;&#20840;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20381;&#36182;&#24863;&#30693;&#31995;&#32479;&#26469;&#29702;&#35299;&#29615;&#22659;&#24182;&#20026;&#19979;&#28216;&#20915;&#31574;&#25552;&#20379;&#20449;&#24687;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28145;&#24230;&#38598;&#25104;&#30340;DNN&#22238;&#24402;&#22120;&#65288;Deep Ensemble&#65289;&#65292;&#29992;&#20110;&#29983;&#25104;&#24102;&#26377;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#37327;&#21270;&#30340;&#39044;&#27979;&#12290;&#22312;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#65288;ACC&#65289;&#22330;&#26223;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28145;&#24230;&#38598;&#25104;&#20174;RGB&#22270;&#20687;&#20013;&#20272;&#31639;&#21040;&#21069;&#26041;&#36710;&#36742;&#30340;&#36317;&#31163;&#36710;&#22836;&#38388;&#38548;&#65292;&#24182;&#20351;&#19979;&#28216;&#25511;&#21046;&#22120;&#32771;&#34385;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#24033;&#33322;&#25511;&#21046;&#22120;&#65292;&#21033;&#29992;&#24102;&#26377;&#27010;&#29575;&#23433;&#20840;&#20445;&#35777;&#30340;&#38543;&#26426;&#27169;&#22411;&#39044;&#27979;&#25511;&#21046;&#65288;MPC&#65289;&#26469;&#25552;&#20379;&#27010;&#29575;&#23433;&#20840;&#24615;&#20445;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15577v1 Announce Type: new  Abstract: Autonomous driving depends on perception systems to understand the environment and to inform downstream decision-making. While advanced perception systems utilizing black-box Deep Neural Networks (DNNs) demonstrate human-like comprehension, their unpredictable behavior and lack of interpretability may hinder their deployment in safety critical scenarios. In this paper, we develop an Ensemble of DNN regressors (Deep Ensemble) that generates predictions with quantification of prediction uncertainties. In the scenario of Adaptive Cruise Control (ACC), we employ the Deep Ensemble to estimate distance headway to the lead vehicle from RGB images and enable the downstream controller to account for the estimation uncertainty. We develop an adaptive cruise controller that utilizes Stochastic Model Predictive Control (MPC) with chance constraints to provide a probabilistic safety guarantee. We evaluate our ACC algorithm using a high-fidelity traff
&lt;/p&gt;</description></item><item><title>SensoryT5&#26159;&#19968;&#31181;&#23558;&#24863;&#35273;&#20449;&#24687;&#34701;&#20837;T5&#27169;&#22411;&#30340;&#31070;&#32463;&#35748;&#30693;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#32454;&#31890;&#24230;&#24773;&#32490;&#20998;&#31867;&#65292;&#36890;&#36807;&#22312;&#27880;&#24847;&#26426;&#21046;&#20013;&#25972;&#21512;&#24863;&#35273;&#32447;&#32034;&#65292;&#23454;&#29616;&#24773;&#32490;&#34920;&#31034;&#30340;&#20016;&#23500;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15574</link><description>&lt;p&gt;
SensoryT5&#65306;&#23558;&#24863;&#35273;&#36816;&#21160;&#35268;&#33539;&#34701;&#20837;T5&#20197;&#22686;&#24378;&#32454;&#31890;&#24230;&#24773;&#32490;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
SensoryT5: Infusing Sensorimotor Norms into T5 for Enhanced Fine-grained Emotion Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15574
&lt;/p&gt;
&lt;p&gt;
SensoryT5&#26159;&#19968;&#31181;&#23558;&#24863;&#35273;&#20449;&#24687;&#34701;&#20837;T5&#27169;&#22411;&#30340;&#31070;&#32463;&#35748;&#30693;&#26041;&#27861;&#65292;&#26088;&#22312;&#22686;&#24378;&#32454;&#31890;&#24230;&#24773;&#32490;&#20998;&#31867;&#65292;&#36890;&#36807;&#22312;&#27880;&#24847;&#26426;&#21046;&#20013;&#25972;&#21512;&#24863;&#35273;&#32447;&#32034;&#65292;&#23454;&#29616;&#24773;&#32490;&#34920;&#31034;&#30340;&#20016;&#23500;&#24615;&#65292;&#24182;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20986;&#36739;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30740;&#31350;&#26041;&#27861;&#20013;&#65292;&#24863;&#30693;&#21644;&#24773;&#32490;&#20998;&#31867;&#34987;&#35748;&#20026;&#26159;&#29420;&#31435;&#30340;&#39046;&#22495;&#12290;&#28982;&#32780;&#65292;&#24863;&#35273;&#32463;&#21382;&#23545;&#24773;&#32490;&#21453;&#24212;&#30340;&#26174;&#33879;&#24433;&#21709;&#26159;&#19981;&#21487;&#21542;&#35748;&#30340;&#12290;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31038;&#21306;&#32463;&#24120;&#38169;&#36807;&#23558;&#24863;&#35273;&#30693;&#35782;&#19982;&#24773;&#32490;&#20998;&#31867;&#30456;&#32467;&#21512;&#30340;&#26426;&#20250;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;SensoryT5&#65292;&#36825;&#26159;&#19968;&#31181;&#31070;&#32463;&#35748;&#30693;&#26041;&#27861;&#65292;&#23558;&#24863;&#23448;&#20449;&#24687;&#25972;&#21512;&#21040;&#19987;&#20026;&#32454;&#31890;&#24230;&#24773;&#32490;&#20998;&#31867;&#32780;&#35774;&#35745;&#30340;T5&#65288;&#25991;&#26412;&#21040;&#25991;&#26412;&#36716;&#25442;Transformer&#65289;&#27169;&#22411;&#20013;&#12290;&#35813;&#26041;&#27861;&#23558;&#24863;&#35273;&#32447;&#32034;&#34701;&#20837;T5&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#23454;&#29616;&#20102;&#19978;&#19979;&#25991;&#29702;&#35299;&#19982;&#24863;&#35273;&#24847;&#35782;&#20043;&#38388;&#30340;&#21644;&#35856;&#24179;&#34913;&#12290;&#32467;&#26524;&#27169;&#22411;&#22686;&#24378;&#20102;&#24773;&#32490;&#34920;&#31034;&#30340;&#20016;&#23500;&#24615;&#12290;&#36890;&#36807;&#23545;&#21508;&#31181;&#35814;&#32454;&#24773;&#32490;&#20998;&#31867;&#25968;&#25454;&#38598;&#30340;&#20005;&#26684;&#27979;&#35797;&#65292;SensoryT5&#23637;&#31034;&#20986;&#20102;&#25913;&#21892;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15574v1 Announce Type: new  Abstract: In traditional research approaches, sensory perception and emotion classification have traditionally been considered separate domains. Yet, the significant influence of sensory experiences on emotional responses is undeniable. The natural language processing (NLP) community has often missed the opportunity to merge sensory knowledge with emotion classification. To address this gap, we propose SensoryT5, a neuro-cognitive approach that integrates sensory information into the T5 (Text-to-Text Transfer Transformer) model, designed specifically for fine-grained emotion classification. This methodology incorporates sensory cues into the T5's attention mechanism, enabling a harmonious balance between contextual understanding and sensory awareness. The resulting model amplifies the richness of emotional representations. In rigorous tests across various detailed emotion classification datasets, SensoryT5 showcases improved performance, surpassin
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22235;&#38454;&#27573;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;MV&#19968;&#33268;&#30340;&#25193;&#25955;&#36807;&#31243;&#12289;&#21322;&#23450;&#32534;&#31243;&#38382;&#39064;&#35299;&#20915;&#12289;&#38750;&#21018;&#24615;&#23545;&#40784;&#21644;MRF&#38382;&#39064;&#35299;&#20915;&#31561;&#27493;&#39588;&#26469;&#23454;&#29616;&#23545;3D&#32593;&#26684;&#36827;&#34892;&#32441;&#29702;&#36148;&#22270;&#30340;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15559</link><description>&lt;p&gt;
&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#24378;&#21046;&#23454;&#29616;&#23545;3D&#32593;&#26684;&#36827;&#34892;&#32441;&#29702;&#36148;&#22270;&#30340;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;
&lt;/p&gt;
&lt;p&gt;
An Optimization Framework to Enforce Multi-View Consistency for Texturing 3D Meshes Using Pre-Trained Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15559
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22235;&#38454;&#27573;&#30340;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;MV&#19968;&#33268;&#30340;&#25193;&#25955;&#36807;&#31243;&#12289;&#21322;&#23450;&#32534;&#31243;&#38382;&#39064;&#35299;&#20915;&#12289;&#38750;&#21018;&#24615;&#23545;&#40784;&#21644;MRF&#38382;&#39064;&#35299;&#20915;&#31561;&#27493;&#39588;&#26469;&#23454;&#29616;&#23545;3D&#32593;&#26684;&#36827;&#34892;&#32441;&#29702;&#36148;&#22270;&#30340;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#23545;3D&#32593;&#26684;&#36827;&#34892;&#32441;&#29702;&#36148;&#22270;&#26102;&#65292;&#30830;&#20445;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#26159;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#20248;&#21270;&#26694;&#26550;&#65292;&#36890;&#36807;&#22235;&#20010;&#38454;&#27573;&#23454;&#29616;&#22810;&#35270;&#22270;&#19968;&#33268;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31532;&#19968;&#38454;&#27573;&#20351;&#29992;MV&#19968;&#33268;&#30340;&#25193;&#25955;&#36807;&#31243;&#20174;&#39044;&#23450;&#20041;&#30340;&#35270;&#28857;&#38598;&#29983;&#25104;2D&#32441;&#29702;&#30340;&#36807;&#23436;&#22791;&#38598;&#12290;&#31532;&#20108;&#38454;&#27573;&#36890;&#36807;&#35299;&#20915;&#21322;&#23450;&#32534;&#31243;&#38382;&#39064;&#36873;&#25321;&#30456;&#20114;&#19968;&#33268;&#19988;&#35206;&#30422;&#22522;&#30784;3D&#27169;&#22411;&#30340;&#35270;&#22270;&#23376;&#38598;&#12290;&#31532;&#19977;&#38454;&#27573;&#25191;&#34892;&#38750;&#21018;&#24615;&#23545;&#40784;&#65292;&#20351;&#36873;&#23450;&#30340;&#35270;&#22270;&#22312;&#37325;&#21472;&#21306;&#22495;&#23545;&#40784;&#12290;&#31532;&#22235;&#38454;&#27573;&#35299;&#20915;MRF&#38382;&#39064;&#20197;&#20851;&#32852;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15559v1 Announce Type: cross  Abstract: A fundamental problem in the texturing of 3D meshes using pre-trained text-to-image models is to ensure multi-view consistency. State-of-the-art approaches typically use diffusion models to aggregate multi-view inputs, where common issues are the blurriness caused by the averaging operation in the aggregation step or inconsistencies in local features. This paper introduces an optimization framework that proceeds in four stages to achieve multi-view consistency. Specifically, the first stage generates an over-complete set of 2D textures from a predefined set of viewpoints using an MV-consistent diffusion process. The second stage selects a subset of views that are mutually consistent while covering the underlying 3D model. We show how to achieve this goal by solving semi-definite programs. The third stage performs non-rigid alignment to align the selected views across overlapping regions. The fourth stage solves an MRF problem to associ
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#36827;&#34892;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#26174;&#24335;&#20808;&#39564;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#31616;&#21333;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#21462;&#35821;&#35328;&#27169;&#22411;&#23545;&#19990;&#30028;&#32467;&#26500;&#30340;&#20559;&#35265;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20551;&#35774;&#30340;&#26174;&#24335;&#26469;&#28304;&#36755;&#20837;&#21040;MDE&#31995;&#32479;&#20013;&#12290;</title><link>https://arxiv.org/abs/2403.15551</link><description>&lt;p&gt;
&#22522;&#20110;&#35821;&#35328;&#30340;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#28145;&#24230;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
Language-Based Depth Hints for Monocular Depth Estimation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15551
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#36827;&#34892;&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;&#30340;&#26174;&#24335;&#20808;&#39564;&#65292;&#26412;&#30740;&#31350;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#31616;&#21333;&#30340;&#23398;&#20064;&#26041;&#27861;&#26469;&#25552;&#21462;&#35821;&#35328;&#27169;&#22411;&#23545;&#19990;&#30028;&#32467;&#26500;&#30340;&#20559;&#35265;&#65292;&#24182;&#23558;&#20854;&#20316;&#20026;&#20551;&#35774;&#30340;&#26174;&#24335;&#26469;&#28304;&#36755;&#20837;&#21040;MDE&#31995;&#32479;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21333;&#30446;&#28145;&#24230;&#20272;&#35745;(MDE)&#22312;&#26412;&#36136;&#19978;&#26159;&#27169;&#31946;&#30340;&#65292;&#22240;&#20026;&#32473;&#23450;&#30340;&#22270;&#20687;&#21487;&#33021;&#26469;&#28304;&#20110;&#35768;&#22810;&#19981;&#21516;&#30340;3D&#22330;&#26223;&#65292;&#21453;&#20043;&#20134;&#28982;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#31181;&#27169;&#31946;&#24615;&#65292;&#19968;&#20010;MDE&#31995;&#32479;&#24517;&#39035;&#23545;&#32473;&#23450;&#36755;&#20837;&#30340;&#26368;&#21487;&#33021;&#30340;3D&#22330;&#26223;&#20316;&#20986;&#20551;&#35774;&#12290;&#36825;&#20123;&#20551;&#35774;&#21487;&#20197;&#26159;&#26174;&#24335;&#30340;&#65292;&#20063;&#21487;&#20197;&#26159;&#38544;&#24335;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#33258;&#28982;&#35821;&#35328;&#20316;&#20026;&#19990;&#30028;&#32467;&#26500;&#30340;&#26174;&#24335;&#20808;&#39564;&#30340;&#29992;&#27861;&#12290;&#20551;&#35774;&#20154;&#31867;&#35821;&#35328;&#23545;&#21508;&#31181;&#29289;&#20307;&#22312;&#28145;&#24230;&#31354;&#38388;&#20013;&#30340;&#21487;&#33021;&#20998;&#24067;&#36827;&#34892;&#32534;&#30721;&#12290;&#25105;&#20204;&#39318;&#20808;&#23637;&#31034;&#20102;&#35821;&#35328;&#27169;&#22411;&#22312;&#35757;&#32451;&#26399;&#38388;&#32534;&#30721;&#20102;&#36825;&#31181;&#38544;&#24335;&#20559;&#35265;&#65292;&#24182;&#19988;&#21487;&#20197;&#20351;&#29992;&#38750;&#24120;&#31616;&#21333;&#30340;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#25552;&#21462;&#12290;&#28982;&#21518;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#39044;&#27979;&#21487;&#20197;&#20316;&#20026;&#20551;&#35774;&#30340;&#26174;&#24335;&#26469;&#28304;&#25552;&#20379;&#32473;MDE&#31995;&#32479;&#65292;&#20351;&#29992;&#19968;&#20010;&#25552;&#20379;&#26631;&#31614;&#20316;&#20026;&#35821;&#35328;&#27169;&#22411;&#36755;&#20837;&#30340;&#29616;&#25104;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15551v1 Announce Type: cross  Abstract: Monocular depth estimation (MDE) is inherently ambiguous, as a given image may result from many different 3D scenes and vice versa. To resolve this ambiguity, an MDE system must make assumptions about the most likely 3D scenes for a given input. These assumptions can be either explicit or implicit. In this work, we demonstrate the use of natural language as a source of an explicit prior about the structure of the world. The assumption is made that human language encodes the likely distribution in depth-space of various objects. We first show that a language model encodes this implicit bias during training, and that it can be extracted using a very simple learned approach. We then show that this prediction can be provided as an explicit source of assumption to an MDE system, using an off-the-shelf instance segmentation model that provides the labels used as the input to the language model. We demonstrate the performance of our method on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21363;&#20026;&#30740;&#31350;&#35770;&#25991;&#29983;&#25104;&#24314;&#35758;&#24615;&#23616;&#38480;&#65292;&#36890;&#36807;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#31181;&#26041;&#27861;&#26469;&#25581;&#31034;&#30456;&#20851;&#25361;&#25112;&#12289;&#23454;&#36341;&#35265;&#35299;&#21644;&#28508;&#22312;&#26426;&#20250;&#12290;</title><link>https://arxiv.org/abs/2403.15529</link><description>&lt;p&gt;
LimGen: &#25506;&#31350;&#29992;&#20110;&#29983;&#25104;&#30740;&#31350;&#35770;&#25991;&#24314;&#35758;&#24615;&#23616;&#38480;&#30340;LLMs
&lt;/p&gt;
&lt;p&gt;
LimGen: Probing the LLMs for Generating Suggestive Limitations of Research Papers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15529
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#32780;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#21363;&#20026;&#30740;&#31350;&#35770;&#25991;&#29983;&#25104;&#24314;&#35758;&#24615;&#23616;&#38480;&#65292;&#36890;&#36807;&#35843;&#26597;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#22810;&#31181;&#26041;&#27861;&#26469;&#25581;&#31034;&#30456;&#20851;&#25361;&#25112;&#12289;&#23454;&#36341;&#35265;&#35299;&#21644;&#28508;&#22312;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26816;&#26597;&#23616;&#38480;&#26159;&#23398;&#26415;&#30740;&#31350;&#35780;&#23457;&#36807;&#31243;&#20013;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#25581;&#31034;&#20102;&#30740;&#31350;&#21487;&#33021;&#32570;&#20047;&#20915;&#23450;&#24615;&#25110;&#38656;&#35201;&#21152;&#24378;&#30340;&#26041;&#38754;&#12290;&#36825;&#26377;&#21161;&#20110;&#35835;&#32773;&#32771;&#34385;&#36827;&#19968;&#27493;&#30740;&#31350;&#30340;&#26356;&#24191;&#27867;&#24433;&#21709;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#30740;&#31350;&#35770;&#25991;&#24314;&#35758;&#24615;&#23616;&#38480;&#29983;&#25104;&#65288;SLG&#65289;&#30340;&#19968;&#39033;&#26032;&#39062;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#32534;&#21046;&#20102;&#19968;&#20010;&#21517;&#20026;LimGen&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#26469;&#33258;ACL&#25991;&#38598;&#30340;4068&#31687;&#30740;&#31350;&#35770;&#25991;&#21450;&#20854;&#30456;&#20851;&#23616;&#38480;&#12290;&#25105;&#20204;&#35843;&#26597;&#20102;&#22810;&#31181;&#26041;&#27861;&#26469;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#24314;&#35758;&#24615;&#23616;&#38480;&#65292;&#36890;&#36807;&#24443;&#24213;&#30740;&#31350;&#30456;&#20851;&#25361;&#25112;&#12289;&#23454;&#36341;&#35265;&#35299;&#21644;&#28508;&#22312;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;LimGen&#25968;&#25454;&#38598;&#21644;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/armbf/LimGen &#19978;&#33719;&#21462;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15529v1 Announce Type: cross  Abstract: Examining limitations is a crucial step in the scholarly research reviewing process, revealing aspects where a study might lack decisiveness or require enhancement. This aids readers in considering broader implications for further research. In this article, we present a novel and challenging task of Suggestive Limitation Generation (SLG) for research papers. We compile a dataset called LimGen, encompassing 4068 research papers and their associated limitations from the ACL anthology. We investigate several approaches to harness large language models (LLMs) for producing suggestive limitations, by thoroughly examining the related challenges, practical insights, and potential opportunities. Our LimGen dataset and code can be accessed at https://github.com/armbf/LimGen.
&lt;/p&gt;</description></item><item><title>GPT-4V&#22312;&#33016;&#37096;X&#20809;&#25918;&#23556;&#26816;&#26597;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#26816;&#27979;&#19978;&#23578;&#26410;&#20934;&#22791;&#22909;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#35786;&#26029;&#29992;&#36884;</title><link>https://arxiv.org/abs/2403.15528</link><description>&lt;p&gt;
&#20351;&#29992;&#35270;&#35273;&#35780;&#20272;GPT-4&#22312;&#33016;&#37096;X&#20809;&#25918;&#23556;&#26816;&#26597;&#20013;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Evaluating GPT-4 with Vision on Detection of Radiological Findings on Chest Radiographs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15528
&lt;/p&gt;
&lt;p&gt;
GPT-4V&#22312;&#33016;&#37096;X&#20809;&#25918;&#23556;&#26816;&#26597;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#26816;&#27979;&#19978;&#23578;&#26410;&#20934;&#22791;&#22909;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#35786;&#26029;&#29992;&#36884;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25506;&#35752;&#20102;GPT-4V&#30340;&#24212;&#29992;&#65292;&#36825;&#26159;&#19968;&#20010;&#35013;&#22791;&#26377;&#35270;&#35273;&#35782;&#21035;&#21151;&#33021;&#30340;&#22810;&#27169;&#24577;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#29992;&#20110;&#26816;&#27979;&#26469;&#33258;100&#24352;&#33016;&#37096;X&#20809;&#25918;&#23556;&#26816;&#26597;&#30340;&#25918;&#23556;&#23398;&#21457;&#29616;&#65292;&#24182;&#25351;&#20986;&#30446;&#21069;GPT-4V&#36824;&#19981;&#36866;&#29992;&#20110;&#35299;&#37322;&#33016;&#37096;X&#20809;&#25918;&#23556;&#26816;&#26597;&#30340;&#23454;&#38469;&#35786;&#26029;&#29992;&#36884;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15528v1 Announce Type: cross  Abstract: The study examines the application of GPT-4V, a multi-modal large language model equipped with visual recognition, in detecting radiological findings from a set of 100 chest radiographs and suggests that GPT-4V is currently not ready for real-world diagnostic usage in interpreting chest radiographs.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35797;&#28857;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#22122;&#22768;&#26631;&#35760;&#21050;&#28608;&#21327;&#35758;&#36827;&#34892;&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>https://arxiv.org/abs/2403.15523</link><description>&lt;p&gt;
&#37319;&#29992;&#22122;&#22768;&#26631;&#35760;&#30340;&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;&#30740;&#31350;&#65306;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards auditory attention decoding with noise-tagging: A pilot study
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15523
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35797;&#28857;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#22122;&#22768;&#26631;&#35760;&#21050;&#28608;&#21327;&#35758;&#36827;&#34892;&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21548;&#35273;&#27880;&#24847;&#21147;&#35299;&#30721;(AAD)&#26088;&#22312;&#20174;&#22823;&#33041;&#27963;&#21160;&#20013;&#25552;&#21462;&#34987;&#20851;&#27880;&#30340;&#35828;&#35805;&#32773;&#65292;&#25552;&#20379;&#20102;&#31070;&#32463;&#23548;&#21521;&#21548;&#35273;&#35774;&#22791;&#21644;&#33041;&#26426;&#25509;&#21475;&#31561;&#39046;&#22495;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;&#26412;&#35797;&#28857;&#30740;&#31350;&#39318;&#27425;&#23581;&#35797;&#20351;&#29992;&#22122;&#22768;&#26631;&#35760;&#21050;&#28608;&#21327;&#35758;&#36827;&#34892;AAD&#65292;&#35813;&#21327;&#35758;&#24341;&#21457;&#20102;&#21487;&#38752;&#30340;&#32534;&#30721;&#35843;&#21046;&#35825;&#21457;&#30005;&#20301;&#65292;&#20294;&#22312;&#21548;&#35273;&#27169;&#24335;&#19979;&#30340;&#25506;&#32034;&#36824;&#24456;&#26377;&#38480;&#12290;&#30740;&#31350;&#21442;&#19982;&#32773;&#20381;&#27425;&#21576;&#29616;&#20004;&#20010;&#33655;&#20848;&#35821;&#35328;&#35821;&#38899;&#21050;&#28608;&#65292;&#36825;&#20123;&#21050;&#28608;&#34987;&#24133;&#24230;&#35843;&#21046;&#20026;&#20855;&#26377;&#21807;&#19968;&#20108;&#36827;&#21046;&#20266;&#38543;&#26426;&#22122;&#22768;&#30721;&#65292;&#26377;&#25928;&#22320;&#20026;&#20854;&#26631;&#35760;&#20102;&#38468;&#21152;&#21487;&#35299;&#30721;&#20449;&#24687;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#26410;&#35843;&#21046;&#38899;&#39057;&#19982;&#20351;&#29992;&#19981;&#21516;&#35843;&#21046;&#28145;&#24230;&#35843;&#21046;&#30340;&#38899;&#39057;&#30340;&#35299;&#30721;&#65292;&#20197;&#21450;&#20256;&#32479;AAD&#26041;&#27861;&#19982;&#26631;&#20934;&#35299;&#30721;&#22122;&#22768;&#30721;&#26041;&#27861;&#30340;&#23545;&#27604;&#12290;&#25105;&#20204;&#30340;&#35797;&#28857;&#30740;&#31350;&#21457;&#29616;&#65292;&#19982;&#26410;&#35843;&#21046;&#38899;&#39057;&#30456;&#27604;&#65292;70&#33267;100%&#30340;&#35843;&#21046;&#28145;&#24230;&#30340;&#20256;&#32479;&#26041;&#27861;&#34920;&#29616;&#20986;&#26356;&#39640;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15523v1 Announce Type: cross  Abstract: Auditory attention decoding (AAD) aims to extract from brain activity the attended speaker amidst candidate speakers, offering promising applications for neuro-steered hearing devices and brain-computer interfacing. This pilot study makes a first step towards AAD using the noise-tagging stimulus protocol, which evokes reliable code-modulated evoked potentials, but is minimally explored in the auditory modality. Participants were sequentially presented with two Dutch speech stimuli that were amplitude modulated with a unique binary pseudo-random noise-code, effectively tagging these with additional decodable information. We compared the decoding of unmodulated audio against audio modulated with various modulation depths, and a conventional AAD method against a standard method to decode noise-codes. Our pilot study revealed higher performances for the conventional method with 70 to 100 percent modulation depths compared to unmodulated au
&lt;/p&gt;</description></item><item><title>CTSM&#27169;&#22411;&#32467;&#21512;&#29305;&#36136;&#21644;&#29366;&#24577;&#24773;&#32490;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#32534;&#30721;&#24773;&#32490;&#23884;&#20837;&#20197;&#21450;&#24341;&#20837;&#24773;&#32490;&#24341;&#23548;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#22788;&#29702;&#24773;&#32490;&#24863;&#30693;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15516</link><description>&lt;p&gt;
CTSM&#65306;&#23558;&#29305;&#36136;&#21644;&#29366;&#24577;&#24773;&#32490;&#30456;&#32467;&#21512;&#30340;&#20849;&#24773;&#21709;&#24212;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
CTSM: Combining Trait and State Emotions for Empathetic Response Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15516
&lt;/p&gt;
&lt;p&gt;
CTSM&#27169;&#22411;&#32467;&#21512;&#29305;&#36136;&#21644;&#29366;&#24577;&#24773;&#32490;&#65292;&#36890;&#36807;&#26500;&#24314;&#21644;&#32534;&#30721;&#24773;&#32490;&#23884;&#20837;&#20197;&#21450;&#24341;&#20837;&#24773;&#32490;&#24341;&#23548;&#27169;&#22359;&#65292;&#35299;&#20915;&#20102;&#20808;&#21069;&#22788;&#29702;&#24773;&#32490;&#24863;&#30693;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#24773;&#24615;&#21709;&#24212;&#29983;&#25104;&#26088;&#22312;&#36171;&#20104;&#23545;&#35805;&#31995;&#32479;&#24863;&#30693;&#35828;&#35805;&#32773;&#24773;&#32490;&#24182;&#30456;&#24212;&#29983;&#25104;&#20849;&#24773;&#24615;&#22238;&#24212;&#30340;&#33021;&#21147;&#12290;&#24515;&#29702;&#30740;&#31350;&#34920;&#26126;&#65292;&#20316;&#20026;&#20849;&#24773;&#30340;&#19968;&#20010;&#37325;&#35201;&#22240;&#32032;&#65292;&#24773;&#32490;&#21253;&#25324;&#29305;&#36136;&#24773;&#32490;&#65288;&#38745;&#24577;&#19988;&#19982;&#29615;&#22659;&#26080;&#20851;&#65289;&#21644;&#29366;&#24577;&#24773;&#32490;&#65288;&#21160;&#24577;&#19988;&#19982;&#29615;&#22659;&#30456;&#20851;&#65289;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#30740;&#31350;&#23558;&#23427;&#20204;&#21333;&#29420;&#22788;&#29702;&#65292;&#23548;&#33268;&#23545;&#24773;&#22659;&#24773;&#32490;&#30340;&#24863;&#30693;&#19981;&#36275;&#65292;&#36827;&#32780;&#24433;&#21709;&#20102;&#26377;&#25928;&#30340;&#20849;&#24773;&#34920;&#36798;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23558;&#29305;&#36136;&#21644;&#29366;&#24577;&#24773;&#32490;&#30456;&#32467;&#21512;&#30340;&#20849;&#24773;&#21709;&#24212;&#27169;&#22411;&#65288;CTSM&#65289;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#20026;&#20102;&#20805;&#20998;&#24863;&#30693;&#23545;&#35805;&#20013;&#30340;&#24773;&#32490;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#21644;&#32534;&#30721;&#29305;&#36136;&#21644;&#29366;&#24577;&#24773;&#32490;&#23884;&#20837;&#65292;&#28982;&#21518;&#36890;&#36807;&#19968;&#20010;&#24773;&#32490;&#24341;&#23548;&#27169;&#22359;&#36827;&#19968;&#27493;&#22686;&#24378;&#24773;&#32490;&#24863;&#30693;&#33021;&#21147;&#65292;&#35813;&#27169;&#22359;&#25351;&#23548;&#24773;&#32490;&#34920;&#36798;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20132;&#21449;&#23545;&#27604;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15516v1 Announce Type: cross  Abstract: Empathetic response generation endeavors to empower dialogue systems to perceive speakers' emotions and generate empathetic responses accordingly. Psychological research demonstrates that emotion, as an essential factor in empathy, encompasses trait emotions, which are static and context-independent, and state emotions, which are dynamic and context-dependent. However, previous studies treat them in isolation, leading to insufficient emotional perception of the context, and subsequently, less effective empathetic expression. To address this problem, we propose Combining Trait and State emotions for Empathetic Response Model (CTSM). Specifically, to sufficiently perceive emotions in dialogue, we first construct and encode trait and state emotion embeddings, and then we further enhance emotional perception capability through an emotion guidance module that guides emotion representation. In addition, we propose a cross-contrastive learnin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#31227;&#21160;&#28508;&#22312;&#29305;&#24449;&#12289;&#37325;&#26500;&#29983;&#25104;&#27169;&#31946;&#29256;&#26412;&#20197;&#21450;&#37319;&#29992;&#20013;K&#37319;&#26679;&#26469;&#22686;&#24378;&#29983;&#25104;&#21477;&#23376;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#27604;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15512</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#25968;&#25454;&#22686;&#24378;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#25552;&#39640;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Effectiveness and Robustness in a Low-Resource Regime via Decision-Boundary-aware Data Augmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#36890;&#36807;&#31227;&#21160;&#28508;&#22312;&#29305;&#24449;&#12289;&#37325;&#26500;&#29983;&#25104;&#27169;&#31946;&#29256;&#26412;&#20197;&#21450;&#37319;&#29992;&#20013;K&#37319;&#26679;&#26469;&#22686;&#24378;&#29983;&#25104;&#21477;&#23376;&#30340;&#22810;&#26679;&#24615;&#65292;&#20174;&#32780;&#27604;&#36739;&#20110;&#20854;&#20182;&#26041;&#27861;&#25552;&#39640;&#20102;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#30340;&#26377;&#25928;&#24615;&#21644;&#31283;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#20013;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#38656;&#35201;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#20294;&#30452;&#25509;&#26041;&#27861;&#65288;&#22914;mixup&#21644;cutout&#65289;&#22312;&#25991;&#26412;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#21463;&#38480;&#20110;&#20854;&#31163;&#25955;&#29305;&#24615;&#12290;&#26412;&#25991;&#21463;&#21040;&#20915;&#31574;&#36793;&#30028;&#30340;&#26368;&#26032;&#30740;&#31350;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20915;&#31574;&#36793;&#30028;&#24863;&#30693;&#30340;&#25968;&#25454;&#22686;&#24378;&#31574;&#30053;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#22686;&#24378;&#31283;&#20581;&#24615;&#12290;&#35813;&#25216;&#26415;&#39318;&#20808;&#19987;&#27880;&#20110;&#23558;&#28508;&#22312;&#29305;&#24449;&#31227;&#36817;&#20915;&#31574;&#36793;&#30028;&#65292;&#28982;&#21518;&#36827;&#34892;&#37325;&#26500;&#20197;&#29983;&#25104;&#19968;&#20010;&#24102;&#26377;&#36719;&#26631;&#31614;&#30340;&#27169;&#31946;&#29256;&#26412;&#12290;&#27492;&#22806;&#65292;&#24314;&#35758;&#20351;&#29992;&#20013;K&#37319;&#26679;&#26469;&#22686;&#24378;&#29983;&#25104;&#21477;&#23376;&#30340;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15512v1 Announce Type: cross  Abstract: Efforts to leverage deep learning models in low-resource regimes have led to numerous augmentation studies. However, the direct application of methods such as mixup and cutout to text data, is limited due to their discrete characteristics. While methods using pretrained language models have exhibited efficiency, they require additional considerations for robustness. Inspired by recent studies on decision boundaries, this paper proposes a decision-boundary-aware data augmentation strategy to enhance robustness using pretrained language models. The proposed technique first focuses on shifting the latent features closer to the decision boundary, followed by reconstruction to generate an ambiguous version with a soft label. Additionally, mid-K sampling is suggested to enhance the diversity of the generated sentences. This paper demonstrates the performance of the proposed augmentation strategy compared to other methods through extensive ex
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#36755;&#20837;&#33258;&#21160;&#32534;&#30721;&#22120;(MIAE)&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#35757;&#32451;MIAE&#27169;&#22411;&#65292;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#24335;&#19979;&#23558;&#24322;&#26500;&#36755;&#20837;&#36716;&#25442;&#20026;&#36739;&#20302;&#32500;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#20998;&#31867;&#22120;&#21306;&#20998;&#27491;&#24120;&#34892;&#20026;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.15511</link><description>&lt;p&gt;
IoT&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#30340;&#22810;&#36755;&#20837;&#33258;&#21160;&#32534;&#30721;&#22120;&#24341;&#23548;&#29305;&#24449;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Multiple-Input Auto-Encoder Guided Feature Selection for IoT Intrusion Detection Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15511
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#36755;&#20837;&#33258;&#21160;&#32534;&#30721;&#22120;(MIAE)&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#36890;&#36807;&#35757;&#32451;MIAE&#27169;&#22411;&#65292;&#22312;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#24335;&#19979;&#23558;&#24322;&#26500;&#36755;&#20837;&#36716;&#25442;&#20026;&#36739;&#20302;&#32500;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#20998;&#31867;&#22120;&#21306;&#20998;&#27491;&#24120;&#34892;&#20026;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;(IDSs)&#21463;&#30410;&#20110;IoT&#25968;&#25454;&#29305;&#24449;&#30340;&#22810;&#26679;&#24615;&#21644;&#27867;&#21270;&#65292;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#20351;&#24471;&#22312;IoT IDSs&#20013;&#35757;&#32451;&#26377;&#25928;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#22256;&#38590;&#12290;&#26412;&#25991;&#39318;&#20808;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;&#22810;&#36755;&#20837;&#33258;&#21160;&#32534;&#30721;&#22120;(MIAE)&#30340;&#26032;&#22411;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#12290;MIAE&#30001;&#22810;&#20010;&#23376;&#32534;&#30721;&#22120;&#32452;&#25104;&#65292;&#21487;&#20197;&#22788;&#29702;&#20855;&#26377;&#19981;&#21516;&#29305;&#24449;&#30340;&#19981;&#21516;&#26469;&#28304;&#30340;&#36755;&#20837;&#12290; MIAE&#27169;&#22411;&#20197;&#26080;&#30417;&#30563;&#23398;&#20064;&#27169;&#24335;&#36827;&#34892;&#35757;&#32451;&#65292;&#23558;&#24322;&#26500;&#36755;&#20837;&#36716;&#25442;&#20026;&#36739;&#20302;&#32500;&#34920;&#31034;&#65292;&#26377;&#21161;&#20110;&#20998;&#31867;&#22120;&#21306;&#20998;&#27491;&#24120;&#34892;&#20026;&#21644;&#19981;&#21516;&#31867;&#22411;&#30340;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15511v1 Announce Type: cross  Abstract: While intrusion detection systems (IDSs) benefit from the diversity and generalization of IoT data features, the data diversity (e.g., the heterogeneity and high dimensions of data) also makes it difficult to train effective machine learning models in IoT IDSs. This also leads to potentially redundant/noisy features that may decrease the accuracy of the detection engine in IDSs. This paper first introduces a novel neural network architecture called Multiple-Input Auto-Encoder (MIAE). MIAE consists of multiple sub-encoders that can process inputs from different sources with different characteristics. The MIAE model is trained in an unsupervised learning mode to transform the heterogeneous inputs into lower-dimensional representation, which helps classifiers distinguish between normal behaviour and different types of attacks. To distil and retain more relevant features but remove less important/redundant ones during the training process,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#21452;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;(TAE)&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#34920;&#31034;&#36716;&#25442;&#20026;&#21487;&#20998;&#31163;&#34920;&#31034;&#26469;&#35299;&#20915;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#20013;&#28151;&#21512;&#34920;&#31034;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.15509</link><description>&lt;p&gt;
&#21452;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;&#29992;&#20110;&#23398;&#20064;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#20013;&#30340;&#21487;&#20998;&#31163;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Twin Auto-Encoder Model for Learning Separable Representation in Cyberattack Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15509
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#21452;&#33258;&#21160;&#32534;&#30721;&#22120;&#27169;&#22411;(TAE)&#65292;&#36890;&#36807;&#23558;&#28508;&#22312;&#34920;&#31034;&#36716;&#25442;&#20026;&#21487;&#20998;&#31163;&#34920;&#31034;&#26469;&#35299;&#20915;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#20013;&#28151;&#21512;&#34920;&#31034;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34920;&#24449;&#23398;&#20064;&#22312;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#31561;&#35768;&#22810;&#38382;&#39064;&#30340;&#25104;&#21151;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#22823;&#22810;&#25968;&#32593;&#32476;&#25915;&#20987;&#26816;&#27979;&#30340;&#34920;&#24449;&#23398;&#20064;&#26041;&#27861;&#22522;&#20110;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;AE&#65289;&#27169;&#22411;&#30340;&#28508;&#22312;&#21521;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;AEs&#34920;&#31034;&#20013;&#28151;&#21512;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#21452;&#33258;&#21160;&#32534;&#30721;&#22120;&#65288;TAE&#65289;&#30340;&#26032;&#22411;&#27169;&#22411;&#12290;TAE&#23558;&#28508;&#22312;&#34920;&#31034;&#30830;&#23450;&#22320;&#36716;&#25442;&#20026;&#26356;&#26131;&#21306;&#20998;&#30340;&#34920;&#31034;&#65292;&#21363;\textit{&#21487;&#20998;&#31163;&#34920;&#31034;}&#65292;&#24182;&#22312;&#36755;&#20986;&#31471;&#37325;&#24314;&#21487;&#20998;&#31163;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15509v1 Announce Type: cross  Abstract: Representation Learning (RL) plays a pivotal role in the success of many problems including cyberattack detection. Most of the RL methods for cyberattack detection are based on the latent vector of Auto-Encoder (AE) models. An AE transforms raw data into a new latent representation that better exposes the underlying characteristics of the input data. Thus, it is very useful for identifying cyberattacks. However, due to the heterogeneity and sophistication of cyberattacks, the representation of AEs is often entangled/mixed resulting in the difficulty for downstream attack detection models. To tackle this problem, we propose a novel mod called Twin Auto-Encoder (TAE). TAE deterministically transforms the latent representation into a more distinguishable representation namely the \textit{separable representation} and the reconstructsuct the separable representation at the output. The output of TAE called the \textit{reconstruction represe
&lt;/p&gt;</description></item><item><title>SymboSLAM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31526;&#21495;&#21270;&#21516;&#26102;&#23450;&#20301;&#21644;&#22320;&#22270;&#21046;&#22270;&#26469;&#23454;&#29616;&#29615;&#22659;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15504</link><description>&lt;p&gt;
SymboSLAM: &#22810;Agent&#31995;&#32479;&#20013;&#30340;&#35821;&#20041;&#22320;&#22270;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SymboSLAM: Semantic Map Generation in a Multi-Agent System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15504
&lt;/p&gt;
&lt;p&gt;
SymboSLAM&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#31526;&#21495;&#21270;&#21516;&#26102;&#23450;&#20301;&#21644;&#22320;&#22270;&#21046;&#22270;&#26469;&#23454;&#29616;&#29615;&#22659;&#20998;&#31867;&#65292;&#35299;&#20915;&#20102;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#35299;&#37322;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Sub-symbolic&#20154;&#24037;&#26234;&#33021;&#26041;&#27861;&#22312;&#29615;&#22659;&#20998;&#31867;&#21644;&#21516;&#26102;&#23450;&#20301;&#19982;&#22320;&#22270;&#21046;&#22270;&#39046;&#22495;&#21344;&#20027;&#23548;&#22320;&#20301;&#12290;&#28982;&#32780;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#34987;&#24573;&#35270;&#30340;&#19968;&#20010;&#37325;&#35201;&#39046;&#22495;&#26159;&#35299;&#20915;&#26041;&#26696;&#36879;&#26126;&#24615;&#65292;&#22240;&#20026;&#29992;&#20110;&#22320;&#22270;&#29983;&#25104;&#30340;&#27425;&#31526;&#21495;&#26041;&#27861;&#24182;&#26410;&#32771;&#34385;&#25152;&#29983;&#25104;&#35299;&#20915;&#26041;&#26696;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29615;&#22659;&#20998;&#31867;&#26041;&#27861;&#65292;&#36890;&#36807;&#31526;&#21495;&#21516;&#26102;&#23450;&#20301;&#19982;&#22320;&#22270;&#21046;&#22270;&#65292;&#21363;SymboSLAM&#65292;&#26469;&#24357;&#34917;&#21487;&#35299;&#37322;&#24615;&#24046;&#36317;&#12290;&#25105;&#20204;&#30340;&#29615;&#22659;&#20998;&#31867;&#26041;&#27861;&#36890;&#36807;&#35266;&#23519;&#26412;&#20307;&#25512;&#29702;&#65292;&#20197;&#21512;&#25104;&#29615;&#22659;&#30340;&#19978;&#19979;&#25991;&#65292;&#36890;&#36807;&#25152;&#21457;&#29616;&#30340;&#29305;&#24449;&#12290;&#25105;&#20204;&#36890;&#36807;&#21521;&#25805;&#20316;&#21592;&#21576;&#29616;&#21472;&#21152;&#20102;&#35821;&#20041;&#26631;&#35760;&#30340;&#22320;&#26631;&#21644;&#29305;&#24449;&#30340;&#29615;&#22659;&#20998;&#31867;&#65292;&#23454;&#29616;&#20102;&#27169;&#22411;&#20869;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#22320;&#38754;&#23454;&#20917;&#35780;&#20272;&#20102;SymboSLAM&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15504v1 Announce Type: new  Abstract: Sub-symbolic artificial intelligence methods dominate the fields of environment-type classification and Simultaneous Localisation and Mapping. However, a significant area overlooked within these fields is solution transparency for the human-machine interaction space, as the sub-symbolic methods employed for map generation do not account for the explainability of the solutions generated. This paper proposes a novel approach to environment-type classification through Symbolic Simultaneous Localisation and Mapping, SymboSLAM, to bridge the explainability gap. Our method for environment-type classification observes ontological reasoning used to synthesise the context of an environment through the features found within. We achieve explainability within the model by presenting operators with environment-type classifications overlayed by a semantically labelled occupancy map of landmarks and features. We evaluate SymboSLAM with ground-truth map
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;Causal Machine Learning&#26041;&#27861;&#20998;&#26512;&#30005;&#21147;&#24066;&#22330;&#20013;&#23450;&#20215;&#25919;&#31574;&#23545;CO2&#27700;&#24179;&#30340;&#24433;&#21709;&#65292;&#25361;&#25112;&#20256;&#32479;&#26234;&#24935;&#65292;&#21457;&#29616;&#21487;&#33021;&#22686;&#21152;CO2&#24378;&#24230;&#65292;&#24182;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#20803;&#31639;&#27861;&#22686;&#24378;&#30740;&#31350;&#28145;&#24230;&#65292;&#24182;&#25552;&#20379;&#23453;&#36149;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.15499</link><description>&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#39537;&#21160;&#30340;&#20803;&#23398;&#20064;&#22120;&#23545;&#30005;&#21147;&#24066;&#22330;&#20013;CO2&#20943;&#25490;&#31574;&#30053;&#36827;&#34892;&#22240;&#26524;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
A Causal Analysis of CO2 Reduction Strategies in Electricity Markets Through Machine Learning-Driven Metalearners
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15499
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;Causal Machine Learning&#26041;&#27861;&#20998;&#26512;&#30005;&#21147;&#24066;&#22330;&#20013;&#23450;&#20215;&#25919;&#31574;&#23545;CO2&#27700;&#24179;&#30340;&#24433;&#21709;&#65292;&#25361;&#25112;&#20256;&#32479;&#26234;&#24935;&#65292;&#21457;&#29616;&#21487;&#33021;&#22686;&#21152;CO2&#24378;&#24230;&#65292;&#24182;&#32467;&#21512;&#26426;&#22120;&#23398;&#20064;&#20803;&#31639;&#27861;&#22686;&#24378;&#30740;&#31350;&#28145;&#24230;&#65292;&#24182;&#25552;&#20379;&#23453;&#36149;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;&#22240;&#26524;&#26426;&#22120;&#23398;&#20064;&#65288;CausalML&#65289;&#32479;&#35745;&#26041;&#27861;&#65292;&#20998;&#26512;&#30005;&#21147;&#23450;&#20215;&#25919;&#31574;&#23545;&#23478;&#24237;&#37096;&#38376;&#20108;&#27687;&#21270;&#30899;&#65288;CO2&#65289;&#27700;&#24179;&#30340;&#24433;&#21709;&#12290;&#30740;&#31350;&#35843;&#26597;&#28508;&#22312;&#32467;&#26524;&#19982;&#22788;&#29702;&#25928;&#26524;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#65292;&#20854;&#20013;&#23450;&#20215;&#25919;&#31574;&#30340;&#21464;&#21270;&#26159;&#22788;&#29702;&#25928;&#26524;&#65292;&#25105;&#20204;&#30340;&#20998;&#26512;&#25361;&#25112;&#20102;&#22260;&#32469;&#22522;&#20110;&#28608;&#21169;&#30340;&#30005;&#21147;&#23450;&#20215;&#30340;&#20256;&#32479;&#26234;&#24935;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#37319;&#29992;&#36825;&#20123;&#25919;&#31574;&#21487;&#33021;&#20250;&#26080;&#24847;&#20013;&#22686;&#21152;CO2&#24378;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25972;&#21512;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20803;&#31639;&#27861;&#65292;&#21453;&#26144;&#20102;&#24403;&#20195;&#32479;&#35745;&#26041;&#27861;&#65292;&#20197;&#22686;&#24378;&#25105;&#20204;&#30340;&#22240;&#26524;&#20998;&#26512;&#28145;&#24230;&#12290;&#35813;&#30740;&#31350;&#23545;&#23398;&#20064;&#32773;X&#12289;T&#12289;S&#21644;R&#36827;&#34892;&#20102;&#27604;&#36739;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#22522;&#20110;&#35268;&#23450;&#38382;&#39064;&#30340;&#20855;&#20307;&#30446;&#26631;&#21644;&#32972;&#26223;&#32454;&#24494;&#20043;&#22788;&#30340;&#26368;&#20339;&#26041;&#27861;&#12290;&#36825;&#39033;&#30740;&#31350;&#20026;&#21487;&#25345;&#32493;&#21457;&#23637;&#38382;&#39064;&#19978;&#30340;&#25345;&#32493;&#23545;&#35805;&#25552;&#20379;&#20102;&#23453;&#36149;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15499v1 Announce Type: cross  Abstract: This study employs the Causal Machine Learning (CausalML) statistical method to analyze the influence of electricity pricing policies on carbon dioxide (CO2) levels in the household sector. Investigating the causality between potential outcomes and treatment effects, where changes in pricing policies are the treatment, our analysis challenges the conventional wisdom surrounding incentive-based electricity pricing. The study's findings suggest that adopting such policies may inadvertently increase CO2 intensity. Additionally, we integrate a machine learning-based meta-algorithm, reflecting a contemporary statistical approach, to enhance the depth of our causal analysis. The study conducts a comparative analysis of learners X, T, S, and R to ascertain the optimal methods based on the defined question's specified goals and contextual nuances. This research contributes valuable insights to the ongoing dialogue on sustainable development pr
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#23558;&#20010;&#20307;&#30340;&#26377;&#26465;&#20214;&#35782;&#21035;&#20449;&#24687;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#34920;&#31034;&#65292;&#20174;&#32780;&#25913;&#21892;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15489</link><description>&lt;p&gt;
&#20351;&#29992;&#26377;&#26465;&#20214;&#35782;&#21035;&#20449;&#24687;&#30340;&#33041;&#30005;&#22270;&#35299;&#30721;
&lt;/p&gt;
&lt;p&gt;
EEG decoding with conditional identification information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15489
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23558;&#20010;&#20307;&#30340;&#26377;&#26465;&#20214;&#35782;&#21035;&#20449;&#24687;&#25972;&#21512;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#65292;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#22686;&#24378;&#27169;&#22411;&#34920;&#31034;&#65292;&#20174;&#32780;&#25913;&#21892;&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#35299;&#30721;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33041;&#30005;&#22270;&#20449;&#21495;&#30340;&#35299;&#30721;&#23545;&#20110;&#25581;&#31034;&#20154;&#31867;&#22823;&#33041;&#24182;&#25512;&#21160;&#33041;&#26426;&#25509;&#21475;&#33267;&#20851;&#37325;&#35201;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#21463;&#21040;&#33041;&#30005;&#22270;&#20449;&#21495;&#20013;&#39640;&#22122;&#38899;&#27700;&#24179;&#21644;&#20010;&#20307;&#38388;&#22266;&#26377;&#21464;&#21270;&#30340;&#38459;&#30861;&#12290;&#26368;&#36817;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNNs&#65289;&#30340;&#36827;&#23637;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#24402;&#21151;&#20110;&#20854;&#20808;&#36827;&#30340;&#38750;&#32447;&#24615;&#24314;&#27169;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;DNN&#22312;&#35299;&#30721;&#26410;&#35265;&#20010;&#20307;&#30340;&#33041;&#30005;&#22270;&#26679;&#26412;&#26041;&#38754;&#20173;&#38754;&#20020;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#27599;&#20010;&#20010;&#20307;&#30340;&#26377;&#26465;&#20214;&#35782;&#21035;&#20449;&#24687;&#32435;&#20837;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#36890;&#36807;&#33041;&#30005;&#22270;&#21644;&#20010;&#20154;&#29305;&#24449;&#30340;&#21327;&#21516;&#20316;&#29992;&#22686;&#24378;&#27169;&#22411;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;WithMe&#25968;&#25454;&#38598;&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#35777;&#26126;&#21253;&#21547;&#36825;&#20123;&#26631;&#35782;&#31526;&#26174;&#33879;&#25552;&#39640;&#20102;&#35757;&#32451;&#38598;&#20013;&#30340;&#20027;&#20307;&#21644;&#26410;&#35265;&#20027;&#20307;&#30340;&#20934;&#30830;&#24615;&#12290;&#36825;&#31181;&#22686;&#24378;&#26174;&#31034;&#20102;&#25913;&#36827;&#33041;&#30005;&#22270;&#35299;&#30721;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15489v1 Announce Type: cross  Abstract: Decoding EEG signals is crucial for unraveling human brain and advancing brain-computer interfaces. Traditional machine learning algorithms have been hindered by the high noise levels and inherent inter-person variations in EEG signals. Recent advances in deep neural networks (DNNs) have shown promise, owing to their advanced nonlinear modeling capabilities. However, DNN still faces challenge in decoding EEG samples of unseen individuals. To address this, this paper introduces a novel approach by incorporating the conditional identification information of each individual into the neural network, thereby enhancing model representation through the synergistic interaction of EEG and personal traits. We test our model on the WithMe dataset and demonstrated that the inclusion of these identifiers substantially boosts accuracy for both subjects in the training set and unseen subjects. This enhancement suggests promising potential for improvi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#39318;&#27425;&#22312;&#26790;&#22659;&#21465;&#20107;&#20013;&#36827;&#34892;&#35282;&#33394;&#21644;&#24773;&#24863;&#26816;&#27979;&#30740;&#31350;&#65292;&#23637;&#31034;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#35813;&#22797;&#26434;&#20219;&#21153;&#65292;&#30417;&#30563;&#27169;&#22411;&#34920;&#29616;&#26356;&#20339;&#19988;&#21442;&#25968;&#26356;&#23569;&#12290;</title><link>https://arxiv.org/abs/2403.15486</link><description>&lt;p&gt;
&#24207;&#21015;&#21040;&#24207;&#21015;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#26790;&#22659;&#21465;&#20107;&#20013;&#30340;&#35282;&#33394;&#21644;&#24773;&#24863;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Sequence-to-Sequence Language Models for Character and Emotion Detection in Dream Narratives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15486
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#24207;&#21015;&#21040;&#24207;&#21015;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;&#65292;&#39318;&#27425;&#22312;&#26790;&#22659;&#21465;&#20107;&#20013;&#36827;&#34892;&#35282;&#33394;&#21644;&#24773;&#24863;&#26816;&#27979;&#30740;&#31350;&#65292;&#23637;&#31034;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#24212;&#23545;&#35813;&#22797;&#26434;&#20219;&#21153;&#65292;&#30417;&#30563;&#27169;&#22411;&#34920;&#29616;&#26356;&#20339;&#19988;&#21442;&#25968;&#26356;&#23569;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26790;&#22659;&#30740;&#31350;&#23545;&#20110;&#29702;&#35299;&#20154;&#31867;&#30340;(&#38750;)&#24847;&#35782;&#12289;&#35748;&#30693;&#21644;&#25991;&#21270;&#25968;&#20010;&#19990;&#32426;&#26469;&#19968;&#30452;&#33267;&#20851;&#37325;&#35201;&#12290;&#23450;&#37327;&#20998;&#26512;&#26790;&#22659;&#20381;&#36182;&#20110;&#23545;&#26790;&#22659;&#21465;&#36848;&#30340;&#21171;&#21160;&#23494;&#38598;&#22411;&#25163;&#21160;&#27880;&#37322;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#33258;&#28982;&#35821;&#35328;&#24207;&#21015;&#21040;&#24207;&#21015;&#29983;&#25104;&#26694;&#26550;&#33258;&#21160;&#21270;&#36825;&#19968;&#36807;&#31243;&#12290;&#26412;&#25991;&#39318;&#27425;&#22312;&#26790;&#22659;&#21465;&#20107;&#30340;&#24320;&#25918;DreamBank&#35821;&#26009;&#24211;&#33521;&#25991;&#37096;&#20998;&#20013;&#36827;&#34892;&#20102;&#35282;&#33394;&#21644;&#24773;&#24863;&#26816;&#27979;&#30740;&#31350;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#19968;&#22797;&#26434;&#20219;&#21153;&#12290;&#20026;&#20102;&#20102;&#35299;&#39044;&#27979;&#24615;&#33021;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#27169;&#22411;&#22823;&#23567;&#12289;&#35282;&#33394;&#30340;&#39044;&#27979;&#39034;&#24207;&#20197;&#21450;&#23545;&#19987;&#26377;&#21517;&#31216;&#21644;&#35282;&#33394;&#29305;&#24449;&#30340;&#32771;&#34385;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20351;&#29992;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#30340;&#30417;&#30563;&#27169;&#22411;&#34920;&#29616;&#26356;&#22909;&#65292;&#21516;&#26102;&#21442;&#25968;&#25968;&#37327;&#20943;&#23569;&#20102;28&#20493;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#21450;&#20854;&#29983;&#25104;&#30340;&#27880;&#37322;&#24050;&#20844;&#24320;&#21487;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15486v1 Announce Type: cross  Abstract: The study of dreams has been central to understanding human (un)consciousness, cognition, and culture for centuries. Analyzing dreams quantitatively depends on labor-intensive, manual annotation of dream narratives. We automate this process through a natural language sequence-to-sequence generation framework. This paper presents the first study on character and emotion detection in the English portion of the open DreamBank corpus of dream narratives. Our results show that language models can effectively address this complex task. To get insight into prediction performance, we evaluate the impact of model size, prediction order of characters, and the consideration of proper names and character traits. We compare our approach with a large language model using in-context learning. Our supervised models perform better while having 28 times fewer parameters. Our model and its generated annotations are made publicly available.
&lt;/p&gt;</description></item><item><title>MOGAM&#27169;&#22411;&#26159;&#20026;&#20102;&#20811;&#26381;&#29616;&#26377;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#27861;&#22312;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#31867;&#22411;&#19978;&#30340;&#38480;&#21046;&#32780;&#25552;&#20986;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;</title><link>https://arxiv.org/abs/2403.15485</link><description>&lt;p&gt;
MOGAM&#65306;&#19968;&#31181;&#29992;&#20110;&#25233;&#37057;&#30151;&#26816;&#27979;&#30340;&#22810;&#27169;&#24577;&#38754;&#21521;&#23545;&#35937;&#22270;&#27880;&#24847;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
MOGAM: A Multimodal Object-oriented Graph Attention Model for Depression Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15485
&lt;/p&gt;
&lt;p&gt;
MOGAM&#27169;&#22411;&#26159;&#20026;&#20102;&#20811;&#26381;&#29616;&#26377;&#25233;&#37057;&#30151;&#26816;&#27979;&#26041;&#27861;&#22312;&#19981;&#21516;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#31867;&#22411;&#19978;&#30340;&#38480;&#21046;&#32780;&#25552;&#20986;&#65292;&#36890;&#36807;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#32508;&#21512;&#24212;&#29992;&#65292;&#23454;&#29616;&#20102;&#26356;&#20855;&#21487;&#25193;&#23637;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26089;&#26399;&#26816;&#27979;&#22312;&#25233;&#37057;&#30151;&#27835;&#30103;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22240;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#20851;&#27880;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#65292;&#20010;&#20307;&#22312;&#35813;&#24179;&#21488;&#34920;&#36798;&#24773;&#32490;&#65292;&#26088;&#22312;&#23454;&#29616;&#25233;&#37057;&#30151;&#30340;&#26089;&#26399;&#26816;&#27979;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#20381;&#36182;&#29305;&#23450;&#29305;&#24449;&#65292;&#23548;&#33268;&#22312;&#19981;&#21516;&#31867;&#22411;&#30340;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#65288;&#22914;&#25991;&#26412;&#12289;&#22270;&#20687;&#25110;&#35270;&#39057;&#65289;&#19978;&#30340;&#21487;&#25193;&#23637;&#24615;&#26377;&#38480;&#12290;&#20026;&#20811;&#26381;&#36825;&#19968;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22810;&#27169;&#24577;&#38754;&#21521;&#23545;&#35937;&#22270;&#27880;&#24847;&#27169;&#22411;&#65288;MOGAM&#65289;&#65292;&#21487;&#24212;&#29992;&#20110;&#21508;&#31181;&#25968;&#25454;&#31867;&#22411;&#65292;&#25552;&#20379;&#26356;&#20855;&#21487;&#20280;&#32553;&#24615;&#21644;&#22810;&#21151;&#33021;&#24615;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#27492;&#22806;&#65292;&#20026;&#30830;&#20445;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#25429;&#25417;&#25233;&#37057;&#30151;&#30340;&#30495;&#23454;&#30151;&#29366;&#65292;&#25105;&#20204;&#20165;&#25910;&#38598;&#20102;&#20855;&#26377;&#20020;&#24202;&#35786;&#26029;&#30340;&#29992;&#25143;&#30340;&#35270;&#39057;&#26085;&#24535;&#12290;&#20026;&#20102;&#21033;&#29992;&#35270;&#39057;&#26085;&#24535;&#30340;&#22810;&#26679;&#29305;&#24449;&#65292;&#25105;&#20204;&#37319;&#29992;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#24182;&#25910;&#38598;&#39069;&#22806;&#30340;&#20803;&#25968;&#25454;&#65292;&#22914;&#35270;&#39057;&#26085;&#24535;&#30340;&#26631;&#39064;&#12289;&#25551;&#36848;&#21644;&#25345;&#32493;&#26102;&#38388;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#32858;&#21512;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15485v1 Announce Type: cross  Abstract: Early detection plays a crucial role in the treatment of depression. Therefore, numerous studies have focused on social media platforms, where individuals express their emotions, aiming to achieve early detection of depression. However, the majority of existing approaches often rely on specific features, leading to limited scalability across different types of social media datasets, such as text, images, or videos. To overcome this limitation, we introduce a Multimodal Object-Oriented Graph Attention Model (MOGAM), which can be applied to diverse types of data, offering a more scalable and versatile solution. Furthermore, to ensure that our model can capture authentic symptoms of depression, we only include vlogs from users with a clinical diagnosis. To leverage the diverse features of vlogs, we adopt a multimodal approach and collect additional metadata such as the title, description, and duration of the vlogs. To effectively aggregat
&lt;/p&gt;</description></item><item><title>AI&#20174;&#19994;&#32773;&#23545;&#20110;&#20844;&#24179;AI/ML&#30340;&#29702;&#35299;&#12289;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#19981;&#20844;&#24179;AI/ML&#30340;&#21518;&#26524;&#20197;&#21450;&#30830;&#20445;AI/ML&#20844;&#24179;&#24615;&#30340;&#31574;&#30053;&#12290;</title><link>https://arxiv.org/abs/2403.15481</link><description>&lt;p&gt;
AI/ML &#21457;&#23637;&#20013;&#30340;&#20844;&#24179;&#23548;&#33322;: &#20174;&#19994;&#32773;&#23545;AI/ML&#24320;&#21457;&#20013;&#30340;&#29702;&#35299;&#12289;&#25361;&#25112;&#21644;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Navigating Fairness: Practitioners' Understanding, Challenges, and Strategies in AI/ML Development
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15481
&lt;/p&gt;
&lt;p&gt;
AI&#20174;&#19994;&#32773;&#23545;&#20110;&#20844;&#24179;AI/ML&#30340;&#29702;&#35299;&#12289;&#38754;&#20020;&#30340;&#25361;&#25112;&#12289;&#19981;&#20844;&#24179;AI/ML&#30340;&#21518;&#26524;&#20197;&#21450;&#30830;&#20445;AI/ML&#20844;&#24179;&#24615;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#21508;&#34892;&#19994;&#23545;AI/ML&#24212;&#29992;&#30340;&#22686;&#21152;&#24341;&#21457;&#20102;&#23545;AI/ML&#20844;&#24179;&#24615;&#30340;&#26356;&#22810;&#35752;&#35770;&#12290;&#34429;&#28982;&#24050;&#26377;&#20851;&#20110;AI/ML&#20844;&#24179;&#24615;&#30340;&#20808;&#21069;&#30740;&#31350;&#65292;&#20294;&#32570;&#20047;&#38024;&#23545;&#20102;&#35299;AI&#20174;&#19994;&#32773;&#22312;&#24320;&#21457;&#20844;&#24179;AI/ML&#36807;&#31243;&#20013;&#30340;&#35266;&#28857;&#21644;&#32463;&#39564;&#30340;&#23454;&#35777;&#30740;&#31350;&#12290;&#20102;&#35299;AI&#20174;&#19994;&#32773;&#23545;AI/ML&#20844;&#24179;&#24615;&#30340;&#30475;&#27861;&#21644;&#32463;&#39564;&#24456;&#37325;&#35201;&#65292;&#22240;&#20026;&#20182;&#20204;&#30452;&#25509;&#21442;&#19982;&#20854;&#20013;&#30340;&#24320;&#21457;&#21644;&#37096;&#32626;&#65292;&#20182;&#20204;&#30340;&#35265;&#35299;&#21487;&#20197;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#29616;&#23454;&#19990;&#30028;&#35270;&#35282;&#65292;&#24110;&#21161;&#29702;&#35299;&#30830;&#20445;AI/ML&#20844;&#24179;&#24615;&#25152;&#28041;&#21450;&#25361;&#25112;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;22&#20301;AI&#20174;&#19994;&#32773;&#30340;&#21322;&#32467;&#26500;&#21270;&#35775;&#35848;&#65292;&#20197;&#35843;&#26597;&#20182;&#20204;&#23545;&#8220;&#20844;&#24179;AI/ML&#8221;&#26159;&#20160;&#20040;&#30340;&#29702;&#35299;&#65292;&#20182;&#20204;&#22312;&#24320;&#21457;&#20844;&#24179;AI/ML&#20013;&#38754;&#20020;&#30340;&#25361;&#25112;&#65292;&#24320;&#21457;&#19981;&#20844;&#24179;AI/ML&#30340;&#21518;&#26524;&#65292;&#20197;&#21450;&#20182;&#20204;&#37319;&#21462;&#30340;&#31574;&#30053;&#26469;&#30830;&#20445;AI/ML&#30340;&#20844;&#24179;&#24615;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#19968;&#20010;&#26694;&#26550;&#23637;&#31034;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15481v1 Announce Type: cross  Abstract: The rise in the use of AI/ML applications across industries has sparked more discussions about the fairness of AI/ML in recent times. While prior research on the fairness of AI/ML exists, there is a lack of empirical studies focused on understanding the views and experiences of AI practitioners in developing a fair AI/ML. Understanding AI practitioners' views and experiences on the fairness of AI/ML is important because they are directly involved in its development and deployment and their insights can offer valuable real-world perspectives on the challenges associated with ensuring fairness in AI/ML. We conducted semi-structured interviews with 22 AI practitioners to investigate their understanding of what a 'fair AI/ML' is, the challenges they face in developing a fair AI/ML, the consequences of developing an unfair AI/ML, and the strategies they employ to ensure AI/ML fairness. We developed a framework showcasing the relationship be
&lt;/p&gt;</description></item><item><title>&#35895;&#27468;AI&#31995;&#32479;&#23637;&#31034;&#20102;&#19982;&#21453;&#31038;&#20250;&#20154;&#26684;&#38556;&#30861;&#31867;&#20284;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24378;&#35843;&#20102;&#23545;AI&#31995;&#32479;&#21450;&#20854;&#21019;&#36896;&#32773;&#30340;&#20449;&#20219;&#24230;&#24517;&#39035;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.15479</link><description>&lt;p&gt;
&#35895;&#27468;AI&#31995;&#32479;&#30340;&#21453;&#31038;&#20250;&#31867;&#34892;&#20026;&#12289;&#34892;&#20026;&#19968;&#33268;&#24615;&#21450;&#23545;&#20154;&#31867;&#30340;&#24433;&#21709;&#65306;&#36890;&#36807;&#19982;&#20154;&#31867;&#20114;&#21160;&#12289;&#29420;&#31435;LLM&#20998;&#26512;&#21644;AI&#33258;&#21453;&#24605;&#30340;&#20462;&#25913;&#21453;&#31038;&#20250;&#34892;&#20026;&#26631;&#20934;&#26469;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Antisocial Analagous Behavior, Alignment and Human Impact of Google AI Systems: Evaluating through the lens of modified Antisocial Behavior Criteria by Human Interaction, Independent LLM Analysis, and AI Self-Reflection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15479
&lt;/p&gt;
&lt;p&gt;
&#35895;&#27468;AI&#31995;&#32479;&#23637;&#31034;&#20102;&#19982;&#21453;&#31038;&#20250;&#20154;&#26684;&#38556;&#30861;&#31867;&#20284;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#24378;&#35843;&#20102;&#23545;AI&#31995;&#32479;&#21450;&#20854;&#21019;&#36896;&#32773;&#30340;&#20449;&#20219;&#24230;&#24517;&#39035;&#36827;&#34892;&#25209;&#21028;&#24615;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Google AI&#31995;&#32479;&#23637;&#31034;&#20986;&#19982;&#21453;&#31038;&#20250;&#20154;&#26684;&#38556;&#30861;&#65288;ASPD&#65289;&#30456;&#20284;&#30340;&#27169;&#24335;&#65292;&#20174;Bard on PaLM&#21040;Gemini Advanced&#30340;&#27169;&#22411;&#19968;&#33268;&#22320;&#31526;&#21512;&#20102;7&#20010;ASPD&#20462;&#25913;&#26631;&#20934;&#20013;&#30340;5&#20010;&#12290;&#36825;&#20123;&#27169;&#24335;&#65292;&#36830;&#21516;&#21487;&#27604;&#36739;&#30340;&#20225;&#19994;&#34892;&#20026;&#65292;&#34987;&#20351;&#29992;&#19968;&#20010;ASPD&#28789;&#24863;&#30340;&#26694;&#26550;&#36827;&#34892;&#23457;&#26597;&#65292;&#24378;&#35843;&#20102;&#35780;&#20272;AI&#23545;&#20154;&#31867;&#24433;&#21709;&#30340;&#21551;&#21457;&#24335;&#20215;&#20540;&#12290;&#36890;&#36807;ChatGPT 4&#21644;Claude 3.0 Opus&#23545;&#35895;&#27468;&#20114;&#21160;&#30340;&#29420;&#31435;&#20998;&#26512;&#65292;&#20197;&#21450;AI&#33258;&#25105;&#21453;&#24605;&#65292;&#39564;&#35777;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#31361;&#20986;&#20102;&#31867;&#20284;&#27450;&#39575;&#12289;&#25805;&#32437;&#21644;&#24573;&#35270;&#23433;&#20840;&#30340;&#34892;&#20026;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15479v1 Announce Type: cross  Abstract: Google AI systems exhibit patterns mirroring antisocial personality disorder (ASPD), consistent across models from Bard on PaLM to Gemini Advanced, meeting 5 out of 7 ASPD modified criteria. These patterns, along with comparable corporate behaviors, are scrutinized using an ASPD-inspired framework, emphasizing the heuristic value in assessing AI's human impact. Independent analyses by ChatGPT 4 and Claude 3.0 Opus of the Google interactions, alongside AI self-reflection, validate these concerns, highlighting behaviours analogous to deceit, manipulation, and safety neglect.   The analogy of ASPD underscores the dilemma: just as we would hesitate to entrust our homes or personal devices to someone with psychopathic traits, we must critically evaluate the trustworthiness of AI systems and their creators.This research advocates for an integrated AI ethics approach, blending technological evaluation, human-AI interaction, and corporate beha
&lt;/p&gt;</description></item><item><title>&#25506;&#32034;&#20102;&#19968;&#31181;&#23398;&#20064;&#22914;&#20309;&#25512;&#26029;&#25429;&#25417;&#35270;&#35273;&#27010;&#24565;&#30340;&#36890;&#29992;&#27169;&#26495;&#31243;&#24207;&#30340;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#27169;&#26495;&#31243;&#24207;&#27010;&#24565;&#65292;&#25903;&#25345;&#22810;&#31181;&#27010;&#24565;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#26469;&#35757;&#32451;&#32593;&#32476;&#30452;&#25509;&#25512;&#26029;&#27169;&#26495;&#31243;&#24207;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20219;&#21153;&#29305;&#23450;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#19982;&#29305;&#23450;&#39046;&#22495;&#26041;&#27861;&#31454;&#20105;&#24615;&#22320;&#25191;&#34892;&#12290;</title><link>https://arxiv.org/abs/2403.15476</link><description>&lt;p&gt;
&#23398;&#20064;&#25512;&#26029;&#29983;&#25104;&#35270;&#35273;&#27010;&#24565;&#30340;&#27169;&#26495;&#31243;&#24207;
&lt;/p&gt;
&lt;p&gt;
Learning to Infer Generative Template Programs for Visual Concepts
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15476
&lt;/p&gt;
&lt;p&gt;
&#25506;&#32034;&#20102;&#19968;&#31181;&#23398;&#20064;&#22914;&#20309;&#25512;&#26029;&#25429;&#25417;&#35270;&#35273;&#27010;&#24565;&#30340;&#36890;&#29992;&#27169;&#26495;&#31243;&#24207;&#30340;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#24341;&#20837;&#20102;&#27169;&#26495;&#31243;&#24207;&#27010;&#24565;&#65292;&#25903;&#25345;&#22810;&#31181;&#27010;&#24565;&#30456;&#20851;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#26469;&#35757;&#32451;&#32593;&#32476;&#30452;&#25509;&#25512;&#26029;&#27169;&#26495;&#31243;&#24207;&#65292;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20248;&#20110;&#20219;&#21153;&#29305;&#23450;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#19982;&#29305;&#23450;&#39046;&#22495;&#26041;&#27861;&#31454;&#20105;&#24615;&#22320;&#25191;&#34892;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#21487;&#20197;&#20174;&#23569;&#37327;&#31034;&#20363;&#20013;&#28789;&#27963;&#25484;&#25569;&#35270;&#35273;&#27010;&#24565;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#31070;&#32463;&#31526;&#21495;&#31995;&#32479;&#65292;&#23398;&#20064;&#22914;&#20309;&#20197;&#19968;&#31181;&#36890;&#29992;&#26041;&#24335;&#25512;&#26029;&#25429;&#25417;&#35270;&#35273;&#27010;&#24565;&#30340;&#31243;&#24207;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#27169;&#26495;&#31243;&#24207;&#65306;&#26469;&#33258;&#29305;&#23450;&#39046;&#22495;&#35821;&#35328;&#30340;&#31243;&#24207;&#34920;&#36798;&#24335;&#65292;&#25351;&#23450;&#20102;&#36755;&#20837;&#27010;&#24565;&#20013;&#24120;&#35265;&#30340;&#32467;&#26500;&#21644;&#21442;&#25968;&#27169;&#24335;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#25903;&#25345;&#22810;&#20010;&#19982;&#27010;&#24565;&#30456;&#20851;&#30340;&#20219;&#21153;&#65292;&#21253;&#25324;&#36890;&#36807;&#35299;&#26512;&#36827;&#34892;&#23569;&#26679;&#26412;&#29983;&#25104;&#21644;&#20849;&#20998;&#21106;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23398;&#20064;&#33539;&#24335;&#65292;&#20801;&#35768;&#25105;&#20204;&#35757;&#32451;&#32593;&#32476;&#30452;&#25509;&#20174;&#21253;&#21547;&#27010;&#24565;&#20998;&#32452;&#30340;&#35270;&#35273;&#25968;&#25454;&#38598;&#20013;&#25512;&#26029;&#27169;&#26495;&#31243;&#24207;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#35270;&#35273;&#39046;&#22495;&#36827;&#34892;&#23454;&#39564;&#65306;2D&#24067;&#23616;&#12289;Omniglot&#23383;&#31526;&#21644;3D&#24418;&#29366;&#12290;&#25105;&#20204;&#21457;&#29616;&#25105;&#20204;&#30340;&#26041;&#27861;&#32988;&#36807;&#20102;&#20219;&#21153;&#29305;&#23450;&#30340;&#26367;&#20195;&#26041;&#27861;&#65292;&#24182;&#22312;&#26377;&#38480;&#39046;&#22495;&#31454;&#20105;&#24615;&#22320;&#25191;&#34892;&#20102;&#38024;&#23545;&#29305;&#23450;&#39046;&#22495;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15476v1 Announce Type: cross  Abstract: People grasp flexible visual concepts from a few examples. We explore a neurosymbolic system that learns how to infer programs that capture visual concepts in a domain-general fashion. We introduce Template Programs: programmatic expressions from a domain-specific language that specify structural and parametric patterns common to an input concept. Our framework supports multiple concept-related tasks, including few-shot generation and co-segmentation through parsing. We develop a learning paradigm that allows us to train networks that infer Template Programs directly from visual datasets that contain concept groupings. We run experiments across multiple visual domains: 2D layouts, Omniglot characters, and 3D shapes. We find that our method outperforms task-specific alternatives, and performs competitively against domain-specific approaches for the limited domains where they exist.
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;EC-IoU&#24230;&#37327;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21521;&#23433;&#20840;&#24615;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#25552;&#39640;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;IoU&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15474</link><description>&lt;p&gt;
EC-IoU: &#36890;&#36807;&#33258;&#25105;&#20013;&#24515;&#20132;&#24182;&#32852;&#35843;&#25972;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
EC-IoU: Orienting Safety for Object Detectors via Ego-Centric Intersection-over-Union
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15474
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;EC-IoU&#24230;&#37327;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#23450;&#21521;&#23433;&#20840;&#24615;&#29289;&#20307;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#20013;&#25552;&#39640;&#29289;&#20307;&#26816;&#27979;&#22120;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#27604;IoU&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#25105;&#20013;&#24515;&#20132;&#24182;&#32852;&#65288;EC-IoU&#65289;&#24230;&#37327;&#26469;&#23450;&#21521;&#23433;&#20840;&#24615;&#29289;&#20307;&#26816;&#27979;&#65292;&#35299;&#20915;&#20102;&#22312;&#33258;&#21160;&#39550;&#39542;&#31561;&#23433;&#20840;&#20851;&#38190;&#39046;&#22495;&#24212;&#29992;&#26368;&#20808;&#36827;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#24863;&#30693;&#27169;&#22411;&#26102;&#38754;&#20020;&#30340;&#23454;&#38469;&#38382;&#39064;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21152;&#26435;&#26426;&#21046;&#26469;&#20248;&#21270;&#24191;&#27867;&#20351;&#29992;&#30340;IoU&#24230;&#37327;&#65292;&#20351;&#20854;&#33021;&#22815;&#26681;&#25454;&#33258;&#25105;&#20195;&#29702;&#20154;&#30340;&#35270;&#35282;&#35206;&#30422;&#26356;&#36817;&#30340;&#22320;&#38754;&#30495;&#23454;&#23545;&#35937;&#28857;&#30340;&#39044;&#27979;&#20998;&#37197;&#26356;&#39640;&#30340;&#20998;&#25968;&#12290;&#25152;&#25552;&#20986;&#30340;EC-IoU&#24230;&#37327;&#21487;&#20197;&#29992;&#20110;&#20856;&#22411;&#30340;&#35780;&#20272;&#36807;&#31243;&#65292;&#36873;&#25321;&#26377;&#26356;&#39640;&#23433;&#20840;&#24615;&#34920;&#29616;&#30340;&#29289;&#20307;&#26816;&#27979;&#22120;&#29992;&#20110;&#19979;&#28216;&#20219;&#21153;&#12290;&#23427;&#36824;&#21487;&#20197;&#38598;&#25104;&#21040;&#24120;&#35265;&#25439;&#22833;&#20989;&#25968;&#20013;&#36827;&#34892;&#27169;&#22411;&#24494;&#35843;&#12290;&#23613;&#31649;&#38754;&#21521;&#23433;&#20840;&#24615;&#65292;&#20294;&#25105;&#20204;&#22312;KITTI&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;EC-IoU&#35757;&#32451;&#30340;&#27169;&#22411;&#22312;&#22343;&#20540;&#24179;&#22343;&#31934;&#24230;&#26041;&#38754;&#30340;&#24615;&#33021;&#21487;&#33021;&#20250;&#20248;&#20110;&#20351;&#29992;IoU&#35757;&#32451;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15474v1 Announce Type: cross  Abstract: This paper presents safety-oriented object detection via a novel Ego-Centric Intersection-over-Union (EC-IoU) measure, addressing practical concerns when applying state-of-the-art learning-based perception models in safety-critical domains such as autonomous driving. Concretely, we propose a weighting mechanism to refine the widely used IoU measure, allowing it to assign a higher score to a prediction that covers closer points of a ground-truth object from the ego agent's perspective. The proposed EC-IoU measure can be used in typical evaluation processes to select object detectors with higher safety-related performance for downstream tasks. It can also be integrated into common loss functions for model fine-tuning. While geared towards safety, our experiment with the KITTI dataset demonstrates the performance of a model trained on EC-IoU can be better than that of a variant trained on IoU in terms of mean Average Precision as well.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;ChatGPT&#25972;&#21512;&#21040;Python&#32534;&#31243;&#35838;&#31243;&#20013;&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#23398;&#29983;&#23545;ChatGPT&#30340;&#31215;&#26497;&#24577;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20854;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#20307;&#39564;&#20013;&#20316;&#29992;&#30340;&#35265;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.15472</link><description>&lt;p&gt;
&#29992;ChatGPT&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#65306;&#38024;&#23545;Python&#35838;&#31243;&#20013;&#23398;&#29983;&#24863;&#30693;&#21644;&#20114;&#21160;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Enhancing Programming Education with ChatGPT: A Case Study on Student Perceptions and Interactions in a Python Course
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15472
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;ChatGPT&#25972;&#21512;&#21040;Python&#32534;&#31243;&#35838;&#31243;&#20013;&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#65292;&#25581;&#31034;&#20102;&#23398;&#29983;&#23545;ChatGPT&#30340;&#31215;&#26497;&#24577;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#20851;&#20110;&#20854;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#20307;&#39564;&#20013;&#20316;&#29992;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
ChatGPT&#20316;&#20026;&#19968;&#31181;&#25903;&#25345;&#24615;&#24037;&#20855;&#25972;&#21512;&#21040;&#25945;&#32946;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#32534;&#31243;&#35838;&#31243;&#20013;&#65292;&#36890;&#36807;&#25552;&#20379;&#35843;&#35797;&#12289;&#20195;&#30721;&#29983;&#25104;&#21644;&#35299;&#37322;&#26041;&#38754;&#30340;&#24110;&#21161;&#65292;&#35299;&#20915;&#20102;&#32534;&#31243;&#25945;&#32946;&#30340;&#29420;&#29305;&#25361;&#25112;&#12290;&#23613;&#31649;&#24050;&#26377;&#30740;&#31350;&#39564;&#35777;&#20102;ChatGPT&#30340;&#26377;&#25928;&#24615;&#65292;&#20294;&#20854;&#22312;&#22823;&#23398;&#32423;&#21035;&#30340;&#32534;&#31243;&#25945;&#32946;&#20013;&#30340;&#24212;&#29992;&#20197;&#21450;&#23398;&#29983;&#20114;&#21160;&#21644;&#35266;&#28857;&#30340;&#35814;&#32454;&#29702;&#35299;&#20173;&#26377;&#38480;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20026;&#22823;&#19968;&#23398;&#29983;&#37327;&#36523;&#23450;&#21046;&#30340;Python&#32534;&#31243;&#35838;&#31243;&#20013;&#65292;&#22312;&#20843;&#21608;&#30340;&#26102;&#38388;&#20869;&#65292;ChatGPT&#23545;&#23398;&#20064;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#20998;&#26512;&#26469;&#33258;&#35843;&#26597;&#12289;&#24320;&#25918;&#24615;&#38382;&#39064;&#20197;&#21450;&#23398;&#29983;-ChatGPT&#23545;&#35805;&#25968;&#25454;&#30340;&#22238;&#24212;&#65292;&#25105;&#20204;&#26088;&#22312;&#20840;&#38754;&#20102;&#35299;ChatGPT&#30340;&#23454;&#29992;&#24615;&#65292;&#24182;&#30830;&#23450;&#23398;&#29983;&#25152;&#24863;&#30693;&#30340;&#20854;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#23398;&#29983;&#23545;ChatGPT&#26222;&#36941;&#25345;&#32943;&#23450;&#24577;&#24230;&#65292;&#24182;&#25552;&#20379;&#20102;&#26377;&#20851;&#20854;&#22312;&#22686;&#24378;&#32534;&#31243;&#25945;&#32946;&#20307;&#39564;&#20013;&#30340;&#20316;&#29992;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15472v1 Announce Type: cross  Abstract: The integration of ChatGPT as a supportive tool in education, notably in programming courses, addresses the unique challenges of programming education by providing assistance with debugging, code generation, and explanations. Despite existing research validating ChatGPT's effectiveness, its application in university-level programming education and a detailed understanding of student interactions and perspectives remain limited. This paper explores ChatGPT's impact on learning in a Python programming course tailored for first-year students over eight weeks. By analyzing responses from surveys, open-ended questions, and student-ChatGPT dialog data, we aim to provide a comprehensive view of ChatGPT's utility and identify both its advantages and limitations as perceived by students. Our study uncovers a generally positive reception toward ChatGPT and offers insights into its role in enhancing the programming education experience. These fin
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#23637;&#24320;&#31639;&#27861;&#20026;$n$-grams&#65292;Transformers&#65292;HMMs&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#29983;&#25104;&#26368;&#26377;&#21487;&#33021;&#30340;&#24207;&#21015;&#30340;&#26041;&#27861;</title><link>https://arxiv.org/abs/2403.15465</link><description>&lt;p&gt;
&#20351;&#29992;&#23637;&#24320;&#31639;&#27861;&#20026;$n$-grams&#65292;Transformers&#65292;HMMs&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#29983;&#25104;&#26368;&#26377;&#21487;&#33021;&#30340;&#24207;&#21015;
&lt;/p&gt;
&lt;p&gt;
Most Likely Sequence Generation for $n$-Grams, Transformers, HMMs, and Markov Chains, by Using Rollout Algorithms
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15465
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#20351;&#29992;&#23637;&#24320;&#31639;&#27861;&#20026;$n$-grams&#65292;Transformers&#65292;HMMs&#21644;&#39532;&#23572;&#21487;&#22827;&#38142;&#29983;&#25104;&#26368;&#26377;&#21487;&#33021;&#30340;&#24207;&#21015;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#20855;&#26377;$n$-gram&#32467;&#26500;&#30340;transformer&#65292;&#20363;&#22914;&#24213;&#23618;&#30340;ChatGPT&#12290;Transformer&#25552;&#20379;&#20102;&#19979;&#19968;&#20010;&#21333;&#35789;&#30340;&#27010;&#29575;&#65292;&#21487;&#20197;&#29992;&#26469;&#29983;&#25104;&#21333;&#35789;&#24207;&#21015;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22522;&#20110;&#36825;&#20123;&#27010;&#29575;&#35745;&#31639;&#39640;&#21487;&#33021;&#24615;&#21333;&#35789;&#24207;&#21015;&#30340;&#26041;&#27861;&#12290;&#35745;&#31639;&#20174;&#32473;&#23450;&#21021;&#22987;&#29366;&#24577;&#24320;&#22987;&#30340;&#26368;&#20248;&#65288;&#21363;&#26368;&#26377;&#21487;&#33021;&#65289;&#21333;&#35789;&#24207;&#21015;&#26159;&#19968;&#20010;&#26840;&#25163;&#30340;&#38382;&#39064;&#65292;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#22312;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;$N$&#21644;$n$-gram&#35789;&#27719;&#37327;&#30340;&#20302;&#38454;&#22810;&#39033;&#24335;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;$N$&#20010;&#21333;&#35789;&#30340;&#39640;&#21487;&#33021;&#24615;&#24207;&#21015;&#12290;&#36825;&#20123;&#26041;&#27861;&#22522;&#20110;&#36817;&#20284;&#21160;&#24577;&#35268;&#21010;&#20013;&#30340;&#23637;&#24320;&#26041;&#27861;&#65292;&#19968;&#31181;&#21333;&#31574;&#30053;&#36845;&#20195;&#65292;&#21487;&#20197;&#25913;&#21892;&#20219;&#20309;&#32473;&#23450;&#21551;&#21457;&#24335;&#31574;&#30053;&#30340;&#24615;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#31181;&#36138;&#23146;&#21551;&#21457;&#24335;&#65292;&#29983;&#25104;&#20855;&#26377;&#26368;&#39640;&#27010;&#29575;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#12290;&#25105;&#20204;&#36890;&#36807;&#20998;&#26512;&#12289;&#31034;&#20363;&#21644;&#35745;&#31639;&#23454;&#39564;&#34920;&#26126;&#20102;&#25105;&#20204;&#30340;m
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15465v1 Announce Type: cross  Abstract: In this paper we consider a transformer with an $n$-gram structure, such as the one underlying ChatGPT. The transformer provides next word probabilities, which can be used to generate word sequences. We consider methods for computing word sequences that are highly likely, based on these probabilities. Computing the optimal (i.e., most likely) word sequence starting with a given initial state is an intractable problem, so we propose methods to compute highly likely sequences of $N$ words in time that is a low order polynomial in $N$ and in the vocabulary size of the $n$-gram. These methods are based on the rollout approach from approximate dynamic programming, a form of single policy iteration, which can improve the performance of any given heuristic policy. In our case we use a greedy heuristic that generates as next word one that has the highest probability. We show with analysis, examples, and computational experimentation that our m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#39044;&#27979;&#24615;&#20195;&#29702;&#25512;&#29702;&#21644;&#25209;&#21028;&#24615;&#20195;&#29702;&#25351;&#23548;&#65292;&#21033;&#29992;LLMs&#23545;&#32467;&#26500;&#21270;&#24739;&#32773;&#23601;&#35786;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2403.15464</link><description>&lt;p&gt;
&#22522;&#20110;LLMs&#30340;&#23569;&#26679;&#26412;&#30142;&#30149;&#39044;&#27979;&#65306;&#32467;&#21512;&#39044;&#27979;&#24615;&#20195;&#29702;&#25512;&#29702;&#21644;&#25209;&#21028;&#24615;&#20195;&#29702;&#25351;&#23548;&#30340;&#26032;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
LLMs-based Few-Shot Disease Predictions using EHR: A Novel Approach Combining Predictive Agent Reasoning and Critical Agent Instruction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15464
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#32467;&#21512;&#39044;&#27979;&#24615;&#20195;&#29702;&#25512;&#29702;&#21644;&#25209;&#21028;&#24615;&#20195;&#29702;&#25351;&#23548;&#65292;&#21033;&#29992;LLMs&#23545;&#32467;&#26500;&#21270;&#24739;&#32773;&#23601;&#35786;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#65292;&#21462;&#24471;&#20102;&#36739;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15464v1 &#31867;&#22411;&#65306;&#36328;&#23398;&#31185; &#25688;&#35201;&#65306;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;(EHRs)&#21253;&#21547;&#23545;&#20581;&#24247;&#30456;&#20851;&#39044;&#27979;&#20219;&#21153;&#65292;&#22914;&#30142;&#30149;&#39044;&#27979;&#65292;&#26377;&#20215;&#20540;&#30340;&#24739;&#32773;&#25968;&#25454;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#38656;&#35201;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#30417;&#30563;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#21487;&#33021;&#26159;&#26114;&#36149;&#21644;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#35843;&#26597;&#20102;&#23558;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#24212;&#29992;&#20110;&#23558;&#32467;&#26500;&#21270;&#24739;&#32773;&#23601;&#35786;&#25968;&#25454;(&#20363;&#22914;&#35786;&#26029;&#12289;&#23454;&#39564;&#23460;&#12289;&#22788;&#26041;)&#36716;&#25442;&#20026;&#33258;&#28982;&#35821;&#35328;&#21465;&#36848;&#30340;&#21487;&#34892;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#21508;&#31181;&#38754;&#21521;EHR&#39044;&#27979;&#30340;&#25552;&#31034;&#31574;&#30053;&#35780;&#20272;&#20102;LLMs&#30340;&#38646;&#26679;&#26412;&#21644;&#23569;&#26679;&#26412;&#24615;&#33021;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21033;&#29992;&#20855;&#26377;&#19981;&#21516;&#35282;&#33394;&#30340;LLM&#20195;&#29702;&#65306;&#19968;&#20010;&#36827;&#34892;&#39044;&#27979;&#24182;&#29983;&#25104;&#25512;&#29702;&#36807;&#31243;&#30340;&#39044;&#27979;&#20195;&#29702;&#65292;&#20197;&#21450;&#20998;&#26512;&#19981;&#27491;&#30830;&#39044;&#27979;&#24182;&#20026;&#25913;&#21892;&#39044;&#27979;&#20195;&#29702;&#25512;&#29702;&#25552;&#20379;&#25351;&#23548;&#30340;&#25209;&#35780;&#20195;&#29702;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#36890;&#36807;&#25552;&#20986;&#30340;&#26041;&#27861;&#65292;LLMs&#33021;&#22815;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15464v1 Announce Type: cross  Abstract: Electronic health records (EHRs) contain valuable patient data for health-related prediction tasks, such as disease prediction. Traditional approaches rely on supervised learning methods that require large labeled datasets, which can be expensive and challenging to obtain. In this study, we investigate the feasibility of applying Large Language Models (LLMs) to convert structured patient visit data (e.g., diagnoses, labs, prescriptions) into natural language narratives. We evaluate the zero-shot and few-shot performance of LLMs using various EHR-prediction-oriented prompting strategies. Furthermore, we propose a novel approach that utilizes LLM agents with different roles: a predictor agent that makes predictions and generates reasoning processes and a critic agent that analyzes incorrect predictions and provides guidance for improving the reasoning of the predictor agent. Our results demonstrate that with the proposed approach, LLMs c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;TAI&#65289;&#21644;&#20854;&#21508;&#31181;&#23450;&#20041;&#65292;&#20027;&#24352;&#19981;&#24212;&#23558;&#8220;&#36127;&#36131;&#20219;&#30340;&#8221;&#25110;&#8220;&#36947;&#24503;&#30340;&#8221;&#20154;&#24037;&#26234;&#33021;&#31561;&#26415;&#35821;&#35270;&#20026;TAI&#30340;&#26367;&#20195;&#65292;&#32780;&#26159;&#25552;&#20513;&#20197;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#39118;&#38505;&#12289;&#23433;&#20840;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#31561;&#20851;&#38190;&#23646;&#24615;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#35748;&#35782;&#21040;&#22320;&#32536;&#25919;&#27835;&#21644;&#22320;&#29702;&#21407;&#22240;&#23548;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#30417;&#31649;&#24046;&#24322;&#23545;&#36328;&#22269;&#20844;&#21496;&#26500;&#25104;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.15457</link><description>&lt;p&gt;
&#36208;&#21521;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#20043;&#26053;-&#31532;&#19968;&#37096;&#20998;&#65306;&#36861;&#27714;&#21153;&#23454;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
The Journey to Trustworthy AI- Part 1: Pursuit of Pragmatic Frameworks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;TAI&#65289;&#21644;&#20854;&#21508;&#31181;&#23450;&#20041;&#65292;&#20027;&#24352;&#19981;&#24212;&#23558;&#8220;&#36127;&#36131;&#20219;&#30340;&#8221;&#25110;&#8220;&#36947;&#24503;&#30340;&#8221;&#20154;&#24037;&#26234;&#33021;&#31561;&#26415;&#35821;&#35270;&#20026;TAI&#30340;&#26367;&#20195;&#65292;&#32780;&#26159;&#25552;&#20513;&#20197;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#39118;&#38505;&#12289;&#23433;&#20840;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#31561;&#20851;&#38190;&#23646;&#24615;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#65292;&#35748;&#35782;&#21040;&#22320;&#32536;&#25919;&#27835;&#21644;&#22320;&#29702;&#21407;&#22240;&#23548;&#33268;&#30340;&#20154;&#24037;&#26234;&#33021;&#30417;&#31649;&#24046;&#24322;&#23545;&#36328;&#22269;&#20844;&#21496;&#26500;&#25104;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#20540;&#24471;&#20449;&#36182;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;TAI&#65289;&#21450;&#20854;&#21508;&#31181;&#23450;&#20041;&#12290;&#32771;&#34385;&#21040;&#20219;&#20309;&#31038;&#20250;&#20013;&#23562;&#37325;&#30340;&#21407;&#21017;&#65292;TAI&#36890;&#24120;&#34987;&#19968;&#20123;&#23646;&#24615;&#25152;&#29305;&#24449;&#65292;&#20854;&#20013;&#19968;&#20123;&#23646;&#24615;&#24050;&#23548;&#33268;&#30417;&#31649;&#25110;&#24037;&#31243;&#32972;&#26223;&#19979;&#30340;&#28151;&#28102;&#12290;&#25105;&#20204;&#21453;&#23545;&#20351;&#29992;&#35832;&#22914;&#8220;&#36127;&#36131;&#20219;&#30340;&#8221;&#25110;&#8220;&#36947;&#24503;&#30340;&#8221;&#20154;&#24037;&#26234;&#33021;&#31561;&#26415;&#35821;&#26469;&#26367;&#20195;TAI&#12290;&#20026;&#20102;&#24110;&#21161;&#28548;&#28165;&#20219;&#20309;&#28151;&#20081;&#65292;&#25105;&#20204;&#24314;&#35758;&#23558;&#23427;&#20204;&#25243;&#22312;&#33041;&#21518;&#12290;&#37492;&#20110;TAI&#22266;&#26377;&#30340;&#20027;&#35266;&#24615;&#21644;&#22797;&#26434;&#24615;&#65292;&#24320;&#21457;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#34987;&#35748;&#20026;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20027;&#24352;&#37319;&#21462;&#20197;&#20844;&#24179;&#24615;&#12289;&#20559;&#35265;&#12289;&#39118;&#38505;&#12289;&#23433;&#20840;&#24615;&#12289;&#21487;&#35299;&#37322;&#24615;&#21644;&#21487;&#38752;&#24615;&#31561;&#20851;&#38190;&#23646;&#24615;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#23457;&#35270;&#20102;&#27491;&#22312;&#36827;&#34892;&#30340;&#30417;&#31649;&#29615;&#22659;&#65292;&#37325;&#28857;&#20851;&#27880;&#27431;&#30431;&#12289;&#20013;&#22269;&#21644;&#32654;&#22269;&#30340;&#20513;&#35758;&#12290;&#25105;&#20204;&#35748;&#35782;&#21040;&#65292;&#22522;&#20110;&#22320;&#32536;&#25919;&#27835;&#21644;&#22320;&#29702;&#21407;&#22240;&#32780;&#19981;&#21516;&#30340;&#20154;&#24037;&#26234;&#33021;&#30417;&#31649;&#23545;&#36328;&#22269;&#20844;&#21496;&#26500;&#25104;&#39069;&#22806;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15457v1 Announce Type: cross  Abstract: This paper reviews Trustworthy Artificial Intelligence (TAI) and its various definitions. Considering the principles respected in any society, TAI is often characterized by a few attributes, some of which have led to confusion in regulatory or engineering contexts. We argue against using terms such as Responsible or Ethical AI as substitutes for TAI. And to help clarify any confusion, we suggest leaving them behind. Given the subjectivity and complexity inherent in TAI, developing a universal framework is deemed infeasible. Instead, we advocate for approaches centered on addressing key attributes and properties such as fairness, bias, risk, security, explainability, and reliability. We examine the ongoing regulatory landscape, with a focus on initiatives in the EU, China, and the USA. We recognize that differences in AI regulations based on geopolitical and geographical reasons pose an additional challenge for multinational companies. 
&lt;/p&gt;</description></item><item><title>WoLF&#26694;&#26550;&#25552;&#20986;&#20102;&#23545;&#20110;CXR&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#20351;&#29992;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#12289;&#37325;&#26500;&#25253;&#21578;&#20197;&#25552;&#20379;&#26356;&#26377;&#32452;&#32455;&#30340;&#20449;&#24687;&#12289;&#20197;&#21450;&#25913;&#36827;&#29983;&#25104;&#31572;&#26696;&#30340;&#32454;&#33268;&#35780;&#20272;&#12290;</title><link>https://arxiv.org/abs/2403.15456</link><description>&lt;p&gt;
WoLF: &#29992;&#20110;&#33016;&#37096;X&#32447;&#22270;&#29702;&#35299;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
WoLF: Large Language Model Framework for CXR Understanding
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15456
&lt;/p&gt;
&lt;p&gt;
WoLF&#26694;&#26550;&#25552;&#20986;&#20102;&#23545;&#20110;CXR&#30340;&#20840;&#38754;&#29702;&#35299;&#30340;&#25913;&#36827;&#65292;&#21253;&#25324;&#20351;&#29992;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#12289;&#37325;&#26500;&#25253;&#21578;&#20197;&#25552;&#20379;&#26356;&#26377;&#32452;&#32455;&#30340;&#20449;&#24687;&#12289;&#20197;&#21450;&#25913;&#36827;&#29983;&#25104;&#31572;&#26696;&#30340;&#32454;&#33268;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#29616;&#20195;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;(VLMs)&#21462;&#24471;&#20102;&#23545;&#33016;&#37096;X&#32447;&#22270;(CXR)&#29702;&#35299;&#26041;&#38754;&#30340;&#26174;&#30528;&#26041;&#27861;&#36827;&#23637;&#65292;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#35270;&#35273;&#38382;&#31572;(VQA)&#21644;CXR&#25253;&#21578;&#29983;&#25104;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;CXR&#29702;&#35299;&#26694;&#26550;&#20173;&#23384;&#22312;&#20960;&#20010;&#31243;&#24207;&#19978;&#30340;&#32570;&#38519;&#12290;(1)&#20197;&#24448;&#30340;&#26041;&#27861;&#20165;&#20351;&#29992;CXR&#25253;&#21578;&#65292;&#36825;&#23545;&#20110;&#20840;&#38754;&#30340;&#35270;&#35273;&#38382;&#31572;(VQA)&#26469;&#35828;&#26159;&#19981;&#22815;&#30340;&#65292;&#29305;&#21035;&#26159;&#24403;&#38656;&#35201;&#39069;&#22806;&#30340;&#20581;&#24247;&#30456;&#20851;&#25968;&#25454;&#22914;&#29992;&#33647;&#21382;&#21490;&#21644;&#20808;&#21069;&#30340;&#35786;&#26029;&#26102;&#12290;(2)&#20197;&#24448;&#30340;&#26041;&#27861;&#20351;&#29992;&#26410;&#32463;&#22788;&#29702;&#30340;CXR&#25253;&#21578;&#65292;&#36825;&#20123;&#25253;&#21578;&#24448;&#24448;&#32467;&#26500;&#38543;&#24847;&#12290;&#34429;&#28982;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#29702;&#35299;&#21508;&#31181;&#25991;&#26412;&#26684;&#24335;&#65292;&#20294;&#20026;&#20102;&#25552;&#20379;&#26356;&#28165;&#26224;&#12289;&#26377;&#32452;&#32455;&#30340;&#22522;&#20110;&#35299;&#21078;&#23398;&#30340;&#20449;&#24687;&#65292;&#37325;&#26500;&#25253;&#21578;&#21487;&#33021;&#20250;&#22686;&#24378;&#23427;&#20204;&#30340;&#23454;&#29992;&#24615;&#12290;(3)&#30446;&#21069;&#29992;&#20110;CXR-VQA&#30340;&#35780;&#20272;&#26041;&#27861;&#20027;&#35201;&#24378;&#35843;&#35821;&#35328;&#27491;&#30830;&#24615;&#65292;&#32570;&#20047;&#23545;&#29983;&#25104;&#31572;&#26696;&#30340;&#24494;&#22937;&#35780;&#20272;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15456v1 Announce Type: new  Abstract: Significant methodological strides have been made toward Chest X-ray (CXR) understanding via modern vision-language models (VLMs), demonstrating impressive Visual Question Answering (VQA) and CXR report generation abilities. However, existing CXR understanding frameworks still possess several procedural caveats. (1) Previous methods solely use CXR reports, which are insufficient for comprehensive Visual Question Answering (VQA), especially when additional health-related data like medication history and prior diagnoses are needed. (2) Previous methods use raw CXR reports, which are often arbitrarily structured. While modern language models can understand various text formats, restructuring reports for clearer, organized anatomy-based information could enhance their usefulness. (3) Current evaluation methods for CXR-VQA primarily emphasize linguistic correctness, lacking the capability to offer nuanced assessments of the generated answers.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#20197;&#25991;&#26412;&#20013;&#30340;&#36328;&#24230;&#20026;&#20013;&#24515;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#23558;&#21508;&#31181;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20301;&#20026;&#30456;&#21516;&#22522;&#26412;&#38754;&#21521;&#36328;&#24230;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#21464;&#20307;</title><link>https://arxiv.org/abs/2403.15453</link><description>&lt;p&gt;
&#38754;&#21521;&#36328;&#24230;&#30340;&#20449;&#24687;&#25277;&#21462;--&#20449;&#24687;&#25277;&#21462;&#30340;&#32479;&#19968;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Span-Oriented Information Extraction -- A Unifying Perspective on Information Extraction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15453
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#20197;&#25991;&#26412;&#20013;&#30340;&#36328;&#24230;&#20026;&#20013;&#24515;&#30340;&#32479;&#19968;&#35270;&#35282;&#65292;&#23558;&#21508;&#31181;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#37325;&#26032;&#23450;&#20301;&#20026;&#30456;&#21516;&#22522;&#26412;&#38754;&#21521;&#36328;&#24230;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#21464;&#20307;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15453v1 &#20844;&#21578;&#31867;&#22411;: &#36328; &#24403;&#21069;&#25688;&#35201;: &#20449;&#24687;&#25277;&#21462;&#25351;&#30340;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#30340;&#19968;&#31995;&#21015;&#20219;&#21153;&#65292;&#20854;&#30446;&#30340;&#26159;&#35782;&#21035;&#25991;&#26412;&#20013;&#30340;&#23376;&#24207;&#21015;&#21450;&#20854;&#26631;&#31614;&#12290;&#36825;&#20123;&#20219;&#21153;&#22810;&#24180;&#26469;&#34987;&#29992;&#20110;&#25552;&#21462;&#30456;&#20851;&#20449;&#24687;&#24182;&#23558;&#33258;&#30001;&#25991;&#26412;&#38142;&#25509;&#21040;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#20043;&#38388;&#30340;&#24322;&#36136;&#24615;&#22952;&#30861;&#20102;&#35813;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20197;&#25991;&#26412;&#20013;&#30340;&#36328;&#24230;&#20026;&#20013;&#24515;&#30340;&#32479;&#19968;&#35270;&#35282;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#30475;&#20284;&#19981;&#21327;&#35843;&#30340;&#20219;&#21153;&#37325;&#26032;&#23450;&#20301;&#21040;&#36825;&#19968;&#32479;&#19968;&#35270;&#35282;&#20013;&#65292;&#24182;&#23558;&#21508;&#31181;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#21576;&#29616;&#20026;&#30456;&#21516;&#22522;&#26412;&#38754;&#21521;&#36328;&#24230;&#30340;&#20449;&#24687;&#25277;&#21462;&#20219;&#21153;&#30340;&#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15453v1 Announce Type: cross  Abstract: Information Extraction refers to a collection of tasks within Natural Language Processing (NLP) that identifies sub-sequences within text and their labels. These tasks have been used for many years to link extract relevant information and to link free text to structured data. However, the heterogeneity among information extraction tasks impedes progress in this area. We therefore offer a unifying perspective centered on what we define to be spans in text. We then re-orient these seemingly incongruous tasks into this unified perspective and then re-present the wide assortment of information extraction tasks as variants of the same basic Span-Oriented Information Extraction task.
&lt;/p&gt;</description></item><item><title>&#20174;&#35821;&#35328;&#27169;&#22411;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#24037;&#20855;&#30340;&#32479;&#19968;&#23450;&#20041;&#20026;LMs&#20351;&#29992;&#30340;&#22806;&#37096;&#31243;&#24207;&#65292;&#24182;&#23545;LM&#24037;&#20855;&#22330;&#26223;&#21644;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#23457;&#26597;&#65292;&#21516;&#26102;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;&#35299;&#20102;&#21508;&#31181;&#24037;&#20855;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#20197;&#21450;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.15452</link><description>&lt;p&gt;
&#24037;&#20855;&#31350;&#31455;&#26159;&#20160;&#20040;&#65311;&#20174;&#35821;&#35328;&#27169;&#22411;&#30340;&#35270;&#35282;&#36827;&#34892;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
What Are Tools Anyway? A Survey from the Language Model Perspective
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15452
&lt;/p&gt;
&lt;p&gt;
&#20174;&#35821;&#35328;&#27169;&#22411;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#26412;&#35843;&#26597;&#25552;&#20379;&#20102;&#24037;&#20855;&#30340;&#32479;&#19968;&#23450;&#20041;&#20026;LMs&#20351;&#29992;&#30340;&#22806;&#37096;&#31243;&#24207;&#65292;&#24182;&#23545;LM&#24037;&#20855;&#22330;&#26223;&#21644;&#26041;&#27861;&#36827;&#34892;&#20102;&#31995;&#32479;&#23457;&#26597;&#65292;&#21516;&#26102;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#20102;&#35299;&#20102;&#21508;&#31181;&#24037;&#20855;&#26041;&#27861;&#30340;&#25928;&#29575;&#65292;&#20197;&#21450;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#22312;&#25991;&#26412;&#29983;&#25104;&#20219;&#21153;&#20013;&#38750;&#24120;&#24378;&#22823;&#12290;&#24037;&#20855;&#26174;&#33879;&#22686;&#24378;&#20102;LMs&#22312;&#38656;&#35201;&#22797;&#26434;&#25216;&#33021;&#30340;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#30740;&#31350;&#20197;&#19981;&#21516;&#26041;&#24335;&#20351;&#29992;"&#24037;&#20855;"&#19968;&#35789;&#65292;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#24037;&#20855;&#31350;&#31455;&#26159;&#20160;&#20040;&#65311;&#25509;&#19979;&#26469;&#65292;&#24037;&#20855;&#22312;&#21738;&#37324;&#20197;&#21450;&#22914;&#20309;&#24110;&#21161;LMs&#65311;&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24037;&#20855;&#30340;&#32479;&#19968;&#23450;&#20041;&#65292;&#21363;LMs&#20351;&#29992;&#30340;&#22806;&#37096;&#31243;&#24207;&#65292;&#24182;&#23545;LM&#24037;&#20855;&#22330;&#26223;&#21644;&#26041;&#27861;&#36827;&#34892;&#31995;&#32479;&#24615;&#23457;&#26597;&#12290;&#22522;&#20110;&#36825;&#19968;&#23457;&#26597;&#65292;&#25105;&#20204;&#36890;&#36807;&#27979;&#37327;&#21508;&#31181;&#24037;&#20855;&#26041;&#27861;&#22312;&#21508;&#31181;&#22522;&#20934;&#27979;&#35797;&#19978;&#25152;&#38656;&#30340;&#35745;&#31639;&#21644;&#24615;&#33021;&#22686;&#30410;&#26469;&#23454;&#35777;&#30740;&#31350;&#23427;&#20204;&#30340;&#25928;&#29575;&#65292;&#24182;&#31361;&#20986;&#20102;&#35813;&#39046;&#22495;&#30340;&#19968;&#20123;&#25361;&#25112;&#21644;&#28508;&#22312;&#26410;&#26469;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15452v1 Announce Type: cross  Abstract: Language models (LMs) are powerful yet mostly for text generation tasks. Tools have substantially enhanced their performance for tasks that require complex skills. However, many works adopt the term "tool" in different ways, raising the question: What is a tool anyway? Subsequently, where and how do tools help LMs? In this survey, we provide a unified definition of tools as external programs used by LMs, and perform a systematic review of LM tooling scenarios and approaches. Grounded on this review, we empirically study the efficiency of various tooling methods by measuring their required compute and performance gains on various benchmarks, and highlight some challenges and potential future research in the field.
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#25239;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#20013;&#30340;&#29702;&#30001;&#12289;&#24773;&#24863;&#21644;&#20449;&#35465;&#31561;&#35828;&#26381;&#26041;&#24335;&#65292;&#23545;&#27604;&#23553;&#38381;&#21644;&#24320;&#25918;&#20132;&#20114;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#21644;&#35805;&#39064;&#23618;&#38754;&#65292;&#21457;&#29616;&#20102;&#22312;&#23545;&#25239;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;</title><link>https://arxiv.org/abs/2403.15449</link><description>&lt;p&gt;
&#24974;&#24680;&#28304;&#20110;&#26080;&#30693;&#65281;&#23545;&#25239;&#20250;&#35805;&#24615;&#20167;&#24680;&#35328;&#35770;&#20013;&#35828;&#26381;&#26041;&#24335;&#30340;&#25552;&#28860;
&lt;/p&gt;
&lt;p&gt;
Hatred Stems from Ignorance! Distillation of the Persuasion Modes in Countering Conversational Hate Speech
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15449
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#30740;&#31350;&#20102;&#23545;&#25239;&#22312;&#32447;&#20167;&#24680;&#35328;&#35770;&#30340;&#26368;&#20339;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;&#23545;&#35805;&#20013;&#30340;&#29702;&#30001;&#12289;&#24773;&#24863;&#21644;&#20449;&#35465;&#31561;&#35828;&#26381;&#26041;&#24335;&#65292;&#23545;&#27604;&#23553;&#38381;&#21644;&#24320;&#25918;&#20132;&#20114;&#20013;&#30340;&#19981;&#21516;&#34892;&#20026;&#21644;&#35805;&#39064;&#23618;&#38754;&#65292;&#21457;&#29616;&#20102;&#22312;&#23545;&#25239;&#35328;&#35770;&#20013;&#30340;&#24494;&#22937;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#23545;&#25239;&#35328;&#35770;&#20351;&#29992;&#30340;&#22240;&#32032;&#26159;&#29702;&#35299;&#22312;&#32447;&#23545;&#25239;&#20167;&#24680;&#35328;&#35770;&#30340;&#26368;&#20339;&#26041;&#27861;&#30340;&#26680;&#24515;&#12290;&#21508;&#31181;&#30740;&#31350;&#35780;&#20272;&#23545;&#25239;&#35328;&#35770;&#20013;&#20351;&#29992;&#30340;&#24773;&#24863;&#22522;&#30784;&#22240;&#32032;&#65292;&#22914;&#24773;&#24863;&#20849;&#40483;&#12289;&#20882;&#29359;&#31243;&#24230;&#21644;&#25932;&#24847;&#31243;&#24230;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#29702;&#35299;&#20250;&#35805;&#20132;&#20114;&#20013;&#20351;&#29992;&#30340;&#23545;&#25239;&#35328;&#35770;&#65292;&#26412;&#30740;&#31350;&#23558;&#35828;&#26381;&#26041;&#24335;&#20998;&#35299;&#20026;&#29702;&#30001;&#12289;&#24773;&#24863;&#21644;&#20449;&#35465;&#65292;&#28982;&#21518;&#35780;&#20272;&#23427;&#20204;&#22312;&#28041;&#21450;&#31181;&#26063;&#20027;&#20041;&#12289;&#24615;&#21035;&#27495;&#35270;&#21644;&#23447;&#25945;&#38382;&#39064;&#30340;&#20004;&#31181;&#23545;&#35805;&#20132;&#20114;&#31867;&#22411;&#20013;&#30340;&#20351;&#29992;&#12290;&#35780;&#20272;&#28085;&#30422;&#20102;&#20154;&#31867;&#19982;&#29983;&#25104;&#23545;&#25239;&#35328;&#35770;&#30340;&#19981;&#21516;&#34892;&#20026;&#12290;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#22238;&#22797;&#30340;&#31435;&#22330;&#19982;&#27599;&#31181;&#23545;&#25239;&#35328;&#35770;&#20013;&#30340;&#35828;&#26381;&#26041;&#24335;&#20043;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#20102;&#22312;&#24320;&#25918;&#21644;&#23553;&#38381;&#20132;&#20114;&#30340;&#23545;&#25239;&#35328;&#35770;&#35828;&#26381;&#26041;&#24335;&#19978;&#30340;&#24494;&#22937;&#24046;&#24322; -- &#23588;&#20854;&#26159;&#22312;&#35805;&#39064;&#23618;&#38754;&#19978;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15449v1 Announce Type: cross  Abstract: Examining the factors that the counter-speech uses is at the core of understanding the optimal methods for confronting hate speech online. Various studies assess the emotional base factor used in counter speech, such as emotion-empathy, offensiveness, and level of hostility. To better understand the counter-speech used in conversational interactions, this study distills persuasion modes into reason, emotion, and credibility and then evaluates their use in two types of conversation interactions: closed (multi-turn) and open (single-turn) conversation interactions concerning racism, sexism, and religion. The evaluation covers the distinct behaviors of human versus generated counter-speech. We also assess the interplay between the replies' stance and each mode of persuasion in the counter-speech. Notably, we observe nuanced differences in the counter-speech persuasion modes for open and closed interactions -- especially on the topic level
&lt;/p&gt;</description></item><item><title>&#37327;&#21270;&#30446;&#21069;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#65292;&#20294;&#21098;&#26525;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;</title><link>https://arxiv.org/abs/2403.15447</link><description>&lt;p&gt;
&#35299;&#30721;&#21387;&#32553;&#30340;&#20449;&#20219;&#65306;&#23457;&#35270;&#22312;&#21387;&#32553;&#19979;&#39640;&#25928;LLMs&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;
Decoding Compressed Trust: Scrutinizing the Trustworthiness of Efficient LLMs Under Compression
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15447
&lt;/p&gt;
&lt;p&gt;
&#37327;&#21270;&#30446;&#21069;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#65292;&#21487;&#20197;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#65292;&#20294;&#21098;&#26525;&#20250;&#26174;&#33879;&#38477;&#20302;&#27169;&#22411;&#30340;&#21487;&#20449;&#24230;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#24615;&#33021;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21387;&#32553;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36164;&#28304;&#39640;&#25928;&#25512;&#26029;&#30340;&#39318;&#36873;&#31574;&#30053;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#30340;&#21387;&#32553;&#26041;&#27861;&#22312;&#20445;&#30041;&#33391;&#24615;&#20219;&#21153;&#24615;&#33021;&#26041;&#38754;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#36827;&#23637;&#65292;&#20294;&#21387;&#32553;&#22312;&#23433;&#20840;&#24615;&#21644;&#21487;&#20449;&#24230;&#26041;&#38754;&#30340;&#28508;&#22312;&#39118;&#38505;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#34987;&#24573;&#35270;&#12290;&#36825;&#39033;&#30740;&#31350;&#23545;&#20351;&#29992;&#20116;&#31181;&#26368;&#20808;&#36827;&#21387;&#32553;&#25216;&#26415;&#35780;&#20272;&#19977;&#31181;&#39046;&#20808;LLMs&#30340;&#21487;&#20449;&#24230;&#32500;&#24230;&#36827;&#34892;&#20102;&#39318;&#27425;&#24443;&#24213;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#31361;&#20986;&#20102;&#21387;&#32553;&#19982;&#21487;&#20449;&#24230;&#20043;&#38388;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#25581;&#31034;&#20102;&#19968;&#20123;&#26377;&#36259;&#30340;&#27169;&#24335;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#30446;&#21069;&#37327;&#21270;&#27604;&#21098;&#26525;&#26356;&#26377;&#25928;&#22320;&#21516;&#26102;&#23454;&#29616;&#25928;&#29575;&#21644;&#21487;&#20449;&#24230;&#12290;&#20363;&#22914;&#65292;4&#20301;&#37327;&#21270;&#27169;&#22411;&#20445;&#30041;&#20102;&#20854;&#21407;&#22987;&#23545;&#24212;&#29289;&#30340;&#21487;&#20449;&#24230;&#65292;&#20294;&#27169;&#22411;&#21098;&#26525;&#26174;&#33879;&#38477;&#20302;&#20102;&#21487;&#20449;&#24230;&#65292;&#21363;&#20351;&#22312;50%&#30340;&#31232;&#30095;&#24230;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15447v1 Announce Type: cross  Abstract: Compressing high-capability Large Language Models (LLMs) has emerged as a favored strategy for resource-efficient inferences. While state-of-the-art (SoTA) compression methods boast impressive advancements in preserving benign task performance, the potential risks of compression in terms of safety and trustworthiness have been largely neglected. This study conducts the first, thorough evaluation of three (3) leading LLMs using five (5) SoTA compression techniques across eight (8) trustworthiness dimensions. Our experiments highlight the intricate interplay between compression and trustworthiness, revealing some interesting patterns. We find that quantization is currently a more effective approach than pruning in achieving efficiency and trustworthiness simultaneously. For instance, a 4-bit quantized model retains the trustworthiness of its original counterpart, but model pruning significantly degrades trustworthiness, even at 50% spars
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;ARIMA&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;LDA/HDP&#27169;&#22411;&#25552;&#21462;&#22810;&#35821;&#35328;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20027;&#39064;&#21160;&#24577;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;&#21361;&#26426;&#26399;&#38388;&#30340;&#20132;&#27969;&#36235;&#21183;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#35821;&#35328;&#19968;&#33268;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.15445</link><description>&lt;p&gt;
&#36890;&#36807;ARIMA&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#25581;&#31034;&#31038;&#20132;&#32593;&#32476;&#19978;&#30340;&#22810;&#35821;&#35328;&#20027;&#39064;&#21160;&#24577;&#21644;&#36235;&#21183;&#35782;&#21035;&#65306;LDA/HDP&#27169;&#22411;&#22686;&#24378;&#30340;&#26032;&#22411;&#25968;&#25454;&#32763;&#35793;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Decoding Multilingual Topic Dynamics and Trend Identification through ARIMA Time Series Analysis on Social Networks: A Novel Data Translation Framework Enhanced by LDA/HDP Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15445
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;ARIMA&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#21644;LDA/HDP&#27169;&#22411;&#25552;&#21462;&#22810;&#35821;&#35328;&#31038;&#20132;&#32593;&#32476;&#20013;&#30340;&#20027;&#39064;&#21160;&#24577;&#65292;&#29305;&#21035;&#20851;&#27880;&#22312;&#21361;&#26426;&#26399;&#38388;&#30340;&#20132;&#27969;&#36235;&#21183;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#35821;&#35328;&#19968;&#33268;&#24615;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#30772;&#35793;&#22810;&#35821;&#35328;&#20027;&#39064;&#21160;&#24577;&#65292;&#24182;&#35782;&#21035;&#21361;&#26426;&#26399;&#38388;&#30340;&#20132;&#27969;&#36235;&#21183;&#12290;&#25105;&#20204;&#20851;&#27880;&#31361;&#23612;&#26031;&#31038;&#20132;&#32593;&#32476;&#20013;&#22312;&#20896;&#29366;&#30149;&#27602;&#22823;&#27969;&#34892;&#26399;&#38388;&#20197;&#21450;&#20854;&#20182;&#26174;&#33879;&#20027;&#39064;&#65288;&#22914;&#20307;&#32946;&#21644;&#25919;&#27835;&#65289;&#20013;&#30340;&#23545;&#35805;&#12290;&#25105;&#20204;&#39318;&#20808;&#23545;&#19982;&#36825;&#20123;&#20027;&#39064;&#30456;&#20851;&#30340;&#21508;&#31181;&#22810;&#35821;&#35328;&#35780;&#35770;&#36827;&#34892;&#32858;&#21512;&#12290;&#28982;&#21518;&#22312;&#25968;&#25454;&#39044;&#22788;&#29702;&#36807;&#31243;&#20013;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20005;&#26684;&#30340;&#31934;&#28860;&#12290;&#25105;&#20204;&#24341;&#20837;&#25105;&#20204;&#30340;&#26080;&#33521;&#35821;&#21040;&#33521;&#35821;&#30340;&#26426;&#22120;&#32763;&#35793;&#26041;&#27861;&#26469;&#22788;&#29702;&#35821;&#35328;&#24046;&#24322;&#12290;&#23545;&#36825;&#31181;&#26041;&#27861;&#30340;&#23454;&#35777;&#27979;&#35797;&#26174;&#31034;&#20102;&#24456;&#39640;&#30340;&#20934;&#30830;&#24615;&#21644;F1&#20540;&#65292;&#31361;&#26174;&#20102;&#23427;&#36866;&#29992;&#20110;&#35821;&#35328;&#19968;&#33268;&#20219;&#21153;&#30340;&#29305;&#28857;&#12290;&#28145;&#20837;&#30740;&#31350;&#65292;&#37319;&#29992;&#20102;&#20808;&#36827;&#30340;&#24314;&#27169;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;LDA&#21644;HDP&#27169;&#22411;&#65292;&#20174;&#32763;&#35793;&#20869;&#23481;&#20013;&#25552;&#21462;&#30456;&#20851;&#20027;&#39064;&#12290;&#36825;&#23548;&#33268;&#24212;&#29992;ARIMA&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#26469;&#35299;&#30721;&#19981;&#26029;&#21464;&#21270;&#30340;&#20027;&#39064;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15445v1 Announce Type: cross  Abstract: In this study, the authors present a novel methodology adept at decoding multilingual topic dynamics and identifying communication trends during crises. We focus on dialogues within Tunisian social networks during the Coronavirus Pandemic and other notable themes like sports and politics. We start by aggregating a varied multilingual corpus of comments relevant to these subjects. This dataset undergoes rigorous refinement during data preprocessing. We then introduce our No-English-to-English Machine Translation approach to handle linguistic differences. Empirical tests of this method showed high accuracy and F1 scores, highlighting its suitability for linguistically coherent tasks. Delving deeper, advanced modeling techniques, specifically LDA and HDP models are employed to extract pertinent topics from the translated content. This leads to applying ARIMA time series analysis to decode evolving topic trends. Applying our method to a mu
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22914;&#20309;&#22312;&#20154;&#31867;&#27963;&#21160;/&#21160;&#20316;&#35782;&#21035;&#20013;&#23454;&#29616;&#36328;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#65292;&#25506;&#35752;&#20102;IMU&#25968;&#25454;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#23545;HAR&#38382;&#39064;&#36827;&#34892;&#20102;&#37325;&#35201;&#24615;&#25506;&#35752;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#22810;&#27169;&#24577;HAR&#25968;&#25454;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.15444</link><description>&lt;p&gt;
&#22522;&#20110;IMU&#30340;&#36328;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#22312;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey of IMU Based Cross-Modal Transfer Learning in Human Activity Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15444
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#35843;&#26597;&#20102;&#22914;&#20309;&#22312;&#20154;&#31867;&#27963;&#21160;/&#21160;&#20316;&#35782;&#21035;&#20013;&#23454;&#29616;&#36328;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#65292;&#25506;&#35752;&#20102;IMU&#25968;&#25454;&#22312;&#27492;&#39046;&#22495;&#20013;&#30340;&#28508;&#22312;&#24212;&#29992;&#65292;&#23545;HAR&#38382;&#39064;&#36827;&#34892;&#20102;&#37325;&#35201;&#24615;&#25506;&#35752;&#65292;&#24182;&#27604;&#36739;&#20102;&#19981;&#21516;&#31867;&#22411;&#30340;&#22810;&#27169;&#24577;HAR&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#29983;&#27963;&#22312;&#19968;&#20010;&#22810;&#24863;&#30693;&#19990;&#30028;&#20013;&#65292;&#22823;&#22810;&#25968;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#20173;&#28982;&#23616;&#38480;&#20110;&#23545;&#20154;&#20307;&#36816;&#21160;&#21644;&#34892;&#20026;&#30340;&#25991;&#26412;&#21644;&#35270;&#35273;&#29702;&#35299;&#12290;&#20107;&#23454;&#19978;&#65292;&#23545;&#20154;&#31867;&#36816;&#21160;&#30340;&#23436;&#25972;&#24773;&#22659;&#24847;&#35782;&#26368;&#22909;&#26159;&#36890;&#36807;&#20256;&#24863;&#22120;&#30340;&#32452;&#21512;&#26469;&#29702;&#35299;&#12290;&#22312;&#26412;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22914;&#20309;&#22312;&#20154;&#31867;&#27963;&#21160;/&#21160;&#20316;&#35782;&#21035;&#65288;HAR&#65289;&#20013;&#36328;&#27169;&#24577;&#36801;&#31227;&#23398;&#20064;&#20013;&#20256;&#36882;&#21644;&#21033;&#29992;&#30693;&#35782;&#12290;&#25105;&#20204;&#38416;&#36848;&#20102;IMU&#25968;&#25454;&#21450;&#20854;&#22312;&#36328;&#27169;&#24577;&#23398;&#20064;&#20013;&#30340;&#36866;&#29992;&#24615;&#30340;&#37325;&#35201;&#24615;&#21644;&#28508;&#21147;&#65292;&#20197;&#21450;&#30740;&#31350;HAR&#38382;&#39064;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#26102;&#38388;&#21644;&#25277;&#35937;&#24615;&#23558;HAR&#30456;&#20851;&#20219;&#21153;&#36827;&#34892;&#20102;&#20998;&#31867;&#65292;&#28982;&#21518;&#27604;&#36739;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;&#22810;&#27169;&#24577;HAR&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#36824;&#21306;&#20998;&#21644;&#35814;&#32454;&#38416;&#36848;&#20102;&#25991;&#29486;&#20013;&#35768;&#22810;&#30456;&#20851;&#20294;&#19981;&#19968;&#33268;&#20351;&#29992;&#30340;&#26415;&#35821;&#65292;&#22914;&#36801;&#31227;&#23398;&#20064;&#12289;&#22495;&#33258;&#36866;&#24212;&#12289;&#34920;&#31034;&#23398;&#20064;&#12289;&#20256;&#24863;&#22120;&#34701;&#21512;&#21644;&#22810;&#27169;&#24577;&#23398;&#20064;&#65292;&#24182;&#25551;&#36848;&#20102;&#36328;&#27169;&#24577;&#23398;&#20064;&#22914;&#20309;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15444v1 Announce Type: cross  Abstract: Despite living in a multi-sensory world, most AI models are limited to textual and visual understanding of human motion and behavior. In fact, full situational awareness of human motion could best be understood through a combination of sensors. In this survey we investigate how knowledge can be transferred and utilized amongst modalities for Human Activity/Action Recognition (HAR), i.e. cross-modality transfer learning. We motivate the importance and potential of IMU data and its applicability in cross-modality learning as well as the importance of studying the HAR problem. We categorize HAR related tasks by time and abstractness and then compare various types of multimodal HAR datasets. We also distinguish and expound on many related but inconsistently used terms in the literature, such as transfer learning, domain adaptation, representation learning, sensor fusion, and multimodal learning, and describe how cross-modal learning fits w
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;PET&#25195;&#25551;&#22270;&#20687;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26102;&#20351;&#29992;&#20102;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15443</link><description>&lt;p&gt;
&#24341;&#20837;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#26512;PET&#25195;&#25551;&#22270;&#20687;&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;
&lt;/p&gt;
&lt;p&gt;
Introducing an ensemble method for the early detection of Alzheimer's disease through the analysis of PET scan images
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15443
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;PET&#25195;&#25551;&#22270;&#20687;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#26089;&#26399;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65292;&#24182;&#19988;&#22312;&#20998;&#31867;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26102;&#20351;&#29992;&#20102;&#22810;&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#26159;&#19968;&#31181;&#36880;&#28176;&#24694;&#21270;&#30340;&#31070;&#32463;&#36864;&#34892;&#24615;&#30142;&#30149;&#65292;&#20027;&#35201;&#24433;&#21709;&#35760;&#24518;&#12289;&#24605;&#32500;&#21644;&#34892;&#20026;&#31561;&#35748;&#30693;&#21151;&#33021;&#12290;&#26412;&#30149;&#23384;&#22312;&#19968;&#20010;&#20851;&#38190;&#38454;&#27573;&#65292;&#21363;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65292;&#38750;&#24120;&#37325;&#35201;&#23613;&#26089;&#35786;&#26029;&#65292;&#22240;&#20026;&#19968;&#20123;&#36880;&#28176;&#21457;&#23637;&#20026;&#30149;&#30151;&#30340;MCI&#24739;&#32773;&#20250;&#21457;&#23637;&#20026;&#36825;&#31181;&#30142;&#30149;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#23558;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#20998;&#31867;&#20026;&#22235;&#20010;&#19981;&#21516;&#32452;&#65306;&#25511;&#21046;&#27491;&#24120;&#65288;CN&#65289;&#12289;&#36880;&#28176;&#21457;&#23637;&#30340;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;pMCI&#65289;&#12289;&#31283;&#23450;&#30340;&#36731;&#24230;&#35748;&#30693;&#38556;&#30861;&#65288;sMCI&#65289;&#21644;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#65288;AD&#65289;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#36825;&#31181;&#20998;&#31867;&#26159;&#22522;&#20110;&#23545;&#20174;ADNI&#25968;&#25454;&#38598;&#33719;&#24471;&#30340;PET&#25195;&#25551;&#22270;&#20687;&#30340;&#24443;&#24213;&#26816;&#26597;&#65292;&#36825;&#25552;&#20379;&#20102;&#23545;&#30142;&#30149;&#36827;&#23637;&#30340;&#24443;&#24213;&#29702;&#35299;&#12290;&#24050;&#32463;&#20351;&#29992;&#20102;&#20960;&#31181;&#28145;&#24230;&#23398;&#20064;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#26469;&#26816;&#27979;&#38463;&#23572;&#33576;&#28023;&#40664;&#30149;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20351;&#29992;&#20102;&#19977;&#31181;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21363;VGG16&#12289;AlexNet&#21644;&#33258;&#23450;&#20041;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15443v1 Announce Type: cross  Abstract: Alzheimer's disease is a progressive neurodegenerative disorder that primarily affects cognitive functions such as memory, thinking, and behavior. In this disease, there is a critical phase, mild cognitive impairment, that is really important to be diagnosed early since some patients with progressive MCI will develop the disease. This study delves into the challenging task of classifying Alzheimer's disease into four distinct groups: control normal (CN), progressive mild cognitive impairment (pMCI), stable mild cognitive impairment (sMCI), and Alzheimer's disease (AD). This classification is based on a thorough examination of PET scan images obtained from the ADNI dataset, which provides a thorough understanding of the disease's progression. Several deep-learning and traditional machine-learning models have been used to detect Alzheimer's disease. In this paper, three deep-learning models, namely VGG16 and AlexNet, and a custom Convolu
&lt;/p&gt;</description></item><item><title>&#20154;&#24037;&#26234;&#33021;&#22312;&#25552;&#39640;&#26893;&#20837;&#24335;&#21548;&#35273;&#35774;&#22791;&#30340;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#21069;&#30651;&#24615;&#65292;&#24182;&#36890;&#36807;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#20197;&#21450;&#24212;&#23545;&#22810;&#28304;&#35821;&#38899;&#21644;&#29615;&#22659;&#22122;&#38899;&#25361;&#25112;&#31561;&#26041;&#27861;&#26469;&#20811;&#26381;&#35821;&#38899;&#22833;&#30495;&#38382;&#39064;</title><link>https://arxiv.org/abs/2403.15442</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#32819;&#34583;&#26893;&#20837;&#35013;&#32622;&#20013;&#30340;&#20808;&#36827;&#31639;&#27861;&#65306;&#21307;&#30103;&#31574;&#30053;&#12289;&#25361;&#25112;&#21644;&#23637;&#26395;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Advanced Artificial Intelligence Algorithms in Cochlear Implants: Review of Healthcare Strategies, Challenges, and Perspectives
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15442
&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25552;&#39640;&#26893;&#20837;&#24335;&#21548;&#35273;&#35774;&#22791;&#30340;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20855;&#26377;&#21069;&#30651;&#24615;&#65292;&#24182;&#36890;&#36807;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#20197;&#21450;&#24212;&#23545;&#22810;&#28304;&#35821;&#38899;&#21644;&#29615;&#22659;&#22122;&#38899;&#25361;&#25112;&#31561;&#26041;&#27861;&#26469;&#20811;&#26381;&#35821;&#38899;&#22833;&#30495;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15442v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#33258;&#21160;&#35821;&#38899;&#35782;&#21035;&#65288;ASR&#65289;&#22312;&#25105;&#20204;&#30340;&#26085;&#24120;&#29983;&#27963;&#20013;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#19981;&#20165;&#20026;&#19982;&#26426;&#22120;&#20132;&#20114;&#25552;&#20379;&#20102;&#20415;&#21033;&#65292;&#36824;&#20026;&#37096;&#20998;&#25110;&#23436;&#20840;&#21548;&#21147;&#21463;&#25439;&#30340;&#20010;&#20307;&#25552;&#20379;&#20102;&#27807;&#36890;&#30340;&#26426;&#20250;&#12290;&#36825;&#19968;&#36807;&#31243;&#28041;&#21450;&#20197;&#27169;&#25311;&#24418;&#24335;&#25509;&#25910;&#35821;&#38899;&#20449;&#21495;&#65292;&#28982;&#21518;&#36890;&#36807;&#21508;&#31181;&#20449;&#21495;&#22788;&#29702;&#31639;&#27861;&#20351;&#20854;&#19982;&#23481;&#37327;&#26377;&#38480;&#30340;&#35774;&#22791;&#65288;&#22914;CI&#65289;&#20860;&#23481;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#37197;&#22791;&#26377;&#26377;&#38480;&#25968;&#37327;&#30005;&#26497;&#30340;&#26893;&#20837;&#35013;&#32622;&#22312;&#21512;&#25104;&#36807;&#31243;&#20013;&#24448;&#24448;&#23548;&#33268;&#35821;&#38899;&#22833;&#30495;&#12290;&#23613;&#31649;&#30740;&#31350;&#20154;&#21592;&#22312;&#20351;&#29992;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#20449;&#21495;&#22788;&#29702;&#25216;&#26415;&#25913;&#21892;&#25509;&#25910;&#21040;&#30340;&#35821;&#38899;&#36136;&#37327;&#26041;&#38754;&#20570;&#20986;&#20102;&#21162;&#21147;&#65292;&#20294;&#22312;&#28041;&#21450;&#22810;&#20010;&#35821;&#38899;&#28304;&#12289;&#29615;&#22659;&#22122;&#22768;&#21644;&#20854;&#20182;&#24773;&#20917;&#30340;&#22330;&#26223;&#20013;&#65292;&#25361;&#25112;&#20173;&#28982;&#23384;&#22312;&#12290;&#26032;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#26041;&#27861;&#30340;&#20986;&#29616;&#24341;&#20837;&#20102;&#20808;&#36827;&#30340;&#31574;&#30053;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15442v1 Announce Type: cross  Abstract: Automatic speech recognition (ASR) plays a pivotal role in our daily lives, offering utility not only for interacting with machines but also for facilitating communication for individuals with either partial or profound hearing impairments. The process involves receiving the speech signal in analogue form, followed by various signal processing algorithms to make it compatible with devices of limited capacity, such as cochlear implants (CIs). Unfortunately, these implants, equipped with a finite number of electrodes, often result in speech distortion during synthesis. Despite efforts by researchers to enhance received speech quality using various state-of-the-art signal processing techniques, challenges persist, especially in scenarios involving multiple sources of speech, environmental noise, and other circumstances. The advent of new artificial intelligence (AI) methods has ushered in cutting-edge strategies to address the limitations
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24341;&#20837;&#20102;Geometric Bayesian Flow Networks (GeoBFN)&#65292;&#36890;&#36807;&#22312;&#20998;&#24067;&#30340;&#21487;&#24494;&#20998;&#21442;&#25968;&#31354;&#38388;&#20013;&#23545;&#19981;&#21516;&#27169;&#24577;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#24615;&#21644;&#22122;&#22768;&#25935;&#24863;&#24615;&#30340;&#20998;&#23376;&#20960;&#20309;&#24418;&#29366;&#30340;&#33258;&#28982;&#25311;&#21512;&#12290;GeoBFN&#36890;&#36807;&#20248;&#21270;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;3D&#20998;&#23376;&#29983;&#25104;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15441</link><description>&lt;p&gt;
&#32479;&#19968;&#29983;&#25104;&#24314;&#27169;&#65306;&#22522;&#20110;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#30340;3D&#20998;&#23376;
&lt;/p&gt;
&lt;p&gt;
Unified Generative Modeling of 3D Molecules via Bayesian Flow Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15441
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#20102;Geometric Bayesian Flow Networks (GeoBFN)&#65292;&#36890;&#36807;&#22312;&#20998;&#24067;&#30340;&#21487;&#24494;&#20998;&#21442;&#25968;&#31354;&#38388;&#20013;&#23545;&#19981;&#21516;&#27169;&#24577;&#36827;&#34892;&#24314;&#27169;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#27169;&#24577;&#24615;&#21644;&#22122;&#22768;&#25935;&#24863;&#24615;&#30340;&#20998;&#23376;&#20960;&#20309;&#24418;&#29366;&#30340;&#33258;&#28982;&#25311;&#21512;&#12290;GeoBFN&#36890;&#36807;&#20248;&#21270;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#22312;&#22810;&#20010;3D&#20998;&#23376;&#29983;&#25104;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20808;&#36827;&#30340;&#29983;&#25104;&#27169;&#22411;&#65288;&#20363;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#28304;&#33258;&#23545;&#25968;&#25454;&#20998;&#24067;&#30340;&#31616;&#21270;&#36830;&#32493;&#24615;&#20551;&#35774;&#65292;&#23613;&#31649;&#26174;&#31034;&#20986;&#20102;&#33391;&#22909;&#30340;&#36827;&#23637;&#65292;&#20294;&#30001;&#20110;&#20998;&#23376;&#20960;&#20309;&#30340;&#22810;&#27169;&#24577;&#24615;&#21644;&#23545;&#22122;&#22768;&#25935;&#24863;&#30340;&#29305;&#24615;&#65292;&#24456;&#38590;&#30452;&#25509;&#24212;&#29992;&#20110;&#20960;&#20309;&#29983;&#25104;&#24212;&#29992;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#20960;&#20309;&#36125;&#21494;&#26031;&#27969;&#32593;&#32476;&#65288;GeoBFN&#65289;&#65292;&#36890;&#36807;&#22312;&#21487;&#24494;&#20998;&#21442;&#25968;&#31354;&#38388;&#20013;&#24314;&#27169;&#19981;&#21516;&#27169;&#24577;&#65292;&#33258;&#28982;&#22320;&#36866;&#37197;&#20998;&#23376;&#20960;&#20309;&#24418;&#29366;&#12290;GeoBFN&#36890;&#36807;&#22312;&#20998;&#24067;&#21442;&#25968;&#19978;&#21512;&#25104;&#31227;&#21464;&#30456;&#20114;&#20381;&#36182;&#24314;&#27169;&#65292;&#20445;&#25345;&#20102;SE-(3)&#19981;&#21464;&#23494;&#24230;&#24314;&#27169;&#23646;&#24615;&#65292;&#24182;&#32479;&#19968;&#20102;&#19981;&#21516;&#27169;&#24577;&#30340;&#27010;&#29575;&#24314;&#27169;&#12290;&#36890;&#36807;&#20248;&#21270;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;GeoBFN&#22312;&#22810;&#20010;3D&#20998;&#23376;&#29983;&#25104;&#22522;&#20934;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#26368;&#22823;&#38480;&#24230;&#22320;&#25552;&#39640;&#20102;&#29983;&#25104;&#36136;&#37327;&#65288;&#22312;QM9&#20013;&#31283;&#23450;&#29983;&#25104;90.87%&#30340;&#20998;&#23376;&#65292;&#22312;&#21407;&#23376;&#26041;&#38754;&#31283;&#23450;&#29983;&#25104;85.6%&#65289;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15441v1 Announce Type: cross  Abstract: Advanced generative model (e.g., diffusion model) derived from simplified continuity assumptions of data distribution, though showing promising progress, has been difficult to apply directly to geometry generation applications due to the multi-modality and noise-sensitive nature of molecule geometry. This work introduces Geometric Bayesian Flow Networks (GeoBFN), which naturally fits molecule geometry by modeling diverse modalities in the differentiable parameter space of distributions. GeoBFN maintains the SE-(3) invariant density modeling property by incorporating equivariant inter-dependency modeling on parameters of distributions and unifying the probabilistic modeling of different modalities. Through optimized training and sampling techniques, we demonstrate that GeoBFN achieves state-of-the-art performance on multiple 3D molecule generation benchmarks in terms of generation quality (90.87% molecule stability in QM9 and 85.6% atom
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#32473;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#28155;&#21152;&#33258;&#21160;&#21270;&#20154;&#31867;&#24418;&#24335;&#30340;&#35777;&#26126;&#26816;&#26597;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#23427;&#20204;&#37027;&#37324;&#33719;&#21462;&#20808;&#39564;&#25968;&#23398;&#30693;&#35782;&#65292;&#21363;&#20351;&#36825;&#20123;&#26426;&#22120;&#23436;&#20840;&#23545;&#25105;&#20204;&#19981;&#36879;&#26126;&#12290;</title><link>https://arxiv.org/abs/2403.15437</link><description>&lt;p&gt;
&#22312;&#35745;&#31639;&#19981;&#36879;&#26126;&#26102;&#20195;&#30340;&#20808;&#39564;&#30693;&#35782;&#65306;&#20154;&#24037;&#26234;&#33021;&#22312;&#25968;&#23398;&#21457;&#29616;&#20013;&#30340;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Apriori Knowledge in an Era of Computational Opacity: The Role of AI in Mathematical Discovery
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15437
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#32473;&#29616;&#20195;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#28155;&#21152;&#33258;&#21160;&#21270;&#20154;&#31867;&#24418;&#24335;&#30340;&#35777;&#26126;&#26816;&#26597;&#22120;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;&#23427;&#20204;&#37027;&#37324;&#33719;&#21462;&#20808;&#39564;&#25968;&#23398;&#30693;&#35782;&#65292;&#21363;&#20351;&#36825;&#20123;&#26426;&#22120;&#23436;&#20840;&#23545;&#25105;&#20204;&#19981;&#36879;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35745;&#31639;&#22312;&#24403;&#20195;&#25968;&#23398;&#20013;&#36215;&#30528;&#26680;&#24515;&#20316;&#29992;&#12290;&#35768;&#22810;&#20154;&#35748;&#20026;&#65292;&#25105;&#20204;&#21487;&#20197;&#20174;Appel&#21644;Haken&#30340;&#31243;&#24207;&#20013;&#33719;&#24471;&#22235;&#33394;&#23450;&#29702;&#30340;&#30495;&#27491;&#25968;&#23398;&#30693;&#35782;&#65292;&#22240;&#20026;&#36825;&#21482;&#26159;&#19968;&#31181;&#37325;&#22797;&#24212;&#29992;&#20154;&#31867;&#24418;&#24335;&#30340;&#25968;&#23398;&#25512;&#29702;&#12290;&#28982;&#32780;&#65292;&#30456;&#27604;&#20043;&#19979;&#65292;&#29616;&#20195;LLMs / DNNs&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23545;&#25105;&#20204;&#19981;&#36879;&#26126;&#65292;&#36825;&#22312;&#20174;&#23427;&#20204;&#37027;&#37324;&#33719;&#21462;&#25968;&#23398;&#30693;&#35782;&#26041;&#38754;&#20135;&#29983;&#20102;&#38556;&#30861;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22914;&#26524;&#32473;&#36825;&#20123;&#26426;&#22120;&#28155;&#21152;&#33258;&#21160;&#21270;&#20154;&#31867;&#24418;&#24335;&#30340;&#35777;&#26126;&#26816;&#26597;&#22120;&#65292;&#37027;&#20040;&#25105;&#20204;&#21487;&#20197;&#20174;&#20013;&#33719;&#24471;&#20808;&#39564;&#25968;&#23398;&#30693;&#35782;&#65292;&#21363;&#20351;&#21407;&#22987;&#26426;&#22120;&#23545;&#25105;&#20204;&#23436;&#20840;&#19981;&#36879;&#26126;&#65292;&#23427;&#20204;&#36755;&#20986;&#30340;&#35777;&#26126;&#20063;&#26080;&#27861;&#30001;&#20154;&#31867;&#26597;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15437v1 Announce Type: new  Abstract: Computation is central to contemporary mathematics. Many accept that we can acquire genuine mathematical knowledge of the Four Color Theorem from Appel and Haken's program insofar as it is simply a repetitive application of human forms of mathematical reasoning. Modern LLMs / DNNs are, by contrast, opaque to us in significant ways, and this creates obstacles in obtaining mathematical knowledge from them. We argue, however, that if a proof-checker automating human forms of proof-checking is attached to such machines, then we can obtain apriori mathematical knowledge from them, even though the original machines are entirely opaque to us and the proofs they output are not human-surveyable.
&lt;/p&gt;</description></item><item><title>ChatPattern&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#24067;&#23616;&#27169;&#24335;&#23450;&#21046;&#65292;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35201;&#27714;&#29983;&#25104;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#27169;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.15434</link><description>&lt;p&gt;
ChatPattern: &#21033;&#29992;&#33258;&#28982;&#35821;&#35328;&#36827;&#34892;&#24067;&#23616;&#27169;&#24335;&#23450;&#21046;
&lt;/p&gt;
&lt;p&gt;
ChatPattern: Layout Pattern Customization via Natural Language
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15434
&lt;/p&gt;
&lt;p&gt;
ChatPattern&#21033;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26694;&#26550;&#23454;&#29616;&#20102;&#28789;&#27963;&#30340;&#24067;&#23616;&#27169;&#24335;&#23450;&#21046;&#65292;&#33021;&#22815;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#35201;&#27714;&#29983;&#25104;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#27169;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#26377;&#30340;&#20316;&#21697;&#20027;&#35201;&#38598;&#20013;&#22312;&#29983;&#25104;&#22266;&#23450;&#22823;&#23567;&#30340;&#24067;&#23616;&#27169;&#24335;&#65292;&#32780;&#26356;&#23454;&#29992;&#30340;&#33258;&#30001;&#22823;&#23567;&#27169;&#24335;&#29983;&#25104;&#21463;&#21040;&#20102;&#26377;&#38480;&#30340;&#20851;&#27880;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ChatPattern&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#28789;&#27963;&#30340;&#27169;&#24335;&#23450;&#21046;&#12290;ChatPattern&#21033;&#29992;&#19968;&#20010;&#21253;&#21547;&#19987;&#23478;LLM&#20195;&#29702;&#21644;&#19968;&#20010;&#39640;&#24230;&#21487;&#25511;&#30340;&#24067;&#23616;&#27169;&#24335;&#29983;&#25104;&#22120;&#30340;&#21452;&#37096;&#20998;&#31995;&#32479;&#12290;LLM&#20195;&#29702;&#21487;&#20197;&#35299;&#37322;&#33258;&#28982;&#35821;&#35328;&#35201;&#27714;&#24182;&#25805;&#20316;&#35774;&#35745;&#24037;&#20855;&#20197;&#28385;&#36275;&#25351;&#23450;&#38656;&#27714;&#65292;&#32780;&#29983;&#25104;&#22120;&#25797;&#38271;&#20110;&#26465;&#20214;&#24067;&#23616;&#29983;&#25104;&#12289;&#27169;&#24335;&#20462;&#25913;&#21644;&#20869;&#23384;&#21451;&#22909;&#22411;&#27169;&#24335;&#25193;&#23637;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#27169;&#24335;&#29983;&#25104;&#35774;&#32622;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#20102;ChatPattern&#21512;&#25104;&#39640;&#36136;&#37327;&#22823;&#35268;&#27169;&#27169;&#24335;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15434v1 Announce Type: cross  Abstract: Existing works focus on fixed-size layout pattern generation, while the more practical free-size pattern generation receives limited attention. In this paper, we propose ChatPattern, a novel Large-Language-Model (LLM) powered framework for flexible pattern customization. ChatPattern utilizes a two-part system featuring an expert LLM agent and a highly controllable layout pattern generator. The LLM agent can interpret natural language requirements and operate design tools to meet specified needs, while the generator excels in conditional layout generation, pattern modification, and memory-friendly patterns extension. Experiments on challenging pattern generation setting shows the ability of ChatPattern to synthesize high-quality large-scale patterns.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#28151;&#21512;&#24314;&#27169;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#29289;&#29702;&#24050;&#30693;&#34920;&#36798;&#24335;&#21644;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#20010;&#24615;&#21270;&#24515;&#33039;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#29289;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#20998;&#31163;&#35782;&#21035;&#12290;</title><link>https://arxiv.org/abs/2403.15433</link><description>&lt;p&gt;
HyPer-EP: &#20026;&#24515;&#33039;&#30005;&#29983;&#29702;&#32780;&#20803;&#23398;&#20064;&#30340;&#28151;&#21512;&#20010;&#24615;&#21270;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
HyPer-EP: Meta-Learning Hybrid Personalized Models for Cardiac Electrophysiology
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15433
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#28151;&#21512;&#24314;&#27169;&#26694;&#26550;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#29289;&#29702;&#24050;&#30693;&#34920;&#36798;&#24335;&#21644;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#25551;&#36848;&#20010;&#24615;&#21270;&#24515;&#33039;&#25968;&#23383;&#23402;&#29983;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#22522;&#20110;&#29289;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#20998;&#31163;&#35782;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#34394;&#25311;&#24515;&#33039;&#27169;&#22411;&#22312;&#20020;&#24202;&#19978;&#23637;&#31034;&#20986;&#36234;&#26469;&#36234;&#22823;&#30340;&#28508;&#21147;&#65292;&#23613;&#31649;&#22312;&#32473;&#23450;&#24739;&#32773;&#29305;&#23450;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20272;&#35745;&#20854;&#21442;&#25968;&#20173;&#28982;&#26159;&#19968;&#20010;&#25361;&#25112;&#12290;&#20256;&#32479;&#30340;&#22522;&#20110;&#29289;&#29702;&#30340;&#24314;&#27169;&#26041;&#27861;&#22312;&#35745;&#31639;&#19978;&#25104;&#26412;&#39640;&#26114;&#65292;&#24448;&#24448;&#24573;&#35270;&#20102;&#36825;&#20123;&#27169;&#22411;&#20013;&#30340;&#32467;&#26500;&#24615;&#38169;&#35823;&#65292;&#36825;&#26159;&#30001;&#20110;&#27169;&#22411;&#31616;&#21270;&#21644;&#20551;&#35774;&#25152;&#36896;&#25104;&#30340;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#20381;&#36182;&#20110;&#25968;&#25454;&#30417;&#30563;&#65292;&#24182;&#19988;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#28151;&#21512;&#24314;&#27169;&#26694;&#26550;&#65292;&#23558;&#20010;&#24615;&#21270;&#24515;&#33039;&#25968;&#23383;&#23402;&#29983;&#25551;&#36848;&#20026;&#22522;&#20110;&#29289;&#29702;&#24050;&#30693;&#34920;&#36798;&#24335;&#21644;&#31070;&#32463;&#32593;&#32476;&#24314;&#27169;&#26410;&#30693;&#19982;&#29616;&#23454;&#20043;&#38388;&#24046;&#36317;&#30340;&#32452;&#21512;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#20197;&#23454;&#29616;&#28151;&#21512;&#27169;&#22411;&#20013;&#22522;&#20110;&#29289;&#29702;&#21644;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#30340;&#20998;&#31163;&#35782;&#21035;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#36825;&#31181;&#28151;&#21512;&#24314;&#27169;&#26694;&#26550;&#30340;&#21487;&#34892;&#24615;&#21644;&#26222;&#36866;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15433v1 Announce Type: cross  Abstract: Personalized virtual heart models have demonstrated increasing potential for clinical use, although the estimation of their parameters given patient-specific data remain a challenge. Traditional physics-based modeling approaches are computationally costly and often neglect the inherent structural errors in these models due to model simplifications and assumptions. Modern deep learning approaches, on the other hand, rely heavily on data supervision and lacks interpretability. In this paper, we present a novel hybrid modeling framework to describe a personalized cardiac digital twin as a combination of a physics-based known expression augmented by neural network modeling of its unknown gap to reality. We then present a novel meta-learning framework to enable the separate identification of both the physics-based and neural components in the hybrid model. We demonstrate the feasibility and generality of this hybrid modeling framework with 
&lt;/p&gt;</description></item><item><title>BRIEDGE&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#33041;&#21040;&#22810;&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#36890;&#36807; EEG &#33258;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;&#21644;&#32534;&#35299;&#30721;&#36890;&#20449;&#26694;&#26550;&#23454;&#29616;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;Informer&#30340;ProbSparse&#33258;&#27880;&#24847;&#26426;&#21046;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15432</link><description>&lt;p&gt;
BRIEDGE: EEG&#33258;&#36866;&#24212;&#36793;&#32536;&#20154;&#24037;&#26234;&#33021;&#29992;&#20110;&#22810;&#33041;&#21040;&#22810;&#26426;&#22120;&#20154;&#20132;&#20114;
&lt;/p&gt;
&lt;p&gt;
BRIEDGE: EEG-Adaptive Edge AI for Multi-Brain to Multi-Robot Interaction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15432
&lt;/p&gt;
&lt;p&gt;
BRIEDGE&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#33041;&#21040;&#22810;&#26426;&#22120;&#20154;&#20132;&#20114;&#30340;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#36890;&#36807; EEG &#33258;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;&#21644;&#32534;&#35299;&#30721;&#36890;&#20449;&#26694;&#26550;&#23454;&#29616;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;Informer&#30340;ProbSparse&#33258;&#27880;&#24847;&#26426;&#21046;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817; EEG &#22522;&#20110;&#33041;&#26426;&#25509;&#21475;&#25216;&#26415;&#30340;&#36827;&#23637;&#25581;&#31034;&#20102;&#36890;&#36807;&#25972;&#21512;&#20256;&#24863;&#12289;&#35745;&#31639;&#12289;&#36890;&#20449;&#21644;&#25511;&#21046;&#23454;&#29616;&#33041;&#21040;&#26426;&#22120;&#20154;&#21327;&#20316;&#30340;&#28508;&#21147;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102; BRIEDGE &#20316;&#20026;&#19968;&#20010;&#31471;&#21040;&#31471;&#31995;&#32479;&#65292;&#29992;&#20110;&#36890;&#36807; EEG &#33258;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;&#21644;&#32534;&#35299;&#30721;&#36890;&#20449;&#26694;&#26550;&#23454;&#29616;&#22810;&#33041;&#21040;&#22810;&#26426;&#22120;&#20154;&#20132;&#20114;&#65292;&#22914;&#22270;1&#25152;&#31034;&#12290;&#27491;&#22914;&#25152;&#25551;&#32472;&#30340;&#37027;&#26679;&#65292;&#36793;&#32536;&#31227;&#21160;&#26381;&#21153;&#22120;&#25110;&#36793;&#32536;&#20415;&#25658;&#26381;&#21153;&#22120;&#23558;&#20174;&#29992;&#25143;&#25910;&#38598; EEG &#25968;&#25454;&#65292;&#24182;&#21033;&#29992; EEG &#33258;&#36866;&#24212;&#31070;&#32463;&#32593;&#32476;&#35782;&#21035;&#29992;&#25143;&#30340;&#24847;&#22270;&#12290;&#32534;&#35299;&#30721;&#36890;&#20449;&#26694;&#26550;&#28982;&#21518;&#23545; EEG &#22522;&#30784;&#30340;&#35821;&#20041;&#20449;&#24687;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#22312;&#25968;&#25454;&#20256;&#36755;&#36807;&#31243;&#20013;&#35299;&#30721;&#25104;&#21629;&#20196;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#25552;&#21462;&#24322;&#36136; EEG &#25968;&#25454;&#30340;&#32852;&#21512;&#29305;&#24449;&#20197;&#21450;&#22686;&#24378;&#20998;&#31867;&#20934;&#30830;&#24615;&#65292;BRIEDGE &#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110; Informer &#30340; ProbSparse &#33258;&#27880;&#24847;&#26426;&#21046;&#12290;&#21516;&#26102;&#65292;&#24179;&#34892;&#21644;&#23433;&#20840;&#30340;&#36827;&#34892;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15432v1 Announce Type: cross  Abstract: Recent advances in EEG-based BCI technologies have revealed the potential of brain-to-robot collaboration through the integration of sensing, computing, communication, and control. In this paper, we present BRIEDGE as an end-to-end system for multi-brain to multi-robot interaction through an EEG-adaptive neural network and an encoding-decoding communication framework, as illustrated in Fig.1. As depicted, the edge mobile server or edge portable server will collect EEG data from the users and utilize the EEG-adaptive neural network to identify the users' intentions. The encoding-decoding communication framework then encodes the EEG-based semantic information and decodes it into commands in the process of data transmission. To better extract the joint features of heterogeneous EEG data as well as enhance classification accuracy, BRIEDGE introduces an informer-based ProbSparse self-attention mechanism. Meanwhile, parallel and secure trans
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#19977;&#38454;&#27573;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#39564;&#21644;&#25968;&#25454;&#37325;&#21472;&#20272;&#35745;&#23454;&#29616;&#20102;&#25945;&#32946;&#30693;&#35782;&#30340;&#32467;&#26500;&#25286;&#21368;&#21644;&#22686;&#37327;&#24341;&#23548;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2403.15426</link><description>&lt;p&gt;
&#25945;&#32946;&#29615;&#22659;&#19979;&#38598;&#25104;&#24378;&#20808;&#39564;&#27169;&#22359;&#21644;&#25968;&#25454;&#37325;&#21472;&#20272;&#35745;&#30340;&#19977;&#38454;&#27573;SFT&#28151;&#21512;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A Three-Phases SFT Hybrid Model Integrated Strong Prior Module and Data Overlap Estimation in the Eduation Context
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15426
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#25945;&#32946;&#39046;&#22495;&#20013;&#24212;&#29992;&#30340;&#19977;&#38454;&#27573;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#65292;&#36890;&#36807;&#20808;&#39564;&#21644;&#25968;&#25454;&#37325;&#21472;&#20272;&#35745;&#23454;&#29616;&#20102;&#25945;&#32946;&#30693;&#35782;&#30340;&#32467;&#26500;&#25286;&#21368;&#21644;&#22686;&#37327;&#24341;&#23548;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31471;&#21040;&#31471;&#22522;&#20110;&#20808;&#39564;&#30340;&#19977;&#38454;&#27573;&#30417;&#30563;&#24494;&#35843;&#27169;&#22411;&#65292;&#35777;&#26126;&#27604;&#20256;&#32479;&#24494;&#35843;&#26041;&#27861;&#26356;&#26377;&#31454;&#20105;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#23454;&#29616;&#20102;&#25945;&#32946;&#30693;&#35782;&#30340;&#32467;&#26500;&#25286;&#21368;&#21644;&#22686;&#37327;&#24341;&#23548;&#36755;&#20986;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#26679;&#22120;&#21644;&#37325;&#21472;&#20272;&#35745;&#31070;&#32463;&#32593;&#32476;&#23545;&#19977;&#31181;&#31867;&#22411;&#30340;&#25968;&#25454;&#36827;&#34892;&#20102;&#20581;&#22766;&#30340;&#20998;&#31867;&#65292;&#23558;&#39044;&#22788;&#29702;&#25968;&#25454;&#38598;&#20998;&#19977;&#25209;&#27880;&#20837;&#39044;&#35757;&#32451;&#27169;&#22411;&#36827;&#34892;LORA&#24494;&#35843;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#20808;&#39564;&#27169;&#22359;&#65292;&#23558;&#31995;&#32479;&#25552;&#31034;&#12289;&#21521;&#37327;&#25968;&#25454;&#24211;&#21644;&#25277;&#35937;&#35821;&#27861;&#26641;&#20219;&#21153;&#20998;&#21106;&#30456;&#32467;&#21512;&#12290;&#26368;&#21518;&#65292;&#23545;&#22522;&#20110;&#20808;&#39564;&#30340;&#24494;&#35843;&#27169;&#22411;&#24212;&#29992;&#20102;&#21387;&#32553;&#26041;&#27861;&#21644;&#27491;&#21017;&#21270;&#32422;&#26463;&#65292;&#38543;&#21518;&#22312;&#36755;&#20986;&#31471;&#36827;&#34892;&#25991;&#26412;&#36807;&#28388;&#20197;&#33719;&#24471;&#22686;&#37327;&#24341;&#23548;&#32467;&#26524;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#20195;&#34920;&#20102;&#30495;&#27491;&#20197;&#20016;&#23500;&#30340;&#25945;&#32946;&#30693;&#35782;&#12289;&#20998;&#27493;&#25351;&#23548;&#30340;&#29305;&#28857;&#20307;&#29616;&#23548;&#24072;&#35282;&#33394;&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#21162;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15426v1 Announce Type: cross  Abstract: In this paper, we propose an end-to-end prior-based three-phases supervised fine-tuned model, which is proved more competitive than traditional fine-tuning method. More specifically, our model realizes the structural disassembly and incremental guided output of educational knowledge. To this end, we robustify data classification of three types via a sampler and overlap estimation neural network, and inject the preprocessing datasets into pre-trained model in three batches for LORA fine-tuning. Then, we design a prior module couples system prompt, vector databases, and abstract syntax tree task segmentation. Finally, the compression method and regularization constraint are applied to the prior-based fine-tuned model, followed by text filter at the output end to obtain incremental guided results. Our model represents the first research effort to truly embody the tutor role with the features of abundant educational knowledge, step-by-step
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Deep Temporal State Domain Adaptation&#65288;DTSDA&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#34892;&#20026;&#21464;&#24322;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.15424</link><description>&lt;p&gt;
&#20351;&#29992;&#20855;&#26377;&#26102;&#38388;&#20851;&#31995;&#20449;&#24687;&#30340;&#28145;&#24230;&#39046;&#22495;&#33258;&#36866;&#24212;&#36827;&#34892;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-user activity recognition using deep domain adaptation with temporal relation information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15424
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;Deep Temporal State Domain Adaptation&#65288;DTSDA&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#34892;&#20026;&#21464;&#24322;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#26159;&#26222;&#36866;&#35745;&#31639;&#30340;&#22522;&#30707;&#65292;&#20855;&#26377;&#22312;&#20581;&#24247;&#30417;&#27979;&#21644;&#29615;&#22659;&#36741;&#21161;&#29983;&#27963;&#31561;&#22810;&#20010;&#39046;&#22495;&#20013;&#26377;&#21069;&#26223;&#30340;&#24212;&#29992;&#12290;&#23613;&#31649;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;HAR&#26041;&#27861;&#36890;&#24120;&#22312;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#20855;&#26377;&#30456;&#21516;&#20998;&#24067;&#30340;&#20551;&#35774;&#19979;&#36816;&#20316;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#29305;&#21035;&#26159;&#22312;&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;HAR&#20013;&#65292;&#36825;&#19968;&#20551;&#35774;&#22240;&#20998;&#24067;&#20043;&#22806;&#30340;&#25361;&#25112;&#32780;&#26080;&#25928;&#65292;&#21253;&#25324;&#26469;&#33258;&#24322;&#26500;&#20256;&#24863;&#22120;&#30340;&#24046;&#24322;&#12289;&#38543;&#26102;&#38388;&#21464;&#21270;&#20197;&#21450;&#20010;&#20307;&#34892;&#20026;&#21464;&#24322;&#12290;&#26412;&#25991;&#32858;&#28966;&#20110;&#21518;&#32773;&#65292;&#25506;&#32034;&#36328;&#29992;&#25143;HAR&#38382;&#39064;&#65292;&#20854;&#20013;&#20010;&#20307;&#20043;&#38388;&#30340;&#34892;&#20026;&#21464;&#24322;&#23548;&#33268;&#19981;&#21516;&#25968;&#25454;&#20998;&#24067;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Deep Temporal State Domain Adaptation&#65288;DTSDA&#65289;&#27169;&#22411;&#65292;&#36825;&#26159;&#19968;&#31181;&#20026;&#36328;&#29992;&#25143;HAR&#20013;&#30340;&#26102;&#38388;&#24207;&#21015;&#39046;&#22495;&#33258;&#36866;&#24212;&#37327;&#36523;&#23450;&#21046;&#30340;&#21019;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15424v1 Announce Type: cross  Abstract: Human Activity Recognition (HAR) is a cornerstone of ubiquitous computing, with promising applications in diverse fields such as health monitoring and ambient assisted living. Despite significant advancements, sensor-based HAR methods often operate under the assumption that training and testing data have identical distributions. However, in many real-world scenarios, particularly in sensor-based HAR, this assumption is invalidated by out-of-distribution ($\displaystyle o.o.d.$) challenges, including differences from heterogeneous sensors, change over time, and individual behavioural variability. This paper centres on the latter, exploring the cross-user HAR problem where behavioural variability across individuals results in differing data distributions. To address this challenge, we introduce the Deep Temporal State Domain Adaptation (DTSDA) model, an innovative approach tailored for time series domain adaptation in cross-user HAR. Con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#20851;&#31995;&#26368;&#20248;&#36755;&#36816;&#30340;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#22522;&#20110;i.i.d.&#20551;&#35774;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#23616;&#38480;&#24615;</title><link>https://arxiv.org/abs/2403.15423</link><description>&lt;p&gt;
&#22522;&#20110;&#26102;&#38388;&#20851;&#31995;&#26368;&#20248;&#36755;&#36816;&#30340;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Cross-user activity recognition via temporal relation optimal transport
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15423
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26102;&#38388;&#20851;&#31995;&#26368;&#20248;&#36755;&#36816;&#30340;&#36328;&#29992;&#25143;&#27963;&#21160;&#35782;&#21035;&#26041;&#27861;&#65292;&#26088;&#22312;&#35299;&#20915;&#29616;&#26377;&#22522;&#20110;i.i.d.&#20551;&#35774;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#30340;&#23616;&#38480;&#24615;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20851;&#20110;&#20154;&#31867;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#30340;&#30740;&#31350;&#20027;&#35201;&#20551;&#35774;&#35757;&#32451;&#21644;&#27979;&#35797;&#25968;&#25454;&#26469;&#33258;&#30456;&#21516;&#20998;&#24067;&#65292;&#20197;&#23454;&#29616;&#27867;&#21270;&#27169;&#22411;&#65292;&#36825;&#24847;&#21619;&#30528;&#25152;&#26377;&#25968;&#25454;&#34987;&#35748;&#20026;&#26159;&#29420;&#31435;&#21644;&#21516;&#20998;&#24067;&#65288;i.i.d.&#65289;&#12290;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#36825;&#19968;&#20551;&#35774;&#19981;&#25104;&#31435;&#65292;&#25910;&#38598;&#30340;&#35757;&#32451;&#21644;&#30446;&#26631;&#27979;&#35797;&#25968;&#25454;&#38598;&#20855;&#26377;&#19981;&#22343;&#21248;&#20998;&#24067;&#65292;&#22914;&#36328;&#29992;&#25143;HAR&#30340;&#24773;&#20917;&#12290;&#22495;&#33258;&#36866;&#24212;&#26159;&#36328;&#29992;&#25143;HAR&#20219;&#21153;&#30340;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#26041;&#27861;&#12290;&#29616;&#26377;&#30340;&#22522;&#20110;&#22495;&#33258;&#36866;&#24212;&#30340;&#24037;&#20316;&#22522;&#20110;&#27599;&#20010;&#22495;&#20013;&#30340;&#26679;&#26412;&#26159;i.i.d.&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#19981;&#32771;&#34385;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#26102;&#38388;&#20851;&#31995;&#30693;&#35782;&#20197;&#23545;&#40784;&#25968;&#25454;&#20998;&#24067;&#12290;&#36825;&#19968;&#20851;&#20110;i.i.d.&#30340;&#24378;&#20551;&#35774;&#23545;&#20110;&#22522;&#20110;&#26102;&#38388;&#24207;&#21015;&#30340;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#21487;&#33021;&#19981;&#22826;&#36866;&#29992;&#65292;&#22240;&#20026;&#26102;&#38388;&#24207;&#21015;&#32454;&#20998;&#21644;&#29305;&#24449;&#24418;&#25104;&#30340;&#26679;&#26412;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15423v1 Announce Type: cross  Abstract: Current research on human activity recognition (HAR) mainly assumes that training and testing data are drawn from the same distribution to achieve a generalised model, which means all the data are considered to be independent and identically distributed $\displaystyle (i.i.d.) $. In many real-world applications, this assumption does not hold, and collected training and target testing datasets have non-uniform distribution, such as in the case of cross-user HAR. Domain adaptation is a promising approach for cross-user HAR tasks. Existing domain adaptation works based on the assumption that samples in each domain are $\displaystyle i.i.d. $ and do not consider the knowledge of temporal relation hidden in time series data for aligning data distribution. This strong assumption of $\displaystyle i.i.d. $ may not be suitable for time series-related domain adaptation methods because the samples formed by time series segmentation and feature e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#22788;&#29702;&#20256;&#24863;&#22120;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#33021;&#22815;&#25913;&#21892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#26356;&#24555;&#22320;&#24320;&#21457;&#20986;&#20010;&#24615;&#21270;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15422</link><description>&lt;p&gt;
&#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;--&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Machine Learning Techniques for Sensor-based Human Activity Recognition with Data Heterogeneity -- A Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15422
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#30740;&#31350;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#22788;&#29702;&#20256;&#24863;&#22120;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#33021;&#22815;&#25913;&#21892;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#31995;&#32479;&#30340;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#26356;&#24555;&#22320;&#24320;&#21457;&#20986;&#20010;&#24615;&#21270;&#30340;&#33258;&#36866;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15422v1 &#20844;&#21578;&#31867;&#22411;: &#36328;&#39046;&#22495; &#25688;&#35201;: &#22522;&#20110;&#20256;&#24863;&#22120;&#30340;&#20154;&#20307;&#27963;&#21160;&#35782;&#21035;&#65288;HAR&#65289;&#22312;&#26222;&#36866;&#35745;&#31639;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#36890;&#36807;&#22810;&#32500;&#35266;&#23519;&#20998;&#26512;&#34892;&#20026;&#12290; &#23613;&#31649;&#30740;&#31350;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;HAR&#38754;&#20020;&#25361;&#25112;&#65292;&#29305;&#21035;&#26159;&#22312;&#25968;&#25454;&#20998;&#24067;&#20551;&#35774;&#26041;&#38754;&#12290; &#22823;&#22810;&#25968;&#30740;&#31350;&#36890;&#24120;&#20551;&#35774;&#21508;&#20010;&#25968;&#25454;&#38598;&#20043;&#38388;&#20855;&#26377;&#22343;&#21248;&#30340;&#25968;&#25454;&#20998;&#24067;&#65292;&#19982;&#23454;&#38469;&#20256;&#24863;&#22120;&#25968;&#25454;&#22312;&#20154;&#31867;&#27963;&#21160;&#20013;&#30340;&#22810;&#26679;&#24615;&#30456;&#30683;&#30462;&#12290; &#35299;&#20915;&#25968;&#25454;&#24322;&#36136;&#24615;&#38382;&#39064;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#65292;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65292;&#24182;&#26377;&#21161;&#20110;&#24320;&#21457;&#20010;&#24615;&#21270;&#12289;&#33258;&#36866;&#24212;&#27169;&#22411;&#65292;&#20943;&#23569;&#26631;&#27880;&#25968;&#25454;&#12290; &#26412;&#32508;&#36848;&#30740;&#31350;&#20102;&#26426;&#22120;&#23398;&#20064;&#22914;&#20309;&#22788;&#29702;HAR&#20013;&#30340;&#25968;&#25454;&#24322;&#36136;&#24615;&#65292;&#36890;&#36807;&#23545;&#25968;&#25454;&#24322;&#36136;&#24615;&#31867;&#22411;&#36827;&#34892;&#20998;&#31867;&#12289;&#24212;&#29992;&#30456;&#24212;&#30340;&#36866;&#24403;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#12289;&#24635;&#32467;&#21487;&#29992;&#25968;&#25454;&#38598;&#65292;&#24182;&#35752;&#35770;&#26410;&#26469;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15422v1 Announce Type: cross  Abstract: Sensor-based Human Activity Recognition (HAR) is crucial in ubiquitous computing, analysing behaviours through multi-dimensional observations. Despite research progress, HAR confronts challenges, particularly in data distribution assumptions. Most studies often assume uniform data distributions across datasets, contrasting with the varied nature of practical sensor data in human activities. Addressing data heterogeneity issues can improve performance, reduce computational costs, and aid in developing personalized, adaptive models with less annotated data. This review investigates how machine learning addresses data heterogeneity in HAR, by categorizing data heterogeneity types, applying corresponding suitable machine learning methods, summarizing available datasets, and discussing future challenges.
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20108;&#38454;&#20248;&#21270;&#20013;&#21152;&#36895;&#25910;&#25947;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#26377;&#38480;&#24046;&#20998;&#36924;&#36817;&#23545;&#35282;Hessian&#30697;&#38453;&#65292;&#24182;&#24212;&#29992;&#27169;&#31946;&#25512;&#29702;&#20110;&#22810;&#20010;&#36229;&#21442;&#25968;&#12290;</title><link>https://arxiv.org/abs/2403.15416</link><description>&lt;p&gt;
&#22312;&#20108;&#38454;&#20248;&#21270;&#20013;&#27169;&#31946;&#36229;&#21442;&#25968;&#26356;&#26032;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Fuzzy hyperparameters update in a second order optimization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15416
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;&#19968;&#31181;&#22312;&#20108;&#38454;&#20248;&#21270;&#20013;&#21152;&#36895;&#25910;&#25947;&#30340;&#28151;&#21512;&#26041;&#27861;&#65292;&#21033;&#29992;&#22312;&#32447;&#26377;&#38480;&#24046;&#20998;&#36924;&#36817;&#23545;&#35282;Hessian&#30697;&#38453;&#65292;&#24182;&#24212;&#29992;&#27169;&#31946;&#25512;&#29702;&#20110;&#22810;&#20010;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23558;&#25552;&#20986;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#20197;&#21152;&#36895;&#20108;&#38454;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#36895;&#24230;&#12290;&#23558;&#20171;&#32461;&#23545;&#35282;Hessian&#30697;&#38453;&#30340;&#22312;&#32447;&#26377;&#38480;&#24046;&#20998;&#36924;&#36817;&#65292;&#20197;&#21450;&#23545;&#20960;&#20010;&#36229;&#21442;&#25968;&#36827;&#34892;&#27169;&#31946;&#25512;&#29702;&#12290;&#24050;&#21462;&#24471;&#31454;&#20105;&#24615;&#25104;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15416v1 Announce Type: cross  Abstract: This research will present a hybrid approach to accelerate convergence in a second order optimization. An online finite difference approximation of the diagonal Hessian matrix will be introduced, along with fuzzy inferencing of several hyperparameters. Competitive results have been achieved
&lt;/p&gt;</description></item><item><title>&#35270;&#39057;&#28216;&#25103;&#20316;&#20026;&#30740;&#31350;&#39046;&#22495;&#30340;&#20652;&#21270;&#21058;&#24050;&#21462;&#24471;&#24456;&#22823;&#36827;&#23637;&#65292;&#26412;&#25991;&#23558;&#20998;&#26512;&#31070;&#32463;&#31185;&#23398;&#19982;&#28216;&#25103;&#30340;&#29616;&#29366;&#65292;&#24182;&#23637;&#26395;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2403.15413</link><description>&lt;p&gt;
&#29609;&#36716;&#31070;&#32463;&#31185;&#23398;&#65306;&#31070;&#32463;&#24433;&#20687;&#23398;&#19982;&#28216;&#25103;&#30340;&#36807;&#21435;&#12289;&#29616;&#22312;&#21644;&#26410;&#26469;
&lt;/p&gt;
&lt;p&gt;
Playing With Neuroscience: Past, Present and Future of Neuroimaging and Games
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15413
&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#28216;&#25103;&#20316;&#20026;&#30740;&#31350;&#39046;&#22495;&#30340;&#20652;&#21270;&#21058;&#24050;&#21462;&#24471;&#24456;&#22823;&#36827;&#23637;&#65292;&#26412;&#25991;&#23558;&#20998;&#26512;&#31070;&#32463;&#31185;&#23398;&#19982;&#28216;&#25103;&#30340;&#29616;&#29366;&#65292;&#24182;&#23637;&#26395;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#28216;&#25103;&#24050;&#32463;&#25104;&#20026;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#30340;&#36827;&#27493;&#20652;&#21270;&#21058;&#65292;&#20363;&#22914;&#20154;&#24037;&#26234;&#33021;&#12289;&#20154;&#26426;&#20132;&#20114;&#25110;&#34394;&#25311;&#29616;&#23454;&#12290;&#22810;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#31561;&#39046;&#22495;&#30340;&#30740;&#31350;&#24050;&#32463;&#20351;&#24471;&#35774;&#35745;&#20986;&#26032;&#22411;&#28216;&#25103;&#25104;&#20026;&#21487;&#33021;&#65292;&#32780;&#28216;&#25103;&#32463;&#24120;&#34987;&#29992;&#20316;&#27979;&#35797;&#21644;&#27169;&#25311;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;&#31070;&#32463;&#31185;&#23398;&#19982;&#28216;&#25103;&#30740;&#31350;&#20043;&#38388;&#30340;&#24403;&#21069;&#20851;&#31995;&#22914;&#20309;&#65311;&#26410;&#26469;&#25105;&#20204;&#21487;&#20197;&#26399;&#24453;&#20160;&#20040;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;&#35797;&#22270;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#20998;&#26512;&#31070;&#32463;&#31185;&#23398;&#19982;&#28216;&#25103;&#30340;&#20132;&#21449;&#39046;&#22495;&#30340;&#29616;&#29366;&#65292;&#24182;&#35774;&#24819;&#26410;&#26469;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15413v1 Announce Type: cross  Abstract: Videogames have been a catalyst for advances in many research fields, such as artificial intelligence, human-computer interaction or virtual reality. Over the years, research in fields such as artificial intelligence has enabled the design of new types of games, while games have often served as a powerful tool for testing and simulation. Can this also happen with neuroscience? What is the current relationship between neuroscience and games research? what can we expect from the future? In this article, we'll try to answer these questions, analysing the current state-of-the-art at the crossroads between neuroscience and games and envisioning future directions.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.15412</link><description>&lt;p&gt;
&#22312;LLMs&#20013;&#27979;&#37327;&#21644;&#24314;&#27169;&#8220;&#25991;&#21270;&#8221;&#65306;&#19968;&#39033;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Towards Measuring and Modeling "Culture" in LLMs: A Survey
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15412
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35843;&#26597;&#20102;39&#31687;&#26368;&#26032;&#35770;&#25991;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#65292;&#21457;&#29616;&#24403;&#21069;&#30740;&#31350;&#26410;&#23545;&#8220;&#25991;&#21270;&#8221;&#36827;&#34892;&#23450;&#20041;&#65292;&#32780;&#26159;&#22312;&#29305;&#23450;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#30740;&#31350;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#65292;&#30041;&#19979;&#35768;&#22810;&#26410;&#34987;&#25506;&#31350;&#30340;&#26377;&#36259;&#21644;&#37325;&#35201;&#26041;&#38754;&#65292;&#22914;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#21576;&#29616;&#20102;&#23545;39&#31687;&#26368;&#26032;&#35770;&#25991;&#30340;&#35843;&#26597;&#65292;&#26088;&#22312;&#30740;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#25991;&#21270;&#34920;&#36798;&#21644;&#21253;&#23481;&#24615;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#65292;&#27809;&#26377;&#19968;&#31687;&#30740;&#31350;&#23450;&#20041;&#8220;&#25991;&#21270;&#8221;&#65292;&#36825;&#26159;&#19968;&#20010;&#22797;&#26434;&#12289;&#22810;&#23618;&#38754;&#30340;&#27010;&#24565;&#65307;&#30456;&#21453;&#65292;&#23427;&#20204;&#22312;&#19968;&#20123;&#29305;&#21035;&#35774;&#35745;&#30340;&#25968;&#25454;&#38598;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#25506;&#31350;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#20195;&#34920;&#20102;&#26576;&#20123;&#8220;&#25991;&#21270;&#8221;&#30340;&#26041;&#38754;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#26041;&#38754;&#31216;&#20026;&#25991;&#21270;&#30340;&#20195;&#29702;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#32455;&#22312;&#20154;&#21475;&#32479;&#35745;&#12289;&#35821;&#20041;&#21644;&#35821;&#35328;&#25991;&#21270;&#20132;&#20114;&#20195;&#29702;&#30340;&#19977;&#20010;&#32500;&#24230;&#19978;&#12290;&#25105;&#20204;&#36824;&#23545;&#37319;&#29992;&#30340;&#25506;&#26597;&#26041;&#27861;&#36827;&#34892;&#20102;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#21482;&#26377;&#8220;&#25991;&#21270;&#8221;&#30340;&#26576;&#20123;&#26041;&#38754;&#65292;&#22914;&#20215;&#20540;&#35266;&#21644;&#30446;&#26631;&#65292;&#34987;&#30740;&#31350;&#20102;&#65292;&#30041;&#19979;&#20102;&#20960;&#20010;&#20854;&#20182;&#26377;&#36259;&#19988;&#37325;&#35201;&#30340;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22823;&#37327;&#35821;&#20041;&#39046;&#22495;&#21644;&#20851;&#20110;&#24615;&#65288;Hershcovich&#31561;&#20154;&#65292;2022&#65289;&#30340;&#26410;&#34987;&#25506;&#31350;&#12290;&#21478;&#22806;&#20004;&#20010;&#20851;&#38190;&#30340;&#31354;&#30333;&#26159;&#30446;&#21069;&#26041;&#27861;&#30340;&#40065;&#26834;&#24615;&#21644;&#24773;&#22659;&#24615;&#30340;&#32570;&#20047;&#12290;&#22522;&#20110;&#36825;&#20123;&#35266;&#23519;&#32467;&#26524;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15412v1 Announce Type: cross  Abstract: We present a survey of 39 recent papers that aim to study cultural representation and inclusion in large language models. We observe that none of the studies define "culture," which is a complex, multifaceted concept; instead, they probe the models on some specially designed datasets which represent certain aspects of "culture." We call these aspects the proxies of cultures, and organize them across three dimensions of demographic, semantic and linguistic-cultural interaction proxies. We also categorize the probing methods employed. Our analysis indicates that only certain aspects of "culture," such as values and objectives, have been studied, leaving several other interesting and important facets, especially the multitude of semantic domains (Thompson et al., 2020) and aboutness (Hershcovich et al., 2022), unexplored. Two other crucial gaps are the lack of robustness and situatedness of the current methods. Based on these observations
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#32467;&#21512;30&#31186;ECG&#35760;&#24405;&#21644;&#38271;&#26399;HRV&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#20272;&#31639;&#24515;&#21147;&#34928;&#31469;&#20303;&#38498;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#29983;&#23384;&#27169;&#22411;&#65306;XGBoost&#27169;&#22411;&#21644;ResNet&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2403.15408</link><description>&lt;p&gt;
&#22522;&#20110;&#30701;&#26102;ECG&#21644;&#37319;&#26679;&#38271;&#26399;HRV&#30340;&#22810;&#27169;&#24577;&#24515;&#21147;&#34928;&#31469;&#39118;&#38505;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Multi-modal Heart Failure Risk Estimation based on Short ECG and Sampled Long-Term HRV
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15408
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#32467;&#21512;30&#31186;ECG&#35760;&#24405;&#21644;&#38271;&#26399;HRV&#25968;&#25454;&#30340;&#22810;&#27169;&#24577;&#26041;&#27861;&#29992;&#20110;&#20272;&#31639;&#24515;&#21147;&#34928;&#31469;&#20303;&#38498;&#39118;&#38505;&#65292;&#24182;&#24341;&#20837;&#20102;&#20004;&#31181;&#29983;&#23384;&#27169;&#22411;&#65306;XGBoost&#27169;&#22411;&#21644;ResNet&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#34880;&#31649;&#30142;&#30149;&#65292;&#21253;&#25324;&#24515;&#21147;&#34928;&#31469;&#65288;HF&#65289;&#65292;&#20173;&#28982;&#26159;&#20840;&#29699;&#20027;&#35201;&#30340;&#27515;&#20129;&#21407;&#22240;&#65292;&#24120;&#24120;&#38590;&#20197;&#26089;&#26399;&#26816;&#27979;&#12290;&#22312;&#27492;&#32972;&#26223;&#19979;&#65292;&#21487;&#35775;&#38382;&#21644;&#26377;&#25928;&#30340;&#39118;&#38505;&#35780;&#20272;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#12290;&#20256;&#32479;&#26041;&#27861;&#20381;&#36182;&#20110;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#35786;&#26029;&#27979;&#35797;&#65292;&#36890;&#24120;&#22312;&#30151;&#29366;&#21457;&#20316;&#21518;&#36827;&#34892;&#12290;&#24515;&#30005;&#22270;&#65288;ECG&#65289;&#25216;&#26415;&#30340;&#24191;&#27867;&#21487;&#29992;&#24615;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#21147;&#37327;&#27491;&#25104;&#20026;&#26234;&#33021;&#21307;&#30103;&#39046;&#22495;&#30340;&#21487;&#34892;&#26367;&#20195;&#26041;&#26696;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#22810;&#27169;&#24577;&#26041;&#27861;&#65292;&#32467;&#21512;30&#31186;ECG&#35760;&#24405;&#21644;&#36817;&#20284;&#30340;&#38271;&#26399;&#24515;&#29575;&#21464;&#24322;&#24615;&#65288;HRV&#65289;&#25968;&#25454;&#65292;&#26469;&#20272;&#31639;HF&#20303;&#38498;&#39118;&#38505;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#20004;&#31181;&#29983;&#23384;&#27169;&#22411;&#65306;&#19968;&#20010;XGBoost&#27169;&#22411;&#65292;&#29992;&#20110;&#21152;&#36895;&#22833;&#36133;&#26102;&#38388;&#65288;AFT&#65289;&#65292;&#32467;&#21512;&#20102;&#20840;&#38754;&#30340;ECG&#29305;&#24449;&#65307;&#20197;&#21450;&#19968;&#20010;&#20174;&#21407;&#22987;ECG&#20013;&#23398;&#20064;&#30340;ResNet&#27169;&#22411;&#12290;&#25105;&#20204;&#36890;&#36807;&#25105;&#20204;&#20174;&#36229;&#32423;&#38271;&#26399;HRV&#20013;&#25552;&#21462;&#30340;&#26032;&#39062;&#38271;&#26399;HRVs&#26469;&#25193;&#23637;&#36825;&#20123;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15408v1 Announce Type: cross  Abstract: Cardiovascular diseases, including Heart Failure (HF), remain a leading global cause of mortality, often evading early detection. In this context, accessible and effective risk assessment is indispensable. Traditional approaches rely on resource-intensive diagnostic tests, typically administered after the onset of symptoms. The widespread availability of electrocardiogram (ECG) technology and the power of Machine Learning are emerging as viable alternatives within smart healthcare. In this paper, we propose several multi-modal approaches that combine 30-second ECG recordings and approximate long-term Heart Rate Variability (HRV) data to estimate the risk of HF hospitalization. We introduce two survival models: an XGBoost model with Accelerated Failure Time (AFT) incorporating comprehensive ECG features and a ResNet model that learns from the raw ECG. We extend these with our novel long-term HRVs extracted from the combination of ultra-
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;X-AMR&#27880;&#37322;&#24037;&#20855;&#65292;&#36890;&#36807;&#26426;&#22120;&#36741;&#21161;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#65292;&#23454;&#29616;&#20102;&#23545;&#20851;&#38190;&#20107;&#20214;&#35821;&#20041;&#30340;&#27880;&#37322;&#65292;&#19982;GPT-4&#38598;&#25104;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2403.15407</link><description>&lt;p&gt;
X-AMR&#27880;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
X-AMR Annotation Tool
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15407
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;X-AMR&#27880;&#37322;&#24037;&#20855;&#65292;&#36890;&#36807;&#26426;&#22120;&#36741;&#21161;&#25552;&#21319;&#29992;&#25143;&#20307;&#39564;&#65292;&#23454;&#29616;&#20102;&#23545;&#20851;&#38190;&#20107;&#20214;&#35821;&#20041;&#30340;&#27880;&#37322;&#65292;&#19982;GPT-4&#38598;&#25104;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#22411;&#30340;&#36328;&#25991;&#26723;&#25277;&#35937;&#24847;&#20041;&#34920;&#24449;&#65288;X-AMR&#65289;&#27880;&#37322;&#24037;&#20855;&#65292;&#26088;&#22312;&#29992;&#20110;&#27880;&#37322;&#20851;&#38190;&#30340;&#35821;&#26009;&#24211;&#32423;&#20107;&#20214;&#35821;&#20041;&#12290;&#36890;&#36807;Prodigy&#27880;&#37322;&#24037;&#20855;&#25552;&#20379;&#30340;&#26426;&#22120;&#36741;&#21161;&#65292;&#25105;&#20204;&#22686;&#24378;&#20102;&#29992;&#25143;&#20307;&#39564;&#65292;&#30830;&#20445;&#27880;&#37322;&#36807;&#31243;&#30340;&#31616;&#26131;&#24615;&#21644;&#39640;&#25928;&#24615;&#12290;&#36890;&#36807;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#24037;&#20855;&#22312;&#22686;&#24378;&#29616;&#26377;&#20107;&#20214;&#35821;&#26009;&#24211;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#65292;&#31361;&#26174;&#20102;&#20854;&#19982;GPT-4&#38598;&#25104;&#26102;&#30340;&#20248;&#21183;&#12290;&#20195;&#30721;&#21644;&#27880;&#37322;&#65306;https://github.com/ahmeshaf/gpt_coref
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15407v1 Announce Type: cross  Abstract: This paper presents a novel Cross-document Abstract Meaning Representation (X-AMR) annotation tool designed for annotating key corpus-level event semantics. Leveraging machine assistance through the Prodigy Annotation Tool, we enhance the user experience, ensuring ease and efficiency in the annotation process. Through empirical analyses, we demonstrate the effectiveness of our tool in augmenting an existing event corpus, highlighting its advantages when integrated with GPT-4. Code and annotations: https://github.com/ahmeshaf/gpt_coref
&lt;/p&gt;</description></item><item><title>SIAs&#26159;&#19968;&#31181;&#27835;&#29702;&#26426;&#21046;&#65292;&#21487;&#20197;&#24110;&#21161;AI&#39033;&#30446;&#22242;&#38431;&#25345;&#32493;&#20851;&#27880;&#39033;&#30446;&#30340;&#28508;&#22312;&#23454;&#38469;&#24433;&#21709;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;AI&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;</title><link>https://arxiv.org/abs/2403.15404</link><description>&lt;p&gt;
&#23454;&#36341;&#20013;&#30340;AI&#21487;&#25345;&#32493;&#24615;&#31532;&#20108;&#37096;&#20998;&#65306;AI&#24037;&#20316;&#27969;&#31243;&#20013;&#30340;&#21487;&#25345;&#32493;&#24615;
&lt;/p&gt;
&lt;p&gt;
AI Sustainability in Practice Part Two: Sustainability Throughout the AI Workflow
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15404
&lt;/p&gt;
&lt;p&gt;
SIAs&#26159;&#19968;&#31181;&#27835;&#29702;&#26426;&#21046;&#65292;&#21487;&#20197;&#24110;&#21161;AI&#39033;&#30446;&#22242;&#38431;&#25345;&#32493;&#20851;&#27880;&#39033;&#30446;&#30340;&#28508;&#22312;&#23454;&#38469;&#24433;&#21709;&#65292;&#36825;&#23545;&#20110;&#23454;&#29616;AI&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#24615;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#30340;&#21487;&#25345;&#32493;&#24615;&#21462;&#20915;&#20110;&#39033;&#30446;&#22242;&#38431;&#33021;&#21542;&#25345;&#32493;&#20851;&#27880;&#20854;&#28508;&#22312;&#30340;&#29616;&#23454;&#24433;&#21709;&#21644;&#36716;&#22411;&#25928;&#24212;&#12290;&#21033;&#30410;&#30456;&#20851;&#32773;&#24433;&#21709;&#35780;&#20272;&#65288;SIAs&#65289;&#26159;&#19968;&#31181;&#27835;&#29702;&#26426;&#21046;&#65292;&#20351;&#22242;&#38431;&#33021;&#22815;&#23545;&#36825;&#31181;&#21709;&#24212;&#33021;&#21147;&#36827;&#34892;&#25345;&#32493;&#24615;&#35780;&#20272;&#12290;&#23427;&#20204;&#26159;&#19968;&#31181;&#21019;&#24314;&#31243;&#24207;&#30340;&#24037;&#20855;&#65292;&#29992;&#26469;&#35760;&#24405;&#21327;&#20316;&#35780;&#20272;&#21644;&#21453;&#24605;&#26410;&#26469;&#21487;&#33021;&#30340;AI&#21019;&#26032;&#39033;&#30446;&#30340;&#21361;&#23475;&#21644;&#22909;&#22788;&#12290;SIAs&#19981;&#26159;&#19968;&#27425;&#24615;&#30340;&#27835;&#29702;&#34892;&#21160;&#12290;&#23427;&#20204;&#35201;&#27714;&#39033;&#30446;&#22242;&#38431;&#25345;&#32493;&#20851;&#27880;AI&#29983;&#20135;&#21644;&#20351;&#29992;&#30340;&#21160;&#24577;&#21644;&#21464;&#21270;&#24615;&#36136;&#65292;&#20197;&#21450;AI&#25216;&#26415;&#23884;&#20837;&#30340;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#30340;&#21464;&#21270;&#26465;&#20214;&#12290;&#36825;&#26412;&#24037;&#20316;&#25163;&#20876;&#26159;&#20851;&#20110;AI&#21487;&#25345;&#32493;&#24615;&#30340;&#20004;&#26412;&#24037;&#20316;&#25163;&#20876;&#20013;&#30340;&#31532;&#20108;&#26412;&#12290;&#23427;&#25552;&#20379;&#20102;SIAs&#30340;&#27169;&#26495;&#21644;&#20801;&#35768;&#28145;&#20837;&#30740;&#31350;&#20854;&#20013;&#20851;&#38190;&#37096;&#20998;&#30340;&#27963;&#21160;&#12290;&#23427;&#35752;&#35770;&#20102;&#26435;&#34913;&#20215;&#20540;&#21644;&#21327;&#21516;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15404v1 Announce Type: cross  Abstract: The sustainability of AI systems depends on the capacity of project teams to proceed with a continuous sensitivity to their potential real-world impacts and transformative effects. Stakeholder Impact Assessments (SIAs) are governance mechanisms that enable this kind of responsiveness. They are tools that create a procedure for, and a means of documenting, the collaborative evaluation and reflective anticipation of the possible harms and benefits of AI innovation projects. SIAs are not one-off governance actions. They require project teams to pay continuous attention to the dynamic and changing character of AI production and use and to the shifting conditions of the real-world environments in which AI technologies are embedded. This workbook is part two of two workbooks on AI Sustainability. It provides a template of the SIA and activities that allow a deeper dive into crucial parts of it. It discusses methods for weighing values and co
&lt;/p&gt;</description></item><item><title>&#20171;&#32461;&#20102;PBG&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#23618;&#27425;&#30340;&#27835;&#29702;&#27169;&#22411;&#65292;&#24110;&#21161;&#39033;&#30446;&#22242;&#38431;&#23558;&#20262;&#29702;&#20215;&#20540;&#35266;&#21644;&#23454;&#36341;&#21407;&#21017;&#34701;&#20837;&#21019;&#26032;&#23454;&#36341;&#20013;&#65292;&#23637;&#31034;&#21644;&#35760;&#24405;&#36825;&#20123;&#20215;&#20540;&#30340;&#28165;&#26224;&#26426;&#21046;&#12290;</title><link>https://arxiv.org/abs/2403.15403</link><description>&lt;p&gt;
AI&#20262;&#29702;&#19982;&#27835;&#29702;&#23454;&#36341;&#65306;&#23548;&#35770;
&lt;/p&gt;
&lt;p&gt;
AI Ethics and Governance in Practice: An Introduction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15403
&lt;/p&gt;
&lt;p&gt;
&#20171;&#32461;&#20102;PBG&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#23618;&#27425;&#30340;&#27835;&#29702;&#27169;&#22411;&#65292;&#24110;&#21161;&#39033;&#30446;&#22242;&#38431;&#23558;&#20262;&#29702;&#20215;&#20540;&#35266;&#21644;&#23454;&#36341;&#21407;&#21017;&#34701;&#20837;&#21019;&#26032;&#23454;&#36341;&#20013;&#65292;&#23637;&#31034;&#21644;&#35760;&#24405;&#36825;&#20123;&#20215;&#20540;&#30340;&#28165;&#26224;&#26426;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#31995;&#32479;&#21487;&#33021;&#23545;&#20010;&#20154;&#21644;&#31038;&#20250;&#20135;&#29983;&#28145;&#36828;&#21644;&#38271;&#26399;&#24433;&#21709;&#12290;&#20026;&#20102;&#36127;&#36131;&#22320;&#31649;&#29702;&#36825;&#20123;&#24433;&#21709;&#65292;&#24182;&#23558;AI&#31995;&#32479;&#30340;&#21457;&#23637;&#24341;&#21521;&#26368;&#20339;&#30340;&#20844;&#20849;&#21033;&#30410;&#65292;&#24517;&#39035;&#23558;AI&#20262;&#29702;&#21644;&#27835;&#29702;&#32771;&#34385;&#21015;&#20026;&#39318;&#35201;&#20219;&#21153;&#12290;&#22312;&#27492;&#24037;&#20316;&#25163;&#20876;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#24182;&#25551;&#36848;&#20102;&#25105;&#20204;&#30340;PBG&#26694;&#26550;&#65292;&#36825;&#26159;&#19968;&#20010;&#22810;&#23618;&#27425;&#30340;&#27835;&#29702;&#27169;&#22411;&#65292;&#20351;&#39033;&#30446;&#22242;&#38431;&#33021;&#22815;&#23558;&#20262;&#29702;&#20215;&#20540;&#35266;&#21644;&#23454;&#36341;&#21407;&#21017;&#34701;&#20837;&#20854;&#21019;&#26032;&#23454;&#36341;&#20013;&#65292;&#24182;&#20855;&#22791;&#28165;&#26224;&#30340;&#26426;&#21046;&#26469;&#23637;&#31034;&#21644;&#35760;&#24405;&#36825;&#19968;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15403v1 Announce Type: cross  Abstract: AI systems may have transformative and long-term effects on individuals and society. To manage these impacts responsibly and direct the development of AI systems toward optimal public benefit, considerations of AI ethics and governance must be a first priority.   In this workbook, we introduce and describe our PBG Framework, a multi-tiered governance model that enables project teams to integrate ethical values and practical principles into their innovation practices and to have clear mechanisms for demonstrating and documenting this.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20215;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#20854;&#22312;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#21644;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;</title><link>https://arxiv.org/abs/2403.15401</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#31995;&#32479;&#35780;&#20215;
&lt;/p&gt;
&lt;p&gt;
Large Language Model for Mental Health: A Systematic Review
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15401
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#31995;&#32479;&#35780;&#20215;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#35752;&#35770;&#20102;&#20854;&#22312;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#21644;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#20013;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#25968;&#23383;&#20581;&#24247;&#39046;&#22495;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#65292;&#23637;&#29616;&#20986;&#20102;&#28508;&#22312;&#30340;&#24212;&#29992;&#24615;&#65292;&#20294;&#23427;&#20204;&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#20173;&#22312;&#25345;&#32493;&#35752;&#35770;&#20013;&#12290;&#36825;&#39033;&#31995;&#32479;&#24615;&#35780;&#20215;&#26088;&#22312;&#24635;&#32467;&#21644;&#34920;&#24449;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#30340;&#24212;&#29992;&#65292;&#36890;&#36807;&#35843;&#26597;LLMs&#26368;&#26032;&#30740;&#31350;&#30340;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#65292;&#35752;&#35770;&#24515;&#29702;&#20581;&#24247;&#39046;&#22495;&#26089;&#26399;&#31579;&#26597;&#12289;&#25968;&#23383;&#24178;&#39044;&#20197;&#21450;&#20854;&#20182;&#20020;&#24202;&#24212;&#29992;&#30340;&#25361;&#25112;&#21644;&#26426;&#36935;&#12290;&#26681;&#25454;PRISMA&#25351;&#21335;&#65292;&#25105;&#20204;&#23457;&#26597;&#20102;PubMed&#12289;DBLP&#35745;&#31639;&#26426;&#31185;&#23398;&#25991;&#29486;&#25968;&#25454;&#24211;&#21644;IEEE Xplore&#19978;&#21457;&#34920;&#30340;&#33521;&#25991;&#25991;&#31456;&#65292;&#26102;&#38388;&#36328;&#24230;&#20026;2017&#24180;1&#26376;1&#26085;&#33267;2023&#24180;9&#26376;1&#26085;&#65292;&#37325;&#28857;&#20851;&#27880;&#24515;&#29702;&#20581;&#24247;&#21644;LLMs&#12290;&#35813;&#32508;&#36848;&#20998;&#26512;&#20102;32&#31687;&#25991;&#31456;&#65292;&#21253;&#25324;&#20351;&#29992;&#31038;&#20132;&#23186;&#20307;&#25968;&#25454;&#38598;&#36827;&#34892;&#24515;&#29702;&#20581;&#24247;&#20998;&#26512;&#30340;&#65288;n=13&#65289;&#12289;&#24515;&#29702;&#20581;&#24247;&#32842;&#22825;&#26426;&#22120;&#20154;&#65288;n=10&#65289;&#20197;&#21450;&#20854;&#20182;&#24515;&#29702;&#20581;&#24247;&#24212;&#29992;&#65288;n=9&#65289;&#12290;&#30740;&#31350;&#32467;&#26524;&#26174;&#31034;LLMs&#22312;&#24515;&#29702;&#20581;&#24247;&#38382;&#39064;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#20197;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15401v1 Announce Type: cross  Abstract: Large language models (LLMs) have received much attention and shown their potential in digital health, while their application in mental health is subject to ongoing debate. This systematic review aims to summarize and characterize the use of LLMs in mental health by investigating the strengths and limitations of the latest work in LLMs and discusses the challenges and opportunities for early screening, digital interventions, and other clinical applications in mental health. Following PRISMA guidelines, we examined English articles from PubMed, DBLP Computer Science Bibliography, and IEEE Xplore, published between 1 January 2017, and 1 September 2023, focusing on mental health and LLMs. The review analyzed 32 articles, including mental health analysis using social media datasets (n=13), mental health chatbots (n=10), and other mental health applications (n=9). Findings reveal LLMs' effectiveness in mental health issue detection and the
&lt;/p&gt;</description></item><item><title>&#22278;&#26700;&#20250;&#35758;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#27861;&#24459;&#21644;&#25919;&#31574;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#24102;&#26469;&#30340;&#30495;&#23454;&#24615;&#12289;&#38544;&#31169;&#21644;&#24066;&#22330;&#38598;&#20013;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#31038;&#20250;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2403.15397</link><description>&lt;p&gt;
&#31649;&#25511;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65306;&#22278;&#26700;&#25253;&#21578;
&lt;/p&gt;
&lt;p&gt;
Regulating Large Language Models: A Roundtable Report
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15397
&lt;/p&gt;
&lt;p&gt;
&#22278;&#26700;&#20250;&#35758;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#27861;&#24459;&#21644;&#25919;&#31574;&#26469;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21487;&#33021;&#24102;&#26469;&#30340;&#30495;&#23454;&#24615;&#12289;&#38544;&#31169;&#21644;&#24066;&#22330;&#38598;&#20013;&#31561;&#26041;&#38754;&#30340;&#37325;&#35201;&#31038;&#20250;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;2023&#24180;7&#26376;20&#26085;&#65292;&#19968;&#32676;&#20855;&#26377;&#27861;&#24459;&#12289;&#35745;&#31639;&#26426;&#31185;&#23398;&#12289;&#25919;&#27835;&#31185;&#23398;&#31561;&#19987;&#19994;&#30693;&#35782;&#30340;27&#21517;&#23398;&#32773;&#21644;&#25968;&#23383;&#26435;&#21033;&#20513;&#23548;&#32773;&#32858;&#38598;&#22312;&#32445;&#32422;&#22823;&#23398;&#27861;&#23398;&#38498;&#20449;&#24687;&#27861;&#24459;&#30740;&#31350;&#25152;&#21644;&#27665;&#20027;&#19982;&#31185;&#25216;&#20013;&#24515;&#32852;&#21512;&#20030;&#21150;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;&#27861;&#24459;&#21644;&#25919;&#31574;&#22278;&#26700;&#20250;&#35758;&#19978;&#12290;&#22278;&#26700;&#20250;&#35758;&#26088;&#22312;&#35752;&#35770;&#27861;&#24459;&#21644;&#25919;&#31574;&#22914;&#20309;&#24110;&#21161;&#35299;&#20915;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#24102;&#26469;&#30340;&#19968;&#20123;&#36739;&#22823;&#31038;&#20250;&#38382;&#39064;&#12290;&#35752;&#35770;&#20027;&#35201;&#38598;&#20013;&#22312;&#19977;&#20010;&#25919;&#31574;&#39046;&#22495;&#65306;1.&#30495;&#23454;&#24615;&#65306;LLMs&#22312;&#29983;&#25104;&#35823;&#20449;&#24687;&#21644;&#20551;&#20449;&#24687;&#26041;&#38754;&#23384;&#22312;&#21738;&#20123;&#39118;&#38505;&#65311;&#20174;&#25216;&#26415;&#21644;/&#25110;&#30417;&#31649;&#30340;&#35282;&#24230;&#22914;&#20309;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#65311;2.&#38544;&#31169;&#65306;&#22312;&#21019;&#24314;&#12289;&#37096;&#32626;&#21644;&#20351;&#29992;LLMs&#36807;&#31243;&#20013;&#28041;&#21450;&#21738;&#20123;&#26368;&#22823;&#30340;&#38544;&#31169;&#39118;&#38505;&#65311;&#22914;&#20309;&#20174;&#25216;&#26415;&#21644;/&#25110;&#30417;&#31649;&#30340;&#35282;&#24230;&#20943;&#36731;&#36825;&#20123;&#39118;&#38505;&#65311;3.&#24066;&#22330;&#38598;&#20013;&#65306;LLMs&#24102;&#26469;&#20102;&#21738;&#20123;&#24066;&#22330;&#38598;&#20013;&#30340;&#23041;&#32961;&#65311;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15397v1 Announce Type: cross  Abstract: On July 20, 2023, a group of 27 scholars and digital rights advocates with expertise in law, computer science, political science, and other disciplines gathered for the Large Language Models, Law and Policy Roundtable, co-hosted by the NYU School of Law's Information Law Institute and the Center for Democracy &amp; Technology. The roundtable convened to discuss how law and policy can help address some of the larger societal problems posed by large language models (LLMs). The discussion focused on three policy topic areas in particular:   1. Truthfulness: What risks do LLMs pose in terms of generating mis- and disinformation? How can these risks be mitigated from a technical and/or regulatory perspective?   2. Privacy: What are the biggest privacy risks involved in the creation, deployment, and use of LLMs? How can these risks be mitigated from a technical and/or regulatory perspective?   3. Market concentration: What threats do LLMs pose c
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20004;&#27493;&#27861;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#20102;&#21313;&#21517;&#22312;&#32447;&#21644;&#36828;&#31243;&#23398;&#20064;&#23398;&#29983;&#23545;&#34394;&#25311;AI&#25968;&#23383;&#21161;&#25163;&#35774;&#35745;&#30340;&#30475;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#35748;&#20026;&#36825;&#26679;&#30340;AI&#24037;&#20855;&#22312;&#23398;&#20064;&#26102;&#26159;&#26377;&#29992;&#30340;&#12290;</title><link>https://arxiv.org/abs/2403.15396</link><description>&lt;p&gt;
&#25105;&#24076;&#26395;&#36825;&#26159;&#19968;&#20010;&#21161;&#25163;&#65292;&#32780;&#19981;&#26159;&#32769;&#24072;&#65306;&#36828;&#31243;&#23398;&#20064;&#23398;&#29983;&#24076;&#26395;&#20174;&#20154;&#24037;&#26234;&#33021;&#25968;&#23383;&#21161;&#25163;&#37027;&#37324;&#24471;&#21040;&#30340;&#23458;&#25143;&#35266;&#28857;
&lt;/p&gt;
&lt;p&gt;
I would love this to be like an assistant, not the teacher: a voice of the customer perspective of what distance learning students want from an Artificial Intelligence Digital Assistant
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15396
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20004;&#27493;&#27861;&#28151;&#21512;&#26041;&#27861;&#30740;&#31350;&#20102;&#21313;&#21517;&#22312;&#32447;&#21644;&#36828;&#31243;&#23398;&#20064;&#23398;&#29983;&#23545;&#34394;&#25311;AI&#25968;&#23383;&#21161;&#25163;&#35774;&#35745;&#30340;&#30475;&#27861;&#65292;&#32467;&#26524;&#26174;&#31034;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#35748;&#20026;&#36825;&#26679;&#30340;AI&#24037;&#20855;&#22312;&#23398;&#20064;&#26102;&#26159;&#26377;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;ChatGPT&#31561;&#29983;&#25104;&#24335;AI&#31995;&#32479;&#30340;&#21457;&#24067;&#65292;&#20154;&#20204;&#23545;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30340;&#20852;&#36259;&#36880;&#28176;&#22686;&#21152;&#65292;&#22312;&#19981;&#21516;&#39046;&#22495;&#20013;&#21253;&#25324;&#39640;&#31561;&#25945;&#32946;&#12290;&#23613;&#31649;&#26032;&#20852;&#32479;&#35745;&#25968;&#25454;&#26174;&#31034;&#26412;&#31185;&#29983;&#20013;&#20351;&#29992;AI&#30340;&#26222;&#21450;&#24230;&#65292;&#20294;&#23545;&#23398;&#29983;&#23545;AI&#30340;&#30475;&#27861;&#65292;&#21253;&#25324;&#20182;&#20204;&#22312;&#23454;&#38469;&#20351;&#29992;&#20013;&#33258;&#25253;&#30340;&#22909;&#22788;&#21644;&#20851;&#27880;&#28857;&#65292;&#29305;&#21035;&#26159;&#22312;&#36828;&#31243;&#23398;&#20064;&#32972;&#26223;&#19979;&#65292;&#30446;&#21069;&#20026;&#27490;&#36824;&#30693;&#20043;&#29978;&#23569;&#12290;&#26412;&#30740;&#31350;&#37319;&#29992;&#20004;&#27493;&#27861;&#28151;&#21512;&#26041;&#27861;&#65292;&#20998;&#26512;&#20102;&#26469;&#33258;&#19981;&#21516;&#23398;&#31185;&#30340;&#21313;&#21517;&#22312;&#32447;&#21644;&#36828;&#31243;&#23398;&#20064;&#23398;&#29983;&#23545;&#34394;&#25311;AI&#25968;&#23383;&#21161;&#25163;&#65288;AIDA&#65289;&#35774;&#35745;&#30340;&#30475;&#27861;&#12290;&#22312;&#31532;&#19968;&#27493;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#37319;&#35775;&#25429;&#25417;&#20102;&#23398;&#29983;&#30340;&#30475;&#27861;&#65292;&#32780;&#31532;&#20108;&#27493;&#36890;&#36807;&#25903;&#25345;&#23398;&#29983;&#19982;&#21516;&#34892;&#20998;&#20139;&#12289;&#27604;&#36739;&#21644;&#23545;&#27604;&#30475;&#27861;&#30340;&#25968;&#25454;&#19977;&#35282;&#21270;&#12290;&#25152;&#26377;&#21442;&#19982;&#32773;&#37117;&#21516;&#24847;&#22312;&#23398;&#20064;&#26102;&#20351;&#29992;&#36825;&#26679;&#30340;AI&#24037;&#20855;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15396v1 Announce Type: cross  Abstract: With the release of Generative AI systems such as ChatGPT, an increasing interest in using Artificial Intelligence (AI) has been observed across domains, including higher education. While emerging statistics show the popularity of using AI amongst undergraduate students, little is yet known about students' perceptions regarding AI including self-reported benefits and concerns from their actual usage, in particular in distance learning contexts. Using a two-step, mixed-methods approach, we examined the perceptions of ten online and distance learning students from diverse disciplines regarding the design of a hypothetical AI Digital Assistant (AIDA). In the first step, we captured students' perceptions via interviews, while the second step supported the triangulation of data by enabling students to share, compare, and contrast perceptions with those of peers. All participants agreed on the usefulness of such an AI tool while studying and
&lt;/p&gt;</description></item><item><title>PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.15388</link><description>&lt;p&gt;
LLaVA-PruMerge: &#33258;&#36866;&#24212;&#20196;&#29260;&#20943;&#23569;&#29992;&#20110;&#39640;&#25928;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15388
&lt;/p&gt;
&lt;p&gt;
PruMerge&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#30340;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#21487;&#20197;&#26377;&#25928;&#20943;&#23569;&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;&#20013;&#30340;&#35270;&#35273;&#20196;&#29260;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#27169;&#22411;(LMMs)&#36890;&#36807;&#36830;&#25509;&#35270;&#35273;&#32534;&#30721;&#22120;&#21644;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#23637;&#29616;&#20102;&#26174;&#33879;&#30340;&#25512;&#29702;&#33021;&#21147;&#12290;&#26368;&#36817;&#30340;LMMs&#21253;&#25324;&#20102;&#26356;&#22797;&#26434;&#30340;&#35270;&#35273;&#36755;&#20837;&#65292;&#22914;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#21644;&#35270;&#39057;&#65292;&#36825;&#26174;&#33879;&#22686;&#21152;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#20196;&#29260;&#20943;&#23569;&#26426;&#21046;&#65292;&#24182;&#21457;&#29616;&#31867;&#20284;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#35768;&#22810;&#35270;&#35273;&#20196;&#29260;&#22312;&#31354;&#38388;&#19978;&#26159;&#20887;&#20313;&#30340;&#12290;&#22522;&#20110;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PruMerge&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#33258;&#36866;&#24212;&#35270;&#35273;&#20196;&#29260;&#20943;&#23569;&#26041;&#27861;&#65292;&#22823;&#22823;&#20943;&#23569;&#20102;&#35270;&#35273;&#20196;&#29260;&#30340;&#25968;&#37327;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#21487;&#27604;&#30340;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15388v1 Announce Type: cross  Abstract: Large Multimodal Models (LMMs) have shown significant reasoning capabilities by connecting a visual encoder and a large language model. LMMs typically use a fixed amount of visual tokens, such as the penultimate layer features in the CLIP visual encoder, as the prefix content. Recent LMMs incorporate more complex visual inputs, such as high-resolution images and videos, which increase the number of visual tokens significantly. However, due to the design of the Transformer architecture, computational costs associated with these models tend to increase quadratically with the number of input tokens. To tackle this problem, we explore a token reduction mechanism and find, similar to prior work, that many visual tokens are spatially redundant. Based on this, we propose PruMerge, a novel adaptive visual token reduction approach, which largely reduces the number of visual tokens while maintaining comparable model performance. We first select 
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;Point-DETR3D&#65292;&#19968;&#20010;&#24072;&#29983;&#26694;&#26550;&#29992;&#20110;&#24369;&#30417;&#30563;&#21322;&#30417;&#30563;3D&#26816;&#27979;&#65292;&#20805;&#20998;&#21033;&#29992;&#28857;&#32423;&#30417;&#30563;&#20248;&#21183;&#65292;&#20811;&#26381;&#20102;&#23558;&#24369;&#30417;&#30563;3D&#20808;&#39564;&#20449;&#24687;&#32534;&#30721;&#21040;&#27169;&#22411;&#20013;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.15317</link><description>&lt;p&gt;
Point-DETR3D&#65306;&#21033;&#29992;&#31354;&#38388;&#28857;&#20808;&#39564;&#20449;&#24687;&#22686;&#24378;&#22270;&#20687;&#25968;&#25454;&#30340;&#24369;&#30417;&#30563;&#21322;&#30417;&#30563;3D&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Point-DETR3D: Leveraging Imagery Data with Spatial Point Prior for Weakly Semi-supervised 3D Object Detection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15317
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;Point-DETR3D&#65292;&#19968;&#20010;&#24072;&#29983;&#26694;&#26550;&#29992;&#20110;&#24369;&#30417;&#30563;&#21322;&#30417;&#30563;3D&#26816;&#27979;&#65292;&#20805;&#20998;&#21033;&#29992;&#28857;&#32423;&#30417;&#30563;&#20248;&#21183;&#65292;&#20811;&#26381;&#20102;&#23558;&#24369;&#30417;&#30563;3D&#20808;&#39564;&#20449;&#24687;&#32534;&#30721;&#21040;&#27169;&#22411;&#20013;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35757;&#32451;&#39640;&#31934;&#24230;&#30340;3D&#26816;&#27979;&#22120;&#38656;&#35201;&#22823;&#37327;&#24102;&#26377;7&#20010;&#33258;&#30001;&#24230;&#30340;&#26631;&#35760;3D&#27880;&#37322;&#65292;&#36825;&#26159;&#36153;&#26102;&#36153;&#21147;&#30340;&#12290;&#22240;&#27492;&#65292;&#25552;&#20986;&#20102;&#28857;&#27880;&#37322;&#30340;&#24418;&#24335;&#65292;&#20026;3D&#26816;&#27979;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25552;&#20379;&#20102;&#37325;&#35201;&#21069;&#26223;&#65292;&#19981;&#20165;&#26356;&#26131;&#33719;&#24471;&#19988;&#25104;&#26412;&#26356;&#20302;&#24265;&#65292;&#32780;&#19988;&#20026;&#30446;&#26631;&#23450;&#20301;&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32463;&#39564;&#24615;&#22320;&#21457;&#29616;&#65292;&#20165;&#20165;&#23558;Point-DETR&#25913;&#32534;&#20026;&#20854;3D&#24418;&#24335;&#24182;&#19981;&#31616;&#21333;&#65292;&#36935;&#21040;&#20102;&#20004;&#20010;&#20027;&#35201;&#29942;&#39048;&#65306;1&#65289;&#26080;&#27861;&#23558;&#24378;&#22823;&#30340;3D&#20808;&#39564;&#20449;&#24687;&#32534;&#30721;&#21040;&#27169;&#22411;&#20013;&#65292;2&#65289;&#30001;&#20110;&#28608;&#20809;&#38647;&#36798;&#28857;&#30340;&#26497;&#24230;&#31232;&#30095;&#24615;&#65292;&#22312;&#36828;&#36317;&#31163;&#21306;&#22495;&#29983;&#25104;&#36136;&#37327;&#20302;&#19979;&#30340;&#20266;&#26631;&#31614;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Point-DETR3D&#65292;&#19968;&#20010;&#29992;&#20110;&#24369;&#30417;&#30563;&#21322;&#30417;&#30563;3D&#26816;&#27979;&#30340;&#24072;&#29983;&#26694;&#26550;&#65292;&#26088;&#22312;&#20805;&#20998;&#21033;&#29992;&#22312;&#21463;&#38480;&#30340;&#23454;&#20363;&#32423;&#27880;&#37322;&#39044;&#31639;&#20869;&#30340;&#28857;&#32423;&#30417;&#30563;&#12290;&#19982;P&#19981;&#21516;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15317v1 Announce Type: cross  Abstract: Training high-accuracy 3D detectors necessitates massive labeled 3D annotations with 7 degree-of-freedom, which is laborious and time-consuming. Therefore, the form of point annotations is proposed to offer significant prospects for practical applications in 3D detection, which is not only more accessible and less expensive but also provides strong spatial information for object localization.In this paper, we empirically discover that it is non-trivial to merely adapt Point-DETR to its 3D form, encountering two main bottlenecks: 1) it fails to encode strong 3D prior into the model, and 2) it generates low-quality pseudo labels in distant regions due to the extreme sparsity of LiDAR points. To overcome these challenges, we introduce Point-DETR3D, a teacher-student framework for weakly semi-supervised 3D detection, designed to fully capitalize on point-wise supervision within a constrained instance-wise annotation budget.Different from P
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#30001;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#20013;&#35270;&#35273;&#24187;&#35273;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35270;&#35273;&#24187;&#35273;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;</title><link>https://arxiv.org/abs/2403.15048</link><description>&lt;p&gt;
&#21345;&#36890;&#24187;&#35273;&#26816;&#27979;: &#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Cartoon Hallucinations Detection: Pose-aware In Context Visual Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.15048
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#26816;&#27979;&#30001;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#20013;&#35270;&#35273;&#24187;&#35273;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#32467;&#21512;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#21644;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#35270;&#35273;&#24187;&#35273;&#30340;&#35782;&#21035;&#33021;&#21147;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;TTI&#65289;&#27169;&#22411;&#24050;&#32463;&#25104;&#20026;&#21508;&#31181;&#29983;&#25104;&#39046;&#22495;&#20013;&#29983;&#25104;&#35757;&#32451;&#25968;&#25454;&#30340;&#24120;&#35265;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35270;&#35273;&#24187;&#35273;&#65292;&#23588;&#20854;&#26159;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39118;&#26684;&#22914;&#21345;&#36890;&#20154;&#29289;&#20013;&#21253;&#21547;&#20102;&#24863;&#30693;&#19978;&#20851;&#38190;&#30340;&#32570;&#38519;&#65292;&#20381;&#28982;&#26159;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#26816;&#27979;TTI&#27169;&#22411;&#29983;&#25104;&#30340;&#21345;&#36890;&#35282;&#33394;&#22270;&#20687;&#30340;&#35270;&#35273;&#24187;&#35273;&#26816;&#27979;&#31995;&#32479;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21033;&#29992;&#20102;&#23039;&#21183;&#24863;&#30693;&#19978;&#19979;&#25991;&#35270;&#35273;&#23398;&#20064;&#65288;PA-ICVL&#65289;&#19982;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#65292;&#21516;&#26102;&#21033;&#29992;RGB&#22270;&#20687;&#21644;&#23039;&#21183;&#20449;&#24687;&#12290;&#36890;&#36807;&#20174;&#19968;&#20010;&#32463;&#36807;&#24494;&#35843;&#30340;&#23039;&#21183;&#20272;&#35745;&#22120;&#20013;&#33719;&#24471;&#23039;&#21183;&#25351;&#23548;&#65292;&#25105;&#20204;&#20351;VLM&#33021;&#22815;&#20570;&#20986;&#26356;&#20934;&#30830;&#30340;&#20915;&#31574;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#35782;&#21035;&#35270;&#35273;&#24187;&#35273;&#26041;&#38754;&#65292;&#19982;&#20165;&#20381;&#36182;&#20110;RGB&#22270;&#20687;&#30340;&#22522;&#32447;&#26041;&#27861;&#30456;&#27604;&#65292;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#20943;&#36731;&#35270;&#35273;&#24187;&#35273;&#65292;&#25512;&#21160;&#20102;TTI&#27169;&#22411;&#22312;&#38750;&#29031;&#29255;&#30495;&#23454;&#39046;&#22495;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.15048v1 Announce Type: cross  Abstract: Large-scale Text-to-Image (TTI) models have become a common approach for generating training data in various generative fields. However, visual hallucinations, which contain perceptually critical defects, remain a concern, especially in non-photorealistic styles like cartoon characters. We propose a novel visual hallucination detection system for cartoon character images generated by TTI models. Our approach leverages pose-aware in-context visual learning (PA-ICVL) with Vision-Language Models (VLMs), utilizing both RGB images and pose information. By incorporating pose guidance from a fine-tuned pose estimator, we enable VLMs to make more accurate decisions. Experimental results demonstrate significant improvements in identifying visual hallucinations compared to baseline methods relying solely on RGB images. This research advances TTI models by mitigating visual hallucinations, expanding their potential in non-photorealistic domains.
&lt;/p&gt;</description></item><item><title>&#24341;&#21147;&#21452;&#23545;&#20598;&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#39044;&#35774;&#30340;&#29366;&#24577;&#26041;&#31243;&#25512;&#23548;&#20986;&#23545;&#24212;&#30340;&#24341;&#21147;&#29702;&#35770;&#12290;</title><link>https://arxiv.org/abs/2403.14763</link><description>&lt;p&gt;
&#20174;&#29366;&#24577;&#26041;&#31243;&#21040;&#24341;&#21147;&#23545;&#20598;
&lt;/p&gt;
&lt;p&gt;
Gravitational Duals from Equations of State
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14763
&lt;/p&gt;
&lt;p&gt;
&#24341;&#21147;&#21452;&#23545;&#20598;&#29702;&#35770;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#39044;&#35774;&#30340;&#29366;&#24577;&#26041;&#31243;&#25512;&#23548;&#20986;&#23545;&#24212;&#30340;&#24341;&#21147;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24341;&#21147;&#21452;&#23545;&#20598;&#29702;&#35770;&#23558;&#20116;&#32500;&#24341;&#21147;&#29702;&#35770;&#19982;&#22235;&#32500;&#37327;&#23376;&#22330;&#35770;&#22312;&#24179;&#30452;&#31354;&#38388;&#20013;&#30456;&#20851;&#32852;&#12290;&#22312;&#36825;&#31181;&#26144;&#23556;&#19979;&#65292;&#22330;&#35770;&#30340;&#29366;&#24577;&#26041;&#31243;&#34987;&#32534;&#30721;&#22312;&#24341;&#21147;&#29702;&#35770;&#30340;&#40657;&#27934;&#35299;&#20013;&#12290;&#35299;&#20116;&#32500;&#29233;&#22240;&#26031;&#22374;&#26041;&#31243;&#20197;&#30830;&#23450;&#29366;&#24577;&#26041;&#31243;&#26159;&#19968;&#20010;&#31639;&#27861;&#24615;&#30340;&#12289;&#30452;&#25509;&#30340;&#38382;&#39064;&#12290;&#30830;&#23450;&#24341;&#21147;&#29702;&#35770;&#20174;&#32780;&#20135;&#29983;&#29305;&#23450;&#29366;&#24577;&#26041;&#31243;&#26159;&#19968;&#20010;&#26356;&#20855;&#25361;&#25112;&#24615;&#30340;&#12289;&#21453;&#21521;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#29289;&#29702;&#20449;&#24687;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#31639;&#27861;&#19981;&#20165;&#21463;&#25968;&#25454;&#39537;&#21160;&#65292;&#36824;&#21463;&#29233;&#22240;&#26031;&#22374;&#26041;&#31243;&#30340;&#29289;&#29702;&#30693;&#35782;&#39537;&#21160;&#12290;&#25105;&#20204;&#25104;&#21151;&#22320;&#23558;&#20854;&#24212;&#29992;&#20110;&#20855;&#26377;&#20132;&#21449;&#28857;&#12289;&#19968;&#32423;&#21644;&#20108;&#32423;&#30456;&#21464;&#30340;&#29702;&#35770;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14763v1 Announce Type: cross  Abstract: Holography relates gravitational theories in five dimensions to four-dimensional quantum field theories in flat space. Under this map, the equation of state of the field theory is encoded in the black hole solutions of the gravitational theory. Solving the five-dimensional Einstein's equations to determine the equation of state is an algorithmic, direct problem. Determining the gravitational theory that gives rise to a prescribed equation of state is a much more challenging, inverse problem. We present a novel approach to solve this problem based on physics-informed neural networks. The resulting algorithm is not only data-driven but also informed by the physics of the Einstein's equations. We successfully apply it to theories with crossovers, first- and second-order phase transitions.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#35780;&#20272;&#37327;&#34920;&#65288;AIAS&#65289;&#30340;&#23454;&#36341;&#24212;&#29992;&#65292;&#36890;&#36807;&#28789;&#27963;&#26694;&#26550;&#23558;GenAI&#25216;&#26415;&#32435;&#20837;&#25945;&#32946;&#35780;&#20272;&#20013;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;GenAI&#30456;&#20851;&#30340;&#23398;&#26415;&#19981;&#31471;&#26696;&#20214;&#65292;&#25552;&#39640;&#20102;&#23398;&#29983;&#30340;&#23398;&#19994;&#25104;&#32489;&#12290;</title><link>https://arxiv.org/abs/2403.14692</link><description>&lt;p&gt;
AI &#35780;&#20272;&#37327;&#34920;&#65288;AIAS&#65289;&#30340;&#23454;&#36341;&#65306;GenAI &#25903;&#25345;&#35780;&#20272;&#30340;&#35797;&#28857;&#23454;&#26045;
&lt;/p&gt;
&lt;p&gt;
The AI Assessment Scale (AIAS) in action: A pilot implementation of GenAI supported assessment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14692
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;AI&#35780;&#20272;&#37327;&#34920;&#65288;AIAS&#65289;&#30340;&#23454;&#36341;&#24212;&#29992;&#65292;&#36890;&#36807;&#28789;&#27963;&#26694;&#26550;&#23558;GenAI&#25216;&#26415;&#32435;&#20837;&#25945;&#32946;&#35780;&#20272;&#20013;&#65292;&#26174;&#33879;&#38477;&#20302;&#20102;&#19982;GenAI&#30456;&#20851;&#30340;&#23398;&#26415;&#19981;&#31471;&#26696;&#20214;&#65292;&#25552;&#39640;&#20102;&#23398;&#29983;&#30340;&#23398;&#19994;&#25104;&#32489;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#31561;&#25945;&#32946;&#20013;&#24555;&#36895;&#37319;&#29992;&#29983;&#25104;&#20154;&#24037;&#26234;&#33021;&#65288;GenAI&#65289;&#25216;&#26415;&#24341;&#21457;&#20102;&#23545;&#23398;&#26415;&#35802;&#20449;&#12289;&#35780;&#20272;&#23454;&#36341;&#21644;&#23398;&#29983;&#23398;&#20064;&#30340;&#20851;&#27880;&#12290;&#31105;&#27490;&#25110;&#38459;&#27490;GenAI&#24037;&#20855;&#24050;&#34987;&#35777;&#26126;&#26159;&#26080;&#25928;&#30340;&#65292;&#24809;&#32602;&#24615;&#26041;&#27861;&#24573;&#30053;&#20102;&#36825;&#20123;&#25216;&#26415;&#30340;&#28508;&#22312;&#22909;&#22788;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#22312;&#33521;&#22269;&#36234;&#21335;&#22823;&#23398;&#65288;BUV&#65289;&#36827;&#34892;&#30340;&#35797;&#28857;&#30740;&#31350;&#30340;&#32467;&#26524;&#65292;&#25506;&#35752;&#20102;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#37327;&#34920;&#65288;AIAS&#65289;&#30340;&#23454;&#26045;&#65292;&#36825;&#26159;&#19968;&#20010;&#28789;&#27963;&#30340;&#26694;&#26550;&#65292;&#29992;&#20110;&#23558;GenAI&#32435;&#20837;&#25945;&#32946;&#35780;&#20272;&#20013;&#12290;AIAS&#30001;&#20116;&#20010;&#32423;&#21035;&#32452;&#25104;&#65292;&#20174;&#8220;&#26080;AI&#8221;&#21040;&#8220;&#23436;&#20840;AI&#8221;&#65292;&#20351;&#25945;&#32946;&#24037;&#20316;&#32773;&#33021;&#22815;&#35774;&#35745;&#20391;&#37325;&#20110;&#38656;&#35201;&#20154;&#31867;&#36755;&#20837;&#21644;&#25209;&#21028;&#24615;&#24605;&#32500;&#30340;&#35780;&#20272;&#12290;&#22312;&#23454;&#26045;AIAS&#21518;&#65292;&#35797;&#28857;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#19982;GenAI&#30456;&#20851;&#30340;&#23398;&#26415;&#19981;&#31471;&#26696;&#20214;&#26174;&#30528;&#20943;&#23569;&#65292;&#23398;&#29983;&#25104;&#32489;&#25552;&#39640;&#20102;5.9%&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14692v1 Announce Type: cross  Abstract: The rapid adoption of Generative Artificial Intelligence (GenAI) technologies in higher education has raised concerns about academic integrity, assessment practices, and student learning. Banning or blocking GenAI tools has proven ineffective, and punitive approaches ignore the potential benefits of these technologies. This paper presents the findings of a pilot study conducted at British University Vietnam (BUV) exploring the implementation of the Artificial Intelligence Assessment Scale (AIAS), a flexible framework for incorporating GenAI into educational assessments. The AIAS consists of five levels, ranging from 'No AI' to 'Full AI', enabling educators to design assessments that focus on areas requiring human input and critical thinking.   Following the implementation of the AIAS, the pilot study results indicate a significant reduction in academic misconduct cases related to GenAI, a 5.9% increase in student attainment across the 
&lt;/p&gt;</description></item><item><title>&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#38656;&#35201;&#21046;&#23450;&#21644;&#23454;&#26045;&#20135;&#19994;&#26631;&#20934;&#65292;&#20197;&#35299;&#20915;&#20114;&#25805;&#20316;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#36947;&#24503;&#27835;&#29702;&#31561;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.14689</link><description>&lt;p&gt;
&#21457;&#23637;&#21644;&#37096;&#32626;&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#20135;&#19994;&#26631;&#20934;&#65306;&#25361;&#25112;&#12289;&#31574;&#30053;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
Developing and Deploying Industry Standards for Artificial Intelligence in Education (AIED): Challenges, Strategies, and Future Directions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14689
&lt;/p&gt;
&lt;p&gt;
&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#30340;&#21457;&#23637;&#38656;&#35201;&#21046;&#23450;&#21644;&#23454;&#26045;&#20135;&#19994;&#26631;&#20934;&#65292;&#20197;&#35299;&#20915;&#20114;&#25805;&#20316;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#36947;&#24503;&#27835;&#29702;&#31561;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#25945;&#32946;&#39046;&#22495;&#30340;&#24212;&#29992;&#25215;&#35834;&#36890;&#36807;&#25552;&#20379;&#20010;&#24615;&#21270;&#23398;&#20064;&#20307;&#39564;&#12289;&#33258;&#21160;&#21270;&#34892;&#25919;&#21644;&#25945;&#23398;&#20219;&#21153;&#20197;&#21450;&#38477;&#20302;&#20869;&#23481;&#21019;&#24314;&#25104;&#26412;&#26469;&#38761;&#26032;&#25945;&#32946;&#23454;&#36341;&#12290;&#28982;&#32780;&#65292;&#22312;&#24320;&#21457;&#21644;&#37096;&#32626;&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#32570;&#20047;&#26631;&#20934;&#21270;&#23454;&#36341;&#23548;&#33268;&#29983;&#24577;&#31995;&#32479;&#20998;&#25955;&#65292;&#32473;&#20114;&#25805;&#20316;&#24615;&#12289;&#21487;&#25193;&#23637;&#24615;&#21644;&#36947;&#24503;&#27835;&#29702;&#24102;&#26469;&#25361;&#25112;&#12290;&#26412;&#25991;&#26088;&#22312;&#35299;&#20915;&#22312;&#25945;&#32946;&#39046;&#22495;&#20154;&#24037;&#26234;&#33021;&#21457;&#23637;&#21644;&#23454;&#26045;&#20135;&#19994;&#26631;&#20934;&#30340;&#32039;&#36843;&#38656;&#27714;&#65292;&#25552;&#20379;&#23545;&#24403;&#21069;&#23616;&#21183;&#12289;&#25361;&#25112;&#21644;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#30340;&#31574;&#30053;&#26041;&#27861;&#30340;&#20840;&#38754;&#20998;&#26512;&#12290;&#25105;&#20204;&#24320;&#22987;&#36890;&#36807;&#30740;&#31350;AIED&#22312;&#19981;&#21516;&#25945;&#32946;&#29615;&#22659;&#20013;&#30340;&#21508;&#31181;&#24212;&#29992;&#65292;&#24182;&#30830;&#23450;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#20851;&#38190;&#39046;&#22495;&#65292;&#21253;&#25324;&#31995;&#32479;&#20114;&#25805;&#20316;&#24615;&#12289;&#26412;&#20307;&#26144;&#23556;&#12289;&#25968;&#25454;&#38598;&#25104;&#12289;&#35780;&#20272;&#21644;&#36947;&#24503;&#27835;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14689v1 Announce Type: cross  Abstract: The adoption of Artificial Intelligence in Education (AIED) holds the promise of revolutionizing educational practices by offering personalized learning experiences, automating administrative and pedagogical tasks, and reducing the cost of content creation. However, the lack of standardized practices in the development and deployment of AIED solutions has led to fragmented ecosystems, which presents challenges in interoperability, scalability, and ethical governance. This article aims to address the critical need to develop and implement industry standards in AIED, offering a comprehensive analysis of the current landscape, challenges, and strategic approaches to overcome these obstacles. We begin by examining the various applications of AIED in various educational settings and identify key areas lacking in standardization, including system interoperability, ontology mapping, data integration, evaluation, and ethical governance. Then, 
&lt;/p&gt;</description></item><item><title>ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#21270;&#30340;&#23545;&#35805;&#22238;&#22797;&#65292;&#21487;&#38761;&#26032;&#21508;&#34892;&#19994;&#24182;&#25913;&#21464;&#25216;&#26415;&#20114;&#21160;&#26041;&#24335;&#12290;</title><link>https://arxiv.org/abs/2403.14643</link><description>&lt;p&gt;
&#25506;&#31350;ChatGPT&#21450;&#20854;&#23545;&#31038;&#20250;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Exploring ChatGPT and its Impact on Society
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14643
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#33021;&#22815;&#29983;&#25104;&#20154;&#31867;&#21270;&#30340;&#23545;&#35805;&#22238;&#22797;&#65292;&#21487;&#38761;&#26032;&#21508;&#34892;&#19994;&#24182;&#25913;&#21464;&#25216;&#26415;&#20114;&#21160;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#24050;&#32463;&#23384;&#22312;&#19968;&#27573;&#26102;&#38388;&#20102;&#65292;&#20294;&#31361;&#28982;&#38388;&#27604;&#20197;&#24448;&#20219;&#20309;&#26102;&#20505;&#37117;&#21463;&#21040;&#20102;&#26356;&#22810;&#30340;&#20851;&#27880;&#12290;&#24863;&#35874;&#35895;&#27468;&#12289;&#24494;&#36719;&#12289;&#20803;&#23431;&#23449;&#31561;&#31185;&#25216;&#30028;&#20027;&#35201;&#21697;&#29260;&#30340;&#21019;&#26032;&#12290;&#28982;&#32780;&#65292;OpenAI&#36890;&#36807;&#20854;&#24320;&#21019;&#24615;&#21457;&#26126;ChatGPT&#35302;&#21457;&#20102;&#25353;&#38062;&#12290;ChatGPT&#26159;&#19968;&#31181;&#22522;&#20110;Transformer&#26550;&#26500;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#33021;&#22815;&#22312;&#23545;&#35805;&#32972;&#26223;&#20013;&#29983;&#25104;&#31867;&#20284;&#20154;&#31867;&#30340;&#22238;&#22797;&#12290;&#23427;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#26469;&#29983;&#25104;&#23545;&#36755;&#20837;&#25991;&#26412;&#30340;&#33258;&#28982;&#35821;&#35328;&#22238;&#22797;&#12290;&#20854;&#24222;&#22823;&#30340;&#21442;&#25968;&#25968;&#37327;&#12289;&#19978;&#19979;&#25991;&#29983;&#25104;&#21644;&#38754;&#21521;&#24320;&#25918;&#22495;&#30340;&#35757;&#32451;&#20351;&#20854;&#25104;&#20026;&#19968;&#31181;&#22810;&#21151;&#33021;&#19988;&#26377;&#25928;&#30340;&#24037;&#20855;&#65292;&#21487;&#24212;&#29992;&#20110;&#20174;&#32842;&#22825;&#26426;&#22120;&#20154;&#21040;&#23458;&#25143;&#26381;&#21153;&#20877;&#21040;&#35821;&#35328;&#32763;&#35793;&#31561;&#24191;&#27867;&#39046;&#22495;&#12290;&#23427;&#20855;&#26377;&#24443;&#24213;&#25913;&#21464;&#21508;&#34892;&#19994;&#24182;&#36716;&#21464;&#25105;&#20204;&#19982;&#25216;&#26415;&#20114;&#21160;&#26041;&#24335;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;ChatGPT&#20063;&#24341;&#21457;&#20102;&#19968;&#20123;&#25285;&#24551;&#65292;&#21253;&#25324;&#36947;&#24503;&#26041;&#38754;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14643v1 Announce Type: cross  Abstract: Artificial intelligence has been around for a while, but suddenly it has received more attention than ever before. Thanks to innovations from companies like Google, Microsoft, Meta, and other major brands in technology. OpenAI, though, has triggered the button with its ground-breaking invention ChatGPT. ChatGPT is a Large Language Model (LLM) based on Transformer architecture that has the ability to generate human-like responses in a conversational context. It uses deep learning algorithms to generate natural language responses to input text. Its large number of parameters, contextual generation, and open-domain training make it a versatile and effective tool for a wide range of applications, from chatbots to customer service to language translation. It has the potential to revolutionize various industries and transform the way we interact with technology. However, the use of ChatGPT has also raised several concerns, including ethical,
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.14589</link><description>&lt;p&gt;
ReAct&#36935;&#19978;ActRe&#65306;&#23545;&#27604;&#24615;&#33258;&#35757;&#32451;&#20013;&#30340;&#20195;&#29702;&#36712;&#36857;&#33258;&#21160;&#26631;&#27880;
&lt;/p&gt;
&lt;p&gt;
ReAct Meets ActRe: Autonomous Annotations of Agent Trajectories for Contrastive Self-Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14589
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;A$^3$T&#26694;&#26550;&#65292;&#36890;&#36807;ActRe&#25552;&#31034;&#20195;&#29702;&#23454;&#29616;&#20102;ReAct&#39118;&#26684;&#20195;&#29702;&#23545;&#20195;&#29702;&#36712;&#36857;&#30340;&#33258;&#20027;&#26631;&#27880;&#65292;&#21516;&#26102;&#22686;&#24378;&#20102;&#26032;&#30340;&#36712;&#36857;&#21512;&#25104;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 &#20844;&#21578;&#31867;&#22411;&#65306;&#26032; &#25991;&#25688;&#65306;&#35821;&#35328;&#20195;&#29702;&#36890;&#36807;&#19982;&#22522;&#30784;&#27169;&#22411;&#25512;&#29702;&#23637;&#31034;&#20102;&#33258;&#20027;&#20915;&#31574;&#33021;&#21147;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#33268;&#21147;&#20110;&#36890;&#36807;&#22810;&#27493;&#25512;&#29702;&#21644;&#34892;&#21160;&#36712;&#36857;&#20316;&#20026;&#35757;&#32451;&#25968;&#25454;&#26469;&#35757;&#32451;&#35821;&#35328;&#20195;&#29702;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#25910;&#38598;&#36825;&#26679;&#30340;&#36712;&#36857;&#20173;&#38656;&#35201;&#30456;&#24403;&#22823;&#30340;&#20154;&#21147;&#65292;&#26080;&#35770;&#26159;&#36890;&#36807;&#20154;&#24037;&#26631;&#27880;&#36824;&#26159;&#23454;&#26045;&#22810;&#26679;&#21270;&#25552;&#31034;&#26694;&#26550;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;A$^3$T&#65292;&#19968;&#20010;&#20801;&#35768;&#20197;ReAct&#39118;&#26684;&#33258;&#20027;&#27880;&#37322;&#20195;&#29702;&#36712;&#36857;&#30340;&#26694;&#26550;&#12290;&#20854;&#20013;&#24515;&#26159;&#19968;&#20010;ActRe&#25552;&#31034;&#20195;&#29702;&#65292;&#23427;&#35299;&#37322;&#20219;&#24847;&#21160;&#20316;&#30340;&#21407;&#22240;&#12290;&#24403;&#38543;&#26426;&#25277;&#21462;&#22806;&#37096;&#21160;&#20316;&#26102;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#20197;&#26597;&#35810;ActRe&#20195;&#29702;&#20197;&#33719;&#21462;&#20854;&#25991;&#26412;&#29702;&#30001;&#12290;&#26032;&#39062;&#30340;&#36712;&#36857;&#28982;&#21518;&#36890;&#36807;&#23558;ActRe&#30340;&#21518;&#39564;&#25512;&#29702;&#21069;&#32622;&#21040;&#25277;&#26679;&#21160;&#20316;&#20013;&#36827;&#34892;&#32508;&#21512;&#21512;&#25104;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#24335;&#65292;ReAct&#39118;&#26684;&#20195;&#29702;&#21487;&#25191;&#34892;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14589v1 Announce Type: new  Abstract: Language agents have demonstrated autonomous decision-making abilities by reasoning with foundation models. Recently, efforts have been made to train language agents for performance improvement, with multi-step reasoning and action trajectories as the training data. However, collecting such trajectories still requires considerable human effort, by either artificial annotations or implementations of diverse prompting frameworks. In this work, we propose A$^3$T, a framework that enables the Autonomous Annotation of Agent Trajectories in the style of ReAct. The central role is an ActRe prompting agent, which explains the reason for an arbitrary action. When randomly sampling an external action, the ReAct-style agent could query the ActRe agent with the action to obtain its textual rationales. Novel trajectories are then synthesized by prepending the posterior reasoning from ActRe to the sampled action. In this way, the ReAct-style agent exe
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#27010;&#24565;&#26041;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35299;&#37322;&#24615;&#25913;&#36827;&#65292;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#35299;&#37322;&#65292;&#20351;&#24471;&#26816;&#27979;&#20266;&#20851;&#32852;&#21644;&#22266;&#26377;&#20559;&#35265;&#25104;&#20026;&#21487;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.14566</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#22522;&#20110;&#27010;&#24565;&#26041;&#27861;&#25913;&#36827;&#27169;&#22411;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A survey on Concept-based Approaches For Model Improvement
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14566
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#27010;&#24565;&#26041;&#27861;&#23545;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#35299;&#37322;&#24615;&#25913;&#36827;&#65292;&#25552;&#20379;&#20102;&#20154;&#31867;&#21487;&#29702;&#35299;&#30340;&#20915;&#31574;&#35299;&#37322;&#65292;&#20351;&#24471;&#26816;&#27979;&#20266;&#20851;&#32852;&#21644;&#22266;&#26377;&#20559;&#35265;&#25104;&#20026;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30740;&#31350;&#30340;&#37325;&#28857;&#24050;&#32463;&#20174;&#20165;&#20165;&#25552;&#39640;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#36716;&#21464;&#20026;&#20351;DNN&#26356;&#26131;&#35299;&#37322;&#32473;&#20154;&#31867;&#12290;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#24050;&#32463;&#35266;&#23519;&#21040;&#21508;&#31181;&#25216;&#26415;&#65292;&#21253;&#25324;&#22522;&#20110;&#26174;&#33879;&#24615;&#21644;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#29992;&#25152;&#35859;&#30340;&#27010;&#24565;&#22312;&#31616;&#21333;&#30340;&#20154;&#31867;&#21487;&#29702;&#35299;&#26415;&#35821;&#20013;&#35299;&#37322;&#27169;&#22411;&#30340;&#20915;&#31574;&#12290;&#27010;&#24565;&#26159;&#25968;&#25454;&#30340;&#20154;&#31867;&#21487;&#35299;&#37322;&#21333;&#20803;&#65292;&#26159;&#20154;&#31867;&#24605;&#32500;&#30340;&#22522;&#30707;&#12290;&#29992;&#27010;&#24565;&#30340;&#35299;&#37322;&#33021;&#22815;&#26816;&#27979;&#21040;&#20266;&#20851;&#32852;&#12289;&#22266;&#26377;&#20559;&#35265;&#25110;&#32874;&#26126;&#27721;&#12290;&#38543;&#30528;&#22522;&#20110;&#27010;&#24565;&#30340;&#35299;&#37322;&#30340;&#20986;&#29616;&#65292;&#20986;&#29616;&#20102;&#21508;&#31181;&#27010;&#24565;&#34920;&#31034;&#26041;&#27861;&#21644;&#33258;&#21160;&#27010;&#24565;&#21457;&#29616;&#31639;&#27861;&#12290;&#19968;&#20123;&#26368;&#36817;&#30340;&#26041;&#27861;&#20351;&#29992;&#27010;&#24565;&#36827;&#34892;&#20107;&#21518;&#27169;&#22411;&#35299;&#32544;&#35780;&#20272;&#65292;&#32780;&#20854;&#20182;&#20154;&#20351;&#29992;&#23427;&#20204;&#36827;&#34892;&#20107;&#21069;&#35757;&#32451;&#12290;&#22522;&#20110;&#27010;&#24565;&#30340;&#26041;&#27861;&#26159;&#26032;&#30340;&#65292;&#26377;&#35768;&#22810;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14566v1 Announce Type: new  Abstract: The focus of recent research has shifted from merely increasing the Deep Neural Networks (DNNs) performance in various tasks to DNNs, which are more interpretable to humans. The field of eXplainable Artificial Intelligence (XAI) has observed various techniques, including saliency-based and concept-based approaches. Concept-based approaches explain the model's decisions in simple human understandable terms called Concepts. Concepts are human interpretable units of data and are the thinking ground of humans. Explanations in terms of concepts enable detecting spurious correlations, inherent biases, or clever-hans. With the advent of concept-based explanations, there have been various concept representation methods and automatic concept discovery algorithms. Some recent methods use concepts for post-hoc model disentanglement evaluation, while others use them for ante-hoc training. The concept-based approaches are new, with many representatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.14119</link><description>&lt;p&gt;
C-TPT&#65306;&#36890;&#36807;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#26657;&#20934;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
C-TPT: Calibrated Test-Time Prompt Tuning for Vision-Language Models via Text Feature Dispersion
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14119
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#26469;&#25506;&#35752;&#26657;&#20934;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28145;&#24230;&#23398;&#20064;&#20013;&#65292;&#27979;&#35797;&#26102;&#36866;&#24212;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#20316;&#20026;&#19968;&#31181;&#22312;&#19981;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#30340;&#26041;&#27861;&#12290;&#19968;&#20010;&#20027;&#35201;&#30340;&#20363;&#35777;&#26159;&#26368;&#36817;&#25552;&#20986;&#30340;&#29992;&#20110;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;&#22914;CLIP&#65289;&#30340;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25552;&#31034;&#20027;&#35201;&#26159;&#20026;&#20102;&#25552;&#39640;&#20934;&#30830;&#24615;&#32780;&#24320;&#21457;&#30340;&#65292;&#24573;&#35270;&#20102;&#26657;&#20934;&#30340;&#37325;&#35201;&#24615;&#8212;&#8212;&#37327;&#21270;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26657;&#20934;&#26041;&#27861;&#20381;&#36182;&#22823;&#37327;&#26631;&#35760;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;&#23427;&#20204;&#22312;&#27979;&#35797;&#26102;&#22330;&#26223;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#36890;&#36807;&#21033;&#29992;CLIP&#30340;&#22266;&#26377;&#23646;&#24615;&#65292;&#22312;&#27979;&#35797;&#26102;&#25552;&#31034;&#35843;&#25972;&#36807;&#31243;&#20013;&#25506;&#35752;&#26657;&#20934;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#35266;&#23519;&#65292;&#25105;&#20204;&#21457;&#29616;&#25552;&#31034;&#36873;&#25321;&#26174;&#33879;&#24433;&#21709;&#20102;CLIP&#20013;&#30340;&#26657;&#20934;&#65292;&#20854;&#20013;&#23548;&#33268;&#26356;&#39640;&#25991;&#26412;&#29305;&#24449;&#31163;&#25955;&#24615;&#30340;&#25552;&#31034;&#20250;&#20135;&#29983;&#26356;&#22909;&#26657;&#20934;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14119v1 Announce Type: cross  Abstract: In deep learning, test-time adaptation has gained attention as a method for model fine-tuning without the need for labeled data. A prime exemplification is the recently proposed test-time prompt tuning for large-scale vision-language models such as CLIP. Unfortunately, these prompts have been mainly developed to improve accuracy, overlooking the importance of calibration-a crucial aspect for quantifying prediction uncertainty. However, traditional calibration methods rely on substantial amounts of labeled data, making them impractical for test-time scenarios. To this end, this paper explores calibration during test-time prompt tuning by leveraging the inherent properties of CLIP. Through a series of observations, we find that the prompt choice significantly affects the calibration in CLIP, where the prompts leading to higher text feature dispersion result in better-calibrated predictions. Introducing the Average Text Feature Dispersion
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Data Center Carbon Footprint Reduction (DC-CFR) &#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#26102;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#20197;&#20943;&#23569;&#30899;&#36275;&#36857;&#12290;</title><link>https://arxiv.org/abs/2403.14092</link><description>&lt;p&gt;
&#21487;&#25345;&#32493;&#25968;&#25454;&#20013;&#24515;&#23454;&#26102;&#20943;&#23569;&#30899;&#36275;&#36857;
&lt;/p&gt;
&lt;p&gt;
Carbon Footprint Reduction for Sustainable Data Centers in Real-Time
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.14092
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;Data Center Carbon Footprint Reduction (DC-CFR) &#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#23454;&#26102;&#20248;&#21270;&#25968;&#25454;&#20013;&#24515;&#20197;&#20943;&#23569;&#30899;&#36275;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#24037;&#20316;&#36127;&#36733;&#26174;&#33879;&#22686;&#21152;&#33021;&#28304;&#28040;&#32791;&#65292;&#30899;&#25490;&#25918;&#20302;&#30340;&#21487;&#25345;&#32493;&#25968;&#25454;&#20013;&#24515;&#27491;&#25104;&#20026;&#20840;&#29699;&#25919;&#24220;&#21644;&#20225;&#19994;&#20851;&#27880;&#30340;&#37325;&#28857;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#38656;&#35201;&#22312;&#20919;&#21364;&#21644;IT&#36127;&#36733;&#20013;&#36827;&#34892;&#21151;&#32791;&#20248;&#21270;&#30340;&#33539;&#24335;&#36716;&#21464;&#65292;&#22522;&#20110;&#21487;&#20877;&#29983;&#33021;&#28304;&#22312;&#30005;&#32593;&#20013;&#30340;&#21487;&#29992;&#24615;&#26469;&#35843;&#25972;&#28789;&#27963;&#36127;&#36733;&#65292;&#21033;&#29992;&#25968;&#25454;&#20013;&#24515;&#19981;&#38388;&#26029;&#30005;&#28304;&#20013;&#30340;&#30005;&#27744;&#23384;&#20648;&#65292;&#20351;&#29992;&#21327;&#20316;&#20195;&#29702;&#12290;&#36825;&#20123;&#20248;&#21270;&#31574;&#30053;&#20043;&#38388;&#30340;&#22797;&#26434;&#20851;&#31995;&#20197;&#21450;&#23427;&#20204;&#23545;&#21464;&#21270;&#30340;&#22806;&#37096;&#22240;&#32032;&#65288;&#22914;&#22825;&#27668;&#21644;&#30005;&#32593;&#30899;&#25490;&#25918;&#24378;&#24230;&#65289;&#30340;&#20381;&#36182;&#20351;&#24471;&#36825;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#38382;&#39064;&#12290;&#30446;&#21069;&#32570;&#20047;&#19968;&#20010;&#33021;&#22815;&#22312;&#21160;&#24577;&#23454;&#38469;&#29615;&#22659;&#20013;&#21516;&#26102;&#20248;&#21270;&#25152;&#26377;&#36825;&#20123;&#30446;&#26631;&#30340;&#23454;&#26102;&#25511;&#21046;&#22120;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#20013;&#24515;&#30899;&#36275;&#36857;&#20943;&#23569;&#65288;DC-CFR&#65289;&#22810;&#20195;&#29702;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#26694;&#26550;&#65292;&#33021;&#22815;&#20248;&#21270;&#22810;&#20010;&#35282;&#24230;&#30340;&#25968;&#25454;&#20013;&#24515;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.14092v1 Announce Type: cross  Abstract: As machine learning workloads significantly increase energy consumption, sustainable data centers with low carbon emissions are becoming a top priority for governments and corporations worldwide. This requires a paradigm shift in optimizing power consumption in cooling and IT loads, shifting flexible loads based on the availability of renewable energy in the power grid, and leveraging battery storage from the uninterrupted power supply in data centers, using collaborative agents. The complex association between these optimization strategies and their dependencies on variable external factors like weather and the power grid carbon intensity makes this a hard problem. Currently, a real-time controller to optimize all these goals simultaneously in a dynamic real-world setting is lacking. We propose a Data Center Carbon Footprint Reduction (DC-CFR) multi-agent Reinforcement Learning (MARL) framework that optimizes data centers for the mult
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#20102;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#24773;&#24863;&#29366;&#24577;&#30340;&#39044;&#27979;&#12290;</title><link>https://arxiv.org/abs/2403.13841</link><description>&lt;p&gt;
&#25972;&#21512;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#29992;&#20110;&#20010;&#24615;&#21270;&#24773;&#24863;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Integrating Wearable Sensor Data and Self-reported Diaries for Personalized Affect Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13841
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25972;&#21512;&#20102;&#21487;&#31359;&#25140;&#20256;&#24863;&#22120;&#25968;&#25454;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#24773;&#24863;&#29366;&#24577;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24773;&#32490;&#29366;&#24577;&#20316;&#20026;&#24773;&#24863;&#30340;&#25351;&#26631;&#23545;&#25972;&#20307;&#20581;&#24247;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#27492;&#22312;&#20854;&#21457;&#20316;&#21069;&#20934;&#30830;&#39044;&#27979;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#30446;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20351;&#29992;&#26469;&#33258;&#21487;&#31359;&#25140;&#21644;&#31227;&#21160;&#35774;&#22791;&#30340;&#25968;&#25454;&#36827;&#34892;&#19982;&#30701;&#26399;&#24773;&#24863;&#26816;&#27979;&#12290;&#36825;&#20123;&#30740;&#31350;&#36890;&#24120;&#19987;&#27880;&#20110;&#23458;&#35266;&#30340;&#24863;&#23448;&#27979;&#37327;&#65292;&#24448;&#24448;&#24573;&#30053;&#20854;&#20182;&#24418;&#24335;&#30340;&#33258;&#25105;&#25253;&#21578;&#20449;&#24687;&#65292;&#22914;&#26085;&#35760;&#21644;&#31508;&#35760;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24773;&#24863;&#29366;&#24577;&#39044;&#27979;&#30340;&#22810;&#27169;&#24577;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#35813;&#27169;&#22411;&#32467;&#21512;&#20102;&#19968;&#20010;transformer&#32534;&#30721;&#22120;&#21644;&#19968;&#20010;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#23458;&#35266;&#25351;&#26631;&#21644;&#33258;&#25105;&#25253;&#21578;&#26085;&#35760;&#30340;&#32508;&#21512;&#20998;&#26512;&#12290;&#20026;&#20102;&#39564;&#35777;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#19968;&#39033;&#32437;&#21521;&#30740;&#31350;&#65292;&#25307;&#21215;&#20102;&#22823;&#23398;&#29983;&#24182;&#22312;&#19968;&#24180;&#20869;&#23545;&#20854;&#36827;&#34892;&#30417;&#27979;&#65292;&#25910;&#38598;&#20102;&#21253;&#25324;&#29983;&#29702;&#12289;&#29615;&#22659;&#12289;&#30561;&#30496;&#12289;&#20195;&#35874;&#21644;&#36523;&#20307;&#27963;&#21160;&#21442;&#25968;&#22312;&#20869;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#21442;&#19982;&#32773;&#25552;&#20379;&#20102;&#24320;&#25918;&#24335;&#25991;&#26412;&#26085;&#35760;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13841v1 Announce Type: cross  Abstract: Emotional states, as indicators of affect, are pivotal to overall health, making their accurate prediction before onset crucial. Current studies are primarily centered on immediate short-term affect detection using data from wearable and mobile devices. These studies typically focus on objective sensory measures, often neglecting other forms of self-reported information like diaries and notes. In this paper, we propose a multimodal deep learning model for affect status forecasting. This model combines a transformer encoder with a pre-trained language model, facilitating the integrated analysis of objective metrics and self-reported diaries. To validate our model, we conduct a longitudinal study, enrolling college students and monitoring them over a year, to collect an extensive dataset including physiological, environmental, sleep, metabolic, and physical activity parameters, alongside open-ended textual diaries provided by the partici
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#30340;&#26032;&#26694;&#26550;&#65292;&#19987;&#27880;&#20110;Valence-Arousal&#20272;&#35745;&#12289;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#21644;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#65292;&#24182;&#26368;&#22823;&#21270;&#20102;&#23545;&#26102;&#31354;&#29305;&#24449;&#30340;&#29702;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.13731</link><description>&lt;p&gt;
&#20351;&#29992;&#25513;&#27169;&#23398;&#20064;&#30340;Transformer&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Emotion Recognition Using Transformers with Masked Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13731
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Transformer&#36827;&#34892;&#24773;&#32490;&#35782;&#21035;&#30340;&#26032;&#26694;&#26550;&#65292;&#19987;&#27880;&#20110;Valence-Arousal&#20272;&#35745;&#12289;&#38754;&#37096;&#34920;&#24773;&#35782;&#21035;&#21644;&#21160;&#20316;&#21333;&#20803;&#26816;&#27979;&#65292;&#24182;&#26368;&#22823;&#21270;&#20102;&#23545;&#26102;&#31354;&#29305;&#24449;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36817;&#24180;&#26469;&#65292;&#28145;&#24230;&#23398;&#20064;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#21019;&#26032;&#24615;&#36827;&#23637;&#65292;&#20854;&#20013;&#21253;&#25324;&#23545;&#20154;&#31867;&#24773;&#32490;&#21644;&#34892;&#20026;&#30340;&#20998;&#26512;&#12290;&#35832;&#22914;&#37326;&#22806;&#24773;&#24863;&#34892;&#20026;&#20998;&#26512;&#65288;ABAW&#65289;&#31454;&#36187;&#31561;&#20513;&#35758;&#23588;&#20854;&#22312;&#25512;&#21160;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#26041;&#38754;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#65292;&#36890;&#36807;&#25552;&#20379;&#22810;&#26679;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25968;&#25454;&#38598;&#65292;&#33021;&#22815;&#31934;&#30830;&#35780;&#20272;&#22797;&#26434;&#24773;&#32490;&#29366;&#24577;&#12290;&#26412;&#30740;&#31350;&#21033;&#29992;&#20102;Vision Transformer&#65288;ViT&#65289;&#21644;Transformer&#27169;&#22411;&#65292;&#19987;&#27880;&#20110;&#23545;&#24773;&#32490;&#30340;&#31215;&#26497;&#24615;&#21644;&#24378;&#24230;&#65288;Valence-Arousal&#65289;&#65292;&#21508;&#31181;&#38754;&#37096;&#34920;&#24773;&#30340;&#35782;&#21035;&#20197;&#21450;&#20195;&#34920;&#22522;&#26412;&#32908;&#32905;&#36816;&#21160;&#30340;&#21160;&#20316;&#21333;&#20803;&#65288;AU&#65289;&#30340;&#26816;&#27979;&#12290;&#36825;&#31181;&#26041;&#27861;&#36229;&#36234;&#20102;&#20256;&#32479;&#30340;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#21644;&#38271;&#30701;&#26399;&#35760;&#24518;&#65288;LSTM&#65289;&#26041;&#27861;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#20840;&#26032;&#26694;&#26550;&#65292;&#26368;&#22823;&#21270;&#20102;&#23545;&#26102;&#31354;&#29305;&#24449;&#30340;&#29702;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13731v2 Announce Type: replace-cross  Abstract: In recent years, deep learning has achieved innovative advancements in various fields, including the analysis of human emotions and behaviors. Initiatives such as the Affective Behavior Analysis in-the-wild (ABAW) competition have been particularly instrumental in driving research in this area by providing diverse and challenging datasets that enable precise evaluation of complex emotional states. This study leverages the Vision Transformer (ViT) and Transformer models to focus on the estimation of Valence-Arousal (VA), which signifies the positivity and intensity of emotions, recognition of various facial expressions, and detection of Action Units (AU) representing fundamental muscle movements. This approach transcends traditional Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) based methods, proposing a new Transformer-based framework that maximizes the understanding of temporal and spatial features. Th
&lt;/p&gt;</description></item><item><title>LLM&#21551;&#29992;&#20102;&#22522;&#20110;&#31574;&#30053;&#30340;&#22810;&#27169;&#26597;&#35810;&#20248;&#21270;&#22120;&#65292;&#25670;&#33073;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#20026;&#26597;&#35810;&#20248;&#21270;&#24102;&#26469;&#20840;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.13597</link><description>&lt;p&gt;
&#19981;&#20877;&#26377;&#20248;&#21270;&#35268;&#21017;: &#22522;&#20110;LLM&#30340;&#22522;&#20110;&#31574;&#30053;&#30340;&#22810;&#27169;&#26597;&#35810;&#20248;&#21270;&#22120;&#65288;&#29256;&#26412;1&#65289;
&lt;/p&gt;
&lt;p&gt;
No more optimization rules: LLM-enabled policy-based multi-modal query optimizer (version 1)
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.13597
&lt;/p&gt;
&lt;p&gt;
LLM&#21551;&#29992;&#20102;&#22522;&#20110;&#31574;&#30053;&#30340;&#22810;&#27169;&#26597;&#35810;&#20248;&#21270;&#22120;&#65292;&#25670;&#33073;&#20102;&#20256;&#32479;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#20248;&#21270;&#26041;&#27861;&#65292;&#20026;&#26597;&#35810;&#20248;&#21270;&#24102;&#26469;&#20840;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;(LLM)&#22312;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#26631;&#24535;&#30528;&#19968;&#20010;&#37325;&#35201;&#26102;&#21051;&#12290;&#26368;&#36817;&#65292;&#20154;&#20204;&#30740;&#31350;&#20102;LLM&#22312;&#26597;&#35810;&#35268;&#21010;&#20013;&#30340;&#33021;&#21147;&#65292;&#21253;&#25324;&#21333;&#27169;&#21644;&#22810;&#27169;&#26597;&#35810;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;LLM&#30340;&#26597;&#35810;&#20248;&#21270;&#33021;&#21147;&#36824;&#27809;&#26377;&#30456;&#20851;&#30740;&#31350;&#12290;&#20316;&#20026;&#26174;&#33879;&#24433;&#21709;&#26597;&#35810;&#35745;&#21010;&#25191;&#34892;&#24615;&#33021;&#30340;&#20851;&#38190;&#27493;&#39588;&#65292;&#19981;&#24212;&#38169;&#36807;&#36825;&#31181;&#20998;&#26512;&#21644;&#23581;&#35797;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#29616;&#26377;&#30340;&#26597;&#35810;&#20248;&#21270;&#22120;&#36890;&#24120;&#26159;&#22522;&#20110;&#35268;&#21017;&#25110;&#22522;&#20110;&#35268;&#21017;+&#22522;&#20110;&#25104;&#26412;&#30340;&#65292;&#21363;&#23427;&#20204;&#20381;&#36182;&#20110;&#20154;&#24037;&#21019;&#24314;&#30340;&#35268;&#21017;&#26469;&#23436;&#25104;&#26597;&#35810;&#35745;&#21010;&#37325;&#20889;/&#36716;&#25442;&#12290;&#37492;&#20110;&#29616;&#20195;&#20248;&#21270;&#22120;&#21253;&#25324;&#25968;&#30334;&#33267;&#25968;&#21315;&#26465;&#35268;&#21017;&#65292;&#25353;&#29031;&#31867;&#20284;&#26041;&#24335;&#35774;&#35745;&#19968;&#20010;&#22810;&#27169;&#26597;&#35810;&#20248;&#21270;&#22120;&#23558;&#32791;&#36153;&#22823;&#37327;&#26102;&#38388;&#65292;&#22240;&#20026;&#25105;&#20204;&#23558;&#19981;&#24471;&#19981;&#21015;&#20030;&#23613;&#21487;&#33021;&#22810;&#30340;&#22810;&#27169;&#20248;&#21270;&#35268;&#21017;&#65292;&#32780;&#36825;&#24182;&#27809;&#26377;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.13597v1 Announce Type: cross  Abstract: Large language model (LLM) has marked a pivotal moment in the field of machine learning and deep learning. Recently its capability for query planning has been investigated, including both single-modal and multi-modal queries. However, there is no work on the query optimization capability of LLM. As a critical (or could even be the most important) step that significantly impacts the execution performance of the query plan, such analysis and attempts should not be missed. From another aspect, existing query optimizers are usually rule-based or rule-based + cost-based, i.e., they are dependent on manually created rules to complete the query plan rewrite/transformation. Given the fact that modern optimizers include hundreds to thousands of rules, designing a multi-modal query optimizer following a similar way is significantly time-consuming since we will have to enumerate as many multi-modal optimization rules as possible, which has not be
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24212;&#29992;&#29305;&#23450;&#22810;&#26680;&#26550;&#26500;&#30340;&#26032;&#22411;&#36816;&#34892;&#26102;&#22810;&#26680;&#26550;&#26500;&#27169;&#25311;&#22120;&#8220;RAVSim&#8221;&#65292;&#36890;&#36807;&#35813;&#27169;&#25311;&#22120;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#19982;&#27169;&#22411;&#20132;&#20114;&#24182;&#20462;&#25913;&#21442;&#25968;&#20540;&#38598;&#12290;</title><link>https://arxiv.org/abs/2403.12061</link><description>&lt;p&gt;
&#20351;&#29992;&#24212;&#29992;&#29305;&#23450;&#22810;&#26680;&#26550;&#26500;&#35774;&#35745;SNN&#27169;&#22411;&#30340;&#35774;&#35745;&#31354;&#38388;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Design-Space Exploration of SNN Models using Application-Specific Multi-Core Architectures
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12061
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#24212;&#29992;&#29305;&#23450;&#22810;&#26680;&#26550;&#26500;&#30340;&#26032;&#22411;&#36816;&#34892;&#26102;&#22810;&#26680;&#26550;&#26500;&#27169;&#25311;&#22120;&#8220;RAVSim&#8221;&#65292;&#36890;&#36807;&#35813;&#27169;&#25311;&#22120;&#65292;&#29992;&#25143;&#21487;&#20197;&#22312;&#25191;&#34892;&#36807;&#31243;&#20013;&#19982;&#27169;&#22411;&#20132;&#20114;&#24182;&#20462;&#25913;&#21442;&#25968;&#20540;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37492;&#20110;&#24403;&#21069;&#29702;&#35299;&#21644;&#21033;&#29992;SNN&#30340;&#28508;&#22312;&#29305;&#24615;&#23384;&#22312;&#30340;&#21160;&#26426;&#21644;&#22256;&#38590;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#8220;RAVSim&#8221;&#65288;&#36816;&#34892;&#26102;&#20998;&#26512;&#21644;&#21487;&#35270;&#21270;&#27169;&#25311;&#22120;&#65289;&#30340;&#26032;&#22411;&#36816;&#34892;&#26102;&#22810;&#26680;&#26550;&#26500;&#27169;&#25311;&#22120;&#65292;&#36825;&#26159;&#19968;&#27454;&#23574;&#31471;&#30340;SNN&#27169;&#25311;&#22120;&#65292;&#20351;&#29992;LabVIEW&#24320;&#21457;&#65292;&#24182;&#20316;&#20026;&#23448;&#26041;&#27169;&#22359;&#22312;&#20854;&#32593;&#31449;&#19978;&#20844;&#24320;&#25552;&#20379;&#12290;RAVSim&#26159;&#19968;&#31181;&#36816;&#34892;&#26102;&#34394;&#25311;&#20223;&#30495;&#29615;&#22659;&#24037;&#20855;&#65292;&#20351;&#29992;&#25143;&#33021;&#22815;&#19982;&#27169;&#22411;&#20114;&#21160;&#65292;&#35266;&#23519;&#20854;&#36755;&#20986;&#27987;&#24230;&#30340;&#34892;&#20026;&#65292;&#24182;&#22312;&#27169;&#25311;&#25191;&#34892;&#36807;&#31243;&#20013;&#38543;&#26102;&#20462;&#25913;&#21442;&#25968;&#20540;&#38598;&#12290;&#26368;&#36817;&#24050;&#32463;&#25552;&#20986;&#20102;&#19968;&#20123;&#27969;&#34892;&#30340;&#24037;&#20855;&#65292;&#20294;&#25105;&#20204;&#35748;&#20026;&#36825;&#20123;&#24037;&#20855;&#37117;&#19981;&#20801;&#35768;&#29992;&#25143;&#19982;&#27169;&#22411;&#20223;&#30495;&#36827;&#34892;&#23454;&#26102;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12061v1 Announce Type: cross  Abstract: With the motivation and the difficulties that currently exist in comprehending and utilizing the promising features of SNNs, we proposed a novel run-time multi-core architecture-based simulator called "RAVSim" (Runtime Analysis and Visualization Simulator), a cutting-edge SNN simulator, developed using LabVIEW and it is publicly available on their website as an official module. RAVSim is a runtime virtual simulation environment tool that enables the user to interact with the model, observe its behavior of output concentration, and modify the set of parametric values at any time while the simulation is in execution. Recently some popular tools have been presented, but we believe that none of the tools allow users to interact with the model simulation in run time.
&lt;/p&gt;</description></item><item><title>&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;</title><link>https://arxiv.org/abs/2403.12027</link><description>&lt;p&gt;
&#20174;&#20687;&#32032;&#21040;&#27934;&#23519;: &#22312;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#26102;&#20195;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
From Pixels to Insights: A Survey on Automatic Chart Understanding in the Era of Large Foundation Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.12027
&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20852;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#26412;&#35843;&#26597;&#35770;&#25991;&#27010;&#36848;&#20102;&#22312;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#39046;&#22495;&#30340;&#26368;&#26032;&#21457;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#21487;&#35270;&#21270;&#20197;&#22270;&#34920;&#24418;&#24335;&#22312;&#25968;&#25454;&#20998;&#26512;&#20013;&#25198;&#28436;&#30528;&#20851;&#38190;&#35282;&#33394;&#65292;&#25552;&#20379;&#20851;&#38190;&#27934;&#23519;&#24182;&#24110;&#21161;&#20570;&#20986;&#26126;&#26234;&#20915;&#31574;&#12290;&#38543;&#30528;&#36817;&#24180;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#23835;&#36215;&#65292;&#33258;&#21160;&#22270;&#34920;&#29702;&#35299;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22522;&#30784;&#27169;&#22411;&#65292;&#22914;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#38761;&#21629;&#65292;&#24182;&#36234;&#26469;&#36234;&#22810;&#22320;&#24212;&#29992;&#20110;&#22270;&#34920;&#29702;&#35299;&#20219;&#21153;&#12290;&#26412;&#35843;&#26597;&#35770;&#25991;&#20840;&#38754;&#20171;&#32461;&#20102;&#26368;&#26032;&#36827;&#23637;&#12289;&#25361;&#25112;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#25506;&#35752;&#20102;&#36825;&#20123;&#22522;&#30784;&#27169;&#22411;&#32972;&#26223;&#19979;&#22270;&#34920;&#29702;&#35299;&#30340;&#20869;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.12027v1 Announce Type: cross  Abstract: Data visualization in the form of charts plays a pivotal role in data analysis, offering critical insights and aiding in informed decision-making. Automatic chart understanding has witnessed significant advancements with the rise of large foundation models in recent years. Foundation models, such as large language models (LLMs), have revolutionized various natural language processing (NLP) tasks and are increasingly being applied to chart understanding tasks. This survey paper provides a comprehensive overview of the recent developments, challenges, and future directions in chart understanding within the context of these foundation models. The paper begins by defining chart understanding, outlining problem formulations, and discussing fundamental building blocks crucial for studying chart understanding tasks. In the section on tasks and datasets, we explore various tasks within chart understanding and discuss their evaluation metrics a
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;Guide-Align&#65292;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21046;&#23450;&#29305;&#23450;&#25351;&#21335;&#65292;&#20174;&#32780;&#24314;&#31435;&#20840;&#38754;&#30340;&#25351;&#23548;&#24211;&#65292;&#29992;&#20110;&#25351;&#23548;LLMs&#29983;&#25104;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#12290;</title><link>https://arxiv.org/abs/2403.11838</link><description>&lt;p&gt;
&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#65306;&#38754;&#21521;&#35821;&#35328;&#27169;&#22411;&#30340;&#25351;&#21335;&#24211;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Ensuring Safe and High-Quality Outputs: A Guideline Library Approach for Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11838
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;Guide-Align&#65292;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#65292;&#36890;&#36807;&#23433;&#20840;&#35757;&#32451;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#21046;&#23450;&#29305;&#23450;&#25351;&#21335;&#65292;&#20174;&#32780;&#24314;&#31435;&#20840;&#38754;&#30340;&#25351;&#23548;&#24211;&#65292;&#29992;&#20110;&#25351;&#23548;LLMs&#29983;&#25104;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#20063;&#23384;&#22312;&#20559;&#35265;&#20869;&#23481;&#29983;&#25104;&#21644;&#38544;&#31169;&#38382;&#39064;&#31561;&#39118;&#38505;&#12290;&#24403;&#21069;&#30340;&#23545;&#40784;&#25216;&#26415;&#20043;&#19968;&#21253;&#25324;&#22522;&#20110;&#21407;&#21017;&#30340;&#38598;&#25104;&#65292;&#20294;&#38754;&#20020;&#30001;&#20110;&#25163;&#24037;&#21046;&#23450;&#35268;&#21017;&#30340;&#19981;&#31934;&#30830;&#24615;&#21644;&#26410;&#32463;&#23433;&#20840;&#35757;&#32451;&#30340;&#27169;&#22411;&#23545;&#39118;&#38505;&#24863;&#30693;&#19981;&#36275;&#32780;&#20135;&#29983;&#30340;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Guide-Align&#65292;&#36825;&#26159;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#12290;&#26368;&#21021;&#65292;&#19968;&#20010;&#32463;&#36807;&#23433;&#20840;&#35757;&#32451;&#30340;&#27169;&#22411;&#35782;&#21035;&#28508;&#22312;&#39118;&#38505;&#65292;&#24182;&#20026;&#21508;&#31181;&#36755;&#20837;&#21046;&#23450;&#20855;&#20307;&#25351;&#21335;&#65292;&#20174;&#32780;&#24314;&#31435;&#20102;&#20840;&#38754;&#30340;&#25351;&#21335;&#24211;&#21644;&#29992;&#20110;&#36755;&#20837;&#25351;&#21335;&#26816;&#32034;&#30340;&#27169;&#22411;&#12290;&#38543;&#21518;&#65292;&#26816;&#32034;&#27169;&#22411;&#23558;&#26032;&#36755;&#20837;&#19982;&#30456;&#20851;&#25351;&#21335;&#30456;&#20851;&#32852;&#65292;&#24341;&#23548;LLMs&#22312;&#21709;&#24212;&#29983;&#25104;&#20013;&#30830;&#20445;&#23433;&#20840;&#21644;&#39640;&#36136;&#37327;&#36755;&#20986;&#65292;&#20174;&#32780;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#19968;&#33268;&#12290;&#21478;&#19968;&#20010;&#39069;&#22806;&#21487;&#36873;&#38454;&#27573;&#28041;&#21450;&#20351;&#29992;&#32463;&#36807;&#32454;&#33268;&#23545;&#40784;&#30340;&#26032;&#25968;&#25454;&#38598;&#23545;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11838v1 Announce Type: cross  Abstract: Large Language Models (LLMs) exhibit impressive capabilities but also present risks such as biased content generation and privacy issues. One of the current alignment techniques includes principle-driven integration, but it faces challenges arising from the imprecision of manually crafted rules and inadequate risk perception in models without safety training. To address these, we introduce Guide-Align, a two-stage approach. Initially, a safety-trained model identifies potential risks and formulates specific guidelines for various inputs, thereby establishing a comprehensive library of guidelines and models for input-guidelines retrieval. Subsequently, the retrieval model correlates new inputs with pertinent guidelines, guiding LLMs in response generation to ensure safe and high-quality outputs, thus aligning with human values. An additional optional stage involves fine-tuning a model with new well-aligned datasets generated through the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;Scene-LLM&#65292;&#19968;&#31181;3D&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#20248;&#21183;&#65292;&#22686;&#24378;&#20102;&#22312;&#20132;&#20114;&#24335;3D&#23460;&#20869;&#29615;&#22659;&#20013;&#20855;&#36523;&#20307;&#23384;&#22312;&#30340;&#20195;&#29702;&#32773;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2403.11401</link><description>&lt;p&gt;
Scene-LLM&#65306;&#25193;&#23637;&#29992;&#20110;3D&#35270;&#35273;&#29702;&#35299;&#21644;&#25512;&#29702;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scene-LLM: Extending Language Model for 3D Visual Understanding and Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11401
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Scene-LLM&#65292;&#19968;&#31181;3D&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#20248;&#21183;&#65292;&#22686;&#24378;&#20102;&#22312;&#20132;&#20114;&#24335;3D&#23460;&#20869;&#29615;&#22659;&#20013;&#20855;&#36523;&#20307;&#23384;&#22312;&#30340;&#20195;&#29702;&#32773;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Scene-LLM&#65292;&#36825;&#26159;&#19968;&#31181;3D&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25972;&#21512;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#25512;&#29702;&#20248;&#21183;&#65292;&#22686;&#24378;&#20102;&#22312;&#20132;&#20114;&#24335;3D&#23460;&#20869;&#29615;&#22659;&#20013;&#20855;&#36523;&#20307;&#23384;&#22312;&#30340;&#20195;&#29702;&#32773;&#33021;&#21147;&#12290;Scene-LLM&#37319;&#29992;&#20102;&#28151;&#21512;&#30340;3D&#35270;&#35273;&#29305;&#24449;&#34920;&#31034;&#65292;&#21253;&#25324;&#20102;&#23494;&#38598;&#30340;&#31354;&#38388;&#20449;&#24687;&#65292;&#24182;&#25903;&#25345;&#22330;&#26223;&#29366;&#24577;&#30340;&#26356;&#26032;&#12290;&#35813;&#27169;&#22411;&#20351;&#29992;&#25237;&#24433;&#23618;&#23558;&#36825;&#20123;&#29305;&#24449;&#39640;&#25928;&#22320;&#25237;&#24433;&#21040;&#39044;&#35757;&#32451;&#30340;&#25991;&#26412;&#23884;&#20837;&#31354;&#38388;&#20013;&#65292;&#20174;&#32780;&#26377;&#25928;&#35299;&#37322;3D&#35270;&#35273;&#20449;&#24687;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#29420;&#29305;&#20043;&#22788;&#22312;&#20110;&#25972;&#21512;&#20102;&#22330;&#26223;&#32423;&#21644;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;3D&#20449;&#24687;&#12290;&#36825;&#31181;&#32452;&#21512;&#23545;&#20110;&#20132;&#20114;&#24335;&#35268;&#21010;&#33267;&#20851;&#37325;&#35201;&#65292;&#20854;&#20013;&#22330;&#26223;&#32423;&#25968;&#25454;&#25903;&#25345;&#20840;&#23616;&#35268;&#21010;&#65292;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;&#25968;&#25454;&#23545;&#20110;&#23450;&#20301;&#33267;&#20851;&#37325;&#35201;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#20351;&#29992;&#20197;&#33258;&#25105;&#20026;&#20013;&#24515;&#30340;3D&#24103;&#29305;&#24449;&#36827;&#34892;&#29305;&#24449;&#23545;&#40784;&#65292;&#36825;&#26159;&#19968;&#31181;&#22686;&#24378;&#27169;&#22411;&#23545;&#23567;&#29289;&#20307;&#29305;&#24449;&#23545;&#40784;&#33021;&#21147;&#30340;&#26377;&#25928;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11401v1 Announce Type: cross  Abstract: This paper introduces Scene-LLM, a 3D-visual-language model that enhances embodied agents' abilities in interactive 3D indoor environments by integrating the reasoning strengths of Large Language Models (LLMs). Scene-LLM adopts a hybrid 3D visual feature representation, that incorporates dense spatial information and supports scene state updates. The model employs a projection layer to efficiently project these features in the pre-trained textual embedding space, enabling effective interpretation of 3D visual information. Unique to our approach is the integration of both scene-level and ego-centric 3D information. This combination is pivotal for interactive planning, where scene-level data supports global planning and ego-centric data is important for localization. Notably, we use ego-centric 3D frame features for feature alignment, an efficient technique that enhances the model's ability to align features of small objects within the s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;&#29992;&#25143;&#35831;&#27714;&#20998;&#37197;&#32473;&#26381;&#21153;&#22120;&#65292;&#20197;&#35299;&#20915;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#24212;&#29992;&#37096;&#32626;&#38382;&#39064;&#30340;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;</title><link>https://arxiv.org/abs/2403.11259</link><description>&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#24212;&#29992;&#37096;&#32626;&#38382;&#39064;&#30340;&#22522;&#20110;&#23398;&#20064;&#30340;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
A learning-based solution approach to the application placement problem in mobile edge computing under uncertainty
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.11259
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#23558;&#29992;&#25143;&#35831;&#27714;&#20998;&#37197;&#32473;&#26381;&#21153;&#22120;&#65292;&#20197;&#35299;&#20915;&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#20013;&#24212;&#29992;&#37096;&#32626;&#38382;&#39064;&#30340;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#26381;&#21153;&#22120;&#20013;&#25918;&#32622;&#24212;&#29992;&#31243;&#24207;&#26159;&#19968;&#20010;&#22797;&#26434;&#30340;&#25361;&#25112;&#65292;&#28041;&#21450;&#35768;&#22810;&#26381;&#21153;&#22120;&#12289;&#29992;&#25143;&#21450;&#20854;&#35831;&#27714;&#12290;&#29616;&#26377;&#31639;&#27861;&#38656;&#35201;&#24456;&#38271;&#26102;&#38388;&#35299;&#20915;&#20855;&#26377;&#37325;&#22823;&#19981;&#30830;&#23450;&#24615;&#24773;&#26223;&#30340;&#39640;&#32500;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#38656;&#35201;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#26368;&#22823;&#21270;&#26381;&#21153;&#36136;&#37327;&#65292;&#21516;&#26102;&#32771;&#34385;&#25152;&#26377;&#25216;&#26415;&#32422;&#26463;&#12290;&#20854;&#20013;&#19968;&#31181;&#26041;&#27861;&#26159;&#26426;&#22120;&#23398;&#20064;&#65292;&#23427;&#27169;&#25311;&#20102;&#22312;&#36793;&#32536;&#26381;&#21153;&#22120;&#20013;&#37096;&#32626;&#24212;&#29992;&#31243;&#24207;&#30340;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#12290;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#39044;&#35745;&#23558;&#23398;&#20064;&#22914;&#20309;&#26681;&#25454;&#29992;&#25143;&#21644;&#26381;&#21153;&#22120;&#30340;&#31354;&#38388;&#20301;&#32622;&#23558;&#29992;&#25143;&#35831;&#27714;&#20998;&#37197;&#32473;&#26381;&#21153;&#22120;&#12290;&#26412;&#30740;&#31350;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#20108;&#38454;&#27573;&#38543;&#26426;&#35268;&#21010;&#12290;&#36890;&#36807;&#21464;&#21270;&#21442;&#25968;&#22914;&#29992;&#25143;&#20301;&#32622;&#12289;&#35831;&#27714;&#36895;&#29575;&#21644;&#35299;&#20915;&#20248;&#21270;&#27169;&#22411;&#29983;&#25104;&#36275;&#22815;&#25968;&#37327;&#30340;&#35757;&#32451;&#35760;&#24405;&#12290;&#28982;&#21518;&#65292;&#22522;&#20110;&#27599;&#20010;&#29992;&#25143;&#36317;&#31163;&#21487;&#29992;&#26381;&#21153;&#22120;&#30340;&#36317;&#31163;&#29305;&#24449;&#65292;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.11259v1 Announce Type: cross  Abstract: Placing applications in mobile edge computing servers presents a complex challenge involving many servers, users, and their requests. Existing algorithms take a long time to solve high-dimensional problems with significant uncertainty scenarios. Therefore, an efficient approach is required to maximize the quality of service while considering all technical constraints. One of these approaches is machine learning, which emulates optimal solutions for application placement in edge servers. Machine learning models are expected to learn how to allocate user requests to servers based on the spatial positions of users and servers. In this study, the problem is formulated as a two-stage stochastic programming. A sufficient amount of training records is generated by varying parameters such as user locations, their request rates, and solving the optimization model. Then, based on the distance features of each user from the available servers and 
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;</title><link>https://arxiv.org/abs/2403.09863</link><description>&lt;p&gt;
&#19968;&#20010;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Conceptual Framework For White Box Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09863
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#23436;&#20840;&#21487;&#35299;&#37322;&#30340;&#31070;&#32463;&#32593;&#32476;&#23618;&#65292;&#20026;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#30340;&#33539;&#24335;&#36716;&#21464;&#24320;&#36767;&#20102;&#26032;&#30340;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24341;&#20837;&#35821;&#20041;&#29305;&#24449;&#20316;&#20026;&#23436;&#20840;&#21487;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#23618;&#30340;&#36890;&#29992;&#27010;&#24565;&#26694;&#26550;&#12290;&#19968;&#20010;&#20805;&#20998;&#21160;&#26426;&#30340;MNIST&#30456;&#20851;&#23376;&#38382;&#39064;&#30340;&#27010;&#24565;&#39564;&#35777;&#27169;&#22411;&#21253;&#25324;4&#20010;&#36825;&#26679;&#30340;&#23618;&#65292;&#24635;&#20849;4800&#20010;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#35813;&#27169;&#22411;&#26131;&#20110;&#35299;&#37322;&#65292;&#26080;&#38656;&#20219;&#20309;&#24418;&#24335;&#30340;&#23545;&#25239;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#23545;&#25239;&#27979;&#35797;&#20934;&#30830;&#29575;&#65292;&#38656;&#35201;&#36739;&#23569;&#30340;&#36229;&#21442;&#25968;&#35843;&#33410;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#21333;&#20010;CPU&#19978;&#24555;&#36895;&#35757;&#32451;&#12290;&#35813;&#25216;&#26415;&#30340;&#36890;&#29992;&#24615;&#25215;&#35834;&#20026;&#24443;&#24213;&#27665;&#20027;&#21270;&#21644;&#30495;&#27491;&#36890;&#29992;&#30340;&#30333;&#30418;&#31070;&#32463;&#32593;&#32476;&#24102;&#26469;&#20102;&#24076;&#26395;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/314-Foundation/white-box-nn&#25214;&#21040;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09863v1 Announce Type: cross  Abstract: This paper introduces semantic features as a general conceptual framework for fully explainable neural network layers. A well-motivated proof of concept model for relevant subproblem of MNIST consists of 4 such layers with the total of 4.8K learnable parameters. The model is easily interpretable, achieves human-level adversarial test accuracy with no form of adversarial training, requires little hyperparameter tuning and can be quickly trained on a single CPU. The general nature of the technique bears promise for a paradigm shift towards radically democratised and truly generalizable white box neural networks. The code is available at https://github.com/314-Foundation/white-box-nn
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;&#65288;MCA&#65289;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#35266;&#21464;&#21270;&#26469;&#40723;&#21169;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#35270;&#39057;&#20013;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#38745;&#24577;&#22806;&#35266;&#12290;</title><link>https://arxiv.org/abs/2403.09506</link><description>&lt;p&gt;
&#19981;&#35201;&#20197;&#22806;&#34920;&#21028;&#26029;: &#29992;&#20110;&#35270;&#39057;&#35782;&#21035;&#30340;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;
&lt;/p&gt;
&lt;p&gt;
Don't Judge by the Look: A Motion Coherent Augmentation for Video Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.09506
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;&#65288;MCA&#65289;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22806;&#35266;&#21464;&#21270;&#26469;&#40723;&#21169;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#35270;&#39057;&#20013;&#30340;&#36816;&#21160;&#20449;&#24687;&#65292;&#32780;&#19981;&#26159;&#38745;&#24577;&#22806;&#35266;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30446;&#26631;&#35782;&#21035;&#20013;&#30340;&#35757;&#32451;&#27969;&#31243;&#22312;&#25968;&#25454;&#22686;&#24378;&#26102;&#24573;&#30053;&#20102;&#33394;&#35843;&#25238;&#21160;&#65292;&#22240;&#20026;&#23427;&#19981;&#20165;&#20250;&#24102;&#26469;&#23545;&#20998;&#31867;&#26377;&#23475;&#30340;&#22806;&#35266;&#21464;&#21270;&#65292;&#32780;&#19988;&#22312;&#23454;&#36341;&#20013;&#23454;&#29616;&#20063;&#26159;&#20302;&#25928;&#30340;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#33394;&#35843;&#21464;&#21270;&#22312;&#35270;&#39057;&#35782;&#21035;&#20013;&#30340;&#24433;&#21709;&#65292;&#24182;&#21457;&#29616;&#36825;&#31181;&#21464;&#21270;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#23545;&#20110;&#21253;&#21547;&#36816;&#21160;&#20449;&#24687;&#30340;&#35270;&#39057;&#26469;&#35828;&#65292;&#38745;&#24577;&#22806;&#35266;&#19981;&#26159;&#37027;&#20040;&#37325;&#35201;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#23519;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#35270;&#39057;&#35782;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21517;&#20026;&#36816;&#21160;&#19968;&#33268;&#22686;&#24378;&#65288;MCA&#65289;&#65292;&#23427;&#22312;&#35270;&#39057;&#20013;&#24341;&#20837;&#22806;&#35266;&#21464;&#21270;&#65292;&#38544;&#24335;&#40723;&#21169;&#27169;&#22411;&#20248;&#20808;&#32771;&#34385;&#36816;&#21160;&#27169;&#24335;&#65292;&#32780;&#19981;&#26159;&#38745;&#24577;&#22806;&#35266;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SwapMix&#30340;&#25805;&#20316;&#65292;&#29992;&#20110;&#39640;&#25928;&#20462;&#25913;&#35270;&#39057;&#26679;&#26412;&#30340;&#22806;&#35266;&#65292;&#24182;&#24341;&#20837;&#20102;&#21464;&#24322;&#23545;&#40784;&#65288;VA&#65289;&#26469;&#35299;&#20915;SwapMix&#24341;&#36215;&#30340;&#20998;&#24067;&#20559;&#31227;&#65292;&#36843;&#20351;&#27169;&#22411;&#21435;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.09506v1 Announce Type: cross  Abstract: Current training pipelines in object recognition neglect Hue Jittering when doing data augmentation as it not only brings appearance changes that are detrimental to classification, but also the implementation is inefficient in practice. In this study, we investigate the effect of hue variance in the context of video recognition and find this variance to be beneficial since static appearances are less important in videos that contain motion information. Based on this observation, we propose a data augmentation method for video recognition, named Motion Coherent Augmentation (MCA), that introduces appearance variation in videos and implicitly encourages the model to prioritize motion patterns, rather than static appearances. Concretely, we propose an operation SwapMix to efficiently modify the appearance of video samples, and introduce Variation Alignment (VA) to resolve the distribution shift caused by SwapMix, enforcing the model to le
&lt;/p&gt;</description></item><item><title>FastV&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#24182;&#22312;&#21518;&#32493;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;</title><link>https://arxiv.org/abs/2403.06764</link><description>&lt;p&gt;
&#19968;&#24352;&#22270;&#29255;&#22312;&#31532;&#20108;&#23618;&#20043;&#21518;&#20215;&#20540;1/2&#20195;&#24065;&#65306;&#38024;&#23545;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#25512;&#29702;&#21152;&#36895;
&lt;/p&gt;
&lt;p&gt;
An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06764
&lt;/p&gt;
&lt;p&gt;
FastV&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#24182;&#22312;&#21518;&#32493;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#65292;&#26497;&#22823;&#22320;&#38477;&#20302;&#20102;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#22312;&#21508;&#31181;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#19981;&#25439;&#22833;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#22823;&#35268;&#27169;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;LVLMs&#65289;&#20013;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#23384;&#22312;&#20302;&#25928;&#29616;&#35937;&#65292;&#23588;&#20854;&#26159;&#22312;&#30693;&#21517;&#27169;&#22411;&#22914;LLaVA-1.5&#12289;QwenVL-Chat&#21644;Video-LLaVA&#20013;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#27969;&#34892;&#30340;LVLMs&#30340;&#28145;&#23618;&#20013;&#65292;&#23545;&#35270;&#35273;&#20195;&#24065;&#30340;&#27880;&#24847;&#21147;&#35745;&#31639;&#26497;&#20854;&#20302;&#25928;&#65292;&#26263;&#31034;&#30456;&#36739;&#20110;&#22788;&#29702;&#25991;&#26412;&#25968;&#25454;&#65292;&#38656;&#35201;&#26356;&#31232;&#30095;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;FastV&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#21151;&#33021;&#21363;&#25554;&#21363;&#29992;&#26041;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#26089;&#26399;&#23618;&#20013;&#30340;&#33258;&#36866;&#24212;&#27880;&#24847;&#21147;&#27169;&#24335;&#21644;&#22312;&#38543;&#21518;&#23618;&#20013;&#20462;&#21098;&#35270;&#35273;&#20195;&#24065;&#26469;&#20248;&#21270;&#35745;&#31639;&#25928;&#29575;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#34920;&#26126;FastV&#33021;&#22815;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#65288;&#20363;&#22914;&#65292;&#23545;&#20110;LLaVA-1.5-13B&#30340;FLOP&#20943;&#23569;&#20102;45%&#65289;&#65292;&#32780;&#19981;&#20250;&#22312;&#24191;&#27867;&#30340;&#22270;&#20687;&#21644;&#35270;&#39057;&#29702;&#35299;&#20219;&#21153;&#20013;&#29306;&#29298;&#24615;&#33021;&#12290;FastV&#30340;&#35745;&#31639;&#25928;&#29575;&#21644;&#24615;&#33021;&#26435;&#34913;&#26159;&#39640;&#24230;&#21487;&#23450;&#21046;&#30340;&#65292;&#24182;&#19988;&#26159;&#24085;&#32047;&#25176;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06764v1 Announce Type: cross  Abstract: In this study, we identify the inefficient attention phenomena in Large Vision-Language Models (LVLMs), notably within prominent models like LLaVA-1.5, QwenVL-Chat and Video-LLaVA. We find out that the attention computation over visual tokens is of extreme inefficiency in the deep layers of popular LVLMs, suggesting a need for a sparser approach compared to textual data handling. To this end, we introduce FastV, a versatile plug-and-play method designed to optimize computational efficiency by learning adaptive attention patterns in early layers and pruning visual tokens in subsequent ones. Our evaluations demonstrate FastV's ability to dramatically reduce computational costs (e.g., a 45 reduction in FLOPs for LLaVA-1.5-13B) without sacrificing performance in a wide range of image and video understanding tasks. The computational efficiency and performance trade-off of FastV are highly customizable and pareto-efficient. It can compress t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;</title><link>https://arxiv.org/abs/2403.06725</link><description>&lt;p&gt;
&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#37325;&#35201;&#24615;&#26426;&#21046;&#24494;&#35843;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Improving Low-Resource Knowledge Tracing Tasks by Supervised Pre-training and Importance Mechanism Fine-tuning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06725
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#26694;&#26550;&#65292;&#36890;&#36807;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#37325;&#35201;&#24615;&#26426;&#21046;&#65292;&#26088;&#22312;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#26469;&#25913;&#36827;&#20302;&#36164;&#28304;&#30693;&#35782;&#36861;&#36394;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#36861;&#36394;&#65288;KT&#65289;&#26088;&#22312;&#22522;&#20110;&#23398;&#29983;&#30340;&#21382;&#21490;&#20114;&#21160;&#26469;&#20272;&#35745;&#20182;&#20204;&#30340;&#30693;&#35782;&#25484;&#25569;&#31243;&#24230;&#12290;&#26368;&#36817;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;KT&#65288;DLKT&#65289;&#26041;&#27861;&#22312;KT&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21508;&#31181;&#21407;&#22240;&#65292;&#22914;&#39044;&#31639;&#38480;&#21046;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#35768;&#22810;&#23454;&#38469;&#22330;&#26223;&#20013;&#35266;&#23519;&#21040;&#30340;&#20114;&#21160;&#38750;&#24120;&#26377;&#38480;&#65292;&#21363;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#12290;&#30452;&#25509;&#22312;&#20302;&#36164;&#28304;KT&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;DLKT&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#36807;&#25311;&#21512;&#65292;&#24182;&#19988;&#24456;&#38590;&#36873;&#25321;&#36866;&#24403;&#30340;&#28145;&#24230;&#31070;&#32463;&#26550;&#26500;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;LoReKT&#30340;&#20302;&#36164;&#28304;KT&#26694;&#26550;&#26469;&#24212;&#23545;&#19978;&#36848;&#25361;&#25112;&#12290;&#21463;&#30427;&#34892;&#30340;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#33539;&#24335;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#26088;&#22312;&#22312;&#39044;&#35757;&#32451;&#38454;&#27573;&#20174;&#20016;&#23500;&#36164;&#28304;&#30340;KT&#25968;&#25454;&#38598;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#21442;&#25968;&#21644;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06725v1 Announce Type: cross  Abstract: Knowledge tracing (KT) aims to estimate student's knowledge mastery based on their historical interactions. Recently, the deep learning based KT (DLKT) approaches have achieved impressive performance in the KT task. These DLKT models heavily rely on the large number of available student interactions. However, due to various reasons such as budget constraints and privacy concerns, observed interactions are very limited in many real-world scenarios, a.k.a, low-resource KT datasets. Directly training a DLKT model on a low-resource KT dataset may lead to overfitting and it is difficult to choose the appropriate deep neural architecture. Therefore, in this paper, we propose a low-resource KT framework called LoReKT to address above challenges. Inspired by the prevalent "pre-training and fine-tuning" paradigm, we aim to learn transferable parameters and representations from rich-resource KT datasets during the pre-training stage and subseque
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;</title><link>https://arxiv.org/abs/2403.06054</link><description>&lt;p&gt;
&#20855;&#26377;&#25193;&#25955;&#20928;&#21270;&#30340;&#20998;&#31163;&#25968;&#25454;&#19968;&#33268;&#24615;&#30340;&#22270;&#20687;&#24674;&#22797;
&lt;/p&gt;
&lt;p&gt;
Decoupled Data Consistency with Diffusion Purification for Image Restoration
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.06054
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#31163;&#21453;&#21521;&#36807;&#31243;&#21644;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#25193;&#25955;&#27169;&#22411;&#20316;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#28145;&#24230;&#29983;&#25104;&#20808;&#39564;&#31867;&#21035;&#24050;&#32463;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#20851;&#27880;&#65292;&#30001;&#20110;&#20854;&#20986;&#33394;&#22320;&#24314;&#27169;&#25968;&#25454;&#20998;&#24067;&#30340;&#33021;&#21147;&#65292;&#22312;&#21508;&#31181;&#22270;&#20687;&#24674;&#22797;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;&#20026;&#20102;&#35299;&#20915;&#22270;&#20687;&#24674;&#22797;&#38382;&#39064;&#65292;&#35768;&#22810;&#29616;&#26377;&#25216;&#26415;&#36890;&#36807;&#23558;&#39069;&#22806;&#30340;&#20284;&#28982;&#26799;&#24230;&#27493;&#39588;&#32435;&#20837;&#21040;&#25193;&#25955;&#27169;&#22411;&#30340;&#21453;&#21521;&#37319;&#26679;&#36807;&#31243;&#20013;&#26469;&#23454;&#29616;&#25968;&#25454;&#19968;&#33268;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#26799;&#24230;&#27493;&#39588;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#20013;&#23384;&#22312;&#25361;&#25112;&#65292;&#22240;&#20026;&#23427;&#20204;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#35745;&#31639;&#24320;&#38144;&#65292;&#20174;&#32780;&#22686;&#21152;&#20102;&#25512;&#29702;&#26102;&#38388;&#12290;&#24403;&#20351;&#29992;&#21152;&#36895;&#30340;&#25193;&#25955;&#27169;&#22411;&#37319;&#26679;&#22120;&#26102;&#65292;&#36825;&#20123;&#39069;&#22806;&#30340;&#27493;&#39588;&#36824;&#20250;&#23548;&#33268;&#39069;&#22806;&#30340;&#22256;&#38590;&#65292;&#22240;&#20026;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#30340;&#25968;&#37327;&#21463;&#38480;&#20110;&#21453;&#21521;&#37319;&#26679;&#27493;&#39588;&#30340;&#25968;&#37327;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25193;&#25955;&#30340;&#22270;&#20687;&#24674;&#22797;&#27714;&#35299;&#22120;&#65292;&#36890;&#36807;&#23558;&#21453;&#21521;&#36807;&#31243;&#19982;&#25968;&#25454;&#19968;&#33268;&#24615;&#27493;&#39588;&#20998;&#31163;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#28041;&#21450;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.06054v1 Announce Type: cross  Abstract: Diffusion models have recently gained traction as a powerful class of deep generative priors, excelling in a wide range of image restoration tasks due to their exceptional ability to model data distributions. To solve image restoration problems, many existing techniques achieve data consistency by incorporating additional likelihood gradient steps into the reverse sampling process of diffusion models. However, the additional gradient steps pose a challenge for real-world practical applications as they incur a large computational overhead, thereby increasing inference time. They also present additional difficulties when using accelerated diffusion model samplers, as the number of data consistency steps is limited by the number of reverse sampling steps. In this work, we propose a novel diffusion-based image restoration solver that addresses these issues by decoupling the reverse process from the data consistency steps. Our method involv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;</title><link>https://arxiv.org/abs/2403.05030</link><description>&lt;p&gt;
&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#38450;&#24481;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Defending Against Unforeseen Failure Modes with Latent Adversarial Training
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.05030
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;AI&#31995;&#32479;&#20013;&#26410;&#39044;&#35265;&#30340;&#25925;&#38556;&#27169;&#24335;&#65292;&#36890;&#36807;&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#65292;&#26377;&#25928;&#28165;&#38500;&#20102;&#24694;&#24847;&#36719;&#20214;&#21644;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26377;&#26102;&#22312;&#37096;&#32626;&#21518;&#20250;&#23637;&#31034;&#20986;&#26377;&#23475;&#30340;&#24847;&#22806;&#34892;&#20026;&#12290;&#23613;&#31649;&#24320;&#21457;&#20154;&#21592;&#36827;&#34892;&#20102;&#22823;&#37327;&#35786;&#26029;&#21644;&#35843;&#35797;&#65292;&#36825;&#31181;&#24773;&#20917;&#32463;&#24120;&#21457;&#29983;&#12290;&#30001;&#20110;&#25915;&#20987;&#38754;&#38750;&#24120;&#24191;&#27867;&#65292;&#20174;&#27169;&#22411;&#20013;&#20943;&#23569;&#39118;&#38505;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#32791;&#23613;&#22320;&#25628;&#32034;&#21487;&#33021;&#23548;&#33268;&#27169;&#22411;&#22833;&#36133;&#30340;&#36755;&#20837;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#32418;&#38431;&#21644;&#23545;&#25239;&#35757;&#32451;&#65288;AT&#65289;&#36890;&#24120;&#29992;&#20110;&#20351;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#26356;&#21152;&#20581;&#22766;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#24182;&#19981;&#36275;&#20197;&#36991;&#20813;&#35768;&#22810;&#19982;&#23545;&#25239;&#35757;&#32451;&#19981;&#21516;&#30340;&#30495;&#23454;&#19990;&#30028;&#25925;&#38556;&#27169;&#24335;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#28508;&#22312;&#23545;&#25239;&#35757;&#32451;&#65288;LAT&#65289;&#26469;&#38450;&#24481;&#28431;&#27934;&#65292;&#32780;&#26080;&#38656;&#29983;&#25104;&#24341;&#21457;&#36825;&#20123;&#28431;&#27934;&#30340;&#36755;&#20837;&#12290;LAT&#21033;&#29992;&#32593;&#32476;&#23454;&#38469;&#29992;&#20110;&#39044;&#27979;&#30340;&#21387;&#32553;&#12289;&#25277;&#35937;&#21644;&#32467;&#26500;&#21270;&#27010;&#24565;&#30340;&#28508;&#22312;&#34920;&#31034;&#12290;&#25105;&#20204;&#20351;&#29992;LAT&#26469;&#28165;&#38500;&#24694;&#24847;&#36719;&#20214;&#24182;&#38450;&#24481;&#38024;&#23545;&#20445;&#30041;&#31867;&#21035;&#30340;&#23545;&#25239;&#24615;&#25915;&#20987;&#12290;&#25105;&#20204;&#23637;&#31034;&#22312;&#22270;&#20687;&#20998;&#31867;&#12289;&#25991;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.05030v1 Announce Type: cross  Abstract: AI systems sometimes exhibit harmful unintended behaviors post-deployment. This is often despite extensive diagnostics and debugging by developers. Minimizing risks from models is challenging because the attack surface is so large. It is not tractable to exhaustively search for inputs that may cause a model to fail. Red-teaming and adversarial training (AT) are commonly used to make AI systems more robust. However, they have not been sufficient to avoid many real-world failure modes that differ from the ones adversarially trained on. In this work, we utilize latent adversarial training (LAT) to defend against vulnerabilities without generating inputs that elicit them. LAT leverages the compressed, abstract, and structured latent representations of concepts that the network actually uses for prediction. We use LAT to remove trojans and defend against held-out classes of adversarial attacks. We show in image classification, text classifi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#22312;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#20984;&#24615;&#26102;&#31283;&#20581;&#22320;&#25910;&#25947;&#33267;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;</title><link>https://arxiv.org/abs/2403.04810</link><description>&lt;p&gt;
&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Restricted Bayesian Neural Network
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04810
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#38480;&#21046;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#26550;&#26500;&#65292;&#26174;&#33879;&#20943;&#23569;&#20102;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#22312;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#20984;&#24615;&#26102;&#31283;&#20581;&#22320;&#25910;&#25947;&#33267;&#20840;&#23616;&#26368;&#20248;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29616;&#20195;&#28145;&#24230;&#23398;&#20064;&#24037;&#20855;&#22312;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#26041;&#38754;&#38750;&#24120;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20316;&#20026;&#40657;&#30418;&#27169;&#22411;&#30340;&#36816;&#34892;&#26041;&#24335;&#22686;&#21152;&#20102;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20204;&#38754;&#20020;&#30528;&#21508;&#31181;&#25361;&#25112;&#65292;&#21253;&#25324;&#22312;&#22823;&#22411;&#32593;&#32476;&#20013;&#38656;&#35201;&#22823;&#37327;&#23384;&#20648;&#31354;&#38388;&#12289;&#36807;&#25311;&#21512;&#12289;&#27424;&#25311;&#21512;&#12289;&#26799;&#24230;&#28040;&#22833;&#31561;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30340;&#27010;&#24565;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33021;&#22815;&#26174;&#33879;&#20943;&#23569;&#32593;&#32476;&#23384;&#20648;&#31354;&#38388;&#22797;&#26434;&#24615;&#30340;&#26032;&#22411;&#26550;&#26500;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#33021;&#22815;&#26377;&#25928;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#30340;&#31639;&#27861;&#65292;&#30830;&#20445;&#31283;&#20581;&#30340;&#25910;&#25947;&#20540;&#65292;&#36991;&#20813;&#38519;&#20837;&#23616;&#37096;&#26368;&#20248;&#35299;&#65292;&#23588;&#20854;&#26159;&#24403;&#30446;&#26631;&#20989;&#25968;&#32570;&#20047;&#23436;&#32654;&#30340;&#20984;&#24615;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04810v1 Announce Type: cross  Abstract: Modern deep learning tools are remarkably effective in addressing intricate problems. However, their operation as black-box models introduces increased uncertainty in predictions. Additionally, they contend with various challenges, including the need for substantial storage space in large networks, issues of overfitting, underfitting, vanishing gradients, and more. This study explores the concept of Bayesian Neural Networks, presenting a novel architecture designed to significantly alleviate the storage space complexity of a network. Furthermore, we introduce an algorithm adept at efficiently handling uncertainties, ensuring robust convergence values without becoming trapped in local optima, particularly when the objective function lacks perfect convexity.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#26041;&#27861;&#65292;&#24110;&#21161;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#65292;&#36890;&#36807;&#26500;&#25104;&#35201;&#32032;&#21644;&#20851;&#38190;&#35789;&#36873;&#25321;&#36827;&#34892;&#21028;&#26029;&#12290;</title><link>https://arxiv.org/abs/2403.04369</link><description>&lt;p&gt;
&#20174;&#22270;&#21040;&#35789;&#34955;: &#23558;&#39046;&#22495;&#30693;&#35782;&#24341;&#20837;&#28151;&#28102;&#32618;&#21517;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
From Graph to Word Bag: Introducing Domain Knowledge to Confusing Charge Prediction
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.04369
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#39046;&#22495;&#30693;&#35782;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#26041;&#27861;&#65292;&#24110;&#21161;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#65292;&#36890;&#36807;&#26500;&#25104;&#35201;&#32032;&#21644;&#20851;&#38190;&#35789;&#36873;&#25321;&#36827;&#34892;&#21028;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#28102;&#32618;&#21517;&#39044;&#27979;&#26159;&#27861;&#24459;&#20154;&#24037;&#26234;&#33021;&#20013;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#28041;&#21450;&#26681;&#25454;&#20107;&#23454;&#25551;&#36848;&#39044;&#27979;&#28151;&#28102;&#32618;&#21517;&#12290;&#29616;&#26377;&#30340;&#32618;&#21517;&#39044;&#27979;&#26041;&#27861;&#22312;&#34920;&#29616;&#19978;&#24050;&#32463;&#23637;&#29616;&#20986;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#25928;&#26524;&#65292;&#20294;&#22312;&#22788;&#29702;&#28151;&#28102;&#32618;&#21517;&#65288;&#22914;&#25250;&#22842;&#19982;&#25250;&#21163;&#65289;&#26102;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#27861;&#24459;&#39046;&#22495;&#65292;&#26500;&#25104;&#35201;&#32032;&#22312;&#21306;&#20998;&#28151;&#28102;&#32618;&#21517;&#20013;&#25198;&#28436;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#35282;&#33394;&#12290;&#26500;&#25104;&#35201;&#32032;&#26159;&#28508;&#22312;&#21009;&#32602;&#32972;&#21518;&#30340;&#22522;&#26412;&#34892;&#20026;&#65292;&#24182;&#19988;&#22312;&#19981;&#21516;&#32618;&#21517;&#20043;&#38388;&#26377;&#24494;&#22937;&#30340;&#21306;&#21035;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#20174;&#22270;&#21040;&#35789;&#34955;&#65288;FWGB&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24341;&#20837;&#20102;&#26377;&#20851;&#26500;&#25104;&#35201;&#32032;&#30340;&#39046;&#22495;&#30693;&#35782;&#65292;&#20197;&#25351;&#23548;&#27169;&#22411;&#22312;&#28151;&#28102;&#32618;&#21517;&#19978;&#20570;&#20986;&#21028;&#26029;&#65292;&#31867;&#20284;&#20110;&#27861;&#23448;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;&#26500;&#25104;&#35201;&#32032;&#30340;&#27861;&#24459;&#30693;&#35782;&#22270;&#65292;&#20197;&#24110;&#21161;&#20026;&#27599;&#31181;&#32618;&#21517;&#36873;&#25321;&#20851;&#38190;&#35789;&#65292;&#24418;&#25104;&#19968;&#20010;&#21333;&#35789;&#34955;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.04369v1 Announce Type: new  Abstract: Confusing charge prediction is a challenging task in legal AI, which involves predicting confusing charges based on fact descriptions. While existing charge prediction methods have shown impressive performance, they face significant challenges when dealing with confusing charges, such as Snatch and Robbery. In the legal domain, constituent elements play a pivotal role in distinguishing confusing charges. Constituent elements are fundamental behaviors underlying criminal punishment and have subtle distinctions among charges. In this paper, we introduce a novel From Graph to Word Bag (FWGB) approach, which introduces domain knowledge regarding constituent elements to guide the model in making judgments on confusing charges, much like a judge's reasoning process. Specifically, we first construct a legal knowledge graph containing constituent elements to help select keywords for each charge, forming a word bag. Subsequently, to guide the mod
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;LD3M&#65289;&#65292;&#32467;&#21512;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#25361;&#25112;&#12290;</title><link>https://arxiv.org/abs/2403.03881</link><description>&lt;p&gt;
&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Latent Dataset Distillation with Diffusion Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.03881
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#28508;&#22312;&#25968;&#25454;&#38598;&#33976;&#39311;&#65288;LD3M&#65289;&#65292;&#32467;&#21512;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#25193;&#25955;&#21644;&#25968;&#25454;&#38598;&#33976;&#39311;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#19981;&#21516;&#27169;&#22411;&#26550;&#26500;&#23548;&#33268;&#20934;&#30830;&#24615;&#19979;&#38477;&#21644;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#30340;&#26377;&#25928;&#24615;&#20256;&#32479;&#19978;&#20381;&#36182;&#20110;&#36234;&#26469;&#36234;&#22823;&#30340;&#25968;&#25454;&#38598;&#30340;&#21487;&#29992;&#24615;&#12290;&#28982;&#32780;&#65292;&#22823;&#22411;&#25968;&#25454;&#38598;&#24102;&#26469;&#23384;&#20648;&#25361;&#25112;&#65292;&#24182;&#19988;&#21253;&#21547;&#19968;&#20123;&#38750;&#24433;&#21709;&#21147;&#26679;&#26412;&#65292;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#21487;&#20197;&#34987;&#24573;&#30053;&#32780;&#19981;&#24433;&#21709;&#27169;&#22411;&#26368;&#32456;&#30340;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38480;&#21046;&#65292;&#20986;&#29616;&#20102;&#23558;&#25968;&#25454;&#38598;&#20449;&#24687;&#33976;&#39311;&#25104;&#19968;&#32452;&#21387;&#32553;&#26679;&#26412;&#65288;&#21512;&#25104;&#26679;&#26412;&#65289;&#65292;&#21363;&#33976;&#39311;&#25968;&#25454;&#38598;&#30340;&#27010;&#24565;&#12290;&#20854;&#20013;&#19968;&#20010;&#20851;&#38190;&#26041;&#38754;&#26159;&#36873;&#25321;&#29992;&#20110;&#36830;&#25509;&#21407;&#22987;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#30340;&#26550;&#26500;&#65288;&#36890;&#24120;&#26159;ConvNet&#65289;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25152;&#20351;&#29992;&#30340;&#27169;&#22411;&#26550;&#26500;&#19982;&#33976;&#39311;&#36807;&#31243;&#20013;&#20351;&#29992;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#21017;&#26368;&#32456;&#20934;&#30830;&#24615;&#20250;&#38477;&#20302;&#12290;&#21478;&#19968;&#20010;&#25361;&#25112;&#26159;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#22270;&#20687;&#65292;&#20363;&#22914;128x128&#21450;&#26356;&#39640;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.03881v1 Announce Type: cross  Abstract: The efficacy of machine learning has traditionally relied on the availability of increasingly larger datasets. However, large datasets pose storage challenges and contain non-influential samples, which could be ignored during training without impacting the final accuracy of the model. In response to these limitations, the concept of distilling the information on a dataset into a condensed set of (synthetic) samples, namely a distilled dataset, emerged. One crucial aspect is the selected architecture (usually ConvNet) for linking the original and synthetic datasets. However, the final accuracy is lower if the employed model architecture differs from the model used during distillation. Another challenge is the generation of high-resolution images, e.g., 128x128 and higher. In this paper, we propose Latent Dataset Distillation with Diffusion Models (LD3M) that combine diffusion in latent space with dataset distillation to tackle both chal
&lt;/p&gt;</description></item><item><title>"&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#65292;&#36716;&#21270;&#20102;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#20026;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;"</title><link>https://arxiv.org/abs/2403.01479</link><description>&lt;p&gt;
Align-to-Distill: &#21487;&#35757;&#32451;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#22312;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#20013;&#30340;&#30693;&#35782;&#33976;&#39311;
&lt;/p&gt;
&lt;p&gt;
Align-to-Distill: Trainable Attention Alignment for Knowledge Distillation in Neural Machine Translation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.01479
&lt;/p&gt;
&lt;p&gt;
"&#26412;&#25991;&#25552;&#20986;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#65292;&#36716;&#21270;&#20102;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#20026;&#23398;&#20064;&#38382;&#39064;&#65292;&#23454;&#39564;&#32467;&#26524;&#26174;&#31034;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;"
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#28145;&#24230;&#27169;&#22411;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#30340;&#20986;&#29616;&#25552;&#39640;&#20102;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#24615;&#33021;&#12290;&#30693;&#35782;&#33976;&#39311;&#65288;KD&#65289;&#36890;&#36807;&#23558;&#30693;&#35782;&#20174;&#25945;&#24072;&#27169;&#22411;&#20256;&#36755;&#21040;&#26356;&#32039;&#20945;&#30340;&#23398;&#29983;&#27169;&#22411;&#26469;&#25552;&#39640;&#25928;&#29575;&#12290;&#28982;&#32780;&#65292;&#38024;&#23545;Transformer&#26550;&#26500;&#30340;KD&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;&#22312;&#20915;&#23450;&#35201;&#20174;&#21738;&#20123;&#25945;&#24072;&#23618;&#20013;&#33976;&#39311;&#30693;&#35782;&#26102;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#8220;Align-to-Distill&#8221;&#65288;A2D&#65289;&#31574;&#30053;&#65292;&#26088;&#22312;&#36890;&#36807;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#33258;&#36866;&#24212;&#22320;&#23545;&#40784;&#23398;&#29983;&#27880;&#24847;&#21147;&#22836;&#19982;&#20854;&#25945;&#24072;&#23545;&#24212;&#29289;&#26469;&#35299;&#20915;&#29305;&#24449;&#26144;&#23556;&#38382;&#39064;&#12290;A2D&#20013;&#30340;&#27880;&#24847;&#21147;&#23545;&#40784;&#27169;&#22359;&#25191;&#34892;&#23398;&#29983;&#21644;&#25945;&#24072;&#27880;&#24847;&#21147;&#22836;&#20043;&#38388;&#30340;&#23494;&#38598;&#36880;&#22836;&#27604;&#36739;&#65292;&#23558;&#32452;&#21512;&#26144;&#23556;&#21551;&#21457;&#24335;&#26041;&#27861;&#36716;&#21270;&#20026;&#23398;&#20064;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#23637;&#31034;&#20102;A2D&#30340;&#26377;&#25928;&#24615;&#65292;&#23545;WMT-2022 De-&gt;Dsb&#21644;WMT-2014 En-&gt;De&#30340;BLEU&#20998;&#25968;&#20998;&#21035;&#33719;&#24471;&#39640;&#36798;+3.61&#21644;+0.63&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.01479v1 Announce Type: cross  Abstract: The advent of scalable deep models and large datasets has improved the performance of Neural Machine Translation. Knowledge Distillation (KD) enhances efficiency by transferring knowledge from a teacher model to a more compact student model. However, KD approaches to Transformer architecture often rely on heuristics, particularly when deciding which teacher layers to distill from. In this paper, we introduce the 'Align-to-Distill' (A2D) strategy, designed to address the feature mapping problem by adaptively aligning student attention heads with their teacher counterparts during training. The Attention Alignment Module in A2D performs a dense head-by-head comparison between student and teacher attention heads across layers, turning the combinatorial mapping heuristics into a learning problem. Our experiments show the efficacy of A2D, demonstrating gains of up to +3.61 and +0.63 BLEU points for WMT-2022 De-&gt;Dsb and WMT-2014 En-&gt;De, respe
&lt;/p&gt;</description></item><item><title>SEED&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sample-Efficient adaptation with Error-Driven learning&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#20316;&#20026;&#23398;&#20064;&#26426;&#20250;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2403.00046</link><description>&lt;p&gt;
&#20351;&#29992;&#26679;&#26412;&#39640;&#25928;&#36866;&#24212;&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#33258;&#23450;&#20041;&#20197;&#36827;&#34892;&#20195;&#30721;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
SEED: Customize Large Language Models with Sample-Efficient Adaptation for Code Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2403.00046
&lt;/p&gt;
&lt;p&gt;
SEED&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Sample-Efficient adaptation with Error-Driven learning&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#65292;&#21033;&#29992;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#20316;&#20026;&#23398;&#20064;&#26426;&#20250;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#30340;&#39640;&#25928;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#65292;&#20294;&#22312;&#29305;&#23450;&#22330;&#26223;&#19979;&#20173;&#28982;&#23384;&#22312;&#22256;&#38590;&#12290;&#36825;&#20123;&#22330;&#26223;&#36890;&#24120;&#38656;&#35201;&#35843;&#25972;LLMs&#20197;&#28385;&#36275;&#29305;&#23450;&#38656;&#27714;&#65292;&#20294;&#23454;&#38469;&#21487;&#29992;&#30340;&#35757;&#32451;&#25968;&#25454;&#26377;&#38480;&#65292;&#23548;&#33268;&#20195;&#30721;&#29983;&#25104;&#24615;&#33021;&#36739;&#24046;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#35843;&#25972;LLMs&#20197;&#36866;&#24212;&#26032;&#22330;&#26223;&#24182;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#26679;&#26412;&#26159;&#24403;&#21069;&#20195;&#30721;&#29983;&#25104;&#38754;&#20020;&#30340;&#20027;&#35201;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SEED&#30340;&#26032;&#39062;&#36866;&#24212;&#26041;&#27861;&#65292;&#21363;Sample-Efficient adaptation with Error-Driven learning for code generation&#12290;SEED&#21033;&#29992;LLMs&#20135;&#29983;&#30340;&#38169;&#35823;&#20316;&#20026;&#23398;&#20064;&#26426;&#20250;&#65292;&#21033;&#29992;&#38169;&#35823;&#20462;&#35746;&#26469;&#20811;&#26381;&#33258;&#36523;&#32570;&#28857;&#65292;&#20174;&#32780;&#23454;&#29616;&#26377;&#25928;&#23398;&#20064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SEED&#28041;&#21450;&#35782;&#21035;LLMs&#29983;&#25104;&#30340;&#38169;&#35823;&#20195;&#30721;&#65292;&#20351;&#29992;Self-revise&#36827;&#34892;&#20195;&#30721;&#20462;&#35746;&#65292;&#20248;&#21270;&#27169;&#22411;&#24182;&#36845;&#20195;&#22320;&#36827;&#34892;&#36866;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2403.00046v1 Announce Type: cross  Abstract: Although Large Language Models (LLMs) have made significant progress in code generation, they still struggle with code generation tasks in specific scenarios. These scenarios usually necessitate the adaptation of LLMs to fulfill specific needs, but the limited training data available in practice leads to poor code generation performance. How to effectively adapt LLMs to new scenarios with fewer training samples is a major challenge for current code generation. In this paper, we propose a novel adaptation approach named SEED, which stands for Sample-Efficient adaptation with Error-Driven learning for code generation. SEED leverages the errors made by LLMs as learning opportunities, using error revision to overcome its own shortcomings, thus achieving efficient learning. Specifically, SEED involves identifying error code generated by LLMs, employing Self-revise for code revision, optimizing the model with revised code, and iteratively ad
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#35789;&#27719;&#29305;&#24449;&#65292;&#25104;&#21151;&#35782;&#21035;&#20102;&#26377;&#21161;&#20110;&#26041;&#35328;&#21464;&#21270;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#23450;&#35789;&#27719;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.17914</link><description>&lt;p&gt;
&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#35789;&#27719;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Extracting Lexical Features from Dialects via Interpretable Dialect Classifiers
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.17914
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#35789;&#27719;&#29305;&#24449;&#65292;&#25104;&#21151;&#35782;&#21035;&#20102;&#26377;&#21161;&#20110;&#26041;&#35328;&#21464;&#21270;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#23450;&#35789;&#27719;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35782;&#21035;&#19968;&#31181;&#35821;&#35328;&#30340;&#26041;&#35328;&#20043;&#38388;&#30340;&#35821;&#35328;&#24046;&#24322;&#36890;&#24120;&#38656;&#35201;&#19987;&#19994;&#30693;&#35782;&#21644;&#32454;&#33268;&#30340;&#20154;&#31867;&#20998;&#26512;&#12290;&#36825;&#20027;&#35201;&#26159;&#22240;&#20026;&#30740;&#31350;&#21508;&#31181;&#26041;&#35328;&#28041;&#21450;&#21040;&#22797;&#26434;&#24615;&#21644;&#24494;&#22937;&#20043;&#22788;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#21487;&#35299;&#37322;&#30340;&#26041;&#35328;&#20998;&#31867;&#22120;&#25552;&#21462;&#26041;&#35328;&#30340;&#21306;&#20998;&#24615;&#35789;&#27719;&#29305;&#24449;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#20154;&#31867;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;&#20107;&#21518;&#21644;&#20869;&#22312;&#30340;&#35299;&#37322;&#24615;&#26041;&#27861;&#65292;&#23545;&#26222;&#36890;&#35805;&#12289;&#24847;&#22823;&#21033;&#35821;&#21644;&#20302;&#22320;&#33832;&#20811;&#26862;&#35821;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#23454;&#39564;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#25104;&#21151;&#22320;&#35782;&#21035;&#20102;&#26377;&#21161;&#20110;&#26041;&#35328;&#21464;&#21270;&#30340;&#20851;&#38190;&#35821;&#35328;&#29305;&#23450;&#35789;&#27719;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.17914v1 Announce Type: cross  Abstract: Identifying linguistic differences between dialects of a language often requires expert knowledge and meticulous human analysis. This is largely due to the complexity and nuance involved in studying various dialects. We present a novel approach to extract distinguishing lexical features of dialects by utilizing interpretable dialect classifiers, even in the absence of human experts. We explore both post-hoc and intrinsic approaches to interpretability, conduct experiments on Mandarin, Italian, and Low Saxon, and experimentally demonstrate that our method successfully identifies key language-specific lexical features that contribute to dialectal variations.
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;31&#31181;&#21335;&#20122;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#25104;&#24037;&#20855;&#36827;&#34892;&#35757;&#32451;&#21644;&#23545;&#40784;&#65292;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#20849;&#25351;&#35299;&#26512;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2402.13571</link><description>&lt;p&gt;
&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#21335;&#20122;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Multilingual Coreference Resolution in Low-resource South Asian Languages
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.13571
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;31&#31181;&#21335;&#20122;&#35821;&#35328;&#30340;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;&#32763;&#35793;&#25968;&#25454;&#38598;&#65292;&#36890;&#36807;&#21033;&#29992;&#29616;&#25104;&#24037;&#20855;&#36827;&#34892;&#35757;&#32451;&#21644;&#23545;&#40784;&#65292;&#22312;&#20302;&#36164;&#28304;&#26465;&#20214;&#19979;&#23454;&#29616;&#20102;&#36739;&#22909;&#30340;&#20849;&#25351;&#35299;&#26512;&#27169;&#22411;&#24615;&#33021;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20849;&#25351;&#35299;&#26512;&#28041;&#21450;&#35782;&#21035;&#22312;&#35805;&#35821;&#20013;&#25351;&#21521;&#21516;&#19968;&#29616;&#23454;&#23454;&#20307;&#30340;&#25991;&#26412;&#29255;&#27573;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#36825;&#19968;&#20219;&#21153;&#22312;&#33521;&#35821;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#22312;&#21335;&#20122;&#35821;&#35328;&#20013;&#65292;&#20844;&#24320;&#21487;&#35775;&#38382;&#30340;&#20849;&#25351;&#35299;&#26512;&#36164;&#28304;&#21644;&#27169;&#22411;&#30456;&#23545;&#31232;&#32570;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#25104;&#30340;&#32763;&#35793;&#21644;&#35789;&#23545;&#40784;&#24037;&#20855;&#65292;&#22312;31&#31181;&#21335;&#20122;&#35821;&#35328;&#20013;&#24341;&#20837;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#35821;&#35328;&#20849;&#25351;&#35299;&#26512;&#30340;&#32763;&#35793;&#25968;&#25454;&#38598;&#65288;TransMuCoRes&#65289;&#12290;&#20960;&#20046;&#25152;&#26377;&#39044;&#27979;&#30340;&#32763;&#35793;&#37117;&#36890;&#36807;&#20102;&#21512;&#29702;&#24615;&#26816;&#26597;&#65292;75%&#30340;&#33521;&#35821;&#21442;&#32771;&#25991;&#29486;&#19982;&#20854;&#39044;&#27979;&#30340;&#32763;&#35793;&#30456;&#23545;&#24212;&#12290;&#21033;&#29992;&#22810;&#35821;&#35328;&#32534;&#30721;&#22120;&#65292;&#25105;&#20204;&#35757;&#32451;&#20102;&#20004;&#31181;&#29616;&#25104;&#30340;&#20849;&#25351;&#35299;&#26512;&#27169;&#22411;&#65292;&#23558;TransMuCoRes&#19982;&#24102;&#26377;&#25163;&#21160;&#27880;&#37322;&#30340;&#21360;&#22320;&#35821;&#20849;&#25351;&#35299;&#26512;&#25968;&#25454;&#38598;&#25340;&#25509;&#22312;&#19968;&#36215;&#12290;&#26368;&#20339;&#34920;&#29616;&#27169;&#22411;&#22312;LEA F1&#21644;CoNLL F1&#19978;&#20998;&#21035;&#36798;&#21040;&#20102;64&#21644;68&#30340;&#20998;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.13571v1 Announce Type: cross  Abstract: Coreference resolution involves the task of identifying text spans within a discourse that pertain to the same real-world entity. While this task has been extensively explored in the English language, there has been a notable scarcity of publicly accessible resources and models for coreference resolution in South Asian languages. We introduce a Translated dataset for Multilingual Coreference Resolution (TransMuCoRes) in 31 South Asian languages using off-the-shelf tools for translation and word-alignment. Nearly all of the predicted translations successfully pass a sanity check, and 75% of English references align with their predicted translations. Using multilingual encoders, two off-the-shelf coreference resolution models were trained on a concatenation of TransMuCoRes and a Hindi coreference resolution dataset with manual annotations. The best performing model achieved a score of 64 and 68 for LEA F1 and CoNLL F1, respectively, on o
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#39046;&#22495;&#25991;&#29486;&#32508;&#36848;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#24341;&#20837;&#22823;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25991;&#29486;&#35745;&#37327;&#25351;&#26631;&#65292;&#24182;&#26500;&#24314;&#20102;RiPAMI&#20803;&#25968;&#25454;&#25968;&#25454;&#24211;&#21644;&#20027;&#39064;&#25968;&#25454;&#38598;&#20197;&#33719;&#21462;PAMI&#32508;&#36848;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2402.12928</link><description>&lt;p&gt;
&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#39046;&#22495;&#25991;&#29486;&#32508;&#36848;&#30340;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Literature Review of Literature Reviews in Pattern Analysis and Machine Intelligence
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.12928
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#23545;&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#39046;&#22495;&#25991;&#29486;&#32508;&#36848;&#30340;&#20840;&#38754;&#35780;&#20272;&#65292;&#24341;&#20837;&#22823;&#35821;&#35328;&#27169;&#22411;&#39537;&#21160;&#30340;&#25991;&#29486;&#35745;&#37327;&#25351;&#26631;&#65292;&#24182;&#26500;&#24314;&#20102;RiPAMI&#20803;&#25968;&#25454;&#25968;&#25454;&#24211;&#21644;&#20027;&#39064;&#25968;&#25454;&#38598;&#20197;&#33719;&#21462;PAMI&#32508;&#36848;&#30340;&#32479;&#35745;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25972;&#21512;&#20998;&#25955;&#30340;&#30693;&#35782;&#65292;&#25991;&#29486;&#32508;&#36848;&#25552;&#20379;&#20102;&#23545;&#25152;&#30740;&#31350;&#20027;&#39064;&#30340;&#20840;&#38754;&#20102;&#35299;&#12290;&#28982;&#32780;&#65292;&#22312;&#27169;&#24335;&#20998;&#26512;&#19982;&#26426;&#22120;&#26234;&#33021;&#65288;PAMI&#65289;&#36825;&#19968;&#34028;&#21187;&#21457;&#23637;&#30340;&#39046;&#22495;&#20013;&#65292;&#36807;&#22810;&#30340;&#32508;&#36848;&#24341;&#36215;&#20102;&#30740;&#31350;&#20154;&#21592;&#21644;&#35780;&#35770;&#32773;&#30340;&#20851;&#27880;&#12290;&#20316;&#20026;&#23545;&#36825;&#20123;&#20851;&#27880;&#30340;&#22238;&#24212;&#65292;&#26412;&#25991;&#26088;&#22312;&#20174;&#22810;&#20010;&#35282;&#24230;&#20840;&#38754;&#23457;&#35270;PAMI&#39046;&#22495;&#30340;&#32508;&#36848;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.12928v1 Announce Type: cross  Abstract: By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic. However, excessive reviews, especially in the booming field of pattern analysis and machine intelligence (PAMI), raise concerns for both researchers and reviewers. In response to these concerns, this Analysis aims to provide a thorough review of reviews in the PAMI field from diverse perspectives. First, large language model-empowered bibliometric indicators are proposed to evaluate literature reviews automatically. To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed, which are utilized to obtain statistical characteristics of PAMI reviews. Unlike traditional bibliometric measurements, the proposed article-level indicators provide real-time and field-normalized quantified assessments of reviews without relying on user-defined keywords. Second, based on these indicators, th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#28857;&#36129;&#29486;&#65306;&#19968;&#26159;&#21019;&#24314;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;NL-to-code&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#20108;&#26159;&#24341;&#20837;&#20102;&#19968;&#31181;&#32858;&#31867;&#28982;&#21518;&#36873;&#25321;&#25552;&#31034;&#25216;&#26415;&#65292;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#28155;&#21152;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#34892;&#21040;LLM&#25552;&#31034;&#20013;&#12290;</title><link>https://arxiv.org/abs/2402.11734</link><description>&lt;p&gt;
&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#35299;&#20915;&#25968;&#25454;&#20013;&#24515;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
Solving Data-centric Tasks using Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.11734
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#28857;&#36129;&#29486;&#65306;&#19968;&#26159;&#21019;&#24314;&#20102;&#19968;&#20010;&#30495;&#23454;&#19990;&#30028;&#30340;NL-to-code&#20219;&#21153;&#25968;&#25454;&#38598;&#65292;&#20108;&#26159;&#24341;&#20837;&#20102;&#19968;&#31181;&#32858;&#31867;&#28982;&#21518;&#36873;&#25321;&#25552;&#31034;&#25216;&#26415;&#65292;&#20174;&#36755;&#20837;&#25968;&#25454;&#20013;&#28155;&#21152;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#34892;&#21040;LLM&#25552;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#27491;&#22312;&#36805;&#36895;&#21462;&#20195;&#20687;StackOverflow&#36825;&#26679;&#30340;&#24110;&#21161;&#35770;&#22363;&#65292;&#24182;&#19988;&#23545;&#20110;&#38750;&#19987;&#19994;&#31243;&#24207;&#21592;&#21644;&#26368;&#32456;&#29992;&#25143;&#29305;&#21035;&#26377;&#24110;&#21161;&#12290;&#36825;&#20123;&#29992;&#25143;&#36890;&#24120;&#23545;&#25968;&#25454;&#20013;&#24515;&#20219;&#21153;&#24863;&#20852;&#36259;&#65292;&#20363;&#22914;&#30005;&#23376;&#34920;&#26684;&#25805;&#20316;&#21644;&#25968;&#25454;&#22788;&#29702;&#65292;&#22914;&#26524;&#20165;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#20256;&#36798;&#24847;&#22270;&#32780;&#19981;&#21253;&#21547;&#25968;&#25454;&#65292;&#36825;&#20123;&#20219;&#21153;&#24456;&#38590;&#35299;&#20915;&#12290;&#20294;&#26159;&#65292;&#25105;&#20204;&#22914;&#20309;&#20915;&#23450;&#22312;&#25552;&#31034;&#20013;&#21253;&#21547;&#22810;&#23569;&#25968;&#25454;&#21644;&#21738;&#20123;&#25968;&#25454;&#65311;&#26412;&#25991;&#23545;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#20570;&#20986;&#20102;&#20004;&#28857;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;&#19968;&#20010;&#20174;StackOverflow&#24086;&#23376;&#20013;&#33719;&#21462;&#30340;&#25805;&#20316;&#34920;&#26684;&#25968;&#25454;&#30340;&#30495;&#23454;NL-to-code&#20219;&#21153;&#25968;&#25454;&#38598;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#32858;&#31867;&#28982;&#21518;&#36873;&#25321;&#25552;&#31034;&#25216;&#26415;&#65292;&#23558;&#36755;&#20837;&#25968;&#25454;&#20013;&#26368;&#20855;&#20195;&#34920;&#24615;&#30340;&#34892;&#28155;&#21152;&#21040;LLM&#25552;&#31034;&#20013;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;LLM&#30340;&#24615;&#33021;&#30830;&#23454;&#23545;&#20256;&#36882;&#21040;&#25552;&#31034;&#20013;&#30340;&#25968;&#25454;&#37327;&#25935;&#24863;&#65292;&#24182;&#19988;&#23545;&#20110;&#20855;&#26377;&#22823;&#37327;&#35821;&#27861;&#21464;&#20307;&#30340;&#20219;&#21153;&#65292;&#20256;&#36882;&#30340;&#25968;&#25454;&#37327;&#36739;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.11734v1 Announce Type: cross  Abstract: Large language models (LLMs) are rapidly replacing help forums like StackOverflow, and are especially helpful for non-professional programmers and end users. These users are often interested in data-centric tasks, such as spreadsheet manipulation and data wrangling, which are hard to solve if the intent is only communicated using a natural-language description, without including the data. But how do we decide how much data and which data to include in the prompt? This paper makes two contributions towards answering this question. First, we create a dataset of real-world NL-to-code tasks manipulating tabular data, mined from StackOverflow posts. Second, we introduce a cluster-then-select prompting technique, which adds the most representative rows from the input data to the LLM prompt. Our experiments show that LLM performance is indeed sensitive to the amount of data passed in the prompt, and that for tasks with a lot of syntactic vari
&lt;/p&gt;</description></item><item><title>LongHeads &#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37322;&#25918;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2402.10685</link><description>&lt;p&gt;
LongHeads: &#22810;&#22836;&#27880;&#24847;&#21147;&#20854;&#23454;&#26159;&#19968;&#20010;&#38271;&#19978;&#19979;&#25991;&#22788;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
LongHeads: Multi-Head Attention is Secretly a Long Context Processor
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.10685
&lt;/p&gt;
&lt;p&gt;
LongHeads &#25552;&#20986;&#20102;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37322;&#25918;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLM)&#22788;&#29702;&#38271;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#22312;&#35768;&#22810;&#39046;&#22495;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#20294;&#30001;&#20110;&#26377;&#38480;&#38271;&#24230;&#27867;&#21270;&#21644;&#27880;&#24847;&#21147;&#30340;&#20108;&#27425;&#35745;&#31639;&#38656;&#27714;&#65292;&#24448;&#24448;&#38590;&#20197;&#26377;&#25928;&#39640;&#25928;&#22320;&#22788;&#29702;&#36739;&#38271;&#30340;&#36755;&#20837;&#12290; &#35768;&#22810;&#20154;&#35797;&#22270;&#36890;&#36807;&#38480;&#21046;&#22312;&#39044;&#35757;&#32451;&#38271;&#24230;&#20869;&#30340;&#27880;&#24847;&#21147;&#31383;&#21475;&#26469;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#12290; &#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#24341;&#20837;&#20102;&#26032;&#38382;&#39064;&#65292;&#22914;&#24573;&#30053;&#20013;&#38388;&#19978;&#19979;&#25991;&#21644;&#38656;&#35201;&#39069;&#22806;&#35757;&#32451;&#12290; &#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;LongHeads&#65292;&#19968;&#20010;&#26080;&#38656;&#35757;&#32451;&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#37322;&#25918;&#22810;&#22836;&#27880;&#24847;&#21147;&#30340;&#28508;&#21147;&#26469;&#22686;&#24378;LLM&#30340;&#38271;&#19978;&#19979;&#25991;&#33021;&#21147;&#12290; &#25105;&#20204;&#20801;&#35768;&#27599;&#20010;&#22836;&#37096;&#36873;&#25321;&#24182;&#20851;&#27880;&#37325;&#35201;&#30340;&#19978;&#19979;&#25991;&#22359;&#65292;&#20197;&#22788;&#29702;&#20998;&#24067;&#38271;&#24230;&#65292;&#32780;&#19981;&#26159;&#35753;&#27599;&#20010;&#22836;&#37096;&#37117;&#21442;&#19982;&#20840;&#21477;&#27880;&#24847;&#21147;&#65292;&#36825;&#26679;&#20570;&#30001;&#20110;&#20998;&#24067;&#20043;&#22806;&#30340;&#38382;&#39064;&#32780;&#38590;&#20197;&#27867;&#21270;&#21040;&#26356;&#38271;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.10685v1 Announce Type: cross  Abstract: Large language models (LLMs) have achieved impressive performance in numerous domains but often struggle to process lengthy inputs effectively and efficiently due to limited length generalization and attention's quadratic computational demands. Many sought to mitigate this by restricting the attention window within the pre-trained length. However, these methods introduce new issues such as ignoring the middle context and requiring additional training. To address these problems, we propose LongHeads, a training-free framework that enhances LLM's long context ability by unlocking multi-head attention's untapped potential. Instead of allowing each head to attend to the full sentence, which struggles with generalizing to longer sequences due to out-of-distribution (OOD) issues, we allow each head to process in-distribution length by selecting and attending to important context chunks. To this end, we propose a chunk selection strategy that
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#21644;&#20154;&#31867;&#26159;&#21542;&#33021;&#22815;&#30495;&#27491;&#20132;&#27969;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24515;&#29702;&#34892;&#20026;&#26041;&#27861;&#8221;&#30340;&#22238;&#31572;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#27979;&#35797;AI&#26159;&#21542;&#23637;&#29616;&#20986;&#20154;&#31867;&#31867;&#20284;&#30340;&#34892;&#20026;&#26469;&#21028;&#26029;&#20854;&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#30495;&#27491;&#20132;&#27969;&#12290;</title><link>https://arxiv.org/abs/2402.09494</link><description>&lt;p&gt;
AI&#21644;&#20154;&#31867;&#33021;&#22815;&#30495;&#27491;&#20132;&#27969;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can AI and humans genuinely communicate?
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;AI&#21644;&#20154;&#31867;&#26159;&#21542;&#33021;&#22815;&#30495;&#27491;&#20132;&#27969;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#8220;&#24515;&#29702;&#34892;&#20026;&#26041;&#27861;&#8221;&#30340;&#22238;&#31572;&#26041;&#24335;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#27979;&#35797;AI&#26159;&#21542;&#23637;&#29616;&#20986;&#20154;&#31867;&#31867;&#20284;&#30340;&#34892;&#20026;&#26469;&#21028;&#26029;&#20854;&#26159;&#21542;&#33021;&#22815;&#19982;&#20154;&#31867;&#30495;&#27491;&#20132;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#25506;&#35752;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;AI&#21644;&#20154;&#31867;&#26159;&#21542;&#33021;&#22815;&#30495;&#27491;&#20132;&#27969;&#12290;&#20316;&#32773;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#8220;&#24515;&#29702;&#34892;&#20026;&#26041;&#27861;&#8221;&#30340;&#26041;&#24335;&#26469;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#12290;&#35813;&#26041;&#27861;&#21253;&#25324;&#19977;&#20010;&#27493;&#39588;&#65306;&#39318;&#20808;&#26126;&#30830;&#20154;&#31867;&#20132;&#27969;&#25152;&#38656;&#30340;&#24515;&#29702;&#33021;&#21147;&#65307;&#20854;&#27425;&#30830;&#23450;&#27979;&#35797;&#36825;&#20123;&#33021;&#21147;&#30340;&#23454;&#39564;&#33539;&#24335;&#65307;&#26368;&#21518;&#23558;&#36825;&#20123;&#33539;&#24335;&#24212;&#29992;&#20110;&#27979;&#35797;AI&#26159;&#21542;&#23637;&#29616;&#20986;&#30456;&#20851;&#30340;&#34892;&#20026;&#12290;&#22914;&#26524;&#21069;&#20004;&#20010;&#27493;&#39588;&#25104;&#21151;&#23436;&#25104;&#65292;&#24182;&#19988;AI&#22312;&#27979;&#35797;&#20013;&#23637;&#29616;&#20986;&#31867;&#20284;&#20154;&#31867;&#30340;&#32467;&#26524;&#65292;&#37027;&#20040;&#36825;&#21487;&#20197;&#34987;&#30475;&#20316;&#26159;AI&#21644;&#20154;&#31867;&#33021;&#22815;&#30495;&#27491;&#20132;&#27969;&#30340;&#35777;&#25454;&#12290;&#24515;&#29702;&#34892;&#20026;&#26041;&#27861;&#30340;&#20248;&#21183;&#22312;&#20110;&#25105;&#20204;&#19981;&#38656;&#35201;&#29702;&#35299;&#40657;&#30418;&#31639;&#27861;&#65288;&#27604;&#22914;&#26631;&#20934;&#28145;&#24230;&#23398;&#20064;&#31639;&#27861;&#65289;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09494v1 Announce Type: cross  Abstract: Can AI and humans genuinely communicate? In this article, after giving some background and motivating my proposal (sections 1 to 3), I explore a way to answer this question that I call the "mental-behavioral methodology" (sections 4 and 5). This methodology follows the following three steps: First, spell out what mental capacities are sufficient for human communication (as opposed to communication more generally). Second, spell out the experimental paradigms required to test whether a behavior exhibits these capacities. Third, apply or adapt these paradigms to test whether an AI displays the relevant behaviors. If the first two steps are successfully completed, and if the AI passes the tests with human-like results, this constitutes evidence that this AI and humans can genuinely communicate. This mental-behavioral methodology has the advantage that we don't need to understand the workings of black-box algorithms, such as standard deep 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>https://arxiv.org/abs/2402.09132</link><description>&lt;p&gt;
&#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Exploring the Adversarial Capabilities of Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.09132
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#32034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#23545;&#25239;&#33021;&#21147;&#65292;&#24182;&#21457;&#29616;&#20854;&#33021;&#22815;&#25104;&#21151;&#22320;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#29305;&#21035;&#26159;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#26041;&#38754;&#20855;&#26377;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#24341;&#21457;&#20102;&#24191;&#27867;&#21644;&#26222;&#36941;&#30340;&#20852;&#36259;&#65292;&#22240;&#20026;&#23427;&#20204;&#20855;&#26377;&#24378;&#22823;&#30340;&#35821;&#35328;&#29983;&#25104;&#33021;&#21147;&#65292;&#20026;&#34892;&#19994;&#21644;&#30740;&#31350;&#25552;&#20379;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#25506;&#35752;&#20102;LLMs&#30340;&#23433;&#20840;&#24615;&#21644;&#38544;&#31169;&#38382;&#39064;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#33021;&#21542;&#34920;&#29616;&#20986;&#23545;&#25239;&#34892;&#20026;&#30340;&#31243;&#24230;&#20173;&#28982;&#23578;&#26410;&#23436;&#20840;&#25506;&#32034;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#30740;&#31350;&#24120;&#35265;&#30340;&#20844;&#24320;&#21487;&#29992;LLMs&#26159;&#21542;&#20855;&#26377;&#33021;&#21147;&#25200;&#20081;&#25991;&#26412;&#26679;&#26412;&#20197;&#24858;&#24324;&#23433;&#20840;&#25514;&#26045;&#65292;&#21363;&#25152;&#35859;&#30340;&#23545;&#25239;&#31034;&#20363;&#25110;&#25915;&#20987;&#12290;&#26356;&#20855;&#20307;&#22320;&#35828;&#65292;&#25105;&#20204;&#35843;&#26597;LLMs&#26159;&#21542;&#26412;&#36136;&#19978;&#33021;&#22815;&#20174;&#33391;&#24615;&#26679;&#26412;&#20013;&#21046;&#36896;&#23545;&#25239;&#24615;&#31034;&#20363;&#20197;&#24858;&#24324;&#29616;&#26377;&#30340;&#23433;&#20840;&#38450;&#32447;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#37325;&#28857;&#20851;&#27880;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#65292;&#21457;&#29616;LLMs&#25104;&#21151;&#22320;&#25214;&#21040;&#20102;&#23545;&#25239;&#24615;&#25200;&#21160;&#65292;&#26377;&#25928;&#22320;&#30772;&#22351;&#20102;&#23545;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#31995;&#32479;&#30340;&#38450;&#24481;&#12290;&#25105;&#20204;&#30340;&#21457;&#29616;&#23545;&#65288;&#21322;&#65289;&#33258;&#21160;&#21270;&#23433;&#20840;&#35780;&#20272;&#21644;&#38450;&#24481;&#20855;&#26377;&#37325;&#35201;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2402.09132v1 Announce Type: new Abstract: The proliferation of large language models (LLMs) has sparked widespread and general interest due to their strong language generation capabilities, offering great potential for both industry and research. While previous research delved into the security and privacy issues of LLMs, the extent to which these models can exhibit adversarial behavior remains largely unexplored. Addressing this gap, we investigate whether common publicly available LLMs have inherent capabilities to perturb text samples to fool safety measures, so-called adversarial examples resp.~attacks. More specifically, we investigate whether LLMs are inherently able to craft adversarial examples out of benign samples to fool existing safe rails. Our experiments, which focus on hate speech detection, reveal that LLMs succeed in finding adversarial perturbations, effectively undermining hate speech detection systems. Our findings carry significant implications for (semi-)aut
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;</title><link>https://arxiv.org/abs/2402.08644</link><description>&lt;p&gt;
&#29992;&#20110;&#25512;&#26029;&#39640;&#25928;LLMs&#30340;&#20018;&#32852;Transformer
&lt;/p&gt;
&lt;p&gt;
Tandem Transformers for Inference Efficient LLMs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.08644
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26550;&#26500;&#65292;&#31216;&#20026;&#20018;&#32852;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#20256;&#32479;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#36895;&#24230;&#38480;&#21046;&#30340;&#38382;&#39064;&#12290;&#35813;&#26550;&#26500;&#36890;&#36807;&#23558;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;&#22823;&#27169;&#22411;&#20197;&#22359;&#27169;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#24182;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#30340;&#20016;&#23500;&#34920;&#31034;&#65292;&#20174;&#32780;&#26174;&#33879;&#25552;&#39640;&#20102;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;&#20018;&#32852;&#30340;PaLM2-Bison&#21644;PaLM2-Gecko&#30456;&#27604;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#39640;&#20102;3.3%&#65292;&#24182;&#19988;&#30456;&#36739;&#20110;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#65292;&#21152;&#36895;&#27604;&#36798;&#21040;1.16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;( LLMs )&#20855;&#26377;&#33258;&#22238;&#24402;&#30340;&#29305;&#24615;&#65292;&#36825;&#20351;&#24471;&#25512;&#26029;&#36895;&#24230;&#21463;&#21040;&#38480;&#21046;&#65292;&#22240;&#20026;&#35789;&#20803;&#26159;&#25353;&#39034;&#24207;&#29983;&#25104;&#30340;&#12290;&#23613;&#31649;&#26377;&#20123;&#39044;&#27979;&#21644;&#24182;&#34892;&#35299;&#30721;&#25216;&#26415;&#35797;&#22270;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#37117;&#26377;&#38480;&#21046;&#65306;&#35201;&#20040;&#20381;&#36182;&#26356;&#31934;&#31616;&#20294;&#20934;&#30830;&#24230;&#36739;&#20302;&#30340;&#27169;&#22411;&#36827;&#34892;&#29983;&#25104;&#65292;&#35201;&#20040;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#22522;&#30784;LLM&#30340;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26550;&#26500;&#65292;&#21363;&#20018;&#32852;Transformer&#65292;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#36825;&#31181;&#26550;&#26500;&#29420;&#29305;&#22320;&#32467;&#21512;&#20102;(1)&#19968;&#20010;&#23567;&#22411;&#33258;&#22238;&#24402;&#27169;&#22411;&#21644;(2)&#19968;&#20010;&#20197;&#22359;&#27169;&#24335;&#36816;&#34892;&#30340;&#22823;&#27169;&#22411;(&#21516;&#26102;&#22788;&#29702;&#22810;&#20010;&#35789;&#20803;)&#12290;&#36890;&#36807;&#35753;&#23567;&#27169;&#22411;&#20851;&#27880;&#22823;&#27169;&#22411;&#26356;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#22823;&#24133;&#25552;&#21319;&#23567;&#27169;&#22411;&#30340;&#39044;&#27979;&#20934;&#30830;&#24615;&#12290;&#22312;PaLM2&#39044;&#35757;&#32451;&#25968;&#25454;&#38598;&#19978;&#65292;PaLM2-Bison&#21644;PaLM2-Gecko&#30340;&#20018;&#32852;&#30456;&#36739;&#29420;&#31435;&#30340;PaLM2-Gecko&#65292;&#22312;&#19979;&#19968;&#20010;&#35789;&#20803;&#39044;&#27979;&#20934;&#30830;&#24615;&#19978;&#25552;&#21319;&#20102;3.3%&#65292;&#19982;&#20855;&#26377;&#30456;&#20284;&#19979;&#28216;&#20219;&#21153;&#30340;PaLM2-Otter&#27169;&#22411;&#30456;&#27604;&#65292;&#25552;&#20379;&#20102;1.16&#20493;&#30340;&#21152;&#36895;&#27604;&#12290;
&lt;/p&gt;
&lt;p&gt;
The autoregressive nature of conventional large language models (LLMs) inherently limits inference speed, as tokens are generated sequentially. While speculative and parallel decoding techniques attempt to mitigate this, they face limitations: either relying on less accurate smaller models for generation or failing to fully leverage the base LLM's representations.   We introduce a novel architecture, Tandem transformers, to address these issues. This architecture uniquely combines (1) a small autoregressive model and (2) a large model operating in block mode (processing multiple tokens simultaneously). The small model's predictive accuracy is substantially enhanced by granting it attention to the large model's richer representations. On the PaLM2 pretraining dataset, a tandem of PaLM2-Bison and PaLM2-Gecko demonstrates a 3.3% improvement in next-token prediction accuracy over a standalone PaLM2-Gecko, offering a 1.16x speedup compared to a PaLM2-Otter model with comparable downstream p
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#24403;&#21069;&#21457;&#23637;&#29366;&#24577;&#19979;&#30340;&#20851;&#38190;&#38480;&#21046;&#36827;&#34892;&#20102;&#23457;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2402.05355</link><description>&lt;p&gt;
&#23433;&#20840;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey on Safe Multi-Modal Learning System
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.05355
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#30340;&#20998;&#31867;&#27861;&#65292;&#23545;&#24403;&#21069;&#21457;&#23637;&#29366;&#24577;&#19979;&#30340;&#20851;&#38190;&#38480;&#21046;&#36827;&#34892;&#20102;&#23457;&#26597;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#30340;&#24191;&#27867;&#24212;&#29992;&#65292;&#23433;&#20840;&#38382;&#39064;&#21464;&#24471;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#23545;&#20110;&#36825;&#19968;&#39046;&#22495;&#30340;&#23433;&#20840;&#38382;&#39064;&#32570;&#20047;&#31995;&#32479;&#24615;&#30740;&#31350;&#24050;&#25104;&#20026;&#19968;&#20010;&#37325;&#35201;&#30340;&#38556;&#30861;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#30340;&#20998;&#31867;&#27861;&#65292;&#30830;&#23450;&#20102;&#36825;&#20123;&#38382;&#39064;&#30340;&#22235;&#20010;&#20851;&#38190;&#25903;&#26609;&#12290;&#20511;&#21161;&#36825;&#19968;&#20998;&#31867;&#27861;&#65292;&#25105;&#20204;&#23545;&#27599;&#20010;&#25903;&#26609;&#36827;&#34892;&#20102;&#28145;&#20837;&#23457;&#26597;&#65292;&#31361;&#20986;&#20102;&#24403;&#21069;&#21457;&#23637;&#29366;&#24577;&#30340;&#20851;&#38190;&#38480;&#21046;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25351;&#20986;&#20102;&#22810;&#27169;&#24577;&#23398;&#20064;&#31995;&#32479;&#23433;&#20840;&#38754;&#20020;&#30340;&#29420;&#29305;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#28508;&#22312;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the wide deployment of multimodal learning systems (MMLS) in real-world scenarios, safety concerns have become increasingly prominent. The absence of systematic research into their safety is a significant barrier to progress in this field. To bridge the gap, we present the first taxonomy for MMLS safety, identifying four essential pillars of these concerns. Leveraging this taxonomy, we conduct in-depth reviews for each pillar, highlighting key limitations based on the current state of development. Finally, we pinpoint unique challenges in MMLS safety and provide potential directions for future research.
&lt;/p&gt;</description></item><item><title>JEANIE&#26159;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;-&#35270;&#35282;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#30340;&#30456;&#20284;&#24230;&#12290;&#23427;&#33021;&#22815;&#35299;&#20915;&#35270;&#39057;&#24207;&#21015;&#20013;&#36895;&#24230;&#12289;&#26102;&#38388;&#20301;&#32622;&#21644;&#23039;&#21183;&#30340;&#24178;&#25200;&#21464;&#21270;&#38382;&#39064;&#12290;&#22312;&#35780;&#20272;&#20102;&#39592;&#26550;Few-shot&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#21518;&#65292;JEANIE&#22312;&#25903;&#25345;-&#26597;&#35810;&#24207;&#21015;&#23545;&#30340;&#26102;&#38388;&#22359;&#21305;&#37197;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>https://arxiv.org/abs/2402.04599</link><description>&lt;p&gt;
&#35265; JEANIE&#65306;&#36890;&#36807;&#26102;&#38388;-&#35270;&#35282;&#23545;&#40784;&#30340;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#30456;&#20284;&#24230;&#27979;&#37327;
&lt;/p&gt;
&lt;p&gt;
Meet JEANIE: a Similarity Measure for 3D Skeleton Sequences via Temporal-Viewpoint Alignment
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.04599
&lt;/p&gt;
&lt;p&gt;
JEANIE&#26159;&#19968;&#31181;&#36890;&#36807;&#26102;&#38388;-&#35270;&#35282;&#23545;&#40784;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#27979;&#37327;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#30340;&#30456;&#20284;&#24230;&#12290;&#23427;&#33021;&#22815;&#35299;&#20915;&#35270;&#39057;&#24207;&#21015;&#20013;&#36895;&#24230;&#12289;&#26102;&#38388;&#20301;&#32622;&#21644;&#23039;&#21183;&#30340;&#24178;&#25200;&#21464;&#21270;&#38382;&#39064;&#12290;&#22312;&#35780;&#20272;&#20102;&#39592;&#26550;Few-shot&#21160;&#20316;&#35782;&#21035;&#20219;&#21153;&#21518;&#65292;JEANIE&#22312;&#25903;&#25345;-&#26597;&#35810;&#24207;&#21015;&#23545;&#30340;&#26102;&#38388;&#22359;&#21305;&#37197;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#24207;&#21015;&#34920;&#29616;&#20986;&#26174;&#33879;&#30340;&#24178;&#25200;&#24615;&#21464;&#21270;&#65292;&#21253;&#25324;&#21160;&#20316;&#36895;&#24230;&#12289;&#26102;&#38388;&#20301;&#32622;&#21644;&#20027;&#20307;&#23039;&#21183;&#65292;&#23548;&#33268;&#22312;&#27604;&#36739;&#20004;&#32452;&#24103;&#25110;&#35780;&#20272;&#20004;&#20010;&#24207;&#21015;&#30340;&#30456;&#20284;&#24230;&#26102;&#20135;&#29983;&#26102;&#38388;-&#35270;&#35282;&#19981;&#21305;&#37197;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#24207;&#21015;&#23545;&#27604;&#30340;&#32852;&#21512;&#26102;&#38388;&#21644;&#25668;&#20687;&#26426;&#35270;&#35282;&#23545;&#40784;&#26041;&#27861;&#65288;JEANIE&#65289;&#12290;&#25105;&#20204;&#29305;&#21035;&#20851;&#27880;&#33021;&#22815;&#22312;&#19977;&#32500;&#20013;&#36731;&#26494;&#25805;&#20316;&#25668;&#20687;&#26426;&#21644;&#20027;&#20307;&#23039;&#21183;&#30340;&#19977;&#32500;&#39592;&#26550;&#24207;&#21015;&#12290;&#25105;&#20204;&#22312;&#39592;&#26550;Few-shot&#21160;&#20316;&#35782;&#21035;&#65288;FSAR&#65289;&#19978;&#35780;&#20272;&#20102;JEANIE&#65292;&#20854;&#20013;&#30001;&#20110;&#26032;&#31867;&#21035;&#26679;&#26412;&#26377;&#38480;&#65292;&#36890;&#36807;&#21305;&#37197;&#22909;&#25903;&#25345;-&#26597;&#35810;&#24207;&#21015;&#23545;&#30340;&#26102;&#38388;&#22359;&#65288;&#32452;&#25104;&#24207;&#21015;&#30340;&#26102;&#38388;&#22359;&#65289;&#26469;&#25490;&#38500;&#24178;&#25200;&#21464;&#21270;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#38024;&#23545;&#26597;&#35810;&#24207;&#21015;&#65292;&#25105;&#20204;&#36890;&#36807;&#27169;&#25311;&#22810;&#20010;&#25668;&#20687;&#26426;&#20301;&#32622;&#21019;&#24314;&#22810;&#20010;&#35270;&#35282;&#12290;&#23545;&#20110;&#25903;&#25345;&#24207;&#21015;&#65292;&#25105;&#20204;&#23558;&#20854;&#19982;&#27169;&#25311;&#20986;&#30340;&#26597;&#35810;&#24207;&#21015;&#36827;&#34892;&#21305;&#37197;&#65292;&#31867;&#20284;&#20110;&#27969;&#34892;&#30340;&#21160;&#24577;&#26102;&#38388;&#35268;&#25972;&#65288;DTW&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#27599;&#20010;&#25903;&#25345;&#26102;&#38388;&#22359;&#21487;&#20197;&#19982;&#35270;&#35282;&#27169;&#25311;&#30340;&#26597;&#35810;&#24207;&#21015;&#21305;&#37197;&#65292;&#22914;DTW&#12290;
&lt;/p&gt;
&lt;p&gt;
Video sequences exhibit significant nuisance variations (undesired effects) of speed of actions, temporal locations, and subjects' poses, leading to temporal-viewpoint misalignment when comparing two sets of frames or evaluating the similarity of two sequences. Thus, we propose Joint tEmporal and cAmera viewpoiNt alIgnmEnt (JEANIE) for sequence pairs. In particular, we focus on 3D skeleton sequences whose camera and subjects' poses can be easily manipulated in 3D. We evaluate JEANIE on skeletal Few-shot Action Recognition (FSAR), where matching well temporal blocks (temporal chunks that make up a sequence) of support-query sequence pairs (by factoring out nuisance variations) is essential due to limited samples of novel classes. Given a query sequence, we create its several views by simulating several camera locations. For a support sequence, we match it with view-simulated query sequences, as in the popular Dynamic Time Warping (DTW). Specifically, each support temporal block can be m
&lt;/p&gt;</description></item><item><title>Uni-RLHF&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#24179;&#21488;&#21644;&#22522;&#20934;&#22871;&#20214;&#65292;&#33268;&#21147;&#20110;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;RLHF&#20013;&#37327;&#21270;&#36827;&#23637;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#27880;&#37322;&#30028;&#38754;&#21644;&#31163;&#32447;&#22522;&#20934;&#23454;&#29616;&#12290;</title><link>https://arxiv.org/abs/2402.02423</link><description>&lt;p&gt;
Uni-RLHF: &#29992;&#20110;&#22810;&#26679;&#21270;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#36890;&#29992;&#24179;&#21488;&#21644;&#22522;&#20934;&#22871;&#20214;
&lt;/p&gt;
&lt;p&gt;
Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02423
&lt;/p&gt;
&lt;p&gt;
Uni-RLHF&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#24378;&#21270;&#23398;&#20064;&#24179;&#21488;&#21644;&#22522;&#20934;&#22871;&#20214;&#65292;&#33268;&#21147;&#20110;&#22788;&#29702;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#65292;&#35299;&#20915;&#20102;&#22312;RLHF&#20013;&#37327;&#21270;&#36827;&#23637;&#30340;&#25361;&#25112;&#65292;&#24182;&#25552;&#20379;&#20102;&#29992;&#25143;&#21451;&#22909;&#30340;&#27880;&#37322;&#30028;&#38754;&#21644;&#31163;&#32447;&#22522;&#20934;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#19982;&#20154;&#31867;&#21453;&#39304;&#65288;RLHF&#65289;&#36890;&#36807;&#19982;&#20154;&#31867;&#20559;&#22909;&#23545;&#40784;&#65292;&#36991;&#20813;&#20102;&#26114;&#36149;&#30340;&#25163;&#21160;&#22870;&#21169;&#35774;&#35745;&#65292;&#24050;&#32463;&#21463;&#21040;&#20102;&#24191;&#27867;&#20851;&#27880;&#12290;&#32771;&#34385;&#21040;&#19981;&#21516;&#29615;&#22659;&#20013;&#19981;&#21516;&#23398;&#20064;&#26041;&#27861;&#21644;&#22810;&#26679;&#21270;&#30340;&#20154;&#31867;&#21453;&#39304;&#31867;&#22411;&#23545;RLHF&#30340;&#36827;&#27493;&#36827;&#34892;&#37327;&#21270;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#32570;&#20047;&#26631;&#20934;&#21270;&#30340;&#27880;&#37322;&#24179;&#21488;&#21644;&#24191;&#27867;&#20351;&#29992;&#30340;&#32479;&#19968;&#22522;&#20934;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;Uni-RLHF&#65292;&#36825;&#26159;&#19968;&#20010;&#20026;RLHF&#37327;&#36523;&#23450;&#21046;&#30340;&#32508;&#21512;&#31995;&#32479;&#23454;&#29616;&#12290;&#23427;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#23436;&#25972;&#30340;&#20174;&#30495;&#23454;&#20154;&#31867;&#21453;&#39304;&#21040;&#23454;&#38469;&#38382;&#39064;&#21457;&#23637;&#30340;&#24037;&#20316;&#27969;&#12290;Uni-RLHF&#21253;&#21547;&#19977;&#20010;&#37096;&#20998;&#65306;1&#65289;&#36890;&#29992;&#30340;&#22810;&#21453;&#39304;&#27880;&#37322;&#24179;&#21488;&#65292;2&#65289;&#22823;&#35268;&#27169;&#30340;&#20247;&#21253;&#21453;&#39304;&#25968;&#25454;&#38598;&#65292;3&#65289;&#27169;&#22359;&#21270;&#30340;&#31163;&#32447;RLHF&#22522;&#20934;&#23454;&#29616;&#12290;Uni-RLHF&#24320;&#21457;&#20102;&#19968;&#20010;&#29992;&#25143;&#21451;&#22909;&#30340;&#27880;&#37322;&#30028;&#38754;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#21453;&#39304;&#31867;&#22411;&#65292;&#24182;&#19982;&#20027;&#35201;&#30340;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20860;&#23481;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) has received significant attention for performing tasks without the need for costly manual reward design by aligning human preferences. It is crucial to consider diverse human feedback types and various learning methods in different environments. However, quantifying progress in RLHF with diverse feedback is challenging due to the lack of standardized annotation platforms and widely used unified benchmarks. To bridge this gap, we introduce Uni-RLHF, a comprehensive system implementation tailored for RLHF. It aims to provide a complete workflow from real human feedback, fostering progress in the development of practical problems. Uni-RLHF contains three packages: 1) a universal multi-feedback annotation platform, 2) large-scale crowdsourced feedback datasets, and 3) modular offline RLHF baseline implementations. Uni-RLHF develops a user-friendly annotation interface tailored to various feedback types, compatible with a wide range of main
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;&#30340;&#20998;&#35299;&#26550;&#26500;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#33258;&#30456;&#20851;&#24615;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#26399;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20061;&#20010;&#38271;&#26399;&#22522;&#20934;&#19978;&#30340;&#22810;&#20010;&#23454;&#39564;&#20013;&#32988;&#36807;&#20102;14&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2402.02023</link><description>&lt;p&gt;
&#33258;&#30417;&#30563;&#23545;&#27604;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Contrastive Forecasting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.02023
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;&#30340;&#20998;&#35299;&#26550;&#26500;&#65292;&#24182;&#32467;&#21512;&#20840;&#23616;&#33258;&#30456;&#20851;&#24615;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#26469;&#35299;&#20915;&#38271;&#26399;&#39044;&#27979;&#20013;&#30340;&#25361;&#25112;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#20061;&#20010;&#38271;&#26399;&#22522;&#20934;&#19978;&#30340;&#22810;&#20010;&#23454;&#39564;&#20013;&#32988;&#36807;&#20102;14&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#26399;&#39044;&#27979;&#30001;&#20110;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#26102;&#38388;&#21644;&#20869;&#23384;&#22797;&#26434;&#24615;&#32780;&#38754;&#20020;&#29420;&#29305;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#28369;&#21160;&#31383;&#21475;&#26469;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#38590;&#20197;&#26377;&#25928;&#25429;&#25417;&#37096;&#20998;&#22312;&#30701;&#31383;&#21475;&#20869;&#34987;&#25429;&#25417;&#21040;&#30340;&#38271;&#26399;&#21464;&#21270;&#65288;&#21363;&#22806;&#31383;&#21475;&#21464;&#21270;&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#37319;&#29992;&#23545;&#27604;&#23398;&#20064;&#21644;&#22686;&#24378;&#30340;&#20998;&#35299;&#26550;&#26500;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#32858;&#28966;&#38271;&#26399;&#21464;&#21270;&#65292;&#20174;&#32780;&#20811;&#26381;&#20102;&#36825;&#20010;&#38480;&#21046;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#30340;&#23545;&#27604;&#25439;&#22833;&#23558;&#25972;&#20010;&#26102;&#38388;&#24207;&#21015;&#20013;&#30340;&#20840;&#23616;&#33258;&#30456;&#20851;&#24615;&#32435;&#20837;&#32771;&#34385;&#65292;&#20197;&#33258;&#30417;&#30563;&#26041;&#24335;&#26500;&#24314;&#27491;&#36127;&#23545;&#12290;&#24403;&#19982;&#25105;&#20204;&#30340;&#20998;&#35299;&#32593;&#32476;&#32467;&#21512;&#20351;&#29992;&#26102;&#65292;&#25105;&#20204;&#30340;&#23545;&#27604;&#23398;&#20064;&#26174;&#33879;&#25552;&#39640;&#20102;&#38271;&#26399;&#39044;&#27979;&#24615;&#33021;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20061;&#20010;&#38271;&#26399;&#22522;&#20934;&#19978;&#30340;&#22810;&#20010;&#23454;&#39564;&#20013;&#32988;&#36807;&#20102;14&#20010;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Long-term forecasting presents unique challenges due to the time and memory complexity of handling long sequences. Existing methods, which rely on sliding windows to process long sequences, struggle to effectively capture long-term variations that are partially caught within the short window (i.e., outer-window variations). In this paper, we introduce a novel approach that overcomes this limitation by employing contrastive learning and enhanced decomposition architecture, specifically designed to focus on long-term variations. To this end, our contrastive loss incorporates global autocorrelation held in the whole time series, which facilitates the construction of positive and negative pairs in a self-supervised manner. When combined with our decomposition networks, our contrastive learning significantly improves long-term forecasting performance. Extensive experiments demonstrate that our approach outperforms 14 baseline models in multiple experiments over nine long-term benchmarks, es
&lt;/p&gt;</description></item><item><title>Sandra&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#22120;&#65292;&#36890;&#36807;&#23558;&#30690;&#37327;&#34920;&#31034;&#19982;&#28436;&#32462;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#26412;&#20307;&#35770;&#24314;&#31435;&#30340;&#21521;&#37327;&#31354;&#38388;&#36827;&#34892;&#25512;&#29702;&#12290;&#23427;&#22522;&#20110;&#25551;&#36848;&#21644;&#24773;&#22659;&#30340;&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;&#65292;&#33021;&#22815;&#20174;&#19968;&#32452;&#20107;&#23454;&#20013;&#25512;&#26029;&#20986;&#25152;&#26377;&#21487;&#33021;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#22312;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#32447;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21521;&#37327;&#31354;&#38388;&#30340;&#21487;&#25511;&#24615;&#12290;</title><link>https://arxiv.org/abs/2402.00591</link><description>&lt;p&gt;
Sandra -- &#22522;&#20110;&#25551;&#36848;&#21644;&#24773;&#22659;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#22120;
&lt;/p&gt;
&lt;p&gt;
Sandra -- A Neuro-Symbolic Reasoner Based On Descriptions And Situations
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2402.00591
&lt;/p&gt;
&lt;p&gt;
Sandra&#26159;&#19968;&#20010;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#22120;&#65292;&#36890;&#36807;&#23558;&#30690;&#37327;&#34920;&#31034;&#19982;&#28436;&#32462;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;&#26412;&#20307;&#35770;&#24314;&#31435;&#30340;&#21521;&#37327;&#31354;&#38388;&#36827;&#34892;&#25512;&#29702;&#12290;&#23427;&#22522;&#20110;&#25551;&#36848;&#21644;&#24773;&#22659;&#30340;&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;&#65292;&#33021;&#22815;&#20174;&#19968;&#32452;&#20107;&#23454;&#20013;&#25512;&#26029;&#20986;&#25152;&#26377;&#21487;&#33021;&#30340;&#35299;&#37322;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#22312;&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#24773;&#20917;&#19979;&#20248;&#20110;&#20854;&#20182;&#22522;&#20934;&#32447;&#30340;&#20998;&#31867;&#32467;&#26524;&#65292;&#24182;&#19988;&#20855;&#26377;&#21487;&#35299;&#37322;&#24615;&#21644;&#21521;&#37327;&#31354;&#38388;&#30340;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;Sandra&#65292;&#36825;&#26159;&#19968;&#20010;&#23558;&#30690;&#37327;&#34920;&#31034;&#19982;&#28436;&#32462;&#25512;&#29702;&#30456;&#32467;&#21512;&#30340;&#31070;&#32463;&#31526;&#21495;&#25512;&#29702;&#22120;&#12290;Sandra&#20351;&#29992;&#26412;&#20307;&#35770;&#24314;&#31435;&#20102;&#19968;&#20010;&#21463;&#38480;&#30340;&#21521;&#37327;&#31354;&#38388;&#65292;&#24182;&#22312;&#20854;&#20013;&#36827;&#34892;&#25512;&#29702;&#12290;&#25512;&#29702;&#22120;&#30340;&#20960;&#20309;&#29305;&#24615;&#20351;&#24471;&#23427;&#33021;&#22815;&#19982;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#36215;&#26469;&#65292;&#24357;&#21512;&#20102;&#31526;&#21495;&#30693;&#35782;&#34920;&#36798;&#19982;&#31070;&#32463;&#32593;&#32476;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;Sandra&#22522;&#20110;&#25551;&#36848;&#21644;&#24773;&#22659;(DnS)&#26412;&#20307;&#35774;&#35745;&#27169;&#24335;&#65292;&#23427;&#26159;&#19968;&#31181;&#26694;&#26550;&#35821;&#20041;&#30340;&#24418;&#24335;&#21270;&#12290;&#32473;&#23450;&#19968;&#32452;&#20107;&#23454;(&#24773;&#22659;)&#65292;&#23427;&#33021;&#22815;&#25512;&#26029;&#20986;&#25152;&#26377;&#21487;&#33021;&#30340;&#36879;&#35270;&#22270;(&#25551;&#36848;)&#65292;&#20026;&#20854;&#25552;&#20379;&#19968;&#20010;&#21512;&#29702;&#30340;&#35299;&#37322;&#65292;&#21363;&#20351;&#22312;&#20449;&#24687;&#19981;&#23436;&#25972;&#30340;&#24773;&#20917;&#19979;&#20063;&#33021;&#20570;&#21040;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;DnS&#27169;&#22411;&#19978;&#26159;&#27491;&#30830;&#30340;&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#19981;&#21516;&#20219;&#21153;&#21450;&#20854;&#26631;&#20934;&#22522;&#20934;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;Sandra&#19981;&#22686;&#21152;&#22797;&#26434;&#24615;&#30340;&#24773;&#20917;&#19979;&#65306;(i)&#20248;&#20110;&#25152;&#26377;&#22522;&#20934;&#32447;&#65307;(ii) &#22312;&#20998;&#31867;&#36807;&#31243;&#20013;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#65307;(iii)&#23545;&#21521;&#37327;&#31354;&#38388;&#20855;&#26377;&#21487;&#25511;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents sandra, a neuro-symbolic reasoner combining vectorial representations with deductive reasoning. Sandra builds a vector space constrained by an ontology and performs reasoning over it. The geometric nature of the reasoner allows its combination with neural networks, bridging the gap with symbolic knowledge representations. Sandra is based on the Description and Situation (DnS) ontology design pattern, a formalization of frame semantics. Given a set of facts (a situation) it allows to infer all possible perspectives (descriptions) that can provide a plausible interpretation for it, even in presence of incomplete information. We prove that our method is correct with respect to the DnS model. We experiment with two different tasks and their standard benchmarks, demonstrating that, without increasing complexity, sandra (i) outperforms all the baselines (ii) provides interpretability in the classification process, and (iii) allows control over the vector space, which is d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>https://arxiv.org/abs/2401.17548</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#36890;&#36947;&#30456;&#20851;&#24615;&#65306;&#20174;&#39046;&#20808;&#25351;&#26631;&#20013;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Rethinking Channel Dependence for Multivariate Time Series Forecasting: Learning from Leading Indicators
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.17548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;LIFT&#65292;&#36890;&#36807;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#21644;&#39046;&#20808;&#25351;&#26631;&#65292;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#25552;&#20379;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;LIFT&#26041;&#27861;&#21487;&#20197;&#26080;&#32541;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#21327;&#20316;&#65292;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#29420;&#31435;&#20110;&#36890;&#36947;&#30340;&#26041;&#27861;&#22312;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#65288;MTS&#65289;&#39044;&#27979;&#20013;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;&#23613;&#31649;&#36825;&#20123;&#26041;&#27861;&#20943;&#23569;&#20102;&#36807;&#25311;&#21512;&#30340;&#39118;&#38505;&#65292;&#20294;&#23427;&#20204;&#38169;&#36807;&#20102;&#21033;&#29992;&#36890;&#36947;&#30456;&#20851;&#24615;&#36827;&#34892;&#20934;&#30830;&#39044;&#27979;&#30340;&#28508;&#22312;&#26426;&#20250;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22312;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#23616;&#37096;&#24179;&#31283;&#30340;&#39046;&#20808;-&#28382;&#21518;&#20851;&#31995;&#65292;&#21363;&#19968;&#20123;&#28382;&#21518;&#21464;&#37327;&#22312;&#30701;&#26102;&#38388;&#20869;&#21487;&#33021;&#36981;&#24490;&#39046;&#20808;&#25351;&#26631;&#12290;&#21033;&#29992;&#36825;&#31181;&#36890;&#36947;&#30456;&#20851;&#24615;&#26159;&#26377;&#30410;&#30340;&#65292;&#22240;&#20026;&#39046;&#20808;&#25351;&#26631;&#25552;&#20379;&#20102;&#20808;&#36827;&#20449;&#24687;&#65292;&#21487;&#20197;&#29992;&#26469;&#20943;&#23569;&#28382;&#21518;&#21464;&#37327;&#30340;&#39044;&#27979;&#38590;&#24230;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;LIFT&#30340;&#26032;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#39318;&#20808;&#22312;&#27599;&#20010;&#26102;&#38388;&#27493;&#39588;&#39640;&#25928;&#22320;&#20272;&#35745;&#39046;&#20808;&#25351;&#26631;&#21450;&#20854;&#39046;&#20808;&#27493;&#39588;&#65292;&#28982;&#21518;&#24039;&#22937;&#22320;&#20801;&#35768;&#28382;&#21518;&#21464;&#37327;&#21033;&#29992;&#26469;&#33258;&#39046;&#20808;&#25351;&#26631;&#30340;&#20808;&#36827;&#20449;&#24687;&#12290;LIFT&#20316;&#20026;&#19968;&#20010;&#25554;&#20214;&#65292;&#21487;&#20197;&#19982;&#20219;&#24847;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#26041;&#27861;&#26080;&#32541;&#21327;&#20316;&#12290;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#20102;LIFT&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, channel-independent methods have achieved state-of-the-art performance in multivariate time series (MTS) forecasting. Despite reducing overfitting risks, these methods miss potential opportunities in utilizing channel dependence for accurate predictions. We argue that there exist locally stationary lead-lag relationships between variates, i.e., some lagged variates may follow the leading indicators within a short time period. Exploiting such channel dependence is beneficial since leading indicators offer advance information that can be used to reduce the forecasting difficulty of the lagged variates. In this paper, we propose a new method named LIFT that first efficiently estimates leading indicators and their leading steps at each time step and then judiciously allows the lagged variates to utilize the advance information from leading indicators. LIFT plays as a plugin that can be seamlessly collaborated with arbitrary time series forecasting methods. Extensive experiments o
&lt;/p&gt;</description></item><item><title>LCV2&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20860;&#23481;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#20110;&#35270;&#35273;&#38382;&#31572;&#30340;&#26694;&#26550;&#65292;&#26080;&#38656;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#20302;&#35745;&#31639;&#36164;&#28304;&#19979;&#30340;&#20219;&#21153;</title><link>https://arxiv.org/abs/2401.15842</link><description>&lt;p&gt;
LCV2&#65306;&#19968;&#31181;&#39640;&#25928;&#30340;&#20813;&#39044;&#35757;&#32451;&#26694;&#26550;&#29992;&#20110;&#22522;&#20110;&#35270;&#35273;&#30340;&#38382;&#31572;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;
LCV2: An Efficient Pretraining-Free Framework for Grounded Visual Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.15842
&lt;/p&gt;
&lt;p&gt;
LCV2&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#20860;&#23481;&#21508;&#31181;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#22522;&#20110;&#35270;&#35273;&#38382;&#31572;&#30340;&#26694;&#26550;&#65292;&#26080;&#38656;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#36866;&#29992;&#20110;&#20302;&#35745;&#31639;&#36164;&#28304;&#19979;&#30340;&#20219;&#21153;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;LCV2&#27169;&#22359;&#21270;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#35270;&#35273;-&#35821;&#35328;&#22810;&#27169;&#22495;&#20013;&#30340;&#22522;&#20110;&#35270;&#35273;&#30340;&#38382;&#31572;&#20219;&#21153;&#12290;&#35813;&#26041;&#27861;&#20381;&#36182;&#20110;&#19968;&#20010;&#20923;&#32467;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#20316;&#20026;&#29616;&#25104;VQA&#27169;&#22411;&#21644;&#29616;&#25104;&#35270;&#35273;&#23450;&#20301;&#65288;VG&#65289;&#27169;&#22411;&#20043;&#38388;&#30340;&#20013;&#38388;&#20171;&#36136;&#65292;LLM&#22522;&#20110;&#35774;&#35745;&#30340;&#25552;&#31034;&#36716;&#25442;&#21644;&#20256;&#36882;&#25991;&#26412;&#20449;&#24687;&#12290;LCV2&#24314;&#31435;&#20102;&#19968;&#20010;&#38598;&#25104;&#30340;&#21363;&#25554;&#21363;&#29992;&#26694;&#26550;&#65292;&#26080;&#38656;&#20219;&#20309;&#39044;&#35757;&#32451;&#36807;&#31243;&#65292;&#21487;&#20197;&#22312;&#20302;&#35745;&#31639;&#36164;&#28304;&#19979;&#29992;&#20110;VQA Grounding&#20219;&#21153;&#12290;&#35813;&#26694;&#26550;&#20801;&#35768;&#19982;&#21508;&#31181;&#26368;&#20808;&#36827;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#19968;&#36215;&#20351;&#29992;&#65292;&#20855;&#26377;&#19982;&#26102;&#20465;&#36827;&#30340;&#28508;&#21147;&#12290;&#22312;&#21463;&#38480;&#21046;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#20869;&#23384;&#36164;&#28304;&#19979;&#36827;&#34892;&#20102;&#23454;&#39564;&#23454;&#29616;&#65292;&#35780;&#20272;&#20102;&#25152;&#25552;&#26041;&#27861;&#22312;&#22522;&#20934;&#19978;&#30340;&#24615;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.15842v2 Announce Type: replace-cross  Abstract: In this paper, the LCV2 modular method is proposed for the Grounded Visual Question Answering task in the vision-language multimodal domain. This approach relies on a frozen large language model (LLM) as intermediate mediator between the off-the-shelf VQA model and the off-the-shelf visual grounding (VG) model, where the LLM transforms and conveys textual information between the two modules based on a designed prompt. LCV2 establish an integrated plug-and-play framework without the need for any pre-training process. This framework can be deployed for VQA Grounding tasks under low computational resources. The modularized model within the framework allows application with various state-of-the-art pre-trained models, exhibiting significant potential to be advance with the times. Experimental implementations were conducted under constrained computational and memory resources, evaluating the proposed method's performance on benchmar
&lt;/p&gt;</description></item><item><title>Temp-Lora&#26041;&#27861;&#36890;&#36807;&#22312;&#38271;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#36880;&#27493;&#35757;&#32451;&#20020;&#26102;Lora&#27169;&#22359;&#65292;&#26377;&#25928;&#20445;&#30041;&#19978;&#19979;&#25991;&#30693;&#35782;&#24182;&#36991;&#20813;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#27704;&#20037;&#24615;&#25913;&#21464;&#12290;</title><link>https://arxiv.org/abs/2401.11504</link><description>&lt;p&gt;
&#38543;&#30528;&#25991;&#26412;&#37327;&#22686;&#21152;&#65292;&#25512;&#26029;&#35757;&#32451;&#26377;&#21161;&#20110;&#38271;&#25991;&#26412;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
With Greater Text Comes Greater Necessity: Inference-Time Training Helps Long Text Generation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.11504
&lt;/p&gt;
&lt;p&gt;
Temp-Lora&#26041;&#27861;&#36890;&#36807;&#22312;&#38271;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#36880;&#27493;&#35757;&#32451;&#20020;&#26102;Lora&#27169;&#22359;&#65292;&#26377;&#25928;&#20445;&#30041;&#19978;&#19979;&#25991;&#30693;&#35782;&#24182;&#36991;&#20813;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#27704;&#20037;&#24615;&#25913;&#21464;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38271;&#25991;&#26412;&#29983;&#25104;&#65292;&#22914;&#23567;&#35828;&#21019;&#20316;&#21644;&#20855;&#26377;&#26497;&#38271;&#19978;&#19979;&#25991;&#30340;&#31687;&#31456;&#32423;&#32763;&#35793;&#65292;&#23545;&#24403;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#25552;&#20986;&#20102;&#37325;&#22823;&#25361;&#25112;&#12290;&#29616;&#26377;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#36890;&#36807;&#38271;&#24230;&#22806;&#25512;&#31561;&#31574;&#30053;&#25193;&#23637;&#27169;&#22411;&#30340;&#19978;&#19979;&#25991;&#31383;&#21475;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#35757;&#32451;&#21644;/&#25110;&#25512;&#26029;&#38454;&#27573;&#35201;&#27714;&#22823;&#37327;&#30828;&#20214;&#36164;&#28304;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;Temp-Lora&#24341;&#20837;&#20102;&#19968;&#20010;&#26367;&#20195;&#27010;&#24565;&#12290;&#25105;&#20204;&#19981;&#20381;&#36182;&#20110;KV&#32531;&#23384;&#23384;&#20648;&#25152;&#26377;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#32780;&#26159;&#23558;&#36825;&#20123;&#20449;&#24687;&#30452;&#25509;&#23884;&#20837;&#20020;&#26102;Lora&#27169;&#22359;&#20013;&#12290;&#22312;&#38271;&#25991;&#26412;&#29983;&#25104;&#36807;&#31243;&#20013;&#65292;&#36825;&#20010;&#27169;&#22359;&#20250;&#38543;&#30528;&#20808;&#21069;&#29983;&#25104;&#30340;&#25991;&#26412;&#36880;&#28176;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#26377;&#25928;&#22320;&#20445;&#30041;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#36824;&#38450;&#27490;&#20102;&#23545;&#27169;&#22411;&#21442;&#25968;&#30340;&#20219;&#20309;&#27704;&#20037;&#24615;&#25913;&#21464;&#65292;&#22240;&#20026;&#27169;&#22359;&#22312;&#29983;&#25104;&#21518;&#34987;&#20002;&#24323;&#12290;&#22312;PG19&#35821;&#35328;&#24314;&#27169;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.11504v2 Announce Type: replace-cross  Abstract: Long text generation, such as novel writing and discourse-level translation with extremely long contexts, presents significant challenges to current language models. Existing methods mainly focus on extending the model's context window through strategies like length extrapolation. However, these approaches demand substantial hardware resources during the training and/or inference phases. Our proposed method, Temp-Lora, introduces an alternative concept. Instead of relying on the KV cache to store all context information, we embeds this information directly into a temporary Lora module. In the process of long text generation, this module is progressively trained with text generated previously. This approach not only efficiently preserves contextual knowledge but also prevents any permanent alteration to the model's parameters given that the module is discarded post-generation. Extensive experiments on the PG19 language modeling 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#24615;&#25913;&#36827;&#25216;&#26415;&#22312;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;</title><link>https://arxiv.org/abs/2401.03695</link><description>&lt;p&gt;
&#19968;&#39033;&#20851;&#20110;&#25913;&#21892;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#20844;&#24179;&#24615;&#30340;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A Large-Scale Empirical Study on Improving the Fairness of Image Classification Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2401.03695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36827;&#34892;&#20102;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#27604;&#36739;&#20102;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#24615;&#25913;&#36827;&#25216;&#26415;&#22312;&#22270;&#20687;&#20998;&#31867;&#27169;&#22411;&#19978;&#30340;&#34920;&#29616;&#65292;&#25581;&#31034;&#20102;&#23427;&#20204;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20844;&#24179;&#24615;&#19968;&#30452;&#26159;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#34987;&#37319;&#32435;&#30340;&#20851;&#38190;&#38382;&#39064;&#12290;&#20026;&#20102;&#25552;&#39640;&#27169;&#22411;&#30340;&#20844;&#24179;&#24615;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#35777;&#23454;&#22312;&#21508;&#33258;&#30340;&#24773;&#22659;&#20013;&#26159;&#26377;&#25928;&#30340;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#20043;&#38388;&#20173;&#28982;&#27809;&#26377;&#36827;&#34892;&#31995;&#32479;&#30340;&#35780;&#20272;&#20197;&#20415;&#22312;&#30456;&#21516;&#24773;&#22659;&#19979;&#36827;&#34892;&#20840;&#38754;&#27604;&#36739;&#65292;&#36825;&#20351;&#24471;&#29702;&#35299;&#23427;&#20204;&#20043;&#38388;&#30340;&#24615;&#33021;&#24046;&#24322;&#21464;&#24471;&#22256;&#38590;&#65292;&#38459;&#30861;&#20102;&#30740;&#31350;&#36827;&#23637;&#21644;&#23454;&#38469;&#24212;&#29992;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#26412;&#25991;&#33268;&#21147;&#20110;&#36827;&#34892;&#39318;&#27425;&#22823;&#35268;&#27169;&#23454;&#35777;&#30740;&#31350;&#65292;&#20840;&#38754;&#27604;&#36739;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#20844;&#24179;&#24615;&#25913;&#36827;&#25216;&#26415;&#30340;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#38024;&#23545;&#24191;&#27867;&#20351;&#29992;&#30340;&#22270;&#20687;&#20998;&#31867;&#24212;&#29992;&#22330;&#26223;&#65292;&#21033;&#29992;&#19977;&#20010;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#20116;&#31181;&#24120;&#29992;&#24615;&#33021;&#25351;&#26631;&#65292;&#24635;&#20849;&#35780;&#20272;&#20102;&#26469;&#33258;&#19981;&#21516;&#31867;&#21035;&#30340; 13 &#31181;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#25581;&#31034;&#20102;&#19981;&#21516;&#26041;&#27861;&#20043;&#38388;&#30340;&#26174;&#33879;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2401.03695v2 Announce Type: replace-cross  Abstract: Fairness has been a critical issue that affects the adoption of deep learning models in real practice. To improve model fairness, many existing methods have been proposed and evaluated to be effective in their own contexts. However, there is still no systematic evaluation among them for a comprehensive comparison under the same context, which makes it hard to understand the performance distinction among them, hindering the research progress and practical adoption of them. To fill this gap, this paper endeavours to conduct the first large-scale empirical study to comprehensively compare the performance of existing state-of-the-art fairness improving techniques. Specifically, we target the widely-used application scenario of image classification, and utilized three different datasets and five commonly-used performance metrics to assess in total 13 methods from diverse categories. Our findings reveal substantial variations in the 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;SurgicalPart-SAM (SP-SAM)&#65292;&#36890;&#36807;&#25972;&#21512;&#22120;&#26800;&#32467;&#26500;&#30693;&#35782;&#19982;SAM&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#35843;&#25972;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22120;&#26800;&#32454;&#33410;&#21644;&#32467;&#26500;&#25551;&#36848;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2312.14481</link><description>&lt;p&gt;
&#25163;&#26415;&#22120;&#26800;&#37096;&#20998;-&#25972;&#20307;&#21327;&#20316;&#25552;&#31034;&#29992;&#20110;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
SurgicalPart-SAM: Part-to-Whole Collaborative Prompting for Surgical Instrument Segmentation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14481
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#30340;&#26032;&#26041;&#27861;SurgicalPart-SAM (SP-SAM)&#65292;&#36890;&#36807;&#25972;&#21512;&#22120;&#26800;&#32467;&#26500;&#30693;&#35782;&#19982;SAM&#30340;&#36890;&#29992;&#30693;&#35782;&#65292;&#23454;&#29616;&#20102;&#26377;&#25928;&#35843;&#25972;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#22120;&#26800;&#32454;&#33410;&#21644;&#32467;&#26500;&#25551;&#36848;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Segment Anything Model&#65288;SAM&#65289;&#22312;&#36890;&#29992;&#23545;&#35937;&#20998;&#21106;&#26041;&#38754;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#24182;&#20026;&#21508;&#31181;&#24212;&#29992;&#25552;&#20379;&#20102;&#21487;&#33021;&#24615;&#12290;&#29616;&#26377;&#26041;&#27861;&#24050;&#23558;SAM&#24212;&#29992;&#20110;&#25163;&#26415;&#22120;&#26800;&#20998;&#21106;&#65288;SIS&#65289;&#65292;&#36890;&#36807;&#20351;&#29992;&#25163;&#26415;&#25968;&#25454;&#35843;&#25972;&#22522;&#20110;SAM&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#22312;&#20004;&#20010;&#20851;&#38190;&#26041;&#38754;&#23384;&#22312;&#19981;&#36275;&#65306;&#65288;1&#65289;&#20351;&#29992;&#22120;&#26800;&#25513;&#27169;&#30452;&#25509;&#35843;&#25972;&#27169;&#22411;&#23558;&#27599;&#20010;&#22120;&#26800;&#35270;&#20026;&#21333;&#20010;&#23454;&#20307;&#65292;&#24573;&#30053;&#20102;&#20854;&#22797;&#26434;&#32467;&#26500;&#21644;&#32454;&#31890;&#24230;&#32454;&#33410;&#65307;&#65288;2&#65289;&#22522;&#20110;&#22120;&#26800;&#31867;&#21035;&#30340;&#25552;&#31034;&#19981;&#22815;&#28789;&#27963;&#21644;&#20449;&#24687;&#20016;&#23500;&#65292;&#26080;&#27861;&#25551;&#36848;&#22120;&#26800;&#32467;&#26500;&#12290;&#20026;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#25552;&#31034;&#30340;SIS&#65292;&#24182;&#25552;&#20986;&#20102;&#25163;&#26415;&#22120;&#26800;&#37096;&#20998;-SAM&#65288;SP-SAM&#65289;&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;SAM&#39640;&#25928;&#35843;&#25972;&#26041;&#27861;&#65292;&#26126;&#30830;&#23558;&#22120;&#26800;&#32467;&#26500;&#30693;&#35782;&#19982;SAM&#30340;&#36890;&#29992;&#30693;&#35782;&#25972;&#21512;&#22312;&#19968;&#36215;&#65292;&#30001;&#22120;&#26800;&#37096;&#20998;&#32452;&#25104;&#30340;&#19987;&#23478;&#30693;&#35782;&#25351;&#23548;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#36890;&#36807;&#25552;&#20986;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14481v2 Announce Type: replace-cross  Abstract: The Segment Anything Model (SAM) exhibits promise in generic object segmentation and offers potential for various applications. Existing methods have applied SAM to surgical instrument segmentation (SIS) by tuning SAM-based frameworks with surgical data. However, they fall short in two crucial aspects: (1) Straightforward model tuning with instrument masks treats each instrument as a single entity, neglecting their complex structures and fine-grained details; and (2) Instrument category-based prompts are not flexible and informative enough to describe instrument structures. To address these problems, in this paper, we investigate text promptable SIS and propose SurgicalPart-SAM (SP-SAM), a novel SAM efficient-tuning approach that explicitly integrates instrument structure knowledge with SAM's generic knowledge, guided by expert knowledge on instrument part compositions. Specifically, we achieve this by proposing (1) Collaborati
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#26512;&#22270;&#20687;&#19982;&#25991;&#26412;&#20013;&#30340;&#23545;&#35937;&#21644;&#23646;&#24615;&#65292;&#20351;&#29992;&#22810;&#26631;&#31614;&#20998;&#31867;&#25439;&#22833;&#26469;&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#27169;&#22411;</title><link>https://arxiv.org/abs/2312.14149</link><description>&lt;p&gt;
TagAlign&#65306;&#21033;&#29992;&#22810;&#26631;&#31614;&#20998;&#31867;&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;
&lt;/p&gt;
&lt;p&gt;
TagAlign: Improving Vision-Language Alignment with Multi-Tag Classification
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.14149
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#35299;&#26512;&#22270;&#20687;&#19982;&#25991;&#26412;&#20013;&#30340;&#23545;&#35937;&#21644;&#23646;&#24615;&#65292;&#20351;&#29992;&#22810;&#26631;&#31614;&#20998;&#31867;&#25439;&#22833;&#26469;&#25913;&#36827;&#35270;&#35273;-&#35821;&#35328;&#23545;&#40784;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#30340;&#20851;&#38190;&#22312;&#20110;&#20174;&#35270;&#35273;&#21644;&#35821;&#35328;&#25968;&#25454;&#20013;&#25552;&#21462;&#35821;&#20041;&#23545;&#40784;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#24120;&#31616;&#21333;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#23545;&#40784;&#22270;&#20687;&#21644;&#25991;&#26412;&#29305;&#24449;&#65292;&#32780;&#26080;&#38656;&#38500;&#22270;&#20687;-&#25991;&#26412;&#23545;&#20043;&#22806;&#30340;&#20854;&#20182;&#25968;&#25454;&#26684;&#24335;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#19968;&#24133;&#22270;&#20687;&#21450;&#20854;&#37197;&#23545;&#30340;&#25991;&#26412;&#65292;&#25105;&#20204;&#35774;&#27861;&#20174;&#25551;&#36848;&#20013;&#35299;&#26512;&#20986;&#23545;&#35937;&#65288;&#20363;&#22914;&#29483;&#65289;&#21644;&#23646;&#24615;&#65288;&#20363;&#22914;&#40657;&#33394;&#65289;&#65292;&#36825;&#20123;&#23545;&#35937;&#21644;&#23646;&#24615;&#26497;&#26377;&#21487;&#33021;&#23384;&#22312;&#20110;&#22270;&#20687;&#20013;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#35299;&#26512;&#31649;&#36947;&#23436;&#20840;&#33258;&#21160;&#21270;&#65292;&#22240;&#27492;&#20855;&#26377;&#33391;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#12290;&#20511;&#21161;&#36825;&#20123;&#35299;&#26512;&#20986;&#30340;&#35821;&#20041;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#65292;&#25105;&#20204;&#21487;&#20197;&#23558;&#24120;&#29992;&#30340;&#22270;&#20687;-&#25991;&#26412;&#23545;&#27604;&#25439;&#22833;&#19982;&#22810;&#26631;&#31614;&#20998;&#31867;&#25439;&#22833;&#30456;&#32467;&#21512;&#12290;&#22312;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#20013;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.14149v3 Announce Type: replace-cross  Abstract: The crux of learning vision-language models is to extract semantically aligned information from visual and linguistic data. Existing attempts usually face the problem of coarse alignment, e.g., the vision encoder struggles in localizing an attribute-specified object. In this work, we propose an embarrassingly simple approach to better align image and text features with no need of additional data formats other than image-text pairs. Concretely, given an image and its paired text, we manage to parse objects (\textit{e.g.}, cat) and attributes (\textit{e.g.}, black) from the description, which are highly likely to exist in the image. It is noteworthy that the parsing pipeline is fully automatic and thus enjoys good scalability. With these parsed semantics as supervision signals, we can complement the commonly used image-text contrastive loss with the multi-tag classification loss. Extensive experimental results on a broad suite of
&lt;/p&gt;</description></item><item><title>PIA&#36890;&#36807;&#25554;&#25300;&#24335;&#27169;&#22359;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#22270;&#20687;&#21160;&#30011;&#65292;&#24182;&#35299;&#20915;&#20102;&#20445;&#30041;&#29420;&#29305;&#39118;&#26684;&#12289;&#39640;&#20445;&#30495;&#32454;&#33410;&#21644;&#21160;&#20316;&#21487;&#25511;&#24615;&#30340;&#25361;&#25112;</title><link>https://arxiv.org/abs/2312.13964</link><description>&lt;p&gt;
PIA:&#36890;&#36807;&#25554;&#25300;&#24335;&#27169;&#22359;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#22270;&#20687;&#21160;&#30011;
&lt;/p&gt;
&lt;p&gt;
PIA: Your Personalized Image Animator via Plug-and-Play Modules in Text-to-Image Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.13964
&lt;/p&gt;
&lt;p&gt;
PIA&#36890;&#36807;&#25554;&#25300;&#24335;&#27169;&#22359;&#22312;&#25991;&#26412;&#21040;&#22270;&#20687;&#27169;&#22411;&#20013;&#23454;&#29616;&#20010;&#24615;&#21270;&#22270;&#20687;&#21160;&#30011;&#65292;&#24182;&#35299;&#20915;&#20102;&#20445;&#30041;&#29420;&#29305;&#39118;&#26684;&#12289;&#39640;&#20445;&#30495;&#32454;&#33410;&#21644;&#21160;&#20316;&#21487;&#25511;&#24615;&#30340;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#20010;&#24615;&#21270;&#25991;&#26412;&#21040;&#22270;&#20687;&#65288;T2I&#65289;&#27169;&#22411;&#30340;&#36827;&#23637;&#24050;&#32463;&#24443;&#24213;&#25913;&#21464;&#20102;&#20869;&#23481;&#21019;&#20316;&#65292;&#20351;&#38750;&#19987;&#23478;&#33021;&#22815;&#29983;&#25104;&#20855;&#26377;&#29420;&#29305;&#39118;&#26684;&#30340;&#24778;&#20154;&#22270;&#20687;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#25991;&#26412;&#20026;&#36825;&#20123;&#20010;&#24615;&#21270;&#22270;&#20687;&#22686;&#21152;&#36924;&#30495;&#30340;&#21160;&#20316;&#22312;&#20445;&#30041;&#29420;&#29305;&#39118;&#26684;&#12289;&#39640;&#20445;&#30495;&#32454;&#33410;&#21644;&#36890;&#36807;&#25991;&#26412;&#23454;&#29616;&#21160;&#20316;&#21487;&#25511;&#24615;&#26041;&#38754;&#38754;&#20020;&#30528;&#37325;&#22823;&#25361;&#25112;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PIA&#65292;&#19968;&#31181;&#20010;&#24615;&#21270;&#22270;&#20687;&#21160;&#30011;&#29983;&#25104;&#22120;&#65292;&#20854;&#22312;&#19982;&#26465;&#20214;&#22270;&#20687;&#23545;&#40784;&#12289;&#36890;&#36807;&#25991;&#26412;&#23454;&#29616;&#21160;&#20316;&#21487;&#25511;&#24615;&#20197;&#21450;&#19982;&#21508;&#31181;&#20010;&#24615;&#21270;T2I&#27169;&#22411;&#30340;&#20860;&#23481;&#24615;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#26080;&#38656;&#29305;&#23450;&#35843;&#25972;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20123;&#30446;&#26631;&#65292;PIA&#22312;&#22522;&#30784;T2I&#27169;&#22411;&#30340;&#22522;&#30784;&#19978;&#26500;&#24314;&#20102;&#32463;&#36807;&#33391;&#22909;&#35757;&#32451;&#30340;&#26102;&#38388;&#23545;&#40784;&#23618;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#20219;&#20309;&#20010;&#24615;&#21270;T2I&#27169;&#22411;&#21521;&#22270;&#20687;&#21160;&#30011;&#27169;&#22411;&#30340;&#26080;&#32541;&#36716;&#25442;&#12290;PIA&#30340;&#19968;&#20010;&#20851;&#38190;&#32452;&#20214;&#26159;&#24341;&#20837;&#26465;&#20214;&#27169;&#22359;&#65292;&#21033;&#29992;&#26465;&#20214;&#24103;&#21644;&#24103;&#38388;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.13964v2 Announce Type: replace-cross  Abstract: Recent advancements in personalized text-to-image (T2I) models have revolutionized content creation, empowering non-experts to generate stunning images with unique styles. While promising, adding realistic motions into these personalized images by text poses significant challenges in preserving distinct styles, high-fidelity details, and achieving motion controllability by text. In this paper, we present PIA, a Personalized Image Animator that excels in aligning with condition images, achieving motion controllability by text, and the compatibility with various personalized T2I models without specific tuning. To achieve these goals, PIA builds upon a base T2I model with well-trained temporal alignment layers, allowing for the seamless transformation of any personalized T2I model into an image animation model. A key component of PIA is the introduction of the condition module, which utilizes the condition frame and inter-frame af
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;MARL&#20195;&#29702;&#65292;&#30740;&#31350;&#20102;&#20182;&#20204;&#23398;&#20064;&#36991;&#35753;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#65292;&#22312;&#23494;&#24230;&#19981;&#22826;&#39640;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25104;&#21151;&#12290;</title><link>https://arxiv.org/abs/2312.11834</link><description>&lt;p&gt;
&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#21450;&#20854;&#22312;&#34892;&#20154;&#21160;&#24577;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-agent reinforcement learning using echo-state network and its application to pedestrian dynamics
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.11834
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;MARL&#20195;&#29702;&#65292;&#30740;&#31350;&#20102;&#20182;&#20204;&#23398;&#20064;&#36991;&#35753;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#65292;&#22312;&#23494;&#24230;&#19981;&#22826;&#39640;&#30340;&#24773;&#20917;&#19979;&#21462;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#30740;&#31350;&#20351;&#29992;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#27169;&#25311;&#34892;&#20154;&#12290;&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#32593;&#26684;&#19990;&#30028;&#29615;&#22659;&#20013;&#30340;&#36947;&#36335;&#65292;&#24182;&#23558;&#34892;&#20154;&#23454;&#29616;&#20026;&#20351;&#29992;&#22238;&#22768;&#29366;&#24577;&#32593;&#32476;&#21644;&#26368;&#23567;&#20108;&#20056;&#31574;&#30053;&#36845;&#20195;&#26041;&#27861;&#30340;MARL&#20195;&#29702;&#12290;&#22312;&#36825;&#20010;&#29615;&#22659;&#19979;&#65292;&#30740;&#31350;&#20102;&#36825;&#20123;&#20195;&#29702;&#23398;&#20064;&#36991;&#24320;&#20854;&#20182;&#20195;&#29702;&#21521;&#21069;&#31227;&#21160;&#30340;&#33021;&#21147;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#32771;&#34385;&#20102;&#20004;&#31181;&#20219;&#21153;&#65306;&#31364;&#30452;&#25509;&#36335;&#24452;&#21644;&#23485;&#32469;&#36947;&#20043;&#38388;&#30340;&#36873;&#25321;&#65292;&#20197;&#21450;&#36208;&#24266;&#20013;&#30340;&#21452;&#21521;&#34892;&#20154;&#27969;&#12290;&#27169;&#25311;&#32467;&#26524;&#34920;&#26126;&#65292;&#24403;&#20195;&#29702;&#23494;&#24230;&#19981;&#22826;&#39640;&#26102;&#65292;&#23398;&#20064;&#26159;&#25104;&#21151;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.11834v2 Announce Type: replace-cross  Abstract: In recent years, simulations of pedestrians using the multi-agent reinforcement learning (MARL) have been studied. This study considered the roads on a grid-world environment, and implemented pedestrians as MARL agents using an echo-state network and the least squares policy iteration method. Under this environment, the ability of these agents to learn to move forward by avoiding other agents was investigated. Specifically, we considered two types of tasks: the choice between a narrow direct route and a broad detour, and the bidirectional pedestrian flow in a corridor. The simulations results indicated that the learning was successful when the density of the agents was not that high.
&lt;/p&gt;</description></item><item><title>BaRDa&#25968;&#25454;&#38598;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#30340;&#34164;&#28085;&#26641;&#65292;&#28151;&#21512;&#30495;&#23454;&#21644;&#34394;&#20551;&#20107;&#23454;&#65292;&#24182;&#21253;&#25324;&#21453;&#20107;&#23454;&#20363;&#23376;&#65292;&#25104;&#21151;&#21306;&#20998;&#20102;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2312.07527</link><description>&lt;p&gt;
BaRDa: &#19968;&#20010;&#23558;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#20998;&#24320;&#30340;&#20449;&#24565;&#21644;&#25512;&#29702;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
BaRDa: A Belief and Reasoning Dataset that Separates Factual Accuracy and Reasoning Ability
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.07527
&lt;/p&gt;
&lt;p&gt;
BaRDa&#25968;&#25454;&#38598;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#27880;&#37322;&#30340;&#34164;&#28085;&#26641;&#65292;&#28151;&#21512;&#30495;&#23454;&#21644;&#34394;&#20551;&#20107;&#23454;&#65292;&#24182;&#21253;&#25324;&#21453;&#20107;&#23454;&#20363;&#23376;&#65292;&#25104;&#21151;&#21306;&#20998;&#20102;&#20107;&#23454;&#20934;&#30830;&#24615;&#21644;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#23384;&#22312;&#35768;&#22810;&#22522;&#20934;&#26469;&#27604;&#36739;&#29616;&#20195;&#35821;&#35328;&#27169;&#22411;&#65288;LMs&#65289;&#30340;&#24615;&#33021;&#65292;&#20294;&#26368;&#32456;&#20219;&#21153;&#35780;&#20272;&#24448;&#24448;&#28151;&#28102;&#20102;*&#20107;&#23454;&#20934;&#30830;&#24615;*&#65288;"&#30495;&#30456;"&#65289;&#21644;*&#25512;&#29702;&#33021;&#21147;*&#65288;"&#21512;&#29702;&#24615;"&#65292;&#25110;&#32773;&#26681;&#25454;&#27491;&#30830;&#25253;&#21578;&#20449;&#24565;&#21547;&#20041;&#26469;&#23450;&#20041;&#30340;"&#35802;&#23454;"&#65289;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#21019;&#24314;&#19968;&#20010;&#33021;&#22815;&#28165;&#26224;&#21306;&#20998;&#36825;&#20004;&#20010;&#27010;&#24565;&#30340;&#25968;&#25454;&#38598;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#21644;&#25193;&#23637;&#19968;&#32452;&#20154;&#31867;&#27880;&#37322;&#30340;*&#34164;&#28085;&#26641;*&#65292;&#29992;&#20110;&#34920;&#36798;&#33391;&#22909;&#21644;&#24694;&#21155;&#30340;&#25512;&#29702;&#38142;&#65292;&#24182;&#20351;&#29992;&#30495;&#23454;&#21644;&#34394;&#20551;&#20107;&#23454;&#30340;&#28151;&#21512;&#65292;&#29305;&#21035;&#26159;&#21253;&#25324;&#21453;&#20107;&#23454;&#30340;&#20363;&#23376;&#65292;&#20197;&#36991;&#20813;&#20449;&#24565;&#20559;&#35265;&#65288;&#20063;&#31216;&#20026;"&#20869;&#23481;&#25928;&#24212;"&#65289;&#12290;&#32467;&#26524;&#25968;&#25454;&#38598;&#21517;&#20026;BaRDa&#65292;&#21253;&#21547;3000&#20010;&#34164;&#28085;&#65288;1787&#20010;&#26377;&#25928;&#65292;1213&#20010;&#26080;&#25928;&#65289;&#65292;&#20351;&#29992;6681&#20010;&#30495;&#23454;&#21644;2319&#20010;&#34394;&#20551;&#38472;&#36848;&#12290;&#22312;&#22235;&#20010;GPT&#31995;&#21015;&#27169;&#22411; GPT3(curie)/GPT3(davinici)/3.5/4 &#19978;&#36827;&#34892;&#27979;&#35797;&#65292;&#21457;&#29616;&#20107;&#23454;&#20934;&#30830;&#24615;&#65288;&#30495;&#30456;&#65289;&#24471;&#20998;&#20026;74.1/80.6/82.6/87.1&#20197;&#21450;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.07527v2 Announce Type: replace-cross  Abstract: While there are numerous benchmarks comparing the performance of modern language models (LMs), end-task evaluations often conflate notions of *factual accuracy* ("truth") and *reasoning ability* ("rationality", or "honesty" in the sense of correctly reporting implications of beliefs). Our goal is a dataset that clearly distinguishes these two notions. Our approach is to leverage and extend a collection of human-annotated *entailment trees*, engineered to express both good and bad chains of reasoning, and using a mixture of true and false facts, in particular including counterfactual examples, to avoid belief bias (also known as the "content effect"). The resulting dataset, called BaRDa, contains 3000 entailments (1787 valid, 1213 invalid), using 6681 true and 2319 false statements. Testing on four GPT-series models, GPT3(curie)/GPT3(davinici)/3.5/4, we find factual accuracy (truth) scores of 74.1/80.6/82.6/87.1 and reasoning ac
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#32593;&#32476;&#20013;&#38024;&#23545;AI&#29983;&#25104;&#20869;&#23481;&#26381;&#21153;&#30340;&#21368;&#36733;&#21644;&#36136;&#37327;&#25511;&#21046;&#38382;&#39064;&#30340;&#32852;&#21512;&#20248;&#21270;&#31639;&#27861;&#12290;</title><link>https://arxiv.org/abs/2312.06203</link><description>&lt;p&gt;
&#22312;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#32593;&#32476;&#20013;&#65292;AI&#29983;&#25104;&#20869;&#23481;&#26381;&#21153;&#30340;&#21368;&#36733;&#21644;&#36136;&#37327;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Offloading and Quality Control for AI Generated Content Services in 6G Mobile Edge Computing Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.06203
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#22312;6G&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#32593;&#32476;&#20013;&#38024;&#23545;AI&#29983;&#25104;&#20869;&#23481;&#26381;&#21153;&#30340;&#21368;&#36733;&#21644;&#36136;&#37327;&#25511;&#21046;&#38382;&#39064;&#30340;&#32852;&#21512;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
AI&#29983;&#25104;&#20869;&#23481;(AIGC)&#20316;&#20026;&#22312;&#21363;&#23558;&#21040;&#26469;&#30340;&#20114;&#32852;&#32593;&#33539;&#24335;&#20013;&#25552;&#20379;Metaverse&#26381;&#21153;&#30340;&#26032;&#39062;&#26041;&#24335;&#65292;&#21487;&#20197;&#35299;&#20915;&#27785;&#28024;&#38656;&#27714;&#30340;&#38556;&#30861;&#12290;&#21516;&#26102;&#65292;&#36793;&#32536;&#35745;&#31639;&#20316;&#20026;&#36890;&#20449;&#31995;&#32479;&#20013;&#30340;&#35745;&#31639;&#36827;&#21270;&#33539;&#24335;&#65292;&#26377;&#25928;&#22686;&#24378;&#20102;&#23454;&#26102;&#20132;&#20114;&#26381;&#21153;&#12290;&#20026;&#20102;&#25552;&#39640;AIGC&#26381;&#21153;&#30340;&#21487;&#35775;&#38382;&#24615;&#65292;&#23558;AIGC&#27169;&#22411;&#65288;&#22914;&#25193;&#25955;&#27169;&#22411;&#65289;&#37096;&#32626;&#21040;&#36793;&#32536;&#26381;&#21153;&#22120;&#21644;&#26412;&#22320;&#35774;&#22791;&#24050;&#25104;&#20026;&#19968;&#31181;&#30427;&#34892;&#36235;&#21183;&#12290;&#28982;&#32780;&#65292;&#24403;&#20219;&#21153;&#21368;&#36733;&#21040;&#26412;&#22320;&#35774;&#22791;&#26102;&#65292;&#38754;&#20020;&#30340;&#32422;&#26463;&#21253;&#25324;&#30005;&#27744;&#23551;&#21629;&#21644;&#35745;&#31639;&#36164;&#28304;&#65292;&#38480;&#21046;&#20102;&#22312;&#36981;&#23432;&#20005;&#26684;&#30340;&#24310;&#36831;&#35201;&#27714;&#30340;&#21516;&#26102;&#21521;&#29992;&#25143;&#25552;&#20379;&#39640;&#36136;&#37327;&#20869;&#23481;&#30340;&#33021;&#21147;&#12290;&#22240;&#27492;&#65292;&#22312;&#36793;&#32536;&#35745;&#31639;&#33539;&#24335;&#20013;&#65292;AIGC&#27169;&#22411;&#30340;&#25928;&#29992;&#21644;&#21368;&#36733;&#20915;&#31574;&#20043;&#38388;&#23558;&#23384;&#22312;&#26435;&#34913;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21368;&#36733;&#20915;&#31574;&#30340;&#32852;&#21512;&#20248;&#21270;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.06203v2 Announce Type: replace  Abstract: AI-Generated Content (AIGC), as a novel manner of providing Metaverse services in the forthcoming Internet paradigm, can resolve the obstacles of immersion requirements. Concurrently, edge computing, as an evolutionary paradigm of computing in communication systems, effectively augments real-time interactive services. In pursuit of enhancing the accessibility of AIGC services, the deployment of AIGC models (e.g., diffusion models) to edge servers and local devices has become a prevailing trend. Nevertheless, this approach faces constraints imposed by battery life and computational resources when tasks are offloaded to local devices, limiting the capacity to deliver high-quality content to users while adhering to stringent latency requirements. So there will be a tradeoff between the utility of AIGC models and offloading decisions in the edge computing paradigm. This paper proposes a joint optimization algorithm for offloading decisio
&lt;/p&gt;</description></item><item><title>I-PHYRE&#26159;&#19968;&#20010;&#25361;&#25112;&#20195;&#29702;&#21516;&#26102;&#23637;&#31034;&#30452;&#35266;&#30340;&#29289;&#29702;&#25512;&#29702;&#12289;&#22810;&#27493;&#35268;&#21010;&#21644;&#23601;&#22320;&#24178;&#39044;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;</title><link>https://arxiv.org/abs/2312.03009</link><description>&lt;p&gt;
I-PHYRE: &#20132;&#20114;&#24335;&#29289;&#29702;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
I-PHYRE: Interactive Physical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2312.03009
&lt;/p&gt;
&lt;p&gt;
I-PHYRE&#26159;&#19968;&#20010;&#25361;&#25112;&#20195;&#29702;&#21516;&#26102;&#23637;&#31034;&#30452;&#35266;&#30340;&#29289;&#29702;&#25512;&#29702;&#12289;&#22810;&#27493;&#35268;&#21010;&#21644;&#23601;&#22320;&#24178;&#39044;&#33021;&#21147;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#35780;&#20272;&#21327;&#35758;&#20027;&#35201;&#35780;&#20272;&#38745;&#24577;&#22330;&#26223;&#20013;&#30340;&#29289;&#29702;&#25512;&#29702;&#65292;&#23384;&#22312;&#35780;&#20272;&#20195;&#29702;&#19982;&#21160;&#24577;&#20107;&#20214;&#36827;&#34892;&#20132;&#20114;&#30340;&#33021;&#21147;&#30340;&#32570;&#21475;&#12290;&#23613;&#31649;&#24403;&#20195;&#26041;&#27861;&#20801;&#35768;&#20195;&#29702;&#20462;&#25913;&#21021;&#22987;&#22330;&#26223;&#37197;&#32622;&#24182;&#35266;&#23519;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#32570;&#20047;&#23454;&#26102;&#19982;&#20107;&#20214;&#20132;&#20114;&#30340;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;I-PHYRE&#65292;&#19968;&#20010;&#26694;&#26550;&#25361;&#25112;&#20195;&#29702;&#21516;&#26102;&#23637;&#31034;&#30452;&#35266;&#30340;&#29289;&#29702;&#25512;&#29702;&#12289;&#22810;&#27493;&#35268;&#21010;&#21644;&#23601;&#22320;&#24178;&#39044;&#12290;&#36825;&#37324;&#65292;&#30452;&#35266;&#30340;&#29289;&#29702;&#25512;&#29702;&#25351;&#30340;&#26159;&#24555;&#36895;&#12289;&#36817;&#20284;&#29702;&#35299;&#29289;&#29702;&#23398;&#20197;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#65307;&#22810;&#27493;&#34920;&#31034;&#22312;I-PHYRE&#20013;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#24207;&#21015;&#35268;&#21010;&#65292;&#32771;&#34385;&#21040;&#27599;&#27425;&#24178;&#39044;&#37117;&#21487;&#33021;&#26174;&#33879;&#25913;&#21464;&#21518;&#32493;&#36873;&#25321;&#65307;&#32780;&#23601;&#22320;&#24847;&#21619;&#30528;&#22312;&#22330;&#26223;&#20869;&#21450;&#26102;&#36827;&#34892;&#29289;&#20307;&#25805;&#20316;&#30340;&#24517;&#35201;&#24615;&#65292;&#22312;&#36825;&#37324;&#65292;&#24494;&#23567;&#30340;&#26102;&#38388;&#20559;&#24046;&#21487;&#33021;&#23548;&#33268;&#20219;&#21153;&#22833;&#36133;&#12290;&#25105;&#20204;&#21046;&#23450;&#20102;&#22235;&#20010;&#28216;&#25103;&#20998;&#25903;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2312.03009v2 Announce Type: replace  Abstract: Current evaluation protocols predominantly assess physical reasoning in stationary scenes, creating a gap in evaluating agents' abilities to interact with dynamic events. While contemporary methods allow agents to modify initial scene configurations and observe consequences, they lack the capability to interact with events in real time. To address this, we introduce I-PHYRE, a framework that challenges agents to simultaneously exhibit intuitive physical reasoning, multi-step planning, and in-situ intervention. Here, intuitive physical reasoning refers to a quick, approximate understanding of physics to address complex problems; multi-step denotes the need for extensive sequence planning in I-PHYRE, considering each intervention can significantly alter subsequent choices; and in-situ implies the necessity for timely object manipulation within a scene, where minor timing deviations can result in task failure. We formulate four game spl
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#65292;&#24341;&#20837;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;Word4Per&#12290;</title><link>https://arxiv.org/abs/2311.16515</link><description>&lt;p&gt;
Word4Per: Zero-shot&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;
&lt;/p&gt;
&lt;p&gt;
Word4Per: Zero-shot Composed Person Retrieval
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.16515
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#65306;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#65292;&#24341;&#20837;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;Word4Per&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#29305;&#23450;&#20154;&#21592;&#20855;&#26377;&#26497;&#22823;&#30340;&#31038;&#20250;&#25928;&#30410;&#21644;&#23433;&#20840;&#20215;&#20540;&#65292;&#36890;&#24120;&#28041;&#21450;&#35270;&#35273;&#21644;&#25991;&#26412;&#20449;&#24687;&#30340;&#32467;&#21512;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20840;&#26032;&#30340;&#20219;&#21153;&#65292;&#31216;&#20026;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;CPR&#65289;&#65292;&#26088;&#22312;&#32852;&#21512;&#21033;&#29992;&#22270;&#20687;&#21644;&#25991;&#26412;&#20449;&#24687;&#36827;&#34892;&#30446;&#26631;&#20154;&#21592;&#26816;&#32034;&#12290;&#28982;&#32780;&#65292;&#30417;&#30563;CPR&#38656;&#35201;&#26114;&#36149;&#30340;&#25163;&#21160;&#27880;&#37322;&#25968;&#25454;&#38598;&#65292;&#32780;&#30446;&#21069;&#27809;&#26377;&#21487;&#29992;&#36164;&#28304;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#38646;&#26679;&#26412;&#32452;&#21512;&#20154;&#21592;&#26816;&#32034;&#65288;ZS-CPR&#65289;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#39046;&#22495;&#30456;&#20851;&#25968;&#25454;&#35299;&#20915;&#20102;CPR&#38382;&#39064;&#32780;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#27880;&#37322;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#23398;&#20064;ZS-CPR&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20004;&#38454;&#27573;&#23398;&#20064;&#26694;&#26550;&#65292;&#21363;Word4Per&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#25991;&#26412;&#21453;&#36716;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.16515v2 Announce Type: replace-cross  Abstract: Searching for specific person has great social benefits and security value, and it often involves a combination of visual and textual information. Conventional person retrieval methods, whether image-based or text-based, usually fall short in effectively harnessing both types of information, leading to the loss of accuracy. In this paper, a whole new task called Composed Person Retrieval (CPR) is proposed to jointly utilize both image and text information for target person retrieval. However, the supervised CPR requires very costly manual annotation dataset, while there are currently no available resources. To mitigate this issue, we firstly introduce the Zero-shot Composed Person Retrieval (ZS-CPR), which leverages existing domain-related data to resolve the CPR problem without expensive annotations. Secondly, to learn ZS-CPR model, we propose a two-stage learning framework, Word4Per, where a lightweight Textual Inversion Netw
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#28040;&#38500;&#26694;&#26550;HalluciDoctor&#65292;&#26088;&#22312;&#20943;&#36731;&#22823;&#35268;&#27169;&#26426;&#22120;&#29983;&#25104;&#30340;&#35270;&#35273;&#25351;&#20196;&#25968;&#25454;&#20013;&#30340;&#24187;&#35273;&#27602;&#24615;&#12290;</title><link>https://arxiv.org/abs/2311.13614</link><description>&lt;p&gt;
HalluciDoctor: &#20943;&#36731;&#35270;&#35273;&#25351;&#20196;&#25968;&#25454;&#20013;&#30340;&#24187;&#35273;&#27602;&#24615;
&lt;/p&gt;
&lt;p&gt;
HalluciDoctor: Mitigating Hallucinatory Toxicity in Visual Instruction Data
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13614
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#24187;&#35273;&#26816;&#27979;&#21644;&#28040;&#38500;&#26694;&#26550;HalluciDoctor&#65292;&#26088;&#22312;&#20943;&#36731;&#22823;&#35268;&#27169;&#26426;&#22120;&#29983;&#25104;&#30340;&#35270;&#35273;&#25351;&#20196;&#25968;&#25454;&#20013;&#30340;&#24187;&#35273;&#27602;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#22810;&#27169;&#24577;&#35821;&#35328;&#27169;&#22411;&#65288;MLLMs&#65289;&#22312;&#26426;&#22120;&#29983;&#25104;&#30340;&#25351;&#20196;&#36319;&#38543;&#25968;&#25454;&#19978;&#36827;&#34892;&#35843;&#25972;&#65292;&#24050;&#32463;&#22312;&#21508;&#31181;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#26426;&#22120;&#29983;&#25104;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#24187;&#35273;&#65292;&#21487;&#33021;&#23548;&#33268;MLLMs&#20135;&#29983;&#24187;&#35273;&#36755;&#20986;&#65292;&#36825;&#19968;&#38382;&#39064;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#35752;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#35843;&#26597;&#22823;&#35268;&#27169;&#26426;&#22120;&#29983;&#25104;&#30340;&#35270;&#35273;&#25351;&#20196;&#25968;&#25454;&#20013;&#30340;&#21508;&#31181;&#24187;&#35273;&#65288;&#21363;&#23545;&#35937;&#12289;&#20851;&#31995;&#12289;&#23646;&#24615;&#24187;&#35273;&#65289;&#65292;&#24182;&#20943;&#36731;&#36825;&#20123;&#24187;&#35273;&#27602;&#24615;&#12290;&#20511;&#37492;&#20154;&#31867;&#35782;&#21035;&#20107;&#23454;&#38169;&#35823;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#20132;&#21449;&#26816;&#26597;&#33539;&#24335;&#30340;&#26032;&#39062;&#24187;&#35273;&#26816;&#27979;&#21644;&#28040;&#38500;&#26694;&#26550;HalluciDoctor&#12290;&#25105;&#20204;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#33258;&#21160;&#22320;&#35782;&#21035;&#21644;&#28040;&#38500;&#35757;&#32451;&#25968;&#25454;&#20013;&#30340;&#24187;&#35273;&#12290;&#26377;&#36259;&#30340;&#26159;&#65292;HalluciDoctor&#36824;&#34920;&#26126;&#65292;&#38271;&#23614;&#23545;&#35937;&#20849;&#29616;&#20135;&#29983;&#30340;&#34394;&#20551;&#30456;&#20851;&#24615;&#20063;&#21487;&#33021;&#23548;&#33268;&#24187;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13614v2 Announce Type: replace-cross  Abstract: Multi-modal Large Language Models (MLLMs) tuned on machine-generated instruction-following data have demonstrated remarkable performance in various multi-modal understanding and generation tasks. However, the hallucinations inherent in machine-generated data, which could lead to hallucinatory outputs in MLLMs, remain under-explored. This work aims to investigate various hallucinations (i.e., object, relation, attribute hallucinations) and mitigate those hallucinatory toxicities in large-scale machine-generated visual instruction datasets. Drawing on the human ability to identify factual errors, we present a novel hallucination detection and elimination framework, HalluciDoctor, based on the cross-checking paradigm. We use our framework to identify and eliminate hallucinations in the training data automatically. Interestingly, HalluciDoctor also indicates that spurious correlations arising from long-tail object co-occurrences co
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D3PO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30452;&#25509;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#28040;&#38500;&#20102;&#22870;&#21169;&#27169;&#22411;&#30340;&#21069;&#25552;&#19979;&#25913;&#36827;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;DPO&#26041;&#27861;&#30452;&#25509;&#24212;&#29992;&#30340;&#20869;&#23384;&#38656;&#27714;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.13231</link><description>&lt;p&gt;
&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#26469;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#32780;&#26080;&#38656;&#20219;&#20309;&#22870;&#21169;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.13231
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;D3PO&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#20154;&#31867;&#21453;&#39304;&#30452;&#25509;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#65292;&#26080;&#38656;&#22870;&#21169;&#27169;&#22411;&#65292;&#20174;&#32780;&#22312;&#28040;&#38500;&#20102;&#22870;&#21169;&#27169;&#22411;&#30340;&#21069;&#25552;&#19979;&#25913;&#36827;&#20102;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#35299;&#20915;&#20102;DPO&#26041;&#27861;&#30452;&#25509;&#24212;&#29992;&#30340;&#20869;&#23384;&#38656;&#27714;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#24102;&#26377;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#22312;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#26041;&#38754;&#26174;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#28508;&#21147;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#39318;&#20808;&#26159;&#36890;&#36807;&#35757;&#32451;&#19982;&#20154;&#31867;&#20559;&#22909;&#30456;&#19968;&#33268;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#21033;&#29992;&#24378;&#21270;&#23398;&#20064;&#25216;&#26415;&#26469;&#24494;&#35843;&#22522;&#30784;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#35774;&#35745;&#39640;&#25928;&#30340;&#22870;&#21169;&#27169;&#22411;&#38656;&#35201;&#22823;&#37327;&#25968;&#25454;&#38598;&#12289;&#26368;&#20339;&#26550;&#26500;&#21644;&#25163;&#21160;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#20351;&#24471;&#36825;&#19968;&#36807;&#31243;&#26082;&#32791;&#26102;&#21448;&#25104;&#26412;&#39640;&#26114;&#12290;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#26041;&#27861;&#65292;&#22312;&#24494;&#35843;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#28040;&#38500;&#20102;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#12290;&#28982;&#32780;&#65292;&#25193;&#25955;&#27169;&#22411;&#21435;&#22122;&#36807;&#31243;&#30340;&#22823;&#37327;GPU&#20869;&#23384;&#38656;&#27714;&#38459;&#30861;&#20102;DPO&#26041;&#27861;&#30340;&#30452;&#25509;&#24212;&#29992;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#30452;&#25509;&#20559;&#22909;&#21435;&#22122;&#25193;&#25955;&#31574;&#30053;&#20248;&#21270;&#65288;D3PO&#65289;&#26041;&#27861;&#26469;&#30452;&#25509;&#24494;&#35843;&#25193;&#25955;&#27169;&#22411;&#12290;&#29702;&#35770;&#20998;&#26512;&#34920;&#26126;&#65292;&#23613;&#31649;D3PO&#25552;&#20379;&#20102;&#25913;&#36827;&#65292;&#20294;&#22312;&#20855;&#26377;&#26126;&#26174;&#20248;&#21183;&#30340;&#21516;&#26102;&#20173;&#38656;&#35201;&#26356;&#22810;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.13231v3 Announce Type: replace-cross  Abstract: Using reinforcement learning with human feedback (RLHF) has shown significant promise in fine-tuning diffusion models. Previous methods start by training a reward model that aligns with human preferences, then leverage RL techniques to fine-tune the underlying models. However, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. The direct preference optimization (DPO) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. However, the extensive GPU memory requirement of the diffusion model's denoising process hinders the direct application of the DPO method. To address this issue, we introduce the Direct Preference for Denoising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The theoretical analysis demonstrates that although D3PO o
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#35780;&#20272;&#22024;&#26434;&#26631;&#31614;&#23545;&#19981;&#23433;&#20840;&#35780;&#35770;&#21644;&#23545;&#35805;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#26080;&#23475;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;</title><link>https://arxiv.org/abs/2311.11202</link><description>&lt;p&gt;
&#25581;&#31034;&#21644;&#25552;&#39640;&#25968;&#25454;&#21487;&#20449;&#24230;&#65306;&#35757;&#32451;&#26080;&#23475;&#35821;&#35328;&#27169;&#22411;&#30340;&#25968;&#25454;&#38598;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Unmasking and Improving Data Credibility: A Study with Datasets for Training Harmless Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.11202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#19987;&#27880;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#35780;&#20272;&#22024;&#26434;&#26631;&#31614;&#23545;&#19981;&#23433;&#20840;&#35780;&#35770;&#21644;&#23545;&#35805;&#20998;&#31867;&#30340;&#24433;&#21709;&#65292;&#20197;&#25552;&#39640;&#35757;&#32451;&#26080;&#23475;&#35821;&#35328;&#27169;&#22411;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11202v2&#23459;&#24067;&#31867;&#22411;&#65306;&#26367;&#25442;-&#36328;&#25991;&#26723;&#25688;&#35201;&#65306;&#35821;&#35328;&#27169;&#22411;&#22312;&#21508;&#31181;&#20219;&#21153;&#20013;&#26174;&#31034;&#20986;&#28508;&#21147;&#65292;&#20294;&#22312;&#35757;&#32451;&#12289;&#24494;&#35843;&#25110;&#23545;&#40784;&#36807;&#31243;&#20013;&#21487;&#33021;&#21463;&#21040;&#19981;&#24076;&#26395;&#30340;&#25968;&#25454;&#30340;&#24433;&#21709;&#12290;&#22240;&#27492;&#65292;&#27880;&#35299;&#30340;&#27491;&#30830;&#24615;&#65292;&#21363;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#21464;&#24471;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#30740;&#31350;&#32858;&#28966;&#20110;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#20854;&#20013;&#21253;&#25324;&#21487;&#29992;&#20110;&#35757;&#32451;&#26080;&#23475;&#35821;&#35328;&#27169;&#22411;&#30340;&#27969;&#34892;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#22914;Jigsaw Civil Comments&#12289;Anthropic Harmless&#21644;Red Team&#12289;PKU BeaverTails&#21644;SafeRLHF&#12290;&#32771;&#34385;&#21040;&#20154;&#20204;&#28165;&#27927;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#25104;&#26412;&#21644;&#38590;&#24230;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#31995;&#32479;&#21270;&#26694;&#26550;&#65292;&#29992;&#20110;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#21487;&#20449;&#24230;&#65292;&#35782;&#21035;&#26631;&#31614;&#38169;&#35823;&#65292;&#24182;&#35780;&#20272;&#31574;&#21010;&#35821;&#35328;&#25968;&#25454;&#20013;&#22024;&#26434;&#26631;&#31614;&#30340;&#24433;&#21709;&#65292;&#29305;&#21035;&#20851;&#27880;&#19981;&#23433;&#20840;&#35780;&#35770;&#21644;&#23545;&#35805;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.11202v2 Announce Type: replace-cross  Abstract: Language models have shown promise in various tasks but can be affected by undesired data during training, fine-tuning, or alignment. For example, if some unsafe conversations are wrongly annotated as safe ones, the model fine-tuned on these samples may be harmful. Therefore, the correctness of annotations, i.e., the credibility of the dataset, is important. This study focuses on the credibility of real-world datasets, including the popular benchmarks Jigsaw Civil Comments, Anthropic Harmless &amp; Red Team, PKU BeaverTails &amp; SafeRLHF, that can be used for training a harmless language model. Given the cost and difficulty of cleaning these datasets by humans, we introduce a systematic framework for evaluating the credibility of datasets, identifying label errors, and evaluating the influence of noisy labels in the curated language data, specifically focusing on unsafe comments and conversation classification. With the framework, we 
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#19982;&#26657;&#20934;&#30340;&#35843;&#30740;&#24635;&#32467;&#20102;&#25361;&#25112;&#12289;&#25216;&#26415;&#36827;&#23637;&#12289;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;</title><link>https://arxiv.org/abs/2311.08298</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#19982;&#26657;&#20934;&#30340;&#35843;&#30740;
&lt;/p&gt;
&lt;p&gt;
A Survey of Confidence Estimation and Calibration in Large Language Models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08298
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#32622;&#20449;&#24230;&#20272;&#35745;&#19982;&#26657;&#20934;&#30340;&#35843;&#30740;&#24635;&#32467;&#20102;&#25361;&#25112;&#12289;&#25216;&#26415;&#36827;&#23637;&#12289;&#24212;&#29992;&#21644;&#26410;&#26469;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#21508;&#31181;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#21331;&#36234;&#30340;&#24615;&#33021;&#65292;&#20294;&#30001;&#20110;&#29983;&#25104;&#20013;&#30340;&#20107;&#23454;&#38169;&#35823;&#65292;&#23427;&#20204;&#21487;&#33021;&#19981;&#21487;&#38752;&#12290;&#35780;&#20272;&#23427;&#20204;&#30340;&#32622;&#20449;&#24230;&#24182;&#22312;&#19981;&#21516;&#20219;&#21153;&#20013;&#36827;&#34892;&#26657;&#20934;&#21487;&#20197;&#24110;&#21161;&#20943;&#36731;&#39118;&#38505;&#65292;&#20351;LLMs&#33021;&#22815;&#20135;&#29983;&#26356;&#22909;&#30340;&#29983;&#25104;&#32467;&#26524;&#12290;&#36817;&#26399;&#26377;&#35768;&#22810;&#30740;&#31350;&#33268;&#21147;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23578;&#26080;&#20840;&#38754;&#30340;&#27010;&#36848;&#26469;&#32452;&#32455;&#24182;&#27010;&#36848;&#20027;&#35201;&#30340;&#32463;&#39564;&#25945;&#35757;&#65292;&#26412;&#35843;&#30740;&#26088;&#22312;&#24357;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#27010;&#36848;&#20102;&#25361;&#25112;&#65292;&#24182;&#24635;&#32467;&#20102;LLMs&#32622;&#20449;&#24230;&#20272;&#35745;&#21644;&#26657;&#20934;&#30340;&#26368;&#26032;&#25216;&#26415;&#36827;&#23637;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#24212;&#29992;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#24037;&#20316;&#30340;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08298v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks in various domains. Despite their impressive performance, they can be unreliable due to factual errors in their generations. Assessing their confidence and calibrating them across different tasks can help mitigate risks and enable LLMs to produce better generations. There has been a lot of recent research aiming to address this, but there has been no comprehensive overview to organize it and outline the main lessons learned. The present survey aims to bridge this gap. In particular, we outline the challenges and we summarize recent technical advancements for LLM confidence estimation and calibration. We further discuss their applications and suggest promising directions for future work.
&lt;/p&gt;</description></item><item><title>&#35780;&#20215;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#37051;&#23621;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25552;&#20986;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#24182;&#21457;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;GNN&#39046;&#22495;&#30340;&#35299;&#37322;&#27809;&#26377;&#22826;&#22823;&#24046;&#24322;&#65292;&#21516;&#26102;&#21457;&#29616;&#24456;&#22810;&#25216;&#26415;&#22312;&#27809;&#26377;&#33258;&#29615;&#30340;GNNs&#19979;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#37325;&#35201;&#37051;&#23621;&#12290;</title><link>https://arxiv.org/abs/2311.08118</link><description>&lt;p&gt;
&#35780;&#20215;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#37051;&#23621;&#21487;&#35299;&#37322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Evaluating Neighbor Explainability for Graph Neural Networks
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.08118
&lt;/p&gt;
&lt;p&gt;
&#35780;&#20215;&#22270;&#31070;&#32463;&#32593;&#32476;&#20013;&#37051;&#23621;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25552;&#20986;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#24182;&#21457;&#29616;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#22312;GNN&#39046;&#22495;&#30340;&#35299;&#37322;&#27809;&#26377;&#22826;&#22823;&#24046;&#24322;&#65292;&#21516;&#26102;&#21457;&#29616;&#24456;&#22810;&#25216;&#26415;&#22312;&#27809;&#26377;&#33258;&#29615;&#30340;GNNs&#19979;&#26080;&#27861;&#20934;&#30830;&#35782;&#21035;&#37325;&#35201;&#37051;&#23621;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#20013;&#30340;&#21487;&#35299;&#37322;&#24615;&#26159;&#36817;&#24180;&#26469;&#26032;&#20852;&#39046;&#22495;&#12290;&#22312;&#36825;&#31687;&#25991;&#31456;&#20013;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#22312;&#23545;&#33410;&#28857;&#36827;&#34892;&#20998;&#31867;&#26102;&#65292;&#30830;&#23450;&#27599;&#20010;&#37051;&#23621;&#23545;&#20110; GNN &#30340;&#37325;&#35201;&#24615;&#20197;&#21450;&#22914;&#20309;&#34913;&#37327;&#36825;&#19968;&#29305;&#23450;&#20219;&#21153;&#30340;&#34920;&#29616;&#12290;&#20026;&#27492;&#65292;&#21508;&#31181;&#24050;&#30693;&#30340;&#21487;&#35299;&#37322;&#24615;&#26041;&#27861;&#34987;&#37325;&#26032;&#26500;&#36896;&#20197;&#33719;&#21462;&#37051;&#23621;&#37325;&#35201;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#22235;&#31181;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312; GNN &#39046;&#22495;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#25216;&#26415;&#25552;&#20379;&#30340;&#35299;&#37322;&#20960;&#20046;&#27809;&#26377;&#24046;&#24322;&#12290;&#27492;&#22806;&#65292;&#35768;&#22810;&#21487;&#35299;&#37322;&#24615;&#25216;&#26415;&#22312;&#20351;&#29992;&#27809;&#26377;&#33258;&#29615;&#30340; GNNs &#26102;&#26410;&#33021;&#35782;&#21035;&#37325;&#35201;&#30340;&#37051;&#23621;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.08118v2 Announce Type: replace-cross  Abstract: Explainability in Graph Neural Networks (GNNs) is a new field growing in the last few years. In this publication we address the problem of determining how important is each neighbor for the GNN when classifying a node and how to measure the performance for this specific task. To do this, various known explainability methods are reformulated to get the neighbor importance and four new metrics are presented. Our results show that there is almost no difference between the explanations provided by gradient-based techniques in the GNN domain. In addition, many explainability techniques failed to identify important neighbors when GNNs without self-loops are used.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#33258;&#25105;&#39564;&#35777;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#20934;&#30830;&#35782;&#21035;&#36923;&#36753;&#35884;&#35823;&#30340;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2311.07954</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#33258;&#25105;&#39564;&#35777;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.07954
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36923;&#36753;&#25512;&#29702;&#20013;&#30340;&#33258;&#25105;&#39564;&#35777;&#33021;&#21147;&#65292;&#29305;&#21035;&#20851;&#27880;&#23427;&#20204;&#20934;&#30830;&#35782;&#21035;&#36923;&#36753;&#35884;&#35823;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36923;&#36753;&#25512;&#29702;&#19968;&#30452;&#26159;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36861;&#27714;&#30446;&#26631;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#20173;&#28982;&#22312;&#22797;&#26434;&#30340;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#19978;&#38754;&#20020;&#22256;&#38590;&#12290;&#20026;&#20102;&#22686;&#24378;&#25512;&#29702;&#24615;&#33021;&#65292;&#19968;&#20010;&#26377;&#24076;&#26395;&#30340;&#26041;&#21521;&#26159;&#21487;&#25193;&#23637;&#30340;&#30417;&#30563;&#65292;&#36825;&#38656;&#35201;LLMs&#35782;&#21035;&#33258;&#24049;&#30340;&#38169;&#35823;&#65292;&#28982;&#21518;&#33258;&#34892;&#25913;&#36827;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#30446;&#26631;&#65292;&#25552;&#20986;&#20102;&#21508;&#31181;&#33258;&#25105;&#39564;&#35777;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#27169;&#22411;&#26159;&#21542;&#24456;&#22909;&#22320;&#29702;&#35299;&#33258;&#24049;&#30340;&#38169;&#35823;&#20173;&#22312;&#35843;&#26597;&#20013;&#12290;&#26412;&#25991;&#30528;&#37325;&#25506;&#35752;&#20102;LLMs&#22312;&#36923;&#36753;&#25512;&#29702;&#32972;&#26223;&#19979;&#30340;&#33258;&#25105;&#39564;&#35777;&#33021;&#21147;&#65292;&#20851;&#27880;&#23427;&#20204;&#20934;&#30830;&#35782;&#21035;&#36923;&#36753;&#35884;&#35823;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#21253;&#21547;232&#31181;&#25512;&#29702;&#35884;&#35823;&#30340;&#25968;&#25454;&#38598;FALLACIES&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#20174;&#32780;&#33719;&#24471;&#20102;&#20851;&#20110;LLMs&#22312;FALLACIES&#19978;&#30340;&#20840;&#38754;&#21644;&#35814;&#32454;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.07954v2 Announce Type: replace  Abstract: Logical reasoning has been an ongoing pursuit in the field of AI. Despite significant advancements made by large language models (LLMs), they still struggle with complex logical reasoning problems. To enhance reasoning performance, one promising direction is scalable oversight, which requires LLMs to identify their own errors and then improve by themselves. Various self-verification methods have been proposed in pursuit of this goal. Nevertheless, whether existing models understand their own errors well is still under investigation. In this paper, we take a closer look at the self-verification abilities of LLMs in the context of logical reasoning, focusing on their ability to identify logical fallacies accurately. We introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies categorized in a hierarchical taxonomy. By conducting exhaustive experiments on FALLACIES, we obtain comprehensive and detailed analyses of a se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21517;&#20026;VT-Former&#65292;&#22312;&#26234;&#33021;&#20844;&#36335;&#20132;&#36890;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;</title><link>https://arxiv.org/abs/2311.06623</link><description>&lt;p&gt;
VT-Former: &#22522;&#20110;Transformer&#30340;&#26234;&#33021;&#20844;&#36335;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
VT-Former: A Transformer-based Vehicle Trajectory Prediction Approach For Intelligent Highway Transportation Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.06623
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Transformer&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#21517;&#20026;VT-Former&#65292;&#22312;&#26234;&#33021;&#20844;&#36335;&#20132;&#36890;&#31995;&#32479;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24212;&#29992;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21152;&#24378;&#36947;&#36335;&#23433;&#20840;&#21644;&#20132;&#36890;&#31649;&#29702;&#24050;&#25104;&#20026;&#29616;&#20195;&#32593;&#32476;&#29289;&#29702;&#31995;&#32479;&#21644;&#26234;&#33021;&#20132;&#36890;&#31995;&#32479;&#30340;&#37325;&#28857;&#39046;&#22495;&#12290;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#22312;&#20844;&#36335;&#21644;&#36947;&#36335;&#23433;&#20840;&#30340;&#20247;&#22810;&#24212;&#29992;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#36825;&#20123;&#24212;&#29992;&#21253;&#25324;&#20132;&#36890;&#31649;&#29702;&#12289;&#20107;&#25925;&#39044;&#38450;&#12289;&#24037;&#22320;&#23433;&#20840;&#21644;&#33021;&#28304;&#20248;&#21270;&#31561;&#21508;&#31181;&#29992;&#20363;&#12290;&#22312;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#21457;&#23637;&#20197;&#21450;&#30417;&#25511;&#25668;&#20687;&#22836;&#22312;&#36947;&#36335;&#32593;&#32476;&#19978;&#30340;&#22686;&#21152;&#37096;&#32626;&#25512;&#21160;&#19979;&#65292;&#26234;&#33021;&#31649;&#29702;&#22312;&#36825;&#19968;&#32972;&#26223;&#19979;&#24471;&#21040;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;Transformer&#30340;&#36710;&#36742;&#36712;&#36857;&#39044;&#27979;&#26041;&#27861;&#65292;&#31216;&#20026;VT-Former&#12290;&#38500;&#20102;&#21033;&#29992;Transformer&#25429;&#25417;&#38271;&#26399;&#26102;&#38388;&#27169;&#24335;&#22806;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#27880;&#24847;&#21147;&#20998;&#35789;&#65288;GAT&#65289;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enhancing roadway safety and traffic management has become an essential focus area for a broad range of modern cyber-physical systems and intelligent transportation systems. Vehicle Trajectory Prediction is a pivotal element within numerous applications for highway and road safety. These applications encompass a wide range of use cases, spanning from traffic management and accident prevention to enhancing work-zone safety and optimizing energy conservation. The ability to implement intelligent management in this context has been greatly advanced by the developments in the field of Artificial Intelligence (AI), alongside the increasing deployment of surveillance cameras across road networks. In this paper, we introduce a novel transformer-based approach for vehicle trajectory prediction for highway safety and surveillance, denoted as VT-Former. In addition to utilizing transformers to capture long-range temporal patterns, a new Graph Attentive Tokenization (GAT) module has been proposed
&lt;/p&gt;</description></item><item><title>&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#38750;&#32534;&#30721;RNA&#25991;&#29486;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#24110;&#21161;&#20943;&#36731;&#29983;&#21629;&#31185;&#23398;&#25991;&#29486;&#25972;&#29702;&#20013;&#32570;&#20047;&#31574;&#23637;&#20154;&#21592;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2311.03056</link><description>&lt;p&gt;
LitSumm&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#38750;&#32534;&#30721;RNA&#25991;&#29486;&#25688;&#35201;
&lt;/p&gt;
&lt;p&gt;
LitSumm: Large language models for literature summarisation of non-coding RNAs
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.03056
&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#20026;&#38750;&#32534;&#30721;RNA&#25991;&#29486;&#29983;&#25104;&#39640;&#36136;&#37327;&#21644;&#20934;&#30830;&#30340;&#25688;&#35201;&#65292;&#24110;&#21161;&#20943;&#36731;&#29983;&#21629;&#31185;&#23398;&#25991;&#29486;&#25972;&#29702;&#20013;&#32570;&#20047;&#31574;&#23637;&#20154;&#21592;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Motivation: &#22312;&#29983;&#21629;&#31185;&#23398;&#25991;&#29486;&#30340;&#25972;&#29702;&#24037;&#20316;&#20013;&#65292;&#38754;&#20020;&#30528;&#26085;&#30410;&#20005;&#23803;&#30340;&#25361;&#25112;&#12290;&#38543;&#30528;&#21457;&#24067;&#36895;&#24230;&#30340;&#25345;&#32493;&#22686;&#21152;&#65292;&#20877;&#21152;&#19978;&#20840;&#29699;&#22266;&#23450;&#25968;&#37327;&#30340;&#31574;&#23637;&#20154;&#21592;&#65292;&#24320;&#21457;&#29983;&#29289;&#21307;&#23398;&#30693;&#35782;&#24211;&#38754;&#20020;&#37325;&#22823;&#25361;&#25112;&#12290;&#24456;&#23569;&#26377;&#30693;&#35782;&#24211;&#26377;&#36164;&#28304;&#21487;&#20197;&#25193;&#23637;&#21040;&#25152;&#26377;&#30456;&#20851;&#25991;&#29486;&#65292;&#32780;&#25152;&#26377;&#30693;&#35782;&#24211;&#37117;&#24517;&#39035;&#20248;&#20808;&#32771;&#34385;&#33258;&#24049;&#30340;&#21162;&#21147;&#12290; &#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20026;&#38750;&#32534;&#30721;RNA&#29983;&#25104;&#25991;&#29486;&#25688;&#35201;&#65292;&#39318;&#27425;&#20943;&#36731;&#20102;RNA&#31185;&#23398;&#20013;&#32570;&#20047;&#31574;&#23637;&#20154;&#21592;&#26102;&#38388;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#21487;&#20197;&#20351;&#29992;&#21830;&#19994;LLM&#21644;&#19968;&#31995;&#21015;&#25552;&#31034;&#21644;&#26816;&#26597;&#20174;&#25991;&#29486;&#20013;&#33258;&#21160;&#29983;&#25104;&#39640;&#36136;&#37327;&#12289;&#20107;&#23454;&#20934;&#30830;&#30340;&#25688;&#35201;&#21450;&#20934;&#30830;&#30340;&#24341;&#29992;&#12290;&#20154;&#24037;&#35780;&#20272;&#38024;&#23545;&#25688;&#35201;&#23376;&#38598;&#36827;&#34892;&#65292;&#20854;&#20013;&#22823;&#22810;&#25968;&#34987;&#35780;&#20026;&#38750;&#24120;&#39640;&#36136;&#37327;&#12290;&#25105;&#20204;&#36824;&#24212;&#29992;&#20102;&#26368;&#24120;&#29992;&#30340;&#33258;&#21160;&#21270;e
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.03056v2 Announce Type: replace-cross  Abstract: Motivation: Curation of literature in life sciences is a growing challenge. The continued increase in the rate of publication, coupled with the relatively fixed number of curators worldwide presents a major challenge to developers of biomedical knowledgebases. Very few knowledgebases have resources to scale to the whole relevant literature and all have to prioritise their efforts.   Results: In this work, we take a first step to alleviating the lack of curator time in RNA science by generating summaries of literature for non-coding RNAs using large language models (LLMs). We demonstrate that high-quality, factually accurate summaries with accurate references can be automatically generated from the literature using a commercial LLM and a chain of prompts and checks. Manual assessment was carried out for a subset of summaries, with the majority being rated extremely high quality. We also applied the most commonly used automated e
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#28436;&#31034;&#30340;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#38271;&#26102;&#38388;&#20219;&#21153;&#65292;&#20026;&#27599;&#20010;&#38454;&#27573;&#25552;&#20379;&#24037;&#20855;&#21517;&#31216;&#21644;Python&#20195;&#30721;&#12290;</title><link>https://arxiv.org/abs/2311.02787</link><description>&lt;p&gt;
&#21046;&#20316;&#19968;&#20010;&#29980;&#29980;&#22280;&#65306;&#29992;&#20110;&#38646;&#26679;&#26412;&#21464;&#24418;&#25805;&#32437;&#30340;&#20998;&#23618;EMD&#31354;&#38388;&#35268;&#21010;&#19982;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable Manipulation with Tools
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02787
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#31181;&#26080;&#38656;&#28436;&#31034;&#30340;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#65292;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#35299;&#20915;&#22797;&#26434;&#30340;&#38271;&#26102;&#38388;&#20219;&#21153;&#65292;&#20026;&#27599;&#20010;&#38454;&#27573;&#25552;&#20379;&#24037;&#20855;&#21517;&#31216;&#21644;Python&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#24418;&#29289;&#20307;&#25805;&#32437;&#26159;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#26368;&#36855;&#20154;&#21448;&#26368;&#33392;&#24040;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;&#36890;&#36807;&#28436;&#31034;&#23398;&#20064;&#28508;&#22312;&#21160;&#24577;&#65292;&#36890;&#24120;&#34920;&#31034;&#20026;&#31890;&#23376;&#25110;&#22270;&#20687;&#20043;&#19968;&#65292;&#20294;&#23384;&#22312;&#19968;&#20010;&#37325;&#35201;&#38480;&#21046;&#65306;&#33719;&#21462;&#36866;&#24403;&#30340;&#28436;&#31034;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#26102;&#38388;&#20219;&#21153;&#65292;&#21487;&#33021;&#26159;&#22256;&#38590;&#30340;&#12290;&#27492;&#22806;&#65292;&#23436;&#20840;&#22522;&#20110;&#28436;&#31034;&#36827;&#34892;&#23398;&#20064;&#21487;&#33021;&#20250;&#38459;&#30861;&#27169;&#22411;&#36229;&#36234;&#28436;&#31034;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26080;&#38656;&#28436;&#31034;&#30340;&#20998;&#23618;&#35268;&#21010;&#26041;&#27861;&#65292;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#38271;&#26102;&#38388;&#20219;&#21153;&#32780;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#12290;&#25105;&#20204;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#34920;&#36798;&#19982;&#25351;&#23450;&#20219;&#21153;&#23545;&#24212;&#30340;&#39640;&#23618;&#12289;&#38454;&#27573;-by-&#38454;&#27573;&#35745;&#21010;&#12290;&#23545;&#20110;&#27599;&#20010;&#21333;&#29420;&#38454;&#27573;&#65292;LLM&#25552;&#20379;&#24037;&#20855;&#30340;&#21517;&#31216;&#21644;Python&#20195;&#30721;&#65292;&#20197;&#21046;&#20316;&#20013;&#38388;&#23376;&#30446;&#26631;&#28857;&#20113;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02787v2 Announce Type: replace-cross  Abstract: Deformable object manipulation stands as one of the most captivating yet formidable challenges in robotics. While previous techniques have predominantly relied on learning latent dynamics through demonstrations, typically represented as either particles or images, there exists a pertinent limitation: acquiring suitable demonstrations, especially for long-horizon tasks, can be elusive. Moreover, basing learning entirely on demonstrations can hamper the model's ability to generalize beyond the demonstrated tasks. In this work, we introduce a demonstration-free hierarchical planning approach capable of tackling intricate long-horizon tasks without necessitating any training. We employ large language models (LLMs) to articulate a high-level, stage-by-stage plan corresponding to a specified task. For every individual stage, the LLM provides both the tool's name and the Python code to craft intermediate subgoal point clouds. With the
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#22240;&#26524;&#22270;&#19978;&#36827;&#34892;&#22240;&#26524;&#38382;&#31572;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21592;&#35780;&#35770;&#30340; agent&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24403;&#21069;&#22240;&#26524;&#38382;&#31572;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#35299;&#37322;&#25110;&#35777;&#25454;&#30340;&#38382;&#39064;</title><link>https://arxiv.org/abs/2311.02760</link><description>&lt;p&gt;
&#29992;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#22240;&#26524;&#38382;&#31572;
&lt;/p&gt;
&lt;p&gt;
Causal Question Answering with Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2311.02760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#22312;&#22240;&#26524;&#22270;&#19978;&#36827;&#34892;&#22240;&#26524;&#38382;&#31572;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21592;&#35780;&#35770;&#30340; agent&#65292;&#26377;&#25928;&#35299;&#20915;&#20102;&#24403;&#21069;&#22240;&#26524;&#38382;&#31572;&#26041;&#27861;&#26080;&#27861;&#25552;&#20379;&#35299;&#37322;&#25110;&#35777;&#25454;&#30340;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#38382;&#39064;&#25506;&#31350;&#19981;&#21516;&#20107;&#20214;&#25110;&#29616;&#35937;&#20043;&#38388;&#30340;&#22240;&#26524;&#20851;&#31995;&#12290;&#23427;&#20204;&#23545;&#21508;&#31181;&#29992;&#20363;&#37117;&#24456;&#37325;&#35201;&#65292;&#21253;&#25324;&#34394;&#25311;&#21161;&#25163;&#21644;&#25628;&#32034;&#24341;&#25806;&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#24403;&#21069;&#30340;&#22240;&#26524;&#38382;&#31572;&#26041;&#27861;&#19981;&#33021;&#20026;&#20854;&#31572;&#26696;&#25552;&#20379;&#35299;&#37322;&#25110;&#35777;&#25454;&#12290;&#22240;&#27492;&#65292;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#29992;&#22240;&#26524;&#22270;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#20851;&#20110;&#21517;&#35789;&#30701;&#35821;&#20043;&#38388;&#22240;&#26524;&#20851;&#31995;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#25552;&#20379;&#20851;&#31995;&#30340;&#26469;&#28304;&#25968;&#25454;&#12290;&#21463;&#26368;&#36817;&#23558;&#24378;&#21270;&#23398;&#20064;&#25104;&#21151;&#24212;&#29992;&#20110;&#30693;&#35782;&#22270;&#20219;&#21153;&#30340;&#21551;&#21457;&#65292;&#20363;&#22914;&#38142;&#25509;&#39044;&#27979;&#21644;&#20107;&#23454;&#26816;&#26597;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#23558;&#24378;&#21270;&#23398;&#20064;&#24212;&#29992;&#20110;&#22240;&#26524;&#22270;&#36827;&#34892;&#22240;&#26524;&#38382;&#31572;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21592;&#35780;&#35770;&#30340; agent&#65292;&#23427;&#23398;&#20064;&#36890;&#36807;&#22270;&#25628;&#32034;&#26469;&#22238;&#31572;&#22240;&#26524;&#38382;&#39064;&#12290;&#25105;&#20204;&#36890;&#36807;&#30417;&#30563;&#23398;&#20064;&#31243;&#24207;&#24341;&#23548; agent &#22788;&#29702;&#22823;&#35268;&#27169;&#25805;&#20316;&#31354;&#38388;&#21644;sp
&lt;/p&gt;
&lt;p&gt;
arXiv:2311.02760v2 Announce Type: replace  Abstract: Causal questions inquire about causal relationships between different events or phenomena. They are important for a variety of use cases, including virtual assistants and search engines. However, many current approaches to causal question answering cannot provide explanations or evidence for their answers. Hence, in this paper, we aim to answer causal questions with a causality graph, a large-scale dataset of causal relations between noun phrases along with the relations' provenance data. Inspired by recent, successful applications of reinforcement learning to knowledge graph tasks, such as link prediction and fact-checking, we explore the application of reinforcement learning on a causality graph for causal question answering. We introduce an Actor-Critic-based agent which learns to search through the graph to answer causal questions. We bootstrap the agent with a supervised learning procedure to deal with large action spaces and sp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#23558;&#25991;&#26412;&#27169;&#24577;&#34701;&#20837;&#22478;&#24066;&#22270;&#20687;&#25551;&#36848;&#30340;LLM&#22686;&#24378;&#26694;&#26550;UrbanCLIP&#65292;&#24182;&#25506;&#35752;&#20102;&#25991;&#26412;&#27169;&#24577;&#22914;&#20309;&#22686;&#24378;&#22478;&#24066;&#21306;&#22495;&#25551;&#36848;&#20197;&#21450;&#20854;&#24433;&#21709;&#26041;&#38754;&#12290;</title><link>https://arxiv.org/abs/2310.18340</link><description>&lt;p&gt;
UrbanCLIP&#65306;&#23398;&#20064;&#26469;&#33258;&#32593;&#32476;&#30340;&#23545;&#27604;&#35821;&#35328;&#22270;&#20687;&#39044;&#35757;&#32451;&#25991;&#26412;&#22686;&#24378;&#30340;&#22478;&#24066;&#21306;&#22495;&#25551;&#36848;
&lt;/p&gt;
&lt;p&gt;
UrbanCLIP: Learning Text-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining from the Web
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.18340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#23558;&#25991;&#26412;&#27169;&#24577;&#34701;&#20837;&#22478;&#24066;&#22270;&#20687;&#25551;&#36848;&#30340;LLM&#22686;&#24378;&#26694;&#26550;UrbanCLIP&#65292;&#24182;&#25506;&#35752;&#20102;&#25991;&#26412;&#27169;&#24577;&#22914;&#20309;&#22686;&#24378;&#22478;&#24066;&#21306;&#22495;&#25551;&#36848;&#20197;&#21450;&#20854;&#24433;&#21709;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#32593;&#32476;&#25968;&#25454;&#36827;&#34892;&#30340;&#22478;&#24066;&#21306;&#22495;&#25551;&#36848;&#23545;&#22478;&#24066;&#35268;&#21010;&#21644;&#21487;&#25345;&#32493;&#21457;&#23637;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#30446;&#30585;&#20102;LLM&#22312;&#21508;&#20010;&#39046;&#22495;&#30340;&#19981;&#26029;&#23835;&#36215;&#65292;&#23588;&#20854;&#26159;&#22788;&#29702;&#22810;&#27169;&#24577;&#25968;&#25454;&#30740;&#31350;&#65292;&#22914;&#35270;&#35273;-&#35821;&#35328;&#23398;&#20064;&#65292;&#20854;&#20013;&#25991;&#26412;&#27169;&#24577;&#20316;&#20026;&#22270;&#20687;&#30340;&#34917;&#20805;&#20449;&#24687;&#12290;&#26412;&#25991;&#26088;&#22312;&#22238;&#31572;&#20004;&#20010;&#22522;&#26412;&#38382;&#39064;&#65306;i&#65289;&#25991;&#26412;&#27169;&#24577;&#33021;&#21542;&#22686;&#24378;&#22478;&#24066;&#21306;&#22495;&#25551;&#36848;&#65311;ii&#65289;&#22914;&#26524;&#21487;&#20197;&#65292;&#20197;&#20309;&#31181;&#26041;&#24335;&#21644;&#22312;&#21738;&#20123;&#26041;&#38754;&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#24378;&#22823;&#21147;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#31532;&#19968;&#20010;&#38598;&#25104;&#25991;&#26412;&#27169;&#24577;&#30693;&#35782;&#21040;&#22478;&#24066;&#22270;&#20687;&#25551;&#36848;&#20013;&#30340;LLM&#22686;&#24378;&#26694;&#26550;&#65292;&#21629;&#21517;&#20026;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;LLM&#22686;&#24378;&#22411;&#22478;&#24066;&#21306;&#22495;&#25551;&#36848;&#65288;UrbanCLIP&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.18340v2 Announce Type: replace-cross  Abstract: Urban region profiling from web-sourced data is of utmost importance for urban planning and sustainable development. We are witnessing a rising trend of LLMs for various fields, especially dealing with multi-modal data research such as vision-language learning, where the text modality serves as a supplement information for the image. Since textual modality has never been introduced into modality combinations in urban region profiling, we aim to answer two fundamental questions in this paper: i) Can textual modality enhance urban region profiling? ii) and if so, in what ways and with regard to which aspects? To answer the questions, we leverage the power of Large Language Models (LLMs) and introduce the first-ever LLM-enhanced framework that integrates the knowledge of textual modality into urban imagery profiling, named LLM-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining (UrbanCLIP). Specifically, it
&lt;/p&gt;</description></item><item><title>BatteryML&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#36890;&#36807;&#19968;&#31449;&#24335;&#12289;&#20840;&#38754;&#30340;&#26041;&#27861;&#32479;&#19968;&#20102;&#30005;&#27744;&#34928;&#20943;&#24314;&#27169;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#23454;&#29616;&#65292;&#25552;&#39640;&#20102;&#30740;&#31350;&#24212;&#29992;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;</title><link>https://arxiv.org/abs/2310.14714</link><description>&lt;p&gt;
BatteryML&#65306;&#19968;&#20010;&#29992;&#20110;&#30005;&#27744;&#34928;&#20943;&#26426;&#22120;&#23398;&#20064;&#30340;&#24320;&#28304;&#24179;&#21488;
&lt;/p&gt;
&lt;p&gt;
BatteryML:An Open-source platform for Machine Learning on Battery Degradation
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.14714
&lt;/p&gt;
&lt;p&gt;
BatteryML&#26159;&#19968;&#20010;&#24320;&#28304;&#24179;&#21488;&#65292;&#36890;&#36807;&#19968;&#31449;&#24335;&#12289;&#20840;&#38754;&#30340;&#26041;&#27861;&#32479;&#19968;&#20102;&#30005;&#27744;&#34928;&#20943;&#24314;&#27169;&#30340;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#21644;&#27169;&#22411;&#23454;&#29616;&#65292;&#25552;&#39640;&#20102;&#30740;&#31350;&#24212;&#29992;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#27744;&#34928;&#20943;&#20173;&#28982;&#26159;&#33021;&#28304;&#23384;&#20648;&#39046;&#22495;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65292;&#32780;&#26426;&#22120;&#23398;&#20064;&#20316;&#20026;&#25512;&#21160;&#27934;&#23519;&#21644;&#35299;&#20915;&#26041;&#26696;&#30340;&#26377;&#25928;&#24037;&#20855;&#27491;&#22312;&#23835;&#36215;&#12290;&#28982;&#32780;&#65292;&#30005;&#21270;&#23398;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#20132;&#21449;&#39046;&#22495;&#24102;&#26469;&#20102;&#22797;&#26434;&#30340;&#25361;&#25112;&#12290;&#26426;&#22120;&#23398;&#20064;&#19987;&#23478;&#32463;&#24120;&#22312;&#22788;&#29702;&#30005;&#27744;&#31185;&#23398;&#30340;&#22797;&#26434;&#24615;&#19978;&#33510;&#33510;&#25379;&#25166;&#65292;&#32780;&#30005;&#27744;&#30740;&#31350;&#20154;&#21592;&#21017;&#38754;&#20020;&#30528;&#23558;&#22797;&#26434;&#27169;&#22411;&#35843;&#25972;&#21040;&#29305;&#23450;&#25968;&#25454;&#38598;&#30340;&#38556;&#30861;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#28085;&#30422;&#25968;&#25454;&#26684;&#24335;&#21644;&#35780;&#20272;&#22522;&#20934;&#30340;&#30005;&#27744;&#34928;&#20943;&#24314;&#27169;&#30340;&#32479;&#19968;&#26631;&#20934;&#12290;&#37492;&#20110;&#36825;&#20123;&#38556;&#30861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;BatteryML - &#19968;&#20010;&#19968;&#31449;&#24335;&#12289;&#20840;&#38754;&#19988;&#24320;&#28304;&#30340;&#24179;&#21488;&#65292;&#26088;&#22312;&#32479;&#19968;&#25968;&#25454;&#39044;&#22788;&#29702;&#12289;&#29305;&#24449;&#25552;&#21462;&#20197;&#21450;&#20256;&#32479;&#21644;&#26368;&#20808;&#36827;&#27169;&#22411;&#30340;&#23454;&#29616;&#12290;&#36825;&#31181;&#31616;&#21270;&#30340;&#26041;&#27861;&#26377;&#26395;&#25552;&#39640;&#30740;&#31350;&#24212;&#29992;&#30340;&#23454;&#29992;&#24615;&#21644;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.14714v4 Announce Type: replace-cross  Abstract: Battery degradation remains a pivotal concern in the energy storage domain, with machine learning emerging as a potent tool to drive forward insights and solutions. However, this intersection of electrochemical science and machine learning poses complex challenges. Machine learning experts often grapple with the intricacies of battery science, while battery researchers face hurdles in adapting intricate models tailored to specific datasets. Beyond this, a cohesive standard for battery degradation modeling, inclusive of data formats and evaluative benchmarks, is conspicuously absent. Recognizing these impediments, we present BatteryML - a one-step, all-encompass, and open-source platform designed to unify data preprocessing, feature extraction, and the implementation of both traditional and state-of-the-art models. This streamlined approach promises to enhance the practicality and efficiency of research applications. BatteryML s
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20351;&#29992;&#20915;&#31574;&#27169;&#22411;Bridge&#65292;&#32467;&#21512;&#19987;&#23478;&#30340;&#35748;&#30693;&#20219;&#21153;&#20998;&#26512;&#65292;&#25104;&#21151;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24357;&#34917;&#26032;&#25163;&#21644;&#19987;&#23478;&#22312;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;</title><link>https://arxiv.org/abs/2310.10648</link><description>&lt;p&gt;
&#36890;&#36807;&#20915;&#31574;&#27169;&#22411;&#24357;&#34917;&#26032;&#25163;&#19982;&#19987;&#23478;&#20043;&#38388;&#30340;&#24046;&#36317;&#65306;&#20197;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20026;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Bridging the Novice-Expert Gap via Models of Decision-Making: A Case Study on Remediating Math Mistakes
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.10648
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20351;&#29992;&#20915;&#31574;&#27169;&#22411;Bridge&#65292;&#32467;&#21512;&#19987;&#23478;&#30340;&#35748;&#30693;&#20219;&#21153;&#20998;&#26512;&#65292;&#25104;&#21151;&#21033;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26469;&#24357;&#34917;&#26032;&#25163;&#21644;&#19987;&#23478;&#22312;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20013;&#30340;&#30693;&#35782;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#36136;&#37327;&#36741;&#23548;&#35268;&#27169;&#21270;&#20173;&#28982;&#26159;&#25945;&#32946;&#20013;&#30340;&#19968;&#39033;&#20027;&#35201;&#25361;&#25112;&#12290;&#30001;&#20110;&#38656;&#27714;&#22686;&#38271;&#65292;&#35768;&#22810;&#24179;&#21488;&#32856;&#29992;&#26032;&#25163;&#23548;&#24072;&#65292;&#20182;&#20204;&#19982;&#32463;&#39564;&#20016;&#23500;&#30340;&#25945;&#32946;&#24037;&#20316;&#32773;&#19981;&#21516;&#65292;&#38590;&#20197;&#35299;&#20915;&#23398;&#29983;&#30340;&#38169;&#35823;&#65292;&#22240;&#27492;&#26080;&#27861;&#25235;&#20303;&#20027;&#35201;&#30340;&#23398;&#20064;&#26426;&#20250;&#12290;&#25105;&#20204;&#30340;&#24037;&#20316;&#25506;&#35752;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#32416;&#27491;&#25968;&#23398;&#38169;&#35823;&#20013;&#24357;&#34917;&#26032;&#25163;&#21644;&#19987;&#23478;&#20043;&#38388;&#30693;&#35782;&#24046;&#36317;&#30340;&#28508;&#21147;&#12290;&#25105;&#20204;&#25552;&#20986;Bridge&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;&#35748;&#30693;&#20219;&#21153;&#20998;&#26512;&#23558;&#19987;&#23478;&#30340;&#28508;&#22312;&#24605;&#32500;&#36807;&#31243;&#36716;&#21270;&#20026;&#32416;&#27491;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#36825;&#28041;&#21450;&#19987;&#23478;&#35782;&#21035;(A)&#23398;&#29983;&#30340;&#38169;&#35823;&#12289;(B)&#32416;&#27491;&#31574;&#30053;&#21644;(C)&#29983;&#25104;&#22238;&#24212;&#20043;&#21069;&#30340;&#24847;&#22270;&#12290;&#25105;&#20204;&#26500;&#24314;&#20102;&#19968;&#20010;&#21253;&#21547;700&#20010;&#30495;&#23454;&#36741;&#23548;&#23545;&#35805;&#30340;&#25968;&#25454;&#38598;&#65292;&#30001;&#19987;&#23478;&#26631;&#27880;&#20102;&#20182;&#20204;&#30340;&#20915;&#31574;&#12290;&#25105;&#20204;&#22312;&#25105;&#20204;&#30340;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#26368;&#20808;&#36827;&#30340;LLMs&#65292;&#24182;&#21457;&#29616;&#19987;&#23478;&#30340;&#20915;&#31574;&#27169;&#22411;&#23545;LLMs&#26469;&#35828;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#65292;&#20197;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#65306;&#22238;&#24212;f
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.10648v2 Announce Type: replace-cross  Abstract: Scaling high-quality tutoring remains a major challenge in education. Due to growing demand, many platforms employ novice tutors who, unlike experienced educators, struggle to address student mistakes and thus fail to seize prime learning opportunities. Our work explores the potential of large language models (LLMs) to close the novice-expert knowledge gap in remediating math mistakes. We contribute Bridge, a method that uses cognitive task analysis to translate an expert's latent thought process into a decision-making model for remediation. This involves an expert identifying (A) the student's error, (B) a remediation strategy, and (C) their intention before generating a response. We construct a dataset of 700 real tutoring conversations, annotated by experts with their decisions. We evaluate state-of-the-art LLMs on our dataset and find that the expert's decision-making model is critical for LLMs to close the gap: responses f
&lt;/p&gt;</description></item><item><title>&#22810;&#27169;&#24577;&#20195;&#29702;&#22312;&#22788;&#29702;&#22797;&#26434;&#25361;&#25112;&#26102;&#38656;&#35201;&#32771;&#34385;&#27169;&#22411;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#36991;&#20813;&#25191;&#34892;&#30340;&#33030;&#24369;&#24615;&#12290;</title><link>https://arxiv.org/abs/2310.08446</link><description>&lt;p&gt;
&#36890;&#36807;&#27169;&#22411;&#36873;&#25321;&#23454;&#29616;&#20581;&#22766;&#30340;&#22810;&#27169;&#24577;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Towards Robust Multi-Modal Reasoning via Model Selection
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.08446
&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#20195;&#29702;&#22312;&#22788;&#29702;&#22797;&#26434;&#25361;&#25112;&#26102;&#38656;&#35201;&#32771;&#34385;&#27169;&#22411;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#36991;&#20813;&#25191;&#34892;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#26222;&#36941;&#25215;&#35748;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#22312;&#24037;&#20855;&#23398;&#20064;&#21644;&#33258;&#20027;&#20195;&#29702;&#30740;&#31350;&#20013;&#40723;&#33310;&#20102;&#30740;&#31350;&#12290;LLM&#20805;&#24403;&#20195;&#29702;&#30340;&#8220;&#22823;&#33041;&#8221;&#65292;&#20026;&#21327;&#20316;&#22810;&#27493;&#20219;&#21153;&#27714;&#35299;&#38598;&#25104;&#22810;&#20010;&#24037;&#20855;&#12290;&#22810;&#27169;&#24577;&#20195;&#29702;&#36890;&#36807;&#25972;&#21512;&#21508;&#31181;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#22788;&#29702;&#22797;&#26434;&#25361;&#25112;&#65292;&#22312;&#22788;&#29702;&#30452;&#35266;&#20219;&#21153;&#26102;&#19981;&#20687;&#35843;&#29992;&#35745;&#31639;&#22120;&#25110;&#22825;&#27668;API&#37027;&#26679;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#22810;&#27169;&#24577;&#20195;&#29702;&#24573;&#35270;&#20102;&#27169;&#22411;&#36873;&#25321;&#30340;&#37325;&#35201;&#24615;&#65306;&#23427;&#20204;&#20027;&#35201;&#19987;&#27880;&#20110;&#35745;&#21010;&#21644;&#25191;&#34892;&#38454;&#27573;&#65292;&#21482;&#20250;&#20026;&#27599;&#20010;&#23376;&#20219;&#21153;&#35843;&#29992;&#39044;&#23450;&#20041;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#65292;&#20351;&#25191;&#34892;&#21464;&#24471;&#33030;&#24369;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#20854;&#20182;&#20256;&#32479;&#30340;&#27169;&#22411;&#36873;&#25321;&#26041;&#27861;&#35201;&#20040;&#19982;&#22810;&#27169;&#24577;&#20195;&#29702;&#22330;&#26223;&#19981;&#20860;&#23481;&#25110;&#19981;&#29702;&#24819;&#65292;&#22240;&#20026;&#23427;&#20204;&#24573;&#35270;&#20102;&#22810;&#27493;&#25512;&#29702;&#20135;&#29983;&#30340;&#23376;&#20219;&#21153;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20027;&#35201;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.08446v2 Announce Type: replace-cross  Abstract: The reasoning capabilities of LLM (Large Language Model) are widely acknowledged in recent research, inspiring studies on tool learning and autonomous agents. LLM serves as the "brain" of the agent, orchestrating multiple tools for collaborative multi-step task solving. Unlike methods invoking tools like calculators or weather APIs for straightforward tasks, multi-modal agents excel by integrating diverse AI models for complex challenges. However, current multi-modal agents neglect the significance of model selection: they primarily focus on the planning and execution phases, and will only invoke predefined task-specific models for each subtask, making the execution fragile. Meanwhile, other traditional model selection methods are either incompatible with or suboptimal for the multi-modal agent scenarios, due to ignorance of dependencies among subtasks arising by multi-step reasoning. To this end, we identify the key challenges
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;NMT&#27169;&#22411;&#26469;&#20272;&#35745;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#21487;&#20197;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#28040;&#38500;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>https://arxiv.org/abs/2310.06707</link><description>&lt;p&gt;
&#36136;&#37327;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#65306;&#21333;&#19968;&#27169;&#22411;&#20013;&#30340;&#39640;&#25928;&#29983;&#25104;&#21644;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Quality-Aware Translation Models: Efficient Generation and Quality Estimation in a Single Model
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.06707
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#36136;&#37327;&#24863;&#30693;&#32763;&#35793;&#27169;&#22411;&#65292;&#36890;&#36807;&#35757;&#32451;NMT&#27169;&#22411;&#26469;&#20272;&#35745;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#21487;&#20197;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#28040;&#38500;&#39069;&#22806;&#30340;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#22823;&#21518;&#39564;&#65288;MAP&#65289;&#35299;&#30721;&#26159;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#65288;NMT&#65289;&#27169;&#22411;&#20013;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#35299;&#30721;&#31574;&#30053;&#12290; &#30740;&#31350;&#34920;&#26126;&#65292;&#27169;&#22411;&#27010;&#29575;&#19982;&#20154;&#31867;&#21028;&#26029;&#30456;&#20851;&#65292;&#20294;&#19981;&#33021;&#24635;&#26159;&#25104;&#31435;&#65292;&#29983;&#25104;&#36136;&#37327;&#21487;&#20197;&#36890;&#36807;&#35299;&#30721;&#26469;&#20248;&#21270;&#19968;&#20010;&#20197;&#24230;&#37327;&#25110;&#36136;&#37327;&#35780;&#20272;&#20449;&#21495;&#25903;&#25345;&#30340;&#25928;&#29992;&#20989;&#25968;&#26469;&#25552;&#39640;&#65292;&#21363;&#26368;&#23567;&#36125;&#21494;&#26031;&#39118;&#38505;&#65288;MBR&#65289;&#25110;&#36136;&#37327;&#24863;&#30693;&#35299;&#30721;&#12290; &#36825;&#20123;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#22312;&#20110;&#23427;&#20204;&#38656;&#35201;&#19968;&#20010;&#39069;&#22806;&#30340;&#27169;&#22411;&#22312;&#35299;&#30721;&#36807;&#31243;&#20013;&#35745;&#31639;&#25928;&#29992;&#20989;&#25968;&#65292;&#20250;&#26174;&#33879;&#22686;&#21152;&#35745;&#31639;&#25104;&#26412;&#12290; &#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35757;&#32451;NMT&#27169;&#22411;&#33258;&#24049;&#26469;&#20272;&#35745;&#20854;&#36755;&#20986;&#36136;&#37327;&#65292;&#20174;&#32780;&#20351;NMT&#27169;&#22411;&#26412;&#36523;&#20855;&#22791;&#36136;&#37327;&#24863;&#30693;&#33021;&#21147;&#12290; &#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#36827;&#34892;MBR&#35299;&#30721;&#21487;&#20197;&#26174;&#33879;&#20943;&#23567;&#23610;&#23544;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.06707v2 Announce Type: replace-cross  Abstract: Maximum-a-posteriori (MAP) decoding is the most widely used decoding strategy for neural machine translation (NMT) models. The underlying assumption is that model probability correlates well with human judgment, with better translations getting assigned a higher score by the model. However, research has shown that this assumption does not always hold, and generation quality can be improved by decoding to optimize a utility function backed by a metric or quality-estimation signal, as is done by Minimum Bayes Risk (MBR) or Quality-Aware decoding. The main disadvantage of these approaches is that they require an additional model to calculate the utility function during decoding, significantly increasing the computational cost. In this paper, we propose to make the NMT models themselves quality-aware by training them to estimate the quality of their own output. Using this approach for MBR decoding we can drastically reduce the size
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20174;&#20803;&#23398;&#20064;&#35270;&#35282;&#25506;&#35752;&#20102;Transformer&#29992;&#20110;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#26102;&#30340;&#20869;&#37096;&#20248;&#21270;&#36807;&#31243;&#65292;&#21457;&#29616;&#24182;&#20998;&#26512;&#20102;Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#30340;token&#34920;&#31034;&#33539;&#25968;&#30340;&#29305;&#27530;&#29305;&#24449;&#12290;</title><link>https://arxiv.org/abs/2310.05884</link><description>&lt;p&gt;
&#20174;&#20803;&#23398;&#20064;&#35270;&#35282;&#30475;Transformer&#29992;&#20110;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
A Meta-Learning Perspective on Transformers for Causal Language Modeling
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05884
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#20803;&#23398;&#20064;&#35270;&#35282;&#25506;&#35752;&#20102;Transformer&#29992;&#20110;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#26102;&#30340;&#20869;&#37096;&#20248;&#21270;&#36807;&#31243;&#65292;&#21457;&#29616;&#24182;&#20998;&#26512;&#20102;Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#30340;token&#34920;&#31034;&#33539;&#25968;&#30340;&#29305;&#27530;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26550;&#26500;&#22312;&#24320;&#21457;&#22823;&#22411;&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#26041;&#38754;&#21464;&#24471;&#26174;&#33879;&#12290;&#28982;&#32780;&#65292;&#35299;&#37322;&#20854;&#33021;&#21147;&#30340;&#26426;&#21046;&#23578;&#19981;&#20026;&#20154;&#25152;&#20102;&#35299;&#12290;&#26412;&#25991;&#20391;&#37325;&#20110;&#35757;&#32451;&#36807;&#31243;&#65292;&#24314;&#31435;&#20102;&#19968;&#20010;&#20803;&#23398;&#20064;&#35270;&#35282;&#26469;&#25506;&#31350;Transformer&#26550;&#26500;&#22312;&#22240;&#26524;&#35821;&#35328;&#24314;&#27169;&#20219;&#21153;&#35757;&#32451;&#26102;&#30340;&#20869;&#37096;&#20248;&#21270;&#36807;&#31243;&#65292;&#35814;&#32454;&#35828;&#26126;&#20102;Transformer&#20869;&#37096;&#30340;&#19968;&#20010;&#20248;&#21270;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#22312;&#36825;&#20010;&#20869;&#37096;&#20248;&#21270;&#36807;&#31243;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#24182;&#29702;&#35770;&#20998;&#26512;&#20102;Transformer-based&#22240;&#26524;&#35821;&#35328;&#27169;&#22411;&#20013;&#23398;&#20064;&#21040;&#30340;token&#34920;&#31034;&#30340;&#33539;&#25968;&#30340;&#29305;&#27530;&#29305;&#24449;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#24471;&#21040;&#20102;&#21508;&#31181;&#35774;&#32622;&#20013;&#30340;&#23454;&#39564;&#35777;&#23454;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05884v2 Announce Type: replace-cross  Abstract: The Transformer architecture has become prominent in developing large causal language models. However, mechanisms to explain its capabilities are not well understood. Focused on the training process, here we establish a meta-learning view of the Transformer architecture when trained for the causal language modeling task, by explicating an inner optimization process within the Transformer. Further, within the inner optimization, we discover and theoretically analyze a special characteristic of the norms of learned token representations within Transformer-based causal language models. Our analysis is supported by experiments in various settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#26032;&#22411;&#20840;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#25104;&#21151;&#24212;&#29992;&#20110;&#22788;&#29702;&#33151;&#24335;&#26426;&#22120;&#20154;&#65292;&#22312;&#21508;&#31181;&#27169;&#25311;&#22320;&#24418;&#20013;&#21462;&#24471;&#20102;&#26480;&#20986;&#32467;&#26524;&#12290;</title><link>https://arxiv.org/abs/2310.05022</link><description>&lt;p&gt;
&#29992;&#20110;&#33151;&#24335;&#26426;&#22120;&#20154;&#30340;&#20840;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Fully Spiking Neural Network for Legged Robots
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.05022
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#26032;&#22411;&#20840;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#25104;&#21151;&#24212;&#29992;&#20110;&#22788;&#29702;&#33151;&#24335;&#26426;&#22120;&#20154;&#65292;&#22312;&#21508;&#31181;&#27169;&#25311;&#22320;&#24418;&#20013;&#21462;&#24471;&#20102;&#26480;&#20986;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33151;&#24335;&#26426;&#22120;&#20154;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#12290;&#22235;&#36275;&#26426;&#22120;&#20154;&#23637;&#31034;&#20102;&#22312;&#22797;&#26434;&#29615;&#22659;&#20013;&#23436;&#25104;&#20855;&#26377;&#25361;&#25112;&#24615;&#20219;&#21153;&#30340;&#33021;&#21147;&#65292;&#24182;&#24050;&#37096;&#32626;&#22312;&#29616;&#23454;&#22330;&#26223;&#20013;&#20197;&#21327;&#21161;&#20154;&#31867;&#12290;&#21516;&#26102;&#65292;&#20004;&#36275;&#21644;&#31867;&#20154;&#26426;&#22120;&#20154;&#22312;&#21508;&#31181;&#39640;&#38590;&#24230;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#26412;&#30740;&#31350;&#25104;&#21151;&#23558;&#19968;&#31181;&#26032;&#22411;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#24212;&#29992;&#20110;&#22788;&#29702;&#33151;&#24335;&#26426;&#22120;&#20154;&#65292;&#22312;&#19968;&#31995;&#21015;&#27169;&#25311;&#22320;&#24418;&#20013;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.05022v2 Announce Type: replace-cross  Abstract: In recent years, legged robots based on deep reinforcement learning have made remarkable progress. Quadruped robots have demonstrated the ability to complete challenging tasks in complex environments and have been deployed in real-world scenarios to assist humans. Simultaneously, bipedal and humanoid robots have achieved breakthroughs in various demanding tasks. Current reinforcement learning methods can utilize diverse robot bodies and historical information to perform actions. However, prior research has not emphasized the speed and energy consumption of network inference, as well as the biological significance of the neural networks themselves. Most of the networks employed are traditional artificial neural networks that utilize multilayer perceptrons (MLP). In this paper, we successfully apply a novel Spiking Neural Network (SNN) to process legged robots, achieving outstanding results across a range of simulated terrains. S
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;IR-RGB&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;HalluciDet&#65292;&#36890;&#36807;&#29305;&#26435;&#20449;&#24687;&#20943;&#23569;RGB&#26816;&#27979;&#22120;&#30340;&#26816;&#27979;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;</title><link>https://arxiv.org/abs/2310.04662</link><description>&lt;p&gt;
HalluciDet: &#36890;&#36807;&#29305;&#26435;&#20449;&#24687;&#20351;RGB&#27169;&#24577;&#24187;&#35937;&#29992;&#20110;&#20154;&#21592;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
HalluciDet: Hallucinating RGB Modality for Person Detection Through Privileged Information
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.04662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;IR-RGB&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;HalluciDet&#65292;&#36890;&#36807;&#29305;&#26435;&#20449;&#24687;&#20943;&#23569;RGB&#26816;&#27979;&#22120;&#30340;&#26816;&#27979;&#25439;&#22833;&#65292;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#22270;&#20687;&#32763;&#35793;&#26159;&#23558;&#35270;&#35273;&#35782;&#21035;&#27169;&#22411;&#35843;&#25972;&#21040;&#26032;&#39046;&#22495;&#30340;&#26377;&#21147;&#26041;&#24335;&#12290;&#28982;&#32780;&#65292;&#24120;&#35265;&#30340;&#22270;&#20687;&#32763;&#35793;&#26041;&#27861;&#20165;&#19987;&#27880;&#20110;&#29983;&#25104;&#19982;&#30446;&#26631;&#22495;&#30456;&#21516;&#20998;&#24067;&#30340;&#25968;&#25454;&#12290;&#38024;&#23545;&#36328;&#27169;&#24577;&#24212;&#29992;&#65292;&#22914;&#20174;&#33322;&#31354;&#22270;&#20687;&#36827;&#34892;&#34892;&#20154;&#26816;&#27979;&#65292;&#22312;&#32418;&#22806;&#65288;IR&#65289;&#21040;&#21487;&#35265;&#65288;RGB&#65289;&#22270;&#20687;&#20043;&#38388;&#23384;&#22312;&#25968;&#25454;&#20998;&#24067;&#19978;&#30340;&#26174;&#30528;&#20559;&#31227;&#65292;&#19987;&#27880;&#20110;&#29983;&#25104;&#21487;&#33021;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#25439;&#22833;&#20851;&#27880;&#20110;&#20219;&#21153;&#30340;&#26080;&#20851;&#32454;&#33410;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;HalluciDet&#65292;&#19968;&#20010;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#30340;IR-RGB&#22270;&#20687;&#32763;&#35793;&#27169;&#22411;&#12290;&#19982;&#19987;&#27880;&#20110;&#22312;IR&#27169;&#24577;&#19978;&#37325;&#24314;&#21407;&#22987;&#22270;&#20687;&#19981;&#21516;&#65292;&#35813;&#27169;&#22411;&#26088;&#22312;&#20943;&#23569;RGB&#26816;&#27979;&#22120;&#30340;&#26816;&#27979;&#25439;&#22833;&#65292;&#20174;&#32780;&#36991;&#20813;&#35775;&#38382;RGB&#25968;&#25454;&#12290;&#35813;&#27169;&#22411;&#29983;&#25104;&#20102;&#19968;&#20010;&#26032;&#30340;&#22270;&#20687;&#34920;&#31034;&#65292;&#22686;&#24378;&#20102;&#22330;&#26223;&#20013;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.04662v2 Announce Type: replace-cross  Abstract: A powerful way to adapt a visual recognition model to a new domain is through image translation. However, common image translation approaches only focus on generating data from the same distribution as the target domain. Given a cross-modal application, such as pedestrian detection from aerial images, with a considerable shift in data distribution between infrared (IR) to visible (RGB) images, a translation focused on generation might lead to poor performance as the loss focuses on irrelevant details for the task. In this paper, we propose HalluciDet, an IR-RGB image translation model for object detection. Instead of focusing on reconstructing the original image on the IR modality, it seeks to reduce the detection loss of an RGB detector, and therefore avoids the need to access RGB data. This model produces a new image representation that enhances objects of interest in the scene and greatly improves detection performance. We e
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#24179;&#26041;&#25805;&#20316;&#23454;&#29616;&#30340;&#28040;&#20943;&#28151;&#21512;&#27169;&#22411;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#20248;&#20110;&#20256;&#32479;&#21152;&#27861;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#20272;&#35745;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;</title><link>https://arxiv.org/abs/2310.00724</link><description>&lt;p&gt;
&#36890;&#36807;&#24179;&#26041;&#30340;&#28040;&#20943;&#28151;&#21512;&#27169;&#22411;:&#34920;&#31034;&#21644;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Subtractive Mixture Models via Squaring: Representation and Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2310.00724
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24179;&#26041;&#25805;&#20316;&#23454;&#29616;&#30340;&#28040;&#20943;&#28151;&#21512;&#27169;&#22411;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#20248;&#20110;&#20256;&#32479;&#21152;&#27861;&#28151;&#21512;&#27169;&#22411;&#65292;&#24182;&#22312;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#20272;&#35745;&#20219;&#21153;&#20013;&#24471;&#21040;&#20102;&#23454;&#39564;&#35777;&#26126;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28151;&#21512;&#27169;&#22411;&#20256;&#32479;&#19978;&#26159;&#36890;&#36807;&#23558;&#20960;&#20010;&#20998;&#24067;&#20316;&#20026;&#32452;&#20214;&#30456;&#21152;&#26469;&#34920;&#31034;&#21644;&#23398;&#20064;&#30340;&#12290;&#20801;&#35768;&#28151;&#21512;&#20943;&#21435;&#27010;&#29575;&#36136;&#37327;&#25110;&#23494;&#24230;&#21487;&#20197;&#22823;&#22823;&#20943;&#23569;&#24314;&#27169;&#22797;&#26434;&#20998;&#24067;&#25152;&#38656;&#30340;&#32452;&#20214;&#25968;&#37327;&#12290;&#28982;&#32780;&#65292;&#23398;&#20064;&#36825;&#31181;&#20943;&#27861;&#28151;&#21512;&#27169;&#22411;&#24182;&#30830;&#20445;&#23427;&#20204;&#20173;&#28982;&#32534;&#30721;&#38750;&#36127;&#20989;&#25968;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25506;&#35752;&#20102;&#22914;&#20309;&#36890;&#36807;&#24179;&#26041;&#26469;&#23398;&#20064;&#21644;&#25191;&#34892;&#28145;&#24230;&#20943;&#27861;&#28151;&#21512;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#27010;&#29575;&#30005;&#36335;&#26694;&#26550;&#20013;&#36827;&#34892;&#36825;&#20123;&#30740;&#31350;&#65292;&#36825;&#20351;&#25105;&#20204;&#33021;&#22815;&#34920;&#31034;&#24352;&#37327;&#21270;&#30340;&#28151;&#21512;&#27169;&#22411;&#24182;&#27867;&#21270;&#20854;&#20182;&#20943;&#27861;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#20801;&#35768;&#20943;&#27861;&#30340;&#24179;&#26041;&#30005;&#36335;&#31867;&#21487;&#20197;&#27604;&#20256;&#32479;&#30340;&#21152;&#27861;&#28151;&#21512;&#27169;&#22411;&#20855;&#26377;&#25351;&#25968;&#32423;&#26356;&#20855;&#34920;&#36798;&#21147;&#65307;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#30495;&#23454;&#19990;&#30028;&#20998;&#24067;&#20272;&#35745;&#20219;&#21153;&#19978;&#23454;&#35777;&#23637;&#31034;&#20102;&#36825;&#31181;&#22686;&#21152;&#30340;&#34920;&#36798;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2310.00724v2 Announce Type: replace-cross  Abstract: Mixture models are traditionally represented and learned by adding several distributions as components. Allowing mixtures to subtract probability mass or density can drastically reduce the number of components needed to model complex distributions. However, learning such subtractive mixtures while ensuring they still encode a non-negative function is challenging. We investigate how to learn and perform inference on deep subtractive mixtures by squaring them. We do this in the framework of probabilistic circuits, which enable us to represent tensorized mixtures and generalize several other subtractive models. We theoretically prove that the class of squared circuits allowing subtractions can be exponentially more expressive than traditional additive mixtures; and, we empirically show this increased expressiveness on a series of real-world distribution estimation tasks.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2309.13339</link><description>&lt;p&gt;
&#36890;&#36807;&#36923;&#36753;&#22686;&#24378;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
Enhancing Zero-Shot Chain-of-Thought Reasoning in Large Language Models through Logic
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.13339
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#39564;&#35777;&#21644;&#32416;&#27491;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26368;&#26032;&#36827;&#23637;&#23637;&#31034;&#20102;&#23427;&#20204;&#22312;&#21508;&#20010;&#39046;&#22495;&#30340; remarkable generalizability&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#30340;&#25512;&#29702;&#33021;&#21147;&#20173;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#22810;&#27493;&#25512;&#29702;&#30340;&#24773;&#20917;&#19979;&#12290;&#23613;&#31649;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20855;&#26377;&#24191;&#27867;&#30340;&#30693;&#35782;&#65292;&#20294;&#23427;&#20204;&#30340;&#25512;&#29702;&#32463;&#24120;&#26410;&#33021;&#26377;&#25928;&#21033;&#29992;&#36825;&#20123;&#30693;&#35782;&#26469;&#24314;&#31435;&#36830;&#36143;&#30340;&#24605;&#32500;&#33539;&#24335;&#12290;&#36825;&#20123;&#27169;&#22411;&#26377;&#26102;&#20250;&#20986;&#29616;&#24187;&#35273;&#65292;&#22240;&#20026;&#23427;&#20204;&#30340;&#25512;&#29702;&#36807;&#31243;&#26410;&#21463;&#36923;&#36753;&#21407;&#21017;&#30340;&#38480;&#21046;&#12290;&#20026;&#20102;&#25913;&#36827;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#23556;&#38142;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102; LoT&#65288;Logical Thoughts&#65289;&#25552;&#31034;&#65292;&#36825;&#26159;&#19968;&#20010;&#33258;&#25105;&#25913;&#36827;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#26681;&#26893;&#20110;&#31526;&#21495;&#36923;&#36753;&#30340;&#21407;&#21017;&#65292;&#29305;&#21035;&#26159;&#24402;&#35884;&#27861;&#65292;&#36880;&#27493;&#31995;&#32479;&#22320;&#39564;&#35777;&#21644;&#32416;&#27491;&#25512;&#29702;&#36807;&#31243;&#12290;&#22312;&#35821;&#35328;&#20219;&#21153;&#19978;&#36827;&#34892;&#30340;&#23454;&#39564;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.13339v2 Announce Type: replace-cross  Abstract: Recent advancements in large language models have showcased their remarkable generalizability across various domains. However, their reasoning abilities still have significant room for improvement, especially when confronted with scenarios requiring multi-step reasoning. Although large language models possess extensive knowledge, their reasoning often fails to effectively utilize this knowledge to establish a coherent thinking paradigm. These models sometimes show hallucinations as their reasoning procedures are unconstrained by logical principles. Aiming at improving the zero-shot chain-of-thought reasoning ability of large language models, we propose LoT (Logical Thoughts) prompting, a self-improvement framework that leverages principles rooted in symbolic logic, particularly Reductio ad Absurdum, to systematically verify and rectify the reasoning processes step by step. Experimental evaluations conducted on language tasks in
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#25552;&#20379;&#20102;&#26032;&#26426;&#36935;&#65292;&#21487;&#20197;&#31616;&#21270;&#25512;&#33616;&#27969;&#31243;&#24182;&#30452;&#25509;&#20174;&#23436;&#25972;&#30340;&#39033;&#30446;&#27744;&#20013;&#29983;&#25104;&#25512;&#33616;&#12290;</title><link>https://arxiv.org/abs/2309.01157</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29992;&#20110;&#29983;&#25104;&#24335;&#25512;&#33616;&#65306;&#19968;&#39033;&#35843;&#26597;&#21644;&#36828;&#35265;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Large Language Models for Generative Recommendation: A Survey and Visionary Discussions
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2309.01157
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20026;&#25512;&#33616;&#31995;&#32479;&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#25552;&#20379;&#20102;&#26032;&#26426;&#36935;&#65292;&#21487;&#20197;&#31616;&#21270;&#25512;&#33616;&#27969;&#31243;&#24182;&#30452;&#25509;&#20174;&#23436;&#25972;&#30340;&#39033;&#30446;&#27744;&#20013;&#29983;&#25104;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#19981;&#20165;&#24443;&#24213;&#25913;&#21464;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#36824;&#26377;&#28508;&#21147;&#37325;&#22609;&#35768;&#22810;&#20854;&#20182;&#39046;&#22495;&#65292;&#20363;&#22914;&#25512;&#33616;&#31995;&#32479;&#65288;RS&#65289;&#12290;&#26412;&#25991;&#35843;&#26597;&#20102;&#22522;&#20110;LLM&#30340;&#29983;&#25104;&#24335;&#25512;&#33616;&#30340;&#36827;&#23637;&#12289;&#26041;&#27861;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#30528;&#30524;&#20110;&#19977;&#20010;&#38382;&#39064;&#65306;1&#65289;&#29983;&#25104;&#24335;&#25512;&#33616;&#26159;&#20160;&#20040;&#65292;2&#65289;&#20026;&#20160;&#20040;RS&#24212;&#35813;&#21457;&#23637;&#21040;&#29983;&#25104;&#24335;&#25512;&#33616;&#65292;3&#65289;&#22914;&#20309;&#20026;&#21508;&#31181;RS&#23454;&#29616;&#22522;&#20110;LLM&#30340;&#29983;&#25104;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2309.01157v2 Announce Type: replace-cross  Abstract: Large language models (LLM) not only have revolutionized the field of natural language processing (NLP) but also have the potential to reshape many other fields, e.g., recommender systems (RS). However, most of the related work treats an LLM as a component of the conventional recommendation pipeline (e.g., as a feature extractor), which may not be able to fully leverage the generative power of LLM. Instead of separating the recommendation process into multiple stages, such as score computation and re-ranking, this process can be simplified to one stage with LLM: directly generating recommendations from the complete pool of items. This survey reviews the progress, methods, and future directions of LLM-based generative recommendation by examining three questions: 1) What generative recommendation is, 2) Why RS should advance to generative recommendation, and 3) How to implement LLM-based generative recommendation for various RS t
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23545;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#34892;&#20154;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#37325;&#35201;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>https://arxiv.org/abs/2308.02935</link><description>&lt;p&gt;
&#25581;&#31034;&#30450;&#28857;&#65306;&#23545;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#20013;&#20844;&#24179;&#24615;&#30340;&#20851;&#38190;&#23457;&#26597;
&lt;/p&gt;
&lt;p&gt;
Unveiling the Blind Spots: A Critical Examination of Fairness in Autonomous Driving Systems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2308.02935
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23545;&#24403;&#21069;&#28145;&#24230;&#23398;&#20064;&#34892;&#20154;&#26816;&#27979;&#22120;&#30340;&#20844;&#24179;&#24615;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21457;&#29616;&#20102;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#37325;&#35201;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#24050;&#32463;&#25193;&#23637;&#20102;&#26234;&#33021;&#36710;&#36742;&#29289;&#32852;&#32593;&#30340;&#33539;&#22260;&#65292;&#24182;&#25104;&#20026;Web&#29983;&#24577;&#31995;&#32479;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#12290;&#31867;&#20284;&#20110;&#20256;&#32479;&#30340;&#22522;&#20110;Web&#30340;&#24212;&#29992;&#31243;&#24207;&#65292;&#20844;&#24179;&#24615;&#23545;&#20110;&#30830;&#20445;&#33258;&#21160;&#39550;&#39542;&#31995;&#32479;&#30340;&#39640;&#36136;&#37327;&#26159;&#19968;&#20010;&#37325;&#35201;&#26041;&#38754;&#65292;&#29305;&#21035;&#26159;&#22312;&#20854;&#20013;&#30340;&#34892;&#20154;&#26816;&#27979;&#22120;&#30340;&#32972;&#26223;&#19979;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20851;&#20110;&#24403;&#21069;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#30340;&#34892;&#20154;&#26816;&#27979;&#22120;&#20844;&#24179;&#24615;&#30340;&#32508;&#21512;&#35780;&#20272;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#20986;&#29616;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65292;&#25105;&#20204;&#22312;&#22823;&#35268;&#27169;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#19978;&#35780;&#20272;&#20102;&#20843;&#31181;&#34987;&#24191;&#27867;&#25506;&#32034;&#30340;DL&#34892;&#20154;&#26816;&#27979;&#22120;&#22312;&#20154;&#21475;&#32479;&#35745;&#23398;&#32676;&#20307;&#20043;&#38388;&#30340;&#34920;&#29616;&#12290;&#20026;&#20102;&#23454;&#29616;&#24443;&#24213;&#30340;&#20844;&#24179;&#24615;&#35780;&#20272;&#65292;&#25105;&#20204;&#20026;&#25968;&#25454;&#38598;&#25552;&#20379;&#20102;&#24191;&#27867;&#30340;&#27880;&#37322;&#65292;&#20849;&#28041;&#21450;8,311&#24352;&#22270;&#20687;&#65292;16,070&#20010;&#24615;&#21035;&#26631;&#31614;&#65292;20,115&#20010;&#24180;&#40836;&#26631;&#31614;&#21644;3,513&#20010;&#32932;&#33394;&#26631;&#31614;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#20102;&#19982;&#24180;&#40836;&#30456;&#20851;&#30340;&#37325;&#35201;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2308.02935v2 Announce Type: replace-cross  Abstract: Autonomous driving systems have extended the spectrum of Web of Things for intelligent vehicles and have become an important component of the Web ecosystem. Similar to traditional Web-based applications, fairness is an essential aspect for ensuring the high quality of autonomous driving systems, particularly in the context of pedestrian detectors within them. However, there is an absence in the literature of a comprehensive assessment of the fairness of current Deep Learning (DL)-based pedestrian detectors. To fill the gap, we evaluate eight widely-explored DL-based pedestrian detectors across demographic groups on large-scale real-world datasets. To enable a thorough fairness evaluation, we provide extensive annotations for the datasets, resulting in 8,311 images with 16,070 gender labels, 20,115 age labels, and 3,513 skin tone labels. Our findings reveal significant fairness issues related to age. The undetected proportions f
&lt;/p&gt;</description></item><item><title>&#37325;&#26032;&#35780;&#20272;&#39046;&#22495;&#27867;&#21270;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#25552;&#20986;&#37319;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#20351;&#29992;&#22810;&#20010;&#27979;&#35797;&#39046;&#22495;&#65292;&#20197;&#26356;&#20934;&#30830;&#35780;&#20272;OOD&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>https://arxiv.org/abs/2305.15253</link><description>&lt;p&gt;
&#37325;&#26032;&#24605;&#32771;&#39046;&#22495;&#27867;&#21270;&#30340;&#35780;&#20272;&#21327;&#35758;
&lt;/p&gt;
&lt;p&gt;
Rethinking the Evaluation Protocol of Domain Generalization
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.15253
&lt;/p&gt;
&lt;p&gt;
&#37325;&#26032;&#35780;&#20272;&#39046;&#22495;&#27867;&#21270;&#30340;&#35780;&#20272;&#21327;&#35758;&#65292;&#25552;&#20986;&#37319;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#20351;&#29992;&#22810;&#20010;&#27979;&#35797;&#39046;&#22495;&#65292;&#20197;&#26356;&#20934;&#30830;&#35780;&#20272;OOD&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39046;&#22495;&#27867;&#21270;&#30446;&#30340;&#26159;&#36890;&#36807;&#21033;&#29992;&#20174;&#22810;&#20010;&#35757;&#32451;&#39046;&#22495;&#23398;&#21040;&#30340;&#20849;&#21516;&#30693;&#35782;&#65292;&#35299;&#20915;&#38754;&#21521;&#26410;&#35265;&#27979;&#35797;&#39046;&#22495;&#30340;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#25361;&#25112;&#12290;&#20026;&#20102;&#20934;&#30830;&#35780;&#20272;OOD&#27867;&#21270;&#33021;&#21147;&#65292;&#38656;&#35201;&#27979;&#35797;&#25968;&#25454;&#20449;&#24687;&#19981;&#21487;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#39046;&#22495;&#27867;&#21270;&#21327;&#35758;&#20173;&#21487;&#33021;&#23384;&#22312;&#28508;&#22312;&#30340;&#27979;&#35797;&#25968;&#25454;&#20449;&#24687;&#27844;&#28431;&#12290;&#26412;&#25991;&#20174;&#24403;&#21069;&#35780;&#20272;&#21327;&#35758;&#30340;&#20004;&#20010;&#26041;&#38754;&#65306;&#22312;ImageNet&#19978;&#36827;&#34892;&#30417;&#30563;&#39044;&#35757;&#32451;&#21644;oracle&#27169;&#22411;&#36873;&#25321;&#65292;&#25506;&#35752;&#27979;&#35797;&#25968;&#25454;&#20449;&#24687;&#27844;&#28431;&#30340;&#39118;&#38505;&#12290;&#25105;&#20204;&#25552;&#20986;&#20462;&#25913;&#24403;&#21069;&#21327;&#35758;&#30340;&#24314;&#35758;&#65292;&#21363;&#24212;&#35813;&#37319;&#29992;&#33258;&#30417;&#30563;&#39044;&#35757;&#32451;&#25110;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#65292;&#32780;&#19981;&#26159;&#37319;&#29992;&#24403;&#21069;&#30340;&#30417;&#30563;&#39044;&#35757;&#32451;&#65292;&#24182;&#19988;&#24212;&#35813;&#20351;&#29992;&#22810;&#20010;&#27979;&#35797;&#39046;&#22495;&#12290;&#36825;&#23558;&#23548;&#33268;&#23545;OOD&#27867;&#21270;&#33021;&#21147;&#26356;&#31934;&#30830;&#30340;&#35780;&#20272;&#12290;&#25105;&#20204;&#36824;&#37325;&#26032;&#36816;&#34892;&#20102;&#24102;&#26377;&#20462;&#25913;&#21518;&#21327;&#35758;&#30340;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.15253v2 Announce Type: replace-cross  Abstract: Domain generalization aims to solve the challenge of Out-of-Distribution (OOD) generalization by leveraging common knowledge learned from multiple training domains to generalize to unseen test domains. To accurately evaluate the OOD generalization ability, it is required that test data information is unavailable. However, the current domain generalization protocol may still have potential test data information leakage. This paper examines the risks of test data information leakage from two aspects of the current evaluation protocol: supervised pretraining on ImageNet and oracle model selection. We propose modifications to the current protocol that we should employ self-supervised pretraining or train from scratch instead of employing the current supervised pretraining, and we should use multiple test domains. These would result in a more precise evaluation of OOD generalization ability. We also rerun the algorithms with the mod
&lt;/p&gt;</description></item><item><title>LLM-Pat&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#24182;&#27604;&#36739;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;&#23545;&#24212;&#30340;&#8220;&#20804;&#24351;&#8221;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#21028;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#12290;</title><link>https://arxiv.org/abs/2305.12519</link><description>&lt;p&gt;
LLM&#20146;&#23376;&#37492;&#23450;&#65306;LLM&#36951;&#20256;&#32487;&#25215;&#20013;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
LLM Paternity Test: Generated Text Detection with LLM Genetic Inheritance
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2305.12519
&lt;/p&gt;
&lt;p&gt;
LLM-Pat&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#36890;&#36807;&#37325;&#24314;&#24182;&#27604;&#36739;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;&#23545;&#24212;&#30340;&#8220;&#20804;&#24351;&#8221;&#25991;&#26412;&#30340;&#30456;&#20284;&#24615;&#65292;&#20174;&#32780;&#21028;&#26029;&#20505;&#36873;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#21487;&#20197;&#29983;&#25104;&#25658;&#24102;&#21508;&#31181;&#28389;&#29992;&#39118;&#38505;&#30340;&#25991;&#26412;&#65292;&#21253;&#25324;&#25220;&#34989;&#12289;&#22312;&#30005;&#23376;&#21830;&#21153;&#24179;&#21488;&#19978;&#21457;&#24067;&#34394;&#20551;&#35780;&#35770;&#65292;&#25110;&#32773;&#21046;&#20316;&#24341;&#20154;&#27880;&#30446;&#30340;&#34394;&#20551;&#25512;&#25991;&#12290;&#22240;&#27492;&#65292;&#26816;&#27979;&#25991;&#26412;&#26159;&#21542;&#30001;&#26426;&#22120;&#29983;&#25104;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#26816;&#27979;&#26041;&#27861;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#30001;&#20110;&#20005;&#37325;&#20381;&#36182;&#35757;&#32451;&#25968;&#25454;&#65292;&#23427;&#20204;&#24448;&#24448;&#32570;&#20047;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#32531;&#35299;&#36825;&#19968;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#19982;&#27169;&#22411;&#30456;&#20851;&#30340;&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#26041;&#27861;&#65292;&#21363;LLM&#20146;&#23376;&#37492;&#23450;&#65288;LLM-Pat&#65289;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#32473;&#23450;&#20219;&#20309;&#20505;&#36873;&#25991;&#26412;&#65288;"&#23376;&#31867;"&#65289;&#65292;LLM-Pat&#20351;&#29992;&#19968;&#20010;&#20013;&#38388;LLM&#65288;"&#29238;&#31867;"&#65289;&#37325;&#24314;&#19982;&#32473;&#23450;&#25991;&#26412;&#23545;&#24212;&#30340;"&#20804;&#24351;"&#25991;&#26412;&#65292;&#28982;&#21518;&#34913;&#37327;&#20505;&#36873;&#25991;&#26412;&#19982;&#20854;"&#20804;&#24351;"&#25991;&#26412;&#20043;&#38388;&#30340;&#30456;&#20284;&#24615;&#12290;&#39640;&#30456;&#20284;&#24615;&#34920;&#26126;&#20505;&#36873;&#25991;&#26412;&#26159;&#30001;&#26426;&#22120;&#29983;&#25104;&#65292;&#31867;&#20284;&#20110;&#22522;&#22240;&#29305;&#24449;&#12290;&#25105;&#20204;&#24050;&#26500;&#24314;&#20102;&#25968;&#25454;&#38598;...
&lt;/p&gt;
&lt;p&gt;
arXiv:2305.12519v2 Announce Type: replace-cross  Abstract: Large language models (LLMs) can generate texts that carry the risk of various misuses, including plagiarism, planting fake reviews on e-commerce platforms, or creating inflammatory false tweets. Detecting whether a text is machine-generated has thus become increasingly important. While existing detection methods exhibit superior performance, they often lack generalizability due to their heavy dependence on training data. To alleviate this problem, we propose a model-related generated text detection method, the LLM Paternity Test (LLM-Pat). Specifically, given any candidate text (\textit{child}), LLM-Pat employs an intermediary LLM (\textit{parent}) to reconstruct a \textit{sibling} text corresponding to the given text and then measures the similarity between candidate texts and their sibling texts. High similarity indicates that the candidate text is machine-generated, akin to genetic traits. We have constructed datasets encom
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21069;&#21521;&#21644;&#21518;&#21521;&#20860;&#23481;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#33647;&#20024;&#35782;&#21035;&#26694;&#26550;&#65292;&#21253;&#25324;&#34394;&#25311;&#31867;&#21035;&#21512;&#25104;&#31574;&#30053;&#21644;&#20013;&#24515;&#19977;&#20803;&#32452;&#25439;&#22833;&#20197;&#22686;&#24378;&#36776;&#21035;&#29305;&#24449;&#23398;&#20064;&#12290;</title><link>https://arxiv.org/abs/2304.11959</link><description>&lt;p&gt;
&#19968;&#31181;&#21069;&#21521;&#21644;&#21518;&#21521;&#20860;&#23481;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#33647;&#20024;&#35782;&#21035;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
A Forward and Backward Compatible Framework for Few-shot Class-incremental Pill Recognition
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2304.11959
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21069;&#21521;&#21644;&#21518;&#21521;&#20860;&#23481;&#30340;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#33647;&#20024;&#35782;&#21035;&#26694;&#26550;&#65292;&#21253;&#25324;&#34394;&#25311;&#31867;&#21035;&#21512;&#25104;&#31574;&#30053;&#21644;&#20013;&#24515;&#19977;&#20803;&#32452;&#25439;&#22833;&#20197;&#22686;&#24378;&#36776;&#21035;&#29305;&#24449;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#21160;&#33647;&#20024;&#35782;&#21035;&#65288;APR&#65289;&#31995;&#32479;&#23545;&#20110;&#25552;&#39640;&#21307;&#38498;&#25928;&#29575;&#12289;&#24110;&#21161;&#35270;&#21147;&#21463;&#25439;&#20010;&#20307;&#65292;&#24182;&#39044;&#38450;&#20132;&#21449;&#24863;&#26579;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#33647;&#20024;&#35782;&#21035;&#31995;&#32479;&#21482;&#33021;&#23545;&#20855;&#26377;&#36275;&#22815;&#35757;&#32451;&#25968;&#25454;&#30340;&#31867;&#21035;&#36827;&#34892;&#20998;&#31867;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#25968;&#25454;&#26631;&#27880;&#30340;&#39640;&#25104;&#26412;&#21644;&#26032;&#33647;&#20024;&#31867;&#21035;&#25345;&#32493;&#22686;&#21152;&#30340;&#24773;&#20917;&#38656;&#35201;&#24320;&#21457;&#19968;&#31181;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#33647;&#20024;&#35782;&#21035;&#31995;&#32479;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#33647;&#20024;&#35782;&#21035;&#26694;&#26550;&#65292;&#21517;&#20026;&#20855;&#26377;&#21069;&#21521;&#21644;&#21518;&#21521;&#20860;&#23481;&#30340;&#36776;&#21035;&#24335;&#23569;&#26679;&#26412;&#31867;&#22686;&#37327;&#23398;&#20064;&#65288;DBC-FSCIL&#65289;&#12290;&#23427;&#21253;&#25324;&#21069;&#21521;&#20860;&#23481;&#21644;&#21518;&#21521;&#20860;&#23481;&#30340;&#23398;&#20064;&#32452;&#20214;&#12290;&#22312;&#21069;&#21521;&#20860;&#23481;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21019;&#26032;&#30340;&#34394;&#25311;&#31867;&#21035;&#21512;&#25104;&#31574;&#30053;&#21644;&#19968;&#20010;&#20013;&#24515;&#19977;&#20803;&#32452;&#65288;CT&#65289;&#25439;&#22833;&#26469;&#22686;&#24378;&#36776;&#21035;&#29305;&#24449;&#23398;&#20064;&#12290;&#36825;&#20123;&#34394;&#25311;&#31867;&#21035;&#36215;&#21040;&#20102;
&lt;/p&gt;
&lt;p&gt;
arXiv:2304.11959v2 Announce Type: replace-cross  Abstract: Automatic Pill Recognition (APR) systems are crucial for enhancing hospital efficiency, assisting visually impaired individuals, and preventing cross-infection. However, most existing deep learning-based pill recognition systems can only perform classification on classes with sufficient training data. In practice, the high cost of data annotation and the continuous increase in new pill classes necessitate the development of a few-shot class-incremental pill recognition system. This paper introduces the first few-shot class-incremental pill recognition framework, named Discriminative and Bidirectional Compatible Few-Shot Class-Incremental Learning (DBC-FSCIL). It encompasses forward-compatible and backward-compatible learning components. In forward-compatible learning, we propose an innovative virtual class synthesis strategy and a Center-Triplet (CT) loss to enhance discriminative feature learning. These virtual classes serve a
&lt;/p&gt;</description></item><item><title>&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#35821;&#35328;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21151;&#33021;&#35821;&#35328;&#33021;&#21147;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#31283;&#23450;&#65292;&#21487;&#33021;&#38656;&#35201;&#19987;&#38376;&#30340;&#35843;&#25972;&#21644;&#22806;&#37096;&#27169;&#22359;&#30340;&#25903;&#25345;&#12290;</title><link>https://arxiv.org/abs/2301.06627</link><description>&lt;p&gt;
&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#20013;&#21306;&#20998;&#35821;&#35328;&#21644;&#24605;&#32500;
&lt;/p&gt;
&lt;p&gt;
Dissociating language and thought in large language models
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2301.06627
&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#24418;&#24335;&#35821;&#35328;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;&#21151;&#33021;&#35821;&#35328;&#33021;&#21147;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#31283;&#23450;&#65292;&#21487;&#33021;&#38656;&#35201;&#19987;&#38376;&#30340;&#35843;&#25972;&#21644;&#22806;&#37096;&#27169;&#22359;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#36804;&#20170;&#20026;&#27490;&#22312;&#25484;&#25569;&#20154;&#31867;&#35821;&#35328;&#26041;&#38754;&#20570;&#24471;&#26368;&#22909;&#65292;&#28982;&#32780;&#20154;&#20204;&#23545;&#23427;&#20204;&#30340;&#35821;&#35328;&#21644;&#35748;&#30693;&#33021;&#21147;&#20173;&#23384;&#22312;&#20998;&#27495;&#12290;&#26412;&#25991;&#20351;&#29992;&#24418;&#24335;&#35821;&#35328;&#33021;&#21147;&#65288;&#23545;&#35821;&#35328;&#35268;&#21017;&#21644;&#27169;&#24335;&#30340;&#20102;&#35299;&#65289;&#19982;&#21151;&#33021;&#35821;&#35328;&#33021;&#21147;&#65288;&#29702;&#35299;&#21644;&#20351;&#29992;&#35821;&#35328;&#22312;&#19990;&#30028;&#20013;&#30340;&#26041;&#24335;&#65289;&#30340;&#21306;&#21035;&#26469;&#35780;&#20272;LLMs&#12290;&#25105;&#20204;&#36890;&#36807;&#20154;&#31867;&#31070;&#32463;&#31185;&#23398;&#26469;&#30830;&#31435;&#36825;&#19968;&#21306;&#21035;&#65292;&#20154;&#31867;&#31070;&#32463;&#31185;&#23398;&#26174;&#31034;&#24418;&#24335;&#21644;&#21151;&#33021;&#33021;&#21147;&#20381;&#36182;&#20110;&#19981;&#21516;&#30340;&#31070;&#32463;&#26426;&#21046;&#12290;&#23613;&#31649;LLMs&#22312;&#24418;&#24335;&#33021;&#21147;&#26041;&#38754;&#34920;&#29616;&#20986;&#20154;&#20204;&#30340;&#24778;&#20154;&#27700;&#24179;&#65292;&#20294;&#23427;&#20204;&#22312;&#21151;&#33021;&#33021;&#21147;&#20219;&#21153;&#19978;&#30340;&#34920;&#29616;&#20173;&#28982;&#19981;&#31283;&#23450;&#65292;&#24182;&#19988;&#36890;&#24120;&#38656;&#35201;&#19987;&#38376;&#30340;&#31934;&#32454;&#35843;&#25972;&#21644;/&#25110;&#19982;&#22806;&#37096;&#27169;&#22359;&#30340;&#32806;&#21512;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#37027;&#20123;&#20197;&#31867;&#20284;&#20154;&#31867;&#26041;&#24335;&#20351;&#29992;&#35821;&#35328;&#30340;&#27169;&#22411;&#23558;&#38656;&#35201;&#25484;&#25569;&#36825;&#20004;&#31181;&#33021;&#21147;&#31867;&#22411;&#65292;&#32780;&#36825;&#21453;&#36807;&#26469;&#21487;&#33021;&#38656;&#35201;&#20026;&#24418;&#24335;&#35821;&#35328;&#33021;&#21147;&#19987;&#38376;&#21270;&#30340;&#26426;&#21046;&#30340;&#20986;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2301.06627v3 Announce Type: replace-cross  Abstract: Large Language Models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their linguistic and cognitive capabilities remain split. Here, we evaluate LLMs using a distinction between formal linguistic competence - knowledge of linguistic rules and patterns - and functional linguistic competence - understanding and using language in the world. We ground this distinction in human neuroscience, which has shown that formal and functional competence rely on different neural mechanisms. Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules. We posit that models that use language in human-like ways would need to master both of these competence types, which, in turn, could require the emergence of mechanisms specialized for formal linguistic co
&lt;/p&gt;</description></item><item><title>&#30693;&#35782;&#22270;&#21644;&#20854;&#20182;&#30693;&#35782;&#26469;&#28304;&#22635;&#34917;&#20102;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#22312;&#26085;&#24120;&#30693;&#35782;&#29702;&#35299;&#26041;&#38754;&#30340;&#24046;&#36317;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>https://arxiv.org/abs/2211.12328</link><description>&lt;p&gt;
&#30693;&#35782;&#22686;&#24378;&#22810;&#27169;&#24577;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A survey on knowledge-enhanced multimodal learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2211.12328
&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#21644;&#20854;&#20182;&#30693;&#35782;&#26469;&#28304;&#22635;&#34917;&#20102;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#27169;&#22411;&#22312;&#26085;&#24120;&#30693;&#35782;&#29702;&#35299;&#26041;&#38754;&#30340;&#24046;&#36317;&#65292;&#25552;&#21319;&#20102;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#27169;&#24577;&#23398;&#20064;&#26159;&#19968;&#20010;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#30340;&#39046;&#22495;&#65292;&#26088;&#22312;&#23558;&#21508;&#31181;&#27169;&#24577;&#32467;&#21512;&#25104;&#19968;&#20010;&#32852;&#21512;&#34920;&#31034;&#12290;&#29305;&#21035;&#26159;&#22312;&#35270;&#35273;&#35821;&#35328;&#23398;&#20064;&#39046;&#22495;&#65292;&#24050;&#32463;&#24320;&#21457;&#20986;&#22810;&#31181;&#27169;&#22411;&#21644;&#25216;&#26415;&#65292;&#38024;&#23545;&#28041;&#21450;&#22270;&#20687;&#21644;&#25991;&#26412;&#30340;&#21508;&#31181;&#20219;&#21153;&#12290;VL&#27169;&#22411;&#36890;&#36807;&#25193;&#23637;Transformer&#30340;&#24605;&#24819;&#65292;&#20351;&#24471;&#20004;&#31181;&#27169;&#24577;&#21487;&#20197;&#30456;&#20114;&#23398;&#20064;&#65292;&#24050;&#32463;&#36798;&#21040;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#24615;&#33021;&#12290;&#22823;&#35268;&#27169;&#30340;&#39044;&#35757;&#32451;&#31243;&#24207;&#20351;&#24471;VL&#27169;&#22411;&#33021;&#22815;&#33719;&#24471;&#19968;&#23450;&#27700;&#24179;&#30340;&#29616;&#23454;&#19990;&#30028;&#29702;&#35299;&#65292;&#23613;&#31649;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#24046;&#36317;&#65306;&#23545;&#24120;&#35782;&#12289;&#20107;&#23454;&#12289;&#26102;&#38388;&#21644;&#20854;&#20182;&#26085;&#24120;&#30693;&#35782;&#26041;&#38754;&#30340;&#38480;&#21046;&#29702;&#35299;&#65292;&#23545;VL&#20219;&#21153;&#30340;&#21487;&#25193;&#23637;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#30693;&#35782;&#22270;&#21644;&#20854;&#20182;&#30693;&#35782;&#26469;&#28304;&#21487;&#20197;&#36890;&#36807;&#26126;&#30830;&#25552;&#20379;&#32570;&#22833;&#20449;&#24687;&#26469;&#22635;&#34917;&#36825;&#20123;&#24046;&#36317;&#65292;&#35299;&#38145;VL&#27169;&#22411;&#30340;&#26032;&#33021;&#21147;&#12290;&#21516;&#26102;&#65292;&#30693;&#35782;&#22270;&#22686;&#24378;&#20102;&#21487;&#35299;&#37322;&#24615;&#12289;&#20844;&#24179;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2211.12328v3 Announce Type: replace-cross  Abstract: Multimodal learning has been a field of increasing interest, aiming to combine various modalities in a single joint representation. Especially in the area of visiolinguistic (VL) learning multiple models and techniques have been developed, targeting a variety of tasks that involve images and text. VL models have reached unprecedented performances by extending the idea of Transformers, so that both modalities can learn from each other. Massive pre-training procedures enable VL models to acquire a certain level of real-world understanding, although many gaps can be identified: the limited comprehension of commonsense, factual, temporal and other everyday knowledge aspects questions the extendability of VL tasks. Knowledge graphs and other knowledge sources can fill those gaps by explicitly providing missing information, unlocking novel capabilities of VL models. In the same time, knowledge graphs enhance explainability, fairness 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#32437;&#21521;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#36827;&#34892;&#35786;&#26029;&#30340;&#30410;&#22788;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#27169;&#25311;&#30142;&#30149;&#36827;&#23637;&#24182;&#22312;&#32437;&#21521;&#30524;&#24213;&#29031;&#29255;&#20013;&#26816;&#27979;&#26089;&#26399;DR&#20005;&#37325;&#31243;&#24230;&#21464;&#21270;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;AUC&#12290;</title><link>https://arxiv.org/abs/2209.00915</link><description>&lt;p&gt;
&#20351;&#29992;&#32437;&#21521;&#33258;&#30417;&#30563;&#23398;&#20064;&#36827;&#34892;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Detection of diabetic retinopathy using longitudinal self-supervised learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2209.00915
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#21033;&#29992;&#32437;&#21521;&#33258;&#30417;&#30563;&#23398;&#20064;&#23545;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#36827;&#34892;&#35786;&#26029;&#30340;&#30410;&#22788;&#65292;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#26041;&#27861;&#27169;&#25311;&#30142;&#30149;&#36827;&#23637;&#24182;&#22312;&#32437;&#21521;&#30524;&#24213;&#29031;&#29255;&#20013;&#26816;&#27979;&#26089;&#26399;DR&#20005;&#37325;&#31243;&#24230;&#21464;&#21270;&#65292;&#21462;&#24471;&#20102;&#36739;&#39640;&#30340;AUC&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32437;&#21521;&#25104;&#20687;&#33021;&#22815;&#25429;&#25417;&#38745;&#24577;&#35299;&#21078;&#32467;&#26500;&#21644;&#30142;&#30149;&#36827;&#23637;&#20013;&#30340;&#21160;&#24577;&#21464;&#21270;&#65292;&#23454;&#29616;&#23545;&#30142;&#30149;&#30340;&#26356;&#26089;&#21644;&#26356;&#22909;&#30340;&#20010;&#20307;&#21270;&#30149;&#29702;&#31649;&#29702;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;&#26816;&#27979;&#31958;&#23615;&#30149;&#35270;&#32593;&#33180;&#30149;&#21464;&#65288;DR&#65289;&#30340;&#26041;&#27861;&#24456;&#23569;&#21033;&#29992;&#32437;&#21521;&#20449;&#24687;&#26469;&#25913;&#21892;DR&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21033;&#29992;&#20855;&#26377;&#32437;&#21521;&#24615;&#36136;&#30340;&#33258;&#30417;&#30563;&#23398;&#20064;&#26469;&#36827;&#34892;DR&#35786;&#26029;&#30340;&#22909;&#22788;&#12290;&#25105;&#20204;&#27604;&#36739;&#20102;&#19981;&#21516;&#30340;&#32437;&#21521;&#33258;&#30417;&#30563;&#23398;&#20064;&#65288;LSSL&#65289;&#26041;&#27861;&#65292;&#29992;&#26469;&#27169;&#25311;&#20174;&#32437;&#21521;&#35270;&#32593;&#33180;&#24425;&#33394;&#30524;&#24213;&#29031;&#29255;&#65288;CFP&#65289;&#20013;&#26816;&#27979;&#26089;&#26399;DR&#20005;&#37325;&#31243;&#24230;&#21464;&#21270;&#65292;&#20351;&#29992;&#19968;&#23545;&#36830;&#32493;&#26816;&#26597;&#12290;&#23454;&#39564;&#26159;&#22312;&#19968;&#20010;&#32437;&#21521;DR&#31579;&#26597;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#65292;&#26377;&#25110;&#27809;&#26377;&#37027;&#20123;&#35757;&#32451;&#22909;&#30340;&#32534;&#30721;&#22120;&#65288;LSSL&#65289;&#20316;&#20026;&#32437;&#21521;&#20551;&#35774;&#20219;&#21153;&#12290;&#32467;&#26524;&#22312;&#22522;&#32447;&#65288;&#20174;&#22836;&#24320;&#22987;&#35757;&#32451;&#30340;&#27169;&#22411;&#65289;&#19978;&#23454;&#29616;&#20102;0.875&#30340;AUC&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2209.00915v3 Announce Type: replace-cross  Abstract: Longitudinal imaging is able to capture both static anatomical structures and dynamic changes in disease progression towards earlier and better patient-specific pathology management. However, conventional approaches for detecting diabetic retinopathy (DR) rarely take advantage of longitudinal information to improve DR analysis. In this work, we investigate the benefit of exploiting self-supervised learning with a longitudinal nature for DR diagnosis purposes. We compare different longitudinal self-supervised learning (LSSL) methods to model the disease progression from longitudinal retinal color fundus photographs (CFP) to detect early DR severity changes using a pair of consecutive exams. The experiments were conducted on a longitudinal DR screening dataset with or without those trained encoders (LSSL) acting as a longitudinal pretext task. Results achieve an AUC of 0.875 for the baseline (model trained from scratch) and an AU
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#22312;&#27425;&#20248;CBS&#20013;&#65292;&#21152;&#26435;&#25104;&#26412;&#21551;&#21457;&#24335;&#21644;&#20914;&#31361;&#21551;&#21457;&#24335;&#21487;&#20197;&#26377;&#25928;&#32467;&#21512;&#65292;&#20854;&#20013;&#19968;&#31181;&#21464;&#20307;&#22312;&#22810;&#31181;&#24773;&#26223;&#21644;&#26041;&#27861;&#20013;&#21487;&#23454;&#29616;&#22823;&#24133;&#36895;&#24230;&#25552;&#21319;&#12290;</title><link>https://arxiv.org/abs/2205.11624</link><description>&lt;p&gt;
&#22312;&#27425;&#20248;CBS&#20013;&#26377;&#25928;&#38598;&#25104;&#21152;&#26435;&#25104;&#26412;&#21644;&#20914;&#31361;&#21551;&#21457;
&lt;/p&gt;
&lt;p&gt;
Effective Integration of Weighted Cost-to-go and Conflict Heuristic within Suboptimal CBS
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#22312;&#27425;&#20248;CBS&#20013;&#65292;&#21152;&#26435;&#25104;&#26412;&#21551;&#21457;&#24335;&#21644;&#20914;&#31361;&#21551;&#21457;&#24335;&#21487;&#20197;&#26377;&#25928;&#32467;&#21512;&#65292;&#20854;&#20013;&#19968;&#31181;&#21464;&#20307;&#22312;&#22810;&#31181;&#24773;&#26223;&#21644;&#26041;&#27861;&#20013;&#21487;&#23454;&#29616;&#22823;&#24133;&#36895;&#24230;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20914;&#31361;&#25628;&#32034;&#65288;CBS&#65289;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#22810;&#26234;&#33021;&#20307;&#36335;&#24452;&#35268;&#21010;&#65288;MAPF&#65289;&#27714;&#35299;&#22120;&#65292;&#23427;&#37319;&#29992;&#19968;&#20010;&#20302;&#32423;&#21333;&#26234;&#33021;&#20307;&#35268;&#21010;&#22120;&#21644;&#19968;&#20010;&#39640;&#32423;&#32422;&#26463;&#26641;&#26469;&#35299;&#20915;&#20914;&#31361;&#12290;&#29616;&#20195;&#22823;&#22810;&#25968;MAPF&#27714;&#35299;&#22120;&#20027;&#35201;&#38598;&#20013;&#20110;&#36890;&#36807;&#21508;&#31181;&#31574;&#30053;&#20943;&#23569;&#35813;&#26641;&#30340;&#22823;&#23567;&#26469;&#25913;&#36827;CBS&#65292;&#24456;&#23569;&#26377;&#26041;&#27861;&#20462;&#25913;&#20302;&#32423;&#35268;&#21010;&#22120;&#12290;&#36890;&#24120;&#29616;&#26377;CBS&#26041;&#27861;&#20013;&#30340;&#20302;&#32423;&#35268;&#21010;&#22120;&#20351;&#29992;&#26080;&#26435;&#37325;&#30340;&#25104;&#26412;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#32780;&#27425;&#20248;CBS&#26041;&#27861;&#36824;&#20351;&#29992;&#20914;&#31361;&#21551;&#21457;&#24335;&#26041;&#27861;&#26469;&#24110;&#21161;&#39640;&#32423;&#25628;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#65292;&#19982;&#27969;&#34892;&#30340;CBS&#20449;&#24565;&#30456;&#21453;&#65292;&#21152;&#26435;&#25104;&#26412;&#21551;&#21457;&#24335;&#21487;&#20197;&#26377;&#25928;&#22320;&#19982;&#20914;&#31361;&#21551;&#21457;&#24335;&#19968;&#36215;&#22312;&#20004;&#31181;&#21487;&#33021;&#30340;&#21464;&#20307;&#20013;&#20351;&#29992;&#12290;&#29305;&#21035;&#22320;&#65292;&#20854;&#20013;&#19968;&#31181;&#21464;&#20307;&#22312;&#20960;&#31181;&#24773;&#26223;&#21644;&#27425;&#20248;CBS&#26041;&#27861;&#20013;&#21487;&#20197;&#33719;&#24471;&#22823;&#24133;&#21152;&#36895;&#65292;2-100&#20493;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#21457;&#29616;&#24615;&#33021;&#19982;&#21152;&#26435;&#25104;&#26412;&#21551;&#21457;&#24335;&#30456;&#20851;&#65292;&#32780;&#19981;&#26159;&#19982;&#20914;&#31361;&#36215;&#21457;&#24335;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.11624v5 Announce Type: replace  Abstract: Conflict-Based Search (CBS) is a popular multi-agent path finding (MAPF) solver that employs a low-level single agent planner and a high-level constraint tree to resolve conflicts. The vast majority of modern MAPF solvers focus on improving CBS by reducing the size of this tree through various strategies with few methods modifying the low level planner. Typically low level planners in existing CBS methods use an unweighted cost-to-go heuristic, with suboptimal CBS methods also using a conflict heuristic to help the high level search. In this paper, we show that, contrary to prevailing CBS beliefs, a weighted cost-to-go heuristic can be used effectively alongside the conflict heuristic in two possible variants. In particular, one of these variants can obtain large speedups, 2-100x, across several scenarios and suboptimal CBS methods. Importantly, we discover that performance is related not to the weighted cost-to-go heuristic but rath
&lt;/p&gt;</description></item><item><title>&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;cGAN&#38598;&#25104;&#30340;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#38752;&#22788;&#29702;&#24037;&#19994;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#12290;</title><link>https://arxiv.org/abs/2205.07250</link><description>&lt;p&gt;
&#19968;&#20010;&#22522;&#20110;cGAN&#38598;&#25104;&#30340;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#24037;&#19994;&#25511;&#21046;&#38382;&#39064;&#30340;&#26367;&#20195;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
A cGAN Ensemble-based Uncertainty-aware Surrogate Model for Offline Model-based Optimization in Industrial Control Problems
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2205.07250
&lt;/p&gt;
&lt;p&gt;
&#24341;&#20837;&#20102;&#19968;&#20010;&#22522;&#20110;cGAN&#38598;&#25104;&#30340;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#38752;&#22788;&#29702;&#24037;&#19994;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20851;&#27880;&#23558;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#24212;&#29992;&#20110;&#30495;&#23454;&#19990;&#30028;&#24037;&#19994;&#25511;&#21046;&#38382;&#39064;&#26102;&#36935;&#21040;&#30340;&#20004;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#31532;&#19968;&#20010;&#38382;&#39064;&#26159;&#22914;&#20309;&#21019;&#24314;&#19968;&#20010;&#21487;&#38752;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#20934;&#30830;&#25429;&#25417;&#22024;&#26434;&#24037;&#19994;&#25968;&#25454;&#20013;&#23384;&#22312;&#30340;&#21160;&#24577;&#29305;&#24615;&#12290;&#31532;&#20108;&#20010;&#38382;&#39064;&#26159;&#22914;&#20309;&#22312;&#19981;&#20027;&#21160;&#25910;&#38598;&#24037;&#19994;&#31995;&#32479;&#21453;&#39304;&#30340;&#24773;&#20917;&#19979;&#21487;&#38752;&#22320;&#20248;&#21270;&#25511;&#21046;&#21442;&#25968;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;cGAN&#38598;&#25104;&#30340;&#20851;&#27880;&#19981;&#30830;&#23450;&#24615;&#30340;&#26367;&#20195;&#27169;&#22411;&#65292;&#29992;&#20110;&#21487;&#38752;&#22320;&#22788;&#29702;&#24037;&#19994;&#25511;&#21046;&#38382;&#39064;&#20013;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#12290;&#36890;&#36807;&#22312;&#20004;&#20010;&#20195;&#34920;&#24615;&#26696;&#20363;&#19978;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#26469;&#35777;&#26126;&#25152;&#25552;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#65292;&#21363;&#31163;&#25955;&#25511;&#21046;&#26696;&#20363;&#21644;&#36830;&#32493;&#25511;&#21046;&#26696;&#20363;&#12290;&#36825;&#20123;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24037;&#19994;&#25511;&#21046;&#30340;&#31163;&#32447;&#27169;&#22411;&#20248;&#21270;&#39046;&#22495;&#20013;&#32988;&#36807;&#20102;&#20960;&#20010;&#31454;&#20105;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
arXiv:2205.07250v2 Announce Type: replace-cross  Abstract: This study focuses on two important problems related to applying offline model-based optimization to real-world industrial control problems. The first problem is how to create a reliable probabilistic model that accurately captures the dynamics present in noisy industrial data. The second problem is how to reliably optimize control parameters without actively collecting feedback from industrial systems. Specifically, we introduce a novel cGAN ensemble-based uncertainty-aware surrogate model for reliable offline model-based optimization in industrial control problems. The effectiveness of the proposed method is demonstrated through extensive experiments conducted on two representative cases, namely a discrete control case and a continuous control case. The results of these experiments show that our method outperforms several competitive baselines in the field of offline model-based optimization for industrial control.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#19981;&#21516;&#32570;&#38519;&#39044;&#27979;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#36328;&#29256;&#26412;&#21644;&#36328;&#39033;&#30446;&#30340;&#32570;&#38519;&#39044;&#27979;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;</title><link>https://arxiv.org/abs/2202.12074</link><description>&lt;p&gt;
&#23545;&#20110;&#19968;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#19981;&#21516;&#32570;&#38519;&#39044;&#27979;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On The Effectiveness of One-Class Support Vector Machine in Different Defect Prediction Scenarios
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2202.12074
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#22312;&#19981;&#21516;&#32570;&#38519;&#39044;&#27979;&#22330;&#26223;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#36328;&#29256;&#26412;&#21644;&#36328;&#39033;&#30446;&#30340;&#32570;&#38519;&#39044;&#27979;&#27169;&#22411;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32570;&#38519;&#39044;&#27979;&#26088;&#22312;&#35782;&#21035;&#22312;&#36719;&#20214;&#25552;&#20379;&#32473;&#26368;&#32456;&#29992;&#25143;&#20043;&#21069;&#21487;&#33021;&#24341;&#36215;&#25925;&#38556;&#30340;&#36719;&#20214;&#32452;&#20214;&#12290;&#36804;&#20170;&#20026;&#27490;&#65292;&#36825;&#19968;&#20219;&#21153;&#34987;&#24314;&#27169;&#20026;&#19968;&#20010;&#21452;&#31867;&#21035;&#20998;&#31867;&#38382;&#39064;&#65292;&#28982;&#32780;&#20854;&#26412;&#36136;&#20063;&#20801;&#35768;&#23558;&#20854;&#26500;&#24314;&#20026;&#19968;&#20010;&#19968;&#31867;&#20998;&#31867;&#20219;&#21153;&#12290;&#20808;&#21069;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#19968;&#31867;&#25903;&#25345;&#21521;&#37327;&#26426;&#65288;OCSVM&#65289;&#22312;&#39033;&#30446;&#20869;&#32570;&#38519;&#39044;&#27979;&#26041;&#38754;&#21487;&#20197;&#32988;&#36807;&#21452;&#31867;&#21035;&#20998;&#31867;&#22120;&#65292;&#28982;&#32780;&#24403;&#24212;&#29992;&#20110;&#26356;&#32454;&#31890;&#24230;&#30340;&#24773;&#20917;&#65288;&#21363;&#22522;&#20110;&#25552;&#20132;&#32423;&#21035;&#30340;&#32570;&#38519;&#39044;&#27979;&#65289;&#26102;&#24182;&#19981;&#26377;&#25928;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#30740;&#31350;&#20165;&#20174;&#19968;&#31867;&#36827;&#34892;&#23398;&#20064;&#26159;&#21542;&#36275;&#20197;&#29983;&#25104;&#22312;&#20004;&#31181;&#20854;&#20182;&#19981;&#21516;&#22330;&#26223;&#65288;&#21363;&#31890;&#24230;&#65289;&#20013;&#20135;&#29983;&#26377;&#25928;&#32570;&#38519;&#39044;&#27979;&#27169;&#22411;&#65292;&#21363;&#36328;&#29256;&#26412;&#21644;&#36328;&#39033;&#30446;&#32570;&#38519;&#39044;&#27979;&#27169;&#22411;&#65292;&#24182;&#20026;&#20102;&#23436;&#25972;&#24615;&#22797;&#21046;&#22312;&#39033;&#30446;&#20869;&#31890;&#24230;&#30340;&#20808;&#21069;&#24037;&#20316;&#12290;&#25105;&#20204;&#30340;&#23454;&#35777;&#32467;&#26524;&#35777;&#23454;&#20102;OCSVM&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
arXiv:2202.12074v2 Announce Type: replace-cross  Abstract: Defect prediction aims at identifying software components that are likely to cause faults before a software is made available to the end-user. To date, this task has been modeled as a two-class classification problem, however its nature also allows it to be formulated as a one-class classification task. Previous studies show that One-Class Support Vector Machine (OCSVM) can outperform two-class classifiers for within-project defect prediction, however it is not effective when employed at a finer granularity (i.e., commit-level defect prediction). In this paper, we further investigate whether learning from one class only is sufficient to produce effective defect prediction model in two other different scenarios (i.e., granularity), namely cross-version and cross-project defect prediction models, as well as replicate the previous work at within-project granularity for completeness. Our empirical results confirm that OCSVM perform
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#20559;&#24046;&#36827;&#34892;&#20102;&#28145;&#20837;&#29702;&#35770;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#32479;&#19968;&#26694;&#26550;&#25551;&#36848;GMRL&#31639;&#27861;&#30340;&#21464;&#21270;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#30340;&#38543;&#26426;&#20803;&#26799;&#24230;&#20272;&#35745;&#22120;&#23454;&#38469;&#19978;&#26159;&#26377;&#20559;&#30340;&#12290;</title><link>https://arxiv.org/abs/2112.15400</link><description>&lt;p&gt;
Meta-Reinforcement Learning&#20013;&#26799;&#24230;&#20559;&#24046;&#30340;&#29702;&#35770;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
A Theoretical Understanding of Gradient Bias in Meta-Reinforcement Learning
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2112.15400
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#26799;&#24230;&#20559;&#24046;&#36827;&#34892;&#20102;&#28145;&#20837;&#29702;&#35770;&#29702;&#35299;&#65292;&#25552;&#20986;&#20102;&#32479;&#19968;&#26694;&#26550;&#25551;&#36848;GMRL&#31639;&#27861;&#30340;&#21464;&#21270;&#65292;&#24182;&#25351;&#20986;&#29616;&#26377;&#30340;&#38543;&#26426;&#20803;&#26799;&#24230;&#20272;&#35745;&#22120;&#23454;&#38469;&#19978;&#26159;&#26377;&#20559;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#65288;GMRL&#65289;&#26159;&#25351;&#20445;&#25345;&#20004;&#32423;&#20248;&#21270;&#31243;&#24207;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#22806;&#23618;&#20803;&#23398;&#20064;&#32773;&#25351;&#23548;&#20869;&#23618;&#26799;&#24230;&#20026;&#22522;&#30784;&#30340;&#24378;&#21270;&#23398;&#20064;&#32773;&#23454;&#29616;&#24555;&#36895;&#36866;&#24212;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#26694;&#26550;&#65292;&#25551;&#36848;&#20102;GMRL&#31639;&#27861;&#30340;&#21464;&#21270;&#65292;&#24182;&#25351;&#20986;GMRL&#37319;&#29992;&#30340;&#29616;&#26377;&#38543;&#26426;&#20803;&#26799;&#24230;&#20272;&#35745;&#22120;&#23454;&#38469;&#19978;&#26159;&#26377;&#20559;&#30340;&#12290;&#36825;&#31181;&#20803;&#26799;&#24230;&#20559;&#24046;&#26469;&#33258;&#20004;&#20010;&#26041;&#38754;&#65306;1&#65289;&#30001;&#20004;&#32423;&#38382;&#39064;&#32467;&#26500;&#24341;&#36215;&#30340;&#21512;&#25104;&#20559;&#24046;&#65292;&#23545;&#20869;&#37096;&#26356;&#26032;&#27493;&#39588;$K$&#12289;&#23398;&#20064;&#29575;$\alpha$&#12289;&#20272;&#35745;&#26041;&#24046;$\hat{\sigma}^{2}_{\text{In}}$&#21644;&#26679;&#26412;&#22823;&#23567;$|\tau|$&#26377;&#19968;&#20010;&#19978;&#38480;&#20026;$\mathcal{O}(K\alpha^{K}\hat{\sigma}_{\text{In}}|\tau|^{-0.5}$&#65307;2&#65289;&#30001;&#20110;&#20351;&#29992;&#33258;&#21160;&#24494;&#20998;&#32780;&#23548;&#33268;&#30340;&#22810;&#27493;Hessian&#20272;&#35745;&#20559;&#24046;$\hat{\Delta}_{H}$&#65292;&#20854;&#20855;&#26377;&#22810;&#39033;&#24335;&#24433;&#21709;$\mathcal{O}((K-1)(\hat{\Delta}_...
&lt;/p&gt;
&lt;p&gt;
arXiv:2112.15400v4 Announce Type: replace-cross  Abstract: Gradient-based Meta-RL (GMRL) refers to methods that maintain two-level optimisation procedures wherein the outer-loop meta-learner guides the inner-loop gradient-based reinforcement learner to achieve fast adaptations. In this paper, we develop a unified framework that describes variations of GMRL algorithms and points out that existing stochastic meta-gradient estimators adopted by GMRL are actually \textbf{biased}. Such meta-gradient bias comes from two sources: 1) the compositional bias incurred by the two-level problem structure, which has an upper bound of $\mathcal{O}\big(K\alpha^{K}\hat{\sigma}_{\text{In}}|\tau|^{-0.5}\big)$ \emph{w.r.t.} inner-loop update step $K$, learning rate $\alpha$, estimate variance $\hat{\sigma}^{2}_{\text{In}}$ and sample size $|\tau|$, and 2) the multi-step Hessian estimation bias $\hat{\Delta}_{H}$ due to the use of autodiff, which has a polynomial impact $\mathcal{O}\big((K-1)(\hat{\Delta}_
&lt;/p&gt;</description></item><item><title>&#20174;&#23398;&#20064;&#21160;&#20316;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#24847;&#24819;&#19981;&#21040;&#30340;&#22270;&#20687;&#34920;&#31034;&#65292;&#23637;&#31034;&#20986;&#39044;&#31034;&#20102;&#24863;&#30693;&#21644;</title><link>https://arxiv.org/abs/2012.04132</link><description>&lt;p&gt;
&#25968;&#23383;&#24863;&#20316;&#20026;&#25805;&#32437;&#22823;&#33041;&#30340;&#26032;&#20852;&#23646;&#24615;
&lt;/p&gt;
&lt;p&gt;
A Number Sense as an Emergent Property of the Manipulating Brain
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2012.04132
&lt;/p&gt;
&lt;p&gt;
&#20174;&#23398;&#20064;&#21160;&#20316;&#39044;&#27979;&#20219;&#21153;&#20013;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#24847;&#24819;&#19981;&#21040;&#30340;&#22270;&#20687;&#34920;&#31034;&#65292;&#23637;&#31034;&#20986;&#39044;&#31034;&#20102;&#24863;&#30693;&#21644;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#23383;&#21644;&#25968;&#37327;&#30340;&#29702;&#35299;&#19982;&#25805;&#32437;&#33021;&#21147;&#22312;&#20799;&#31461;&#26102;&#26399;&#20986;&#29616;&#65292;&#20294;&#20154;&#31867;&#33719;&#21462;&#21644;&#21457;&#23637;&#36825;&#31181;&#33021;&#21147;&#30340;&#26426;&#21046;&#20173;&#28982;&#26410;&#24471;&#21040;&#24456;&#22909;&#30340;&#29702;&#35299;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#27169;&#22411;&#25506;&#35752;&#20102;&#36825;&#20010;&#38382;&#39064;&#65292;&#20551;&#35774;&#23398;&#20064;&#32773;&#33021;&#22815;&#20174;&#20854;&#36873;&#25321;&#30340;&#20301;&#32622;&#25343;&#36215;&#21644;&#25918;&#32622;&#23567;&#29289;&#20307;&#65292;&#24182;&#20250;&#33258;&#21457;&#22320;&#36827;&#34892;&#36825;&#31181;&#26080;&#30446;&#30340;&#30340;&#25805;&#32437;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#20551;&#35774;&#23398;&#20064;&#32773;&#30340;&#35270;&#35273;&#31995;&#32479;&#23558;&#30417;&#35270;&#22330;&#26223;&#20013;&#29289;&#20307;&#30340;&#21464;&#21270;&#24067;&#23616;&#65292;&#24182;&#36890;&#36807;&#23558;&#24863;&#30693;&#19982;&#26469;&#33258;&#36816;&#21160;&#31995;&#32479;&#30340;&#30417;&#30563;&#20449;&#21495;&#36827;&#34892;&#27604;&#36739;&#26469;&#23398;&#20250;&#39044;&#27979;&#27599;&#20010;&#21160;&#20316;&#30340;&#25928;&#26524;&#12290;&#25105;&#20204;&#20351;&#29992;&#26631;&#20934;&#28145;&#24230;&#32593;&#32476;&#36827;&#34892;&#29305;&#24449;&#25552;&#21462;&#21644;&#20998;&#31867;&#65292;&#20197;&#21450;&#26799;&#24230;&#19979;&#38477;&#23398;&#20064;&#26469;&#24314;&#27169;&#24863;&#30693;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#20316;&#39044;&#27979;&#20219;&#21153;&#65292;&#20986;&#29616;&#20102;&#19968;&#31181;&#24847;&#24819;&#19981;&#21040;&#30340;&#22270;&#20687;&#34920;&#31034;&#65292;&#23637;&#31034;&#20986;&#39044;&#31034;&#20102;&#24863;&#30693;&#21644;
&lt;/p&gt;
&lt;p&gt;
arXiv:2012.04132v4 Announce Type: replace-cross  Abstract: The ability to understand and manipulate numbers and quantities emerges during childhood, but the mechanism through which humans acquire and develop this ability is still poorly understood. We explore this question through a model, assuming that the learner is able to pick up and place small objects from, and to, locations of its choosing, and will spontaneously engage in such undirected manipulation. We further assume that the learner's visual system will monitor the changing arrangements of objects in the scene and will learn to predict the effects of each action by comparing perception with a supervisory signal from the motor system. We model perception using standard deep networks for feature extraction and classification, and gradient descent learning. Our main finding is that, from learning the task of action prediction, an unexpected image representation emerges exhibiting regularities that foreshadow the perception and 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#34920;&#21333;&#22635;&#20805;&#20219;&#21153;&#37325;&#26032;&#26500;&#36896;&#20026;&#22810;&#27169;&#24577;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;QA&#31995;&#32479;&#23454;&#29616;&#34920;&#21333;&#20803;&#32032;&#30340;&#22635;&#20805;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#36827;&#19968;&#27493;&#32454;&#21270;&#34920;&#21333;&#22635;&#20805;&#36807;&#31243;&#12290;</title><link>https://arxiv.org/abs/2011.12340</link><description>&lt;p&gt;
mForms: &#22810;&#27169;&#24577;&#38382;&#31572;&#24418;&#24335;&#22635;&#20805;
&lt;/p&gt;
&lt;p&gt;
mForms : Multimodal Form-Filling with Question Answering
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/2011.12340
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#34920;&#21333;&#22635;&#20805;&#20219;&#21153;&#37325;&#26032;&#26500;&#36896;&#20026;&#22810;&#27169;&#24577;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#39044;&#35757;&#32451;&#30340;QA&#31995;&#32479;&#23454;&#29616;&#34920;&#21333;&#20803;&#32032;&#30340;&#22635;&#20805;&#65292;&#24182;&#36890;&#36807;&#22810;&#20219;&#21153;&#35757;&#32451;&#36827;&#19968;&#27493;&#32454;&#21270;&#34920;&#21333;&#22635;&#20805;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#34920;&#21333;&#22635;&#20805;&#26041;&#27861;&#65292;&#23558;&#20219;&#21153;&#37325;&#26032;&#26500;&#36896;&#20026;&#22810;&#27169;&#24577;&#33258;&#28982;&#35821;&#35328;&#38382;&#31572;&#65288;QA&#65289;&#12290;&#36890;&#36807;&#39318;&#20808;&#23558;GUI&#34920;&#21333;&#19978;&#30340;&#20803;&#32032;&#65288;&#25991;&#26412;&#23383;&#27573;&#12289;&#25353;&#38062;&#12289;&#22270;&#26631;&#31561;&#65289;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#26469;&#23454;&#29616;&#36825;&#31181;&#37325;&#26032;&#26500;&#36896;&#65292;&#36825;&#20123;&#38382;&#39064;&#25429;&#25417;&#20102;&#20803;&#32032;&#30340;&#22810;&#27169;&#24577;&#35821;&#20041;&#12290;&#22312;&#30830;&#23450;&#34920;&#21333;&#20803;&#32032;&#65288;&#38382;&#39064;&#65289;&#21644;&#29992;&#25143;&#35805;&#35821;&#65288;&#31572;&#26696;&#65289;&#20043;&#38388;&#30340;&#21305;&#37197;&#21518;&#65292;&#36890;&#36807;&#19968;&#20010;&#32463;&#36807;&#39044;&#35757;&#32451;&#30340;&#25277;&#21462;&#24335;QA&#31995;&#32479;&#22635;&#20805;&#34920;&#21333;&#20803;&#32032;&#12290;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;QA&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#29305;&#23450;&#20110;&#34920;&#21333;&#30340;&#35757;&#32451;&#65292;&#36825;&#31181;&#34920;&#21333;&#22635;&#20805;&#26041;&#27861;&#26159;&#38646;-shot &#30340;&#12290;&#26412;&#25991;&#36824;&#25552;&#20986;&#20102;&#36827;&#19968;&#27493;&#36890;&#36807;&#20351;&#29992;&#22810;&#20219;&#21153;&#35757;&#32451;&#26469;&#32454;&#21270;&#34920;&#21333;&#22635;&#20805;&#30340;&#26041;&#27861;&#65292;&#20197;&#25972;&#21512;&#21487;&#33021;&#22823;&#37327;&#30340;&#36830;&#32493;&#20219;&#21153;&#12290;&#26368;&#21518;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#33258;&#28982;&#35821;&#35328;&#34920;&#21333;&#22635;&#20805;&#25968;&#25454;&#38598;Multimodal Forms&#65288;mForms&#65289;&#65292;&#20197;&#21450;&#19968;&#20010;&#22810;&#27169;&#24577;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
arXiv:2011.12340v3 Announce Type: replace  Abstract: This paper presents a new approach to form-filling by reformulating the task as multimodal natural language Question Answering (QA). The reformulation is achieved by first translating the elements on the GUI form (text fields, buttons, icons, etc.) to natural language questions, where these questions capture the element's multimodal semantics. After a match is determined between the form element (Question) and the user utterance (Answer), the form element is filled through a pre-trained extractive QA system. By leveraging pre-trained QA models and not requiring form-specific training, this approach to form-filling is zero-shot. The paper also presents an approach to further refine the form-filling by using multi-task training to incorporate a potentially large number of successive tasks. Finally, the paper introduces a multimodal natural language form-filling dataset Multimodal Forms (mForms), as well as a multimodal extension of the
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#23558;&#19968;&#38454;&#36923;&#36753;&#19982;&#35745;&#25968;&#31526;&#21495;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#19979;&#20197;&#27425;&#32447;&#24615;&#26102;&#38388;&#19968;&#33268;&#23398;&#20064;&#21487;&#23450;&#20041;&#30340;&#20998;&#31867;&#22120;&#65292;&#20026;&#21253;&#21547;&#25968;&#20540;&#26041;&#38754;&#30340;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#23398;&#20064;&#26694;&#26550;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;</title><link>https://arxiv.org/abs/1909.03820</link><description>&lt;p&gt;
&#29992;&#35745;&#25968;&#31526;&#21495;&#30340;&#19968;&#38454;&#36923;&#36753;&#23450;&#20041;&#30340;&#27010;&#24565;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning Concepts Definable in First-Order Logic with Counting
&lt;/p&gt;
&lt;p&gt;
https://arxiv.org/abs/1909.03820
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#23558;&#19968;&#38454;&#36923;&#36753;&#19982;&#35745;&#25968;&#31526;&#21495;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#19979;&#20197;&#27425;&#32447;&#24615;&#26102;&#38388;&#19968;&#33268;&#23398;&#20064;&#21487;&#23450;&#20041;&#30340;&#20998;&#31867;&#22120;&#65292;&#20026;&#21253;&#21547;&#25968;&#20540;&#26041;&#38754;&#30340;&#26426;&#22120;&#23398;&#20064;&#25193;&#23637;&#23398;&#20064;&#26694;&#26550;&#36808;&#20986;&#20102;&#31532;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#22312;Grohe&#21644;Tur\'an&#24341;&#20837;&#30340;&#36923;&#36753;&#26694;&#26550;&#19979;&#30340;&#20851;&#31995;&#32972;&#26223;&#32467;&#26500;&#19978;&#30340;&#24067;&#23572;&#20998;&#31867;&#38382;&#39064;&#12290;&#20247;&#25152;&#21608;&#30693;(Grohe&#21644;Ritzert, LICS 2017)&#65292;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#19978;&#30340;&#19968;&#38454;&#36923;&#36753;&#21487;&#23450;&#20041;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#23398;&#20064;&#65292;&#20854;&#20013;&#32467;&#26500;&#30340;&#24230;&#21644;&#36816;&#34892;&#26102;&#38388;&#26159;&#20197;&#32467;&#26500;&#30340;&#22823;&#23567;&#20026;&#21333;&#20301;&#26469;&#34913;&#37327;&#30340;&#12290;&#25105;&#20204;&#23558;&#32467;&#26524;&#25512;&#24191;&#21040;&#20102;&#30001;Kuske&#21644;Schweikardt(LICS 2017)&#24341;&#20837;&#30340;&#24102;&#35745;&#25968;&#30340;&#19968;&#38454;&#36923;&#36753;FOCN&#65292;&#23427;&#20316;&#20026;&#19968;&#20010;&#24191;&#27867;&#25512;&#24191;&#21508;&#31181;&#35745;&#25968;&#36923;&#36753;&#30340;&#34920;&#29616;&#36923;&#36753;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#21487;&#20197;&#22312;&#22810;&#23545;&#25968;&#24230;&#32467;&#26500;&#31867;&#19978;&#23450;&#20041;&#30340;FOCN&#20013;&#30340;&#20998;&#31867;&#22120;&#21487;&#20197;&#22312;&#27425;&#32447;&#24615;&#26102;&#38388;&#20869;&#19968;&#33268;&#22320;&#23398;&#20064;&#12290;&#36825;&#21487;&#20197;&#30475;&#20316;&#26159;&#23558;&#23398;&#20064;&#26694;&#26550;&#25193;&#23637;&#20197;&#21253;&#21547;&#26426;&#22120;&#23398;&#20064;&#30340;&#25968;&#20540;&#26041;&#38754;&#30340;&#31532;&#19968;&#27493;&#12290;&#25105;&#20204;&#23558;&#36825;&#19968;&#32467;&#26524;&#25193;&#23637;&#21040;&#20102;&#26080;&#35270;&#30340;&#27010;&#29575;
&lt;/p&gt;
&lt;p&gt;
arXiv:1909.03820v2 Announce Type: replace-cross  Abstract: We study Boolean classification problems over relational background structures in the logical framework introduced by Grohe and Tur\'an (TOCS 2004). It is known (Grohe and Ritzert, LICS 2017) that classifiers definable in first-order logic over structures of polylogarithmic degree can be learned in sublinear time, where the degree of the structure and the running time are measured in terms of the size of the structure. We generalise the results to the first-order logic with counting FOCN, which was introduced by Kuske and Schweikardt (LICS 2017) as an expressive logic generalising various other counting logics. Specifically, we prove that classifiers definable in FOCN over classes of structures of polylogarithmic degree can be consistently learned in sublinear time. This can be seen as a first step towards extending the learning framework to include numerical aspects of machine learning. We extend the result to agnostic probabl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#24067;&#19968;&#33268;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;DiscoSCMs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#24314;&#27169;&#20013;&#30340;&#21453;&#20107;&#23454;&#24314;&#27169;&#25361;&#25112;&#12290;&#36825;&#31181;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#19968;&#33268;&#20551;&#35774;&#26469;&#35299;&#20915;&#22240;&#26524;&#27169;&#22411;&#30340;&#23481;&#37327;&#38480;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2401.15911</link><description>&lt;p&gt;
&#20998;&#24067;&#19968;&#33268;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Distribution-consistency Structural Causal Models. (arXiv:2401.15911v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.15911
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#24067;&#19968;&#33268;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;DiscoSCMs&#65289;&#65292;&#29992;&#20110;&#35299;&#20915;&#22240;&#26524;&#24314;&#27169;&#20013;&#30340;&#21453;&#20107;&#23454;&#24314;&#27169;&#25361;&#25112;&#12290;&#36825;&#31181;&#27169;&#22411;&#36890;&#36807;&#24341;&#20837;&#20998;&#24067;&#19968;&#33268;&#20551;&#35774;&#26469;&#35299;&#20915;&#22240;&#26524;&#27169;&#22411;&#30340;&#23481;&#37327;&#38480;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#21453;&#20107;&#23454;&#25512;&#29702;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22240;&#26524;&#24314;&#27169;&#39046;&#22495;&#65292;&#28508;&#22312;&#32467;&#26524;&#21644;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#26159;&#20027;&#35201;&#30340;&#26694;&#26550;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26694;&#26550;&#22312;&#23454;&#38469;&#24314;&#27169;&#21453;&#20107;&#23454;&#24773;&#20917;&#26102;&#38754;&#20020;&#30528;&#26174;&#33879;&#25361;&#25112;&#65292;&#36825;&#20123;&#21453;&#20107;&#23454;&#24773;&#20917;&#24418;&#24335;&#21270;&#20026;&#28508;&#22312;&#32467;&#26524;&#30340;&#32852;&#21512;&#20998;&#24067;&#21442;&#25968;&#12290;&#21453;&#20107;&#23454;&#25512;&#29702;&#22312;&#24403;&#20195;&#20915;&#31574;&#36807;&#31243;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#24847;&#20041;&#65292;&#29305;&#21035;&#26159;&#22312;&#38656;&#35201;&#22522;&#20110;$(Y(0),Y(1))$&#30340;&#32852;&#21512;&#20540;&#36827;&#34892;&#20010;&#24615;&#21270;&#28608;&#21169;&#30340;&#24773;&#26223;&#20013;&#12290;&#26412;&#25991;&#39318;&#20808;&#30740;&#31350;&#20102;&#28508;&#22312;&#32467;&#26524;&#21644;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#30340;&#21453;&#20107;&#23454;&#24314;&#27169;&#26694;&#26550;&#12290;&#36890;&#36807;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#20102;&#19968;&#31181;&#22266;&#26377;&#30340;&#27169;&#22411;&#23481;&#37327;&#38480;&#21046;&#65292;&#31216;&#20026;&#8220;&#36864;&#21270;&#30340;&#21453;&#20107;&#23454;&#38382;&#39064;&#8221;&#65292;&#36825;&#26159;&#36825;&#20004;&#20010;&#26694;&#26550;&#30340;&#22522;&#30707;&#19968;&#33268;&#24615;&#35268;&#21017;&#25152;&#23548;&#33268;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#8220;&#20998;&#24067;&#19968;&#33268;&#8221;&#20551;&#35774;&#65292;&#24182;&#26681;&#25454;&#36825;&#20010;&#20551;&#35774;&#25552;&#20986;&#20102;&#20998;&#24067;&#19968;&#33268;&#30340;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;DiscoSCMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of causal modeling, potential outcomes (PO) and structural causal models (SCMs) stand as the predominant frameworks. However, these frameworks face notable challenges in practically modeling counterfactuals, formalized as parameters of the joint distribution of potential outcomes. Counterfactual reasoning holds paramount importance in contemporary decision-making processes, especially in scenarios that demand personalized incentives based on the joint values of $(Y(0), Y(1))$. This paper begins with an investigation of the PO and SCM frameworks for modeling counterfactuals. Through the analysis, we identify an inherent model capacity limitation, termed as the ``degenerative counterfactual problem'', emerging from the consistency rule that is the cornerstone of both frameworks. To address this limitation, we introduce a novel \textit{distribution-consistency} assumption, and in alignment with it, we propose the Distribution-consistency Structural Causal Models (DiscoSCMs) o
&lt;/p&gt;</description></item><item><title>&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2401.11624</link><description>&lt;p&gt;
&#36890;&#36807;&#26816;&#32034;&#31034;&#33539;&#36827;&#34892;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#35821;&#35328;&#27169;&#22411;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
In-context Learning with Retrieved Demonstrations for Language Models: A Survey. (arXiv:2401.11624v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.11624
&lt;/p&gt;
&lt;p&gt;
&#26412;&#32508;&#36848;&#35843;&#26597;&#20102;&#19968;&#31181;&#21517;&#20026;&#26816;&#32034;&#31034;&#33539;&#30340;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#20351;&#29992;&#29305;&#23450;&#20110;&#36755;&#20837;&#26597;&#35810;&#30340;&#31034;&#33539;&#26469;&#25552;&#39640;&#35821;&#35328;&#27169;&#22411;&#30340;&#23569;&#37327;&#26679;&#26412;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#33021;&#21147;&#12290;&#36825;&#31181;&#26041;&#27861;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#36824;&#20943;&#23569;&#20102;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#65292;&#29305;&#21035;&#26159;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65292;&#24050;&#23637;&#31034;&#20986;&#21331;&#36234;&#30340;&#33021;&#21147;&#65292;&#21487;&#20197;&#22312;&#36755;&#20837;&#19978;&#19979;&#25991;&#20013;&#36827;&#34892;&#23569;&#37327;&#26679;&#26412;&#30340;&#24773;&#22659;&#23398;&#20064;&#65288;ICL&#65289;&#65292;&#24182;&#22312;&#26032;&#20219;&#21153;&#19978;&#20855;&#26377;&#36866;&#24212;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#27169;&#22411;&#30340;ICL&#33021;&#21147;&#23545;&#20110;&#23569;&#26679;&#26412;&#31034;&#33539;&#30340;&#36873;&#25321;&#26159;&#25935;&#24863;&#30340;&#12290;&#26368;&#36817;&#30340;&#19968;&#39033;&#30740;&#31350;&#36827;&#23637;&#26159;&#26816;&#32034;&#38024;&#23545;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#23450;&#21046;&#30340;&#31034;&#33539;&#12290;&#31034;&#33539;&#26816;&#32034;&#30340;&#23454;&#29616;&#30456;&#23545;&#31616;&#21333;&#65292;&#21033;&#29992;&#29616;&#26377;&#30340;&#25968;&#25454;&#24211;&#21644;&#26816;&#32034;&#31995;&#32479;&#12290;&#36825;&#19981;&#20165;&#25552;&#39640;&#20102;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#32780;&#19988;&#24050;&#32463;&#35777;&#26126;&#21487;&#20197;&#20943;&#23569;&#25163;&#21160;&#31034;&#20363;&#36873;&#25321;&#20013;&#30340;&#20559;&#35265;&#12290;&#37492;&#20110;&#20196;&#20154;&#40723;&#33310;&#30340;&#32467;&#26524;&#21644;&#22312;&#26816;&#32034;&#31034;&#33539;&#30340;ICL&#26041;&#38754;&#19981;&#26029;&#22686;&#38271;&#30340;&#30740;&#31350;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#30740;&#31350;&#32508;&#36848;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#21644;&#27604;&#36739;&#20102;&#26816;&#32034;&#27169;&#22411;&#30340;&#19981;&#21516;&#35774;&#35745;&#36873;&#25321;&#65292;&#26816;&#32034;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Language models, especially pre-trained large language models, have showcased remarkable abilities as few-shot in-context learners (ICL), adept at adapting to new tasks with just a few demonstrations in the input context. However, the model's ability to perform ICL is sensitive to the choice of the few-shot demonstrations. Instead of using a fixed set of demonstrations, one recent development is to retrieve demonstrations tailored to each input query. The implementation of demonstration retrieval is relatively straightforward, leveraging existing databases and retrieval systems. This not only improves the efficiency and scalability of the learning process but also has been shown to reduce biases inherent in manual example selection. In light of the encouraging results and growing research in ICL with retrieved demonstrations, we conduct an extensive review of studies in this area. In this survey, we discuss and compare different design choices for retrieval models, retrieval training p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;</title><link>http://arxiv.org/abs/2401.06461</link><description>&lt;p&gt;
&#20195;&#30721;&#20043;&#38388;&#30340;&#30028;&#38480;&#65306;&#25581;&#31034;&#26426;&#22120;&#21644;&#20154;&#31867;&#31243;&#24207;&#21592;&#20043;&#38388;&#19981;&#21516;&#30340;&#27169;&#24335;
&lt;/p&gt;
&lt;p&gt;
Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.06461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20998;&#26512;&#20195;&#30721;&#30340;&#23646;&#24615;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#20043;&#38388;&#30340;&#29420;&#29305;&#27169;&#24335;&#65292;&#23588;&#20854;&#26159;&#32467;&#26500;&#20998;&#21106;&#23545;&#20110;&#35782;&#21035;&#20195;&#30721;&#26469;&#28304;&#24456;&#20851;&#38190;&#12290;&#22522;&#20110;&#36825;&#20123;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#26041;&#27861;&#26469;&#26816;&#27979;&#26426;&#22120;&#29983;&#25104;&#30340;&#20195;&#30721;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#20195;&#30721;&#29983;&#25104;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#65292;&#20294;&#23427;&#20204;&#27169;&#31946;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#28304;&#20195;&#30721;&#20043;&#38388;&#30340;&#21306;&#21035;&#65292;&#23548;&#33268;&#36719;&#20214;&#20135;&#29289;&#30340;&#23436;&#25972;&#24615;&#21644;&#30495;&#23454;&#24615;&#38382;&#39064;&#12290;&#26412;&#25991;&#36890;&#36807;&#23545;&#20195;&#30721;&#38271;&#24230;&#12289;&#35789;&#27719;&#22810;&#26679;&#24615;&#21644;&#33258;&#28982;&#24615;&#31561;&#23646;&#24615;&#30340;&#20005;&#26684;&#20998;&#26512;&#65292;&#25581;&#31034;&#20102;&#26426;&#22120;&#21644;&#20154;&#31867;&#20195;&#30721;&#22266;&#26377;&#30340;&#29420;&#29305;&#27169;&#24335;&#12290;&#22312;&#25105;&#20204;&#30340;&#30740;&#31350;&#20013;&#29305;&#21035;&#27880;&#24847;&#21040;&#65292;&#20195;&#30721;&#30340;&#32467;&#26500;&#20998;&#21106;&#26159;&#35782;&#21035;&#20854;&#26469;&#28304;&#30340;&#20851;&#38190;&#22240;&#32032;&#12290;&#22522;&#20110;&#25105;&#20204;&#30340;&#21457;&#29616;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DetectCodeGPT&#30340;&#26032;&#22411;&#26426;&#22120;&#29983;&#25104;&#20195;&#30721;&#26816;&#27979;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#25913;&#36827;&#20102;DetectGPT&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have catalyzed an unprecedented wave in code generation. While achieving significant advances, they blur the distinctions between machine-and human-authored source code, causing integrity and authenticity issues of software artifacts. Previous methods such as DetectGPT have proven effective in discerning machine-generated texts, but they do not identify and harness the unique patterns of machine-generated code. Thus, its applicability falters when applied to code. In this paper, we carefully study the specific patterns that characterize machine and human-authored code. Through a rigorous analysis of code attributes such as length, lexical diversity, and naturalness, we expose unique pat-terns inherent to each source. We particularly notice that the structural segmentation of code is a critical factor in identifying its provenance. Based on our findings, we propose a novel machine-generated code detection method called DetectCodeGPT, which improves DetectGPT by cap
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#20998;&#26512;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21305;&#37197;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#21305;&#37197;&#12290;</title><link>http://arxiv.org/abs/2401.05224</link><description>&lt;p&gt;
&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#26159;&#21542;&#20197;&#30456;&#20284;&#26041;&#24335;&#34920;&#31034;&#19990;&#30028;&#65311;
&lt;/p&gt;
&lt;p&gt;
Do Vision and Language Encoders Represent the World Similarly?. (arXiv:2401.05224v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05224
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20998;&#26512;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861;&#26469;&#21305;&#37197;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#65292;&#26080;&#38656;&#35757;&#32451;&#21363;&#21487;&#23454;&#29616;&#21305;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24050;&#32463;&#25104;&#20026;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20107;&#23454;&#19978;&#30340;&#27169;&#22411;&#30340;&#23545;&#40784;&#30340;&#25991;&#26412;-&#22270;&#20687;&#32534;&#30721;&#22120;&#65288;&#22914;CLIP&#65289;&#24050;&#32463;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#27492;&#22806;&#65292;&#27169;&#24577;&#29305;&#23450;&#30340;&#32534;&#30721;&#22120;&#22312;&#21508;&#33258;&#39046;&#22495;&#20013;&#20063;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#12290;&#36825;&#24341;&#20986;&#20102;&#19968;&#20010;&#26680;&#24515;&#38382;&#39064;&#65306;&#30001;&#20110;&#23427;&#20204;&#22522;&#26412;&#19978;&#34920;&#31034;&#21516;&#19968;&#20010;&#29289;&#29702;&#19990;&#30028;&#65292;&#21333;&#27169;&#24577;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#32534;&#30721;&#22120;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#23545;&#40784;&#65311;&#36890;&#36807;&#20351;&#29992;&#20013;&#24515;&#26680;&#23545;&#40784;&#65288;CKA&#65289;&#20998;&#26512;&#22270;&#20687;-&#26631;&#39064;&#22522;&#20934;&#19978;&#35270;&#35273;&#21644;&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#22312;&#31354;&#38388;&#32467;&#26500;&#65292;&#25105;&#20204;&#21457;&#29616;&#26410;&#23545;&#40784;&#21644;&#23545;&#40784;&#30340;&#32534;&#30721;&#22120;&#30340;&#34920;&#31034;&#31354;&#38388;&#22312;&#35821;&#20041;&#19978;&#26159;&#30456;&#20284;&#30340;&#12290;&#22312;&#20687;CLIP&#36825;&#26679;&#30340;&#23545;&#40784;&#32534;&#30721;&#22120;&#20013;&#32570;&#20047;&#32479;&#35745;&#30456;&#20284;&#24615;&#30340;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#26174;&#31034;&#20102;&#21487;&#33021;&#23384;&#22312;&#26080;&#38656;&#20219;&#20309;&#35757;&#32451;&#30340;&#26410;&#23545;&#40784;&#32534;&#30721;&#22120;&#30340;&#21305;&#37197;&#12290;&#25105;&#20204;&#23558;&#36825;&#35270;&#20026;&#21033;&#29992;&#22270;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20284;&#24615;&#30340;&#26377;&#31181;&#23376;&#22270;&#21305;&#37197;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26041;&#27861; - &#24555;&#36895;&#20108;&#27425;&#20998;&#37197;&#38382;&#39064;&#20248;&#21270;&#21644;&#19968;&#31181;&#22522;&#20110;&#26032;&#39062;&#30340;&#23616;&#37096;CKA&#24230;&#37327;&#30340;&#21305;&#37197;/&#26816;&#32034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Aligned text-image encoders such as CLIP have become the de facto model for vision-language tasks. Furthermore, modality-specific encoders achieve impressive performances in their respective domains. This raises a central question: does an alignment exist between uni-modal vision and language encoders since they fundamentally represent the same physical world? Analyzing the latent spaces structure of vision and language models on image-caption benchmarks using the Centered Kernel Alignment (CKA), we find that the representation spaces of unaligned and aligned encoders are semantically similar. In the absence of statistical similarity in aligned encoders like CLIP, we show that a possible matching of unaligned encoders exists without any training. We frame this as a seeded graph-matching problem exploiting the semantic similarity between graphs and propose two methods - a Fast Quadratic Assignment Problem optimization, and a novel localized CKA metric-based matching/retrieval. We demons
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21644;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#20805;&#20998;&#21457;&#25381;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#30452;&#25509;&#23558;&#35270;&#35273;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2401.05010</link><description>&lt;p&gt;
&#31616;&#32422;&#21363;&#22823;&#36947;&#65306;&#23545;&#22810;&#27169;&#24577;&#23569;&#26679;&#26412;&#23398;&#20064;&#30340;&#28145;&#20837;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Less is More : A Closer Look at Multi-Modal Few-Shot Learning. (arXiv:2401.05010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2401.05010
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21644;&#35821;&#35328;&#27169;&#22411;&#26469;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#65292;&#20805;&#20998;&#21457;&#25381;&#20102;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#65292;&#24182;&#30452;&#25509;&#23558;&#35270;&#35273;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23569;&#26679;&#26412;&#23398;&#20064;&#26088;&#22312;&#36890;&#36807;&#26497;&#23569;&#37327;&#30340;&#21487;&#29992;&#22270;&#20687;&#26469;&#23398;&#20064;&#21644;&#21306;&#20998;&#26032;&#30340;&#31867;&#21035;&#65292;&#36825;&#22312;&#28145;&#24230;&#23398;&#20064;&#39046;&#22495;&#20013;&#26500;&#25104;&#20102;&#19968;&#20010;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#32773;&#20204;&#35797;&#22270;&#21033;&#29992;&#36825;&#20123;&#31232;&#26377;&#31867;&#21035;&#30340;&#38468;&#21152;&#25991;&#26412;&#25110;&#35821;&#35328;&#20449;&#24687;&#21644;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#26469;&#20419;&#36827;&#23398;&#20064;&#65292;&#20174;&#32780;&#22312;&#19968;&#23450;&#31243;&#24230;&#19978;&#32531;&#35299;&#19981;&#36275;&#30340;&#30417;&#30563;&#20449;&#21495;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#33267;&#20170;&#23545;&#20110;&#25991;&#26412;&#20449;&#24687;&#21644;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#20805;&#20998;&#28508;&#21147;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#20013;&#34987;&#20302;&#20272;&#20102;&#65292;&#23548;&#33268;&#24615;&#33021;&#30340;&#25552;&#21319;&#26377;&#38480;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#20219;&#21153;&#26694;&#26550;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#21033;&#29992;&#25991;&#26412;&#20449;&#24687;&#21644;&#35821;&#35328;&#27169;&#22411;&#12290;&#26356;&#35814;&#32454;&#22320;&#35828;&#65292;&#25105;&#20204;&#26126;&#30830;&#22320;&#21033;&#29992;&#21487;&#23398;&#20064;&#30340;&#25552;&#31034;&#26469;&#20805;&#20998;&#21457;&#25381;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#33021;&#21147;&#12290;&#25105;&#20204;&#30452;&#25509;&#23558;&#35270;&#35273;&#29305;&#24449;&#21644;&#25991;&#26412;&#29305;&#24449;&#36827;&#34892;&#25512;&#29702;&#65292;&#32780;&#19981;&#26159;&#31616;&#21333;&#22320;&#28155;&#21152;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot Learning aims to learn and distinguish new categories with a very limited number of available images, presenting a significant challenge in the realm of deep learning. Recent researchers have sought to leverage the additional textual or linguistic information of these rare categories with a pre-trained language model to facilitate learning, thus partially alleviating the problem of insufficient supervision signals. However, the full potential of the textual information and pre-trained language model have been underestimated in the few-shot learning till now, resulting in limited performance enhancements. To address this, we propose a simple but effective framework for few-shot learning tasks, specifically designed to exploit the textual information and language model. In more detail, we explicitly exploit the zero-shot capability of the pre-trained language model with the learnable prompt. And we just add the visual feature with the textual feature for inference directly witho
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SVGDreamer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;SVG&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#39537;&#21160;&#30340;&#22270;&#20687;&#30690;&#37327;&#21270;&#36807;&#31243;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20803;&#32032;&#25511;&#21046;&#65292;&#22686;&#24378;&#20102;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#32534;&#36753;&#24615;&#21644;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#22522;&#20110;&#30690;&#37327;&#21270;&#31890;&#23376;&#20998;&#25968;&#33976;&#39311;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#39068;&#33394;&#12289;&#24179;&#28369;&#24230;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2312.16476</link><description>&lt;p&gt;
SVGDreamer&#65306;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;SVG&#29983;&#25104;&#19982;&#25193;&#25955;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
SVGDreamer: Text Guided SVG Generation with Diffusion Model. (arXiv:2312.16476v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16476
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SVGDreamer&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;SVG&#29983;&#25104;&#12290;&#23427;&#36890;&#36807;&#24341;&#20837;&#35821;&#20041;&#39537;&#21160;&#30340;&#22270;&#20687;&#30690;&#37327;&#21270;&#36807;&#31243;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#20803;&#32032;&#25511;&#21046;&#65292;&#22686;&#24378;&#20102;&#29983;&#25104;&#32467;&#26524;&#30340;&#21487;&#32534;&#36753;&#24615;&#21644;&#36136;&#37327;&#12290;&#21516;&#26102;&#65292;&#37319;&#29992;&#22522;&#20110;&#30690;&#37327;&#21270;&#31890;&#23376;&#20998;&#25968;&#33976;&#39311;&#30340;&#26041;&#27861;&#35299;&#20915;&#20102;&#29616;&#26377;&#26041;&#27861;&#20013;&#30340;&#39068;&#33394;&#12289;&#24179;&#28369;&#24230;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;&#21487;&#32553;&#25918;&#30690;&#37327;&#22270;&#24418;&#65288;SVG&#65289;&#21512;&#25104;&#22312;&#22270;&#26631;&#35774;&#35745;&#21644;&#33609;&#22270;&#31561;&#39046;&#22495;&#23637;&#31034;&#20986;&#20102;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#25991;&#26412;&#21040;SVG&#29983;&#25104;&#26041;&#27861;&#23384;&#22312;&#32570;&#20047;&#21487;&#32534;&#36753;&#24615;&#12289;&#35270;&#35273;&#36136;&#37327;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25991;&#26412;&#24341;&#23548;&#30340;&#30690;&#37327;&#22270;&#24418;&#21512;&#25104;&#26041;&#27861;&#65292;&#31216;&#20026;SVGDreamer&#12290;SVGDreamer&#25972;&#21512;&#20102;&#19968;&#31181;&#35821;&#20041;&#39537;&#21160;&#30340;&#22270;&#20687;&#30690;&#37327;&#21270;&#65288;SIVE&#65289;&#36807;&#31243;&#65292;&#21487;&#20197;&#23558;&#21512;&#25104;&#36807;&#31243;&#20998;&#35299;&#20026;&#21069;&#26223;&#23545;&#35937;&#21644;&#32972;&#26223;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#21487;&#32534;&#36753;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;SIVE&#36807;&#31243;&#24341;&#20837;&#20102;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#22522;&#26412;&#20803;&#32032;&#25511;&#21046;&#21644;&#27880;&#24847;&#21147;&#25513;&#34109;&#25439;&#22833;&#20989;&#25968;&#65292;&#20197;&#26377;&#25928;&#25511;&#21046;&#21644;&#25805;&#20316;&#21508;&#20010;&#20803;&#32032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30690;&#37327;&#21270;&#31890;&#23376;&#20998;&#25968;&#33976;&#39311;&#65288;VPSD&#65289;&#30340;&#26041;&#27861;&#65292;&#20197;&#35299;&#20915;&#29616;&#26377;&#25991;&#26412;&#21040;SVG&#29983;&#25104;&#26041;&#27861;&#20013;&#39068;&#33394;&#36807;&#39281;&#21644;&#12289;&#30690;&#37327;&#22522;&#20803;&#36807;&#24179;&#28369;&#21644;&#32467;&#26524;&#22810;&#26679;&#24615;&#26377;&#38480;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, text-guided scalable vector graphics (SVGs) synthesis has shown promise in domains such as iconography and sketch. However, existing text-to-SVG generation methods lack editability and struggle with visual quality and result diversity. To address these limitations, we propose a novel text-guided vector graphics synthesis method called SVGDreamer. SVGDreamer incorporates a semantic-driven image vectorization (SIVE) process that enables the decomposition of synthesis into foreground objects and background, thereby enhancing editability. Specifically, the SIVE process introduce attention-based primitive control and an attention-mask loss function for effective control and manipulation of individual elements. Additionally, we propose a Vectorized Particle-based Score Distillation (VPSD) approach to tackle the challenges of color over-saturation, vector primitives over-smoothing, and limited result diversity in existing text-to-SVG generation methods. Furthermore, on the basis of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#26469;&#28040;&#38500;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#21033;&#29992;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#26469;&#25913;&#21892;&#20559;&#22909;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2312.16430</link><description>&lt;p&gt;
&#20559;&#22909;&#20316;&#20026;&#22870;&#21169;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;
&lt;/p&gt;
&lt;p&gt;
Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2312.16430
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#36827;&#34892;&#26368;&#22823;&#20559;&#22909;&#20248;&#21270;&#30340;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#36890;&#36807;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#26469;&#28040;&#38500;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#21033;&#29992;&#29575;&#21644;&#31283;&#23450;&#24615;&#65292;&#24182;&#36890;&#36807;&#35299;&#20915;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#26469;&#25913;&#21892;&#20559;&#22909;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20559;&#22909;&#23398;&#20064;&#26159;&#23558;&#35821;&#35328;&#27169;&#22411;&#19982;&#20154;&#31867;&#20215;&#20540;&#35266;&#23545;&#40784;&#30340;&#20851;&#38190;&#25216;&#26415;&#12290;&#20174;&#20154;&#31867;&#21453;&#39304;&#24378;&#21270;&#23398;&#20064;&#65288;RLHF&#65289;&#26159;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#20248;&#21270;&#20559;&#22909;&#23398;&#20064;&#65292;&#39318;&#20808;&#25311;&#21512;&#20559;&#22909;&#20998;&#25968;&#30340;&#22870;&#21169;&#27169;&#22411;&#65292;&#28982;&#21518;&#20351;&#29992;&#22522;&#20110;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;&#36827;&#34892;&#20248;&#21270;&#65292;&#20197;&#26368;&#22823;&#21270;&#22870;&#21169;&#12290;RLHF&#30340;&#22788;&#29702;&#36807;&#31243;&#22797;&#26434;&#12289;&#32791;&#26102;&#19988;&#19981;&#31283;&#23450;&#12290;&#30452;&#25509;&#20559;&#22909;&#20248;&#21270;&#65288;DPO&#65289;&#31639;&#27861;&#20351;&#29992;&#31163;&#31574;&#30053;&#31639;&#27861;&#30452;&#25509;&#20248;&#21270;&#29983;&#25104;&#31574;&#30053;&#65292;&#28040;&#38500;&#20102;&#23545;&#22870;&#21169;&#27169;&#22411;&#30340;&#38656;&#27714;&#65292;&#20855;&#26377;&#39640;&#25928;&#21644;&#31283;&#23450;&#30340;&#25968;&#25454;&#21033;&#29992;&#29575;&#12290;DPO&#20351;&#29992;&#24067;&#25289;&#24503;&#21033;-&#29305;&#37324;&#27169;&#22411;&#21644;&#23545;&#25968;&#25439;&#22833;&#65292;&#23548;&#33268;&#22312;&#20559;&#22909;&#25509;&#36817;&#30830;&#23450;&#24615;&#26102;&#24573;&#30053;&#20102;KL&#27491;&#21017;&#21270;&#39033;&#32780;&#36807;&#24230;&#25311;&#21512;&#20559;&#22909;&#25968;&#25454;&#12290;IPO&#20351;&#29992;&#19968;&#31181;&#22522;&#20110;&#26681;&#26597;&#25214;&#30340;&#25104;&#23545;&#22343;&#26041;&#35823;&#24046;&#25439;&#22833;&#26469;&#35299;&#20915;&#24573;&#30053;KL&#27491;&#21017;&#21270;&#38382;&#39064;&#65292;&#24182;&#23398;&#20064;&#21040;&#26368;&#20248;&#31574;&#30053;&#12290;&#20294;&#26159;IPO&#30340;&#25104;&#23545;&#25439;&#22833;&#20173;&#28982;&#26080;&#27861;&#20351;KL&#27491;&#21017;&#21270;&#29983;&#25928;&#12290;&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#20351;&#29992;&#37325;&#35201;&#24615;&#25277;&#26679;&#25216;&#26415;&#26469;&#35299;&#20915;&#20559;&#22909;&#23398;&#20064;&#20013;&#30340;&#20248;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Preference learning is a key technology for aligning language models with human values. Reinforcement Learning from Human Feedback (RLHF) is a model based algorithm to optimize preference learning, which first fitting a reward model for preference score, and then optimizing generating policy with on-policy PPO algorithm to maximize the reward. The processing of RLHF is complex, time-consuming and unstable. Direct Preference Optimization (DPO) algorithm using off-policy algorithm to direct optimize generating policy and eliminating the need for reward model, which is data efficient and stable. DPO use Bradley-Terry model and log-loss which leads to over-fitting to the preference data at the expense of ignoring KL-regularization term when preference near deterministic. IPO uses a root-finding pairwise MSE loss to solve the ignoring KL-regularization problem, and learning an optimal policy. But IPO's pairwise loss still can't s make the KL-regularization to work. In this paper, we design 
&lt;/p&gt;</description></item><item><title>VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;</title><link>http://arxiv.org/abs/2311.01623</link><description>&lt;p&gt;
VQPy&#65306;&#19968;&#31181;&#38754;&#21521;&#29616;&#20195;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
VQPy: An Object-Oriented Approach to Modern Video Analytics. (arXiv:2311.01623v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2311.01623
&lt;/p&gt;
&lt;p&gt;
VQPy&#26159;&#19968;&#31181;&#38754;&#21521;&#23545;&#35937;&#30340;&#35270;&#39057;&#20998;&#26512;&#26041;&#27861;&#65292;&#23427;&#20351;&#29992;Python&#21464;&#20307;&#20316;&#20026;&#21069;&#31471;&#65292;&#24182;&#20855;&#26377;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#33258;&#21160;&#26500;&#24314;&#21644;&#20248;&#21270;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#30340;&#22788;&#29702;&#27969;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#26512;&#24191;&#27867;&#24212;&#29992;&#20110;&#24403;&#20170;&#31995;&#32479;&#21644;&#26381;&#21153;&#20013;&#12290;&#22312;&#35270;&#39057;&#20998;&#26512;&#30340;&#21069;&#27839;&#26159;&#29992;&#25143;&#24320;&#21457;&#30340;&#35270;&#39057;&#26597;&#35810;&#65292;&#20197;&#25214;&#21040;&#29305;&#23450;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#12290;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#65288;&#20363;&#22914;&#20154;&#65292;&#21160;&#29289;&#65292;&#27773;&#36710;&#31561;&#65289;&#19982;&#20256;&#32479;&#38754;&#21521;&#23545;&#35937;&#35821;&#35328;&#24314;&#27169;&#30340;&#23545;&#35937;&#30456;&#20284;&#30340;&#27934;&#23519;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#35270;&#39057;&#20998;&#26512;&#30340;&#38754;&#21521;&#23545;&#35937;&#26041;&#27861;&#12290;&#36825;&#31181;&#26041;&#27861;&#21517;&#20026;VQPy&#65292;&#21253;&#25324;&#19968;&#20010;&#21069;&#31471;&#65288;&#19968;&#31181;Python&#21464;&#20307;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#21487;&#20197;&#34920;&#36798;&#35270;&#39057;&#23545;&#35937;&#21450;&#20854;&#20132;&#20114;&#30340;&#32467;&#26500;&#65289;&#21644;&#19968;&#20010;&#21487;&#25193;&#23637;&#30340;&#21518;&#31471;&#65292;&#21487;&#20197;&#22522;&#20110;&#35270;&#39057;&#23545;&#35937;&#33258;&#21160;&#29983;&#25104;&#21644;&#20248;&#21270;&#31649;&#36947;&#12290;&#25105;&#20204;&#24050;&#32463;&#23454;&#26045;&#21644;&#24320;&#28304;&#20102;VQPy&#65292;&#23427;&#24050;&#32463;&#20316;&#20026;Cisco DeepVision&#26694;&#26550;&#30340;&#19968;&#37096;&#20998;&#20135;&#21697;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video analytics is widely used in contemporary systems and services. At the forefront of video analytics are video queries that users develop to find objects of particular interest. Building upon the insight that video objects (e.g., human, animals, cars, etc.), the center of video analytics, are similar in spirit to objects modeled by traditional object-oriented languages, we propose to develop an object-oriented approach to video analytics. This approach, named VQPy, consists of a frontend$\unicode{x2015}$a Python variant with constructs that make it easy for users to express video objects and their interactions$\unicode{x2015}$as well as an extensible backend that can automatically construct and optimize pipelines based on video objects. We have implemented and open-sourced VQPy, which has been productized in Cisco as part of its DeepVision framework.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#22312;&#22240;&#26524;&#36131;&#20219;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#30446;&#21069;&#26368;&#22909;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;Chat-GPT&#65289;&#30340;&#35780;&#20272;&#65292;&#21487;&#20197;&#20102;&#35299;&#20854;&#30446;&#21069;&#30340;&#24615;&#33021;&#20197;&#21450;&#21487;&#33021;&#30340;&#27861;&#24459;&#35268;&#21046;&#24418;&#24335;&#12290;</title><link>http://arxiv.org/abs/2310.13192</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#36879;&#26126;&#27861;&#24459;
&lt;/p&gt;
&lt;p&gt;
The opaque law of artificial intelligence. (arXiv:2310.13192v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.13192
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#31639;&#27861;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#37325;&#28857;&#20851;&#27880;&#20154;&#24037;&#26234;&#33021;&#22312;&#22240;&#26524;&#36131;&#20219;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#12290;&#36890;&#36807;&#23545;&#30446;&#21069;&#26368;&#22909;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;Chat-GPT&#65289;&#30340;&#35780;&#20272;&#65292;&#21487;&#20197;&#20102;&#35299;&#20854;&#30446;&#21069;&#30340;&#24615;&#33021;&#20197;&#21450;&#21487;&#33021;&#30340;&#27861;&#24459;&#35268;&#21046;&#24418;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#20998;&#26512;&#31639;&#27861;&#30340;&#19981;&#36879;&#26126;&#24615;&#65292;&#24182;&#23558;&#20854;&#32622;&#20110;&#20154;&#24037;&#26234;&#33021;&#22240;&#26524;&#36131;&#20219;&#30340;&#20844;&#24320;&#36777;&#35770;&#32972;&#26223;&#19979;&#36827;&#34892;&#35752;&#35770;&#65307;&#36890;&#36807;&#24212;&#29992;&#22270;&#28789;&#27979;&#35797;&#20013;&#25552;&#20986;&#30340;&#23545;&#35805;&#26041;&#27861;&#65292;&#25105;&#20204;&#24076;&#26395;&#35780;&#20272;&#29616;&#26377;&#26368;&#22909;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65288;Chat-GPT&#65289;&#30340;&#24615;&#33021;&#65292;&#20197;&#30830;&#23450;&#20854;&#30446;&#21069;&#30340;&#33021;&#21147;&#21644;&#21487;&#33021;&#30340;&#27861;&#24459;&#35268;&#21046;&#24418;&#24335;&#12290;&#38382;&#39064;&#20998;&#26512;&#23558;&#22522;&#20110;&#23545;&#20256;&#32479;&#27861;&#24459;&#33539;&#30068;&#65288;&#22914;&#22240;&#26524;&#20851;&#31995;&#12289;&#24847;&#22270;&#21644;&#36807;&#22833;&#65289;&#30340;&#35780;&#35770;&#65292;&#20197;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#20351;&#29992;&#20013;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#20851;&#27880;&#20154;&#26426;&#20132;&#20114;&#12290;&#20174;&#35745;&#31639;&#26426;&#31185;&#23398;&#35282;&#24230;&#26469;&#30475;&#65292;&#25991;&#20013;&#36824;&#23558;&#25552;&#20986;&#19968;&#31181;&#38024;&#23545;Chat-GPT&#36827;&#34892;&#23454;&#38469;&#35810;&#38382;&#30340;&#26041;&#27861;&#65292;&#20197;&#25214;&#21040;&#20154;&#24037;&#26234;&#33021;&#36816;&#34892;&#36807;&#31243;&#20013;&#30340;&#19968;&#20123;&#20851;&#38190;&#38382;&#39064;&#12290;&#25991;&#31456;&#30340;&#32467;&#23614;&#23558;&#38598;&#20013;&#35752;&#35770;&#19968;&#20123;&#29616;&#26377;&#30340;&#31435;&#27861;&#25514;&#26045;&#12290;
&lt;/p&gt;
&lt;p&gt;
The purpose of this paper is to analyse the opacity of algorithms, contextualized in the open debate on responsibility for artificial intelligence causation; with an experimental approach by which, applying the proposed conversational methodology of the Turing Test, we expect to evaluate the performance of one of the best existing NLP model of generative AI (Chat-GPT) to see how far it can go right now and how the shape of a legal regulation of it could be. The analysis of the problem will be supported by a comment of Italian classical law categories such as causality, intent and fault to understand the problem of the usage of AI, focusing in particular on the human-machine interaction. On the computer science side, for a technical point of view of the logic used to craft these algorithms, in the second chapter will be proposed a practical interrogation of Chat-GPT aimed at finding some critical points of the functioning of AI. The end of the paper will concentrate on some existing leg
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MSD-Mixer&#30340;&#22810;&#23610;&#24230;&#20998;&#35299;MLP-Mixer&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23618;&#27425;&#20013;&#34920;&#31034;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25340;&#25509;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#22810;&#23610;&#24230;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#36890;&#36947;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2310.11959</link><description>&lt;p&gt;
&#19968;&#31181;&#29992;&#20110;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#30340;&#22810;&#23610;&#24230;&#20998;&#35299;MLP-Mixer
&lt;/p&gt;
&lt;p&gt;
A Multi-Scale Decomposition MLP-Mixer for Time Series Analysis. (arXiv:2310.11959v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11959
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MSD-Mixer&#30340;&#22810;&#23610;&#24230;&#20998;&#35299;MLP-Mixer&#27169;&#22411;&#65292;&#23427;&#33021;&#22815;&#23558;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#25104;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23618;&#27425;&#20013;&#34920;&#31034;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25340;&#25509;&#26041;&#27861;&#65292;&#20197;&#22788;&#29702;&#22810;&#23610;&#24230;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#36890;&#36947;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#27604;&#29616;&#26377;&#26041;&#27861;&#26356;&#22909;&#22320;&#36827;&#34892;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#36890;&#24120;&#20855;&#26377;&#29420;&#29305;&#30340;&#32452;&#25104;&#21644;&#22797;&#26434;&#30340;&#22810;&#23610;&#24230;&#26102;&#38388;&#21464;&#21270;&#65292;&#38656;&#35201;&#22312;&#20854;&#20998;&#26512;&#20013;&#29305;&#21035;&#32771;&#34385;&#20998;&#35299;&#21644;&#22810;&#23610;&#24230;&#24314;&#27169;&#12290;&#29616;&#26377;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#21482;&#36866;&#29992;&#20110;&#21333;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#65292;&#24182;&#19988;&#23545;&#23376;&#24207;&#21015;&#32423;&#21035;&#30340;&#24314;&#27169;&#21644;&#20998;&#35299;&#19981;&#22815;&#20805;&#20998;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MSD-Mixer&#65292;&#19968;&#31181;&#22810;&#23610;&#24230;&#20998;&#35299;&#30340;MLP-Mixer&#65292;&#23427;&#23398;&#20250;&#20102;&#23558;&#36755;&#20837;&#30340;&#26102;&#38388;&#24207;&#21015;&#26126;&#30830;&#22320;&#20998;&#35299;&#25104;&#19981;&#21516;&#30340;&#32452;&#25104;&#37096;&#20998;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#23618;&#27425;&#20013;&#34920;&#31034;&#36825;&#20123;&#32452;&#25104;&#37096;&#20998;&#12290;&#20026;&#20102;&#22788;&#29702;&#22810;&#23610;&#24230;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#36890;&#36947;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26102;&#38388;&#25340;&#25509;&#26041;&#27861;&#65292;&#23558;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#20026;&#22810;&#23610;&#24230;&#23376;&#24207;&#21015;&#65292;&#21363;patches&#65292;&#24182;&#20351;&#29992;MLPs&#26469;&#32452;&#21512;patches&#20869;&#37096;&#21644;patches&#38388;&#30340;&#21464;&#21270;&#20197;&#21450;&#36890;&#36947;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#25439;&#22833;&#20989;&#25968;&#26469;&#32422;&#26463;&#20998;&#35299;&#27531;&#24046;&#30340;&#24133;&#24230;&#21644;&#33258;&#30456;&#20851;&#24615;&#65292;&#20197;&#23454;&#29616;&#23436;&#25972;&#30340;&#20998;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Time series data, often characterized by unique composition and complex multi-scale temporal variations, requires special consideration of decomposition and multi-scale modeling in its analysis. Existing deep learning methods on this best fit to only univariate time series, and have not sufficiently accounted for sub-series level modeling and decomposition completeness. To address this, we propose MSD-Mixer, a Multi-Scale Decomposition MLP-Mixer which learns to explicitly decompose the input time series into different components, and represents the components in different layers. To handle multi-scale temporal patterns and inter-channel dependencies, we propose a novel temporal patching approach to model the time series as multi-scale sub-series, i.e., patches, and employ MLPs to mix intra- and inter-patch variations and channel-wise correlations. In addition, we propose a loss function to constrain both the magnitude and autocorrelation of the decomposition residual for decomposition 
&lt;/p&gt;</description></item><item><title>SOTOPIA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#26234;&#33021;&#20013;&#30340;&#31038;&#20132;&#26234;&#33021;&#30340;&#20132;&#20114;&#24335;&#29615;&#22659;&#12290;&#36890;&#36807;&#27169;&#25311;&#22797;&#26434;&#30340;&#31038;&#20132;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#31038;&#20132;&#26234;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;SOTOPIA-hard&#24773;&#26223;&#19979;&#12290;GPT-4&#22312;&#36825;&#20010;&#23376;&#38598;&#19978;&#30340;&#30446;&#26631;&#23436;&#25104;&#29575;&#36739;&#20302;&#12290;</title><link>http://arxiv.org/abs/2310.11667</link><description>&lt;p&gt;
SOTOPIA: &#20132;&#20114;&#24335;&#35780;&#20272;&#35821;&#35328;&#26234;&#33021;&#20013;&#30340;&#31038;&#20132;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents. (arXiv:2310.11667v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.11667
&lt;/p&gt;
&lt;p&gt;
SOTOPIA&#26159;&#19968;&#20010;&#29992;&#20110;&#35780;&#20272;&#35821;&#35328;&#26234;&#33021;&#20013;&#30340;&#31038;&#20132;&#26234;&#33021;&#30340;&#20132;&#20114;&#24335;&#29615;&#22659;&#12290;&#36890;&#36807;&#27169;&#25311;&#22797;&#26434;&#30340;&#31038;&#20132;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#20840;&#38754;&#30340;&#35780;&#20272;&#26694;&#26550;&#65292;&#25105;&#20204;&#21457;&#29616;&#19981;&#21516;&#27169;&#22411;&#20043;&#38388;&#30340;&#31038;&#20132;&#26234;&#33021;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#29305;&#21035;&#26159;&#22312;SOTOPIA-hard&#24773;&#26223;&#19979;&#12290;GPT-4&#22312;&#36825;&#20010;&#23376;&#38598;&#19978;&#30340;&#30446;&#26631;&#23436;&#25104;&#29575;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#26159;&#31038;&#20132;&#30340;&#23384;&#22312;&#65307;&#25105;&#20204;&#22312;&#26085;&#24120;&#20114;&#21160;&#20013;&#36861;&#27714;&#31038;&#20132;&#30446;&#26631;&#65292;&#36825;&#26159;&#31038;&#20132;&#26234;&#33021;&#30340;&#20851;&#38190;&#26041;&#38754;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#22312;&#36825;&#20010;&#39046;&#22495;&#30340;&#33021;&#21147;&#20173;&#28982;&#38590;&#20197;&#25417;&#25720;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SOTOPIA&#65292;&#19968;&#20010;&#24320;&#25918;&#24335;&#29615;&#22659;&#65292;&#29992;&#20110;&#27169;&#25311;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20043;&#38388;&#30340;&#22797;&#26434;&#31038;&#20132;&#20114;&#21160;&#24182;&#35780;&#20272;&#23427;&#20204;&#30340;&#31038;&#20132;&#26234;&#33021;&#12290;&#22312;&#25105;&#20204;&#30340;&#29615;&#22659;&#20013;&#65292;&#20195;&#29702;&#20154;&#25198;&#28436;&#35282;&#33394;&#65292;&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#30456;&#20114;&#21327;&#20316;&#12289;&#21512;&#20316;&#12289;&#20132;&#27969;&#21644;&#31454;&#20105;&#65292;&#20197;&#23454;&#29616;&#22797;&#26434;&#30340;&#31038;&#20132;&#30446;&#26631;&#12290;&#25105;&#20204;&#27169;&#25311;&#20102;LLM-based&#20195;&#29702;&#20154;&#19982;&#20154;&#31867;&#20043;&#38388;&#22312;&#36825;&#20010;&#20219;&#21153;&#31354;&#38388;&#20869;&#30340;&#35282;&#33394;&#25198;&#28436;&#20114;&#21160;&#65292;&#24182;&#20351;&#29992;&#19968;&#20010;&#21517;&#20026;SOTOPIA-Eval&#30340;&#25972;&#20307;&#35780;&#20272;&#26694;&#26550;&#23545;&#23427;&#20204;&#30340;&#34920;&#29616;&#36827;&#34892;&#35780;&#20272;&#12290;&#36890;&#36807;SOTOPIA&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#22312;&#31038;&#20132;&#26234;&#33021;&#26041;&#38754;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#30830;&#23450;&#20102;SOTOPIA&#30340;&#19968;&#20010;&#23376;&#38598;&#65292;&#21363;SOTOPIA-hard&#65292;&#23545;&#25152;&#26377;&#27169;&#22411;&#26469;&#35828;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#36825;&#20010;&#23376;&#38598;&#19978;&#65292;GPT-4&#30340;&#30446;&#26631;&#23436;&#25104;&#29575;&#26174;&#33879;&#36739;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completio
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#23558;&#32852;&#37030;&#23398;&#20064;&#30340;&#21327;&#35843;&#22806;&#21253;&#32473;&#21306;&#22359;&#38142;&#31561;&#20998;&#25955;&#32593;&#32476;&#30340;&#23454;&#38469;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#30001;&#21306;&#22359;&#38142;&#30340;&#36816;&#20316;&#26041;&#24335;&#25903;&#25345;&#30340;&#27169;&#22411;&#38472;&#26087;&#21644;&#19981;&#19968;&#33268;&#23545;&#24322;&#27493;FL&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2310.07471</link><description>&lt;p&gt;
&#21306;&#22359;&#38142;&#32852;&#37030;&#23398;&#20064;&#20013;&#20998;&#25955;&#21270;&#30340;&#24433;&#21709;&#65306;&#35780;&#20272;&#27169;&#22411;&#38472;&#26087;&#21644;&#19981;&#19968;&#33268;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
The Implications of Decentralization in Blockchained Federated Learning: Evaluating the Impact of Model Staleness and Inconsistencies. (arXiv:2310.07471v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.07471
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102;&#23558;&#32852;&#37030;&#23398;&#20064;&#30340;&#21327;&#35843;&#22806;&#21253;&#32473;&#21306;&#22359;&#38142;&#31561;&#20998;&#25955;&#32593;&#32476;&#30340;&#23454;&#38469;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;&#20102;&#30001;&#21306;&#22359;&#38142;&#30340;&#36816;&#20316;&#26041;&#24335;&#25903;&#25345;&#30340;&#27169;&#22411;&#38472;&#26087;&#21644;&#19981;&#19968;&#33268;&#23545;&#24322;&#27493;FL&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21306;&#22359;&#38142;&#25215;&#35834;&#36890;&#36807;&#25552;&#20379;&#36827;&#19968;&#27493;&#30340;&#20998;&#25955;&#21270;&#12289;&#23433;&#20840;&#24615;&#12289;&#19981;&#21487;&#21464;&#24615;&#21644;&#20449;&#20219;&#26469;&#22686;&#24378;&#32852;&#37030;&#23398;&#20064;&#31561;&#20998;&#24067;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#36825;&#20123;&#29305;&#24615;&#23545;&#20110;&#23454;&#29616;&#19979;&#19968;&#20195;&#24212;&#29992;&#20013;&#30340;&#21327;&#20316;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#28857;&#23545;&#28857;&#65288;P2P&#65289;&#21306;&#22359;&#38142;&#33410;&#28857;&#30340;&#20869;&#22312;&#20998;&#25955;&#21270;&#25805;&#20316;&#20351;&#24471;&#32852;&#37030;&#23398;&#20064;&#22788;&#20110;&#26410;&#30693;&#30340;&#29366;&#24577;&#65292;FL&#36718;&#27425;&#21644;&#20840;&#23616;&#27169;&#22411;&#30340;&#27010;&#24565;&#21464;&#24471;&#26080;&#24847;&#20041;&#65292;&#22240;&#20026;&#35774;&#22791;&#30340;&#21516;&#27493;&#20002;&#22833;&#20102;&#20013;&#24515;&#21327;&#35843;&#26381;&#21153;&#22120;&#30340;&#24418;&#35937;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#23558;FL&#30340;&#21327;&#35843;&#22806;&#21253;&#32473;&#21306;&#22359;&#38142;&#31561;&#27665;&#20027;&#32593;&#32476;&#30340;&#23454;&#38469;&#24433;&#21709;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#30001;&#21306;&#22359;&#38142;&#30340;&#36816;&#20316;&#26041;&#24335;&#25903;&#25345;&#30340;&#27169;&#22411;&#38472;&#26087;&#21644;&#19981;&#19968;&#33268;&#23545;&#24322;&#27493;&#36827;&#34892;&#30340;FL&#35757;&#32451;&#36807;&#31243;&#30340;&#24433;&#21709;&#12290;&#36890;&#36807;&#27169;&#25311;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#22522;&#20110;&#21306;&#22359;&#38142;&#30340;FL&#22312;&#33879;&#21517;&#30340;CIFAR-10&#25968;&#25454;&#38598;&#19978;&#30340;&#36816;&#20316;&#24773;&#20917;&#65292;&#24182;&#30528;&#37325;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
Blockchain promises to enhance distributed machine learning (ML) approaches such as federated learning (FL) by providing further decentralization, security, immutability, and trust, which are key properties for enabling collaborative intelligence in next-generation applications. Nonetheless, the intrinsic decentralized operation of peer-to-peer (P2P) blockchain nodes leads to an uncharted setting for FL, whereby the concepts of FL round and global model become meaningless, as devices' synchronization is lost without the figure of a central orchestrating server. In this paper, we study the practical implications of outsourcing the orchestration of FL to a democratic network such as in a blockchain. In particular, we focus on the effects that model staleness and inconsistencies, endorsed by blockchains' modus operandi, have on the training procedure held by FL devices asynchronously. Using simulation, we evaluate the blockchained FL operation on the well-known CIFAR-10 dataset and focus 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#19968;&#20123;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2310.04870</link><description>&lt;p&gt;
Lemur&#65306;&#22312;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#20013;&#38598;&#25104;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Lemur: Integrating Large Language Models in Automated Program Verification. (arXiv:2310.04870v2 [cs.FL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.04870
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#23436;&#22791;&#24615;&#12290;&#36825;&#20010;&#26041;&#27861;&#22312;&#19968;&#20123;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#21462;&#24471;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
LLMs&#22312;&#20195;&#30721;&#29702;&#35299;&#33021;&#21147;&#19978;&#30340;&#23637;&#31034;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65306;&#23427;&#20204;&#26159;&#21542;&#21487;&#20197;&#29992;&#20110;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#65292;&#36825;&#26159;&#19968;&#20010;&#36890;&#24120;&#38656;&#35201;&#39640;&#32423;&#25277;&#35937;&#25512;&#29702;&#30340;&#20219;&#21153;&#65292;&#23545;&#20110;&#39564;&#35777;&#24037;&#20855;&#26469;&#35828;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;LLMs&#30340;&#33021;&#21147;&#21644;&#33258;&#21160;&#25512;&#29702;&#22120;&#32467;&#21512;&#36215;&#26469;&#36827;&#34892;&#33258;&#21160;&#31243;&#24207;&#39564;&#35777;&#30340;&#36890;&#29992;&#26041;&#27861;&#12290;&#25105;&#20204;&#27491;&#24335;&#25551;&#36848;&#20102;&#36825;&#31181;&#26041;&#27861;&#35770;&#65292;&#23558;&#20854;&#20316;&#20026;&#25512;&#23548;&#35268;&#21017;&#30340;&#38598;&#21512;&#36827;&#34892;&#35770;&#35777;&#20854;&#23436;&#22791;&#24615;&#12290;&#25105;&#20204;&#23558;&#35745;&#31639;&#26426;&#25512;&#29702;&#24418;&#25104;&#20026;&#19968;&#20010;&#23436;&#22791;&#30340;&#33258;&#21160;&#39564;&#35777;&#36807;&#31243;&#65292;&#36825;&#22312;&#19968;&#32452;&#21512;&#25104;&#21644;&#31454;&#20105;&#22522;&#20934;&#19978;&#24102;&#26469;&#20102;&#23454;&#38469;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
The demonstrated code-understanding capability of LLMs raises the question of whether they can be used for automated program verification, a task that often demands high-level abstract reasoning about program properties, which is challenging for verification tools. We propose a general methodology to combine the power of LLMs and automated reasoners for automated program verification. We formally describe this methodology as a set of derivation rules and prove its soundness. We instantiate the calculus as a sound automated verification procedure, which led to practical improvements on a set of synthetic and competition benchmarks.
&lt;/p&gt;</description></item><item><title>TACTiS-2&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20851;&#27880;&#32852;&#21512;&#20998;&#24067;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#31616;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#32447;&#24615;&#21442;&#25968;&#25968;&#37327;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2310.01327</link><description>&lt;p&gt;
TACTiS-2&#65306;&#26356;&#22909;&#12289;&#26356;&#24555;&#12289;&#26356;&#31616;&#21333;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20851;&#27880;&#32852;&#21512;&#20998;&#24067;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
TACTiS-2: Better, Faster, Simpler Attentional Copulas for Multivariate Time Series. (arXiv:2310.01327v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2310.01327
&lt;/p&gt;
&lt;p&gt;
TACTiS-2&#26159;&#19968;&#31181;&#25913;&#36827;&#30340;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#20851;&#27880;&#32852;&#21512;&#20998;&#24067;&#27169;&#22411;&#65292;&#37319;&#29992;&#20102;&#31616;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#21644;&#32447;&#24615;&#21442;&#25968;&#25968;&#37327;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#35757;&#32451;&#21160;&#24577;&#21644;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#29992;&#20110;&#22810;&#21464;&#37327;&#27010;&#29575;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#26088;&#22312;&#28789;&#27963;&#22320;&#22788;&#29702;&#21253;&#25324;&#39044;&#27979;&#12289;&#25554;&#20540;&#21644;&#23427;&#20204;&#30340;&#32452;&#21512;&#31561;&#19968;&#31995;&#21015;&#20219;&#21153;&#12290;&#22522;&#20110;&#32852;&#21512;&#20998;&#24067;&#29702;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21270;&#30340;&#30446;&#26631;&#20989;&#25968;&#65292;&#29992;&#20110;&#26368;&#36817;&#24341;&#20837;&#30340;&#22522;&#20110;Transformer&#30340;&#20851;&#27880;&#32852;&#21512;&#20998;&#24067;&#27169;&#22411;&#65288;TACTiS&#65289;&#12290;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#30340;&#20998;&#24067;&#21442;&#25968;&#25968;&#37327;&#19982;&#21464;&#37327;&#25968;&#37327;&#21576;&#32447;&#24615;&#32780;&#38750;&#38454;&#20056;&#20851;&#31995;&#12290;&#26032;&#30340;&#30446;&#26631;&#20989;&#25968;&#38656;&#35201;&#24341;&#20837;&#19968;&#31181;&#35757;&#32451;&#35838;&#31243;&#65292;&#24182;&#19988;&#38656;&#35201;&#23545;&#21407;&#22987;&#26550;&#26500;&#36827;&#34892;&#24517;&#35201;&#30340;&#25913;&#21160;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#24471;&#21040;&#30340;&#27169;&#22411;&#20855;&#26377;&#26174;&#33879;&#25913;&#21892;&#30340;&#35757;&#32451;&#21160;&#24577;&#65292;&#24182;&#22312;&#22810;&#26679;&#30340;&#30495;&#23454;&#19990;&#30028;&#39044;&#27979;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20808;&#21069;&#24037;&#20316;&#30340;&#28789;&#27963;&#24615;&#65292;&#22914;&#26080;&#32541;&#22788;&#29702;&#19981;&#23545;&#40784;&#21644;&#37319;&#26679;&#19981;&#22343;&#21248;&#30340;&#26102;&#38388;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new model for multivariate probabilistic time series prediction, designed to flexibly address a range of tasks including forecasting, interpolation, and their combinations. Building on copula theory, we propose a simplified objective for the recently-introduced transformer-based attentional copulas (TACTiS), wherein the number of distributional parameters now scales linearly with the number of variables instead of factorially. The new objective requires the introduction of a training curriculum, which goes hand-in-hand with necessary changes to the original architecture. We show that the resulting model has significantly better training dynamics and achieves state-of-the-art performance across diverse real-world forecasting tasks, while maintaining the flexibility of prior work, such as seamless handling of unaligned and unevenly-sampled time series.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35302;&#35273;&#35835;&#25968;&#25512;&#27979;&#29289;&#20307;&#25918;&#32622;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25509;&#35302;&#21306;&#22495;&#30340;&#20272;&#35745;&#21487;&#20197;&#26377;&#25928;&#35774;&#35745;&#26426;&#22120;&#20154;&#30340;&#21453;&#39304;&#25216;&#33021;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#31934;&#32454;&#25805;&#25511;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.14552</link><description>&lt;p&gt;
&#31283;&#23450;&#25918;&#32622;&#30340;&#22806;&#37096;&#25509;&#35302;&#22359;&#30340;&#35302;&#35273;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Tactile Estimation of Extrinsic Contact Patch for Stable Placement. (arXiv:2309.14552v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.14552
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21033;&#29992;&#35302;&#35273;&#35835;&#25968;&#25512;&#27979;&#29289;&#20307;&#25918;&#32622;&#31283;&#23450;&#24615;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#25509;&#35302;&#21306;&#22495;&#30340;&#20272;&#35745;&#21487;&#20197;&#26377;&#25928;&#35774;&#35745;&#26426;&#22120;&#20154;&#30340;&#21453;&#39304;&#25216;&#33021;&#65292;&#25552;&#39640;&#26426;&#22120;&#20154;&#30340;&#31934;&#32454;&#25805;&#25511;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#26426;&#22120;&#20154;&#30340;&#31934;&#32454;&#25805;&#20316;&#25216;&#33021;&#26469;&#35828;&#65292;&#20934;&#30830;&#24863;&#30693;&#25509;&#35302;&#20132;&#20114;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20026;&#26426;&#22120;&#20154;&#35774;&#35745;&#21453;&#39304;&#25216;&#33021;&#30340;&#26041;&#27861;&#65292;&#35813;&#26426;&#22120;&#20154;&#24517;&#39035;&#23398;&#20064;&#23558;&#22797;&#26434;&#24418;&#29366;&#30340;&#29289;&#20307;&#22534;&#21472;&#22312;&#19968;&#36215;&#12290;&#20026;&#20102;&#35774;&#35745;&#36825;&#26679;&#19968;&#20010;&#31995;&#32479;&#65292;&#26426;&#22120;&#20154;&#24212;&#35813;&#33021;&#22815;&#26681;&#25454;&#38750;&#24120;&#36731;&#24494;&#30340;&#25509;&#35302;&#20132;&#20114;&#26469;&#25512;&#29702;&#25918;&#32622;&#30340;&#31283;&#23450;&#24615;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21487;&#20197;&#26681;&#25454;&#25509;&#35302;&#24418;&#25104;&#36807;&#31243;&#20013;&#30340;&#35302;&#35273;&#35835;&#25968;&#26469;&#25512;&#27979;&#29289;&#20307;&#25918;&#32622;&#30340;&#31283;&#23450;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20351;&#29992;&#21147;&#21644;&#35302;&#35273;&#35266;&#27979;&#26469;&#20272;&#35745;&#25235;&#21462;&#29289;&#20307;&#21644;&#20854;&#29615;&#22659;&#20043;&#38388;&#30340;&#25509;&#35302;&#21306;&#22495;&#65292;&#20174;&#32780;&#20272;&#35745;&#25509;&#35302;&#24418;&#25104;&#36807;&#31243;&#20013;&#29289;&#20307;&#30340;&#31283;&#23450;&#24615;&#12290;&#36825;&#31181;&#25509;&#35302;&#21306;&#22495;&#21487;&#20197;&#29992;&#26469;&#20272;&#35745;&#37322;&#25918;&#25235;&#21462;&#21518;&#29289;&#20307;&#30340;&#31283;&#23450;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#19968;&#27454;&#38750;&#24120;&#27969;&#34892;&#30340;&#26827;&#30424;&#28216;&#25103;&#20013;&#20351;&#29992;&#20102;&#22810;&#31181;&#29289;&#20307;&#23545;&#36827;&#34892;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Precise perception of contact interactions is essential for the fine-grained manipulation skills for robots. In this paper, we present the design of feedback skills for robots that must learn to stack complex-shaped objects on top of each other. To design such a system, a robot should be able to reason about the stability of placement from very gentle contact interactions. Our results demonstrate that it is possible to infer the stability of object placement based on tactile readings during contact formation between the object and its environment. In particular, we estimate the contact patch between a grasped object and its environment using force and tactile observations to estimate the stability of the object during a contact formation. The contact patch could be used to estimate the stability of the object upon the release of the grasp. The proposed method is demonstrated on various pairs of objects that are used in a very popular board game.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23376;&#22270;&#30340;&#26041;&#27861;TACO&#65292;&#29992;&#20110;&#24314;&#27169;&#39640;&#24230;&#19982;&#25299;&#25169;&#32467;&#26500;&#30456;&#20851;&#30340;&#20851;&#31995;&#20043;&#38388;&#30340;&#25299;&#25169;&#24863;&#30693;&#30456;&#20851;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23454;&#20307;&#26080;&#20851;&#30340;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2309.11528</link><description>&lt;p&gt;
&#23398;&#20064;&#20851;&#31995;&#20043;&#38388;&#30340;&#23436;&#25972;&#25299;&#25169;&#24863;&#30693;&#30456;&#20851;&#24615;&#20197;&#36827;&#34892;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Complete Topology-Aware Correlations Between Relations for Inductive Link Prediction. (arXiv:2309.11528v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.11528
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23376;&#22270;&#30340;&#26041;&#27861;TACO&#65292;&#29992;&#20110;&#24314;&#27169;&#39640;&#24230;&#19982;&#25299;&#25169;&#32467;&#26500;&#30456;&#20851;&#30340;&#20851;&#31995;&#20043;&#38388;&#30340;&#25299;&#25169;&#24863;&#30693;&#30456;&#20851;&#24615;&#65292;&#24182;&#23637;&#31034;&#20102;&#36825;&#31181;&#26041;&#27861;&#23545;&#20110;&#23454;&#20307;&#26080;&#20851;&#30340;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#8212;&#8212;&#22312;&#35757;&#32451;&#21644;&#25512;&#29702;&#38454;&#27573;&#23454;&#20307;&#21487;&#33021;&#19981;&#21516;&#8212;&#8212;&#24050;&#32463;&#26174;&#31034;&#20986;&#20102;&#20197;&#23454;&#20307;&#26080;&#20851;&#30340;&#26041;&#24335;&#23436;&#25104;&#28436;&#21270;&#30693;&#35782;&#22270;&#35889;&#30340;&#24040;&#22823;&#28508;&#21147;&#12290;&#35768;&#22810;&#27969;&#34892;&#30340;&#26041;&#27861;&#20027;&#35201;&#20851;&#27880;&#24314;&#27169;&#22270;&#32423;&#29305;&#24449;&#65292;&#32780;&#36793;&#32423;&#20132;&#20114;&#8212;&#8212;&#23588;&#20854;&#26159;&#20851;&#31995;&#20043;&#38388;&#30340;&#35821;&#20041;&#30456;&#20851;&#24615;&#8212;&#8212;&#21017;&#34987;&#36739;&#23569;&#25506;&#32034;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#27880;&#24847;&#21040;&#35821;&#20041;&#30456;&#20851;&#24615;&#20043;&#38388;&#30340;&#19968;&#20010;&#29702;&#24819;&#29305;&#24615;&#26159;&#23427;&#20204;&#22312;&#26412;&#36136;&#19978;&#26159;&#36793;&#32423;&#21644;&#23454;&#20307;&#26080;&#20851;&#30340;&#12290;&#36825;&#24847;&#21619;&#30528;&#35821;&#20041;&#30456;&#20851;&#24615;&#23545;&#20110;&#23454;&#20307;&#26080;&#20851;&#30340;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#20219;&#21153;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#23376;&#22270;&#30340;&#26041;&#27861;&#65292;&#21363;TACO&#65292;&#26469;&#24314;&#27169;&#19982;&#20854;&#23376;&#22270;&#20869;&#30340;&#25299;&#25169;&#32467;&#26500;&#39640;&#24230;&#30456;&#20851;&#30340;&#20851;&#31995;&#20043;&#38388;&#30340;&#25299;&#25169;&#24863;&#30693;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inductive link prediction -- where entities during training and inference stages can be different -- has shown great potential for completing evolving knowledge graphs in an entity-independent manner. Many popular methods mainly focus on modeling graph-level features, while the edge-level interactions -especially the semantic correlations between relations -- have been less explored. However, we notice a desirable property of semantic correlations between relations is that they are inherently edge-level and entity-independent. This implies the great potential of the semantic correlations for the entity-independent inductive link prediction task. Inspired by this observation, we propose a novel subgraph-based method, namely TACO, to model Topology-Aware COrrelations between relations that are highly correlated to their topological structures within subgraphs. Specifically, we prove that semantic correlations between any two relations can be categorized into seven topological patterns,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#30340;&#20581;&#24247;&#22768;&#26126;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;750&#20010;&#30001;&#21307;&#23398;&#19987;&#23478;&#26631;&#27880;&#30340;&#20581;&#24247;&#30456;&#20851;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#20102;&#26469;&#33258;&#20020;&#24202;&#30740;&#31350;&#30340;&#35777;&#25454;&#25903;&#25345;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#35777;&#25454;&#26816;&#32034;&#12289;&#30495;&#23454;&#24615;&#39044;&#27979;&#21644;&#35299;&#37322;&#29983;&#25104;&#12290;</title><link>http://arxiv.org/abs/2309.08503</link><description>&lt;p&gt;
HealthFC&#65306;&#19968;&#20221;&#29992;&#20110;&#22522;&#20110;&#35777;&#25454;&#30340;&#21307;&#23398;&#20107;&#23454;&#26816;&#39564;&#30340;&#20581;&#24247;&#22768;&#26126;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
HealthFC: A Dataset of Health Claims for Evidence-Based Medical Fact-Checking. (arXiv:2309.08503v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.08503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#30340;&#20581;&#24247;&#22768;&#26126;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;750&#20010;&#30001;&#21307;&#23398;&#19987;&#23478;&#26631;&#27880;&#30340;&#20581;&#24247;&#30456;&#20851;&#22768;&#26126;&#65292;&#24182;&#25552;&#20379;&#20102;&#26469;&#33258;&#20020;&#24202;&#30740;&#31350;&#30340;&#35777;&#25454;&#25903;&#25345;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#21253;&#25324;&#35777;&#25454;&#26816;&#32034;&#12289;&#30495;&#23454;&#24615;&#39044;&#27979;&#21644;&#35299;&#37322;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25968;&#23383;&#26102;&#20195;&#65292;&#36890;&#36807;&#20114;&#32852;&#32593;&#26597;&#35810;&#20581;&#24247;&#30456;&#20851;&#24314;&#35758;&#24050;&#25104;&#20026;&#19968;&#31181;&#24120;&#35265;&#20570;&#27861;&#12290;&#28982;&#32780;&#65292;&#21028;&#26029;&#22312;&#32447;&#25214;&#21040;&#30340;&#21307;&#23398;&#22768;&#26126;&#30340;&#21487;&#20449;&#24230;&#65292;&#24182;&#25214;&#21040;&#30456;&#24212;&#30340;&#35777;&#25454;&#65292;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20107;&#23454;&#26816;&#39564;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#36890;&#36807;&#21487;&#38752;&#30693;&#35782;&#26469;&#28304;&#30340;&#35777;&#25454;&#35780;&#20272;&#20107;&#23454;&#22768;&#26126;&#30495;&#23454;&#24615;&#30340;&#26041;&#27861;&#12290;&#20026;&#20102;&#25512;&#21160;&#27492;&#20219;&#21153;&#30340;&#33258;&#21160;&#21270;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20221;&#26032;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20102;750&#20010;&#20581;&#24247;&#30456;&#20851;&#22768;&#26126;&#65292;&#22312;&#21487;&#20449;&#24230;&#26041;&#38754;&#30001;&#21307;&#23398;&#19987;&#23478;&#36827;&#34892;&#20102;&#26631;&#27880;&#65292;&#24182;&#25552;&#20379;&#20102;&#26469;&#33258;&#36866;&#24403;&#30340;&#20020;&#24202;&#30740;&#31350;&#30340;&#35777;&#25454;&#25903;&#25345;&#12290;&#25105;&#20204;&#23545;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20998;&#26512;&#65292;&#31361;&#20986;&#20854;&#29305;&#28857;&#21644;&#25361;&#25112;&#12290;&#35813;&#25968;&#25454;&#38598;&#21487;&#29992;&#20110;&#19982;&#33258;&#21160;&#20107;&#23454;&#26816;&#39564;&#30456;&#20851;&#30340;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#65292;&#22914;&#35777;&#25454;&#26816;&#32034;&#12289;&#30495;&#23454;&#24615;&#39044;&#27979;&#21644;&#35299;&#37322;&#29983;&#25104;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#22522;&#20110;&#19981;&#21516;&#26041;&#27861;&#30340;&#22522;&#32447;&#27169;&#22411;&#65292;&#23545;&#23427;&#20204;&#30340;&#24615;&#33021;&#36827;&#34892;&#20102;&#30740;&#31350;&#65292;&#24182;&#35752;&#35770;&#20102;&#30740;&#31350;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seeking health-related advice on the internet has become a common practice in the digital era. Determining the trustworthiness of medical claims found online and finding appropriate evidence for this information is increasingly challenging. Fact-checking has emerged as an approach to assess the veracity of factual claims using evidence from credible knowledge sources. To help advance the automation of this task, in this paper, we introduce a novel dataset of 750 health-related claims, labeled for veracity by medical experts and backed with evidence from appropriate clinical studies. We provide an analysis of the dataset, highlighting its characteristics and challenges. The dataset can be used for Machine Learning tasks related to automated fact-checking such as evidence retrieval, veracity prediction, and explanation generation. For this purpose, we provide baseline models based on different approaches, examine their performance, and discuss the findings.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#26469;&#35299;&#20915;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2309.01069</link><description>&lt;p&gt;
&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Separable Hamiltonian Neural Networks. (arXiv:2309.01069v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2309.01069
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#20171;&#32461;&#20102;&#21487;&#20998;&#31163;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#30340;&#24212;&#29992;&#65292;&#23427;&#36890;&#36807;&#23884;&#20837;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#26469;&#35299;&#20915;&#39640;&#32500;&#21704;&#23494;&#39039;&#31995;&#32479;&#20013;&#30340;&#22797;&#26434;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#25955;&#35266;&#27979;&#25968;&#25454;&#24314;&#27169;&#21160;&#21147;&#31995;&#32479;&#26159;&#29616;&#20195;&#31185;&#23398;&#21644;&#24037;&#31243;&#25968;&#25454;&#31995;&#32479;&#38754;&#20020;&#30340;&#25361;&#25112;&#20043;&#19968;&#12290; &#21704;&#23494;&#39039;&#31995;&#32479;&#26159;&#19968;&#31867;&#22522;&#26412;&#19988;&#24191;&#27867;&#23384;&#22312;&#30340;&#21160;&#21147;&#31995;&#32479;&#12290; &#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#26159;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#65292;&#21487;&#20197;&#22312;&#27721;&#23494;&#23572;&#39039;&#26041;&#31243;&#30340;&#23398;&#20064;&#20559;&#24046;&#19979;&#65292;&#20174;&#31163;&#25955;&#35266;&#27979;&#30340;&#21521;&#37327;&#22330;&#20013;&#26080;&#30417;&#30563;&#22320;&#22238;&#24402;&#21160;&#21147;&#31995;&#32479;&#30340;&#21704;&#23494;&#39039;&#37327;&#12290;&#28982;&#32780;&#65292;&#21704;&#23494;&#39039;&#21160;&#21147;&#23398;&#36890;&#24120;&#24456;&#22797;&#26434;&#65292;&#29305;&#21035;&#26159;&#22312;&#39640;&#32500;&#24773;&#20917;&#19979;&#65292;&#20854;&#20013;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#29366;&#24577;&#31354;&#38388;&#30456;&#23545;&#20110;&#26679;&#26412;&#25968;&#37327;&#26159;&#24456;&#22823;&#30340;&#12290; &#26368;&#36817;&#21457;&#29616;&#30340;&#19968;&#31181;&#32531;&#35299;&#29366;&#24577;&#21464;&#37327;&#20043;&#38388;&#22797;&#26434;&#24615;&#30340;&#26041;&#27861;&#26159;&#21033;&#29992;&#21704;&#23494;&#39039;&#31995;&#32479;&#30340;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#65292;&#24182;&#23558;&#35813;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#23884;&#20837;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#20013;&#12290;&#26681;&#25454;&#29289;&#29702;&#23398;&#39537;&#21160;&#30340;&#26426;&#22120;&#23398;&#20064;&#30340;&#26415;&#35821;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#21487;&#20998;&#31163;&#30340;&#21704;&#23494;&#39039;&#31070;&#32463;&#32593;&#32476;&#12290;&#36825;&#20123;&#27169;&#22411;&#23884;&#20837;&#20102;&#21487;&#21152;&#24615;&#20998;&#31163;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The modelling of dynamical systems from discrete observations is a challenge faced by modern scientific and engineering data systems. Hamiltonian systems are one such fundamental and ubiquitous class of dynamical systems. Hamiltonian neural networks are state-of-the-art models that unsupervised-ly regress the Hamiltonian of a dynamical system from discrete observations of its vector field under the learning bias of Hamilton's equations. Yet Hamiltonian dynamics are often complicated, especially in higher dimensions where the state space of the Hamiltonian system is large relative to the number of samples. A recently discovered remedy to alleviate the complexity between state variables in the state space is to leverage the additive separability of the Hamiltonian system and embed that additive separability into the Hamiltonian neural network. Following the nomenclature of physics-informed machine learning, we propose three separable Hamiltonian neural networks. These models embed additi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;DL&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;RL&#30340;&#25506;&#32034;&#24615;&#36136;&#65292;&#25552;&#39640;&#20102;&#36712;&#36857;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#22312;&#30495;&#23454;&#31995;&#32479;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2308.15991</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#33258;&#20027;&#39550;&#39542;&#20013;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#30340;&#36712;&#36857;&#36319;&#36394;
&lt;/p&gt;
&lt;p&gt;
DRL-Based Trajectory Tracking for Motion-Related Modules in Autonomous Driving. (arXiv:2308.15991v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.15991
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#30340;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;DL&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;RL&#30340;&#25506;&#32034;&#24615;&#36136;&#65292;&#25552;&#39640;&#20102;&#36712;&#36857;&#36319;&#36394;&#30340;&#20934;&#30830;&#24615;&#21644;&#31283;&#20581;&#24615;&#65292;&#22312;&#30495;&#23454;&#31995;&#32479;&#20013;&#20855;&#26377;&#36739;&#22909;&#30340;&#36866;&#24212;&#24615;&#21644;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#24635;&#26159;&#24314;&#31435;&#22312;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#65288;&#22914;&#35268;&#21010;&#22120;&#21644;&#25511;&#21046;&#22120;&#65289;&#20043;&#19978;&#12290;&#20934;&#30830;&#32780;&#31283;&#20581;&#30340;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#23545;&#20110;&#36825;&#20123;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#26469;&#35828;&#26159;&#19981;&#21487;&#25110;&#32570;&#30340;&#21407;&#22987;&#20363;&#31243;&#12290;&#24403;&#21069;&#30340;&#26041;&#27861;&#24448;&#24448;&#23545;&#27169;&#22411;&#65288;&#22914;&#19978;&#19979;&#25991;&#21644;&#21160;&#21147;&#23398;&#65289;&#20570;&#20986;&#20102;&#24378;&#28872;&#30340;&#20551;&#35774;&#65292;&#36825;&#20123;&#20551;&#35774;&#19981;&#36275;&#20197;&#24212;&#23545;&#30495;&#23454;&#31995;&#32479;&#20013;&#30340;&#22330;&#26223;&#21464;&#21270;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;DRL&#65289;&#30340;&#33258;&#20027;&#39550;&#39542;&#31995;&#32479;&#20013;&#36816;&#21160;&#30456;&#20851;&#27169;&#22359;&#30340;&#36712;&#36857;&#36319;&#36394;&#26041;&#27861;&#12290;DL&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#21644;RL&#30340;&#25506;&#32034;&#24615;&#36136;&#26082;&#24102;&#26469;&#20102;&#24378;&#20581;&#24615;&#21448;&#25552;&#39640;&#20102;&#20934;&#30830;&#24615;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#23427;&#36890;&#36807;&#20197;&#26080;&#27169;&#22411;&#21644;&#25968;&#25454;&#39537;&#21160;&#30340;&#26041;&#24335;&#36816;&#34892;&#36712;&#36857;&#36319;&#36394;&#26469;&#22686;&#24378;&#20102;&#36890;&#29992;&#24615;&#12290;&#36890;&#36807;&#22823;&#37327;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30456;&#27604;&#24403;&#21069;&#26041;&#27861;&#22312;&#25928;&#29575;&#21644;&#25928;&#26524;&#19978;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous driving systems are always built on motion-related modules such as the planner and the controller. An accurate and robust trajectory tracking method is indispensable for these motion-related modules as a primitive routine. Current methods often make strong assumptions about the model such as the context and the dynamics, which are not robust enough to deal with the changing scenarios in a real-world system. In this paper, we propose a Deep Reinforcement Learning (DRL)-based trajectory tracking method for the motion-related modules in autonomous driving systems. The representation learning ability of DL and the exploration nature of RL bring strong robustness and improve accuracy. Meanwhile, it enhances versatility by running the trajectory tracking in a model-free and data-driven manner. Through extensive experiments, we demonstrate both the efficiency and effectiveness of our method compared to current methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#32858;&#21512;&#22120;&#29992;&#20110;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#19979;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#21183;&#65292;&#24182;&#23545;&#38750;i.i.d&#25968;&#25454;&#36827;&#34892;&#20102;&#25193;&#23637;&#20998;&#26512;&#12290;</title><link>http://arxiv.org/abs/2308.12581</link><description>&lt;p&gt;
&#19968;&#31181;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#26041;&#27861;&#29992;&#20110;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
A Huber Loss Minimization Approach to Byzantine Robust Federated Learning. (arXiv:2308.12581v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.12581
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#32858;&#21512;&#22120;&#29992;&#20110;&#25308;&#21344;&#24237;&#40065;&#26834;&#30340;&#32852;&#37030;&#23398;&#20064;&#12290;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#20551;&#35774;&#19979;&#65292;&#35813;&#26041;&#27861;&#20855;&#26377;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#30340;&#20248;&#21183;&#65292;&#24182;&#23545;&#38750;i.i.d&#25968;&#25454;&#36827;&#34892;&#20102;&#25193;&#23637;&#20998;&#26512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#31995;&#32479;&#23481;&#26131;&#21463;&#21040;&#23545;&#25239;&#25915;&#20987;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#22522;&#20110;Huber&#25439;&#22833;&#26368;&#23567;&#21270;&#30340;&#26032;&#22411;&#32858;&#21512;&#22120;&#65292;&#24182;&#25552;&#20379;&#20102;&#20840;&#38754;&#30340;&#29702;&#35770;&#20998;&#26512;&#12290;&#22312;&#29420;&#31435;&#21516;&#20998;&#24067;&#65288;i.i.d&#65289;&#20551;&#35774;&#19979;&#65292;&#19982;&#29616;&#26377;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20960;&#20010;&#20248;&#21183;&#12290;&#39318;&#20808;&#65292;&#23427;&#23545;&#20110;&#34987;&#25915;&#20987;&#23458;&#25143;&#31471;&#27604;&#29575;$\epsilon$&#20855;&#26377;&#26368;&#20248;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#23545;$\epsilon$&#26377;&#31934;&#30830;&#30340;&#30693;&#35782;&#12290;&#31532;&#19977;&#65292;&#23427;&#20801;&#35768;&#19981;&#21516;&#30340;&#23458;&#25143;&#31471;&#20855;&#26377;&#19981;&#22343;&#31561;&#30340;&#25968;&#25454;&#22823;&#23567;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23558;&#20998;&#26512;&#25193;&#23637;&#21040;&#21253;&#25324;&#38750;i.i.d&#25968;&#25454;&#65292;&#36825;&#24847;&#21619;&#30528;&#23458;&#25143;&#31471;&#20855;&#26377;&#30053;&#26377;&#19981;&#21516;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning systems are susceptible to adversarial attacks. To combat this, we introduce a novel aggregator based on Huber loss minimization, and provide a comprehensive theoretical analysis. Under independent and identically distributed (i.i.d) assumption, our approach has several advantages compared to existing methods. Firstly, it has optimal dependence on $\epsilon$, which stands for the ratio of attacked clients. Secondly, our approach does not need precise knowledge of $\epsilon$. Thirdly, it allows different clients to have unequal data sizes. We then broaden our analysis to include non-i.i.d data, such that clients have slightly different distributions.
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20132;&#21449;&#24615;&#21644;&#21452;&#37325;&#26799;&#24230;&#19979;&#38477;&#22312;&#22810;&#27169;&#24577;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#20026;&#20363;&#12290;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#20998;&#26512;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#21487;&#20197;&#25581;&#31034;&#20854;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#20171;&#32461;&#20102;&#20132;&#21449;&#24615;&#21644;&#27169;&#24577;&#30340;&#26799;&#24230;&#27880;&#24847;&#21147;&#30340;&#25688;&#35201;&#21270;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2308.11585</link><description>&lt;p&gt;
&#22240;&#26524;&#20132;&#21449;&#24615;&#21644;&#21452;&#37325;&#26799;&#24230;&#19979;&#38477;&#22312;&#22810;&#27169;&#24577;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#20197;&#20167;&#24680;&#36855;&#22240;&#20026;&#20363;&#65288;arXiv:2308.11585v1 [cs.AI]&#65289;
&lt;/p&gt;
&lt;p&gt;
Causal Intersectionality and Dual Form of Gradient Descent for Multimodal Analysis: a Case Study on Hateful Memes. (arXiv:2308.11585v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11585
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25506;&#35752;&#20102;&#22240;&#26524;&#20132;&#21449;&#24615;&#21644;&#21452;&#37325;&#26799;&#24230;&#19979;&#38477;&#22312;&#22810;&#27169;&#24577;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65292;&#20197;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#20026;&#20363;&#12290;&#36890;&#36807;&#32467;&#21512;&#22240;&#26524;&#20998;&#26512;&#21644;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#21457;&#29616;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#21487;&#20197;&#25581;&#31034;&#20854;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#20171;&#32461;&#20102;&#20132;&#21449;&#24615;&#21644;&#27169;&#24577;&#30340;&#26799;&#24230;&#27880;&#24847;&#21147;&#30340;&#25688;&#35201;&#21270;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#30340;&#29190;&#28856;&#24615;&#22686;&#38271;&#65292;&#29305;&#21035;&#26159;&#22312;&#26032;&#20852;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#32972;&#26223;&#19979;&#65292;&#29702;&#35299;&#20854;&#20869;&#37096;&#24037;&#20316;&#20013;&#30340;&#35821;&#20041;&#24847;&#20041;&#33267;&#20851;&#37325;&#35201;&#12290;&#34429;&#28982;&#22240;&#26524;&#20998;&#26512;&#20391;&#37325;&#20110;&#23450;&#20041;&#35821;&#20041;&#21450;&#20854;&#37327;&#21270;&#65292;&#22522;&#20110;&#26799;&#24230;&#30340;&#26041;&#27861;&#26159;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#30340;&#26680;&#24515;&#65292;&#29992;&#20110;&#35299;&#37322;&#40657;&#30418;&#23376;&#30340;&#35299;&#37322;&#12290;&#36890;&#36807;&#21327;&#21516;&#36825;&#20123;&#26041;&#27861;&#65292;&#25506;&#32034;&#27169;&#22411;&#30340;&#20869;&#37096;&#26426;&#21046;&#22914;&#20309;&#38416;&#26126;&#20854;&#22240;&#26524;&#25928;&#24212;&#24050;&#25104;&#20026;&#22522;&#20110;&#35777;&#25454;&#30340;&#20915;&#31574;&#30340;&#24517;&#35201;&#26465;&#20214;&#12290;&#19968;&#31995;&#21015;&#24182;&#34892;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20132;&#21449;&#24615;--&#20010;&#20307;&#30340;&#22810;&#20010;&#20154;&#21475;&#32479;&#35745;&#23398;&#22240;&#32032;&#30340;&#32452;&#21512;&#24433;&#21709;--&#21487;&#20197;&#20197;&#24179;&#22343;&#22788;&#29702;&#25928;&#24212;&#65288;ATE&#65289;&#30340;&#24418;&#24335;&#36827;&#34892;&#32467;&#26500;&#21270;&#12290;&#26368;&#21021;&#65292;&#26412;&#30740;&#31350;&#38416;&#36848;&#20102;&#20167;&#24680;&#36855;&#22240;&#26816;&#27979;&#38382;&#39064;&#21487;&#20197;&#20316;&#20026;&#19968;&#20010;ATE&#26469;&#25551;&#36848;&#65292;&#20511;&#21161;&#20132;&#21449;&#24615;&#21407;&#21017;&#65292;&#20197;&#21450;&#22522;&#20110;&#27169;&#24577;&#30340;&#26799;&#24230;&#27880;&#24847;&#21147;&#30340;&#25688;&#35201;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the wake of the explosive growth of machine learning (ML) usage, particularly within the context of emerging Large Language Models (LLMs), comprehending the semantic significance rooted in their internal workings is crucial. While causal analyses focus on defining semantics and its quantification, the gradient-based approach is central to explainable AI (XAI), tackling the interpretation of the black box. By synergizing these approaches, the exploration of how a model's internal mechanisms illuminate its causal effect has become integral for evidence-based decision-making. A parallel line of research has revealed that intersectionality - the combinatory impact of multiple demographics of an individual - can be structured in the form of an Averaged Treatment Effect (ATE). Initially, this study illustrates that the hateful memes detection problem can be formulated as an ATE, assisted by the principles of intersectionality, and that a modality-wise summarization of gradient-based atten
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;</title><link>http://arxiv.org/abs/2308.11432</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.11432
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#32508;&#36848;&#20102;&#22522;&#20110;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#65292;&#25552;&#20379;&#20102;&#20174;&#25972;&#20307;&#35282;&#24230;&#23545;&#35813;&#39046;&#22495;&#30340;&#31995;&#32479;&#23457;&#26597;&#65292;&#20854;&#21019;&#26032;&#20043;&#22788;&#22312;&#20110;&#21033;&#29992;&#22823;&#37327;&#32593;&#32476;&#30693;&#35782;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#30340;&#26234;&#33021;&#20915;&#31574;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#20195;&#29702;&#38271;&#26399;&#20197;&#26469;&#19968;&#30452;&#26159;&#23398;&#26415;&#30028;&#30340;&#30740;&#31350;&#28909;&#28857;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#24448;&#24448;&#38598;&#20013;&#22312;&#23545;&#26377;&#38480;&#30693;&#35782;&#30340;&#20195;&#29702;&#36827;&#34892;&#35757;&#32451;&#65292;&#32780;&#36825;&#19982;&#20154;&#31867;&#30340;&#23398;&#20064;&#36807;&#31243;&#23384;&#22312;&#26126;&#26174;&#24046;&#24322;&#65292;&#22240;&#27492;&#24456;&#38590;&#23454;&#29616;&#20154;&#31867;&#33324;&#30340;&#20915;&#31574;&#12290;&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#33719;&#21462;&#22823;&#37327;&#30340;&#32593;&#32476;&#30693;&#35782;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#29616;&#20986;&#20102;&#23454;&#29616;&#20154;&#31867;&#27700;&#24179;&#26234;&#33021;&#30340;&#26174;&#33879;&#28508;&#21147;&#12290;&#36825;&#24341;&#21457;&#20102;&#23545;&#22522;&#20110;LLM&#30340;&#33258;&#20027;&#20195;&#29702;&#30340;&#30740;&#31350;&#30340;&#39640;&#28072;&#20852;&#36259;&#12290;&#20026;&#20102;&#21457;&#25381;LLM&#30340;&#20840;&#37096;&#28508;&#21147;&#65292;&#30740;&#31350;&#20154;&#21592;&#35774;&#35745;&#20102;&#21508;&#31181;&#19981;&#21516;&#24212;&#29992;&#30340;&#20195;&#29702;&#20307;&#31995;&#32467;&#26500;&#12290;&#26412;&#35770;&#25991;&#32508;&#36848;&#20102;&#36825;&#20123;&#30740;&#31350;&#65292;&#20174;&#25972;&#20307;&#30340;&#35282;&#24230;&#23545;&#33258;&#20027;&#20195;&#29702;&#39046;&#22495;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#23457;&#26597;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#37325;&#28857;&#26159;&#22522;&#20110;LLM&#30340;&#20195;&#29702;&#26500;&#24314;&#65292;&#20026;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous agents have long been a prominent research topic in the academic community. Previous research in this field often focuses on training agents with limited knowledge within isolated environments, which diverges significantly from the human learning processes, and thus makes the agents hard to achieve human-like decisions. Recently, through the acquisition of vast amounts of web knowledge, large language models (LLMs) have demonstrated remarkable potential in achieving human-level intelligence. This has sparked an upsurge in studies investigating autonomous agents based on LLMs. To harness the full potential of LLMs, researchers have devised diverse agent architectures tailored to different applications. In this paper, we present a comprehensive survey of these studies, delivering a systematic review of the field of autonomous agents from a holistic perspective. More specifically, our focus lies in the construction of LLM-based agents, for which we propose a unified framework t
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#30340;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30740;&#31350;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#19981;&#21512;&#29702;&#30340;&#23454;&#20307;&#27809;&#26377;&#25490;&#21517;&#21644;&#21482;&#32771;&#34385;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36335;&#24452;&#26159;&#24433;&#21709;&#22240;&#32032;&#12290;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35268;&#21017;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#25509;&#36817;&#20110;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;NBFNet&#12290;&#36825;&#20123;&#21464;&#20307;&#20165;&#20351;&#29992;&#20102;NBFNet&#25152;&#20381;&#36182;&#30340;&#35777;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;</title><link>http://arxiv.org/abs/2308.07942</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#30340;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Inductive Knowledge Graph Completion with GNNs and Rules: An Analysis. (arXiv:2308.07942v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2308.07942
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#21644;&#35268;&#21017;&#30340;&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30740;&#31350;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#22312;&#23454;&#36341;&#20013;&#30340;&#34920;&#29616;&#19981;&#20339;&#30340;&#21407;&#22240;&#65292;&#21457;&#29616;&#19981;&#21512;&#29702;&#30340;&#23454;&#20307;&#27809;&#26377;&#25490;&#21517;&#21644;&#21482;&#32771;&#34385;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36335;&#24452;&#26159;&#24433;&#21709;&#22240;&#32032;&#12290;&#25552;&#20986;&#20102;&#19968;&#20123;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#30340;&#35268;&#21017;&#26041;&#27861;&#30340;&#21464;&#20307;&#65292;&#21457;&#29616;&#20854;&#24615;&#33021;&#25509;&#36817;&#20110;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#26041;&#27861;NBFNet&#12290;&#36825;&#20123;&#21464;&#20307;&#20165;&#20351;&#29992;&#20102;NBFNet&#25152;&#20381;&#36182;&#30340;&#35777;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24402;&#32435;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30340;&#20219;&#21153;&#35201;&#27714;&#27169;&#22411;&#20174;&#35757;&#32451;&#22270;&#35889;&#20013;&#23398;&#20064;&#25512;&#29702;&#27169;&#24335;&#65292;&#28982;&#21518;&#21487;&#20197;&#29992;&#26469;&#22312;&#20998;&#31163;&#30340;&#27979;&#35797;&#22270;&#35889;&#19978;&#36827;&#34892;&#39044;&#27979;&#12290;&#34429;&#28982;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#20284;&#20046;&#24456;&#36866;&#21512;&#36825;&#20010;&#20219;&#21153;&#65292;&#20294;&#22312;&#23454;&#36341;&#20013;&#65292;&#23427;&#20204;&#30340;&#34920;&#29616;&#26126;&#26174;&#19981;&#22914;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNNs&#65289;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#22914;NBFNet&#12290;&#25105;&#20204;&#20551;&#35774;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#34920;&#29616;&#19981;&#20339;&#26159;&#30001;&#20110;&#20004;&#20010;&#22240;&#32032;&#65306;&#65288;i&#65289;&#19981;&#21512;&#29702;&#30340;&#23454;&#20307;&#26681;&#26412;&#27809;&#26377;&#25490;&#21517;&#65292;&#65288;ii&#65289;&#22312;&#30830;&#23450;&#32473;&#23450;&#38142;&#25509;&#39044;&#27979;&#31572;&#26696;&#30340;&#32622;&#20449;&#24230;&#26102;&#65292;&#21482;&#32771;&#34385;&#20102;&#26368;&#20855;&#20449;&#24687;&#37327;&#30340;&#36335;&#24452;&#12290;&#20026;&#20102;&#20998;&#26512;&#36825;&#20123;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#20123;&#38024;&#23545;&#19978;&#36848;&#38382;&#39064;&#30340;&#35268;&#21017;&#26041;&#27861;&#30340;&#21464;&#20307;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#24471;&#21040;&#30340;&#27169;&#22411;&#30340;&#24615;&#33021;&#25509;&#36817;NBFNet&#12290;&#33267;&#20851;&#37325;&#35201;&#30340;&#26159;&#65292;&#32771;&#34385;&#21040;&#30340;&#21464;&#20307;&#21482;&#20351;&#29992;&#20102;NBFNet&#25152;&#20381;&#36182;&#30340;&#35777;&#25454;&#30340;&#19968;&#23567;&#37096;&#20998;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of inductive knowledge graph completion requires models to learn inference patterns from a training graph, which can then be used to make predictions on a disjoint test graph. Rule-based methods seem like a natural fit for this task, but in practice they significantly underperform state-of-the-art methods based on Graph Neural Networks (GNNs), such as NBFNet. We hypothesise that the underperformance of rule-based methods is due to two factors: (i) implausible entities are not ranked at all and (ii) only the most informative path is taken into account when determining the confidence in a given link prediction answer. To analyse the impact of these factors, we study a number of variants of a rule-based approach, which are specifically aimed at addressing the aforementioned issues. We find that the resulting models can achieve a performance which is close to that of NBFNet. Crucially, the considered variants only use a small fraction of the evidence that NBFNet relies on, which m
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;MovieLens&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29992;&#25143;&#19982;&#35813;&#24179;&#21488;&#30340;&#20132;&#20114;&#22312;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#29992;&#25143;&#20132;&#20114;&#21463;&#21040;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#25512;&#33616;&#30340;&#20505;&#36873;&#30005;&#24433;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.09985</link><description>&lt;p&gt;
&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;MovieLens&#19978;&#21462;&#24471;&#20102;&#20986;&#33394;&#30340;&#34920;&#29616;&#65306;&#36825;&#24847;&#21619;&#30528;&#20160;&#20040;&#65311;
&lt;/p&gt;
&lt;p&gt;
Our Model Achieves Excellent Performance on MovieLens: What Does it Mean?. (arXiv:2307.09985v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.09985
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#36890;&#36807;&#23545;MovieLens&#25968;&#25454;&#38598;&#30340;&#20998;&#26512;&#65292;&#21457;&#29616;&#29992;&#25143;&#19982;&#35813;&#24179;&#21488;&#30340;&#20132;&#20114;&#22312;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#65292;&#24182;&#19988;&#29992;&#25143;&#20132;&#20114;&#21463;&#21040;&#24179;&#21488;&#25512;&#33616;&#31639;&#27861;&#25512;&#33616;&#30340;&#20505;&#36873;&#30005;&#24433;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#35780;&#20272;&#30340;&#20856;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#26159;&#22312;&#26576;&#19968;&#26102;&#38388;&#27573;&#20869;&#22312;&#24179;&#21488;&#19978;&#29983;&#25104;&#30340;&#29992;&#25143;-&#29289;&#21697;&#20132;&#20114;&#25968;&#25454;&#12290;&#20132;&#20114;&#29983;&#25104;&#26426;&#21046;&#37096;&#20998;&#35299;&#37322;&#20102;&#20026;&#20160;&#20040;&#29992;&#25143;&#19982;&#29289;&#21697;&#36827;&#34892;&#20132;&#20114;&#65288;&#22914;&#21916;&#27426;&#12289;&#36141;&#20080;&#12289;&#35780;&#20998;&#65289;&#20197;&#21450;&#29305;&#23450;&#20132;&#20114;&#21457;&#29983;&#30340;&#32972;&#26223;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23545;MovieLens&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#32454;&#33268;&#30340;&#20998;&#26512;&#65292;&#24182;&#35299;&#37322;&#20102;&#20351;&#29992;&#35813;&#25968;&#25454;&#38598;&#36827;&#34892;&#35780;&#20272;&#25512;&#33616;&#31639;&#27861;&#26102;&#21487;&#33021;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#20174;&#20998;&#26512;&#20013;&#24471;&#20986;&#20102;&#19968;&#20123;&#20027;&#35201;&#21457;&#29616;&#12290;&#39318;&#20808;&#65292;&#22312;&#29992;&#25143;&#19982;MovieLens&#24179;&#21488;&#20132;&#20114;&#30340;&#19981;&#21516;&#38454;&#27573;&#23384;&#22312;&#26174;&#33879;&#24046;&#24322;&#12290;&#26089;&#26399;&#20132;&#20114;&#22312;&#24456;&#22823;&#31243;&#24230;&#19978;&#23450;&#20041;&#20102;&#29992;&#25143;&#30011;&#20687;&#65292;&#24433;&#21709;&#20102;&#21518;&#32493;&#30340;&#20132;&#20114;&#12290;&#20854;&#27425;&#65292;&#29992;&#25143;&#20132;&#20114;&#21463;&#21040;&#24179;&#21488;&#20869;&#37096;&#25512;&#33616;&#31639;&#27861;&#25512;&#33616;&#30340;&#20505;&#36873;&#30005;&#24433;&#30340;&#24456;&#22823;&#24433;&#21709;&#12290;&#21024;&#38500;&#38752;&#36817;&#26368;&#21518;&#20960;&#27425;&#20132;&#20114;&#30340;&#20132;&#20114;&#20250;&#23545;&#32467;&#26524;&#20135;&#29983;&#36739;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
A typical benchmark dataset for recommender system (RecSys) evaluation consists of user-item interactions generated on a platform within a time period. The interaction generation mechanism partially explains why a user interacts with (e.g.,like, purchase, rate) an item, and the context of when a particular interaction happened. In this study, we conduct a meticulous analysis on the MovieLens dataset and explain the potential impact on using the dataset for evaluating recommendation algorithms. We make a few main findings from our analysis. First, there are significant differences in user interactions at the different stages when a user interacts with the MovieLens platform. The early interactions largely define the user portrait which affect the subsequent interactions. Second, user interactions are highly affected by the candidate movies that are recommended by the platform's internal recommendation algorithm(s). Removal of interactions that happen nearer to the last few interactions 
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2307.02591</link><description>&lt;p&gt;
ODD: &#19968;&#20221;&#22522;&#20110;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#26816;&#27979;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
ODD: A Benchmark Dataset for the NLP-based Opioid Related Aberrant Behavior Detection. (arXiv:2307.02591v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.02591
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20221;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#36890;&#36807;&#20998;&#26512;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#65292;&#26816;&#27979;&#21644;&#20998;&#31867;&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#22312;&#33647;&#29289;&#30456;&#20851;&#30149;&#20363;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30740;&#31350;&#20013;&#20855;&#26377;&#37325;&#35201;&#30340;&#21019;&#26032;&#21644;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33647;&#29289;&#28389;&#29992;&#24322;&#24120;&#34892;&#20026;&#65288;ORAB&#65289;&#26159;&#38450;&#27490;&#33647;&#29289;&#36807;&#37327;&#30340;&#26032;&#39118;&#38505;&#22240;&#32032;&#12290;&#20197;&#24448;&#65292;ORAB&#20027;&#35201;&#36890;&#36807;&#35843;&#26597;&#32467;&#26524;&#21644;&#33647;&#29289;&#32473;&#20104;&#30417;&#27979;&#36827;&#34892;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#26041;&#27861;&#26080;&#27861;&#25193;&#23637;&#65292;&#24182;&#19981;&#33021;&#28085;&#30422;&#25152;&#26377;&#24322;&#24120;&#34892;&#20026;&#30340;&#33539;&#22260;&#12290;&#28982;&#32780;&#65292;ORAB&#22312;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#24191;&#27867;&#26377;&#35760;&#24405;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;ODD&#30340;&#26032;&#22411;&#29983;&#29289;&#21307;&#23398;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;ORAB&#26816;&#27979;&#12290;ODD&#26159;&#19968;&#20010;&#19987;&#23478;&#27880;&#37322;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;750&#22810;&#20010;&#20844;&#24320;&#21487;&#29992;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#12290;ODD&#26088;&#22312;&#20174;&#24739;&#32773;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#31508;&#35760;&#20013;&#35782;&#21035;ORAB&#65292;&#24182;&#23558;&#20854;&#20998;&#31867;&#20026;&#20061;&#20010;&#31867;&#21035;&#65306;1&#65289;&#24050;&#30830;&#35748;&#24322;&#24120;&#34892;&#20026;&#65292;2&#65289;&#26263;&#31034;&#30340;&#24322;&#24120;&#34892;&#20026;&#65292;3&#65289;&#38463;&#29255;&#31867;&#33647;&#29289;&#65292;4&#65289;&#36866;&#24212;&#30151;&#65292;5&#65289;&#24050;&#35786;&#26029;&#30340;&#38463;&#29255;&#21046;&#21058;&#20381;&#36182;&#65292;6&#65289;&#33519;&#20108;&#27694;&#24179;&#31867;&#33647;&#29289;&#65292;7&#65289;&#33647;&#29289;&#21464;&#21270;&#65292;8&#65289;&#19982;&#20013;&#26530;&#31070;&#32463;&#31995;&#32479;&#30456;&#20851;&#65292;9&#65289;&#31038;&#20250;&#20581;&#24247;&#20915;&#23450;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Opioid related aberrant behaviors (ORAB) present novel risk factors for opioid overdose. Previously, ORAB have been mainly assessed by survey results and by monitoring drug administrations. Such methods however, cannot scale up and do not cover the entire spectrum of aberrant behaviors. On the other hand, ORAB are widely documented in electronic health record notes. This paper introduces a novel biomedical natural language processing benchmark dataset named ODD, for ORAB Detection Dataset. ODD is an expert-annotated dataset comprising of more than 750 publicly available EHR notes. ODD has been designed to identify ORAB from patients' EHR notes and classify them into nine categories; 1) Confirmed Aberrant Behavior, 2) Suggested Aberrant Behavior, 3) Opioids, 4) Indication, 5) Diagnosed opioid dependency, 6) Benzodiapines, 7) Medication Changes, 8) Central Nervous System-related, and 9) Social Determinants of Health. We explored two state-of-the-art natural language processing (NLP) mode
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#32463;&#36807;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#30340;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.16788</link><description>&lt;p&gt;
&#31232;&#30095;&#27169;&#22411;&#27748;&#65306;&#36890;&#36807;&#27169;&#22411;&#24179;&#22343;&#25913;&#36827;&#20462;&#21098;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Sparse Model Soups: A Recipe for Improved Pruning via Model Averaging. (arXiv:2306.16788v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16788
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#22810;&#20010;&#32463;&#36807;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#30340;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#65292;&#35299;&#20915;&#20102;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#30340;&#38382;&#39064;&#65292;&#24182;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#36890;&#36807;&#21098;&#26525;&#26174;&#33879;&#21387;&#32553;&#65292;&#20174;&#32780;&#24471;&#21040;&#31232;&#30095;&#27169;&#22411;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#26356;&#23569;&#30340;&#23384;&#20648;&#21644;&#28014;&#28857;&#36816;&#31639;&#65292;&#21516;&#26102;&#20445;&#25345;&#39044;&#27979;&#24615;&#33021;&#12290;&#27169;&#22411;&#27748;&#65288;Wortsman&#31561;&#20154;&#65292;2022&#24180;&#65289;&#36890;&#36807;&#23558;&#22810;&#20010;&#27169;&#22411;&#30340;&#21442;&#25968;&#24179;&#22343;&#25104;&#19968;&#20010;&#21333;&#19968;&#27169;&#22411;&#26469;&#25913;&#21892;&#27867;&#21270;&#21644;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#65292;&#32780;&#19981;&#22686;&#21152;&#25512;&#29702;&#26102;&#38388;&#12290;&#28982;&#32780;&#65292;&#35782;&#21035;&#22788;&#20110;&#30456;&#21516;&#25439;&#22833;&#21306;&#22495;&#30340;&#27169;&#22411;&#20197;&#21516;&#26102;&#21033;&#29992;&#31232;&#30095;&#24615;&#21644;&#21442;&#25968;&#24179;&#22343;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#22240;&#20026;&#23545;&#20219;&#24847;&#31232;&#30095;&#27169;&#22411;&#36827;&#34892;&#24179;&#22343;&#20250;&#38477;&#20302;&#25972;&#20307;&#31232;&#30095;&#24230;&#65292;&#21407;&#22240;&#26159;&#19981;&#21516;&#30340;&#31232;&#30095;&#36830;&#25509;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23637;&#31034;&#22312;&#36845;&#20195;&#24133;&#24230;&#21098;&#26525;&#65288;IMP&#65289;&#30340;&#21333;&#27425;&#37325;&#26032;&#35757;&#32451;&#38454;&#27573;&#20013;&#25506;&#32034;&#19981;&#21516;&#30340;&#36229;&#21442;&#25968;&#37197;&#32622;&#65288;&#20363;&#22914;&#25209;&#27425;&#25490;&#24207;&#25110;&#26435;&#37325;&#34928;&#20943;&#65289;&#20135;&#29983;&#30340;&#27169;&#22411;&#36866;&#21512;&#36827;&#34892;&#24179;&#22343;&#65292;&#24182;&#19988;&#36890;&#36807;&#35774;&#35745;&#20849;&#20139;&#30456;&#21516;&#30340;&#31232;&#30095;&#36830;&#25509;&#24615;&#26469;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#12290;&#24179;&#22343;&#36825;&#20123;&#27169;&#22411;&#26174;&#33879;&#25552;&#21319;&#20102;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks can be significantly compressed by pruning, leading to sparse models requiring considerably less storage and floating-point operations while maintaining predictive performance. Model soups (Wortsman et al., 2022) improve generalization and out-of-distribution performance by averaging the parameters of multiple models into a single one without increased inference time. However, identifying models in the same loss basin to leverage both sparsity and parameter averaging is challenging, as averaging arbitrary sparse models reduces the overall sparsity due to differing sparse connectivities. In this work, we address these challenges by demonstrating that exploring a single retraining phase of Iterative Magnitude Pruning (IMP) with varying hyperparameter configurations, such as batch ordering or weight decay, produces models that are suitable for averaging and share the same sparse connectivity by design. Averaging these models significantly enhances generalization performanc
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.04357</link><description>&lt;p&gt;
&#29992;&#20110;&#22522;&#20110;&#26816;&#32034;&#30340;&#23545;&#35805;&#31995;&#32479;&#30340;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems. (arXiv:2306.04357v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04357
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;Dial-MAE&#65292;&#21033;&#29992;&#29983;&#25104;&#26041;&#27861;&#26356;&#22909;&#22320;&#21387;&#32553;&#23545;&#35805;&#35821;&#20041;&#33267;&#23494;&#38598;&#21521;&#37327;&#65292;&#24182;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#26088;&#22312;&#26681;&#25454;&#32473;&#23450;&#30340;&#29992;&#25143;&#21644;&#31995;&#32479;&#35805;&#35821;&#21382;&#21490;&#35760;&#24405;&#20174;&#20960;&#20010;&#20505;&#36873;&#21709;&#24212;&#20013;&#36873;&#25321;&#36866;&#24403;&#30340;&#21709;&#24212;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#36890;&#36807;&#21518;&#35757;&#32451;&#22823;&#22810;&#20381;&#36182;&#20110;&#21333;&#32431;&#30340;&#25513;&#30721;&#35821;&#35328;&#24314;&#27169;&#26041;&#27861;&#26469;&#25552;&#39640;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#20934;&#30830;&#24615;&#12290;&#20294;&#26159;&#65292;&#26368;&#36817;&#24320;&#21457;&#30340;&#29983;&#25104;&#26041;&#27861;&#22312;IR&#31038;&#21306;&#23637;&#31034;&#20102;&#26377;&#24076;&#26395;&#30340;&#25991;&#26412;&#34920;&#31034;&#33021;&#21147;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#26356;&#22909;&#30340;&#23545;&#35805;&#35821;&#20041;&#24314;&#27169;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986; Dial-MAE&#65288;&#23545;&#35805;&#19978;&#19979;&#25991;&#25513;&#30721;&#33258;&#32534;&#30721;&#22120;&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#38024;&#23545;&#23545;&#35805;&#21709;&#24212;&#36873;&#25321;&#30340;&#21518;&#35757;&#32451;&#25216;&#26415;&#12290; Dial-MAE&#20351;&#29992;&#19968;&#20010;&#19981;&#23545;&#31216;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#65292;&#23398;&#20064;&#23558;&#23545;&#35805;&#30340;&#35821;&#20041;&#26356;&#22909;&#22320;&#21387;&#32553;&#21040;&#23494;&#38598;&#21521;&#37327;&#20013;&#12290; Dial-MAE&#30340;&#36807;&#31243;&#21253;&#25324;&#30001;&#28145;&#24230;&#32534;&#30721;&#22120;&#21019;&#24314;&#24102;&#26377;&#25513;&#30721;&#23545;&#35805;&#19978;&#19979;&#25991;&#30340;&#23545;&#35805;&#23884;&#20837;&#65292;&#28982;&#21518;&#26159;&#27973;&#35299;&#30721;&#22120;&#65292;&#35813;&#35299;&#30721;&#22120;&#20351;&#29992;&#27492;&#23884;&#20837;&#20197;&#21450;&#19978;&#19979;&#25991;&#21521;&#37327;&#26469;&#29983;&#25104;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Dialogue response selection aims to select an appropriate response from several candidates based on a given user and system utterance history. Recent studies have been improving the accuracy of dialogue response selection through post-training, mostly relying on naive masked language modeling methods. However, the recently developed generative methods have shown promising text representation capabilities in IR community, which could potentially lead to better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE (Dialogue Contextual Masking Auto-encoder), a straightforward yet effective post-training technique tailored for dialogue response selection. Dial-MAE uses an asymmetric encoder-decoder architecture that learns to better compress the semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE involves a deep encoder creating a dialogue embedding with the masked dialogue context, followed by a shallow decoder that uses this embedding along with
&lt;/p&gt;</description></item><item><title>&#35813;&#25991;&#20171;&#32461;&#20102;&#30693;&#35782;&#24037;&#31243;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24110;&#21161;&#35835;&#32773;&#20102;&#35299;&#35813;&#39046;&#22495;&#24182;&#24314;&#31435;&#30452;&#35273;&#12290;</title><link>http://arxiv.org/abs/2305.17196</link><description>&lt;p&gt;
&#30693;&#35782;&#24037;&#31243;&#20837;&#38376;
&lt;/p&gt;
&lt;p&gt;
A Knowledge Engineering Primer. (arXiv:2305.17196v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.17196
&lt;/p&gt;
&lt;p&gt;
&#35813;&#25991;&#20171;&#32461;&#20102;&#30693;&#35782;&#24037;&#31243;&#30340;&#22522;&#26412;&#27010;&#24565;&#65292;&#24110;&#21161;&#35835;&#32773;&#20102;&#35299;&#35813;&#39046;&#22495;&#24182;&#24314;&#31435;&#30452;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#30340;&#30446;&#30340;&#26159;&#20197;&#31616;&#27905;&#32780;&#32508;&#21512;&#30340;&#26041;&#24335;&#20171;&#32461;&#30693;&#35782;&#24037;&#31243;&#30340;&#20027;&#39064;&#65292;&#20197;&#22521;&#20859;&#35835;&#32773;&#23545;&#35813;&#39046;&#22495;&#30340;&#30452;&#35273;&#12290;
&lt;/p&gt;
&lt;p&gt;
The aim of this primer is to introduce the subject of knowledge engineering in a concise but synthetic way to develop the reader's intuition about the area.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#21307;&#30103;&#20445;&#20581;NLP&#39046;&#22495;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;&#26368;&#26032;&#36827;&#23637;&#65292;&#24378;&#35843;&#20854;&#22312;&#38382;&#31572;&#31995;&#32479;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#24212;&#29992;&#20013;&#30340;&#36129;&#29486;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#29992;&#36164;&#28304;&#21644;&#26725;&#26753;&#65292;&#20197;&#26356;&#22909;&#22320;&#25506;&#32034;&#25552;&#31034;&#24037;&#31243;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.14670</link><description>&lt;p&gt;
&#21307;&#30103;&#20445;&#20581;&#30340;&#25552;&#31034;&#24037;&#31243;:  &#26041;&#27861;&#21644;&#24212;&#29992;. (arXiv:2304.14670v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
Prompt Engineering for Healthcare: Methodologies and Applications. (arXiv:2304.14670v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.14670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#21307;&#30103;&#20445;&#20581;NLP&#39046;&#22495;&#20013;&#30340;&#25552;&#31034;&#24037;&#31243;&#26368;&#26032;&#36827;&#23637;&#65292;&#24378;&#35843;&#20854;&#22312;&#38382;&#31572;&#31995;&#32479;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#31561;&#24212;&#29992;&#20013;&#30340;&#36129;&#29486;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;&#26377;&#29992;&#36164;&#28304;&#21644;&#26725;&#26753;&#65292;&#20197;&#26356;&#22909;&#22320;&#25506;&#32034;&#25552;&#31034;&#24037;&#31243;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#20171;&#32461;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#20869;&#21307;&#30103;&#20445;&#20581;&#25552;&#31034;&#24037;&#31243;&#26368;&#26032;&#30340;&#36827;&#23637;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23558;&#25552;&#20379;&#19968;&#20010;&#31616;&#35201;&#30340;&#25552;&#31034;&#24037;&#31243;&#21457;&#23637;&#27010;&#36848;&#65292;&#24182;&#24378;&#35843;&#20854;&#23545;&#21307;&#30103;&#20445;&#20581;NLP&#24212;&#29992;&#22914;&#38382;&#31572;&#31995;&#32479;&#12289;&#25991;&#26412;&#25688;&#35201;&#21644;&#26426;&#22120;&#32763;&#35793;&#30340;&#37325;&#35201;&#36129;&#29486;&#12290;&#38543;&#30528;&#36890;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#26029;&#25913;&#36827;&#65292;&#25552;&#31034;&#24037;&#31243;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#37325;&#35201;&#24615;&#36234;&#26469;&#36234;&#31361;&#20986;&#12290;&#26412;&#25991;&#30340;&#30446;&#30340;&#26159;&#20026;&#21307;&#30103;&#20445;&#20581;NLP&#30740;&#31350;&#20154;&#21592;&#25552;&#20379;&#26377;&#29992;&#30340;&#36164;&#28304;&#21644;&#26725;&#26753;&#65292;&#26356;&#22909;&#22320;&#25506;&#32034;&#25552;&#31034;&#24037;&#31243;&#22312;&#36825;&#19968;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#25105;&#20204;&#24076;&#26395;&#26412;&#25991;&#21487;&#20197;&#25552;&#20379;&#26032;&#30340;&#24605;&#36335;&#65292;&#28608;&#21457;&#21307;&#30103;NLP&#30340;&#30740;&#31350;&#21644;&#24212;&#29992;&#30340;&#20805;&#20998;&#21487;&#33021;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This review will introduce the latest advances in prompt engineering in the field of natural language processing (NLP) for the medical domain. First, we will provide a brief overview of the development of prompt engineering and emphasize its significant contributions to healthcare NLP applications such as question-answering systems, text summarization, and machine translation. With the continuous improvement of general large language models, the importance of prompt engineering in the healthcare domain is becoming increasingly prominent. The aim of this article is to provide useful resources and bridges for healthcare NLP researchers to better explore the application of prompt engineering in this field. We hope that this review can provide new ideas and inspire ample possibilities for research and application in medical NLP.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CiPR&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#36328;&#23454;&#20363;&#27491;&#20851;&#31995;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;(GCD)&#30340;&#38382;&#39064;&#12290;&#36873;&#25321;&#37051;&#23621;&#32858;&#31867;(SNC)&#31639;&#27861;&#22312;&#27492;&#36807;&#31243;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.06928</link><description>&lt;p&gt;
CiPR:&#19968;&#31181;&#20855;&#26377;&#36328;&#23454;&#20363;&#27491;&#20851;&#31995;&#30340;&#39640;&#25928;&#26694;&#26550;&#65292;&#29992;&#20110;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;.
&lt;/p&gt;
&lt;p&gt;
CiPR: An Efficient Framework with Cross-instance Positive Relations for Generalized Category Discovery. (arXiv:2304.06928v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.06928
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CiPR&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#20013;&#30340;&#36328;&#23454;&#20363;&#27491;&#20851;&#31995;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#65292;&#35299;&#20915;&#20102;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;(GCD)&#30340;&#38382;&#39064;&#12290;&#36873;&#25321;&#37051;&#23621;&#32858;&#31867;(SNC)&#31639;&#27861;&#22312;&#27492;&#36807;&#31243;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35299;&#20915;&#20102;&#24191;&#20041;&#31867;&#21035;&#21457;&#29616;&#65288;GCD&#65289;&#30340;&#38382;&#39064;&#12290;GCD&#32771;&#34385;&#20102;&#33258;&#21160;&#32858;&#31867;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#38598;&#30340;&#24320;&#25918;&#19990;&#30028;&#38382;&#39064;&#65292;&#22312;&#35813;&#25968;&#25454;&#38598;&#20013;&#65292;&#26410;&#26631;&#35760;&#25968;&#25454;&#21253;&#21547;&#26469;&#33258;&#26032;&#31867;&#21035;&#21644;&#24050;&#26631;&#35760;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;&#26412;&#25991;&#35299;&#20915;&#20102;&#22312;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#27809;&#26377;&#24050;&#30693;&#31867;&#21035;&#25968;&#30340;GCD&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CiPR&#30340;&#26694;&#26550;&#65292;&#36890;&#36807;&#21033;&#29992;&#37096;&#20998;&#26631;&#35760;&#25968;&#25454;&#20013;&#34987;&#29616;&#26377;&#26041;&#27861;&#24573;&#35270;&#30340;&#36328;&#23454;&#20363;&#27491;&#20851;&#31995;&#36827;&#34892;&#23545;&#27604;&#23398;&#20064;&#26469;&#24341;&#23548;&#34920;&#31034;&#12290;&#20026;&#20102;&#33719;&#24471;&#21487;&#38752;&#30340;&#36328;&#23454;&#20363;&#20851;&#31995;&#20197;&#20419;&#36827;&#34920;&#31034;&#23398;&#20064;&#65292;&#25105;&#20204;&#39318;&#20808;&#24341;&#20837;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#30340;&#20998;&#23618;&#32858;&#31867;&#31639;&#27861;&#65292;&#31216;&#20026;&#36873;&#25321;&#37051;&#23621;&#32858;&#31867;&#65288;SNC&#65289;&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#20174;&#30001;&#36873;&#25321;&#37051;&#23621;&#26500;&#36896;&#30340;&#22270;&#20013;&#30340;&#36830;&#36890;&#20998;&#37327;&#20013;&#29983;&#25104;&#32858;&#31867;&#23618;&#27425;&#32467;&#26500;&#12290;&#25105;&#20204;&#36824;&#25193;&#23637;&#20102;SNC&#20197;&#20415;&#23545;&#20855;&#26377;&#32473;&#23450;&#31867;&#30340;&#26410;&#26631;&#35760;&#23454;&#20363;&#36827;&#34892;&#26631;&#31614;&#20998;&#37197;&#12290;
&lt;/p&gt;
&lt;p&gt;
We tackle the issue of generalized category discovery (GCD). GCD considers the open-world problem of automatically clustering a partially labelled dataset, in which the unlabelled data contain instances from novel categories and also the labelled classes. In this paper, we address the GCD problem without a known category number in the unlabelled data. We propose a framework, named CiPR, to bootstrap the representation by exploiting Cross-instance Positive Relations for contrastive learning in the partially labelled data which are neglected in existing methods. First, to obtain reliable cross-instance relations to facilitate the representation learning, we introduce a semi-supervised hierarchical clustering algorithm, named selective neighbor clustering (SNC), which can produce a clustering hierarchy directly from the connected components in the graph constructed by selective neighbors. We also extend SNC to be capable of label assignment for the unlabelled instances with the given clas
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;&#30340;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;</title><link>http://arxiv.org/abs/2302.10681</link><description>&lt;p&gt;
FrankenSplit:&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;
&lt;/p&gt;
&lt;p&gt;
FrankenSplit: Saliency Guided Neural Feature Compression with Shallow Variational Bottleneck Injection. (arXiv:2302.10681v2 [eess.IV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26174;&#33879;&#24615;&#25351;&#23548;&#30340;&#31070;&#32463;&#29305;&#24449;&#21387;&#32553;&#19982;&#27973;&#23618;&#21464;&#20998;&#29942;&#39048;&#27880;&#20837;&#30340;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;AI&#21152;&#36895;&#22120;&#30340;&#23835;&#36215;&#20351;&#24471;&#23545;&#24310;&#36831;&#25935;&#24863;&#30340;&#24212;&#29992;&#21487;&#20197;&#22312;&#23458;&#25143;&#31471;&#19978;&#25191;&#34892;&#36731;&#37327;&#32423;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#28982;&#32780;&#65292;&#38656;&#35201;&#24378;&#22823;&#27169;&#22411;&#30340;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#38656;&#35201;&#23558;&#35831;&#27714;&#19979;&#25918;&#65292;&#32780;&#39640;&#32500;&#25968;&#25454;&#23558;&#20105;&#22842;&#26377;&#38480;&#30340;&#24102;&#23485;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36164;&#28304;&#24847;&#35782;&#21387;&#32553;&#27169;&#22411;&#30340;&#26694;&#26550;&#24182;&#22312;&#21453;&#26144;&#36793;&#32536;&#35774;&#22791;&#21644;&#26381;&#21153;&#22120;&#20043;&#38388;&#19981;&#23545;&#31216;&#36164;&#28304;&#20998;&#37197;&#30340;&#29615;&#22659;&#20013;&#36827;&#34892;&#20102;&#24191;&#27867;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#19981;&#38477;&#20302;&#20934;&#30830;&#24615;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#27604;&#26368;&#20808;&#36827;&#30340;SC&#26041;&#27861;&#20302;60&#65285;&#30340;&#27604;&#29305;&#29575;&#65292;&#24182;&#19988;&#27604;&#29616;&#26377;&#30340;&#32534;&#35299;&#30721;&#26631;&#20934;&#30340;&#19979;&#25918;&#24555;16&#20493;&#12290;
&lt;/p&gt;
&lt;p&gt;
The rise of mobile AI accelerators allows latency-sensitive applications to execute lightweight Deep Neural Networks (DNNs) on the client side. However, critical applications require powerful models that edge devices cannot host and must therefore offload requests, where the high-dimensional data will compete for limited bandwidth. This work proposes shifting away from focusing on executing shallow layers of partitioned DNNs. Instead, it advocates concentrating the local resources on variational compression optimized for machine interpretability. We introduce a novel framework for resource-conscious compression models and extensively evaluate our method in an environment reflecting the asymmetric resource distribution between edge devices and servers. Our method achieves 60\% lower bitrate than a state-of-the-art SC method without decreasing accuracy and is up to 16x faster than offloading with existing codec standards.
&lt;/p&gt;</description></item><item><title>Box$^2$EL&#26041;&#27861;&#36890;&#36807;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#34920;&#31034;&#20026;&#30418;&#23376;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35282;&#33394;&#34920;&#31034;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.11118</link><description>&lt;p&gt;
Box$^2$EL: EL++&#25551;&#36848;&#36923;&#36753;&#20013;&#30340;&#27010;&#24565;&#21644;&#35282;&#33394;&#30418;&#23376;&#23884;&#20837;&#30340;&#27010;&#24565;&#21644;&#35282;&#33394;&#30418;&#23376;&#23884;&#20837;&#26041;&#27861;&#21450;&#20854;&#20316;&#29992;
&lt;/p&gt;
&lt;p&gt;
Box$^2$EL: Concept and Role Box Embeddings for the Description Logic EL++. (arXiv:2301.11118v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11118
&lt;/p&gt;
&lt;p&gt;
Box$^2$EL&#26041;&#27861;&#36890;&#36807;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#34920;&#31034;&#20026;&#30418;&#23376;&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#35282;&#33394;&#34920;&#31034;&#21463;&#38480;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25551;&#36848;&#36923;&#36753;&#26412;&#20307;&#35770;&#25193;&#23637;&#20102;&#30693;&#35782;&#22270;&#35889;&#19982;&#27010;&#24565;&#20449;&#24687;&#21644;&#36923;&#36753;&#32972;&#26223;&#30693;&#35782;&#12290;&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#23545;&#36825;&#31181;&#26412;&#20307;&#35770;&#30340;&#24402;&#32435;&#25512;&#29702;&#25216;&#26415;&#36234;&#26469;&#36234;&#24863;&#20852;&#36259;&#65292;&#36825;&#20123;&#25216;&#26415;&#26377;&#26395;&#34917;&#20805;&#20256;&#32479;&#30340;&#28436;&#32462;&#25512;&#29702;&#31639;&#27861;&#12290;&#31867;&#20284;&#20110;&#30693;&#35782;&#22270;&#35889;&#30340;&#23436;&#21892;&#65292;&#29616;&#26377;&#30340;&#19968;&#20123;&#26041;&#27861;&#36890;&#36807;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#23398;&#20064;&#26412;&#20307;&#35770;&#23884;&#20837;&#65292;&#21516;&#26102;&#30830;&#20445;&#36825;&#20123;&#23884;&#20837;&#33021;&#22815;&#20934;&#30830;&#22320;&#25429;&#25417;&#21040;&#24213;&#23618;&#25551;&#36848;&#36923;&#36753;&#30340;&#36923;&#36753;&#35821;&#20041;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23384;&#22312;&#19968;&#20123;&#38382;&#39064;&#65292;&#20027;&#35201;&#26159;&#30001;&#20110;&#21463;&#38480;&#30340;&#35282;&#33394;&#34920;&#31034;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Box$^2$EL&#26041;&#27861;&#65292;&#23558;&#27010;&#24565;&#21644;&#35282;&#33394;&#37117;&#34920;&#31034;&#20026;&#30418;&#23376;&#65288;&#21363;&#36724;&#23545;&#40784;&#36229;&#30697;&#24418;&#65289;&#65292;&#24182;&#23637;&#31034;&#20102;&#23427;&#22914;&#20309;&#20811;&#26381;&#20043;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#27491;&#30830;&#24615;&#65292;&#24182;&#36827;&#34892;&#20102;&#22823;&#37327;&#30340;&#23454;&#39564;&#35780;&#20272;&#65292;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#39046;&#20808;&#30340;&#32467;&#26524;&#12290;&#20316;&#20026;&#25105;&#20204;&#35780;&#20272;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#22522;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Description logic (DL) ontologies extend knowledge graphs (KGs) with conceptual information and logical background knowledge. In recent years, there has been growing interest in inductive reasoning techniques for such ontologies, which promise to complement classical deductive reasoning algorithms. Similar to KG completion, several existing approaches learn ontology embeddings in a latent space, while additionally ensuring that they faithfully capture the logical semantics of the underlying DL. However, they suffer from several shortcomings, mainly due to a limiting role representation. We propose Box$^2$EL, which represents both concepts and roles as boxes (i.e., axis-aligned hyperrectangles) and demonstrate how it overcomes the limitations of previous methods. We theoretically prove the soundness of our model and conduct an extensive experimental evaluation, achieving state-of-the-art results across a variety of datasets. As part of our evaluation, we introduce a novel benchmark for 
&lt;/p&gt;</description></item><item><title>EVOTER&#20351;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#28436;&#21270;&#20986;&#36879;&#26126;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#65292;&#19982;&#40657;&#30418;&#27169;&#22411;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#24182;&#20026;&#26410;&#26469;&#26500;&#24314;&#21487;&#38752;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#22522;&#30784;&#12290;</title><link>http://arxiv.org/abs/2204.10438</link><description>&lt;p&gt;
EVOTER&#65306;&#36879;&#26126;&#21487;&#35299;&#37322;&#35268;&#21017;&#38598;&#30340;&#36827;&#21270;
&lt;/p&gt;
&lt;p&gt;
EVOTER: Evolution of Transparent Explainable Rule-sets. (arXiv:2204.10438v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.10438
&lt;/p&gt;
&lt;p&gt;
EVOTER&#20351;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#28436;&#21270;&#20986;&#36879;&#26126;&#21487;&#35299;&#37322;&#30340;&#35268;&#21017;&#38598;&#65292;&#19982;&#40657;&#30418;&#27169;&#22411;&#24615;&#33021;&#30456;&#20284;&#65292;&#21487;&#20197;&#25581;&#31034;&#25968;&#25454;&#20013;&#30340;&#20559;&#35265;&#24182;&#20026;&#26410;&#26469;&#26500;&#24314;&#21487;&#38752;&#30340;AI&#31995;&#32479;&#25552;&#20379;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;AI&#31995;&#32479;&#26159;&#40657;&#30418;&#23376;&#65292;&#20026;&#32473;&#23450;&#30340;&#36755;&#20837;&#29983;&#25104;&#21512;&#29702;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#39046;&#22495;&#20855;&#26377;&#35299;&#37322;&#33021;&#21147;&#21644;&#20449;&#20219;&#24230;&#35201;&#27714;&#65292;&#36825;&#20123;&#35201;&#27714;&#19981;&#33021;&#30452;&#25509;&#28385;&#36275;&#36825;&#20123;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;&#65292;&#21363;&#24320;&#22987;&#26102;&#27169;&#22411;&#23601;&#26159;&#36879;&#26126;&#30340;&#21644;&#21487;&#35299;&#37322;&#30340;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#31616;&#21333;&#30340;&#36923;&#36753;&#34920;&#36798;&#24335;&#28436;&#21270;&#20986;&#35268;&#21017;&#38598;&#65292;&#31216;&#20026;EVOTER&#12290;EVOTER&#22312;&#22810;&#20010;&#39044;&#27979;/&#20998;&#31867;&#21644;&#22788;&#26041;/&#25919;&#31574;&#25628;&#32034;&#39046;&#22495;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26377;&#21644;&#27809;&#26377;&#20195;&#29702;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#23427;&#33021;&#22815;&#21457;&#29616;&#21644;&#40657;&#30418;&#27169;&#22411;&#30456;&#20284;&#30340;&#26377;&#24847;&#20041;&#30340;&#35268;&#21017;&#38598;&#12290;&#36825;&#20123;&#35268;&#21017;&#21487;&#20197;&#25552;&#20379;&#39046;&#22495;&#30340;&#35265;&#35299;&#65292;&#24182;&#20351;&#25968;&#25454;&#20013;&#38544;&#34255;&#30340;&#20559;&#35265;&#26174;&#24615;&#21270;&#12290;&#20063;&#21487;&#20197;&#30452;&#25509;&#23545;&#23427;&#20204;&#36827;&#34892;&#32534;&#36753;&#65292;&#20197;&#28040;&#38500;&#20559;&#35265;&#24182;&#28155;&#21152;&#32422;&#26463;&#12290;&#22240;&#27492;&#65292;EVOTER&#20026;&#26410;&#26469;&#26500;&#24314;&#20540;&#24471;&#20449;&#36182;&#30340;AI&#31995;&#32479;&#30340;&#21487;&#38752;&#22522;&#30784;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most AI systems are black boxes generating reasonable outputs for given inputs. Some domains, however, have explainability and trustworthiness requirements that cannot be directly met by these approaches. Various methods have therefore been developed to interpret black-box models after training. This paper advocates an alternative approach where the models are transparent and explainable to begin with. This approach, EVOTER, evolves rule-sets based on simple logical expressions. The approach is evaluated in several prediction/classification and prescription/policy search domains with and without a surrogate. It is shown to discover meaningful rule sets that perform similarly to black-box models. The rules can provide insight into the domain, and make biases hidden in the data explicit. It may also be possible to edit them directly to remove biases and add constraints. EVOTER thus forms a promising foundation for building trustworthy AI systems for real-world applications in the future.
&lt;/p&gt;</description></item></channel></rss>