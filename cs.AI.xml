<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>DiT-3D&#26159;&#19968;&#31181;&#38024;&#23545;3D&#24418;&#29366;&#29983;&#25104;&#30340;&#26032;&#22411;&#25193;&#25955;Transformer&#65292;&#36890;&#36807;&#22312;&#32431;Transformer&#19978;&#36827;&#34892;&#21435;&#22122;&#22788;&#29702;&#65292;&#32467;&#21512;3D&#20301;&#32622;&#21644;&#34917;&#19969;&#23884;&#20837;&#26469;&#32858;&#21512;&#20307;&#32032;&#21270;&#28857;&#20113;&#30340;&#36755;&#20837;&#65292;&#24182;&#24341;&#20837;&#20102;3D&#31383;&#21475;&#27880;&#24847;&#21147;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;</title><link>http://arxiv.org/abs/2307.01831</link><description>&lt;p&gt;
DiT-3D: &#25506;&#32034;&#29992;&#20110;3D&#24418;&#29366;&#29983;&#25104;&#30340;&#32431;&#25193;&#25955;Transformer
&lt;/p&gt;
&lt;p&gt;
DiT-3D: Exploring Plain Diffusion Transformers for 3D Shape Generation. (arXiv:2307.01831v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01831
&lt;/p&gt;
&lt;p&gt;
DiT-3D&#26159;&#19968;&#31181;&#38024;&#23545;3D&#24418;&#29366;&#29983;&#25104;&#30340;&#26032;&#22411;&#25193;&#25955;Transformer&#65292;&#36890;&#36807;&#22312;&#32431;Transformer&#19978;&#36827;&#34892;&#21435;&#22122;&#22788;&#29702;&#65292;&#32467;&#21512;3D&#20301;&#32622;&#21644;&#34917;&#19969;&#23884;&#20837;&#26469;&#32858;&#21512;&#20307;&#32032;&#21270;&#28857;&#20113;&#30340;&#36755;&#20837;&#65292;&#24182;&#24341;&#20837;&#20102;3D&#31383;&#21475;&#27880;&#24847;&#21147;&#20197;&#38477;&#20302;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#25193;&#25955;Transformer&#65288;&#20363;&#22914;DiT&#65289;&#24050;&#32463;&#22312;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;2D&#22270;&#20687;&#26041;&#38754;&#23637;&#31034;&#20102;&#24378;&#22823;&#30340;&#25928;&#26524;&#12290;&#20294;&#26159;&#65292;&#26159;&#21542;Transformer&#26550;&#26500;&#22312;3D&#24418;&#29366;&#29983;&#25104;&#26041;&#38754;&#21516;&#26679;&#26377;&#25928;&#20173;&#28982;&#26377;&#24453;&#30830;&#23450;&#65292;&#22240;&#20026;&#20808;&#21069;&#30340;3D&#25193;&#25955;&#26041;&#27861;&#22823;&#37096;&#20998;&#37319;&#29992;&#20102;U-Net&#26550;&#26500;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#24046;&#36317;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;3D&#24418;&#29366;&#29983;&#25104;&#30340;&#25193;&#25955;Transformer&#65292;&#31216;&#20026;DiT-3D&#65292;&#23427;&#21487;&#20197;&#30452;&#25509;&#22312;&#29992;&#20110;&#20307;&#32032;&#21270;&#28857;&#20113;&#30340;&#32431;Transformer&#19978;&#36827;&#34892;&#21435;&#22122;&#22788;&#29702;&#12290;&#19982;&#29616;&#26377;&#30340;U-Net&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;DiT-3D&#22312;&#27169;&#22411;&#23610;&#23544;&#19978;&#20855;&#26377;&#26356;&#22909;&#30340;&#21487;&#25193;&#23637;&#24615;&#65292;&#24182;&#19988;&#20135;&#29983;&#30340;&#29983;&#25104;&#32467;&#26524;&#36136;&#37327;&#26356;&#39640;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;DiT-3D&#37319;&#29992;&#20102;DiT&#30340;&#35774;&#35745;&#29702;&#24565;&#65292;&#20294;&#36890;&#36807;&#34701;&#21512;3D&#20301;&#32622;&#21644;&#34917;&#19969;&#23884;&#20837;&#26469;&#33258;&#36866;&#24212;&#22320;&#32858;&#21512;&#26469;&#33258;&#20307;&#32032;&#21270;&#28857;&#20113;&#30340;&#36755;&#20837;&#12290;&#20026;&#20102;&#20943;&#23569;3D&#24418;&#29366;&#29983;&#25104;&#20013;&#33258;&#27880;&#24847;&#21147;&#30340;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#22312;Transformer&#22359;&#20013;&#24341;&#20837;&#20102;3D&#31383;&#21475;&#27880;&#24847;&#21147;&#65292;&#20197;&#22686;&#21152;3D&#20196;&#29260;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent Diffusion Transformers (e.g., DiT) have demonstrated their powerful effectiveness in generating high-quality 2D images. However, it is still being determined whether the Transformer architecture performs equally well in 3D shape generation, as previous 3D diffusion methods mostly adopted the U-Net architecture. To bridge this gap, we propose a novel Diffusion Transformer for 3D shape generation, namely DiT-3D, which can directly operate the denoising process on voxelized point clouds using plain Transformers. Compared to existing U-Net approaches, our DiT-3D is more scalable in model size and produces much higher quality generations. Specifically, the DiT-3D adopts the design philosophy of DiT but modifies it by incorporating 3D positional and patch embeddings to adaptively aggregate input from voxelized point clouds. To reduce the computational cost of self-attention in 3D shape generation, we incorporate 3D window attention into Transformer blocks, as the increased 3D token le
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#31867;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;BNSP-SFM&#65292;&#23558;&#34892;&#20026;SDE&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#25552;&#20379;&#36739;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#19982;11&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;BNSP-SFM&#22312;&#39044;&#27979;&#31934;&#24230;&#19978;&#21462;&#24471;&#20102;&#22810;&#36798;50&#65285;&#30340;&#25913;&#36827;&#12290;&#36825;&#31181;&#27169;&#22411;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#20154;&#32676;&#23494;&#24230;&#30340;&#22330;&#26223;&#20013;&#20063;&#33021;&#22815;&#26356;&#22909;&#22320;&#25512;&#24191;&#12290;</title><link>http://arxiv.org/abs/2307.01817</link><description>&lt;p&gt;
&#20855;&#26377;&#21487;&#35299;&#37322;&#34892;&#20026;&#19981;&#30830;&#23450;&#24615;&#30340;&#20154;&#31867;&#36712;&#36857;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Human Trajectory Forecasting with Explainable Behavioral Uncertainty. (arXiv:2307.01817v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01817
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20154;&#31867;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;BNSP-SFM&#65292;&#23558;&#34892;&#20026;SDE&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#33021;&#22815;&#25552;&#20379;&#36739;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;&#21644;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#19982;11&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;BNSP-SFM&#22312;&#39044;&#27979;&#31934;&#24230;&#19978;&#21462;&#24471;&#20102;&#22810;&#36798;50&#65285;&#30340;&#25913;&#36827;&#12290;&#36825;&#31181;&#27169;&#22411;&#22312;&#19981;&#21516;&#29615;&#22659;&#21644;&#20154;&#32676;&#23494;&#24230;&#30340;&#22330;&#26223;&#20013;&#20063;&#33021;&#22815;&#26356;&#22909;&#22320;&#25512;&#24191;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#31867;&#36712;&#36857;&#39044;&#27979;&#26377;&#21161;&#20110;&#29702;&#35299;&#21644;&#39044;&#27979;&#20154;&#31867;&#34892;&#20026;&#65292;&#21487;&#20197;&#24212;&#29992;&#20110;&#31038;&#20132;&#26426;&#22120;&#20154;&#21040;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#31561;&#21508;&#31181;&#22330;&#26223;&#65292;&#24182;&#19988;&#19968;&#30452;&#21463;&#21040;&#28145;&#20837;&#30740;&#31350;&#12290;&#29616;&#26377;&#26041;&#27861;&#21487;&#20197;&#20998;&#20026;&#22522;&#20110;&#27169;&#22411;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#24615;&#65292;&#20294;&#39044;&#27979;&#33021;&#21147;&#26377;&#38480;&#65292;&#32780;&#22522;&#20110;&#27169;&#22411;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#36739;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#12290;&#32467;&#21512;&#36825;&#20004;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#36125;&#21494;&#26031;&#31070;&#32463;&#38543;&#26426;&#24494;&#20998;&#26041;&#31243;&#27169;&#22411;BNSP-SFM&#65292;&#20854;&#20013;&#34892;&#20026;SDE&#27169;&#22411;&#19982;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30456;&#32467;&#21512;&#12290;&#34429;&#28982;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#36739;&#22909;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;SDE&#25552;&#20379;&#20102;&#24378;&#22823;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#34892;&#20026;&#21644;&#35266;&#23519;&#30340;&#21487;&#37327;&#21270;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;BNSP-SFM&#30456;&#27604;&#20110;11&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#39044;&#27979;&#31934;&#24230;&#25552;&#39640;&#20102;&#22810;&#36798;50&#65285;&#12290;BNSP-SFM&#36824;&#33021;&#26356;&#22909;&#22320;&#25512;&#24191;&#21040;&#19981;&#21516;&#29615;&#22659;&#21644;&#20154;&#32676;&#23494;&#24230;&#30340;&#25130;&#28982;&#19981;&#21516;&#22330;&#26223;&#20013;&#65288;&#32422;20&#20493;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Human trajectory forecasting helps to understand and predict human behaviors, enabling applications from social robots to self-driving cars, and therefore has been heavily investigated. Most existing methods can be divided into model-free and model-based methods. Model-free methods offer superior prediction accuracy but lack explainability, while model-based methods provide explainability but cannot predict well. Combining both methodologies, we propose a new Bayesian Neural Stochastic Differential Equation model BNSP-SFM, where a behavior SDE model is combined with Bayesian neural networks (BNNs). While the NNs provide superior predictive power, the SDE offers strong explainability with quantifiable uncertainty in behavior and observation. We show that BNSP-SFM achieves up to a 50% improvement in prediction accuracy, compared with 11 state-of-the-art methods. BNSP-SFM also generalizes better to drastically different scenes with different environments and crowd densities (~ 20 times hi
&lt;/p&gt;</description></item><item><title>DeepFlorist&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#20316;&#20026;&#20803;&#20998;&#31867;&#22120;&#23454;&#29616;&#33457;&#21321;&#20998;&#31867;&#12290;&#36890;&#36807;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#39640;&#32423;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#65292;DeepFlorist&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.01806</link><description>&lt;p&gt;
DeepFlorist: &#37325;&#26032;&#24605;&#32771;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21644;&#38598;&#25104;&#23398;&#20064;&#20316;&#20026;&#30446;&#26631;&#20998;&#31867;&#30340;&#20803;&#20998;&#31867;&#22120;
&lt;/p&gt;
&lt;p&gt;
DeepFlorist: Rethinking Deep Neural Networks and Ensemble Learning as A Meta-Classifier For Object Classification. (arXiv:2307.01806v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01806
&lt;/p&gt;
&lt;p&gt;
DeepFlorist&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#23398;&#20064;&#33539;&#24335;&#65292;&#36890;&#36807;&#38598;&#25104;&#23398;&#20064;&#20316;&#20026;&#20803;&#20998;&#31867;&#22120;&#23454;&#29616;&#33457;&#21321;&#20998;&#31867;&#12290;&#36890;&#36807;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#39640;&#32423;&#29305;&#24449;&#65292;&#24182;&#37319;&#29992;&#22810;&#20010;&#27169;&#22411;&#36827;&#34892;&#38598;&#25104;&#65292;DeepFlorist&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;"DeepFlorist"&#30340;&#26032;&#22411;&#23398;&#20064;&#33539;&#24335;&#65292;&#21033;&#29992;&#38598;&#25104;&#23398;&#20064;&#20316;&#20026;&#20803;&#20998;&#31867;&#22120;&#36827;&#34892;&#33457;&#21321;&#20998;&#31867;&#12290;DeepFlorist&#23558;&#28145;&#24230;&#23398;&#20064;&#30340;&#24378;&#22823;&#24615;&#33021;&#19982;&#38598;&#25104;&#26041;&#27861;&#30340;&#31283;&#20581;&#24615;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#23454;&#29616;&#20934;&#30830;&#21487;&#38752;&#30340;&#33457;&#21321;&#20998;&#31867;&#32467;&#26524;&#12290;&#25152;&#25552;&#20986;&#30340;&#32593;&#32476;&#26550;&#26500;&#21033;&#29992;&#23494;&#38598;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;DCNNs&#65289;&#21644;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNNs&#65289;&#30340;&#32452;&#21512;&#26469;&#25552;&#21462;&#33457;&#21321;&#22270;&#20687;&#30340;&#39640;&#32423;&#29305;&#24449;&#65292;&#28982;&#21518;&#36890;&#36807;&#20840;&#36830;&#25509;&#23618;&#36827;&#34892;&#20998;&#31867;&#12290;&#20026;&#20102;&#22686;&#24378;DeepFlorist&#30340;&#24615;&#33021;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#37319;&#29992;&#20102;&#38598;&#25104;&#23398;&#20064;&#26041;&#27861;&#65292;&#32467;&#21512;&#22810;&#20010;&#19981;&#21516;&#30340;&#27169;&#22411;&#20197;&#25552;&#39640;&#20998;&#31867;&#20934;&#30830;&#24615;&#12290;&#22312;&#22522;&#20934;&#33457;&#21321;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;DeepFlorist&#30340;&#26377;&#25928;&#24615;&#65292;&#22312;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#26041;&#38754;&#34920;&#29616;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#23545;&#20110;&#33258;&#21160;&#21270;&#33457;&#21321;&#35782;&#21035;&#31995;&#32479;&#20855;&#26377;&#37325;&#35201;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we propose a novel learning paradigm called "DeepFlorist" for flower classification using ensemble learning as a meta-classifier. DeepFlorist combines the power of deep learning with the robustness of ensemble methods to achieve accurate and reliable flower classification results. The proposed network architecture leverages a combination of dense convolutional and convolutional neural networks (DCNNs and CNNs) to extract high-level features from flower images, followed by a fully connected layer for classification. To enhance the performance and generalization of DeepFlorist, an ensemble learning approach is employed, incorporating multiple diverse models to improve the classification accuracy. Experimental results on benchmark flower datasets demonstrate the effectiveness of DeepFlorist, outperforming state-of-the-art methods in terms of accuracy and robustness. The proposed framework holds significant potential for automated flower recognition systems in real-world app
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21477;&#23376;&#20869;&#37096;&#24773;&#24863;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#20102;&#39044;&#27979;&#22120;&#26469;&#20998;&#26512;&#21477;&#23376;&#30340;&#24773;&#24863;&#20998;&#24067;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26222;&#36890;&#30340;&#36830;&#25509;&#35789;&#20063;&#21487;&#20197;&#26174;&#33879;&#25913;&#21464;&#35805;&#35821;&#30340;&#24773;&#24863;&#36712;&#36857;&#12290;</title><link>http://arxiv.org/abs/2307.01784</link><description>&lt;p&gt;
&#19968;&#20010;&#24605;&#24819;&#30340;&#20869;&#22312;&#24773;&#24863;
&lt;/p&gt;
&lt;p&gt;
The Inner Sentiments of a Thought. (arXiv:2307.01784v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01784
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#32034;&#20102;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#21477;&#23376;&#20869;&#37096;&#24773;&#24863;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#35757;&#32451;&#20102;&#39044;&#27979;&#22120;&#26469;&#20998;&#26512;&#21477;&#23376;&#30340;&#24773;&#24863;&#20998;&#24067;&#65292;&#24182;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26222;&#36890;&#30340;&#36830;&#25509;&#35789;&#20063;&#21487;&#20197;&#26174;&#33879;&#25913;&#21464;&#35805;&#35821;&#30340;&#24773;&#24863;&#36712;&#36857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#39640;&#24230;&#36924;&#30495;&#30340;&#25991;&#26412;&#12290;&#23427;&#20204;&#33021;&#22815;&#34920;&#36798;&#24182;&#33267;&#23569;&#26263;&#31034;&#20986;&#19968;&#31995;&#21015;&#24773;&#24863;&#21644;&#33394;&#24425;&#65292;&#20174;&#26126;&#26174;&#30340;&#20215;&#20540;&#21644;&#21796;&#36215;&#21040;&#24494;&#22937;&#30340;&#20915;&#24515;&#21644;&#36190;&#36175;&#12290;&#25105;&#20204;&#39318;&#27425;&#25506;&#32034;&#20102;&#36825;&#20123;&#34920;&#31034;&#21450;&#20854;&#22914;&#20309;&#29992;&#20110;&#29702;&#35299;&#21333;&#20010;&#21477;&#23376;&#20869;&#37096;&#30340;&#24773;&#24863;&#36816;&#20316;&#12290;&#25105;&#20204;&#35757;&#32451;&#20102;&#20174;&#22686;&#38271;&#38271;&#24230;&#30340;&#21069;&#32512;&#20013;&#24212;&#29992;&#21040;LLM&#30340;&#38544;&#34255;&#34920;&#31034;&#30340;&#21477;&#23376;&#30340;&#26368;&#32456;&#24773;&#24863;&#30340;&#20998;&#24067;&#30340;&#23450;&#37327;&#39044;&#27979;&#22120;&#12290;&#22312;&#23637;&#31034;&#20102;&#20215;&#20540;&#12289;&#20915;&#24515;&#12289;&#36190;&#36175;&#12289;&#28966;&#34385;&#21644;&#28902;&#24700;&#30340;&#20998;&#24067;&#39044;&#27979;&#22120;&#26159;&#33391;&#22909;&#26657;&#20934;&#30340;&#22522;&#30784;&#19978;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20351;&#29992;&#36825;&#20123;&#39044;&#27979;&#22120;&#20998;&#26512;&#21477;&#23376;&#30340;&#31034;&#20363;&#65292;&#20363;&#22914;&#65292;&#23637;&#31034;&#20102;&#21363;&#20351;&#26159;&#26222;&#36890;&#30340;&#36830;&#25509;&#35789;&#65288;&#20363;&#22914;&#65292;&#8220;&#20294;&#26159;&#8221;&#65289;&#20063;&#21487;&#20197;&#26497;&#22823;&#22320;&#25913;&#21464;&#35805;&#35821;&#30340;&#24773;&#24863;&#36712;&#36857;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#36825;&#20123;&#39044;&#27979;&#22120;&#26469;&#21033;&#29992;&#20998;&#24067;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based large-scale language models (LLMs) are able to generate highly realistic text. They are duly able to express, and at least implicitly represent, a wide range of sentiments and color, from the obvious, such as valence and arousal to the subtle, such as determination and admiration. We provide a first exploration of these representations and how they can be used for understanding the inner sentimental workings of single sentences. We train predictors of the quantiles of the distributions of final sentiments of sentences from the hidden representations of an LLM applied to prefixes of increasing lengths. After showing that predictors of distributions of valence, determination, admiration, anxiety and annoyance are well calibrated, we provide examples of using these predictors for analyzing sentences, illustrating, for instance, how even ordinary conjunctions (e.g., "but") can dramatically alter the emotional trajectory of an utterance. We then show how to exploit the dis
&lt;/p&gt;</description></item><item><title>GHOST&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#30789;&#20809;&#23376;&#23398;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#65292;&#39640;&#25928;&#22320;&#22788;&#29702;&#21644;&#21152;&#36895;GNN&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#21152;&#36895;&#22120;&#30340;&#32570;&#28857;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;GNN&#27169;&#22411;&#21644;&#26550;&#26500;&#30340;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.01782</link><description>&lt;p&gt;
GHOST:&#19968;&#31181;&#20351;&#29992;&#30789;&#20809;&#23376;&#23398;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;
&lt;/p&gt;
&lt;p&gt;
GHOST: A Graph Neural Network Accelerator using Silicon Photonics. (arXiv:2307.01782v1 [cs.AR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01782
&lt;/p&gt;
&lt;p&gt;
GHOST&#26159;&#31532;&#19968;&#20010;&#20351;&#29992;&#30789;&#20809;&#23376;&#23398;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#65292;&#39640;&#25928;&#22320;&#22788;&#29702;&#21644;&#21152;&#36895;GNN&#65292;&#20811;&#26381;&#20102;&#20256;&#32479;&#21152;&#36895;&#22120;&#30340;&#32570;&#28857;&#65292;&#20351;&#24471;&#23427;&#21487;&#20197;&#24212;&#29992;&#20110;&#21508;&#31181;GNN&#27169;&#22411;&#21644;&#26550;&#26500;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#24378;&#22823;&#30340;&#24314;&#27169;&#21644;&#23398;&#20064;&#22270;&#32467;&#26500;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;GNN&#30340;&#33021;&#21147;&#24050;&#32463;&#22312;&#25512;&#33616;&#31995;&#32479;&#12289;&#31038;&#20132;&#32593;&#32476;&#20998;&#26512;&#12289;&#33647;&#29289;&#21457;&#29616;&#21644;&#26426;&#22120;&#20154;&#39046;&#22495;&#31561;&#22810;&#20010;&#39046;&#22495;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#30410;&#22788;&#12290;&#28982;&#32780;&#65292;&#21152;&#36895;&#21644;&#39640;&#25928;&#22788;&#29702;GNN&#38656;&#35201;&#19968;&#31181;&#36229;&#36234;&#20256;&#32479;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#29420;&#29305;&#26041;&#27861;&#65292;&#36825;&#26159;&#30001;&#20110;GNN&#20855;&#26377;&#24040;&#22823;&#30340;&#35745;&#31639;&#21644;&#23384;&#20648;&#38656;&#27714;&#12290;CMOS&#24179;&#21488;&#32553;&#25918;&#30340;&#20943;&#36895;&#20063;&#25512;&#21160;&#20102;&#23545;&#26367;&#20195;&#23454;&#29616;&#22522;&#20307;&#30340;&#23547;&#27714;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;GHOST&#65292;&#36825;&#26159;&#31532;&#19968;&#20010;&#29992;&#20110;GNN&#30340;&#30789;&#20809;&#23376;&#30828;&#20214;&#21152;&#36895;&#22120;&#12290;GHOST&#39640;&#25928;&#22320;&#20943;&#36731;&#20102;&#19982;&#39030;&#28857;&#21644;&#36793;&#25805;&#20316;&#30456;&#20851;&#30340;&#25104;&#26412;&#12290;&#23427;&#22312;&#20809;&#23398;&#39046;&#22495;&#20013;&#20998;&#21035;&#23454;&#29616;&#20102;&#36816;&#34892;GNN&#25152;&#28041;&#21450;&#30340;&#19977;&#20010;&#20027;&#35201;&#38454;&#27573;&#65292;&#20351;&#20854;&#21487;&#20197;&#29992;&#20110;&#21508;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;GNN&#27169;&#22411;&#21644;&#26550;&#26500;&#30340;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph neural networks (GNNs) have emerged as a powerful approach for modelling and learning from graph-structured data. Multiple fields have since benefitted enormously from the capabilities of GNNs, such as recommendation systems, social network analysis, drug discovery, and robotics. However, accelerating and efficiently processing GNNs require a unique approach that goes beyond conventional artificial neural network accelerators, due to the substantial computational and memory requirements of GNNs. The slowdown of scaling in CMOS platforms also motivates a search for alternative implementation substrates. In this paper, we present GHOST, the first silicon-photonic hardware accelerator for GNNs. GHOST efficiently alleviates the costs associated with both vertex-centric and edge-centric operations. It implements separately the three main stages involved in running GNNs in the optical domain, allowing it to be used for the inference of various widely used GNN models and architectures, 
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;3D&#24314;&#27169;&#65292;&#21046;&#20316;&#20986;&#19982;&#26085;&#24120;&#26381;&#35013;&#32441;&#29702;&#30456;&#20284;&#30340;&#23545;&#25239;&#24615;&#20266;&#35013;&#32441;&#29702;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#35270;&#35282;&#19979;&#36991;&#24320;&#20154;&#29289;&#26816;&#27979;&#65292;&#23454;&#29616;&#33258;&#28982;&#22806;&#35266;&#30340;&#26381;&#35013;&#32441;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.01778</link><description>&lt;p&gt;
&#36890;&#36807;3D&#24314;&#27169;&#65292;&#23454;&#29616;&#33258;&#28982;&#22806;&#35266;&#30340;&#26381;&#35013;&#32441;&#29702;&#20197;&#36867;&#36991;&#20154;&#29289;&#26816;&#27979;&#22120;
&lt;/p&gt;
&lt;p&gt;
Physically Realizable Natural-Looking Clothing Textures Evade Person Detectors via 3D Modeling. (arXiv:2307.01778v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01778
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;3D&#24314;&#27169;&#65292;&#21046;&#20316;&#20986;&#19982;&#26085;&#24120;&#26381;&#35013;&#32441;&#29702;&#30456;&#20284;&#30340;&#23545;&#25239;&#24615;&#20266;&#35013;&#32441;&#29702;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#35270;&#35282;&#19979;&#36991;&#24320;&#20154;&#29289;&#26816;&#27979;&#65292;&#23454;&#29616;&#33258;&#28982;&#22806;&#35266;&#30340;&#26381;&#35013;&#32441;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25552;&#20986;&#20102;&#21046;&#20316;&#23545;&#25239;&#24615;&#26381;&#35013;&#26469;&#36867;&#36991;&#20154;&#29289;&#26816;&#27979;&#22120;&#65292;&#20294;&#35201;&#20040;&#21482;&#23545;&#38480;&#23450;&#30340;&#35270;&#35282;&#26377;&#25928;&#65292;&#35201;&#20040;&#23545;&#20154;&#31867;&#38750;&#24120;&#26126;&#26174;&#12290;&#25105;&#20204;&#26088;&#22312;&#22522;&#20110;3D&#24314;&#27169;&#26469;&#21046;&#20316;&#23545;&#25239;&#24615;&#30340;&#26381;&#35013;&#32441;&#29702;&#65292;&#36825;&#20010;&#24819;&#27861;&#24050;&#32463;&#34987;&#29992;&#20110;&#21046;&#20316;&#21018;&#24615;&#30340;&#23545;&#25239;&#24615;&#29289;&#20307;&#65292;&#22914;3D&#25171;&#21360;&#30340;&#20044;&#40863;&#12290;&#19982;&#21018;&#24615;&#29289;&#20307;&#19981;&#21516;&#65292;&#20154;&#31867;&#21644;&#26381;&#35013;&#26159;&#38750;&#21018;&#24615;&#30340;&#65292;&#36825;&#23548;&#33268;&#20102;&#22312;&#23454;&#38469;&#21046;&#20316;&#20013;&#30340;&#22256;&#38590;&#12290;&#20026;&#20102;&#21046;&#20316;&#20986;&#30475;&#36215;&#26469;&#33258;&#28982;&#30340;&#23545;&#25239;&#24615;&#26381;&#35013;&#65292;&#21487;&#20197;&#22312;&#22810;&#20010;&#35270;&#35282;&#19979;&#36991;&#24320;&#20154;&#29289;&#26816;&#27979;&#22120;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31867;&#20284;&#20110;&#26085;&#24120;&#26381;&#35013;&#32441;&#29702;&#20043;&#19968;&#30340;&#23545;&#25239;&#24615;&#20266;&#35013;&#32441;&#29702;&#65288;AdvCaT&#65289;&#65292;&#21363;&#20266;&#35013;&#32441;&#29702;&#12290;&#25105;&#20204;&#21033;&#29992;Voronoi&#22270;&#21644;Gumbel-softmax&#25216;&#24039;&#26469;&#21442;&#25968;&#21270;&#20266;&#35013;&#32441;&#29702;&#65292;&#24182;&#36890;&#36807;3D&#24314;&#27169;&#26469;&#20248;&#21270;&#21442;&#25968;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#39640;&#25928;&#30340;&#22686;&#24378;&#31649;&#36947;&#65292;&#23558;&#25299;&#25169;&#21512;&#29702;&#30340;&#25237;&#24433;&#65288;TopoProj&#65289;&#21644;Thin Plate Spline&#65288;TPS&#65289;&#32467;&#21512;&#22312;3D&#32593;&#26684;&#19978;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent works have proposed to craft adversarial clothes for evading person detectors, while they are either only effective at limited viewing angles or very conspicuous to humans. We aim to craft adversarial texture for clothes based on 3D modeling, an idea that has been used to craft rigid adversarial objects such as a 3D-printed turtle. Unlike rigid objects, humans and clothes are non-rigid, leading to difficulties in physical realization. In order to craft natural-looking adversarial clothes that can evade person detectors at multiple viewing angles, we propose adversarial camouflage textures (AdvCaT) that resemble one kind of the typical textures of daily clothes, camouflage textures. We leverage the Voronoi diagram and Gumbel-softmax trick to parameterize the camouflage textures and optimize the parameters via 3D modeling. Moreover, we propose an efficient augmentation pipeline on 3D meshes combining topologically plausible projection (TopoProj) and Thin Plate Spline (TPS) to narr
&lt;/p&gt;</description></item><item><title>MOPO-LSI&#26159;&#19968;&#27454;&#24320;&#28304;&#30340;&#22810;&#30446;&#26631;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#24211;&#65292;&#20026;&#21487;&#25345;&#32493;&#25237;&#36164;&#25552;&#20379;&#29992;&#25143;&#25351;&#21335;&#65292;&#24182;&#20171;&#32461;&#20102;&#29256;&#26412;1.0&#30340;&#38382;&#39064;&#35774;&#32622;&#12289;&#24037;&#20316;&#27969;&#31243;&#21644;&#36229;&#21442;&#25968;&#12290;</title><link>http://arxiv.org/abs/2307.01719</link><description>&lt;p&gt;
MOPO-LSI&#65306;&#29992;&#25143;&#25351;&#21335;
&lt;/p&gt;
&lt;p&gt;
MOPO-LSI: A User Guide. (arXiv:2307.01719v1 [q-fin.PM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01719
&lt;/p&gt;
&lt;p&gt;
MOPO-LSI&#26159;&#19968;&#27454;&#24320;&#28304;&#30340;&#22810;&#30446;&#26631;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#24211;&#65292;&#20026;&#21487;&#25345;&#32493;&#25237;&#36164;&#25552;&#20379;&#29992;&#25143;&#25351;&#21335;&#65292;&#24182;&#20171;&#32461;&#20102;&#29256;&#26412;1.0&#30340;&#38382;&#39064;&#35774;&#32622;&#12289;&#24037;&#20316;&#27969;&#31243;&#21644;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MOPO-LSI&#26159;&#19968;&#31181;&#24320;&#28304;&#30340;&#21487;&#25345;&#32493;&#25237;&#36164;&#22810;&#30446;&#26631;&#25237;&#36164;&#32452;&#21512;&#20248;&#21270;&#24211;&#12290;&#26412;&#25991;&#25552;&#20379;&#20102;MOPO-LSI&#29256;&#26412;1.0&#30340;&#29992;&#25143;&#25351;&#21335;&#65292;&#21253;&#25324;&#38382;&#39064;&#35774;&#32622;&#12289;&#24037;&#20316;&#27969;&#31243;&#21644;&#37197;&#32622;&#20013;&#30340;&#36229;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
MOPO-LSI is an open-source Multi-Objective Portfolio Optimization Library for Sustainable Investments. This document provides a user guide for MOPO-LSI version 1.0, including problem setup, workflow and the hyper-parameters in configurations.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#34987;&#24191;&#27867;&#29992;&#20110;&#22686;&#24378;&#21382;&#21490;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25918;&#22823;&#31232;&#26377;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#20197;&#21450;&#21019;&#24314;&#21453;&#20107;&#23454;&#24773;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#28385;&#36275;&#32422;&#26463;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#19988;&#35745;&#31639;&#20195;&#20215;&#39640;&#65292;&#25110;&#32773;&#22312;&#22797;&#26434;&#32422;&#26463;&#26465;&#20214;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;</title><link>http://arxiv.org/abs/2307.01717</link><description>&lt;p&gt;
&#20851;&#20110;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Constrained Time-Series Generation Problem. (arXiv:2307.01717v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01717
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#29983;&#25104;&#38382;&#39064;&#12290;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#65292;&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#34987;&#24191;&#27867;&#29992;&#20110;&#22686;&#24378;&#21382;&#21490;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25918;&#22823;&#31232;&#26377;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#20197;&#21450;&#21019;&#24314;&#21453;&#20107;&#23454;&#24773;&#26223;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#28385;&#36275;&#32422;&#26463;&#26041;&#38754;&#23384;&#22312;&#38382;&#39064;&#65292;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#19988;&#35745;&#31639;&#20195;&#20215;&#39640;&#65292;&#25110;&#32773;&#22312;&#22797;&#26434;&#32422;&#26463;&#26465;&#20214;&#19979;&#19981;&#20999;&#23454;&#38469;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#26102;&#38388;&#24207;&#21015;&#32463;&#24120;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#29992;&#20110;&#22686;&#21152;&#21382;&#21490;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#38598;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#24615;&#33021;&#65292;&#25918;&#22823;&#31232;&#26377;&#20107;&#20214;&#30340;&#21457;&#29983;&#65292;&#24182;&#21019;&#24314;&#30001;&#26102;&#38388;&#24207;&#21015;&#25551;&#36848;&#30340;&#21453;&#20107;&#23454;&#24773;&#26223;&#12290;&#20998;&#24067;&#30456;&#20284;&#24615;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30495;&#23454;&#24615;&#65289;&#20197;&#21450;&#28385;&#36275;&#19968;&#23450;&#25968;&#20540;&#32422;&#26463;&#26159;&#21453;&#20107;&#23454;&#26102;&#38388;&#24207;&#21015;&#22330;&#26223;&#29983;&#25104;&#35831;&#27714;&#20013;&#24120;&#35265;&#30340;&#35201;&#27714;&#12290;&#20363;&#22914;&#65292;&#32654;&#32852;&#20648;&#21457;&#24067;&#20102;&#32473;&#23450;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#30340;&#21512;&#25104;&#24066;&#22330;&#21387;&#21147;&#24773;&#26223;&#65292;&#20379;&#37329;&#34701;&#26426;&#26500;&#35780;&#20272;&#20854;&#22312;&#20551;&#35774;&#24615;&#34928;&#36864;&#20013;&#30340;&#34920;&#29616;&#12290;&#29616;&#26377;&#30340;&#29983;&#25104;&#32422;&#26463;&#26102;&#38388;&#24207;&#21015;&#30340;&#26041;&#27861;&#36890;&#24120;&#36890;&#36807;&#23545;&#25439;&#22833;&#20989;&#25968;&#36827;&#34892;&#24809;&#32602;&#26469;&#24378;&#21046;&#28385;&#36275;&#32422;&#26463;&#65292;&#24182;&#25298;&#32477;&#19981;&#31526;&#21512;&#32422;&#26463;&#30340;&#26679;&#26412;&#12290;&#28982;&#32780;&#65292;&#22914;&#26524;&#25105;&#20204;&#25913;&#21464;&#32422;&#26463;&#26465;&#20214;&#65292;&#36825;&#20123;&#26041;&#27861;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#65292;&#32780;&#25298;&#32477;&#25277;&#26679;&#21487;&#33021;&#22312;&#35745;&#31639;&#19978;&#26159;&#26114;&#36149;&#30340;&#65292;&#25110;&#32773;&#22312;&#22797;&#26434;&#32422;&#26463;&#26465;&#20214;&#19979;&#26159;&#19981;&#20999;&#23454;&#38469;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic time series are often used in practical applications to augment the historical time series dataset for better performance of machine learning algorithms, amplify the occurrence of rare events, and also create counterfactual scenarios described by the time series. Distributional-similarity (which we refer to as realism) as well as the satisfaction of certain numerical constraints are common requirements in counterfactual time series scenario generation requests. For instance, the US Federal Reserve publishes synthetic market stress scenarios given by the constrained time series for financial institutions to assess their performance in hypothetical recessions. Existing approaches for generating constrained time series usually penalize training loss to enforce constraints, and reject non-conforming samples. However, these approaches would require re-training if we change constraints, and rejection sampling can be computationally expensive, or impractical for complex constraints.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#27169;&#22411;&#31561;&#25928;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#27169;&#22411;&#31561;&#20215;&#24615;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#27010;&#24565;&#24212;&#29992;&#20110;&#22686;&#24378;&#20219;&#20309;&#22522;&#20110;&#27169;&#22411;&#30340;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.01708</link><description>&lt;p&gt;
&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#20998;&#24067;&#27169;&#22411;&#31561;&#25928;&#24615;
&lt;/p&gt;
&lt;p&gt;
Distributional Model Equivalence for Risk-Sensitive Reinforcement Learning. (arXiv:2307.01708v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01708
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#20998;&#24067;&#27169;&#22411;&#31561;&#25928;&#24615;&#38382;&#39064;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#27169;&#22411;&#31561;&#20215;&#24615;&#27010;&#24565;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#23558;&#36825;&#20123;&#27010;&#24565;&#24212;&#29992;&#20110;&#22686;&#24378;&#20219;&#20309;&#22522;&#20110;&#27169;&#22411;&#30340;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#23398;&#20064;&#29992;&#20110;&#39118;&#38505;&#25935;&#24863;&#24378;&#21270;&#23398;&#20064;&#30340;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#36866;&#24403;&#30340;&#20215;&#20540;&#31561;&#20215;&#24615;&#65292;&#36825;&#26159;&#19968;&#31181;&#23398;&#20064;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#22312;&#39118;&#38505;&#20013;&#24615;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#26368;&#20248;&#35268;&#21010;&#65292;&#20294;&#22312;&#39118;&#38505;&#25935;&#24863;&#30340;&#24773;&#20917;&#19979;&#26080;&#27861;&#36827;&#34892;&#26368;&#20248;&#35268;&#21010;&#12290;&#25105;&#20204;&#21033;&#29992;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#24341;&#20837;&#20102;&#20004;&#31181;&#26032;&#30340;&#27169;&#22411;&#31561;&#20215;&#24615;&#27010;&#24565;&#65292;&#20854;&#20013;&#19968;&#20010;&#26159;&#36890;&#29992;&#30340;&#65292;&#21487;&#20197;&#29992;&#20110;&#38024;&#23545;&#20219;&#20309;&#39118;&#38505;&#24230;&#37327;&#36827;&#34892;&#35268;&#21010;&#65292;&#20294;&#26159;&#35745;&#31639;&#22797;&#26434;&#65307;&#21478;&#19968;&#20010;&#26159;&#23454;&#38469;&#30340;&#21464;&#20307;&#65292;&#20801;&#35768;&#36873;&#25321;&#21487;&#20197;&#36827;&#34892;&#26368;&#20248;&#35268;&#21010;&#30340;&#39118;&#38505;&#24230;&#37327;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22914;&#20309;&#20351;&#29992;&#25105;&#20204;&#30340;&#26694;&#26550;&#26469;&#22686;&#24378;&#20219;&#20309;&#22522;&#20110;&#27169;&#22411;&#30340;&#39118;&#38505;&#25935;&#24863;&#31639;&#27861;&#65292;&#24182;&#25552;&#20379;&#20102;&#34920;&#26684;&#21644;&#22823;&#35268;&#27169;&#23454;&#39564;&#26469;&#23637;&#31034;&#20854;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the problem of learning models for risk-sensitive reinforcement learning. We theoretically demonstrate that proper value equivalence, a method of learning models which can be used to plan optimally in the risk-neutral setting, is not sufficient to plan optimally in the risk-sensitive setting. We leverage distributional reinforcement learning to introduce two new notions of model equivalence, one which is general and can be used to plan for any risk measure, but is intractable; and a practical variation which allows one to choose which risk measures they may plan optimally for. We demonstrate how our framework can be used to augment any model-free risk-sensitive algorithm, and provide both tabular and large-scale experiments to demonstrate its ability.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31227;&#38500;&#20102;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#23545;&#36741;&#21161;&#25968;&#25454;&#30340;&#20551;&#35774;&#65292;&#20351;&#29992;&#21482;&#26377;&#21512;&#25104;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#33021;&#22815;&#25104;&#21151;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2307.01701</link><description>&lt;p&gt;
&#21512;&#25104;&#23601;&#26159;&#20320;&#38656;&#35201;&#30340;&#65306;&#31227;&#38500;&#38024;&#23545;&#21512;&#25104;&#25968;&#25454;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#30340;&#36741;&#21161;&#25968;&#25454;&#20551;&#35774;
&lt;/p&gt;
&lt;p&gt;
Synthetic is all you need: removing the auxiliary data assumption for membership inference attacks against synthetic data. (arXiv:2307.01701v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01701
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#31227;&#38500;&#20102;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#23545;&#36741;&#21161;&#25968;&#25454;&#30340;&#20551;&#35774;&#65292;&#20351;&#29992;&#21482;&#26377;&#21512;&#25104;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#20173;&#28982;&#33021;&#22815;&#25104;&#21151;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21512;&#25104;&#25968;&#25454;&#27491;&#22312;&#25104;&#20026;&#22312;&#20445;&#25252;&#38544;&#31169;&#30340;&#21516;&#26102;&#20849;&#20139;&#20010;&#20307;&#32423;&#25968;&#25454;&#30340;&#26368;&#26377;&#24076;&#26395;&#30340;&#35299;&#20915;&#26041;&#26696;&#12290;&#22522;&#20110;&#24433;&#23376;&#24314;&#27169;&#30340;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#24050;&#32463;&#25104;&#20026;&#35780;&#20272;&#21512;&#25104;&#25968;&#25454;&#38544;&#31169;&#30340;&#26631;&#20934;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#25915;&#20987;&#30446;&#21069;&#20551;&#35774;&#25915;&#20987;&#32773;&#21487;&#20197;&#35775;&#38382;&#19982;&#35757;&#32451;&#25968;&#25454;&#38598;&#30340;&#31867;&#20284;&#20998;&#24067;&#30340;&#36741;&#21161;&#25968;&#25454;&#38598;&#12290;&#36825;&#24448;&#24448;&#26159;&#19968;&#20010;&#38750;&#24120;&#24378;&#30340;&#20551;&#35774;&#65292;&#22312;&#23454;&#36341;&#20013;&#24456;&#38590;&#21457;&#29983;&#25915;&#20987;&#12290;&#25105;&#20204;&#22312;&#36825;&#37324;&#23637;&#31034;&#20102;&#22914;&#20309;&#31227;&#38500;&#36825;&#20010;&#20551;&#35774;&#65292;&#20197;&#21450;&#22914;&#20309;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#36827;&#34892;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#19977;&#31181;&#19981;&#21516;&#30340;&#25915;&#20987;&#22330;&#26223;&#20013;&#20165;&#20351;&#29992;&#21512;&#25104;&#25968;&#25454;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#20173;&#28982;&#25104;&#21151;&#65292;&#28041;&#21450;&#20004;&#20010;&#30495;&#23454;&#19990;&#30028;&#25968;&#25454;&#38598;&#21644;&#20004;&#20010;&#21512;&#25104;&#25968;&#25454;&#29983;&#25104;&#22120;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#23457;&#35745;&#21512;&#25104;&#25968;&#25454;&#21457;&#24067;&#35775;&#38382;&#36741;&#21161;&#25968;&#25454;&#38598;&#30340;&#24378;&#20551;&#35774;&#21487;&#20197;&#25918;&#26494;&#20197;&#36827;&#34892;&#23454;&#38469;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;
Synthetic data is emerging as the most promising solution to share individual-level data while safeguarding privacy. Membership inference attacks (MIAs), based on shadow modeling, have become the standard to evaluate the privacy of synthetic data. These attacks, however, currently assume the attacker to have access to an auxiliary dataset sampled from a similar distribution as the training dataset. This often is a very strong assumption that would make an attack unlikely to happen in practice. We here show how this assumption can be removed and how MIAs can be performed using only the synthetic data. More specifically, in three different attack scenarios using only synthetic data, our results demonstrate that MIAs are still successful, across two real-world datasets and two synthetic data generators. These results show how the strong hypothesis made when auditing synthetic data releases access to an auxiliary dataset - can be relaxed to perform an actual attack.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20381;&#36182;ERM&#39044;&#35328;&#26426;&#35843;&#29992;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#36951;&#25022;&#65292;&#24182;&#22312;&#19981;&#21487;&#30693;&#24773;&#20917;&#19979;&#20855;&#26377;&#20122;&#32447;&#24615;&#22686;&#38271;&#30340;&#36951;&#25022;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#29992;&#20110;&#38750;&#21442;&#25968;&#21338;&#24328;&#29615;&#22659;&#20013;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;&#20165;&#20381;&#36182;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25910;&#25947;&#21040;&#36817;&#20284;&#26497;&#23567;-&#26497;&#22823;&#22343;&#34913;&#28857;&#12290;</title><link>http://arxiv.org/abs/2307.01689</link><description>&lt;p&gt;
&#22312;&#32447;&#23398;&#20064;&#21644;&#20351;&#29992;ERM&#39044;&#35328;&#26426;&#35299;&#20915;&#26080;&#31351;&#21338;&#24328;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Online Learning and Solving Infinite Games with an ERM Oracle. (arXiv:2307.01689v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01689
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#24037;&#20316;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20381;&#36182;ERM&#39044;&#35328;&#26426;&#35843;&#29992;&#30340;&#22312;&#32447;&#23398;&#20064;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#22312;&#21487;&#23454;&#29616;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#36951;&#25022;&#65292;&#24182;&#22312;&#19981;&#21487;&#30693;&#24773;&#20917;&#19979;&#20855;&#26377;&#20122;&#32447;&#24615;&#22686;&#38271;&#30340;&#36951;&#25022;&#12290;&#21516;&#26102;&#65292;&#36824;&#25552;&#20379;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#29992;&#20110;&#38750;&#21442;&#25968;&#21338;&#24328;&#29615;&#22659;&#20013;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#21363;&#20165;&#20381;&#36182;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25910;&#25947;&#21040;&#36817;&#20284;&#26497;&#23567;-&#26497;&#22823;&#22343;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22522;&#20110;&#22312;&#32447;&#23398;&#20064;&#30340;&#24773;&#20917;&#19979;&#65292;ERM&#36275;&#20197;&#36798;&#21040;&#25509;&#36817;&#26368;&#20248;&#27867;&#21270;&#35823;&#24046;&#30340;&#30446;&#26631;&#65292;&#20294;&#22312;&#22312;&#32447;&#23398;&#20064;&#29615;&#22659;&#19979;&#24182;&#38750;&#22914;&#27492;&#65292;&#36890;&#24120;&#30340;&#27010;&#24565;&#31867;&#31639;&#27861;&#20381;&#36182;&#35745;&#31639;&#25928;&#29575;&#36739;&#20302;&#30340;&#39044;&#35328;&#26426;&#65292;&#22914;&#26631;&#20934;&#26368;&#20248;&#31639;&#27861;(SOA)&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20165;&#20381;&#36182;ERM&#39044;&#35328;&#26426;&#35843;&#29992;&#30340;&#22312;&#32447;&#20108;&#20998;&#31867;&#31639;&#27861;&#65292;&#24182;&#35777;&#26126;&#22312;&#21487;&#23454;&#29616;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#26377;&#38480;&#30340;&#36951;&#25022;(regret)&#65292;&#22312;&#19981;&#21487;&#30693;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#20122;&#32447;&#24615;&#22686;&#38271;&#30340;&#36951;&#25022;&#12290;&#25105;&#20204;&#36890;&#36807;&#24213;&#23618;&#27010;&#24565;&#31867;&#30340;Littlestone&#21644;&#38408;&#20540;&#32500;&#24230;&#26469;&#38480;&#21046;&#36951;&#25022;&#12290;&#25105;&#20204;&#33719;&#24471;&#20102;&#31867;&#20284;&#30340;&#32467;&#26524;&#29992;&#20110;&#38750;&#21442;&#25968;&#21338;&#24328;&#65292;&#20854;&#20013;ERM&#39044;&#35328;&#26426;&#21487;&#20197;&#34987;&#29702;&#35299;&#20026;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#65292;&#26681;&#25454;&#20854;&#20182;&#29609;&#23478;&#30340;&#28216;&#25103;&#21382;&#21490;&#25214;&#21040;&#19968;&#20010;&#29609;&#23478;&#30340;&#26368;&#20339;&#21709;&#24212;&#12290;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20165;&#20381;&#36182;&#26368;&#20339;&#21709;&#24212;&#39044;&#35328;&#26426;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#24182;&#25910;&#25947;&#21040;&#20004;&#20154;&#38646;&#21644;&#21338;&#24328;&#30340;&#36817;&#20284;&#26497;&#23567;-&#26497;&#22823;&#22343;&#34913;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
While ERM suffices to attain near-optimal generalization error in the stochastic learning setting, this is not known to be the case in the online learning setting, where algorithms for general concept classes rely on computationally inefficient oracles such as the Standard Optimal Algorithm (SOA). In this work, we propose an algorithm for online binary classification setting that relies solely on ERM oracle calls, and show that it has finite regret in the realizable setting and sublinearly growing regret in the agnostic setting. We bound the regret in terms of the Littlestone and threshold dimensions of the underlying concept class.  We obtain similar results for nonparametric games, where the ERM oracle can be interpreted as a best response oracle, finding the best response of a player to a given history of play of the other players. In this setting, we provide learning algorithms that only rely on best response oracles and converge to approximate-minimax equilibria in two-player zero
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23454;&#26102;GNN&#25512;&#26029;&#26694;&#26550;Fograph&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#38752;&#36817;&#29289;&#32852;&#32593;&#25968;&#25454;&#28304;&#30340;&#22810;&#20010;&#38654;&#33410;&#28857;&#30340;&#36164;&#28304;&#65292;&#36890;&#36807;&#24341;&#20837;&#24322;&#26500;&#24863;&#30693;&#25191;&#34892;&#35268;&#21010;&#21644;GNN&#29305;&#23450;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21457;&#25381;&#38654;&#35745;&#31639;&#30340;&#26550;&#26500;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2307.01684</link><description>&lt;p&gt;
&#20351;&#29992;&#20998;&#24067;&#24335;&#38654;&#26381;&#21153;&#22120;&#20026;&#26234;&#33021;&#29289;&#32852;&#32593;&#26381;&#21153;&#25552;&#20379;&#22270;&#31070;&#32463;&#32593;&#32476;&#25903;&#25345;
&lt;/p&gt;
&lt;p&gt;
Serving Graph Neural Networks With Distributed Fog Servers For Smart IoT Services. (arXiv:2307.01684v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01684
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#23454;&#26102;GNN&#25512;&#26029;&#26694;&#26550;Fograph&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#38752;&#36817;&#29289;&#32852;&#32593;&#25968;&#25454;&#28304;&#30340;&#22810;&#20010;&#38654;&#33410;&#28857;&#30340;&#36164;&#28304;&#65292;&#36890;&#36807;&#24341;&#20837;&#24322;&#26500;&#24863;&#30693;&#25191;&#34892;&#35268;&#21010;&#21644;GNN&#29305;&#23450;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;&#20197;&#26368;&#22823;&#38480;&#24230;&#22320;&#21457;&#25381;&#38654;&#35745;&#31639;&#30340;&#26550;&#26500;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#20854;&#22312;&#22270;&#32467;&#26500;&#19978;&#25552;&#21462;&#28508;&#22312;&#34920;&#31034;&#33021;&#21147;&#30340;&#31361;&#20986;&#20248;&#21183;&#65292;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#20026;&#20102;&#20026;&#29289;&#32852;&#32593;&#39537;&#21160;&#30340;&#26234;&#33021;&#24212;&#29992;&#25552;&#20379;&#22522;&#20110;GNN&#30340;&#26381;&#21153;&#65292;&#20256;&#32479;&#30340;&#27169;&#22411;&#26381;&#21153;&#33539;&#20363;&#36890;&#24120;&#36890;&#36807;&#23436;&#20840;&#19978;&#20256;&#22320;&#29702;&#20998;&#24067;&#30340;&#36755;&#20837;&#25968;&#25454;&#21040;&#36828;&#31243;&#25968;&#25454;&#20013;&#24515;&#26469;&#20381;&#36182;&#20113;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#23454;&#35777;&#27979;&#37327;&#26174;&#31034;&#36825;&#31181;&#22522;&#20110;&#20113;&#30340;&#26381;&#21153;&#23384;&#22312;&#26174;&#33879;&#30340;&#36890;&#20449;&#24320;&#38144;&#65292;&#24182;&#19988;&#24378;&#35843;&#20102;&#24212;&#29992;&#26032;&#20852;&#30340;&#38654;&#35745;&#31639;&#30340;&#28508;&#22312;&#24040;&#22823;&#28508;&#21147;&#12290;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#21457;&#25381;&#38654;&#35745;&#31639;&#24102;&#26469;&#30340;&#26550;&#26500;&#20248;&#21183;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#20998;&#24067;&#24335;&#23454;&#26102;GNN&#25512;&#26029;&#26694;&#26550;Fograph&#65292;&#35813;&#26694;&#26550;&#21033;&#29992;&#20102;&#38752;&#36817;&#29289;&#32852;&#32593;&#25968;&#25454;&#28304;&#30340;&#22810;&#20010;&#38654;&#33410;&#28857;&#30340;&#22810;&#26679;&#24615;&#21644;&#21160;&#24577;&#36164;&#28304;&#12290;&#36890;&#36807;&#24341;&#20837;&#24322;&#26500;&#24863;&#30693;&#25191;&#34892;&#35268;&#21010;&#21644;GNN&#29305;&#23450;&#30340;&#21387;&#32553;&#25216;&#26415;&#65292;Fograph&#30340;&#35774;&#35745;&#19982;GNN&#26381;&#21153;&#30340;&#29420;&#29305;&#29305;&#24449;&#30456;&#22865;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) have gained growing interest in miscellaneous applications owing to their outstanding ability in extracting latent representation on graph structures. To render GNN-based service for IoT-driven smart applications, traditional model serving paradigms usually resort to the cloud by fully uploading geo-distributed input data to remote datacenters. However, our empirical measurements reveal the significant communication overhead of such cloud-based serving and highlight the profound potential in applying the emerging fog computing. To maximize the architectural benefits brought by fog computing, in this paper, we present Fograph, a novel distributed real-time GNN inference framework that leverages diverse and dynamic resources of multiple fog nodes in proximity to IoT data sources. By introducing heterogeneity-aware execution planning and GNN-specific compression techniques, Fograph tailors its design to well accommodate the unique characteristics of GNN servin
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23616;&#37096;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#23398;&#20064;&#31163;&#25955;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20108;&#20540;&#21270;&#32593;&#32476;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.01683</link><description>&lt;p&gt;
&#20351;&#29992;&#23616;&#37096;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#23398;&#20064;&#31163;&#25955;&#26435;&#37325;&#21644;&#28608;&#27963;
&lt;/p&gt;
&lt;p&gt;
Learning Discrete Weights and Activations Using the Local Reparameterization Trick. (arXiv:2307.01683v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01683
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#23616;&#37096;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#23398;&#20064;&#31163;&#25955;&#26435;&#37325;&#21644;&#28608;&#27963;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20108;&#20540;&#21270;&#32593;&#32476;&#26469;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#65292;&#25552;&#39640;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#19968;&#20010;&#20851;&#38190;&#30340;&#25361;&#25112;&#26159;&#38477;&#20302;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#30340;&#35745;&#31639;&#21644;&#20869;&#23384;&#38656;&#27714;&#12290;&#19968;&#20010;&#24120;&#35265;&#30340;&#35299;&#20915;&#26041;&#26696;&#26159;&#20351;&#29992;&#20108;&#20540;&#21270;&#12290;&#36890;&#36807;&#23545;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#36827;&#34892;&#20108;&#20540;&#21270;&#65292;&#21487;&#20197;&#36890;&#36807;&#29992;&#26356;&#24555;&#30340;&#20301;&#36816;&#31639;&#26367;&#20195;&#35745;&#31639;&#22797;&#26434;&#30340;&#28014;&#28857;&#25805;&#20316;&#26469;&#26174;&#33879;&#38477;&#20302;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#36825;&#23548;&#33268;&#20102;&#19968;&#20010;&#26356;&#39640;&#25928;&#30340;&#31070;&#32463;&#32593;&#32476;&#25512;&#29702;&#65292;&#21487;&#20197;&#37096;&#32626;&#22312;&#36164;&#28304;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25193;&#23637;&#20102;&#20043;&#21069;&#20351;&#29992;&#23616;&#37096;&#37325;&#26032;&#21442;&#25968;&#21270;&#25216;&#24039;&#35757;&#32451;&#20855;&#26377;&#31163;&#25955;&#26435;&#37325;&#30340;&#32593;&#32476;&#30340;&#26041;&#27861;&#65292;&#20197;&#20801;&#35768;&#31163;&#25955;&#28608;&#27963;&#12290;&#21407;&#22987;&#26041;&#27861;&#20248;&#21270;&#20102;&#31163;&#25955;&#26435;&#37325;&#30340;&#20998;&#24067;&#65292;&#24182;&#20351;&#29992;&#20013;&#24515;&#26497;&#38480;&#23450;&#29702;&#23558;&#39044;&#28608;&#27963;&#36817;&#20284;&#20026;&#36830;&#32493;&#30340;&#39640;&#26031;&#20998;&#24067;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#27010;&#29575;&#24314;&#27169;&#20063;&#21487;&#20197;&#26377;&#25928;&#22320;&#35757;&#32451;&#20855;&#26377;&#31163;&#25955;&#28608;&#27963;&#30340;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
In computer vision and machine learning, a crucial challenge is to lower the computation and memory demands for neural network inference. A commonplace solution to address this challenge is through the use of binarization. By binarizing the network weights and activations, one can significantly reduce computational complexity by substituting the computationally expensive floating operations with faster bitwise operations. This leads to a more efficient neural network inference that can be deployed on low-resource devices. In this work, we extend previous approaches that trained networks with discrete weights using the local reparameterization trick to also allow for discrete activations. The original approach optimized a distribution over the discrete weights and uses the central limit theorem to approximate the pre-activation with a continuous Gaussian distribution. Here we show that the probabilistic modeling can also allow effective training of networks with discrete activation as w
&lt;/p&gt;</description></item><item><title>RaidEnv&#26159;&#19968;&#20010;&#26032;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#65292;&#29992;&#20110;MMORPG&#28216;&#25103;&#20013;boss raid&#24773;&#26223;&#30340;&#33258;&#21160;&#20869;&#23481;&#24179;&#34913;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20004;&#20010;&#22522;&#20934;&#26469;&#36741;&#21161;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.01676</link><description>&lt;p&gt;
RaidEnv: &#25506;&#32034;Boss Raid&#28216;&#25103;&#20013;&#33258;&#21160;&#20869;&#23481;&#24179;&#34913;&#30340;&#26032;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
RaidEnv: Exploring New Challenges in Automated Content Balancing for Boss Raid Games. (arXiv:2307.01676v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01676
&lt;/p&gt;
&lt;p&gt;
RaidEnv&#26159;&#19968;&#20010;&#26032;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#65292;&#29992;&#20110;MMORPG&#28216;&#25103;&#20013;boss raid&#24773;&#26223;&#30340;&#33258;&#21160;&#20869;&#23481;&#24179;&#34913;&#30740;&#31350;&#65292;&#24182;&#25552;&#20379;&#20004;&#20010;&#22522;&#20934;&#26469;&#36741;&#21161;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28216;&#25103;&#20869;&#23481;&#30340;&#24179;&#34913;&#26174;&#33879;&#24433;&#21709;&#28216;&#25103;&#20307;&#39564;&#12290;&#19981;&#24179;&#34913;&#30340;&#28216;&#25103;&#20869;&#23481;&#20250;&#38477;&#20302;&#21442;&#19982;&#24230;&#25110;&#22686;&#21152;&#37325;&#22797;&#22833;&#36133;&#30340;&#25387;&#36133;&#24863;&#12290;&#34429;&#28982;&#28216;&#25103;&#35774;&#35745;&#24072;&#26377;&#24847;&#35843;&#25972;&#28216;&#25103;&#20869;&#23481;&#30340;&#38590;&#24230;&#65292;&#20294;&#36825;&#26159;&#19968;&#20010;&#32321;&#29712;&#12289;&#21171;&#21160;&#23494;&#38598;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#36807;&#31243;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#20869;&#23481;&#20016;&#23500;&#30340;&#21830;&#19994;&#32423;&#28216;&#25103;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#28216;&#25103;&#30740;&#31350;&#30028;&#25506;&#32034;&#20102;&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25216;&#26415;&#36827;&#34892;&#33258;&#21160;&#28216;&#25103;&#24179;&#34913;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#20197;&#24448;&#30340;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#26377;&#38480;&#30340;&#28216;&#25103;&#20869;&#23481;&#19978;&#65292;&#24182;&#27809;&#26377;&#32771;&#34385;&#21040;&#28216;&#25103;&#27979;&#35797;&#20195;&#29702;&#22312;&#36935;&#21040;&#20869;&#23481;&#21464;&#21270;&#26102;&#30340;&#27867;&#21270;&#33021;&#21147;&#30340;&#37325;&#35201;&#24615;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RaidEnv&#65292;&#36825;&#26159;&#19968;&#20010;&#26032;&#30340;&#28216;&#25103;&#27169;&#25311;&#22120;&#65292;&#21253;&#25324;&#22810;&#26679;&#21270;&#19988;&#21487;&#23450;&#21046;&#30340;&#20869;&#23481;&#65292;&#36866;&#29992;&#20110;MMORPG&#28216;&#25103;&#20013;&#30340;boss raid&#24773;&#26223;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;boss raid&#24773;&#26223;&#30340;&#22522;&#20934;&#65292;&#21487;&#20197;&#36741;&#21161;&#28216;&#25103;&#20154;&#24037;&#26234;&#33021;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#36825;&#20123;&#22522;&#20934;&#35299;&#20915;&#20102;&#20004;&#20010;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The balance of game content significantly impacts the gaming experience. Unbalanced game content diminishes engagement or increases frustration because of repetitive failure. Although game designers intend to adjust the difficulty of game content, this is a repetitive, labor-intensive, and challenging process, especially for commercial-level games with extensive content. To address this issue, the game research community has explored automated game balancing using artificial intelligence (AI) techniques. However, previous studies have focused on limited game content and did not consider the importance of the generalization ability of playtesting agents when encountering content changes. In this study, we propose RaidEnv, a new game simulator that includes diverse and customizable content for the boss raid scenario in MMORPG games. Additionally, we design two benchmarks for the boss raid scenario that can aid in the practical application of game AI. These benchmarks address two open pro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;</title><link>http://arxiv.org/abs/2307.01646</link><description>&lt;p&gt;
SwinGNN:&#37325;&#26032;&#24605;&#32771;&#22312;&#22270;&#29983;&#25104;&#30340;&#25193;&#25955;&#27169;&#22411;&#20013;&#30340;&#32622;&#25442;&#19981;&#21464;&#24615;
&lt;/p&gt;
&lt;p&gt;
SwinGNN: Rethinking Permutation Invariance in Diffusion Models for Graph Generation. (arXiv:2307.01646v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01646
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22270;&#29983;&#25104;&#25193;&#25955;&#27169;&#22411;SwinGNN&#65292;&#36890;&#36807;&#20351;&#29992;&#39640;&#25928;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#21644;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#65292;&#20197;&#21450;&#32467;&#21512;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#65292;&#24182;&#24341;&#20837;&#20102;&#38543;&#26426;&#32622;&#25442;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#36716;&#25442;&#29983;&#25104;&#30340;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#32622;&#25442;&#31561;&#21464;&#32593;&#32476;&#30340;&#25193;&#25955;&#27169;&#22411;&#21487;&#20197;&#23398;&#20064;&#22270;&#25968;&#25454;&#30340;&#32622;&#25442;&#19981;&#21464;&#20998;&#24067;&#12290;&#28982;&#32780;&#65292;&#30456;&#23545;&#20110;&#38750;&#19981;&#21464;&#27169;&#22411;&#65292;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#19981;&#21464;&#27169;&#22411;&#36935;&#21040;&#20102;&#26356;&#22823;&#30340;&#23398;&#20064;&#25361;&#25112;&#65292;&#22240;&#20026;1&#65289;&#23427;&#20204;&#30340;&#30446;&#26631;&#20998;&#24067;&#26356;&#20855;&#27169;&#24577;&#24615;&#65307;2&#65289;&#23427;&#20204;&#30340;&#26368;&#20248;&#19968;&#27493;&#21435;&#22122;&#24471;&#20998;&#26159;&#20855;&#26377;&#26356;&#22810;&#25104;&#20998;&#30340;&#39640;&#26031;&#28151;&#21512;&#29289;&#30340;&#24471;&#20998;&#20989;&#25968;&#12290;&#21463;&#21040;&#36825;&#20010;&#20998;&#26512;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38750;&#19981;&#21464;&#30340;&#25193;&#25955;&#27169;&#22411;&#65292;&#31216;&#20026;&#8220;SwinGNN&#8221;&#65292;&#23427;&#37319;&#29992;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#36793;&#21040;&#36793;&#30340;2-WL&#28040;&#24687;&#20256;&#36882;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;SwinTransformers&#20013;&#30340;&#31227;&#21160;&#31383;&#21475;&#33258;&#27880;&#24847;&#21147;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#31995;&#32479;&#24615;&#30340;&#23454;&#39564;&#21644;&#21078;&#26512;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#20960;&#31181;&#20851;&#38190;&#30340;&#35757;&#32451;&#21644;&#37319;&#26679;&#25216;&#26415;&#65292;&#26174;&#33879;&#25552;&#39640;&#20102;&#22270;&#29983;&#25104;&#26679;&#26412;&#30340;&#36136;&#37327;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#30340;&#21518;&#22788;&#29702;&#25216;&#24039;&#65292;&#21363;&#38543;&#26426;&#32622;&#25442;&#29983;&#25104;&#30340;&#22270;&#65292;&#21487;&#20197;&#35777;&#26126;&#23558;&#20219;&#20309;&#22270;&#36716;&#25442;&#25104;&#22270;&#24418;&#32479;&#35745;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Diffusion models based on permutation-equivariant networks can learn permutation-invariant distributions for graph data. However, in comparison to their non-invariant counterparts, we have found that these invariant models encounter greater learning challenges since 1) their effective target distributions exhibit more modes; 2) their optimal one-step denoising scores are the score functions of Gaussian mixtures with more components. Motivated by this analysis, we propose a non-invariant diffusion model, called $\textit{SwinGNN}$, which employs an efficient edge-to-edge 2-WL message passing network and utilizes shifted window based self-attention inspired by SwinTransformers. Further, through systematic ablations, we identify several critical training and sampling techniques that significantly improve the sample quality of graph generation. At last, we introduce a simple post-processing trick, $\textit{i.e.}$, randomly permuting the generated graphs, which provably converts any graph ge
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#29305;&#23450;&#24037;&#20855;&#25903;&#25345;&#19979;&#30340;&#23545;&#35805;&#24335;&#26234;&#33021;&#21161;&#25163;&#30340;&#25554;&#20837;&#25193;&#23637;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;&#20316;&#20026;&#24037;&#20855;&#65292;&#22312;&#29983;&#25104;&#26126;&#30830;&#25512;&#29702;&#36335;&#24452;&#26102;&#25552;&#20379;&#26356;&#22810;&#32454;&#33410;&#21644;&#26356;&#31934;&#30830;&#30340;&#35831;&#27714;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24037;&#20855;&#24178;&#25200;&#29992;&#25143;&#24847;&#22270;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20004;&#20010;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#25512;&#33616;&#39046;&#22495;&#20013;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#24102;&#26469;&#20102;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2307.01644</link><description>&lt;p&gt;
&#29305;&#23450;&#24037;&#20855;&#25903;&#25345;&#19979;&#30340;&#23545;&#35805;&#24335;&#26234;&#33021;&#21161;&#25163;&#30340;&#25554;&#20837;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Insert-expansions for Tool-enabled Conversational Agents. (arXiv:2307.01644v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01644
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#29305;&#23450;&#24037;&#20855;&#25903;&#25345;&#19979;&#30340;&#23545;&#35805;&#24335;&#26234;&#33021;&#21161;&#25163;&#30340;&#25554;&#20837;&#25193;&#23637;&#12290;&#36890;&#36807;&#23558;&#29992;&#25143;&#20316;&#20026;&#24037;&#20855;&#65292;&#22312;&#29983;&#25104;&#26126;&#30830;&#25512;&#29702;&#36335;&#24452;&#26102;&#25552;&#20379;&#26356;&#22810;&#32454;&#33410;&#21644;&#26356;&#31934;&#30830;&#30340;&#35831;&#27714;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#24037;&#20855;&#24178;&#25200;&#29992;&#25143;&#24847;&#22270;&#30340;&#38382;&#39064;&#12290;&#36890;&#36807;&#20004;&#20010;&#23454;&#35777;&#30740;&#31350;&#65292;&#21457;&#29616;&#22312;&#25512;&#33616;&#39046;&#22495;&#20013;&#20351;&#29992;&#36825;&#31181;&#26041;&#27861;&#24102;&#26469;&#20102;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28145;&#20837;&#30740;&#31350;&#20102;&#22312;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#23454;&#29616;Chain-of-Thought-Prompting&#30340;&#39640;&#32423;&#26041;&#24335;&#65292;&#37325;&#28857;&#30740;&#31350;&#20102;&#22312;&#36825;&#31181;&#25552;&#31034;&#26041;&#27861;&#29983;&#25104;&#30340;&#26126;&#30830;&#25512;&#29702;&#36335;&#24452;&#20013;&#20351;&#29992;&#24037;&#20855;&#65288;&#25110;&#8220;&#25554;&#20214;&#8221;&#65289;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#20351;&#29992;&#24037;&#20855;&#30340;&#23545;&#35805;&#24335;&#26234;&#33021;&#21161;&#25163;&#24448;&#24448;&#20250;&#36208;&#20559;&#65292;&#22240;&#20026;&#26469;&#33258;&#25628;&#32034;&#24341;&#25806;&#25110;&#35745;&#31639;&#22120;&#31561;&#24037;&#20855;&#30340;&#39069;&#22806;&#19978;&#19979;&#25991;&#20250;&#20559;&#31163;&#21407;&#22987;&#29992;&#25143;&#24847;&#22270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#19968;&#31181;&#27010;&#24565;&#65292;&#21363;&#29992;&#25143;&#25104;&#20026;&#24037;&#20855;&#65292;&#25552;&#20379;&#24517;&#35201;&#30340;&#32454;&#33410;&#24182;&#23436;&#21892;&#20182;&#20204;&#30340;&#35831;&#27714;&#12290;&#36890;&#36807;&#23545;&#35805;&#20998;&#26512;&#65292;&#25105;&#20204;&#23558;&#36825;&#31181;&#20132;&#20114;&#31216;&#20026;&#25554;&#20837;&#25193;&#23637;-&#19968;&#31181;&#26088;&#22312;&#20419;&#36827;&#25152;&#38656;&#21709;&#24212;&#30340;&#20013;&#38388;&#23545;&#35805;&#12290;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#23454;&#35777;&#30740;&#31350;&#20351;&#29992;&#30452;&#25509;&#27604;&#36739;&#30340;&#26041;&#27861;&#25506;&#32034;&#20102;&#20174;&#36825;&#31181;&#8220;&#29992;&#25143;&#20316;&#20026;&#24037;&#20855;&#8221;&#30340;&#26041;&#27861;&#20013;&#20135;&#29983;&#30340;&#21487;&#33021;&#24615;&#65292;&#24182;&#22312;&#25512;&#33616;&#39046;&#22495;&#20013;&#21457;&#29616;&#20102;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper delves into an advanced implementation of Chain-of-Thought-Prompting in Large Language Models, focusing on the use of tools (or "plug-ins") within the explicit reasoning paths generated by this prompting method. We find that tool-enabled conversational agents often become sidetracked, as additional context from tools like search engines or calculators diverts from original user intents. To address this, we explore a concept wherein the user becomes the tool, providing necessary details and refining their requests. Through Conversation Analysis, we characterize this interaction as insert-expansion - an intermediary conversation designed to facilitate the preferred response. We explore possibilities arising from this 'user-as-a-tool' approach in two empirical studies using direct comparison, and find benefits in the recommendation domain.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36817;&#20284;&#20114;&#36830;&#24615;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#35745;&#31639;&#20114;&#36830;&#24615;&#25351;&#26631;&#12290;&#31639;&#27861;&#36890;&#36807;&#24314;&#27169;&#30830;&#35748;&#20540;&#30340;&#20998;&#24067;&#24182;&#20272;&#35745;&#20854;&#27169;&#22411;&#21442;&#25968;&#65292;&#28982;&#21518;&#21033;&#29992;&#20998;&#24067;&#30340;&#26399;&#26395;&#20540;&#26469;&#36817;&#20284;&#35745;&#31639;&#20114;&#36830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01639</link><description>&lt;p&gt;
&#36817;&#20284;&#20114;&#36830;&#24615;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Heuristic Algorithms for the Approximation of Mutual Coherence. (arXiv:2307.01639v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01639
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36817;&#20284;&#20114;&#36830;&#24615;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#29992;&#20110;&#21152;&#36895;&#35745;&#31639;&#20114;&#36830;&#24615;&#25351;&#26631;&#12290;&#31639;&#27861;&#36890;&#36807;&#24314;&#27169;&#30830;&#35748;&#20540;&#30340;&#20998;&#24067;&#24182;&#20272;&#35745;&#20854;&#27169;&#22411;&#21442;&#25968;&#65292;&#28982;&#21518;&#21033;&#29992;&#20998;&#24067;&#30340;&#26399;&#26395;&#20540;&#26469;&#36817;&#20284;&#35745;&#31639;&#20114;&#36830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20114;&#36830;&#24615;&#26159;&#34913;&#37327;&#20004;&#31181;&#35266;&#28857;&#30456;&#20284;&#24615;&#30340;&#25351;&#26631;&#12290;&#23613;&#31649;&#35813;&#27010;&#24565;&#26469;&#33258;&#21746;&#23398;&#65292;&#20294;&#23427;&#22312;&#24191;&#27867;&#30340;&#25216;&#26415;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20363;&#22914;Wahl-O-Mat&#31995;&#32479;&#12290;&#22312;&#24503;&#22269;&#65292;&#35813;&#31995;&#32479;&#24110;&#21161;&#36873;&#27665;&#25214;&#21040;&#26368;&#31526;&#21512;&#20182;&#20204;&#25919;&#27835;&#20559;&#22909;&#30340;&#20505;&#36873;&#20154;&#12290;&#30001;&#20110;&#35201;&#36845;&#20195;&#36941;&#21382;&#35266;&#28857;&#30340;&#25152;&#26377;&#23376;&#38598;&#65292;&#20934;&#30830;&#35745;&#31639;&#20114;&#36830;&#24615;&#38750;&#24120;&#32791;&#26102;&#12290;&#32780;&#19988;&#65292;&#23545;&#20110;&#27599;&#20010;&#23376;&#38598;&#65292;&#24517;&#39035;&#35299;&#20915;&#19968;&#20010;SAT&#27169;&#22411;&#35745;&#25968;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#36825;&#22312;&#35745;&#31639;&#26426;&#31185;&#23398;&#20013;&#34987;&#35748;&#20026;&#26159;&#19968;&#20010;&#30828;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26159;&#21152;&#36895;&#36825;&#31181;&#35745;&#31639;&#30340;&#31532;&#19968;&#39033;&#30740;&#31350;&#12290;&#25105;&#20204;&#23558;&#25152;&#35859;&#30340;&#30830;&#35748;&#20540;&#30340;&#20998;&#24067;&#24314;&#27169;&#20026;&#19977;&#20010;&#39640;&#26031;&#28151;&#21512;&#65292;&#24182;&#25552;&#20986;&#20102;&#26377;&#25928;&#30340;&#21551;&#21457;&#24335;&#31639;&#27861;&#26469;&#20272;&#35745;&#20854;&#27169;&#22411;&#21442;&#25968;&#12290;&#28982;&#21518;&#65292;&#36890;&#36807;&#20998;&#24067;&#30340;&#26399;&#26395;&#20540;&#26469;&#36817;&#20284;&#20114;&#36830;&#24615;&#12290;&#20854;&#20013;&#19968;&#20123;&#31639;&#27861;&#26159;&#20840;&#22810;&#39033;&#24335;&#26102;&#38388;&#30340;&#65292;&#20854;&#20182;&#31639;&#27861;&#21482;&#38656;&#35201;&#35299;&#20915;&#23569;&#37327;&#30340;SA&#27169;&#22411;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Mutual coherence is a measure of similarity between two opinions. Although the notion comes from philosophy, it is essential for a wide range of technologies, e.g., the Wahl-O-Mat system. In Germany, this system helps voters to find candidates that are the closest to their political preferences. The exact computation of mutual coherence is highly time-consuming due to the iteration over all subsets of an opinion. Moreover, for every subset, an instance of the SAT model counting problem has to be solved which is known to be a hard problem in computer science. This work is the first study to accelerate this computation. We model the distribution of the so-called confirmation values as a mixture of three Gaussians and present efficient heuristics to estimate its model parameters. The mutual coherence is then approximated with the expected value of the distribution. Some of the presented algorithms are fully polynomial-time, others only require solving a small number of instances of the SA
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20010;&#32593;&#32476;&#19978;&#36827;&#34892;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#26469;&#36827;&#34892;&#26356;&#22909;&#30340;&#23454;&#20307;&#25512;&#26029;&#21644;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#25903;&#25345;&#22810;&#37325;&#32593;&#32476;&#21644;&#19968;&#33324;&#22810;&#32593;&#32476;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#25910;&#25947;&#24615;&#36136;&#12290;</title><link>http://arxiv.org/abs/2307.01637</link><description>&lt;p&gt;
&#22810;&#32593;&#32476;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;
&lt;/p&gt;
&lt;p&gt;
Random Walk on Multiple Networks. (arXiv:2307.01637v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22810;&#20010;&#32593;&#32476;&#19978;&#36827;&#34892;&#38543;&#26426;&#28216;&#36208;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#32593;&#32476;&#20013;&#30340;&#20449;&#24687;&#26469;&#36827;&#34892;&#26356;&#22909;&#30340;&#23454;&#20307;&#25512;&#26029;&#21644;&#20998;&#26512;&#12290;&#35813;&#26041;&#27861;&#25903;&#25345;&#22810;&#37325;&#32593;&#32476;&#21644;&#19968;&#33324;&#22810;&#32593;&#32476;&#65292;&#24182;&#19988;&#22312;&#29702;&#35770;&#19978;&#20855;&#26377;&#25910;&#25947;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#26426;&#28216;&#36208;&#26159;&#19968;&#31181;&#25506;&#32034;&#32593;&#32476;&#32467;&#26500;&#30340;&#22522;&#26412;&#31639;&#27861;&#65292;&#21487;&#20197;&#29992;&#20110;&#35768;&#22810;&#20219;&#21153;&#65292;&#22914;&#23616;&#37096;&#31038;&#21306;&#26816;&#27979;&#21644;&#32593;&#32476;&#23884;&#20837;&#12290;&#29616;&#26377;&#30340;&#38543;&#26426;&#28216;&#36208;&#26041;&#27861;&#22522;&#20110;&#21253;&#21547;&#26377;&#38480;&#20449;&#24687;&#30340;&#21333;&#20010;&#32593;&#32476;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#23454;&#38469;&#25968;&#25454;&#36890;&#24120;&#21253;&#21547;&#19981;&#21516;&#31867;&#22411;&#25110;&#26469;&#28304;&#30340;&#23454;&#20307;&#65292;&#36825;&#20123;&#23454;&#20307;&#26159;&#20840;&#38754;&#30340;&#65292;&#24182;&#19988;&#21487;&#20197;&#36890;&#36807;&#22810;&#20010;&#32593;&#32476;&#26356;&#22909;&#22320;&#24314;&#27169;&#12290;&#20026;&#20102;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#32593;&#32476;&#20013;&#30340;&#20016;&#23500;&#20449;&#24687;&#65292;&#24182;&#23545;&#23454;&#20307;&#36827;&#34892;&#26356;&#22909;&#30340;&#25512;&#26029;&#65292;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22810;&#32593;&#32476;&#19978;&#30340;&#38543;&#26426;&#28216;&#36208;&#65288;RWM&#65289;&#12290;RWM&#26159;&#28789;&#27963;&#30340;&#65292;&#25903;&#25345;&#22810;&#37325;&#32593;&#32476;&#21644;&#19968;&#33324;&#30340;&#22810;&#32593;&#32476;&#65292;&#21487;&#20197;&#22312;&#32593;&#32476;&#20043;&#38388;&#24418;&#25104;&#22810;&#23545;&#22810;&#30340;&#33410;&#28857;&#26144;&#23556;&#12290;RWM&#22312;&#27599;&#20010;&#32593;&#32476;&#19978;&#21457;&#36865;&#19968;&#20010;&#38543;&#26426;&#34892;&#32773;&#20197;&#33719;&#24471;&#19982;&#36215;&#22987;&#33410;&#28857;&#30456;&#20851;&#30340;&#23616;&#37096;&#25509;&#36817;&#24230;&#65288;&#21363;&#33410;&#28857;&#35775;&#38382;&#27010;&#29575;&#65289;&#12290;&#20855;&#26377;&#30456;&#20284;&#35775;&#38382;&#27010;&#29575;&#30340;&#34892;&#32773;&#30456;&#20114;&#24378;&#21270;&#12290;&#25105;&#20204;&#20174;&#29702;&#35770;&#19978;&#20998;&#26512;&#20102;RWM&#30340;&#25910;&#25947;&#24615;&#36136;&#12290;
&lt;/p&gt;
&lt;p&gt;
Random Walk is a basic algorithm to explore the structure of networks, which can be used in many tasks, such as local community detection and network embedding. Existing random walk methods are based on single networks that contain limited information. In contrast, real data often contain entities with different types or/and from different sources, which are comprehensive and can be better modeled by multiple networks. To take advantage of rich information in multiple networks and make better inferences on entities, in this study, we propose random walk on multiple networks, RWM. RWM is flexible and supports both multiplex networks and general multiple networks, which may form many-to-many node mappings between networks. RWM sends a random walker on each network to obtain the local proximity (i.e., node visiting probabilities) w.r.t. the starting nodes. Walkers with similar visiting probabilities reinforce each other. We theoretically analyze the convergence properties of RWM. Two appr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;SageFormer&#65292;&#19968;&#31181;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#26377;&#25928;&#25429;&#25417;&#21644;&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#34920;&#31034;&#19981;&#21516;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#20943;&#23569;&#24207;&#21015;&#38388;&#20887;&#20313;&#20449;&#24687;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01616</link><description>&lt;p&gt;
SageFormer&#65306;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer
&lt;/p&gt;
&lt;p&gt;
SageFormer: Series-Aware Graph-Enhanced Transformers for Multivariate Time Series Forecasting. (arXiv:2307.01616v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01616
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;SageFormer&#65292;&#19968;&#31181;&#38754;&#21521;&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#30340;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#36890;&#36807;&#22270;&#32467;&#26500;&#26377;&#25928;&#25429;&#25417;&#21644;&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#22312;&#34920;&#31034;&#19981;&#21516;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#21644;&#20943;&#23569;&#24207;&#21015;&#38388;&#20887;&#20313;&#20449;&#24687;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#21464;&#37327;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#22312;&#21508;&#20010;&#39046;&#22495;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#34429;&#28982;&#36817;&#26399;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#29305;&#21035;&#26159;Transformer&#65292;&#23637;&#31034;&#20102;&#24456;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#22312;&#35299;&#20915;&#36328;&#24207;&#21015;&#20381;&#36182;&#24615;&#30340;&#37325;&#35201;&#24615;&#38382;&#39064;&#19978;&#20173;&#23384;&#22312;&#24046;&#36317;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;SageFormer&#65292;&#19968;&#31181;&#31995;&#21015;&#24863;&#30693;&#22270;&#22686;&#24378;Transformer&#27169;&#22411;&#65292;&#26088;&#22312;&#20351;&#29992;&#22270;&#32467;&#26500;&#26377;&#25928;&#25429;&#25417;&#21644;&#24314;&#27169;&#24207;&#21015;&#20043;&#38388;&#30340;&#20381;&#36182;&#20851;&#31995;&#12290;SageFormer&#35299;&#20915;&#20102;&#20004;&#20010;&#20851;&#38190;&#25361;&#25112;&#65306;&#26377;&#25928;&#22320;&#34920;&#31034;&#19981;&#21516;&#24207;&#21015;&#20013;&#30340;&#26102;&#38388;&#27169;&#24335;&#20197;&#21450;&#20943;&#23569;&#24207;&#21015;&#20043;&#38388;&#30340;&#20887;&#20313;&#20449;&#24687;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25152;&#25552;&#35758;&#30340;&#31995;&#21015;&#24863;&#30693;&#26694;&#26550;&#21487;&#20197;&#26080;&#32541;&#38598;&#25104;&#21040;&#29616;&#26377;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#20013;&#65292;&#22686;&#24378;&#20102;&#27169;&#22411;&#23545;&#36328;&#24207;&#21015;&#20381;&#36182;&#24615;&#30340;&#24314;&#27169;&#33021;&#21147;&#12290;&#36890;&#36807;&#23545;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#25968;&#25454;&#38598;&#36827;&#34892;&#24191;&#27867;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;SageFormer&#30456;&#27604;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#23637;&#31034;&#20986;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multivariate time series forecasting plays a critical role in diverse domains. While recent advancements in deep learning methods, especially Transformers, have shown promise, there remains a gap in addressing the significance of inter-series dependencies. This paper introduces SageFormer, a Series-aware Graph-enhanced Transformer model designed to effectively capture and model dependencies between series using graph structures. SageFormer tackles two key challenges: effectively representing diverse temporal patterns across series and mitigating redundant information among series. Importantly, the proposed series-aware framework seamlessly integrates with existing Transformer-based models, augmenting their ability to model inter-series dependencies. Through extensive experiments on real-world and synthetic datasets, we showcase the superior performance of SageFormer compared to previous state-of-the-art approaches.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#24481;&#25216;&#26415;HAMP&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#36827;&#34892;&#19981;&#22826;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#36798;&#21040;&#24378;&#22823;&#30340;&#25104;&#21592;&#38544;&#31169;&#20445;&#25252;&#21644;&#39640;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;</title><link>http://arxiv.org/abs/2307.01610</link><description>&lt;p&gt;
&#36807;&#24230;&#33258;&#20449;&#26159;&#19968;&#20214;&#21361;&#38505;&#30340;&#20107;&#24773;&#65306;&#36890;&#36807;&#24378;&#21046;&#19981;&#22826;&#33258;&#20449;&#30340;&#39044;&#27979;&#26469;&#32531;&#35299;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;
&lt;/p&gt;
&lt;p&gt;
Overconfidence is a Dangerous Thing: Mitigating Membership Inference Attacks by Enforcing Less Confident Prediction. (arXiv:2307.01610v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#24481;&#25216;&#26415;HAMP&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#24378;&#21046;&#27169;&#22411;&#36827;&#34892;&#19981;&#22826;&#33258;&#20449;&#30340;&#39044;&#27979;&#65292;&#36798;&#21040;&#24378;&#22823;&#30340;&#25104;&#21592;&#38544;&#31169;&#20445;&#25252;&#21644;&#39640;&#20934;&#30830;&#24615;&#30340;&#30446;&#26631;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#27169;&#22411;&#23481;&#26131;&#21463;&#21040;&#25104;&#21592;&#25512;&#26029;&#25915;&#20987;&#65288;MIAs&#65289;&#30340;&#23041;&#32961;&#65292;&#36825;&#20123;&#25915;&#20987;&#30830;&#23450;&#32473;&#23450;&#30340;&#36755;&#20837;&#26159;&#21542;&#34987;&#29992;&#20110;&#35757;&#32451;&#30446;&#26631;&#27169;&#22411;&#12290;&#23613;&#31649;&#26377;&#24456;&#22810;&#21162;&#21147;&#26469;&#32531;&#35299;MIAs&#65292;&#20294;&#23427;&#20204;&#24448;&#24448;&#20250;&#21463;&#21040;&#26377;&#38480;&#30340;&#38544;&#31169;&#20445;&#25252;&#12289;&#22823;&#24133;&#38477;&#20302;&#20934;&#30830;&#24615;&#21644;/&#25110;&#38656;&#35201;&#38590;&#20197;&#33719;&#24471;&#30340;&#39069;&#22806;&#25968;&#25454;&#30340;&#22256;&#25200;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#24481;&#25216;&#26415;HAMP&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#39069;&#22806;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#25104;&#21592;&#38544;&#31169;&#21644;&#39640;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#32531;&#35299;&#19981;&#21516;&#24418;&#24335;&#30340;MIAs&#65292;&#25105;&#20204;&#35266;&#23519;&#21040;&#23427;&#20204;&#21487;&#20197;&#32479;&#19968;&#65292;&#22240;&#20026;&#23427;&#20204;&#37117;&#21033;&#29992;&#20102;ML&#27169;&#22411;&#22312;&#36890;&#36807;&#19981;&#21516;&#30340;&#20195;&#29702;&#39044;&#27979;&#35757;&#32451;&#26679;&#26412;&#26102;&#30340;&#36807;&#24230;&#33258;&#20449;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#36890;&#36807;&#27169;&#22411;&#24378;&#21046;&#36827;&#34892;&#19981;&#22826;&#33258;&#20449;&#39044;&#27979;&#30340;&#26041;&#27861;&#65292;&#20174;&#32780;&#36843;&#20351;&#27169;&#22411;&#22312;&#35757;&#32451;&#26679;&#26412;&#21644;&#27979;&#35797;&#26679;&#26412;&#19978;&#34920;&#29616;&#31867;&#20284;&#12290;HAMP&#21253;&#25324;&#19968;&#20010;&#26032;&#39062;&#30340;&#35757;&#32451;&#26694;&#26550;&#65292;&#20351;&#29992;&#39640;&#29109;&#36719;&#26631;&#31614;&#21644;&#22522;&#20110;&#29109;&#30340;&#27491;&#21017;&#21270;&#22120;&#26469;&#32422;&#26463;&#27169;&#22411;&#30340;&#39044;&#27979;&#65292;&#21516;&#26102;&#23454;&#29616;&#39640;&#20934;&#30830;&#24615;&#21644;&#25104;&#21592;&#38544;&#31169;&#20445;&#25252;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models are vulnerable to membership inference attacks (MIAs), which determine whether a given input is used for training the target model. While there have been many efforts to mitigate MIAs, they often suffer from limited privacy protection, large accuracy drop, and/or requiring additional data that may be difficult to acquire. This work proposes a defense technique, HAMP that can achieve both strong membership privacy and high accuracy, without requiring extra data. To mitigate MIAs in different forms, we observe that they can be unified as they all exploit the ML model's overconfidence in predicting training samples through different proxies. This motivates our design to enforce less confident prediction by the model, hence forcing the model to behave similarly on the training and testing samples. HAMP consists of a novel training framework with high-entropy soft labels and an entropy-based regularizer to constrain the model's prediction while still achieving h
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Seq2Peak&#26694;&#26550;&#65292;&#38024;&#23545;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;&#39640;&#24230;&#38750;&#24179;&#31283;&#24615;&#21644;&#24615;&#33021;&#35780;&#20272;&#38382;&#39064;&#65292;&#25104;&#21151;&#32553;&#23567;&#20102;&#22312;&#24120;&#35268;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2307.01597</link><description>&lt;p&gt;
&#22312;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#20013;&#32553;&#23567;&#24615;&#33021;&#24046;&#36317;: Seq2Peak&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Bridge the Performance Gap in Peak-hour Series Forecasting: The Seq2Peak Framework. (arXiv:2307.01597v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01597
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Seq2Peak&#26694;&#26550;&#65292;&#38024;&#23545;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#20219;&#21153;&#65292;&#35813;&#26694;&#26550;&#36890;&#36807;&#35299;&#20915;&#39640;&#24230;&#38750;&#24179;&#31283;&#24615;&#21644;&#24615;&#33021;&#35780;&#20272;&#38382;&#39064;&#65292;&#25104;&#21151;&#32553;&#23567;&#20102;&#22312;&#24120;&#35268;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#39044;&#27979;&#65288;PHSF&#65289;&#26159;&#21508;&#20010;&#39046;&#22495;&#20013;&#19968;&#20010;&#37325;&#35201;&#20294;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#30340;&#20219;&#21153;&#12290;&#34429;&#28982;&#26368;&#20808;&#36827;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#24120;&#35268;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65288;TSF&#65289;&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20294;&#22312;PHSF&#20013;&#21364;&#38590;&#20197;&#36798;&#21040;&#21487;&#27604;&#36739;&#30340;&#32467;&#26524;&#12290;&#36825;&#21487;&#33021;&#24402;&#22240;&#20110;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#20013;&#39640;&#24230;&#38750;&#24179;&#31283;&#24615;&#30340;&#25361;&#25112;&#65292;&#20351;&#24471;&#30452;&#25509;&#39044;&#27979;&#27604;&#26631;&#20934;&#30340;TSF&#26356;&#21152;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#25163;&#21160;&#20174;&#24120;&#35268;&#39044;&#27979;&#32467;&#26524;&#20013;&#25552;&#21462;&#26368;&#22823;&#20540;&#20250;&#23548;&#33268;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#27169;&#22411;&#20250;&#26368;&#23567;&#21270;&#24179;&#22343;&#24046;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;Seq2Peak&#65292;&#19968;&#20010;&#19987;&#20026;PHSF&#20219;&#21153;&#32780;&#35774;&#35745;&#30340;&#26032;&#39062;&#26694;&#26550;&#65292;&#20197;&#24357;&#21512;&#22312;TSF&#27169;&#22411;&#20013;&#35266;&#23519;&#21040;&#30340;&#24615;&#33021;&#24046;&#36317;&#12290;Seq2Peak&#20855;&#26377;&#20004;&#20010;&#20851;&#38190;&#32452;&#20214;&#65306;CyclicNorm&#27969;&#31243;&#26469;&#20943;&#36731;&#38750;&#24179;&#31283;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#33258;&#30001;&#23792;&#20540;&#23567;&#26102;&#35299;&#30721;&#22120;&#65292;&#37319;&#29992;&#28151;&#21512;&#25439;&#22833;&#20989;&#25968;&#26469;&#21033;&#29992;&#21407;&#22987;&#24207;&#21015;&#21644;&#39640;&#23792;&#23567;&#26102;&#24207;&#21015;&#20316;&#20026;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Peak-Hour Series Forecasting (PHSF) is a crucial yet underexplored task in various domains. While state-of-the-art deep learning models excel in regular Time Series Forecasting (TSF), they struggle to achieve comparable results in PHSF. This can be attributed to the challenges posed by the high degree of non-stationarity in peak-hour series, which makes direct forecasting more difficult than standard TSF. Additionally, manually extracting the maximum value from regular forecasting results leads to suboptimal performance due to models minimizing the mean deficit. To address these issues, this paper presents Seq2Peak, a novel framework designed specifically for PHSF tasks, bridging the performance gap observed in TSF models. Seq2Peak offers two key components: the CyclicNorm pipeline to mitigate the non-stationarity issue, and a simple yet effective trainable-parameter-free peak-hour decoder with a hybrid loss function that utilizes both the original series and peak-hour series as superv
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#25552;&#31034;&#22686;&#24378;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#21435;&#20559;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#25512;&#36827;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#12290;</title><link>http://arxiv.org/abs/2307.01595</link><description>&lt;p&gt;
&#25552;&#31034;&#35843;&#25972;&#36827;&#19968;&#27493;&#25512;&#36827;&#65292;&#23545;&#27604;&#23398;&#20064;&#25289;&#36817;&#36317;&#31163;&#65306;&#19968;&#31181;&#20004;&#38454;&#27573;&#26041;&#27861;&#26469;&#20943;&#36731;&#31038;&#20250;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases. (arXiv:2307.01595v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01595
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#36830;&#32493;&#25552;&#31034;&#22686;&#24378;&#30340;&#23545;&#27604;&#23398;&#20064;&#30340;&#20004;&#38454;&#27573;&#21435;&#20559;&#27169;&#22411;&#65292;&#20197;&#20943;&#36731;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#36890;&#36807;&#25552;&#31034;&#35843;&#25972;&#25512;&#36827;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLMs&#65289;&#30340;&#34920;&#31034;&#33021;&#21147;&#30340;&#25552;&#39640;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#25285;&#24515;&#23427;&#20204;&#20250;&#32487;&#25215;&#26410;&#32463;&#22788;&#29702;&#30340;&#35821;&#26009;&#24211;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#21435;&#20559;&#25216;&#26415;&#20351;&#29992;&#23545;&#27604;&#25968;&#25454;&#22686;&#24378;&#65288;CDA&#65289;&#26469;&#24179;&#34913;&#35757;&#32451;&#35821;&#26009;&#24211;&#12290;&#28982;&#32780;&#65292;CDA&#30053;&#24494;&#20462;&#25913;&#20102;&#21407;&#22987;&#35821;&#26009;&#24211;&#65292;&#38480;&#21046;&#20102;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#22312;&#19968;&#20010;&#29421;&#31364;&#33539;&#22260;&#20869;&#12290;&#32467;&#26524;&#65292;&#21435;&#20559;&#27169;&#22411;&#23481;&#26131;&#36866;&#24212;&#23545;&#27604;&#20107;&#23454;&#23545;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#36825;&#24433;&#21709;&#20102;&#23427;&#22312;&#26377;&#38480;&#30340;&#25991;&#26412;&#36164;&#28304;&#19979;&#30340;&#21435;&#20559;&#24615;&#33021;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21463;&#23545;&#25239;&#35757;&#32451;&#21551;&#21457;&#30340;&#20004;&#38454;&#27573;&#21435;&#20559;&#27169;&#22411;&#65292;&#20351;&#29992;&#36830;&#32493;&#25552;&#31034;&#22686;&#24378;&#30340;&#23545;&#27604;&#23398;&#20064;&#65288;&#31216;&#20026;CCPA&#65289;&#26469;&#20943;&#36731;PLMs&#32534;&#30721;&#20013;&#30340;&#31038;&#20250;&#20559;&#35265;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36830;&#32493;&#25552;&#31034;&#35843;&#25972;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#65292;&#21487;&#20197;&#36827;&#19968;&#27493;&#25512;&#36827;&#19981;&#21516;&#20154;&#21475;&#32676;&#20307;&#20043;&#38388;&#30340;&#34920;&#31034;&#36317;&#31163;&#12290;&#22312;&#31532;&#20108;&#38454;&#27573;&#65292;
&lt;/p&gt;
&lt;p&gt;
As the representation capability of Pre-trained Language Models (PLMs) improve, there is growing concern that they will inherit social biases from unprocessed corpora. Most previous debiasing techniques used Counterfactual Data Augmentation (CDA) to balance the training corpus. However, CDA slightly modifies the original corpus, limiting the representation distance between different demographic groups to a narrow range. As a result, the debiasing model easily fits the differences between counterfactual pairs, which affects its debiasing performance with limited text resources. In this paper, we propose an adversarial training-inspired two-stage debiasing model using Contrastive learning with Continuous Prompt Augmentation (named CCPA) to mitigate social biases in PLMs' encoding. In the first stage, we propose a data augmentation method based on continuous prompt tuning to push farther the representation distance between sample pairs along different demographic groups. In the second sta
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#20803;&#32032;&#32452;&#21512;&#36873;&#25321;&#26694;&#26550;CECS&#65292;&#29992;&#20110;&#35299;&#20915;&#26174;&#31034;&#24191;&#21578;&#20013;&#22810;&#20803;&#32032;&#21019;&#24847;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#36328;&#20803;&#32032;&#20132;&#20114;&#30340;&#26041;&#24335;&#36827;&#34892;&#32534;&#30721;&#65292;&#23558;&#21019;&#24847;&#32452;&#21512;&#38382;&#39064;&#36716;&#21270;&#20026;&#22810;&#20010;&#21019;&#24847;&#20803;&#32032;&#32423;&#32852;&#36873;&#25321;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01593</link><description>&lt;p&gt;
&#26174;&#31034;&#24191;&#21578;&#20013;&#22810;&#20803;&#32032;&#21019;&#24847;&#30340;&#36328;&#20803;&#32032;&#32452;&#21512;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Cross-Element Combinatorial Selection for Multi-Element Creative in Display Advertising. (arXiv:2307.01593v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01593
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36328;&#20803;&#32032;&#32452;&#21512;&#36873;&#25321;&#26694;&#26550;CECS&#65292;&#29992;&#20110;&#35299;&#20915;&#26174;&#31034;&#24191;&#21578;&#20013;&#22810;&#20803;&#32032;&#21019;&#24847;&#36873;&#25321;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#37319;&#29992;&#36328;&#20803;&#32032;&#20132;&#20114;&#30340;&#26041;&#24335;&#36827;&#34892;&#32534;&#30721;&#65292;&#23558;&#21019;&#24847;&#32452;&#21512;&#38382;&#39064;&#36716;&#21270;&#20026;&#22810;&#20010;&#21019;&#24847;&#20803;&#32032;&#32423;&#32852;&#36873;&#25321;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24191;&#21578;&#21019;&#24847;&#30340;&#26377;&#25928;&#24615;&#24456;&#22823;&#31243;&#24230;&#19978;&#21463;&#20854;&#35270;&#35273;&#22806;&#35266;&#30340;&#24433;&#21709;&#12290;&#24191;&#21578;&#24179;&#21488;&#21487;&#20197;&#36890;&#36807;&#32452;&#21512;&#24191;&#21578;&#21019;&#24847;&#20013;&#30340;&#19981;&#21516;&#20803;&#32032;&#26469;&#29983;&#25104;&#20855;&#26377;&#19981;&#21516;&#22806;&#35266;&#30340;&#24191;&#21578;&#21019;&#24847;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#24191;&#21578;&#21019;&#24847;&#20803;&#32032;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#20174;&#26080;&#25968;&#21487;&#33021;&#24615;&#20013;&#36873;&#25321;&#21512;&#36866;&#30340;&#32452;&#21512;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#34892;&#19994;&#30340;&#20027;&#27969;&#26041;&#27861;&#26159;&#29420;&#31435;&#36873;&#25321;&#21508;&#20010;&#21019;&#24847;&#20803;&#32032;&#65292;&#36825;&#32463;&#24120;&#24573;&#35270;&#20102;&#24314;&#27169;&#36807;&#31243;&#20013;&#21019;&#24847;&#20803;&#32032;&#20043;&#38388;&#30456;&#20114;&#20316;&#29992;&#30340;&#37325;&#35201;&#24615;&#12290;&#20316;&#20026;&#22238;&#24212;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22810;&#20010;&#21019;&#24847;&#20803;&#32032;&#30340;&#36328;&#20803;&#32032;&#32452;&#21512;&#36873;&#25321;&#26694;&#26550;&#65292;&#31216;&#20026;CECS&#12290;&#22312;&#32534;&#30721;&#22120;&#36807;&#31243;&#20013;&#65292;&#37319;&#29992;&#20102;&#36328;&#20803;&#32032;&#20132;&#20114;&#65292;&#26681;&#25454;&#24403;&#21069;&#20505;&#36873;&#21019;&#24847;&#21160;&#24577;&#35843;&#25972;&#21333;&#20010;&#21019;&#24847;&#20803;&#32032;&#30340;&#34920;&#36798;&#12290;&#22312;&#35299;&#30721;&#22120;&#36807;&#31243;&#20013;&#65292;&#23558;&#21019;&#24847;&#32452;&#21512;&#38382;&#39064;&#36716;&#21270;&#20026;&#22810;&#20010;&#21019;&#24847;&#20803;&#32032;&#32423;&#32852;&#36873;&#25321;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The effectiveness of ad creatives is greatly influenced by their visual appearance. Advertising platforms can generate ad creatives with different appearances by combining creative elements provided by advertisers. However, with the increasing number of ad creative elements, it becomes challenging to select a suitable combination from the countless possibilities. The industry's mainstream approach is to select individual creative elements independently, which often overlooks the importance of interaction between creative elements during the modeling process. In response, this paper proposes a Cross-Element Combinatorial Selection framework for multiple creative elements, termed CECS. In the encoder process, a cross-element interaction is adopted to dynamically adjust the expression of a single creative element based on the current candidate creatives. In the decoder process, the creative combination problem is transformed into a cascade selection problem of multiple creative elements. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#31616;&#21333;&#30340;&#20154;&#26426;&#20132;&#20114;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#26234;&#33021;&#27880;&#37322;&#65288;IA&#65289;&#31574;&#30053;&#21644;&#24320;&#28304;&#30340;IAdet&#24037;&#20855;&#12290;&#23545;&#20110;PASCAL VOC&#25968;&#25454;&#38598;&#65292;IAdet&#24037;&#20855;&#33021;&#22815;&#20943;&#23569;&#25968;&#25454;&#24211;&#26631;&#27880;&#26102;&#38388;&#65292;&#24182;&#25552;&#20379;&#20813;&#36153;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#65292;&#20026;&#24378;&#22823;&#30340;&#20154;&#26426;&#20132;&#20114;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#21487;&#33021;&#12290;</title><link>http://arxiv.org/abs/2307.01582</link><description>&lt;p&gt;
IAdet&#65306;&#26368;&#31616;&#21333;&#30340;&#20154;&#26426;&#20132;&#20114;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
IAdet: Simplest human-in-the-loop object detection. (arXiv:2307.01582v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01582
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26368;&#31616;&#21333;&#30340;&#20154;&#26426;&#20132;&#20114;&#30446;&#26631;&#26816;&#27979;&#26041;&#27861;&#65292;&#20854;&#20013;&#30340;&#20851;&#38190;&#21019;&#26032;&#26159;&#26234;&#33021;&#27880;&#37322;&#65288;IA&#65289;&#31574;&#30053;&#21644;&#24320;&#28304;&#30340;IAdet&#24037;&#20855;&#12290;&#23545;&#20110;PASCAL VOC&#25968;&#25454;&#38598;&#65292;IAdet&#24037;&#20855;&#33021;&#22815;&#20943;&#23569;&#25968;&#25454;&#24211;&#26631;&#27880;&#26102;&#38388;&#65292;&#24182;&#25552;&#20379;&#20813;&#36153;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#65292;&#20026;&#24378;&#22823;&#30340;&#20154;&#26426;&#20132;&#20114;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#25552;&#20379;&#20102;&#25913;&#36827;&#30340;&#21487;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26631;&#27880;&#25968;&#25454;&#30340;&#21516;&#26102;&#35757;&#32451;&#27169;&#22411;&#30340;&#31574;&#30053;&#65292;&#31216;&#20026;&#26234;&#33021;&#27880;&#37322;&#65288;IA&#65289;&#12290;IA&#21253;&#25324;&#19977;&#20010;&#27169;&#22359;&#65306;&#65288;1&#65289;&#36741;&#21161;&#25968;&#25454;&#26631;&#27880;&#65292;&#65288;2&#65289;&#32972;&#26223;&#27169;&#22411;&#35757;&#32451;&#21644;&#65288;3&#65289;&#20027;&#21160;&#36873;&#25321;&#19979;&#19968;&#20010;&#25968;&#25454;&#28857;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#19979;&#65292;&#25105;&#20204;&#24320;&#28304;&#20102;&#29305;&#23450;&#20110;&#21333;&#31867;&#30446;&#26631;&#26816;&#27979;&#30340;IAdet&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#33258;&#21160;&#35780;&#20272;&#36825;&#31181;&#20154;&#26426;&#20132;&#20114;&#31995;&#32479;&#30340;&#26041;&#27861;&#12290;&#23545;&#20110;PASCAL VOC&#25968;&#25454;&#38598;&#65292;IAdet&#24037;&#20855;&#22312;&#20943;&#23569;&#25968;&#25454;&#24211;&#26631;&#27880;&#26102;&#38388;&#30340;&#21516;&#26102;&#25552;&#20379;&#20102;&#19968;&#20010;&#35757;&#32451;&#22909;&#30340;&#27169;&#22411;&#12290;&#36825;&#20123;&#32467;&#26524;&#26159;&#22522;&#20110;&#19968;&#20010;&#25925;&#24847;&#35774;&#35745;&#38750;&#24120;&#31616;&#21333;&#30340;IAdet&#31995;&#32479;&#12290;&#22240;&#27492;&#65292;IAdet&#26131;&#20110;&#25913;&#36827;&#65292;&#20026;&#24378;&#22823;&#30340;&#20154;&#26426;&#20132;&#20114;&#30446;&#26631;&#26816;&#27979;&#31995;&#32479;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work proposes a strategy for training models while annotating data named Intelligent Annotation (IA). IA involves three modules: (1) assisted data annotation, (2) background model training, and (3) active selection of the next datapoints. Under this framework, we open-source the IAdet tool, which is specific for single-class object detection. Additionally, we devise a method for automatically evaluating such a human-in-the-loop system. For the PASCAL VOC dataset, the IAdet tool reduces the database annotation time by $25\%$ while providing a trained model for free. These results are obtained for a deliberately very simple IAdet design. As a consequence, IAdet is susceptible to multiple easy improvements, paving the way for powerful human-in-the-loop object detection systems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#26426;&#21327;&#20316;&#27880;&#37322;&#20013;&#30340;&#20108;&#20803;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20174;&#26368;&#20248;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#21040;&#23454;&#38469;&#39640;&#25928;&#26041;&#27861;&#30340;&#19968;&#31995;&#21015;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#22312;&#32473;&#23450;&#39044;&#27979;&#22120;&#30340;&#24773;&#20917;&#19979;&#29992;&#26368;&#23569;&#30340;&#26159;/&#21542;&#38382;&#39064;&#26469;&#23436;&#20840;&#27880;&#37322;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#32534;&#30721;&#29702;&#35770;&#21644;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35745;&#31639;&#21487;&#34892;&#19988;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;</title><link>http://arxiv.org/abs/2307.01578</link><description>&lt;p&gt;
&#20154;&#26426;&#21327;&#20316;&#27880;&#37322;&#30340;&#26368;&#20248;&#39640;&#25928;&#20108;&#20803;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Optimal and Efficient Binary Questioning for Human-in-the-Loop Annotation. (arXiv:2307.01578v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01578
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20154;&#26426;&#21327;&#20316;&#27880;&#37322;&#20013;&#30340;&#20108;&#20803;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20174;&#26368;&#20248;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#21040;&#23454;&#38469;&#39640;&#25928;&#26041;&#27861;&#30340;&#19968;&#31995;&#21015;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#23558;&#38382;&#39064;&#26500;&#24314;&#20026;&#22312;&#32473;&#23450;&#39044;&#27979;&#22120;&#30340;&#24773;&#20917;&#19979;&#29992;&#26368;&#23569;&#30340;&#26159;/&#21542;&#38382;&#39064;&#26469;&#23436;&#20840;&#27880;&#37322;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#32534;&#30721;&#29702;&#35770;&#21644;&#21551;&#21457;&#24335;&#31639;&#27861;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#35745;&#31639;&#21487;&#34892;&#19988;&#39640;&#25928;&#30340;&#26367;&#20195;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25968;&#25454;&#27880;&#37322;&#23545;&#35299;&#37322;&#33021;&#21147;&#12289;&#20154;&#24037;&#26234;&#33021;&#35299;&#20915;&#26041;&#26696;&#30340;&#30740;&#31350;&#21644;&#24320;&#21457;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#22823;&#22810;&#25968;&#30740;&#31350;&#22914;&#20027;&#21160;&#23398;&#20064;&#25110;&#23569;&#26679;&#26412;&#23398;&#20064;&#37117;&#38598;&#20013;&#22312;&#26679;&#26412;&#25928;&#29575;&#38382;&#39064;&#19978;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#21478;&#19968;&#20010;&#34987;&#24573;&#35270;&#30340;&#38382;&#39064;&#65292;&#21363;&#22312;&#32473;&#23450;&#39044;&#27979;&#22120;&#30340;&#24773;&#20917;&#19979;&#22914;&#20309;&#33719;&#24471;&#24102;&#27880;&#37322;&#30340;&#25968;&#25454;&#12290;&#38024;&#23545;&#31616;&#21333;&#30340;&#20108;&#20803;&#20998;&#31867;&#35774;&#32622;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20174;&#26368;&#20248;&#36890;&#29992;&#35299;&#20915;&#26041;&#26696;&#21040;&#23454;&#38469;&#39640;&#25928;&#26041;&#27861;&#30340;&#19968;&#31995;&#21015;&#35299;&#20915;&#26041;&#26696;&#12290;&#38382;&#39064;&#34987;&#26500;&#24314;&#20026;&#32473;&#23450;&#39044;&#27979;&#22120;&#24773;&#20917;&#19979;&#25317;&#26377;&#26368;&#23569;&#30340;&#26159;/&#21542;&#38382;&#39064;&#26469;&#23436;&#20840;&#27880;&#37322;&#19968;&#20010;&#20108;&#20803;&#20998;&#31867;&#25968;&#25454;&#38598;&#12290;&#23545;&#20110;&#19968;&#33324;&#20108;&#20803;&#38382;&#39064;&#65292;&#35299;&#20915;&#26041;&#26696;&#21487;&#20197;&#22312;&#32534;&#30721;&#29702;&#35770;&#20013;&#25214;&#21040;&#65292;&#20854;&#20013;&#26368;&#20248;&#30340;&#25552;&#38382;&#31574;&#30053;&#30001;&#21487;&#33021;&#30340;&#26631;&#31614;&#32534;&#30721;&#30340;&#38669;&#22827;&#26364;&#32534;&#30721;&#32473;&#20986;&#12290;&#28982;&#32780;&#65292;&#21363;&#20351;&#23545;&#20110;&#36739;&#23567;&#30340;&#25968;&#25454;&#38598;&#22823;&#23567;&#65292;&#36825;&#31181;&#26041;&#27861;&#30340;&#35745;&#31639;&#22797;&#26434;&#24230;&#20063;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22810;&#31181;&#21551;&#21457;&#24335;&#21644;&#21069;&#30651;&#24615;&#26368;&#23567;&#21270;&#30340;&#26367;&#20195;&#23454;&#38469;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
Even though data annotation is extremely important for interpretability, research and development of artificial intelligence solutions, most research efforts such as active learning or few-shot learning focus on the sample efficiency problem. This paper studies the neglected complementary problem of getting annotated data given a predictor. For the simple binary classification setting, we present the spectrum ranging from optimal general solutions to practical efficient methods. The problem is framed as the full annotation of a binary classification dataset with the minimal number of yes/no questions when a predictor is available. For the case of general binary questions the solution is found in coding theory, where the optimal questioning strategy is given by the Huffman encoding of the possible labelings. However, this approach is computationally intractable even for small dataset sizes. We propose an alternative practical solution based on several heuristics and lookahead minimizati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#21518;&#32487;&#32593;&#32476;&#21644;&#35789;&#23884;&#20837;&#21521;&#37327;&#26500;&#24314;&#20102;&#19968;&#20010;&#27010;&#24565;&#35748;&#30693;&#22320;&#22270;&#65292;&#35813;&#22320;&#22270;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#23610;&#24230;&#23398;&#20064;&#24182;&#23558;&#26032;&#20449;&#24687;&#19982;&#30456;&#20851;&#30340;&#26082;&#26377;&#34920;&#31034;&#30456;&#36830;&#25509;&#12290;</title><link>http://arxiv.org/abs/2307.01577</link><description>&lt;p&gt;
&#20351;&#29992;&#31070;&#32463;&#21518;&#32487;&#32593;&#32476;&#21644;&#35789;&#23884;&#20837;&#24418;&#25104;&#27010;&#24565;&#35748;&#30693;&#22320;&#22270;
&lt;/p&gt;
&lt;p&gt;
Conceptual Cognitive Maps Formation with Neural Successor Networks and Word Embeddings. (arXiv:2307.01577v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01577
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#20351;&#29992;&#31070;&#32463;&#21518;&#32487;&#32593;&#32476;&#21644;&#35789;&#23884;&#20837;&#21521;&#37327;&#26500;&#24314;&#20102;&#19968;&#20010;&#27010;&#24565;&#35748;&#30693;&#22320;&#22270;&#65292;&#35813;&#22320;&#22270;&#33021;&#22815;&#26681;&#25454;&#19981;&#21516;&#30340;&#23610;&#24230;&#23398;&#20064;&#24182;&#23558;&#26032;&#20449;&#24687;&#19982;&#30456;&#20851;&#30340;&#26082;&#26377;&#34920;&#31034;&#30456;&#36830;&#25509;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#33041;&#20855;&#26377;&#20174;&#29615;&#22659;&#20013;&#25509;&#25910;&#20449;&#24687;&#24182;&#36827;&#34892;&#19978;&#19979;&#25991;&#22788;&#29702;&#30340;&#38750;&#20961;&#33021;&#21147;&#12290;&#20391;&#38548;&#26680;-&#28023;&#39532;&#22312;&#36825;&#19968;&#21151;&#33021;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#23427;&#28145;&#24230;&#21442;&#19982;&#35760;&#24518;&#22788;&#29702;&#21644;&#20351;&#29992;&#22320;&#28857;&#21644;&#32593;&#26684;&#32454;&#32990;&#26500;&#24314;&#35748;&#30693;&#22320;&#22270;&#12290;&#29702;&#35299;&#21644;&#21033;&#29992;&#36825;&#31181;&#33021;&#21147;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#12290;&#22810;&#23610;&#24230;&#21518;&#32493;&#34920;&#31034;&#20316;&#20026;&#22320;&#28857;&#21644;&#32593;&#26684;&#32454;&#32990;&#21151;&#33021;&#30340;&#33391;&#22909;&#27169;&#22411;&#65292;&#24182;&#24050;&#26174;&#31034;&#20986;&#28508;&#21147;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#21033;&#29992;&#21518;&#32493;&#34920;&#31034;&#12289;&#31070;&#32463;&#32593;&#32476;&#20197;&#21450;&#35789;&#23884;&#20837;&#21521;&#37327;&#26469;&#26500;&#24314;&#19977;&#20010;&#29420;&#31435;&#27010;&#24565;&#30340;&#35748;&#30693;&#22320;&#22270;&#12290;&#35813;&#32593;&#32476;&#33021;&#22815;&#28789;&#27963;&#23398;&#20064;&#20004;&#31181;&#19981;&#21516;&#30340;&#23610;&#24230;&#22320;&#22270;&#65292;&#24182;&#23558;&#26032;&#20449;&#24687;&#25918;&#32622;&#22312;&#30456;&#20851;&#30340;&#26082;&#26377;&#34920;&#31034;&#38468;&#36817;&#12290;&#20449;&#24687;&#22312;&#35748;&#30693;&#22320;&#22270;&#19978;&#30340;&#20998;&#24067;&#26681;&#25454;&#23610;&#24230;&#30340;&#19981;&#21516;&#32780;&#21464;&#21270;&#65292;&#21487;&#20197;&#26159;&#39640;&#24230;&#38598;&#20013;&#30340;&#65292;&#23548;&#33268;...
&lt;/p&gt;
&lt;p&gt;
The human brain possesses the extraordinary capability to contextualize the information it receives from our environment. The entorhinal-hippocampal plays a critical role in this function, as it is deeply engaged in memory processing and constructing cognitive maps using place and grid cells. Comprehending and leveraging this ability could significantly augment the field of artificial intelligence. The multi-scale successor representation serves as a good model for the functionality of place and grid cells and has already shown promise in this role. Here, we introduce a model that employs successor representations and neural networks, along with word embedding vectors, to construct a cognitive map of three separate concepts. The network adeptly learns two different scaled maps and situates new information in proximity to related pre-existing representations. The dispersion of information across the cognitive map varies according to its scale - either being heavily concentrated, resulti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#20110;&#20837;&#20405;&#26816;&#27979;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#19982;&#29305;&#24449;&#25552;&#21462;&#20004;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#24615;&#33021;&#25351;&#26631;&#19979;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;&#30340;&#32508;&#36848;&#12290;</title><link>http://arxiv.org/abs/2307.01570</link><description>&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#20837;&#20405;&#26816;&#27979;&#65306;&#29305;&#24449;&#36873;&#25321;&#19982;&#29305;&#24449;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Machine Learning-Based Intrusion Detection: Feature Selection versus Feature Extraction. (arXiv:2307.01570v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01570
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#20110;&#20837;&#20405;&#26816;&#27979;&#20013;&#30340;&#29305;&#24449;&#36873;&#25321;&#19982;&#29305;&#24449;&#25552;&#21462;&#20004;&#31181;&#26041;&#27861;&#22312;&#19981;&#21516;&#24615;&#33021;&#25351;&#26631;&#19979;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#65292;&#24182;&#25552;&#20379;&#20102;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29289;&#32852;&#32593;&#20837;&#20405;&#26816;&#27979;&#30340;&#32508;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29289;&#32852;&#32593;&#22312;&#26234;&#24935;&#22478;&#24066;&#12289;&#26234;&#24935;&#20892;&#19994;&#12289;&#26234;&#24935;&#21307;&#30103;&#21644;&#26234;&#24935;&#21046;&#36896;&#31561;&#35768;&#22810;&#39046;&#22495;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;&#29289;&#32852;&#32593;&#35774;&#22791;&#38750;&#24120;&#23481;&#26131;&#21463;&#21040;&#32593;&#32476;&#25915;&#20987;&#65292;&#21487;&#33021;&#23548;&#33268;&#23433;&#20840;&#28431;&#27934;&#21644;&#25968;&#25454;&#27844;&#28431;&#12290;&#20026;&#20102;&#26377;&#25928;&#39044;&#38450;&#36825;&#20123;&#25915;&#20987;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#29289;&#32852;&#32593;&#32593;&#32476;&#20837;&#20405;&#26816;&#27979;&#26041;&#27861;&#65292;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#20381;&#36182;&#20110;&#29305;&#24449;&#25552;&#21462;&#25110;&#29305;&#24449;&#36873;&#25321;&#25216;&#26415;&#65292;&#20197;&#20943;&#23569;&#36755;&#20837;&#25968;&#25454;&#30340;&#32500;&#25968;&#65292;&#28982;&#21518;&#20877;&#23558;&#20854;&#36755;&#20837;&#21040;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20013;&#12290;&#36825;&#26088;&#22312;&#38477;&#20302;&#26816;&#27979;&#22797;&#26434;&#24230;&#65292;&#20197;&#36866;&#24212;&#23454;&#26102;&#25805;&#20316;&#65292;&#36825;&#22312;&#20219;&#20309;&#20837;&#20405;&#26816;&#27979;&#31995;&#32479;&#20013;&#37117;&#38750;&#24120;&#37325;&#35201;&#12290;&#26412;&#25991;&#22312;&#31934;&#30830;&#29575;&#12289;&#21484;&#22238;&#29575;&#12289;&#26816;&#27979;&#20934;&#30830;&#24230;&#20197;&#21450;&#36816;&#34892;&#26102;&#38388;&#22797;&#26434;&#24230;&#31561;&#24615;&#33021;&#25351;&#26631;&#26041;&#38754;&#23545;&#36825;&#20004;&#31181;&#29305;&#24449;&#38477;&#32500;&#26041;&#27861;&#36827;&#34892;&#20102;&#20840;&#38754;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
Internet of things (IoT) has been playing an important role in many sectors, such as smart cities, smart agriculture, smart healthcare, and smart manufacturing. However, IoT devices are highly vulnerable to cyber-attacks, which may result in security breaches and data leakages. To effectively prevent these attacks, a variety of machine learning-based network intrusion detection methods for IoT networks have been developed, which often rely on either feature extraction or feature selection techniques for reducing the dimension of input data before being fed into machine learning models. This aims to make the detection complexity low enough for real-time operations, which is particularly vital in any intrusion detection systems. This paper provides a comprehensive comparison between these two feature reduction methods of intrusion detection in terms of various performance metrics, namely, precision rate, recall rate, detection accuracy, as well as runtime complexity, in the presence of t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21464;&#37327;&#36873;&#25321;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20004;&#35270;&#22270;&#23398;&#20064;&#20219;&#21153;&#25110;&#21521;&#37327;&#20540;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#33021;&#22815;&#22788;&#29702;&#35268;&#27169;&#26497;&#22823;&#30340;&#36873;&#25321;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#25237;&#24433;&#31639;&#23376;&#20197;&#21450;&#26680;&#20989;&#25968;&#36827;&#34892;&#30456;&#20851;&#24615;&#34913;&#37327;&#21644;&#38750;&#32447;&#24615;&#30456;&#20851;&#27169;&#22411;&#30340;&#22788;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.01558</link><description>&lt;p&gt;
&#21487;&#25193;&#23637;&#30340;&#25237;&#24433;&#31639;&#23376;&#29992;&#20110;&#20004;&#35270;&#22270;&#23398;&#20064;&#20219;&#21153;&#30340;&#21464;&#37327;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scalable variable selection for two-view learning tasks with projection operators. (arXiv:2307.01558v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01558
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#25193;&#23637;&#30340;&#21464;&#37327;&#36873;&#25321;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20004;&#35270;&#22270;&#23398;&#20064;&#20219;&#21153;&#25110;&#21521;&#37327;&#20540;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#65292;&#33021;&#22815;&#22788;&#29702;&#35268;&#27169;&#26497;&#22823;&#30340;&#36873;&#25321;&#20219;&#21153;&#65292;&#24182;&#21033;&#29992;&#25237;&#24433;&#31639;&#23376;&#20197;&#21450;&#26680;&#20989;&#25968;&#36827;&#34892;&#30456;&#20851;&#24615;&#34913;&#37327;&#21644;&#38750;&#32447;&#24615;&#30456;&#20851;&#27169;&#22411;&#30340;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#20004;&#35270;&#22270;&#35774;&#32622;&#25110;&#21521;&#37327;&#20540;&#30417;&#30563;&#23398;&#20064;&#38382;&#39064;&#30340;&#26032;&#22411;&#21464;&#37327;&#36873;&#25321;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#33021;&#22815;&#22788;&#29702;&#35268;&#27169;&#26497;&#22823;&#30340;&#36873;&#25321;&#20219;&#21153;&#65292;&#26679;&#26412;&#25968;&#29978;&#33267;&#21487;&#20197;&#36798;&#21040;&#30334;&#19975;&#32423;&#12290;&#31616;&#35328;&#20043;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#36890;&#36807;&#36873;&#25321;&#19982;&#36755;&#20986;&#21464;&#37327;&#39640;&#24230;&#30456;&#20851;&#20294;&#19982;&#20808;&#21069;&#36873;&#25321;&#30340;&#21464;&#37327;&#26080;&#20851;&#30340;&#21464;&#37327;&#26469;&#36827;&#34892;&#21464;&#37327;&#36873;&#25321;&#12290;&#20026;&#20102;&#34913;&#37327;&#30456;&#20851;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#29992;&#20102;&#25237;&#24433;&#31639;&#23376;&#21450;&#20854;&#20195;&#25968;&#30340;&#27010;&#24565;&#12290;&#36890;&#36807;&#25237;&#24433;&#31639;&#23376;&#65292;&#36755;&#20837;&#21644;&#36755;&#20986;&#21464;&#37327;&#38598;&#20043;&#38388;&#30340;&#20851;&#31995;&#12289;&#30456;&#20851;&#24615;&#20063;&#21487;&#20197;&#36890;&#36807;&#26680;&#20989;&#25968;&#26469;&#34920;&#36798;&#65292;&#20174;&#32780;&#21487;&#20197;&#21033;&#29992;&#38750;&#32447;&#24615;&#30456;&#20851;&#27169;&#22411;&#12290;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#21487;&#25193;&#23637;&#24615;&#20197;&#21450;&#25152;&#36873;&#25321;&#29305;&#24449;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we propose a novel variable selection method for two-view settings, or for vector-valued supervised learning problems. Our framework is able to handle extremely large scale selection tasks, where number of data samples could be even millions. In a nutshell, our method performs variable selection by iteratively selecting variables that are highly correlated with the output variables, but which are not correlated with the previously chosen variables. To measure the correlation, our method uses the concept of projection operators and their algebra. With the projection operators the relationship, correlation, between sets of input and output variables can also be expressed by kernel functions, thus nonlinear correlation models can be exploited as well. We experimentally validate our approach, showing on both synthetic and real data its scalability and the relevance of the selected features. Keywords: Supervised variable selection, vector-valued learning, projection-valued mea
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#30340;RoadTopoFormer&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#36710;&#36947;&#20013;&#24515;&#32447;&#21644;&#20132;&#36890;&#20803;&#32032;&#65292;&#24182;&#25512;&#29702;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2307.01557</link><description>&lt;p&gt;
&#20998;&#31163;&#30340;RoadTopoFormer
&lt;/p&gt;
&lt;p&gt;
Separated RoadTopoFormer. (arXiv:2307.01557v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01557
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#31163;&#30340;RoadTopoFormer&#26694;&#26550;&#65292;&#29992;&#20110;&#26816;&#27979;&#36710;&#36947;&#20013;&#24515;&#32447;&#21644;&#20132;&#36890;&#20803;&#32032;&#65292;&#24182;&#25512;&#29702;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#39550;&#39542;&#22330;&#26223;&#23545;&#23454;&#29616;&#33258;&#21160;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#22914;&#22320;&#22270;&#23398;&#20064;&#21644;BEV&#36710;&#36947;&#26816;&#27979;&#24573;&#35270;&#20102;&#36710;&#36947;&#23454;&#20363;&#20043;&#38388;&#30340;&#36830;&#25509;&#20851;&#31995;&#65292;&#20132;&#36890;&#20803;&#32032;&#26816;&#27979;&#20219;&#21153;&#36890;&#24120;&#24573;&#35270;&#19982;&#36710;&#36947;&#32447;&#30340;&#20851;&#31995;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;4&#20010;&#23376;&#20219;&#21153;&#65292;&#21363;&#20132;&#36890;&#20803;&#32032;&#26816;&#27979;&#12289;&#36710;&#36947;&#20013;&#24515;&#32447;&#26816;&#27979;&#12289;&#25512;&#29702;&#36710;&#36947;&#20043;&#38388;&#30340;&#36830;&#25509;&#20851;&#31995;&#21644;&#25512;&#29702;&#36710;&#36947;&#19982;&#20132;&#36890;&#20803;&#32032;&#20043;&#38388;&#30340;&#20851;&#32852;&#20851;&#31995;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#31163;&#30340;RoadTopoFormer&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#36825;&#26159;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;&#65292;&#21487;&#20197;&#26816;&#27979;&#21040;&#36710;&#36947;&#20013;&#24515;&#32447;&#21644;&#20132;&#36890;&#20803;&#32032;&#65292;&#24182;&#25512;&#29702;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#20998;&#21035;&#20248;&#21270;&#27599;&#20010;&#27169;&#22359;&#65292;&#20197;&#38450;&#27490;&#23427;&#20204;&#20043;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#65292;&#24182;&#36890;&#36807;&#23569;&#37327;&#30340;&#24494;&#35843;&#23558;&#23427;&#20204;&#32858;&#21512;&#22312;&#19968;&#36215;&#12290;&#23545;&#20110;&#20004;&#20010;&#26816;&#27979;&#22836;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#31867;&#20284;DETR&#30340;&#26550;&#26500;&#26469;&#26816;&#27979;&#29289;&#20307;&#65292;&#23545;&#20110;&#20851;&#31995;&#22836;&#65292;&#25105;&#20204;&#23558;&#20004;&#20010;&#23454;&#20363;&#29305;&#24449;&#36830;&#25509;&#22312;&#19968;&#36215;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding driving scenarios is crucial to realizing autonomous driving. Previous works such as map learning and BEV lane detection neglect the connection relationship between lane instances, and traffic elements detection tasks usually neglect the relationship with lane lines. To address these issues, the task is presented which includes 4 sub-tasks, the detection of traffic elements, the detection of lane centerlines, reasoning connection relationships among lanes, and reasoning assignment relationships between lanes and traffic elements. We present Separated RoadTopoFormer to tackle the issues, which is an end-to-end framework that detects lane centerline and traffic elements with reasoning relationships among them. We optimize each module separately to prevent interaction with each other and aggregate them together with few finetunes. For two detection heads, we adopted a DETR-like architecture to detect objects, and for the relationship head, we concat two instance features fro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#22238;&#39038;&#20102;&#23545;&#35805;&#20195;&#29702;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#30340;&#30693;&#35782;&#22270;&#35889;&#21040;&#25991;&#26412;&#29983;&#25104;&#26550;&#26500;&#65292;&#24182;&#35752;&#35770;&#20102;&#27599;&#31181;&#26550;&#26500;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#26681;&#25454;&#20855;&#20307;&#20219;&#21153;&#30340;&#35201;&#27714;&#36873;&#25321;&#21512;&#36866;&#30340;&#26550;&#26500;&#65292;&#24182;&#24378;&#35843;&#20102;&#32771;&#34385;&#25191;&#34892;&#26102;&#38388;&#21644;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#35805;&#20195;&#29702;&#32972;&#26223;&#19979;&#12290;</title><link>http://arxiv.org/abs/2307.01548</link><description>&lt;p&gt;
&#23545;&#35805;&#20195;&#29702;&#20013;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#30340;&#30693;&#35782;&#22270;&#35889;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph for NLG in the context of conversational agents. (arXiv:2307.01548v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#22238;&#39038;&#20102;&#23545;&#35805;&#20195;&#29702;&#20013;&#20351;&#29992;&#30340;&#19981;&#21516;&#30340;&#30693;&#35782;&#22270;&#35889;&#21040;&#25991;&#26412;&#29983;&#25104;&#26550;&#26500;&#65292;&#24182;&#35752;&#35770;&#20102;&#27599;&#31181;&#26550;&#26500;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;&#26681;&#25454;&#20855;&#20307;&#20219;&#21153;&#30340;&#35201;&#27714;&#36873;&#25321;&#21512;&#36866;&#30340;&#26550;&#26500;&#65292;&#24182;&#24378;&#35843;&#20102;&#32771;&#34385;&#25191;&#34892;&#26102;&#38388;&#21644;&#27169;&#22411;&#26377;&#25928;&#24615;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#35805;&#20195;&#29702;&#32972;&#26223;&#19979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#30340;&#20351;&#29992;&#25552;&#39640;&#20102;&#23545;&#35805;&#20195;&#29702;&#25552;&#20379;&#30340;&#21709;&#24212;&#30340;&#20934;&#30830;&#24615;&#21644;&#20840;&#38754;&#24615;&#12290;&#22312;&#23545;&#35805;&#20013;&#29983;&#25104;&#31572;&#26696;&#21253;&#25324;&#20174;&#36825;&#20123;&#30693;&#35782;&#22270;&#35889;&#29983;&#25104;&#25991;&#26412;&#65292;&#36825;&#20173;&#34987;&#35270;&#20026;&#19968;&#20010;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#30340;&#20855;&#26377;&#25361;&#25112;&#30340;&#20219;&#21153;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23545;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#19981;&#21516;&#26550;&#26500;&#36827;&#34892;&#20102;&#22238;&#39038;&#65292;&#21253;&#25324;&#22270;&#31070;&#32463;&#32593;&#32476;&#12289;&#22270;&#36716;&#25442;&#22120;&#21644;&#24102;&#26377;seq2seq&#27169;&#22411;&#30340;&#32447;&#24615;&#21270;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#27599;&#31181;&#26550;&#26500;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#24471;&#20986;&#32467;&#35770;&#65292;&#36873;&#25321;&#26550;&#26500;&#23558;&#21462;&#20915;&#20110;&#20855;&#20307;&#20219;&#21153;&#30340;&#35201;&#27714;&#12290;&#25105;&#20204;&#36824;&#24378;&#35843;&#20102;&#32771;&#34385;&#35832;&#22914;&#25191;&#34892;&#26102;&#38388;&#21644;&#27169;&#22411;&#26377;&#25928;&#24615;&#31561;&#32422;&#26463;&#30340;&#37325;&#35201;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23545;&#35805;&#20195;&#29702;&#30340;&#32972;&#26223;&#19979;&#12290;&#22522;&#20110;&#36825;&#20123;&#32422;&#26463;&#20197;&#21450;DAVI&#39046;&#22495;&#30340;&#26631;&#35760;&#25968;&#25454;&#30340;&#21487;&#29992;&#24615;&#65292;&#25105;&#20204;&#36873;&#25321;&#20351;&#29992;&#22522;&#20110;seq2seq&#30340;Transformer&#27169;&#22411;&#65288;PLMs&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of knowledge graphs (KGs) enhances the accuracy and comprehensiveness of the responses provided by a conversational agent. While generating answers during conversations consists in generating text from these KGs, it is still regarded as a challenging task that has gained significant attention in recent years. In this document, we provide a review of different architectures used for knowledge graph-to-text generation including: Graph Neural Networks, the Graph Transformer, and linearization with seq2seq models. We discuss the advantages and limitations of each architecture and conclude that the choice of architecture will depend on the specific requirements of the task at hand. We also highlight the importance of considering constraints such as execution time and model validity, particularly in the context of conversational agents. Based on these constraints and the availability of labeled data for the domains of DAVI, we choose to use seq2seq Transformer-based models (PLMs) for
&lt;/p&gt;</description></item><item><title>&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#23398;&#20064;&#25552;&#31034;&#65292;&#35797;&#22270;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#36127;&#38754;&#24773;&#32490;&#12290;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#38480;&#21046;&#34987;&#24573;&#35270;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#33258;&#20449;&#21644;&#19981;&#20934;&#30830;&#30340;&#24314;&#35758;&#12290;&#25215;&#35748;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#21487;&#38752;&#24615;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;</title><link>http://arxiv.org/abs/2307.01540</link><description>&lt;p&gt;
&#22312;&#35838;&#22530;&#19978;&#23398;&#20064;&#25552;&#31034;&#20197;&#20102;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#65306;&#19968;&#39033;&#35797;&#28857;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Learning to Prompt in the Classroom to Understand AI Limits: A pilot study. (arXiv:2307.01540v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01540
&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#36890;&#36807;&#23398;&#20064;&#25552;&#31034;&#65292;&#35797;&#22270;&#22312;&#35838;&#22530;&#29615;&#22659;&#20013;&#29702;&#35299;&#20154;&#24037;&#26234;&#33021;&#30340;&#38480;&#21046;&#12290;&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#24102;&#26469;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#20063;&#24341;&#21457;&#20102;&#36127;&#38754;&#24773;&#32490;&#12290;&#24403;&#21069;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#38480;&#21046;&#34987;&#24573;&#35270;&#65292;&#23548;&#33268;&#20102;&#38169;&#35823;&#30340;&#33258;&#20449;&#21644;&#19981;&#20934;&#30830;&#30340;&#24314;&#35758;&#12290;&#25215;&#35748;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#21487;&#38752;&#24615;&#26159;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#20851;&#38190;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#30340;&#36827;&#23637;&#22312;&#24110;&#21161;&#31038;&#20250;&#35299;&#20915;&#32039;&#36843;&#30340;&#31038;&#20250;&#38382;&#39064;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#30340;&#28508;&#21147;&#12290;&#29305;&#21035;&#26159;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#21644;&#27966;&#29983;&#30340;&#32842;&#22825;&#26426;&#22120;&#20154;&#65292;&#22914;ChatGPT&#65292;&#22823;&#22823;&#25913;&#36827;&#20102;AI&#31995;&#32479;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#65292;&#20351;&#20854;&#33021;&#22815;&#22788;&#29702;&#21069;&#25152;&#26410;&#26377;&#30340;&#22823;&#37327;&#38750;&#32467;&#26500;&#21270;&#25968;&#25454;&#12290;&#30001;&#27492;&#20135;&#29983;&#30340;&#28818;&#20316;&#20063;&#20135;&#29983;&#20102;&#36127;&#38754;&#24773;&#32490;&#65292;&#21363;&#20351;&#22312;&#26032;&#39062;&#30340;AI&#26041;&#27861;&#21462;&#24471;&#20196;&#20154;&#24778;&#35766;&#30340;&#36129;&#29486;&#20043;&#21518;&#12290;&#36896;&#25104;&#36825;&#31181;&#24773;&#20917;&#30340;&#21407;&#22240;&#20043;&#19968;&#65292;&#20294;&#20063;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#26412;&#36523;&#65292;&#26159;&#36234;&#26469;&#36234;&#22810;&#20154;&#38169;&#35823;&#22320;&#35748;&#20026;&#33258;&#24049;&#33021;&#22815;&#36731;&#26494;&#35775;&#38382;&#21644;&#22788;&#29702;&#20219;&#20309;&#24418;&#24335;&#30340;&#30693;&#35782;&#65292;&#20197;&#35299;&#20915;&#20219;&#20309;&#39046;&#22495;&#30340;&#38382;&#39064;&#65292;&#26080;&#38656;&#23545;AI&#25110;&#38382;&#39064;&#39046;&#22495;&#26377;&#20219;&#20309;&#19987;&#19994;&#30693;&#35782;&#65292;&#32780;&#24573;&#35270;&#20102;&#24403;&#21069;LLMs&#30340;&#38480;&#21046;&#65292;&#20363;&#22914;&#24187;&#35273;&#21644;&#25512;&#29702;&#38480;&#21046;&#12290;&#25215;&#35748;&#20154;&#24037;&#26234;&#33021;&#30340;&#19981;&#21487;&#38752;&#24615;&#23545;&#20110;&#35299;&#20915;&#30001;LLMs&#29983;&#25104;&#30340;&#21487;&#33021;&#38169;&#35823;&#24314;&#35758;&#21487;&#33021;&#20135;&#29983;&#30340;&#30450;&#30446;&#36807;&#24230;&#33258;&#20449;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;&#21516;&#26102;&#65292;&#36825;&#21487;&#20197;&#20943;&#23569;&#24656;&#24807;&#21644;&#20854;&#20182;&#36127;&#38754;&#24577;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial intelligence's progress holds great promise in assisting society in addressing pressing societal issues. In particular Large Language Models (LLM) and the derived chatbots, like ChatGPT, have highly improved the natural language processing capabilities of AI systems allowing them to process an unprecedented amount of unstructured data. The consequent hype has also backfired, raising negative sentiment even after novel AI methods' surprising contributions. One of the causes, but also an important issue per se, is the rising and misleading feeling of being able to access and process any form of knowledge to solve problems in any domain with no effort or previous expertise in AI or problem domain, disregarding current LLMs limits, such as hallucinations and reasoning limits. Acknowledging AI fallibility is crucial to address the impact of dogmatic overconfidence in possibly erroneous suggestions generated by LLMs. At the same time, it can reduce fear and other negative attitude
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20998;&#26512;&#33258;&#20027;&#20195;&#29702;&#26377;&#24847;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#24230;&#37327;&#35777;&#25454;&#26469;&#21306;&#20998;&#26377;&#24847;&#32467;&#26524;&#12289;&#30095;&#24573;&#35774;&#35745;&#21644;&#23454;&#38469;&#24847;&#22806;&#12290;&#26041;&#27861;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#24314;&#27169;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#27169;&#22411;&#26816;&#26597;&#35745;&#31639;&#20195;&#29702;&#30340;&#24433;&#21709;&#33021;&#21147;&#12290;&#37319;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#33258;&#21160;&#29983;&#25104;&#30456;&#20851;&#24773;&#26223;&#22686;&#21152;&#35780;&#20272;&#20449;&#24515;&#12290;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#25104;&#21151;&#21306;&#20998;&#8220;&#26377;&#24847;&#8221;&#21644;&#8220;&#24847;&#22806;&#8221;&#30340;&#20132;&#36890;&#30896;&#25758;&#12290;</title><link>http://arxiv.org/abs/2307.01532</link><description>&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20998;&#26512;&#33258;&#20027;&#20195;&#29702;&#30340;&#26377;&#24847;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Analyzing Intentional Behavior in Autonomous Agents under Uncertainty. (arXiv:2307.01532v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#19979;&#20998;&#26512;&#33258;&#20027;&#20195;&#29702;&#26377;&#24847;&#34892;&#20026;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23450;&#37327;&#24230;&#37327;&#35777;&#25454;&#26469;&#21306;&#20998;&#26377;&#24847;&#32467;&#26524;&#12289;&#30095;&#24573;&#35774;&#35745;&#21644;&#23454;&#38469;&#24847;&#22806;&#12290;&#26041;&#27861;&#20351;&#29992;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#24314;&#27169;&#19981;&#30830;&#23450;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#27010;&#29575;&#27169;&#22411;&#26816;&#26597;&#35745;&#31639;&#20195;&#29702;&#30340;&#24433;&#21709;&#33021;&#21147;&#12290;&#37319;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#33258;&#21160;&#29983;&#25104;&#30456;&#20851;&#24773;&#26223;&#22686;&#21152;&#35780;&#20272;&#20449;&#24515;&#12290;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#25104;&#21151;&#21306;&#20998;&#8220;&#26377;&#24847;&#8221;&#21644;&#8220;&#24847;&#22806;&#8221;&#30340;&#20132;&#36890;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19981;&#30830;&#23450;&#29615;&#22659;&#20013;&#65292;&#23545;&#33258;&#20027;&#20915;&#31574;&#34892;&#20026;&#36827;&#34892;&#26377;&#25928;&#30340;&#38382;&#36131;&#38656;&#35201;&#21306;&#20998;&#26377;&#24847;&#32467;&#26524;&#12289;&#30095;&#24573;&#35774;&#35745;&#21644;&#23454;&#38469;&#24847;&#22806;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#26377;&#24847;&#34892;&#20026;&#30340;&#35777;&#25454;&#30340;&#23450;&#37327;&#24230;&#37327;&#26469;&#20998;&#26512;&#33258;&#20027;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#25105;&#20204;&#23558;&#19981;&#30830;&#23450;&#29615;&#22659;&#24314;&#27169;&#20026;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#65288;MDP&#65289;&#12290;&#23545;&#20110;&#32473;&#23450;&#30340;&#24773;&#26223;&#65292;&#25105;&#20204;&#20381;&#38752;&#27010;&#29575;&#24615;&#27169;&#22411;&#26816;&#26597;&#26469;&#35745;&#31639;&#20195;&#29702;&#24433;&#21709;&#36798;&#25104;&#29305;&#23450;&#20107;&#20214;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#31216;&#20043;&#20026;&#20195;&#29702;&#30340;&#33539;&#22260;&#12290;&#22914;&#26524;&#20195;&#29702;&#30340;&#33539;&#22260;&#36739;&#22823;&#19988;&#20854;&#20915;&#31574;&#25509;&#36817;&#36798;&#21040;&#35813;&#20107;&#20214;&#30340;&#26368;&#20339;&#29366;&#24577;&#65292;&#21017;&#23384;&#22312;&#26377;&#24847;&#34892;&#20026;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#33258;&#21160;&#29983;&#25104;&#30456;&#20851;&#24773;&#26223;&#65292;&#20197;&#22686;&#21152;&#25105;&#20204;&#35780;&#20272;&#30340;&#20449;&#24515;&#12290;&#22312;&#19968;&#20010;&#26696;&#20363;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22914;&#20309;&#21306;&#20998;&#8220;&#26377;&#24847;&#8221;&#21644;&#8220;&#24847;&#22806;&#8221;&#30340;&#20132;&#36890;&#30896;&#25758;&#12290;
&lt;/p&gt;
&lt;p&gt;
Principled accountability for autonomous decision-making in uncertain environments requires distinguishing intentional outcomes from negligent designs from actual accidents. We propose analyzing the behavior of autonomous agents through a quantitative measure of the evidence of intentional behavior. We model an uncertain environment as a Markov Decision Process (MDP). For a given scenario, we rely on probabilistic model checking to compute the ability of the agent to influence reaching a certain event. We call this the scope of agency. We say that there is evidence of intentional behavior if the scope of agency is high and the decisions of the agent are close to being optimal for reaching the event. Our method applies counterfactual reasoning to automatically generate relevant scenarios that can be analyzed to increase the confidence of our assessment. In a case study, we show how our method can distinguish between 'intentional' and 'accidental' traffic collisions.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21367;&#31215;&#21464;&#25442;&#22120;&#26550;&#26500;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#20809;&#29031;&#12289;&#36974;&#25377;&#21644;&#25104;&#29087;&#24230;&#26465;&#20214;&#19979;&#33258;&#20027;&#35782;&#21035;&#21644;&#20998;&#32423;&#35199;&#32418;&#26623;&#12290;</title><link>http://arxiv.org/abs/2307.01530</link><description>&lt;p&gt;
&#24212;&#23545;&#19981;&#21516;&#20809;&#29031;&#12289;&#36974;&#25377;&#21644;&#25104;&#29087;&#24230;&#26465;&#20214;&#19979;&#30340;&#35199;&#32418;&#26623;&#33258;&#20027;&#35782;&#21035;&#21644;&#20998;&#32423;&#30340;&#21367;&#31215;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Convolutional Transformer for Autonomous Recognition and Grading of Tomatoes Under Various Lighting, Occlusion, and Ripeness Conditions. (arXiv:2307.01530v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01530
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21367;&#31215;&#21464;&#25442;&#22120;&#26550;&#26500;&#30340;&#26694;&#26550;&#65292;&#33021;&#22815;&#22312;&#19981;&#21516;&#20809;&#29031;&#12289;&#36974;&#25377;&#21644;&#25104;&#29087;&#24230;&#26465;&#20214;&#19979;&#33258;&#20027;&#35782;&#21035;&#21644;&#20998;&#32423;&#35199;&#32418;&#26623;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#65292;&#29992;&#31227;&#21160;&#26426;&#22120;&#20154;&#37319;&#25688;&#23436;&#20840;&#25104;&#29087;&#30340;&#35199;&#32418;&#26623;&#38754;&#20020;&#30528;&#35832;&#22810;&#25361;&#25112;&#12290;&#36825;&#20123;&#25361;&#25112;&#26469;&#33258;&#20110;&#21494;&#23376;&#21644;&#26641;&#26525;&#36896;&#25104;&#30340;&#36974;&#25377;&#65292;&#20197;&#21450;&#22312;&#26524;&#23454;&#21457;&#32946;&#38454;&#27573;&#65292;&#35199;&#32418;&#26623;&#21644;&#21608;&#22260;&#26893;&#34987;&#20043;&#38388;&#30340;&#39068;&#33394;&#30456;&#20284;&#24615;&#12290;&#33258;&#28982;&#29615;&#22659;&#30340;&#22810;&#26679;&#24615;&#36827;&#19968;&#27493;&#21152;&#21095;&#20102;&#36825;&#20123;&#38382;&#39064;&#65292;&#21253;&#25324;&#19981;&#21516;&#30340;&#20809;&#29031;&#26465;&#20214;&#12289;&#35270;&#35282;&#12289;&#36974;&#25377;&#22240;&#32032;&#21644;&#19981;&#21516;&#30340;&#25104;&#29087;&#24230;&#27700;&#24179;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20123;&#38556;&#30861;&#65292;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#21367;&#31215;&#21464;&#25442;&#22120;&#26550;&#26500;&#33258;&#20027;&#35782;&#21035;&#21644;&#20998;&#32423;&#35199;&#32418;&#26623;&#65292;&#26080;&#35770;&#20854;&#36974;&#25377;&#27700;&#24179;&#12289;&#20809;&#29031;&#26465;&#20214;&#21644;&#25104;&#29087;&#24230;&#22914;&#20309;&#12290;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32463;&#36807;&#29305;&#21035;&#20026;&#27492;&#30446;&#30340;&#31934;&#24515;&#27880;&#37322;&#30340;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#25968;&#25454;&#38598;&#22312;&#19981;&#21516;&#30340;&#20809;&#29031;&#26465;&#20214;&#12289;&#35270;&#35282;&#21644;&#20351;&#29992;&#19981;&#21516;&#30340;&#31227;&#21160;&#30456;&#26426;&#20256;&#24863;&#22120;&#19979;&#20934;&#22791;&#65292;&#19982;&#29616;&#26377;&#30340;&#25968;&#25454;&#38598;&#65288;&#22914;Laboro&#65289;&#26377;&#25152;&#21306;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
Harvesting fully ripe tomatoes with mobile robots presents significant challenges in real-world scenarios. These challenges arise from factors such as occlusion caused by leaves and branches, as well as the color similarity between tomatoes and the surrounding foliage during the fruit development stage. The natural environment further compounds these issues with varying light conditions, viewing angles, occlusion factors, and different maturity levels. To overcome these obstacles, this research introduces a novel framework that leverages a convolutional transformer architecture to autonomously recognize and grade tomatoes, irrespective of their occlusion level, lighting conditions, and ripeness. The proposed model is trained and tested using carefully annotated images curated specifically for this purpose. The dataset is prepared under various lighting conditions, viewing perspectives, and employs different mobile camera sensors, distinguishing it from existing datasets such as Laboro 
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;LEAT&#30340;&#26041;&#27861;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#23545;&#28145;&#24230;&#20266;&#36896;&#30340;&#21487;&#38752;&#24178;&#25200;&#12290;LEAT&#36890;&#36807;&#25915;&#20987;&#29420;&#31435;&#30340;&#28508;&#22312;&#32534;&#30721;&#36807;&#31243;&#26469;&#24178;&#25200;&#28145;&#24230;&#20266;&#36896;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01520</link><description>&lt;p&gt;
LEAT: &#36890;&#36807;&#28508;&#22312;&#32534;&#30721;&#38598;&#25104;&#25915;&#20987;&#65292;&#23454;&#29616;&#23545;&#30495;&#23454;&#22330;&#26223;&#20013;&#28145;&#24230;&#20266;&#36896;&#30340;&#21487;&#38752;&#24178;&#25200;
&lt;/p&gt;
&lt;p&gt;
LEAT: Towards Robust Deepfake Disruption in Real-World Scenarios via Latent Ensemble Attack. (arXiv:2307.01520v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01520
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#19968;&#31181;&#21517;&#20026;LEAT&#30340;&#26041;&#27861;&#65292;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#23454;&#29616;&#20102;&#23545;&#28145;&#24230;&#20266;&#36896;&#30340;&#21487;&#38752;&#24178;&#25200;&#12290;LEAT&#36890;&#36807;&#25915;&#20987;&#29420;&#31435;&#30340;&#28508;&#22312;&#32534;&#30721;&#36807;&#31243;&#26469;&#24178;&#25200;&#28145;&#24230;&#20266;&#36896;&#65292;&#25552;&#39640;&#20102;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20266;&#36896;&#26159;&#30001;&#29983;&#25104;&#27169;&#22411;&#21019;&#24314;&#30340;&#24694;&#24847;&#35270;&#35273;&#20869;&#23481;&#65292;&#23545;&#31038;&#20250;&#36896;&#25104;&#36234;&#26469;&#36234;&#22823;&#30340;&#23041;&#32961;&#12290;&#20026;&#20102;&#20027;&#21160;&#20943;&#36731;&#28145;&#24230;&#20266;&#36896;&#30340;&#25439;&#23475;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#37319;&#29992;&#20102;&#23545;&#25239;&#25200;&#21160;&#26469;&#24178;&#25200;&#28145;&#24230;&#20266;&#36896;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#28982;&#32780;&#65292;&#20043;&#21069;&#30340;&#26041;&#27861;&#20027;&#35201;&#38598;&#20013;&#22312;&#22522;&#20110;&#39044;&#23450;&#30446;&#26631;&#23646;&#24615;&#29983;&#25104;&#25197;&#26354;&#36755;&#20986;&#65292;&#23548;&#33268;&#22312;&#30446;&#26631;&#23646;&#24615;&#26410;&#30693;&#30340;&#30495;&#23454;&#22330;&#26223;&#20013;&#32570;&#20047;&#40065;&#26834;&#24615;&#12290;&#27492;&#22806;&#65292;&#20004;&#31181;&#20027;&#35201;&#29983;&#25104;&#27169;&#22411;&#8212;&#8212;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65288;GAN&#65289;&#21644;&#25193;&#25955;&#27169;&#22411;&#20043;&#38388;&#30340;&#25200;&#21160;&#21487;&#36716;&#31227;&#24615;&#20173;&#28982;&#26410;&#34987;&#25506;&#32034;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#30446;&#26631;&#23646;&#24615;&#21487;&#36716;&#31227;&#24615;&#21644;&#27169;&#22411;&#21487;&#36716;&#31227;&#24615;&#22312;&#23454;&#29616;&#23545;&#28145;&#24230;&#20266;&#36896;&#30340;&#21487;&#38752;&#24178;&#25200;&#26041;&#38754;&#30340;&#37325;&#35201;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#24178;&#25200;&#26041;&#27861;&#65292;&#21363;&#28508;&#22312;&#32534;&#30721;&#38598;&#25104;&#25915;&#20987;&#65288;LEAT&#65289;&#65292;&#23427;&#25915;&#20987;&#20102;&#29420;&#31435;&#30340;&#28508;&#22312;&#32534;&#30721;&#36807;&#31243;&#12290;&#36890;&#36807;&#24178;&#25200;&#28508;&#22312;&#32534;&#30721;&#36807;&#31243;&#65292;&#23427;&#21487;&#20197;&#25552;&#39640;&#22312;&#30495;&#23454;&#22330;&#26223;&#20013;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deepfakes, malicious visual contents created by generative models, pose an increasingly harmful threat to society. To proactively mitigate deepfake damages, recent studies have employed adversarial perturbation to disrupt deepfake model outputs. However, previous approaches primarily focus on generating distorted outputs based on only predetermined target attributes, leading to a lack of robustness in real-world scenarios where target attributes are unknown. Additionally, the transferability of perturbations between two prominent generative models, Generative Adversarial Networks (GANs) and Diffusion Models, remains unexplored. In this paper, we emphasize the importance of target attribute-transferability and model-transferability for achieving robust deepfake disruption. To address this challenge, we propose a simple yet effective disruption method called Latent Ensemble ATtack (LEAT), which attacks the independent latent encoding process. By disrupting the latent encoding process, it
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Deep Attention Q-Network&#65292;&#21033;&#29992;Transformer&#26550;&#26500;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;&#20010;&#24615;&#21270;&#25512;&#33616;&#27835;&#30103;&#26041;&#26696;&#65292;&#36890;&#36807;&#39640;&#25928;&#25972;&#21512;&#36807;&#21435;&#30340;&#30149;&#24739;&#35266;&#23519;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#20165;&#20381;&#36182;&#24403;&#21069;&#35266;&#23519;&#20449;&#24687;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27835;&#30103;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01519</link><description>&lt;p&gt;
&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#30340;&#28145;&#24230;&#27880;&#24847;&#21147;Q&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Deep Attention Q-Network for Personalized Treatment Recommendation. (arXiv:2307.01519v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Deep Attention Q-Network&#65292;&#21033;&#29992;Transformer&#26550;&#26500;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;&#20010;&#24615;&#21270;&#25512;&#33616;&#27835;&#30103;&#26041;&#26696;&#65292;&#36890;&#36807;&#39640;&#25928;&#25972;&#21512;&#36807;&#21435;&#30340;&#30149;&#24739;&#35266;&#23519;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#20165;&#20381;&#36182;&#24403;&#21069;&#35266;&#23519;&#20449;&#24687;&#30340;&#38480;&#21046;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27835;&#30103;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#27835;&#30103;&#23545;&#20110;&#27599;&#20010;&#30149;&#24739;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#22312;&#36798;&#21040;&#26368;&#20339;&#30340;&#21307;&#30103;&#25928;&#26524;&#26041;&#38754;&#20063;&#38754;&#20020;&#25361;&#25112;&#12290;&#26368;&#36817;&#24378;&#21270;&#23398;&#20064;&#30340;&#36827;&#23637;&#20026;&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#26041;&#27861;&#65307;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#20165;&#20381;&#38752;&#24403;&#21069;&#30149;&#24739;&#30340;&#35266;&#23519;&#20449;&#24687;&#65288;&#29983;&#21629;&#20307;&#24449;&#12289;&#20154;&#21475;&#32479;&#35745;&#25968;&#25454;&#65289;&#20316;&#20026;&#30149;&#24739;&#30340;&#29366;&#24577;&#65292;&#21487;&#33021;&#26080;&#27861;&#20934;&#30830;&#22320;&#20195;&#34920;&#30149;&#24739;&#30340;&#30495;&#23454;&#20581;&#24247;&#29366;&#20917;&#12290;&#36825;&#31181;&#38480;&#21046;&#22952;&#30861;&#20102;&#31574;&#30053;&#23398;&#20064;&#21644;&#35780;&#20272;&#65292;&#26368;&#32456;&#38480;&#21046;&#20102;&#27835;&#30103;&#25928;&#26524;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#28145;&#24230;&#27880;&#24847;&#21147;Q&#32593;&#32476;&#26469;&#36827;&#34892;&#20010;&#24615;&#21270;&#27835;&#30103;&#25512;&#33616;&#65292;&#21033;&#29992;Transformer&#26550;&#26500;&#22312;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#20869;&#65292;&#39640;&#25928;&#22320;&#25972;&#21512;&#20102;&#25152;&#26377;&#36807;&#21435;&#30340;&#30149;&#24739;&#35266;&#23519;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#30340;&#36133;&#34880;&#30151;&#21644;&#24613;&#24615;&#20302;&#34880;&#21387;&#30149;&#20154;&#32676;&#20013;&#35780;&#20272;&#20102;&#35813;&#27169;&#22411;&#65292;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#29616;&#26377;&#27169;&#22411;&#30340;&#20248;&#36234;&#24615;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#28304;&#20195;&#30721;&#21487;&#20197;&#22312;https://github.com/stevenmsm/RL-ICU-DAQN&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Tailoring treatment for individual patients is crucial yet challenging in order to achieve optimal healthcare outcomes. Recent advances in reinforcement learning offer promising personalized treatment recommendations; however, they rely solely on current patient observations (vital signs, demographics) as the patient's state, which may not accurately represent the true health status of the patient. This limitation hampers policy learning and evaluation, ultimately limiting treatment effectiveness. In this study, we propose the Deep Attention Q-Network for personalized treatment recommendations, utilizing the Transformer architecture within a deep reinforcement learning framework to efficiently incorporate all past patient observations. We evaluated the model on real-world sepsis and acute hypotension cohorts, demonstrating its superiority to state-of-the-art models. The source code for our model is available at https://github.com/stevenmsm/RL-ICU-DAQN.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#25552;&#31034;&#21644;&#35821;&#35328;&#25552;&#31034;&#30340;&#26684;&#24335;&#65292;&#22635;&#34917;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#21508;&#31181;&#22270;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;</title><link>http://arxiv.org/abs/2307.01504</link><description>&lt;p&gt;
&#19968;&#20307;&#21270;&#65306;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;
&lt;/p&gt;
&lt;p&gt;
All in One: Multi-task Prompting for Graph Neural Networks. (arXiv:2307.01504v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01504
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22270;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#32479;&#19968;&#22270;&#25552;&#31034;&#21644;&#35821;&#35328;&#25552;&#31034;&#30340;&#26684;&#24335;&#65292;&#22635;&#34917;&#20102;&#39044;&#35757;&#32451;&#27169;&#22411;&#19982;&#21508;&#31181;&#22270;&#20219;&#21153;&#20043;&#38388;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#8220;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#8221;&#24050;&#25104;&#20026;&#35768;&#22810;&#22270;&#20219;&#21153;&#30340;&#26631;&#20934;&#24037;&#20316;&#27969;&#31243;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#21033;&#29992;&#36890;&#29992;&#30340;&#22270;&#30693;&#35782;&#26469;&#32531;&#35299;&#27599;&#20010;&#24212;&#29992;&#20013;&#32570;&#20047;&#22270;&#27880;&#37322;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#33410;&#28857;&#32423;&#12289;&#36793;&#32423;&#21644;&#22270;&#32423;&#30340;&#22270;&#20219;&#21153;&#24046;&#24322;&#24456;&#22823;&#65292;&#23548;&#33268;&#39044;&#35757;&#32451;&#39044;&#25991;&#26412;&#36890;&#24120;&#19982;&#36825;&#20123;&#22810;&#20219;&#21153;&#19981;&#20860;&#23481;&#12290;&#36825;&#31181;&#24046;&#36317;&#29978;&#33267;&#21487;&#33021;&#23548;&#33268;&#23545;&#29305;&#23450;&#24212;&#29992;&#30340;&#8220;&#36127;&#36801;&#31227;&#8221;&#65292;&#20174;&#32780;&#23548;&#33268;&#32467;&#26524;&#19981;&#20339;&#12290;&#21463;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20013;&#25552;&#31034;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;NLP&#20219;&#21153;&#20013;&#21033;&#29992;&#20808;&#21069;&#30693;&#35782;&#24050;&#32463;&#26174;&#31034;&#20986;&#36739;&#22823;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#22635;&#34917;&#39044;&#35757;&#32451;&#27169;&#22411;&#21644;&#21508;&#31181;&#22270;&#20219;&#21153;&#20043;&#38388;&#24046;&#36317;&#30340;&#25552;&#31034;&#20027;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29992;&#20110;&#22270;&#27169;&#22411;&#30340;&#22810;&#20219;&#21153;&#25552;&#31034;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#39318;&#20808;&#36890;&#36807;&#25552;&#31034;&#20196;&#29260;&#12289;&#20196;&#29260;&#32467;&#26500;&#21644;&#25554;&#20837;&#27169;&#24335;&#32479;&#19968;&#22270;&#25552;&#31034;&#21644;&#35821;&#35328;&#25552;&#31034;&#30340;&#26684;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, ''pre-training and fine-tuning'' has been adopted as a standard workflow for many graph tasks since it can take general graph knowledge to relieve the lack of graph annotations from each application. However, graph tasks with node level, edge level, and graph level are far diversified, making the pre-training pretext often incompatible with these multiple tasks. This gap may even cause a ''negative transfer'' to the specific application, leading to poor results. Inspired by the prompt learning in natural language processing (NLP), which has presented significant effectiveness in leveraging prior knowledge for various NLP tasks, we study the prompting topic for graphs with the motivation of filling the gap between pre-trained models and various graph tasks. In this paper, we propose a novel multi-task prompting method for graph models. Specifically, we first unify the format of graph prompts and language prompts with the prompt token, token structure, and inserting pattern. In
&lt;/p&gt;</description></item><item><title>HEDI&#26159;&#19968;&#31181;&#29992;&#20110;&#20999;&#21475;&#30109;&#20462;&#22797;&#30340;&#29983;&#29289;&#21147;&#23398;&#35780;&#20272;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#32771;&#34385;&#33145;&#22721;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#33021;&#22815;&#33258;&#21160;&#26816;&#27979;&#21644;&#35780;&#20272;&#30109;&#30340;&#22823;&#23567;&#12289;&#20307;&#31215;&#21644;&#33145;&#22721;&#19981;&#31283;&#23450;&#24615;&#12290;&#22312;31&#21517;&#24739;&#32773;&#30340;&#39044;&#25163;&#26415;&#35780;&#20272;&#20013;&#65292;HEDI&#26174;&#31034;&#20986;&#26126;&#26174;&#25913;&#21892;&#30340;&#25104;&#21151;&#29575;&#65292;&#25152;&#26377;&#24739;&#32773;&#22312;&#38543;&#35775;&#19977;&#24180;&#21518;&#20173;&#28982;&#27809;&#26377;&#30140;&#30171;&#21644;&#30109;&#20877;&#21457;&#12290;</title><link>http://arxiv.org/abs/2307.01502</link><description>&lt;p&gt;
HEDI: &#31532;&#19968;&#27425;&#20020;&#24202;&#24212;&#29992;&#30340;&#20999;&#21475;&#30109;&#20462;&#22797;&#29983;&#29289;&#21147;&#23398;&#35780;&#20272;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#30340;&#32467;&#26524;
&lt;/p&gt;
&lt;p&gt;
HEDI: First-Time Clinical Application and Results of a Biomechanical Evaluation and Visualisation Tool for Incisional Hernia Repair. (arXiv:2307.01502v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01502
&lt;/p&gt;
&lt;p&gt;
HEDI&#26159;&#19968;&#31181;&#29992;&#20110;&#20999;&#21475;&#30109;&#20462;&#22797;&#30340;&#29983;&#29289;&#21147;&#23398;&#35780;&#20272;&#21644;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#36890;&#36807;&#32771;&#34385;&#33145;&#22721;&#30340;&#19981;&#31283;&#23450;&#24615;&#65292;&#33021;&#22815;&#33258;&#21160;&#26816;&#27979;&#21644;&#35780;&#20272;&#30109;&#30340;&#22823;&#23567;&#12289;&#20307;&#31215;&#21644;&#33145;&#22721;&#19981;&#31283;&#23450;&#24615;&#12290;&#22312;31&#21517;&#24739;&#32773;&#30340;&#39044;&#25163;&#26415;&#35780;&#20272;&#20013;&#65292;HEDI&#26174;&#31034;&#20986;&#26126;&#26174;&#25913;&#21892;&#30340;&#25104;&#21151;&#29575;&#65292;&#25152;&#26377;&#24739;&#32773;&#22312;&#38543;&#35775;&#19977;&#24180;&#21518;&#20173;&#28982;&#27809;&#26377;&#30140;&#30171;&#21644;&#30109;&#20877;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33145;&#22721;&#32570;&#38519;&#36890;&#24120;&#23548;&#33268;&#30140;&#30171;&#12289;&#19981;&#36866;&#20197;&#21450;&#20999;&#21475;&#30109;&#20877;&#21457;&#65292;&#20840;&#29699;&#33539;&#22260;&#20869;&#36896;&#25104;&#37325;&#22823;&#21457;&#30149;&#29575;&#21644;&#22810;&#27425;&#25163;&#26415;&#20462;&#22797;&#12290;&#23545;&#20110;&#22823;&#22411;&#30109;&#65292;&#32593;&#26684;&#20462;&#22797;&#36890;&#24120;&#22522;&#20110;&#32570;&#38519;&#21306;&#22495;&#19982;&#22266;&#23450;&#37325;&#21472;&#65292;&#32780;&#19981;&#32771;&#34385;&#29983;&#29289;&#21147;&#23398;&#26041;&#38754;&#30340;&#22240;&#32032;&#65292;&#22914;&#32908;&#32905;&#28608;&#27963;&#12289;&#33145;&#33108;&#20869;&#21387;&#21147;&#12289;&#32452;&#32455;&#24377;&#24615;&#21644;&#33145;&#22721;&#25193;&#24352;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32771;&#34385;&#19981;&#31283;&#23450;&#33145;&#22721;&#30340;&#20999;&#21475;&#30109;&#20462;&#22797;&#30340;&#29983;&#29289;&#21147;&#23398;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;HEDI&#65292;&#36825;&#26159;&#19968;&#31181;&#21033;&#29992;Valsalva&#21160;&#20316;&#30340;&#21160;&#24577;&#35745;&#31639;&#26426;&#26029;&#23618;&#25195;&#25551;&#25216;&#26415;&#26469;&#33258;&#21160;&#26816;&#27979;&#21644;&#35780;&#20272;&#30109;&#22823;&#23567;&#12289;&#20307;&#31215;&#21644;&#33145;&#22721;&#19981;&#31283;&#23450;&#24615;&#30340;&#24037;&#20855;&#12290;&#25105;&#20204;&#22312;31&#21517;&#24739;&#32773;&#39044;&#25163;&#26415;&#35780;&#20272;&#20013;&#39318;&#27425;&#20020;&#24202;&#24212;&#29992;&#20102;HEDI&#65292;&#19982;&#25253;&#36947;&#30340;&#25104;&#21151;&#29575;&#30456;&#27604;&#65292;&#26174;&#31034;&#20986;&#26126;&#26174;&#25913;&#21892;&#65292;&#25152;&#26377;&#24739;&#32773;&#22312;&#38543;&#35775;&#19977;&#24180;&#21518;&#20173;&#28982;&#27809;&#26377;&#30140;&#30171;&#21644;&#30109;&#20877;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abdominal wall defects often lead to pain, discomfort, and recurrence of incisional hernias, resulting in significant morbidity and repeated surgical repairs worldwide. Mesh repair for large hernias is usually based on the defect area with a fixed overlap, without considering biomechanical aspects such as muscle activation, intra-abdominal pressure, tissue elasticity, and abdominal wall distention. To address this issue, we present a biomechanical approach to incisional hernia repair that takes into account the unstable abdominal wall. Additionally, we introduce HEDI, a tool that uses dynamic computed tomography with Valsalva maneuver to automatically detect and assess hernia size, volume, and abdominal wall instability. Our first clinical application of HEDI in the preoperative evaluation of 31 patients shows significantly improved success rates compared to reported rates, with all patients remaining pain-free and showing no hernia recurrence after three years of follow-up.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#21521;&#21069;&#26223;&#38598;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#23545;&#20027;&#35201;&#27010;&#24565;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2307.01473</link><description>&lt;p&gt;
&#32531;&#35299;&#20559;&#35265;&#65306;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#35299;&#37322;&#26469;&#25552;&#21319;&#22270;&#20687;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Mitigating Bias: Enhancing Image Classification by Improving Model Explanations. (arXiv:2307.01473v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01473
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#36827;&#27169;&#22411;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#32531;&#35299;&#22270;&#20687;&#20998;&#31867;&#20013;&#30340;&#20559;&#35265;&#38382;&#39064;&#65292;&#36890;&#36807;&#24341;&#23548;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#21521;&#21069;&#26223;&#38598;&#20013;&#65292;&#20174;&#32780;&#25552;&#21319;&#23545;&#20027;&#35201;&#27010;&#24565;&#30340;&#23398;&#20064;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#22312;&#20174;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#22797;&#26434;&#27169;&#24335;&#21644;&#27010;&#24565;&#26041;&#38754;&#23637;&#31034;&#20986;&#20102;&#26174;&#33879;&#30340;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#21457;&#29616;&#36825;&#20123;&#27169;&#22411;&#20542;&#21521;&#20110;&#36807;&#20998;&#20381;&#36182;&#20110;&#22270;&#29255;&#32972;&#26223;&#20013;&#30340;&#31616;&#21333;&#21644;&#23481;&#26131;&#35782;&#21035;&#30340;&#29305;&#24449;&#65292;&#32780;&#19981;&#26159;&#23427;&#20204;&#26412;&#24212;&#20998;&#31867;&#30340;&#20027;&#35201;&#27010;&#24565;&#25110;&#23545;&#35937;&#12290;&#36825;&#31181;&#29616;&#35937;&#32473;&#22270;&#20687;&#20998;&#31867;&#22120;&#24102;&#26469;&#20102;&#25361;&#25112;&#65292;&#22240;&#20026;&#22270;&#29255;&#20013;&#30340;&#20851;&#38190;&#20803;&#32032;&#21487;&#33021;&#20250;&#34987;&#25513;&#30422;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#24182;&#25913;&#21892;&#22270;&#20687;&#20998;&#31867;&#22120;&#23545;&#20027;&#35201;&#27010;&#24565;&#30340;&#23398;&#20064;&#12290;&#25105;&#20204;&#30340;&#26680;&#24515;&#24605;&#24819;&#26159;&#22312;&#20998;&#31867;&#20219;&#21153;&#36807;&#31243;&#20013;&#21516;&#26102;&#24341;&#23548;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#21521;&#21069;&#26223;&#38598;&#20013;&#12290;&#36890;&#36807;&#24378;&#35843;&#21069;&#26223;&#65292;&#21363;&#20027;&#35201;&#24863;&#20852;&#36259;&#30340;&#23545;&#35937;&#65292;&#25105;&#20204;&#26088;&#22312;&#23558;&#27169;&#22411;&#30340;&#27880;&#24847;&#21147;&#20174;&#32972;&#26223;&#30340;&#20027;&#23548;&#24433;&#21709;&#19978;&#36716;&#31227;&#24320;&#26469;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26426;&#21046;&#26469;&#40723;&#21169;&#27169;&#22411;&#36275;&#22815;&#22320;&#20998;&#37197;&#27880;&#24847;&#21147;&#32473;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning models have demonstrated remarkable capabilities in learning complex patterns and concepts from training data. However, recent findings indicate that these models tend to rely heavily on simple and easily discernible features present in the background of images rather than the main concepts or objects they are intended to classify. This phenomenon poses a challenge to image classifiers as the crucial elements of interest in images may be overshadowed. In this paper, we propose a novel approach to address this issue and improve the learning of main concepts by image classifiers. Our central idea revolves around concurrently guiding the model's attention toward the foreground during the classification task. By emphasizing the foreground, which encapsulates the primary objects of interest, we aim to shift the focus of the model away from the dominant influence of the background. To accomplish this, we introduce a mechanism that encourages the model to allocate sufficient att
&lt;/p&gt;</description></item><item><title>DOM2&#26159;&#19968;&#31181;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#31574;&#30053;&#30340;&#25913;&#36827;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#24615;&#33021;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;DOM2&#22312;&#22810;&#26234;&#33021;&#20307;&#31890;&#23376;&#21644;&#22810;&#26234;&#33021;&#20307;MuJoCo&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#22312;&#31227;&#20301;&#29615;&#22659;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;DOM2&#36824;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#21482;&#20351;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#21363;&#21487;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2307.01472</link><description>&lt;p&gt;
&#36229;&#36234;&#20445;&#23432;&#20027;&#20041;&#65306;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#25193;&#25955;&#31574;&#30053;
&lt;/p&gt;
&lt;p&gt;
Beyond Conservatism: Diffusion Policies in Offline Multi-agent Reinforcement Learning. (arXiv:2307.01472v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01472
&lt;/p&gt;
&lt;p&gt;
DOM2&#26159;&#19968;&#31181;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#65292;&#36890;&#36807;&#25193;&#25955;&#31574;&#30053;&#30340;&#25913;&#36827;&#65292;&#25552;&#39640;&#20102;&#31639;&#27861;&#22312;&#24615;&#33021;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;DOM2&#22312;&#22810;&#26234;&#33021;&#20307;&#31890;&#23376;&#21644;&#22810;&#26234;&#33021;&#20307;MuJoCo&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#31639;&#27861;&#65292;&#24182;&#22312;&#31227;&#20301;&#29615;&#22659;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;DOM2&#36824;&#23637;&#29616;&#20102;&#21331;&#36234;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#21482;&#20351;&#29992;&#36739;&#23569;&#30340;&#25968;&#25454;&#21363;&#21487;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#27169;&#22411;&#65288;DOM2&#65289;&#65292;&#29992;&#20110;&#31163;&#32447;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#12290;&#19982;&#29616;&#26377;&#31639;&#27861;&#22312;&#31574;&#30053;&#35774;&#35745;&#20013;&#20027;&#35201;&#20381;&#36182;&#20445;&#23432;&#20027;&#20041;&#19981;&#21516;&#65292;DOM2&#22522;&#20110;&#25193;&#25955;&#22686;&#24378;&#20102;&#31574;&#30053;&#30340;&#34920;&#36798;&#33021;&#21147;&#21644;&#22810;&#26679;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23558;&#25193;&#25955;&#27169;&#22411;&#32435;&#20837;&#31574;&#30053;&#32593;&#32476;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36712;&#36857;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#26696;&#36827;&#34892;&#35757;&#32451;&#12290;&#36825;&#20123;&#20851;&#38190;&#22240;&#32032;&#20351;&#25105;&#20204;&#30340;&#31639;&#27861;&#22312;&#29615;&#22659;&#21464;&#21270;&#26041;&#38754;&#26356;&#21152;&#31283;&#20581;&#65292;&#24182;&#22312;&#24615;&#33021;&#12289;&#27867;&#21270;&#33021;&#21147;&#21644;&#25968;&#25454;&#25928;&#29575;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;&#25105;&#20204;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;DOM2&#22312;&#22810;&#26234;&#33021;&#20307;&#31890;&#23376;&#21644;&#22810;&#26234;&#33021;&#20307;MuJoCo&#29615;&#22659;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#19988;&#30001;&#20110;&#20854;&#39640;&#34920;&#36798;&#33021;&#21147;&#21644;&#22810;&#26679;&#24615;&#65292;&#22312;&#31227;&#20301;&#29615;&#22659;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;DOM2&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#25968;&#25454;&#25928;&#29575;&#65292;&#22312;&#19982;&#29616;&#26377;&#31639;&#27861;&#30456;&#27604;&#21482;&#20351;&#29992;$20+$&#20493;&#23569;&#30340;&#25968;&#25454;&#19979;&#65292;&#23601;&#33021;&#36798;&#21040;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel Diffusion Offline Multi-agent Model (DOM2) for offline Multi-Agent Reinforcement Learning (MARL). Different from existing algorithms that rely mainly on conservatism in policy design, DOM2 enhances policy expressiveness and diversity based on diffusion. Specifically, we incorporate a diffusion model into the policy network and propose a trajectory-based data-augmentation scheme in training. These key ingredients make our algorithm more robust to environment changes and achieve significant improvements in performance, generalization and data-efficiency. Our extensive experimental results demonstrate that DOM2 outperforms existing state-of-the-art methods in multi-agent particle and multi-agent MuJoCo environments, and generalizes significantly better in shifted environments thanks to its high expressiveness and diversity. Furthermore, DOM2 shows superior data efficiency and can achieve state-of-the-art performance with $20+$ times less data compared to existing algori
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#32508;&#36848;&#24635;&#32467;&#20102;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#25991;&#29486;&#65292;&#24378;&#35843;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#33021;&#22815;&#24418;&#24335;&#21270;&#30693;&#35782;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;</title><link>http://arxiv.org/abs/2307.01452</link><description>&lt;p&gt;
&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#65306;&#19968;&#39033;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Causal Reinforcement Learning: A Survey. (arXiv:2307.01452v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01452
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#32508;&#36848;&#24635;&#32467;&#20102;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#30740;&#31350;&#25991;&#29486;&#65292;&#24378;&#35843;&#22240;&#26524;&#20851;&#31995;&#30340;&#37325;&#35201;&#20316;&#29992;&#65292;&#23427;&#33021;&#22815;&#24418;&#24335;&#21270;&#30693;&#35782;&#24182;&#23454;&#29616;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#22312;&#19981;&#30830;&#23450;&#24615;&#26465;&#20214;&#19979;&#35299;&#20915;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#30340;&#19968;&#31181;&#37325;&#35201;&#33539;&#24335;&#12290;&#23613;&#31649;&#36817;&#20960;&#21313;&#24180;&#26469;&#21462;&#24471;&#20102;&#35768;&#22810;&#26174;&#33879;&#30340;&#25104;&#23601;&#65292;&#20294;&#23558;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#24212;&#29992;&#20110;&#29616;&#23454;&#19990;&#30028;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20854;&#20013;&#19968;&#20010;&#20027;&#35201;&#38556;&#30861;&#26159;&#24378;&#21270;&#23398;&#20064;&#20195;&#29702;&#32570;&#20047;&#23545;&#19990;&#30028;&#30340;&#22522;&#26412;&#29702;&#35299;&#65292;&#22240;&#27492;&#24517;&#39035;&#36890;&#36807;&#22823;&#37327;&#30340;&#35797;&#38169;&#20132;&#20114;&#23398;&#20064;&#12290;&#20182;&#20204;&#21487;&#33021;&#22312;&#35299;&#37322;&#33258;&#24049;&#30340;&#20915;&#31574;&#20197;&#21450;&#25512;&#24191;&#25152;&#33719;&#24471;&#30340;&#30693;&#35782;&#26041;&#38754;&#38754;&#20020;&#25361;&#25112;&#12290;&#28982;&#32780;&#65292;&#22240;&#26524;&#20851;&#31995;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#65292;&#23427;&#21487;&#20197;&#20197;&#31995;&#32479;&#21270;&#30340;&#26041;&#24335;&#24418;&#24335;&#21270;&#30693;&#35782;&#65292;&#24182;&#21033;&#29992;&#19981;&#21464;&#24615;&#36827;&#34892;&#26377;&#25928;&#30340;&#30693;&#35782;&#20256;&#36882;&#12290;&#36825;&#23548;&#33268;&#20102;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#20986;&#29616;&#65292;&#23427;&#26159;&#24378;&#21270;&#23398;&#20064;&#30340;&#19968;&#20010;&#23376;&#39046;&#22495;&#65292;&#26088;&#22312;&#36890;&#36807;&#23558;&#22240;&#26524;&#20851;&#31995;&#32435;&#20837;&#23398;&#20064;&#36807;&#31243;&#26469;&#22686;&#24378;&#29616;&#26377;&#31639;&#27861;&#12290;&#22312;&#36825;&#31687;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#20840;&#38754;&#22238;&#39038;&#20102;&#26377;&#20851;&#22240;&#26524;&#24378;&#21270;&#23398;&#20064;&#30340;&#25991;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is an essential paradigm for solving sequential decision problems under uncertainty. Despite many remarkable achievements in recent decades, applying reinforcement learning methods in the real world remains challenging. One of the main obstacles is that reinforcement learning agents lack a fundamental understanding of the world and must therefore learn from scratch through numerous trial-and-error interactions. They may also face challenges in providing explanations for their decisions and generalizing the acquired knowledge. Causality, however, offers a notable advantage as it can formalize knowledge in a systematic manner and leverage invariance for effective knowledge transfer. This has led to the emergence of causal reinforcement learning, a subfield of reinforcement learning that seeks to enhance existing algorithms by incorporating causal relationships into the learning process. In this survey, we comprehensively review the literature on causal reinforcemen
&lt;/p&gt;</description></item><item><title>&#36825;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23558;&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#27979;&#35797;&#20551;&#35774;&#30340;&#36829;&#21453;&#24773;&#20917;&#24182;&#19968;&#33268;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#23427;&#25552;&#20379;&#20102;&#21322;&#21442;&#25968;&#39640;&#25928;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#26159;&#21487;&#34892;&#30340;&#12290;</title><link>http://arxiv.org/abs/2307.01449</link><description>&lt;p&gt;
&#23558;&#23454;&#39564;&#25968;&#25454;&#19982;&#35266;&#27979;&#25968;&#25454;&#32467;&#21512;&#30340;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Double Machine Learning Approach to Combining Experimental and Observational Data. (arXiv:2307.01449v1 [stat.ME])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01449
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#23558;&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#65292;&#33021;&#22815;&#27979;&#35797;&#20551;&#35774;&#30340;&#36829;&#21453;&#24773;&#20917;&#24182;&#19968;&#33268;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#23427;&#25552;&#20379;&#20102;&#21322;&#21442;&#25968;&#39640;&#25928;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#12290;&#36825;&#31181;&#26041;&#27861;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#36890;&#24120;&#30001;&#20110;&#26080;&#27861;&#27979;&#35797;&#30340;&#20551;&#35774;&#32780;&#32570;&#20047;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#23454;&#39564;&#21644;&#35266;&#27979;&#30740;&#31350;&#32467;&#21512;&#36215;&#26469;&#65292;&#20351;&#20174;&#19994;&#20154;&#21592;&#33021;&#22815;&#27979;&#35797;&#20551;&#35774;&#36829;&#21453;&#24773;&#20917;&#24182;&#19968;&#33268;&#20272;&#35745;&#22788;&#29702;&#25928;&#24212;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#22312;&#36739;&#36731;&#30340;&#20551;&#35774;&#19979;&#27979;&#35797;&#22806;&#37096;&#25928;&#24230;&#21644;&#21487;&#24573;&#35270;&#24615;&#30340;&#36829;&#21453;&#24773;&#20917;&#12290;&#24403;&#21482;&#26377;&#19968;&#20010;&#20551;&#35774;&#34987;&#36829;&#21453;&#26102;&#65292;&#25105;&#20204;&#25552;&#20379;&#21322;&#21442;&#25968;&#39640;&#25928;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#22120;&#12290;&#28982;&#32780;&#65292;&#25105;&#20204;&#30340;&#26080;&#20813;&#36153;&#21320;&#39184;&#23450;&#29702;&#24378;&#35843;&#20102;&#20934;&#30830;&#35782;&#21035;&#36829;&#21453;&#30340;&#20551;&#35774;&#23545;&#19968;&#33268;&#30340;&#22788;&#29702;&#25928;&#24212;&#20272;&#35745;&#30340;&#24517;&#35201;&#24615;&#12290;&#25105;&#20204;&#36890;&#36807;&#19977;&#20010;&#23454;&#38469;&#26696;&#20363;&#30740;&#31350;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#36866;&#29992;&#24615;&#65292;&#24182;&#31361;&#20986;&#20102;&#20854;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Experimental and observational studies often lack validity due to untestable assumptions. We propose a double machine learning approach to combine experimental and observational studies, allowing practitioners to test for assumption violations and estimate treatment effects consistently. Our framework tests for violations of external validity and ignorability under milder assumptions. When only one assumption is violated, we provide semi-parametrically efficient treatment effect estimators. However, our no-free-lunch theorem highlights the necessity of accurately identifying the violated assumption for consistent treatment effect estimation. We demonstrate the applicability of our approach in three real-world case studies, highlighting its relevance for practical settings.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#21452;&#26354;&#31354;&#38388;&#36827;&#34892;&#29305;&#24449;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#22270;&#20687;&#20043;&#38388;&#30340;&#36317;&#31163;&#20173;&#28982;&#34920;&#31034;&#20854;&#30456;&#20284;&#24230;&#65292;&#32780;&#20301;&#32622;&#21017;&#34920;&#31034;&#20854;&#20856;&#22411;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01421</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#29305;&#24449;&#23398;&#20064;&#19982;&#26032;&#20852;&#25968;&#25454;&#39537;&#21160;&#30340;&#20856;&#22411;&#24615;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Feature Learning with Emergent Data-Driven Prototypicality. (arXiv:2307.01421v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01421
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22312;&#26080;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#21033;&#29992;&#21452;&#26354;&#31354;&#38388;&#36827;&#34892;&#29305;&#24449;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#22270;&#20687;&#20043;&#38388;&#30340;&#36317;&#31163;&#20173;&#28982;&#34920;&#31034;&#20854;&#30456;&#20284;&#24230;&#65292;&#32780;&#20301;&#32622;&#21017;&#34920;&#31034;&#20854;&#20856;&#22411;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27809;&#26377;&#20219;&#20309;&#26631;&#31614;&#30340;&#22270;&#20687;&#38598;&#20013;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#35757;&#32451;&#19968;&#20010;&#27169;&#22411;&#65292;&#23558;&#27599;&#20010;&#22270;&#20687;&#26144;&#23556;&#21040;&#19968;&#20010;&#29305;&#24449;&#31354;&#38388;&#20013;&#30340;&#19968;&#20010;&#28857;&#65292;&#20351;&#24471;&#19981;&#20165;&#25509;&#36817;&#24615;&#34920;&#31034;&#35270;&#35273;&#30456;&#20284;&#24230;&#65292;&#32780;&#19988;&#20301;&#32622;&#30452;&#25509;&#32534;&#30721;&#20102;&#22270;&#20687;&#22312;&#25968;&#25454;&#38598;&#20013;&#30340;&#20856;&#22411;&#24615;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#35265;&#35299;&#26159;&#22312;&#21452;&#26354;&#31354;&#38388;&#32780;&#19981;&#26159;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#36827;&#34892;&#26080;&#30417;&#30563;&#29305;&#24449;&#23398;&#20064;&#65292;&#20854;&#20013;&#28857;&#20043;&#38388;&#30340;&#36317;&#31163;&#20173;&#28982;&#21453;&#26144;&#22270;&#20687;&#30340;&#30456;&#20284;&#24230;&#65292;&#20294;&#25105;&#20204;&#22312;&#20301;&#32622;&#19978;&#22686;&#21152;&#20102;&#34920;&#31034;&#20856;&#22411;&#24615;&#30340;&#33021;&#21147;&#65306;&#23427;&#31163;&#21407;&#28857;&#36234;&#36817;&#65292;&#23601;&#36234;&#20856;&#22411;&#12290;&#21518;&#19968;&#23646;&#24615;&#20165;&#20165;&#26159;&#20174;&#20248;&#21270;&#36890;&#24120;&#30340;&#24230;&#37327;&#23398;&#20064;&#30446;&#26631;&#20013;&#24471;&#21040;&#30340;&#65306;&#19982;&#35768;&#22810;&#35757;&#32451;&#23454;&#20363;&#30456;&#20284;&#30340;&#22270;&#20687;&#26368;&#22909;&#25918;&#32622;&#22312;&#27431;&#20960;&#37324;&#24503;&#31354;&#38388;&#20013;&#30456;&#24212;&#28857;&#30340;&#20013;&#24515;&#20301;&#32622;&#65292;&#20294;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#26356;&#38752;&#36817;&#21407;&#28857;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21452;&#26354;&#31354;&#38388;&#20013;&#21033;&#29992;&#29699;&#20307;&#23494;&#38598;&#32534;&#30721;&#65288;sphere pACKing&#65289;&#30340;&#26080;&#30417;&#30563;&#29305;&#24449;&#23398;&#20064;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Given an image set without any labels, our goal is to train a model that maps each image to a point in a feature space such that, not only proximity indicates visual similarity, but where it is located directly encodes how prototypical the image is according to the dataset.  Our key insight is to perform unsupervised feature learning in hyperbolic instead of Euclidean space, where the distance between points still reflect image similarity, and yet we gain additional capacity for representing prototypicality with the location of the point: The closer it is to the origin, the more prototypical it is. The latter property is simply emergent from optimizing the usual metric learning objective: The image similar to many training instances is best placed at the center of corresponding points in Euclidean space, but closer to the origin in hyperbolic space.  We propose an unsupervised feature learning algorithm in Hyperbolic space with sphere pACKing. HACK first generates uniformly packed part
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#26368;&#22823;&#21270;&#21453;&#26144;&#21457;&#36865;&#21644;&#25509;&#25910;&#28040;&#24687;&#20851;&#31995;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#12290;</title><link>http://arxiv.org/abs/2307.01403</link><description>&lt;p&gt;
&#23398;&#20064;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;
&lt;/p&gt;
&lt;p&gt;
Learning to Communicate using Contrastive Learning. (arXiv:2307.01403v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01403
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#36827;&#34892;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#36890;&#36807;&#26368;&#22823;&#21270;&#21453;&#26144;&#21457;&#36865;&#21644;&#25509;&#25910;&#28040;&#24687;&#20851;&#31995;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#33021;&#22815;&#25429;&#33719;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#65292;&#23454;&#29616;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#20449;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#21327;&#35843;&#30340;&#26377;&#21147;&#24037;&#20855;&#12290;&#20294;&#22312;&#20998;&#25955;&#30340;&#29615;&#22659;&#20013;&#35825;&#23548;&#19968;&#20010;&#26377;&#25928;&#30340;&#20849;&#21516;&#35821;&#35328;&#26159;&#19968;&#20010;&#22256;&#38590;&#30340;&#25361;&#25112;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#26367;&#20195;&#35270;&#35282;&#65292;&#21363;&#23558;&#26234;&#33021;&#20307;&#20043;&#38388;&#21457;&#36865;&#30340;&#36890;&#20449;&#28040;&#24687;&#35270;&#20026;&#29615;&#22659;&#29366;&#24577;&#30340;&#19981;&#23436;&#25972;&#35270;&#22270;&#12290;&#36890;&#36807;&#26816;&#26597;&#21457;&#36865;&#21644;&#25509;&#25910;&#30340;&#28040;&#24687;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#23545;&#27604;&#23398;&#20064;&#26469;&#26368;&#22823;&#21270;&#32473;&#23450;&#36712;&#36857;&#30340;&#28040;&#24687;&#20043;&#38388;&#30340;&#20114;&#20449;&#24687;&#26469;&#23398;&#20064;&#36890;&#20449;&#12290;&#22312;&#36890;&#20449;&#20851;&#38190;&#30340;&#29615;&#22659;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#24615;&#33021;&#21644;&#23398;&#20064;&#36895;&#24230;&#26041;&#38754;&#20248;&#20110;&#20808;&#21069;&#30340;&#24037;&#20316;&#12290;&#20351;&#29992;&#23450;&#24615;&#25351;&#26631;&#21644;&#34920;&#31034;&#25506;&#27979;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#35825;&#23548;&#20102;&#26356;&#23545;&#31216;&#30340;&#36890;&#20449;&#24182;&#20174;&#29615;&#22659;&#20013;&#25429;&#33719;&#20102;&#20840;&#23616;&#29366;&#24577;&#20449;&#24687;&#12290;&#24635;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#23545;&#27604;&#23398;&#20064;&#30340;&#21147;&#37327;&#20197;&#21450;&#21033;&#29992;&#28040;&#24687;&#20316;&#20026;&#32534;&#30721;&#23454;&#29616;&#26377;&#25928;&#36890;&#20449;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Communication is a powerful tool for coordination in multi-agent RL. But inducing an effective, common language is a difficult challenge, particularly in the decentralized setting. In this work, we introduce an alternative perspective where communicative messages sent between agents are considered as different incomplete views of the environment state. By examining the relationship between messages sent and received, we propose to learn to communicate using contrastive learning to maximize the mutual information between messages of a given trajectory. In communication-essential environments, our method outperforms previous work in both performance and learning speed. Using qualitative metrics and representation probing, we show that our method induces more symmetric communication and captures global state information from the environment. Overall, we show the power of contrastive learning and the importance of leveraging messages as encodings for effective communication.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#20174;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#38754;&#21521;&#39640;&#24615;&#33021;&#25968;&#25454;&#26694;&#30340;&#24182;&#34892;&#22788;&#29702;&#27169;&#24335;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#20018;&#34892;&#25968;&#25454;&#26694;&#22312;&#22788;&#29702;&#20013;&#31561;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#26102;&#23384;&#22312;&#24615;&#33021;&#38480;&#21046;&#65292;&#25552;&#20986;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;</title><link>http://arxiv.org/abs/2307.01394</link><description>&lt;p&gt;
&#38754;&#21521;&#39640;&#24615;&#33021;&#25968;&#25454;&#26694;&#30340;&#24182;&#34892;&#22788;&#29702;&#27169;&#24335;&#30340;&#28145;&#20837;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
In-depth Analysis On Parallel Processing Patterns for High-Performance Dataframes. (arXiv:2307.01394v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01394
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#20174;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#23545;&#38754;&#21521;&#39640;&#24615;&#33021;&#25968;&#25454;&#26694;&#30340;&#24182;&#34892;&#22788;&#29702;&#27169;&#24335;&#36827;&#34892;&#20102;&#28145;&#20837;&#20998;&#26512;&#65292;&#24182;&#25351;&#20986;&#24403;&#21069;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#20018;&#34892;&#25968;&#25454;&#26694;&#22312;&#22788;&#29702;&#20013;&#31561;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#26102;&#23384;&#22312;&#24615;&#33021;&#38480;&#21046;&#65292;&#25552;&#20986;&#25913;&#36827;&#30340;&#31354;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#30001;&#20110;&#22823;&#25968;&#25454;&#38761;&#21629;&#65292;&#25968;&#25454;&#31185;&#23398;&#39046;&#22495;&#22312;&#30740;&#31350;&#21644;&#24037;&#19994;&#30028;&#37117;&#24471;&#21040;&#20102;&#24040;&#22823;&#30340;&#25193;&#23637;&#12290;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#32473;&#25968;&#25454;&#24037;&#31243;&#24212;&#29992;&#24102;&#26469;&#20102;&#26356;&#22810;&#30340;&#22797;&#26434;&#24615;&#65292;&#36825;&#20123;&#24212;&#29992;&#29616;&#22312;&#34987;&#38598;&#25104;&#21040;&#25968;&#25454;&#22788;&#29702;&#31649;&#36947;&#20013;&#20197;&#22788;&#29702;&#22823;&#37327;&#30340;&#25968;&#25454;&#12290;&#36890;&#24120;&#65292;&#22312;&#36825;&#20123;&#31649;&#36947;&#20013;&#33457;&#36153;&#22823;&#37327;&#26102;&#38388;&#36827;&#34892;&#25968;&#25454;&#39044;&#22788;&#29702;&#65292;&#22240;&#27492;&#25552;&#39640;&#20854;&#25928;&#29575;&#30452;&#25509;&#24433;&#21709;&#25972;&#20307;&#31649;&#36947;&#30340;&#24615;&#33021;&#12290;&#36817;&#24180;&#26469;&#65292;&#31038;&#21306;&#24050;&#32463;&#25509;&#21463;&#20102;&#25968;&#25454;&#26694;&#20316;&#20026;&#20107;&#23454;&#19978;&#30340;&#25968;&#25454;&#34920;&#31034;&#21644;&#25805;&#20316;&#30340;&#25968;&#25454;&#32467;&#26500;&#30340;&#27010;&#24565;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#26368;&#24191;&#27867;&#20351;&#29992;&#30340;&#20018;&#34892;&#25968;&#25454;&#26694;&#65288;R&#12289;pandas&#65289;&#22312;&#22788;&#29702;&#20013;&#31561;&#35268;&#27169;&#30340;&#25968;&#25454;&#38598;&#26102;&#23384;&#22312;&#24615;&#33021;&#38480;&#21046;&#12290;&#25105;&#20204;&#30456;&#20449;&#20174;&#39640;&#24615;&#33021;&#35745;&#31639;&#30340;&#35282;&#24230;&#20986;&#21457;&#65292;&#36824;&#26377;&#24456;&#22823;&#30340;&#25552;&#21319;&#31354;&#38388;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Data Science domain has expanded monumentally in both research and industry communities during the past decade, predominantly owing to the Big Data revolution. Artificial Intelligence (AI) and Machine Learning (ML) are bringing more complexities to data engineering applications, which are now integrated into data processing pipelines to process terabytes of data. Typically, a significant amount of time is spent on data preprocessing in these pipelines, and hence improving its e fficiency directly impacts the overall pipeline performance. The community has recently embraced the concept of Dataframes as the de-facto data structure for data representation and manipulation. However, the most widely used serial Dataframes today (R, pandas) experience performance limitations while working on even moderately large data sets. We believe that there is plenty of room for improvement by taking a look at this problem from a high-performance computing point of view. In a prior publication, we p
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#35270;&#39057;&#25968;&#25454;&#39044;&#27979;&#20102;&#22902;&#29275;&#30340;&#38271;&#26399;&#20307;&#37325;&#65292;&#24182;&#27604;&#36739;&#20102;&#38408;&#20540;&#21644;Mask R-CNN&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#35777;&#23454;&#20102;&#22522;&#20110;&#35270;&#39057;&#30340;&#20307;&#37325;&#39044;&#27979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20419;&#36827;&#20102;&#21160;&#29289;&#31185;&#23398;&#30028;&#30340;&#24320;&#25918;&#31185;&#23398;&#12290;</title><link>http://arxiv.org/abs/2307.01383</link><description>&lt;p&gt;
&#21033;&#29992;&#38408;&#20540;&#21644;Mask R-CNN&#31639;&#27861;&#30340;&#28145;&#24230;&#35270;&#39057;&#25968;&#25454;&#39044;&#27979;&#22902;&#29275;&#38271;&#26399;&#20307;&#37325;
&lt;/p&gt;
&lt;p&gt;
Depth video data-enabled predictions of longitudinal dairy cow body weight using thresholding and Mask R-CNN algorithms. (arXiv:2307.01383v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01383
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#21033;&#29992;&#28145;&#24230;&#35270;&#39057;&#25968;&#25454;&#39044;&#27979;&#20102;&#22902;&#29275;&#30340;&#38271;&#26399;&#20307;&#37325;&#65292;&#24182;&#27604;&#36739;&#20102;&#38408;&#20540;&#21644;Mask R-CNN&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#35777;&#23454;&#20102;&#22522;&#20110;&#35270;&#39057;&#30340;&#20307;&#37325;&#39044;&#27979;&#30340;&#26377;&#25928;&#24615;&#65292;&#24182;&#20419;&#36827;&#20102;&#21160;&#29289;&#31185;&#23398;&#30028;&#30340;&#24320;&#25918;&#31185;&#23398;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#27979;&#22902;&#29275;&#20307;&#37325;&#23545;&#20110;&#25903;&#25345;&#20892;&#22330;&#31649;&#29702;&#20915;&#31574;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#23427;&#19982;&#22902;&#29275;&#30340;&#29983;&#38271;&#12289;&#33829;&#20859;&#29366;&#20917;&#21644;&#20581;&#24247;&#30452;&#25509;&#30456;&#20851;&#12290;&#28982;&#32780;&#65292;&#32477;&#22823;&#22810;&#25968;&#20808;&#21069;&#30340;&#20307;&#37325;&#39044;&#27979;&#30740;&#31350;&#21482;&#20351;&#29992;&#20102;&#22312;&#21333;&#19968;&#26102;&#38388;&#28857;&#37319;&#38598;&#30340;&#25968;&#25454;&#12290;&#27492;&#22806;&#65292;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#35270;&#39057;&#20998;&#21106;&#22312;&#20307;&#37325;&#39044;&#27979;&#26041;&#38754;&#30340;&#25928;&#29992;&#23578;&#26410;&#24471;&#21040;&#35299;&#31572;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#20174;&#21453;&#22797;&#27979;&#37327;&#30340;&#35270;&#39057;&#25968;&#25454;&#20013;&#39044;&#27979;&#22902;&#29275;&#20307;&#37325;&#65292;&#27604;&#36739;&#38408;&#20540;&#21644;Mask R-CNN&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#35780;&#20272;&#20307;&#37325;&#22238;&#24402;&#27169;&#22411;&#30340;&#39044;&#27979;&#33021;&#21147;&#65292;&#24182;&#36890;&#36807;&#20844;&#24320;&#21457;&#24067;&#22522;&#20110;&#35270;&#39057;&#30340;&#20307;&#37325;&#39044;&#27979;&#30340;&#28304;&#20195;&#30721;&#65292;&#22312;&#21160;&#29289;&#31185;&#23398;&#30028;&#25512;&#24191;&#24320;&#25918;&#31185;&#23398;&#12290;&#20849;&#33719;&#24471;10&#22836;&#27852;&#20083;&#33655;&#26031;&#22374;&#29275;&#21644;2&#22836;&#38750;&#27852;&#20083;&#27901;&#35199;&#29275;&#30340;40,405&#20010;&#28145;&#24230;&#22270;&#20687;&#21644;&#28145;&#24230;&#22270;&#25991;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monitoring cow body weight is crucial to support farm management decisions due to its direct relationship with the growth, nutritional status, and health of dairy cows. Cow body weight is a repeated trait, however, the majority of previous body weight prediction research only used data collected at a single point in time. Furthermore, the utility of deep learning-based segmentation for body weight prediction using videos remains unanswered. Therefore, the objectives of this study were to predict cow body weight from repeatedly measured video data, to compare the performance of the thresholding and Mask R-CNN deep learning approaches, to evaluate the predictive ability of body weight regression models, and to promote open science in the animal science community by releasing the source code for video-based body weight prediction. A total of 40,405 depth images and depth map files were obtained from 10 lactating Holstein cows and 2 non-lactating Jersey cows. Three approaches were investig
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#37325;&#35201;&#30340;&#20196;&#29260;&#21644;&#21547;&#26377;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#34987;&#21516;&#31561;&#25110;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01379</link><description>&lt;p&gt;
&#23558;&#20851;&#27880;&#28857;&#36716;&#31227;&#21040;&#30456;&#20851;&#24615;&#19978;: &#25506;&#32034;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01379
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#33258;&#21160;&#29983;&#25104;&#30340;&#20851;&#38190;&#35789;&#19981;&#24179;&#31561;&#38382;&#39064;&#65292;&#21457;&#29616;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#65292;&#37325;&#35201;&#30340;&#20196;&#29260;&#21644;&#21547;&#26377;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#34987;&#21516;&#31561;&#25110;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#22312;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#28508;&#21147;&#65292;&#20294;&#26159;&#23545;&#20110;&#27169;&#22411;&#29983;&#25104;&#30340;&#19981;&#30830;&#23450;&#24615;&#30340;&#29305;&#24449;&#21270;&#20173;&#28982;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#21363;&#29992;&#25143;&#20309;&#26102;&#21487;&#20197;&#20449;&#20219;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#22522;&#20110;&#19968;&#20123;&#21551;&#21457;&#24615;&#30340;&#20107;&#23454;&#65292;&#21363;&#22312;&#33258;&#22238;&#24402;&#30340;LLMs&#20013;&#65292;&#20196;&#29260;&#22312;&#21453;&#26144;&#29983;&#25104;&#30340;&#21547;&#20041;&#26041;&#38754;&#26159;&#19981;&#24179;&#31561;&#30340;&#65292;&#21363;&#19968;&#20123;&#20196;&#29260;&#27604;&#20854;&#20182;&#20196;&#29260;&#26356;&#30456;&#20851;&#65288;&#25110;&#26356;&#20855;&#20195;&#34920;&#24615;&#65289;&#65292;&#28982;&#32780;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#25152;&#26377;&#30340;&#20196;&#29260;&#34987;&#31561;&#20540;&#23545;&#24453;&#12290;&#36825;&#26159;&#30001;&#20110;&#35821;&#35328;&#20887;&#20313;&#65292;&#20854;&#20013;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#65292;&#21482;&#38656;&#35201;&#20960;&#20010;&#20851;&#38190;&#35789;&#23601;&#36275;&#20197;&#20256;&#36798;&#19968;&#20010;&#38271;&#21477;&#30340;&#21547;&#20041;&#12290;&#25105;&#20204;&#23558;&#36825;&#20123;&#19981;&#24179;&#31561;&#31216;&#20026;&#29983;&#25104;&#30340;&#19981;&#24179;&#31561;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#22914;&#20309;&#24433;&#21709;&#19981;&#30830;&#23450;&#24615;&#30340;&#20272;&#35745;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#25581;&#31034;&#65292;&#30456;&#24403;&#25968;&#37327;&#30340;&#20196;&#29260;&#21644;&#21253;&#21547;&#26377;&#38480;&#35821;&#20041;&#30340;&#21477;&#23376;&#65292;&#22312;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#26102;&#34987;&#21516;&#31561;&#25110;&#29978;&#33267;&#26356;&#21152;&#37325;&#35270;&#12290;&#20026;&#20102;&#35299;&#20915;&#30001;&#29983;&#25104;&#30340;&#19981;&#24179;&#31561;&#24341;&#36215;&#30340;&#36825;&#20123;&#20559;&#24046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20849;&#21516;&#36716;&#31227;&#20851;&#27880;&#28857;&#26469;&#26356;&#22909;&#22320;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although Large Language Models (LLMs) have shown great potential in Natural Language Generation, it is still challenging to characterize the uncertainty of model generations, i.e., when users could trust model outputs. Our research is derived from the heuristic facts that tokens are created unequally in reflecting the meaning of generations by auto-regressive LLMs, i.e., some tokens are more relevant (or representative) than others, yet all the tokens are equally valued when estimating uncertainty. It is because of the linguistic redundancy where mostly a few keywords are sufficient to convey the meaning of a long sentence. We name these inequalities as generative inequalities and investigate how they affect uncertainty estimation. Our results reveal that considerable tokens and sentences containing limited semantics are weighted equally or even heavily when estimating uncertainty. To tackle these biases posed by generative inequalities, we propose to jointly Shifting Attention to more
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sentinel-1 SAR&#21644;Sentinel-2 MSI&#26102;&#38388;&#24207;&#21015;&#30340;CNN&#22238;&#24402;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#24314;&#31569;&#29289;&#30340;&#39640;&#24230;&#65292;&#24182;&#22312;&#33655;&#20848;&#30340;10&#20010;&#22478;&#24066;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;</title><link>http://arxiv.org/abs/2307.01378</link><description>&lt;p&gt;
&#20351;&#29992;Sentinel-1 SAR&#21644;Sentinel-2 MSI&#26102;&#38388;&#24207;&#21015;&#30340;CNN&#22238;&#24402;&#27169;&#22411;&#20272;&#35745;&#24314;&#31569;&#29289;&#39640;&#24230;&#22270;
&lt;/p&gt;
&lt;p&gt;
A CNN regression model to estimate buildings height maps using Sentinel-1 SAR and Sentinel-2 MSI time series. (arXiv:2307.01378v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01378
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;Sentinel-1 SAR&#21644;Sentinel-2 MSI&#26102;&#38388;&#24207;&#21015;&#30340;CNN&#22238;&#24402;&#27169;&#22411;&#65292;&#33021;&#22815;&#20934;&#30830;&#20272;&#35745;&#24314;&#31569;&#29289;&#30340;&#39640;&#24230;&#65292;&#24182;&#22312;&#33655;&#20848;&#30340;10&#20010;&#22478;&#24066;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#20272;&#35745;&#24314;&#31569;&#29289;&#39640;&#24230;&#23545;&#20110;&#22478;&#24066;&#35268;&#21010;&#12289;&#22522;&#30784;&#35774;&#26045;&#31649;&#29702;&#21644;&#29615;&#22659;&#20998;&#26512;&#33267;&#20851;&#37325;&#35201;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30417;&#30563;&#24335;&#22810;&#27169;&#24577;&#24314;&#31569;&#29289;&#39640;&#24230;&#22238;&#24402;&#32593;&#32476;&#65288;MBHR-Net&#65289;&#65292;&#29992;&#20110;&#20351;&#29992;Sentinel-1&#65288;S1&#65289;&#21644;Sentinel-2&#65288;S2&#65289;&#21355;&#26143;&#26102;&#38388;&#24207;&#21015;&#20272;&#35745;10m&#31354;&#38388;&#20998;&#36776;&#29575;&#30340;&#24314;&#31569;&#29289;&#39640;&#24230;&#12290;S1&#25552;&#20379;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#65288;SAR&#65289;&#25968;&#25454;&#65292;&#23545;&#24314;&#31569;&#32467;&#26500;&#25552;&#20379;&#20102;&#26377;&#20215;&#20540;&#30340;&#20449;&#24687;&#65292;&#32780;S2&#25552;&#20379;&#20102;&#23545;&#19981;&#21516;&#22303;&#22320;&#35206;&#30422;&#31867;&#22411;&#12289;&#26893;&#34987;&#23395;&#33410;&#24615;&#21644;&#24314;&#31569;&#38452;&#24433;&#25935;&#24863;&#30340;&#22810;&#20809;&#35889;&#25968;&#25454;&#12290;&#25105;&#20204;&#30340;MBHR-Net&#26088;&#22312;&#20174;S1&#21644;S2&#22270;&#20687;&#20013;&#25552;&#21462;&#26377;&#24847;&#20041;&#30340;&#29305;&#24449;&#65292;&#23398;&#20064;&#22270;&#20687;&#27169;&#24335;&#19982;&#24314;&#31569;&#29289;&#39640;&#24230;&#20043;&#38388;&#30340;&#22797;&#26434;&#26102;&#31354;&#20851;&#31995;&#12290;&#35813;&#27169;&#22411;&#22312;&#33655;&#20848;&#30340;10&#20010;&#22478;&#24066;&#36827;&#34892;&#35757;&#32451;&#21644;&#27979;&#35797;&#12290;&#37319;&#29992;&#22343;&#26041;&#26681;&#35823;&#24046;&#65288;RMSE&#65289;&#12289;&#20132;&#24182;&#27604;&#65288;IOU&#65289;&#21644;R&#24179;&#26041;&#65288;R2&#65289;&#20998;&#25968;&#25351;&#26631;&#35780;&#20272;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65288;3.73m RMSE, 0.&#65289;
&lt;/p&gt;
&lt;p&gt;
Accurate estimation of building heights is essential for urban planning, infrastructure management, and environmental analysis. In this study, we propose a supervised Multimodal Building Height Regression Network (MBHR-Net) for estimating building heights at 10m spatial resolution using Sentinel-1 (S1) and Sentinel-2 (S2) satellite time series. S1 provides Synthetic Aperture Radar (SAR) data that offers valuable information on building structures, while S2 provides multispectral data that is sensitive to different land cover types, vegetation phenology, and building shadows. Our MBHR-Net aims to extract meaningful features from the S1 and S2 images to learn complex spatio-temporal relationships between image patterns and building heights. The model is trained and tested in 10 cities in the Netherlands. Root Mean Squared Error (RMSE), Intersection over Union (IOU), and R-squared (R2) score metrics are used to evaluate the performance of the model. The preliminary results (3.73m RMSE, 0.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30830;&#23450;&#24863;&#30693;&#31995;&#32479;&#23433;&#20840;&#35201;&#27714;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#35270;&#35273;&#39134;&#34892;&#22120;&#36991;&#30896;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.01371</link><description>&lt;p&gt;
&#24863;&#30693;&#31995;&#32479;&#30340;&#23433;&#20840;&#35201;&#27714;&#30340;&#39640;&#25928;&#30830;&#23450;
&lt;/p&gt;
&lt;p&gt;
Efficient Determination of Safety Requirements for Perception Systems. (arXiv:2307.01371v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30830;&#23450;&#24863;&#30693;&#31995;&#32479;&#23433;&#20840;&#35201;&#27714;&#30340;&#26041;&#27861;&#65292;&#24182;&#19988;&#22312;&#35270;&#35273;&#39134;&#34892;&#22120;&#36991;&#30896;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#31934;&#24230;&#21644;&#25928;&#29575;&#26041;&#38754;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24863;&#30693;&#31995;&#32479;&#20316;&#20026;&#25972;&#20307;&#33258;&#20027;&#26632;&#30340;&#19968;&#20010;&#23376;&#32452;&#20214;&#36816;&#34892;&#65292;&#24863;&#30693;&#31995;&#32479;&#35774;&#35745;&#32773;&#24120;&#24120;&#38656;&#35201;&#22312;&#32500;&#25252;&#25972;&#20307;&#38381;&#29615;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#30340;&#21516;&#26102;&#20248;&#21270;&#24615;&#33021;&#29305;&#24615;&#12290;&#22240;&#27492;&#65292;&#23558;&#39640;&#23618;&#23433;&#20840;&#35201;&#27714;&#21270;&#31616;&#20026;&#24863;&#30693;&#31995;&#32479;&#30340;&#32452;&#20214;&#32423;&#35201;&#27714;&#26159;&#38750;&#24120;&#26377;&#29992;&#30340;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#19987;&#27880;&#20110;&#22312;&#32473;&#23450;&#23436;&#20840;&#38598;&#25104;&#30340;&#38381;&#29615;&#31995;&#32479;&#30340;&#40657;&#30418;&#27169;&#25311;&#22120;&#30340;&#24773;&#20917;&#19979;&#65292;&#39640;&#25928;&#22320;&#30830;&#23450;&#19968;&#32452;&#23433;&#20840;&#30340;&#24863;&#30693;&#31995;&#32479;&#24615;&#33021;&#29305;&#24615;&#12290;&#25105;&#20204;&#32467;&#21512;&#20102;&#24120;&#35265;&#30340;&#40657;&#30418;&#20272;&#35745;&#25216;&#26415;&#65288;&#22914;&#39640;&#26031;&#36807;&#31243;&#21644;&#38408;&#20540;&#36125;&#21494;&#26031;&#65289;&#30340;&#20248;&#28857;&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#30340;&#20272;&#35745;&#26041;&#27861;&#65292;&#31216;&#20026;&#24179;&#28369;&#36125;&#21494;&#26031;&#12290;&#25105;&#20204;&#22312;&#22522;&#20110;&#35270;&#35273;&#30340;&#39134;&#34892;&#22120;&#36991;&#30896;&#38382;&#39064;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#23637;&#31034;&#20102;&#19982;&#39640;&#26031;&#36807;&#31243;&#21644;&#38408;&#20540;&#36125;&#21494;&#26031;&#22522;&#32447;&#30456;&#27604;&#65292;&#31934;&#24230;&#21644;&#25928;&#29575;&#19978;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Perception systems operate as a subcomponent of the general autonomy stack, and perception system designers often need to optimize performance characteristics while maintaining safety with respect to the overall closed-loop system. For this reason, it is useful to distill high-level safety requirements into component-level requirements on the perception system. In this work, we focus on efficiently determining sets of safe perception system performance characteristics given a black-box simulator of the fully-integrated, closed-loop system. We combine the advantages of common black-box estimation techniques such as Gaussian processes and threshold bandits to develop a new estimation method, which we call smoothing bandits. We demonstrate our method on a vision-based aircraft collision avoidance problem and show improvements in terms of both accuracy and efficiency over the Gaussian process and threshold bandit baselines.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;&#32034;&#24341;&#26694;&#26550;&#21450;&#31574;&#30053;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#22810;&#29992;&#25143;&#30340;&#20449;&#24687;&#26102;&#24310;&#65292;&#24182;&#21033;&#29992;&#20449;&#24687;&#26102;&#24310;(AoI)&#25351;&#26631;&#35780;&#20272;&#20449;&#24687;&#26032;&#40092;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.01366</link><description>&lt;p&gt;
&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#26368;&#23567;&#21270;&#20449;&#24687;&#26102;&#24310;&#65306;&#19968;&#31181;&#23884;&#22871;&#32034;&#24341;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Minimizing Age of Information for Mobile Edge Computing Systems: A Nested Index Approach. (arXiv:2307.01366v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01366
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23884;&#22871;&#32034;&#24341;&#26694;&#26550;&#21450;&#31574;&#30053;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;&#31995;&#32479;&#20013;&#22810;&#29992;&#25143;&#30340;&#20449;&#24687;&#26102;&#24310;&#65292;&#24182;&#21033;&#29992;&#20449;&#24687;&#26102;&#24310;(AoI)&#25351;&#26631;&#35780;&#20272;&#20449;&#24687;&#26032;&#40092;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31227;&#21160;&#35774;&#22791;&#21644;&#36793;&#32536;&#33410;&#28857;&#30340;&#35745;&#31639;&#24322;&#26500;&#24615;&#65292;&#31227;&#21160;&#36793;&#32536;&#35745;&#31639;(MEC)&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#23454;&#29616;&#23545;&#20449;&#24687;&#26032;&#40092;&#24230;&#25935;&#24863;&#30340;&#23454;&#26102;&#24212;&#29992;&#65292;&#36890;&#36807;&#23558;&#20219;&#21153;&#20174;&#31227;&#21160;&#35774;&#22791;&#36716;&#31227;&#21040;&#36793;&#32536;&#33410;&#28857;&#12290;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#26102;&#24310;(AoI)&#25351;&#26631;&#26469;&#35780;&#20272;&#20449;&#24687;&#26032;&#40092;&#24230;&#12290;&#30001;&#20110;&#38543;&#26426;&#30340;&#35745;&#31639;&#26102;&#38388;&#65292;&#35201;&#25214;&#21040;&#26368;&#23567;&#21270;MEC&#31995;&#32479;&#20013;&#22810;&#20010;&#29992;&#25143;&#30340;AoI&#30340;&#26377;&#25928;&#35299;&#20915;&#26041;&#26696;&#24182;&#19981;&#23481;&#26131;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#32771;&#34385;&#22810;&#20010;&#29992;&#25143;&#23558;&#20219;&#21153;&#20174;&#24322;&#26500;&#30340;&#36793;&#32536;&#26381;&#21153;&#22120;&#36716;&#31227;&#21040;MEC&#31995;&#32479;&#20013;&#12290;&#25105;&#20204;&#39318;&#20808;&#23558;&#38382;&#39064;&#37325;&#26032;&#23450;&#20041;&#20026;&#19968;&#20010;&#19981;&#23433;&#20840;&#22810;&#33218;&#36172;&#21338;&#26426;(RMAB)&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#19968;&#20010;&#23618;&#27425;&#21270;&#30340;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#26469;&#25551;&#36848;MEC&#31995;&#32479;&#30340;AoI&#30340;&#26356;&#26032;&#12290;&#22522;&#20110;&#23618;&#27425;&#21270;MDP&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23884;&#22871;&#32034;&#24341;&#26694;&#26550;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;&#21487;&#35777;&#26126;&#28176;&#36827;&#20248;&#21270;&#24615;&#33021;&#30340;&#23884;&#22871;&#32034;&#24341;&#31574;&#30053;&#12290;&#26368;&#21518;&#65292;&#24471;&#21040;&#20102;&#23884;&#22871;&#32034;&#24341;&#30340;&#38381;&#21512;&#24418;&#24335;&#65292;&#20174;&#32780;&#20351;&#24471;
&lt;/p&gt;
&lt;p&gt;
Exploiting the computational heterogeneity of mobile devices and edge nodes, mobile edge computation (MEC) provides an efficient approach to achieving real-time applications that are sensitive to information freshness, by offloading tasks from mobile devices to edge nodes. We use the metric Age-of-Information (AoI) to evaluate information freshness. An efficient solution to minimize the AoI for the MEC system with multiple users is non-trivial to obtain due to the random computing time. In this paper, we consider multiple users offloading tasks to heterogeneous edge servers in a MEC system. We first reformulate the problem as a Restless Multi-Arm-Bandit (RMAB) problem and establish a hierarchical Markov Decision Process (MDP) to characterize the updating of AoI for the MEC system. Based on the hierarchical MDP, we propose a nested index framework and design a nested index policy with provably asymptotic optimality. Finally, the closed form of the nested index is obtained, which enables
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01316</link><description>&lt;p&gt;
&#29992;&#31070;&#32463;&#31526;&#21495;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#23454;&#29616;&#23433;&#20840;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Safe Autonomous Driving Policies using a Neuro-Symbolic Deep Reinforcement Learning Approach. (arXiv:2307.01316v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01316
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DRL with Symbolic Logics (DRLSL)&#30340;&#26032;&#39062;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#26088;&#22312;&#23454;&#29616;&#22312;&#30495;&#23454;&#29615;&#22659;&#20013;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#21644;&#31526;&#21495;&#36923;&#36753;&#39537;&#21160;&#30340;&#25512;&#29702;&#65292;&#20801;&#35768;&#36890;&#36807;&#19982;&#29289;&#29702;&#29615;&#22659;&#30340;&#23454;&#26102;&#20132;&#20114;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#31574;&#30053;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#39550;&#39542;&#20013;&#30340;&#21160;&#24577;&#39550;&#39542;&#29615;&#22659;&#21644;&#22810;&#26679;&#21270;&#36947;&#36335;&#20351;&#29992;&#32773;&#30340;&#23384;&#22312;&#32473;&#20915;&#31574;&#36896;&#25104;&#20102;&#24040;&#22823;&#30340;&#25361;&#25112;&#12290;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;(DRL)&#24050;&#25104;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#30340;&#19968;&#31181;&#27969;&#34892;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23433;&#20840;&#38382;&#39064;&#30340;&#38480;&#21046;&#65292;&#29616;&#26377;&#30340;DRL&#35299;&#20915;&#26041;&#26696;&#30340;&#24212;&#29992;&#20027;&#35201;&#23616;&#38480;&#20110;&#27169;&#25311;&#29615;&#22659;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#37096;&#32626;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#19968;&#23616;&#38480;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#31070;&#32463;&#31526;&#21495;&#26080;&#27169;&#22411;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#31216;&#20026;&#24102;&#26377;&#31526;&#21495;&#36923;&#36753;&#30340;DRL(DRLSL)&#65292;&#23427;&#23558;DRL(&#20174;&#32463;&#39564;&#20013;&#23398;&#20064;)&#21644;&#31526;&#21495;&#19968;&#38454;&#36923;&#36753;&#30693;&#35782;&#39537;&#21160;&#30340;&#25512;&#29702;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#22312;&#23454;&#38469;&#29615;&#22659;&#19979;&#23433;&#20840;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#30340;&#23454;&#26102;&#20132;&#20114;&#12290;&#36825;&#31181;&#21019;&#26032;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#19968;&#31181;&#36890;&#36807;&#31215;&#26497;&#19982;&#29289;&#29702;&#29615;&#22659;&#20114;&#21160;&#26469;&#23398;&#20064;&#33258;&#20027;&#39550;&#39542;&#25919;&#31574;&#24182;&#30830;&#20445;&#23433;&#20840;&#24615;&#30340;&#26041;&#24335;&#12290;&#25105;&#20204;&#20351;&#29992;&#39640;&#32500;&#24230;&#25968;&#25454;&#23454;&#29616;&#20102;&#33258;&#20027;&#39550;&#39542;&#30340;DRLSL&#26694;&#26550;&#12290;
&lt;/p&gt;
&lt;p&gt;
The dynamic nature of driving environments and the presence of diverse road users pose significant challenges for decision-making in autonomous driving. Deep reinforcement learning (DRL) has emerged as a popular approach to tackle this problem. However, the application of existing DRL solutions is mainly confined to simulated environments due to safety concerns, impeding their deployment in real-world. To overcome this limitation, this paper introduces a novel neuro-symbolic model-free DRL approach, called DRL with Symbolic Logics (DRLSL) that combines the strengths of DRL (learning from experience) and symbolic first-order logics knowledge-driven reasoning) to enable safe learning in real-time interactions of autonomous driving within real environments. This innovative approach provides a means to learn autonomous driving policies by actively engaging with the physical environment while ensuring safety. We have implemented the DRLSL framework in autonomous driving using the highD data
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#31070;&#32463;&#32467;&#26500;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#23039;&#24577;&#21644;&#39640;&#24230;&#25511;&#21046;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#35843;&#25972;PID&#22686;&#30410;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01312</link><description>&lt;p&gt;
&#36890;&#36807;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#31070;&#32463;&#32467;&#26500;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#65292;&#23454;&#29616;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#25511;&#21046;
&lt;/p&gt;
&lt;p&gt;
Self-Tuning PID Control via a Hybrid Actor-Critic-Based Neural Structure for Quadcopter Control. (arXiv:2307.01312v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01312
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28151;&#21512;Actor-Critic&#31070;&#32463;&#32467;&#26500;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#65292;&#29992;&#20110;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#30340;&#23039;&#24577;&#21644;&#39640;&#24230;&#25511;&#21046;&#65292;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#30340;&#26041;&#27861;&#35843;&#25972;PID&#22686;&#30410;&#65292;&#25552;&#39640;&#20102;&#31995;&#32479;&#30340;&#31283;&#20581;&#24615;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27604;&#20363;&#31215;&#20998;&#24494;&#20998;&#65288;PID&#65289;&#25511;&#21046;&#22120;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#24037;&#19994;&#21644;&#23454;&#39564;&#36807;&#31243;&#20013;&#65292;&#29616;&#26377;&#30340;&#31163;&#32447;&#26041;&#27861;&#21487;&#20197;&#29992;&#20110;&#35843;&#25972;PID&#22686;&#30410;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#27169;&#22411;&#21442;&#25968;&#30340;&#19981;&#30830;&#23450;&#24615;&#21644;&#22806;&#37096;&#24178;&#25200;&#30340;&#23384;&#22312;&#65292;&#23454;&#38469;&#31995;&#32479;&#65288;&#22914;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#65289;&#38656;&#35201;&#26356;&#31283;&#20581;&#21487;&#38752;&#30340;PID&#25511;&#21046;&#22120;&#12290;&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#20351;&#29992;&#24378;&#21270;&#23398;&#20064;&#30340;&#31070;&#32463;&#32593;&#32476;&#26469;&#23454;&#29616;&#22235;&#26059;&#32764;&#39134;&#34892;&#22120;&#23039;&#24577;&#21644;&#39640;&#24230;&#25511;&#21046;&#30340;&#33258;&#25972;&#23450;PID&#25511;&#21046;&#22120;&#12290;&#37319;&#29992;&#20102;&#22686;&#37327;&#24335;PID&#25511;&#21046;&#22120;&#65292;&#24182;&#20165;&#23545;&#21487;&#21464;&#22686;&#30410;&#36827;&#34892;&#20102;&#35843;&#25972;&#12290;&#20026;&#20102;&#35843;&#25972;&#21160;&#24577;&#22686;&#30410;&#65292;&#20351;&#29992;&#20102;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#30340;&#26080;&#27169;&#22411;Actor-Critic&#28151;&#21512;&#31070;&#32463;&#32467;&#26500;&#65292;&#33021;&#22815;&#36866;&#24403;&#35843;&#25972;PID&#22686;&#30410;&#65292;&#21516;&#26102;&#20805;&#24403;&#26368;&#20339;&#35782;&#21035;&#22120;&#12290;&#22312;&#35843;&#25972;&#21644;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;&#20351;&#29992;&#20102;&#19968;&#20010;&#20855;&#26377;&#20004;&#20010;&#38544;&#34255;&#23618;&#21644;Sigmoid&#28608;&#27963;&#20989;&#25968;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#21033;&#29992;&#33258;&#36866;&#24212;&#21160;&#37327;&#65288;ADAM&#65289;&#20248;&#21270;&#22120;&#21644;&#21453;&#21521;&#20256;&#25773;&#31639;&#27861;&#36827;&#34892;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proportional-Integrator-Derivative (PID) controller is used in a wide range of industrial and experimental processes. There are a couple of offline methods for tuning PID gains. However, due to the uncertainty of model parameters and external disturbances, real systems such as Quadrotors need more robust and reliable PID controllers. In this research, a self-tuning PID controller using a Reinforcement-Learning-based Neural Network for attitude and altitude control of a Quadrotor has been investigated. An Incremental PID, which contains static and dynamic gains, has been considered and only the variable gains have been tuned. To tune dynamic gains, a model-free actor-critic-based hybrid neural structure was used that was able to properly tune PID gains, and also has done the best as an identifier. In both tunning and identification tasks, a Neural Network with two hidden layers and sigmoid activation functions has been learned using Adaptive Momentum (ADAM) optimizer and Back-Propagatio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#38656;&#35201;&#37327;&#23376;&#35745;&#31639;&#65292;&#21457;&#29616;&#25968;&#23383;&#30828;&#20214;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01301</link><description>&lt;p&gt;
&#21487;&#38752;&#30340;&#20154;&#24037;&#26234;&#33021;&#65306;&#19979;&#19968;&#20195;&#26159;&#21542;&#38656;&#35201;&#37327;&#23376;&#35745;&#31639;&#65311;
&lt;/p&gt;
&lt;p&gt;
Reliable AI: Does the Next Generation Require Quantum Computing?. (arXiv:2307.01301v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01301
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#35843;&#26597;&#30740;&#31350;&#20102;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#38656;&#35201;&#37327;&#23376;&#35745;&#31639;&#65292;&#21457;&#29616;&#25968;&#23383;&#30828;&#20214;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#21487;&#38752;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36825;&#39033;&#35843;&#26597;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#35752;&#19968;&#20010;&#22522;&#26412;&#38382;&#39064;&#65292;&#21363;&#19979;&#19968;&#20195;&#20154;&#24037;&#26234;&#33021;&#26159;&#21542;&#38656;&#35201;&#37327;&#23376;&#35745;&#31639;&#12290;&#20154;&#24037;&#26234;&#33021;&#22312;&#25105;&#20204;&#26085;&#24120;&#29983;&#27963;&#30340;&#35768;&#22810;&#26041;&#38754;&#37117;&#36215;&#30528;&#37325;&#35201;&#20316;&#29992;&#65292;&#24182;&#19988;&#26159;&#31532;&#22235;&#27425;&#24037;&#19994;&#38761;&#21629;&#30340;&#26680;&#24515;&#12290;&#22240;&#27492;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#38752;&#24615;&#21644;&#20540;&#24471;&#20449;&#36182;&#24615;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#28982;&#32780;&#65292;&#20154;&#24037;&#26234;&#33021;&#30340;&#21487;&#38752;&#24615;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#20363;&#22914;&#38544;&#31169;&#12289;&#36131;&#20219;&#12289;&#23433;&#20840;&#31561;&#65292;&#22312;&#33258;&#21160;&#39550;&#39542;&#12289;&#21307;&#30103;&#20445;&#20581;&#12289;&#26426;&#22120;&#20154;&#31561;&#39046;&#22495;&#37117;&#23384;&#22312;&#30528;&#12290;&#36825;&#20123;&#38382;&#39064;&#21487;&#33021;&#26377;&#21508;&#31181;&#21407;&#22240;&#65292;&#21253;&#25324;&#25968;&#25454;&#19981;&#36275;&#12289;&#20559;&#35265;&#12289;&#40065;&#26834;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#22522;&#26412;&#30340;&#35745;&#31639;&#38382;&#39064;&#65292;&#22914;&#25968;&#23383;&#30828;&#20214;&#19978;&#30340;&#21487;&#35745;&#31639;&#24615;&#38382;&#39064;&#12290;&#36825;&#20123;&#21487;&#35745;&#31639;&#24615;&#38382;&#39064;&#30340;&#26681;&#28304;&#22312;&#20110;&#25968;&#23383;&#30828;&#20214;&#22522;&#20110;&#22270;&#28789;&#26426;&#30340;&#35745;&#31639;&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26412;&#36136;&#19978;&#26159;&#31163;&#25955;&#30340;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#25968;&#23383;&#30828;&#20214;&#26080;&#27861;&#23436;&#20840;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this survey, we aim to explore the fundamental question of whether the next generation of artificial intelligence requires quantum computing. Artificial intelligence is increasingly playing a crucial role in many aspects of our daily lives and is central to the fourth industrial revolution. It is therefore imperative that artificial intelligence is reliable and trustworthy. However, there are still many issues with reliability of artificial intelligence, such as privacy, responsibility, safety, and security, in areas such as autonomous driving, healthcare, robotics, and others. These problems can have various causes, including insufficient data, biases, and robustness problems, as well as fundamental issues such as computability problems on digital hardware. The cause of these computability problems is rooted in the fact that digital hardware is based on the computing model of the Turing machine, which is inherently discrete. Notably, our findings demonstrate that digital hardware i
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#26381;&#21153;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26597;&#35810;&#39640;&#25928;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#20351;&#24471;&#25915;&#20987;&#32773;&#33021;&#22815;&#19968;&#33268;&#22320;&#35302;&#21457;&#20219;&#20309;&#24819;&#35201;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23545;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01292</link><description>&lt;p&gt;
Pareto-&#23433;&#20840;&#30340;&#26426;&#22120;&#23398;&#20064;&#65288;PSML&#65289;&#65306;&#25351;&#32441;&#21644;&#20445;&#25252;&#25512;&#26029;&#26381;&#21153;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pareto-Secure Machine Learning (PSML): Fingerprinting and Securing Inference Serving Systems. (arXiv:2307.01292v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01292
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#26381;&#21153;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#36890;&#36807;&#24341;&#20837;&#19968;&#20010;&#26597;&#35810;&#39640;&#25928;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#20351;&#24471;&#25915;&#20987;&#32773;&#33021;&#22815;&#19968;&#33268;&#22320;&#35302;&#21457;&#20219;&#20309;&#24819;&#35201;&#30340;&#27169;&#22411;&#65292;&#20174;&#32780;&#22686;&#24378;&#20102;&#23545;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#22823;&#22411;&#22522;&#30784;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#27169;&#22411;&#26381;&#21153;&#31995;&#32479;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#22312;&#36825;&#26679;&#30340;&#31995;&#32479;&#20013;&#65292;&#29992;&#25143;&#23558;&#26597;&#35810;&#21457;&#36865;&#21040;&#26381;&#21153;&#22120;&#65292;&#24182;&#25351;&#23450;&#25152;&#38656;&#30340;&#24615;&#33021;&#25351;&#26631;&#65288;&#20363;&#22914;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#31561;&#65289;&#12290;&#26381;&#21153;&#22120;&#22312;&#21518;&#31471;&#32500;&#25252;&#19968;&#32452;&#27169;&#22411;&#65288;&#27169;&#22411;&#24211;&#65289;&#65292;&#24182;&#26681;&#25454;&#25351;&#23450;&#30340;&#25351;&#26631;&#25552;&#20379;&#26597;&#35810;&#26381;&#21153;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#65292;&#29305;&#21035;&#26159;&#23545;&#27169;&#22411;&#25552;&#21462;&#25915;&#20987;&#30340;&#40065;&#26834;&#24615;&#12290;&#29616;&#26377;&#30340;&#40657;&#30418;&#25915;&#20987;&#19981;&#33021;&#30452;&#25509;&#24212;&#29992;&#20110;&#25552;&#21462;&#21463;&#23475;&#27169;&#22411;&#65292;&#22240;&#20026;&#27169;&#22411;&#38544;&#34255;&#22312;&#25512;&#29702;&#26381;&#21153;&#25509;&#21475;&#32972;&#21518;&#30340;&#27169;&#22411;&#24211;&#20013;&#65292;&#25915;&#20987;&#32773;&#26080;&#27861;&#30830;&#23450;&#20351;&#29992;&#30340;&#26159;&#21738;&#20010;&#27169;&#22411;&#12290;&#38656;&#35201;&#19968;&#20010;&#20013;&#38388;&#27493;&#39588;&#26469;&#30830;&#20445;&#27599;&#20010;&#36755;&#20837;&#26597;&#35810;&#37117;&#33021;&#24471;&#21040;&#21463;&#23475;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26597;&#35810;&#39640;&#25928;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#20351;&#25915;&#20987;&#32773;&#33021;&#22815;&#19968;&#33268;&#22320;&#35302;&#21457;&#20219;&#20309;&#24819;&#35201;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#36890;&#36807;&#20351;&#29992;&#25105;&#20204;&#30340;&#25351;&#32441;&#31639;&#27861;&#65292;&#27169;&#22411;&#25552;&#21462;&#21487;&#20197;&#20855;&#26377;&#20445;&#30495;&#24230;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the emergence of large foundational models, model-serving systems are becoming popular. In such a system, users send the queries to the server and specify the desired performance metrics (e.g., accuracy, latency, etc.). The server maintains a set of models (model zoo) in the back-end and serves the queries based on the specified metrics. This paper examines the security, specifically robustness against model extraction attacks, of such systems. Existing black-box attacks cannot be directly applied to extract a victim model, as models hide among the model zoo behind the inference serving interface, and attackers cannot identify which model is being used. An intermediate step is required to ensure that every input query gets the output from the victim model. To this end, we propose a query-efficient fingerprinting algorithm to enable the attacker to trigger any desired model consistently. We show that by using our fingerprinting algorithm, model extraction can have fidelity and accu
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#20845;&#31181;&#20849;&#35782;&#20989;&#25968;&#29992;&#20110;&#35299;&#37322;&#20116;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#30528;&#20998;&#27495;&#38382;&#39064;&#65292;&#23545;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#23578;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2307.01288</link><description>&lt;p&gt;
&#29992;&#20849;&#35782;&#26041;&#24335;&#35299;&#20915;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#20013;&#30340;&#20998;&#27495;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Fighting the disagreement in Explainable Machine Learning with consensus. (arXiv:2307.01288v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01288
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35780;&#20272;&#20102;&#20845;&#31181;&#20849;&#35782;&#20989;&#25968;&#29992;&#20110;&#35299;&#37322;&#20116;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#21457;&#29616;&#20102;&#22312;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#23384;&#22312;&#30528;&#20998;&#27495;&#38382;&#39064;&#65292;&#23545;&#20110;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#23578;&#38656;&#36827;&#19968;&#27493;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20215;&#20540;&#36890;&#24120;&#36890;&#36807;&#20854;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#26469;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#22312;&#26576;&#20123;&#31185;&#23398;&#39046;&#22495;&#20013;&#65292;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#26041;&#24335;&#19982;&#20854;&#20934;&#30830;&#24615;&#21516;&#31561;&#37325;&#35201;&#12290;&#20026;&#20102;&#29702;&#35299;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20869;&#37096;&#24037;&#20316;&#21407;&#29702;&#65292;&#35299;&#37322;&#24615;&#31639;&#27861;&#26159;&#39318;&#36873;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#26377;&#22810;&#31181;&#31639;&#27861;&#21487;&#20379;&#36873;&#25321;&#65292;&#23427;&#20204;&#22312;&#35299;&#37322;&#27169;&#22411;&#26041;&#38754;&#32463;&#24120;&#23384;&#22312;&#20998;&#27495;&#65292;&#23548;&#33268;&#30456;&#20114;&#30683;&#30462;&#30340;&#35299;&#37322;&#32467;&#26524;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#22312;&#27169;&#22411;&#34987;&#35299;&#37322;&#20043;&#21518;&#21487;&#20197;&#24212;&#29992;&#20849;&#35782;&#20989;&#25968;&#12290;&#28982;&#32780;&#65292;&#38382;&#39064;&#24182;&#27809;&#26377;&#23436;&#20840;&#35299;&#20915;&#65292;&#22240;&#20026;&#26368;&#32456;&#32467;&#26524;&#23558;&#21462;&#20915;&#20110;&#36873;&#25321;&#30340;&#20849;&#35782;&#20989;&#25968;&#21644;&#20854;&#20182;&#22240;&#32032;&#12290;&#26412;&#25991;&#35780;&#20272;&#20102;&#20845;&#31181;&#20849;&#35782;&#20989;&#25968;&#29992;&#20110;&#35299;&#37322;&#20116;&#20010;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20808;&#21069;&#22312;&#22235;&#20010;&#24050;&#30693;&#20869;&#37096;&#35268;&#21017;&#30340;&#21512;&#25104;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35757;&#32451;&#12290;&#28982;&#21518;&#65292;&#20351;&#29992;&#19982;&#27169;&#22411;&#26080;&#20851;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#21487;&#35299;&#37322;&#24615;&#31639;&#27861;&#23545;&#27169;&#22411;&#36827;&#34892;&#20102;&#35299;&#37322;&#12290;&#26368;&#21518;&#65292;&#36827;&#34892;&#20849;&#35782;&#22788;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) models are often valued by the accuracy of their predictions. However, in some areas of science, the inner workings of models are as relevant as their accuracy. To understand how ML models work internally, the use of interpretability algorithms is the preferred option. Unfortunately, despite the diversity of algorithms available, they often disagree in explaining a model, leading to contradictory explanations. To cope with this issue, consensus functions can be applied once the models have been explained. Nevertheless, the problem is not completely solved because the final result will depend on the selected consensus function and other factors. In this paper, six consensus functions have been evaluated for the explanation of five ML models. The models were previously trained on four synthetic datasets whose internal rules were known in advance. The models were then explained with model-agnostic local and global interpretability algorithms. Finally, consensus was c
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#36798;&#30456;&#20284;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#26816;&#32034;&#24037;&#20855;MWPRanker&#65292;&#35813;&#24037;&#20855;&#33021;&#22815;&#26816;&#32034;&#20855;&#26377;&#30456;&#21516;&#38382;&#39064;&#27169;&#22411;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#21253;&#25324;&#31639;&#26415;&#21644;&#36923;&#36753;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2307.01240</link><description>&lt;p&gt;
MWPRanker: &#19968;&#31181;&#22522;&#20110;&#34920;&#36798;&#30456;&#20284;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#26816;&#32034;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
MWPRanker: An Expression Similarity Based Math Word Problem Retriever. (arXiv:2307.01240v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01240
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#34920;&#36798;&#30456;&#20284;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#26816;&#32034;&#24037;&#20855;MWPRanker&#65292;&#35813;&#24037;&#20855;&#33021;&#22815;&#26816;&#32034;&#20855;&#26377;&#30456;&#21516;&#38382;&#39064;&#27169;&#22411;&#30340;&#25968;&#23398;&#38382;&#39064;&#65292;&#21253;&#25324;&#31639;&#26415;&#21644;&#36923;&#36753;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22312;&#32447;&#35780;&#20272;&#20013;&#65292;&#25968;&#23398;&#38382;&#39064;&#65288;MWPs&#65289;&#24110;&#21161;&#27979;&#35797;&#23398;&#20064;&#32773;&#36890;&#36807;&#35299;&#37322;&#20854;&#20013;&#30340;&#35821;&#35328;&#20449;&#24687;&#26469;&#36827;&#34892;&#20851;&#38190;&#25512;&#29702;&#33021;&#21147;&#30340;&#27979;&#35797;&#12290;&#20026;&#20102;&#27979;&#35797;&#23398;&#20064;&#32773;&#30340;&#25968;&#23398;&#25512;&#29702;&#33021;&#21147;&#65292;&#26377;&#26102;&#20250;&#37325;&#26032;&#34920;&#36848;&#38382;&#39064;&#25110;&#26356;&#25913;&#21407;&#22987;MWP&#30340;&#20027;&#39064;&#35774;&#32622;&#12290;&#30001;&#20110;&#25163;&#21160;&#35782;&#21035;&#20855;&#26377;&#30456;&#20284;&#38382;&#39064;&#27169;&#22411;&#30340;MWPs&#24456;&#40635;&#28902;&#65292;&#22240;&#27492;&#25105;&#20204;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#25552;&#20986;&#20102;&#19968;&#20010;&#20026;MWP&#26816;&#32034;&#25552;&#20379;&#24037;&#20855;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#26469;&#26816;&#32034;&#20855;&#26377;&#30456;&#21516;&#38382;&#39064;&#27169;&#22411;&#30340;&#31867;&#20284;MWPs&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#38382;&#39064;&#27169;&#22411;&#26159;&#25351;&#35201;&#25191;&#34892;&#30340;&#25805;&#20316;&#24207;&#21015;&#20197;&#36798;&#21040;&#35299;&#20915;&#26041;&#26696;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#24037;&#20855;&#23545;&#25152;&#36848;&#20219;&#21153;&#38750;&#24120;&#26377;&#29992;&#65292;&#24182;&#20248;&#20110;&#22522;&#20110;&#35821;&#20041;&#30456;&#20284;&#24230;&#30340;&#26041;&#27861;&#65292;&#21518;&#32773;&#26080;&#27861;&#25429;&#25417;MWPs&#30340;&#31639;&#26415;&#21644;&#36923;&#36753;&#39034;&#24207;&#12290;&#21487;&#20197;&#22312;https://www.youtube.com/watch?v=gSQWP3chFIs&#25214;&#21040;&#35813;&#24037;&#20855;&#30340;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Math Word Problems (MWPs) in online assessments help test the ability of the learner to make critical inferences by interpreting the linguistic information in them. To test the mathematical reasoning capabilities of the learners, sometimes the problem is rephrased or the thematic setting of the original MWP is changed. Since manual identification of MWPs with similar problem models is cumbersome, we propose a tool in this work for MWP retrieval. We propose a hybrid approach to retrieve similar MWPs with the same problem model. In our work, the problem model refers to the sequence of operations to be performed to arrive at the solution. We demonstrate that our tool is useful for the mentioned tasks and better than semantic similarity-based approaches, which fail to capture the arithmetic and logical sequence of the MWPs. A demo of the tool can be found at https://www.youtube.com/watch?v=gSQWP3chFIs
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39184;&#21518;&#34880;&#31958;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#35821;&#27861;&#28436;&#21270;&#23398;&#20064;&#24046;&#20998;&#26041;&#31243;&#65292;&#24182;&#32467;&#21512;&#32858;&#31867;&#20998;&#26512;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#24378;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2307.01238</link><description>&lt;p&gt;
&#29992;&#32467;&#26500;&#21270;&#35821;&#27861;&#28436;&#21270;&#23398;&#20064;&#24046;&#20998;&#26041;&#31243;&#29992;&#20110;&#39184;&#21518;&#34880;&#31958;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Learning Difference Equations with Structured Grammatical Evolution for Postprandial Glycaemia Prediction. (arXiv:2307.01238v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#39184;&#21518;&#34880;&#31958;&#39044;&#27979;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#32467;&#26500;&#21270;&#35821;&#27861;&#28436;&#21270;&#23398;&#20064;&#24046;&#20998;&#26041;&#31243;&#65292;&#24182;&#32467;&#21512;&#32858;&#31867;&#20998;&#26512;&#25552;&#20379;&#21487;&#35299;&#37322;&#24615;&#24378;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31958;&#23615;&#30149;&#24739;&#32773;&#24517;&#39035;&#23494;&#20999;&#30417;&#27979;&#20182;&#20204;&#30340;&#34880;&#31958;&#27700;&#24179;&#65292;&#23588;&#20854;&#26159;&#36827;&#39184;&#21518;&#12290;&#34880;&#31958;&#35843;&#33410;&#38656;&#35201;&#27491;&#30830;&#32452;&#21512;&#30340;&#39135;&#29289;&#25668;&#20837;&#21644;&#33008;&#23707;&#32032;&#27880;&#23556;&#12290;&#34880;&#31958;&#39044;&#27979;&#23545;&#20110;&#36991;&#20813;&#27835;&#30103;&#31958;&#23615;&#30149;&#24739;&#32773;&#39184;&#21518;&#24182;&#21457;&#30151;&#38750;&#24120;&#37325;&#35201;&#12290;&#34429;&#28982;&#20256;&#32479;&#30340;&#26041;&#27861;&#65292;&#22914;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65292;&#26174;&#31034;&#20986;&#24456;&#39640;&#30340;&#20934;&#30830;&#29575;&#65292;&#20294;&#30001;&#20110;&#20854;&#32570;&#20047;&#21487;&#35299;&#37322;&#24615;&#65292;&#26377;&#26102;&#19981;&#36866;&#29992;&#20110;&#21307;&#29983;&#24320;&#21457;&#20010;&#24615;&#21270;&#27835;&#30103;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#24378;&#35843;&#21487;&#35299;&#37322;&#24615;&#30340;&#26032;&#22411;&#34880;&#31958;&#39044;&#27979;&#26041;&#27861;&#65306;&#21487;&#35299;&#37322;&#31232;&#30095;&#35782;&#21035;&#36890;&#36807;&#35821;&#27861;&#28436;&#21270;&#12290;&#32467;&#21512;&#20808;&#21069;&#30340;&#32858;&#31867;&#38454;&#27573;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25552;&#20379;&#20102;&#26377;&#38480;&#24046;&#20998;&#26041;&#31243;&#26469;&#39044;&#27979;&#39184;&#21518;&#20004;&#23567;&#26102;&#20869;&#30340;&#34880;&#31958;&#27700;&#24179;&#12290;&#25105;&#20204;&#23558;&#25968;&#25454;&#38598;&#20998;&#20026;&#22235;&#23567;&#26102;&#30340;&#27573;&#65292;&#26681;&#25454;&#36827;&#39184;&#21069;&#20004;&#23567;&#26102;&#30340;&#34880;&#31958;&#20540;&#36827;&#34892;&#32858;&#31867;&#12290;&#39044;&#27979;&#27169;&#22411;&#34987;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
People with diabetes must carefully monitor their blood glucose levels, especially after eating. Blood glucose regulation requires a proper combination of food intake and insulin boluses. Glucose prediction is vital to avoid dangerous post-meal complications in treating individuals with diabetes. Although traditional methods, such as artificial neural networks, have shown high accuracy rates, sometimes they are not suitable for developing personalised treatments by physicians due to their lack of interpretability. In this study, we propose a novel glucose prediction method emphasising interpretability: Interpretable Sparse Identification by Grammatical Evolution. Combined with a previous clustering stage, our approach provides finite difference equations to predict postprandial glucose levels up to two hours after meals. We divide the dataset into four-hour segments and perform clustering based on blood glucose values for the twohour window before the meal. Prediction models are traine
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#23454;&#29616;&#20102;&#29289;&#32852;&#32593;&#25925;&#38556;&#26816;&#27979;&#21644;&#20998;&#31867;&#31995;&#32479;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;&#29305;&#24322;&#24615;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20540;&#31561;&#26041;&#38754;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2307.01234</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#23454;&#29616;&#29289;&#32852;&#32593;&#25925;&#38556;&#26816;&#27979;&#21644;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Internet of Things Fault Detection and Classification via Multitask Learning. (arXiv:2307.01234v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01234
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#22810;&#20219;&#21153;&#23398;&#20064;&#23454;&#29616;&#20102;&#29289;&#32852;&#32593;&#25925;&#38556;&#26816;&#27979;&#21644;&#20998;&#31867;&#31995;&#32479;&#65292;&#24182;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#22312;&#29305;&#24322;&#24615;&#12289;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20540;&#31561;&#26041;&#38754;&#22343;&#26377;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#23454;&#38469;&#24037;&#19994;&#29289;&#32852;&#32593;&#24212;&#29992;&#24320;&#21457;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;&#25925;&#38556;&#26816;&#27979;&#21644;&#20998;&#31867;&#31995;&#32479;&#36827;&#34892;&#20102;&#28145;&#20837;&#30740;&#31350;&#12290;&#30740;&#31350;&#35299;&#20915;&#20102;&#25968;&#25454;&#25910;&#38598;&#12289;&#26631;&#27880;&#12289;&#31639;&#27861;&#24320;&#21457;&#21644;&#37096;&#32626;&#26041;&#38754;&#30340;&#25361;&#25112;&#12290;&#36890;&#36807;&#20351;&#29992;&#19968;&#20010;&#30495;&#23454;&#30340;&#24037;&#19994;&#29289;&#32852;&#32593;&#31995;&#32479;&#65292;&#25105;&#20204;&#23545;11&#20010;&#39044;&#23450;&#20041;&#30340;&#25925;&#38556;&#31867;&#21035;&#36827;&#34892;&#20102;&#19977;&#20010;&#38454;&#27573;&#30340;&#25968;&#25454;&#25910;&#38598;&#27169;&#25311;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;SMTCNN&#29992;&#20110;&#24037;&#19994;&#29289;&#32852;&#32593;&#20013;&#30340;&#25925;&#38556;&#26816;&#27979;&#21644;&#20998;&#31867;&#65292;&#35780;&#20272;&#20854;&#22312;&#30495;&#23454;&#25968;&#25454;&#19978;&#30340;&#24615;&#33021;&#12290;&#19982;&#29616;&#26377;&#25216;&#26415;&#30456;&#27604;&#65292;SMTCNN&#22312;&#29305;&#24322;&#24615;&#65288;3.5%&#65289;&#26041;&#38754;&#21462;&#24471;&#20102;&#21331;&#36234;&#30340;&#25928;&#26524;&#65292;&#24182;&#19988;&#22312;&#31934;&#30830;&#24230;&#12289;&#21484;&#22238;&#29575;&#21644;F1&#20540;&#26041;&#38754;&#26174;&#31034;&#20986;&#26174;&#33879;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a comprehensive investigation into developing a fault detection and classification system for real-world IIoT applications. The study addresses challenges in data collection, annotation, algorithm development, and deployment. Using a real-world IIoT system, three phases of data collection simulate 11 predefined fault categories. We propose SMTCNN for fault detection and category classification in IIoT, evaluating its performance on real-world data. SMTCNN achieves superior specificity (3.5%) and shows significant improvements in precision, recall, and F1 measures compared to existing techniques.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;(&#28145;&#24230;)&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#20854;&#20013;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#37117;&#23646;&#20110;&#30456;&#23545;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2307.01231</link><description>&lt;p&gt;
&#23545;(&#28145;&#24230;)&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#20851;&#38190;&#37325;&#26032;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A Critical Re-evaluation of Benchmark Datasets for (Deep) Learning-Based Matching Algorithms. (arXiv:2307.01231v1 [cs.DB])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01231
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37325;&#26032;&#35780;&#20272;&#20102;(&#28145;&#24230;)&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#21457;&#29616;&#20854;&#20013;&#22823;&#22810;&#25968;&#25968;&#25454;&#38598;&#37117;&#23646;&#20110;&#30456;&#23545;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20307;&#35299;&#26512;(ER)&#26159;&#35782;&#21035;&#22312;&#19968;&#20010;&#25110;&#22810;&#20010;&#25968;&#25454;&#24211;&#20013;&#25351;&#21521;&#30456;&#21516;&#23454;&#20307;&#30340;&#35760;&#24405;&#30340;&#36807;&#31243;&#12290;&#22810;&#24180;&#26469;&#65292;&#24050;&#32463;&#24320;&#21457;&#20102;&#35768;&#22810;&#25216;&#26415;&#26469;&#35299;&#20915;ER&#25361;&#25112;&#65292;&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#21644;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#22312;&#21305;&#37197;&#38454;&#27573;&#21463;&#21040;&#20102;&#37325;&#35270;&#12290;&#28982;&#32780;&#65292;&#22312;&#25991;&#29486;&#20013;&#23578;&#26410;&#23545;&#23454;&#39564;&#35780;&#20272;&#20013;&#24120;&#29992;&#30340;&#23398;&#20064;&#21305;&#37197;&#31639;&#27861;&#30340;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#36136;&#37327;&#36827;&#34892;&#26816;&#26597;&#12290;&#20026;&#20102;&#24357;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22235;&#31181;&#19981;&#21516;&#30340;&#26041;&#27861;&#26469;&#35780;&#20272;13&#20010;&#24050;&#24314;&#31435;&#25968;&#25454;&#38598;&#30340;&#38590;&#24230;&#21644;&#36866;&#29992;&#24615;&#65306;&#20004;&#31181;&#29702;&#35770;&#26041;&#27861;&#65292;&#28041;&#21450;&#26032;&#30340;&#32447;&#24615;&#24230;&#37327;&#21644;&#29616;&#26377;&#30340;&#22797;&#26434;&#24230;&#24230;&#37327;&#65292;&#20197;&#21450;&#20004;&#31181;&#23454;&#38469;&#26041;&#27861;&#65306;&#26368;&#20339;&#38750;&#32447;&#24615;&#21644;&#32447;&#24615;&#21305;&#37197;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#65292;&#20197;&#21450;&#26368;&#20339;&#23398;&#20064;&#21305;&#37197;&#22120;&#21644;&#23436;&#32654;&#39044;&#27979;&#22120;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#65292;&#22823;&#22810;&#25968;&#27969;&#34892;&#25968;&#25454;&#38598;&#37117;&#25552;&#20986;&#20102;&#30456;&#24403;&#31616;&#21333;&#30340;&#20998;&#31867;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Entity resolution (ER) is the process of identifying records that refer to the same entities within one or across multiple databases. Numerous techniques have been developed to tackle ER challenges over the years, with recent emphasis placed on machine and deep learning methods for the matching phase. However, the quality of the benchmark datasets typically used in the experimental evaluations of learning-based matching algorithms has not been examined in the literature. To cover this gap, we propose four different approaches to assessing the difficulty and appropriateness of 13 established datasets: two theoretical approaches, which involve new measures of linearity and existing measures of complexity, and two practical approaches: the difference between the best non-linear and linear matchers, as well as the difference between the best learning-based matcher and the perfect oracle. Our analysis demonstrates that most of the popular datasets pose rather easy classification tasks. As a
&lt;/p&gt;</description></item><item><title>EmoGen&#26159;&#19968;&#31181;&#28040;&#38500;&#24773;&#24863;&#38899;&#20048;&#29983;&#25104;&#20013;&#20027;&#35266;&#20559;&#24046;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#38899;&#20048;&#23646;&#24615;&#20316;&#20026;&#26725;&#26753;&#65292;&#23558;&#29983;&#25104;&#20998;&#20026;&#24773;&#24863;&#21040;&#23646;&#24615;&#30340;&#26144;&#23556;&#20197;&#21450;&#23646;&#24615;&#21040;&#38899;&#20048;&#30340;&#29983;&#25104;&#20004;&#20010;&#38454;&#27573;&#65292;&#24182;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#28040;&#38500;&#20027;&#35266;&#20559;&#24046;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#26222;&#36941;&#24773;&#24863;&#30340;&#38899;&#20048;&#12290;</title><link>http://arxiv.org/abs/2307.01229</link><description>&lt;p&gt;
EmoGen: &#28040;&#38500;&#24773;&#24863;&#38899;&#20048;&#29983;&#25104;&#20013;&#30340;&#20027;&#35266;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
EmoGen: Eliminating Subjective Bias in Emotional Music Generation. (arXiv:2307.01229v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01229
&lt;/p&gt;
&lt;p&gt;
EmoGen&#26159;&#19968;&#31181;&#28040;&#38500;&#24773;&#24863;&#38899;&#20048;&#29983;&#25104;&#20013;&#20027;&#35266;&#20559;&#24046;&#30340;&#31995;&#32479;&#65292;&#36890;&#36807;&#21033;&#29992;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#38899;&#20048;&#23646;&#24615;&#20316;&#20026;&#26725;&#26753;&#65292;&#23558;&#29983;&#25104;&#20998;&#20026;&#24773;&#24863;&#21040;&#23646;&#24615;&#30340;&#26144;&#23556;&#20197;&#21450;&#23646;&#24615;&#21040;&#38899;&#20048;&#30340;&#29983;&#25104;&#20004;&#20010;&#38454;&#27573;&#65292;&#24182;&#22312;&#23398;&#20064;&#36807;&#31243;&#20013;&#28040;&#38500;&#20027;&#35266;&#20559;&#24046;&#65292;&#23454;&#29616;&#29983;&#25104;&#20855;&#26377;&#26222;&#36941;&#24773;&#24863;&#30340;&#38899;&#20048;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#29992;&#20110;&#20256;&#36798;&#24773;&#24863;&#65292;&#22240;&#27492;&#22312;&#33258;&#21160;&#29983;&#25104;&#38899;&#20048;&#26102;&#29983;&#25104;&#24773;&#24863;&#38899;&#20048;&#38750;&#24120;&#37325;&#35201;&#12290;&#20043;&#21069;&#20851;&#20110;&#24773;&#24863;&#38899;&#20048;&#29983;&#25104;&#30340;&#24037;&#20316;&#30452;&#25509;&#20351;&#29992;&#26631;&#27880;&#30340;&#24773;&#24863;&#26631;&#31614;&#20316;&#20026;&#25511;&#21046;&#20449;&#21495;&#65292;&#20294;&#23384;&#22312;&#20027;&#35266;&#20559;&#24046;&#65306;&#19981;&#21516;&#30340;&#20154;&#21487;&#33021;&#20250;&#22312;&#21516;&#26679;&#30340;&#38899;&#20048;&#19978;&#26631;&#27880;&#19981;&#21516;&#30340;&#24773;&#24863;&#65292;&#21516;&#19968;&#20010;&#20154;&#22312;&#19981;&#21516;&#24773;&#22659;&#19979;&#20063;&#21487;&#33021;&#24863;&#21463;&#21040;&#19981;&#21516;&#30340;&#24773;&#24863;&#12290;&#22240;&#27492;&#65292;&#30452;&#25509;&#23558;&#24773;&#24863;&#26631;&#31614;&#26144;&#23556;&#21040;&#38899;&#20048;&#24207;&#21015;&#20013;&#20250;&#28151;&#28102;&#23398;&#20064;&#36807;&#31243;&#65292;&#24182;&#38459;&#30861;&#27169;&#22411;&#29983;&#25104;&#20855;&#26377;&#26222;&#36941;&#24773;&#24863;&#30340;&#38899;&#20048;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;EmoGen&#65292;&#19968;&#31181;&#24773;&#24863;&#38899;&#20048;&#29983;&#25104;&#31995;&#32479;&#65292;&#23427;&#21033;&#29992;&#19968;&#32452;&#19982;&#24773;&#24863;&#30456;&#20851;&#30340;&#38899;&#20048;&#23646;&#24615;&#20316;&#20026;&#24773;&#24863;&#21644;&#38899;&#20048;&#20043;&#38388;&#30340;&#26725;&#26753;&#65292;&#24182;&#23558;&#29983;&#25104;&#20998;&#20026;&#20004;&#20010;&#38454;&#27573;&#65306;&#22522;&#20110;&#30417;&#30563;&#32858;&#31867;&#30340;&#24773;&#24863;&#21040;&#23646;&#24615;&#26144;&#23556;&#20197;&#21450;&#22522;&#20110;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#23646;&#24615;&#21040;&#38899;&#20048;&#29983;&#25104;&#12290;&#36825;&#20004;&#20010;&#38454;&#27573;&#37117;&#26159;&#26377;&#30410;&#30340;&#65306;&#22312;&#31532;&#19968;&#20010;&#38454;&#27573;&#65292;&#32858;&#31867;&#21608;&#22260;&#30340;&#23646;&#24615;&#20540;&#26377;&#21161;&#20110;&#28040;&#38500;&#20027;&#35266;&#20559;&#24046;&#65292;&#31532;&#20108;&#20010;&#38454;&#27573;&#21017;&#23454;&#29616;&#20102;&#38899;&#20048;&#30340;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music is used to convey emotions, and thus generating emotional music is important in automatic music generation. Previous work on emotional music generation directly uses annotated emotion labels as control signals, which suffers from subjective bias: different people may annotate different emotions on the same music, and one person may feel different emotions under different situations. Therefore, directly mapping emotion labels to music sequences in an end-to-end way would confuse the learning process and hinder the model from generating music with general emotions. In this paper, we propose EmoGen, an emotional music generation system that leverages a set of emotion-related music attributes as the bridge between emotion and music, and divides the generation into two stages: emotion-to-attribute mapping with supervised clustering, and attribute-to-music generation with self-supervised learning. Both stages are beneficial: in the first stage, the attribute values around the clusterin
&lt;/p&gt;</description></item><item><title>ESGCN&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#36793;&#32536;&#21387;&#32553;&#27880;&#24847;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#21644;&#24341;&#20837;&#36793;&#32536;&#29305;&#24449;&#21644;&#36793;&#32536;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01227</link><description>&lt;p&gt;
ESGCN: &#36793;&#32536;&#21387;&#32553;&#27880;&#24847;&#22270;&#21367;&#31215;&#32593;&#32476;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
ESGCN: Edge Squeeze Attention Graph Convolutional Network for Traffic Flow Forecasting. (arXiv:2307.01227v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01227
&lt;/p&gt;
&lt;p&gt;
ESGCN&#26159;&#19968;&#31181;&#29992;&#20110;&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#30340;&#36793;&#32536;&#21387;&#32553;&#27880;&#24847;&#22270;&#21367;&#31215;&#32593;&#32476;&#65292;&#36890;&#36807;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#21644;&#24341;&#20837;&#36793;&#32536;&#29305;&#24449;&#21644;&#36793;&#32536;&#27880;&#24847;&#26426;&#21046;&#26469;&#25552;&#39640;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27969;&#37327;&#39044;&#27979;&#26159;&#19968;&#20010;&#26497;&#20855;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#65292;&#30001;&#20110;&#20132;&#36890;&#27969;&#30340;&#21160;&#24577;&#26102;&#31354;&#20381;&#36182;&#20851;&#31995;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#30528;&#37325;&#20110;&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Edge Squeeze Graph Convolutional Network (ESGCN)&#30340;&#32593;&#32476;&#26469;&#39044;&#27979;&#22810;&#20010;&#22320;&#21306;&#30340;&#20132;&#36890;&#27969;&#37327;&#12290;ESGCN&#30001;&#20004;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;W&#27169;&#22359;&#21644;ES&#27169;&#22359;&#12290;W&#27169;&#22359;&#26159;&#19968;&#20010;&#23436;&#20840;&#20197;&#33410;&#28857;&#20026;&#22522;&#30784;&#30340;&#21367;&#31215;&#32593;&#32476;&#12290;&#23427;&#20998;&#21035;&#23545;&#27599;&#20010;&#20132;&#36890;&#21306;&#22495;&#30340;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#22312;&#19981;&#21516;&#23610;&#24230;&#19978;&#20998;&#35299;&#26102;&#38388;&#24207;&#21015;&#20197;&#25429;&#25417;&#32454;&#31890;&#24230;&#21644;&#31895;&#31890;&#24230;&#30340;&#29305;&#24449;&#12290;ES&#27169;&#22359;&#20351;&#29992;&#22270;&#21367;&#31215;&#32593;&#32476;(GCN)&#24314;&#27169;&#26102;&#31354;&#21160;&#24577;&#65292;&#24182;&#21033;&#29992;&#26102;&#24207;&#29305;&#24449;&#29983;&#25104;&#33258;&#36866;&#24212;&#37051;&#25509;&#30697;&#38453;(AAM)&#12290;&#20026;&#20102;&#25552;&#39640;AAM&#30340;&#20934;&#30830;&#24615;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#20851;&#38190;&#27010;&#24565;&#12290;1&#65289;&#20351;&#29992;&#36793;&#32536;&#29305;&#24449;&#30452;&#25509;&#25429;&#25417;&#21306;&#22495;&#20043;&#38388;&#30340;&#26102;&#31354;&#27969;&#21160;&#34920;&#31034;&#12290;2&#65289;&#23558;&#36793;&#32536;&#27880;&#24847;&#26426;&#21046;&#24212;&#29992;&#20110;GCN&#65292;&#20174;&#36793;&#32536;&#29305;&#24449;&#20013;&#25552;&#21462;AAM&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic forecasting is a highly challenging task owing to the dynamical spatio-temporal dependencies of traffic flows. To handle this, we focus on modeling the spatio-temporal dynamics and propose a network termed Edge Squeeze Graph Convolutional Network (ESGCN) to forecast traffic flow in multiple regions. ESGCN consists of two modules: W-module and ES module. W-module is a fully node-wise convolutional network. It encodes the time-series of each traffic region separately and decomposes the time-series at various scales to capture fine and coarse features. The ES module models the spatio-temporal dynamics using Graph Convolutional Network (GCN) and generates an Adaptive Adjacency Matrix (AAM) with temporal features. To improve the accuracy of AAM, we introduce three key concepts. 1) Using edge features to directly capture the spatiotemporal flow representation among regions. 2) Applying an edge attention mechanism to GCN to extract the AAM from the edge features. Here, the attention m
&lt;/p&gt;</description></item><item><title>vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;</title><link>http://arxiv.org/abs/2307.01226</link><description>&lt;p&gt;
vONTSS&#65306;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01226
&lt;/p&gt;
&lt;p&gt;
vONTSS&#26159;&#19968;&#31181;&#22522;&#20110;vMF&#21644;&#26368;&#20248;&#20256;&#36755;&#30340;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#65292;&#23427;&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#19988;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#27604;&#26368;&#36817;&#30340;NTM&#26356;&#24555;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#21463;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21551;&#21457;&#30340;&#31070;&#32463;&#20027;&#39064;&#27169;&#22411;&#65288;NTM&#65289;&#24341;&#36215;&#20102;&#24456;&#22810;&#30740;&#31350;&#20852;&#36259;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#25972;&#21512;&#20154;&#31867;&#30693;&#35782;&#30340;&#25361;&#25112;&#65292;&#36825;&#20123;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21463;&#21040;&#20102;&#38480;&#21046;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#21322;&#30417;&#30563;&#31070;&#32463;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;vONTSS&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#22522;&#20110;von Mises-Fisher&#65288;vMF&#65289;&#30340;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#26368;&#20248;&#20256;&#36755;&#12290;&#22312;&#21322;&#30417;&#30563;&#35774;&#32622;&#20013;&#65292;&#24403;&#25552;&#20379;&#27599;&#20010;&#20027;&#39064;&#30340;&#23569;&#37327;&#20851;&#38190;&#35789;&#26102;&#65292;vONTSS&#29983;&#25104;&#28508;&#22312;&#20027;&#39064;&#24182;&#20248;&#21270;&#20027;&#39064;-&#20851;&#38190;&#35789;&#36136;&#37327;&#21644;&#20027;&#39064;&#20998;&#31867;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#20998;&#31867;&#20934;&#30830;&#29575;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#21322;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#12290;vONTSS&#36824;&#25903;&#25345;&#26080;&#30417;&#30563;&#20027;&#39064;&#24314;&#27169;&#12290;&#23450;&#37327;&#21644;&#23450;&#24615;&#23454;&#39564;&#35777;&#26126;&#65292;vONTSS&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#22312;&#22810;&#20010;&#26041;&#38754;&#20248;&#20110;&#26368;&#36817;&#30340;NTM&#65306;vONTSS&#22312;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21457;&#29616;&#39640;&#24230;&#32858;&#31867;&#21644;&#36830;&#36143;&#30340;&#20027;&#39064;&#12290;&#23427;&#20063;&#27604;&#29616;&#26377;-&#25163;&#27861;&#24555;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, Neural Topic Models (NTM), inspired by variational autoencoders, have attracted a lot of research interest; however, these methods have limited applications in the real world due to the challenge of incorporating human knowledge. This work presents a semi-supervised neural topic modeling method, vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and optimal transport. When a few keywords per topic are provided, vONTSS in the semi-supervised setting generates potential topics and optimizes topic-keyword quality and topic classification. Experiments show that vONTSS outperforms existing semi-supervised topic modeling methods in classification accuracy and diversity. vONTSS also supports unsupervised topic modeling. Quantitative and qualitative experiments show that vONTSS in the unsupervised setting outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered and coherent topics on benchmark datasets. It is also much faster than the state
&lt;/p&gt;</description></item><item><title>&#36890;&#36807;&#25552;&#20986;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#26816;&#27979;&#21644;&#36716;&#25442;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#26041;&#38754;&#27880;&#37325;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#22270;&#12289;&#38598;&#25104;&#26799;&#24230;&#21644;&#27169;&#22411;&#21453;&#39304;&#31561;&#25216;&#26415;&#65292;&#22312;&#26816;&#27979;&#38454;&#27573;&#26377;&#21161;&#20110;&#35782;&#21035;&#23545;&#23545;&#25239;&#24615;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;&#26174;&#33879;&#29305;&#24449;&#21644;&#25200;&#21160;&#35789;&#35821;&#65292;&#24182;&#22312;&#36716;&#25442;&#38454;&#27573;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#27169;&#22411;&#21453;&#39304;&#26469;&#29983;&#25104;&#25200;&#21160;&#35789;&#35821;&#30340;&#26368;&#20339;&#26367;&#20195;&#65292;&#20197;&#23558;&#23545;&#25239;&#24615;&#31034;&#20363;&#36716;&#25442;&#20026;&#27491;&#24120;&#31034;&#20363;&#12290;</title><link>http://arxiv.org/abs/2307.01225</link><description>&lt;p&gt;
&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;
&lt;/p&gt;
&lt;p&gt;
Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT). (arXiv:2307.01225v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01225
&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#25552;&#20986;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;&#26694;&#26550;&#65292;&#25105;&#20204;&#22312;&#26816;&#27979;&#21644;&#36716;&#25442;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#26041;&#38754;&#27880;&#37325;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;&#27880;&#24847;&#21147;&#22270;&#12289;&#38598;&#25104;&#26799;&#24230;&#21644;&#27169;&#22411;&#21453;&#39304;&#31561;&#25216;&#26415;&#65292;&#22312;&#26816;&#27979;&#38454;&#27573;&#26377;&#21161;&#20110;&#35782;&#21035;&#23545;&#23545;&#25239;&#24615;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;&#26174;&#33879;&#29305;&#24449;&#21644;&#25200;&#21160;&#35789;&#35821;&#65292;&#24182;&#22312;&#36716;&#25442;&#38454;&#27573;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#27169;&#22411;&#21453;&#39304;&#26469;&#29983;&#25104;&#25200;&#21160;&#35789;&#35821;&#30340;&#26368;&#20339;&#26367;&#20195;&#65292;&#20197;&#23558;&#23545;&#25239;&#24615;&#31034;&#20363;&#36716;&#25442;&#20026;&#27491;&#24120;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#25991;&#26412;&#20998;&#31867;&#22120;&#22914;BERT&#12289;Roberta&#12289;T5&#21644;GPT-3&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#26041;&#38754;&#23637;&#31034;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#20110;&#23545;&#25239;&#24615;&#31034;&#20363;&#30340;&#33030;&#24369;&#24615;&#25552;&#20986;&#20102;&#23433;&#20840;&#39118;&#38505;&#12290;&#29616;&#26377;&#30340;&#38450;&#24481;&#26041;&#27861;&#32570;&#20047;&#35299;&#37322;&#24615;&#65292;&#24456;&#38590;&#29702;&#35299;&#23545;&#25239;&#24615;&#20998;&#31867;&#24182;&#35782;&#21035;&#27169;&#22411;&#30340;&#28431;&#27934;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#39537;&#21160;&#30340;&#26816;&#27979;&#19982;&#36716;&#25442;&#65288;IT-DT&#65289;&#26694;&#26550;&#12290;&#23427;&#19987;&#27880;&#20110;&#22312;&#26816;&#27979;&#21644;&#36716;&#25442;&#25991;&#26412;&#23545;&#25239;&#31034;&#20363;&#26102;&#30340;&#35299;&#37322;&#24615;&#21644;&#36879;&#26126;&#24615;&#12290;IT-DT&#21033;&#29992;&#27880;&#24847;&#21147;&#22270;&#12289;&#38598;&#25104;&#26799;&#24230;&#21644;&#27169;&#22411;&#21453;&#39304;&#31561;&#25216;&#26415;&#36827;&#34892;&#35299;&#37322;&#24615;&#26816;&#27979;&#12290;&#36825;&#26377;&#21161;&#20110;&#35782;&#21035;&#23545;&#23545;&#25239;&#24615;&#20998;&#31867;&#26377;&#36129;&#29486;&#30340;&#26174;&#33879;&#29305;&#24449;&#21644;&#25200;&#21160;&#35789;&#35821;&#12290;&#22312;&#36716;&#25442;&#38454;&#27573;&#65292;IT-DT&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#23884;&#20837;&#21644;&#27169;&#22411;&#21453;&#39304;&#26469;&#29983;&#25104;&#25200;&#21160;&#35789;&#35821;&#30340;&#26368;&#20339;&#26367;&#20195;&#12290;&#36890;&#36807;&#25214;&#21040;&#21512;&#36866;&#30340;&#26367;&#25442;&#65292;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#23558;&#23545;&#25239;&#24615;&#31034;&#20363;&#36716;&#25442;&#20026;&#27491;&#24120;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have shown impressive performance in NLP. However, their vulnerability to adversarial examples poses a security risk. Existing defense methods lack interpretability, making it hard to understand adversarial classifications and identify model vulnerabilities. To address this, we propose the Interpretability and Transparency-Driven Detection and Transformation (IT-DT) framework. It focuses on interpretability and transparency in detecting and transforming textual adversarial examples. IT-DT utilizes techniques like attention maps, integrated gradients, and model feedback for interpretability during detection. This helps identify salient features and perturbed words contributing to adversarial classifications. In the transformation phase, IT-DT uses pre-trained embeddings and model feedback to generate optimal replacements for perturbed words. By finding suitable substitutions, we aim to convert adversarial examples into
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#21457;&#29616;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#31579;&#36873;&#27873;&#27819;&#30340;&#23384;&#22312;&#65292;&#25581;&#31034;&#20102;&#23548;&#33268;&#31579;&#36873;&#27873;&#27819;&#30340;&#22810;&#31181;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#24037;&#20855;&#65292;&#24110;&#21161;&#29992;&#25143;&#36991;&#20813;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21463;&#21040;&#31579;&#36873;&#27873;&#27819;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2307.01221</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#31579;&#36873;&#27873;&#27819;&#65306;&#20107;&#23454;&#36824;&#26159;&#35884;&#35770;--&#19968;&#39033;&#31995;&#32479;&#24615;&#22238;&#39038;
&lt;/p&gt;
&lt;p&gt;
Filter Bubbles in Recommender Systems: Fact or Fallacy -- A Systematic Review. (arXiv:2307.01221v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01221
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#21457;&#29616;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#31579;&#36873;&#27873;&#27819;&#30340;&#23384;&#22312;&#65292;&#25581;&#31034;&#20102;&#23548;&#33268;&#31579;&#36873;&#27873;&#27819;&#30340;&#22810;&#31181;&#20559;&#35265;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#38598;&#25104;&#24037;&#20855;&#65292;&#24110;&#21161;&#29992;&#25143;&#36991;&#20813;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#21463;&#21040;&#31579;&#36873;&#27873;&#27819;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31579;&#36873;&#27873;&#27819;&#26159;&#25351;&#20114;&#32852;&#32593;&#23450;&#21046;&#21270;&#20351;&#20010;&#20307;&#19982;&#22810;&#26679;&#30340;&#35266;&#28857;&#25110;&#26448;&#26009;&#26377;&#25928;&#38548;&#31163;&#65292;&#23548;&#33268;&#20182;&#20204;&#21482;&#21463;&#21040;&#19968;&#32452;&#29305;&#23450;&#20869;&#23481;&#30340;&#24433;&#21709;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#29616;&#26377;&#24577;&#24230;&#12289;&#20449;&#24565;&#25110;&#26465;&#20214;&#30340;&#24378;&#21270;&#12290;&#22312;&#36825;&#39033;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20027;&#35201;&#20851;&#27880;&#31579;&#36873;&#27873;&#27819;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#24433;&#21709;&#12290;&#36825;&#39033;&#24320;&#21019;&#24615;&#30340;&#30740;&#31350;&#26088;&#22312;&#25581;&#31034;&#36825;&#20010;&#38382;&#39064;&#32972;&#21518;&#30340;&#21407;&#22240;&#65292;&#25506;&#32034;&#28508;&#22312;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#25552;&#20986;&#19968;&#20010;&#38598;&#25104;&#24037;&#20855;&#65292;&#24110;&#21161;&#29992;&#25143;&#36991;&#20813;&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#31579;&#36873;&#27873;&#27819;&#12290;&#20026;&#20102;&#36798;&#21040;&#36825;&#20010;&#30446;&#26631;&#65292;&#25105;&#20204;&#23545;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#31579;&#36873;&#27873;&#27819;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#25991;&#29486;&#32508;&#36848;&#12290;&#23545;&#25152;&#32508;&#36848;&#30340;&#25991;&#31456;&#36827;&#34892;&#20102;&#20180;&#32454;&#20998;&#26512;&#21644;&#20998;&#31867;&#65292;&#25552;&#20379;&#20102;&#23545;&#38598;&#25104;&#26041;&#27861;&#21457;&#23637;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32508;&#36848;&#25581;&#31034;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#31579;&#36873;&#27873;&#27819;&#30340;&#35777;&#25454;&#65292;&#24378;&#35843;&#20102;&#20960;&#20010;&#23548;&#33268;&#20854;&#23384;&#22312;&#30340;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;
A filter bubble refers to the phenomenon where Internet customization effectively isolates individuals from diverse opinions or materials, resulting in their exposure to only a select set of content. This can lead to the reinforcement of existing attitudes, beliefs, or conditions. In this study, our primary focus is to investigate the impact of filter bubbles in recommender systems. This pioneering research aims to uncover the reasons behind this problem, explore potential solutions, and propose an integrated tool to help users avoid filter bubbles in recommender systems. To achieve this objective, we conduct a systematic literature review on the topic of filter bubbles in recommender systems. The reviewed articles are carefully analyzed and classified, providing valuable insights that inform the development of an integrated approach. Notably, our review reveals evidence of filter bubbles in recommendation systems, highlighting several biases that contribute to their existence. Moreove
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCP&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#26465;&#20214;&#31574;&#30053;&#20998;&#31163;&#29305;&#24449;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#24182;&#20998;&#21035;&#36827;&#34892;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;6.69%&#12290;</title><link>http://arxiv.org/abs/2307.01217</link><description>&lt;p&gt;
FedCP:&#36890;&#36807;&#26465;&#20214;&#31574;&#30053;&#23545;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#20013;&#30340;&#29305;&#24449;&#20449;&#24687;&#36827;&#34892;&#20998;&#31163;
&lt;/p&gt;
&lt;p&gt;
FedCP: Separating Feature Information for Personalized Federated Learning via Conditional Policy. (arXiv:2307.01217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01217
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedCP&#30340;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#26465;&#20214;&#31574;&#30053;&#20998;&#31163;&#29305;&#24449;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#24182;&#20998;&#21035;&#36827;&#34892;&#22788;&#29702;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#30340;&#24615;&#33021;&#36229;&#36807;&#20102;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;6.69%&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#20010;&#24615;&#21270;&#32852;&#37030;&#23398;&#20064;&#65288;pFL&#65289;&#22312;&#38544;&#31169;&#20445;&#25252;&#12289;&#21327;&#20316;&#23398;&#20064;&#20197;&#21450;&#35299;&#20915;&#23458;&#25143;&#31471;&#20043;&#38388;&#30340;&#32479;&#35745;&#24322;&#36136;&#24615;&#31561;&#26041;&#38754;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#65292;&#20363;&#22914;&#21307;&#38498;&#12289;&#31227;&#21160;&#26234;&#33021;&#25163;&#26426;&#31561;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;pFL&#26041;&#27861;&#20391;&#37325;&#20110;&#21033;&#29992;&#23458;&#25143;&#31471;&#32423;&#27169;&#22411;&#21442;&#25968;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#20294;&#24573;&#30053;&#20102;&#25968;&#25454;&#26159;&#36825;&#20004;&#31181;&#20449;&#24687;&#30340;&#28304;&#22836;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#32852;&#37030;&#26465;&#20214;&#31574;&#30053;&#65288;FedCP&#65289;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20026;&#27599;&#20010;&#26679;&#26412;&#29983;&#25104;&#19968;&#20010;&#26465;&#20214;&#31574;&#30053;&#65292;&#20197;&#20998;&#31163;&#20854;&#29305;&#24449;&#20013;&#30340;&#20840;&#23616;&#20449;&#24687;&#21644;&#20010;&#24615;&#21270;&#20449;&#24687;&#65292;&#28982;&#21518;&#20998;&#21035;&#36890;&#36807;&#20840;&#23616;&#22836;&#21644;&#20010;&#24615;&#21270;&#22836;&#36827;&#34892;&#22788;&#29702;&#12290;&#19982;&#29616;&#26377;&#30340;pFL&#26041;&#27861;&#30456;&#27604;&#65292;FedCP&#26356;&#21152;&#32454;&#31890;&#24230;&#22320;&#32771;&#34385;&#20010;&#24615;&#21270;&#30340;&#26679;&#26412;&#29305;&#23450;&#26041;&#24335;&#12290;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#36827;&#34892;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;FedCP&#22312;&#24615;&#33021;&#19978;&#36229;&#36807;&#20102;&#21313;&#19968;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#65292;&#26368;&#39640;&#21487;&#25552;&#39640;6.69%&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, personalized federated learning (pFL) has attracted increasing attention in privacy protection, collaborative learning, and tackling statistical heterogeneity among clients, e.g., hospitals, mobile smartphones, etc. Most existing pFL methods focus on exploiting the global information and personalized information in the client-level model parameters while neglecting that data is the source of these two kinds of information. To address this, we propose the Federated Conditional Policy (FedCP) method, which generates a conditional policy for each sample to separate the global information and personalized information in its features and then processes them by a global head and a personalized head, respectively. FedCP is more fine-grained to consider personalization in a sample-specific manner than existing pFL methods. Extensive experiments in computer vision and natural language processing domains show that FedCP outperforms eleven state-of-the-art methods by up to 6.69%. Furthe
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35789;&#32452;&#25628;&#32034;&#30340;&#33258;&#21160;&#21453;&#20107;&#23454;&#25193;&#20805;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25429;&#25417;&#20851;&#38190;&#23383;&#32452;&#21512;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25490;&#24207;&#26368;&#24433;&#21709;&#39044;&#27979;&#30340;&#32452;&#21512;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#30001;&#20110;&#21482;&#20851;&#27880;&#21333;&#20010;&#21333;&#35789;&#32780;&#23548;&#33268;&#30340;&#38169;&#35823;&#22240;&#26524;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2307.01214</link><description>&lt;p&gt;
&#22522;&#20110;&#21333;&#35789;&#32452;&#25628;&#32034;&#30340;&#40065;&#26834;&#25991;&#26412;&#20998;&#31867;&#30340;&#33258;&#21160;&#21453;&#20107;&#23454;&#25193;&#20805;
&lt;/p&gt;
&lt;p&gt;
Automatic Counterfactual Augmentation for Robust Text Classification Based on Word-Group Search. (arXiv:2307.01214v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01214
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21333;&#35789;&#32452;&#25628;&#32034;&#30340;&#33258;&#21160;&#21453;&#20107;&#23454;&#25193;&#20805;&#26041;&#27861;&#65292;&#29992;&#20110;&#40065;&#26834;&#25991;&#26412;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#36890;&#36807;&#25429;&#25417;&#20851;&#38190;&#23383;&#32452;&#21512;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#25490;&#24207;&#26368;&#24433;&#21709;&#39044;&#27979;&#30340;&#32452;&#21512;&#65292;&#20174;&#32780;&#35299;&#20915;&#20102;&#30001;&#20110;&#21482;&#20851;&#27880;&#21333;&#20010;&#21333;&#35789;&#32780;&#23548;&#33268;&#30340;&#38169;&#35823;&#22240;&#26524;&#29305;&#24449;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#35268;&#27169;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#22312;&#25991;&#26412;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25104;&#26524;&#65292;&#20294;&#26368;&#36817;&#30340;&#30740;&#31350;&#23545;&#20110;&#25463;&#24452;&#23398;&#20064;&#30340;&#25361;&#25112;&#25552;&#20986;&#20102;&#25285;&#24551;&#12290;&#36890;&#24120;&#24773;&#20917;&#19979;&#65292;&#22914;&#26524;&#20851;&#38190;&#23383;&#19982;&#26631;&#31614;&#20135;&#29983;&#34920;&#38754;&#20851;&#32852;&#65292;&#20174;&#32780;&#23548;&#33268;&#38169;&#35823;&#39044;&#27979;&#65292;&#37027;&#20040;&#23427;&#34987;&#35270;&#20026;&#19968;&#31181;&#25463;&#24452;&#12290;&#30456;&#21453;&#65292;&#22914;&#26524;&#27169;&#22411;&#20381;&#36182;&#20110;&#33021;&#22815;&#20135;&#29983;&#20934;&#30830;&#39044;&#27979;&#30340;&#40065;&#26834;&#22240;&#26524;&#29305;&#24449;&#65292;&#23601;&#21487;&#20197;&#32531;&#35299;&#25463;&#24452;&#23398;&#20064;&#12290;&#20026;&#27492;&#65292;&#35768;&#22810;&#30740;&#31350;&#25506;&#32034;&#20102;&#20107;&#21518;&#21487;&#35299;&#37322;&#30340;&#26041;&#27861;&#26469;&#25366;&#25496;&#40065;&#26834;&#24615;&#21644;&#27867;&#21270;&#24615;&#30340;&#25463;&#24452;&#21644;&#22240;&#26524;&#29305;&#24449;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#21482;&#20851;&#27880;&#21477;&#23376;&#20013;&#30340;&#21333;&#20010;&#21333;&#35789;&#65292;&#24573;&#35270;&#20102;&#21333;&#35789;&#32452;&#30340;&#32771;&#34385;&#65292;&#23548;&#33268;&#38169;&#35823;&#30340;&#22240;&#26524;&#29305;&#24449;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21333;&#35789;&#32452;&#25366;&#25496;&#26041;&#27861;&#65292;&#23427;&#33021;&#22815;&#25429;&#25417;&#20219;&#20309;&#20851;&#38190;&#23383;&#32452;&#21512;&#30340;&#22240;&#26524;&#25928;&#24212;&#65292;&#24182;&#23545;&#26368;&#24433;&#21709;&#39044;&#27979;&#30340;&#32452;&#21512;&#36827;&#34892;&#25490;&#24207;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22522;&#20110;&#26377;&#25928;&#30340;&#20107;&#21518;&#20998;&#26512;&#21644;&#27874;&#26463;&#25628;&#32034;&#65292;&#30830;&#20445;&#20102;&#31639;&#27861;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite large-scale pre-trained language models have achieved striking results for text classificaion, recent work has raised concerns about the challenge of shortcut learning. In general, a keyword is regarded as a shortcut if it creates a superficial association with the label, resulting in a false prediction. Conversely, shortcut learning can be mitigated if the model relies on robust causal features that help produce sound predictions. To this end, many studies have explored post-hoc interpretable methods to mine shortcuts and causal features for robustness and generalization. However, most existing methods focus only on single word in a sentence and lack consideration of word-group, leading to wrong causal features. To solve this problem, we propose a new Word-Group mining approach, which captures the causal effect of any keyword combination and orders the combinations that most affect the prediction. Our approach bases on effective post-hoc analysis and beam search, which ensures
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#27861;&#24459;&#25991;&#20214;&#33258;&#21160;&#36716;&#21270;&#20026;&#26412;&#20307;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#26412;&#20307;&#21457;&#23637;&#21407;&#21017;&#30340;&#32467;&#21512;&#65292;&#23637;&#31034;&#20102;&#22312;&#27431;&#27954;&#32593;&#32476;&#21644;&#20449;&#24687;&#31995;&#32479;&#23433;&#20840;&#25351;&#20196;&#19978;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2307.01211</link><description>&lt;p&gt;
&#19968;&#20010;&#29992;&#20110;&#23433;&#20840;&#25351;&#20196;&#26412;&#20307;&#34920;&#31034;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An automated method for the ontological representation of security directives. (arXiv:2307.01211v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01211
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#22823;&#22411;&#27861;&#24459;&#25991;&#20214;&#33258;&#21160;&#36716;&#21270;&#20026;&#26412;&#20307;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#21644;&#26412;&#20307;&#21457;&#23637;&#21407;&#21017;&#30340;&#32467;&#21512;&#65292;&#23637;&#31034;&#20102;&#22312;&#27431;&#27954;&#32593;&#32476;&#21644;&#20449;&#24687;&#31995;&#32479;&#23433;&#20840;&#25351;&#20196;&#19978;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38590;&#20197;&#35299;&#37322;&#30340;&#22823;&#22411;&#27861;&#24459;&#25991;&#20214;&#65292;&#38271;&#21477;&#23548;&#33268;&#21517;&#35789;&#20043;&#38388;&#38169;&#32508;&#22797;&#26434;&#30340;&#20851;&#31995;&#12290;&#26412;&#25991;&#23558;&#36825;&#20010;&#38382;&#39064;&#25918;&#22312;&#26368;&#36817;&#27431;&#27954;&#23433;&#20840;&#25351;&#20196;&#30340;&#32972;&#26223;&#19979;&#12290;&#36890;&#36807;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#25216;&#26415;&#30340;&#29305;&#23450;&#23450;&#21046;&#65292;&#33258;&#21160;&#21270;&#25552;&#21462;&#20851;&#38190;&#20449;&#24687;&#65292;&#21363;&#27599;&#20010;&#20174;&#21477;&#30340;&#35789;&#31867;&#12290;&#36825;&#20123;&#19982;&#26412;&#20307;&#21457;&#23637;&#21407;&#21017;&#30456;&#32467;&#21512;&#65292;&#35774;&#35745;&#20102;&#25105;&#20204;&#30340;&#23433;&#20840;&#25351;&#20196;&#26412;&#20307;&#34920;&#31034;&#30340;&#33258;&#21160;&#21270;&#26041;&#27861;&#12290;&#35813;&#26041;&#27861;&#22312;&#19968;&#20010;&#23454;&#38469;&#38382;&#39064;&#19978;&#23637;&#31034;&#65292;&#21363;&#25512;&#23548;&#20986;&#34920;&#31034;&#27431;&#27954;&#23618;&#38754;&#19978;&#32593;&#32476;&#21644;&#20449;&#24687;&#31995;&#32479;&#23433;&#20840;&#25351;&#20196;&#30340;&#26412;&#20307;&#12290;&#23613;&#31649;&#37319;&#29992;&#30340;NLP&#25216;&#26415;&#23384;&#22312;&#19968;&#20123;&#38480;&#21046;&#65292;&#24182;&#19988;&#38656;&#35201;&#36890;&#36807;&#25163;&#21160;&#20998;&#26512;&#36827;&#34892;&#34917;&#20805;&#65292;&#20294;&#24635;&#20307;&#32467;&#26524;&#20026;&#25351;&#20196;&#25552;&#20379;&#20102;&#26377;&#25928;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large documents written in juridical language are difficult to interpret, with long sentences leading to intricate and intertwined relations between the nouns. The present paper frames this problem in the context of recent European security directives. The complexity of their language is here thwarted by automating the extraction of the relevant information, namely of the parts of speech from each clause, through a specific tailoring of Natural Language Processing (NLP) techniques. These contribute, in combination with ontology development principles, to the design of our automated method for the representation of security directives as ontologies. The method is showcased on a practical problem, namely to derive an ontology representing the NIS 2 directive, which is the peak of cybersecurity prescripts at the European level. Although the NLP techniques adopted showed some limitations and had to be complemented by manual analysis, the overall results provide valid support for directive 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#38750;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#32769;&#24180;&#30196;&#21574;&#30340;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20026;&#21307;&#29983;&#21644;AI&#24037;&#31243;&#24072;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#25216;&#26415;&#21644;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2307.01210</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#21644;&#38750;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#32769;&#24180;&#30196;&#21574;
&lt;/p&gt;
&lt;p&gt;
AI and Non AI Assessments for Dementia. (arXiv:2307.01210v1 [q-bio.OT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24635;&#32467;&#20102;&#20154;&#24037;&#26234;&#33021;&#21644;&#38750;&#20154;&#24037;&#26234;&#33021;&#35780;&#20272;&#32769;&#24180;&#30196;&#21574;&#30340;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#65292;&#24182;&#20026;&#21307;&#29983;&#21644;AI&#24037;&#31243;&#24072;&#25552;&#20379;&#20102;&#30456;&#20851;&#30340;&#25216;&#26415;&#21644;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#30340;&#36827;&#23637;&#24050;&#32463;&#23548;&#33268;&#20102;&#21508;&#31181;&#31867;&#22411;&#30340;AI&#39537;&#21160;&#30340;&#32769;&#24180;&#30196;&#21574;&#35780;&#20272;&#26041;&#27861;&#30340;&#21457;&#23637;&#65292;&#21487;&#20197;&#29992;&#20110;&#35782;&#21035;&#26089;&#26399;&#32769;&#24180;&#30196;&#21574;&#24739;&#32773;&#12290;&#23427;&#26377;&#21487;&#33021;&#23545;&#32769;&#24180;&#30196;&#21574;&#30340;&#25252;&#29702;&#27169;&#24335;&#36827;&#34892;&#38761;&#21629;&#24615;&#30340;&#25913;&#21464;&#12290;&#21307;&#30103;&#30028;&#24517;&#39035;&#20102;&#35299;&#21508;&#31181;AI&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#26681;&#25454;&#20854;&#22312;&#26089;&#26399;&#35782;&#21035;&#32769;&#24180;&#30196;&#21574;&#24739;&#32773;&#26041;&#38754;&#30340;&#20934;&#30830;&#24615;&#12289;&#25928;&#29575;&#12289;&#23454;&#29992;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#20934;&#30830;&#24615;&#36873;&#25321;&#36866;&#21512;&#30340;&#26041;&#27861;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;AI&#24320;&#21457;&#20154;&#21592;&#20063;&#24212;&#20102;&#35299;&#21508;&#31181;&#38750;AI&#35780;&#20272;&#26041;&#27861;&#20197;&#21450;&#26368;&#36817;&#24320;&#21457;&#30340;AI&#35780;&#20272;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#22635;&#34917;&#20102;&#25991;&#29486;&#20013;&#23545;&#32769;&#24180;&#30196;&#21574;&#35782;&#21035;&#29616;&#26377;&#35299;&#20915;&#26041;&#26696;&#21521;&#21307;&#29983;&#35299;&#37322;&#30340;&#31354;&#30333;&#65292;&#21516;&#26102;&#20063;&#21521;AI&#24037;&#31243;&#24072;&#20171;&#32461;&#20102;&#20351;&#29992;&#30340;&#25216;&#26415;&#21644;&#26368;&#26222;&#36941;&#30340;&#32769;&#24180;&#30196;&#21574;&#25968;&#25454;&#38598;&#12290;&#23427;&#23545;&#21307;&#29983;&#21644;AI&#24037;&#31243;&#24072;&#37117;&#20855;&#26377;&#21487;&#35835;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current progress in the artificial intelligence domain has led to the development of various types of AI-powered dementia assessments, which can be employed to identify patients at the early stage of dementia. It can revolutionize the dementia care settings. It is essential that the medical community be aware of various AI assessments and choose them considering their degrees of validity, efficiency, practicality, reliability, and accuracy concerning the early identification of patients with dementia (PwD). On the other hand, AI developers should be informed about various non-AI assessments as well as recently developed AI assessments. Thus, this paper, which can be readable by both clinicians and AI engineers, fills the gap in the literature in explaining the existing solutions for the recognition of dementia to clinicians, as well as the techniques used and the most widespread dementia datasets to AI engineers. It follows a review of papers on AI and non-AI assessments for dementia t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#21311;&#21517;&#28459;&#27493;&#24341;&#23548;&#30340;&#31070;&#32463;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23569;&#26679;&#26412;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26410;&#35265;&#23454;&#20307;&#21608;&#22260;&#30340;&#23376;&#22270;&#26469;&#33719;&#21462;&#35821;&#20041;&#24182;&#24402;&#32435;&#22320;&#39044;&#27979;&#38142;&#25509;&#65292;&#22312;&#25429;&#25417;&#19968;&#33324;&#30340;&#24402;&#32435;&#27169;&#24335;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#23454;&#20307;&#24182;&#20272;&#35745;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2307.01204</link><description>&lt;p&gt;
&#38754;&#21521;&#30693;&#35782;&#22270;&#35889;&#30340;&#23569;&#26679;&#26412;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#65306;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#21311;&#21517;&#28459;&#27493;&#24341;&#23548;&#30340;&#31070;&#32463;&#36807;&#31243;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Towards Few-shot Inductive Link Prediction on Knowledge Graphs: A Relational Anonymous Walk-guided Neural Process Approach. (arXiv:2307.01204v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01204
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20851;&#31995;&#21311;&#21517;&#28459;&#27493;&#24341;&#23548;&#30340;&#31070;&#32463;&#36807;&#31243;&#26041;&#27861;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23569;&#26679;&#26412;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#26410;&#35265;&#23454;&#20307;&#21608;&#22260;&#30340;&#23376;&#22270;&#26469;&#33719;&#21462;&#35821;&#20041;&#24182;&#24402;&#32435;&#22320;&#39044;&#27979;&#38142;&#25509;&#65292;&#22312;&#25429;&#25417;&#19968;&#33324;&#30340;&#24402;&#32435;&#27169;&#24335;&#30340;&#21516;&#26102;&#65292;&#36824;&#33021;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#23454;&#20307;&#24182;&#20272;&#35745;&#39044;&#27979;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23569;&#26679;&#26412;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#26088;&#22312;&#29992;&#23569;&#26679;&#26412;&#30340;&#38142;&#25509;&#26469;&#39044;&#27979;&#26410;&#35265;&#23454;&#20307;&#30340;&#32570;&#22833;&#38142;&#25509;&#12290;&#20197;&#24448;&#30340;&#26041;&#27861;&#20165;&#38480;&#20110;&#20256;&#23548;&#22330;&#26223;&#65292;&#21363;&#22312;&#30693;&#35782;&#22270;&#35889;&#20013;&#23384;&#22312;&#23454;&#20307;&#65292;&#22240;&#27492;&#26080;&#27861;&#22788;&#29702;&#26410;&#35265;&#23454;&#20307;&#12290;&#22240;&#27492;&#65292;&#36817;&#26399;&#30340;&#24402;&#32435;&#26041;&#27861;&#21033;&#29992;&#26410;&#35265;&#23454;&#20307;&#21608;&#22260;&#30340;&#23376;&#22270;&#26469;&#33719;&#21462;&#35821;&#20041;&#24182;&#24402;&#32435;&#22320;&#39044;&#27979;&#38142;&#25509;&#12290;&#28982;&#32780;&#65292;&#22312;&#23569;&#26679;&#26412;&#22330;&#26223;&#19979;&#65292;&#23376;&#22270;&#36890;&#24120;&#26159;&#31232;&#30095;&#30340;&#65292;&#26080;&#27861;&#25552;&#20379;&#26377;&#24847;&#20041;&#30340;&#24402;&#32435;&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#20851;&#31995;&#21311;&#21517;&#28459;&#27493;&#24341;&#23548;&#30340;&#31070;&#32463;&#36807;&#31243;&#65292;&#29992;&#20110;&#30693;&#35782;&#22270;&#35889;&#19978;&#30340;&#23569;&#26679;&#26412;&#24402;&#32435;&#38142;&#25509;&#39044;&#27979;&#65292;&#31216;&#20026;RawNP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#23545;&#38142;&#25509;&#39044;&#27979;&#20989;&#25968;&#24314;&#27169;&#20026;&#28789;&#27963;&#30340;&#20998;&#24067;&#12290;&#36825;&#20351;&#24471;&#27169;&#22411;&#33021;&#22815;&#24555;&#36895;&#36866;&#24212;&#26032;&#30340;&#23454;&#20307;&#24182;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#20272;&#35745;&#19981;&#30830;&#23450;&#24615;&#12290;&#20026;&#20102;&#25429;&#25417;&#19968;&#33324;&#30340;&#24402;&#32435;&#27169;&#24335;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20851;&#31995;&#21311;&#21517;&#28459;&#27493;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Few-shot inductive link prediction on knowledge graphs (KGs) aims to predict missing links for unseen entities with few-shot links observed. Previous methods are limited to transductive scenarios, where entities exist in the knowledge graphs, so they are unable to handle unseen entities. Therefore, recent inductive methods utilize the sub-graphs around unseen entities to obtain the semantics and predict links inductively. However, in the few-shot setting, the sub-graphs are often sparse and cannot provide meaningful inductive patterns. In this paper, we propose a novel relational anonymous walk-guided neural process for few-shot inductive link prediction on knowledge graphs, denoted as RawNP. Specifically, we develop a neural process-based method to model a flexible distribution over link prediction functions. This enables the model to quickly adapt to new entities and estimate the uncertainty when making predictions. To capture general inductive patterns, we present a relational anony
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#37319;&#29992;ChatGPT&#25216;&#26415;&#65292;&#20197;OpenAI&#30340;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#23454;&#29616;&#20102;&#23545;&#19987;&#21033;&#21019;&#26032;&#25104;&#21151;&#21644;&#20272;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#20026;&#19987;&#21033;&#20272;&#20540;&#25552;&#20379;&#20102;&#38761;&#21629;&#24615;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#39044;&#27979;&#25509;&#21463;&#29575;&#26500;&#24314;&#30340;&#22810;&#31354;&#25237;&#36164;&#32452;&#21512;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24322;&#24120;&#25910;&#30410;&#29575;&#12290;</title><link>http://arxiv.org/abs/2307.01202</link><description>&lt;p&gt;
&#39044;&#27979;&#24615;&#19987;&#21033;&#23398;&#65306;&#20351;&#29992;ChatGPT&#25216;&#26415;&#39044;&#27979;&#21019;&#26032;&#25104;&#21151;&#21644;&#20272;&#20540;
&lt;/p&gt;
&lt;p&gt;
Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT. (arXiv:2307.01202v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01202
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#37319;&#29992;ChatGPT&#25216;&#26415;&#65292;&#20197;OpenAI&#30340;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#20026;&#22522;&#30784;&#65292;&#36890;&#36807;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#23454;&#29616;&#20102;&#23545;&#19987;&#21033;&#21019;&#26032;&#25104;&#21151;&#21644;&#20272;&#20540;&#30340;&#20934;&#30830;&#39044;&#27979;&#65292;&#20026;&#19987;&#21033;&#20272;&#20540;&#25552;&#20379;&#20102;&#38761;&#21629;&#24615;&#30340;&#25913;&#36827;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#39044;&#27979;&#25509;&#21463;&#29575;&#26500;&#24314;&#30340;&#22810;&#31354;&#25237;&#36164;&#32452;&#21512;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#24322;&#24120;&#25910;&#30410;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#26041;&#27861;&#23545;&#20110;&#21019;&#26032;&#30340;&#20998;&#26512;&#22312;&#24191;&#27867;&#30340;&#32467;&#26500;&#21464;&#37327;&#26041;&#38754;&#23384;&#22312;&#26681;&#26412;&#24615;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#24320;&#21019;&#24615;&#30340;ChatGPT&#25216;&#26415;&#37319;&#29992;LLM&#26041;&#27861;&#23545;&#19987;&#21033;&#36827;&#34892;&#20998;&#26512;&#65292;&#31361;&#30772;&#20102;&#36793;&#30028;&#12290;OpenAI&#30340;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#23884;&#20837;&#33021;&#22815;&#35775;&#38382;&#20851;&#20110;&#27599;&#20010;&#21457;&#26126;&#30340;&#36136;&#37327;&#21644;&#24433;&#21709;&#30340;&#22797;&#26434;&#20449;&#24687;&#65292;&#29992;&#20110;&#39537;&#21160;&#28145;&#24230;&#23398;&#20064;&#39044;&#27979;&#27169;&#22411;&#12290;&#32454;&#33268;&#30340;&#23884;&#20837;&#20351;&#39044;&#27979;&#19987;&#21033;&#20215;&#20540;&#30340;R-squared&#25552;&#39640;&#20102;24&#65285;&#65292;&#24182;&#26126;&#30830;&#22320;&#23558;&#26368;&#24046;&#21644;&#26368;&#20339;&#24212;&#29992;&#31243;&#24207;&#20998;&#31163;&#24320;&#26469;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#39044;&#27979;&#25509;&#21463;&#29575;&#26500;&#24314;&#30340;&#22810;&#31354;&#25237;&#36164;&#32452;&#21512;&#27599;&#24180;&#23454;&#29616;&#26174;&#33879;&#30340;&#24322;&#24120;&#25910;&#30410;&#29575;&#39640;&#36798;3.3%&#65292;&#21516;&#26102;&#20063;&#35777;&#26126;&#20102;&#24066;&#22330;&#26080;&#27861;&#21450;&#26102;&#25972;&#21512;&#26377;&#20851;&#24212;&#29992;&#31243;&#24207;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#27169;&#22411;&#20026;&#38761;&#21629;&#24615;&#25913;&#21464;&#31185;&#26681;&#12289;&#24085;&#24085;&#23612;&#31185;&#27931;&#12289;&#22622;&#40065;&#21644;&#26031;&#25176;&#22827;&#26364;&#65288;2017&#65289;&#23545;&#19987;&#21033;&#20272;&#20540;&#30340;&#26356;&#27491;&#25552;&#20379;&#20102;&#26426;&#20250;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analysis of innovation has been fundamentally limited by conventional approaches to broad, structural variables. This paper pushes the boundaries, taking an LLM approach to patent analysis with the groundbreaking ChatGPT technology. OpenAI's state-of-the-art textual embedding accesses complex information about the quality and impact of each invention to power deep learning predictive models. The nuanced embedding drives a 24% incremental improvement in R-squared predicting patent value and clearly isolates the worst and best applications. These models enable a revision of the contemporary Kogan, Papanikolaou, Seru, and Stoffman (2017) valuation of patents by a median deviation of 1.5 times, accounting for potential institutional predictions. Furthermore, the market fails to incorporate timely information about applications; a long-short portfolio based on predicted acceptance rates achieves significant abnormal returns of 3.3% annually. The models provide an opportunity to revolutioniz
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20351;&#29992;&#20811;&#38534;&#32467;&#26500;&#22240;&#26524;&#22270;&#21487;&#20197;&#23454;&#29616;&#19982;transformer-based&#35821;&#35328;&#27169;&#22411;&#30456;&#20284;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;</title><link>http://arxiv.org/abs/2307.01201</link><description>&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#21644;&#20986;&#29616;&#20013;&#30340;&#27169;&#24335;&#23398;&#20064;&#21644;&#37325;&#26032;&#32465;&#23450;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Schema-learning and rebinding as mechanisms of in-context learning and emergence. (arXiv:2307.01201v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.01201
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#26426;&#21046;&#65292;&#21457;&#29616;&#20351;&#29992;&#20811;&#38534;&#32467;&#26500;&#22240;&#26524;&#22270;&#21487;&#20197;&#23454;&#29616;&#19982;transformer-based&#35821;&#35328;&#27169;&#22411;&#30456;&#20284;&#30340;&#33021;&#21147;&#65292;&#24182;&#19988;&#36825;&#31181;&#26041;&#27861;&#21487;&#20197;&#35299;&#37322;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#24037;&#20316;&#21407;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19978;&#19979;&#25991;&#23398;&#20064;&#65288;ICL&#65289;&#26159;&#36817;&#26399;&#22522;&#20110;Transformer&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#20013;&#26368;&#24378;&#22823;&#19988;&#26368;&#20196;&#20154;&#24847;&#22806;&#30340;&#33021;&#21147;&#20043;&#19968;&#12290;&#28982;&#32780;&#65292;&#23427;&#30340;&#22522;&#30784;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35777;&#26126;&#21487;&#20197;&#36890;&#36807;&#20351;&#29992;&#20811;&#38534;&#32467;&#26500;&#22240;&#26524;&#22270;&#65288;CSCGs&#65289;&#36825;&#31181;&#26367;&#20195;&#30340;&#24207;&#21015;&#39044;&#27979;&#23398;&#20064;&#26041;&#27861;&#33719;&#24471;&#21487;&#27604;&#36739;&#30340;ICL&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;CSCGs&#30340;&#19968;&#20010;&#20851;&#38190;&#29305;&#24615;&#26159;&#65292;&#19982;&#22522;&#20110;Transformer&#30340;LLMs&#19981;&#21516;&#65292;&#23427;&#20204;&#26159;&#21487;&#35299;&#37322;&#30340;&#65292;&#36825;&#22823;&#22823;&#31616;&#21270;&#20102;&#35299;&#37322;ICL&#24037;&#20316;&#21407;&#29702;&#30340;&#20219;&#21153;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#26174;&#31034;&#23427;&#20351;&#29992;&#20102;&#20197;&#19979;&#32452;&#21512;&#65306;&#65288;a&#65289;&#23398;&#20064;&#27169;&#26495;&#65288;&#27169;&#24335;&#65289;&#30005;&#36335;&#36827;&#34892;&#27169;&#24335;&#23436;&#25104;&#65292;&#65288;b&#65289;&#20197;&#19978;&#19979;&#25991;&#25935;&#24863;&#26041;&#24335;&#26816;&#32034;&#30456;&#20851;&#27169;&#26495;&#65292;&#20197;&#21450;&#65288;c&#65289;&#23558;&#26032;&#26631;&#35760;&#37325;&#26032;&#32465;&#23450;&#21040;&#27169;&#26495;&#30340;&#36866;&#24403;&#20301;&#32622;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;ICL&#22312;LLMs&#20013;&#37319;&#29992;&#31867;&#20284;&#26426;&#21046;&#30340;&#20551;&#35774;&#35777;&#25454;&#12290;&#20363;&#22914;&#65292;&#25105;&#20204;&#21457;&#29616;&#65292;&#19982;LLMs&#19968;&#26679;&#65292;&#20351;&#29992;CSCGs&#26102;&#20250;&#20986;&#29616;&#19981;&#21516;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-context learning (ICL) is one of the most powerful and most unexpected capabilities to emerge in recent transformer-based large language models (LLMs). Yet the mechanisms that underlie it are poorly understood. In this paper, we demonstrate that comparable ICL capabilities can be acquired by an alternative sequence prediction learning method using clone-structured causal graphs (CSCGs). Moreover, a key property of CSCGs is that, unlike transformer-based LLMs, they are {\em interpretable}, which considerably simplifies the task of explaining how ICL works. Specifically, we show that it uses a combination of (a) learning template (schema) circuits for pattern completion, (b) retrieving relevant templates in a context-sensitive manner, and (c) rebinding of novel tokens to appropriate slots in the templates. We go on to marshall evidence for the hypothesis that similar mechanisms underlie ICL in LLMs. For example, we find that, with CSCGs as with LLMs, different capabilities emerge at d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#20808;&#21069;&#31639;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2307.00677</link><description>&lt;p&gt;
SDC-HSDD-NDSA: &#20351;&#29992;&#23618;&#27425;&#27425;&#32423;&#23548;&#21521;&#24046;&#24322;&#21644;&#24402;&#19968;&#21270;&#23494;&#24230;&#33258;&#36866;&#24212;&#30340;&#32467;&#26500;&#26816;&#27979;&#32858;&#31867;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
SDC-HSDD-NDSA: Structure Detecting Cluster by Hierarchical Secondary Directed Differential with Normalized Density and Self-Adaption. (arXiv:2307.00677v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00677
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#65292;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#30340;&#32467;&#26500;&#65292;&#20855;&#26377;&#20808;&#21069;&#31639;&#27861;&#25152;&#19981;&#20855;&#22791;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#32858;&#31867;&#31639;&#27861;&#20043;&#19968;&#65292;&#22240;&#20026;&#23427;&#33021;&#22815;&#35782;&#21035;&#20219;&#24847;&#24418;&#29366;&#30340;&#32858;&#31867;&#65292;&#21482;&#35201;&#19981;&#21516;&#30340;&#39640;&#23494;&#24230;&#32858;&#31867;&#20043;&#38388;&#26377;&#20302;&#23494;&#24230;&#21306;&#22495;&#20998;&#38548;&#12290;&#28982;&#32780;&#65292;&#36890;&#36807;&#20302;&#23494;&#24230;&#21306;&#22495;&#23558;&#32858;&#31867;&#20998;&#38548;&#24320;&#30340;&#35201;&#27714;&#24182;&#19981;&#26159;&#24494;&#19981;&#36275;&#36947;&#30340;&#65292;&#22240;&#20026;&#39640;&#23494;&#24230;&#21306;&#22495;&#21487;&#33021;&#20855;&#26377;&#19981;&#21516;&#30340;&#32467;&#26500;&#65292;&#24212;&#35813;&#34987;&#32858;&#31867;&#21040;&#19981;&#21516;&#30340;&#32452;&#20013;&#12290;&#36825;&#31181;&#24773;&#20917;&#35828;&#26126;&#20102;&#25105;&#20204;&#24050;&#30693;&#30340;&#25152;&#26377;&#20808;&#21069;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#31639;&#27861;&#30340;&#20027;&#35201;&#32570;&#38519;--&#26080;&#27861;&#26816;&#27979;&#39640;&#23494;&#24230;&#32858;&#31867;&#20013;&#30340;&#32467;&#26500;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#26088;&#22312;&#25552;&#20379;&#19968;&#31181;&#22522;&#20110;&#23494;&#24230;&#30340;&#32858;&#31867;&#26041;&#26696;&#65292;&#26082;&#20855;&#26377;&#20808;&#21069;&#26041;&#27861;&#30340;&#33021;&#21147;&#65292;&#21448;&#33021;&#22815;&#26816;&#27979;&#21040;&#39640;&#23494;&#24230;&#21306;&#22495;&#20013;&#26410;&#34987;&#20302;&#23494;&#24230;&#21306;&#20998;&#24320;&#30340;&#32467;&#26500;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#23618;&#27425;&#27425;&#32423;&#23548;&#21521;&#24046;&#24322;&#12289;&#23618;&#27425;&#21270;&#12289;&#24402;&#19968;&#21270;&#23494;&#24230;&#20197;&#21450;&#33258;&#36866;&#24212;&#31995;&#25968;&#65292;&#22240;&#27492;&#34987;&#31216;&#20026;&#32467;&#26500;&#26816;&#27979;&#32858;&#31867;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Density-based clustering could be the most popular clustering algorithm since it can identify clusters of arbitrary shape as long as different (high-density) clusters are separated by low-density regions. However, the requirement of the separateness of clusters by low-density regions is not trivial since a high-density region might have different structures which should be clustered into different groups. Such a situation demonstrates the main flaw of all previous density-based clustering algorithms we have known--structures in a high-density cluster could not be detected. Therefore, this paper aims to provide a density-based clustering scheme that not only has the ability previous ones have but could also detect structures in a high-density region not separated by low-density ones. The algorithm employs secondary directed differential, hierarchy, normalized density, as well as the self-adaption coefficient, and thus is called Structure Detecting Cluster by Hierarchical Secondary Direc
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#24863;&#30693;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#38544;&#34255;&#23618;&#28608;&#27963;&#30340;&#31232;&#30095;&#31243;&#24230;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31232;&#30095;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#32780;&#19988;&#32467;&#26524;&#23545;&#27169;&#22411;&#30340;&#31232;&#30095;&#31243;&#24230;&#27809;&#26377;&#24378;&#28872;&#30340;&#20551;&#35774;&#65292;&#24182;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#38750;&#31354;&#30340;&#30028;&#38480;&#12290;</title><link>http://arxiv.org/abs/2307.00426</link><description>&lt;p&gt;
&#22522;&#20110;&#31232;&#30095;&#24863;&#30693;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27867;&#21270;&#29702;&#35770;
&lt;/p&gt;
&lt;p&gt;
Sparsity-aware generalization theory for deep neural networks. (arXiv:2307.00426v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00426
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31232;&#30095;&#24863;&#30693;&#30340;&#20998;&#26512;&#26041;&#27861;&#12290;&#36890;&#36807;&#32771;&#34385;&#38544;&#34255;&#23618;&#28608;&#27963;&#30340;&#31232;&#30095;&#31243;&#24230;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#31232;&#30095;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26435;&#34913;&#65292;&#32780;&#19988;&#32467;&#26524;&#23545;&#27169;&#22411;&#30340;&#31232;&#30095;&#31243;&#24230;&#27809;&#26377;&#24378;&#28872;&#30340;&#20551;&#35774;&#65292;&#24182;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#30340;&#25968;&#20540;&#23454;&#39564;&#20013;&#24471;&#21040;&#20102;&#38750;&#31354;&#30340;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#21462;&#24471;&#20102;&#20196;&#20154;&#24778;&#35766;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#20294;&#20854;&#20855;&#20307;&#26426;&#21046;&#23578;&#19981;&#28165;&#26970;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#26469;&#20998;&#26512;&#21069;&#21521;&#28145;&#24230;ReLU&#32593;&#32476;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#21033;&#29992;&#38544;&#34255;&#23618;&#28608;&#27963;&#30340;&#31232;&#30095;&#31243;&#24230;&#12290;&#36890;&#36807;&#26500;&#24314;&#19968;&#20010;&#32771;&#34385;&#27599;&#20010;&#36755;&#20837;&#26679;&#26412;&#20943;&#23567;&#26377;&#25928;&#27169;&#22411;&#22823;&#23567;&#30340;&#26694;&#26550;&#65292;&#25105;&#20204;&#33021;&#22815;&#23637;&#31034;&#20986;&#31232;&#30095;&#21644;&#27867;&#21270;&#20043;&#38388;&#30340;&#26681;&#26412;&#26435;&#34913;&#12290;&#37325;&#35201;&#30340;&#26159;&#65292;&#25105;&#20204;&#30340;&#32467;&#26524;&#23545;&#27169;&#22411;&#23454;&#29616;&#30340;&#31232;&#30095;&#31243;&#24230;&#27809;&#26377;&#24378;&#28872;&#30340;&#20551;&#35774;&#65292;&#24182;&#19988;&#25913;&#36827;&#20102;&#26368;&#36817;&#30340;&#22522;&#20110;&#33539;&#25968;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#25968;&#20540;&#23454;&#20363;&#23637;&#31034;&#20102;&#32467;&#26524;&#65292;&#21363;&#20351;&#22312;&#36807;&#21442;&#25968;&#21270;&#30340;&#27169;&#22411;&#20013;&#65292;&#22312;&#29305;&#23450;&#24773;&#20917;&#19979;&#19982;&#25968;&#25454;&#30456;&#20851;&#30340;&#20808;&#39564;&#32467;&#21512;&#26102;&#20063;&#33021;&#24471;&#21040;&#38750;&#31354;&#30028;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep artificial neural networks achieve surprising generalization abilities that remain poorly understood. In this paper, we present a new approach to analyzing generalization for deep feed-forward ReLU networks that takes advantage of the degree of sparsity that is achieved in the hidden layer activations. By developing a framework that accounts for this reduced effective model size for each input sample, we are able to show fundamental trade-offs between sparsity and generalization. Importantly, our results make no strong assumptions about the degree of sparsity achieved by the model, and it improves over recent norm-based approaches. We illustrate our results numerically, demonstrating non-vacuous bounds when coupled with data-dependent priors in specific settings, even in over-parametrized models.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#21327;&#21516;&#25512;&#29702;&#65288;CCR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#21453;&#20107;&#23454;&#25512;&#29702;&#21644;&#36923;&#36753;&#25512;&#29702;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#29983;&#25104;&#22256;&#38590;&#30340;&#21453;&#20107;&#23454;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;CCR&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#31034;&#20102;&#22914;&#20309;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#12289;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;&#36879;&#26126;&#24230;&#12290;</title><link>http://arxiv.org/abs/2307.00165</link><description>&lt;p&gt;
&#21453;&#20107;&#23454;&#21327;&#21516;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Counterfactual Collaborative Reasoning. (arXiv:2307.00165v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00165
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#21327;&#21516;&#25512;&#29702;&#65288;CCR&#65289;&#26041;&#27861;&#65292;&#36890;&#36807;&#25972;&#21512;&#21453;&#20107;&#23454;&#25512;&#29702;&#21644;&#36923;&#36753;&#25512;&#29702;&#26469;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#29983;&#25104;&#22256;&#38590;&#30340;&#21453;&#20107;&#23454;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;CCR&#22312;&#25512;&#33616;&#31995;&#32479;&#20013;&#23637;&#31034;&#20102;&#22914;&#20309;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#12289;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;&#36879;&#26126;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22240;&#26524;&#25512;&#29702;&#21644;&#36923;&#36753;&#25512;&#29702;&#26159;&#20154;&#31867;&#26234;&#33021;&#30340;&#20004;&#31181;&#37325;&#35201;&#25512;&#29702;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#26234;&#33021;&#32972;&#26223;&#19979;&#65292;&#23427;&#20204;&#30340;&#20851;&#31995;&#36824;&#26410;&#24471;&#21040;&#24191;&#27867;&#25506;&#32034;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#22914;&#20309;&#20849;&#21516;&#24314;&#27169;&#36825;&#20004;&#31181;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#25552;&#39640;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#25972;&#21512;&#21453;&#20107;&#23454;&#25512;&#29702;&#21644;&#65288;&#31070;&#32463;&#65289;&#36923;&#36753;&#25512;&#29702;&#20004;&#31181;&#37325;&#35201;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#21453;&#20107;&#23454;&#21327;&#21516;&#25512;&#29702;&#65288;CCR&#65289;&#65292;&#23427;&#36890;&#36807;&#36827;&#34892;&#21453;&#20107;&#23454;&#36923;&#36753;&#25512;&#29702;&#26469;&#25913;&#36827;&#24615;&#33021;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#20197;&#25512;&#33616;&#31995;&#32479;&#20026;&#20363;&#65292;&#23637;&#31034;&#20102;CCR&#22914;&#20309;&#32531;&#35299;&#25968;&#25454;&#31232;&#32570;&#12289;&#25552;&#39640;&#20934;&#30830;&#24615;&#21644;&#22686;&#24378;&#36879;&#26126;&#24230;&#12290;&#20174;&#25216;&#26415;&#19978;&#35762;&#65292;&#25105;&#20204;&#21033;&#29992;&#21453;&#20107;&#23454;&#25512;&#29702;&#26469;&#29983;&#25104;&#8220;&#22256;&#38590;&#8221;&#30340;&#21453;&#20107;&#23454;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#25968;&#25454;&#22686;&#24378;&#65292;&#36825;&#19982;&#21407;&#22987;&#30340;&#35757;&#32451;&#26679;&#26412;&#19968;&#36215;&#21487;&#20197;&#25552;&#21319;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Causal reasoning and logical reasoning are two important types of reasoning abilities for human intelligence. However, their relationship has not been extensively explored under machine intelligence context. In this paper, we explore how the two reasoning abilities can be jointly modeled to enhance both accuracy and explainability of machine learning models. More specifically, by integrating two important types of reasoning ability -- counterfactual reasoning and (neural) logical reasoning -- we propose Counterfactual Collaborative Reasoning (CCR), which conducts counterfactual logic reasoning to improve the performance. In particular, we use recommender system as an example to show how CCR alleviate data scarcity, improve accuracy and enhance transparency. Technically, we leverage counterfactual reasoning to generate "difficult" counterfactual training examples for data augmentation, which -together with the original training examples -- can enhance the model performance. Since the 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26234;&#33021;&#20914;&#28010;&#32773;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;Ulman&#32593;&#32476;&#22312;&#28151;&#27788;&#21160;&#21147;&#23398;&#20013;&#36827;&#34892;&#25628;&#32034;&#12290;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#26234;&#33021;&#20914;&#28010;&#32773;&#33021;&#22815;&#22312;&#32593;&#32476;&#20013;&#25214;&#21040;&#26368;&#30701;&#36335;&#24452;&#65292;&#24182;&#19988;&#27604;&#23581;&#35797;&#23613;&#37327;&#20943;&#23567;&#19982;&#30446;&#26631;&#36317;&#31163;&#30340;&#24858;&#34850;&#20914;&#28010;&#32773;&#26356;&#21152;&#39640;&#25928;&#12290;&#36825;&#20026;&#22312;&#28151;&#27788;&#27969;&#21160;&#20013;&#30340;&#36816;&#21160;&#25511;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2307.00019</link><description>&lt;p&gt;
&#26234;&#33021;&#20914;&#28010;&#32773;&#22312;&#28151;&#27788;&#27969;&#21160;&#20013;&#30340;&#30446;&#26631;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Goal quest for an intelligent surfer moving in a chaotic flow. (arXiv:2307.00019v1 [physics.soc-ph] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2307.00019
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26234;&#33021;&#20914;&#28010;&#32773;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#20351;&#29992;Ulman&#32593;&#32476;&#22312;&#28151;&#27788;&#21160;&#21147;&#23398;&#20013;&#36827;&#34892;&#25628;&#32034;&#12290;&#20351;&#29992;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#65292;&#26234;&#33021;&#20914;&#28010;&#32773;&#33021;&#22815;&#22312;&#32593;&#32476;&#20013;&#25214;&#21040;&#26368;&#30701;&#36335;&#24452;&#65292;&#24182;&#19988;&#27604;&#23581;&#35797;&#23613;&#37327;&#20943;&#23567;&#19982;&#30446;&#26631;&#36317;&#31163;&#30340;&#24858;&#34850;&#20914;&#28010;&#32773;&#26356;&#21152;&#39640;&#25928;&#12290;&#36825;&#20026;&#22312;&#28151;&#27788;&#27969;&#21160;&#20013;&#30340;&#36816;&#21160;&#25511;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#32771;&#34385;&#20102;&#19968;&#20010;&#27169;&#22411;&#65292;&#20854;&#20013;&#26234;&#33021;&#20914;&#28010;&#32773;&#22312;&#30001;Chirikov&#26631;&#20934;&#26144;&#23556;&#20013;&#30340;&#28151;&#27788;&#21160;&#21147;&#23398;&#29983;&#25104;&#30340;Ulam&#32593;&#32476;&#19978;&#31227;&#21160;&#12290;&#35813;&#26377;&#21521;&#32593;&#32476;&#26159;&#36890;&#36807;Ulam&#26041;&#27861;&#33719;&#24471;&#30340;&#65292;&#21033;&#29992;&#22266;&#23450;&#22823;&#23567;&#30340;&#30456;&#31354;&#38388;&#21333;&#20803;&#24418;&#25104;&#39532;&#23572;&#21487;&#22827;&#38142;&#30340;&#33410;&#28857;&#12290;&#35813;&#20914;&#28010;&#32773;&#30340;&#30446;&#26631;&#26159;&#30830;&#23450;&#20174;&#21021;&#22987;&#33410;&#28857;A&#21040;&#26368;&#32456;&#33410;&#28857;B&#30340;&#32593;&#32476;&#36335;&#24452;&#65292;&#20854;&#38459;&#21147;&#26368;&#23567;&#65292;&#21363;&#36716;&#31227;&#27010;&#29575;&#30340;&#20498;&#25968;&#20043;&#21644;&#26368;&#23567;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#31639;&#27861;&#65292;&#20351;&#26234;&#33021;&#20914;&#28010;&#32773;&#33021;&#22815;&#22312;&#23569;&#37327;&#30340;&#36716;&#31227;&#20013;&#23436;&#25104;&#25628;&#32034;&#65292;&#20854;&#25968;&#37327;&#20165;&#20197;&#32593;&#32476;&#22823;&#23567;&#30340;&#23545;&#25968;&#22686;&#38271;&#12290;&#26368;&#20248;&#36335;&#24452;&#25628;&#32034;&#22312;&#20855;&#26377;&#27491;&#21521;&#21644;&#36870;&#21521;&#32593;&#32476;&#30340;&#23567;Erd\"os&#25968;&#33410;&#28857;&#24418;&#25104;&#30340;&#20998;&#24418;&#20132;&#38598;&#38598;&#21512;&#19978;&#36827;&#34892;&#12290;&#26234;&#33021;&#20914;&#28010;&#32773;&#22312;&#23613;&#37327;&#20943;&#23567;&#19982;&#30446;&#26631;B&#30340;&#30456;&#31354;&#38388;&#36317;&#31163;&#30340;&#24858;&#34850;&#20914;&#28010;&#32773;&#19978;&#21576;&#25351;&#25968;&#32423;&#20248;&#21183;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#31639;&#27861;&#20026;&#22312;&#28151;&#27788;&#27969;&#21160;&#20013;&#36827;&#34892;&#36816;&#21160;&#25511;&#21046;&#25552;&#20379;&#20102;&#26032;&#30340;&#32447;&#32034;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider a model of an intelligent surfer moving on the Ulam network generated by a chaotic dynamics in the Chirikov standard map. This directed network is obtained by the Ulam method with a division of the phase space in cells of fixed size forming the nodes of a Markov chain. The goal quest for this surfer is to determine the network path from an initial node A to a final node B with minimal resistance given by the sum of inverse transition probabilities. We develop an algorithm for the intelligent surfer that allows to perform the quest in a small number of transitions which grows only logarithmically with the network size. The optimal path search is done on a fractal intersection set formed by nodes with small Erd\"os numbers of the forward and inverted networks. The intelligent surfer exponentially outperforms a naive surfer who tries to minimize its phase space distance to the target B. We argue that such an algorithm provides new hints for motion control in chaotic flows.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#36827;&#21270;&#21338;&#24328;&#29702;&#35770;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#30456;&#27604;&#20110;&#21482;&#24110;&#21161;&#31526;&#21512;&#26465;&#20214;&#30340;&#20154;&#30340;&#27495;&#35270;&#24615;&#20154;&#24037;&#26234;&#33021;&#65292;&#25746;&#39532;&#21033;&#20122;&#24335;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26356;&#22909;&#22320;&#20419;&#36827;&#20154;&#31867;&#38388;&#30340;&#21512;&#20316;&#12290;&#36825;&#23545;&#20110;&#32531;&#24930;&#21457;&#23637;&#30340;&#31038;&#20250;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#22312;&#36825;&#31181;&#31038;&#20250;&#20013;&#65292;&#21464;&#21270;&#34987;&#35270;&#20026;&#35880;&#24910;&#21644;&#25269;&#25239;&#12290;</title><link>http://arxiv.org/abs/2306.17747</link><description>&lt;p&gt;
&#27495;&#35270;&#24615;&#25110;&#25746;&#39532;&#21033;&#20122;&#24335; -- &#20154;&#31867;&#38656;&#35201;&#21738;&#31181;&#20154;&#24037;&#26234;&#33021;&#65311;&#28151;&#21512;&#20154;&#24037;&#26234;&#33021;&#20154;&#21475;&#30340;&#36827;&#21270;&#21338;&#24328;&#35770;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Discriminatory or Samaritan -- which AI is needed for humanity? An Evolutionary Game Theory Analysis of Hybrid Human-AI populations. (arXiv:2306.17747v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17747
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#36827;&#21270;&#21338;&#24328;&#29702;&#35770;&#30340;&#20998;&#26512;&#21457;&#29616;&#65292;&#30456;&#27604;&#20110;&#21482;&#24110;&#21161;&#31526;&#21512;&#26465;&#20214;&#30340;&#20154;&#30340;&#27495;&#35270;&#24615;&#20154;&#24037;&#26234;&#33021;&#65292;&#25746;&#39532;&#21033;&#20122;&#24335;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26356;&#22909;&#22320;&#20419;&#36827;&#20154;&#31867;&#38388;&#30340;&#21512;&#20316;&#12290;&#36825;&#23545;&#20110;&#32531;&#24930;&#21457;&#23637;&#30340;&#31038;&#20250;&#23588;&#20026;&#37325;&#35201;&#65292;&#22240;&#20026;&#22312;&#36825;&#31181;&#31038;&#20250;&#20013;&#65292;&#21464;&#21270;&#34987;&#35270;&#20026;&#35880;&#24910;&#21644;&#25269;&#25239;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#36234;&#26469;&#36234;&#23884;&#20837;&#25105;&#20204;&#30340;&#29983;&#27963;&#65292;&#23427;&#20204;&#30340;&#23384;&#22312;&#23548;&#33268;&#20102;&#22609;&#36896;&#25105;&#20204;&#30340;&#34892;&#20026;&#12289;&#20915;&#31574;&#21644;&#31038;&#20250;&#20114;&#21160;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#29616;&#26377;&#30340;&#29702;&#35770;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#20114;&#21160;&#19978;&#65292;&#24573;&#35270;&#20102;&#20154;&#24037;&#26234;&#33021;&#23384;&#22312;&#23548;&#33268;&#30340;&#29420;&#29305;&#21160;&#24577;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#37319;&#29992;&#36827;&#21270;&#21338;&#24328;&#29702;&#35770;&#30340;&#26041;&#27861;&#65292;&#30740;&#31350;&#20102;&#19981;&#21516;&#24418;&#24335;&#30340;&#20154;&#24037;&#26234;&#33021;&#23545;&#22312;&#28151;&#21512;&#20154;&#24037;&#26234;&#33021;&#20154;&#21475;&#20013;&#36827;&#34892;&#19968;&#27425;&#24615;&#22234;&#24466;&#22256;&#22659;&#28216;&#25103;&#30340;&#21512;&#20316;&#28436;&#21270;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#23436;&#20840;&#28151;&#21512;&#20154;&#21475;&#21644;&#32467;&#26500;&#21270;&#20154;&#21475;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#26080;&#35770;&#26159;&#21521;&#27599;&#20010;&#20154;&#37117;&#26080;&#26465;&#20214;&#25552;&#20379;&#24110;&#21161;&#65292;&#21253;&#25324;&#21467;&#24466;&#65292;&#36824;&#26159;&#21482;&#24110;&#21161;&#34987;&#35748;&#20026;&#20540;&#24471;&#25110;&#21512;&#20316;&#30340;&#20154;&#30340;&#25746;&#39532;&#21033;&#20122;&#20154;&#24037;&#26234;&#33021;&#20195;&#29702;&#20154;&#65292;&#23545;&#20110;&#25512;&#21160;&#20154;&#31867;&#21512;&#20316;&#30340;&#27700;&#24179;&#26469;&#35828;&#65292;&#21518;&#32773;&#35201;&#27604;&#21069;&#32773;&#26356;&#26377;&#21033;&#65292;&#29305;&#21035;&#26159;&#22312;&#21464;&#21270;&#34987;&#35880;&#24910;&#25110;&#25269;&#25239;&#35270;&#20026;&#30340;&#32531;&#24930;&#21457;&#23637;&#30340;&#31038;&#20250;&#20013;&#65288;&#36873;&#25321;&#30340;&#24378;&#24230;&#36739;&#23567;&#65289;&#12290;&#30452;&#35266;&#19978;&#65292;&#22312;&#21464;&#21270;&#24555;&#36895;&#30340;&#31038;&#20250;&#20013;&#65288;&#36873;&#25321;&#30340;&#24378;&#24230;&#36739;&#39640;&#65289;&#65292;&#27495;&#35270;&#24615;&#20154;&#24037;&#26234;&#33021;&#21487;&#20197;&#26356;&#22909;&#22320;&#25512;&#21160;&#21512;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;
As artificial intelligence (AI) systems are increasingly embedded in our lives, their presence leads to interactions that shape our behaviour, decision-making, and social interactions. Existing theoretical research has primarily focused on human-to-human interactions, overlooking the unique dynamics triggered by the presence of AI. In this paper, resorting to methods from evolutionary game theory, we study how different forms of AI influence the evolution of cooperation in a human population playing the one-shot Prisoner's Dilemma game in both well-mixed and structured populations. We found that Samaritan AI agents that help everyone unconditionally, including defectors, can promote higher levels of cooperation in humans than Discriminatory AI that only help those considered worthy/cooperative, especially in slow-moving societies where change is viewed with caution or resistance (small intensities of selection). Intuitively, in fast-moving societies (high intensities of selection), Dis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;</title><link>http://arxiv.org/abs/2306.17256</link><description>&lt;p&gt;
&#20197;&#25552;&#31034;&#20026;&#22522;&#30784;&#30340;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17256
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#35299;&#20915;&#20010;&#24615;&#21270;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#65292;&#36890;&#36807;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#25552;&#20379;&#36866;&#29992;&#20110;&#21019;&#19994;&#20225;&#19994;&#21644;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#30340;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#22312;&#26681;&#25454;&#29992;&#25143;&#36807;&#21435;&#30340;&#34892;&#20026;&#24110;&#21161;&#29992;&#25143;&#21457;&#29616;&#19982;&#20854;&#20852;&#36259;&#30456;&#31526;&#30340;&#20449;&#24687;&#26041;&#38754;&#21457;&#25381;&#30528;&#20851;&#38190;&#20316;&#29992;&#12290;&#28982;&#32780;&#65292;&#24403;&#29992;&#25143;&#21644;&#29289;&#21697;&#20043;&#38388;&#30340;&#21382;&#21490;&#20132;&#20114;&#35760;&#24405;&#19981;&#21487;&#29992;&#26102;&#65292;&#24320;&#21457;&#20010;&#24615;&#21270;&#25512;&#33616;&#31995;&#32479;&#21464;&#24471;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#36825;&#23601;&#26159;&#25152;&#35859;&#30340;&#31995;&#32479;&#20919;&#21551;&#21160;&#25512;&#33616;&#38382;&#39064;&#12290;&#27492;&#38382;&#39064;&#22312;&#21019;&#19994;&#20225;&#19994;&#25110;&#29992;&#25143;&#21442;&#19982;&#21382;&#21490;&#19981;&#36275;&#30340;&#24179;&#21488;&#20013;&#23588;&#20026;&#31361;&#20986;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#29992;&#25143;&#25110;&#29289;&#21697;&#30340;&#20919;&#21551;&#21160;&#22330;&#26223;&#65292;&#20854;&#20013;&#31995;&#32479;&#20173;&#28982;&#36890;&#36807;&#22312;&#21516;&#19968;&#39046;&#22495;&#20013;&#30340;&#21382;&#21490;&#29992;&#25143;&#21644;&#29289;&#21697;&#20132;&#20114;&#36827;&#34892;&#35757;&#32451;&#26469;&#20026;&#26032;&#29992;&#25143;&#25110;&#29289;&#21697;&#25552;&#20379;&#25512;&#33616;&#65292;&#32780;&#26080;&#27861;&#35299;&#20915;&#25105;&#20204;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#24357;&#21512;&#36825;&#19968;&#40511;&#27807;&#65292;&#25105;&#20204;&#30340;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#21019;&#26032;&#19988;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#23558;&#25512;&#33616;&#36807;&#31243;&#36716;&#21270;&#20026;&#33258;&#28982;&#35821;&#35328;&#24773;&#24863;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#21547;&#29992;&#25143;&#36164;&#26009;&#21644;&#29289;&#21697;&#23646;&#24615;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems play a crucial role in helping users discover information that aligns with their interests based on their past behaviors. However, developing personalized recommendation systems becomes challenging when historical records of user-item interactions are unavailable, leading to what is known as the system cold-start recommendation problem. This issue is particularly prominent in start-up businesses or platforms with insufficient user engagement history. Previous studies focus on user or item cold-start scenarios, where systems could make recommendations for new users or items but are still trained with historical user-item interactions in the same domain, which cannot solve our problem. To bridge the gap, our research introduces an innovative and effective approach, capitalizing on the capabilities of pre-trained language models. We transform the recommendation process into sentiment analysis of natural languages containing information of user profiles and item attribu
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mapKurator&#30340;&#31995;&#32479;&#65292;&#33021;&#23436;&#25972;&#22320;&#20174;&#21382;&#21490;&#22320;&#22270;&#20013;&#25552;&#21462;&#21644;&#38142;&#25509;&#25991;&#26412;&#20449;&#24687;&#12290;&#35813;&#31995;&#32479;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#20301;&#32622;&#30456;&#20851;&#35789;&#35821;&#30340;&#24573;&#30053;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#32771;&#34385;&#26356;&#24191;&#30340;&#20027;&#39064;&#33539;&#22260;&#65292;&#33021;&#22815;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#12290;</title><link>http://arxiv.org/abs/2306.17059</link><description>&lt;p&gt;
The mapKurator&#31995;&#32479;&#65306;&#20174;&#21382;&#21490;&#22320;&#22270;&#20013;&#25552;&#21462;&#21644;&#38142;&#25509;&#25991;&#26412;&#30340;&#23436;&#25972;&#31649;&#36947;
&lt;/p&gt;
&lt;p&gt;
The mapKurator System: A Complete Pipeline for Extracting and Linking Text from Historical Maps. (arXiv:2306.17059v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17059
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;mapKurator&#30340;&#31995;&#32479;&#65292;&#33021;&#23436;&#25972;&#22320;&#20174;&#21382;&#21490;&#22320;&#22270;&#20013;&#25552;&#21462;&#21644;&#38142;&#25509;&#25991;&#26412;&#20449;&#24687;&#12290;&#35813;&#31995;&#32479;&#35299;&#20915;&#20102;&#20256;&#32479;&#26041;&#27861;&#20013;&#23545;&#20301;&#32622;&#30456;&#20851;&#35789;&#35821;&#30340;&#24573;&#30053;&#38382;&#39064;&#65292;&#24182;&#21033;&#29992;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#32771;&#34385;&#26356;&#24191;&#30340;&#20027;&#39064;&#33539;&#22260;&#65292;&#33021;&#22815;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#20855;&#26377;&#31354;&#38388;&#28966;&#28857;&#21644;&#26377;&#20215;&#20540;&#30340;&#22320;&#26041;&#29305;&#24449;&#12290;&#20363;&#22914;&#65292;&#25151;&#22320;&#20135;&#25110;&#26053;&#34892;&#21338;&#23458;&#20013;&#30340;&#21015;&#34920;&#25551;&#36848;&#21253;&#21547;&#26377;&#20851;&#29305;&#23450;&#22320;&#21306;&#31038;&#21306;&#30340;&#20449;&#24687;&#12290;&#36825;&#20123;&#20449;&#24687;&#23545;&#20110;&#25551;&#36848;&#20154;&#31867;&#22914;&#20309;&#24863;&#30693;&#20182;&#20204;&#30340;&#29615;&#22659;&#26159;&#26377;&#20215;&#20540;&#30340;&#12290;&#28982;&#32780;&#65292;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#30340;&#31532;&#19968;&#27493;&#26159;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#65288;&#20363;&#22914;&#65292;&#22478;&#24066;&#65289;&#12290;&#20256;&#32479;&#26041;&#27861;&#29992;&#20110;&#35782;&#21035;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#20381;&#36182;&#20110;&#20174;&#25991;&#26723;&#20013;&#26816;&#27979;&#21644;&#28040;&#27495;&#21270;&#22320;&#21517;&#12290;&#36825;&#31181;&#26041;&#27861;&#38656;&#35201;&#19968;&#20010;&#21253;&#21547;&#20301;&#32622;&#30701;&#35821;&#21644;&#20020;&#26102;&#35268;&#21017;&#30340;&#35789;&#27719;&#38598;&#65292;&#36825;&#20123;&#35268;&#21017;&#24573;&#30053;&#20102;&#19982;&#20301;&#32622;&#30456;&#20851;&#30340;&#37325;&#35201;&#35789;&#35821;&#12290;&#26368;&#36817;&#65292;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20027;&#39064;&#24314;&#27169;&#26041;&#27861;&#36890;&#24120;&#32771;&#34385;&#20960;&#20010;&#24191;&#24230;&#30340;&#20027;&#39064;&#12290;&#30456;&#27604;&#20043;&#19979;&#65292;&#25991;&#26723;&#30340;&#31354;&#38388;&#28966;&#28857;&#21487;&#20197;&#26159;&#19968;&#20010;&#22269;&#23478;&#12289;&#19968;&#20010;&#22478;&#24066;&#65292;&#29978;&#33267;&#26159;&#19968;&#20010;&#31038;&#21306;&#65292;&#36825;&#20123;&#33539;&#22260;&#27604;&#36825;&#20123;&#26041;&#27861;&#32771;&#34385;&#30340;&#20027;&#39064;&#25968;&#35201;&#22823;&#24471;&#22810;&#12290;
&lt;/p&gt;
&lt;p&gt;
Documents hold spatial focus and valuable locality characteristics. For example, descriptions of listings in real estate or travel blogs contain information about specific local neighborhoods. This information is valuable to characterize how humans perceive their environment. However, the first step to making use of this information is to identify the spatial focus (e.g., a city) of a document. Traditional approaches for identifying the spatial focus of a document rely on detecting and disambiguating toponyms from the document. This approach requires a vocabulary set of location phrases and ad-hoc rules, which ignore important words related to location. Recent topic modeling approaches using large language models often consider a few topics, each with broad coverage. In contrast, the spatial focus of a document can be a country, a city, or even a neighborhood, which together, is much larger than the number of topics considered in these approaches. Additionally, topic modeling methods a
&lt;/p&gt;</description></item><item><title>milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.17010</link><description>&lt;p&gt;
milliFlow&#65306;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#22330;&#26223;&#27969;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.17010
&lt;/p&gt;
&lt;p&gt;
milliFlow&#26159;&#19968;&#31181;&#29992;&#20110;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#30340;&#26032;&#22411;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#33021;&#22815;&#25552;&#20379;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#24182;&#30452;&#25509;&#29992;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#20013;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#26222;&#36866;&#35745;&#31639;&#26102;&#20195;&#30340;&#21040;&#26469;&#65292;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#22312;&#26234;&#33021;&#31995;&#32479;&#20013;&#36215;&#30528;&#20851;&#38190;&#20316;&#29992;&#65292;&#29992;&#20110;&#20915;&#31574;&#12289;&#29992;&#25143;&#20132;&#20114;&#21644;&#20010;&#24615;&#21270;&#26381;&#21153;&#12290;&#22312;&#20256;&#32479;&#26041;&#27861;&#20013;&#65292;&#20154;&#20307;&#36319;&#36394;&#12289;&#23039;&#21183;&#20272;&#35745;&#12289;&#25163;&#21183;&#35782;&#21035;&#21644;&#27963;&#21160;&#35782;&#21035;&#31561;&#26041;&#38754;&#36827;&#34892;&#20102;&#22823;&#37327;&#30740;&#31350;&#65292;&#36825;&#20123;&#26041;&#27861;&#20027;&#35201;&#22522;&#20110;&#25668;&#20687;&#26426;&#12290;&#28982;&#32780;&#65292;&#25668;&#20687;&#26426;&#30340;&#20405;&#20837;&#24615;&#29305;&#28857;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#26234;&#33021;&#23478;&#23621;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#27627;&#31859;&#27874;&#38647;&#36798;&#30001;&#20110;&#20854;&#20445;&#25252;&#38544;&#31169;&#30340;&#29305;&#28857;&#32780;&#21463;&#21040;&#27426;&#36814;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;milliFlow&#65292;&#29992;&#20110;&#23545;&#27627;&#31859;&#27874;&#38647;&#36798;&#28857;&#20113;&#36827;&#34892;&#22330;&#26223;&#27969;&#20272;&#35745;&#65292;&#20316;&#20026;&#20013;&#38388;&#23618;&#30340;&#29305;&#24449;&#65292;&#30452;&#25509;&#21463;&#30410;&#20110;&#19979;&#28216;&#30340;&#20154;&#20307;&#36816;&#21160;&#24863;&#30693;&#20219;&#21153;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20855;&#26377;&#20248;&#36234;&#30340;&#24615;&#33021;&#65292;&#24179;&#22343;3D&#31471;&#28857;&#35823;&#24046;&#20026;4.6cm&#65292;&#26126;&#26174;&#36229;&#36807;&#31454;&#20105;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#32467;&#21512;...
&lt;/p&gt;
&lt;p&gt;
Approaching the era of ubiquitous computing, human motion sensing plays a crucial role in smart systems for decision making, user interaction, and personalized services. Extensive research has been conducted on human tracking, pose estimation, gesture recognition, and activity recognition, which are predominantly based on cameras in traditional methods. However, the intrusive nature of cameras limits their use in smart home applications. To address this, mmWave radars have gained popularity due to their privacy-friendly features. In this work, we propose \textit{milliFlow}, a novel deep learning method for scene flow estimation as a complementary motion information for mmWave point cloud, serving as an intermediate level of features and directly benefiting downstream human motion sensing tasks. Experimental results demonstrate the superior performance of our method with an average 3D endpoint error of 4.6cm, significantly surpassing the competing approaches. Furthermore, by incorporati
&lt;/p&gt;</description></item><item><title>SRL&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#65292;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#26694;&#26550;&#32479;&#19968;&#20102;&#21508;&#31181;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;</title><link>http://arxiv.org/abs/2306.16688</link><description>&lt;p&gt;
SRL: &#23558;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#25193;&#23637;&#21040;&#19968;&#19975;&#22810;&#20010;&#26680;&#24515;
&lt;/p&gt;
&lt;p&gt;
SRL: Scaling Distributed Reinforcement Learning to Over Ten Thousand Cores. (arXiv:2306.16688v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.16688
&lt;/p&gt;
&lt;p&gt;
SRL&#26159;&#19968;&#20010;&#21487;&#25193;&#23637;&#65292;&#39640;&#25928;&#65292;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#30340;&#25277;&#35937;&#26694;&#26550;&#32479;&#19968;&#20102;&#21508;&#31181;&#23454;&#38469;&#24378;&#21270;&#23398;&#20064;&#35757;&#32451;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#65288;RL&#65289;&#20219;&#21153;&#30340;&#19981;&#26029;&#22797;&#26434;&#21270;&#35201;&#27714;&#20998;&#24067;&#24335;RL&#31995;&#32479;&#21487;&#20197;&#39640;&#25928;&#22320;&#29983;&#25104;&#21644;&#22788;&#29702;&#22823;&#37327;&#25968;&#25454;&#20197;&#35757;&#32451;&#26234;&#33021;Agent&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#24320;&#28304;&#24211;&#23384;&#22312;&#21508;&#31181;&#38480;&#21046;&#65292;&#38459;&#30861;&#20102;&#23427;&#20204;&#22312;&#38656;&#35201;&#22823;&#35268;&#27169;&#35757;&#32451;&#30340;&#25361;&#25112;&#24615;&#22330;&#26223;&#20013;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;&#34429;&#28982;OpenAI&#21644;DeepMind&#30340;&#24037;&#19994;&#31995;&#32479;&#24050;&#32463;&#25104;&#21151;&#23454;&#29616;&#20102;&#22823;&#35268;&#27169;RL&#35757;&#32451;&#65292;&#20294;&#26159;&#23427;&#20204;&#30340;&#31995;&#32479;&#26550;&#26500;&#21644;&#23454;&#29616;&#32454;&#33410;&#23545;&#31038;&#21306;&#26469;&#35828;&#20173;&#28982;&#19981;&#20844;&#24320;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;RL&#35757;&#32451;&#25968;&#25454;&#27969;&#30340;&#26032;&#25277;&#35937;&#65292;&#23558;&#21508;&#31181;&#24212;&#29992;&#20013;&#30340;&#23454;&#38469;RL&#35757;&#32451;&#32479;&#19968;&#25104;&#19968;&#20010;&#36890;&#29992;&#26694;&#26550;&#65292;&#24182;&#23454;&#29616;&#20102;&#31934;&#32454;&#20248;&#21270;&#12290;&#26681;&#25454;&#36825;&#20010;&#25277;&#35937;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#21487;&#25193;&#23637;&#12289;&#39640;&#25928;&#12289;&#21487;&#25193;&#23637;&#30340;&#20998;&#24067;&#24335;RL&#31995;&#32479;&#65292;&#21517;&#20026;"ReaLly Scalable RL&#65288;SRL&#65289;"&#12290;
&lt;/p&gt;
&lt;p&gt;
The ever-growing complexity of reinforcement learning (RL) tasks demands a distributed RL system to efficiently generate and process a massive amount of data to train intelligent agents. However, existing open-source libraries suffer from various limitations, which impede their practical use in challenging scenarios where large-scale training is necessary. While industrial systems from OpenAI and DeepMind have achieved successful large-scale RL training, their system architecture and implementation details remain undisclosed to the community. In this paper, we present a novel abstraction on the dataflows of RL training, which unifies practical RL training across diverse applications into a general framework and enables fine-grained optimizations. Following this abstraction, we develop a scalable, efficient, and extensible distributed RL system called ReaLly Scalable RL (SRL). The system architecture of SRL separates major RL computation components and allows massively parallelized trai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23637;&#24320;&#22411;&#21407;&#22987;-&#23545;&#20598;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32416;&#27491;&#24425;&#33394;&#22810;&#26222;&#21202;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20013;&#30340;&#30456;&#20301;&#21253;&#35065;&#20266;&#24433;&#65292;&#19982;&#20854;&#20182;&#26368;&#26032;&#20998;&#21106;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#31361;&#20986;&#12290;</title><link>http://arxiv.org/abs/2306.13695</link><description>&lt;p&gt;
&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#24425;&#33394;&#22810;&#26222;&#21202;&#24515;&#33039;&#36229;&#22768;&#30456;&#20301;&#23637;&#24320;&#25216;&#26415;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Phase Unwrapping of Color Doppler Echocardiography using Deep Learning. (arXiv:2306.13695v1 [eess.IV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.13695
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23637;&#24320;&#22411;&#21407;&#22987;-&#23545;&#20598;&#32593;&#32476;&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#32416;&#27491;&#24425;&#33394;&#22810;&#26222;&#21202;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#20013;&#30340;&#30456;&#20301;&#21253;&#35065;&#20266;&#24433;&#65292;&#19982;&#20854;&#20182;&#26368;&#26032;&#20998;&#21106;&#25216;&#26415;&#30456;&#27604;&#65292;&#35813;&#26041;&#27861;&#22312;&#24615;&#33021;&#19978;&#34920;&#29616;&#31361;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24425;&#33394;&#22810;&#26222;&#21202;&#24515;&#33039;&#36229;&#22768;&#26159;&#19968;&#31181;&#24191;&#27867;&#20351;&#29992;&#30340;&#38750;&#20405;&#20837;&#24615;&#25104;&#20687;&#25216;&#26415;&#65292;&#21487;&#20197;&#25552;&#20379;&#20851;&#20110;&#24515;&#33039;&#34880;&#27969;&#30340;&#23454;&#26102;&#20449;&#24687;&#12290;&#22312;&#24038;&#24515;&#23460;&#38271;&#36724;&#35270;&#22270;&#20013;&#65292;&#24425;&#33394;&#22810;&#26222;&#21202;&#23481;&#26131;&#20986;&#29616;&#30456;&#20301;&#21253;&#35065;&#29616;&#35937;&#65292;&#29305;&#21035;&#26159;&#22312;&#24515;&#33039;&#25910;&#32553;&#21644;&#33298;&#24352;&#26399;&#12290;&#24403;&#22522;&#20110;&#24425;&#33394;&#22810;&#26222;&#21202;&#30340;&#23450;&#37327;&#26041;&#27861;&#26102;&#65292;&#24517;&#39035;&#32416;&#27491;&#36825;&#31181;&#21253;&#35065;&#20266;&#24433;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#20010;&#22522;&#20110;&#23637;&#24320;&#22411;&#21407;&#22987;-&#23545;&#20598;&#32593;&#32476;&#30340;&#26041;&#27861;&#26469;&#35299;&#21253;(&#21435;&#20266;&#24433;)&#24425;&#33394;&#22810;&#26222;&#21202;&#24515;&#33039;&#36229;&#22768;&#22270;&#20687;&#65292;&#23558;&#20854;&#26377;&#25928;&#24615;&#19982;&#22522;&#20110;nnU-Net&#21644;Transformer&#27169;&#22411;&#30340;&#20004;&#31181;&#26368;&#26032;&#20998;&#21106;&#26041;&#27861;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#22312;&#33258;&#26377;&#25968;&#25454;&#38598;&#19978;&#23545;&#27599;&#31181;&#26041;&#27861;&#36827;&#34892;&#20102;&#35757;&#32451;&#21644;&#35780;&#20272;&#65292;&#24182;&#21457;&#29616;nnU-Net&#26041;&#27861;&#25552;&#20379;&#20102;&#26368;&#20339;&#30340;&#21435;&#20266;&#24433;&#32467;&#26524;&#65292;&#20854;&#27425;&#26159;&#23637;&#24320;&#22411;&#21407;&#22987;-&#23545;&#20598;&#26041;&#27861;&#21644;&#22522;&#20110;Transformer&#30340;&#25216;&#26415;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#23637;&#24320;&#22411;&#21407;&#22987;-&#23545;&#20598;&#32593;&#32476;&#25317;&#26377;&#26174;&#33879;&#26356;&#23569;&#30340;&#21487;&#35757;&#32451;&#21442;&#25968;&#65292;&#20294;&#24615;&#33021;&#20173;&#33021;&#19982;&#20854;&#20182;&#26041;&#27861;&#30456;&#23218;&#32654;&#12290;
&lt;/p&gt;
&lt;p&gt;
Color Doppler echocardiography is a widely used non-invasive imaging modality that provides real-time information about the intracardiac blood flow. In an apical long-axis view of the left ventricle, color Doppler is subject to phase wrapping, or aliasing, especially during cardiac filling and ejection. When setting up quantitative methods based on color Doppler, it is necessary to correct this wrapping artifact. We developed an unfolded primal-dual network to unwrap (dealias) color Doppler echocardiographic images and compared its effectiveness against two state-of-the-art segmentation approaches based on nnU-Net and transformer models. We trained and evaluated the performance of each method on an in-house dataset and found that the nnU-Net-based method provided the best dealiased results, followed by the primal-dual approach and the transformer-based technique. Noteworthy, the primal-dual network, which had significantly fewer trainable parameters, performed competitively with respec
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#32508;&#36848;&#20840;&#38754;&#24635;&#32467;&#20102;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#36235;&#21183;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#20998;&#31867;&#65292;&#30693;&#35782;&#25512;&#33616;&#31995;&#32479;&#65292;&#40065;&#26834;&#24615;&#65292;&#25968;&#25454;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#35780;&#20272;&#24230;&#37327;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26032;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2306.12680</link><description>&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#30340;&#26368;&#26032;&#21457;&#23637;&#65306;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Recent Developments in Recommender Systems: A Survey. (arXiv:2306.12680v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.12680
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#32508;&#36848;&#20840;&#38754;&#24635;&#32467;&#20102;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#21644;&#36235;&#21183;&#65292;&#21253;&#25324;&#25512;&#33616;&#31995;&#32479;&#20998;&#31867;&#65292;&#30693;&#35782;&#25512;&#33616;&#31995;&#32479;&#65292;&#40065;&#26834;&#24615;&#65292;&#25968;&#25454;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#20197;&#21450;&#35780;&#20272;&#24230;&#37327;&#12290;&#35813;&#30740;&#31350;&#36824;&#25552;&#20379;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25216;&#26415;&#32508;&#36848;&#20840;&#38754;&#24635;&#32467;&#20102;&#25512;&#33616;&#31995;&#32479;&#39046;&#22495;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#30340;&#26159;&#25552;&#20379;&#39046;&#22495;&#20869;&#29616;&#29366;&#30340;&#27010;&#36848;&#65292;&#24182;&#24378;&#35843;&#25512;&#33616;&#31995;&#32479;&#21457;&#23637;&#30340;&#26368;&#26032;&#36235;&#21183;&#12290;&#35813;&#30740;&#31350;&#39318;&#20808;&#20840;&#38754;&#24635;&#32467;&#20102;&#20027;&#35201;&#25512;&#33616;&#31995;&#32479;&#20998;&#31867;&#26041;&#27861;&#65292;&#21253;&#25324;&#20010;&#24615;&#21270;&#21644;&#32676;&#32452;&#25512;&#33616;&#31995;&#32479;&#65292;&#28982;&#21518;&#28145;&#20837;&#25506;&#35752;&#20102;&#22522;&#20110;&#30693;&#35782;&#30340;&#25512;&#33616;&#31995;&#32479;&#31867;&#21035;&#12290;&#27492;&#22806;&#65292;&#35813;&#32508;&#36848;&#20998;&#26512;&#20102;&#25512;&#33616;&#31995;&#32479;&#20013;&#30340;&#40065;&#26834;&#24615;&#12289;&#25968;&#25454;&#20559;&#35265;&#21644;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24182;&#24635;&#32467;&#20102;&#35780;&#20272;&#24230;&#37327;&#29992;&#20110;&#35780;&#20272;&#36825;&#20123;&#31995;&#32479;&#30340;&#24615;&#33021;&#12290;&#26368;&#21518;&#65292;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#20851;&#25512;&#33616;&#31995;&#32479;&#21457;&#23637;&#30340;&#26368;&#26032;&#36235;&#21183;&#30340;&#35265;&#35299;&#65292;&#24182;&#24378;&#35843;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26032;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this technical survey, we comprehensively summarize the latest advancements in the field of recommender systems. The objective of this study is to provide an overview of the current state-of-the-art in the field and highlight the latest trends in the development of recommender systems. The study starts with a comprehensive summary of the main taxonomy of recommender systems, including personalized and group recommender systems, and then delves into the category of knowledge-based recommender systems. In addition, the survey analyzes the robustness, data bias, and fairness issues in recommender systems, summarizing the evaluation metrics used to assess the performance of these systems. Finally, the study provides insights into the latest trends in the development of recommender systems and highlights the new directions for future research in the field.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GrAMMI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#19979;&#24314;&#27169;&#23545;&#25239;&#23545;&#25163;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#30446;&#26631;&#26469;&#39044;&#27979;&#23545;&#25239;&#23545;&#25163;&#30340;&#24403;&#21069;&#21644;&#26410;&#26469;&#29366;&#24577;&#65292;GrAMMI&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#36861;&#36880;-&#36867;&#36991;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2306.11168</link><description>&lt;p&gt;
&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#19979;&#23398;&#20064;&#23545;&#25239;&#20195;&#29702;&#34892;&#20026;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Models of Adversarial Agent Behavior under Partial Observability. (arXiv:2306.11168v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.11168
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;GrAMMI&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#37096;&#20998;&#21487;&#35266;&#27979;&#24615;&#19979;&#24314;&#27169;&#23545;&#25239;&#23545;&#25163;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#26368;&#22823;&#21270;&#20114;&#20449;&#24687;&#20316;&#20026;&#36741;&#21161;&#30446;&#26631;&#26469;&#39044;&#27979;&#23545;&#25239;&#23545;&#25163;&#30340;&#24403;&#21069;&#21644;&#26410;&#26469;&#29366;&#24577;&#65292;GrAMMI&#22312;&#20004;&#20010;&#22823;&#35268;&#27169;&#36861;&#36880;-&#36867;&#36991;&#39046;&#22495;&#20013;&#34920;&#29616;&#20986;&#20102;&#26174;&#33879;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#65292;&#27604;&#22914;&#32844;&#19994;&#20307;&#32946;&#12289;&#35270;&#39057;&#28216;&#25103;&#35774;&#35745;&#21644;&#27602;&#21697;&#25130;&#33719;&#20013;&#65292;&#23545;&#25163;&#24314;&#27169;&#21644;&#36319;&#36394;&#30340;&#38656;&#27714;&#36234;&#26469;&#36234;&#22810;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#30340;&#23545;&#25239;&#24314;&#27169;&#26041;&#27861;&#65292;&#21517;&#20026;GrAMMI&#65292;&#29992;&#20110;&#24314;&#27169;&#23545;&#25239;&#23545;&#25163;&#20195;&#29702;&#30340;&#34892;&#20026;&#12290;GrAMMI&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;(GNN)&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#20114;&#20449;&#24687;&#26368;&#22823;&#21270;&#20316;&#20026;&#36741;&#21161;&#30446;&#26631;&#26469;&#39044;&#27979;&#37096;&#20998;&#21487;&#35266;&#27979;&#30340;&#23545;&#25239;&#23545;&#25163;&#30340;&#24403;&#21069;&#21644;&#26410;&#26469;&#29366;&#24577;&#12290;&#20026;&#20102;&#35780;&#20272;GrAMMI&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#20004;&#20010;&#22823;&#35268;&#27169;&#30340;&#36861;&#36880;-&#36867;&#36991;&#39046;&#22495;&#65292;&#28789;&#24863;&#26469;&#33258;&#20110;&#30495;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#65292;&#19968;&#20010;&#24322;&#26500;&#20195;&#29702;&#22242;&#38431;&#30340;&#20219;&#21153;&#26159;&#36861;&#36394;&#21644;&#25130;&#33719;&#19968;&#20010;&#23545;&#25239;&#23545;&#25163;&#20195;&#29702;&#65292;&#32780;&#23545;&#25239;&#23545;&#25163;&#20195;&#29702;&#24517;&#39035;&#22312;&#21516;&#26102;&#36798;&#21040;&#33258;&#24049;&#30446;&#26631;&#30340;&#24773;&#20917;&#19979;&#36867;&#36991;&#34987;&#21457;&#29616;&#12290;&#36890;&#36807;&#20114;&#20449;&#24687;&#30340;&#24418;&#24335;&#21270;&#65292;GrAMMI&#22312;&#20004;&#20010;&#39046;&#22495;&#20013;&#22343;&#20248;&#20110;&#25152;&#26377;&#22522;&#32447;&#65292;&#24182;&#23454;&#29616;&#20102;&#26410;&#26469;&#23545;&#25239;&#30340;&#24179;&#22343;&#23545;&#25968;&#20284;&#28982;&#20540;&#25552;&#39640;&#20102;31.68%&#12290;
&lt;/p&gt;
&lt;p&gt;
The need for opponent modeling and tracking arises in several real-world scenarios, such as professional sports, video game design, and drug-trafficking interdiction. In this work, we present Graph based Adversarial Modeling with Mutal Information (GrAMMI) for modeling the behavior of an adversarial opponent agent. GrAMMI is a novel graph neural network (GNN) based approach that uses mutual information maximization as an auxiliary objective to predict the current and future states of an adversarial opponent with partial observability. To evaluate GrAMMI, we design two large-scale, pursuit-evasion domains inspired by real-world scenarios, where a team of heterogeneous agents is tasked with tracking and interdicting a single adversarial agent, and the adversarial agent must evade detection while achieving its own objectives. With the mutual information formulation, GrAMMI outperforms all baselines in both domains and achieves 31.68% higher log-likelihood on average for future adversarial
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2306.09459</link><description>&lt;p&gt;
&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;
&lt;/p&gt;
&lt;p&gt;
Recurrent Memory Decision Transformer. (arXiv:2306.09459v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.09459
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#22788;&#29702;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#38271;&#24207;&#21015;&#38382;&#39064;&#12290;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#37319;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;RMDT&#27169;&#22411;&#26174;&#30528;&#20248;&#20110;&#20854;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21464;&#38761;&#24615;&#27169;&#22411;&#26368;&#21021;&#26159;&#20026;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#32780;&#24320;&#21457;&#30340;&#65292;&#26368;&#36817;&#22312;&#31163;&#32447;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#20013;&#24471;&#21040;&#24191;&#27867;&#24212;&#29992;&#12290;&#36825;&#26159;&#22240;&#20026;&#20195;&#29702;&#30340;&#21382;&#21490;&#21487;&#20197;&#34920;&#31034;&#20026;&#24207;&#21015;&#65292;&#24182;&#19988;&#25972;&#20010;&#20219;&#21153;&#21487;&#20197;&#32553;&#20943;&#20026;&#24207;&#21015;&#24314;&#27169;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#21464;&#21387;&#22120;&#25805;&#20316;&#30340;&#20108;&#27425;&#22797;&#26434;&#24615;&#38480;&#21046;&#20102;&#19978;&#19979;&#25991;&#30340;&#28508;&#22312;&#22686;&#21152;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#22312;&#33258;&#28982;&#35821;&#35328;&#20013;&#22788;&#29702;&#38271;&#24207;&#21015;&#65292;&#20351;&#29992;&#20102;&#19981;&#21516;&#29256;&#26412;&#30340;&#35760;&#24518;&#26426;&#21046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#24490;&#29615;&#35760;&#24518;&#20915;&#31574;&#21464;&#21387;&#22120;&#65288;RMDT&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22312;&#24378;&#21270;&#23398;&#20064;&#38382;&#39064;&#20013;&#20351;&#29992;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;Atari&#28216;&#25103;&#21644;MoJoCo&#25511;&#21046;&#38382;&#39064;&#19978;&#36827;&#34892;&#20102;&#24443;&#24213;&#30340;&#23454;&#39564;&#65292;&#24182;&#34920;&#26126;&#25105;&#20204;&#25552;&#20986;&#30340;&#27169;&#22411;&#22312;Atari&#28216;&#25103;&#19978;&#26174;&#30528;&#20248;&#20110;&#27809;&#26377;&#24490;&#29615;&#35760;&#24518;&#26426;&#21046;&#30340;&#23545;&#24212;&#27169;&#22411;&#12290;&#25105;&#20204;&#36824;&#20180;&#32454;&#30740;&#31350;&#20102;&#35760;&#24518;&#23545;&#25152;&#25552;&#20986;&#30340;&#27169;&#22411;&#32489;&#25928;&#30340;&#24433;&#21709;&#12290;&#36825;&#20123;&#21457;&#29616;&#20026;&#24320;&#21457;&#26356;&#39640;&#25928;&#21644;&#26356;&#26377;&#25928;&#30340;&#22788;&#29702;&#38271;&#24207;&#21015;&#30340;&#24378;&#21270;&#23398;&#20064;&#27169;&#22411;&#25552;&#20379;&#20102;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Transformative models, originally developed for natural language problems, have recently been widely used in offline reinforcement learning tasks. This is due to the fact that the agent's history can be represented as a sequence, and the whole task can be reduced to the sequence modeling task. However, the quadratic complexity of the transformer operation limits the potential increase in context. Therefore, to work with long sequences in a natural language, different versions of the memory mechanism are used. In this paper, we propose the Recurrent Memory Decision Transformer (RMDT), a model that uses a recurrent memory mechanism for reinforcement learning problems. We conduct thorough experiments on Atari games and MoJoCo control problems, and show that our proposed model is significantly superior to its counterparts without the recurrent memory mechanism on Atari games. We also carefully study the effect of memory on the performance of the proposed model. These findings shed light on
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#32771;&#34385;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36235;&#21183;&#12290;</title><link>http://arxiv.org/abs/2306.08149</link><description>&lt;p&gt;
&#38750;&#32447;&#24615;&#20010;&#24615;&#21270;&#39044;&#27979;&#30340;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Neural Mixed Effects for Nonlinear Personalized Predictions. (arXiv:2306.08149v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.08149
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#29992;&#20110;&#20010;&#24615;&#21270;&#39044;&#27979;&#65292;&#24182;&#36890;&#36807;&#32467;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#32771;&#34385;&#32447;&#24615;&#21644;&#38750;&#32447;&#24615;&#36235;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20010;&#24615;&#21270;&#39044;&#27979;&#26159;&#19968;&#31181;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#26681;&#25454;&#36807;&#21435;&#26631;&#35760;&#35266;&#27979;&#39044;&#27979;&#19968;&#20010;&#20154;&#26410;&#26469;&#30340;&#35266;&#27979;&#20540;&#65292;&#36890;&#24120;&#29992;&#20110;&#36830;&#32493;&#20219;&#21153;&#65292;&#20363;&#22914;&#39044;&#27979;&#26085;&#24120;&#24773;&#32490;&#35780;&#20998;&#12290;&#22312;&#36827;&#34892;&#20010;&#24615;&#21270;&#39044;&#27979;&#26102;&#65292;&#27169;&#22411;&#21487;&#20197;&#32467;&#21512;&#20004;&#31181;&#36235;&#21183;&#65306;&#65288;a&#65289;&#36328;&#20154;&#20849;&#20139;&#30340;&#36235;&#21183;&#65292;&#21363;&#20010;&#20154;&#36890;&#29992;&#36235;&#21183;&#65292;&#20363;&#22914;&#21608;&#26411;&#26356;&#24320;&#24515;&#65292;&#21644;&#65288;b&#65289;&#27599;&#20010;&#20154;&#29420;&#29305;&#30340;&#36235;&#21183;&#65292;&#21363;&#20010;&#20154;&#29305;&#23450;&#30340;&#36235;&#21183;&#65292;&#20363;&#22914;&#27599;&#21608;&#26377;&#19968;&#27425;&#21387;&#21147;&#22823;&#30340;&#20250;&#35758;&#12290;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#26159;&#19968;&#31181;&#27969;&#34892;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#29992;&#20110;&#36890;&#36807;&#32452;&#21512;&#20010;&#20154;&#36890;&#29992;&#21644;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#26469;&#30740;&#31350;&#36825;&#20004;&#31181;&#36235;&#21183;&#12290;&#23613;&#31649;&#29616;&#22312;&#32447;&#24615;&#28151;&#21512;&#25928;&#24212;&#27169;&#22411;&#36890;&#36807;&#23558;&#20854;&#19982;&#31070;&#32463;&#32593;&#32476;&#25972;&#21512;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#27969;&#34892;&#65292;&#20294;&#36825;&#31181;&#25972;&#21512;&#30446;&#21069;&#20165;&#38480;&#20110;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#65306;&#25490;&#38500;&#38750;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#36235;&#21183;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#31070;&#32463;&#28151;&#21512;&#25928;&#24212;&#65288;NME&#65289;&#27169;&#22411;&#65292;&#20197;&#20248;&#21270;&#38750;&#32447;&#24615;&#20010;&#20154;&#29305;&#23450;&#21442;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Personalized prediction is a machine learning approach that predicts a person's future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#32676;&#31561;&#21464;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#20301;&#32622;&#32534;&#30721;&#25805;&#20316;&#35299;&#20915;&#20102;&#35270;&#35273;Transformer&#20013;&#30340;&#31561;&#21464;&#24615;&#23398;&#20064;&#38590;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26126;&#26174;&#20248;&#20110;&#38750;&#31561;&#21464;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2306.06722</link><description>&lt;p&gt;
$E(2)$-&#31561;&#21464;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
$E(2)$-Equivariant Vision Transformer. (arXiv:2306.06722v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.06722
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#35774;&#35745;&#20102;&#19968;&#20010;&#32676;&#31561;&#21464;&#35270;&#35273;Transformer&#65292;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#20301;&#32622;&#32534;&#30721;&#25805;&#20316;&#35299;&#20915;&#20102;&#35270;&#35273;Transformer&#20013;&#30340;&#31561;&#21464;&#24615;&#23398;&#20064;&#38590;&#39064;&#65292;&#24182;&#36890;&#36807;&#23454;&#39564;&#35777;&#26126;&#20102;&#20854;&#26126;&#26174;&#20248;&#20110;&#38750;&#31561;&#21464;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#65288;ViT&#65289;&#22312;&#35745;&#31639;&#26426;&#35270;&#35273;&#39046;&#22495;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;ViT&#20013;&#30340;&#20301;&#32622;&#32534;&#30721;&#20351;&#24471;&#23398;&#20064;&#25968;&#25454;&#30340;&#20869;&#22312;&#31561;&#21464;&#24615;&#21464;&#24471;&#38750;&#24120;&#22256;&#38590;&#12290;&#26412;&#25991;&#23545;&#35774;&#35745;&#30340;&#31561;&#21464;ViT&#36827;&#34892;&#20102;&#21021;&#27493;&#23581;&#35797;&#65292;&#20294;&#35777;&#26126;&#22312;&#26576;&#20123;&#24773;&#20917;&#19979;&#23384;&#22312;&#32570;&#38519;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#26032;&#39062;&#26377;&#25928;&#30340;&#20301;&#32622;&#32534;&#30721;&#25805;&#20316;&#35774;&#35745;&#20102;&#19968;&#20010;&#32676;&#31561;&#21464;&#35270;&#35273;Transformer&#65288;GE-ViT&#65289;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;GE-ViT&#28385;&#36275;&#31561;&#21464;&#31070;&#32463;&#32593;&#32476;&#30340;&#25152;&#26377;&#29702;&#35770;&#35201;&#27714;&#12290;&#22312;&#26631;&#20934;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;GE-ViT&#26126;&#26174;&#20248;&#20110;&#38750;&#31561;&#21464;&#30340;&#33258;&#27880;&#24847;&#21147;&#32593;&#32476;&#12290;&#20195;&#30721;&#21487;&#22312;https://github.com/ZJUCDSYangKaifan/GEVit&#20013;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformer (ViT) has achieved remarkable performance in computer vision. However, positional encoding in ViT makes it substantially difficult to learn the intrinsic equivariance in data. Initial attempts have been made on designing equivariant ViT but are proved defective in some cases in this paper. To address this issue, we design a Group Equivariant Vision Transformer (GE-ViT) via a novel, effective positional encoding operator. We prove that GE-ViT meets all the theoretical requirements of an equivariant neural network. Comprehensive experiments are conducted on standard benchmark datasets, demonstrating that GE-ViT significantly outperforms non-equivariant self-attention networks. The code is available at https://github.com/ZJUCDSYangKaifan/GEVit.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;</title><link>http://arxiv.org/abs/2306.04719</link><description>&lt;p&gt;
&#19981;&#35201;&#30456;&#20449;&#20320;&#30340;&#30524;&#30555;&#65306;&#20851;&#20110;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#65288;&#19981;&#65289;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Don't trust your eyes: on the (un)reliability of feature visualizations. (arXiv:2306.04719v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.04719
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#38382;&#39064;&#65292;&#24182;&#30740;&#31350;&#20102;&#29305;&#24449;&#21487;&#35270;&#21270;&#30340;&#21487;&#38752;&#24615;&#12290;&#23454;&#39564;&#35777;&#25454;&#34920;&#26126;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#29305;&#24449;&#21487;&#35270;&#21270;&#33021;&#22815;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#38750;&#24120;&#26377;&#38480;&#65292;&#23545;&#20110;&#35299;&#37322;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#26159;&#22914;&#20309;&#20174;&#20687;&#32032;&#20013;&#25552;&#21462;&#27169;&#24335;&#30340;&#65311;&#29305;&#24449;&#21487;&#35270;&#21270;&#36890;&#36807;&#20248;&#21270;&#26469;&#21487;&#35270;&#21270;&#39640;&#28608;&#27963;&#30340;&#27169;&#24335;&#65292;&#35797;&#22270;&#22238;&#31572;&#36825;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22914;&#20170;&#65292;&#21487;&#35270;&#21270;&#26041;&#27861;&#26500;&#25104;&#20102;&#25105;&#20204;&#23545;&#31070;&#32463;&#32593;&#32476;&#20869;&#37096;&#24037;&#20316;&#30340;&#20102;&#35299;&#30340;&#22522;&#30784;&#65292;&#20316;&#20026;&#19968;&#31181;&#26426;&#26800;&#24335;&#30340;&#21487;&#35299;&#37322;&#24615;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#38382;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#26377;&#22810;&#21487;&#38752;&#65311;&#25105;&#20204;&#36890;&#36807;&#24320;&#21457;&#32593;&#32476;&#30005;&#36335;&#26469;&#35784;&#39575;&#29305;&#24449;&#21487;&#35270;&#21270;&#65292;&#20351;&#20854;&#26174;&#31034;&#23436;&#20840;&#19982;&#33258;&#28982;&#36755;&#20837;&#30340;&#27491;&#24120;&#32593;&#32476;&#34892;&#20026;&#27627;&#26080;&#32852;&#31995;&#30340;&#20219;&#24847;&#27169;&#24335;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#35777;&#25454;&#34920;&#26126;&#22312;&#26631;&#20934;&#65292;&#26410;&#25805;&#32437;&#32593;&#32476;&#20013;&#21457;&#29983;&#20102;&#31867;&#20284;&#30340;&#29616;&#35937;&#65306;&#29305;&#24449;&#21487;&#35270;&#21270;&#19982;&#26631;&#20934;&#36755;&#20837;&#22788;&#29702;&#38750;&#24120;&#19981;&#21516;&#65292;&#23545;&#31070;&#32463;&#32593;&#32476;&#22914;&#20309;&#22788;&#29702;&#33258;&#28982;&#22270;&#20687;&#30340;&#35299;&#37322;&#33021;&#21147;&#20135;&#29983;&#24576;&#30097;&#12290;&#25105;&#20204;&#36890;&#36807;&#29702;&#35770;&#35777;&#26126;&#25903;&#25745;&#36825;&#19968;&#32463;&#39564;&#21457;&#29616;&#65292;&#30001;&#20110;&#20248;&#21270;&#36807;&#31243;&#20013;&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#21487;&#20197;&#36890;&#36807;&#29305;&#24449;&#21487;&#35270;&#21270;&#21487;&#38752;&#29702;&#35299;&#30340;&#21151;&#33021;&#38598;&#26497;&#20854;&#26377;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do neural networks extract patterns from pixels? Feature visualizations attempt to answer this important question by visualizing highly activating patterns through optimization. Today, visualization methods form the foundation of our knowledge about the internal workings of neural networks, as a type of mechanistic interpretability. Here we ask: How reliable are feature visualizations? We start our investigation by developing network circuits that trick feature visualizations into showing arbitrary patterns that are completely disconnected from normal network behavior on natural input. We then provide evidence for a similar phenomenon occurring in standard, unmanipulated networks: feature visualizations are processed very differently from standard input, casting doubt on their ability to "explain" how neural networks process natural images. We underpin this empirical finding by theory proving that the set of functions that can be reliably understood by feature visualization is extr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21830;&#19994;&#29615;&#22659;&#30340;&#12289;&#33021;&#22815;&#24179;&#34913;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102; $\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$ &#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#23545;&#35805;&#31995;&#32479;&#21709;&#24212;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2306.03361</link><description>&lt;p&gt;
&#35774;&#35745;&#29992;&#25143;&#35282;&#33394;&#24863;&#30693;&#30340;&#23545;&#35805;&#20195;&#29702;&#36827;&#34892;&#26377;&#36259;&#30340;&#23545;&#35805;&#65306;$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground
&lt;/p&gt;
&lt;p&gt;
$\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue. (arXiv:2306.03361v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2306.03361
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38024;&#23545;&#21830;&#19994;&#29615;&#22659;&#30340;&#12289;&#33021;&#22815;&#24179;&#34913;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#30340;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#26041;&#27861;&#65292;&#36890;&#36807;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#65292;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#35299;&#20915;&#20102; $\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$ &#31561;&#38382;&#39064;&#65292;&#21516;&#26102;&#25552;&#39640;&#20102;&#23545;&#35805;&#31995;&#32479;&#21709;&#24212;&#30340;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24314;&#31435;&#20010;&#24615;&#21270;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20197;&#35299;&#20915;&#21830;&#19994;&#35774;&#32622;&#20013;&#28041;&#21450;&#20010;&#24615;&#21270;&#23545;&#35805;&#21709;&#24212;&#19982;&#38750;&#27491;&#24335;&#21709;&#24212;&#20132;&#26367;&#30340;$\textit{WWH}$&#65288;$\textit{WHAT}$&#12289;$\textit{WHEN}$&#21644;$\textit{HOW}$&#65289;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#28041;&#21450;&#21152;&#26435;&#25968;&#25454;&#38598;&#28151;&#21512;&#12289;&#36127;&#35282;&#33394;&#20449;&#24687;&#22686;&#24378;&#26041;&#27861;&#20197;&#21450;&#35774;&#35745;&#20010;&#24615;&#21270;&#23545;&#35805;&#25968;&#25454;&#38598;&#65292;&#20197;&#24212;&#23545;&#20010;&#24615;&#21270;&#12289;&#24320;&#25918;&#39046;&#22495;&#23545;&#35805;&#31995;&#32479;&#20013;$\textit{WWH}$&#30340;&#25361;&#25112;&#12290;&#26412;&#25991;&#26377;&#25928;&#22320;&#24179;&#34913;&#20102;&#23545;&#35805;&#27969;&#30021;&#24615;&#21644;&#36235;&#21521;&#20110;&#29702;&#35299;&#23545;&#35805;&#31995;&#32479;&#65292;&#21516;&#26102;&#36824;&#24341;&#20837;&#20102;&#21709;&#24212;&#31867;&#22411;&#26631;&#31614;&#26469;&#25552;&#39640;&#21487;&#25511;&#24615;&#21644;&#35299;&#37322;&#24615;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#32452;&#21512;&#23548;&#33268;&#20102;&#26356;&#21152;&#27969;&#30021;&#30340;&#23545;&#35805;&#65292;&#35777;&#26126;&#20102;&#22522;&#20110;&#20027;&#35266;&#20154;&#31867;&#35780;&#20272;&#21644;&#23458;&#35266;&#35780;&#20272;&#30340;&#23454;&#39564;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a method for building a personalized open-domain dialogue system to address the $\textit{WWH}$ ($\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$) problem for natural response generation in a commercial setting, where personalized dialogue responses are heavily interleaved with casual response turns. The proposed approach involves weighted dataset blending, negative persona information augmentation methods, and the design of personalized conversation datasets to address the challenges of $\textit{WWH}$ in personalized, open-domain dialogue systems. Our work effectively balances dialogue fluency and tendency to ground, while also introducing a response-type label to improve the controllability and explainability of the grounded responses. The combination of these methods leads to more fluent conversations, as evidenced by subjective human evaluations as well as objective evaluations.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#26080;&#38656;&#27880;&#37322;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#29983;&#25104;&#38899;&#35270;&#39057;&#20998;&#21106;&#20219;&#21153;&#30340;&#20154;&#24037;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#38899;&#39057;&#24863;&#30693;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;Transformer&#35299;&#30721;&#22120;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#38899;&#39057;&#20449;&#21495;&#30340;&#25351;&#23548;&#19979;&#25628;&#32034;&#22768;&#38899;&#23545;&#35937;&#65292;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;</title><link>http://arxiv.org/abs/2305.11019</link><description>&lt;p&gt;
&#26080;&#26631;&#27880;&#38899;&#35270;&#39057;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Annotation-free Audio-Visual Segmentation. (arXiv:2305.11019v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.11019
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#26080;&#38656;&#27880;&#37322;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#29983;&#25104;&#38899;&#35270;&#39057;&#20998;&#21106;&#20219;&#21153;&#30340;&#20154;&#24037;&#25968;&#25454;&#65292;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#38899;&#39057;&#24863;&#30693;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;Transformer&#35299;&#30721;&#22120;&#65292;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#38899;&#39057;&#20449;&#21495;&#30340;&#25351;&#23548;&#19979;&#25628;&#32034;&#22768;&#38899;&#23545;&#35937;&#65292;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#35270;&#39057;&#20998;&#21106;&#30340;&#30446;&#26631;&#26159;&#36890;&#36807;&#20934;&#30830;&#22320;&#39044;&#27979;&#20687;&#32032;&#32423;&#20998;&#21106;&#25513;&#30721;&#22312;&#35270;&#35273;&#22330;&#26223;&#20013;&#23450;&#20301;&#22768;&#38899;&#23545;&#35937;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20197;&#19979;&#36129;&#29486;&#65306;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20280;&#32553;&#19988;&#26080;&#38656;&#27880;&#37322;&#30340;&#31649;&#36947;&#65292;&#29992;&#20110;&#29983;&#25104;&#38899;&#35270;&#39057;&#20998;&#21106;&#20219;&#21153;&#30340;&#20154;&#24037;&#25968;&#25454;&#12290;&#25105;&#20204;&#21033;&#29992;&#29616;&#26377;&#30340;&#22270;&#20687;&#20998;&#21106;&#21644;&#38899;&#39057;&#25968;&#25454;&#38598;&#65292;&#24314;&#31435;&#31867;&#21035;&#26631;&#31614;&#12289;&#22270;&#20687;&#25513;&#27169;&#23545;&#21644;&#38899;&#39057;&#26679;&#26412;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#20174;&#32780;&#21487;&#20197;&#36731;&#26494;&#32452;&#21512;&#35757;&#32451;AVS&#27169;&#22411;&#30340;&#65288;&#22270;&#20687;&#12289;&#38899;&#39057;&#12289;&#25513;&#27169;&#65289;&#19977;&#20803;&#32452;&#65307;&#65288;ii&#65289;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38899;&#39057;&#24863;&#30693;&#21464;&#21387;&#22120;&#65288;AuTR&#65289;&#26550;&#26500;&#65292;&#20854;&#20013;&#21253;&#21547;&#19968;&#20010;&#38899;&#39057;&#24863;&#30693;&#30340;&#22522;&#20110;&#26597;&#35810;&#30340;Transformer&#35299;&#30721;&#22120;&#12290;&#35813;&#26550;&#26500;&#20351;&#27169;&#22411;&#33021;&#22815;&#22312;&#38899;&#39057;&#20449;&#21495;&#30340;&#25351;&#23548;&#19979;&#25628;&#32034;&#22768;&#38899;&#23545;&#35937;&#65292;&#20174;&#32780;&#24471;&#21040;&#26356;&#20934;&#30830;&#30340;&#20998;&#21106;&#65307;&#65288;iii&#65289;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#20351;&#29992;&#25105;&#20204;&#30340;&#31649;&#36947;&#29983;&#25104;&#30340;&#21512;&#25104;&#25968;&#25454;&#35757;&#32451;AVS&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The objective of Audio-Visual Segmentation (AVS) is to locate sounding objects within visual scenes by accurately predicting pixelwise segmentation masks. In this paper, we present the following contributions: (i), we propose a scalable and annotation-free pipeline for generating artificial data for the AVS task. We leverage existing image segmentation and audio datasets to draw links between category labels, image-mask pairs, and audio samples, which allows us to easily compose (image, audio, mask) triplets for training AVS models; (ii), we introduce a novel Audio-Aware Transformer (AuTR) architecture that features an audio-aware query-based transformer decoder. This architecture enables the model to search for sounding objects with the guidance of audio signals, resulting in more accurate segmentation; (iii), we present extensive experiments conducted on both synthetic and real datasets, which demonstrate the effectiveness of training AVS models with synthetic data generated by our p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;MCTS/THTS&#31639;&#27861;GreedyUCT-Normal&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#37319;&#29992;&#22870;&#21169;&#21464;&#21270;&#30340;&#23610;&#24230;&#22788;&#29702;&#19981;&#21516;&#23610;&#24230;&#30340;&#20998;&#24067;&#65292;&#20197;&#22312;&#32463;&#20856;&#35745;&#21010;&#20013;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2305.09840</link><description>&lt;p&gt;
&#32463;&#20856;&#35268;&#21010;&#20013;&#25506;&#32034;&#21644;&#24320;&#21457;&#30340;&#33258;&#36866;&#24212;&#24179;&#34913;
&lt;/p&gt;
&lt;p&gt;
Scale-Adaptive Balancing of Exploration and Exploitation in Classical Planning. (arXiv:2305.09840v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.09840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;MCTS/THTS&#31639;&#27861;GreedyUCT-Normal&#65292;&#35813;&#31639;&#27861;&#33021;&#22815;&#36890;&#36807;&#37319;&#29992;&#22870;&#21169;&#21464;&#21270;&#30340;&#23610;&#24230;&#22788;&#29702;&#19981;&#21516;&#23610;&#24230;&#30340;&#20998;&#24067;&#65292;&#20197;&#22312;&#32463;&#20856;&#35745;&#21010;&#20013;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#28216;&#25103;&#26641;&#25628;&#32034;&#21644;&#33258;&#21160;&#21270;&#35268;&#21010;&#20013;&#65292;&#24179;&#34913;&#25506;&#32034;&#21644;&#24320;&#21457;&#19968;&#30452;&#26159;&#19968;&#20010;&#37325;&#35201;&#30340;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#34429;&#28982;&#36825;&#20010;&#38382;&#39064;&#22312;&#22810;&#33218;&#36172;&#21338;&#26426;&#65288;MAB&#65289;&#25991;&#29486;&#20013;&#24050;&#32463;&#34987;&#24191;&#27867;&#20998;&#26512;&#65292;&#20294;&#35268;&#21010;&#31038;&#21306;&#22312;&#35797;&#22270;&#24212;&#29992;&#36825;&#20123;&#32467;&#26524;&#26102;&#21462;&#24471;&#30340;&#25104;&#21151;&#26377;&#38480;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;MAB&#25991;&#29486;&#26356;&#35814;&#32454;&#30340;&#29702;&#35770;&#29702;&#35299;&#26377;&#21161;&#20110;&#25913;&#36827;&#22522;&#20110;&#33945;&#29305;&#21345;&#32599;&#26641;&#25628;&#32034;&#65288;MCTS&#65289;/&#22522;&#20110;&#35797;&#39564;&#30340;&#21551;&#21457;&#24335;&#26641;&#25628;&#32034;&#65288;THTS&#65289;&#30340;&#29616;&#26377;&#35268;&#21010;&#31639;&#27861;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;THTS&#22312;&#19968;&#31181;&#20020;&#26102;&#26041;&#27861;&#20013;&#20351;&#29992;UCB1 MAB&#31639;&#27861;&#65292;&#22240;&#20026;&#22312;&#21551;&#21457;&#24335;&#25628;&#32034;&#20013;UCB1&#29702;&#35770;&#19978;&#38656;&#35201;&#26377;&#30028;&#25903;&#25345;&#22870;&#21169;&#20998;&#24067;&#30340;&#35201;&#27714;&#22312;&#32463;&#20856;&#35268;&#21010;&#20013;&#19981;&#34987;&#28385;&#36275;&#12290;&#26680;&#24515;&#38382;&#39064;&#22312;&#20110;UCB1&#32570;&#20047;&#23545;&#19981;&#21516;&#22870;&#21169;&#23610;&#24230;&#30340;&#33258;&#36866;&#24212;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;GreedyUCT-Normal&#65292;&#36825;&#26159;&#19968;&#31181;&#20855;&#26377;UCB1-Normal&#36172;&#21338;&#26426;&#30340;MCTS/THTS&#31639;&#27861;&#65292;&#29992;&#20110;&#25935;&#25463;&#32463;&#20856;&#35745;&#21010;&#65292;&#23427;&#36890;&#36807;&#37319;&#29992;&#22870;&#21169;&#21464;&#21270;&#30340;&#23610;&#24230;&#22788;&#29702;&#19981;&#21516;&#23610;&#24230;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Balancing exploration and exploitation has been an important problem in both game tree search and automated planning. However, while the problem has been extensively analyzed within the Multi-Armed Bandit (MAB) literature, the planning community has had limited success when attempting to apply those results. We show that a more detailed theoretical understanding of MAB literature helps improve existing planning algorithms that are based on Monte Carlo Tree Search (MCTS) / Trial Based Heuristic Tree Search (THTS). In particular, THTS uses UCB1 MAB algorithms in an ad hoc manner, as UCB1's theoretical requirement of fixed bounded support reward distributions is not satisfied within heuristic search for classical planning. The core issue lies in UCB1's lack of adaptations to the different scales of the rewards. We propose GreedyUCT-Normal, a MCTS/THTS algorithm with UCB1-Normal bandit for agile classical planning, which handles distributions with different scales by taking the reward vari
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;</title><link>http://arxiv.org/abs/2305.02531</link><description>&lt;p&gt;
&#35821;&#35328;&#12289;&#26102;&#38388;&#20559;&#22909;&#21644;&#28040;&#36153;&#34892;&#20026;&#65306;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Language, Time Preferences, and Consumer Behavior: Evidence from Large Language Models. (arXiv:2305.02531v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#21516;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#21457;&#29616;GPT&#22312;&#20855;&#26377;&#36739;&#24369;&#26410;&#26469;&#26102;&#24577;&#30340;&#35821;&#35328;&#19979;&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#36825;&#19982;&#20351;&#29992;&#35813;&#35821;&#35328;&#30340;&#20154;&#31867;&#30340;&#20559;&#22909;&#30456;&#20284;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#23545;&#25105;&#20204;&#23545;&#26102;&#38388;&#21644;&#22870;&#21169;&#30340;&#24863;&#30693;&#26377;&#24456;&#22823;&#30340;&#24433;&#21709;&#12290;&#36825;&#24341;&#21457;&#20102;&#19968;&#20010;&#38382;&#39064;&#65292;&#21363;&#24403;&#20197;&#19981;&#21516;&#30340;&#35821;&#35328;&#35810;&#38382;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26102;&#65292;&#23427;&#20204;&#26159;&#21542;&#26174;&#31034;&#20986;&#19981;&#21516;&#30340;&#22870;&#21169;&#26102;&#38388;&#20559;&#22909;&#65292;&#24182;&#19988;&#23427;&#20204;&#30340;&#36873;&#25321;&#26159;&#21542;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#36873;&#25321;&#12290;&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;GPT-3.5&#65288;&#20197;&#19979;&#31616;&#31216;GPT&#65289;&#22312;&#22810;&#31181;&#35821;&#35328;&#25552;&#31034;&#19979;&#30340;&#21709;&#24212;&#65292;&#25506;&#32034;&#20102;&#36739;&#23567;&#12289;&#36739;&#26089;&#30340;&#22870;&#21169;&#21644;&#36739;&#22823;&#12289;&#36739;&#26202;&#30340;&#22870;&#21169;&#20043;&#38388;&#30340;&#20559;&#22909;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#26174;&#31034;&#65292;&#24403;&#20197;&#35821;&#20041;&#21547;&#20041;&#36739;&#24369;&#30340;&#26410;&#26469;&#26102;&#24577;&#21442;&#32771;&#65288;FTR&#65289;&#65292;&#22914;&#24503;&#35821;&#21644;&#27721;&#35821;&#65292;&#20026;&#25552;&#31034;&#35821;&#26102;&#65292;GPT&#34920;&#29616;&#20986;&#26356;&#22823;&#30340;&#32784;&#24515;&#65292;&#30456;&#27604;&#33521;&#35821;&#21644;&#27861;&#35821;&#31561;&#20855;&#26377;&#24378;&#22823;FTR&#30340;&#35821;&#35328;&#12290;&#36825;&#20123;&#21457;&#29616;&#19982;&#29616;&#26377;&#25991;&#29486;&#19968;&#33268;&#65292;&#24182;&#34920;&#26126;&#20102;GPT&#30340;&#36873;&#25321;&#19982;&#36825;&#20123;&#35821;&#35328;&#30340;&#20351;&#29992;&#32773;&#30340;&#20559;&#22909;&#20043;&#38388;&#30340;&#20851;&#32852;&#12290;&#28982;&#32780;&#65292;&#36827;&#19968;&#27493;&#30340;&#20998;&#26512;&#25581;&#31034;&#20102;&#36739;&#26089;&#25110;&#36739;&#26202;&#22870;&#21169;&#30340;&#20559;&#22909;&#24182;&#27809;&#26377;&#38543;&#30528;&#22870;&#21169;&#24046;&#24322;&#31995;&#32479;&#22320;&#25913;&#21464;&#65292;&#36825;&#34920;&#26126;&#20102;&#19968;&#31181;&#35789;&#20856;&#24207;&#20248;&#20808;&#30340;&#36873;&#25321;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language has a strong influence on our perceptions of time and rewards. This raises the question of whether large language models, when asked in different languages, show different preferences for rewards over time and if their choices are similar to those of humans. In this study, we analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in multiple languages, exploring preferences between smaller, sooner rewards and larger, later rewards. Our results show that GPT displays greater patience when prompted in languages with weak future tense references (FTR), such as German and Mandarin, compared to languages with strong FTR, like English and French. These findings are consistent with existing literature and suggest a correlation between GPT's choices and the preferences of speakers of these languages. However, further analysis reveals that the preference for earlier or later rewards does not systematically change with reward gaps, indicating a lexicographic preferen
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Distilling Step-by-Step&#26426;&#21046;&#65292;&#36890;&#36807;&#25552;&#21462;LLM&#22522;&#30784;&#20449;&#24687;&#20026;&#23567;&#22411;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#32988;&#36807;&#26356;&#22823;&#30340;LLM&#27169;&#22411;&#65292;&#24182;&#38656;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;</title><link>http://arxiv.org/abs/2305.02301</link><description>&lt;p&gt;
Distilling Step-by-Step&#65281;&#20351;&#29992;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#26356;&#23567;&#30340;&#27169;&#22411;&#23610;&#23544;&#32988;&#36807;&#26356;&#22823;&#30340;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.02301
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;Distilling Step-by-Step&#26426;&#21046;&#65292;&#36890;&#36807;&#25552;&#21462;LLM&#22522;&#30784;&#20449;&#24687;&#20026;&#23567;&#22411;&#27169;&#22411;&#25552;&#20379;&#39069;&#22806;&#30340;&#30417;&#30563;&#35757;&#32451;&#65292;&#20174;&#32780;&#20351;&#23427;&#20204;&#32988;&#36807;&#26356;&#22823;&#30340;LLM&#27169;&#22411;&#65292;&#24182;&#38656;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37096;&#32626;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#38754;&#20020;&#20869;&#23384;&#25928;&#29575;&#20302;&#21644;&#35745;&#31639;&#23494;&#38598;&#24230;&#39640;&#30340;&#38382;&#39064;&#65292;&#30740;&#31350;&#20154;&#21592;&#36890;&#36807;&#24494;&#35843;&#25110;&#31934;&#28860;&#20351;&#29992;LLM&#29983;&#25104;&#30340;&#26631;&#31614;&#26469;&#35757;&#32451;&#36739;&#23567;&#30340;&#20219;&#21153;&#29305;&#23450;&#27169;&#22411;&#12290;&#20294;&#26159;&#65292;&#35201;&#24819;&#36798;&#21040;LLM&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;Distilling Step-by-Step&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#26426;&#21046;&#65292; (a)&#35757;&#32451;&#36739;&#23567;&#30340;&#27169;&#22411;&#27604;LLM&#34920;&#29616;&#26356;&#22909;&#65292;(b)&#24182;&#36890;&#36807;&#21033;&#29992;&#24494;&#35843;&#25110;&#31934;&#28860;&#25152;&#38656;&#30340;&#26356;&#23569;&#30340;&#35757;&#32451;&#25968;&#25454;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#22810;&#20219;&#21153;&#35757;&#32451;&#26694;&#26550;&#20013;&#25552;&#21462;LLM&#22522;&#30784;&#65292;&#24182;&#20316;&#20026;&#39069;&#22806;&#30340;&#30417;&#30563;&#26469;&#35757;&#32451;&#23567;&#22411;&#27169;&#22411;&#12290;&#22312;&#22235;&#20010;NLP&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#21457;&#29616;&#65306;&#31532;&#19968;&#65292;&#19982;&#24494;&#35843;&#21644;&#31934;&#28860;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#26426;&#21046;&#20351;&#29992;&#36739;&#23569;&#30340;&#26631;&#35760;/&#26410;&#26631;&#35760;&#35757;&#32451;&#31034;&#20363;&#21462;&#24471;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#31532;&#20108;&#65292;&#19982;LLM&#30456;&#27604;&#65292;&#21363;&#20351;&#20351;&#29992;&#26356;&#23567;&#30340;&#27169;&#22411;&#65292;&#25105;&#20204;&#20063;&#23454;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deploying large language models (LLMs) is challenging because they are memory inefficient and compute-intensive for practical applications. In reaction, researchers train smaller task-specific models by either finetuning with human labels or distilling using LLM-generated labels. However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. Our method extracts LLM rationales as additional supervision for small models within a multi-task training framework. We present three findings across 4 NLP benchmarks: First, compared to both finetuning and distillation, our mechanism achieves better performance with much fewer labeled/unlabeled training examples. Second, compared to LLMs, we achieve better performance using substantially smaller m
&lt;/p&gt;</description></item><item><title>Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2304.10557</link><description>&lt;p&gt;
Transformer&#20171;&#32461;
&lt;/p&gt;
&lt;p&gt;
An Introduction to Transformers. (arXiv:2304.10557v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10557
&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#65292;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#65292;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#26412;&#35770;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Transformer&#26159;&#19968;&#31181;&#21487;&#20197;&#23398;&#20064;&#24207;&#21015;&#25110;&#25968;&#25454;&#38598;&#34920;&#31034;&#30340;&#31070;&#32463;&#32593;&#32476;&#32452;&#20214;&#12290;Transformer&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#21644;&#26102;&#31354;&#24314;&#27169;&#26041;&#38754;&#21462;&#24471;&#20102;&#37325;&#22823;&#36827;&#23637;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;Transformer&#30340;&#20171;&#32461;&#65292;&#20294;&#22823;&#22810;&#25968;&#37117;&#32570;&#23569;&#23545;&#20854;&#26550;&#26500;&#30340;&#31934;&#30830;&#25968;&#23398;&#25551;&#36848;&#65292;&#20854;&#35774;&#35745;&#36873;&#25321;&#30340;&#30452;&#35273;&#20063;&#24120;&#24120;&#32570;&#22833;&#12290;&#27492;&#22806;&#65292;&#38543;&#30528;&#30740;&#31350;&#36335;&#24452;&#30340;&#26354;&#25240;&#65292;Transformer&#37096;&#20214;&#30340;&#35299;&#37322;&#21487;&#33021;&#26159;&#24322;&#36136;&#30340;&#12290;&#22312;&#36825;&#31687;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#26088;&#22312;&#25552;&#20379;&#19968;&#20010;&#25968;&#23398;&#31934;&#30830;&#12289;&#30452;&#35266;&#12289;&#31616;&#27905;&#30340;Transformer&#26550;&#26500;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The transformer is a neural network component that can be used to learn useful representations of sequences or sets of datapoints. The transformer has driven recent advances in natural language processing, computer vision, and spatio-temporal modelling. There are many introductions to transformers, but most do not contain precise mathematical descriptions of the architecture and the intuitions behind the design choices are often also missing. Moreover, as research takes a winding path, the explanations for the components of the transformer can be idiosyncratic. In this note we aim for a mathematically precise, intuitive, and clean description of the transformer architecture.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2303.13988</link><description>&lt;p&gt;
&#26426;&#22120;&#24515;&#29702;&#23398;&#65306;&#21033;&#29992;&#24515;&#29702;&#23398;&#26041;&#27861;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#32039;&#23494;&#32467;&#21512;&#30340;&#20808;&#38155;&#12290;&#30001;&#20110;&#24555;&#36895;&#25216;&#26415;&#36827;&#27493;&#21644;&#20854;&#26497;&#39640;&#30340;&#36890;&#29992;&#24615;&#65292;&#29616;&#20170;LLM&#24050;&#32463;&#25317;&#26377;&#25968;&#30334;&#19975;&#29992;&#25143;&#65292;&#24182;&#27491;&#22788;&#20110;&#25104;&#20026;&#20027;&#35201;&#20449;&#24687;&#26816;&#32034;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#38382;&#39064;&#35299;&#20915;&#31561;&#25216;&#26415;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#23545;&#20854;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#21644;&#23457;&#26597;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#30001;&#20110;&#24403;&#21069;LLM&#20013;&#20986;&#29616;&#24840;&#21152;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#21487;&#23558;&#20854;&#35270;&#20026;&#21442;&#19982;&#20154;&#31867;&#24515;&#29702;&#23454;&#39564;&#30340;&#23545;&#35937;&#65292;&#20197;&#20415;&#26356;&#20026;&#20840;&#38754;&#22320;&#35780;&#20272;&#20854;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;"&#26426;&#22120;&#24515;&#29702;&#23398;"&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#21508;&#31867;&#24515;&#29702;&#23398;&#20998;&#25903;&#22914;&#20309;&#20026;LLM&#30340;&#34892;&#20026;&#27979;&#35797;&#25552;&#20379;&#26377;&#29992;&#21442;&#32771;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25551;&#36848;&#20102;&#34892;&#20026;&#27979;&#35797;&#32467;&#26524;&#22914;&#20309;&#20026;&#26410;&#26469;&#30340;LLM&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behaviora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#20114;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#24182;&#19988;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2303.06455</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#19978;&#19979;&#25991;&#23884;&#20837;&#22312;&#34920;&#26684;&#25968;&#25454;&#28145;&#24230;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Network contextual embedding for Deep Learning on Tabular Data. (arXiv:2303.06455v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06455
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20351;&#29992;&#20132;&#20114;&#32593;&#32476;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#65292;&#29992;&#20110;&#34920;&#26684;&#25968;&#25454;&#30340;&#22788;&#29702;&#12290;&#35813;&#27169;&#22411;&#22312;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#20248;&#20110;&#26368;&#36817;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#24182;&#19988;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper introduces a novel deep learning model based on Graph Neural Network (GNN) with Interaction Network (IN) for contextual embedding, which outperforms the recent DL benchmark on five public datasets and achieves competitive results compared to boosted-tree solutions in tabular data processing.
&lt;/p&gt;
&lt;p&gt;
&#25152;&#26377;&#34892;&#19994;&#37117;&#35797;&#22270;&#21033;&#29992;&#29616;&#26377;&#30340;&#22823;&#25968;&#25454;&#36827;&#34892;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24212;&#29992;&#65292;&#36825;&#20123;&#25968;&#25454;&#36890;&#24120;&#20197;&#25152;&#35859;&#30340;&#34920;&#26684;&#24418;&#24335;&#23384;&#22312;&#65292;&#20854;&#20013;&#27599;&#20010;&#35760;&#24405;&#30001;&#35768;&#22810;&#24322;&#26500;&#30340;&#36830;&#32493;&#21644;&#20998;&#31867;&#21015;&#32452;&#25104;&#65292;&#20063;&#31216;&#20026;&#29305;&#24449;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#31561;&#19982;&#20154;&#31867;&#25216;&#33021;&#30456;&#20851;&#30340;&#39046;&#22495;&#20013;&#24050;&#32463;&#21462;&#24471;&#20102;&#37325;&#22823;&#31361;&#30772;&#65292;&#20294;&#20854;&#22312;&#34920;&#26684;&#25968;&#25454;&#19978;&#30340;&#24212;&#29992;&#26356;&#20855;&#25361;&#25112;&#24615;&#12290;&#26356;&#32463;&#20856;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#22914;&#22522;&#20110;&#26641;&#30340;&#38598;&#25104;&#27169;&#22411;&#36890;&#24120;&#34920;&#29616;&#26356;&#22909;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#20351;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#65292;&#26356;&#20855;&#20307;&#22320;&#35828;&#26159;&#20132;&#20114;&#32593;&#32476;&#65288;IN&#65289;&#65292;&#36827;&#34892;&#19978;&#19979;&#25991;&#23884;&#20837;&#12290;&#20854;&#32467;&#26524;&#20248;&#20110;&#26368;&#36817;&#21457;&#24067;&#30340;&#22522;&#20110;&#20116;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#30340;&#28145;&#24230;&#23398;&#20064;&#22522;&#20934;&#35843;&#26597;&#65292;&#19982;&#25552;&#21319;&#26641;&#35299;&#20915;&#26041;&#26696;&#30456;&#27604;&#20063;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
All industries are trying to leverage Artificial Intelligence (AI) based on their existing big data which is available in so called tabular form, where each record is composed of a number of heterogeneous continuous and categorical columns also known as features. Deep Learning (DL) has consituted a major breathrough for AI in fields related to human skills like natural language processing, but its applicability to tabular data has been more challenging. More classical Machine Learning (ML) models like tree-based ensemble ones usually perform better. In this manuscript a novel DL model that uses Graph Neural Network (GNN), more specifically Interaction Network (IN), for contextual embedding is introduced. Its results outperform those of the recently published survey with DL benchmark based on five public datasets, achieving also competitive results when compared to boosted-tree solutions.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#20381;&#36182;&#22270;&#65288;CDPs&#65289;&#26469;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;CDPs&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#21487;&#20197;&#27169;&#22359;&#21270;&#22320;&#32467;&#21512;&#22240;&#26524;&#23398;&#20064;&#25110;&#25935;&#24863;&#24230;&#20998;&#26512;&#26041;&#27861;&#12290;&#36825;&#20123;&#22270;&#34920;&#21487;&#20197;&#25104;&#20026;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#21253;&#20013;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#24182;&#23545;&#30456;&#20851;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;</title><link>http://arxiv.org/abs/2303.04209</link><description>&lt;p&gt;
&#22240;&#26524;&#20381;&#36182;&#22270;
&lt;/p&gt;
&lt;p&gt;
Causal Dependence Plots. (arXiv:2303.04209v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.04209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22240;&#26524;&#20381;&#36182;&#22270;&#65288;CDPs&#65289;&#26469;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;CDPs&#19982;&#20256;&#32479;&#26041;&#27861;&#19981;&#21516;&#65292;&#21487;&#20197;&#27169;&#22359;&#21270;&#22320;&#32467;&#21512;&#22240;&#26524;&#23398;&#20064;&#25110;&#25935;&#24863;&#24230;&#20998;&#26512;&#26041;&#27861;&#12290;&#36825;&#20123;&#22270;&#34920;&#21487;&#20197;&#25104;&#20026;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#21253;&#20013;&#30340;&#24378;&#22823;&#24037;&#20855;&#65292;&#24182;&#23545;&#30456;&#20851;&#24212;&#29992;&#20570;&#20986;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#25110;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#37325;&#35201;&#24615;&#36234;&#26469;&#36234;&#22823;&#12290;&#20026;&#20102;&#26126;&#26234;&#22320;&#20351;&#29992;&#36825;&#20123;&#25968;&#25454;&#39537;&#21160;&#30340;&#31995;&#32479;&#65292;&#25105;&#20204;&#24517;&#39035;&#20102;&#35299;&#23427;&#20204;&#22914;&#20309;&#19982;&#19990;&#30028;&#20114;&#21160;&#65292;&#21253;&#25324;&#23427;&#20204;&#22312;&#25968;&#25454;&#36755;&#20837;&#19978;&#30340;&#22240;&#26524;&#20381;&#36182;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22240;&#26524;&#20381;&#36182;&#22270; (CDPs)&#65292;&#29992;&#20110;&#21487;&#35270;&#21270;&#19968;&#20010;&#21464;&#37327;&#65288;&#32467;&#26524;&#65289;&#22914;&#20309;&#38543;&#21478;&#19968;&#20010;&#21464;&#37327;&#65288;&#39044;&#27979;&#22120;&#65289;&#30340;&#21464;&#21270;&#32780;&#21464;&#21270;&#65292;&#20197;&#21450;&#20854;&#20182;&#39044;&#27979;&#22120;&#21464;&#37327;&#30340;&#22240;&#26524;&#21464;&#21270;&#12290;&#20851;&#38190;&#26159;&#65292;CDPs&#19982;&#22522;&#20110;&#20445;&#25345;&#20854;&#20182;&#39044;&#27979;&#22120;&#24658;&#23450;&#25110;&#20551;&#35774;&#23427;&#20204;&#29420;&#31435;&#30340;&#26631;&#20934;&#26041;&#27861;&#19981;&#21516;&#12290;CDPs&#21033;&#29992;&#36741;&#21161;&#22240;&#26524;&#27169;&#22411;&#65292;&#22240;&#20026;&#22240;&#26524;&#32467;&#35770;&#38656;&#35201;&#22240;&#26524;&#20551;&#35774;&#12290;&#36890;&#36807;&#27169;&#25311;&#21644;&#30495;&#23454;&#25968;&#25454;&#23454;&#39564;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;CDPs&#21487;&#20197;&#19982;&#22240;&#26524;&#23398;&#20064;&#25110;&#25935;&#24863;&#24615;&#20998;&#26512;&#26041;&#27861;&#20197;&#27169;&#22359;&#21270;&#30340;&#26041;&#24335;&#32467;&#21512;&#20351;&#29992;&#12290;&#30001;&#20110;&#20154;&#20204;&#32463;&#24120;&#22312;&#36755;&#20837;-&#36755;&#20986;&#20381;&#36182;&#24615;&#26041;&#38754;&#36827;&#34892;&#22240;&#26524;&#24605;&#32771;&#65292;CDPs&#21487;&#20197;&#25104;&#20026;xAI&#25110;&#21487;&#35299;&#37322;&#26426;&#22120;&#23398;&#20064;&#24037;&#20855;&#21253;&#20013;&#24378;&#26377;&#21147;&#30340;&#24037;&#20855;&#65292;&#24182;&#23545;&#24212;&#29992;&#26377;&#25152;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Explaining artificial intelligence or machine learning models is increasingly important. To use such data-driven systems wisely we must understand how they interact with the world, including how they depend causally on data inputs. In this work we develop Causal Dependence Plots (CDPs) to visualize how one variable--an outcome--depends on changes in another variable--a predictor--$\textit{along with any consequent causal changes in other predictor variables}$. Crucially, CDPs differ from standard methods based on holding other predictors constant or assuming they are independent. CDPs make use of an auxiliary causal model because causal conclusions require causal assumptions. With simulations and real data experiments, we show CDPs can be combined in a modular way with methods for causal learning or sensitivity analysis. Since people often think causally about input-output dependence, CDPs can be powerful tools in the xAI or interpretable machine learning toolkit and contribute to appl
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#35843;&#25972;&#23398;&#20064;&#38382;&#39064;&#30340;&#26465;&#20214;&#65292;&#21487;&#20197;&#36991;&#20813;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#20351;&#29992;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#12290;&#36825;&#23545;&#20110;&#23547;&#25214;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#20248;&#22823;&#23567;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2302.13259</link><description>&lt;p&gt;
&#26159;&#21542;&#21487;&#20197;&#22312;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#36991;&#20813;&#21452;&#19979;&#38477;&#65311;
&lt;/p&gt;
&lt;p&gt;
Can we avoid Double Descent in Deep Neural Networks?. (arXiv:2302.13259v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.13259
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#35843;&#25972;&#23398;&#20064;&#38382;&#39064;&#30340;&#26465;&#20214;&#65292;&#21487;&#20197;&#36991;&#20813;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#29305;&#21035;&#26159;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#20351;&#29992;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#12290;&#36825;&#23545;&#20110;&#23547;&#25214;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#20248;&#22823;&#23567;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23547;&#25214;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26368;&#20248;&#22823;&#23567;&#38750;&#24120;&#37325;&#35201;&#19988;&#20855;&#26377;&#24191;&#27867;&#24433;&#21709;&#65292;&#23588;&#20854;&#22312;&#33410;&#33021;&#26041;&#26696;&#20013;&#12290;&#26368;&#36817;&#65292;&#19968;&#20010;&#24847;&#22806;&#30340;&#29616;&#35937;&#65292;&#8220;&#21452;&#19979;&#38477;&#8221;&#65292;&#24341;&#36215;&#20102;&#28145;&#24230;&#23398;&#20064;&#30028;&#30340;&#20851;&#27880;&#12290;&#38543;&#30528;&#27169;&#22411;&#22823;&#23567;&#30340;&#22686;&#21152;&#65292;&#24615;&#33021;&#39318;&#20808;&#21464;&#24046;&#65292;&#28982;&#21518;&#24674;&#22797;&#25552;&#21319;&#12290;&#36825;&#23545;&#20110;&#32500;&#25345;&#39640;&#27867;&#21270;&#30340;&#26368;&#20248;&#27169;&#22411;&#22823;&#23567;&#25552;&#20986;&#20102;&#20005;&#37325;&#30340;&#38382;&#39064;&#65306;&#27169;&#22411;&#38656;&#35201;&#36275;&#22815;&#30340;&#36229;&#21442;&#25968;&#21270;&#65292;&#20294;&#28155;&#21152;&#36807;&#22810;&#30340;&#21442;&#25968;&#20250;&#28010;&#36153;&#35757;&#32451;&#36164;&#28304;&#12290;&#26159;&#21542;&#21487;&#33021;&#20197;&#39640;&#25928;&#30340;&#26041;&#24335;&#25214;&#21040;&#26368;&#20339;&#25240;&#34935;&#26041;&#26696;&#65311;&#25105;&#20204;&#30340;&#24037;&#20316;&#34920;&#26126;&#65292;&#36890;&#36807;&#36866;&#24403;&#35843;&#25972;&#23398;&#20064;&#38382;&#39064;&#30340;&#26465;&#20214;&#65292;&#21487;&#33021;&#21487;&#20197;&#36991;&#20813;&#21452;&#19979;&#38477;&#29616;&#35937;&#65292;&#20294;&#26368;&#32456;&#31572;&#26696;&#20173;&#24453;&#30830;&#23450;&#12290;&#25105;&#20204;&#32463;&#39564;&#22320;&#35266;&#23519;&#21040;&#65292;&#22312;&#22797;&#26434;&#24773;&#20917;&#19979;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#27491;&#21017;&#21270;&#26377;&#26395;&#36991;&#24320;&#21452;&#19979;&#38477;&#65292;&#31616;&#21333;&#30340;$\ell_2$&#27491;&#21017;&#21270;&#24050;&#32463;&#23545;&#27492;&#26377;&#31215;&#26497;&#30340;&#36129;&#29486;&#12290;
&lt;/p&gt;
&lt;p&gt;
Finding the optimal size of deep learning models is very actual and of broad impact, especially in energy-saving schemes. Very recently, an unexpected phenomenon, the ``double descent'', has caught the attention of the deep learning community. As the model's size grows, the performance gets first worse, and then goes back to improving. It raises serious questions about the optimal model's size to maintain high generalization: the model needs to be sufficiently over-parametrized, but adding too many parameters wastes training resources. Is it possible to find, in an efficient way, the best trade-off? Our work shows that the double descent phenomenon is potentially avoidable with proper conditioning of the learning problem, but a final answer is yet to be found. We empirically observe that there is hope to dodge the double descent in complex scenarios with proper regularization, as a simple $\ell_2$ regularization is already positively contributing to such a perspective.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#35270;&#39057;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26174;&#24335;&#22320;&#21033;&#29992;&#26102;&#38388;&#32500;&#24230;&#24182;&#32467;&#21512;Swin Transformer&#30340;&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#33021;&#21147;&#65292;&#25913;&#36827;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.11325</link><description>&lt;p&gt;
Video-SwinUNet&#65306;VFSS &#23454;&#20363;&#20998;&#21106;&#30340;&#26102;&#31354;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
Video-SwinUNet: Spatio-temporal Deep Learning Framework for VFSS Instance Segmentation. (arXiv:2302.11325v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#21307;&#23398;&#35270;&#39057;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#26174;&#24335;&#22320;&#21033;&#29992;&#26102;&#38388;&#32500;&#24230;&#24182;&#32467;&#21512;Swin Transformer&#30340;&#20840;&#23616;&#29305;&#24449;&#32534;&#30721;&#33021;&#21147;&#65292;&#25913;&#36827;&#20102;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#21307;&#23398;&#35270;&#39057;&#20998;&#21106;&#30340;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#12290;&#30001;&#20110;&#20854;&#20855;&#26377;&#20196;&#20154;&#38590;&#20197;&#32622;&#20449;&#30340;&#35821;&#20041;&#29305;&#24449;&#32534;&#30721;&#21644;&#20840;&#23616;&#20449;&#24687;&#29702;&#35299;&#33021;&#21147;&#65292;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#21644;&#22522;&#20110;Transformer&#30340;&#26041;&#27861;&#22312;&#21307;&#23398;&#22270;&#20687;&#20998;&#21106;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#37324;&#31243;&#30865;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#21307;&#23398;&#35270;&#39057;&#25968;&#25454;&#30340;&#19968;&#20010;&#26174;&#33879;&#26041;&#38754; - &#26102;&#38388;&#32500;&#24230;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#26694;&#26550;&#26126;&#30830;&#22320;&#20174;&#26102;&#38388;&#32500;&#24230;&#19978;&#30340;&#30456;&#37051;&#24103;&#20013;&#25552;&#21462;&#29305;&#24449;&#65292;&#24182;&#23558;&#20854;&#19982;&#19968;&#20010;&#26102;&#38388;&#29305;&#24449;&#28151;&#21512;&#22120;&#32467;&#21512;&#36215;&#26469;&#65292;&#28982;&#21518;&#20351;&#29992;Swin Transformer&#23545;&#39640;&#27700;&#24179;&#30340;&#26102;&#31354;&#29305;&#24449;&#36827;&#34892;&#26631;&#35760;&#12290;&#26368;&#32456;&#30340;&#20998;&#21106;&#32467;&#26524;&#36890;&#36807;&#19968;&#20010;&#31867;&#20284;UNet&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#29983;&#25104;&#12290;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;VFSS2022&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#33394;&#65292;&#36739;&#20854;&#20182;&#26041;&#27861;&#25552;&#21319;&#26174;&#33879;&#65292;&#23545;&#20110;&#20004;&#20010;&#27979;&#35797;&#25968;&#25454;&#38598;&#30340;Dice&#31995;&#25968;&#20998;&#21035;&#36798;&#21040;&#20102;0.8986&#21644;0.8186&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a deep learning framework for medical video segmentation. Convolution neural network (CNN) and transformer-based methods have achieved great milestones in medical image segmentation tasks due to their incredible semantic feature encoding and global information comprehension abilities. However, most existing approaches ignore a salient aspect of medical video data - the temporal dimension. Our proposed framework explicitly extracts features from neighbouring frames across the temporal dimension and incorporates them with a temporal feature blender, which then tokenises the high-level spatio-temporal feature to form a strong global feature encoded via a Swin Transformer. The final segmentation results are produced via a UNet-like encoder-decoder architecture. Our model outperforms other approaches by a significant margin and improves the segmentation benchmarks on the VFSS2022 dataset, achieving a dice coefficient of 0.8986 and 0.8186 for the two datasets tested. Our 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#23545;&#27604;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#38752;&#30340;&#36127;&#26679;&#26412;&#23545;&#26469;&#25913;&#36827;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;</title><link>http://arxiv.org/abs/2302.09532</link><description>&lt;p&gt;
&#22522;&#20110;&#20266;&#23545;&#27604;&#23398;&#20064;&#30340;&#22522;&#20110;&#22270;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Pseudo Contrastive Learning for Graph-based Semi-supervised Learning. (arXiv:2302.09532v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.09532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20266;&#23545;&#27604;&#23398;&#20064;&#30340;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#65292;&#36890;&#36807;&#29983;&#25104;&#21487;&#38752;&#30340;&#36127;&#26679;&#26412;&#23545;&#26469;&#25913;&#36827;&#20266;&#26631;&#31614;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20266;&#26631;&#31614;&#26159;&#19968;&#31181;&#29992;&#20110;&#25913;&#36827;&#21322;&#30417;&#30563;&#22270;&#31070;&#32463;&#32593;&#32476;(GNNs)&#24615;&#33021;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#26681;&#25454;&#33258;&#20449;&#30340;&#39044;&#27979;&#29983;&#25104;&#38468;&#21152;&#30340;&#20266;&#26631;&#31614;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#20998;&#31867;&#30446;&#26631;&#23545;&#32473;&#23450;&#26631;&#31614;&#30340;&#25935;&#24863;&#24615;&#65292;&#29983;&#25104;&#30340;&#20266;&#26631;&#31614;&#36136;&#37327;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#38382;&#39064;&#12290;&#20026;&#20102;&#36991;&#20813;&#19981;&#21487;&#38752;&#30340;&#20998;&#31867;&#30417;&#30563;&#8220;&#19968;&#20010;&#33410;&#28857;&#23646;&#20110;&#29305;&#23450;&#31867;&#8221;&#65292;&#25105;&#20204;&#26356;&#21916;&#27426;&#23481;&#38169;&#24615;&#23545;&#27604;&#30417;&#30563;&#8220;&#20004;&#20010;&#33410;&#28857;&#19981;&#23646;&#20110;&#21516;&#19968;&#31867;&#8221;&#12290;&#22240;&#27492;&#65292;&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#38382;&#39064;&#36716;&#21270;&#20026;&#19968;&#20010;&#25918;&#26494;&#30340;&#29256;&#26412;&#65292;&#21363;&#35782;&#21035;&#21487;&#38752;&#30340;&#36127;&#26679;&#26412;&#23545;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;GNNs&#26694;&#26550;&#65292;&#31216;&#20043;&#20026;&#20266;&#23545;&#27604;&#23398;&#20064;(PCL)&#12290;&#23427;&#23558;&#30446;&#26631;&#20026;&#30456;&#21516;&#31867;&#30340;&#27491;&#20266;&#26631;&#31614;&#21644;&#36127;&#20266;&#26631;&#31614;&#30340;&#20004;&#20010;&#33410;&#28857;&#20998;&#24320;&#12290;&#20026;&#20102;&#23558;&#25299;&#25169;&#30693;&#35782;&#32435;&#20837;&#23398;&#20064;&#20013;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#25299;&#25169;&#21152;&#26435;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Pseudo Labeling is a technique used to improve the performance of semi-supervised Graph Neural Networks (GNNs) by generating additional pseudo-labels based on confident predictions. However, the quality of generated pseudo-labels has been a longstanding concern due to the sensitivity of the classification objective with respect to the given labels. To avoid the untrustworthy classification supervision indicating ``a node belongs to a specific class,'' we favor the fault-tolerant contrasting supervision demonstrating ``two nodes do not belong to the same class.'' Thus, the problem of generating high-quality pseudo-labels is then transformed into a relaxed version, i.e., identifying reliable negative pairs. To achieve this, we propose a general framework for GNNs, termed Pseudo Contrastive Learning (PCL). It separates two nodes whose positive and negative pseudo-labels target the same class. To incorporate topological knowledge into learning, we devise a topologically weighted contrastiv
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20165;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#30340;&#24402;&#19968;&#21270;&#23618;&#21442;&#25968;&#23601;&#21487;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#24615;&#65292;&#29978;&#33267;&#21487;&#20197;&#37325;&#24314;&#27604;&#21407;&#32593;&#32476;&#23567;O(&#26681;&#21495;&#23485;&#24230;)&#20493;&#30340;&#30446;&#26631;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2302.07937</link><description>&lt;p&gt;
&#20165;&#35843;&#25972;&#24402;&#19968;&#21270;&#23618;&#30340;&#34920;&#36798;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
The Expressive Power of Tuning Only the Normalization Layers. (arXiv:2302.07937v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07937
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21457;&#29616;&#65292;&#20165;&#35843;&#25972;&#31070;&#32463;&#32593;&#32476;&#30340;&#24402;&#19968;&#21270;&#23618;&#21442;&#25968;&#23601;&#21487;&#20197;&#36798;&#21040;&#39640;&#20934;&#30830;&#24615;&#65292;&#29978;&#33267;&#21487;&#20197;&#37325;&#24314;&#27604;&#21407;&#32593;&#32476;&#23567;O(&#26681;&#21495;&#23485;&#24230;)&#20493;&#30340;&#30446;&#26631;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29305;&#24449;&#24402;&#19968;&#21270;&#36716;&#25442;&#65292;&#22914;&#25209;&#37327;&#24402;&#19968;&#21270;&#21644;&#23618;&#24402;&#19968;&#21270;&#65292;&#24050;&#25104;&#20026;&#24403;&#20170;&#20808;&#36827;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#19981;&#21487;&#25110;&#32570;&#30340;&#32452;&#25104;&#37096;&#20998;&#12290;&#20851;&#20110;&#24494;&#35843;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#26368;&#36817;&#30740;&#31350;&#34920;&#26126;&#65292;&#20165;&#35843;&#25972;&#36825;&#20123;&#20223;&#23556;&#21464;&#25442;&#30340;&#21442;&#25968;&#23601;&#21487;&#20197;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#33719;&#24471;&#39640;&#20934;&#30830;&#24615;&#12290;&#36825;&#20123;&#30740;&#31350;&#32467;&#26524;&#24341;&#21457;&#20102;&#23545;&#35843;&#25972;&#20923;&#32467;&#32593;&#32476;&#30340;&#24402;&#19968;&#21270;&#23618;&#30340;&#34920;&#36798;&#33021;&#21147;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#39318;&#27425;&#25506;&#35752;&#36825;&#20010;&#38382;&#39064;&#65292;&#24182;&#26174;&#31034;&#23545;&#20110;&#38543;&#26426;ReLU&#32593;&#32476;&#65292;&#20165;&#24494;&#35843;&#20854;&#24402;&#19968;&#21270;&#23618;&#21487;&#20197;&#37325;&#24314;&#20219;&#20309;&#22823;&#23567;&#20026;O(&#26681;&#21495;&#23485;&#24230;)&#20493;&#23567;&#30340;&#30446;&#26631;&#32593;&#32476;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#21363;&#20351;&#22312;&#38543;&#26426;&#31232;&#30095;&#32593;&#32476;&#20013;&#65292;&#22312;&#36275;&#22815;&#36229;&#21442;&#25968;&#21270;&#30340;&#24773;&#20917;&#19979;&#65292;&#36825;&#20010;&#32467;&#35770;&#20063;&#25104;&#31435;&#65292;&#19982;&#20808;&#21069;&#30340;&#23454;&#35777;&#24037;&#20316;&#19968;&#33268;&#12290;
&lt;/p&gt;
&lt;p&gt;
Feature normalization transforms such as Batch and Layer-Normalization have become indispensable ingredients of state-of-the-art deep neural networks. Recent studies on fine-tuning large pretrained models indicate that just tuning the parameters of these affine transforms can achieve high accuracy for downstream tasks. These findings open the questions about the expressive power of tuning the normalization layers of frozen networks. In this work, we take the first step towards this question and show that for random ReLU networks, fine-tuning only its normalization layers can reconstruct any target network that is $O(\sqrt{\text{width}})$ times smaller. We show that this holds even for randomly sparsified networks, under sufficient overparameterization, in agreement with prior empirical work.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#38024;&#23545;&#26631;&#31614;&#39044;&#31639;&#32422;&#26463;&#19979;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#26631;&#35760;&#31574;&#30053;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.07832</link><description>&lt;p&gt;
&#26631;&#31614;&#39044;&#31639;&#32422;&#26463;&#19979;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Deep Anomaly Detection under Labeling Budget Constraints. (arXiv:2302.07832v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.07832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#38024;&#23545;&#26631;&#31614;&#39044;&#31639;&#32422;&#26463;&#19979;&#30340;&#28145;&#24230;&#24322;&#24120;&#26816;&#27979;&#38382;&#39064;&#25552;&#20986;&#20102;&#19968;&#31181;&#25968;&#25454;&#26631;&#35760;&#31574;&#30053;&#21644;&#21322;&#30417;&#30563;&#23398;&#20064;&#26694;&#26550;&#65292;&#22312;&#22810;&#31181;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21508;&#31181;&#22330;&#26223;&#19979;&#65292;&#36873;&#25321;&#20449;&#24687;&#20016;&#23500;&#30340;&#25968;&#25454;&#28857;&#36827;&#34892;&#19987;&#23478;&#21453;&#39304;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24322;&#24120;&#26816;&#27979;&#30340;&#24615;&#33021;&#65292;&#20363;&#22914;&#21307;&#23398;&#35786;&#26029;&#25110;&#27450;&#35784;&#26816;&#27979;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#19968;&#32452;&#29702;&#35770;&#26465;&#20214;&#65292;&#20351;&#24471;&#20174;&#26631;&#35760;&#30340;&#26597;&#35810;&#21040;&#26410;&#26631;&#35760;&#30340;&#25968;&#25454;&#20013;&#24322;&#24120;&#20998;&#25968;&#33021;&#22815;&#25512;&#24191;&#12290;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26631;&#31614;&#39044;&#31639;&#32422;&#26463;&#19979;&#20855;&#26377;&#26368;&#20248;&#25968;&#25454;&#35206;&#30422;&#30340;&#25968;&#25454;&#26631;&#35760;&#31574;&#30053;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#22270;&#20687;&#12289;&#34920;&#26684;&#21644;&#35270;&#39057;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#26631;&#31614;&#39044;&#31639;&#32422;&#26463;&#19979;&#20855;&#26377;&#26368;&#20808;&#36827;&#30340;&#21322;&#30417;&#30563;&#24322;&#24120;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Selecting informative data points for expert feedback can significantly improve the performance of anomaly detection (AD) in various contexts, such as medical diagnostics or fraud detection. In this paper, we determine a set of theoretical conditions under which anomaly scores generalize from labeled queries to unlabeled data. Motivated by these results, we propose a data labeling strategy with optimal data coverage under labeling budget constraints. In addition, we propose a new learning framework for semi-supervised AD. Extensive experiments on image, tabular, and video data sets show that our approach results in state-of-the-art semi-supervised AD performance under labeling budget constraints.
&lt;/p&gt;</description></item><item><title>&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#23494;&#24230;&#24863;&#30693;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#20449;&#24687;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#22312;&#39046;&#22495;&#36716;&#31227;&#21644;&#39046;&#22495;&#22806;&#24773;&#26223;&#19979;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#22312;&#20445;&#25345;&#20986;&#33394;&#39046;&#22495;&#20869;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21516;&#26102;&#25552;&#39640;&#26657;&#20934;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2302.05118</link><description>&lt;p&gt;
&#36229;&#36234;&#39046;&#22495;&#24773;&#26223;&#65306;&#40065;&#26834;&#30340;&#23494;&#24230;&#24863;&#30693;&#26657;&#20934;
&lt;/p&gt;
&lt;p&gt;
Beyond In-Domain Scenarios: Robust Density-Aware Calibration. (arXiv:2302.05118v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.05118
&lt;/p&gt;
&lt;p&gt;
&#36825;&#20010;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#40065;&#26834;&#30340;&#23494;&#24230;&#24863;&#30693;&#26657;&#20934;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#38544;&#34255;&#23618;&#30340;&#20449;&#24687;&#65292;&#35813;&#26041;&#27861;&#33021;&#22815;&#25552;&#20379;&#22312;&#39046;&#22495;&#36716;&#31227;&#21644;&#39046;&#22495;&#22806;&#24773;&#26223;&#19979;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#24182;&#22312;&#20445;&#25345;&#20986;&#33394;&#39046;&#22495;&#20869;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#21516;&#26102;&#25552;&#39640;&#26657;&#20934;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#37096;&#32626;&#36234;&#26469;&#36234;&#22810;&#65292;&#26657;&#20934;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20197;&#33719;&#24471;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#24847;&#35782;&#30340;&#39044;&#27979;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#23613;&#31649;&#29616;&#26377;&#30340;&#20107;&#21518;&#26657;&#20934;&#26041;&#27861;&#22312;&#39046;&#22495;&#20869;&#30340;&#27979;&#35797;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#22312;&#39046;&#22495;&#36716;&#31227;&#21644;&#39046;&#22495;&#22806;&#65288;OOD&#65289;&#24773;&#26223;&#19979;&#26080;&#27861;&#20135;&#29983;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#26088;&#22312;&#36890;&#36807;&#25552;&#20986;&#22522;&#20110;k&#36817;&#37051;&#30340;DAC&#65288;Density-Aware Calibration&#65289;&#26041;&#27861;&#26469;&#24357;&#34917;&#36825;&#20010;&#24046;&#36317;&#12290;&#19982;&#29616;&#26377;&#30340;&#20107;&#21518;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#21033;&#29992;&#20998;&#31867;&#22120;&#30340;&#38544;&#34255;&#23618;&#20316;&#20026;&#19982;&#19981;&#30830;&#23450;&#24615;&#30456;&#20851;&#20449;&#24687;&#30340;&#26469;&#28304;&#65292;&#24182;&#30740;&#31350;&#23427;&#20204;&#30340;&#37325;&#35201;&#24615;&#12290;&#25105;&#20204;&#34920;&#26126;DAC&#26159;&#19968;&#20010;&#36890;&#29992;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#26041;&#20415;&#22320;&#19982;&#26368;&#20808;&#36827;&#30340;&#20107;&#21518;&#26041;&#27861;&#32467;&#21512;&#20351;&#29992;&#12290;DAC&#25552;&#39640;&#20102;&#22312;&#39046;&#22495;&#36716;&#31227;&#21644;OOD&#24773;&#26223;&#19979;&#26657;&#20934;&#24615;&#33021;&#30340;&#31283;&#20581;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#20102;&#20986;&#33394;&#30340;&#39046;&#22495;&#20869;&#39044;&#27979;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;DAC&#33021;&#22815;&#19968;&#30452;&#24341;&#23548;&#30528;&#19968;&#33268;
&lt;/p&gt;
&lt;p&gt;
Calibrating deep learning models to yield uncertainty-aware predictions is crucial as deep neural networks get increasingly deployed in safety-critical applications. While existing post-hoc calibration methods achieve impressive results on in-domain test datasets, they are limited by their inability to yield reliable uncertainty estimates in domain-shift and out-of-domain (OOD) scenarios. We aim to bridge this gap by proposing DAC, an accuracy-preserving as well as Density-Aware Calibration method based on k-nearest-neighbors (KNN). In contrast to existing post-hoc methods, we utilize hidden layers of classifiers as a source for uncertainty-related information and study their importance. We show that DAC is a generic method that can readily be combined with state-of-the-art post-hoc methods. DAC boosts the robustness of calibration performance in domain-shift and OOD, while maintaining excellent in-domain predictive uncertainty estimates. We demonstrate that DAC leads to consistently b
&lt;/p&gt;</description></item><item><title>RPN&#26159;&#19968;&#31181;&#22522;&#20110;&#35789;&#21521;&#37327;&#32423;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#20462;&#25913;&#21407;&#22987;&#25991;&#26412;&#30340;&#35789;&#23884;&#20837;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2212.05961</link><description>&lt;p&gt;
RPN: &#19968;&#31181;&#28145;&#24230;&#23398;&#20064;&#20013;&#22522;&#20110;&#35789;&#21521;&#37327;&#30340;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#29992;&#20110;&#35821;&#35328;&#29702;&#35299;
&lt;/p&gt;
&lt;p&gt;
RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning for Language Understanding. (arXiv:2212.05961v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05961
&lt;/p&gt;
&lt;p&gt;
RPN&#26159;&#19968;&#31181;&#22522;&#20110;&#35789;&#21521;&#37327;&#32423;&#21035;&#30340;&#25968;&#25454;&#22686;&#24378;&#31639;&#27861;&#65292;&#36890;&#36807;&#24341;&#20837;&#22122;&#22768;&#20462;&#25913;&#21407;&#22987;&#25991;&#26412;&#30340;&#35789;&#23884;&#20837;&#65292;&#26356;&#22909;&#22320;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#21464;&#21270;&#65292;&#24182;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25968;&#25454;&#22686;&#24378;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#24191;&#27867;&#20351;&#29992;&#20197;&#25552;&#39640;&#27169;&#22411;&#24615;&#33021;&#30340;&#25216;&#26415;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#21487;&#33021;&#26080;&#27861;&#23436;&#20840;&#25429;&#25417;&#21040;&#33258;&#28982;&#35821;&#35328;&#30340;&#22797;&#26434;&#21464;&#21270;&#65292;&#24182;&#19988;&#22312;&#22823;&#22411;&#25968;&#25454;&#38598;&#20013;&#24212;&#29992;&#36215;&#26469;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#8212;&#8212;&#38543;&#26426;&#20301;&#32622;&#22122;&#22768;&#65288;RPN&#65289;&#31639;&#27861;&#65292;&#23427;&#22312;&#35789;&#21521;&#37327;&#32423;&#21035;&#19978;&#36827;&#34892;&#25805;&#20316;&#12290;RPN&#36890;&#36807;&#26681;&#25454;&#36873;&#23450;&#35789;&#21521;&#37327;&#30340;&#29616;&#26377;&#20540;&#24341;&#20837;&#22122;&#22768;&#20462;&#25913;&#21407;&#22987;&#25991;&#26412;&#30340;&#35789;&#23884;&#20837;&#65292;&#20801;&#35768;&#26356;&#32454;&#31890;&#24230;&#30340;&#20462;&#25913;&#24182;&#26356;&#22909;&#22320;&#25429;&#25417;&#33258;&#28982;&#35821;&#35328;&#21464;&#21270;&#12290;&#19982;&#20256;&#32479;&#30340;&#25968;&#25454;&#22686;&#24378;&#26041;&#27861;&#19981;&#21516;&#65292;RPN&#19981;&#38656;&#35201;&#35745;&#31639;&#22270;&#20013;&#30340;&#26799;&#24230;&#26469;&#36827;&#34892;&#34394;&#25311;&#26679;&#26412;&#26356;&#26032;&#65292;&#20351;&#20854;&#26356;&#23481;&#26131;&#24212;&#29992;&#20110;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#21508;&#31181;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#20013;&#65292;&#21253;&#25324;&#24773;&#24863;&#20998;&#26512;&#31561;&#65292;RPN&#22987;&#32456;&#20248;&#20110;&#29616;&#26377;&#25968;&#25454;&#22686;&#24378;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Data augmentation is a widely used technique in machine learning to improve model performance. However, existing data augmentation techniques in natural language understanding (NLU) may not fully capture the complexity of natural language variations, and they can be challenging to apply to large datasets. This paper proposes the Random Position Noise (RPN) algorithm, a novel data augmentation technique that operates at the word vector level. RPN modifies the word embeddings of the original text by introducing noise based on the existing values of selected word vectors, allowing for more fine-grained modifications and better capturing natural language variations. Unlike traditional data augmentation methods, RPN does not require gradients in the computational graph during virtual sample updates, making it simpler to apply to large datasets. Experimental results demonstrate that RPN consistently outperforms existing data augmentation techniques across various NLU tasks, including sentime
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20844;&#24179;&#24178;&#39044;&#21644;&#32416;&#27491;&#37319;&#26679;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26465;&#20214;&#29983;&#25104;&#20013;&#30340;&#20266;&#22240;&#26524;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#33021;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.02090</link><description>&lt;p&gt;
&#36890;&#36807;&#20844;&#24179;&#24178;&#39044;&#21644;&#32416;&#27491;&#37319;&#26679;&#26469;&#25171;&#30772;&#26465;&#20214;&#29983;&#25104;&#30340;&#20266;&#22240;&#26524;&#20851;&#31995;
&lt;/p&gt;
&lt;p&gt;
Breaking the Spurious Causality of Conditional Generation via Fairness Intervention with Corrective Sampling. (arXiv:2212.02090v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02090
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#20844;&#24179;&#24178;&#39044;&#21644;&#32416;&#27491;&#37319;&#26679;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#26465;&#20214;&#29983;&#25104;&#20013;&#30340;&#20266;&#22240;&#26524;&#20851;&#31995;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#35813;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#37117;&#33021;&#26377;&#25928;&#22320;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#25429;&#25417;&#26679;&#26412;&#21644;&#26631;&#31614;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#26465;&#20214;&#29983;&#25104;&#27169;&#22411;&#32463;&#24120;&#20174;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#32487;&#25215;&#20102;&#20266;&#30456;&#20851;&#24615;&#12290;&#36825;&#21487;&#33021;&#23548;&#33268;&#26631;&#31614;&#26465;&#20214;&#20998;&#24067;&#22312;&#21478;&#19968;&#20010;&#28508;&#22312;&#23646;&#24615;&#19978;&#19981;&#24179;&#34913;&#12290;&#20026;&#20102;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#20004;&#27493;&#31574;&#30053;&#12290; &#65288;a&#65289;&#20844;&#24179;&#24178;&#39044;&#65288;FI&#65289;&#65306;&#24378;&#35843;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#30001;&#20110;&#20266;&#30456;&#20851;&#24615;&#38590;&#20197;&#29983;&#25104;&#30340;&#23569;&#25968;&#26679;&#26412;&#12290;&#65288;b&#65289;&#32416;&#27491;&#37319;&#26679;&#65288;CS&#65289;&#65306;&#26174;&#24335;&#36807;&#28388;&#29983;&#25104;&#30340;&#26679;&#26412;&#65292;&#24182;&#30830;&#20445;&#23427;&#20204;&#36981;&#24490;&#25152;&#38656;&#30340;&#28508;&#22312;&#23646;&#24615;&#20998;&#24067;&#12290;&#25105;&#20204;&#35774;&#35745;&#30340;&#20844;&#24179;&#24178;&#39044;&#21487;&#20197;&#36866;&#29992;&#20110;&#19981;&#21516;&#31243;&#24230;&#30340;&#23545;&#20266;&#23646;&#24615;&#30340;&#30417;&#30563;&#65292;&#21253;&#25324;&#26080;&#30417;&#30563;&#12289;&#24369;&#30417;&#30563;&#21644;&#21322;&#30417;&#30563;&#22330;&#26223;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#65292;FICS&#21487;&#20197;&#26377;&#25928;&#22320;&#35299;&#20915;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#30340;&#26465;&#20214;&#29983;&#25104;&#30340;&#20266;&#22240;&#26524;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
To capture the relationship between samples and labels, conditional generative models often inherit spurious correlations from the training dataset. This can result in label-conditional distributions that are imbalanced with respect to another latent attribute. To mitigate this issue, which we call spurious causality of conditional generation, we propose a general two-step strategy. (a) Fairness Intervention (FI): emphasize the minority samples that are hard to generate due to the spurious correlation in the training dataset. (b) Corrective Sampling (CS): explicitly filter the generated samples and ensure that they follow the desired latent attribute distribution. We have designed the fairness intervention to work for various degrees of supervision on the spurious attribute, including unsupervised, weakly-supervised, and semi-supervised scenarios. Our experimental results demonstrate that FICS can effectively resolve spurious causality of conditional generation across various datasets.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TAX-Pose&#30340;&#31995;&#32479;&#65292;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#23454;&#29616;&#20102;&#20219;&#21153;&#29305;&#23450;&#36328;&#23039;&#21183;&#30340;&#20272;&#35745;&#12290;&#36890;&#36807;&#23398;&#20064;&#23545;&#35937;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#36825;&#31181;&#31995;&#32479;&#33021;&#22815;&#22312;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#20272;&#35745;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#36328;&#23039;&#21183;&#65292;&#24182;&#21033;&#29992;&#20272;&#35745;&#32467;&#26524;&#25351;&#23548;&#19979;&#28216;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;</title><link>http://arxiv.org/abs/2211.09325</link><description>&lt;p&gt;
TAX-Pose&#65306;&#26426;&#22120;&#20154;&#25805;&#20316;&#30340;&#20219;&#21153;&#29305;&#23450;&#36328;&#23039;&#21183;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation. (arXiv:2211.09325v2 [cs.RO] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09325
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TAX-Pose&#30340;&#31995;&#32479;&#65292;&#22312;&#26426;&#22120;&#20154;&#25805;&#20316;&#20013;&#23454;&#29616;&#20102;&#20219;&#21153;&#29305;&#23450;&#36328;&#23039;&#21183;&#30340;&#20272;&#35745;&#12290;&#36890;&#36807;&#23398;&#20064;&#23545;&#35937;&#20043;&#38388;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#36825;&#31181;&#31995;&#32479;&#33021;&#22815;&#22312;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#30340;&#24773;&#20917;&#19979;&#20934;&#30830;&#20272;&#35745;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#36328;&#23039;&#21183;&#65292;&#24182;&#21033;&#29992;&#20272;&#35745;&#32467;&#26524;&#25351;&#23548;&#19979;&#28216;&#30340;&#36816;&#21160;&#35268;&#21010;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#22914;&#20309;&#36171;&#20104;&#26426;&#22120;&#20154;&#26377;&#25928;&#22320;&#25805;&#20316;&#26410;&#30693;&#29289;&#20307;&#30340;&#33021;&#21147;&#65292;&#24182;&#22522;&#20110;&#31034;&#33539;&#36716;&#31227;&#30456;&#20851;&#25216;&#33021;&#65311;&#31471;&#21040;&#31471;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#26080;&#27861;&#27867;&#21270;&#21040;&#26032;&#30340;&#29289;&#20307;&#25110;&#26410;&#35265;&#36807;&#30340;&#37197;&#32622;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#20851;&#27880;&#20132;&#20114;&#23545;&#35937;&#30456;&#20851;&#37096;&#20998;&#30340;&#20219;&#21153;&#29305;&#23450;&#23039;&#21183;&#20851;&#31995;&#12290;&#25105;&#20204;&#25512;&#27979;&#36825;&#31181;&#20851;&#31995;&#26159;&#19968;&#31181;&#21487;&#20197;&#36716;&#31227;&#21040;&#21516;&#19968;&#31867;&#21035;&#26032;&#29289;&#20307;&#30340;&#25805;&#20316;&#20219;&#21153;&#30340;&#21487;&#27867;&#21270;&#27010;&#24565;&#65307;&#20363;&#22914;&#65292;&#24179;&#24213;&#38149;&#30456;&#23545;&#20110;&#28900;&#31665;&#30340;&#23039;&#21183;&#20851;&#31995;&#25110;&#32773;&#26479;&#23376;&#30456;&#23545;&#20110;&#26479;&#26550;&#30340;&#23039;&#21183;&#20851;&#31995;&#12290;&#25105;&#20204;&#31216;&#36825;&#31181;&#20219;&#21153;&#29305;&#23450;&#23039;&#21183;&#20851;&#31995;&#20026;&#8220;&#36328;&#23039;&#21183;&#8221;&#65292;&#24182;&#25552;&#20379;&#20102;&#35813;&#27010;&#24565;&#30340;&#25968;&#23398;&#23450;&#20041;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#35270;&#35273;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#23398;&#20064;&#30340;&#23545;&#35937;&#38388;&#23545;&#24212;&#20851;&#31995;&#26469;&#23398;&#20064;&#20272;&#35745;&#32473;&#23450;&#25805;&#20316;&#20219;&#21153;&#30340;&#20004;&#20010;&#23545;&#35937;&#20043;&#38388;&#30340;&#36328;&#23039;&#21183;&#12290;&#28982;&#21518;&#65292;&#20272;&#35745;&#30340;&#36328;&#23039;&#21183;&#29992;&#20110;&#24341;&#23548;&#19979;&#28216;&#30340;&#36816;&#21160;&#35268;&#21010;&#22120;&#23558;&#23545;&#35937;&#25805;&#32437;&#21040;&#25152;&#38656;&#30340;&#23039;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
How do we imbue robots with the ability to efficiently manipulate unseen objects and transfer relevant skills based on demonstrations? End-to-end learning methods often fail to generalize to novel objects or unseen configurations. Instead, we focus on the task-specific pose relationship between relevant parts of interacting objects. We conjecture that this relationship is a generalizable notion of a manipulation task that can transfer to new objects in the same category; examples include the relationship between the pose of a pan relative to an oven or the pose of a mug relative to a mug rack. We call this task-specific pose relationship "cross-pose" and provide a mathematical definition of this concept. We propose a vision-based system that learns to estimate the cross-pose between two objects for a given manipulation task using learned cross-object correspondences. The estimated cross-pose is then used to guide a downstream motion planner to manipulate the objects into the desired po
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Sirius&#26694;&#26550;&#65292;&#36890;&#36807;&#20219;&#21153;&#20998;&#24037;&#23454;&#29616;&#20154;&#26426;&#21327;&#20316;&#65292;&#37096;&#20998;&#33258;&#20027;&#30340;&#26426;&#22120;&#20154;&#36127;&#36131;&#20915;&#31574;&#24037;&#20316;&#65292;&#20154;&#31867;&#25805;&#20316;&#21592;&#22312;&#38656;&#35201;&#26102;&#36827;&#34892;&#24178;&#39044;&#12290;&#36825;&#31181;&#20154;&#26426;&#22242;&#38431;&#21487;&#20197;&#30830;&#20445;&#22797;&#26434;&#20219;&#21153;&#30340;&#23433;&#20840;&#37096;&#32626;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#35757;&#32451;&#26679;&#26412;&#26469;&#25913;&#36827;&#31574;&#30053;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2211.08416</link><description>&lt;p&gt;
&#22312;&#24037;&#20316;&#20013;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#65306;&#20154;&#20026;&#29615;&#22659;&#19979;&#30340;&#33258;&#20027;&#23398;&#20064;&#21644;&#37096;&#32626;&#26399;&#38388;&#30340;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Robot Learning on the Job: Human-in-the-Loop Autonomy and Learning During Deployment. (arXiv:2211.08416v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08416
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;Sirius&#26694;&#26550;&#65292;&#36890;&#36807;&#20219;&#21153;&#20998;&#24037;&#23454;&#29616;&#20154;&#26426;&#21327;&#20316;&#65292;&#37096;&#20998;&#33258;&#20027;&#30340;&#26426;&#22120;&#20154;&#36127;&#36131;&#20915;&#31574;&#24037;&#20316;&#65292;&#20154;&#31867;&#25805;&#20316;&#21592;&#22312;&#38656;&#35201;&#26102;&#36827;&#34892;&#24178;&#39044;&#12290;&#36825;&#31181;&#20154;&#26426;&#22242;&#38431;&#21487;&#20197;&#30830;&#20445;&#22797;&#26434;&#20219;&#21153;&#30340;&#23433;&#20840;&#37096;&#32626;&#12290;&#21516;&#26102;&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#65292;&#36890;&#36807;&#37325;&#26032;&#21152;&#26435;&#35757;&#32451;&#26679;&#26412;&#26469;&#25913;&#36827;&#31574;&#30053;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#33021;&#21147;&#30340;&#24555;&#36895;&#22686;&#38271;&#21644;&#28145;&#24230;&#23398;&#20064;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25105;&#20204;&#22312;&#30740;&#31350;&#29615;&#22659;&#20013;&#35265;&#35777;&#20102;&#26032;&#22411;&#26426;&#22120;&#20154;&#33021;&#21147;&#30340;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23637;&#31034;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#23398;&#20064;&#31995;&#32479;&#34920;&#29616;&#20986;&#33030;&#24369;&#30340;&#27867;&#21270;&#33021;&#21147;&#65292;&#24182;&#19988;&#22312;&#23454;&#38469;&#20219;&#21153;&#20013;&#38656;&#35201;&#22823;&#37327;&#35757;&#32451;&#25968;&#25454;&#12290;&#20026;&#20102;&#21033;&#29992;&#26368;&#20808;&#36827;&#30340;&#26426;&#22120;&#20154;&#23398;&#20064;&#27169;&#22411;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#25509;&#21463;&#23427;&#20204;&#30340;&#19981;&#23436;&#32654;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;Sirius&#65292;&#19968;&#20010;&#20026;&#20154;&#31867;&#21644;&#26426;&#22120;&#20154;&#36890;&#36807;&#20998;&#24037;&#21512;&#20316;&#32780;&#35774;&#35745;&#30340;&#21407;&#21017;&#24615;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#37096;&#20998;&#33258;&#20027;&#30340;&#26426;&#22120;&#20154;&#36127;&#36131;&#22788;&#29702;&#22823;&#37096;&#20998;&#20915;&#31574;&#24037;&#20316;&#65292;&#22312;&#36825;&#20123;&#24037;&#20316;&#20013;&#23427;&#20204;&#33021;&#21487;&#38752;&#22320;&#24037;&#20316;&#65307;&#19982;&#27492;&#21516;&#26102;&#65292;&#20154;&#31867;&#25805;&#20316;&#21592;&#30417;&#25511;&#36825;&#20010;&#36807;&#31243;&#65292;&#24182;&#22312;&#25361;&#25112;&#24615;&#24773;&#20917;&#19979;&#36827;&#34892;&#24178;&#39044;&#12290;&#36825;&#26679;&#30340;&#20154;&#26426;&#22242;&#38431;&#30830;&#20445;&#20102;&#22797;&#26434;&#20219;&#21153;&#30340;&#23433;&#20840;&#37096;&#32626;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#23398;&#20064;&#31639;&#27861;&#26469;&#25913;&#36827;&#20174;&#20219;&#21153;&#25191;&#34892;&#20013;&#25910;&#38598;&#30340;&#25968;&#25454;&#23545;&#31574;&#30053;&#30340;&#24615;&#33021;&#30340;&#24433;&#21709;&#12290;&#26680;&#24515;&#24605;&#24819;&#26159;&#20351;&#29992;&#36817;&#20284;&#20154;&#31867;&#34892;&#20026;&#30340;&#26679;&#26412;&#23545;&#35757;&#32451;&#26679;&#26412;&#36827;&#34892;&#37325;&#26032;&#21152;&#26435;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid growth of computing powers and recent advances in deep learning, we have witnessed impressive demonstrations of novel robot capabilities in research settings. Nonetheless, these learning systems exhibit brittle generalization and require excessive training data for practical tasks. To harness the capabilities of state-of-the-art robot learning models while embracing their imperfections, we present Sirius, a principled framework for humans and robots to collaborate through a division of work. In this framework, partially autonomous robots are tasked with handling a major portion of decision-making where they work reliably; meanwhile, human operators monitor the process and intervene in challenging situations. Such a human-robot team ensures safe deployments in complex tasks. Further, we introduce a new learning algorithm to improve the policy's performance on the data collected from the task executions. The core idea is re-weighing training samples with approximated human
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#20449;&#24687;&#35770;&#27604;&#36739;&#20102;&#24120;&#29992;&#30340;&#30417;&#30563;&#20449;&#21495;&#23545;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#24182;&#20026;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#20351;&#29992;&#30828;&#26631;&#31614;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#20294;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20998;&#24067;&#22806;&#27867;&#21270;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20016;&#23500;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;</title><link>http://arxiv.org/abs/2211.01407</link><description>&lt;p&gt;
&#20851;&#20110;&#30417;&#30563;&#20449;&#21495;&#30340;&#20449;&#24687;&#37327;
&lt;/p&gt;
&lt;p&gt;
On the Informativeness of Supervision Signals. (arXiv:2211.01407v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.01407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#20449;&#24687;&#35770;&#27604;&#36739;&#20102;&#24120;&#29992;&#30340;&#30417;&#30563;&#20449;&#21495;&#23545;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#24182;&#20026;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#20351;&#29992;&#30828;&#26631;&#31614;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#20294;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20998;&#24067;&#22806;&#27867;&#21270;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20016;&#23500;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#30563;&#23398;&#20064;&#36890;&#24120;&#20391;&#37325;&#20110;&#20174;&#20154;&#31867;&#26631;&#27880;&#30340;&#35757;&#32451;&#31034;&#20363;&#20013;&#23398;&#20064;&#21487;&#36716;&#31227;&#30340;&#34920;&#31034;&#12290;&#34429;&#28982;&#20016;&#23500;&#30340;&#27880;&#37322;&#65288;&#22914;&#36719;&#26631;&#31614;&#65289;&#27604;&#31232;&#30095;&#30340;&#27880;&#37322;&#65288;&#22914;&#30828;&#26631;&#31614;&#65289;&#25552;&#20379;&#26356;&#22810;&#20449;&#24687;&#65292;&#20294;&#23427;&#20204;&#30340;&#25910;&#38598;&#25104;&#26412;&#20063;&#26356;&#39640;&#12290;&#25105;&#20204;&#20351;&#29992;&#20449;&#24687;&#35770;&#27604;&#36739;&#20102;&#35768;&#22810;&#24120;&#29992;&#30340;&#30417;&#30563;&#20449;&#21495;&#23545;&#20110;&#34920;&#31034;&#23398;&#20064;&#24615;&#33021;&#30340;&#36129;&#29486;&#65292;&#20197;&#21450;&#23427;&#20204;&#30340;&#33021;&#21147;&#22914;&#20309;&#21463;&#21040;&#26631;&#31614;&#25968;&#12289;&#31867;&#21035;&#12289;&#32500;&#24230;&#21644;&#22122;&#22768;&#31561;&#22240;&#32032;&#30340;&#24433;&#21709;&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#20026;&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#20351;&#29992;&#30828;&#26631;&#31614;&#25552;&#20379;&#20102;&#29702;&#35770;&#19978;&#30340;&#35777;&#26126;&#65292;&#20294;&#23545;&#20110;&#23569;&#26679;&#26412;&#23398;&#20064;&#21644;&#20998;&#24067;&#22806;&#27867;&#21270;&#65292;&#38656;&#35201;&#20351;&#29992;&#26356;&#20016;&#23500;&#30340;&#30417;&#30563;&#20449;&#21495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Supervised learning typically focuses on learning transferable representations from training examples annotated by humans. While rich annotations (like soft labels) carry more information than sparse annotations (like hard labels), they are also more expensive to collect. For example, while hard labels only provide information about the closest class an object belongs to (e.g., "this is a dog"), soft labels provide information about the object's relationship with multiple classes (e.g., "this is most likely a dog, but it could also be a wolf or a coyote"). We use information theory to compare how a number of commonly-used supervision signals contribute to representation-learning performance, as well as how their capacity is affected by factors such as the number of labels, classes, dimensions, and noise. Our framework provides theoretical justification for using hard labels in the big-data regime, but richer supervision signals for few-shot learning and out-of-distribution generalizati
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#24515;&#29702;&#23398;&#26041;&#27861;&#65292;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#23454;&#39564;&#24515;&#29702;&#23398;&#65292;&#36890;&#36807;&#20998;&#26512;ASMR&#38899;&#39057;&#20013;&#30340;&#24490;&#29615;&#29305;&#24449;&#65292;&#21512;&#25104;&#33021;&#35302;&#21457;ASMR&#25928;&#24212;&#30340;ASMR&#29255;&#27573;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;ASMR&#25928;&#24212;&#35302;&#21457;&#30340;&#19968;&#31181;&#21487;&#33021;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2210.14321</link><description>&lt;p&gt;
&#20154;&#24037;ASMR&#65306;&#19968;&#31181;&#32593;&#32476;&#24515;&#29702;&#23398;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Artificial ASMR: A Cyber-Psychological Approach. (arXiv:2210.14321v3 [eess.AS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14321
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#32593;&#32476;&#24515;&#29702;&#23398;&#26041;&#27861;&#65292;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#23454;&#39564;&#24515;&#29702;&#23398;&#65292;&#36890;&#36807;&#20998;&#26512;ASMR&#38899;&#39057;&#20013;&#30340;&#24490;&#29615;&#29305;&#24449;&#65292;&#21512;&#25104;&#33021;&#35302;&#21457;ASMR&#25928;&#24212;&#30340;ASMR&#29255;&#27573;&#65292;&#20174;&#32780;&#25581;&#31034;&#20102;ASMR&#25928;&#24212;&#35302;&#21457;&#30340;&#19968;&#31181;&#21487;&#33021;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#24863;&#23448;&#32463;&#32476;&#21453;&#24212;&#65288;ASMR&#65289;&#22312;&#36807;&#21435;&#21313;&#24180;&#20013;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#19968;&#30452;&#22312;&#39129;&#21319;&#65292;&#20294;&#20851;&#20110;ASMR&#25928;&#24212;&#30340;&#20934;&#30830;&#35302;&#21457;&#22240;&#32032;&#30340;&#31185;&#23398;&#30740;&#31350;&#20173;&#28982;&#24456;&#23569;&#19988;&#19981;&#25104;&#29087;&#65292;&#20854;&#20013;&#19968;&#20010;&#34987;&#24191;&#20026;&#35748;&#21487;&#30340;&#35302;&#21457;&#22240;&#32032;&#26159;ASMR&#29255;&#27573;&#36890;&#24120;&#25552;&#20379;&#20016;&#23500;&#30340;&#35821;&#20041;&#20449;&#24687;&#12290;&#36890;&#36807;&#23545;ASMR&#38899;&#39057;&#20013;&#24120;&#35265;&#22768;&#23398;&#27169;&#24335;&#30340;&#20851;&#27880;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#38899;&#39057;&#20449;&#21495;&#30340;&#24490;&#29615;&#29305;&#24449;&#19982;&#20854;&#35302;&#21457;ASMR&#25928;&#24212;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#37319;&#21462;&#20102;&#19968;&#31181;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#12289;&#20154;&#24037;&#26234;&#33021;&#21644;&#23454;&#39564;&#24515;&#29702;&#23398;&#30340;&#32593;&#32476;&#24515;&#29702;&#23398;&#26041;&#27861;&#65292;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#25105;&#20204;&#33021;&#22815;&#37327;&#21270;&#19982;ASMR&#30456;&#20851;&#30340;&#22768;&#23398;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#38543;&#26426;&#24490;&#29615;&#27169;&#24335;&#21512;&#25104;ASMR&#29255;&#27573;&#65292;&#20294;&#19981;&#21521;&#35266;&#20247;&#25552;&#20379;&#21487;&#35782;&#21035;&#30340;&#24773;&#26223;&#65292;&#36825;&#34987;&#35777;&#26126;&#23545;&#35302;&#21457;ASMR&#25928;&#24212;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
The popularity of Autonomous Sensory Meridian Response (ASMR) has skyrockted over the past decade, but scientific studies on what exactly triggered ASMR effect remain few and immature, one most commonly acknowledged trigger is that ASMR clips typically provide rich semantic information. With our attention caught by the common acoustic patterns in ASMR audios, we investigate the correlation between the cyclic features of audio signals and their effectiveness in triggering ASMR effects. A cyber-psychological approach that combines signal processing, artificial intelligence, and experimental psychology is taken, with which we are able to quantize ASMR-related acoustic features, and therewith synthesize ASMR clips with random cyclic patterns but not delivering identifiably scenarios to the audience, which were proven to be effective in triggering ASMR effects.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23646;&#24615;&#36776;&#21035;&#28508;&#31354;&#38388;&#36827;&#34892;&#35821;&#35328;&#21435;&#27602;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#25237;&#24433;&#21040;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#23646;&#24615;&#23558;&#25991;&#26412;&#36827;&#34892;&#33391;&#22909;&#20998;&#31163;&#30340;&#28508;&#31354;&#38388;&#19978;&#65292;&#26368;&#23567;&#21270;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#65292;&#23454;&#29616;&#20102;&#23545;&#26377;&#27602;&#25991;&#26412;&#30340;&#25511;&#21046;&#12290;</title><link>http://arxiv.org/abs/2210.10329</link><description>&lt;p&gt;
&#24102;&#23646;&#24615;&#36776;&#21035;&#28508;&#31354;&#38388;&#30340;&#35821;&#35328;&#21435;&#27602;&#21270;
&lt;/p&gt;
&lt;p&gt;
Language Detoxification with Attribute-Discriminative Latent Space. (arXiv:2210.10329v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10329
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20351;&#29992;&#23646;&#24615;&#36776;&#21035;&#28508;&#31354;&#38388;&#36827;&#34892;&#35821;&#35328;&#21435;&#27602;&#21270;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#21407;&#22987;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#25237;&#24433;&#21040;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#23646;&#24615;&#23558;&#25991;&#26412;&#36827;&#34892;&#33391;&#22909;&#20998;&#31163;&#30340;&#28508;&#31354;&#38388;&#19978;&#65292;&#26368;&#23567;&#21270;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#65292;&#23454;&#29616;&#20102;&#23545;&#26377;&#27602;&#25991;&#26412;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#22312;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#20219;&#21153;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20063;&#21487;&#33021;&#29983;&#25104;&#21253;&#21547;&#20398;&#36785;&#12289;&#23041;&#32961;&#21644;&#20149;&#28174;&#31561;&#26377;&#27602;&#25991;&#26412;&#65292;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#25991;&#26412;&#29983;&#25104;&#26041;&#27861;&#26088;&#22312;&#20351;&#29992;&#39069;&#22806;&#30340;&#35821;&#35328;&#27169;&#22411;&#25110;&#25200;&#21160;&#26469;&#21435;&#27602;&#21270;&#26377;&#27602;&#25991;&#26412;&#12290;&#28982;&#32780;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#38656;&#35201;&#36807;&#22810;&#30340;&#20869;&#23384;&#12289;&#35745;&#31639;&#21644;&#26102;&#38388;&#65292;&#36825;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#25104;&#20026;&#20102;&#20005;&#37325;&#30340;&#29942;&#39048;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928; yet &#39640;&#25928;&#30340;&#35821;&#35328;&#21435;&#27602;&#21270;&#26041;&#27861;&#65292;&#20351;&#29992;&#19968;&#20010;&#24102;&#23646;&#24615;&#36776;&#21035;&#30340;&#28508;&#31354;&#38388;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#25237;&#24433;&#22359;&#21644;&#23646;&#24615;&#36776;&#21035;&#22120;&#65292;&#23558;&#21407;&#22987;Transformer&#35821;&#35328;&#27169;&#22411;&#30340;&#28508;&#31354;&#38388;&#25237;&#24433;&#21040;&#19968;&#20010;&#33021;&#22815;&#36890;&#36807;&#23646;&#24615;&#23558;&#25991;&#26412;&#36827;&#34892;&#33391;&#22909;&#20998;&#31163;&#30340;&#36776;&#21035;&#24615;&#28508;&#31354;&#38388;&#19978;&#12290;&#36825;&#20801;&#35768;&#35821;&#35328;&#27169;&#22411;&#22312;&#26368;&#23567;&#30340;&#20869;&#23384;&#21644;&#35745;&#31639;&#24320;&#38144;&#19979;&#25511;&#21046;&#25991;&#26412;&#29983;&#25104;&#20026;&#38750;&#26377;&#27602;&#30340;&#12290;&#25105;&#20204;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;
&lt;/p&gt;
&lt;p&gt;
Transformer-based Language Models (LMs) have achieved impressive results on natural language understanding tasks, but they can also generate toxic text such as insults, threats, and profanity, limiting their real-world applications. To overcome this issue, a few text generation approaches aim to detoxify toxic texts using additional LMs or perturbations. However, previous methods require excessive memory, computations, and time which are serious bottlenecks in their real-world application. To address such limitations, we propose an effective yet efficient method for language detoxification using an attribute-discriminative latent space. Specifically, we project the latent space of an original Transformer LM onto a discriminative latent space that well-separates texts by their attributes using a projection block and an attribute discriminator. This allows the LM to control the text generation to be non-toxic with minimal memory and computation overhead. We validate our model, Attribute-
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;ExSSNeT&#65292;&#36890;&#36807;&#29420;&#21344;&#36229;&#25513;&#30721;&#23376;&#32593;&#32476;&#35757;&#32451;&#21644;KNN-based&#30693;&#35782;&#20256;&#36882;&#65292;&#35299;&#20915;&#20102;&#22266;&#23450;&#26435;&#37325;&#38480;&#21046;&#21644;&#30693;&#35782;&#31215;&#32047;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.10209</link><description>&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#30340;&#29420;&#21344;&#36229;&#25513;&#30721;&#23376;&#32593;&#32476;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Exclusive Supermask Subnetwork Training for Continual Learning. (arXiv:2210.10209v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.10209
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;ExSSNeT&#65292;&#36890;&#36807;&#29420;&#21344;&#36229;&#25513;&#30721;&#23376;&#32593;&#32476;&#35757;&#32451;&#21644;KNN-based&#30693;&#35782;&#20256;&#36882;&#65292;&#35299;&#20915;&#20102;&#22266;&#23450;&#26435;&#37325;&#38480;&#21046;&#21644;&#30693;&#35782;&#31215;&#32047;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#20851;&#27880;&#22312;&#36991;&#20813;&#28798;&#38590;&#24615;&#36951;&#24536;&#30340;&#21516;&#26102;&#38543;&#30528;&#26102;&#38388;&#32047;&#31215;&#30693;&#35782;&#12290;&#26368;&#36817;&#65292;Wortsman&#31561;&#20154;&#25552;&#20986;&#20102;&#19968;&#31181;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;SupSup&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#19968;&#20010;&#38543;&#26426;&#21021;&#22987;&#21270;&#30340;&#22266;&#23450;&#22522;&#30784;&#32593;&#32476;&#65292;&#24182;&#20026;&#27599;&#20010;&#26032;&#20219;&#21153;&#25214;&#21040;&#19968;&#20010;&#36229;&#25513;&#30721;&#65292;&#20197;&#36873;&#25321;&#24615;&#22320;&#20445;&#30041;&#25110;&#31227;&#38500;&#27599;&#20010;&#26435;&#37325;&#20197;&#20135;&#29983;&#19968;&#20010;&#23376;&#32593;&#32476;&#12290;&#20182;&#20204;&#36890;&#36807;&#19981;&#26356;&#26032;&#32593;&#32476;&#26435;&#37325;&#26469;&#36991;&#20813;&#36951;&#24536;&#12290;&#34429;&#28982;&#27809;&#26377;&#36951;&#24536;&#65292;&#20294;SupSup&#30340;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#22266;&#23450;&#26435;&#37325;&#38480;&#21046;&#20102;&#20854;&#34920;&#24449;&#33021;&#21147;&#12290;&#27492;&#22806;&#65292;&#22312;&#23398;&#20064;&#26032;&#20219;&#21153;&#26102;&#65292;&#27169;&#22411;&#20869;&#37096;&#27809;&#26377;&#30693;&#35782;&#30340;&#31215;&#32047;&#25110;&#20256;&#36882;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;ExSSNeT&#65288;&#29420;&#21344;&#36229;&#25513;&#30721;&#23376;&#32593;&#32476;&#35757;&#32451;&#65289;&#65292;&#23427;&#36827;&#34892;&#20102;&#29420;&#26377;&#19988;&#19981;&#37325;&#21472;&#30340;&#23376;&#32593;&#32476;&#26435;&#37325;&#35757;&#32451;&#65292;&#36991;&#20813;&#20102;&#21518;&#32493;&#20219;&#21153;&#23545;&#20849;&#20139;&#26435;&#37325;&#30340;&#20914;&#31361;&#26356;&#26032;&#65292;&#20174;&#32780;&#25552;&#39640;&#24615;&#33021;&#30340;&#21516;&#26102;&#20173;&#28982;&#38450;&#27490;&#36951;&#24536;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;KNN&#30340;&#30693;&#35782;&#20256;&#36882;&#65288;KKT&#65289;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual Learning (CL) methods focus on accumulating knowledge over time while avoiding catastrophic forgetting. Recently, Wortsman et al. (2020) proposed a CL method, SupSup, which uses a randomly initialized, fixed base network (model) and finds a supermask for each new task that selectively keeps or removes each weight to produce a subnetwork. They prevent forgetting as the network weights are not being updated. Although there is no forgetting, the performance of SupSup is sub-optimal because fixed weights restrict its representational power. Furthermore, there is no accumulation or transfer of knowledge inside the model when new tasks are learned. Hence, we propose ExSSNeT (Exclusive Supermask SubNEtwork Training), that performs exclusive and non-overlapping subnetwork weight training. This avoids conflicting updates to the shared weights by subsequent tasks to improve performance while still preventing forgetting. Furthermore, we propose a novel KNN-based Knowledge Transfer (KKT)
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;</title><link>http://arxiv.org/abs/2209.15240</link><description>&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Universal Prompt Tuning for Graph Neural Networks. (arXiv:2209.15240v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15240
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature&#65288;GPF&#65289;&#30340;&#26032;&#26041;&#27861;&#65292;&#21487;&#36890;&#29992;&#22320;&#35843;&#25972;&#39044;&#20808;&#35757;&#32451;&#36807;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#25805;&#20316;&#20110;&#36755;&#20837;&#30340;&#29305;&#24449;&#31354;&#38388;&#65292;&#33021;&#22815;&#23545;&#24212;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;Prompt&#35843;&#25972;&#22312;&#36866;&#24212;&#39044;&#35757;&#32451;&#27169;&#22411;&#26041;&#38754;&#24341;&#36215;&#20102;&#30740;&#31350;&#28909;&#28526;&#12290;&#19982;&#35821;&#35328;&#39046;&#22495;&#37319;&#29992;&#30340;&#32479;&#19968;&#39044;&#35757;&#32451;&#31574;&#30053;&#19981;&#21516;&#65292;&#22270;&#24418;&#39046;&#22495;&#23637;&#31034;&#20102;&#22810;&#26679;&#21270;&#30340;&#39044;&#35757;&#32451;&#31574;&#30053;&#65292;&#35774;&#35745;&#36866;&#24403;&#30340;&#22522;&#20110;Prompt&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#35843;&#25972;&#26041;&#27861;&#38754;&#20020;&#25361;&#25112;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;Graph Prompt Feature (GPF) &#30340;&#36890;&#29992;Prompt&#35843;&#25972;&#26041;&#27861;&#65292;&#21487;&#36866;&#29992;&#20110;&#20219;&#20309;&#39044;&#35757;&#32451;&#31574;&#30053;&#19979;&#30340;&#39044;&#35757;&#32451;&#22270;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#12290;GPF&#22312;&#36755;&#20837;&#22270;&#24418;&#30340;&#29305;&#24449;&#31354;&#38388;&#19978;&#25805;&#20316;&#65292;&#29702;&#35770;&#19978;&#21487;&#23454;&#29616;&#19982;&#20219;&#20309;&#24418;&#24335;&#30340;Prompt&#20989;&#25968;&#31561;&#25928;&#30340;&#25928;&#26524;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#19981;&#20877;&#38656;&#35201;&#26126;&#30830;&#35828;&#26126;&#27599;&#20010;&#39044;&#35757;&#32451;&#31574;&#30053;&#23545;&#24212;&#30340;Prompt&#20989;&#25968;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#37319;&#29992;GPF&#26469;&#23454;&#29616;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, prompt tuning has sparked a research surge in adapting pre-trained models. Unlike the unified pre-training strategy employed in the language field, the graph field exhibits diverse pre-training strategies, posing challenges in designing appropriate prompt-based tuning methods for graph neural networks. While some pioneering work has devised specialized prompting functions for models that employ edge prediction as their pre-training tasks, these methods are limited to specific pre-trained GNN models and lack broader applicability. In this paper, we introduce a universal prompt-based tuning method called Graph Prompt Feature (GPF) for pre-trained GNN models under any pre-training strategy. GPF operates on the input graph's feature space and can theoretically achieve an equivalent effect to any form of prompting function. Consequently, we no longer need to illustrate the prompting function corresponding to each pre-training strategy explicitly. Instead, we employ GPF to o
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#32467;&#21512;&#23545;&#31216;&#34892;&#20026;&#21644;&#35821;&#35328;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26497;&#22823;&#22320;&#25913;&#21892;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2209.10656</link><description>&lt;p&gt;
&#20174;&#23545;&#31216;&#20013;&#23398;&#20064;&#65306;&#20855;&#26377;&#23545;&#31216;&#34892;&#20026;&#21644;&#35821;&#35328;&#25351;&#20196;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning from Symmetry: Meta-Reinforcement Learning with Symmetrical Behaviors and Language Instructions. (arXiv:2209.10656v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10656
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#30740;&#31350;&#20102;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#32467;&#21512;&#23545;&#31216;&#34892;&#20026;&#21644;&#35821;&#35328;&#25351;&#20196;&#30340;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26497;&#22823;&#22320;&#25913;&#21892;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20803;&#24378;&#21270;&#23398;&#20064;&#26159;&#19968;&#31181;&#26377;&#26395;&#33021;&#22815;&#20351;&#20195;&#29702;&#24555;&#36895;&#23398;&#20064;&#26032;&#20219;&#21153;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#21482;&#26377;&#22870;&#21169;&#25552;&#20379;&#30340;&#20219;&#21153;&#20449;&#24687;&#19981;&#36275;&#65292;&#22823;&#22810;&#25968;&#20803;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#22312;&#22810;&#20219;&#21153;&#22330;&#26223;&#20013;&#34920;&#29616;&#20986;&#24456;&#24046;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20197;&#35821;&#35328;&#20026;&#26465;&#20214;&#30340;&#20803;&#24378;&#21270;&#23398;&#20064;&#36890;&#36807;&#23558;&#35821;&#35328;&#25351;&#20196;&#19982;&#20195;&#29702;&#30340;&#34892;&#20026;&#36827;&#34892;&#21305;&#37197;&#65292;&#25552;&#39640;&#20102;&#27867;&#21270;&#33021;&#21147;&#12290;&#34892;&#20026;&#21644;&#35821;&#35328;&#25351;&#20196;&#37117;&#20855;&#26377;&#23545;&#31216;&#24615;&#65292;&#36825;&#21487;&#20197;&#21152;&#36895;&#20154;&#31867;&#23545;&#26032;&#30693;&#35782;&#30340;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#23558;&#23545;&#31216;&#24615;&#21644;&#35821;&#35328;&#25351;&#20196;&#32467;&#21512;&#21040;&#20803;&#24378;&#21270;&#23398;&#20064;&#20013;&#21487;&#20197;&#25913;&#21892;&#31639;&#27861;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;&#20803;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23545;&#31216;&#34892;&#20026;&#21644;&#35821;&#35328;&#25351;&#20196;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#23398;&#20064;&#26032;&#20219;&#21153;&#12290;&#25105;&#20204;&#22312;&#22810;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#25805;&#20316;&#20219;&#21153;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26497;&#22823;&#22320;&#25913;&#21892;&#20803;&#24378;&#21270;&#23398;&#20064;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#23398;&#20064;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Meta-reinforcement learning (meta-RL) is a promising approach that enables the agent to learn new tasks quickly. However, most meta-RL algorithms show poor generalization in multi-task scenarios due to the insufficient task information provided only by rewards. Language-conditioned meta-RL improves the generalization capability by matching language instructions with the agent's behaviors. While both behaviors and language instructions have symmetry, which can speed up human learning of new knowledge. Thus, combining symmetry and language instructions into meta-RL can help improve the algorithm's generalization and learning efficiency. We propose a dual-MDP meta-reinforcement learning method that enables learning new tasks efficiently with symmetrical behaviors and language instructions. We evaluate our method in multiple challenging manipulation tasks, and experimental results show that our method can greatly improve the generalization and learning efficiency of meta-reinforcement lear
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#26500;&#24314;&#21516;&#24577;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29615;&#22659;&#21160;&#21147;&#23398;&#30340;&#37096;&#20998;&#27169;&#22411;&#26469;&#25512;&#26029;&#30456;&#21516;&#29366;&#24577;&#30340;&#29366;&#24577;&#21160;&#20316;&#23545;&#65292;&#20174;&#32780;&#20943;&#23567;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;</title><link>http://arxiv.org/abs/2209.06356</link><description>&lt;p&gt;
&#20351;&#29992;&#23398;&#20064;&#21040;&#30340;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#31616;&#21333;&#29366;&#24577;-&#21160;&#20316;&#25277;&#35937;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
A Simple Approach for State-Action Abstraction using a Learned MDP Homomorphism. (arXiv:2209.06356v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.06356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#26500;&#24314;&#21516;&#24577;&#26144;&#23556;&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#20351;&#29992;&#29615;&#22659;&#21160;&#21147;&#23398;&#30340;&#37096;&#20998;&#27169;&#22411;&#26469;&#25512;&#26029;&#30456;&#21516;&#29366;&#24577;&#30340;&#29366;&#24577;&#21160;&#20316;&#23545;&#65292;&#20174;&#32780;&#20943;&#23567;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21160;&#29289;&#33021;&#22815;&#22312;&#26377;&#38480;&#30340;&#32463;&#39564;&#20013;&#36805;&#36895;&#25512;&#26029;&#20986;&#31561;&#20215;&#22870;&#21169;&#21644;&#36716;&#31227;&#21160;&#21147;&#23398;&#30340;&#29366;&#24577;&#21160;&#20316;&#23545;&#38598;&#21512;&#12290;&#19982;&#27492;&#30456;&#21453;&#65292;&#29616;&#20195;&#24378;&#21270;&#23398;&#20064;&#31995;&#32479;&#24517;&#39035;&#36890;&#36807;&#21453;&#22797;&#35797;&#38169;&#26469;&#23398;&#20064;&#29366;&#24577;&#21160;&#20316;&#23545;&#38598;&#21512;&#30340;&#20540;&#31561;&#20215;&#24615;&#65292;&#36825;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#26679;&#26412;&#12290;&#24050;&#32463;&#25552;&#20986;&#20102;MDP&#21516;&#24577;&#26144;&#23556;&#30340;&#26041;&#27861;&#65292;&#23558;&#29615;&#22659;&#30340;&#35266;&#23519;MDP&#31616;&#21270;&#20026;&#25277;&#35937;MDP&#65292;&#21487;&#20197;&#23454;&#29616;&#26356;&#39640;&#25928;&#30340;&#31574;&#30053;&#23398;&#20064;&#12290;&#22240;&#27492;&#65292;&#24403;&#21487;&#20197;&#20107;&#20808;&#26500;&#24314;&#36866;&#24403;&#30340;MDP&#21516;&#24577;&#26144;&#23556;&#26102;&#65292;&#21487;&#20197;&#21462;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#26679;&#26412;&#25928;&#29575;&#25913;&#36827;&#65292;&#36890;&#24120;&#36890;&#36807;&#21033;&#29992;&#29615;&#22659;&#30340;&#23545;&#31216;&#24615;&#26469;&#23454;&#29616;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#26469;&#26500;&#24314;&#31163;&#25955;&#21160;&#20316;&#31354;&#38388;&#20013;&#30340;&#21516;&#24577;&#26144;&#23556;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#29615;&#22659;&#21160;&#21147;&#23398;&#30340;&#37096;&#20998;&#27169;&#22411;&#26469;&#25512;&#26029;&#21738;&#20123;&#29366;&#24577;&#21160;&#20316;&#23545;&#23548;&#33268;&#30456;&#21516;&#30340;&#29366;&#24577;&#65292;&#20174;&#32780;&#20943;&#23567;&#29366;&#24577;-&#21160;&#20316;&#31354;&#38388;&#30340;&#22823;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;
Animals are able to rapidly infer from limited experience when sets of state action pairs have equivalent reward and transition dynamics. On the other hand, modern reinforcement learning systems must painstakingly learn through trial and error that sets of state action pairs are value equivalent -- requiring an often prohibitively large amount of samples from their environment. MDP homomorphisms have been proposed that reduce the observed MDP of an environment to an abstract MDP, which can enable more sample efficient policy learning. Consequently, impressive improvements in sample efficiency have been achieved when a suitable MDP homomorphism can be constructed a priori -- usually by exploiting a practioner's knowledge of environment symmetries. We propose a novel approach to constructing a homomorphism in discrete action spaces, which uses a partial model of environment dynamics to infer which state action pairs lead to the same state -- reducing the size of the state-action space by
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#35821;&#35328;&#31867;&#22411;&#21644;&#31639;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#26816;&#27979;&#21644;&#21453;&#24212;&#21512;&#25104;&#31561;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#12290;</title><link>http://arxiv.org/abs/2209.02307</link><description>&lt;p&gt;
&#23433;&#20840;&#21644;&#21327;&#23433;&#20840;&#35821;&#35328;&#30340;&#19968;&#38454;&#36923;&#36753;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
A first-order logic characterization of safety and co-safety languages. (arXiv:2209.02307v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.02307
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20123;&#26032;&#30340;&#35821;&#35328;&#31867;&#22411;&#21644;&#31639;&#27861;&#65292;&#21487;&#20197;&#38477;&#20302;&#27169;&#22411;&#26816;&#27979;&#21644;&#21453;&#24212;&#21512;&#25104;&#31561;&#38382;&#39064;&#30340;&#22797;&#26434;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32447;&#24615;&#26102;&#38388;&#36923;&#36753;&#65288;LTL&#65289;&#26159;&#26368;&#21463;&#27426;&#36814;&#30340;&#26102;&#38388;&#36923;&#36753;&#20043;&#19968;&#65292;&#24191;&#27867;&#24212;&#29992;&#20110;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#21508;&#20010;&#39046;&#22495;&#12290;LTL&#30340;&#24378;&#22823;&#22522;&#30784;&#24615;&#36136;&#20043;&#19968;&#26159;&#20854;&#31561;&#20215;&#20110;&#26080;&#35745;&#25968;&#969;-&#33258;&#21160;&#26426;&#12289;&#26143;&#33258;&#30001;&#969;-&#27491;&#21017;&#34920;&#36798;&#24335;&#20197;&#21450;&#65288;&#36890;&#36807;Kamp&#23450;&#29702;&#65289;&#32447;&#24615;&#24207;&#30340;&#19968;&#38454;&#29702;&#35770;&#65288;FO-TLO&#65289;&#12290;&#23433;&#20840;&#35821;&#35328;&#21644;&#21327;&#23433;&#20840;&#35821;&#35328;&#20998;&#21035;&#25351;&#21482;&#38656;&#26377;&#38480;&#21069;&#32512;&#20415;&#21487;&#30830;&#23450;&#35813;&#21333;&#35789;&#23646;&#20110;&#25110;&#19981;&#23646;&#20110;&#35813;&#35821;&#35328;&#30340;&#35821;&#35328;&#31867;&#22411;&#12290;SafetyLTL&#65288;&#21644;coSafetyLTL&#65289;&#26159;LTL&#30340;&#19968;&#20010;&#29255;&#27573;&#65292;&#20854;&#20013;&#20165;&#20801;&#35768;&#20351;&#29992;&#20840;&#23616;&#65288;&#23384;&#22312;&#65289;&#26102;&#38388;&#20462;&#39280;&#35789;&#26469;&#35782;&#21035;&#23433;&#20840;&#65288;&#21327;&#23433;&#20840;&#65289;&#35821;&#35328;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#24341;&#20837;FO-TLO&#30340;&#29255;&#27573;SafetyFO&#20197;&#21450;&#20854;&#23545;&#20598;&#30340;coSafetyFO&#65292;&#23427;&#20204;&#22312;&#34920;&#36798;&#33021;&#21147;&#19978;&#37117;&#26159;&#23436;&#22791;&#30340;&#65288;except&#19968;&#20123;&#36793;&#30028;&#24773;&#20917;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Linear Temporal Logic (LTL) is one of the most popular temporal logics, that comes into play in a variety of branches of computer science. Among the various reasons of its widespread use there are its strong foundational properties: LTL is equivalent to counter-free omega-automata, to star-free omega-regular expressions, and (by Kamp's theorem) to the First-Order Theory of Linear Orders (FO-TLO). Safety and co-safety languages, where a finite prefix suffices to establish whether a word does not belong or belongs to the language, respectively, play a crucial role in lowering the complexity of problems like model checking and reactive synthesis for LTL. SafetyLTL (resp., coSafetyLTL) is a fragment of LTL where only universal (resp., existential) temporal modalities are allowed, that recognises safety (resp., co-safety) languages only. The main contribution of this paper is the introduction of a fragment of FO-TLO, called SafetyFO, and of its dual coSafetyFO, which are expressively comple
&lt;/p&gt;</description></item><item><title>SFusion&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#34701;&#21512;&#27169;&#22359;&#65292;&#29992;&#20110;&#35299;&#20915;N&#23545;&#19968;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#38382;&#39064;&#65292;&#19981;&#38656;&#35201;&#21512;&#25104;&#25110;&#22635;&#20805;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#23427;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#34701;&#21512;&#21487;&#29992;&#30340;&#27169;&#24577;&#65292;&#24182;&#26500;&#24314;&#20849;&#20139;&#34920;&#31034;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;&#30340;&#34701;&#21512;&#65292;&#20197;&#20415;&#19979;&#28216;&#20915;&#31574;&#27169;&#22411;&#20351;&#29992;&#12290;</title><link>http://arxiv.org/abs/2208.12776</link><description>&lt;p&gt;
SFusion: &#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;N&#23545;&#19968;&#22810;&#27169;&#24577;&#34701;&#21512;&#27169;&#22359;
&lt;/p&gt;
&lt;p&gt;
SFusion: Self-attention based N-to-One Multimodal Fusion Block. (arXiv:2208.12776v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.12776
&lt;/p&gt;
&lt;p&gt;
SFusion&#26159;&#19968;&#20010;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#34701;&#21512;&#27169;&#22359;&#65292;&#29992;&#20110;&#35299;&#20915;N&#23545;&#19968;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#38382;&#39064;&#65292;&#19981;&#38656;&#35201;&#21512;&#25104;&#25110;&#22635;&#20805;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#23427;&#36890;&#36807;&#33258;&#21160;&#23398;&#20064;&#34701;&#21512;&#21487;&#29992;&#30340;&#27169;&#24577;&#65292;&#24182;&#26500;&#24314;&#20849;&#20139;&#34920;&#31034;&#26469;&#23454;&#29616;&#22810;&#27169;&#24577;&#30340;&#34701;&#21512;&#65292;&#20197;&#20415;&#19979;&#28216;&#20915;&#31574;&#27169;&#22411;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#20204;&#36890;&#36807;&#19981;&#21516;&#30340;&#24863;&#23448;&#65288;&#22914;&#35270;&#35273;&#12289;&#21548;&#35273;&#12289;&#21957;&#35273;&#21644;&#35302;&#35273;&#65289;&#26469;&#24863;&#30693;&#19990;&#30028;&#12290;&#22788;&#29702;&#21644;&#34701;&#21512;&#26469;&#33258;&#22810;&#20010;&#27169;&#24577;&#30340;&#20449;&#24687;&#20351;&#24471;&#20154;&#24037;&#26234;&#33021;&#26356;&#23481;&#26131;&#29702;&#35299;&#25105;&#20204;&#21608;&#22260;&#30340;&#19990;&#30028;&#12290;&#28982;&#32780;&#65292;&#24403;&#23384;&#22312;&#32570;&#22833;&#30340;&#27169;&#24577;&#26102;&#65292;&#22312;&#19981;&#21516;&#30340;&#24773;&#20917;&#19979;&#21487;&#29992;&#30340;&#27169;&#24577;&#25968;&#37327;&#26159;&#19981;&#21516;&#30340;&#65292;&#36825;&#23601;&#23548;&#33268;&#20102;&#19968;&#20010;N&#23545;&#19968;&#30340;&#34701;&#21512;&#38382;&#39064;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#33258;&#27880;&#24847;&#21147;&#30340;&#34701;&#21512;&#27169;&#22359;&#65292;&#31216;&#20026;SFusion&#12290;&#19982;&#39044;&#35774;&#30340;&#20844;&#24335;&#21270;&#25110;&#22522;&#20110;&#21367;&#31215;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;&#25152;&#25552;&#20986;&#30340;&#27169;&#22359;&#33258;&#21160;&#23398;&#20064;&#34701;&#21512;&#21487;&#29992;&#30340;&#27169;&#24577;&#65292;&#32780;&#19981;&#26159;&#21512;&#25104;&#25110;&#22635;&#20805;&#32570;&#22833;&#30340;&#27169;&#24577;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#20174;&#19978;&#28216;&#22788;&#29702;&#27169;&#22411;&#25552;&#21462;&#30340;&#29305;&#24449;&#34920;&#31034;&#34987;&#25237;&#24433;&#20026;&#20196;&#29260;&#65292;&#24182;&#36755;&#20837;&#33258;&#27880;&#24847;&#21147;&#27169;&#22359;&#20197;&#29983;&#25104;&#28508;&#22312;&#30340;&#22810;&#27169;&#24577;&#30456;&#20851;&#24615;&#12290;&#28982;&#21518;&#65292;&#24341;&#20837;&#27169;&#24577;&#27880;&#24847;&#26426;&#21046;&#26500;&#24314;&#19968;&#20010;&#20849;&#20139;&#34920;&#31034;&#65292;&#21487;&#20197;&#34987;&#19979;&#28216;&#20915;&#31574;&#27169;&#22411;&#20351;&#29992;&#12290;&#25152;&#25552;&#20986;&#30340;SFusion&#27169;&#22359;&#33021;&#22815;&#26377;&#25928;&#22320;&#35299;&#20915;N&#23545;&#19968;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
People perceive the world with different senses, such as sight, hearing, smell, and touch. Processing and fusing information from multiple modalities enables Artificial Intelligence to understand the world around us more easily. However, when there are missing modalities, the number of available modalities is different in diverse situations, which leads to an N-to-One fusion problem. To solve this problem, we propose a self-attention based fusion block called SFusion. Different from preset formulations or convolution based methods, the proposed block automatically learns to fuse available modalities without synthesizing or zero-padding missing ones. Specifically, the feature representations extracted from upstream processing model are projected as tokens and fed into self-attention module to generate latent multimodal correlations. Then, a modal attention mechanism is introduced to build a shared representation, which can be applied by the downstream decision model. The proposed SFusio
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26080;softmax&#30340;&#32447;&#24615;&#21464;&#25442;&#22120;(SOFT)&#65292;&#29992;&#39640;&#26031;&#26680;&#20989;&#25968;&#26469;&#36924;&#36817;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;&#35782;&#21035;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.03341</link><description>&lt;p&gt;
&#26080;Softmax&#30340;&#32447;&#24615;&#21464;&#25442;&#22120;
&lt;/p&gt;
&lt;p&gt;
Softmax-free Linear Transformers. (arXiv:2207.03341v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.03341
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#26080;softmax&#30340;&#32447;&#24615;&#21464;&#25442;&#22120;(SOFT)&#65292;&#29992;&#39640;&#26031;&#26680;&#20989;&#25968;&#26469;&#36924;&#36817;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#20197;&#25913;&#21892;&#35270;&#35273;&#35782;&#21035;&#39046;&#22495;&#20013;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#21464;&#25442;&#22120;(ViTs)&#22312;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#30340;&#26368;&#26032;&#25104;&#26524;&#20013;&#36215;&#21040;&#20102;&#25512;&#21160;&#20316;&#29992;&#12290;ViTs&#30340;&#26680;&#24515;&#33258;&#27880;&#24847;&#26426;&#21046;&#22312;&#35745;&#31639;&#21644;&#20869;&#23384;&#20351;&#29992;&#26041;&#38754;&#20855;&#26377;&#20108;&#27425;&#22797;&#26434;&#24230;&#12290;&#36825;&#20419;&#20351;&#25105;&#20204;&#24320;&#21457;&#20986;&#22312;&#32447;&#24615;&#22797;&#26434;&#24230;&#19979;&#36924;&#36817;&#33258;&#27880;&#24847;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#26412;&#30740;&#31350;&#30340;&#28145;&#20837;&#20998;&#26512;&#21457;&#29616;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#35270;&#35273;&#35782;&#21035;&#26041;&#38754;&#35201;&#20040;&#22312;&#29702;&#35770;&#19978;&#26377;&#32570;&#38519;&#65292;&#35201;&#20040;&#22312;&#23454;&#36341;&#20013;&#26080;&#25928;&#12290;&#25105;&#20204;&#21457;&#29616;&#23427;&#20204;&#30340;&#23616;&#38480;&#24615;&#26469;&#28304;&#20110;&#22312;&#36924;&#36817;&#36807;&#31243;&#20013;&#32487;&#25215;&#20102;&#22522;&#20110;softmax&#30340;&#33258;&#27880;&#24847;&#26426;&#21046;&#65292;&#21363;&#20351;&#29992;softmax&#20989;&#25968;&#23545;&#20196;&#29260;&#29305;&#24449;&#21521;&#37327;&#20043;&#38388;&#30340;&#32553;&#25918;&#28857;&#31215;&#36827;&#34892;&#24402;&#19968;&#21270;&#12290;&#30001;&#20110;&#23384;&#22312;&#36825;&#20010;softmax&#25805;&#20316;&#65292;&#25361;&#25112;&#20102;&#20219;&#20309;&#21518;&#32493;&#30340;&#32447;&#24615;&#21270;&#24037;&#20316;&#12290;&#22522;&#20110;&#36825;&#19968;&#35266;&#28857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31995;&#21015;&#26080;softmax&#30340;&#21464;&#25442;&#22120;(SOFT)&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#37319;&#29992;&#39640;&#26031;&#26680;&#20989;&#25968;&#26469;&#26367;&#20195;&#28857;&#31215;&#30456;&#20284;&#24230;&#65292;&#20174;&#32780;&#23454;&#29616;&#20840;&#33258;&#27880;&#24847;&#30697;&#38453;&#30340;&#36924;&#36817;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision transformers (ViTs) have pushed the state-of-the-art for visual perception tasks. The self-attention mechanism underpinning the strength of ViTs has a quadratic complexity in both computation and memory usage. This motivates the development of approximating the self-attention at linear complexity. However, an in-depth analysis in this work reveals that existing methods are either theoretically flawed or empirically ineffective for visual recognition. We identify that their limitations are rooted in the inheritance of softmax-based self-attention during approximations, that is, normalizing the scaled dot-product between token feature vectors using the softmax function. As preserving the softmax operation challenges any subsequent linearization efforts. By this insight, a family of Softmax-Free Transformers (SOFT) are proposed. Specifically, a Gaussian kernel function is adopted to replace the dot-product similarity, enabling a full self-attention matrix to be approximated under l
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;200&#20010;AI&#27835;&#29702;&#25351;&#21335;&#21644;&#24314;&#35758;&#65292;&#36890;&#36807;&#23545;&#36825;&#20123;&#25991;&#26723;&#20869;&#23481;&#21644;&#24615;&#36136;&#30340;&#21487;&#35270;&#21270;&#65292;&#26088;&#22312;&#20102;&#35299;&#21508;&#26426;&#26500;&#38388;AI&#20262;&#29702;&#21407;&#21017;&#23384;&#22312;&#30340;&#20849;&#35782;&#21644;&#30456;&#20284;&#28857;&#65292;&#20026;&#26410;&#26469;&#30340;&#27861;&#35268;&#36777;&#35770;&#25552;&#20379;&#21551;&#31034;&#12290;</title><link>http://arxiv.org/abs/2206.11922</link><description>&lt;p&gt;
&#12298;&#20840;&#29699;AI&#20262;&#29702;&#65306;200&#20010;AI&#27835;&#29702;&#25351;&#21335;&#21644;&#24314;&#35758;&#30340;&#32508;&#36848;&#12299;
&lt;/p&gt;
&lt;p&gt;
Worldwide AI Ethics: a review of 200 guidelines and recommendations for AI governance. (arXiv:2206.11922v5 [cs.CY] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.11922
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#22238;&#39038;&#20102;200&#20010;AI&#27835;&#29702;&#25351;&#21335;&#21644;&#24314;&#35758;&#65292;&#36890;&#36807;&#23545;&#36825;&#20123;&#25991;&#26723;&#20869;&#23481;&#21644;&#24615;&#36136;&#30340;&#21487;&#35270;&#21270;&#65292;&#26088;&#22312;&#20102;&#35299;&#21508;&#26426;&#26500;&#38388;AI&#20262;&#29702;&#21407;&#21017;&#23384;&#22312;&#30340;&#20849;&#35782;&#21644;&#30456;&#20284;&#28857;&#65292;&#20026;&#26410;&#26469;&#30340;&#27861;&#35268;&#36777;&#35770;&#25552;&#20379;&#21551;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#19968;&#20123;&#32452;&#32455;&#24050;&#32463;&#21046;&#23450;&#20102;&#25991;&#20214;&#65292;&#26088;&#22312;&#35268;&#33539;&#21644;&#25512;&#21160;&#25105;&#20204;&#26368;&#36817;&#21644;&#24555;&#36895;&#21457;&#23637;&#30340;AI&#25216;&#26415;&#30340;&#25351;&#21335;&#12290;&#28982;&#32780;&#65292;&#38500;&#20102;&#19968;&#20123;&#39046;&#22495;&#30340;&#20803;&#20998;&#26512;&#21644;&#25209;&#21028;&#24615;&#35780;&#35770;&#22806;&#65292;&#36825;&#20123;&#25991;&#20214;&#20013;&#21576;&#29616;&#30340;&#23436;&#25972;&#24605;&#24819;&#20809;&#35889;&#23578;&#26410;&#24471;&#21040;&#20998;&#26512;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#35797;&#22270;&#25193;&#23637;&#36807;&#21435;&#30740;&#31350;&#20154;&#21592;&#25152;&#20570;&#30340;&#24037;&#20316;&#65292;&#21019;&#24314;&#19968;&#20010;&#26356;&#22909;&#30340;&#25968;&#25454;&#21487;&#35270;&#21270;&#24037;&#20855;&#65292;&#20197;&#20102;&#35299;&#21508;&#26426;&#26500;&#25152;&#20513;&#23548;&#21407;&#21017;&#20043;&#38388;&#26159;&#21542;&#23384;&#22312;&#20849;&#35782;&#25110;&#30456;&#20284;&#20043;&#22788;&#65292;&#36825;&#21487;&#33021;&#20250;&#28608;&#21457;&#26410;&#26469;&#30340;&#27861;&#35268;&#36777;&#35770;&#12290;&#25105;&#20204;&#36824;&#36890;&#36807;&#23545;200&#20221;&#25991;&#26723;&#26679;&#26412;&#30340;&#26041;&#27861;&#35770;&#32467;&#26524;&#36827;&#34892;&#25209;&#21028;&#24615;&#20998;&#26512;&#65292;&#25552;&#20986;&#20102;&#19968;&#20123;&#21021;&#27493;&#30340;&#24819;&#27861;&#21644;&#38382;&#39064;&#65292;&#20197;&#25351;&#23548;&#30740;&#31350;&#30340;&#36830;&#32493;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, several organizations have produced documents intended to standardize, in the normative sense, and promote guidance to our recent and rapid AI development. However, the full spectrum of ideas presented in these documents has not yet been analyzed, except for a few meta-analyses and critical reviews of the field. In this work, we seek to expand on the work done by past researchers and create a tool for better data visualization of the contents and nature of these documents, to understand whether there is consensus or similarity between the principles espoused by various institutions, which may inspire debates on future regulations. We also provide some preliminary thoughts and questions that could guide the continuity of the research through a critical analysis of the results acquired by our methodology into a sample size of 200 documents.
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#19978;&#33021;&#22815;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#30456;&#23218;&#32654;&#65292;&#26263;&#31034;&#28040;&#24687;&#20256;&#36882;&#21487;&#33021;&#19981;&#20687;&#20043;&#21069;&#35748;&#20026;&#30340;&#37027;&#26679;&#20851;&#38190;&#12290;&#35780;&#20998;&#20989;&#25968;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#23545;&#20110;&#27169;&#22411;&#24615;&#33021;&#26377;&#26356;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2205.10652</link><description>&lt;p&gt;
&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#23545;&#20110;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#30495;&#30340;&#26377;&#24110;&#21161;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Are Message Passing Neural Networks Really Helpful for Knowledge Graph Completion?. (arXiv:2205.10652v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.10652
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#21457;&#29616;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#22312;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#20219;&#21153;&#19978;&#33021;&#22815;&#19982;&#28040;&#24687;&#20256;&#36882;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#30456;&#23218;&#32654;&#65292;&#26263;&#31034;&#28040;&#24687;&#20256;&#36882;&#21487;&#33021;&#19981;&#20687;&#20043;&#21069;&#35748;&#20026;&#30340;&#37027;&#26679;&#20851;&#38190;&#12290;&#35780;&#20998;&#20989;&#25968;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#23545;&#20110;&#27169;&#22411;&#24615;&#33021;&#26377;&#26356;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#22312;&#21508;&#31181;&#24212;&#29992;&#20013;&#21457;&#25381;&#20102;&#37325;&#35201;&#20316;&#29992;&#12290;&#23613;&#31649;&#22312;&#21019;&#24314;&#21644;&#32500;&#25252;&#26041;&#38754;&#20570;&#20986;&#20102;&#24040;&#22823;&#21162;&#21147;&#65292;&#20294;&#21363;&#20351;&#26159;&#26368;&#22823;&#30340;KG&#20063;&#36828;&#26410;&#23436;&#22791;&#12290;&#22240;&#27492;&#65292;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#65288;KGC&#65289;&#24050;&#25104;&#20026;KG&#30740;&#31350;&#20013;&#26368;&#20851;&#38190;&#30340;&#20219;&#21153;&#20043;&#19968;&#12290;&#26368;&#36817;&#65292;&#36825;&#19968;&#39046;&#22495;&#30340;&#22823;&#37327;&#25991;&#29486;&#37117;&#38598;&#20013;&#22312;&#20351;&#29992;&#28040;&#24687;&#20256;&#36882;&#65288;&#22270;&#65289;&#31070;&#32463;&#32593;&#32476;&#65288;MPNNs&#65289;&#26469;&#23398;&#20064;&#24378;&#22823;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;&#36825;&#20123;&#26041;&#27861;&#30340;&#25104;&#21151;&#33258;&#28982;&#24402;&#22240;&#20110;&#30456;&#27604;&#20110;&#31616;&#21333;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#27169;&#22411;&#65292;&#20351;&#29992;&#20102;&#39069;&#22806;&#30340;&#28040;&#24687;&#20256;&#36882;&#65288;MP&#65289;&#32452;&#20214;&#30340;MPNNs&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#21457;&#29616;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#31616;&#21333;&#30340;MLP&#27169;&#22411;&#33021;&#22815;&#36798;&#21040;&#19982;MPNNs&#30456;&#24403;&#30340;&#24615;&#33021;&#65292;&#36825;&#34920;&#26126;MP&#21487;&#33021;&#24182;&#19981;&#20687;&#20043;&#21069;&#35748;&#20026;&#30340;&#37027;&#26679;&#20851;&#38190;&#12290;&#36890;&#36807;&#36827;&#19968;&#27493;&#25506;&#32034;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#20180;&#32454;&#30340;&#35780;&#20998;&#20989;&#25968;&#21644;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#23545;&#20110;KGC&#27169;&#22411;&#24615;&#33021;&#26377;&#26356;&#24378;&#30340;&#24433;&#21709;&#12290;&#36825;&#34920;&#26126;&#20197;&#21069;&#30340;&#24037;&#20316;&#20013;&#23545;&#35780;&#20998;&#20989;&#25968;&#35774;&#35745;&#12289;&#25439;&#22833;&#20989;&#25968;&#35774;&#35745;&#21644;MP&#30340;&#28151;&#28102;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) facilitate a wide variety of applications. Despite great efforts in creation and maintenance, even the largest KGs are far from complete. Hence, KG completion (KGC) has become one of the most crucial tasks for KG research. Recently, considerable literature in this space has centered around the use of Message Passing (Graph) Neural Networks (MPNNs), to learn powerful embeddings. The success of these methods is naturally attributed to the use of MPNNs over simpler multi-layer perceptron (MLP) models, given their additional message passing (MP) component. In this work, we find that surprisingly, simple MLP models are able to achieve comparable performance to MPNNs, suggesting that MP may not be as crucial as previously believed. With further exploration, we show careful scoring function and loss function design has a much stronger influence on KGC model performance. This suggests a conflation of scoring function design, loss function design, and MP in prior work, wi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#25668;&#20687;&#22836;&#36712;&#36857;&#29983;&#25104;&#30340;&#34892;&#20154;&#26816;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#20013;&#36827;&#34892;&#20102;&#32467;&#21512;&#12290;&#36890;&#36807;&#25552;&#21462;&#36328;&#25668;&#20687;&#22836;&#36712;&#36857;&#24182;&#36827;&#34892;&#26465;&#20214;&#38543;&#26426;&#22330;&#27169;&#22411;&#21644;&#21463;&#38480;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#20248;&#21270;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;&#26816;&#32034;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2204.12900</link><description>&lt;p&gt;
&#8220;&#36328;&#25668;&#20687;&#22836;&#36712;&#36857;&#36741;&#21161;&#30456;&#26426;&#32593;&#32476;&#20013;&#30340;&#20154;&#21592;&#26816;&#32034;&#8221;
&lt;/p&gt;
&lt;p&gt;
Cross-Camera Trajectories Help Person Retrieval in a Camera Network. (arXiv:2204.12900v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2204.12900
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#25668;&#20687;&#22836;&#36712;&#36857;&#29983;&#25104;&#30340;&#34892;&#20154;&#26816;&#32034;&#26694;&#26550;&#65292;&#35813;&#26694;&#26550;&#22312;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#20013;&#36827;&#34892;&#20102;&#32467;&#21512;&#12290;&#36890;&#36807;&#25552;&#21462;&#36328;&#25668;&#20687;&#22836;&#36712;&#36857;&#24182;&#36827;&#34892;&#26465;&#20214;&#38543;&#26426;&#22330;&#27169;&#22411;&#21644;&#21463;&#38480;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#30340;&#20248;&#21270;&#65292;&#26368;&#32456;&#25552;&#39640;&#20102;&#26816;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20851;&#27880;&#22914;&#20309;&#20174;&#19968;&#20010;&#38750;&#35206;&#30422;&#30340;&#25668;&#20687;&#22836;&#32593;&#32476;&#20013;&#30340;&#22810;&#20010;&#35270;&#39057;&#20013;&#26816;&#32034;&#20986;&#19968;&#20010;&#26597;&#35810;&#23545;&#35937;&#30340;&#36523;&#20221;&#12290;&#30446;&#21069;&#24050;&#26377;&#30340;&#26041;&#27861;&#24448;&#24448;&#20381;&#36182;&#20110;&#35270;&#35273;&#21305;&#37197;&#25110;&#32773;&#32771;&#34385;&#26102;&#38388;&#32422;&#26463;&#65292;&#20294;&#26159;&#24573;&#30053;&#20102;&#25668;&#20687;&#22836;&#32593;&#26684;&#30340;&#31354;&#38388;&#20449;&#24687;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36328;&#25668;&#20687;&#22836;&#36712;&#36857;&#29983;&#25104;&#30340;&#34892;&#20154;&#26816;&#32034;&#26694;&#26550;&#65292;&#23558;&#26102;&#38388;&#21644;&#31354;&#38388;&#20449;&#24687;&#32467;&#21512;&#36215;&#26469;&#12290;&#20026;&#20102;&#33719;&#21462;&#34892;&#20154;&#36712;&#36857;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36328;&#25668;&#20687;&#22836;&#26102;&#31354;&#27169;&#22411;&#65292;&#23427;&#32467;&#21512;&#20102;&#34892;&#20154;&#30340;&#34892;&#36208;&#20064;&#24815;&#21644;&#30456;&#37051;&#25668;&#20687;&#22836;&#20043;&#38388;&#30340;&#36335;&#24452;&#24067;&#23616;&#65292;&#24418;&#25104;&#20102;&#19968;&#20010;&#32852;&#21512;&#27010;&#29575;&#20998;&#24067;&#12290;&#36825;&#26679;&#30340;&#26102;&#31354;&#27169;&#22411;&#21487;&#20197;&#22522;&#20110;&#31232;&#30095;&#37319;&#26679;&#30340;&#34892;&#20154;&#25968;&#25454;&#22312;&#25668;&#20687;&#22836;&#32593;&#32476;&#20013;&#23454;&#29616;&#12290;&#22522;&#20110;&#36825;&#20010;&#26102;&#31354;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#26465;&#20214;&#38543;&#26426;&#22330;&#27169;&#22411;&#25552;&#21462;&#36328;&#25668;&#20687;&#22836;&#36712;&#36857;&#65292;&#24182;&#36890;&#36807;&#21463;&#38480;&#38750;&#36127;&#30697;&#38453;&#20998;&#35299;&#36827;&#34892;&#36827;&#19968;&#27493;&#20248;&#21270;&#12290;&#26368;&#21518;&#65292;&#25991;&#31456;&#25552;&#20986;&#20102;&#19968;&#31181;&#36712;&#36857;&#37325;&#26032;&#25490;&#24207;&#25216;&#26415;&#26469;&#25552;&#39640;&#26816;&#32034;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
We are concerned with retrieving a query person from multiple videos captured by a non-overlapping camera network. Existing methods often rely on purely visual matching or consider temporal constraints but ignore the spatial information of the camera network. To address this issue, we propose a pedestrian retrieval framework based on cross-camera trajectory generation, which integrates both temporal and spatial information. To obtain pedestrian trajectories, we propose a novel cross-camera spatio-temporal model that integrates pedestrians' walking habits and the path layout between cameras to form a joint probability distribution. Such a spatio-temporal model among a camera network can be specified using sparsely sampled pedestrian data. Based on the spatio-temporal model, cross-camera trajectories can be extracted by the conditional random field model and further optimized by restricted non-negative matrix factorization. Finally, a trajectory re-ranking technique is proposed to improv
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35270;&#35282;RGBD&#35270;&#35273;&#21487;&#31649;&#29702;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;37&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;47210&#20010;&#22270;&#20687;&#65292;&#24182;&#19988;&#27599;&#20010;&#22270;&#20687;&#37117;&#24102;&#26377;15&#20010;&#35270;&#35273;&#21487;&#31649;&#29702;&#31867;&#21035;&#30340;&#27880;&#37322;&#12290;</title><link>http://arxiv.org/abs/2203.14092</link><description>&lt;p&gt;
&#21521;&#35270;&#35273;&#21487;&#31649;&#29702;&#23398;&#20064;&#36808;&#36827;&#65306;&#19968;&#20010;&#29992;&#20110;&#21487;&#31649;&#29702;&#20998;&#21106;&#21644;&#35782;&#21035;&#30340;&#22522;&#20934;&#12290; (arXiv:2203.14092v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
Towards Visual Affordance Learning: A Benchmark for Affordance Segmentation and Recognition. (arXiv:2203.14092v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.14092
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35270;&#35282;RGBD&#35270;&#35273;&#21487;&#31649;&#29702;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;37&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;47210&#20010;&#22270;&#20687;&#65292;&#24182;&#19988;&#27599;&#20010;&#22270;&#20687;&#37117;&#24102;&#26377;15&#20010;&#35270;&#35273;&#21487;&#31649;&#29702;&#31867;&#21035;&#30340;&#27880;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#20013;&#30340;&#35782;&#21035;&#12289;&#26816;&#27979;&#21644;&#20998;&#21106;&#20219;&#21153;&#65292;&#23545;&#35937;&#30340;&#29289;&#29702;&#21644;&#32441;&#29702;&#23646;&#24615;&#24050;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#35768;&#22810;&#25968;&#25454;&#38598;&#65292;&#22914;&#22823;&#35268;&#27169;&#30340;ImageNet&#65292;&#24050;&#32463;&#34987;&#25552;&#20986;&#29992;&#20110;&#20351;&#29992;&#25968;&#25454;&#39269;&#39295;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#29305;&#24449;&#23398;&#20064;&#21644;&#25163;&#24037;&#29305;&#24449;&#25552;&#21462;&#12290;&#20026;&#20102;&#26234;&#33021;&#22320;&#19982;&#23545;&#35937;&#36827;&#34892;&#20132;&#20114;&#65292;&#26426;&#22120;&#20154;&#21644;&#26234;&#33021;&#26426;&#22120;&#38656;&#35201;&#38500;&#20102;&#20256;&#32479;&#30340;&#29289;&#29702;/&#32441;&#29702;&#23646;&#24615;&#20043;&#22806;&#30340;&#25512;&#29702;&#33021;&#21147;&#65292;&#20197;&#21450;&#29702;&#35299;/&#23398;&#20064;&#34987;&#31216;&#20026;&#35270;&#35273;&#21487;&#31649;&#29702;&#30340;&#35270;&#35273;&#25552;&#31034;&#65292;&#29992;&#20110;&#21487;&#31649;&#29702;&#30340;&#35782;&#21035;&#12289;&#26816;&#27979;&#21644;&#20998;&#21106;&#12290;&#21040;&#30446;&#21069;&#20026;&#27490;&#65292;&#36824;&#27809;&#26377;&#20844;&#24320;&#21487;&#29992;&#30340;&#29992;&#20110;&#35270;&#35273;&#21487;&#31649;&#29702;&#29702;&#35299;&#21644;&#23398;&#20064;&#30340;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#22810;&#35270;&#35282;RGBD&#35270;&#35273;&#21487;&#31649;&#29702;&#23398;&#20064;&#25968;&#25454;&#38598;&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;37&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;47210&#20010;RGBD&#22270;&#20687;&#30340;&#22522;&#20934;&#65292;&#27599;&#20010;&#22270;&#20687;&#37117;&#24102;&#26377;15&#20010;&#35270;&#35273;&#21487;&#31649;&#29702;&#31867;&#21035;&#30340;&#27880;&#37322;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#36825;&#26159;&#26377;&#21490;&#20197;&#26469;&#31532;&#19968;&#20010;&#20063;&#26159;&#26368;&#22823;&#30340;&#22810;&#35270;&#35282;RGBD&#35270;&#35273;&#21487;&#31649;&#29702;&#23398;&#20064;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
The physical and textural attributes of objects have been widely studied for recognition, detection and segmentation tasks in computer vision.~A number of datasets, such as large scale ImageNet, have been proposed for feature learning using data hungry deep neural networks and for hand-crafted feature extraction. To intelligently interact with objects, robots and intelligent machines need the ability to infer beyond the traditional physical/textural attributes, and understand/learn visual cues, called visual affordances, for affordance recognition, detection and segmentation. To date there is no publicly available large dataset for visual affordance understanding and learning. In this paper, we introduce a large scale multi-view RGBD visual affordance learning dataset, a benchmark of 47210 RGBD images from 37 object categories, annotated with 15 visual affordance categories. To the best of our knowledge, this is the first ever and the largest multi-view RGBD visual affordance learning 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22312;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#31616;&#21333;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#39318;&#20010;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#27966;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#36125;&#21494;&#26031;&#31639;&#27861;&#20855;&#26377;&#20808;&#39564;&#20998;&#24067;&#24182;&#19988;&#20855;&#26377;&#36739;&#23567;&#30340;&#20803;&#31616;&#21333;&#36951;&#25022;&#65292;&#32780;&#39057;&#29575;&#27966;&#31639;&#27861;&#26356;&#36890;&#29992;&#19988;&#21487;&#20197;&#22312;&#26356;&#22810;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20998;&#26512;&#12290;&#36890;&#36807;&#23558;&#31639;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#29702;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2202.12888</link><description>&lt;p&gt;
&#29992;&#20110;&#31616;&#21333;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20803;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Meta-Learning for Simple Regret Minimization. (arXiv:2202.12888v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12888
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#29992;&#20110;&#22312;&#36172;&#21338;&#26426;&#20013;&#36827;&#34892;&#31616;&#21333;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#24182;&#25552;&#20986;&#20102;&#39318;&#20010;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#27966;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#36125;&#21494;&#26031;&#31639;&#27861;&#20855;&#26377;&#20808;&#39564;&#20998;&#24067;&#24182;&#19988;&#20855;&#26377;&#36739;&#23567;&#30340;&#20803;&#31616;&#21333;&#36951;&#25022;&#65292;&#32780;&#39057;&#29575;&#27966;&#31639;&#27861;&#26356;&#36890;&#29992;&#19988;&#21487;&#20197;&#22312;&#26356;&#22810;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20998;&#26512;&#12290;&#36890;&#36807;&#23558;&#31639;&#27861;&#24212;&#29992;&#20110;&#19981;&#21516;&#30340;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;&#29702;&#35770;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#29992;&#20110;&#22312;&#36172;&#21338;&#26426;&#20013;&#31616;&#21333;&#36951;&#25022;&#26368;&#23567;&#21270;&#30340;&#20803;&#23398;&#20064;&#26694;&#26550;&#12290;&#22312;&#36825;&#20010;&#26694;&#26550;&#20013;&#65292;&#23398;&#20064;&#20195;&#29702;&#19982;&#19968;&#31995;&#21015;&#36172;&#21338;&#26426;&#20219;&#21153;&#36827;&#34892;&#20132;&#20114;&#65292;&#36825;&#20123;&#20219;&#21153;&#26159;&#20174;&#19968;&#20010;&#26410;&#30693;&#30340;&#20808;&#39564;&#20998;&#24067;&#20013;&#29420;&#31435;&#37319;&#26679;&#30340;&#65292;&#24182;&#23398;&#20064;&#20854;&#20803;&#21442;&#25968;&#20197;&#22312;&#26410;&#26469;&#20219;&#21153;&#20013;&#34920;&#29616;&#26356;&#22909;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#36825;&#20010;&#35774;&#32622;&#30340;&#31532;&#19968;&#20010;&#36125;&#21494;&#26031;&#21644;&#39057;&#29575;&#27966;&#20803;&#23398;&#20064;&#31639;&#27861;&#12290;&#36125;&#21494;&#26031;&#31639;&#27861;&#21487;&#20197;&#35775;&#38382;&#20803;&#21442;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#20854;&#22312;$m$&#20010;&#36172;&#21338;&#26426;&#20219;&#21153;&#20013;&#65292;&#26102;&#38388;&#30028;&#20026;$n$&#30340;&#20803;&#31616;&#21333;&#36951;&#25022;&#20165;&#20026;$\tilde{O}(m / \sqrt{n})$&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#39057;&#29575;&#27966;&#31639;&#27861;&#30340;&#20803;&#31616;&#21333;&#36951;&#25022;&#20026;$\tilde{O}(\sqrt{m} n + m/ \sqrt{n})$&#12290;&#23613;&#31649;&#36951;&#25022;&#26356;&#22823;&#65292;&#20294;&#39057;&#29575;&#27966;&#31639;&#27861;&#26356;&#36890;&#29992;&#65292;&#22240;&#20026;&#23427;&#19981;&#38656;&#35201;&#20803;&#21442;&#25968;&#30340;&#20808;&#39564;&#20998;&#24067;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#26356;&#22810;&#30340;&#35774;&#32622;&#20013;&#36827;&#34892;&#20998;&#26512;&#12290;&#25105;&#20204;&#36890;&#36807;&#23558;&#31639;&#27861;&#24212;&#29992;&#20110;&#20960;&#31867;&#36172;&#21338;&#26426;&#38382;&#39064;&#26469;&#39564;&#35777;&#25105;&#20204;&#30340;&#29702;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a meta-learning framework for simple regret minimization in bandits. In this framework, a learning agent interacts with a sequence of bandit tasks, which are sampled i.i.d.\ from an unknown prior distribution, and learns its meta-parameters to perform better on future tasks. We propose the first Bayesian and frequentist meta-learning algorithms for this setting. The Bayesian algorithm has access to a prior distribution over the meta-parameters and its meta simple regret over $m$ bandit tasks with horizon $n$ is mere $\tilde{O}(m / \sqrt{n})$. On the other hand, the meta simple regret of the frequentist algorithm is $\tilde{O}(\sqrt{m} n + m/ \sqrt{n})$. While its regret is worse, the frequentist algorithm is more general because it does not need a prior distribution over the meta-parameters. It can also be analyzed in more settings. We instantiate our algorithms for several classes of bandit problems. Our algorithms are general and we complement our theory by evaluating them
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#20154;&#20204;&#22312;&#21028;&#26029;&#25919;&#27835;&#28436;&#35762;&#30340;&#30495;&#23454;&#24615;&#26102;&#65292;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#23545;&#20110;&#20934;&#30830;&#21306;&#20998;&#30495;&#23454;&#21644;&#20266;&#36896;&#30340;Deepfakes&#26356;&#20026;&#37325;&#35201;&#65292;&#32780;&#38169;&#35823;&#20449;&#24687;&#30340;&#22522;&#20934;&#24433;&#21709;&#36739;&#23567;&#12290;</title><link>http://arxiv.org/abs/2202.12883</link><description>&lt;p&gt;
&#36328;&#36716;&#24405;&#26412;&#12289;&#38899;&#39057;&#21644;&#35270;&#39057;&#30340;&#25919;&#27835;&#28436;&#35762;Deepfakes&#30340;&#20154;&#31867;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Human Detection of Political Speech Deepfakes across Transcripts, Audio, and Video. (arXiv:2202.12883v3 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.12883
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#36890;&#36807;&#23454;&#39564;&#21457;&#29616;&#65292;&#20154;&#20204;&#22312;&#21028;&#26029;&#25919;&#27835;&#28436;&#35762;&#30340;&#30495;&#23454;&#24615;&#26102;&#65292;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#23545;&#20110;&#20934;&#30830;&#21306;&#20998;&#30495;&#23454;&#21644;&#20266;&#36896;&#30340;Deepfakes&#26356;&#20026;&#37325;&#35201;&#65292;&#32780;&#38169;&#35823;&#20449;&#24687;&#30340;&#22522;&#20934;&#24433;&#21709;&#36739;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#25216;&#26415;&#30340;&#39134;&#36895;&#36827;&#27493;&#20351;&#24471;&#36229;&#36924;&#30495;&#30340;&#35270;&#35273;&#25928;&#26524;&#24341;&#21457;&#20102;&#20154;&#20204;&#23545;&#28145;&#24230;&#20266;&#36896;&#30340;&#25919;&#27835;&#28436;&#35762;&#35270;&#39057;&#26159;&#21542;&#24456;&#24555;&#23601;&#20250;&#19982;&#30495;&#23454;&#24405;&#20687;&#26080;&#27861;&#21306;&#20998;&#30340;&#25285;&#24551;&#12290;&#20256;&#25773;&#29702;&#35770;&#20013;&#30340;&#24120;&#35782;&#39044;&#27979;&#65292;&#24403;&#21516;&#19968;&#20010;&#25925;&#20107;&#20197;&#35270;&#39057;&#24418;&#24335;&#21644;&#25991;&#26412;&#24418;&#24335;&#23637;&#31034;&#26102;&#65292;&#20154;&#20204;&#26356;&#23481;&#26131;&#19978;&#24403;&#21463;&#39575;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;4&#20010;&#39044;&#20808;&#30331;&#35760;&#30340;&#38543;&#26426;&#23454;&#39564;&#65292;&#28041;&#21450;2015&#21517;&#21442;&#19982;&#32773;&#65292;&#20197;&#35780;&#20272;&#20154;&#31867;&#22312;&#19981;&#21516;&#30340;&#38169;&#35823;&#20449;&#24687;&#22522;&#20934;&#12289;&#38899;&#39057;&#26469;&#28304;&#21644;&#23186;&#20307;&#24418;&#24335;&#19979;&#65292;&#26159;&#21542;&#33021;&#22815;&#20934;&#30830;&#21306;&#20998;&#30495;&#23454;&#30340;&#25919;&#27835;&#28436;&#35762;&#21644;&#20266;&#36896;&#30340;&#28436;&#35762;&#12290;&#25105;&#20204;&#21457;&#29616;&#38169;&#35823;&#20449;&#24687;&#30340;&#22522;&#20934;&#23545;&#36776;&#21035;&#21028;&#26029;&#24433;&#21709;&#36739;&#23567;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#25991;&#26412;&#36716;&#35821;&#38899;&#31639;&#27861;&#29983;&#25104;&#30340;&#24102;&#26377;&#38899;&#39057;&#30340;Deepfakes&#36739;&#20351;&#29992;&#22768;&#38899;&#28436;&#21592;&#38899;&#39057;&#30340;&#30456;&#21516;Deepfakes&#26356;&#38590;&#36776;&#21035;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#21457;&#29616;&#38899;&#39057;&#21644;&#35270;&#35273;&#20449;&#24687;&#27604;&#20165;&#26377;&#25991;&#26412;&#33021;&#22815;&#26356;&#20934;&#30830;&#22320;&#36827;&#34892;&#36776;&#21035;&#65306;&#20154;&#31867;&#30340;&#36776;&#21035;&#26356;&#20381;&#36182;&#20110;&#20107;&#29289;&#26159;&#22914;&#20309;&#34987;&#34920;&#36798;&#30340;&#65292;&#38899;&#39057;-&#35270;&#35273;
&lt;/p&gt;
&lt;p&gt;
Recent advances in technology for hyper-realistic visual effects provoke the concern that deepfake videos of political speeches will soon be visually indistinguishable from authentic video recordings. The conventional wisdom in communication theory predicts people will fall for fake news more often when the same version of a story is presented as a video versus text. We conduct 4 pre-registered randomized experiments with 2,015 participants to evaluate how accurately humans distinguish real political speeches from fabrications across base rates of misinformation, audio sources, and media modalities. We find base rates of misinformation minimally influence discernment and deepfakes with audio produced by the state-of-the-art text-to-speech algorithms are harder to discern than the same deepfakes with voice actor audio. Moreover, we find audio and visual information enables more accurate discernment than text alone: human discernment relies more on how something is said, the audio-visual
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36229;&#36234;&#21333;&#19968;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#38598;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#25913;&#21892;&#25345;&#32493;&#24615;&#33021;&#65292;&#20294;&#38543;&#30528;&#27169;&#22411;&#25968;&#37327;&#22686;&#21152;&#65292;&#35745;&#31639;&#25104;&#26412;&#20063;&#20250;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#19978;&#19982;&#21333;&#19968;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#20139;&#26377;&#38598;&#25104;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2202.09826</link><description>&lt;p&gt;
&#36229;&#36234;&#21333;&#19968;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Continual Learning Beyond a Single Model. (arXiv:2202.09826v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.09826
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#36229;&#36234;&#21333;&#19968;&#27169;&#22411;&#30340;&#25345;&#32493;&#23398;&#20064;&#12290;&#36890;&#36807;&#20351;&#29992;&#38598;&#25104;&#27169;&#22411;&#65292;&#33021;&#22815;&#25913;&#21892;&#25345;&#32493;&#24615;&#33021;&#65292;&#20294;&#38543;&#30528;&#27169;&#22411;&#25968;&#37327;&#22686;&#21152;&#65292;&#35745;&#31639;&#25104;&#26412;&#20063;&#20250;&#22686;&#21152;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#22312;&#36816;&#34892;&#26102;&#38388;&#19978;&#19982;&#21333;&#19968;&#27169;&#22411;&#30456;&#24403;&#65292;&#24182;&#20139;&#26377;&#38598;&#25104;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#65292;&#36234;&#26469;&#36234;&#22810;&#30340;&#30740;&#31350;&#38598;&#20013;&#22312;&#28798;&#38590;&#24615;&#36951;&#24536;&#38382;&#39064;&#19978;&#12290;&#34429;&#28982;&#26377;&#24456;&#22810;&#26041;&#27861;&#35797;&#22270;&#32531;&#35299;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#22823;&#22810;&#25968;&#26041;&#27861;&#37117;&#20551;&#35774;&#22312;&#25345;&#32493;&#23398;&#20064;&#20013;&#21482;&#26377;&#19968;&#20010;&#27169;&#22411;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36136;&#30097;&#36825;&#20010;&#20551;&#35774;&#65292;&#24182;&#23637;&#31034;&#20102;&#20351;&#29992;&#38598;&#25104;&#27169;&#22411;&#21487;&#20197;&#26159;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#25552;&#39640;&#25345;&#32493;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#27169;&#22411;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#38598;&#25104;&#30340;&#35757;&#32451;&#21644;&#25512;&#26029;&#25104;&#26412;&#21487;&#33021;&#20250;&#26174;&#33879;&#22686;&#21152;&#12290;&#21463;&#21040;&#36825;&#20010;&#38480;&#21046;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19981;&#21516;&#30340;&#38598;&#25104;&#27169;&#22411;&#65292;&#20197;&#20102;&#35299;&#23427;&#20204;&#22312;&#25345;&#32493;&#23398;&#20064;&#22330;&#26223;&#20013;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#12290;&#26368;&#21518;&#65292;&#20026;&#20102;&#20811;&#26381;&#38598;&#25104;&#30340;&#39640;&#35745;&#31639;&#25104;&#26412;&#65292;&#25105;&#20204;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#23376;&#31354;&#38388;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#35745;&#31639;&#25104;&#26412;&#36739;&#20302;&#30340;&#31639;&#27861;&#65292;&#20854;&#36816;&#34892;&#26102;&#38388;&#19982;&#21333;&#19968;&#27169;&#22411;&#30456;&#24403;&#65292;&#20294;&#20139;&#26377;&#38598;&#25104;&#30340;&#24615;&#33021;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
A growing body of research in continual learning focuses on the catastrophic forgetting problem. While many attempts have been made to alleviate this problem, the majority of the methods assume a single model in the continual learning setup. In this work, we question this assumption and show that employing ensemble models can be a simple yet effective method to improve continual performance. However, ensembles' training and inference costs can increase significantly as the number of models grows. Motivated by this limitation, we study different ensemble models to understand their benefits and drawbacks in continual learning scenarios. Finally, to overcome the high compute cost of ensembles, we leverage recent advances in neural network subspace to propose a computationally cheap algorithm with similar runtime to a single model yet enjoying the performance benefits of ensembles.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#20998;&#24067;&#24335;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20219;&#21153;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#21033;&#29992;&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;&#65288;NSGA-II&#65289;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#22870;&#21169;&#37027;&#20123;&#21487;&#20197;&#34987;&#39046;&#22495;&#19987;&#23478;&#26356;&#22909;&#29702;&#35299;&#30340;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2112.08645</link><description>&lt;p&gt;
&#36890;&#36807;&#22810;&#30446;&#26631;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#23398;&#20064;&#21487;&#35299;&#37322;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Learning Interpretable Models Through Multi-Objective Neural Architecture Search. (arXiv:2112.08645v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2112.08645
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#30446;&#26631;&#20998;&#24067;&#24335;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20219;&#21153;&#24615;&#33021;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#21033;&#29992;&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;&#65288;NSGA-II&#65289;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#22870;&#21169;&#37027;&#20123;&#21487;&#20197;&#34987;&#39046;&#22495;&#19987;&#23478;&#26356;&#22909;&#29702;&#35299;&#30340;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#30340;&#24040;&#22823;&#36827;&#23637;&#22312;&#21508;&#20010;&#39046;&#22495;&#21462;&#24471;&#20102;&#21069;&#25152;&#26410;&#26377;&#30340;&#25104;&#23601;&#12290;&#23613;&#31649;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24615;&#33021;&#26080;&#21487;&#32622;&#30097;&#65292;&#20294;&#20854;&#26550;&#26500;&#35774;&#35745;&#21644;&#21487;&#35299;&#37322;&#24615;&#21364;&#24182;&#38750;&#26131;&#20107;&#12290;&#20026;&#20102;&#33258;&#21160;&#21270;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#35774;&#35745;&#65292;&#24341;&#20837;&#20102;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#30340;&#30740;&#31350;&#12290;&#26368;&#36817;&#30340;&#36827;&#23637;&#36890;&#36807;&#21033;&#29992;&#20998;&#24067;&#24335;&#35745;&#31639;&#21644;&#26032;&#39062;&#30340;&#20248;&#21270;&#31639;&#27861;&#20351;&#36825;&#20123;&#26041;&#27861;&#26356;&#20026;&#23454;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#20248;&#21270;&#21487;&#35299;&#37322;&#24615;&#26041;&#38754;&#30340;&#30740;&#31350;&#20173;&#30456;&#23545;&#36739;&#23569;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#30446;&#26631;&#20998;&#24067;&#24335;NAS&#26694;&#26550;&#65292;&#26088;&#22312;&#20248;&#21270;&#20219;&#21153;&#24615;&#33021;&#21644;&#8220;&#21487;&#35270;&#35782;&#21035;&#24615;&#8221;&#65292;&#36825;&#26159;&#35299;&#37322;&#24615;&#30340;&#19968;&#31181;&#20195;&#29702;&#24230;&#37327;&#12290;&#25105;&#20204;&#21033;&#29992;&#38750;&#25903;&#37197;&#25490;&#24207;&#36951;&#20256;&#31639;&#27861;&#65288;NSGA-II&#65289;&#21644;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#25216;&#26415;&#65292;&#22870;&#21169;&#37027;&#20123;&#21487;&#20197;&#34987;&#39046;&#22495;&#19987;&#23478;&#26356;&#22909;&#29702;&#35299;&#30340;&#26550;&#26500;&#12290;&#35813;&#26694;&#26550;&#22312;&#20960;&#20010;&#22270;&#20687;&#20998;&#31867;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Monumental advances in deep learning have led to unprecedented achievements across various domains. While the performance of deep neural networks is indubitable, the architectural design and interpretability of such models are nontrivial. Research has been introduced to automate the design of neural network architectures through neural architecture search (NAS). Recent progress has made these methods more pragmatic by exploiting distributed computation and novel optimization algorithms. However, there is little work in optimizing architectures for interpretability. To this end, we propose a multi-objective distributed NAS framework that optimizes for both task performance and "introspectability," a surrogate metric for aspects of interpretability. We leverage the non-dominated sorting genetic algorithm (NSGA-II) and explainable AI (XAI) techniques to reward architectures that can be better comprehended by domain experts. The framework is evaluated on several image classification datase
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21069;&#21521;&#27169;&#25311;&#26102;&#38388; (AFST) &#22312;&#21322;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#20013;&#36827;&#34892;&#24314;&#27169;&#65292;&#20811;&#26381;&#20102;&#23548;&#33322;&#20013;&#30340;&#23616;&#37096;&#26368;&#23567;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#21160;&#20316;&#31354;&#38388;&#30340;&#32500;&#24230;&#21644;&#25913;&#36827;&#20998;&#24067;&#24335;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270; (DPPO) &#31639;&#27861;&#26469;&#20248;&#21270;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#26410;&#30693;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2108.06161</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#21069;&#21521;&#27169;&#25311;&#26102;&#38388;&#30340;&#24378;&#21270;&#23398;&#20064;&#22312;&#21322;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#20013;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning for Robot Navigation with Adaptive Forward Simulation Time (AFST) in a Semi-Markov Model. (arXiv:2108.06161v4 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.06161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26426;&#22120;&#20154;&#23548;&#33322;&#26041;&#27861;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#21069;&#21521;&#27169;&#25311;&#26102;&#38388; (AFST) &#22312;&#21322;&#39532;&#23572;&#31185;&#22827;&#27169;&#22411;&#20013;&#36827;&#34892;&#24314;&#27169;&#65292;&#20811;&#26381;&#20102;&#23548;&#33322;&#20013;&#30340;&#23616;&#37096;&#26368;&#23567;&#38382;&#39064;&#65292;&#24182;&#36890;&#36807;&#20943;&#23569;&#21160;&#20316;&#31354;&#38388;&#30340;&#32500;&#24230;&#21644;&#25913;&#36827;&#20998;&#24067;&#24335;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270; (DPPO) &#31639;&#27861;&#26469;&#20248;&#21270;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#26410;&#30693;&#29615;&#22659;&#20013;&#21462;&#24471;&#20102;&#33391;&#22909;&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#31639;&#27861;&#24050;&#32463;&#34987;&#35777;&#26126;&#23545;&#26426;&#22120;&#20154;&#23548;&#33322;&#38750;&#24120;&#26377;&#25928;&#65292;&#23588;&#20854;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#65292;&#36890;&#36807;&#23558;&#24863;&#30693;&#36755;&#20837;&#30452;&#25509;&#26144;&#23556;&#20026;&#26426;&#22120;&#20154;&#25511;&#21046;&#21629;&#20196;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#24573;&#30053;&#20102;&#23548;&#33322;&#20013;&#30340;&#23616;&#37096;&#26368;&#23567;&#38382;&#39064;&#65292;&#22240;&#27492;&#26080;&#27861;&#22788;&#29702;&#22797;&#26434;&#30340;&#26410;&#30693;&#29615;&#22659;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#31532;&#19968;&#20010;&#22522;&#20110;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#23548;&#33322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#36830;&#32493;&#21160;&#20316;&#31354;&#38388;&#30340;&#21322;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#27169;&#22411;&#65292;&#24182;&#21629;&#21517;&#20026;&#33258;&#36866;&#24212;&#21069;&#21521;&#27169;&#25311;&#26102;&#38388; (AFST)&#65292;&#20197;&#20811;&#26381;&#36825;&#20010;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#36890;&#36807;&#20943;&#23569;&#21160;&#20316;&#31354;&#38388;&#30340;&#32500;&#24230;&#65292;&#24182;&#25913;&#36827;&#20998;&#24067;&#24335;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270; (DPPO) &#31639;&#27861;&#26469;&#36866;&#24212;&#25351;&#23450;&#30340;&#21322;&#39532;&#23572;&#31185;&#22827;&#38382;&#39064;&#65292;&#20462;&#25913;&#20854;&#24191;&#20041;&#20248;&#21183;&#20272;&#35745; (GAE) &#20197;&#26356;&#22909;&#22320;&#20272;&#35745; SMDP &#20013;&#30340;&#31574;&#30053;&#26799;&#24230;&#12290;&#22312;&#21508;&#31181;&#26410;&#30693;&#29615;&#22659;&#20013;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102; AFST &#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep reinforcement learning (DRL) algorithms have proven effective in robot navigation, especially in unknown environments, by directly mapping perception inputs into robot control commands. However, most existing methods ignore the local minimum problem in navigation and thereby cannot handle complex unknown environments. In this paper, we propose the first DRL-based navigation method modeled by a semi-Markov decision process (SMDP) with continuous action space, named Adaptive Forward Simulation Time (AFST), to overcome this problem. Specifically, we reduce the dimensions of the action space and improve the distributed proximal policy optimization (DPPO) algorithm for the specified SMDP problem by modifying its GAE to better estimate the policy gradient in SMDPs. Experiments in various unknown environments demonstrate the effectiveness of AFST.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#21160;&#25968;&#25454;&#37319;&#38598;&#30340;&#20195;&#20215;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#19982;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;&#30456;&#27604;&#65292;&#34987;&#21160;&#37319;&#38598;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#21576;&#25351;&#25968;&#32423;&#22686;&#21152;&#12290;</title><link>http://arxiv.org/abs/2106.09973</link><description>&lt;p&gt;
&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#21160;&#25968;&#25454;&#37319;&#38598;&#30340;&#35781;&#21650;
&lt;/p&gt;
&lt;p&gt;
The Curse of Passive Data Collection in Batch Reinforcement Learning. (arXiv:2106.09973v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.09973
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25209;&#37327;&#24378;&#21270;&#23398;&#20064;&#20013;&#34987;&#21160;&#25968;&#25454;&#37319;&#38598;&#30340;&#20195;&#20215;&#38382;&#39064;&#65292;&#24182;&#21457;&#29616;&#19982;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;&#30456;&#27604;&#65292;&#34987;&#21160;&#37319;&#38598;&#30340;&#26679;&#26412;&#22797;&#26434;&#24615;&#21576;&#25351;&#25968;&#32423;&#22686;&#21152;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#39640;&#39118;&#38505;&#24212;&#29992;&#20013;&#65292;&#20027;&#21160;&#23454;&#39564;&#21487;&#33021;&#34987;&#35748;&#20026;&#39118;&#38505;&#22826;&#22823;&#65292;&#22240;&#27492;&#36890;&#24120;&#20250;&#34987;&#21160;&#37319;&#38598;&#25968;&#25454;&#12290;&#34429;&#28982;&#22312;&#31616;&#21333;&#24773;&#20917;&#19979;&#65292;&#22914;&#22312;&#36172;&#21338;&#26426;&#20013;&#65292;&#34987;&#21160;&#21644;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;&#30340;&#25928;&#26524;&#30456;&#20284;&#65292;&#20294;&#22312;&#20174;&#24102;&#26377;&#21487;&#25511;&#29366;&#24577;&#30340;&#31995;&#32479;&#20013;&#25910;&#38598;&#25968;&#25454;&#26102;&#65292;&#34987;&#21160;&#37319;&#26679;&#30340;&#20195;&#20215;&#21487;&#33021;&#20250;&#26356;&#39640;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#37325;&#28857;&#26159;&#23545;&#36825;&#31181;&#20195;&#20215;&#30340;&#29305;&#24449;&#21270;&#12290;&#20363;&#22914;&#65292;&#22312;&#20855;&#26377;$\mathrm{S}$&#20010;&#29366;&#24577;&#21644;$\mathrm{A}$&#20010;&#21160;&#20316;&#30340;&#31163;&#25955;&#29366;&#24577;-&#21160;&#20316;&#39532;&#23572;&#21487;&#22827;&#20915;&#31574;&#36807;&#31243;(MDP)&#20013;&#23398;&#20064;&#26102;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#21363;&#20351;&#20351;&#29992;&#26368;&#20339;&#65288;&#20294;&#34987;&#21160;&#36873;&#25321;&#30340;&#65289;&#26085;&#24535;&#35760;&#24405;&#31574;&#30053;&#65292;&#20063;&#38656;&#35201;&#65288;&#19988;&#36275;&#22815;&#65289;&#33719;&#24471;$\epsilon$-&#26368;&#20248;&#31574;&#30053;&#30340;$\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H)}/\varepsilon^2)$&#20010;&#22238;&#21512;&#65292;&#20854;&#20013;$H$&#26159;&#22238;&#21512;&#38271;&#24230;&#12290;&#35831;&#27880;&#24847;&#65292;&#36825;&#34920;&#26126;&#19982;&#20027;&#21160;&#25968;&#25454;&#37319;&#38598;&#30456;&#27604;&#65292;&#26679;&#26412;&#22797;&#26434;&#24615;&#21576;&#25351;&#25968;&#32423;&#22686;&#21152;&#65292;&#36825;&#20010;&#32467;&#26524;&#26159;&#21487;&#20197;&#39044;&#26009;&#30340;&#65292;&#20294;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;&#23578;&#26410;&#21457;&#34920;&#12290;
&lt;/p&gt;
&lt;p&gt;
In high stake applications, active experimentation may be considered too risky and thus data are often collected passively. While in simple cases, such as in bandits, passive and active data collection are similarly effective, the price of passive sampling can be much higher when collecting data from a system with controlled states. The main focus of the current paper is the characterization of this price. For example, when learning in episodic finite state-action Markov decision processes (MDPs) with $\mathrm{S}$ states and $\mathrm{A}$ actions, we show that even with the best (but passively chosen) logging policy, $\Omega(\mathrm{A}^{\min(\mathrm{S}-1, H)}/\varepsilon^2)$ episodes are necessary (and sufficient) to obtain an $\epsilon$-optimal policy, where $H$ is the length of episodes. Note that this shows that the sample complexity blows up exponentially compared to the case of active data collection, a result which is not unexpected, but, as far as we know, have not been published
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;ceteris paribus&#20026;&#22522;&#30784;&#30340;&#27169;&#24577;&#35821;&#35328;&#65292;&#29992;&#20110;&#35299;&#37322;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#21450;&#20854;&#23646;&#24615;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#20851;&#20110;&#35821;&#35328;&#22522;&#25968;&#30340;&#35777;&#26126;&#31995;&#32479;&#30340;&#23436;&#22791;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#26080;&#38480;&#21464;&#37327;&#21644;&#26377;&#38480;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#36825;&#31181;&#35821;&#35328;&#26469;&#24418;&#24335;&#21270;&#22810;&#31181;&#35299;&#37322;&#27010;&#24565;&#65292;&#21253;&#25324;&#23545;&#20107;&#23454;&#12289;&#23545;&#27604;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#20197;&#21450;&#20559;&#35265;&#12290;</title><link>http://arxiv.org/abs/2105.14452</link><description>&lt;p&gt;
&#19968;&#31181;&#32479;&#19968;&#30340;&#36923;&#36753;&#26694;&#26550;&#29992;&#20110;&#20998;&#31867;&#22120;&#31995;&#32479;&#30340;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
A unified logical framework for explanations in classifier systems. (arXiv:2105.14452v6 [cs.LO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2105.14452
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;ceteris paribus&#20026;&#22522;&#30784;&#30340;&#27169;&#24577;&#35821;&#35328;&#65292;&#29992;&#20110;&#35299;&#37322;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#21450;&#20854;&#23646;&#24615;&#30340;&#25512;&#29702;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#20004;&#20010;&#20851;&#20110;&#35821;&#35328;&#22522;&#25968;&#30340;&#35777;&#26126;&#31995;&#32479;&#30340;&#23436;&#22791;&#24615;&#65292;&#24182;&#30740;&#31350;&#20102;&#26080;&#38480;&#21464;&#37327;&#21644;&#26377;&#38480;&#21464;&#37327;&#24773;&#20917;&#19979;&#30340;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#38382;&#39064;&#12290;&#25105;&#20204;&#36824;&#20351;&#29992;&#36825;&#31181;&#35821;&#35328;&#26469;&#24418;&#24335;&#21270;&#22810;&#31181;&#35299;&#37322;&#27010;&#24565;&#65292;&#21253;&#25324;&#23545;&#20107;&#23454;&#12289;&#23545;&#27604;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#20197;&#21450;&#20559;&#35265;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#39046;&#22495;&#20013;&#65292;&#24067;&#23572;&#20989;&#25968;&#23545;&#20110;&#35299;&#37322;&#20108;&#36827;&#21046;&#20998;&#31867;&#22120;&#36234;&#26469;&#36234;&#21463;&#20851;&#27880;&#12290;&#20256;&#32479;&#30340;&#24067;&#23572;&#20989;&#25968;&#26041;&#27861;&#37319;&#29992;&#21629;&#39064;&#36923;&#36753;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20197;ceteris paribus&#20026;&#22522;&#30784;&#30340;&#27169;&#24577;&#35821;&#35328;&#65292;&#25903;&#25345;&#23545;&#20108;&#36827;&#21046;&#36755;&#20837;&#20998;&#31867;&#22120;&#21450;&#20854;&#23646;&#24615;&#36827;&#34892;&#25512;&#29702;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31995;&#21015;&#20998;&#31867;&#22120;&#27169;&#22411;&#65292;&#23558;&#20854;&#20844;&#29702;&#21270;&#20026;&#20851;&#20110;&#35821;&#35328;&#22522;&#25968;&#30340;&#20004;&#20010;&#35777;&#26126;&#31995;&#32479;&#65292;&#24182;&#35777;&#26126;&#20102;&#25105;&#20204;&#20844;&#29702;&#31995;&#32479;&#30340;&#23436;&#22791;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26080;&#38480;&#21464;&#37327;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#27169;&#24577;&#35821;&#35328;&#30340;&#21487;&#28385;&#36275;&#24615;&#26816;&#26597;&#38382;&#39064;&#26159;NEXPTIME&#23436;&#20840;&#30340;&#65292;&#32780;&#22312;&#26377;&#38480;&#21464;&#37327;&#24773;&#20917;&#19979;&#65292;&#35813;&#38382;&#39064;&#21464;&#20026;&#22810;&#39033;&#24335;&#22797;&#26434;&#24230;&#12290;&#25105;&#20204;&#36824;&#22312;&#26080;&#38480;&#21464;&#37327;&#24773;&#20917;&#19979;&#30830;&#23450;&#20102;&#19968;&#20010;&#26377;&#36259;&#30340;NP&#29255;&#27573;&#12290;&#25105;&#20204;&#21033;&#29992;&#36825;&#31181;&#35821;&#35328;&#26469;&#24418;&#24335;&#21270;&#23545;&#20107;&#23454;&#26465;&#20214;&#20197;&#21450;&#21253;&#25324;&#20174;&#23646;&#12289;&#23545;&#27604;&#21644;&#21453;&#20107;&#23454;&#35299;&#37322;&#20197;&#21450;&#20559;&#35265;&#22312;&#20869;&#30340;&#21508;&#31181;&#35299;&#37322;&#27010;&#24565;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25193;&#23637;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent years have witnessed a renewed interest in Boolean function in explaining binary classifiers in the field of explainable AI (XAI). The standard approach of Boolean function is propositional logic. We present a modal language of a ceteris paribus nature which supports reasoning about binary input classifiers and their properties. We study a family of classifier models, axiomatize it as two proof systems regarding the cardinality of the language and show completeness of our axiomatics. Moreover, we prove that satisfiability checking problem for our modal language is NEXPTIME-complete in the infinite-variable case, while it becomes polynomial in the finite-variable case. We furthermore identify an interesting NP fragment of our language in the infinite-variable case. We leverage the language to formalize counterfactual conditional as well as a variety of notions of explanation including abductive, contrastive and counterfactual explanations, and biases. Finally, we present two exte
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#30340;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#38024;&#23545;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22256;&#38590;&#24773;&#20917;&#12290;&#36890;&#36807;&#19968;&#27493;&#24335;&#32499;&#32034;&#22810;&#30446;&#26631;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#31614;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#22122;&#22768;&#21644;&#35780;&#20272;&#31574;&#30053;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2011.14956</link><description>&lt;p&gt;
&#36890;&#36807;&#19968;&#27493;&#24335;&#32499;&#32034;&#22810;&#30446;&#26631;&#23398;&#20064;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#21450;&#20854;&#22312;&#24189;&#38376;&#34746;&#26438;&#33740;&#20998;&#21106;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Handling Noisy Labels via One-Step Abductive Multi-Target Learning and Its Application to Helicobacter Pylori Segmentation. (arXiv:2011.14956v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2011.14956
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22788;&#29702;&#22122;&#22768;&#26631;&#31614;&#30340;&#26032;&#26041;&#27861;&#65292;&#29305;&#21035;&#38024;&#23545;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#22270;&#20687;&#20998;&#26512;&#20013;&#30340;&#22256;&#38590;&#24773;&#20917;&#12290;&#36890;&#36807;&#19968;&#27493;&#24335;&#32499;&#32034;&#22810;&#30446;&#26631;&#23398;&#20064;&#65292;&#35813;&#26041;&#27861;&#20811;&#26381;&#20102;&#26631;&#31614;&#20013;&#23384;&#22312;&#30340;&#22797;&#26434;&#22122;&#22768;&#21644;&#35780;&#20272;&#31574;&#30053;&#19981;&#26126;&#30830;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#35768;&#22810;&#29616;&#23454;&#22330;&#26223;&#20013;&#32570;&#20047;&#20934;&#30830;&#30340;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#65292;&#22240;&#27492;&#20174;&#22122;&#22768;&#26631;&#31614;&#20013;&#23398;&#20064;&#26159;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#12290;&#22312;&#23454;&#36341;&#20013;&#65292;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#19981;&#21516;&#26041;&#27861;&#39318;&#20808;&#23545;&#21487;&#33021;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23454;&#20363;&#36827;&#34892;&#19968;&#20123;&#32416;&#27491;&#65292;&#28982;&#21518;&#29992;&#32416;&#27491;&#20449;&#24687;&#26356;&#26032;&#39044;&#27979;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#22312;&#21307;&#23398;&#32452;&#32455;&#30149;&#29702;&#23398;&#20840;&#20999;&#29255;&#22270;&#20687;&#20998;&#26512;&#65288;MHWSIA&#65289;&#31561;&#29305;&#23450;&#39046;&#22495;&#20013;&#65292;&#19987;&#23478;&#24448;&#24448;&#38590;&#20197;&#25110;&#29978;&#33267;&#26080;&#27861;&#25163;&#21160;&#23454;&#29616;&#26080;&#22122;&#22768;&#30340;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#65292;&#23548;&#33268;&#26631;&#31614;&#23384;&#22312;&#22797;&#26434;&#22122;&#22768;&#12290;&#36825;&#31181;&#24773;&#20917;&#24341;&#21457;&#20102;&#20004;&#20010;&#26356;&#21152;&#22256;&#38590;&#30340;&#38382;&#39064;&#65306;1&#65289;&#30001;&#20110;&#26631;&#31614;&#20013;&#23384;&#22312;&#22797;&#26434;&#22122;&#22768;&#65292;&#20808;&#21069;&#26041;&#27861;&#32416;&#27491;&#21487;&#33021;&#26377;&#22122;&#22768;&#26631;&#31614;&#30340;&#23454;&#20363;&#30340;&#26041;&#27861;&#23398;&#23384;&#22312;&#23616;&#38480;&#24615;&#65307;2&#65289;&#30001;&#20110;&#25910;&#38598;&#26080;&#22122;&#22768;&#22320;&#38754;&#23454;&#20917;&#26631;&#31614;&#38750;&#24120;&#22256;&#38590;&#65292;&#39564;&#35777;/&#27979;&#35797;&#30340;&#36866;&#24403;&#35780;&#20272;&#31574;&#30053;&#19981;&#26126;&#30830;&#12290;&#26412;&#25991;&#37325;&#28857;&#30740;&#31350;&#20102;&#32531;&#35299;&#20197;&#19978;&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning from noisy labels is an important concern because of the lack of accurate ground-truth labels in plenty of real-world scenarios. In practice, various approaches for this concern first make some corrections corresponding to potentially noisy-labeled instances, and then update predictive model with information of the made corrections. However, in specific areas, such as medical histopathology whole slide image analysis (MHWSIA), it is often difficult or even impossible for experts to manually achieve the noisy-free ground-truth labels which leads to labels with complex noise. This situation raises two more difficult problems: 1) the methodology of approaches making corrections corresponding to potentially noisy-labeled instances has limitations due to the complex noise existing in labels; and 2) the appropriate evaluation strategy for validation/testing is unclear because of the great difficulty in collecting the noisy-free ground-truth labels. In this paper, we focus on allevia
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#23398;&#12289;&#20860;&#23481;&#30340;&#24378;&#21270;&#23398;&#20064;&#32972;&#26223;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#19982;&#20854;&#20182;&#30456;&#20851;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;</title><link>http://arxiv.org/abs/2009.07888</link><description>&lt;p&gt;
&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Transfer Learning in Deep Reinforcement Learning: A Survey. (arXiv:2009.07888v6 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2009.07888
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#32508;&#36848;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#65292;&#24182;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#36825;&#20123;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26694;&#26550;&#12290;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#23398;&#12289;&#20860;&#23481;&#30340;&#24378;&#21270;&#23398;&#20064;&#32972;&#26223;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#65292;&#24182;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#19982;&#20854;&#20182;&#30456;&#20851;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#26159;&#35299;&#20915;&#24207;&#21015;&#20915;&#31574;&#38382;&#39064;&#30340;&#23398;&#20064;&#33539;&#24335;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#24378;&#21270;&#23398;&#20064;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#38500;&#20102;&#22312;&#26426;&#22120;&#20154;&#21644;&#28216;&#25103;&#31561;&#35832;&#22810;&#39046;&#22495;&#20013;&#20855;&#26377;&#33391;&#22909;&#21069;&#26223;&#30340;&#24378;&#21270;&#23398;&#20064;&#65292;&#36801;&#31227;&#23398;&#20064;&#20316;&#20026;&#19968;&#31181;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#38754;&#20020;&#30340;&#21508;&#31181;&#25361;&#25112;&#30340;&#26041;&#27861;&#24050;&#32463;&#20986;&#29616;&#65292;&#36890;&#36807;&#20174;&#22806;&#37096;&#19987;&#19994;&#30693;&#35782;&#20013;&#36716;&#31227;&#30693;&#35782;&#65292;&#20197;&#25552;&#39640;&#23398;&#20064;&#36807;&#31243;&#30340;&#25928;&#29575;&#21644;&#25928;&#26524;&#12290;&#22312;&#36825;&#39033;&#32508;&#36848;&#20013;&#65292;&#25105;&#20204;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#20013;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#30340;&#26368;&#26032;&#36827;&#23637;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#23545;&#26368;&#20808;&#36827;&#30340;&#36801;&#31227;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#20998;&#31867;&#30340;&#26694;&#26550;&#65292;&#22312;&#27492;&#26694;&#26550;&#19979;&#20998;&#26512;&#20102;&#23427;&#20204;&#30340;&#30446;&#26631;&#12289;&#26041;&#27861;&#23398;&#12289;&#20860;&#23481;&#30340;&#24378;&#21270;&#23398;&#20064;&#32972;&#26223;&#20197;&#21450;&#23454;&#38469;&#24212;&#29992;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#36801;&#31227;&#23398;&#20064;&#19982;&#20854;&#20182;&#30456;&#20851;&#20027;&#39064;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement learning is a learning paradigm for solving sequential decision-making problems. Recent years have witnessed remarkable progress in reinforcement learning upon the fast development of deep neural networks. Along with the promising prospects of reinforcement learning in numerous domains such as robotics and game-playing, transfer learning has arisen to tackle various challenges faced by reinforcement learning, by transferring knowledge from external expertise to facilitate the efficiency and effectiveness of the learning process. In this survey, we systematically investigate the recent progress of transfer learning approaches in the context of deep reinforcement learning. Specifically, we provide a framework for categorizing the state-of-the-art transfer learning approaches, under which we analyze their goals, methodologies, compatible reinforcement learning backbones, and practical applications. We also draw connections between transfer learning and other relevant topics 
&lt;/p&gt;</description></item></channel></rss>