<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>EGC&#26159;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36739;&#22909;&#22320;&#29983;&#25104;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#22810;&#39033;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39046;&#20808;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.02012</link><description>&lt;p&gt;
EGC: &#19968;&#31181;&#36890;&#36807;&#21333;&#19968;&#33021;&#37327;&#27169;&#22411;&#29983;&#25104;&#19982;&#20998;&#31867;&#22270;&#20687;&#30340;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
EGC: Image Generation and Classification via a Single Energy-Based Model. (arXiv:2304.02012v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02012
&lt;/p&gt;
&lt;p&gt;
EGC&#26159;&#19968;&#31181;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#22270;&#20687;&#20998;&#31867;&#21644;&#22270;&#20687;&#29983;&#25104;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36739;&#22909;&#22320;&#29983;&#25104;&#20986;&#39640;&#36136;&#37327;&#22270;&#20687;&#65292;&#24182;&#22312;&#22810;&#39033;&#25968;&#25454;&#38598;&#19978;&#23454;&#29616;&#20102;&#39046;&#20808;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#30456;&#21516;&#30340;&#32593;&#32476;&#21442;&#25968;&#23398;&#20064;&#22270;&#20687;&#20998;&#31867;&#21644;&#29983;&#25104;&#22270;&#20687;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#26368;&#36817;&#30340;&#20808;&#36827;&#26041;&#27861;&#22312;&#19968;&#39033;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#65292;&#20294;&#22312;&#21478;&#19968;&#39033;&#20219;&#21153;&#19978;&#21364;&#34920;&#29616;&#19981;&#20339;&#12290;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#21517;&#20026;EGC&#30340;&#22522;&#20110;&#33021;&#37327;&#30340;&#20998;&#31867;&#22120;&#21644;&#29983;&#25104;&#22120;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#21333;&#20010;&#31070;&#32463;&#32593;&#32476;&#22312;&#20004;&#20010;&#20219;&#21153;&#20013;&#23454;&#29616;&#21331;&#36234;&#24615;&#33021;&#12290;&#19982;&#20256;&#32479;&#30340;&#20998;&#31867;&#22120;&#36755;&#20986;&#32473;&#23450;&#22270;&#20687;&#30340;&#26631;&#31614;&#65288;&#21363;&#26465;&#20214;&#20998;&#24067;$p(y|\mathbf{x})$&#65289;&#19981;&#21516;&#65292;EGC&#30340;&#21069;&#21521;&#20256;&#36882;&#22120;&#26159;&#19968;&#20010;&#20998;&#31867;&#22120;&#65292;&#23427;&#36755;&#20986;&#19968;&#20010;&#32852;&#21512;&#20998;&#24067;$p(\mathbf{x},y)$&#65292;&#22312;&#21518;&#21521;&#20256;&#36882;&#22120;&#20013;&#36890;&#36807;&#36793;&#32536;&#21270;&#26631;&#31614;$y$&#23454;&#29616;&#29983;&#25104;&#22120;&#12290;&#22312;&#21069;&#21521;&#20256;&#36882;&#20013;&#65292;&#20272;&#35745;&#32473;&#23450;&#22122;&#22768;&#22270;&#20687;&#30340;&#33021;&#37327;&#21644;&#20998;&#31867;&#27010;&#29575;&#65292;&#32780;&#22312;&#21518;&#21521;&#20256;&#36882;&#20013;&#65292;&#36890;&#36807;&#20272;&#35745;&#24471;&#20998;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#21435;&#22122;&#12290;EGC&#22312;ImageNet-1k&#12289;CelebA-HQ&#21644;LSUN Church&#19978;&#23454;&#29616;&#20102;&#19982;&#26368;&#20808;&#36827;&#26041;&#27861;&#30456;&#24403;&#30340;&#29983;&#25104;&#32467;&#26524;&#65292;&#21516;&#26102;&#22312;CIFAR-10&#12289;CIFAR-100&#21644;ImageNet-1k&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#20998;&#31867;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning image classification and image generation using the same set of network parameters is a challenging problem. Recent advanced approaches perform well in one task often exhibit poor performance in the other. This work introduces an energy-based classifier and generator, namely EGC, which can achieve superior performance in both tasks using a single neural network. Unlike a conventional classifier that outputs a label given an image (i.e., a conditional distribution $p(y|\mathbf{x})$), the forward pass in EGC is a classifier that outputs a joint distribution $p(\mathbf{x},y)$, enabling an image generator in its backward pass by marginalizing out the label $y$. This is done by estimating the energy and classification probability given a noisy image in the forward pass, while denoising it using the score function estimated in the backward pass. EGC achieves competitive generation results compared with state-of-the-art approaches on ImageNet-1k, CelebA-HQ and LSUN Church, while achi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#32423;&#23545;&#27604;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#21306;&#22495;&#32423;&#29305;&#24449;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21487;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#20026;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21306;&#22495;&#35821;&#20041;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#24179;&#31227;&#21644;&#23610;&#24230;&#19968;&#33268;&#24615;&#65292;&#24182;&#23558;&#39044;&#35757;&#32451;&#36718;&#25968;&#38477;&#33267;&#19982;&#30417;&#30563;&#39044;&#35757;&#32451;&#30456;&#21516;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.02010</link><description>&lt;p&gt;
&#22810;&#23618;&#32423;&#23545;&#27604;&#23398;&#20064;&#22312;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Multi-Level Contrastive Learning for Dense Prediction Task. (arXiv:2304.02010v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02010
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23618;&#32423;&#23545;&#27604;&#23398;&#20064;&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#21306;&#22495;&#32423;&#29305;&#24449;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#21487;&#36890;&#36807;&#19968;&#20010;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#20026;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21306;&#22495;&#35821;&#20041;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#24179;&#31227;&#21644;&#23610;&#24230;&#19968;&#33268;&#24615;&#65292;&#24182;&#23558;&#39044;&#35757;&#32451;&#36718;&#25968;&#38477;&#33267;&#19982;&#30417;&#30563;&#39044;&#35757;&#32451;&#30456;&#21516;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#19978;&#35777;&#26126;&#20102;&#20854;&#20248;&#36234;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;MCL&#30340;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#23494;&#38598;&#39044;&#27979;&#20219;&#21153;&#30340;&#21306;&#22495;&#32423;&#29305;&#24449;&#34920;&#31034;&#12290;&#35813;&#26041;&#27861;&#32771;&#34385;&#20102;&#30446;&#26631;&#26816;&#27979;&#30340;&#19977;&#20010;&#20851;&#38190;&#22240;&#32032;&#65306;&#23450;&#20301;&#24615;&#12289;&#23610;&#24230;&#19968;&#33268;&#24615;&#21644;&#35782;&#21035;&#24615;&#12290;&#20026;&#20102;&#26126;&#30830;&#22320;&#32534;&#30721;&#32477;&#23545;&#20301;&#32622;&#21644;&#23610;&#24230;&#20449;&#24687;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#39044;&#35757;&#32451;&#20219;&#21153;&#65292;&#23558;&#22810;&#23610;&#24230;&#22270;&#20687;&#20197;&#25340;&#36148;&#30340;&#26041;&#24335;&#32452;&#35013;&#36215;&#26469;&#65292;&#27169;&#25311;&#22810;&#30446;&#26631;&#22330;&#26223;&#12290;&#19982;&#29616;&#26377;&#30340;&#22270;&#20687;&#32423;&#33258;&#30417;&#30563;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26500;&#24314;&#20102;&#19968;&#20010;&#22810;&#23618;&#32423;&#23545;&#27604;&#25439;&#22833;&#65292;&#23558;&#25340;&#36148;&#22270;&#20687;&#30340;&#27599;&#20010;&#23376;&#21306;&#22495;&#35270;&#20026;&#20855;&#26377;&#21333;&#20010;&#26679;&#26412;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#20351;&#31070;&#32463;&#32593;&#32476;&#23398;&#20064;&#21306;&#22495;&#35821;&#20041;&#34920;&#31034;&#65292;&#20174;&#32780;&#23454;&#29616;&#24179;&#31227;&#21644;&#23610;&#24230;&#19968;&#33268;&#24615;&#65292;&#24182;&#23558;&#39044;&#35757;&#32451;&#36718;&#25968;&#38477;&#33267;&#19982;&#30417;&#30563;&#39044;&#35757;&#32451;&#30456;&#21516;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;MCL&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19978;&#22987;&#32456;&#20248;&#20110;&#26368;&#26032;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we present Multi-Level Contrastive Learning for Dense Prediction Task (MCL), an efficient self-supervised method for learning region-level feature representation for dense prediction tasks. Our method is motivated by the three key factors in detection: localization, scale consistency and recognition. To explicitly encode absolute position and scale information, we propose a novel pretext task that assembles multi-scale images in a montage manner to mimic multi-object scenarios. Unlike the existing image-level self-supervised methods, our method constructs a multi-level contrastive loss that considers each sub-region of the montage image as a singleton. Our method enables the neural network to learn regional semantic representations for translation and scale consistency while reducing pre-training epochs to the same as supervised pre-training. Extensive experiments demonstrate that MCL consistently outperforms the recent state-of-the-art methods on various datasets with si
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#39118;&#38505;&#24847;&#35782;&#21160;&#20316;&#35299;&#20915;&#20102;&#26410;&#30693;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.02005</link><description>&lt;p&gt;
&#38754;&#21521;&#39118;&#38505;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Risk-Aware Distributed Multi-Agent Reinforcement Learning. (arXiv:2304.02005v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.02005
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#39118;&#38505;&#24847;&#35782;&#21160;&#20316;&#35299;&#20915;&#20102;&#26410;&#30693;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20027;&#30340;&#32593;&#32476;&#21644;&#29289;&#29702;&#31995;&#32479;&#38656;&#35201;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#36827;&#34892;&#20915;&#31574;&#12289;&#23398;&#20064;&#21644;&#25511;&#21046;&#12290;&#36825;&#31181;&#20915;&#31574;&#21487;&#33021;&#20250;&#21463;&#21040;&#22810;&#31181;&#22240;&#32032;&#30340;&#24433;&#21709;&#65292;&#21253;&#25324;&#24314;&#27169;&#35823;&#24046;&#12289;&#25104;&#26412;&#21464;&#21270;&#20197;&#21450;&#27010;&#29575;&#20998;&#24067;&#23614;&#37096;&#20107;&#20214;&#30340;&#24433;&#21709;&#12290;&#34429;&#28982;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20026;&#36890;&#36807;&#19982;&#29615;&#22659;&#21453;&#22797;&#20132;&#20114;&#20197;&#26368;&#23567;&#21270;&#24179;&#22343;&#25104;&#26412;&#26469;&#23398;&#20064;&#34892;&#20026;&#25552;&#20379;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20294;&#23427;&#26080;&#27861;&#20811;&#26381;&#19978;&#36848;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#22312;&#23398;&#20064;&#39118;&#38505;&#24847;&#35782;&#21160;&#20316;&#30340;&#21516;&#26102;&#35299;&#20915;&#20102;&#26410;&#30693;&#29615;&#22659;&#19979;&#30340;&#20915;&#31574;&#38382;&#39064;&#12290;&#25105;&#20204;&#20351;&#29992;&#26465;&#20214;&#39118;&#38505;&#20215;&#20540;&#65288;CVaR&#65289;&#26469;&#34920;&#24449;&#34987;&#26368;&#23567;&#21270;&#30340;&#25104;&#26412;&#20989;&#25968;&#65292;&#24182;&#23450;&#20041;&#20102;&#36125;&#23572;&#26364;&#31639;&#23376;&#26469;&#34920;&#24449;&#19982;&#32473;&#23450;&#29366;&#24577;-&#21160;&#20316;&#23545;&#30456;&#20851;&#32852;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#31639;&#23376;&#28385;&#36275;&#25910;&#32553;&#29305;&#24615;&#65292;&#24182;&#19988;&#25910;&#25947;&#20110;&#26368;&#20248;&#30340;&#20215;&#20540;&#20989;&#25968;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous cyber and cyber-physical systems need to perform decision-making, learning, and control in unknown environments. Such decision-making can be sensitive to multiple factors, including modeling errors, changes in costs, and impacts of events in the tails of probability distributions. Although multi-agent reinforcement learning (MARL) provides a framework for learning behaviors through repeated interactions with the environment by minimizing an average cost, it will not be adequate to overcome the above challenges. In this paper, we develop a distributed MARL approach to solve decision-making problems in unknown environments by learning risk-aware actions. We use the conditional value-at-risk (CVaR) to characterize the cost function that is being minimized, and define a Bellman operator to characterize the value function associated to a given state-action pair. We prove that this operator satisfies a contraction property, and that it converges to the optimal value function. We t
&lt;/p&gt;</description></item><item><title>MEGClass&#26159;&#19968;&#31181;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#23454;&#29616;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#23618;&#20851;&#27880;&#26426;&#21046;&#20174;&#25991;&#26723;&#12289;&#21477;&#23376;&#21644;&#21333;&#35789;&#20013;&#25552;&#21462;&#36830;&#36143;&#19988;&#22810;&#26679;&#30340;&#23376;&#25991;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#30340;&#25991;&#26723;&#65292;&#24182;&#19988;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2304.01969</link><description>&lt;p&gt;
MEGClass: &#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#23454;&#29616;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;
MEGClass: Text Classification with Extremely Weak Supervision via Mutually-Enhancing Text Granularities. (arXiv:2304.01969v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01969
&lt;/p&gt;
&lt;p&gt;
MEGClass&#26159;&#19968;&#31181;&#36890;&#36807;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#23454;&#29616;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;&#20998;&#23618;&#20851;&#27880;&#26426;&#21046;&#20174;&#25991;&#26723;&#12289;&#21477;&#23376;&#21644;&#21333;&#35789;&#20013;&#25552;&#21462;&#36830;&#36143;&#19988;&#22810;&#26679;&#30340;&#23376;&#25991;&#26412;&#65292;&#33021;&#22815;&#20934;&#30830;&#20998;&#31867;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#30340;&#25991;&#26723;&#65292;&#24182;&#19988;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#20998;&#31867;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#30340;&#20154;&#24037;&#26631;&#27880;&#25968;&#25454;&#20316;&#20026;&#30417;&#30563;&#65292;&#36825;&#22312;&#21160;&#24577;&#26032;&#20852;&#39046;&#22495;&#20013;&#26159;&#26114;&#36149;&#30340;&#12290;&#26576;&#20123;&#26041;&#27861;&#36890;&#36807;&#20165;&#20381;&#36182;&#31867;&#21517;&#34920;&#38754;&#25991;&#26412;&#20316;&#20026;&#26497;&#24369;&#30417;&#30563;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#33021;&#32771;&#34385;&#21040;&#21333;&#19968;&#31867;&#21035;&#25991;&#26723;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#30340;&#24773;&#20917;&#12290;&#20027;&#39064;&#22810;&#26679;&#24615;&#21644;&#27169;&#31946;&#30340;&#21477;&#23376;&#21487;&#33021;&#20250;&#24341;&#20837;&#22122;&#22768;&#21040;&#25991;&#26723;&#30340;&#24213;&#23618;&#34920;&#31034;&#65292;&#20174;&#32780;&#24433;&#21709;&#39044;&#27979;&#31867;&#21035;&#30340;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#26041;&#27861;&#29420;&#31435;&#22320;&#20851;&#27880;&#25991;&#26723;&#12289;&#21477;&#23376;&#25110;&#21333;&#35789;&#30340;&#25991;&#26412;&#31890;&#24230;&#65292;&#20174;&#32780;&#38480;&#21046;&#20102;&#25105;&#20204;&#32852;&#21512;&#20174;&#25152;&#26377;&#19977;&#32773;&#20013;&#25552;&#21462;&#31895;&#31890;&#24230;&#25110;&#32454;&#31890;&#24230;&#19978;&#19979;&#25991;&#30340;&#33021;&#21147;&#26469;&#35782;&#21035;&#20998;&#31867;&#30340;&#37325;&#35201;&#23376;&#25991;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MEGClass&#65292;&#19968;&#31181;&#21033;&#29992;&#30456;&#20114;&#22686;&#24378;&#30340;&#25991;&#26412;&#31890;&#24230;&#36827;&#34892;&#26497;&#24369;&#30417;&#30563;&#25991;&#26412;&#20998;&#31867;&#30340;&#26041;&#27861;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;MEGClass&#36890;&#36807;&#20998;&#23618;&#20851;&#27880;&#26426;&#21046;&#20174;&#25991;&#26723;&#12289;&#21477;&#23376;&#21644;&#21333;&#35789;&#20013;&#25552;&#21462;&#36830;&#36143;&#19988;&#22810;&#26679;&#30340;&#23376;&#25991;&#26412;&#65292;&#20351;&#25105;&#20204;&#33021;&#22815;&#35782;&#21035;&#21644;&#25972;&#21512;&#26469;&#33258;&#22810;&#20010;&#31890;&#24230;&#30340;&#24369;&#20449;&#21495;&#65292;&#20197;&#20934;&#30830;&#20998;&#31867;&#25991;&#26723;&#65292;&#21363;&#20351;&#23427;&#20204;&#35752;&#35770;&#22810;&#20010;&#20027;&#39064;&#12290;&#22312;&#20116;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20351;&#29992;&#26497;&#24369;&#30417;&#30563;&#30340;&#24773;&#20917;&#19979;&#65292;&#27604;&#29616;&#26377;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#26377;&#30528;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Text classification typically requires a substantial amount of human-annotated data to serve as supervision, which is costly to obtain in dynamic emerging domains. Certain methods seek to address this problem by solely relying on the surface text of class names to serve as extremely weak supervision. However, existing methods fail to account for single-class documents discussing multiple topics. Both topic diversity and vague sentences may introduce noise into the document's underlying representation and consequently the precision of the predicted class. Furthermore, current work focuses on text granularities (documents, sentences, or words) independently, which limits the degree of coarse- or fine-grained context that we can jointly extract from all three to identify significant subtext for classification. In order to address this problem, we propose MEGClass, an extremely weakly-supervised text classification method to exploit Mutually-Enhancing Text Granularities. Specifically, MEGC
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#22522;&#20110;&#26415;&#35821;&#21644;&#20851;&#38190;&#35789;&#20849;&#29616;&#32593;&#32476;&#20998;&#26512;&#65292;&#35782;&#21035;&#20986;ChatGPT&#30340;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#65292;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21644;&#20854;&#30456;&#20851;&#26415;&#35821;&#26159;&#26368;&#24120;&#20986;&#29616;&#30340;&#20851;&#38190;&#35789;&#12290;</title><link>http://arxiv.org/abs/2304.01948</link><description>&lt;p&gt;
&#22522;&#20110;&#26415;&#35821;&#21644;&#20851;&#38190;&#35789;&#20849;&#29616;&#32593;&#32476;&#20998;&#26512;&#30340;ChatGPT&#30740;&#31350;&#30340;&#32593;&#32476;&#21487;&#35270;&#21270;
&lt;/p&gt;
&lt;p&gt;
Network Visualization of ChatGPT Research: a study based on term and keyword co-occurrence network analysis. (arXiv:2304.01948v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#22522;&#20110;&#26415;&#35821;&#21644;&#20851;&#38190;&#35789;&#20849;&#29616;&#32593;&#32476;&#20998;&#26512;&#65292;&#35782;&#21035;&#20986;ChatGPT&#30340;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#65292;&#32467;&#26524;&#34920;&#26126;ChatGPT&#21644;&#20854;&#30456;&#20851;&#26415;&#35821;&#26159;&#26368;&#24120;&#20986;&#29616;&#30340;&#20851;&#38190;&#35789;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#26415;&#35821;&#21644;&#20851;&#38190;&#35789;&#20849;&#29616;&#32593;&#32476;&#26144;&#23556;&#25216;&#26415;&#35782;&#21035;ChatGPT&#30340;&#20027;&#35201;&#30740;&#31350;&#39046;&#22495;&#12290;&#20026;&#36827;&#34892;&#26412;&#30740;&#31350;&#65292;&#20849;&#20174;Lens&#25968;&#25454;&#24211;&#20013;&#26816;&#32034;&#21040;577&#31687;&#25991;&#31456;&#36827;&#34892;&#32593;&#32476;&#21487;&#35270;&#21270;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#8220;chatgpt&#8221;&#26159;&#26368;&#24120;&#20986;&#29616;&#30340;&#35789;&#65292;&#20854;&#27425;&#26159;&#20854;&#30456;&#20851;&#26415;&#35821;&#65292;&#22914;&#20154;&#24037;&#26234;&#33021;&#12289;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12289;GPT&#21644;&#23398;&#20064;&#31561;&#12290;&#26412;&#30740;&#31350;&#23545;&#22270;&#20070;&#39302;&#21644;&#20449;&#24687;&#31185;&#23398;&#20197;&#21450;&#35745;&#31639;&#26426;&#25110;&#20449;&#24687;&#25216;&#26415;&#19987;&#19994;&#20154;&#21592;&#23558;&#26159;&#26377;&#30410;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The main objective of this paper is to identify the major research areas of ChatGPT through term and keyword co-occurrence network mapping techniques. For conducting the present study, total of 577 publications were retrieved from the Lens database for the network visualization. The findings of the study showed that chatgpt occurrence in maximum number of times followed by its related terms such as artificial intelligence, large language model, gpt, study etc. This study will be helpful to library and information science as well as computer or information technology professionals.
&lt;/p&gt;</description></item><item><title>&#25991;&#26412;&#23548;&#21521;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#36328;&#36234;&#39046;&#22495;&#24046;&#24322;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#38754;&#21521;&#20855;&#26377;&#26174;&#30528;&#39046;&#22495;&#24046;&#24322;&#30340;&#30446;&#26631;&#39046;&#22495;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;PODIA-3D&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23039;&#24577;&#20445;&#30041;&#30340;&#25991;&#26412;&#22270;&#20687;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#20197;&#24357;&#34917;&#36825;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;</title><link>http://arxiv.org/abs/2304.01900</link><description>&lt;p&gt;
&#22522;&#20110;&#23039;&#24577;&#20445;&#30041;&#30340;&#25991;&#26412;&#22270;&#20687;&#25193;&#25955;&#30340;PODIA-3D&#65306;&#36328;&#36234;&#22823;&#39046;&#22495;&#38388;&#38553;&#30340;3D&#29983;&#25104;&#27169;&#22411;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;
&lt;/p&gt;
&lt;p&gt;
PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion. (arXiv:2304.01900v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01900
&lt;/p&gt;
&lt;p&gt;
&#25991;&#26412;&#23548;&#21521;&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#22312;&#36328;&#36234;&#39046;&#22495;&#24046;&#24322;&#26041;&#38754;&#21462;&#24471;&#20102;&#36827;&#23637;&#65292;&#20294;&#38754;&#21521;&#20855;&#26377;&#26174;&#30528;&#39046;&#22495;&#24046;&#24322;&#30340;&#30446;&#26631;&#39046;&#22495;&#20173;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;PODIA-3D&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23039;&#24577;&#20445;&#30041;&#30340;&#25991;&#26412;&#22270;&#20687;&#25193;&#25955;&#30340;&#26041;&#27861;&#65292;&#20197;&#24357;&#34917;&#36825;&#26041;&#38754;&#30340;&#19981;&#36275;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;3D&#29983;&#25104;&#27169;&#22411;&#21462;&#24471;&#20102;&#26174;&#33879;&#36827;&#23637;&#65292;&#20294;&#36328;&#36234;&#19981;&#21516;&#39046;&#22495;&#36827;&#34892;&#35757;&#32451;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#65292;&#24182;&#38656;&#35201;&#22823;&#37327;&#30340;&#35757;&#32451;&#25968;&#25454;&#21644;&#23039;&#24577;&#20998;&#24067;&#30693;&#35782;&#12290;&#36890;&#36807;&#20351;&#29992;&#25991;&#26412;&#25552;&#31034;&#30340;&#25991;&#26412;&#23548;&#21521;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#20351;&#29983;&#25104;&#22120;&#33021;&#22815;&#36866;&#24212;&#30446;&#26631;&#39046;&#22495;&#65292;&#20174;&#32780;&#30465;&#21435;&#20102;&#32452;&#35013;&#22823;&#37327;&#25968;&#25454;&#30340;&#38656;&#27714;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;PODIA-3D&#30340;&#39046;&#22495;&#33258;&#36866;&#24212;&#26041;&#27861;&#65292;&#36890;&#36807;&#20445;&#30041;&#23039;&#24577;&#30340;&#25991;&#26412;&#22270;&#20687;&#25193;&#25955;&#26469;&#36328;&#36234;&#22823;&#39046;&#22495;&#38388;&#38553;&#35757;&#32451;3D&#29983;&#25104;&#27169;&#22411;&#65292;&#20197;&#24448;&#30340;&#26041;&#27861;&#30001;&#20110;&#25193;&#25955;&#24335;&#32763;&#35793;&#20013;&#30340;&#24418;&#29366; - &#23039;&#24577;&#26435;&#34913;&#65292;&#23039;&#24577;&#20559;&#35265;&#20197;&#21450;&#30446;&#26631;&#39046;&#22495;&#20013;&#30340;&#23454;&#20363;&#20559;&#35265;&#31561;&#38382;&#39064;&#65292;&#23548;&#33268;&#29983;&#25104;&#30340;&#26679;&#26412;&#20013;3D&#24418;&#29366;&#36739;&#24046;&#65292;&#25991;&#26412;-&#22270;&#20687;&#23545;&#24212;&#24230;&#20302;&#65292;&#29983;&#25104;&#26679;&#26412;&#20013;&#20869;&#37096;&#39046;&#22495;&#22810;&#26679;&#24615;&#19981;&#36275;&#30340;&#38382;&#39064;&#24471;&#21040;&#20102;&#35299;&#20915;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, significant advancements have been made in 3D generative models, however training these models across diverse domains is challenging and requires an huge amount of training data and knowledge of pose distribution. Text-guided domain adaptation methods have allowed the generator to be adapted to the target domains using text prompts, thereby obviating the need for assembling numerous data. Recently, DATID-3D presents impressive quality of samples in text-guided domain, preserving diversity in text by leveraging text-to-image diffusion. However, adapting 3D generators to domains with significant domain gaps from the source domain still remains challenging due to issues in current text-to-image diffusion models as following: 1) shape-pose trade-off in diffusion-based translation, 2) pose bias, and 3) instance bias in the target domain, resulting in inferior 3D shapes, low text-image correspondence, and low intra-domain diversity in the generated samples. To address these issues,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;InfluencerRank&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#30340;&#26041;&#27861;&#35780;&#20272;&#24433;&#21709;&#32773;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#24110;&#21161;&#20225;&#19994;&#22312;&#31038;&#20132;&#24433;&#21709;&#32773;&#33829;&#38144;&#20013;&#23547;&#25214;&#21512;&#36866;&#30340;&#24433;&#21709;&#32773;&#12290;</title><link>http://arxiv.org/abs/2304.01897</link><description>&lt;p&gt;
InfluencerRank&#65306;&#22522;&#20110;&#22270;&#21367;&#31215;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#21457;&#29616;&#26377;&#25928;&#30340;&#24433;&#21709;&#32773;
&lt;/p&gt;
&lt;p&gt;
InfluencerRank: Discovering Effective Influencers via Graph Convolutional Attentive Recurrent Neural Networks. (arXiv:2304.01897v1 [cs.SI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01897
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;InfluencerRank&#65292;&#36890;&#36807;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#21644;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#32467;&#21512;&#30340;&#26041;&#27861;&#35780;&#20272;&#24433;&#21709;&#32773;&#30340;&#26377;&#25928;&#24615;&#65292;&#20197;&#24110;&#21161;&#20225;&#19994;&#22312;&#31038;&#20132;&#24433;&#21709;&#32773;&#33829;&#38144;&#20013;&#23547;&#25214;&#21512;&#36866;&#30340;&#24433;&#21709;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#24433;&#21709;&#32773;&#22312;&#31038;&#20132;&#23186;&#20307;&#33829;&#38144;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#65292;&#20225;&#19994;&#22686;&#21152;&#20102;&#24433;&#21709;&#32773;&#33829;&#38144;&#30340;&#39044;&#31639;&#12290;&#38599;&#29992;&#26377;&#25928;&#30340;&#24433;&#21709;&#32773;&#22312;&#31038;&#20132;&#24433;&#21709;&#32773;&#33829;&#38144;&#20013;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#22312;&#25968;&#20159;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#20013;&#25214;&#21040;&#21512;&#36866;&#30340;&#24433;&#21709;&#32773;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;InfluencerRank&#65292;&#23427;&#22522;&#20110;&#24433;&#21709;&#32773;&#30340;&#21457;&#24067;&#34892;&#20026;&#21644;&#31038;&#20132;&#20851;&#31995;&#35780;&#20272;&#24433;&#21709;&#32773;&#30340;&#26377;&#25928;&#24615;&#12290;&#20026;&#20102;&#34920;&#31034;&#21457;&#24067;&#34892;&#20026;&#21644;&#31038;&#20132;&#20851;&#31995;&#65292;&#24212;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#26469;&#22312;&#19981;&#21516;&#30340;&#21382;&#21490;&#26102;&#38388;&#27573;&#20869;&#23545;&#20855;&#26377;&#24322;&#26500;&#32593;&#32476;&#30340;&#24433;&#21709;&#32773;&#36827;&#34892;&#24314;&#27169;&#12290;&#36890;&#36807;&#23398;&#20064;&#23884;&#20837;&#24335;&#33410;&#28857;&#29305;&#24449;&#30340;&#32593;&#32476;&#32467;&#26500;&#65292;InfluencerRank&#21487;&#20197;&#22312;&#27599;&#20010;&#26102;&#38388;&#27573;&#20026;&#24433;&#21709;&#32773;&#27966;&#29983;&#20449;&#24687;&#20016;&#23500;&#30340;&#34920;&#31034;&#12290;&#26368;&#32456;&#65292;&#19968;&#20010;&#27880;&#24847;&#21147;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#36890;&#36807;&#25429;&#25417;&#24433;&#21709;&#32773;&#34920;&#31034;&#30340;&#21160;&#24577;&#30693;&#35782;&#65292;&#21306;&#20998;&#39640;&#25928;&#30340;&#24433;&#21709;&#32773;&#21644;&#20854;&#20182;&#24433;&#21709;&#32773;&#12290;
&lt;/p&gt;
&lt;p&gt;
As influencers play considerable roles in social media marketing, companies increase the budget for influencer marketing. Hiring effective influencers is crucial in social influencer marketing, but it is challenging to find the right influencers among hundreds of millions of social media users. In this paper, we propose InfluencerRank that ranks influencers by their effectiveness based on their posting behaviors and social relations over time. To represent the posting behaviors and social relations, the graph convolutional neural networks are applied to model influencers with heterogeneous networks during different historical periods. By learning the network structure with the embedded node features, InfluencerRank can derive informative representations for influencers at each period. An attentive recurrent neural network finally distinguishes highly effective influencers from other influencers by capturing the knowledge of the dynamics of influencer representations over time. Extensiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#22235;&#20010;&#36947;&#36335;&#29992;&#25143;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#36935;&#21040;&#19981;&#21516;&#25200;&#21160;&#26102;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#22312;&#21508;&#31181;&#25200;&#21160;&#30340;&#23384;&#22312;&#19979;&#26174;&#33879;&#38477;&#20302;&#65292;&#38656;&#35201;&#36890;&#36807;&#25968;&#25454;&#25193;&#20805;&#31561;&#31574;&#30053;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01895</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#20013;&#36947;&#36335;&#29992;&#25143;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Robustness Benchmark of Road User Trajectory Prediction Models for Automated Driving. (arXiv:2304.01895v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01895
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#22235;&#20010;&#36947;&#36335;&#29992;&#25143;&#36712;&#36857;&#39044;&#27979;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#36935;&#21040;&#19981;&#21516;&#25200;&#21160;&#26102;&#30340;&#24615;&#33021;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#24615;&#33021;&#22312;&#21508;&#31181;&#25200;&#21160;&#30340;&#23384;&#22312;&#19979;&#26174;&#33879;&#38477;&#20302;&#65292;&#38656;&#35201;&#36890;&#36807;&#25968;&#25454;&#25193;&#20805;&#31561;&#31574;&#30053;&#25552;&#39640;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#19988;&#40065;&#26834;&#30340;&#36947;&#36335;&#29992;&#25143;&#36712;&#36857;&#39044;&#27979;&#23545;&#20110;&#23454;&#29616;&#23433;&#20840;&#33258;&#21160;&#39550;&#39542;&#33267;&#20851;&#37325;&#35201;&#12290;&#20026;&#27492;&#65292;&#36890;&#24120;&#20351;&#29992;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#20294;&#24403;&#36935;&#21040;&#20043;&#21069;&#26410;&#35265;&#36807;&#30340;&#36755;&#20837;&#26102;&#65292;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#34920;&#29616;&#20986;&#19981;&#31283;&#23450;&#30340;&#34892;&#20026;&#12290;&#26412;&#25991;&#23545;&#20004;&#20010;&#29615;&#22659;&#24863;&#30693;&#27169;&#22411;&#65288;MotionCNN&#21644;MultiPath++&#65289;&#20197;&#21450;&#20004;&#20010;&#24120;&#35265;&#22522;&#32447;&#27169;&#22411;&#65288;Constant Velocity&#21644;LSTM&#65289;&#36827;&#34892;&#20102;&#40065;&#26834;&#24615;&#22522;&#20934;&#27979;&#35797;&#65292;&#35780;&#20272;&#36825;&#20123;&#27169;&#22411;&#22312;&#38754;&#23545;&#36710;&#36742;&#37096;&#32626;&#26399;&#38388;&#35266;&#23519;&#21040;&#30340;&#21151;&#33021;&#19981;&#36275;&#30340;&#21508;&#31181;&#25200;&#21160;&#26102;&#30340;&#34920;&#29616;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#20123;&#25200;&#21160;&#30340;&#23384;&#22312;&#19979;&#65292;&#24615;&#33021;&#26174;&#33879;&#19979;&#38477;&#65292;&#36890;&#24120;&#20351;&#29992;&#30340;&#36712;&#36857;&#39044;&#27979;&#35780;&#20272;&#25351;&#26631;&#30340;&#35823;&#24046;&#22686;&#21152;&#20102;&#39640;&#36798;+1444.8&#65285;&#12290;&#20351;&#29992;&#31867;&#20284;&#25200;&#21160;&#30340;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#26377;&#25928;&#20943;&#23569;&#20102;&#24615;&#33021;&#38477;&#20302;&#65292;&#35823;&#24046;&#22686;&#38271;&#26368;&#39640;&#21487;&#36798;+87.5&#65285;&#12290;&#26412;&#25991;&#35748;&#20026;&#65292;&#23613;&#31649;&#25968;&#25454;&#25193;&#20805;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#32531;&#35299;&#31574;&#30053;&#65292;&#20294;&#20173;&#38656;&#35201;&#26356;&#22810;&#24037;&#20316;&#26469;&#25552;&#39640;&#27169;&#22411;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate and robust trajectory predictions of road users are needed to enable safe automated driving. To do this, machine learning models are often used, which can show erratic behavior when presented with previously unseen inputs. In this work, two environment-aware models (MotionCNN and MultiPath++) and two common baselines (Constant Velocity and an LSTM) are benchmarked for robustness against various perturbations that simulate functional insufficiencies observed during model deployment in a vehicle: unavailability of road information, late detections, and noise. Results show significant performance degradation under the presence of these perturbations, with errors increasing up to +1444.8\% in commonly used trajectory prediction evaluation metrics. Training the models with similar perturbations effectively reduces performance degradation, with error increases of up to +87.5\%. We argue that despite being an effective mitigation strategy, data augmentation through perturbations duri
&lt;/p&gt;</description></item><item><title>HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01890</link><description>&lt;p&gt;
&#31038;&#20250;&#25991;&#21270;&#30693;&#35782;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#23545;&#36873;&#39033;&#30340;&#36873;&#25321;&#26159;&#24517;&#35201;&#30340;
&lt;/p&gt;
&lt;p&gt;
Sociocultural knowledge is needed for selection of shots in hate speech detection tasks. (arXiv:2304.01890v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01890
&lt;/p&gt;
&lt;p&gt;
HATELEXICON&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#20167;&#24680;&#35328;&#35770;&#30340;&#35789;&#27719;&#34920;&#65292;&#21033;&#29992;&#20854;&#21487;&#20197;&#25552;&#39640;&#27169;&#22411;&#22312;&#35757;&#32451;&#20013;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#24341;&#20837;&#20102;HATELEXICON&#65292;&#36825;&#26159;&#19968;&#20010;&#21253;&#21547;&#24052;&#35199;&#65292;&#24503;&#22269;&#65292;&#21360;&#24230;&#21644;&#32943;&#23612;&#20122;&#30340;&#34065;&#31216;&#21644;&#20167;&#24680;&#35328;&#35770;&#30446;&#26631;&#30340;&#35789;&#27719;&#34920;&#65292;&#20197;&#24110;&#21161;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#22914;&#20309;&#29992;&#20110;&#35299;&#37322;&#27169;&#22411;&#39044;&#27979;&#65292;&#34920;&#26126;&#21457;&#23637;&#29992;&#20110;&#20998;&#31867;&#26497;&#31471;&#35328;&#35770;&#30340;&#27169;&#22411;&#65292;&#22312;&#36827;&#34892;&#39044;&#27979;&#26102;&#20005;&#37325;&#20381;&#36182;&#30446;&#26631;&#35789;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;HATELEXICON&#26469;&#36741;&#21161;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#35757;&#32451;&#36873;&#39033;&#30340;&#26041;&#27861;&#65292;&#36873;&#39033;&#36873;&#25321;&#22312;&#23567;&#26679;&#26412;&#23398;&#20064;&#20013;&#23588;&#20026;&#37325;&#35201;&#12290;&#22312;&#25105;&#20204;&#30340;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;HASOC&#25968;&#25454;&#23545;&#24503;&#35821;&#21644;&#21360;&#22320;&#35821;&#36827;&#34892;&#20102;&#20960;&#20010;&#31034;&#33539;&#23398;&#20064;&#65292;&#24182;&#23558;Multilingual HateCheck&#65288;MHC&#65289;&#20316;&#20026;&#22522;&#20934;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26681;&#25454;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#36873;&#25321;&#26679;&#26412;&#65292;&#30456;&#23545;&#20110;&#38543;&#26426;&#37319;&#26679;&#30340;&#27169;&#22411;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#22312;MHC&#19978;&#34920;&#29616;&#12290;&#22240;&#27492;&#65292;&#24403;&#20165;&#26377;&#23569;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#26102;&#65292;&#20351;&#29992;&#25105;&#20204;&#30340;&#35789;&#27719;&#34920;&#26469;&#36873;&#25321;&#21253;&#21547;&#26356;&#22810;&#31038;&#20250;&#25991;&#21270;&#20449;&#24687;&#30340;&#26679;&#26412;&#33021;&#22815;&#26356;&#22909;&#22320;&#25552;&#39640;&#22312;&#20167;&#24680;&#35328;&#35770;&#26816;&#27979;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce HATELEXICON, a lexicon of slurs and targets of hate speech for the countries of Brazil, Germany, India and Kenya, to aid training and interpretability of models. We demonstrate how our lexicon can be used to interpret model predictions, showing that models developed to classify extreme speech rely heavily on target words when making predictions. Further, we propose a method to aid shot selection for training in low-resource settings via HATELEXICON. In few-shot learning, the selection of shots is of paramount importance to model performance. In our work, we simulate a few-shot setting for German and Hindi, using HASOC data for training and the Multilingual HateCheck (MHC) as a benchmark. We show that selecting shots based on our lexicon leads to models performing better on MHC than models trained on shots sampled randomly. Thus, when given only a few training examples, using our lexicon to select shots containing more sociocultural information leads to better few-shot perf
&lt;/p&gt;</description></item><item><title>Deep-BIAS&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#25193;&#23637;&#24037;&#20855;&#31665;&#65292;&#29992;&#20110;&#26816;&#27979;&#25628;&#32034;&#31639;&#27861;&#20013;&#30340;&#32467;&#26500;&#20559;&#24046;&#12290;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#24555;&#36895;&#26816;&#27979;&#22522;&#20110;&#21407;&#22987;&#24615;&#33021;&#20998;&#24067;&#30340;&#20559;&#24046;&#30340;&#24378;&#24230;&#21644;&#31867;&#22411;&#12290;&#35813;&#24037;&#20855;&#31665;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20855;&#26377;&#32467;&#26500;&#20559;&#24046;&#24773;&#20917;&#30340;&#23454;&#39564;&#20013;&#21313;&#20998;&#26377;&#25928;&#65292;&#24182;&#22312;336&#31181;&#26368;&#20808;&#36827;&#30340;&#20248;&#21270;&#31639;&#27861;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.01869</link><description>&lt;p&gt;
Deep-BIAS:&#20351;&#29992;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26816;&#27979;&#32467;&#26500;&#20559;&#24046;
&lt;/p&gt;
&lt;p&gt;
Deep-BIAS: Detecting Structural Bias using Explainable AI. (arXiv:2304.01869v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01869
&lt;/p&gt;
&lt;p&gt;
Deep-BIAS&#26159;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#25193;&#23637;&#24037;&#20855;&#31665;&#65292;&#29992;&#20110;&#26816;&#27979;&#25628;&#32034;&#31639;&#27861;&#20013;&#30340;&#32467;&#26500;&#20559;&#24046;&#12290;&#36890;&#36807;&#35757;&#32451;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#24555;&#36895;&#26816;&#27979;&#22522;&#20110;&#21407;&#22987;&#24615;&#33021;&#20998;&#24067;&#30340;&#20559;&#24046;&#30340;&#24378;&#24230;&#21644;&#31867;&#22411;&#12290;&#35813;&#24037;&#20855;&#31665;&#24050;&#34987;&#35777;&#26126;&#22312;&#21508;&#31181;&#20855;&#26377;&#32467;&#26500;&#20559;&#24046;&#24773;&#20917;&#30340;&#23454;&#39564;&#20013;&#21313;&#20998;&#26377;&#25928;&#65292;&#24182;&#22312;336&#31181;&#26368;&#20808;&#36827;&#30340;&#20248;&#21270;&#31639;&#27861;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35780;&#20272;&#21551;&#21457;&#24335;&#20248;&#21270;&#31639;&#27861;&#30340;&#24615;&#33021;&#23545;&#20110;&#30830;&#23450;&#23427;&#20204;&#22312;&#21508;&#31181;&#26465;&#20214;&#19979;&#30340;&#34920;&#29616;&#22914;&#20309;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#24341;&#20837;&#20102;BIAS&#24037;&#20855;&#21253;&#20316;&#20026;&#34892;&#20026;&#22522;&#20934;&#26469;&#26816;&#27979;&#25628;&#32034;&#31639;&#27861;&#20013;&#30340;&#32467;&#26500;&#20559;&#24046;&#65288;SB&#65289;&#12290;&#35813;&#24037;&#20855;&#21253;&#21487;&#29992;&#20110;&#35782;&#21035;&#29616;&#26377;&#31639;&#27861;&#20013;&#30340;&#20559;&#24046;&#65292;&#20197;&#21450;&#27979;&#35797;&#26032;&#24320;&#21457;&#30340;&#31639;&#27861;&#30340;&#20559;&#24046;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;BIAS&#24037;&#20855;&#31665;&#30340;&#19968;&#31181;&#26032;&#39062;&#19988;&#21487;&#35299;&#37322;&#30340;&#28145;&#24230;&#23398;&#20064;&#25193;&#23637;&#65292;&#31216;&#20026;Deep-BIAS&#12290;&#21407;&#22987;&#24037;&#20855;&#31665;&#20351;&#29992;39&#20010;&#32479;&#35745;&#27979;&#35797;&#21644;&#19968;&#20010;&#38543;&#26426;&#26862;&#26519;&#27169;&#22411;&#26469;&#39044;&#27979;SB&#30340;&#23384;&#22312;&#21644;&#31867;&#22411;&#65292;&#32780;Deep-BIAS&#26041;&#27861;&#20351;&#29992;&#35757;&#32451;&#26377;&#32032;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#26469;&#31435;&#21363;&#26816;&#27979;&#22522;&#20110;&#21407;&#22987;&#24615;&#33021;&#20998;&#24067;&#30340;SB&#30340;&#24378;&#24230;&#21644;&#31867;&#22411;&#12290;&#36890;&#36807;&#19968;&#31995;&#21015;&#20855;&#26377;&#21508;&#31181;&#32467;&#26500;&#20559;&#24046;&#24773;&#20917;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;Deep-BIAS&#30340;&#26377;&#25928;&#24615;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#20351;&#29992;&#24037;&#20855;&#31665;&#23545;336&#31181;&#26368;&#20808;&#36827;&#30340;&#20248;&#21270;&#31639;&#27861;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Evaluating the performance of heuristic optimisation algorithms is essential to determine how well they perform under various conditions. Recently, the BIAS toolbox was introduced as a behaviour benchmark to detect structural bias (SB) in search algorithms. The toolbox can be used to identify biases in existing algorithms, as well as to test for bias in newly developed algorithms. In this article, we introduce a novel and explainable deep-learning expansion of the BIAS toolbox, called Deep-BIAS. Where the original toolbox uses 39 statistical tests and a Random Forest model to predict the existence and type of SB, the Deep-BIAS method uses a trained deep-learning model to immediately detect the strength and type of SB based on the raw performance distributions. Through a series of experiments with a variety of structurally biased scenarios, we demonstrate the effectiveness of Deep-BIAS. We also present the results of using the toolbox on 336 state-of-the-art optimisation algorithms, whi
&lt;/p&gt;</description></item><item><title>&#25512;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#19977;&#32500;&#20154;&#20307;&#23039;&#24577;&#25968;&#25454;&#38598;SportsPose&#65292;&#23427;&#21253;&#21547;&#19968;&#20010;&#22810;&#26679;&#21644;&#20840;&#38754;&#30340;3D&#23039;&#24577;&#38598;&#65292;&#21453;&#26144;&#20102;&#20307;&#32946;&#36816;&#21160;&#30340;&#22797;&#26434;&#21644;&#21160;&#24577;&#24615;&#65307;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23616;&#37096;&#36816;&#21160;&#65292;&#23427;&#25551;&#36848;&#20102;&#25163;&#33109;&#21644;&#36381;&#20851;&#33410;&#30340;&#36816;&#21160;&#12290;</title><link>http://arxiv.org/abs/2304.01865</link><description>&lt;p&gt;
SportsPose -- &#19968;&#31181;&#21160;&#24577;&#30340;&#19977;&#32500;&#20307;&#32946;&#23039;&#21183;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
SportsPose -- A Dynamic 3D sports pose dataset. (arXiv:2304.01865v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01865
&lt;/p&gt;
&lt;p&gt;
&#25512;&#20986;&#20102;&#19968;&#20010;&#22823;&#22411;&#30340;&#19977;&#32500;&#20154;&#20307;&#23039;&#24577;&#25968;&#25454;&#38598;SportsPose&#65292;&#23427;&#21253;&#21547;&#19968;&#20010;&#22810;&#26679;&#21644;&#20840;&#38754;&#30340;3D&#23039;&#24577;&#38598;&#65292;&#21453;&#26144;&#20102;&#20307;&#32946;&#36816;&#21160;&#30340;&#22797;&#26434;&#21644;&#21160;&#24577;&#24615;&#65307;&#24182;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#23616;&#37096;&#36816;&#21160;&#65292;&#23427;&#25551;&#36848;&#20102;&#25163;&#33109;&#21644;&#36381;&#20851;&#33410;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20934;&#30830;&#30340;&#19977;&#32500;&#20154;&#20307;&#23039;&#21183;&#20272;&#35745;&#23545;&#20110;&#20307;&#32946;&#20998;&#26512;&#12289;&#25945;&#32451;&#21644;&#39044;&#38450;&#36816;&#21160;&#20260;&#23475;&#33267;&#20851;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#21333;&#30446;&#23039;&#21183;&#20272;&#35745;&#25968;&#25454;&#38598;&#26080;&#27861;&#20805;&#20998;&#25429;&#25417;&#21040;&#36816;&#21160;&#21160;&#20316;&#30340;&#25361;&#25112;&#24615;&#21644;&#21160;&#24577;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25512;&#20986;&#20102;SportsPose&#65292;&#19968;&#20010;&#22823;&#22411;&#30340;&#19977;&#32500;&#20154;&#20307;&#23039;&#24577;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#39640;&#24230;&#21160;&#24577;&#30340;&#20307;&#32946;&#36816;&#21160;&#12290;SportsPose&#25552;&#20379;&#20102;&#36229;&#36807;176,000&#20010;&#26469;&#33258;24&#20010;&#19981;&#21516;&#21463;&#35797;&#32773;&#36827;&#34892;5&#31181;&#19981;&#21516;&#20307;&#32946;&#27963;&#21160;&#30340;3D&#23039;&#24577;&#65292;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#26679;&#19988;&#20840;&#38754;&#30340;3D&#23039;&#24577;&#38598;&#65292;&#21453;&#26144;&#20102;&#20307;&#32946;&#36816;&#21160;&#30340;&#22797;&#26434;&#21644;&#21160;&#24577;&#24615;&#12290;&#19982;&#20854;&#20182;&#26080;&#26631;&#35760;&#25968;&#25454;&#38598;&#19981;&#21516;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#25105;&#20204;&#30340;&#23039;&#24577;&#19982;&#21830;&#19994;&#26631;&#35760;&#31995;&#32479;&#36827;&#34892;&#27604;&#36739;&#26469;&#23450;&#37327;&#35780;&#20272;&#20102;SportsPose&#30340;&#31934;&#24230;&#65292;&#24179;&#22343;&#35823;&#24046;&#22312;&#25152;&#26377;&#35780;&#20272;&#24207;&#21015;&#20013;&#20026;34.5mm&#12290;&#36825;&#19982;&#24120;&#29992;&#30340;3DPW&#25968;&#25454;&#38598;&#25253;&#36947;&#30340;&#35823;&#24046;&#30456;&#24403;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#26032;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;&#31216;&#20026;&#23616;&#37096;&#36816;&#21160;&#65292;&#23427;&#25551;&#36848;&#20102;&#25163;&#33109;&#21644;&#36381;&#20851;&#33410;&#30340;&#36816;&#21160;&#12290;
&lt;/p&gt;
&lt;p&gt;
Accurate 3D human pose estimation is essential for sports analytics, coaching, and injury prevention. However, existing datasets for monocular pose estimation do not adequately capture the challenging and dynamic nature of sports movements. In response, we introduce SportsPose, a large-scale 3D human pose dataset consisting of highly dynamic sports movements. With more than 176,000 3D poses from 24 different subjects performing 5 different sports activities, SportsPose provides a diverse and comprehensive set of 3D poses that reflect the complex and dynamic nature of sports movements. Contrary to other markerless datasets we have quantitatively evaluated the precision of SportsPose by comparing our poses with a commercial marker-based system and achieve a mean error of 34.5 mm across all evaluation sequences. This is comparable to the error reported on the commonly used 3DPW dataset. We further introduce a new metric, local movement, which describes the movement of the wrist and ankle 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Grid-SD2E&#30340;&#35748;&#30693;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#24314;&#31435;&#22312;&#32593;&#26684;&#32454;&#32990;&#30340;&#22522;&#30784;&#19978;&#65292;&#20351;&#29992;&#31354;&#38388;&#21010;&#20998;&#21644;&#25506;&#32034;&#21033;&#29992;&#26041;&#27861;&#23454;&#29616;&#20132;&#20114;&#21644;&#33258;&#25105;&#24378;&#21270;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#24037;&#20316;&#26426;&#21046;&#12289;&#27835;&#30103;&#33041;&#37096;&#30142;&#30149;&#21644;&#29702;&#35299;&#26234;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01844</link><description>&lt;p&gt;
Grid-SD2E&#65306;&#19968;&#31181;&#35748;&#30693;&#23398;&#20064;&#31995;&#32479;&#20013;&#30340;&#36890;&#29992;&#32593;&#26684;&#21453;&#39304;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Grid-SD2E: A General Grid-Feedback in a System for Cognitive Learning. (arXiv:2304.01844v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01844
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Grid-SD2E&#30340;&#35748;&#30693;&#23398;&#20064;&#31995;&#32479;&#65292;&#20854;&#24314;&#31435;&#22312;&#32593;&#26684;&#32454;&#32990;&#30340;&#22522;&#30784;&#19978;&#65292;&#20351;&#29992;&#31354;&#38388;&#21010;&#20998;&#21644;&#25506;&#32034;&#21033;&#29992;&#26041;&#27861;&#23454;&#29616;&#20132;&#20114;&#21644;&#33258;&#25105;&#24378;&#21270;&#65292;&#26377;&#21161;&#20110;&#29702;&#35299;&#22823;&#33041;&#30340;&#24037;&#20316;&#26426;&#21046;&#12289;&#27835;&#30103;&#33041;&#37096;&#30142;&#30149;&#21644;&#29702;&#35299;&#26234;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#22823;&#33041;&#22914;&#20309;&#36890;&#36807;&#20135;&#29983;&#31070;&#32463;&#20449;&#21495;&#19982;&#22806;&#37096;&#19990;&#30028;&#30456;&#20114;&#20316;&#29992;&#23545;&#20110;&#30830;&#23450;&#20854;&#24037;&#20316;&#26426;&#21046;&#12289;&#27835;&#30103;&#33041;&#37096;&#30142;&#30149;&#21644;&#29702;&#35299;&#26234;&#33021;&#38750;&#24120;&#37325;&#35201;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#24050;&#32463;&#25552;&#20986;&#20102;&#35768;&#22810;&#29702;&#35770;&#27169;&#22411;&#65292;&#20294;&#23427;&#20204;&#36804;&#20170;&#38590;&#20197;&#25972;&#21512;&#21644;&#21457;&#23637;&#12290;&#21463;&#32593;&#26684;&#32454;&#32990;&#21551;&#21457;&#65292;&#26412;&#30740;&#31350;&#21019;&#24314;&#20102;&#19968;&#20010;&#26356;&#36890;&#29992;&#21644;&#24378;&#22823;&#30340;&#32593;&#26684;&#27169;&#22359;&#65292;&#24182;&#19982;&#36125;&#21494;&#26031;&#25512;&#29702;&#19968;&#36215;&#26500;&#24314;&#20102;&#19968;&#20010;&#20114;&#21160;&#21644;&#33258;&#25105;&#24378;&#21270;&#30340;&#35748;&#30693;&#31995;&#32479;&#12290;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#24102;&#26377;&#32593;&#26684;&#21453;&#39304;&#30340;&#31354;&#38388;&#21010;&#20998;&#21644;&#25506;&#32034;&#21033;&#29992;&#65288;Grid-SD2E&#65289;&#12290;&#22312;&#36825;&#37324;&#65292;&#32593;&#26684;&#27169;&#22359;&#21487;&#20197;&#29992;&#20316;&#22806;&#37096;&#19990;&#30028;&#21644;&#31995;&#32479;&#20043;&#38388;&#30340;&#20132;&#20114;&#20171;&#36136;&#65292;&#20063;&#21487;&#20197;&#29992;&#20316;&#31995;&#32479;&#20869;&#30340;&#33258;&#25105;&#24378;&#21270;&#20171;&#36136;&#12290;&#31354;&#38388;&#21010;&#20998;&#21644;&#25506;&#32034;&#21033;&#29992;&#65288;SD2E&#65289;&#36890;&#36807;&#20854;&#31354;&#38388;&#21010;&#20998;&#65288;SD&#65289;&#27169;&#22359;&#25509;&#25910;&#32593;&#26684;&#30340;0/1&#20449;&#21495;&#12290;&#26412;&#25991;&#25551;&#36848;&#30340;&#31995;&#32479;&#20063;&#26159;&#20174;&#36827;&#34892;&#30340;&#23454;&#39564;&#24471;&#20986;&#30340;&#29702;&#35770;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Comprehending how the brain interacts with the external world through generated neural signals is crucial for determining its working mechanism, treating brain diseases, and understanding intelligence. Although many theoretical models have been proposed, they have thus far been difficult to integrate and develop. In this study, we were inspired in part by grid cells in creating a more general and robust grid module and constructing an interactive and self-reinforcing cognitive system together with Bayesian reasoning, an approach called space-division and exploration-exploitation with grid-feedback (Grid-SD2E). Here, a grid module can be used as an interaction medium between the outside world and a system, as well as a self-reinforcement medium within the system. The space-division and exploration-exploitation (SD2E) receives the 0/1 signals of a grid through its space-division (SD) module. The system described in this paper is also a theoretical model derived from experiments conducted
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BugNIST&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;12&#31181;&#26118;&#34411;&#21644;&#24188;&#34411;&#30340;&#24494;-CT&#25195;&#25551;&#32452;&#25104;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#27979;&#35797;&#26816;&#27979;&#27169;&#22411;&#65292;BugNIST&#26088;&#22312;&#35780;&#20272;&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#31867;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.01838</link><description>&lt;p&gt;
BugNIST -- &#19968;&#31181;&#26032;&#30340;&#22823;&#35268;&#27169;&#20307;&#31215;&#19977;&#32500;&#22270;&#20687;&#25968;&#25454;&#38598;&#65292;&#29992;&#20110;&#20998;&#31867;&#21644;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
BugNIST -- A New Large Scale Volumetric 3D Image Dataset for Classification and Detection. (arXiv:2304.01838v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01838
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#20010;&#21517;&#20026;BugNIST&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#30001;12&#31181;&#26118;&#34411;&#21644;&#24188;&#34411;&#30340;&#24494;-CT&#25195;&#25551;&#32452;&#25104;&#12290;&#36890;&#36807;&#35757;&#32451;&#21644;&#27979;&#35797;&#26816;&#27979;&#27169;&#22411;&#65292;BugNIST&#26088;&#22312;&#35780;&#20272;&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#31867;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#35299;&#20915;&#19978;&#19979;&#25991;&#26080;&#20851;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#30740;&#31350;&#30340;&#36827;&#23637;&#21463;&#21040;&#25968;&#25454;&#38598;&#32570;&#20047;&#30340;&#38480;&#21046;&#65292;&#22823;&#22810;&#25968;&#38024;&#23545;&#20307;&#31215;&#22270;&#20687;&#30340;&#20998;&#26512;&#26041;&#27861;&#37117;&#22522;&#20110;&#21307;&#23398;&#25968;&#25454;&#12290;&#28982;&#32780;&#65292;&#21307;&#23398;&#25968;&#25454;&#24182;&#19981;&#19968;&#23450;&#20855;&#26377;&#20854;&#20182;&#20307;&#31215;&#22270;&#20687;&#65288;&#20363;&#22914;&#24494;-CT&#65289;&#30340;&#29305;&#24449;&#12290;&#20026;&#20102;&#20419;&#36827;&#19977;&#32500;&#20307;&#31215;&#22270;&#20687;&#20998;&#26512;&#30340;&#30740;&#31350;&#36229;&#36234;&#21307;&#23398;&#25968;&#25454;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;BugNIST&#25968;&#25454;&#38598;&#24182;&#20813;&#36153;&#25552;&#20379;&#12290;BugNIST&#26159;&#19968;&#32452;&#30001;12&#31181;&#26118;&#34411;&#21644;&#24188;&#34411;&#30340;&#24494;-CT&#25195;&#25551;&#32452;&#25104;&#30340;&#24191;&#27867;&#25968;&#25454;&#38598;&#12290;BugNIST&#21253;&#21547;9437&#20010;&#20307;&#31215;&#65292;&#20854;&#20013;9087&#20010;&#26159;&#21333;&#20010;&#26118;&#34411;&#30340;&#25195;&#25551;&#65292;350&#20010;&#26159;&#26118;&#34411;&#21644;&#20854;&#20182;&#26448;&#26009;&#30340;&#28151;&#21512;&#29289;&#12290;BugNIST&#30340;&#30446;&#26631;&#26159;&#35780;&#20272;&#20998;&#31867;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#26816;&#27979;&#25361;&#25112;&#65292;&#20351;&#24471;&#26816;&#27979;&#27169;&#22411;&#22312;&#21333;&#20010;&#26118;&#34411;&#30340;&#25195;&#25551;&#19978;&#35757;&#32451;&#24182;&#22312;&#26118;&#34411;&#28151;&#21512;&#29289;&#19978;&#36827;&#34892;&#27979;&#35797;&#12290;&#33021;&#22815;&#35299;&#20915;&#27492;&#20219;&#21153;&#30340;&#27169;&#22411;&#23558;&#29420;&#31435;&#20110;&#19978;&#19979;&#25991;&#65288;&#21363;&#21608;&#22260;&#26448;&#26009;&#65289;&#65292;&#36825;&#26159;&#19968;&#20010;&#24456;&#22823;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
Progress in 3D volumetric image analysis research is limited by the lack of datasets and most advances in analysis methods for volumetric images are based on medical data. However, medical data do not necessarily resemble the characteristics of other volumetric images such as micro-CT. To promote research in 3D volumetric image analysis beyond medical data, we have created the BugNIST dataset and made it freely available. BugNIST is an extensive dataset of micro-CT scans of 12 types of bugs, such as insects and larvae. BugNIST contains 9437 volumes where 9087 are of individual bugs and 350 are mixtures of bugs and other material. The goal of BugNIST is to benchmark classification and detection methods, and we have designed the detection challenge such that detection models are trained on scans of individual bugs and tested on bug mixtures. Models capable of solving this task will be independent of the context, i.e., the surrounding material. This is a great advantage if the context is 
&lt;/p&gt;</description></item><item><title>&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#25968;&#25454;&#22402;&#30452;&#20998;&#21306;&#24773;&#20917;&#30340;&#26377;&#21069;&#36884;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20016;&#23500;&#20102;&#26679;&#26412;&#25551;&#36848;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23481;&#37327;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#30828;&#20214;&#23618;&#21040;&#22402;&#30452;&#32852;&#37030;&#31995;&#32479;&#23618;&#21508;&#20010;&#26041;&#38754;&#20570;&#20102;&#36129;&#29486;&#65292;VFL&#30340;&#24212;&#29992;&#24050;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#65292;&#23588;&#20854;&#38544;&#31169;&#20445;&#25252;&#26159;&#20851;&#38190;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01829</link><description>&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#32508;&#36848;&#65306;&#20197;&#20998;&#23618;&#35270;&#35282;&#20026;&#22522;&#30784;
&lt;/p&gt;
&lt;p&gt;
A Survey on Vertical Federated Learning: From a Layered Perspective. (arXiv:2304.01829v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01829
&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#36866;&#29992;&#20110;&#25968;&#25454;&#22402;&#30452;&#20998;&#21306;&#24773;&#20917;&#30340;&#26377;&#21069;&#36884;&#30340;&#32852;&#37030;&#23398;&#20064;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20016;&#23500;&#20102;&#26679;&#26412;&#25551;&#36848;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23481;&#37327;&#12290;&#30740;&#31350;&#20154;&#21592;&#22312;&#30828;&#20214;&#23618;&#21040;&#22402;&#30452;&#32852;&#37030;&#31995;&#32479;&#23618;&#21508;&#20010;&#26041;&#38754;&#20570;&#20102;&#36129;&#29486;&#65292;VFL&#30340;&#24212;&#29992;&#24050;&#28085;&#30422;&#22810;&#20010;&#39046;&#22495;&#65292;&#23588;&#20854;&#38544;&#31169;&#20445;&#25252;&#26159;&#20851;&#38190;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22402;&#30452;&#32852;&#37030;&#23398;&#20064;&#65288;VFL&#65289;&#26159;&#19968;&#31181;&#26377;&#21069;&#36884;&#30340;&#32852;&#37030;&#23398;&#20064;&#33539;&#30068;&#65292;&#36866;&#29992;&#20110;&#25968;&#25454;&#22402;&#30452;&#20998;&#21306;&#24182;&#20998;&#24067;&#22312;&#21508;&#26041;&#20043;&#38388;&#30340;&#24773;&#20917;&#12290;VFL&#20351;&#29992;&#26469;&#33258;&#19981;&#21516;&#26041;&#30340;&#29305;&#24449;&#26469;&#20016;&#23500;&#26679;&#26412;&#25551;&#36848;&#65292;&#20197;&#25552;&#39640;&#27169;&#22411;&#23481;&#37327;&#12290;&#19982;&#27700;&#24179;&#32852;&#37030;&#23398;&#20064;&#30456;&#27604;&#65292;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;VFL&#24212;&#29992;&#20110;&#20844;&#21496;&#30340;&#21830;&#19994;&#21512;&#20316;&#22330;&#26223;&#20013;&#65292;&#22240;&#27492;VFL&#21253;&#21547;&#24040;&#22823;&#30340;&#21830;&#19994;&#20215;&#20540;&#12290;&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;VFL&#22312;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#26412;&#25991;&#20174;&#20998;&#23618;&#35270;&#35282;&#31995;&#32479;&#22320;&#35843;&#26597;&#20102;VFL&#30340;&#29616;&#26377;&#30740;&#31350;&#24037;&#20316;&#12290;&#20174;&#30828;&#20214;&#23618;&#21040;&#22402;&#30452;&#32852;&#37030;&#31995;&#32479;&#23618;&#65292;&#30740;&#31350;&#20154;&#21592;&#36129;&#29486;&#20102;VFL&#30340;&#21508;&#20010;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;VFL&#30340;&#24212;&#29992;&#24050;&#35206;&#30422;&#20102;&#24191;&#27867;&#30340;&#39046;&#22495;&#65292;&#20363;&#22914;&#37329;&#34701;&#12289;&#21307;&#30103;&#31561;&#12290;&#22312;&#27599;&#20010;&#23618;&#38754;&#19978;&#65292;&#25105;&#20204;&#23545;&#29616;&#26377;&#24037;&#20316;&#36827;&#34892;&#20998;&#31867;&#24182;&#25506;&#35752;&#20102;&#36827;&#19968;&#27493;&#30740;&#31350;&#21644;&#21457;&#23637;VFL&#30340;&#25361;&#25112;&#12290;&#29305;&#21035;&#20851;&#27880;&#38544;&#31169;&#20445;&#25252;&#65292;&#36825;&#26159;VFL&#20013;&#30001;&#20110;&#25968;&#25454;&#20998;&#24067;&#22312;&#21508;&#26041;&#20043;&#38388;&#32780;&#20855;&#26377;&#20851;&#38190;&#24615;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vertical federated learning (VFL) is a promising category of federated learning for the scenario where data is vertically partitioned and distributed among parties. VFL enriches the description of samples using features from different parties to improve model capacity. Compared with horizontal federated learning, in most cases, VFL is applied in the commercial cooperation scenario of companies. Therefore, VFL contains tremendous business values. In the past few years, VFL has attracted more and more attention in both academia and industry. In this paper, we systematically investigate the current work of VFL from a layered perspective. From the hardware layer to the vertical federated system layer, researchers contribute to various aspects of VFL. Moreover, the application of VFL has covered a wide range of areas, e.g., finance, healthcare, etc. At each layer, we categorize the existing work and explore the challenges for the convenience of further research and development of VFL. Espec
&lt;/p&gt;</description></item><item><title>HarsanyiNet &#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#25773;&#20013;&#35745;&#31639;&#36755;&#20837;&#21464;&#37327;&#30340;&#31934;&#30830; Shapley &#20540;&#12290;</title><link>http://arxiv.org/abs/2304.01811</link><description>&lt;p&gt;
HarsanyiNet: &#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#25773;&#20013;&#35745;&#31639;&#20934;&#30830;&#30340; Shapley &#20540;
&lt;/p&gt;
&lt;p&gt;
HarsanyiNet: Computing Accurate Shapley Values in a Single Forward Propagation. (arXiv:2304.01811v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01811
&lt;/p&gt;
&lt;p&gt;
HarsanyiNet &#26159;&#19968;&#31181;&#26032;&#22411;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#23427;&#21487;&#20197;&#22312;&#21333;&#27425;&#21069;&#21521;&#20256;&#25773;&#20013;&#35745;&#31639;&#36755;&#20837;&#21464;&#37327;&#30340;&#31934;&#30830; Shapley &#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Shapley &#20540;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#19968;&#31181;&#21487;&#20449;&#30340;&#23646;&#24615;&#24230;&#37327;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#24403;&#20154;&#20204;&#20351;&#29992; Shapley &#20540;&#26469;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#30340;&#36755;&#20837;&#21464;&#37327;&#30340;&#23646;&#24615;&#26102;&#65292;&#36890;&#24120;&#38656;&#35201;&#38750;&#24120;&#39640;&#30340;&#35745;&#31639;&#25104;&#26412;&#25165;&#33021;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#36817;&#20284;&#35745;&#31639;&#20986;&#27604;&#36739;&#31934;&#30830;&#30340; Shapley &#20540;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#19968;&#31181;&#26032;&#22411;&#32593;&#32476;&#26550;&#26500; HarsanyiNet&#65292;&#22312;&#36755;&#20837;&#26679;&#26412;&#30340;&#25512;&#29702;&#36807;&#31243;&#20013;&#21516;&#26102;&#35745;&#31639;&#36755;&#20837;&#21464;&#37327;&#30340;&#31934;&#30830; Shapley &#20540;&#65292;&#21482;&#38656;&#35201;&#19968;&#27425;&#21069;&#21521;&#20256;&#25773;&#21363;&#21487;&#12290;HarsanyiNet &#26159;&#26500;&#24314;&#22312; Shapley &#20540;&#21487;&#20197;&#34987;&#37325;&#26032;&#26500;&#24314;&#20026;&#32593;&#32476;&#32534;&#30721;&#30340; Harsanyi &#20132;&#20114;&#37325;&#26032;&#20998;&#37197;&#30340;&#29702;&#35770;&#22522;&#30784;&#20043;&#19978;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Shapley value is widely regarded as a trustworthy attribution metric. However, when people use Shapley values to explain the attribution of input variables of a deep neural network (DNN), it usually requires a very high computational cost to approximate relatively accurate Shapley values in real-world applications. Therefore, we propose a novel network architecture, the HarsanyiNet, which makes inferences on the input sample and simultaneously computes the exact Shapley values of the input variables in a single forward propagation. The HarsanyiNet is designed on the theoretical foundation that the Shapley value can be reformulated as the redistribution of Harsanyi interactions encoded by the network.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;Transformer&#23454;&#29616;&#36731;&#37327;&#21270;&#21333;&#22270;&#20687;&#21435;&#22122;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19971;&#31181;&#27604;&#36739;&#22522;&#32447;Transformer&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#21098;&#35009;&#34917;&#19969;&#30340;&#37096;&#20998;&#23545;&#21435;&#22122;&#24615;&#33021;&#30340;&#24433;&#21709;&#65307;&#32780;&#20197;&#24448;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#20197;&#30495;&#27491;&#20844;&#24179;&#30340;&#26041;&#24335;&#35757;&#32451;&#22522;&#32447;Transformer&#65292;&#24182;&#36827;&#34892;&#20102;&#21508;&#31181;&#32452;&#20214;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#26500;&#24314;&#36731;&#37327;&#21270;&#21435;&#22122;Transformer&#30340;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2304.01805</link><description>&lt;p&gt;
&#37319;&#29992;Transformer&#23454;&#29616;&#36731;&#37327;&#21270;&#21333;&#22270;&#20687;&#21435;&#22122;&#24182;&#36827;&#34892;&#30495;&#27491;&#20844;&#24179;&#35757;&#32451;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Exploration of Lightweight Single Image Denoising with Transformers and Truly Fair Training. (arXiv:2304.01805v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01805
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#37319;&#29992;Transformer&#23454;&#29616;&#36731;&#37327;&#21270;&#21333;&#22270;&#20687;&#21435;&#22122;&#30340;&#38382;&#39064;&#65292;&#25552;&#20379;&#20102;&#19971;&#31181;&#27604;&#36739;&#22522;&#32447;Transformer&#65292;&#24182;&#25506;&#35752;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#38543;&#26426;&#21098;&#35009;&#34917;&#19969;&#30340;&#37096;&#20998;&#23545;&#21435;&#22122;&#24615;&#33021;&#30340;&#24433;&#21709;&#65307;&#32780;&#20197;&#24448;&#30340;&#30740;&#31350;&#24573;&#30053;&#20102;&#36825;&#19968;&#38382;&#39064;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#20197;&#30495;&#27491;&#20844;&#24179;&#30340;&#26041;&#24335;&#35757;&#32451;&#22522;&#32447;Transformer&#65292;&#24182;&#36827;&#34892;&#20102;&#21508;&#31181;&#32452;&#20214;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#26500;&#24314;&#36731;&#37327;&#21270;&#21435;&#22122;Transformer&#30340;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#25968;&#23383;&#35774;&#22791;&#30340;&#22266;&#26377;&#32570;&#38519;&#65292;&#22810;&#23186;&#20307;&#20869;&#23481;&#36890;&#24120;&#21253;&#21547;&#22122;&#22768;&#12290;&#22240;&#27492;&#65292;&#22312;&#39640;&#32423;&#35270;&#35273;&#35782;&#21035;&#20219;&#21153;&#20013;&#65292;&#22270;&#20687;&#21435;&#22122;&#26159;&#37325;&#35201;&#30340;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;&#34429;&#28982;&#24050;&#32463;&#26377;&#19968;&#20123;&#30740;&#31350;&#37319;&#29992;&#20808;&#36827;&#30340;Transformer&#24320;&#21457;&#20102;&#21435;&#22122;&#39046;&#22495;&#65292;&#20294;&#26159;&#36825;&#20123;&#32593;&#32476;&#23545;&#20110;&#23454;&#38469;&#24212;&#29992;&#26469;&#35828;&#36807;&#20110;&#21344;&#29992;&#20869;&#23384;&#12290;&#27492;&#22806;&#65292;&#23545;&#20110;&#37319;&#29992;Transformer&#30340;&#36731;&#37327;&#21270;&#21435;&#22122;&#39046;&#22495;&#30340;&#30740;&#31350;&#36824;&#32570;&#20047;&#12290;&#20026;&#24212;&#23545;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#19971;&#31181;&#36731;&#37327;&#32423;&#21435;&#22122;&#30340;&#27604;&#36739;&#22522;&#32447;Transformer&#65292;&#20026;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#22522;&#30784;&#12290;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#65292;&#38543;&#26426;&#21098;&#35009;&#34917;&#19969;&#30340;&#37096;&#20998;&#20250;&#26174;&#33879;&#24433;&#21709;&#21435;&#22122;&#24615;&#33021;&#65292;&#32780;&#20197;&#21069;&#30340;&#30740;&#31350;&#21364;&#24573;&#30053;&#20102;&#36825;&#19968;&#26041;&#38754;&#12290;&#25105;&#20204;&#30340;&#30446;&#26631;&#26159;&#20197;&#30495;&#27491;&#20844;&#24179;&#30340;&#26041;&#24335;&#35757;&#32451;&#22522;&#32447;Transformer&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#36827;&#34892;&#20102;&#21508;&#31181;&#32452;&#20214;&#30340;&#23454;&#35777;&#20998;&#26512;&#65292;&#20197;&#30830;&#23450;&#26500;&#24314;&#36731;&#37327;&#21270;&#21435;&#22122;Transformer&#30340;&#20851;&#38190;&#32771;&#34385;&#22240;&#32032;&#12290;&#20195;&#30721;&#21487;&#22312; https://github.com/rami0205/LWDN &#19978;&#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
As multimedia content often contains noise from intrinsic defects of digital devices, image denoising is an important step for high-level vision recognition tasks. Although several studies have developed the denoising field employing advanced Transformers, these networks are too momory-intensive for real-world applications. Additionally, there is a lack of research on lightweight denosing (LWDN) with Transformers. To handle this, this work provides seven comparative baseline Transformers for LWDN, serving as a foundation for future research. We also demonstrate the parts of randomly cropped patches significantly affect the denoising performances during training. While previous studies have overlooked this aspect, we aim to train our baseline Transformers in a truly fair manner. Furthermore, we conduct empirical analyses of various components to determine the key considerations for constructing LWDN Transformers. Codes are available at https://github.com/rami0205/LWDN.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38382;&#39064;&#20013;&#33719;&#21462;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#26041;&#24335;&#24182;&#24471;&#20986;&#21518;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30693;&#35782;&#26356;&#21512;&#36866;&#30340;&#32467;&#35770;&#12290;</title><link>http://arxiv.org/abs/2304.01771</link><description>&lt;p&gt;
&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38382;&#39064;&#20013;&#33719;&#21462;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Using Language Models For Knowledge Acquisition in Natural Language Reasoning Problems. (arXiv:2304.01771v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01771
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#33258;&#28982;&#35821;&#35328;&#25512;&#29702;&#38382;&#39064;&#20013;&#33719;&#21462;&#30693;&#35782;&#30340;&#26041;&#27861;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#26041;&#24335;&#24182;&#24471;&#20986;&#21518;&#19968;&#31181;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#25552;&#21462;&#30693;&#35782;&#26356;&#21512;&#36866;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#20110;&#38656;&#35201;&#19968;&#20123;&#38750;&#24179;&#20961;&#25512;&#29702;&#25165;&#33021;&#35299;&#20915;&#30340;&#33258;&#28982;&#35821;&#35328;&#38382;&#39064;&#65292;&#21487;&#20197;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#33267;&#23569;&#20004;&#31181;&#26041;&#27861;&#36827;&#34892;&#35299;&#20915;&#12290;&#19968;&#31181;&#26041;&#27861;&#26159;&#30452;&#25509;&#35201;&#27714;&#23427;&#35299;&#20915;&#38382;&#39064;&#65292;&#21478;&#19968;&#31181;&#26041;&#27861;&#26159;&#20351;&#29992;&#23427;&#20174;&#38382;&#39064;&#25991;&#26412;&#20013;&#25552;&#21462;&#20107;&#23454;&#65292;&#28982;&#21518;&#20351;&#29992;&#23450;&#29702;&#35777;&#26126;&#22120;&#36827;&#34892;&#35299;&#20915;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#21644;GPT4&#23545;&#19968;&#31995;&#21015;&#36923;&#36753;&#21333;&#35789;&#35868;&#39064;&#36827;&#34892;&#27604;&#36739;&#65292;&#24182;&#24471;&#20986;&#20102;&#21518;&#32773;&#26159;&#27491;&#30830;&#30340;&#26041;&#27861;&#30340;&#32467;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
For a natural language problem that requires some non-trivial reasoning to solve, there are at least two ways to do it using a large language model (LLM). One is to ask it to solve it directly. The other is to use it to extract the facts from the problem text and then use a theorem prover to solve it. In this note, we compare the two methods using ChatGPT and GPT4 on a series of logic word puzzles, and conclude that the latter is the right approach.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#29992;BNN&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2304.01762</link><description>&lt;p&gt;
&#23558;&#26410;&#26631;&#35760;&#25968;&#25454;&#32435;&#20837;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#20013;
&lt;/p&gt;
&lt;p&gt;
Incorporating Unlabelled Data into Bayesian Neural Networks. (arXiv:2304.01762v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01762
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#23398;&#20064;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#36890;&#36807;&#35813;&#26694;&#26550;&#25552;&#20986;&#20102;&#19968;&#31181;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#30340;&#23454;&#29992;BNN&#31639;&#27861;&#12290;&#26368;&#21518;&#65292;&#35813;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#23637;&#29616;&#20986;&#20102;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#30340;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23545;&#36125;&#21494;&#26031;&#31070;&#32463;&#32593;&#32476;&#65288;BNNs&#65289;&#20013;&#20808;&#39564;&#20998;&#24067;&#36827;&#34892;&#23398;&#20064;&#30340;&#23545;&#27604;&#26694;&#26550;&#65292;&#21033;&#29992;&#26410;&#26631;&#35760;&#25968;&#25454;&#26469;&#20248;&#21270;&#12290;&#22522;&#20110;&#35813;&#26694;&#26550;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#29992;&#30340;BNN&#31639;&#27861;&#65292;&#21516;&#26102;&#20855;&#22791;&#33258;&#30417;&#30563;&#23398;&#20064;&#30340;&#26631;&#31614;&#25928;&#29575;&#21644;&#36125;&#21494;&#26031;&#26041;&#27861;&#20013;&#30340;&#26681;&#25454;&#21407;&#21017;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#21322;&#30417;&#30563;&#21644;&#20302;&#39044;&#31639;&#20027;&#21160;&#23398;&#20064;&#38382;&#39064;&#20013;&#30340;&#25968;&#25454;&#39640;&#25928;&#23398;&#20064;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;
We develop a contrastive framework for learning better prior distributions for Bayesian Neural Networks (BNNs) using unlabelled data. With this framework, we propose a practical BNN algorithm that offers the label-efficiency of self-supervised learning and the principled uncertainty estimates of Bayesian methods. Finally, we demonstrate the advantages of our approach for data-efficient learning in semi-supervised and low-budget active learning problems.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#29305;&#24449;&#23545;&#40784;(CFA)&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#20197;&#23454;&#29616;&#40065;&#26834;&#35782;&#21035;&#12290;&#26412;&#25991;&#26041;&#27861;&#25552;&#20986;&#20102;&#28151;&#26434;&#26434;&#27874;&#21464;&#20307;&#29983;&#25104;&#31574;&#30053;&#21644;&#37197;&#22791;&#36890;&#36947;&#21152;&#26435;&#22343;&#26041;&#35823;&#24046;(CWMSE)&#25439;&#22833;&#30340;&#26032;&#25512;&#26029;&#20998;&#25903;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MSTAR&#25968;&#25454;&#38598;&#30340;&#31227;&#21160;&#21644;&#38745;&#27490;&#22320;&#38754;&#36710;&#36742;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01747</link><description>&lt;p&gt;
&#36890;&#36807;&#23545;&#27604;&#29305;&#24449;&#23545;&#40784;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#20197;&#23454;&#29616;&#25239;&#28151;&#26434;SAR&#30446;&#26631;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Learning Invariant Representation via Contrastive Feature Alignment for Clutter Robust SAR Target Recognition. (arXiv:2304.01747v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01747
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#29305;&#24449;&#23545;&#40784;(CFA)&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#36890;&#36807;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#20197;&#23454;&#29616;&#40065;&#26834;&#35782;&#21035;&#12290;&#26412;&#25991;&#26041;&#27861;&#25552;&#20986;&#20102;&#28151;&#26434;&#26434;&#27874;&#21464;&#20307;&#29983;&#25104;&#31574;&#30053;&#21644;&#37197;&#22791;&#36890;&#36947;&#21152;&#26435;&#22343;&#26041;&#35823;&#24046;(CWMSE)&#25439;&#22833;&#30340;&#26032;&#25512;&#26029;&#20998;&#25903;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MSTAR&#25968;&#25454;&#38598;&#30340;&#31227;&#21160;&#21644;&#38745;&#27490;&#22320;&#38754;&#36710;&#36742;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;(DNNs)&#37322;&#25918;&#20102;&#21512;&#25104;&#23380;&#24452;&#38647;&#36798;&#33258;&#21160;&#30446;&#26631;&#35782;&#21035;(SAR ATR)&#20351;&#20854;&#25670;&#33073;&#20102;&#22522;&#20110;&#19987;&#19994;&#30693;&#35782;&#30340;&#29305;&#24449;&#35774;&#35745;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#20248;&#20110;&#20256;&#32479;&#35299;&#20915;&#26041;&#26696;&#30340;&#20248;&#21183;&#12290;&#22312;&#24378;&#32972;&#26223;&#30456;&#20851;&#24615;&#30340;&#24418;&#29366;&#26041;&#38754;&#65292;&#24050;&#32463;&#26174;&#31034;&#20102;&#22320;&#38754;&#36710;&#36742;&#22522;&#20934;&#30340;&#29420;&#29305;&#32570;&#38519;&#65292;&#20854;&#32467;&#26524;&#22312;DNNs&#36807;&#25311;&#21512;&#26434;&#27874;&#19988;&#23545;&#38476;&#29983;&#29615;&#22659;&#19981;&#20855;&#26377;&#31283;&#20581;&#24615;&#12290;&#28982;&#32780;&#65292;&#22266;&#23450;&#32972;&#26223;&#27169;&#22411;&#35757;&#32451;&#21644;&#21487;&#21464;&#32972;&#26223;&#24212;&#29992;&#20043;&#38388;&#30340;&#24046;&#36317;&#20173;&#26410;&#24471;&#21040;&#20805;&#20998;&#25506;&#32034;&#12290;&#21463;&#23545;&#27604;&#23398;&#20064;&#30340;&#21551;&#21457;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#23545;&#27604;&#29305;&#24449;&#23545;&#40784;(CFA)&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#26088;&#22312;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#20197;&#23454;&#29616;&#40065;&#26834;&#35782;&#21035;&#12290;&#26412;&#25991;&#26041;&#27861;&#25552;&#20986;&#20102;&#28151;&#26434;&#26434;&#27874;&#21464;&#20307;&#29983;&#25104;&#31574;&#30053;&#21644;&#37197;&#22791;&#36890;&#36947;&#21152;&#26435;&#22343;&#26041;&#35823;&#24046;(CWMSE)&#25439;&#22833;&#30340;&#26032;&#25512;&#26029;&#20998;&#25903;&#65292;&#29992;&#20110;&#23398;&#20064;&#19981;&#21464;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#29983;&#25104;&#31574;&#30053;&#32463;&#36807;&#31934;&#24515;&#35774;&#35745;&#65292;&#20197;&#26356;&#22909;&#22320;&#21560;&#24341;&#20998;&#31867;&#22120;&#30340;&#26434;&#27874;&#25935;&#24863;&#24615;&#24182;&#25193;&#22823;&#35757;&#32451;&#25968;&#25454;&#30340;&#22810;&#26679;&#24615;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;CWMSE&#25439;&#22833;&#21487;&#24573;&#30053;&#21464;&#37327;&#32972;&#26223;&#20013;&#26080;&#20851;&#26434;&#27874;&#30340;&#24433;&#21709;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;MSTAR&#25968;&#25454;&#38598;&#30340;&#31227;&#21160;&#21644;&#38745;&#27490;&#22320;&#38754;&#36710;&#36742;&#22522;&#20934;&#19978;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The deep neural networks (DNNs) have freed the synthetic aperture radar automatic target recognition (SAR ATR) from expertise-based feature designing and demonstrated superiority over conventional solutions. There has been shown the unique deficiency of ground vehicle benchmarks in shapes of strong background correlation results in DNNs overfitting the clutter and being non-robust to unfamiliar surroundings. However, the gap between fixed background model training and varying background application remains underexplored. Inspired by contrastive learning, this letter proposes a solution called Contrastive Feature Alignment (CFA) aiming to learn invariant representation for robust recognition. The proposed method contributes a mixed clutter variants generation strategy and a new inference branch equipped with channel-weighted mean square error (CWMSE) loss for invariant representation learning. In specific, the generation strategy is delicately designed to better attract clutter-sensitiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#20219;&#21153;--&#24320;&#25918;&#35789;&#27719;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#65292;&#24182;&#25910;&#38598;&#20102;&#22823;&#35268;&#27169;&#30340;LV-VIS&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;MindVLT&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#36817;&#23454;&#26102;&#30340;&#36895;&#24230;&#23454;&#29616;&#24320;&#25918;&#38598;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;</title><link>http://arxiv.org/abs/2304.01715</link><description>&lt;p&gt;
&#24320;&#25918;&#35789;&#27719;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#30340;&#25506;&#32034;
&lt;/p&gt;
&lt;p&gt;
Towards Open-Vocabulary Video Instance Segmentation. (arXiv:2304.01715v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01715
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26032;&#20219;&#21153;--&#24320;&#25918;&#35789;&#27719;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#65292;&#24182;&#25910;&#38598;&#20102;&#22823;&#35268;&#27169;&#30340;LV-VIS&#25968;&#25454;&#38598;&#65292;&#21516;&#26102;&#25552;&#20986;&#20102;&#39640;&#25928;&#30340;MindVLT&#26041;&#27861;&#65292;&#33021;&#22815;&#20197;&#36817;&#23454;&#26102;&#30340;&#36895;&#24230;&#23454;&#29616;&#24320;&#25918;&#38598;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#65292;&#30456;&#27604;&#29616;&#26377;&#26041;&#27861;&#26377;&#26174;&#33879;&#30340;&#25552;&#21319;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#65288;VIS&#65289;&#26088;&#22312;&#20174;&#19968;&#32452;&#23553;&#38381;&#30340;&#35757;&#32451;&#31867;&#21035;&#20013;&#23545;&#35270;&#39057;&#20013;&#30340;&#23545;&#35937;&#36827;&#34892;&#20998;&#21106;&#21644;&#20998;&#31867;&#65292;&#32570;&#20047;&#22788;&#29702;&#30495;&#23454;&#19990;&#30028;&#20013;&#26032;&#31867;&#21035;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19977;&#20010;&#26041;&#26696;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#24320;&#25918;&#35789;&#27719;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#30340;&#26032;&#20219;&#21153;&#65292;&#26088;&#22312;&#21516;&#26102;&#20174;&#24320;&#25918;&#38598;&#31867;&#21035;&#20013;&#23545;&#35270;&#39057;&#20013;&#30340;&#23545;&#35937;&#36827;&#34892;&#20998;&#21106;&#12289;&#36319;&#36394;&#21644;&#20998;&#31867;&#65292;&#21253;&#25324;&#35757;&#32451;&#26399;&#38388;&#26410;&#35265;&#36807;&#30340;&#26032;&#31867;&#21035;&#12290;&#20854;&#27425;&#65292;&#20026;&#20102;&#35780;&#27979;&#24320;&#25918;&#35789;&#27719;&#23454;&#20363;&#20998;&#21106;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#21253;&#21547;1,212&#20010;&#19981;&#21516;&#31867;&#21035;&#30340;&#22823;&#35268;&#27169;&#35789;&#27719;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#25968;&#25454;&#38598;&#65288;LV-VIS&#65289;&#65292;&#26174;&#33879;&#36229;&#20986;&#20102;&#29616;&#26377;&#25968;&#25454;&#38598;&#30340;&#31867;&#21035;&#35268;&#27169;&#19968;&#20010;&#25968;&#37327;&#32423;&#20197;&#19978;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#35760;&#24518;&#39537;&#21160;&#35270;&#35273;&#35821;&#35328;&#21464;&#25442;&#22120;MindVLT&#65292;&#20197;&#23454;&#29616;&#36817;&#23454;&#26102;&#31471;&#21040;&#31471;&#30340;&#24320;&#25918;&#35789;&#27719;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;MindVLT&#22312;&#23553;&#38381;&#38598;&#21644;&#24320;&#25918;&#38598;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;&#20219;&#21153;&#19978;&#26174;&#33879;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Video Instance Segmentation(VIS) aims at segmenting and categorizing objects in videos from a closed set of training categories, lacking the generalization ability to handle novel categories in real-world videos. To address this limitation, we make the following three contributions. First, we introduce the novel task of Open-Vocabulary Video Instance Segmentation, which aims to simultaneously segment, track, and classify objects in videos from open-set categories, including novel categories unseen during training. Second, to benchmark Open-Vocabulary VIS, we collect a Large-Vocabulary Video Instance Segmentation dataset(LV-VIS), that contains well-annotated objects from 1,212 diverse categories, significantly surpassing the category size of existing datasets by more than one order of magnitude. Third, we propose an efficient Memory-Induced Vision-Language Transformer, MindVLT, to first achieve Open-Vocabulary VIS in an end-to-end manner with near real-time inference speed. Extensive ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#35875;&#35328;&#26816;&#27979;&#24182;&#20998;&#26512;&#20102;&#26032;&#20896;&#30123;&#24773;&#30456;&#20851;&#25512;&#25991;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36890;&#36807;&#27604;&#36739;&#35821;&#35328;&#32467;&#26500;&#21644;&#20256;&#25773;&#36335;&#24452;&#31561;&#29305;&#24449;&#26469;&#21306;&#20998;&#35875;&#35328;&#21644;&#20107;&#23454;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#32467;&#26500;&#26159;&#26356;&#22909;&#30340;&#21306;&#20998;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2304.01712</link><description>&lt;p&gt;
&#25512;&#29305;&#35875;&#35328;&#26816;&#27979;&#19982;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Rumour Detection and Analysis on Twitter. (arXiv:2304.01712v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01712
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#31038;&#20132;&#23186;&#20307;&#24179;&#21488;&#19978;&#30340;&#35875;&#35328;&#26816;&#27979;&#24182;&#20998;&#26512;&#20102;&#26032;&#20896;&#30123;&#24773;&#30456;&#20851;&#25512;&#25991;&#30340;&#25968;&#25454;&#12290;&#30740;&#31350;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#36890;&#36807;&#27604;&#36739;&#35821;&#35328;&#32467;&#26500;&#21644;&#20256;&#25773;&#36335;&#24452;&#31561;&#29305;&#24449;&#26469;&#21306;&#20998;&#35875;&#35328;&#21644;&#20107;&#23454;&#65292;&#24182;&#21457;&#29616;&#35821;&#35328;&#32467;&#26500;&#26159;&#26356;&#22909;&#30340;&#21306;&#20998;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#20204;&#36234;&#26469;&#36234;&#20381;&#36182;&#31038;&#20132;&#23186;&#20307;&#33719;&#21462;&#26032;&#38395;&#21644;&#20449;&#24687;&#65292;&#19968;&#20123;&#31038;&#20132;&#23186;&#20307;&#29992;&#25143;&#21457;&#24067;&#32570;&#20047;&#35777;&#23454;&#30340;&#20449;&#24687;&#20197;&#33719;&#21462;&#20851;&#27880;&#65292;&#36825;&#31181;&#20449;&#24687;&#34987;&#31216;&#20026;&#35875;&#35328;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#31995;&#32479;&#26469;&#39044;&#27979;&#35875;&#35328;&#12290;&#20854;&#20013;&#65292;&#26368;&#20339;&#27169;&#22411;&#34987;&#24212;&#29992;&#20110;&#26032;&#20896;&#30123;&#24773;&#30456;&#20851;&#25512;&#25991;&#30340;&#25506;&#32034;&#24615;&#25968;&#25454;&#20998;&#26512;&#12290;&#26412;&#30740;&#31350;&#30340;&#36129;&#29486;&#26377;&#20004;&#20010;&#26041;&#38754;&#65306;&#65288;1&#65289;&#36890;&#36807;&#20004;&#20010;&#32500;&#24230;&#65306;&#35821;&#35328;&#32467;&#26500;&#21644;&#20256;&#25773;&#36335;&#24452;&#65292;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#27169;&#22411;&#26469;&#27604;&#36739;&#35875;&#35328;&#21644;&#20107;&#23454;&#12290;&#65288;2&#65289;&#36890;&#36807;&#20998;&#26512;&#35875;&#35328;&#19982;&#20107;&#23454;&#22312;&#35789;&#27719;&#20351;&#29992;&#21644;&#26263;&#31034;&#30340;&#24773;&#24863;&#26041;&#38754;&#30340;&#19981;&#21516;&#65292;&#25506;&#31350;&#35875;&#35328;&#19982;&#20107;&#23454;&#20043;&#38388;&#30340;&#19981;&#21516;&#12290;&#35813;&#30740;&#31350;&#34920;&#26126;&#65292;&#30456;&#27604;&#20256;&#25773;&#36335;&#24452;&#65292;&#35821;&#35328;&#32467;&#26500;&#26159;&#26356;&#22909;&#30340;&#29305;&#24449;&#26469;&#21306;&#20998;&#35875;&#35328;&#21644;&#20107;&#23454;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years people have become increasingly reliant on social media to read news and get information, and some social media users post unsubstantiated information to gain attention. Such information is known as rumours. Nowadays, rumour detection is receiving a growing amount of attention because of the pandemic of the New Coronavirus, which has led to a large number of rumours being spread. In this paper, a Natural Language Processing (NLP) system is built to predict rumours. The best model is applied to the COVID-19 tweets to conduct exploratory data analysis. The contribution of this study is twofold: (1) to compare rumours and facts using state-of-the-art natural language processing models in two dimensions: language structure and propagation route. (2) An analysis of how rumours differ from facts in terms of their lexical use and the emotions they imply. This study shows that linguistic structure is a better feature to distinguish rumours from facts compared to the propagation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#25490;&#24207;&#26041;&#27861;&#26469;&#35299;&#20915;&#21333;&#27169;&#31946;&#22270;&#20687;&#21040;&#35270;&#39057;&#24207;&#21015;&#30340;&#37325;&#24314;&#38382;&#39064;&#65292;&#26174;&#24335;&#22320;&#20998;&#37197;&#27599;&#20010;&#24207;&#21015;&#30340;&#39034;&#24207;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#21435;&#27169;&#31946;&#27169;&#22411;&#30340;&#35757;&#32451;&#36136;&#37327;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.01686</link><description>&lt;p&gt;
&#26080;&#30417;&#30563;&#25490;&#24207;&#30340;&#21333;&#27169;&#31946;&#22270;&#20687;&#21040;&#35270;&#39057;&#24207;&#21015;&#26041;&#27861;&#8212;&#8212;HyperCUT
&lt;/p&gt;
&lt;p&gt;
HyperCUT: Video Sequence from a Single Blurry Image using Unsupervised Ordering. (arXiv:2304.01686v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01686
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#25490;&#24207;&#26041;&#27861;&#26469;&#35299;&#20915;&#21333;&#27169;&#31946;&#22270;&#20687;&#21040;&#35270;&#39057;&#24207;&#21015;&#30340;&#37325;&#24314;&#38382;&#39064;&#65292;&#26174;&#24335;&#22320;&#20998;&#37197;&#27599;&#20010;&#24207;&#21015;&#30340;&#39034;&#24207;&#65292;&#26377;&#25928;&#25552;&#39640;&#20102;&#21435;&#27169;&#31946;&#27169;&#22411;&#30340;&#35757;&#32451;&#36136;&#37327;&#65292;&#24182;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#33719;&#24471;&#20102;&#26368;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22270;&#20687;&#21040;&#35270;&#39057;&#21435;&#27169;&#31946;&#30340;&#27169;&#22411;&#35757;&#32451;&#20013;&#30340;&#19968;&#20010;&#20851;&#38190;&#38382;&#39064;&#65306;&#30001;&#20110;&#21069;&#21521;&#21644;&#21518;&#21521;&#24207;&#21015;&#37117;&#21487;&#20197;&#20316;&#20026;&#27169;&#31946;&#22270;&#20687;&#23545;&#24212;&#30340;&#28165;&#26224;&#22270;&#20687;&#24207;&#21015;&#65292;&#22240;&#27492;&#24207;&#21015;&#39034;&#24207;&#30340;&#27169;&#31946;&#24615;&#26497;&#22823;&#22320;&#24178;&#25200;&#20102;&#27169;&#22411;&#30340;&#35757;&#32451;&#25928;&#26524;&#12290;&#20026;&#35299;&#20915;&#36825;&#19968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#33258;&#30417;&#30563;&#25490;&#24207;&#26041;&#27861;&#65292;&#23558;&#27599;&#20010;&#35270;&#39057;&#24207;&#21015;&#26144;&#23556;&#21040;&#19968;&#20010;&#39640;&#32500;&#28508;&#22312;&#31354;&#38388;&#20013;&#30340;&#21521;&#37327;&#65292;&#24182;&#21033;&#29992;&#36229;&#24179;&#38754;&#23558;&#20854;&#19982;&#20854;&#21453;&#21521;&#24207;&#21015;&#21306;&#20998;&#24320;&#26469;&#65292;&#26174;&#24335;&#22320;&#20998;&#37197;&#20102;&#27599;&#20010;&#24207;&#21015;&#30340;&#39034;&#24207;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#22312;&#21512;&#25104;&#21644;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#19982;&#29616;&#26377;&#30340;&#22270;&#20687;&#21040;&#35270;&#39057;&#21435;&#27169;&#31946;&#26041;&#27861;&#30456;&#27604;&#21462;&#24471;&#20102;&#26368;&#20248;&#31168;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We consider the challenging task of training models for image-to-video deblurring, which aims to recover a sequence of sharp images corresponding to a given blurry image input. A critical issue disturbing the training of an image-to-video model is the ambiguity of the frame ordering since both the forward and backward sequences are plausible solutions. This paper proposes an effective self-supervised ordering scheme that allows training high-quality image-to-video deblurring models. Unlike previous methods that rely on order-invariant losses, we assign an explicit order for each video sequence, thus avoiding the order-ambiguity issue. Specifically, we map each video sequence to a vector in a latent high-dimensional space so that there exists a hyperplane such that for every video sequence, the vectors extracted from it and its reversed sequence are on different sides of the hyperplane. The side of the vectors will be used to define the order of the corresponding sequence. Last but not 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20934;&#30830;&#30340;&#21160;&#20316;&#27880;&#37322;&#26041;&#27861;Motion-R3&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25968;&#25454;&#20013;&#24515;&#21746;&#23398;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#20316;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#34920;&#24449;&#24615;&#36827;&#34892;&#25490;&#21517;&#65292;&#21487;&#24555;&#36895;&#24212;&#23545;&#38656;&#27714;&#21464;&#21270;&#24182;&#23454;&#29616;&#25935;&#25463;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2304.01672</link><description>&lt;p&gt;
Motion-R3:&#22522;&#20110;&#34920;&#24449;&#30456;&#20851;&#24615;&#25490;&#21517;&#30340;&#24555;&#36895;&#20934;&#30830;&#30340;&#21160;&#20316;&#27880;&#37322;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Motion-R3: Fast and Accurate Motion Annotation via Representation-based Representativeness Ranking. (arXiv:2304.01672v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01672
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#24555;&#36895;&#20934;&#30830;&#30340;&#21160;&#20316;&#27880;&#37322;&#26041;&#27861;Motion-R3&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#25968;&#25454;&#20013;&#24515;&#21746;&#23398;&#65292;&#36890;&#36807;&#23398;&#20064;&#21160;&#20316;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#34920;&#24449;&#24615;&#36827;&#34892;&#25490;&#21517;&#65292;&#21487;&#24555;&#36895;&#24212;&#23545;&#38656;&#27714;&#21464;&#21270;&#24182;&#23454;&#29616;&#25935;&#25463;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25968;&#25454;&#20013;&#24515;&#21746;&#23398;&#30340;&#26032;&#22411;&#21160;&#20316;&#27880;&#37322;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#22522;&#20110;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#21160;&#20316;&#25968;&#25454;&#30340;&#20869;&#22312;&#30456;&#20851;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;&#34920;&#24449;&#30456;&#20851;&#24615;&#25490;&#21517;R3&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#26681;&#25454;&#21160;&#20316;&#22312;&#23398;&#20064;&#30340;&#21160;&#20316;&#34920;&#31034;&#31354;&#38388;&#20013;&#30340;&#34920;&#24449;&#24615;&#23545;&#20854;&#22312;&#32473;&#23450;&#25968;&#25454;&#38598;&#20013;&#36827;&#34892;&#25490;&#21517;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21452;&#32423;&#21160;&#20316;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#26469;&#20197;&#26356;&#21152;&#20449;&#24687;&#21270;&#30340;&#26041;&#24335;&#23398;&#20064;&#21160;&#20316;&#34920;&#31034;&#31354;&#38388;&#12290;&#30001;&#20110;&#20854;&#39640;&#25928;&#24615;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#26356;&#24555;&#22320;&#24212;&#23545;&#39057;&#32321;&#30340;&#38656;&#27714;&#21464;&#21270;&#65292;&#24182;&#23454;&#29616;&#21160;&#20316;&#27880;&#37322;&#27169;&#22411;&#30340;&#25935;&#25463;&#24320;&#21457;&#12290;&#23545;HDM05&#25968;&#25454;&#38598;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#29616;&#26377;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we follow a data-centric philosophy and propose a novel motion annotation method based on the inherent representativeness of motion data in a given dataset. Specifically, we propose a Representation-based Representativeness Ranking R3 method that ranks all motion data in a given dataset according to their representativeness in a learned motion representation space. We further propose a novel dual-level motion constrastive learning method to learn the motion representation space in a more informative way. Thanks to its high efficiency, our method is particularly responsive to frequent requirements change and enables agile development of motion annotation models. Experimental results on the HDM05 dataset against state-of-the-art methods demonstrate the superiority of our method.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#19968;&#33268;&#24615;&#26412;&#20307;&#30340;&#23481;&#38169;&#25512;&#29702;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35821;&#20041;&#21521;&#37327;&#35745;&#31639;&#20844;&#29702;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#23450;&#20041;&#20102;&#36873;&#25321;&#26368;&#22823;&#19968;&#33268;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25512;&#29702;&#20851;&#31995;&#30340;&#21512;&#29702;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01664</link><description>&lt;p&gt;
&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#30340;&#22788;&#29702;&#20855;&#26377;&#19981;&#19968;&#33268;&#24615;&#26412;&#20307;&#30340;&#23481;&#38169;&#25512;&#29702;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
An Embedding-based Approach to Inconsistency-tolerant Reasoning with Inconsistent Ontologies. (arXiv:2304.01664v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01664
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#30340;&#26041;&#27861;&#26469;&#22788;&#29702;&#20855;&#26377;&#19981;&#19968;&#33268;&#24615;&#26412;&#20307;&#30340;&#23481;&#38169;&#25512;&#29702;&#38382;&#39064;&#65292;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#35821;&#20041;&#21521;&#37327;&#35745;&#31639;&#20844;&#29702;&#20043;&#38388;&#30340;&#20851;&#32852;&#65292;&#23450;&#20041;&#20102;&#36873;&#25321;&#26368;&#22823;&#19968;&#33268;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#20102;&#25512;&#29702;&#20851;&#31995;&#30340;&#21512;&#29702;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22788;&#29702;&#19981;&#19968;&#33268;&#24615;&#26159;&#30693;&#35782;&#31649;&#29702;&#20013;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#22312;&#26412;&#20307;&#24037;&#31243;&#20013;&#65292;&#26412;&#20307;&#26500;&#24314;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#36923;&#36753;&#19981;&#19968;&#33268;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25551;&#36848;&#36923;&#36753;&#30340;&#23884;&#20837;&#26041;&#27861;&#65292;&#36890;&#36807;&#23558;&#20844;&#29702;&#36716;&#21270;&#20026;&#20998;&#24067;&#24335;&#35821;&#20041;&#21521;&#37327;&#35745;&#31639;&#20844;&#29702;&#20043;&#38388;&#30340;&#35821;&#20041;&#20851;&#32852;&#65292;&#36827;&#32780;&#23450;&#20041;&#20102;&#19968;&#31181;&#22522;&#20110;&#23884;&#20837;&#30340;&#36873;&#25321;&#26368;&#22823;&#19968;&#33268;&#23376;&#38598;&#30340;&#26041;&#27861;&#65292;&#24182;&#23558;&#20854;&#29992;&#20110;&#23450;&#20041;&#23481;&#38169;&#25512;&#29702;&#20851;&#31995;&#12290;&#36890;&#36807;&#32771;&#34385;&#26576;&#20123;&#36923;&#36753;&#23646;&#24615;&#65292;&#35777;&#26126;&#20102;&#25512;&#29702;&#20851;&#31995;&#30340;&#21512;&#29702;&#24615;&#12290;&#26368;&#21518;&#65292;&#22312;&#20960;&#20010;&#25968;&#25454;&#38598;&#19978;&#39564;&#35777;&#20102;&#23454;&#39564;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inconsistency handling is an important issue in knowledge management. Especially in ontology engineering, logical inconsistencies may occur during ontology construction. A natural way to reason with an inconsistent ontology is to utilize the maximal consistent subsets of the ontology. However, previous studies on selecting maximum consistent subsets have rarely considered the semantics of the axioms, which may result in irrational inference. In this paper, we propose a novel approach to reasoning with inconsistent ontologies in description logics based on the embeddings of axioms. We first give a method for turning axioms into distributed semantic vectors to compute the semantic connections between the axioms. We then define an embedding-based method for selecting the maximum consistent subsets and use it to define an inconsistency-tolerant inference relation. We show the rationality of our inference relation by considering some logical properties. Finally, we conduct experiments on se
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#37492;&#21035;&#24335;&#27807;&#36890;&#30446;&#26631;&#65292;&#24494;&#35843;&#24050;&#26377;&#30340;&#31070;&#32463;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#22312;&#20445;&#25345;&#35821;&#35328;&#30340;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#20449;&#24687;&#25552;&#21462;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2304.01662</link><description>&lt;p&gt;
&#36890;&#36807;&#37492;&#21035;&#24494;&#35843;&#36827;&#34892;&#36328;&#39046;&#22495;&#22270;&#20687;&#23383;&#24149;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Cross-Domain Image Captioning with Discriminative Finetuning. (arXiv:2304.01662v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01662
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#33258;&#30417;&#30563;&#37492;&#21035;&#24335;&#27807;&#36890;&#30446;&#26631;&#65292;&#24494;&#35843;&#24050;&#26377;&#30340;&#31070;&#32463;&#23383;&#24149;&#29983;&#25104;&#22120;&#65292;&#22312;&#20445;&#25345;&#35821;&#35328;&#30340;&#35270;&#35273;&#25551;&#36848;&#24615;&#30340;&#21516;&#26102;&#65292;&#25552;&#39640;&#20102;&#23545;&#22270;&#20687;&#20869;&#23481;&#30340;&#20449;&#24687;&#25552;&#21462;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#23383;&#24149;&#29983;&#25104;&#22120;&#36890;&#24120;&#34987;&#35757;&#32451;&#25104;&#27169;&#20223;&#20154;&#31867;&#29983;&#25104;&#30340;&#21442;&#32771;&#25991;&#26412;&#32780;&#27809;&#26377;&#38024;&#23545;&#29305;&#23450;&#27807;&#36890;&#30446;&#26631;&#36827;&#34892;&#20248;&#21270;&#65292;&#23548;&#33268;&#20135;&#29983;&#27169;&#31946;&#30340;&#23383;&#24149;&#31561;&#38382;&#39064;&#12290;&#26412;&#25991;&#34920;&#26126;&#65292;&#20351;&#29992;&#33258;&#30417;&#30563;&#37492;&#21035;&#24335;&#27807;&#36890;&#30446;&#26631;&#24494;&#35843;&#31070;&#32463;&#23383;&#24149;&#29983;&#25104;&#22120;&#33021;&#22815;&#24110;&#21161;&#24674;&#22797;&#24179;&#23454;&#12289;&#35270;&#35273;&#25551;&#36848;&#24615;&#26356;&#24378;&#12289;&#20851;&#20110;&#22270;&#20687;&#20869;&#23481;&#26356;&#20016;&#23500;&#30340;&#35821;&#35328;&#12290;&#32473;&#23450;&#19968;&#20010;&#30446;&#26631;&#22270;&#20687;&#65292;&#31995;&#32479;&#24517;&#39035;&#23398;&#20064;&#20135;&#29983;&#19968;&#27573;&#25551;&#36848;&#65292;&#20174;&#32780;&#20351;&#19968;&#20010;&#24320;&#31665;&#21363;&#29992;&#30340;&#25991;&#26412;&#26465;&#20214;&#22270;&#20687;&#26816;&#32034;&#22120;&#33021;&#22815;&#22312;&#19968;&#32452;&#20505;&#36873;&#22270;&#20687;&#20013;&#35782;&#21035;&#35813;&#22270;&#20687;&#12290;&#25105;&#20204;&#20351;&#29992;&#20102;&#27969;&#34892;&#30340;ClipCap&#23383;&#24149;&#29983;&#25104;&#22120;&#36827;&#34892;&#23454;&#39564;&#65292;&#24182;&#20351;&#29992;BLIP&#22797;&#21046;&#20102;&#20027;&#35201;&#32467;&#26524;&#12290;&#23601;&#19982;&#20154;&#31867;&#25551;&#36848;&#30340;&#30456;&#20284;&#24230;&#32780;&#35328;&#65292;&#22312;&#30456;&#21516;&#30340;&#23383;&#24149;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#21644;&#27979;&#35797;&#38750;&#24494;&#35843;&#27169;&#22411;&#29983;&#25104;&#30340;&#23383;&#24149;&#30053;&#20248;&#20110;&#37492;&#21035;&#24494;&#35843;&#30340;&#23383;&#24149;&#12290;&#28982;&#32780;&#65292;&#24403;&#27169;&#22411;&#26410;&#32463;&#36827;&#19968;&#27493;&#35757;&#32451;&#26102;&#65292;&#37492;&#21035;&#24494;&#35843;&#30340;&#23383;&#24149;&#27604;&#20197;&#21069;&#30340;&#26041;&#27861;&#29983;&#25104;&#30340;&#21442;&#32771;&#25991;&#26412;&#26356;&#33021;&#25552;&#20379;&#20851;&#20110;&#22270;&#20687;&#35270;&#35273;&#20869;&#23481;&#26356;&#20016;&#23500;&#30340;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural captioners are typically trained to mimic human-generated references without optimizing for any specific communication goal, leading to problems such as the generation of vague captions. In this paper, we show that fine-tuning an out-of-the-box neural captioner with a self-supervised discriminative communication objective helps to recover a plain, visually descriptive language that is more informative about image contents. Given a target image, the system must learn to produce a description that enables an out-of-the-box text-conditioned image retriever to identify such image among a set of candidates. We experiment with the popular ClipCap captioner, also replicating the main results with BLIP. In terms of similarity to ground-truth human descriptions, the captions emerging from discriminative finetuning lag slightly behind those generated by the non-finetuned model, when the latter is trained and tested on the same caption dataset. However, when the model is used without furth
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SWIPE&#30340;&#22810;&#32500;&#24863;&#30693;&#22120;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#25972;&#20010;&#25991;&#26412;&#30340;&#26631;&#31614;&#24182;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#24863;&#30693;&#27573;&#33853;&#30340;&#26631;&#31614;&#12290;SWIPE&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#22120;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#32534;&#30721;&#22120;&#65292;&#22312;&#20998;&#31867;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;</title><link>http://arxiv.org/abs/2304.01638</link><description>&lt;p&gt;
&#39640;&#25928;&#21487;&#35299;&#37322;&#30340;&#38271;&#25991;&#26412;&#20998;&#31867;&#30340;&#22810;&#32500;&#24863;&#30693;&#22120;
&lt;/p&gt;
&lt;p&gt;
Multidimensional Perceptron for Efficient and Explainable Long Text Classification. (arXiv:2304.01638v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01638
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;SWIPE&#30340;&#22810;&#32500;&#24863;&#30693;&#22120;&#27169;&#22411;&#65292;&#26377;&#25928;&#22320;&#23398;&#20064;&#25972;&#20010;&#25991;&#26412;&#30340;&#26631;&#31614;&#24182;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#24863;&#30693;&#27573;&#33853;&#30340;&#26631;&#31614;&#12290;SWIPE&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#22120;&#65292;&#25903;&#25345;&#19981;&#21516;&#30340;&#32534;&#30721;&#22120;&#65292;&#22312;&#20998;&#31867;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;Transformer&#21644;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#19981;&#21487;&#36991;&#20813;&#30340;&#25104;&#26412;&#21644;&#22797;&#26434;&#24615;&#65292;&#22312;&#38271;&#25991;&#26412;&#20998;&#31867;&#20013;&#24341;&#36215;&#20102;&#25928;&#29575;&#38382;&#39064;&#12290;&#21516;&#26102;&#65292;&#22312;&#39640;&#24230;&#25935;&#24863;&#30340;&#39046;&#22495;&#65292;&#20363;&#22914;&#21307;&#30103;&#20445;&#20581;&#21644;&#27861;&#24459;&#38271;&#25991;&#26412;&#25366;&#25496;&#20013;&#65292;&#28508;&#22312;&#30340;&#27169;&#22411;&#19981;&#20449;&#20219;&#65292;&#34429;&#28982;&#34987;&#20302;&#20272;&#21644;&#26410;&#34987;&#25506;&#32034;&#65292;&#20294;&#21487;&#33021;&#23381;&#32946;&#37325;&#35201;&#30340;&#24551;&#34385;&#12290;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#23558;&#38271;&#25991;&#26412;&#20998;&#21106;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#23545;&#27599;&#20010;&#29255;&#27573;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#20351;&#29992;&#27880;&#24847;&#21147;&#25110;RNN&#26469;&#33719;&#21462;&#38271;&#25991;&#26412;&#34920;&#31034;&#20197;&#36827;&#34892;&#20998;&#31867;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#21363;Segment-aWare multIdimensional PErceptron&#65288;SWIPE&#65289;&#65292;&#20197;&#21462;&#20195;&#19978;&#36848;&#26694;&#26550;&#20013;&#30340;&#27880;&#24847;&#21147;/RNN&#12290;&#19982;&#20043;&#21069;&#30340;&#24037;&#20316;&#19981;&#21516;&#65292;SWIPE&#21487;&#20197;&#36890;&#36807;&#26377;&#30417;&#30563;&#30340;&#35757;&#32451;&#26377;&#25928;&#22320;&#23398;&#20064;&#25972;&#20010;&#25991;&#26412;&#30340;&#26631;&#31614;&#65292;&#21516;&#26102;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#24863;&#30693;&#27573;&#33853;&#30340;&#26631;&#31614;&#24182;&#20272;&#35745;&#23427;&#20204;&#23545;&#38271;&#25991;&#26412;&#26631;&#31614;&#30340;&#36129;&#29486;&#12290;&#20316;&#20026;&#19968;&#31181;&#36890;&#29992;&#20998;&#31867;&#22120;&#65292;SWIPE&#21487;&#20197;&#25903;&#25345;&#19981;&#21516;&#30340;&#32534;&#30721;&#22120;&#65292;&#22312;&#20998;&#31867;&#19978;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#12290;
&lt;/p&gt;
&lt;p&gt;
Because of the inevitable cost and complexity of transformer and pre-trained models, efficiency concerns are raised for long text classification. Meanwhile, in the highly sensitive domains, e.g., healthcare and legal long-text mining, potential model distrust, yet underrated and underexplored, may hatch vital apprehension. Existing methods generally segment the long text, encode each piece with the pre-trained model, and use attention or RNNs to obtain long text representation for classification. In this work, we propose a simple but effective model, Segment-aWare multIdimensional PErceptron (SWIPE), to replace attention/RNNs in the above framework. Unlike prior efforts, SWIPE can effectively learn the label of the entire text with supervised training, while perceive the labels of the segments and estimate their contributions to the long-text labeling in an unsupervised manner. As a general classifier, SWIPE can endorse different encoders, and it outperforms SOTA models in terms of cla
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;STS&#27169;&#22411;&#26469;&#39044;&#27979;&#22478;&#24066;&#24322;&#24120;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#24322;&#24120;&#25968;&#25454;&#31232;&#30095;&#38646;&#33192;&#32960;&#23548;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#32479;&#19968;&#39044;&#27979;&#22810;&#31181;&#24322;&#24120;&#65292;&#22312;&#20132;&#36890;&#20107;&#25925;&#21644;&#29359;&#32618;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;</title><link>http://arxiv.org/abs/2304.01569</link><description>&lt;p&gt;
&#26102;&#31354;&#35821;&#20041;&#38646;&#33192;&#32960;&#22478;&#24066;&#24322;&#24120;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Spatiotemporal and Semantic Zero-inflated Urban Anomaly Prediction. (arXiv:2304.01569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01569
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;STS&#27169;&#22411;&#26469;&#39044;&#27979;&#22478;&#24066;&#24322;&#24120;&#65292;&#35299;&#20915;&#20102;&#30001;&#20110;&#24322;&#24120;&#25968;&#25454;&#31232;&#30095;&#38646;&#33192;&#32960;&#23548;&#33268;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#21487;&#20197;&#32479;&#19968;&#39044;&#27979;&#22810;&#31181;&#24322;&#24120;&#65292;&#22312;&#20132;&#36890;&#20107;&#25925;&#21644;&#29359;&#32618;&#39044;&#27979;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22478;&#24066;&#24322;&#24120;&#39044;&#27979;&#65292;&#22914;&#20132;&#36890;&#20107;&#25925;&#39044;&#27979;&#21644;&#29359;&#32618;&#39044;&#27979;&#65292;&#23545;&#26234;&#24935;&#22478;&#24066;&#30340;&#23433;&#20840;&#21644;&#32500;&#25252;&#33267;&#20851;&#37325;&#35201;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#25429;&#25417;&#31354;&#38388;&#21644;&#26102;&#38388;&#32500;&#24230;&#20869;&#30340;&#20869;&#37096;&#20381;&#36182;&#20851;&#31995;&#12290;&#28982;&#32780;&#65292;&#20173;&#23384;&#22312;&#35768;&#22810;&#20851;&#38190;&#25361;&#25112;&#65292;&#20363;&#22914;&#65292;&#30001;&#20110;&#22478;&#24066;&#24322;&#24120;&#21457;&#29983;&#39057;&#29575;&#20302;&#23548;&#33268;&#30340;&#31232;&#30095;&#38646;&#33192;&#32960;&#25968;&#25454;&#65288;&#21487;&#33021;&#20250;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#19981;&#20339;&#65289;&#65292;&#20197;&#21450;&#36328;&#36234;&#31354;&#38388;&#65292;&#26102;&#38388;&#21644;&#35821;&#20041;&#32500;&#24230;&#30340;&#24322;&#24120;&#27169;&#24335;&#30340;&#20869;&#37096;&#21644;&#20114;&#38388;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#36824;&#38656;&#35201;&#25506;&#32034;&#32479;&#19968;&#30340;&#26041;&#27861;&#26469;&#39044;&#27979;&#22810;&#31181;&#24322;&#24120;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;STS&#26469;&#20849;&#21516;&#25429;&#25417;&#19977;&#20010;&#32500;&#24230;&#20869;&#30340;&#27169;&#24335;&#21644;&#24433;&#21709;&#22240;&#32032;&#20043;&#38388;&#30340;&#20869;&#37096;&#21644;&#20114;&#38388;&#20381;&#36182;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20351;&#29992;&#19968;&#20010;&#24102;&#26377;&#23450;&#21046;&#25439;&#22833;&#20989;&#25968;&#30340;&#22810;&#20219;&#21153;&#39044;&#27979;&#27169;&#22359;&#26469;&#35299;&#20915;&#38646;&#33192;&#32960;&#38382;&#39064;&#12290;&#20026;&#20102;&#39564;&#35777;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;&#20854;&#24212;&#29992;&#20110;&#20004;&#20010;&#22478;&#24066;&#24322;&#24120;&#25968;&#25454;&#38598;&#65306;&#20132;&#36890;&#20107;&#25925;&#39044;&#27979;&#21644;&#29359;&#32618;&#39044;&#27979;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20248;&#20110;&#19968;&#31995;&#21015;&#22522;&#32447;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Urban anomaly predictions, such as traffic accident prediction and crime prediction, are of vital importance to smart city security and maintenance. Existing methods typically use deep learning to capture the intra-dependencies in spatial and temporal dimensions. However, numerous key challenges remain unsolved, for instance, sparse zero-inflated data due to urban anomalies occurring with low frequency (which can lead to poor performance on real-world datasets), and both intra- and inter-dependencies of abnormal patterns across spatial, temporal, and semantic dimensions. Moreover, a unified approach to predict multiple kinds of anomaly is left to explore. In this paper, we propose STS to jointly capture the intra- and inter-dependencies between the patterns and the influential factors in three dimensions. Further, we use a multi-task prediction module with a customized loss function to solve the zero-inflated issue. To verify the effectiveness of the model, we apply it to two urban ano
&lt;/p&gt;</description></item><item><title>G2PTL&#26159;&#19968;&#31181;&#38754;&#21521;&#29289;&#27969;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#23398;&#20064;&#33021;&#21147;&#21644;&#22270;&#24314;&#27169;&#30340;&#22320;&#29702;&#20851;&#31995;&#32534;&#30721;&#33021;&#21147;&#65292;&#33021;&#26377;&#25928;&#22320;&#32534;&#30721;&#20132;&#20184;&#22320;&#22336;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#22312;&#29289;&#27969;&#31995;&#32479;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;</title><link>http://arxiv.org/abs/2304.01559</link><description>&lt;p&gt;
G2PTL&#65306;&#36866;&#29992;&#20110;&#29289;&#27969;&#31995;&#32479;&#30340;&#20132;&#20184;&#22320;&#22336;&#39044;&#35757;&#32451;&#27169;&#22411;&#21450;&#20854;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
G2PTL: A Pre-trained Model for Delivery Address and its Applications in Logistics System. (arXiv:2304.01559v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01559
&lt;/p&gt;
&lt;p&gt;
G2PTL&#26159;&#19968;&#31181;&#38754;&#21521;&#29289;&#27969;&#39046;&#22495;&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#32467;&#21512;&#20102;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#23398;&#20064;&#33021;&#21147;&#21644;&#22270;&#24314;&#27169;&#30340;&#22320;&#29702;&#20851;&#31995;&#32534;&#30721;&#33021;&#21147;&#65292;&#33021;&#26377;&#25928;&#22320;&#32534;&#30721;&#20132;&#20184;&#22320;&#22336;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#22312;&#29289;&#27969;&#31995;&#32479;&#20013;&#20855;&#26377;&#24191;&#27867;&#30340;&#24212;&#29992;&#21069;&#26223;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20316;&#20026;&#29289;&#27969;&#31995;&#32479;&#30340;&#25968;&#25454;&#22522;&#30784;&#65292;&#22522;&#20110;&#25991;&#26412;&#30340;&#20132;&#20184;&#22320;&#22336;&#21253;&#21547;&#20016;&#23500;&#19988;&#20851;&#38190;&#30340;&#20301;&#32622;&#20449;&#24687;&#12290;&#22914;&#20309;&#26377;&#25928;&#22320;&#32534;&#30721;&#20132;&#20184;&#22320;&#22336;&#26159;&#25552;&#39640;&#21518;&#32493;&#20219;&#21153;&#22312;&#29289;&#27969;&#31995;&#32479;&#20013;&#24615;&#33021;&#30340;&#26680;&#24515;&#20219;&#21153;&#12290;&#38754;&#21521;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;(NLP)&#30340;&#39044;&#35757;&#32451;&#27169;&#22411;(PTMs)&#24050;&#25104;&#20026;&#32534;&#30721;&#25991;&#26412;&#20013;&#35821;&#20041;&#20449;&#24687;&#30340;&#20027;&#35201;&#24037;&#20855;&#12290;&#34429;&#28982;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#30456;&#24403;&#26377;&#21069;&#36884;&#65292;&#20294;&#36825;&#20123;&#22522;&#20110;NLP&#30340;PTMs&#26410;&#33021;&#32534;&#30721;&#20132;&#20184;&#22320;&#22336;&#20013;&#30340;&#22320;&#29702;&#30693;&#35782;&#65292;&#36825;&#22312;&#29289;&#27969;&#31995;&#32479;(&#22914;&#33756;&#40479;&#31995;&#32479;)&#20013;&#22823;&#22823;&#21066;&#24369;&#20102;&#19982;&#20132;&#20184;&#30456;&#20851;&#30340;&#20219;&#21153;&#30340;&#24615;&#33021;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#29289;&#27969;&#39046;&#22495;&#30340;&#22495;&#29305;&#23450;&#39044;&#35757;&#32451;&#27169;&#22411;&#65292;&#21517;&#20026;G2PTL&#65292;&#21363;&#20132;&#20184;&#22320;&#22336;&#22320;&#29702;&#20851;&#31995;-&#22270;&#39044;&#35757;&#32451;&#27169;&#22411;&#12290;G2PTL&#23558;&#25991;&#26412;&#39044;&#35757;&#32451;&#30340;&#35821;&#20041;&#23398;&#20064;&#33021;&#21147;&#19982;&#22270;&#24314;&#27169;&#30340;&#22320;&#29702;&#20851;&#31995;&#32534;&#30721;&#33021;&#21147;&#30456;&#32467;&#21512;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#39318;&#20808;&#21033;&#29992;&#30495;&#23454;&#30340;&#29289;&#27969;&#20132;&#20184;&#25968;&#25454;&#26500;&#24314;&#22320;&#29702;&#22270;&#65292;&#28982;&#21518;&#22312;&#22270;&#32467;&#26500;&#21270;&#25968;&#25454;&#19978;&#23545;&#27169;&#22411;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;G2PTL&#33021;&#26377;&#25928;&#22320;&#32534;&#30721;&#22522;&#20110;&#25991;&#26412;&#30340;&#20132;&#20184;&#22320;&#22336;&#20013;&#30340;&#20301;&#32622;&#20449;&#24687;&#65292;&#24182;&#22312;&#19982;&#20132;&#20184;&#22320;&#22336;&#22788;&#29702;&#30456;&#20851;&#30340;&#20219;&#21153;&#20013;&#20248;&#20110;&#36890;&#29992;PTMs&#12290;
&lt;/p&gt;
&lt;p&gt;
Text-based delivery addresses, as the data foundation for logistics systems, contain abundant and crucial location information. How to effectively encode the delivery address is a core task to boost the performance of downstream tasks in the logistics system. Pre-trained Models (PTMs) designed for Natural Language Process (NLP) have emerged as the dominant tools for encoding semantic information in text. Though promising, those NLP-based PTMs fall short of encoding geographic knowledge in the delivery address, which considerably trims down the performance of delivery-related tasks in logistic systems such as Cainiao. To tackle the above problem, we propose a domain-specific pre-trained model, named G2PTL, a Geography-Graph Pre-trained model for delivery address in Logistics field. G2PTL combines the semantic learning capabilities of text pre-training with the geographical-relationship encoding abilities of graph modeling. Specifically, we first utilize real-world logistics delivery dat
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36816;&#34892;&#22312;&#36793;&#32536;AI&#35774;&#22791;&#19978;&#30340;&#23454;&#26102;&#39550;&#39542;&#21592;&#30417;&#25511;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#32463;&#36807;&#27169;&#22411;&#25163;&#26415;&#65292;&#22312;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#24110;&#21161;&#19979;&#23454;&#29616;&#20102;&#39640;&#24103;&#29575;&#30340;&#22788;&#29702;&#25928;&#26524;&#12290;</title><link>http://arxiv.org/abs/2304.01555</link><description>&lt;p&gt;
&#36793;&#32536;AI&#35774;&#22791;&#19978;&#30340;&#23454;&#26102;&#39550;&#39542;&#21592;&#30417;&#25511;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
Real-time Driver Monitoring Systems on Edge AI Device. (arXiv:2304.01555v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01555
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#36816;&#34892;&#22312;&#36793;&#32536;AI&#35774;&#22791;&#19978;&#30340;&#23454;&#26102;&#39550;&#39542;&#21592;&#30417;&#25511;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#32463;&#36807;&#27169;&#22411;&#25163;&#26415;&#65292;&#22312;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#24110;&#21161;&#19979;&#23454;&#29616;&#20102;&#39640;&#24103;&#29575;&#30340;&#22788;&#29702;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#21496;&#26426;&#19981;&#27880;&#24847;&#21147;&#23548;&#33268;&#36335;&#19978;&#20107;&#25925;&#30340;&#22686;&#21152;&#65292;&#33258;&#21160;&#21270;&#39550;&#39542;&#21592;&#30417;&#25511;&#31995;&#32479;(DMS)&#24471;&#21040;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#35748;&#21487;&#12290;&#22312;&#26412;&#25253;&#21578;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#20010;&#36816;&#34892;&#20110;&#30828;&#20214;&#21152;&#36895;&#22120;&#30340;&#36793;&#32536;&#35774;&#22791;&#19978;&#30340;&#23454;&#26102;DMS&#31995;&#32479;&#12290;&#35813;&#31995;&#32479;&#30001;&#32418;&#22806;&#25668;&#20687;&#22836;&#35760;&#24405;&#39550;&#39542;&#21592;&#30340;&#30011;&#38754;&#21644;&#36793;&#32536;&#35774;&#22791;&#22788;&#29702;&#25968;&#25454;&#32452;&#25104;&#12290;&#20026;&#20102;&#22312;&#36793;&#32536;&#35774;&#22791;&#19978;&#25104;&#21151;&#31227;&#26893;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#20805;&#20998;&#21033;&#29992;&#30828;&#20214;&#21152;&#36895;&#22120;&#65292;&#36827;&#34892;&#20102;&#27169;&#22411;&#25163;&#26415;&#12290;&#26368;&#32456;&#30340;DMS&#31995;&#32479;&#22312;TI-TDA4VM&#36793;&#32536;&#35774;&#22791;&#19978;&#23454;&#29616;&#20102;63&#24103;&#27599;&#31186;(FPS)&#30340;&#25928;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
As road accident cases are increasing due to the inattention of the driver, automated driver monitoring systems (DMS) have gained an increase in acceptance. In this report, we present a real-time DMS system that runs on a hardware-accelerator-based edge device. The system consists of an InfraRed camera to record the driver footage and an edge device to process the data. To successfully port the deep learning models to run on the edge device taking full advantage of the hardware accelerators, model surgery was performed. The final DMS system achieves 63 frames per second (FPS) on the TI-TDA4VM edge device.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MTDA&#26041;&#27861;&#65292;&#21517;&#20026;\emph{MEnsA}&#65292;&#21033;&#29992;&#28151;&#21512;&#38598;&#25104;&#24179;&#22343;&#26041;&#27861;&#25552;&#39640;&#20102;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01554</link><description>&lt;p&gt;
\emph{MEnsA}: &#19977;&#32500;&#28857;&#20113;&#26080;&#30417;&#30563;&#22810;&#30446;&#26631;&#22495;&#33258;&#36866;&#24212;&#30340;&#28151;&#21512;&#38598;&#25104;&#24179;&#22343;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
\emph{MEnsA}: Mix-up Ensemble Average for Unsupervised Multi Target Domain Adaptation on 3D Point Clouds. (arXiv:2304.01554v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01554
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;MTDA&#26041;&#27861;&#65292;&#21517;&#20026;\emph{MEnsA}&#65292;&#21033;&#29992;&#28151;&#21512;&#38598;&#25104;&#24179;&#22343;&#26041;&#27861;&#25552;&#39640;&#20102;&#22495;&#33258;&#36866;&#24212;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#22495;&#33258;&#36866;&#24212;&#65288;UDA&#65289;&#35299;&#20915;&#20102;&#26631;&#35760;&#28304;&#22495;&#21644;&#26410;&#26631;&#35760;&#30446;&#26631;&#22495;&#20043;&#38388;&#20998;&#24067;&#24046;&#24322;&#30340;&#38382;&#39064;&#12290;&#34429;&#28982;&#21333;&#30446;&#26631;&#22495;&#33258;&#36866;&#24212;&#65288;STDA&#65289;&#24050;&#32463;&#22312;2D&#21644;3D&#35270;&#35273;&#25991;&#29486;&#20013;&#24471;&#21040;&#20102;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#22312;3D&#25968;&#25454;&#30340;&#22810;&#30446;&#26631;&#22495;&#33258;&#36866;&#24212;&#65288;MTDA&#65289;&#26041;&#38754;&#20960;&#20046;&#27809;&#26377;&#24471;&#21040;&#30740;&#31350;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;\emph{{\bf M}ixup {\bf Ens}emble {\bf A}verage}&#25110;&#31616;&#31216;{\bf \emph{MEnsA}}&#30340;&#28151;&#21512;&#38598;&#25104;&#24179;&#22343;&#26041;&#27861;&#65292;&#23558;&#25152;&#26377;&#39046;&#22495;&#30340;&#29305;&#24449;&#34920;&#31034;&#28151;&#21512;&#22312;&#19968;&#36215;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#22495;&#33258;&#36866;&#24212;&#24615;&#33021;&#30340;MTDA&#22522;&#32447;&#12290;&#36890;&#36807;&#28151;&#21512;&#34920;&#31034;&#65292;&#25105;&#20204;&#20351;&#29992;&#22495;&#20998;&#31867;&#22120;&#22312;&#20849;&#20139;&#30340;&#28508;&#22312;&#31354;&#38388;&#20013;&#25552;&#39640;&#20102;&#28304;&#22495;&#29305;&#24449;&#34920;&#31034;&#19982;&#30446;&#26631;&#22495;&#29305;&#24449;&#34920;&#31034;&#30340;&#21306;&#20998;&#33021;&#21147;&#12290;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;PointDA-10&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised domain adaptation (UDA) addresses the problem of distribution shift between the unlabeled target domain and labelled source domain. While the single target domain adaptation (STDA) is well studied in both 2D and 3D vision literature, multi-target domain adaptation (MTDA) is barely explored for 3D data despite its wide real-world applications such as autonomous driving systems for various geographical and climatic conditions. We establish an MTDA baseline for 3D point cloud data by proposing to mix the feature representations from all domains together to achieve better domain adaptation performance by an ensemble average, which we call \emph{{\bf M}ixup {\bf Ens}emble {\bf A}verage} or {\bf \emph{MEnsA}}. With the mixed representation, we use a domain classifier to improve at distinguishing the feature representations of source domain from those of target domains in a shared latent space. In extensive empirical validations on the challenging PointDA-10 dataset, we showcase 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22343;&#22330;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65288;MF-PPO&#65289;&#65292;&#20197;&#31283;&#23450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;OpenSpiel&#26694;&#26550;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;</title><link>http://arxiv.org/abs/2304.01547</link><description>&lt;p&gt;
&#23545;&#31574;&#30053;&#26356;&#26032;&#30340;&#27491;&#21017;&#21270;&#20197;&#31283;&#23450;&#22343;&#22330;&#21338;&#24328;
&lt;/p&gt;
&lt;p&gt;
Regularization of the policy updates for stabilizing Mean Field Games. (arXiv:2304.01547v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22343;&#22330;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#31639;&#27861;&#65288;MF-PPO&#65289;&#65292;&#20197;&#31283;&#23450;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#22312;&#22343;&#22330;&#21338;&#24328;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#22312;OpenSpiel&#26694;&#26550;&#20013;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#38750;&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;(MARL)&#65292;&#20854;&#20013;&#22810;&#20010;&#26234;&#33021;&#20307;&#22312;&#21516;&#19968;&#29615;&#22659;&#20013;&#30456;&#20114;&#20316;&#29992;&#65292;&#30446;&#26631;&#26159;&#26368;&#22823;&#21270;&#20010;&#20307;&#22238;&#25253;&#12290;&#24403;&#26234;&#33021;&#20307;&#25968;&#37327;&#25193;&#22823;&#26102;&#65292;&#30001;&#20110;&#35768;&#22810;&#26234;&#33021;&#20307;&#24341;&#20837;&#30340;&#38750;&#38745;&#27490;&#24615;&#65292;&#20250;&#20135;&#29983;&#25361;&#25112;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#22343;&#22330;&#21338;&#24328;&#65288;MFG&#65289;&#20381;&#38752;&#23545;&#31216;&#24615;&#21644;&#21516;&#36136;&#24615;&#20551;&#35774;&#65292;&#36817;&#20284;&#20855;&#26377;&#24456;&#22823;&#32676;&#20307;&#30340;&#21338;&#24328;&#12290;&#26368;&#36817;&#65292;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#34987;&#29992;&#20110;&#23558;MFG&#25193;&#23637;&#21040;&#20855;&#26377;&#26356;&#22810;&#29366;&#24577;&#30340;&#21338;&#24328;&#20013;&#12290;&#30446;&#21069;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#24179;&#28369;&#25216;&#26415;&#65292;&#22914;&#23545;q&#20540;&#25110;&#22343;&#22330;&#20998;&#24067;&#26356;&#26032;&#36827;&#34892;&#24179;&#22343;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22343;&#22330;&#31574;&#30053;&#19978;&#36827;&#34892;&#36817;&#20284;&#26356;&#26032;&#20197;&#31283;&#23450;&#23398;&#20064;&#30340;&#19981;&#21516;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#31639;&#27861;&#21629;&#21517;&#20026;&#22343;&#22330;&#36817;&#31471;&#31574;&#30053;&#20248;&#21270;&#65288;MF-PPO&#65289;&#65292;&#24182;&#22312;OpenSpiel&#26694;&#26550;&#20013;&#32463;&#39564;&#24615;&#22320;&#23637;&#31034;&#20102;&#25105;&#20204;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work studies non-cooperative Multi-Agent Reinforcement Learning (MARL) where multiple agents interact in the same environment and whose goal is to maximize the individual returns. Challenges arise when scaling up the number of agents due to the resultant non-stationarity that the many agents introduce. In order to address this issue, Mean Field Games (MFG) rely on the symmetry and homogeneity assumptions to approximate games with very large populations. Recently, deep Reinforcement Learning has been used to scale MFG to games with larger number of states. Current methods rely on smoothing techniques such as averaging the q-values or the updates on the mean-field distribution. This work presents a different approach to stabilize the learning based on proximal updates on the mean-field policy. We name our algorithm \textit{Mean Field Proximal Policy Optimization (MF-PPO)}, and we empirically show the effectiveness of our method in the OpenSpiel framework.
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#25991;&#31456;&#31995;&#32479;&#35780;&#20272;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21508;&#31181;&#25361;&#25112;&#21644;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;AI&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#23545;&#20020;&#24202;&#21307;&#29983;&#26356;&#21152;&#36879;&#26126;&#21487;&#20449;&#12290;</title><link>http://arxiv.org/abs/2304.01543</link><description>&lt;p&gt;
&#35299;&#26512;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#31616;&#35201;&#35780;&#35770;
&lt;/p&gt;
&lt;p&gt;
A Brief Review of Explainable Artificial Intelligence in Healthcare. (arXiv:2304.01543v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01543
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#25991;&#31456;&#31995;&#32479;&#35780;&#20272;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#21508;&#31181;&#25361;&#25112;&#21644;&#26041;&#27861;&#65292;&#26088;&#22312;&#25552;&#39640;AI&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20351;&#20854;&#23545;&#20020;&#24202;&#21307;&#29983;&#26356;&#21152;&#36879;&#26126;&#21487;&#20449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
XAI&#26159;&#25351;&#26500;&#24314;AI&#24212;&#29992;&#31243;&#24207;&#30340;&#25216;&#26415;&#21644;&#26041;&#27861;&#65292;&#21487;&#20197;&#24110;&#21161;&#26368;&#32456;&#29992;&#25143;&#35299;&#37322;AI&#27169;&#22411;&#30340;&#36755;&#20986;&#21644;&#39044;&#27979;&#12290;&#22312;&#39640;&#39118;&#38505;&#20915;&#31574;&#24773;&#22659;&#20013;&#65288;&#22914;&#21307;&#23398;&#39046;&#22495;&#65289;&#20351;&#29992;&#40657;&#30418;AI&#24212;&#29992;&#31243;&#24207;&#22686;&#21152;&#20102;&#36879;&#26126;&#24615;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#38656;&#27714;&#65292;&#22240;&#20026;&#38169;&#35823;&#30340;&#39044;&#27979;&#21487;&#33021;&#20250;&#20135;&#29983;&#20005;&#37325;&#21518;&#26524;&#12290;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#23545;&#20110;&#22312;&#21307;&#30103;&#20445;&#20581;&#23454;&#36341;&#20013;&#25104;&#21151;&#37096;&#32626;AI&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#38656;&#35201;&#35753;&#20020;&#24202;&#21307;&#29983;&#36879;&#26126;&#22320;&#20102;&#35299;AI&#24212;&#29992;&#31243;&#24207;&#30340;&#22522;&#26412;&#25512;&#29702;&#36807;&#31243;&#20197;&#33719;&#24471;&#20182;&#20204;&#30340;&#20449;&#20219;&#12290;&#26412;&#25991;&#31995;&#32479;&#22320;&#35780;&#20272;&#20102;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#30340;&#26041;&#26041;&#38754;&#38754;&#21644;&#25361;&#25112;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#22238;&#39038;&#21508;&#31181;XAI&#26041;&#27861;&#12289;&#20854;&#25361;&#25112;&#20197;&#21450;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#30456;&#20851;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#36825;&#20123;&#26041;&#27861;&#20998;&#20026;&#20845;&#31867;&#65306;&#29305;&#24449;&#23548;&#21521;&#26041;&#27861;&#12289;&#20840;&#23616;&#26041;&#27861;&#12289;&#27010;&#24565;&#27169;&#22411;&#12289;&#20195;&#29702;&#27169;&#22411;&#12289;&#26412;&#22320;&#20687;&#32032;&#26041;&#27861;&#21644;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
XAI refers to the techniques and methods for building AI applications which assist end users to interpret output and predictions of AI models. Black box AI applications in high-stakes decision-making situations, such as medical domain have increased the demand for transparency and explainability since wrong predictions may have severe consequences. Model explainability and interpretability are vital successful deployment of AI models in healthcare practices. AI applications' underlying reasoning needs to be transparent to clinicians in order to gain their trust. This paper presents a systematic review of XAI aspects and challenges in the healthcare domain. The primary goals of this study are to review various XAI methods, their challenges, and related machine learning models in healthcare. The methods are discussed under six categories: Features-oriented methods, global methods, concept models, surrogate models, local pixel-based methods, and human-centric methods. Most importantly, th
&lt;/p&gt;</description></item><item><title>CoLweb&#26159;&#19968;&#31181;&#20248;&#31168;&#30340;&#31639;&#27861;&#35821;&#35328;&#65292;&#21487;&#20197;&#36890;&#36807;&#39640;&#23618;&#27425;&#12289;&#35777;&#26126;&#25658;&#24102;&#65292;&#20998;&#24067;&#24335;&#39118;&#26684;&#30340;&#26041;&#27861;&#32479;&#19968;&#22810;&#31181;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;Horn&#23376;&#21477;&#23450;&#20041;&#32454;&#21270;&#26041;&#38754;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.01539</link><description>&lt;p&gt;
&#22312;&#21487;&#35745;&#31639;&#36923;&#36753; Web &#20013;&#23454;&#29616;&#21160;&#24577;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Implementing Dynamic Programming in Computability Logic Web. (arXiv:2304.01539v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01539
&lt;/p&gt;
&lt;p&gt;
CoLweb&#26159;&#19968;&#31181;&#20248;&#31168;&#30340;&#31639;&#27861;&#35821;&#35328;&#65292;&#21487;&#20197;&#36890;&#36807;&#39640;&#23618;&#27425;&#12289;&#35777;&#26126;&#25658;&#24102;&#65292;&#20998;&#24067;&#24335;&#39118;&#26684;&#30340;&#26041;&#27861;&#32479;&#19968;&#22810;&#31181;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#65292;&#32780;&#19988;&#22312;Horn&#23376;&#21477;&#23450;&#20041;&#32454;&#21270;&#26041;&#38754;&#26377;&#24456;&#22909;&#30340;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31639;&#27861;&#23450;&#20041;&#21450;&#20854;&#30456;&#24212;&#30340;&#31639;&#27861;&#35821;&#35328; CoLweb&#12290;CoLweb &#30340;&#20248;&#28857;&#22312;&#20110;&#23427;&#20351;&#24471;&#31639;&#27861;&#35774;&#35745;&#26356;&#21152;&#28789;&#27963;&#65292;&#24378;&#21046;&#25105;&#20204;&#37319;&#29992;&#39640;&#23618;&#27425;&#12289;&#35777;&#26126;&#25658;&#24102;&#65292;&#20998;&#24067;&#24335;&#39118;&#26684;&#30340;&#31639;&#27861;&#35774;&#35745;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#20998;&#24067;&#24335;&#19982;&#38750;&#20998;&#24067;&#24335;&#35745;&#31639;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#36825;&#31181;&#26041;&#27861;&#31616;&#21270;&#20102;&#31639;&#27861;&#35774;&#35745;&#65292;&#24182;&#23558;&#20854;&#20182;&#26041;&#27861;&#32479;&#19968;&#36215;&#26469;&#65292;&#21253;&#25324;&#36882;&#24402;&#36923;&#36753;/&#20989;&#25968;&#31639;&#27861;&#12289;&#21629;&#20196;&#24335;&#31639;&#27861;&#12289;&#38754;&#21521;&#23545;&#35937;&#30340;&#21629;&#20196;&#24335;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#12289;&#20132;&#20114;&#24335;&#32593;&#32476;&#12289;&#35777;&#26126;&#25658;&#24102;&#20195;&#30721;&#31561;&#12290;&#20316;&#20026;&#24212;&#29992;&#65292;&#25105;&#20204;&#23558; Horn &#23376;&#21477;&#23450;&#20041;&#32454;&#21270;&#20026;&#20004;&#31181;&#31867;&#22411;&#65306;&#30450;&#37327;&#21270;&#30340;&#20840;&#37327;&#21270;&#23450;&#20041;&#65288;BUQ&#65289;&#21644;&#24182;&#34892;&#37327;&#21270;&#30340;&#20840;&#37327;&#21270;&#23450;&#20041;&#65288;PUQ&#65289;&#12290;BUQ &#23450;&#20041;&#23545;&#24212;&#20110;&#20256;&#32479;&#30340;&#23450;&#20041;&#65292;&#20363;&#22914; Prolog &#20013;&#30340;&#23450;&#20041;&#65292;&#20854;&#20013;&#30693;&#35782;&#24211;&#19981;&#20250;&#25193;&#23637;&#65292;&#20854;&#35777;&#26126;&#36807;&#31243;&#22522;&#20110;&#21521;&#21518;&#38142;&#25509;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#22312; PUQ &#23450;&#20041;&#20013;&#65292;&#30693;&#35782;&#24211;&#27491;&#22312;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a novel definition of an algorithm and its corresponding algorithm language called CoLweb. The merit of CoLweb [1] is that it makes algorithm design so versatile. That is, it forces us to a high-level, proof-carrying, distributed-style approach to algorithm design for both non-distributed computing and distributed one. We argue that this approach simplifies algorithm design. In addition, it unifies other approaches including recursive logical/functional algorithms, imperative algorithms, object-oriented imperative algorithms, neural-nets, interaction nets, proof-carrying code, etc. As an application, we refine Horn clause definitions into two kinds: blind-univerally-quantified (BUQ) ones and parallel-universally-quantified (PUQ) ones. BUQ definitions corresponds to the traditional ones such as those in Prolog where knowledgebase is $not$ expanding and its proof procedure is based on the backward chaining. On the other hand, in PUQ definitions, knowledgebase is $expanding$ an
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#36807;&#31243;&#27169;&#22411;&#65292;&#21363;&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#65292;&#29992;&#20110;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21160;&#24577;&#19978;&#19979;&#25991;&#35760;&#24518;&#12289;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#32858;&#21512;&#21644;&#26657;&#20934;&#39044;&#27979;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#32463;&#23454;&#39564;&#34920;&#26126;&#22312;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#24615;&#33021;&#26368;&#20808;&#36827;&#65292;&#23545;&#20110;&#22122;&#22768;&#26679;&#26412;&#20855;&#26377;&#33391;&#22909;&#25269;&#25239;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20110;&#39046;&#22495;&#20043;&#22806;&#30340;&#26816;&#27979;&#26159;&#21487;&#38752;&#30340;&#12290;</title><link>http://arxiv.org/abs/2304.01518</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#29992;&#20110;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
Multimodal Neural Processes for Uncertainty Estimation. (arXiv:2304.01518v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01518
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#31070;&#32463;&#36807;&#31243;&#27169;&#22411;&#65292;&#21363;&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#65292;&#29992;&#20110;&#23545;&#22810;&#27169;&#24577;&#25968;&#25454;&#36827;&#34892;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#21160;&#24577;&#19978;&#19979;&#25991;&#35760;&#24518;&#12289;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#32858;&#21512;&#21644;&#26657;&#20934;&#39044;&#27979;&#30340;&#27880;&#24847;&#26426;&#21046;&#65292;&#32463;&#23454;&#39564;&#34920;&#26126;&#22312;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#26041;&#38754;&#24615;&#33021;&#26368;&#20808;&#36827;&#65292;&#23545;&#20110;&#22122;&#22768;&#26679;&#26412;&#20855;&#26377;&#33391;&#22909;&#25269;&#25239;&#33021;&#21147;&#65292;&#24182;&#19988;&#23545;&#20110;&#39046;&#22495;&#20043;&#22806;&#30340;&#26816;&#27979;&#26159;&#21487;&#38752;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#36807;&#31243;( Neural Processes, NPs)&#23558;&#21442;&#25968;&#21270;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#33021;&#21147;&#21644;&#38750;&#21442;&#25968;&#39640;&#26031;&#36807;&#31243;&#21487;&#38752;&#30340;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#32467;&#21512;&#22312;&#20102;&#19968;&#36215;&#12290;&#34429;&#28982;&#26368;&#36817;NPs&#30340;&#21457;&#23637;&#24050;&#32463;&#22312;&#22238;&#24402;&#21644;&#20998;&#31867;&#26041;&#38754;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#26159;&#22914;&#20309;&#23558;NPs&#36866;&#24212;&#22810;&#27169;&#24577;&#25968;&#25454;&#23578;&#26410;&#21463;&#21040;&#20180;&#32454;&#30340;&#30740;&#31350;&#12290;&#25105;&#20204;&#39318;&#27425;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;NP&#23478;&#26063;&#27169;&#22411;&#65292;&#29992;&#20110;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#65292;&#21363;&#22810;&#27169;&#24577;&#31070;&#32463;&#36807;&#31243;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31181;&#25972;&#20307;&#30340;&#12289;&#22522;&#20110;&#21407;&#21017;&#30340;&#26041;&#27861;&#65292;&#24320;&#21457;&#20102;&#19968;&#20010;&#30001;&#20998;&#31867;&#35823;&#24046;&#26356;&#26032;&#30340;&#21160;&#24577;&#19978;&#19979;&#25991;&#35760;&#24518;&#65292;&#19968;&#20010;&#32858;&#21512;&#22810;&#27169;&#24577;&#34920;&#31034;&#30340;&#22810;&#27169;&#24577;&#36125;&#21494;&#26031;&#32858;&#21512;&#26426;&#21046;&#65292;&#20197;&#21450;&#19968;&#20010;&#29992;&#20110;&#26657;&#20934;&#39044;&#27979;&#30340;&#26032;&#30340;&#27880;&#24847;&#26426;&#21046;&#12290;&#22312;&#24191;&#27867;&#30340;&#32463;&#39564;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#23454;&#29616;&#20102;&#26368;&#20808;&#36827;&#30340;&#22810;&#27169;&#24577;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;&#24615;&#33021;&#65292;&#23637;&#31034;&#20102;&#23427;&#30340;&#21560;&#24341;&#21147;&#65292;&#21363;&#33021;&#22815;&#25269;&#25239;&#22122;&#22768;&#26679;&#26412;&#30340;&#24178;&#25200;&#65292;&#24182;&#21487;&#38752;&#22320;&#22312;&#39046;&#22495;&#20043;&#22806;&#36827;&#34892;&#26816;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural processes (NPs) have brought the representation power of parametric deep neural networks and the reliable uncertainty estimation of non-parametric Gaussian processes together. Although recent development of NPs has shown success in both regression and classification, how to adapt NPs to multimodal data has not be carefully studied. For the first time, we propose a new model of NP family for multimodal uncertainty estimation, namely Multimodal Neural Processes. In a holistic and principled way, we develop a dynamic context memory updated by the classification error, a multimodal Bayesian aggregation mechanism to aggregate multimodal representations, and a new attention mechanism for calibrated predictions. In extensive empirical evaluation, our method achieves the state-of-the-art multimodal uncertainty estimation performance, showing its appealing ability of being robust against noisy samples and reliable in out-of-domain detection.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#27010;&#24565;&#28418;&#31227;&#22788;&#29702;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#20840;&#29699;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#22635;&#34917;&#20102;&#22788;&#29702;&#20998;&#31867;&#39046;&#22495;&#20013;&#27010;&#24565;&#28418;&#31227;&#26041;&#27861;&#30340;&#31354;&#30333;&#12290;</title><link>http://arxiv.org/abs/2304.01512</link><description>&lt;p&gt;
&#20840;&#29699;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#22788;&#29702;
&lt;/p&gt;
&lt;p&gt;
Handling Concept Drift in Global Time Series Forecasting. (arXiv:2304.01512v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01512
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20004;&#31181;&#26032;&#30340;&#27010;&#24565;&#28418;&#31227;&#22788;&#29702;&#26041;&#27861;&#65292;&#24212;&#29992;&#20110;&#20840;&#29699;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#65292;&#22635;&#34917;&#20102;&#22788;&#29702;&#20998;&#31867;&#39046;&#22495;&#20013;&#27010;&#24565;&#28418;&#31227;&#26041;&#27861;&#30340;&#31354;&#30333;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26426;&#22120;&#23398;&#20064;&#30340;&#26102;&#38388;&#24207;&#21015;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#38656;&#35201;&#24182;&#20551;&#35774;&#25968;&#25454;&#22312;&#20135;&#29983;&#39044;&#27979;&#26102;&#20855;&#26377;&#19968;&#23450;&#31243;&#24230;&#30340;&#24179;&#31283;&#24615;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#23454;&#38469;&#24773;&#20917;&#19979;&#65292;&#25968;&#25454;&#20998;&#24067;&#19981;&#26159;&#31283;&#24577;&#30340;&#65292;&#32780;&#23427;&#20204;&#38543;&#30528;&#26102;&#38388;&#30340;&#25512;&#31227;&#32780;&#25913;&#21464;&#65292;&#20174;&#32780;&#38477;&#20302;&#20102;&#39044;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#22312;&#26426;&#22120;&#23398;&#20064;&#25991;&#29486;&#20013;&#34987;&#31216;&#20026;&#27010;&#24565;&#28418;&#31227;&#12290;&#22788;&#29702;&#39044;&#27979;&#20013;&#30340;&#27010;&#24565;&#28418;&#31227;&#23545;&#20110;&#35768;&#22810;&#29616;&#20170;&#20351;&#29992;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25506;&#35752;&#20102;&#29992;&#20110;&#20840;&#29699;&#39044;&#27979;&#27169;&#22411;&#65288;GFM&#65289;&#20013;&#22788;&#29702;&#27010;&#24565;&#28418;&#31227;&#30340;&#26041;&#27861;&#65292;&#24182;&#25552;&#20986;&#20102;&#20004;&#31181;&#26032;&#30340;&#27010;&#24565;&#28418;&#31227;&#22788;&#29702;&#26041;&#27861;&#65306;&#35823;&#24046;&#36129;&#29486;&#21152;&#26435;&#65288;ECW&#65289;&#21644;&#26799;&#24230;&#19979;&#38477;&#21152;&#26435;&#65288;GDW&#65289;&#65292;&#22522;&#20110;&#36830;&#32493;&#30340;&#33258;&#36866;&#24212;&#26435;&#37325;&#27010;&#24565;&#20351;&#29992;&#20004;&#20010;&#21333;&#29420;&#35757;&#32451;&#30340;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) based time series forecasting models often require and assume certain degrees of stationarity in the data when producing forecasts. However, in many real-world situations, the data distributions are not stationary and they can change over time while reducing the accuracy of the forecasting models, which in the ML literature is known as concept drift. Handling concept drift in forecasting is essential for many ML methods in use nowadays, however, the prior work only proposes methods to handle concept drift in the classification domain. To fill this gap, we explore concept drift handling methods in particular for Global Forecasting Models (GFM) which recently have gained popularity in the forecasting domain. We propose two new concept drift handling methods, namely: Error Contribution Weighting (ECW) and Gradient Descent Weighting (GDW), based on a continuous adaptive weighting concept. These methods use two forecasting models which are separately trained with the m
&lt;/p&gt;</description></item><item><title>EPVT&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#30142;&#30149;&#19981;&#30456;&#20851;&#22270;&#20687;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23884;&#20837;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#21644;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#36827;&#34892;&#39046;&#22495;&#19968;&#33324;&#21270;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#12290;</title><link>http://arxiv.org/abs/2304.01508</link><description>&lt;p&gt;
EPVT: &#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#22312;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#39046;&#22495;&#19968;&#33324;&#21270;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
EPVT: Environment-aware Prompt Vision Transformer for Domain Generalization in Skin Lesion Recognition. (arXiv:2304.01508v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01508
&lt;/p&gt;
&lt;p&gt;
EPVT&#26159;&#19968;&#31181;&#22522;&#20110;&#29615;&#22659;&#24863;&#30693;&#30340;&#25552;&#31034;&#35270;&#35273;Transformer&#65292;&#29992;&#20110;&#35299;&#20915;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#20013;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#30142;&#30149;&#19981;&#30456;&#20851;&#22270;&#20687;&#29305;&#24449;&#30340;&#38382;&#39064;&#65292;&#36890;&#36807;&#23884;&#20837;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#21644;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#36827;&#34892;&#39046;&#22495;&#19968;&#33324;&#21270;&#65292;&#24182;&#19988;&#24341;&#20837;&#20102;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#36827;&#34892;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#24050;&#21462;&#24471;&#37325;&#22823;&#36827;&#23637;&#65292;&#32780;&#22312;&#29616;&#23454;&#19990;&#30028;&#22330;&#26223;&#20013;&#37096;&#32626;&#36825;&#20123;&#31995;&#32479;&#30340;&#38656;&#27714;&#19981;&#26029;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#29992;&#20110;&#30382;&#32932;&#30149;&#21464;&#35782;&#21035;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#21487;&#33021;&#36807;&#24230;&#20381;&#36182;&#20110;&#19982;&#30142;&#30149;&#19981;&#30456;&#20851;&#30340;&#22270;&#20687;&#29305;&#24449;&#65288;&#22914;&#26263;&#35282;&#12289;&#27987;&#23494;&#27611;&#21457;&#65289;&#65292;&#23548;&#33268;&#22312;&#30475;&#19981;&#35265;&#30340;&#29615;&#22659;&#20013;&#34920;&#29616;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#39046;&#22495;&#19968;&#33324;&#21270;&#26041;&#27861;&#8212;&#8212;EPVT&#65292;&#23427;&#23558;&#25552;&#31034;&#23884;&#20837;&#21040;Vision Transformer&#20013;&#65292;&#20197;&#21327;&#21516;&#23398;&#20064;&#26469;&#33258;&#19981;&#21516;&#39046;&#22495;&#30340;&#30693;&#35782;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EPVT&#21033;&#29992;&#19968;&#32452;&#39046;&#22495;&#25552;&#31034;&#65292;&#27599;&#20010;&#39046;&#22495;&#25552;&#31034;&#37117;&#25198;&#28436;&#39046;&#22495;&#19987;&#23478;&#30340;&#35282;&#33394;&#65292;&#20197;&#25429;&#33719;&#39046;&#22495;&#29305;&#23450;&#30340;&#30693;&#35782;&#65307;&#20197;&#21450;&#19968;&#20010;&#20849;&#20139;&#25552;&#31034;&#26469;&#33719;&#24471;&#25972;&#20010;&#25968;&#25454;&#38598;&#30340;&#36890;&#29992;&#30693;&#35782;&#12290;&#20026;&#20102;&#20419;&#36827;&#30693;&#35782;&#20849;&#20139;&#21644;&#19981;&#21516;&#25552;&#31034;&#20043;&#38388;&#30340;&#20132;&#20114;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#20010;&#39046;&#22495;&#25552;&#31034;&#29983;&#25104;&#22120;&#65292;&#23427;&#20351;&#24471;&#39046;&#22495;&#25552;&#31034;&#19982;&#20849;&#20139;&#25552;&#31034;&#20043;&#38388;&#21487;&#20197;&#36827;&#34892;&#20302;&#31209;&#20056;&#24615;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
Skin lesion recognition using deep learning has made remarkable progress, and there is an increasing need for deploying these systems in real-world scenarios. However, recent research has revealed that deep neural networks for skin lesion recognition may overly depend on disease-irrelevant image artifacts (i.e. dark corners, dense hairs), leading to poor generalization in unseen environments. To address this issue, we propose a novel domain generalization method called EPVT, which involves embedding prompts into the vision transformer to collaboratively learn knowledge from diverse domains. Concretely, EPVT leverages a set of domain prompts, each of which plays as a domain expert, to capture domain-specific knowledge; and a shared prompt for general knowledge over the entire dataset. To facilitate knowledge sharing and the interaction of different prompts, we introduce a domain prompt generator that enables low-rank multiplicative updates between domain prompts and the shared prompt. A
&lt;/p&gt;</description></item><item><title>RARE&#26159;&#19968;&#31181;&#40065;&#26834;&#24615;&#25239;&#24178;&#25200;&#30340;&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#22312;&#39640;&#38454;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#25513;&#30721;&#21644;&#37325;&#26500;&#33410;&#28857;&#26679;&#26412;&#26469;&#25552;&#39640;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#30830;&#23450;&#24615;&#21644;&#33258;&#30417;&#30563;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;SGP&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01507</link><description>&lt;p&gt;
RARE&#65306;&#40065;&#26834;&#24615;&#25239;&#24178;&#25200;&#30340;&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;
&lt;/p&gt;
&lt;p&gt;
RARE: Robust Masked Graph Autoencoder. (arXiv:2304.01507v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01507
&lt;/p&gt;
&lt;p&gt;
RARE&#26159;&#19968;&#31181;&#40065;&#26834;&#24615;&#25239;&#24178;&#25200;&#30340;&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;&#65292;&#36890;&#36807;&#22312;&#39640;&#38454;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#36827;&#34892;&#25513;&#30721;&#21644;&#37325;&#26500;&#33410;&#28857;&#26679;&#26412;&#26469;&#25552;&#39640;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#30830;&#23450;&#24615;&#21644;&#33258;&#30417;&#30563;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#65292;&#24182;&#22312;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;SGP&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25513;&#30721;&#22270;&#33258;&#32534;&#30721;&#22120;&#65288;MGAE&#65289;&#30001;&#20110;&#20854;&#31616;&#21333;&#21644;&#26377;&#25928;&#30340;&#29305;&#24615;&#65292;&#22312;&#33258;&#30417;&#30563;&#22270;&#39044;&#35757;&#32451;&#65288;SGP&#65289;&#26041;&#38754;&#24050;&#25104;&#20026;&#19968;&#31181;&#24456;&#26377;&#21069;&#36884;&#30340;&#33539;&#20363;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#26041;&#27861;&#22312;&#21407;&#22987;&#25968;&#25454;&#31354;&#38388;&#20013;&#25191;&#34892;&#25513;&#30721;-&#37325;&#26500;&#25805;&#20316;&#65292;&#31867;&#20284;&#20110;&#35745;&#31639;&#26426;&#35270;&#35273;&#65288;CV&#65289;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#39046;&#22495;&#65292;&#32780;&#24573;&#30053;&#20102;&#22270;&#25968;&#25454;&#30340;&#37325;&#35201;&#38750;&#27431;&#20960;&#37324;&#24471;&#23646;&#24615;&#12290;&#32467;&#26524;&#65292;&#39640;&#24230;&#19981;&#31283;&#23450;&#30340;&#23616;&#37096;&#36830;&#25509;&#32467;&#26500;&#22823;&#22823;&#22686;&#21152;&#20102;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#38477;&#20302;&#20102;&#21033;&#29992;&#33258;&#30417;&#30563;&#20449;&#21495;&#30340;&#21487;&#38752;&#24615;&#65292;&#23548;&#33268;&#19979;&#28216;&#35780;&#20272;&#20013;&#30340;&#34920;&#31034;&#25928;&#26524;&#19981;&#20339;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;SGP&#26041;&#27861;&#65292;&#31216;&#20026;Robust mAsked gRaph autoEncoder&#65288;RARE&#65289;&#65292;&#36890;&#36807;&#39640;&#38454;&#28508;&#22312;&#29305;&#24449;&#31354;&#38388;&#20013;&#26356;&#22810;&#30340;&#25513;&#30721;&#21644;&#37325;&#26500;&#33410;&#28857;&#26679;&#26412;&#26469;&#25552;&#39640;&#25512;&#26029;&#25513;&#30721;&#25968;&#25454;&#30340;&#30830;&#23450;&#24615;&#21644;&#33258;&#30417;&#30563;&#26426;&#21046;&#30340;&#21487;&#38752;&#24615;&#12290;&#36890;&#36807;&#29702;&#35770;&#21644;&#23454;&#35777;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;RARE&#33021;&#22815;&#26377;&#25928;&#22320;&#25429;&#25417;&#22270;&#25968;&#25454;&#30340;&#20869;&#22312;&#32467;&#26500;&#65292;&#24182;&#22312;&#19981;&#21516;&#30340;&#19979;&#28216;&#20219;&#21153;&#20013;&#20248;&#20110;&#29616;&#26377;&#30340;SGP&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Masked graph autoencoder (MGAE) has emerged as a promising self-supervised graph pre-training (SGP) paradigm due to its simplicity and effectiveness. However, existing efforts perform the mask-then-reconstruct operation in the raw data space as is done in computer vision (CV) and natural language processing (NLP) areas, while neglecting the important non-Euclidean property of graph data. As a result, the highly unstable local connection structures largely increase the uncertainty in inferring masked data and decrease the reliability of the exploited self-supervision signals, leading to inferior representations for downstream evaluations. To address this issue, we propose a novel SGP method termed Robust mAsked gRaph autoEncoder (RARE) to improve the certainty in inferring masked data and the reliability of the self-supervision mechanism by further masking and reconstructing node samples in the high-order latent feature space. Through both theoretical and empirical analyses, we have dis
&lt;/p&gt;</description></item><item><title>OneShotSTL&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#20934;&#30830;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#65292;&#22312;&#22788;&#29702;&#26102;&#38388;&#19978;&#20165;&#38656;O(1)&#30340;&#26356;&#26032;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#21516;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25209;&#22788;&#29702;&#26041;&#27861;&#26080;&#27861;&#25903;&#25345;&#23454;&#26102;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2304.01506</link><description>&lt;p&gt;
OneShotSTL&#65306;&#19968;&#31181;&#21333;&#27425;&#23395;&#33410;&#36235;&#21183;&#20998;&#35299;&#26041;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
OneShotSTL: One-Shot Seasonal-Trend Decomposition For Online Time Series Anomaly Detection And Forecasting. (arXiv:2304.01506v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01506
&lt;/p&gt;
&lt;p&gt;
OneShotSTL&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#20934;&#30830;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#22312;&#32447;&#26102;&#38388;&#24207;&#21015;&#20998;&#35299;&#65292;&#22312;&#22788;&#29702;&#26102;&#38388;&#19978;&#20165;&#38656;O(1)&#30340;&#26356;&#26032;&#26102;&#38388;&#22797;&#26434;&#24230;&#65292;&#24182;&#21487;&#21516;&#26102;&#20445;&#25345;&#36739;&#39640;&#30340;&#31934;&#24230;&#65292;&#35299;&#20915;&#20102;&#29616;&#26377;&#25209;&#22788;&#29702;&#26041;&#27861;&#26080;&#27861;&#25903;&#25345;&#23454;&#26102;&#20998;&#26512;&#30340;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23395;&#33410;&#36235;&#21183;&#20998;&#35299;&#26159;&#26102;&#38388;&#24207;&#21015;&#20998;&#26512;&#20013;&#26368;&#22522;&#26412;&#30340;&#27010;&#24565;&#20043;&#19968;&#65292;&#23427;&#25903;&#25345;&#21253;&#25324;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#22312;&#20869;&#30340;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#20998;&#35299;&#26041;&#27861;&#20381;&#36182;&#20110;&#25209;&#22788;&#29702;&#65292;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(W)&#65292;&#20854;&#20013;W&#26159;&#26102;&#38388;&#31383;&#21475;&#20869;&#30340;&#25968;&#25454;&#28857;&#25968;&#12290;&#22240;&#27492;&#65292;&#23427;&#20204;&#19981;&#33021;&#22987;&#32456;&#26377;&#25928;&#22320;&#25903;&#25345;&#38656;&#35201;&#20302;&#22788;&#29702;&#24310;&#36831;&#30340;&#23454;&#26102;&#20998;&#26512;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#19968;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;OneShotSTL&#65292;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#21644;&#20934;&#30830;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#32447;&#19978;&#23545;&#26102;&#38388;&#24207;&#21015;&#36827;&#34892;&#20998;&#35299;&#65292;&#26356;&#26032;&#26102;&#38388;&#22797;&#26434;&#24230;&#20026;O(1)&#12290;OneShotSTL&#27604;&#25209;&#22788;&#29702;&#26041;&#27861;&#24555;10&#20493;&#20197;&#19978;&#65292;&#31934;&#24230;&#19982;&#26368;&#20339;&#23545;&#25163;&#30456;&#24403;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#22312;&#19979;&#28216;&#26102;&#38388;&#24207;&#21015;&#24322;&#24120;&#26816;&#27979;&#21644;&#39044;&#27979;&#20219;&#21153;&#30340;&#30495;&#23454;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#34920;&#26126;&#65292;OneShotSTL&#27604;&#29616;&#26377;&#25216;&#26415;&#24555;10&#20493;&#20197;&#19978;&#65292;&#21516;&#26102;&#20173;&#25552;&#20379;&#30456;&#24403;&#29978;&#33267;&#26356;&#22909;&#30340;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
Seasonal-trend decomposition is one of the most fundamental concepts in time series analysis that supports various downstream tasks, including time series anomaly detection and forecasting. However, existing decomposition methods rely on batch processing with a time complexity of O(W), where W is the number of data points within a time window. Therefore, they cannot always efficiently support real-time analysis that demands low processing delay. To address this challenge, we propose OneShotSTL, an efficient and accurate algorithm that can decompose time series online with an update time complexity of O(1). OneShotSTL is more than $1,000$ times faster than the batch methods, with accuracy comparable to the best counterparts. Extensive experiments on real-world benchmark datasets for downstream time series anomaly detection and forecasting tasks demonstrate that OneShotSTL is from 10 to over 1,000 times faster than the state-of-the-art methods, while still providing comparable or even be
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#35780;&#20272;&#20102; OpenAI &#30340; GPT &#22312;&#25972;&#24418;&#22806;&#31185;&#20303;&#38498;&#21307;&#24072;&#22521;&#35757;&#32771;&#35797;&#19978;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#30456;&#27604;&#20110; GPT-3.5&#65292;GPT-4 &#26377;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#32771;&#35797;&#24471;&#20998;&#19982;&#25104;&#20026;&#33719;&#24471;&#35748;&#35777;&#30340;&#25972;&#24418;&#22806;&#31185;&#21307;&#29983;&#25152;&#38656;&#30340;&#20070;&#38754;&#32771;&#35797;&#39640;&#24230;&#30456;&#20851;&#12290;</title><link>http://arxiv.org/abs/2304.01503</link><description>&lt;p&gt;
&#20174; GPT-4 &#21040; GPT-3.5&#65306;&#8220;&#25343;&#36215;&#25105;&#30340;&#25163;&#26415;&#20992;&#8221;&#8212;&#8212;&#35780;&#20272; OpenAI &#30340; GPT &#22312;&#25972;&#24418;&#22806;&#31185;&#20303;&#38498;&#21307;&#24072;&#22521;&#35757;&#32771;&#35797;&#20013;&#30340;&#33021;&#21147;
&lt;/p&gt;
&lt;p&gt;
GPT-4 to GPT-3.5: 'Hold My Scalpel' -- A Look at the Competency of OpenAI's GPT on the Plastic Surgery In-Service Training Exam. (arXiv:2304.01503v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01503
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#35780;&#20272;&#20102; OpenAI &#30340; GPT &#22312;&#25972;&#24418;&#22806;&#31185;&#20303;&#38498;&#21307;&#24072;&#22521;&#35757;&#32771;&#35797;&#19978;&#30340;&#33021;&#21147;&#65292;&#21457;&#29616;&#30456;&#27604;&#20110; GPT-3.5&#65292;GPT-4 &#26377;&#20102;&#26174;&#33879;&#25552;&#39640;&#12290;&#32771;&#35797;&#24471;&#20998;&#19982;&#25104;&#20026;&#33719;&#24471;&#35748;&#35777;&#30340;&#25972;&#24418;&#22806;&#31185;&#21307;&#29983;&#25152;&#38656;&#30340;&#20070;&#38754;&#32771;&#35797;&#39640;&#24230;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25972;&#24418;&#22806;&#31185;&#20303;&#38498;&#21307;&#24072;&#22521;&#35757;&#32771;&#35797; (PSITE) &#26159;&#35780;&#20272;&#20303;&#38498;&#21307;&#24072;&#29087;&#32451;&#31243;&#24230;&#30340;&#37325;&#35201;&#25351;&#26631;&#65292;&#20063;&#26159;&#35780;&#20272; OpenAI &#30340; GPT &#30340;&#26377;&#29992;&#22522;&#20934;&#12290;&#19982; GPT-4 &#25216;&#26415;&#25991;&#31456;&#20013;&#23637;&#31034;&#30340;&#35768;&#22810;&#27169;&#25311;&#27979;&#35797;&#25110;&#32451;&#20064;&#39064;&#19981;&#21516;&#65292;&#36825;&#37324;&#35780;&#20272;&#30340;&#22810;&#39033;&#36873;&#25321;&#39064;&#26159;&#30495;&#23454;&#30340; PSITE &#38382;&#39064;&#12290;&#36825;&#20123;&#38382;&#39064;&#25552;&#20379;&#20102;&#25972;&#24418;&#22806;&#31185;&#21307;&#29983;&#22312;&#23454;&#36341;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#29616;&#23454;&#20020;&#24202;&#26696;&#20363;&#65292;&#24471;&#20998;&#39640;&#24230;&#30456;&#20851;&#20110;&#36890;&#36807;&#25104;&#20026;&#33719;&#24471;&#35748;&#35777;&#30340;&#25972;&#24418;&#22806;&#31185;&#21307;&#29983;&#25152;&#38656;&#30340;&#20070;&#38754;&#32771;&#35797;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#26174;&#31034;&#65292;GPT-4&#65288;&#27809;&#26377;&#35270;&#35273;&#65289;&#30456;&#23545;&#20110; GPT-3.5 &#26377;&#20102;&#26174;&#33879;&#30340;&#25552;&#39640;&#65292;&#20998;&#21035;&#23558; 2022 &#24180;&#21644; 2021 &#24180;&#30340;&#32771;&#35797;&#24471;&#20998;&#20174;&#31532; 8 &#30334;&#20998;&#20301;&#25968;&#25552;&#39640;&#21040;&#31532; 88 &#30334;&#20998;&#20301;&#25968;&#21644;&#31532; 3 &#30334;&#20998;&#20301;&#25968;&#25552;&#39640;&#21040;&#31532; 99 &#30334;&#20998;&#20301;&#25968;&#12290;2023 &#24180;&#30340; PSITE &#30340;&#26368;&#32456;&#32467;&#26524;&#23558;&#20110; 2023 &#24180; 4 &#26376; 11 &#26085;&#20844;&#24067;&#65292;&#36825;&#26159;&#19968;&#20010;&#20196;&#20154;&#20852;&#22859;&#30340;&#26102;&#21051;&#65292;&#25105;&#20204;&#23558;&#32487;&#32493;&#36827;&#34892;&#30740;&#31350;&#24182;&#20351;&#29992;&#26032;&#30340;&#32771;&#35797;&#36827;&#34892;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#35780;&#20272;&#27969;&#31243;&#24050;&#20934;&#22791;&#23601;&#32490;&#65292;&#21482;&#35201;&#32771;&#35797;&#21457;&#24067;&#65292;&#25105;&#20204;&#23601;&#21487;&#20197;&#31435;&#21051;&#36827;&#34892;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Plastic Surgery In-Service Training Exam (PSITE) is an important indicator of resident proficiency and serves as a useful benchmark for evaluating OpenAI's GPT. Unlike many of the simulated tests or practice questions shown in the GPT-4 Technical Paper, the multiple-choice questions evaluated here are authentic PSITE questions. These questions offer realistic clinical vignettes that a plastic surgeon commonly encounters in practice and scores highly correlate with passing the written boards required to become a Board Certified Plastic Surgeon. Our evaluation shows dramatic improvement of GPT-4 (without vision) over GPT-3.5 with both the 2022 and 2021 exams respectively increasing the score from 8th to 88th percentile and 3rd to 99th percentile. The final results of the 2023 PSITE are set to be released on April 11, 2023, and this is an exciting moment to continue our research with a fresh exam. Our evaluation pipeline is ready for the moment that the exam is released so long as we 
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#35780;&#20272;&#20102;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#20854;&#20182;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01487</link><description>&lt;p&gt;
&#32842;&#22825;GPT&#65292;&#36824;&#26159;&#19981;&#32842;&#22825;GPT&#65306;&#36825;&#26159;&#19968;&#20010;&#38382;&#39064;&#65281;
&lt;/p&gt;
&lt;p&gt;
To ChatGPT, or not to ChatGPT: That is the question!. (arXiv:2304.01487v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01487
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#35780;&#20272;&#20102;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#30340;&#26368;&#26032;&#25216;&#26415;&#21644;&#20854;&#20182;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#20986;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#25991;&#26412;&#30340;&#37325;&#35201;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32842;&#22825;GPT&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#20840;&#29699;&#24863;&#30693;&#12290;&#38543;&#30528;&#32842;&#22825;GPT&#21644;&#20854;&#20182;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#20986;&#29616;&#65292;&#23545;&#20110;&#20182;&#20204;&#30340;&#35823;&#29992;&#30340;&#25285;&#24551;&#20063;&#22686;&#21152;&#20102;&#65292;&#20363;&#22914;&#20256;&#25773;&#34394;&#20551;&#28040;&#24687;&#65292;&#25220;&#34989;&#65292;&#25805;&#32437;&#20844;&#20247;&#33286;&#35770;&#65292;&#27450;&#39575;&#21644;&#27450;&#35784;&#12290;&#22240;&#27492;&#65292;&#21306;&#20998;&#20154;&#24037;&#29983;&#25104;&#21644;AI&#29983;&#25104;&#30340;&#25991;&#26412;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#30740;&#31350;&#20154;&#21592;&#25552;&#20986;&#20102;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#65292;&#20174;&#22522;&#26412;&#30340;&#20108;&#20803;&#20998;&#31867;&#22120;&#21040;&#26356;&#22797;&#26434;&#30340;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#12290;&#19968;&#20123;&#26816;&#27979;&#25216;&#26415;&#20381;&#36182;&#20110;&#32479;&#35745;&#29305;&#24449;&#25110;&#21477;&#27861;&#27169;&#24335;&#65292;&#32780;&#20854;&#20182;&#19968;&#20123;&#21017;&#21253;&#21547;&#35821;&#20041;&#25110;&#19978;&#19979;&#25991;&#20449;&#24687;&#20197;&#25552;&#39640;&#20934;&#30830;&#24615;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#26631;&#26159;&#23545;&#32842;&#22825;GPT&#26816;&#27979;&#20013;&#26368;&#26032;&#25216;&#26415;&#36827;&#34892;&#20840;&#38754;&#21644;&#29616;&#20195;&#21270;&#30340;&#35780;&#20272;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#20854;&#20182;&#26410;&#19987;&#38376;&#22768;&#31216;&#26816;&#27979;&#32842;&#22825;GPT&#29983;&#25104;&#20869;&#23481;&#30340;AI&#29983;&#25104;&#25991;&#26412;&#26816;&#27979;&#24037;&#20855;&#20197;&#35780;&#20272;&#23427;&#20204;&#22312;&#26816;&#27979;&#32842;&#22825;GPT&#29983;&#25104;&#20869;&#23481;&#26041;&#38754;&#30340;&#34920;&#29616;&#12290;&#22312;&#25105;&#20204;&#30340;&#35780;&#20272;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#19968;&#20010;&#21253;&#21547;&#20154;&#24037;&#32534;&#20889;&#21644;&#32842;&#22825;GPT&#29983;&#25104;&#30340;&#25991;&#26412;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
ChatGPT has become a global sensation. As ChatGPT and other Large Language Models (LLMs) emerge, concerns of misusing them in various ways increase, such as disseminating fake news, plagiarism, manipulating public opinion, cheating, and fraud. Hence, distinguishing AI-generated from human-generated becomes increasingly essential. Researchers have proposed various detection methodologies, ranging from basic binary classifiers to more complex deep-learning models. Some detection techniques rely on statistical characteristics or syntactic patterns, while others incorporate semantic or contextual information to improve accuracy. The primary objective of this study is to provide a comprehensive and contemporary assessment of the most recent techniques in ChatGPT detection. Additionally, we evaluated other AI-generated text detection tools that do not specifically claim to detect ChatGPT-generated content to assess their performance in detecting ChatGPT-generated content. For our evaluation,
&lt;/p&gt;</description></item><item><title>DLRover&#26159;&#19968;&#20010;&#33258;&#21160;&#37197;&#32622;&#21021;&#22987;&#36164;&#28304;&#24182;&#23454;&#26102;&#35843;&#25972;&#36164;&#28304;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36164;&#28304;&#20849;&#20139;&#21644;&#25163;&#21160;&#37197;&#32622;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01468</link><description>&lt;p&gt;
DLRover&#65306;&#19968;&#31181;&#20855;&#26377;&#33258;&#21160;&#20316;&#19994;&#36164;&#28304;&#25512;&#33616;&#30340;&#24377;&#24615;&#28145;&#24230;&#35757;&#32451;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
DLRover: An Elastic Deep Training Extension with Auto Job Resource Recommendation. (arXiv:2304.01468v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01468
&lt;/p&gt;
&lt;p&gt;
DLRover&#26159;&#19968;&#20010;&#33258;&#21160;&#37197;&#32622;&#21021;&#22987;&#36164;&#28304;&#24182;&#23454;&#26102;&#35843;&#25972;&#36164;&#28304;&#30340;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#65292;&#35299;&#20915;&#20102;&#36164;&#28304;&#20849;&#20139;&#21644;&#25163;&#21160;&#37197;&#32622;&#24102;&#26469;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#20113;&#24179;&#21488;&#19978;&#36827;&#34892;&#36164;&#28304;&#20849;&#20139;&#21487;&#20197;&#25552;&#39640;&#36164;&#28304;&#21033;&#29992;&#29575;&#24182;&#38477;&#20302;&#24635;&#20307;&#25104;&#26412;&#65292;&#22240;&#27492;&#20113;&#20173;&#28982;&#26159;&#20998;&#24067;&#24335;&#28145;&#24230;&#23398;&#20064;&#65288;DL&#65289;&#35757;&#32451;&#20316;&#19994;&#30340;&#27969;&#34892;&#24179;&#21488;&#12290;&#28982;&#32780;&#65292;&#27492;&#31867;&#20849;&#20139;&#20063;&#20026;DL&#35757;&#32451;&#20316;&#19994;&#24102;&#26469;&#20102;&#22810;&#37325;&#25361;&#25112;&#65292;&#20363;&#22914;&#39640;&#20248;&#20808;&#32423;&#20316;&#19994;&#21487;&#33021;&#20250;&#24433;&#21709;&#12289;&#29978;&#33267;&#20013;&#26029;&#20302;&#20248;&#20808;&#32423;&#20316;&#19994;&#12290;&#21516;&#26102;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#20998;&#24067;&#24335;DL&#35757;&#32451;&#31995;&#32479;&#35201;&#27714;&#29992;&#25143;&#22312;&#20316;&#19994;&#25552;&#20132;&#20043;&#21069;&#25163;&#21160;&#37197;&#32622;&#20316;&#19994;&#30340;&#36164;&#28304;&#65288;&#21363;&#20998;&#37197;&#32473;&#27599;&#20010;&#33410;&#28857;&#30340;&#33410;&#28857;&#25968;&#21644;CPU&#12289;&#20869;&#23384;&#31561;&#36164;&#28304;&#65289;&#65292;&#24182;&#19988;&#19981;&#33021;&#22312;&#36816;&#34892;&#26102;&#35843;&#25972;&#20316;&#19994;&#30340;&#36164;&#28304;&#12290;&#20316;&#19994;&#30340;&#36164;&#28304;&#37197;&#32622;&#20250;&#28145;&#21051;&#24433;&#21709;&#35813;&#20316;&#19994;&#30340;&#24615;&#33021;&#65288;&#20363;&#22914;&#35757;&#32451;&#21534;&#21520;&#37327;&#12289;&#36164;&#28304;&#21033;&#29992;&#29575;&#21644;&#23436;&#25104;&#29575;&#65289;&#12290;&#28982;&#32780;&#65292;&#36825;&#36890;&#24120;&#20250;&#23548;&#33268;&#20316;&#19994;&#24615;&#33021;&#19981;&#20339;&#65292;&#22240;&#20026;&#29992;&#25143;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#26080;&#27861;&#25552;&#20379;&#26368;&#20339;&#30340;&#36164;&#28304;&#37197;&#32622;&#12290;DLRover&#26159;&#19968;&#31181;&#20998;&#24067;&#24335;DL&#26694;&#26550;&#65292;&#21487;&#20197;&#33258;&#21160;&#37197;&#32622;DL&#20316;&#19994;&#30340;&#21021;&#22987;&#36164;&#28304;&#24182;&#21160;&#24577;&#35843;&#25972;&#20316;&#19994;&#30340;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
The cloud is still a popular platform for distributed deep learning (DL) training jobs since resource sharing in the cloud can improve resource utilization and reduce overall costs. However, such sharing also brings multiple challenges for DL training jobs, e.g., high-priority jobs could impact, even interrupt, low-priority jobs. Meanwhile, most existing distributed DL training systems require users to configure the resources (i.e., the number of nodes and resources like CPU and memory allocated to each node) of jobs manually before job submission and can not adjust the job's resources during the runtime. The resource configuration of a job deeply affect this job's performance (e.g., training throughput, resource utilization, and completion rate). However, this usually leads to poor performance of jobs since users fail to provide optimal resource configuration in most cases. \system~is a distributed DL framework can auto-configure a DL job's initial resources and dynamically tune the j
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#32593;&#32476;&#26550;&#26500;TSFF-Net&#65292;&#23558;&#26102;&#38388;-&#31354;&#38388;-&#39057;&#29575;&#29305;&#24449;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#21333;&#27169;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#25110;&#26102;&#38388;-&#39057;&#29575;&#27169;&#24577;&#19979;&#30340;&#38480;&#21046;&#12290;TSFF-Net&#21253;&#25324;&#26102;&#38388;-&#39057;&#29575;&#34920;&#31034;&#12289;&#26102;&#38388;-&#39057;&#29575;&#29305;&#24449;&#25552;&#21462;&#12289;&#26102;&#38388;-&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#21644;&#29305;&#24449;&#34701;&#21512;&#19982;&#20998;&#31867;&#22235;&#20010;&#20027;&#35201;&#32452;&#20214;&#12290;</title><link>http://arxiv.org/abs/2304.01461</link><description>&lt;p&gt;
&#26102;&#38388;-&#31354;&#38388;-&#39057;&#29575;&#29305;&#24449;&#34701;&#21512;&#30340;3&#36890;&#36947;&#36816;&#21160;&#24819;&#35937;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Time-space-frequency feature Fusion for 3-channel motor imagery classification. (arXiv:2304.01461v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01461
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#32593;&#32476;&#26550;&#26500;TSFF-Net&#65292;&#23558;&#26102;&#38388;-&#31354;&#38388;-&#39057;&#29575;&#29305;&#24449;&#34701;&#21512;&#65292;&#35299;&#20915;&#20102;&#21333;&#27169;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#22312;&#26102;&#38388;&#24207;&#21015;&#25110;&#26102;&#38388;-&#39057;&#29575;&#27169;&#24577;&#19979;&#30340;&#38480;&#21046;&#12290;TSFF-Net&#21253;&#25324;&#26102;&#38388;-&#39057;&#29575;&#34920;&#31034;&#12289;&#26102;&#38388;-&#39057;&#29575;&#29305;&#24449;&#25552;&#21462;&#12289;&#26102;&#38388;-&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#21644;&#29305;&#24449;&#34701;&#21512;&#19982;&#20998;&#31867;&#22235;&#20010;&#20027;&#35201;&#32452;&#20214;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20302;&#36890;&#36947;EEG&#35774;&#22791;&#23545;&#20110;&#20415;&#25658;&#21644;&#23089;&#20048;&#24212;&#29992;&#33267;&#20851;&#37325;&#35201;&#65292;&#28982;&#32780;EEG&#20302;&#31354;&#38388;&#20998;&#36776;&#29575;&#23384;&#22312;&#25361;&#25112;&#65292;&#38590;&#20197;&#35299;&#30721;&#20302;&#36890;&#36947;&#36816;&#21160;&#24819;&#35937;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;TSFF-Net&#65292;&#19968;&#31181;&#26032;&#39062;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#23558;&#26102;&#38388;-&#31354;&#38388;-&#39057;&#29575;&#29305;&#24449;&#34701;&#21512;&#65292;&#22312;&#26102;&#38388;&#24207;&#21015;&#25110;&#26102;&#38388;-&#39057;&#29575;&#27169;&#24577;&#19979;&#30340;&#21333;&#27169;&#29305;&#24449;&#25552;&#21462;&#32593;&#32476;&#30340;&#38480;&#21046;&#19979;&#21457;&#25381;&#20316;&#29992;&#12290;TSFF-Net&#21253;&#25324;&#22235;&#20010;&#20027;&#35201;&#32452;&#20214;&#65306;&#26102;&#38388;-&#39057;&#29575;&#34920;&#31034;&#12289;&#26102;&#38388;-&#39057;&#29575;&#29305;&#24449;&#25552;&#21462;&#12289;&#26102;&#38388;-&#31354;&#38388;&#29305;&#24449;&#25552;&#21462;&#21644;&#29305;&#24449;&#34701;&#21512;&#19982;&#20998;&#31867;&#12290;&#26102;&#38388;-&#39057;&#29575;&#34920;&#31034;&#21644;&#29305;&#24449;&#25552;&#21462;&#23558;&#21407;&#22987;EEG&#20449;&#21495;&#36716;&#25442;&#20026;&#26102;&#38388;-&#39057;&#29575;&#35889;&#22270;&#24182;&#25552;&#21462;&#30456;&#20851;&#29305;&#24449;&#12290;&#26102;&#31354;&#32593;&#32476;&#23558;&#26102;&#38388;&#24207;&#21015;EEG&#35797;&#39564;&#20316;&#20026;&#36755;&#20837;&#22788;&#29702;&#65292;&#24182;&#25552;&#21462;&#26102;&#38388;-&#31354;&#38388;&#29305;&#24449;&#12290;&#29305;&#24449;&#34701;&#21512;&#37319;&#29992;MMD&#25439;&#22833;&#22312;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#32422;&#26463;&#26102;&#38388;-&#39057;&#29575;&#21644;&#26102;&#38388;-&#31354;&#38388;&#29305;&#24449;&#30340;&#20998;&#24067;&#12290;
&lt;/p&gt;
&lt;p&gt;
Low-channel EEG devices are crucial for portable and entertainment applications. However, the low spatial resolution of EEG presents challenges in decoding low-channel motor imagery. This study introduces TSFF-Net, a novel network architecture that integrates time-space-frequency features, effectively compensating for the limitations of single-mode feature extraction networks based on time-series or time-frequency modalities. TSFF-Net comprises four main components: time-frequency representation, time-frequency feature extraction, time-space feature extraction, and feature fusion and classification. Time-frequency representation and feature extraction transform raw EEG signals into time-frequency spectrograms and extract relevant features. The time-space network processes time-series EEG trials as input and extracts temporal-spatial features. Feature fusion employs MMD loss to constrain the distribution of time-frequency and time-space features in the Reproducing Kernel Hilbert Space, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21521;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#21644;&#21033;&#29992;&#19981;&#24179;&#34913;&#31639;&#27861;&#26469;&#25913;&#36827;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#25913;&#36827;&#21518;&#30340;VLM&#22312;iNaturalist18&#12289;CIFAR-100&#21644;Visual Genome&#25968;&#25454;&#38598;&#19978;&#20998;&#31867;&#20934;&#30830;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#31867;&#65292;&#24615;&#33021;&#25552;&#21319;&#24456;&#22823;&#12290;</title><link>http://arxiv.org/abs/2304.01457</link><description>&lt;p&gt;
&#25506;&#32034;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#22312;&#19981;&#24179;&#34913;&#23398;&#20064;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Exploring Vision-Language Models for Imbalanced Learning. (arXiv:2304.01457v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01457
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#22914;&#20309;&#36890;&#36807;&#21521;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#28155;&#21152;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#21644;&#21033;&#29992;&#19981;&#24179;&#34913;&#31639;&#27861;&#26469;&#25913;&#36827;&#24615;&#33021;&#65292;&#23454;&#39564;&#34920;&#26126;&#25913;&#36827;&#21518;&#30340;VLM&#22312;iNaturalist18&#12289;CIFAR-100&#21644;Visual Genome&#25968;&#25454;&#38598;&#19978;&#20998;&#31867;&#20934;&#30830;&#24230;&#26174;&#33879;&#25552;&#39640;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#23569;&#25968;&#31867;&#65292;&#24615;&#33021;&#25552;&#21319;&#24456;&#22823;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#23545;&#27604;&#35821;&#35328;-&#22270;&#20687;&#39044;&#35757;&#32451;&#30340;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#65288;VLMs&#65289;&#24050;&#32463;&#26174;&#31034;&#20986;&#26377;&#24076;&#26395;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#34920;&#29616;&#12290;&#28982;&#32780;&#65292;&#22312;&#19981;&#24179;&#34913;&#25968;&#25454;&#38598;&#19978;&#65292;&#23427;&#20204;&#30340;&#24615;&#33021;&#30456;&#23545;&#36739;&#24046;&#65292;&#22312;&#35757;&#32451;&#25968;&#25454;&#38598;&#20013;&#31867;&#30340;&#20998;&#24067;&#20542;&#26012;&#65292;&#23548;&#33268;&#22312;&#39044;&#27979;&#23569;&#25968;&#31867;&#26041;&#38754;&#24615;&#33021;&#19981;&#20339;&#12290;&#25105;&#20204;&#25552;&#20986;&#21521;VLM&#28155;&#21152;&#36731;&#37327;&#32423;&#35299;&#30721;&#22120;&#65292;&#20197;&#36991;&#20813;&#30001;&#20110;&#22823;&#37327;&#31867;&#21035;&#23548;&#33268;&#30340;&#20869;&#23384;&#19981;&#36275;&#38382;&#39064;&#65292;&#24182;&#25429;&#25417;&#23614;&#37096;&#31867;&#21035;&#30340;&#24494;&#22937;&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#21033;&#29992;&#25552;&#31034;&#35843;&#25972;&#12289;&#24494;&#35843;&#20197;&#21450;&#21152;&#20837;&#19981;&#24179;&#34913;&#31639;&#27861;&#65288;&#20363;&#22914;Focal Loss&#12289;Balanced SoftMax&#21644;Distribution Alignment&#65289;&#26469;&#25913;&#36827;VLM&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#22312;&#20351;&#29992;&#35299;&#30721;&#22120;&#21644;&#19981;&#24179;&#34913;&#26041;&#27861;&#26102;&#65292;VLM&#30340;&#24615;&#33021;&#21487;&#20197;&#36827;&#19968;&#27493;&#25552;&#39640;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25913;&#36827;&#30340;VLM&#22312;iNaturalist18&#12289;CIFAR-100&#21644;Visual Genome&#25968;&#25454;&#38598;&#19978;&#30340;&#20998;&#31867;&#20934;&#30830;&#24230;&#24179;&#22343;&#25552;&#39640;&#20102;6.58%&#12289;69.82%&#21644;10.43%&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision-Language models (VLMs) that use contrastive language-image pre-training have shown promising zero-shot classification performance. However, their performance on imbalanced dataset is relatively poor, where the distribution of classes in the training dataset is skewed, leading to poor performance in predicting minority classes. For instance, CLIP achieved only 5% accuracy on the iNaturalist18 dataset. We propose to add a lightweight decoder to VLMs to avoid OOM (out of memory) problem caused by large number of classes and capture nuanced features for tail classes. Then, we explore improvements of VLMs using prompt tuning, fine-tuning, and incorporating imbalanced algorithms such as Focal Loss, Balanced SoftMax and Distribution Alignment. Experiments demonstrate that the performance of VLMs can be further boosted when used with decoder and imbalanced methods. Specifically, our improved VLMs significantly outperforms zero-shot classification by an average accuracy of 6.58%, 69.82%,
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OffPA2&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#31163;&#32447;&#31574;&#30053;&#34892;&#21160;&#39044;&#27979;&#26041;&#27861;&#26469;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23398;&#20064;&#39044;&#27979;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.01447</link><description>&lt;p&gt;
&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#31163;&#32447;&#31574;&#30053;&#34892;&#21160;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Off-Policy Action Anticipation in Multi-Agent Reinforcement Learning. (arXiv:2304.01447v1 [cs.MA])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01447
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;OffPA2&#30340;&#26032;&#26694;&#26550;&#65292;&#36890;&#36807;&#31163;&#32447;&#31574;&#30053;&#34892;&#21160;&#39044;&#27979;&#26041;&#27861;&#26469;&#25552;&#39640;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23398;&#20064;&#39044;&#27979;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23398;&#20064;&#39044;&#27979;&#26159;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#65288;MARL&#65289;&#20013;&#30340;&#19968;&#31181;&#25512;&#29702;&#33539;&#24335;&#65292;&#20854;&#20013;&#26234;&#33021;&#20307;&#39044;&#27979;&#20854;&#20182;&#26234;&#33021;&#20307;&#30340;&#23398;&#20064;&#27493;&#39588;&#65292;&#20197;&#25552;&#39640;&#23427;&#20204;&#20043;&#38388;&#30340;&#21512;&#20316;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#22522;&#20110;&#39640;&#38454;&#26799;&#24230;&#65288;HOG&#65289;&#26041;&#27861;&#22312;&#38750;&#21487;&#24494;&#20998;&#21338;&#24328;&#25110;&#29366;&#24577;&#31354;&#38388;&#36739;&#22823;&#30340;&#21338;&#24328;&#20013;&#25928;&#29575;&#20302;&#19979;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;OffPA2&#26694;&#26550;&#65292;&#21033;&#29992;&#31163;&#32447;&#31574;&#30053;&#34892;&#21160;&#39044;&#27979;&#26041;&#27861;&#26469;&#25552;&#39640;&#23398;&#20064;&#39044;&#27979;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning anticipation in Multi-Agent Reinforcement Learning (MARL) is a reasoning paradigm where agents anticipate the learning steps of other agents to improve cooperation among themselves. As MARL uses gradient-based optimization, learning anticipation requires using Higher-Order Gradients (HOG), with so-called HOG methods. Existing HOG methods are based on policy parameter anticipation, i.e., agents anticipate the changes in policy parameters of other agents. Currently, however, these existing HOG methods have only been applied to differentiable games or games with small state spaces. In this work, we demonstrate that in the case of non-differentiable games with large state spaces, existing HOG methods do not perform well and are inefficient due to their inherent limitations related to policy parameter anticipation and multiple sampling stages. To overcome these problems, we propose Off-Policy Action Anticipation (OffPA2), a novel framework that approaches learning anticipation thro
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#35268;&#33539;&#21270;&#34920;&#31034;&#30340;von Neumann&#29109;( VNE ) &#26469;&#25913;&#21892;&#28145;&#24230;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25805;&#32437;&#29305;&#24449;&#20540;&#20998;&#24067;&#26469;&#20248;&#21270;&#34920;&#31034;&#21697;&#36136;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#20854;&#39046;&#22495;&#36890;&#29992;&#24615;&#12289;&#20803;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#27169;&#22411;&#31561;&#26041;&#38754;&#12290;</title><link>http://arxiv.org/abs/2304.01434</link><description>&lt;p&gt;
VNE: &#36890;&#36807;&#25805;&#32437;&#29305;&#24449;&#20540;&#20998;&#24067;&#26469;&#25552;&#39640;&#28145;&#24230;&#34920;&#31034;&#30340;&#26377;&#25928;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
VNE: An Effective Method for Improving Deep Representation by Manipulating Eigenvalue Distribution. (arXiv:2304.01434v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01434
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#36890;&#36807;&#35268;&#33539;&#21270;&#34920;&#31034;&#30340;von Neumann&#29109;( VNE ) &#26469;&#25913;&#21892;&#28145;&#24230;&#34920;&#31034;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#25805;&#32437;&#29305;&#24449;&#20540;&#20998;&#24067;&#26469;&#20248;&#21270;&#34920;&#31034;&#21697;&#36136;&#65292;&#24191;&#27867;&#36866;&#29992;&#20110;&#19981;&#21516;&#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22686;&#24378;&#20854;&#39046;&#22495;&#36890;&#29992;&#24615;&#12289;&#20803;&#23398;&#20064;&#12289;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#27169;&#22411;&#31561;&#26041;&#38754;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#20174;&#28145;&#24230;&#23398;&#20064;&#34987;&#24341;&#20837;&#20197;&#26469;&#65292;&#24456;&#22810;&#34920;&#31034;&#29305;&#24615; (&#22914;&#21435;&#30456;&#20851;&#12289;&#30333;&#21270;&#12289;&#35299;&#32544;&#12289;&#31209;&#12289;&#31561;&#24230;&#24615;&#21644;&#20114;&#20449;&#24687;) &#24050;&#32463;&#34987;&#30740;&#31350;&#20986;&#26469;&#65292;&#20197;&#25552;&#39640;&#34920;&#31034;&#21697;&#36136;&#12290;&#28982;&#32780;&#65292;&#25805;&#32437;&#36825;&#20123;&#29305;&#24615;&#22312;&#23454;&#29616;&#26377;&#25928;&#24615;&#21644;&#26222;&#36866;&#36866;&#29992;&#24615;&#26041;&#38754;&#37117;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#23545;&#34920;&#31034;&#30340;von Neumann&#29109;(VNE)&#36827;&#34892;&#35268;&#33539;&#21270;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;VNE&#30340;&#25968;&#23398;&#34920;&#36848;&#22312;&#26377;&#25928;&#25805;&#32437;&#34920;&#31034;&#33258;&#30456;&#20851;&#30697;&#38453;&#30340;&#29305;&#24449;&#20540;&#26041;&#38754;&#26159;&#20248;&#36234;&#30340;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#35843;&#26597;&#39046;&#22495;&#36890;&#29992;&#24615;&#65292;&#20803;&#23398;&#20064;&#65292;&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#29983;&#25104;&#27169;&#22411;&#31561;&#26041;&#38754;&#65292;&#35777;&#26126;&#20102;&#23427;&#22312;&#25552;&#39640;&#29616;&#26377;&#20808;&#36827;&#31639;&#27861;&#25110;&#27969;&#34892;&#22522;&#20934;&#31639;&#27861;&#20013;&#30340;&#24191;&#27867;&#36866;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#22312;&#29702;&#35770;&#19978;&#24314;&#31435;&#20102;&#34920;&#31034;&#30340;&#31209;&#12289;&#35299;&#32544;&#21644;&#31561;&#24230;&#24615;&#30340;&#32852;&#31995;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#35752;&#35770;&#12290;
&lt;/p&gt;
&lt;p&gt;
Since the introduction of deep learning, a wide scope of representation properties, such as decorrelation, whitening, disentanglement, rank, isotropy, and mutual information, have been studied to improve the quality of representation. However, manipulating such properties can be challenging in terms of implementational effectiveness and general applicability. To address these limitations, we propose to regularize von Neumann entropy~(VNE) of representation. First, we demonstrate that the mathematical formulation of VNE is superior in effectively manipulating the eigenvalues of the representation autocorrelation matrix. Then, we demonstrate that it is widely applicable in improving state-of-the-art algorithms or popular benchmark algorithms by investigating domain-generalization, meta-learning, self-supervised learning, and generative models. In addition, we formally establish theoretical connections with rank, disentanglement, and isotropy of representation. Finally, we provide discuss
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#25913;&#36827;&#26041;&#27861;&#65306;&#19968;&#20010;&#22810;&#27493;&#30340;Frank-Wolfe&#26041;&#27861;&#65292;&#30452;&#25509;&#24212;&#29992;&#20248;&#21270;&#30340;&#39640;&#38454;&#31163;&#25955;&#21270;&#26041;&#26696;&#65307;&#20197;&#21450;&#19968;&#31181;&#20855;&#26377;&#36739;&#23569;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;LMO-&#24179;&#22343;&#26041;&#26696;&#65292;&#20854;&#25910;&#25947;&#36895;&#29575;&#21152;&#36895;&#21040;$O(1/k^{3/2})$&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;Frank-Wolfe&#26041;&#27861;&#20013;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2304.01432</link><description>&lt;p&gt;
&#38477;&#20302;Frank-Wolfe&#26041;&#27861;&#20013;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;
&lt;/p&gt;
&lt;p&gt;
Reducing Discretization Error in the Frank-Wolfe Method. (arXiv:2304.01432v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01432
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#25913;&#36827;&#26041;&#27861;&#65306;&#19968;&#20010;&#22810;&#27493;&#30340;Frank-Wolfe&#26041;&#27861;&#65292;&#30452;&#25509;&#24212;&#29992;&#20248;&#21270;&#30340;&#39640;&#38454;&#31163;&#25955;&#21270;&#26041;&#26696;&#65307;&#20197;&#21450;&#19968;&#31181;&#20855;&#26377;&#36739;&#23569;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;LMO-&#24179;&#22343;&#26041;&#26696;&#65292;&#20854;&#25910;&#25947;&#36895;&#29575;&#21152;&#36895;&#21040;$O(1/k^{3/2})$&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#35299;&#20915;&#20102;Frank-Wolfe&#26041;&#27861;&#20013;&#30340;&#31163;&#25955;&#21270;&#35823;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Frank-Wolfe&#31639;&#27861;&#26159;&#32467;&#26500;&#21463;&#38480;&#26426;&#22120;&#23398;&#20064;&#24212;&#29992;&#20013;&#24120;&#29992;&#30340;&#26041;&#27861;&#65292;&#22240;&#20854;&#24555;&#36895;&#36845;&#20195;&#22797;&#26434;&#24230;&#32780;&#21463;&#27426;&#36814;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#30340;&#19968;&#20010;&#20027;&#35201;&#38480;&#21046;&#26159;&#25910;&#25947;&#36895;&#24230;&#32531;&#24930;&#65292;&#30001;&#20110;&#27493;&#38271;&#26041;&#21521;&#30340;&#19981;&#35268;&#21017;&#38663;&#33633;&#32780;&#38590;&#20197;&#21152;&#36895;&#65292;&#21363;&#20351;&#22312;&#25509;&#36817;&#35299;&#30340;&#28176;&#36817;&#24773;&#20917;&#19979;&#20063;&#26159;&#22914;&#27492;&#12290;&#25105;&#20204;&#35748;&#20026;&#36825;&#26159;&#31163;&#25955;&#21270;&#30340;&#20135;&#29289;&#65307;&#20063;&#23601;&#26159;&#35828;&#65292;Frank-Wolfe&#30340;&#27969;&#65288;&#21363;&#28176;&#36817;&#23567;&#27493;&#38271;&#24773;&#20917;&#19979;&#30340;&#36712;&#36857;&#65289;&#19981;&#20250;&#20986;&#29616;&#19981;&#35268;&#21017;&#38663;&#33633;&#65292;&#22240;&#27492;&#20943;&#23569;&#31163;&#25955;&#21270;&#35823;&#24046;&#23558;&#19982;&#20135;&#29983;&#26356;&#31283;&#23450;&#30340;&#26041;&#27861;&#21644;&#26356;&#22909;&#30340;&#25910;&#25947;&#29305;&#24615;&#30456;&#36741;&#30456;&#25104;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#20010;&#25913;&#36827;&#65306;&#19968;&#20010;&#22810;&#27493;Frank-Wolfe&#26041;&#27861;&#65292;&#30452;&#25509;&#24212;&#29992;&#20248;&#21270;&#30340;&#39640;&#38454;&#31163;&#25955;&#21270;&#26041;&#26696;&#65307;&#21644;&#19968;&#20010;&#20855;&#26377;&#38477;&#20302;&#31163;&#25955;&#21270;&#35823;&#24046;&#30340;LMO-&#24179;&#22343;&#26041;&#26696;&#65292;&#20854;&#22312;&#19968;&#33324;&#20984;&#38598;&#19978;&#30340;&#23616;&#37096;&#25910;&#25947;&#36895;&#29575;&#20174;$O(1/k)$&#21152;&#36895;&#21040;$O(1/k^{3/2})$ &#12290;
&lt;/p&gt;
&lt;p&gt;
The Frank-Wolfe algorithm is a popular method in structurally constrained machine learning applications, due to its fast per-iteration complexity. However, one major limitation of the method is a slow rate of convergence that is difficult to accelerate due to erratic, zig-zagging step directions, even asymptotically close to the solution. We view this as an artifact of discretization; that is to say, the Frank-Wolfe \emph{flow}, which is its trajectory at asymptotically small step sizes, does not zig-zag, and reducing discretization error will go hand-in-hand in producing a more stabilized method, with better convergence properties. We propose two improvements: a multistep Frank-Wolfe method that directly applies optimized higher-order discretization schemes; and an LMO-averaging scheme with reduced discretization error, and whose local convergence rate over general convex sets accelerates from a rate of $O(1/k)$ to up to $O(1/k^{3/2})$.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#19978;&#19979;&#25991;&#20998;&#38548;&#30340;&#27133;&#32467;&#26500;&#26469;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#65292;&#24182;&#29992;&#23545;&#25239;&#24615;&#26631;&#20934;&#26469;&#20445;&#35777;&#35299;&#30721;&#22120;&#26080;&#27861;&#37325;&#26500;&#25972;&#20010;&#20809;&#27969;&#12290;</title><link>http://arxiv.org/abs/2304.01430</link><description>&lt;p&gt;
&#20998;&#31163;&#30340;&#20851;&#27880;&#21147;&#65306;&#22522;&#20110;&#19978;&#19979;&#25991;&#20998;&#31163;&#27133;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;
&lt;/p&gt;
&lt;p&gt;
Divided Attention: Unsupervised Multi-Object Discovery with Contextually Separated Slots. (arXiv:2304.01430v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01430
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#22810;&#23545;&#35937;&#21457;&#29616;&#26041;&#27861;&#65292;&#36890;&#36807;&#19968;&#31181;&#19978;&#19979;&#25991;&#20998;&#38548;&#30340;&#27133;&#32467;&#26500;&#26469;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#65292;&#24182;&#29992;&#23545;&#25239;&#24615;&#26631;&#20934;&#26469;&#20445;&#35777;&#35299;&#30721;&#22120;&#26080;&#27861;&#37325;&#26500;&#25972;&#20010;&#20809;&#27969;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#35270;&#35273;&#22330;&#20998;&#21106;&#20026;&#29420;&#31435;&#36816;&#21160;&#21306;&#22495;&#30340;&#26041;&#27861;&#65292;&#19981;&#38656;&#35201;&#20219;&#20309;&#22522;&#30784;&#30495;&#20540;&#25110;&#30417;&#30563;&#12290;&#23427;&#30001;&#22522;&#20110;&#27133;&#20851;&#27880;&#30340;&#23545;&#25239;&#26465;&#20214;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26550;&#26500;&#32452;&#25104;&#65292;&#20462;&#25913;&#20026;&#20351;&#29992;&#22270;&#20687;&#20316;&#20026;&#19978;&#19979;&#25991;&#26469;&#35299;&#30721;&#20809;&#27969;&#65292;&#32780;&#19981;&#26159;&#23581;&#35797;&#37325;&#26500;&#22270;&#20687;&#26412;&#36523;&#12290;&#22312;&#32467;&#26524;&#30340;&#22810;&#27169;&#24335;&#34920;&#31034;&#20013;&#65292;&#19968;&#31181;&#27169;&#24335;&#65288;&#27969;&#65289;&#23558;&#39304;&#36865;&#32473;&#32534;&#30721;&#22120;&#20197;&#20135;&#29983;&#21333;&#29420;&#30340;&#28508;&#22312;&#20195;&#30721;&#65288;&#27133;&#65289;&#65292;&#32780;&#21478;&#19968;&#31181;&#27169;&#24335;&#65288;&#22270;&#20687;&#65289;&#23558;&#20915;&#23450;&#35299;&#30721;&#22120;&#20174;&#27133;&#29983;&#25104;&#31532;&#19968;&#20010;&#27169;&#24335;&#65288;&#27969;&#65289;&#12290;&#30001;&#20110;&#24815;&#24120;&#30340;&#33258;&#32534;&#30721;&#22522;&#20110;&#26368;&#23567;&#21270;&#37325;&#26500;&#35823;&#24046;&#65292;&#24182;&#19981;&#33021;&#38450;&#27490;&#25972;&#20010;&#27969;&#34987;&#32534;&#30721;&#21040;&#19968;&#20010;&#27133;&#20013;&#65292;&#22240;&#27492;&#25105;&#20204;&#23558;&#25439;&#22833;&#20462;&#25913;&#20026;&#22522;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#20998;&#31163;&#30340;&#23545;&#25239;&#24615;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a method to segment the visual field into independently moving regions, trained with no ground truth or supervision. It consists of an adversarial conditional encoder-decoder architecture based on Slot Attention, modified to use the image as context to decode optical flow without attempting to reconstruct the image itself. In the resulting multi-modal representation, one modality (flow) feeds the encoder to produce separate latent codes (slots), whereas the other modality (image) conditions the decoder to generate the first (flow) from the slots. This design frees the representation from having to encode complex nuisance variability in the image due to, for instance, illumination and reflectance properties of the scene. Since customary autoencoding based on minimizing the reconstruction error does not preclude the entire flow from being encoded into a single slot, we modify the loss to an adversarial criterion based on Contextual Information Separation. The resulting min-m
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;SoRTS&#65292;&#29992;&#20110;&#22312;&#20849;&#20139;&#31354;&#22495;&#20013;&#23454;&#29616;&#31038;&#20132;&#26426;&#22120;&#20154;&#38271;&#26399;&#23548;&#33322;&#65292;&#24182;&#36890;&#36807;FAA&#35748;&#35777;&#39134;&#34892;&#21592;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#34920;&#29616;&#19982;&#19968;&#21517;&#29087;&#32451;&#30340;&#20154;&#31867;&#39134;&#34892;&#21592;&#30456;&#24403;&#65292;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01428</link><description>&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;&#29992;&#20110;&#22312;&#20849;&#20139;&#31354;&#22495;&#20013;&#23454;&#29616;&#31038;&#20132;&#26426;&#22120;&#20154;&#38271;&#26399;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Learned Tree Search for Long-Horizon Social Robot Navigation in Shared Airspace. (arXiv:2304.01428v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01428
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23398;&#20064;&#30340;&#26641;&#25628;&#32034;&#31639;&#27861;SoRTS&#65292;&#29992;&#20110;&#22312;&#20849;&#20139;&#31354;&#22495;&#20013;&#23454;&#29616;&#31038;&#20132;&#26426;&#22120;&#20154;&#38271;&#26399;&#23548;&#33322;&#65292;&#24182;&#36890;&#36807;FAA&#35748;&#35777;&#39134;&#34892;&#21592;&#30340;&#35780;&#20272;&#35777;&#26126;&#20102;&#20854;&#34920;&#29616;&#19982;&#19968;&#21517;&#29087;&#32451;&#30340;&#20154;&#31867;&#39134;&#34892;&#21592;&#30456;&#24403;&#65292;&#26126;&#26174;&#20248;&#20110;&#22522;&#20934;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#23545;&#26080;&#20154;&#26426;&#22312;&#25317;&#25380;&#21160;&#24577;&#30340;&#20849;&#20139;&#31354;&#38388;&#20013;&#36827;&#34892;&#33258;&#20027;&#25805;&#20316;&#30340;&#38656;&#27714;&#36805;&#36895;&#22686;&#38271;&#65292;&#22240;&#27492;&#38656;&#35201;&#24320;&#21457;&#21487;&#20449;&#30340;&#20195;&#29702;&#31243;&#24207;&#20197;&#23454;&#29616;&#26080;&#32541;&#23433;&#20840;&#23548;&#33322;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Social Robot Tree Search (SoRTS)&#65292;&#19968;&#31181;&#29992;&#20110;&#22312;&#31038;&#20132;&#39046;&#22495;&#20013;&#31227;&#21160;&#26426;&#22120;&#20154;&#23433;&#20840;&#23548;&#33322;&#30340;&#31639;&#27861;&#12290;SoRTS&#26088;&#22312;&#36890;&#36807;Monte Carlo Tree Search&#35268;&#21010;&#22120;&#22686;&#24378;&#29616;&#26377;&#30340;&#31038;&#20132;&#24863;&#30693;&#36712;&#36857;&#39044;&#27979;&#31574;&#30053;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#19979;&#28216;&#23548;&#33322;&#25928;&#26524;&#12290;&#20026;&#20102;&#35780;&#20272;&#25105;&#20204;&#26041;&#27861;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#36873;&#25321;&#20102;&#19968;&#33324;&#33322;&#31354;&#39046;&#22495;&#30340;&#31038;&#20132;&#23548;&#33322;&#24212;&#29992;&#12290;&#20026;&#20102;&#20419;&#36827;&#36825;&#19968;&#35780;&#20272;&#65292;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#39640;&#20445;&#30495;&#24230;&#30340;X-Plane ROS&#65288;&#26426;&#36733;&#25805;&#20316;&#31995;&#32479;&#65289;&#39134;&#34892;&#27169;&#25311;&#22120;&#65292;&#20197;&#23454;&#29616;&#22312;&#23436;&#20840;&#33258;&#20027;&#25805;&#20316;&#19978;&#30340;&#26356;&#22810;&#30740;&#31350;&#12290;&#36890;&#36807;&#23545;26&#21517;FAA&#35748;&#35777;&#39134;&#34892;&#21592;&#30340;&#34892;&#19994;&#35780;&#20272;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;SoRTS&#30340;&#34920;&#29616;&#19982;&#19968;&#21517;&#29087;&#32451;&#30340;&#20154;&#31867;&#39134;&#34892;&#21592;&#30456;&#24403;&#65292;&#26126;&#26174;&#20248;&#20110;&#25105;&#20204;&#30340;&#22522;&#20934;&#31639;&#27861;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#34917;&#20805;&#20102;&#36825;&#20123;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
The fast-growing demand for fully autonomous aerial operations in shared spaces necessitates developing trustworthy agents that can safely and seamlessly navigate in crowded, dynamic spaces. In this work, we propose Social Robot Tree Search (SoRTS), an algorithm for the safe navigation of mobile robots in social domains. SoRTS aims to augment existing socially-aware trajectory prediction policies with a Monte Carlo Tree Search planner for improved downstream navigation of mobile robots. To evaluate the performance of our method, we choose the use case of social navigation for general aviation. To aid this evaluation, within this work, we also introduce X-PlaneROS, a high-fidelity aerial simulator, to enable more research in full-scale aerial autonomy. By conducting a user study based on the assessments of 26 FAA certified pilots, we show that SoRTS performs comparably to a competent human pilot, significantly outperforming our baseline algorithm. We further complement these results wit
&lt;/p&gt;</description></item><item><title>&#20108;&#21313;&#20301;&#31185;&#23398;&#23478;&#23601;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20854;&#23398;&#31185;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#24182;&#21457;&#29616;&#23427;&#26377;&#21161;&#20110;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#21644;&#25552;&#39640;&#31185;&#23398;&#30740;&#31350;&#30340;&#25945;&#32946;&#21644;&#27807;&#36890;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2304.01420</link><description>&lt;p&gt;
&#31185;&#23398;&#23478;&#23545;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20854;&#39046;&#22495;&#20013;&#28508;&#22312;&#24212;&#29992;&#30340;&#30475;&#27861;
&lt;/p&gt;
&lt;p&gt;
Scientists' Perspectives on the Potential for Generative AI in their Fields. (arXiv:2304.01420v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01420
&lt;/p&gt;
&lt;p&gt;
&#20108;&#21313;&#20301;&#31185;&#23398;&#23478;&#23601;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#20854;&#23398;&#31185;&#20013;&#30340;&#24212;&#29992;&#36827;&#34892;&#20102;&#35752;&#35770;&#65292;&#24182;&#21457;&#29616;&#23427;&#26377;&#21161;&#20110;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#21644;&#25552;&#39640;&#31185;&#23398;&#30740;&#31350;&#30340;&#25945;&#32946;&#21644;&#27807;&#36890;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#27169;&#22411;&#65292;&#21253;&#25324;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#21253;&#25324;&#25991;&#26412;&#21644;&#20854;&#20182;&#23186;&#20307;&#30340;&#22810;&#27169;&#22411;&#65292;&#21363;&#23558;&#25913;&#21464;&#29616;&#20195;&#29983;&#27963;&#30340;&#35768;&#22810;&#26041;&#38754;&#65292;&#21253;&#25324;&#23089;&#20048;&#12289;&#25945;&#32946;&#12289;&#20844;&#27665;&#29983;&#27963;&#12289;&#33402;&#26415;&#21644;&#19968;&#31995;&#21015;&#32844;&#19994;&#12290;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#26377;&#28508;&#21147;&#23545;&#35768;&#22810;&#31185;&#23398;&#23398;&#31185;&#30340;&#26041;&#27861;&#21644;&#21457;&#29616;&#36895;&#24230;&#20135;&#29983;&#23454;&#36136;&#24615;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#37319;&#35775;&#20102;&#26469;&#33258;&#21508;&#20010;&#39046;&#22495;&#30340;&#20108;&#21313;&#20301;&#31185;&#23398;&#23478;&#65288;&#21253;&#25324;&#29289;&#29702;&#12289;&#29983;&#21629;&#21644;&#31038;&#20250;&#31185;&#23398;&#65289;&#65292;&#20197;&#20102;&#35299;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#26159;&#21542;&#25110;&#22914;&#20309;&#33021;&#22815;&#22686;&#21152;&#20854;&#21508;&#33258;&#23398;&#31185;&#30340;&#23454;&#36341;&#30340;&#20215;&#20540;&#65292;&#21253;&#25324;AI&#21487;&#33021;&#21152;&#36895;&#31185;&#23398;&#21457;&#29616;&#65288;&#21363;&#30740;&#31350;&#65289;&#30340;&#26041;&#24335;&#65292;&#20197;&#21450;&#20854;&#32844;&#19994;&#30340;&#20854;&#20182;&#26041;&#38754;&#65292;&#21253;&#25324;&#22521;&#20859;&#26410;&#26469;&#23398;&#32773;&#21644;&#20256;&#36798;&#31185;&#23398;&#21457;&#29616;&#12290;&#38500;&#20102;&#30830;&#23450;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22686;&#24378;&#31185;&#23398;&#23478;&#24403;&#21069;&#23454;&#36341;&#30340;&#26426;&#20250;&#22806;&#65292;&#25105;&#20204;&#36824;&#35201;&#27714;&#21442;&#19982;&#32773;&#21453;&#24605;
&lt;/p&gt;
&lt;p&gt;
Generative AI models, including large language models and multimodal models that include text and other media, are on the cusp of transforming many aspects of modern life, including entertainment, education, civic life, the arts, and a range of professions. There is potential for Generative AI to have a substantive impact on the methods and pace of discovery for a range of scientific disciplines. We interviewed twenty scientists from a range of fields (including the physical, life, and social sciences) to gain insight into whether or how Generative AI technologies might add value to the practice of their respective disciplines, including not only ways in which AI might accelerate scientific discovery (i.e., research), but also other aspects of their profession, including the education of future scholars and the communication of scientific findings. In addition to identifying opportunities for Generative AI to augment scientists' current practices, we also asked participants to reflect 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35757;&#32451;&#29615;&#22659;CyGIL&#65292;&#20801;&#35768;&#32593;&#32476;&#23433;&#20840;&#25805;&#20316;&#33258;&#20027;&#20195;&#29702;&#35757;&#32451;&#12290;&#35813;&#29615;&#22659;&#36890;&#36807;&#23545;&#20110;&#30495;&#23454;&#32593;&#32476;&#30340;&#20223;&#30495;&#19982;&#34394;&#25311;&#29615;&#22659;&#19979;&#22823;&#37327;&#35757;&#32451;&#21608;&#26399;&#30340;&#24179;&#34913;&#65292;&#25104;&#21151;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#20986;&#20102;&#20840;&#38754;&#24615;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#20026;&#23454;&#29616;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;&#23433;&#20840;&#30340;RL&#20195;&#29702;&#25552;&#20379;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2304.01366</link><description>&lt;p&gt;
&#23454;&#29616;&#32593;&#32476;AI Gym&#65292;&#29992;&#20110;&#33258;&#20027;&#32593;&#32476;&#23433;&#20840;&#25805;&#20316;
&lt;/p&gt;
&lt;p&gt;
Enabling A Network AI Gym for Autonomous Cyber Agents. (arXiv:2304.01366v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01366
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35757;&#32451;&#29615;&#22659;CyGIL&#65292;&#20801;&#35768;&#32593;&#32476;&#23433;&#20840;&#25805;&#20316;&#33258;&#20027;&#20195;&#29702;&#35757;&#32451;&#12290;&#35813;&#29615;&#22659;&#36890;&#36807;&#23545;&#20110;&#30495;&#23454;&#32593;&#32476;&#30340;&#20223;&#30495;&#19982;&#34394;&#25311;&#29615;&#22659;&#19979;&#22823;&#37327;&#35757;&#32451;&#21608;&#26399;&#30340;&#24179;&#34913;&#65292;&#25104;&#21151;&#30701;&#26102;&#38388;&#20869;&#35757;&#32451;&#20986;&#20102;&#20840;&#38754;&#24615;&#30340;&#20915;&#31574;&#33021;&#21147;&#65292;&#20026;&#23454;&#29616;&#30495;&#23454;&#19990;&#30028;&#32593;&#32476;&#23433;&#20840;&#30340;RL&#20195;&#29702;&#25552;&#20379;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26088;&#22312;&#23454;&#29616;&#24212;&#29992;&#24378;&#21270;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL/DRL&#65289;&#30340;&#32593;&#32476;&#23433;&#20840;&#25805;&#20316;&#65288;CyOps&#65289;&#33258;&#20027;&#20195;&#29702;&#12290;&#25152;&#38656;&#30340;RL&#35757;&#32451;&#29615;&#22659;&#29305;&#21035;&#20855;&#26377;&#25361;&#25112;&#24615;&#65292;&#22240;&#20026;&#23427;&#24517;&#39035;&#24179;&#34913;&#38656;&#35201;&#39640;&#20445;&#30495;&#24230;&#30340;&#30495;&#23454;&#32593;&#32476;&#20223;&#30495;&#21644;&#36816;&#34892;&#22823;&#37327;&#35757;&#32451;&#21608;&#26399;&#30340;&#38656;&#27714;&#65292;&#26368;&#22909;&#20351;&#29992;&#27169;&#25311;&#22120;&#12290;&#24320;&#21457;&#20102;&#19968;&#20010;&#32479;&#19968;&#30340;&#35757;&#32451;&#29615;&#22659;&#65292;&#21363;&#26234;&#33021;&#23398;&#20064;&#30340;&#32593;&#32476;&#23433;&#20840;&#20581;&#36523;&#25151;(CyGIL)&#65292;&#20854;&#20013;&#27169;&#25311;&#30340;CyGIL-S&#26159;&#30001;&#27169;&#25311;&#30340;CyGIL-E&#33258;&#21160;&#29983;&#25104;&#30340;&#12290;&#20174;&#21021;&#27493;&#23454;&#39564;&#32467;&#26524;&#26469;&#30475;&#65292;CyGIL-S&#33021;&#22815;&#22312;&#20960;&#20998;&#38047;&#20869;&#35757;&#32451;&#20195;&#29702;&#65292;&#32780;&#22312;CyGIL-E&#20013;&#38656;&#35201;&#25968;&#22825;&#12290;&#22312;CyGIL-S&#20013;&#35757;&#32451;&#30340;&#20195;&#29702;&#21487;&#20197;&#30452;&#25509;&#36716;&#31227;&#21040;CyGIL-E&#65292;&#26174;&#31034;&#20102;&#22312;&#27169;&#25311;&#30340;&#8220;&#30495;&#23454;&#8221;&#32593;&#32476;&#20013;&#30340;&#23436;&#20840;&#20915;&#31574;&#32452;&#32455;&#33021;&#21147;&#12290;&#36890;&#36807;&#23454;&#29616;&#31163;&#32447;RL&#65292;CyGIL&#35299;&#20915;&#26041;&#26696;&#20026;&#21033;&#29992;RL&#20195;&#29702;&#22312;&#30495;&#23454;&#19990;&#30028;&#30340;&#32593;&#32476;&#20013;&#25552;&#20379;&#20102;&#19968;&#20010;&#26377;&#21069;&#36884;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work aims to enable autonomous agents for network cyber operations (CyOps) by applying reinforcement and deep reinforcement learning (RL/DRL). The required RL training environment is particularly challenging, as it must balance the need for high-fidelity, best achieved through real network emulation, with the need for running large numbers of training episodes, best achieved using simulation. A unified training environment, namely the Cyber Gym for Intelligent Learning (CyGIL) is developed where an emulated CyGIL-E automatically generates a simulated CyGIL-S. From preliminary experimental results, CyGIL-S is capable to train agents in minutes compared with the days required in CyGIL-E. The agents trained in CyGIL-S are transferrable directly to CyGIL-E showing full decision proficiency in the emulated "real" network. Enabling offline RL, the CyGIL solution presents a promising direction towards sim-to-real for leveraging RL agents in real-world cyber networks.
&lt;/p&gt;</description></item><item><title>Ada-SpikeDeep-Classifier&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#33041;&#26426;&#25509;&#21475;&#20449;&#21495;&#22788;&#29702;&#30340;&#33258;&#36866;&#24212;&#33258;&#32452;&#32455;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#20102;SpikeDeeptector&#36827;&#34892;&#20449;&#36947;&#36873;&#25321;&#12289;Ada-BAR&#36827;&#34892;&#20449;&#21495;&#39044;&#22788;&#29702;&#12289;OCM&#36827;&#34892;&#20998;&#31867;&#65292;&#26088;&#22312;&#25552;&#39640;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#30340;&#35299;&#30721;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#24378;&#20581;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01355</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;SpikeDeep-&#20998;&#31867;&#22120;:&#29992;&#20110;&#23454;&#26102;&#33041;&#26426;&#25509;&#21475;&#20449;&#21495;&#22788;&#29702;&#30340;&#33258;&#32452;&#32455;&#33258;&#30417;&#30563;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Adaptive SpikeDeep-Classifier: Self-organizing and self-supervised machine learning algorithm for online spike sorting. (arXiv:2304.01355v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01355
&lt;/p&gt;
&lt;p&gt;
Ada-SpikeDeep-Classifier&#26159;&#19968;&#31181;&#29992;&#20110;&#23454;&#26102;&#33041;&#26426;&#25509;&#21475;&#20449;&#21495;&#22788;&#29702;&#30340;&#33258;&#36866;&#24212;&#33258;&#32452;&#32455;&#31639;&#27861;&#65292;&#23427;&#20351;&#29992;&#20102;SpikeDeeptector&#36827;&#34892;&#20449;&#36947;&#36873;&#25321;&#12289;Ada-BAR&#36827;&#34892;&#20449;&#21495;&#39044;&#22788;&#29702;&#12289;OCM&#36827;&#34892;&#20998;&#31867;&#65292;&#26088;&#22312;&#25552;&#39640;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#30340;&#35299;&#30721;&#25928;&#26524;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#24378;&#20581;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#26088;&#22312;&#25552;&#39640;&#33041;-&#35745;&#31639;&#26426;&#25509;&#21475;&#30340;&#35299;&#30721;&#25928;&#26524;&#65292;&#36890;&#36807;&#38024;&#23545;&#23494;&#38598;&#24494;&#30005;&#26497;&#38453;&#21015;&#25968;&#25454;&#30340;&#22788;&#29702;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#36866;&#24212;&#33258;&#32452;&#32455;&#31639;&#27861;&#8212;&#8212;&#33258;&#36866;&#24212;SpikeDeep-&#20998;&#31867;&#22120;(Ada-SpikeDeepClassifier)&#12290;&#35813;&#31639;&#27861;&#20351;&#29992;SpikeDeeptector&#36827;&#34892;&#20449;&#36947;&#36873;&#25321;&#65292;&#37319;&#29992;&#33258;&#36866;&#24212;&#32972;&#26223;&#27963;&#21160;&#25298;&#32477;&#22120;(Ada-BAR)&#36827;&#34892;&#20449;&#21495;&#39044;&#22788;&#29702;&#65292;&#24182;&#37319;&#29992;&#33258;&#30417;&#30563;&#22312;&#32447;&#32858;&#31867;&#27169;&#22359;(OCM)&#36827;&#34892;&#20998;&#31867;&#12290;Ada-SpikeDeep-Classifier&#22312;&#27169;&#25311;&#21644;&#23454;&#38469;&#25968;&#25454;&#35760;&#24405;&#20013;&#22343;&#34920;&#29616;&#20986;&#39640;&#31934;&#24230;&#21644;&#24378;&#20581;&#24615;&#65292;&#20248;&#20110;&#29616;&#26377;&#30340;&#31639;&#27861;&#12290;&#35813;&#31639;&#27861;&#21487;&#26395;&#25193;&#23637;&#21040;&#31070;&#32463;&#31185;&#23398;&#21644;&#26426;&#22120;&#23398;&#20064;&#31038;&#21306;&#30340;&#20854;&#20182;&#31070;&#32463;&#25968;&#25454;&#20998;&#26512;&#20219;&#21153;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
Objective. Research on brain-computer interfaces (BCIs) is advancing towards rehabilitating severely disabled patients in the real world. Two key factors for successful decoding of user intentions are the size of implanted microelectrode arrays and a good online spike sorting algorithm. A small but dense microelectrode array with 3072 channels was recently developed for decoding user intentions. The process of spike sorting determines the spike activity (SA) of different sources (neurons) from recorded neural data. Unfortunately, current spike sorting algorithms are unable to handle the massively increasing amount of data from dense microelectrode arrays, making spike sorting a fragile component of the online BCI decoding framework. Approach. We proposed an adaptive and self-organized algorithm for online spike sorting, named Adaptive SpikeDeep-Classifier (Ada-SpikeDeepClassifier), which uses SpikeDeeptector for channel selection, an adaptive background activity rejector (Ada-BAR) for 
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20248;&#21270;EEG&#24773;&#32490;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20449;&#21495;&#22788;&#29702;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33041;&#26426;&#25509;&#21475;&#65292;&#36890;&#36807;Savitzky-Golay&#28388;&#27874;&#21644;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#23545;EEG&#20449;&#21495;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102; Blackman&#31383;&#21475;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#31639;&#27861;&#65292;&#21019;&#26032;&#25552;&#39640;&#20102;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01349</link><description>&lt;p&gt;
&#37319;&#29992;&#20449;&#21495;&#22788;&#29702;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20248;&#21270;EEG&#24773;&#32490;&#26816;&#27979;&#27169;&#22411;&#29992;&#20110;&#33041;&#26426;&#25509;&#21475;
&lt;/p&gt;
&lt;p&gt;
Optimized EEG based mood detection with signal processing and deep neural networks for brain-computer interface. (arXiv:2304.01349v1 [q-bio.NC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01349
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20248;&#21270;EEG&#24773;&#32490;&#26816;&#27979;&#27169;&#22411;&#65292;&#36890;&#36807;&#20449;&#21495;&#22788;&#29702;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#33041;&#26426;&#25509;&#21475;&#65292;&#36890;&#36807;Savitzky-Golay&#28388;&#27874;&#21644;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#23545;EEG&#20449;&#21495;&#36827;&#34892;&#39044;&#22788;&#29702;&#65292;&#23454;&#29616;&#20102; Blackman&#31383;&#21475;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#31639;&#27861;&#65292;&#21019;&#26032;&#25552;&#39640;&#20102;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30005;&#23376;&#33041;&#22270;&#65288;EEG&#65289;&#26159;&#19968;&#31181;&#38750;&#24120;&#26377;&#21069;&#36884;&#19988;&#24191;&#27867;&#24212;&#29992;&#30340;&#31243;&#24207;&#65292;&#36890;&#36807;&#25918;&#22823;&#24182;&#27979;&#37327;&#31070;&#32463;&#20803;&#20135;&#29983;&#30340;&#30005;&#33033;&#20914;&#24182;&#30001;&#29305;&#27530;&#30005;&#26497;&#26816;&#27979;&#22836;&#30382;&#19978;&#29305;&#23450;&#28857;&#20135;&#29983;&#30340;&#30005;&#20301;&#26469;&#30740;&#31350;&#22823;&#33041;&#20449;&#21495;&#21644;&#27963;&#21160;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#26816;&#27979;&#33041;&#37096;&#24322;&#24120;&#12289;&#22836;&#30171;&#21644;&#20854;&#20182;&#30142;&#30149;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#23578;&#26377;&#38480;&#30340;&#30740;&#31350;&#29992;&#20110;&#24314;&#31435;&#26234;&#33021;&#20915;&#31574;&#27169;&#22411;&#20197;&#30830;&#23450;EEG&#19982;&#34987;&#35797;&#24773;&#32490;&#30340;&#20851;&#31995;&#12290;&#22312;&#36825;&#39033;&#23454;&#39564;&#20013;&#65292;&#23545;28&#21517;&#20581;&#24247;&#20154;&#31867;&#34987;&#35797;&#30340;EEG&#20449;&#21495;&#36827;&#34892;&#20102;&#35266;&#23519;&#65292;&#24182;&#35797;&#22270;&#30740;&#31350;&#21644;&#35782;&#21035;&#24773;&#32490;&#12290;&#37319;&#29992;Savitzky-Golay&#24102;&#36890;&#28388;&#27874;&#21644;&#29420;&#31435;&#25104;&#20998;&#20998;&#26512;&#23545;&#25968;&#25454;&#36827;&#34892;&#20102;&#36807;&#28388;&#12290;&#23454;&#29616;&#20102;&#19981;&#21516;&#30340;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#26469;&#20998;&#26512;&#21644;&#20998;&#31867;&#22522;&#20110;&#34987;&#35797;&#24773;&#32490;&#30340;EEG&#25968;&#25454;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#20110;Blackman&#31383;&#21475;&#30340;&#20613;&#37324;&#21494;&#21464;&#25442;&#31639;&#27861;&#65292;&#23545;&#25968;&#25454;&#19981;&#22826;&#37325;&#35201;&#30340;&#26435;&#37325;&#36827;&#34892;&#38477;&#20302;&#20197;&#25552;&#39640;&#20998;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;&#23454;&#39564;&#34920;&#26126;&#65292;&#26032;&#24320;&#21457;&#30340;&#27169;&#22411;&#22312;&#24773;&#32490;&#26816;&#27979;&#30340;&#20934;&#30830;&#24615;&#26041;&#38754;&#32988;&#36807;&#29616;&#26377;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Electroencephalogram (EEG) is a very promising and widely implemented procedure to study brain signals and activities by amplifying and measuring the post-synaptical potential arising from electrical impulses produced by neurons and detected by specialized electrodes attached to specific points in the scalp. It can be studied for detecting brain abnormalities, headaches, and other conditions. However, there are limited studies performed to establish a smart decision-making model to identify EEG's relation with the mood of the subject. In this experiment, EEG signals of 28 healthy human subjects have been observed with consent and attempts have been made to study and recognise moods. Savitzky-Golay band-pass filtering and Independent Component Analysis have been used for data filtration.Different neural network algorithms have been implemented to analyze and classify the EEG data based on the mood of the subject. The model is further optimised by the usage of Blackman window-based Fouri
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#25991;&#26723;&#30456;&#20284;&#24615;&#31639;&#27861;&#65292;&#20998;&#20026;&#19977;&#31867;&#36827;&#34892;&#25506;&#35752;&#65306;&#32479;&#35745;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#21644;&#22522;&#20110;&#35821;&#26009;&#24211;/&#30693;&#35782;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#30830;&#23450;&#21738;&#20123;&#31639;&#27861;&#26368;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2304.01330</link><description>&lt;p&gt;
&#25991;&#26723;&#30456;&#20284;&#24615;&#31639;&#27861;&#27604;&#36739;
&lt;/p&gt;
&lt;p&gt;
A Comparison of Document Similarity Algorithms. (arXiv:2304.01330v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01330
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#27604;&#36739;&#20102;&#25991;&#26723;&#30456;&#20284;&#24615;&#31639;&#27861;&#65292;&#20998;&#20026;&#19977;&#31867;&#36827;&#34892;&#25506;&#35752;&#65306;&#32479;&#35745;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#21644;&#22522;&#20110;&#35821;&#26009;&#24211;/&#30693;&#35782;&#30340;&#31639;&#27861;&#65292;&#36890;&#36807;&#27604;&#36739;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#26469;&#30830;&#23450;&#21738;&#20123;&#31639;&#27861;&#26368;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#30456;&#20284;&#24615;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#30340;&#37325;&#35201;&#32452;&#25104;&#37096;&#20998;&#65292;&#26368;&#24120;&#29992;&#20110;&#25220;&#34989;&#26816;&#27979;&#21644;&#25991;&#26412;&#25688;&#35201;&#12290;&#22240;&#27492;&#65292;&#25214;&#21040;&#26368;&#26377;&#25928;&#30340;&#25991;&#26723;&#30456;&#20284;&#24615;&#31639;&#27861;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#39046;&#22495;&#26377;&#24456;&#22823;&#30340;&#27491;&#38754;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#30740;&#31350;&#20247;&#22810;&#25991;&#26723;&#30456;&#20284;&#24615;&#31639;&#27861;&#65292;&#24182;&#30830;&#23450;&#21738;&#20123;&#31639;&#27861;&#26368;&#26377;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#23558;&#25991;&#26723;&#30456;&#20284;&#24615;&#31639;&#27861;&#20998;&#20026;&#19977;&#31867;&#65306;&#32479;&#35745;&#31639;&#27861;&#12289;&#31070;&#32463;&#32593;&#32476;&#31639;&#27861;&#21644;&#22522;&#20110;&#35821;&#26009;&#24211;/&#30693;&#35782;&#30340;&#31639;&#27861;&#65292;&#26469;&#25506;&#35752;&#26368;&#26377;&#25928;&#30340;&#25991;&#26723;&#30456;&#20284;&#24615;&#31639;&#27861;&#12290;&#25105;&#20204;&#20351;&#29992;&#19968;&#31995;&#21015;&#22522;&#20934;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#26041;&#27861;&#26469;&#27604;&#36739;&#27599;&#20010;&#31867;&#21035;&#20013;&#26368;&#26377;&#25928;&#30340;&#31639;&#27861;&#65292;&#35780;&#20272;&#20102;&#27599;&#20010;&#31639;&#27861;&#21487;&#33021;&#29992;&#20110;&#30340;&#25152;&#26377;&#21487;&#33021;&#39046;&#22495;&#12290;
&lt;/p&gt;
&lt;p&gt;
Document similarity is an important part of Natural Language Processing and is most commonly used for plagiarism-detection and text summarization. Thus, finding the overall most effective document similarity algorithm could have a major positive impact on the field of Natural Language Processing. This report sets out to examine the numerous document similarity algorithms, and determine which ones are the most useful. It addresses the most effective document similarity algorithm by categorizing them into 3 types of document similarity algorithms: statistical algorithms, neural networks, and corpus/knowledge-based algorithms. The most effective algorithms in each category are also compared in our work using a series of benchmark datasets and evaluations that test every possible area that each algorithm could be used in.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#26159;&#19968;&#20010;&#20851;&#20110;&#22914;&#20309;&#36827;&#34892;&#33391;&#22909;&#23454;&#39564;&#30340;&#36164;&#28304;&#65292;&#26088;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#35777;&#35774;&#35745;&#30340;&#25361;&#25112;&#65292;&#24182;&#24357;&#34917;&#23454;&#35777;&#30740;&#31350;&#20013;&#21487;&#33021;&#23548;&#33268;&#30340;&#24369;&#30340;&#32479;&#35745;&#35777;&#25454;&#12290;</title><link>http://arxiv.org/abs/2304.01315</link><description>&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23454;&#35777;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Empirical Design in Reinforcement Learning. (arXiv:2304.01315v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01315
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#26159;&#19968;&#20010;&#20851;&#20110;&#22914;&#20309;&#36827;&#34892;&#33391;&#22909;&#23454;&#39564;&#30340;&#36164;&#28304;&#65292;&#26088;&#22312;&#35299;&#20915;&#24378;&#21270;&#23398;&#20064;&#20013;&#23454;&#35777;&#35774;&#35745;&#30340;&#25361;&#25112;&#65292;&#24182;&#24357;&#34917;&#23454;&#35777;&#30740;&#31350;&#20013;&#21487;&#33021;&#23548;&#33268;&#30340;&#24369;&#30340;&#32479;&#35745;&#35777;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#23454;&#35777;&#35774;&#35745;&#19981;&#26159;&#23567;&#20219;&#21153;&#12290;&#36827;&#34892;&#33391;&#22909;&#23454;&#39564;&#38656;&#35201;&#35762;&#31350;&#32454;&#33410;&#65292;&#24182;&#19988;&#22312;&#26576;&#20123;&#26102;&#20505;&#38656;&#35201;&#22823;&#37327;&#35745;&#31639;&#36164;&#28304;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#24120;&#29992;&#31639;&#27861;&#23545;&#36229;&#21442;&#25968;&#35774;&#32622;&#21644;&#23454;&#29616;&#32454;&#33410;&#25935;&#24863;&#65292;&#24182;&#19988;&#24120;&#35265;&#30340;&#23454;&#35777;&#20570;&#27861;&#20250;&#23548;&#33268;&#24369;&#30340;&#32479;&#35745;&#35777;&#25454;&#12290;&#26412;&#25991;&#19981;&#20165;&#21628;&#21505;&#34892;&#21160;&#65292;&#32780;&#19988;&#26159;&#22914;&#20309;&#22312;&#24378;&#21270;&#23398;&#20064;&#20013;&#36827;&#34892;&#33391;&#22909;&#23454;&#39564;&#30340;&#20840;&#38754;&#36164;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Empirical design in reinforcement learning is no small task. Running good experiments requires attention to detail and at times significant computational resources. While compute resources available per dollar have continued to grow rapidly, so have the scale of typical experiments in reinforcement learning. It is now common to benchmark agents with millions of parameters against dozens of tasks, each using the equivalent of 30 days of experience. The scale of these experiments often conflict with the need for proper statistical evidence, especially when comparing algorithms. Recent studies have highlighted how popular algorithms are sensitive to hyper-parameter settings and implementation details, and that common empirical practice leads to weak statistical evidence (Machado et al., 2018; Henderson et al., 2018). Here we take this one step further.  This manuscript represents both a call to action, and a comprehensive resource for how to do good experiments in reinforcement learning. 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01300</link><description>&lt;p&gt;
&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#24046;&#20998;&#38544;&#31169;&#23398;&#20064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Kernel Affine Hull Machines for Differentially Private Learning. (arXiv:2304.01300v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01300
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26680;&#20984;&#21253;&#26426;&#30340;&#26041;&#27861;&#26469;&#30830;&#20445;&#25968;&#25454;&#38544;&#31169;&#20445;&#25252;&#65292;&#21516;&#26102;&#20445;&#30041;&#25968;&#25454;&#32467;&#26500;&#65292;&#29992;&#20110;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#30340;&#20998;&#31867;&#24212;&#29992;&#20013;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#36890;&#36807;&#23398;&#20064;&#20877;&#29983;&#26680;&#24076;&#23572;&#20271;&#29305;&#31354;&#38388;&#20013;&#30340;&#28857;&#30340;&#20984;&#21253;&#26469;&#34920;&#31034;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#26088;&#22312;&#23558;&#25968;&#25454;&#31354;&#38388;&#21010;&#20998;&#20026;&#20960;&#20309;&#20307;&#65292;&#20174;&#32780;&#38544;&#34255;&#26377;&#20851;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#38544;&#31169;&#20449;&#24687;&#65292;&#21516;&#26102;&#20445;&#30041;&#21407;&#22987;&#23398;&#20064;&#38382;&#39064;&#30340;&#32467;&#26500;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#26680;&#20984;&#21253;&#26426;&#65288;KAHM&#65289;&#65292;&#23427;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35745;&#31639;&#20174;&#32467;&#26524;&#26377;&#30028;&#20960;&#20309;&#20307;&#20013;&#30340;&#36317;&#31163;&#24230;&#37327;&#12290;KAHM&#26159;&#24191;&#27867;&#21644;&#28145;&#20837;&#30340;&#33258;&#32534;&#30721;&#22120;&#30340;&#20851;&#38190;&#26500;&#24314;&#22359;&#65292;&#23427;&#20204;&#20351;&#25968;&#25454;&#34920;&#31034;&#23398;&#20064;&#29992;&#20110;&#20998;&#31867;&#24212;&#29992;&#12290;&#20026;&#20102;&#30830;&#20445;&#38544;&#31169;&#20445;&#25252;&#23398;&#20064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#29983;&#25104;&#34394;&#20551;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#28041;&#21450;&#23558;&#24046;&#20998;&#38544;&#31169;&#25968;&#25454;&#26679;&#26412;&#36890;&#36807;&#36716;&#25442;&#36807;&#31243;&#36827;&#34892;&#24179;&#28369;&#22788;&#29702;&#12290;&#29983;&#25104;&#30340;&#34394;&#20551;&#25968;&#25454;&#19981;&#20165;&#20445;&#35777;&#24046;&#20998;&#38544;&#31169;&#65292;&#32780;&#19988;&#30830;&#20445;KAHM&#24314;&#27169;&#35823;&#24046;&#19981;&#22823;&#20110;&#21407;&#22987;&#25968;&#25454;&#35823;&#24046;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper explores the use of affine hulls of points as a means of representing data via learning in Reproducing Kernel Hilbert Spaces (RKHS), with the goal of partitioning the data space into geometric bodies that conceal privacy-sensitive information about individual data points, while preserving the structure of the original learning problem. To this end, we introduce the Kernel Affine Hull Machine (KAHM), which provides an effective way of computing a distance measure from the resulting bounded geometric body. KAHM is a critical building block in wide and deep autoencoders, which enable data representation learning for classification applications. To ensure privacy-preserving learning, we propose a novel method for generating fabricated data, which involves smoothing differentially private data samples through a transformation process. The resulting fabricated data guarantees not only differential privacy but also ensures that the KAHM modeling error is not larger than that of the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2304.01295</link><description>&lt;p&gt;
&#26377;&#25928;&#22320;&#23545;&#40784;&#36328;&#35821;&#35328;&#20250;&#35805;&#20219;&#21153;&#30340;&#25552;&#31034;&#35843;&#25972;&#36328;&#35821;&#35328;&#36716;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Efficiently Aligned Cross-Lingual Transfer Learning for Conversational Tasks using Prompt-Tuning. (arXiv:2304.01295v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01295
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;XSGD&#65292;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#65292;&#21516;&#26102;&#30740;&#31350;&#20102;&#36328;&#35821;&#35328;&#20219;&#21153;&#30340;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#22312;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#20219;&#21153;&#19978;&#35780;&#20272;&#20102;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#65292;&#36328;&#35821;&#35328;&#36716;&#31227;&#30340;&#35821;&#35328;&#27169;&#22411;&#24050;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#20294;&#26159;&#23545;&#20110;&#20250;&#35805;&#20219;&#21153;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;XSGD&#65292;&#36825;&#26159;&#19968;&#20010;&#30001;Schema-Guided Dialogue&#65288;SGD&#65289;&#32763;&#35793;&#25104;105&#31181;&#20854;&#20182;&#35821;&#35328;&#30340;&#24179;&#34892;&#22823;&#35268;&#27169;&#22810;&#35821;&#31181;&#20250;&#35805;&#25968;&#25454;&#38598;&#12290;&#20026;&#20102;&#23454;&#29616;&#23545;&#40784;&#30340;&#36328;&#35821;&#35328;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#22522;&#20110;&#25552;&#31034;&#35843;&#25972;&#30340;&#26041;&#27861;&#26469;&#23398;&#20064;&#23545;&#40784;&#25552;&#31034;&#12290;&#25105;&#20204;&#36824;&#30740;&#31350;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#20998;&#31867;&#22120;&#65306;NLI-based&#21644;vanilla&#20998;&#31867;&#22120;&#65292;&#24182;&#27979;&#35797;&#20102;&#23545;&#40784;&#25552;&#31034;&#25152;&#23454;&#29616;&#30340;&#36328;&#35821;&#35328;&#33021;&#21147;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#23545;&#35805;&#20219;&#21153;&#65288;&#25554;&#27133;&#22635;&#20805;&#21644;&#24847;&#22270;&#20998;&#31867;&#65289;&#19978;&#35780;&#20272;&#20102;&#25105;&#20204;&#27169;&#22411;&#30340;&#36328;&#35821;&#35328;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-lingual transfer of language models trained on high-resource languages like English has been widely studied for many NLP tasks, but focus on conversational tasks has been rather limited. This is partly due to the high cost of obtaining non-English conversational data, which results in limited coverage. In this work, we introduce XSGD, a parallel and large-scale multilingual conversation dataset that we created by translating the English-only Schema-Guided Dialogue (SGD) dataset (Rastogi et al., 2020) into 105 other languages. XSGD contains approximately 330k utterances per language. To facilitate aligned cross-lingual representations, we develop an efficient prompt-tuning-based method for learning alignment prompts. We also investigate two different classifiers: NLI-based and vanilla classifiers, and test cross-lingual capability enabled by the aligned prompts. We evaluate our model's cross-lingual generalization capabilities on two conversation tasks: slot-filling and intent cla
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36923;&#36753;&#31995;&#32479;&#65292;&#23558;&#20449;&#24565;&#12289;&#30693;&#35782;&#21644;&#35777;&#25454;&#30456;&#32467;&#21512;&#65292;&#24182;&#20197;&#8220;&#35777;&#25454;&#20135;&#29983;&#20449;&#24565;&#21644;&#30693;&#35782;&#8221;&#30340;&#30452;&#35273;&#21407;&#21017;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;$S5$&#27169;&#24577;&#31995;&#32479;&#19982;&#21476;&#20856;&#35748;&#35782;&#21407;&#21017;&#30340;&#32467;&#21512;&#12290;</title><link>http://arxiv.org/abs/2304.01283</link><description>&lt;p&gt;
&#20449;&#24565;&#12289;&#30693;&#35782;&#21644;&#35777;&#25454;
&lt;/p&gt;
&lt;p&gt;
Belief, knowledge and evidence. (arXiv:2304.01283v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01283
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#36923;&#36753;&#31995;&#32479;&#65292;&#23558;&#20449;&#24565;&#12289;&#30693;&#35782;&#21644;&#35777;&#25454;&#30456;&#32467;&#21512;&#65292;&#24182;&#20197;&#8220;&#35777;&#25454;&#20135;&#29983;&#20449;&#24565;&#21644;&#30693;&#35782;&#8221;&#30340;&#30452;&#35273;&#21407;&#21017;&#20026;&#22522;&#30784;&#65292;&#23454;&#29616;&#20102;$S5$&#27169;&#24577;&#31995;&#32479;&#19982;&#21476;&#20856;&#35748;&#35782;&#21407;&#21017;&#30340;&#32467;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#36923;&#36753;&#31995;&#32479;&#65292;&#23558;&#20247;&#25152;&#21608;&#30693;&#30340;&#21476;&#20856;&#35748;&#35782;&#27010;&#24565;&#65292;&#21363;&#20449;&#24565;&#21644;&#30693;&#35782;&#65292;&#19982;&#35777;&#25454;&#30340;&#27010;&#24565;&#30456;&#32467;&#21512;&#65292;&#20351;&#24471;&#8220;&#35777;&#25454;&#20135;&#29983;&#20449;&#24565;&#21644;&#30693;&#35782;&#8221;&#30340;&#30452;&#35273;&#21407;&#21017;&#24471;&#21040;&#28385;&#36275;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20381;&#36182;&#20110;&#31532;&#19968;&#20316;&#32773;&#20197;&#21069;&#30340;&#30740;&#31350;&#65292;&#20182;&#20204;&#20171;&#32461;&#20102;&#21253;&#21547;&#29992;&#20110;&#30452;&#35273;&#30495;&#30456;&#65288;&#21363;&#35777;&#26126;&#65289;&#25512;&#29702;&#30340;$S5$&#26679;&#24335;&#21407;&#29702;&#30340;&#27169;&#24577;&#31995;&#32479;&#65292;&#24182;&#21463;&#21040;\cite{artpro}&#30340;&#21551;&#21457;&#65292;&#23558;&#35813;&#31995;&#32479;&#19982;&#30452;&#35273;&#20027;&#20041;&#30340;&#20449;&#24565;&#21644;&#30693;&#35782;&#27010;&#24565;&#30456;&#32467;&#21512;&#12290;&#25105;&#20204;&#32771;&#34385;&#23558;&#36825;&#20010;&#32452;&#21512;&#31995;&#32479;&#20013;&#30340;&#24314;&#35774;&#24615;&#35777;&#26126;&#27010;&#24565;&#26367;&#25442;&#20026;&#21476;&#20856;&#35777;&#25454;&#27010;&#24565;&#12290;&#36825;&#23548;&#33268;&#36923;&#36753;&#23558;$S5$&#27169;&#24577;&#31995;&#32479;&#19982;&#21476;&#20856;&#35748;&#35782;&#21407;&#21017;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20854;&#20013;$\square\varphi$&#22312;&#35748;&#30693;&#24847;&#20041;&#19978;&#35299;&#37322;&#20026;&#8220;$\varphi$&#26159;&#26174;&#28982;&#30340;&#8221;&#12290;&#21463;\cite{lewapal}&#30340;&#21551;&#21457;&#65292;&#19982;&#25991;&#29486;&#20013;&#36890;&#24120;&#30340;&#21487;&#33021;&#19990;&#30028;&#35821;&#20041;&#24418;&#25104;&#23545;&#27604;&#65292;&#25105;&#20204;&#22312;&#27492;&#25552;&#20986;&#19968;&#31181;&#26032;&#30340;&#35821;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a logical system that combines the well-known classical epistemic concepts of belief and knowledge with a concept of evidence such that the intuitive principle \textit{`evidence yields belief and knowledge'} is satisfied. Our approach relies on previous works of the first author \cite{lewjlc2, lewigpl, lewapal} who introduced a modal system containing $S5$-style principles for the reasoning about intutionistic truth (i.e. \textit{proof}) and, inspired by \cite{artpro}, combined that system with concepts of \textit{intuitionistic} belief and knowledge. We consider that combined system and replace the constructive concept of \textit{proof} with a classical notion of \textit{evidence}. This results in a logic that combines modal system $S5$ with classical epistemic principles where $\square\varphi$ reads as `$\varphi$ is evident' in an epistemic sense. Inspired by \cite{lewapal}, and in contrast to the usual possible worlds semantics found in the literature, we propose here a r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31995;&#32479;&#35770;&#36807;&#31243;&#20998;&#26512;&#65288;STPA&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37319;&#29992;ChatGPT&#23545;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#31995;&#32479;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#26041;&#27861;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;STPA&#30340;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#24212;&#29992;&#20110;&#23433;&#20840;&#20998;&#26512;&#65292;&#24182;&#20026;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;</title><link>http://arxiv.org/abs/2304.01246</link><description>&lt;p&gt;
&#22823;&#35821;&#35328;&#27169;&#22411;&#26102;&#20195;&#30340;&#23433;&#20840;&#20998;&#26512;&#65306;&#32842;&#22825;GPT&#22312;STPA&#26696;&#20363;&#30740;&#31350;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT. (arXiv:2304.01246v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01246
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#31995;&#32479;&#35770;&#36807;&#31243;&#20998;&#26512;&#65288;STPA&#65289;&#20013;&#30340;&#24212;&#29992;&#65292;&#24182;&#37319;&#29992;ChatGPT&#23545;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#31995;&#32479;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#26041;&#27861;&#26159;&#26368;&#26377;&#25928;&#30340;&#65292;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;STPA&#30340;&#36136;&#37327;&#12290;&#26412;&#30740;&#31350;&#35777;&#26126;&#65292;LLMs&#21487;&#20197;&#24212;&#29992;&#20110;&#23433;&#20840;&#20998;&#26512;&#65292;&#24182;&#20026;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#65292;&#22914;ChatGPT&#21644;BERT&#65292;&#30001;&#20110;&#20854;&#20855;&#26377;&#31867;&#20284;&#20110;&#20154;&#31867;&#30340;&#23545;&#35805;&#65292;&#22312;&#35768;&#22810;&#30693;&#35782;&#39046;&#22495;&#20013;&#20855;&#26377;&#35814;&#32454;&#21644;&#26126;&#30830;&#30340;&#31572;&#26696;&#65292;&#27491;&#22312;&#24341;&#39046;&#19968;&#22330;&#26032;&#30340;&#20154;&#24037;&#26234;&#33021;&#28909;&#28526;&#12290;&#34429;&#28982;LLMs&#27491;&#22312;&#36805;&#36895;&#24212;&#29992;&#20110;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#39046;&#22495;&#65292;&#20294;&#25105;&#20204;&#23545;&#20197;&#19979;&#38382;&#39064;&#24863;&#20852;&#36259;&#65306;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#30340;&#23433;&#20840;&#20998;&#26512;&#26159;&#21542;&#21487;&#20197;&#21033;&#29992;LLMs&#65311;&#20026;&#20102;&#22238;&#31572;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20351;&#29992;ChatGPT&#23545;&#33258;&#21160;&#32039;&#24613;&#21046;&#21160;&#65288;AEB&#65289;&#31995;&#32479;&#30340;&#31995;&#32479;&#35770;&#36807;&#31243;&#20998;&#26512;&#65288;STPA&#65289;&#36827;&#34892;&#20102;&#26696;&#20363;&#30740;&#31350;&#12290;STPA&#26159;&#26368;&#26222;&#36941;&#30340;&#21361;&#38505;&#20998;&#26512;&#25216;&#26415;&#20043;&#19968;&#65292;&#20294;&#23427;&#23384;&#22312;&#35832;&#22810;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#39640;&#22797;&#26434;&#24615;&#21644;&#20027;&#35266;&#24615;&#65292;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;ChatGPT&#30340;&#24212;&#29992;&#65292;&#20197;&#35299;&#20915;&#36825;&#20123;&#23616;&#38480;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#36890;&#36807;&#32771;&#34385;&#20854;&#19982;&#20154;&#31867;&#19987;&#23478;&#30340;&#20132;&#20114;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#23558;ChatGPT&#32435;&#20837;STPA&#20013;&#30340;&#26041;&#27861;&#65306;&#19968;&#27425;&#24615;&#21333;&#24037;&#20132;&#20114;&#12289;&#37325;&#22797;&#21333;&#24037;&#20132;&#20114;&#21644;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#12290;&#27604;&#36739;&#32467;&#26524;&#34920;&#26126;&#65306;&#65288;i&#65289;&#22312;&#27809;&#26377;&#20154;&#31867;&#19987;&#23478;&#30340;&#24773;&#20917;&#19979;&#20351;&#29992;ChatGPT&#19981;&#33021;&#20026;STPA&#25552;&#20379;&#36275;&#22815;&#30340;&#20449;&#24687;&#65307;&#65288;ii&#65289;&#19968;&#27425;&#24615;&#21333;&#24037;&#20132;&#20114;&#23545;STPA&#26377;&#24110;&#21161;&#65292;&#20294;&#19981;&#22914;&#37325;&#22797;&#20132;&#20114;&#26377;&#25928;&#65307;&#65288;iii&#65289;&#37325;&#22797;&#21452;&#24037;&#20132;&#20114;&#19968;&#33268;&#20248;&#20110;&#20854;&#20182;&#26041;&#27861;&#65292;&#24182;&#26174;&#30528;&#25552;&#39640;&#20102;STPA&#30340;&#36136;&#37327;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;LLMs&#21487;&#20197;&#24212;&#29992;&#20110;&#23433;&#20840;&#20998;&#26512;&#65292;&#24182;&#20026;AEB&#20197;&#22806;&#30340;&#20854;&#20182;&#23433;&#20840;&#20851;&#38190;&#31995;&#32479;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#35265;&#35299;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large Language Models (LLMs), such as ChatGPT and BERT, are leading a new AI heatwave due to its human-like conversations with detailed and articulate answers across many domains of knowledge. While LLMs are being quickly applied to many AI application domains, we are interested in the following question: Can safety analysis for safety-critical systems make use of LLMs? To answer, we conduct a case study of Systems Theoretic Process Analysis (STPA) on Automatic Emergency Brake (AEB) systems using ChatGPT. STPA, one of the most prevalent techniques for hazard analysis, is known to have limitations such as high complexity and subjectivity, which this paper aims to explore the use of ChatGPT to address. Specifically, three ways of incorporating ChatGPT into STPA are investigated by considering its interaction with human experts: one-off simplex interaction, recurring simplex interaction, and recurring duplex interaction. Comparative results reveal that: (i) using ChatGPT without human exp
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#22120;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#26234;&#33021;&#23398;&#20064;&#30340;Cyber Gym for Intelligent Learning&#65288;CyGIL&#65289;&#20013;&#25552;&#20379;&#39640;&#24230;&#30495;&#23454;&#30340;&#32593;&#32476;Cyber Operations&#65288;CyOp&#65289;&#35757;&#32451;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#27169;&#25311;&#22120;&#29983;&#25104;&#21644;&#20195;&#29702;&#35757;&#32451;&#36807;&#31243;&#26469;&#38477;&#20302;&#20195;&#29702;&#35757;&#32451;&#26102;&#38388;&#12290;</title><link>http://arxiv.org/abs/2304.01244</link><description>&lt;p&gt;
&#33258;&#20027;&#32593;&#32476;&#25915;&#20987;&#20195;&#29702;&#30340;&#32479;&#19968;&#20223;&#30495;&#27169;&#25311;&#35757;&#32451;&#29615;&#22659;
&lt;/p&gt;
&lt;p&gt;
Unified Emulation-Simulation Training Environment for Autonomous Cyber Agents. (arXiv:2304.01244v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01244
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#21160;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#22120;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#26234;&#33021;&#23398;&#20064;&#30340;Cyber Gym for Intelligent Learning&#65288;CyGIL&#65289;&#20013;&#25552;&#20379;&#39640;&#24230;&#30495;&#23454;&#30340;&#32593;&#32476;Cyber Operations&#65288;CyOp&#65289;&#35757;&#32451;&#29615;&#22659;&#65292;&#24182;&#36890;&#36807;&#38598;&#25104;&#27169;&#25311;&#22120;&#29983;&#25104;&#21644;&#20195;&#29702;&#35757;&#32451;&#36807;&#31243;&#26469;&#38477;&#20302;&#20195;&#29702;&#35757;&#32451;&#26102;&#38388;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#21644;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65288;RL / DRL&#65289;&#65292;&#21487;&#20197;&#24320;&#21457;&#33258;&#20027;&#32593;&#32476;&#25915;&#20987;&#20195;&#29702;&#65292;&#24182;&#22312;&#20195;&#34920;&#24615;&#29615;&#22659;&#20013;&#23545;&#20195;&#29702;&#36827;&#34892;&#35757;&#32451;&#12290;&#35757;&#32451;&#29615;&#22659;&#24517;&#39035;&#39640;&#24230;&#30495;&#23454;&#22320;&#27169;&#25311;&#20195;&#29702;&#25152;&#35201;&#25506;&#32034;&#30340;&#32593;&#32476;Cyber Operations&#65288;CyOp&#65289;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#31995;&#32479;&#35299;&#20915;&#26041;&#26696;&#65292;&#22312;&#26234;&#33021;&#23398;&#20064;&#30340;Cyber Gym for Intelligent Learning&#65288;CyGIL&#65289;&#20013;&#33258;&#21160;&#29983;&#25104;&#39640;&#20445;&#30495;&#24230;&#30340;&#27169;&#25311;&#22120;&#12290;&#36890;&#36807;&#34920;&#24449;&#23398;&#20064;&#21644;&#36830;&#32493;&#23398;&#20064;&#65292;CyGIL&#25552;&#20379;&#32479;&#19968;&#30340;CyOp&#22521;&#35757;&#29615;&#22659;&#65292;&#20854;&#20013;&#20223;&#30495;&#30340;CyGIL-S&#30001;&#33258;&#21160;&#29983;&#25104;&#30340;CyGIL-E&#29983;&#25104;&#12290;&#23558;&#27169;&#25311;&#22120;&#29983;&#25104;&#19982;&#20195;&#29702;&#35757;&#32451;&#36807;&#31243;&#38598;&#25104;&#65292;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#25152;&#38656;&#30340;&#20195;&#29702;&#35757;&#32451;&#26102;&#38388;&#12290;&#22312;CyGIL-S&#20013;&#35757;&#32451;&#30340;&#20195;&#29702;&#21487;&#20197;&#30452;&#25509;&#34987;&#20256;&#36755;&#21040;CyGIL-E&#65292;&#23436;&#20840;&#21487;&#36716;&#31227;&#33267;&#20223;&#30495;&#30340;&#8220;&#30495;&#23454;&#8221;&#32593;&#32476;&#12290;&#23454;&#39564;&#32467;&#26524;&#23637;&#31034;&#20102;&#36825;&#20123;&#35299;&#20915;&#26041;&#26696;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Autonomous cyber agents may be developed by applying reinforcement and deep reinforcement learning (RL/DRL), where agents are trained in a representative environment. The training environment must simulate with high-fidelity the network Cyber Operations (CyOp) that the agent aims to explore. Given the complexity of net-work CyOps, a good simulator is difficult to achieve. This work presents a systematic solution to automatically generate a high-fidelity simulator in the Cyber Gym for Intelligent Learning (CyGIL). Through representation learning and continuous learning, CyGIL provides a unified CyOp training environment where an emulated CyGIL-E automatically generates a simulated CyGIL-S. The simulator generation is integrated with the agent training process to further reduce the required agent training time. The agent trained in CyGIL-S is transferrable directly to CyGIL-E showing full transferability to the emulated "real" network. Experimental results are presented to demonstrate th
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35777;&#25454;&#20849;&#25351;&#22270;&#21644;&#35777;&#25454;&#25991;&#26412;&#22270;&#34920;&#26469;&#35299;&#20915;&#20020;&#24202;&#35777;&#25454;&#25512;&#33616;&#20013;&#30340;&#32852;&#31995;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#36890;&#36947;&#24322;&#26500;&#23398;&#20064;&#27169;&#22411;&#26469;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2304.01242</link><description>&lt;p&gt;
&#22312;&#35777;&#25454;&#22270;&#19978;&#36816;&#29992;&#22810;&#36890;&#36947;&#24322;&#26500;&#23398;&#20064;&#22686;&#24378;&#20020;&#24202;&#35777;&#25454;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Enhancing Clinical Evidence Recommendation with Multi-Channel Heterogeneous Learning on Evidence Graphs. (arXiv:2304.01242v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01242
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20351;&#29992;&#35777;&#25454;&#20849;&#25351;&#22270;&#21644;&#35777;&#25454;&#25991;&#26412;&#22270;&#34920;&#26469;&#35299;&#20915;&#20020;&#24202;&#35777;&#25454;&#25512;&#33616;&#20013;&#30340;&#32852;&#31995;&#31232;&#30095;&#24615;&#38382;&#39064;&#65292;&#24182;&#20171;&#32461;&#20102;&#19968;&#20010;&#22810;&#36890;&#36947;&#24322;&#26500;&#23398;&#20064;&#27169;&#22411;&#26469;&#25903;&#25345;&#20020;&#24202;&#20915;&#31574;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35777;&#25454;&#21253;&#25324;&#24739;&#32773;&#12289;&#24178;&#39044;&#65288;&#22914;&#33647;&#29289;&#25110;&#29289;&#29702;&#27835;&#30103;&#65289;&#12289;&#38382;&#39064;&#21644;&#32467;&#26524;&#20043;&#38388;&#30340;&#20851;&#32852;&#21644;&#24433;&#21709;&#12290;&#25512;&#33616;&#20020;&#24202;&#35777;&#25454;&#30340;&#30446;&#30340;&#26159;&#20026;&#21307;&#21153;&#20154;&#21592;&#25552;&#20379;&#30456;&#20851;&#20449;&#24687;&#65292;&#20197;&#25903;&#25345;&#20182;&#20204;&#30340;&#20915;&#31574;&#36807;&#31243;&#24182;&#29983;&#25104;&#26032;&#30340;&#35777;&#25454;&#12290;&#25105;&#20204;&#30340;&#20855;&#20307;&#20219;&#21153;&#20391;&#37325;&#20110;&#26681;&#25454;&#20020;&#24202;&#38382;&#39064;&#25512;&#33616;&#35777;&#25454;&#12290;&#28982;&#32780;&#65292;&#26576;&#20123;&#20020;&#24202;&#38382;&#39064;&#21644;&#30456;&#20851;&#35777;&#25454;&#20043;&#38388;&#30340;&#30452;&#25509;&#32852;&#31995;&#24448;&#24448;&#26159;&#31232;&#30095;&#30340;&#65292;&#36825;&#23601;&#20135;&#29983;&#20102;&#32852;&#31995;&#31232;&#30095;&#24615;&#30340;&#25361;&#25112;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#25512;&#33616;&#36866;&#24403;&#30340;&#35777;&#25454;&#65292;&#26377;&#24517;&#35201;&#21516;&#26102;&#21033;&#29992;&#35777;&#25454;&#20043;&#38388;&#30340;&#25299;&#25169;&#20851;&#31995;&#21644;&#25551;&#36848;&#23427;&#20204;&#30340;&#25991;&#26412;&#20449;&#24687;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#20004;&#20010;&#30693;&#35782;&#22270;&#34920;&#65306;&#35777;&#25454;&#20849;&#25351;&#22270;&#21644;&#35777;&#25454;&#25991;&#26412;&#22270;&#34920;&#65292;&#20197;&#34920;&#31034;&#35777;&#25454;&#20803;&#32032;&#20043;&#38388;&#30340;&#25299;&#25169;&#21644;&#35821;&#35328;&#20851;&#31995;&#12290;&#25105;&#20204;&#36824;&#24341;&#20837;&#20102;&#19968;&#20010;&#22810;&#36890;&#36947;&#24322;&#26500;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical evidence encompasses the associations and impacts between patients, interventions (such as drugs or physiotherapy), problems, and outcomes. The goal of recommending clinical evidence is to provide medical practitioners with relevant information to support their decision-making processes and to generate new evidence. Our specific task focuses on recommending evidence based on clinical problems. However, the direct connections between certain clinical problems and related evidence are often sparse, creating a challenge of link sparsity. Additionally, to recommend appropriate evidence, it is essential to jointly exploit both topological relationships among evidence and textual information describing them. To address these challenges, we define two knowledge graphs: an Evidence Co-reference Graph and an Evidence Text Graph, to represent the topological and linguistic relations among evidential elements, respectively. We also introduce a multi-channel heterogeneous learning model a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#32463;&#36807;&#25913;&#36827;&#21644;&#24494;&#35843;&#30340;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2304.01238</link><description>&lt;p&gt;
Spam-T5&#65306;&#22522;&#20110;&#23567;&#26679;&#26412;&#30340;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22522;&#20934;&#27979;&#35797;
&lt;/p&gt;
&lt;p&gt;
Spam-T5: Benchmarking Large Language Models for Few-Shot Email Spam Detection. (arXiv:2304.01238v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01238
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#34920;&#29616;&#65292;&#21457;&#29616;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20248;&#20110;&#20256;&#32479;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#26679;&#26412;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#36824;&#20171;&#32461;&#20102;&#32463;&#36807;&#25913;&#36827;&#21644;&#24494;&#35843;&#30340;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#27604;&#36739;&#19977;&#31181;&#19981;&#21516;&#31867;&#22411;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;BERT-like&#12289;Sentence Transformers&#21644;Seq2Seq&#65289;&#20197;&#21450;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#65288;&#22914;&#26420;&#32032;&#36125;&#21494;&#26031;&#21644;LightGBM&#65289;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#26377;&#25928;&#24615;&#65292;&#30740;&#31350;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20013;&#30340;&#20316;&#29992;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#36824;&#35780;&#20272;&#20102;&#36825;&#20123;&#27169;&#22411;&#22312;&#22235;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#30340;&#34920;&#29616;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#25968;&#37327;&#30340;&#35757;&#32451;&#26679;&#26412;&#65288;&#23436;&#25972;&#35757;&#32451;&#38598;&#21644;&#23567;&#26679;&#26412;&#65289;&#36827;&#34892;&#20102;&#27979;&#35797;&#12290; &#21457;&#29616;&#22312;&#22823;&#22810;&#25968;&#24773;&#20917;&#19979;&#65292;LLMs&#20248;&#20110;&#22522;&#32447;&#25216;&#26415;&#65292;&#29305;&#21035;&#26159;&#22312;&#23567;&#26679;&#26412;&#24773;&#20917;&#19979;&#12290;&#36825;&#31181;&#36866;&#24212;&#24615;&#20351;LLMs&#22312;&#37038;&#20214;&#22403;&#22334;&#26816;&#27979;&#20219;&#21153;&#20013;&#20855;&#26377;&#29420;&#29305;&#30340;&#20248;&#21183;&#65292;&#22240;&#20026;&#26631;&#35760;&#26679;&#26412;&#25968;&#37327;&#26377;&#38480;&#65292;&#24182;&#19988;&#27169;&#22411;&#38656;&#35201;&#32463;&#24120;&#26356;&#26032;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;Spam-T5&#27169;&#22411;&#65292;&#35813;&#27169;&#22411;&#26159;&#19987;&#38376;&#20026;&#26816;&#27979;&#30005;&#23376;&#37038;&#20214;&#22403;&#22334;&#32780;&#36827;&#34892;&#20102;&#25913;&#36827;&#21644;&#24494;&#35843;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;Spam-T5&#27169;&#22411;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the effectiveness of large language models (LLMs) in email spam detection by comparing prominent models from three distinct families: BERT-like, Sentence Transformers, and Seq2Seq. Additionally, we examine well-established machine learning techniques for spam detection, such as Na\"ive Bayes and LightGBM, as baseline methods. We assess the performance of these models across four public datasets, utilizing different numbers of training samples (full training set and few-shot settings). Our findings reveal that, in the majority of cases, LLMs surpass the performance of the popular baseline techniques, particularly in few-shot scenarios. This adaptability renders LLMs uniquely suited to spam detection tasks, where labeled samples are limited in number and models require frequent updates. Additionally, we introduce Spam-T5, a Flan-T5 model that has been specifically adapted and fine-tuned for the purpose of detecting email spam. Our results demonstrate that Spam-T5 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#26469;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20026;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#25688;&#35201;&#31561;&#20219;&#21153;&#24494;&#35843;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01228</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#25913;&#36827;&#30340;&#26041;&#24335;&#23454;&#29616;&#26356;&#22909;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Better Language Models of Code through Self-Improvement. (arXiv:2304.01228v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#26469;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20026;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#25688;&#35201;&#31561;&#20219;&#21153;&#24494;&#35843;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#22810;&#27169;&#24335;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20294;&#26159;&#65292;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#30417;&#30563;&#65292;&#24182;&#19988;&#21463;&#21040;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#20197;&#25913;&#21892;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#33719;&#24471;&#30340;&#30693;&#35782;&#26469;&#29983;&#25104;&#20266;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#19979;&#19968;&#27493;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#21040;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#22914;CodeT5&#12289;CodeBERT&#21644;UnixCoder&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;PLMC&#22312;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22914;CodeXGLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20195;&#30721;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve this issue by proposing a simple data augmentation framework. Our framework utilizes knowledge gained during the pre-training and fine-tuning stage to generate pseudo data, which is then used as training data for the next step. We incorporate this framework into the state-of-the-art language models, such as CodeT5, CodeBERT, and UnixCoder. The results show that our framework significantly improves PLMCs' performance in code-related sequence generation tasks, such as code summarization and code generation in the CodeXGLUE benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23436;&#20840;&#25429;&#25417;&#24322;&#24120;&#20107;&#20214;&#27169;&#24335;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01226</link><description>&lt;p&gt;
&#22522;&#20110;&#36229;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Abnormal Event Detection via Hypergraph Contrastive Learning. (arXiv:2304.01226v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01226
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#36229;&#22270;&#23545;&#27604;&#23398;&#20064;&#30340;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#26041;&#27861;&#65292;&#21487;&#20197;&#23436;&#20840;&#25429;&#25417;&#24322;&#24120;&#20107;&#20214;&#27169;&#24335;&#65292;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#22312;&#20004;&#20010;&#20844;&#20849;&#25968;&#25454;&#38598;&#19978;&#26126;&#26174;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#22312;&#35768;&#22810;&#23454;&#38469;&#24212;&#29992;&#20013;&#25198;&#28436;&#30528;&#37325;&#35201;&#35282;&#33394;&#65292;&#23427;&#25351;&#30340;&#26159;&#25366;&#25496;&#28041;&#21450;&#23454;&#20307;&#20043;&#38388;&#19981;&#23547;&#24120;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290; &#20197;&#24448;&#30340;&#30740;&#31350;&#22823;&#22810;&#23558;&#27492;&#20219;&#21153;&#31616;&#21270;&#20026;&#26816;&#27979;&#24322;&#24120;&#30340;&#25104;&#23545;&#20132;&#20114;&#20316;&#29992;&#12290; &#28982;&#32780;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#20107;&#20214;&#21487;&#33021;&#21253;&#21547;&#22810;&#31181;&#31867;&#22411;&#30340;&#23646;&#24615;&#23454;&#20307;&#21644;&#23427;&#20204;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20316;&#29992;&#65292;&#36825;&#24418;&#25104;&#20102;&#23646;&#24615;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#12290;&#38543;&#30528;&#31038;&#20132;&#32593;&#32476;&#30340;&#34028;&#21187;&#21457;&#23637;&#65292;&#23646;&#24615;&#24322;&#26500;&#20449;&#24687;&#32593;&#32476;&#20013;&#30340;&#24322;&#24120;&#20107;&#20214;&#26816;&#27979;&#24050;&#25104;&#20026;&#19968;&#39033;&#37325;&#35201;&#20294;&#24456;&#23569;&#34987;&#25506;&#32034;&#30340;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
Abnormal event detection, which refers to mining unusual interactions among involved entities, plays an important role in many real applications. Previous works mostly over-simplify this task as detecting abnormal pair-wise interactions. However, real-world events may contain multi-typed attributed entities and complex interactions among them, which forms an Attributed Heterogeneous Information Network (AHIN). With the boom of social networks, abnormal event detection in AHIN has become an important, but seldom explored task. In this paper, we firstly study the unsupervised abnormal event detection problem in AHIN. The events are considered as star-schema instances of AHIN and are further modeled by hypergraphs. A novel hypergraph contrastive learning method, named AEHCL, is proposed to fully capture abnormal event patterns. AEHCL designs the intra-event and inter-event contrastive modules to exploit self-supervised AHIN information. The intra-event contrastive module captures the pair
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; "STI-KNN" &#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#30701;&#26102;&#38388;&#20869;&#23545; KNN &#27169;&#22411;&#36827;&#34892;&#31934;&#30830;&#30340;&#37197;&#23545;&#20132;&#20114; Shapley &#20540;&#35745;&#31639;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35780;&#20272;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#20215;&#20540;&#65292;&#25552;&#39640;&#35757;&#32451;&#32467;&#26524;&#21644;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2304.01224</link><description>&lt;p&gt;
&#20248;&#21270; KNN &#27169;&#22411;&#30340; Shapley Interaction &#35745;&#31639;&#20174; O(2^n) &#21040; O(t n^2)&#12290;
&lt;/p&gt;
&lt;p&gt;
Optimizing Data Shapley Interaction Calculation from O(2^n) to O(t n^2) for KNN models. (arXiv:2304.01224v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01224
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026; "STI-KNN" &#30340;&#31639;&#27861;&#65292;&#21487;&#20197;&#22312;&#30701;&#26102;&#38388;&#20869;&#23545; KNN &#27169;&#22411;&#36827;&#34892;&#31934;&#30830;&#30340;&#37197;&#23545;&#20132;&#20114; Shapley &#20540;&#35745;&#31639;&#65292;&#20174;&#32780;&#26377;&#25928;&#22320;&#35780;&#20272;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#20215;&#20540;&#65292;&#25552;&#39640;&#35757;&#32451;&#32467;&#26524;&#21644;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#25968;&#25454;&#21487;&#29992;&#24615;&#21644;&#20351;&#29992;&#29575;&#30340;&#24555;&#36895;&#22686;&#38271;&#65292;&#37327;&#21270;&#27599;&#20010;&#35757;&#32451;&#25968;&#25454;&#28857;&#30340;&#38468;&#21152;&#20215;&#20540;&#24050;&#25104;&#20026;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#20851;&#38190;&#30340;&#36807;&#31243;&#12290;Shapley &#20540;&#24050;&#34987;&#20844;&#35748;&#20026;&#26159;&#19968;&#31181;&#26377;&#25928;&#30340;&#25968;&#25454;&#20272;&#20540;&#26041;&#27861;&#65292;&#20351;&#24471;&#35757;&#32451;&#38598;&#27719;&#24635;&#12289;&#33719;&#21462;&#21644;&#24322;&#24120;&#20540;&#21024;&#38500;&#21464;&#24471;&#26356;&#21152;&#39640;&#25928;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21019;&#26032;&#31639;&#27861; "STI-KNN"&#65292;&#23427;&#21487;&#20197;&#22312;O(t n^2)&#26102;&#38388;&#20869;&#35745;&#31639;&#20934;&#30830;&#30340; KNN &#27169;&#22411;&#30340;&#31934;&#30830;&#37197;&#23545;&#20132;&#20114; Shapley &#20540;&#65292;&#36825;&#26159;&#27604;&#22522;&#32447;&#26041;&#27861;&#30340; O(2^n) &#26102;&#38388;&#22797;&#26434;&#24230;&#26174;&#33879;&#25552;&#39640;&#20102;&#12290;&#20351;&#29992; STI-KNN&#65292;&#25105;&#20204;&#21487;&#20197;&#39640;&#25928;&#20934;&#30830;&#22320;&#35780;&#20272;&#21333;&#20010;&#25968;&#25454;&#28857;&#30340;&#20215;&#20540;&#65292;&#20174;&#32780;&#25913;&#21892;&#35757;&#32451;&#32467;&#26524;&#65292;&#26368;&#32456;&#22686;&#24378;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid growth of data availability and usage, quantifying the added value of each training data point has become a crucial process in the field of artificial intelligence. The Shapley values have been recognized as an effective method for data valuation, enabling efficient training set summarization, acquisition, and outlier removal. In this paper, we introduce "STI-KNN", an innovative algorithm that calculates the exact pair-interaction Shapley values for KNN models in O(t n^2) time, which is a significant improvement over the O(2^n)$ time complexity of baseline methods. By using STI-KNN, we can efficiently and accurately evaluate the value of individual data points, leading to improved training outcomes and ultimately enhancing the effectiveness of artificial intelligence applications.
&lt;/p&gt;</description></item><item><title>NeuroDAVIS&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#25968;&#25454;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#24182;&#22312;&#26356;&#20302;&#30340;&#32500;&#24230;&#19978;&#36827;&#34892;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;</title><link>http://arxiv.org/abs/2304.01222</link><description>&lt;p&gt;
NeuroDAVIS: &#29992;&#20110;&#25968;&#25454;&#21487;&#35270;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
NeuroDAVIS: A neural network model for data visualization. (arXiv:2304.01222v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01222
&lt;/p&gt;
&lt;p&gt;
NeuroDAVIS&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#25968;&#25454;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#24182;&#22312;&#26356;&#20302;&#30340;&#32500;&#24230;&#19978;&#36827;&#34892;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#39640;&#32500;&#25968;&#25454;&#30340;&#38477;&#32500;&#21644;&#21487;&#35270;&#21270;&#19968;&#30452;&#26159;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38382;&#39064;&#12290;&#29616;&#20195;&#39640;&#36890;&#37327;&#25216;&#26415;&#20135;&#29983;&#20102;&#22810;&#31181;&#35270;&#22270;&#30340;&#26032;&#30340;&#39640;&#32500;&#25968;&#25454;&#38598;&#65292;&#36825;&#20123;&#25968;&#25454;&#38598;&#21253;&#21547;&#20102;&#26032;&#30340;&#25968;&#25454;&#31867;&#22411;&#12290;&#23545;&#36825;&#20123;&#25968;&#25454;&#38598;&#30340;&#21487;&#35270;&#21270;&#38656;&#35201;&#36866;&#24403;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#22312;&#19981;&#24433;&#21709;&#25968;&#25454;&#20013;&#30340;&#23616;&#37096;&#21644;&#20840;&#23616;&#32467;&#26500;&#30340;&#24773;&#20917;&#19979;&#21457;&#29616;&#25968;&#25454;&#20013;&#30340;&#38544;&#34255;&#27169;&#24335;&#12290;&#28982;&#32780;&#65292;&#33021;&#22815;&#23454;&#29616;&#36825;&#20010;&#20219;&#21153;&#30340;&#26041;&#27861;&#38750;&#24120;&#23569;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#26080;&#30417;&#30563;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;NeuroDAVIS&#65292;&#29992;&#20110;&#25968;&#25454;&#21487;&#35270;&#21270;&#12290;NeuroDAVIS&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#25552;&#21462;&#37325;&#35201;&#29305;&#24449;&#65292;&#32780;&#19981;&#38656;&#35201;&#20551;&#35774;&#20219;&#20309;&#25968;&#25454;&#20998;&#24067;&#65292;&#24182;&#22312;&#26356;&#20302;&#30340;&#32500;&#24230;&#19978;&#36827;&#34892;&#26377;&#25928;&#30340;&#21487;&#35270;&#21270;&#12290;&#29702;&#35770;&#19978;&#35777;&#26126;&#20102;&#39640;&#32500;&#25968;&#25454;&#30340;&#37051;&#36817;&#20851;&#31995;&#22312;&#20302;&#32500;&#31354;&#38388;&#20013;&#24471;&#21040;&#20102;&#20445;&#30041;&#12290;NeuroDAVIS&#30340;&#24615;&#33021;&#24050;&#22312;&#21508;&#31181;&#21512;&#25104;&#21644;&#23454;&#38469;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
The task of dimensionality reduction and visualization of high-dimensional datasets remains a challenging problem since long. Modern high-throughput technologies produce newer high-dimensional datasets having multiple views with relatively new data types. Visualization of these datasets require proper methodology that can uncover hidden patterns in the data without affecting the local and global structures within the data. To this end, however, very few such methodology exist, which can realise this task. In this work, we have introduced a novel unsupervised deep neural network model, called NeuroDAVIS, for data visualization. NeuroDAVIS is capable of extracting important features from the data, without assuming any data distribution, and visualize effectively in lower dimension. It has been shown theoritically that neighbourhood relationship of the data in high dimension remains preserved in lower dimension. The performance of NeuroDAVIS has been evaluated on a wide variety of synthet
&lt;/p&gt;</description></item><item><title>DoE2Vec &#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20219;&#20309;&#23454;&#39564;&#35774;&#35745;&#65288;DoE&#65289;&#30340;&#20449;&#24687;&#28508;&#22312;&#34920;&#36798;&#65292;&#24182;&#21487;&#20197;&#28385;&#36275;&#20248;&#21270;&#26223;&#35266;&#29305;&#24449;&#30340;&#19979;&#28216;&#20803;&#23398;&#20064;&#20219;&#21153;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#32463;&#20856;ELA&#20998;&#26512;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#38382;&#39064;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#19982;&#32463;&#20856;ELA&#29305;&#24449;&#20114;&#34917;&#20351;&#29992;&#26102;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01219</link><description>&lt;p&gt;
DoE2Vec&#65306;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#29305;&#24449;&#29992;&#20110;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
DoE2Vec: Deep-learning Based Features for Exploratory Landscape Analysis. (arXiv:2304.01219v1 [math.OC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01219
&lt;/p&gt;
&lt;p&gt;
DoE2Vec &#26159;&#19968;&#31181;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20219;&#20309;&#23454;&#39564;&#35774;&#35745;&#65288;DoE&#65289;&#30340;&#20449;&#24687;&#28508;&#22312;&#34920;&#36798;&#65292;&#24182;&#21487;&#20197;&#28385;&#36275;&#20248;&#21270;&#26223;&#35266;&#29305;&#24449;&#30340;&#19979;&#28216;&#20803;&#23398;&#20064;&#20219;&#21153;&#65292;&#21516;&#26102;&#36991;&#20813;&#20102;&#32463;&#20856;ELA&#20998;&#26512;&#20013;&#30340;&#29305;&#24449;&#24037;&#31243;&#38382;&#39064;&#12290;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#19982;&#32463;&#20856;ELA&#29305;&#24449;&#20114;&#34917;&#20351;&#29992;&#26102;&#65292;&#21487;&#26174;&#30528;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;DoE2Vec&#65292;&#36825;&#26159;&#19968;&#31181;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65288;VAE&#65289;&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#23398;&#20064;&#20248;&#21270;&#26223;&#35266;&#29305;&#24449;&#65292;&#20197;&#29992;&#20110;&#19979;&#28216;&#20803;&#23398;&#20064;&#20219;&#21153;&#65292;&#20363;&#22914;&#33258;&#21160;&#36873;&#25321;&#20248;&#21270;&#31639;&#27861;&#12290;&#36890;&#36807;&#20351;&#29992;&#38543;&#26426;&#20989;&#25968;&#29983;&#25104;&#22120;&#29983;&#25104;&#30340;&#22823;&#22411;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;DoE2Vec&#21487;&#20197;&#33258;&#23398;&#20064;&#20219;&#20309;&#23454;&#39564;&#35774;&#35745;&#65288;DoE&#65289;&#30340;&#20449;&#24687;&#28508;&#22312;&#34920;&#36798;&#12290;&#19982;&#32463;&#20856;&#30340;&#25506;&#32034;&#24615;&#26223;&#35266;&#20998;&#26512;&#65288;ELA&#65289;&#26041;&#27861;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#19981;&#38656;&#35201;&#20219;&#20309;&#29305;&#24449;&#24037;&#31243;&#65292;&#24182;&#19988;&#26131;&#20110;&#24212;&#29992;&#20110;&#39640;&#32500;&#25628;&#32034;&#31354;&#38388;&#12290;&#20026;&#20102;&#39564;&#35777;&#65292;&#25105;&#20204;&#26816;&#26597;&#20102;&#28508;&#22312;&#37325;&#24314;&#30340;&#36136;&#37327;&#65292;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#23454;&#39564;&#20998;&#26512;&#20102;&#28508;&#22312;&#34920;&#36798;&#24335;&#12290;&#36825;&#20123;&#28508;&#22312;&#34920;&#36798;&#24335;&#19981;&#20165;&#22312;&#35782;&#21035;&#31867;&#20284;&#65288;&#26131;&#20110;&#35780;&#20272;&#65289;&#30340;&#26367;&#20195;&#20989;&#25968;&#19978;&#26174;&#31034;&#20986;&#26377;&#21069;&#36884;&#30340;&#28508;&#21147;&#65292;&#32780;&#19988;&#22312;&#20998;&#31867;&#20219;&#21153;&#20013;&#19982;&#32463;&#20856;&#30340;ELA&#29305;&#24449;&#20114;&#34917;&#20351;&#29992;&#26102;&#20063;&#21487;&#20197;&#26174;&#30528;&#25552;&#21319;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose DoE2Vec, a variational autoencoder (VAE)-based methodology to learn optimization landscape characteristics for downstream meta-learning tasks, e.g., automated selection of optimization algorithms. Principally, using large training data sets generated with a random function generator, DoE2Vec self-learns an informative latent representation for any design of experiments (DoE). Unlike the classical exploratory landscape analysis (ELA) method, our approach does not require any feature engineering and is easily applicable for high dimensional search spaces. For validation, we inspect the quality of latent reconstructions and analyze the latent representations using different experiments. The latent representations not only show promising potentials in identifying similar (cheap-to-evaluate) surrogate functions, but also can significantly boost performances when being used complementary to the classical ELA features in classification tasks.
&lt;/p&gt;</description></item><item><title>POLAR-Express &#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23427;&#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#21644;&#36880;&#23618;&#20256;&#25773;&#25216;&#26415;&#65292;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01218</link><description>&lt;p&gt;
POLAR-Express: &#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#39640;&#25928;&#20934;&#30830;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
POLAR-Express: Efficient and Precise Formal Reachability Analysis of Neural-Network Controlled Systems. (arXiv:2304.01218v1 [eess.SY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01218
&lt;/p&gt;
&lt;p&gt;
POLAR-Express &#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479;&#30340;&#23433;&#20840;&#24615;&#12290;&#23427;&#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#21644;&#36880;&#23618;&#20256;&#25773;&#25216;&#26415;&#65292;&#21487;&#20197;&#20998;&#26512;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#65292;&#24182;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#25552;&#20379;&#20102;&#19968;&#31181;&#26356;&#26377;&#25928;&#30340;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25361;&#25112;&#24615;&#30340;&#25511;&#21046;&#38382;&#39064;&#19978;&#65292;&#25198;&#28436;&#25511;&#21046;&#22120;&#35282;&#33394;&#30340;&#31070;&#32463;&#32593;&#32476; (NN) &#23637;&#31034;&#20986;&#20102;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#23454;&#39564;&#24615;&#33021;&#12290;&#20294;&#31070;&#32463;&#32593;&#32476;&#25511;&#21046;&#31995;&#32479; (NNCS) &#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#22312;&#37319;&#29992;&#20063;&#24341;&#36215;&#20102;&#26085;&#30410;&#22686;&#38271;&#30340;&#23545;&#36825;&#20123; NNCS &#23433;&#20840;&#24615;&#30340;&#25285;&#24551;&#65292;&#29305;&#21035;&#26159;&#22312;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102; POLAR-Express&#65292;&#19968;&#31181;&#39640;&#25928;&#19988;&#20934;&#30830;&#30340;&#24418;&#24335;&#21487;&#36798;&#24615;&#20998;&#26512;&#24037;&#20855;&#65292;&#29992;&#20110;&#39564;&#35777; NNCS &#30340;&#23433;&#20840;&#24615;&#12290;POLAR-Express &#20351;&#29992; Taylor &#27169;&#22411;&#31639;&#26415;&#65292;&#36880;&#23618;&#27178;&#36328;&#31070;&#32463;&#32593;&#32476;&#26469;&#20256;&#25773; Taylor &#27169;&#22411; (TM) &#20197;&#35745;&#31639;&#31070;&#32463;&#32593;&#32476;&#20989;&#25968;&#30340;&#36817;&#20284;&#20540;&#12290;&#23427;&#21487;&#20197;&#29992;&#20110;&#20998;&#26512;&#20219;&#20309;&#20855;&#26377;&#36830;&#32493;&#28608;&#27963;&#21151;&#33021;&#30340;&#21069;&#39304;&#31070;&#32463;&#32593;&#32476;&#12290;&#25105;&#20204;&#36824;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312; ReLU &#28608;&#27963;&#20989;&#25968;&#19978;&#26356;&#26377;&#25928;&#22320;&#31934;&#30830;&#20256;&#25773; TM &#30340;&#26032;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;POLAR-Express &#20026;&#36880;&#23618;&#20256;&#25773;&#25552;&#20379;&#20102;&#24182;&#34892;&#35745;&#31639;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural networks (NNs) playing the role of controllers have demonstrated impressive empirical performances on challenging control problems. However, the potential adoption of NN controllers in real-life applications also gives rise to a growing concern over the safety of these neural-network controlled systems (NNCSs), especially when used in safety-critical applications. In this work, we present POLAR-Express, an efficient and precise formal reachability analysis tool for verifying the safety of NNCSs. POLAR-Express uses Taylor model arithmetic to propagate Taylor models (TMs) across a neural network layer-by-layer to compute an overapproximation of the neural-network function. It can be applied to analyze any feed-forward neural network with continuous activation functions. We also present a novel approach to propagate TMs more efficiently and precisely across ReLU activation functions. In addition, POLAR-Express provides parallel computation support for the layer-by-layer propagation
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102; MADAR &#31639;&#27861;&#65292;&#35299;&#20915;&#20102; Wi-Fi &#28459;&#28216;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230; Q &#23398;&#20064;&#65292;&#26377;&#25928;&#38477;&#20302;&#24310;&#36831;&#12290;</title><link>http://arxiv.org/abs/2304.01210</link><description>&lt;p&gt;
&#35770;&#25991;&#26631;&#39064;&#65306;IEEE 802.11ax Wi-Fi &#31995;&#32479;&#20013;&#30340;&#20998;&#24067;&#24335;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230; Q &#23398;&#20064;&#23454;&#29616;&#24555;&#36895;&#28459;&#28216;
&lt;/p&gt;
&lt;p&gt;
Distributed Multi-Agent Deep Q-Learning for Fast Roaming in IEEE 802.11ax Wi-Fi Systems. (arXiv:2304.01210v1 [cs.NI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01210
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102; MADAR &#31639;&#27861;&#65292;&#35299;&#20915;&#20102; Wi-Fi &#28459;&#28216;&#38382;&#39064;&#12290;&#35813;&#31639;&#27861;&#37319;&#29992;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230; Q &#23398;&#20064;&#65292;&#26377;&#25928;&#38477;&#20302;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Wi-Fi 6&#65292;&#21363; IEEE 802.11ax &#30340;&#21019;&#26032;&#65292;&#36890;&#36807;&#25552;&#39640;&#24310;&#36831;&#12289;&#21534;&#21520;&#37327;&#31561;&#22522;&#26412;&#24615;&#33021;&#26469;&#25913;&#36827;&#26080;&#32447;&#23616;&#22495;&#32593;&#65288;WLAN&#65289;&#30340;&#25216;&#26415;&#12290; &#27491;&#20132;&#39057;&#20998;&#22810;&#22336;&#65288;OFDMA&#65289;&#30340;&#20027;&#35201;&#25216;&#26415;&#29305;&#28857;&#25903;&#25345;&#22810;&#20010;&#29992;&#25143;&#36890;&#36807;&#30456;&#24212;&#30340;&#25509;&#20837;&#28857;&#65288;AP&#65289;&#24182;&#21457;&#20256;&#36755;&#21508;&#33258;&#30340;&#25968;&#25454;&#12290; &#20294;&#26159;&#65292;&#29992;&#20110; Wi-Fi &#28459;&#28216;&#30340;&#20256;&#32479; IEEE 802.11 &#21327;&#35758;&#20165;&#26681;&#25454;&#20174; AP &#25509;&#25910;&#21040;&#30340;&#21709;&#24212;&#24103;&#33719;&#21462;&#30340;&#25509;&#25910;&#20449;&#21495;&#24378;&#24230;&#25351;&#31034;&#65288;RSSI&#65289;&#36873;&#25321;&#30446;&#26631; AP&#12290; &#38271;&#26399;&#26469;&#30475;&#65292;&#22312;&#23494;&#38598;&#29992;&#25143;&#22330;&#26223;&#19979;&#65292;&#36825;&#21487;&#33021;&#20250;&#23548;&#33268;&#21333;&#20010;&#36890;&#36947;&#25317;&#22622;&#65292;&#36827;&#19968;&#27493;&#22686;&#21152;&#20851;&#32852;&#24310;&#36831;&#21644;&#25968;&#25454;&#21253;&#20002;&#22833;&#29575;&#65292;&#29978;&#33267;&#38477;&#20302;&#25972;&#20010;&#31995;&#32479;&#30340;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#12290; &#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#23454;&#29616;&#24555;&#36895;&#28459;&#28216;&#30340;&#22810;&#26234;&#33021;&#20307;&#28145;&#24230; Q &#23398;&#20064;&#31639;&#27861; - MADAR &#31639;&#27861;&#65292;&#20197;&#26377;&#25928;&#20943;&#23569;&#31449;&#28857;&#28459;&#28216;&#26102;&#30340;&#24310;&#36831;&#12290;
&lt;/p&gt;
&lt;p&gt;
The innovation of Wi-Fi 6, IEEE 802.11ax, was be approved as the next sixth-generation (6G) technology of wireless local area networks (WLANs) by improving the fundamental performance of latency, throughput, and so on. The main technical feature of orthogonal frequency division multiple access (OFDMA) supports multi-users to transmit respective data concurrently via the corresponding access points (APs). However, the conventional IEEE 802.11 protocol for Wi-Fi roaming selects the target AP only depending on received signal strength indication (RSSI) which is obtained by the received Response frame from the APs. In the long term, it may lead to congestion in a single channel under the scenarios of dense users further increasing the association delay and packet drop rate, even reducing the quality of service (QoS) of the overall system. In this paper, we propose a multi-agent deep Q-learning for fast roaming (MADAR) algorithm to effectively minimize the latency during the station roaming
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#25552;&#31034;&#30340;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#8221;&#27169;&#22411;&#65292;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2304.01209</link><description>&lt;p&gt;
PromptORE -- &#19968;&#31181;&#20840;&#26032;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PromptORE -- A Novel Approach Towards Fully Unsupervised Relation Extraction. (arXiv:2304.01209v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01209
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#8220;&#22522;&#20110;&#25552;&#31034;&#30340;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#8221;&#27169;&#22411;&#65292;&#22312;&#26080;&#30417;&#30563;&#35774;&#32622;&#19979;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#65292;&#23454;&#29616;&#20102;&#20840;&#26032;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#26088;&#22312;&#35782;&#21035;&#25991;&#26412;&#20013;&#23454;&#20307;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#32780;&#22312;&#35757;&#32451;&#26399;&#38388;&#27809;&#26377;&#26631;&#35760;&#30340;&#25968;&#25454;&#21487;&#29992;&#12290;&#36825;&#23545;&#20110;&#27809;&#26377;&#27880;&#37322;&#25968;&#25454;&#38598;&#30340;&#29305;&#23450;&#39046;&#22495;&#20851;&#31995;&#25277;&#21462;&#21644;&#20808;&#39564;&#26410;&#30693;&#20851;&#31995;&#31867;&#22411;&#30340;&#24320;&#25918;&#39046;&#22495;&#20851;&#31995;&#25277;&#21462;&#29305;&#21035;&#30456;&#20851;&#12290;&#34429;&#28982;&#26368;&#36817;&#30340;&#26041;&#27861;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#65292;&#20294;&#23427;&#20204;&#20005;&#37325;&#20381;&#36182;&#20110;&#36229;&#21442;&#25968;&#65292;&#35843;&#25972;&#36825;&#20123;&#36229;&#21442;&#25968;&#36890;&#24120;&#38656;&#35201;&#26631;&#35760;&#25968;&#25454;&#12290;&#20026;&#20102;&#20943;&#36731;&#23545;&#36229;&#21442;&#25968;&#30340;&#20381;&#36182;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;PromptORE&#65292;&#21363;&#8220;&#22522;&#20110;&#25552;&#31034;&#30340;&#24320;&#25918;&#20851;&#31995;&#25277;&#21462;&#8221;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#26032;&#30340;&#25552;&#31034;&#35843;&#25972;&#33539;&#20363;&#36866;&#24212;&#20110;&#26080;&#30417;&#30563;&#35774;&#32622;&#65292;&#24182;&#29992;&#23427;&#26469;&#23884;&#20837;&#34920;&#36798;&#20851;&#31995;&#30340;&#21477;&#23376;&#12290;&#28982;&#21518;&#25105;&#20204;&#23545;&#36825;&#20123;&#23884;&#20837;&#36827;&#34892;&#32858;&#31867;&#65292;&#21457;&#29616;&#20505;&#36873;&#20851;&#31995;&#65292;&#24182;&#23581;&#35797;&#19981;&#21516;&#30340;&#31574;&#30053;&#26469;&#33258;&#21160;&#20272;&#35745;&#36866;&#24403;&#30340;&#32858;&#31867;&#25968;&#37327;&#12290;&#25454;&#25105;&#20204;&#25152;&#30693;&#65292;PromptORE&#26159;&#31532;&#19968;&#20010;&#19981;&#38656;&#35201;&#36229;&#21442;&#25968;&#35843;&#25972;&#30340;&#26080;&#30417;&#30563;&#20851;&#31995;&#25277;&#21462;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Unsupervised Relation Extraction (RE) aims to identify relations between entities in text, without having access to labeled data during training. This setting is particularly relevant for domain specific RE where no annotated dataset is available and for open-domain RE where the types of relations are a priori unknown. Although recent approaches achieve promising results, they heavily depend on hyperparameters whose tuning would most often require labeled data. To mitigate the reliance on hyperparameters, we propose PromptORE, a ''Prompt-based Open Relation Extraction'' model. We adapt the novel prompt-tuning paradigm to work in an unsupervised setting, and use it to embed sentences expressing a relation. We then cluster these embeddings to discover candidate relations, and we experiment different strategies to automatically estimate an adequate number of clusters. To the best of our knowledge, PromptORE is the first unsupervised RE model that does not need hyperparameter tuning. Resul
&lt;/p&gt;</description></item><item><title>&#36827;&#21270;&#35745;&#31639;&#32467;&#21512;&#38544;&#31169;&#20445;&#25252;&#25104;&#20026;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20294;&#30446;&#21069;&#38544;&#31169;&#38382;&#39064;&#32570;&#20047;&#31995;&#32479;&#24615;&#30740;&#31350;&#12290;&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#19977;&#31181;&#20856;&#22411;&#30340;&#20248;&#21270;&#27169;&#24335;&#24182;&#25552;&#20986;&#20102;BOOM&#26469;&#25972;&#29702;&#38544;&#31169;&#38382;&#39064;&#12290;BOOM&#21253;&#25324;&#38544;&#31169;&#23450;&#20041;&#12289;&#38544;&#31169;&#38382;&#39064;&#20998;&#31867;&#12289;&#38544;&#31169;&#24863;&#30693;&#30340;&#36827;&#21270;&#35745;&#31639;&#26500;&#24314;&#21644;&#36890;&#36807;&#26696;&#20363;&#23637;&#31034;&#38544;&#31169;&#24863;&#30693;&#30340;&#36827;&#21270;&#35745;&#31639;&#12290;</title><link>http://arxiv.org/abs/2304.01205</link><description>&lt;p&gt;
&#24403;&#36827;&#21270;&#35745;&#31639;&#36935;&#35265;&#38544;&#31169;
&lt;/p&gt;
&lt;p&gt;
When Evolutionary Computation Meets Privacy. (arXiv:2304.01205v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01205
&lt;/p&gt;
&lt;p&gt;
&#36827;&#21270;&#35745;&#31639;&#32467;&#21512;&#38544;&#31169;&#20445;&#25252;&#25104;&#20026;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#65292;&#20294;&#30446;&#21069;&#38544;&#31169;&#38382;&#39064;&#32570;&#20047;&#31995;&#32479;&#24615;&#30740;&#31350;&#12290;&#35813;&#35770;&#25991;&#35752;&#35770;&#20102;&#19977;&#31181;&#20856;&#22411;&#30340;&#20248;&#21270;&#27169;&#24335;&#24182;&#25552;&#20986;&#20102;BOOM&#26469;&#25972;&#29702;&#38544;&#31169;&#38382;&#39064;&#12290;BOOM&#21253;&#25324;&#38544;&#31169;&#23450;&#20041;&#12289;&#38544;&#31169;&#38382;&#39064;&#20998;&#31867;&#12289;&#38544;&#31169;&#24863;&#30693;&#30340;&#36827;&#21270;&#35745;&#31639;&#26500;&#24314;&#21644;&#36890;&#36807;&#26696;&#20363;&#23637;&#31034;&#38544;&#31169;&#24863;&#30693;&#30340;&#36827;&#21270;&#35745;&#31639;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#12289;&#20998;&#24067;&#24335;&#35745;&#31639;&#21644;&#22823;&#25968;&#25454;&#25216;&#26415;&#25512;&#21160;&#20102;&#36827;&#21270;&#35745;&#31639;(EC)&#30340;&#21457;&#23637;&#65292;&#20351;&#24471;EC&#30340;&#30740;&#31350;&#26041;&#21521;&#19981;&#26029;&#25299;&#23637;&#65292;&#27604;&#22914;&#20998;&#24067;&#24335;&#30340;EC&#21644;&#22522;&#20110;&#26367;&#20195;&#27169;&#22411;&#30340;EC&#12290;&#36825;&#20123;&#36827;&#23637;&#26174;&#33879;&#25552;&#39640;&#20102;EC&#30340;&#24615;&#33021;&#21644;&#24212;&#29992;&#33539;&#22260;&#65292;&#20294;&#20063;&#20250;&#23548;&#33268;&#38544;&#31169;&#27844;&#38706;&#65292;&#27604;&#22914;&#26368;&#20248;&#32467;&#26524;&#21644;&#26367;&#20195;&#27169;&#22411;&#30340;&#27844;&#38706;&#12290;&#22240;&#27492;&#65292;&#36827;&#21270;&#35745;&#31639;&#19982;&#38544;&#31169;&#20445;&#25252;&#25104;&#20026;&#26032;&#20852;&#30340;&#30740;&#31350;&#26041;&#21521;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#36827;&#21270;&#35745;&#31639;&#20013;&#38544;&#31169;&#38382;&#39064;&#30340;&#31995;&#32479;&#24615;&#30740;&#31350;&#23578;&#19981;&#23436;&#21892;&#65292;&#29305;&#21035;&#26159;&#32570;&#20047;&#38544;&#31169;&#20445;&#25252;&#30340;&#23545;&#35937;&#12289;&#21160;&#26426;&#12289;&#20301;&#32622;&#21644;&#26041;&#27861;&#30340;&#25506;&#32034;&#12290;&#22240;&#27492;&#65292;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#19977;&#31181;&#20856;&#22411;&#30340;&#20248;&#21270;&#27169;&#24335;(&#21363;&#38598;&#20013;&#24335;&#20248;&#21270;&#12289;&#20998;&#24067;&#24335;&#20248;&#21270;&#21644;&#25968;&#25454;&#39537;&#21160;&#20248;&#21270;)&#26469;&#34920;&#24449;&#36827;&#21270;&#35745;&#31639;&#30340;&#20248;&#21270;&#27169;&#24335;&#65292;&#24182;&#25552;&#20986;&#20102;BOOM&#26469;&#25972;&#29702;&#36827;&#21270;&#35745;&#31639;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;BOOM&#21253;&#25324;&#22235;&#20010;&#37096;&#20998;&#65306;&#38544;&#31169;&#20445;&#25252;&#30340;&#23450;&#20041;&#12289;&#38544;&#31169;&#38382;&#39064;&#30340;&#20998;&#31867;&#12289;&#38544;&#31169;&#24863;&#30693;&#30340;&#36827;&#21270;&#35745;&#31639;&#30340;&#26500;&#24314;&#20197;&#21450;&#36890;&#36807;&#26696;&#20363;&#23637;&#31034;&#38544;&#31169;&#24863;&#30693;&#30340;&#36827;&#21270;&#35745;&#31639;&#12290;&#25105;&#20204;&#24076;&#26395;BOOM&#33021;&#22815;&#20316;&#20026;&#19968;&#20010;&#25351;&#21335;&#65292;&#32654;&#23398;&#22320;&#20998;&#26512;&#36827;&#21270;&#35745;&#31639;&#20013;&#30340;&#38544;&#31169;&#38382;&#39064;&#65292;&#24182;&#22312;&#26410;&#26469;&#30340;&#30740;&#31350;&#20013;&#25512;&#21160;&#38544;&#31169;&#24863;&#30693;&#30340;&#20248;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recently, evolutionary computation (EC) has been promoted by machine learning, distributed computing, and big data technologies, resulting in new research directions of EC like distributed EC and surrogate-assisted EC. These advances have significantly improved the performance and the application scope of EC, but also trigger privacy leakages, such as the leakage of optimal results and surrogate model. Accordingly, evolutionary computation combined with privacy protection is becoming an emerging topic. However, privacy concerns in evolutionary computation lack a systematic exploration, especially for the object, motivation, position, and method of privacy protection. To this end, in this paper, we discuss three typical optimization paradigms (i.e., \textit{centralized optimization, distributed optimization, and data-driven optimization}) to characterize optimization modes of evolutionary computation and propose BOOM to sort out privacy concerns in evolutionary computation. Specifically
&lt;/p&gt;</description></item><item><title>&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;AI&#36719;&#20214;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#32763;&#35793;&#21644;&#29983;&#25104;&#25554;&#22270;&#65292;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#65292;&#26368;&#20339;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#20851;&#38190;&#23383;&#30340;&#25552;&#31034;&#22686;&#24378;&#12290;&#36825;&#23545;&#20110;&#21521;&#24191;&#27867;&#30340;&#25991;&#23398;&#21463;&#20247;&#25552;&#20379;&#26174;&#30528;&#30340;&#25104;&#26412;&#25928;&#30410;&#25552;&#39640;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;</title><link>http://arxiv.org/abs/2304.01204</link><description>&lt;p&gt;
&#20799;&#31461;&#25925;&#20107;&#20070;&#20013;&#33402;&#26415;&#20316;&#21697;&#30340;&#33258;&#21160;&#22320;&#29702;&#23545;&#20934;
&lt;/p&gt;
&lt;p&gt;
Automatic Geo-alignment of Artwork in Children's Story Books. (arXiv:2304.01204v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01204
&lt;/p&gt;
&lt;p&gt;
&#36825;&#39033;&#30740;&#31350;&#35777;&#26126;AI&#36719;&#20214;&#21487;&#20197;&#22312;&#27809;&#26377;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#32763;&#35793;&#21644;&#29983;&#25104;&#25554;&#22270;&#65292;&#36890;&#36807;&#27604;&#36739;&#30740;&#31350;&#65292;&#26368;&#20339;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#20851;&#38190;&#23383;&#30340;&#25552;&#31034;&#22686;&#24378;&#12290;&#36825;&#23545;&#20110;&#21521;&#24191;&#27867;&#30340;&#25991;&#23398;&#21463;&#20247;&#25552;&#20379;&#26174;&#30528;&#30340;&#25104;&#26412;&#25928;&#30410;&#25552;&#39640;&#20855;&#26377;&#37325;&#35201;&#24847;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36827;&#34892;&#20102;&#19968;&#39033;&#30740;&#31350;&#65292;&#35777;&#26126;&#20102;&#21487;&#20197;&#20351;&#29992;AI&#36719;&#20214;&#22312;&#27809;&#26377;&#20154;&#31867;&#24178;&#39044;&#30340;&#24773;&#20917;&#19979;&#32763;&#35793;&#21644;&#29983;&#25104;&#25554;&#22270;&#12290;&#36825;&#26679;&#20570;&#30340;&#30446;&#30340;&#26159;&#23637;&#31034;&#32473;&#22806;&#37096;&#23458;&#25143;Pratham Books&#24182;&#36827;&#34892;&#20998;&#21457;&#12290;&#35813;&#39033;&#30446;&#31526;&#21512;&#20844;&#21496;&#30340;&#24895;&#26223;&#65292;&#21033;&#29992;&#26426;&#22120;&#23398;&#20064;&#31639;&#27861;&#30340;&#26222;&#36866;&#24615;&#21644;&#21487;&#25193;&#23637;&#24615;&#65292;&#20026;&#24191;&#27867;&#30340;&#25991;&#23398;&#21463;&#20247;&#22312;&#19981;&#21516;&#22320;&#29702;&#20301;&#32622;&#19978;&#25552;&#20379;&#26174;&#30528;&#30340;&#25104;&#26412;&#25928;&#30410;&#25552;&#39640;&#12290;&#37319;&#29992;&#27604;&#36739;&#30740;&#31350;&#26041;&#27861;&#65292;&#30830;&#23450;&#20102;3&#31181;&#26041;&#27861;&#20013;&#26368;&#20339;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#20351;&#29992;&#20851;&#38190;&#23383;&#30340;&#25552;&#31034;&#22686;&#24378;&#65292;CLIP&#23884;&#20837;&#25513;&#33180;&#21644;&#24102;&#26377;&#32534;&#36753;&#25552;&#31034;&#30340;&#20132;&#21449;&#27880;&#24847;&#21147;&#25511;&#21046;&#12290;&#20351;&#29992;&#23450;&#37327;&#21644;&#23450;&#24615;&#25514;&#26045;&#23436;&#25104;&#20102;&#24443;&#24213;&#30340;&#35780;&#20272;&#36807;&#31243;&#12290;&#27599;&#31181;&#26041;&#27861;&#37117;&#26377;&#33258;&#24049;&#30340;&#20248;&#28857;&#21644;&#32570;&#28857;&#65292;&#20294;&#36890;&#36807;&#35780;&#20272;&#65292;&#21457;&#29616;&#26041;&#27861;1&#30340;&#20135;&#37327;&#26368;&#20339;&#12290;&#26410;&#26469;&#26377;&#26395;&#36827;&#34892;&#26377;&#21069;&#36884;&#30340;&#36827;&#27493;&#65292;&#36827;&#19968;&#27493;&#25552;&#39640;&#22270;&#29255;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
A study was conducted to prove AI software could be used to translate and generate illustrations without any human intervention. This was done with the purpose of showing and distributing it to the external customer, Pratham Books. The project aligns with the company's vision by leveraging the generalisation and scalability of Machine Learning algorithms, offering significant cost efficiency increases to a wide range of literary audiences in varied geographical locations. A comparative study methodology was utilised to determine the best performant method out of the 3 devised, Prompt Augmentation using Keywords, CLIP Embedding Mask, and Cross Attention Control with Editorial Prompts. A thorough evaluation process was completed using both quantitative and qualitative measures. Each method had its own strengths and weaknesses, but through the evaluation, method 1 was found to have the best yielding results. Promising future advancements may be made to further increase image quality by in
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2304.01046</link><description>&lt;p&gt;
&#8220;Polytuplet Loss: &#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#8221;
&lt;/p&gt;
&lt;p&gt;
Polytuplet Loss: A Reverse Approach to Training Reading Comprehension and Logical Reasoning Models. (arXiv:2304.01046v1 [cs.CL] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01046
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#35757;&#32451;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#27169;&#22411;&#30340;&#21453;&#21521;&#26041;&#27861;&#65292;&#21033;&#29992;&#30456;&#23545;&#20934;&#30830;&#24615;&#30340;&#31574;&#30053;&#26469;&#35757;&#32451;&#27169;&#22411;&#65292;&#36890;&#36807;Polytuplet Loss&#20989;&#25968;&#26469;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#65292;&#33719;&#24471;&#20102;&#19981;&#38169;&#30340;&#25104;&#26524;&#65292;&#25552;&#20986;&#20102;&#20855;&#26377;&#19968;&#33324;&#24615;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#25972;&#20010;&#23398;&#26657;&#25945;&#32946;&#36807;&#31243;&#20013;&#65292;&#23398;&#29983;&#20204;&#23558;&#21463;&#21040;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#30340;&#32771;&#39564;&#12290;&#23398;&#29983;&#20204;&#24050;&#32463;&#24320;&#21457;&#20102;&#21508;&#31181;&#31574;&#30053;&#26469;&#23436;&#25104;&#27492;&#31867;&#32771;&#35797;&#65292;&#20854;&#20013;&#26377;&#20123;&#34987;&#35748;&#20026;&#26159;&#36890;&#24120;&#34920;&#29616;&#20248;&#20110;&#20854;&#20182;&#31574;&#30053;&#30340;&#12290;&#36825;&#26679;&#19968;&#31181;&#31574;&#30053;&#28041;&#21450;&#24378;&#35843;&#30456;&#23545;&#20934;&#30830;&#24615;&#32780;&#38750;&#32477;&#23545;&#20934;&#30830;&#24615;&#65292;&#29702;&#35770;&#19978;&#21487;&#20197;&#22312;&#19981;&#23436;&#20840;&#25484;&#25569;&#35299;&#39064;&#25152;&#38656;&#20449;&#24687;&#30340;&#24773;&#20917;&#19979;&#24471;&#20986;&#27491;&#30830;&#31572;&#26696;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#24212;&#29992;&#36825;&#31181;&#31574;&#30053;&#26469;&#35757;&#32451;&#36801;&#31227;&#23398;&#20064;&#27169;&#22411;&#20197;&#35299;&#20915;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#38382;&#39064;&#30340;&#26377;&#25928;&#24615;&#12290;&#36825;&#20123;&#27169;&#22411;&#22312;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#38405;&#35835;&#29702;&#35299;&#21644;&#36923;&#36753;&#25512;&#29702;&#22522;&#20934;&#25968;&#25454;&#38598;ReClor&#19978;&#36827;&#34892;&#20102;&#35780;&#20272;&#12290;&#23613;&#31649;&#20197;&#21069;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#36923;&#36753;&#25512;&#29702;&#25216;&#33021;&#65292;&#20294;&#25105;&#20204;&#19987;&#27880;&#20110;&#19968;&#31181;&#36890;&#29992;&#30340;&#35757;&#32451;&#26041;&#27861;&#21644;&#27169;&#22411;&#26550;&#26500;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;Polytuplet Loss&#20989;&#25968;&#65292;&#26159;&#19977;&#20803;&#32452;&#25439;&#22833;&#20989;&#25968;&#30340;&#25193;&#23637;&#65292;&#20197;&#30830;&#20445;&#20248;&#20808;&#23398;&#20064;&#31572;&#26696;&#36873;&#25321;&#30340;&#30456;&#23545;&#27491;&#30830;&#24615;&#32780;&#38750;&#23398;&#20064;&#32477;&#23545;&#27491;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Throughout schooling, students are tested on reading comprehension and logical reasoning. Students have developed various strategies for completing such exams, some of which are generally thought to outperform others. One such strategy involves emphasizing relative accuracy over absolute accuracy and can theoretically produce the correct answer without full knowledge of the information required to solve the question. This paper examines the effectiveness of applying such a strategy to train transfer learning models to solve reading comprehension and logical reasoning questions. The models were evaluated on the ReClor dataset, a challenging reading comprehension and logical reasoning benchmark. While previous studies targeted logical reasoning skills, we focus on a general training method and model architecture. We propose the polytuplet loss function, an extension of the triplet loss function, to ensure prioritization of learning the relative correctness of answer choices over learning
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#25366;&#25496;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20219;&#21153;&#27010;&#36848;&#12289;&#25216;&#26415;&#32508;&#36848;&#21644;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#35813;&#32508;&#36848;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2304.00485</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#25366;&#25496;&#30340;&#32593;&#32476;&#23433;&#20840;&#30740;&#31350;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
Graph Mining for Cybersecurity: A Survey. (arXiv:2304.00485v1 [cs.CR] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00485
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#22270;&#25366;&#25496;&#22312;&#32593;&#32476;&#23433;&#20840;&#20013;&#30340;&#24212;&#29992;&#65292;&#21253;&#25324;&#20219;&#21153;&#27010;&#36848;&#12289;&#25216;&#26415;&#32508;&#36848;&#21644;&#19981;&#21516;&#39046;&#22495;&#30340;&#24212;&#29992;&#12290;&#35813;&#32508;&#36848;&#23545;&#26410;&#26469;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#20170;&#32593;&#32476;&#25915;&#20987;&#65288;&#22914;&#24694;&#24847;&#36719;&#20214;&#12289;&#22403;&#22334;&#37038;&#20214;&#21644;&#20837;&#20405;&#31561;&#65289;&#30340;&#29467;&#22686;&#24050;&#32463;&#23545;&#31038;&#20250;&#36896;&#25104;&#20102;&#20005;&#37325;&#21518;&#26524;&#12290;&#20445;&#38556;&#32593;&#32476;&#23433;&#20840;&#24050;&#25104;&#20026;&#32452;&#32455;&#21644;&#25919;&#24220;&#30340;&#24403;&#21153;&#20043;&#24613;&#12290;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#34429;&#28982;&#24191;&#27867;&#24212;&#29992;&#20110;&#26816;&#27979;&#32593;&#32476;&#23041;&#32961;&#65292;&#20294;&#23427;&#20204;&#38590;&#20197;&#24314;&#27169;&#29616;&#23454;&#19990;&#30028;&#32593;&#32476;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#36817;&#24180;&#26469;&#65292;&#38543;&#30528;&#22270;&#25366;&#25496;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#35768;&#22810;&#30740;&#31350;&#32773;&#30740;&#31350;&#20102;&#36825;&#20123;&#25216;&#26415;&#65292;&#20197;&#25429;&#25417;&#32593;&#32476;&#23454;&#20307;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#65292;&#24182;&#21462;&#24471;&#20102;&#24456;&#39640;&#30340;&#24615;&#33021;&#12290;&#22240;&#27492;&#65292;&#24635;&#32467;&#29616;&#26377;&#22522;&#20110;&#22270;&#30340;&#32593;&#32476;&#23433;&#20840;&#35299;&#20915;&#26041;&#26696;&#65292;&#20026;&#26410;&#26469;&#30740;&#31350;&#25552;&#20379;&#25351;&#23548;&#26159;&#38750;&#24120;&#24517;&#35201;&#30340;&#12290;&#26412;&#25991;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#25366;&#25496;&#25216;&#26415;&#30340;&#32593;&#32476;&#23433;&#20840;&#30740;&#31350;&#32508;&#36848;&#65292;&#21253;&#25324;&#32593;&#32476;&#23433;&#20840;&#20219;&#21153;&#30340;&#27010;&#36848;&#65292;&#20856;&#22411;&#30340;&#22270;&#25366;&#25496;&#25216;&#26415;&#65292;&#24212;&#29992;&#23427;&#20204;&#21040;&#32593;&#32476;&#23433;&#20840;&#30340;&#19968;&#33324;&#36807;&#31243;&#65292;&#20197;&#21450;&#19981;&#21516;&#32593;&#32476;&#23433;&#20840;&#38382;&#39064;&#30340;&#21508;&#31181;&#35299;&#20915;&#26041;&#26696;&#12290;
&lt;/p&gt;
&lt;p&gt;
The explosive growth of cyber attacks nowadays, such as malware, spam, and intrusions, caused severe consequences on society. Securing cyberspace has become an utmost concern for organizations and governments. Traditional Machine Learning (ML) based methods are extensively used in detecting cyber threats, but they hardly model the correlations between real-world cyber entities. In recent years, with the proliferation of graph mining techniques, many researchers investigated these techniques for capturing correlations between cyber entities and achieving high performance. It is imperative to summarize existing graph-based cybersecurity solutions to provide a guide for future studies. Therefore, as a key contribution of this paper, we provide a comprehensive review of graph mining for cybersecurity, including an overview of cybersecurity tasks, the typical graph mining techniques, and the general process of applying them to cybersecurity, as well as various solutions for different cybers
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;RL&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#38544;&#34255;&#22312;&#20195;&#29702;&#20013;&#30340;&#21518;&#38376;&#12290;</title><link>http://arxiv.org/abs/2304.00252</link><description>&lt;p&gt;
RL&#20013;&#30340;&#21453;&#21521;&#25915;&#20987;&#20445;&#25252;&#65306;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Recover Triggered States: Protect Model Against Backdoor Attack in Reinforcement Learning. (arXiv:2304.00252v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.00252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#26041;&#27861;&#65292;&#29992;&#20110;&#20445;&#25252;RL&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290;&#35813;&#26041;&#27861;&#28041;&#21450;&#26500;&#24314;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#65292;&#24182;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#38544;&#34255;&#22312;&#20195;&#29702;&#20013;&#30340;&#21518;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21453;&#21521;&#25915;&#20987;&#21487;&#20197;&#20351;&#24694;&#24847;&#29992;&#25143;&#25805;&#32437;&#29615;&#22659;&#25110;&#30772;&#22351;&#35757;&#32451;&#25968;&#25454;&#65292;&#24182;&#23558;&#19968;&#20010;&#38544;&#34255;&#30340;&#21518;&#38376;&#25554;&#20837;&#21040;&#35757;&#32451;&#20195;&#29702;&#31243;&#24207;&#20013;&#12290;&#36825;&#31181;&#25915;&#20987;&#21361;&#21450;RL&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#65292;&#22312;&#21508;&#20010;&#20851;&#38190;&#39046;&#22495;&#21487;&#33021;&#20250;&#36896;&#25104;&#28798;&#38590;&#24615;&#30340;&#24433;&#21709;&#12290;&#19982;&#27492;&#30456;&#27604;&#65292;&#23545;&#20110;RL&#20013;&#30340;&#21453;&#21521;&#25915;&#20987;&#26377;&#25928;&#30340;&#38450;&#24481;&#25514;&#26045;&#30340;&#30740;&#31350;&#30456;&#23545;&#36739;&#23569;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#8212;&#8212;&#24674;&#22797;&#35302;&#21457;&#29366;&#24577;(RTS)&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#20445;&#25252;&#21463;&#23475;&#20195;&#29702;&#20813;&#21463;&#21453;&#21521;&#25915;&#20987;&#12290; RTS&#38656;&#35201;&#26500;&#24314;&#19968;&#20010;&#26367;&#20195;&#32593;&#32476;&#26469;&#36817;&#20284;&#21160;&#24577;&#27169;&#22411;&#12290;&#24320;&#21457;&#20154;&#21592;&#21487;&#20197;&#36890;&#36807;&#23558;&#35302;&#21457;&#29366;&#24577;&#24674;&#22797;&#20026;&#24178;&#20928;&#29366;&#24577;&#26469;&#38450;&#27490;&#25915;&#20987;&#32773;&#36890;&#36807;&#35302;&#21457;&#22120;&#28608;&#27963;&#20195;&#29702;&#20013;&#38544;&#34255;&#30340;&#21518;&#38376;&#12290;&#22312;&#35757;&#32451;&#26367;&#20195;&#32593;&#32476;&#26469;&#39044;&#27979;&#29366;&#24577;&#26102;&#65292;&#25105;&#20204;&#23558;&#20195;&#29702;&#21160;&#20316;&#20449;&#24687;&#24182;&#20837;&#65292;&#20943;&#23569;&#20195;&#29702;&#22312;&#39044;&#27979;&#29366;&#24577;&#19978;&#37319;&#21462;&#30340;&#21160;&#20316;&#21644;&#23454;&#38469;&#29366;&#24577;&#19978;&#37319;&#21462;&#30340;&#21160;&#20316;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;
A backdoor attack allows a malicious user to manipulate the environment or corrupt the training data, thus inserting a backdoor into the trained agent. Such attacks compromise the RL system's reliability, leading to potentially catastrophic results in various key fields. In contrast, relatively limited research has investigated effective defenses against backdoor attacks in RL. This paper proposes the Recovery Triggered States (RTS) method, a novel approach that effectively protects the victim agents from backdoor attacks. RTS involves building a surrogate network to approximate the dynamics model. Developers can then recover the environment from the triggered state to a clean state, thereby preventing attackers from activating backdoors hidden in the agent by presenting the trigger. When training the surrogate to predict states, we incorporate agent action information to reduce the discrepancy between the actions taken by the agent on predicted states and the actions taken on real sta
&lt;/p&gt;</description></item><item><title>oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.17612</link><description>&lt;p&gt;
oBERTa: &#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#21644;&#21098;&#26525;&#26469;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
oBERTa: Improving Sparse Transfer Learning via improved initialization, distillation, and pruning regimes. (arXiv:2303.17612v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.17612
&lt;/p&gt;
&lt;p&gt;
oBERTa&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#36890;&#36807;&#25913;&#36827;&#21021;&#22987;&#21270;&#12289;&#33976;&#39311;&#12289;&#21098;&#26525;&#31561;&#25216;&#26415;&#65292;&#21487;&#20197;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#25552;&#39640;&#31232;&#30095;&#36801;&#31227;&#23398;&#20064;&#30340;&#25928;&#29575;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;oBERTa&#35821;&#35328;&#27169;&#22411;&#30340;&#33539;&#22260;&#65292;&#23427;&#26159;&#19968;&#32452;&#26131;&#20110;&#20351;&#29992;&#30340;&#35821;&#35328;&#27169;&#22411;&#65292;&#20801;&#35768;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#20174;&#19994;&#32773;&#22312;&#19981;&#38656;&#35201;&#27169;&#22411;&#21387;&#32553;&#26041;&#38754;&#30340;&#19987;&#19994;&#30693;&#35782;&#30340;&#24773;&#20917;&#19979;&#33719;&#24471;3.8&#21040;24.3&#20493;&#30340;&#26356;&#24555;&#36895;&#30340;&#27169;&#22411;&#12290;oBERTa&#25193;&#23637;&#20102;&#29616;&#26377;&#30340;&#21098;&#26525;&#12289;&#30693;&#35782;&#33976;&#39311;&#21644;&#37327;&#21270;&#24037;&#20316;&#65292;&#24182;&#21033;&#29992;&#20923;&#32467;&#30340;&#23884;&#20837;&#26469;&#25913;&#36827;&#30693;&#35782;&#33976;&#39311;&#65292;&#24182;&#25913;&#36827;&#27169;&#22411;&#21021;&#22987;&#21270;&#65292;&#20197;&#22312;&#24191;&#27867;&#30340;&#20256;&#36882;&#20219;&#21153;&#19978;&#25552;&#20379;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#29983;&#25104;oBERTa&#26102;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#39640;&#24230;&#20248;&#21270;&#30340;RoBERTa&#19982;BERT&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#26399;&#38388;&#21098;&#26525;&#26041;&#38754;&#30340;&#19981;&#21516;&#20043;&#22788;&#65292;&#24182;&#21457;&#29616;&#23427;&#22312;&#24494;&#35843;&#26399;&#38388;&#19981;&#22826;&#36866;&#21512;&#21387;&#32553;&#12290;&#25105;&#20204;&#25506;&#32034;&#20102;oBERTa&#22312;&#19971;&#20010;&#20855;&#26377;&#20195;&#34920;&#24615;&#30340;NLP&#20219;&#21153;&#19978;&#30340;&#20351;&#29992;&#65292;&#24182;&#21457;&#29616;&#25913;&#36827;&#30340;&#21387;&#32553;&#25216;&#26415;&#20351;&#24471;&#32463;&#36807;&#21098;&#26525;&#30340;oBERTa&#27169;&#22411;&#33021;&#22815;&#21305;&#37197;BERTBASE&#30340;&#24615;&#33021;&#65292;&#24182;&#36229;&#36807;SQUAD V1.1&#38382;&#31572;&#25968;&#25454;&#30340;Prune OFA Large&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we introduce the range of oBERTa language models, an easy-to-use set of language models, which allows Natural Language Processing (NLP) practitioners to obtain between 3.8 and 24.3 times faster models without expertise in model compression. Specifically, oBERTa extends existing work on pruning, knowledge distillation, and quantization and leverages frozen embeddings to improve knowledge distillation, and improved model initialization to deliver higher accuracy on a a broad range of transfer tasks. In generating oBERTa, we explore how the highly optimized RoBERTa differs from the BERT with respect to pruning during pre-training and fine-tuning and find it less amenable to compression during fine-tuning. We explore the use of oBERTa on a broad seven representative NLP tasks and find that the improved compression techniques allow a pruned oBERTa model to match the performance of BERTBASE and exceed the performance of Prune OFA Large on the SQUAD V1.1 Question Answering data
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#21644;&#20943;&#23567;GAHP&#20013;&#25805;&#32437;&#32773;&#30340;&#24433;&#21709;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#22522;&#20110;&#25805;&#32437;&#32773;&#25552;&#20379;&#30340;&#21028;&#26029;&#20026;&#24322;&#24120;&#20540;&#30340;&#20551;&#35774;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#20551;&#35774;&#19981;&#35802;&#23454;&#30340;&#21028;&#26029;&#27604;&#32676;&#20307;&#30340;&#24179;&#22343;&#19968;&#33268;&#24615;&#23569;&#19968;&#33268;&#24615;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#26377;&#25928;&#22320;&#20445;&#35777;&#32676;&#20307;&#20849;&#35782;&#30340;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2303.15099</link><description>&lt;p&gt;
AHP&#20013;&#23433;&#20840;&#30340;&#20915;&#31574;&#32858;&#21512;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards secure judgments aggregation in AHP. (arXiv:2303.15099v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.15099
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20004;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#29992;&#20110;&#26816;&#27979;&#21644;&#20943;&#23567;GAHP&#20013;&#25805;&#32437;&#32773;&#30340;&#24433;&#21709;&#12290;&#31532;&#19968;&#31181;&#26041;&#27861;&#26159;&#22522;&#20110;&#25805;&#32437;&#32773;&#25552;&#20379;&#30340;&#21028;&#26029;&#20026;&#24322;&#24120;&#20540;&#30340;&#20551;&#35774;&#65292;&#31532;&#20108;&#31181;&#26041;&#27861;&#20551;&#35774;&#19981;&#35802;&#23454;&#30340;&#21028;&#26029;&#27604;&#32676;&#20307;&#30340;&#24179;&#22343;&#19968;&#33268;&#24615;&#23569;&#19968;&#33268;&#24615;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#26377;&#25928;&#22320;&#20445;&#35777;&#32676;&#20307;&#20849;&#35782;&#30340;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20915;&#31574;&#26041;&#27861;&#20013;&#65292;&#36890;&#24120;&#20551;&#35774;&#19987;&#23478;&#35802;&#23454;&#21644;&#19987;&#19994;&#12290;&#20294;&#26159;&#65292;&#22312;&#32676;&#20307;&#20915;&#31574;&#26694;&#26550;&#20013;&#65292;&#22914;&#32676;&#20307;&#23618;&#27425;&#20998;&#26512;&#36807;&#31243;&#65288;GAHP&#65289;&#20013;&#65292;&#19968;&#20010;&#25110;&#22810;&#20010;&#19987;&#23478;&#35797;&#22270;&#25805;&#32437;&#32467;&#26524;&#20197;&#31526;&#21512;&#33258;&#24049;&#30340;&#21033;&#30410;&#12290;&#26412;&#25991;&#26088;&#22312;&#20171;&#32461;GAHP&#20013;&#30340;&#20004;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#65292;&#36890;&#36807;&#20943;&#23567;&#26435;&#37325;&#26469;&#26816;&#27979;&#25805;&#32437;&#32773;&#24182;&#26368;&#23567;&#21270;&#20854;&#23545;&#32676;&#20307;&#20849;&#35782;&#30340;&#24433;&#21709;&#12290;&#31532;&#19968;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#22522;&#20110;&#36825;&#26679;&#30340;&#20551;&#35774;&#65292;&#21363;&#25805;&#32437;&#32773;&#25552;&#20379;&#30340;&#21028;&#26029;&#21487;&#20197;&#34987;&#35748;&#20026;&#26159;&#30456;&#23545;&#20110;&#32676;&#20307;&#20013;&#20854;&#20313;&#19987;&#23478;&#30340;&#21028;&#26029;&#32780;&#35328;&#30340;&#24322;&#24120;&#20540;&#12290;&#31532;&#20108;&#31181;&#21551;&#21457;&#24335;&#26041;&#27861;&#20551;&#23450;&#19981;&#35802;&#23454;&#30340;&#21028;&#26029;&#27604;&#32676;&#20307;&#30340;&#24179;&#22343;&#19968;&#33268;&#24615;&#23569;&#19968;&#33268;&#24615;&#12290;&#20004;&#31181;&#26041;&#27861;&#37117;&#36890;&#36807;&#25968;&#20540;&#21644;&#27169;&#25311;&#31034;&#20363;&#36827;&#34892;&#20102;&#28436;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
In decision-making methods, it is common to assume that the experts are honest and professional. However, this is not the case when one or more experts in the group decision making framework, such as the group analytic hierarchy process (GAHP), try to manipulate results in their favor. The aim of this paper is to introduce two heuristics in the GAHP, setting allowing to detect the manipulators and minimize their effect on the group consensus by diminishing their weights. The first heuristic is based on the assumption that manipulators will provide judgments which can be considered outliers with respect to those of the rest of the experts in the group. The second heuristic assumes that dishonest judgments are less consistent than the average consistency of the group. Both approaches are illustrated with numerical examples and simulations.
&lt;/p&gt;</description></item><item><title>HAT&#26159;&#19968;&#31181;&#35745;&#31639;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#32467;&#26500;&#21644;&#31616;&#21270;&#30340;&#20985;&#35270;&#32593;&#33180;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#30446;&#26631;&#23384;&#22312;&#21644;&#30446;&#26631;&#32570;&#22833;&#25628;&#32034;&#26399;&#38388;&#27880;&#35270;&#34892;&#20026;&#30340;&#25195;&#25551;&#36335;&#24452;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;</title><link>http://arxiv.org/abs/2303.09383</link><description>&lt;p&gt;
&#20351;&#29992;&#35745;&#31639;&#27880;&#24847;&#21147;&#39044;&#27979;&#20154;&#31867;&#35270;&#35273;&#27880;&#24847;&#21147;
&lt;/p&gt;
&lt;p&gt;
Predicting Human Attention using Computational Attention. (arXiv:2303.09383v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09383
&lt;/p&gt;
&lt;p&gt;
HAT&#26159;&#19968;&#31181;&#35745;&#31639;&#27880;&#24847;&#21147;&#27169;&#22411;&#65292;&#20351;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#32467;&#26500;&#21644;&#31616;&#21270;&#30340;&#20985;&#35270;&#32593;&#33180;&#65292;&#23454;&#29616;&#20102;&#23545;&#20110;&#30446;&#26631;&#23384;&#22312;&#21644;&#30446;&#26631;&#32570;&#22833;&#25628;&#32034;&#26399;&#38388;&#27880;&#35270;&#34892;&#20026;&#30340;&#25195;&#25551;&#36335;&#24452;&#30340;&#26368;&#26032;&#25216;&#26415;&#27700;&#24179;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22810;&#25968;&#35270;&#35273;&#27880;&#24847;&#21147;&#27169;&#22411;&#26088;&#22312;&#39044;&#27979;&#33258;&#19978;&#32780;&#19979;&#25110;&#33258;&#19979;&#32780;&#19978;&#25511;&#21046;&#65292;&#37319;&#29992;&#19981;&#21516;&#30340;&#35270;&#35273;&#25628;&#32034;&#21644;&#33258;&#30001;&#35266;&#30475;&#20219;&#21153;&#36827;&#34892;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31216;&#20026;&#20154;&#31867;&#27880;&#24847;&#21147;&#21464;&#25442;&#22120;&#30340;&#21333;&#19968;&#27169;&#22411;&#65292;&#21487;&#20197;&#39044;&#27979;&#36825;&#20004;&#31181;&#24418;&#24335;&#30340;&#27880;&#24847;&#21147;&#25511;&#21046;&#12290;HAT&#22312;&#39044;&#27979;&#30446;&#26631;&#23384;&#22312;&#21644;&#30446;&#26631;&#32570;&#22833;&#25628;&#32034;&#26399;&#38388;&#36827;&#34892;&#27880;&#35270;&#34892;&#20026;&#30340;&#25195;&#25551;&#36335;&#24452;&#26041;&#38754;&#26159;&#26032;&#30340;&#26368;&#20808;&#36827;&#25216;&#26415;&#65292;&#22312;&#39044;&#27979;&#26080;&#20219;&#21153;&#33258;&#30001;&#35266;&#30475;&#27880;&#35270;&#36335;&#24452;&#26041;&#38754;&#21305;&#37197;&#25110;&#36229;&#36807;&#20102;&#24403;&#21069;&#25216;&#26415;&#27700;&#24179;&#12290;HAT&#36890;&#36807;&#20351;&#29992;&#26032;&#39062;&#30340;&#22522;&#20110;&#21464;&#25442;&#22120;&#30340;&#32467;&#26500;&#21644;&#31616;&#21270;&#30340;&#20985;&#35270;&#32593;&#33180;&#65292;&#20849;&#21516;&#21019;&#24314;&#31867;&#20284;&#20110;&#20154;&#31867;&#21160;&#24577;&#35270;&#35273;&#24037;&#20316;&#35760;&#24518;&#30340;&#26102;&#31354;&#24847;&#35782;&#65292;&#23454;&#29616;&#20102;&#36825;&#31181;&#26032;&#30340;&#25216;&#26415;&#27700;&#24179;&#12290;&#19982;&#20197;&#21069;&#20381;&#36182;&#20110;&#31895;&#31961;&#30340;&#27880;&#35270;&#21333;&#20803;&#26684;&#32593;&#26684;&#24182;&#30001;&#20110;&#31163;&#25955;&#21270;&#22266;&#23450;&#32780;&#32463;&#21382;&#20449;&#24687;&#20002;&#22833;&#30340;&#26041;&#27861;&#19981;&#21516;&#65292;HAT&#20855;&#26377;&#23494;&#38598;&#39044;&#27979;&#26550;&#26500;&#65292;&#24182;&#20026;&#27599;&#20010;&#27880;&#35270;&#36755;&#20986;&#23494;&#38598;&#28909;&#22270;&#65292;&#20174;&#32780;&#36991;&#20813;&#31163;&#25955;&#27880;&#35270;&#12290;HAT&#22312;&#35745;&#31639;&#35270;&#35273;&#27880;&#24847;&#21147;&#26041;&#38754;&#35774;&#23450;&#20102;&#19968;&#20010;&#26032;&#30340;&#26631;&#20934;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most models of visual attention are aimed at predicting either top-down or bottom-up control, as studied using different visual search and free-viewing tasks. We propose Human Attention Transformer (HAT), a single model predicting both forms of attention control. HAT is the new state-of-the-art (SOTA) in predicting the scanpath of fixations made during target-present and target-absent search, and matches or exceeds SOTA in the prediction of taskless free-viewing fixation scanpaths. HAT achieves this new SOTA by using a novel transformer-based architecture and a simplified foveated retina that collectively create a spatio-temporal awareness akin to the dynamic visual working memory of humans. Unlike previous methods that rely on a coarse grid of fixation cells and experience information loss due to fixation discretization, HAT features a dense-prediction architecture and outputs a dense heatmap for each fixation, thus avoiding discretizing fixations. HAT sets a new standard in computati
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26426;&#22120;&#20154;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#20013;&#30340;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;100&#20010;&#23454;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;</title><link>http://arxiv.org/abs/2303.06531</link><description>&lt;p&gt;
&#38754;&#21521;&#33258;&#20027;&#28165;&#27905;&#30340;&#22810;&#26426;&#22120;&#20154;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#30340;&#23454;&#36341;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Practical Multi-Robot Hybrid Tasks Allocation for Autonomous Cleaning. (arXiv:2303.06531v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.06531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#29992;&#20110;&#35299;&#20915;&#22810;&#26426;&#22120;&#20154;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#20013;&#30340;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#38382;&#39064;&#65292;&#24182;&#24314;&#31435;&#20102;&#19968;&#20010;&#21253;&#25324;100&#20010;&#23454;&#20363;&#30340;&#25968;&#25454;&#38598;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a novel robust mixed-integer linear programming model for multi-robot hybrid-task allocation in uncertain autonomous cleaning systems, and establishes a dataset of 100 instances.
&lt;/p&gt;
&lt;p&gt;
&#20219;&#21153;&#20998;&#37197;&#22312;&#22810;&#26426;&#22120;&#20154;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#65292;&#22810;&#20010;&#26426;&#22120;&#20154;&#19968;&#36215;&#24037;&#20316;&#20197;&#28165;&#27905;&#22823;&#38754;&#31215;&#21306;&#22495;&#12290;&#28982;&#32780;&#65292;&#36804;&#20170;&#20026;&#27490;&#30456;&#20851;&#30740;&#31350;&#23384;&#22312;&#20960;&#20010;&#38382;&#39064;&#12290;&#22823;&#22810;&#25968;&#24403;&#21069;&#30740;&#31350;&#20027;&#35201;&#20851;&#27880;&#20110;&#30830;&#23450;&#24615;&#30340;&#21333;&#20219;&#21153;&#20998;&#37197;&#65292;&#32780;&#19981;&#32771;&#34385;&#19981;&#30830;&#23450;&#24037;&#20316;&#29615;&#22659;&#20013;&#30340;&#28151;&#21512;&#20219;&#21153;&#12290;&#27492;&#22806;&#65292;&#32570;&#20047;&#30456;&#20851;&#30740;&#31350;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#20026;&#19981;&#30830;&#23450;&#30340;&#33258;&#20027;&#28165;&#27905;&#31995;&#32479;&#30340;&#22810;&#26426;&#22120;&#20154;&#28151;&#21512;&#20219;&#21153;&#20998;&#37197;&#20570;&#20986;&#20102;&#36129;&#29486;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#36890;&#36807;&#40065;&#26834;&#20248;&#21270;&#27169;&#22411;&#26469;&#24314;&#27169;&#28165;&#27905;&#29615;&#22659;&#20013;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#40065;&#26834;&#28151;&#21512;&#25972;&#25968;&#32447;&#24615;&#35268;&#21010;&#27169;&#22411;&#65292;&#20854;&#20013;&#21253;&#25324;&#28151;&#21512;&#28165;&#27905;&#20219;&#21153;&#39034;&#24207;&#21644;&#26426;&#22120;&#20154;&#30340;&#33021;&#21147;&#31561;&#23454;&#38469;&#32422;&#26463;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#24314;&#31435;&#20102;&#19968;&#20010;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;100&#20010;&#23454;&#20363;&#65292;&#27599;&#20010;&#23454;&#20363;&#37117;&#26377;2D&#25163;&#21160;&#26631;&#35760;&#30340;&#22270;&#20687;&#21644;3D&#27169;&#22411;&#12290;&#31532;&#19977;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#20851;&#20110;&#25152;&#25552;&#20986;&#27169;&#22411;&#30340;&#20840;&#38754;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Task allocation plays a vital role in multi-robot autonomous cleaning systems, where multiple robots work together to clean a large area. However, there are several problems in relevant research to date. Most current studies mainly focus on deterministic, single-task allocation for cleaning robots, without considering hybrid tasks in uncertain working environments. Moreover, there is a lack of datasets and benchmarks for relevant research. In this paper, we contribute to multi-robot hybrid-task allocation for uncertain autonomous cleaning systems by addressing these problems. First, we model the uncertainties in the cleaning environment via robust optimization and propose a novel robust mixed-integer linear programming model with practical constraints including hybrid cleaning task order and robot's ability. Second, we establish a dataset of 100 instances made from floor plans, each of which has 2D manually-labeled images and a 3D model. Third, we provide comprehensive results on the c
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22810;&#35821;&#35328;&#20013;&#36827;&#34892;&#35821;&#35328;&#26080;&#20851;&#30340;&#26080;&#30417;&#30563;&#20998;&#35789;&#30340;&#20803;&#23398;&#20064;&#21487;&#33021;&#24615;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#36866;&#24212;&#24230;&#20989;&#25968;&#33258;&#21160;&#30830;&#23450;&#26080;&#30417;&#30563;&#20998;&#35789;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#12290;&#32467;&#26524;&#34920;&#26126;&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#20013;&#65292;&#21069;&#19977;&#31181;&#24230;&#37327;&#30340;&#21152;&#24615;&#32452;&#21512;&#19982; F1 &#20998;&#35789;&#24471;&#20998;&#20043;&#38388;&#26377;&#30456;&#24403;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#20013;&#25991;&#20013;&#65292;F1 &#24471;&#20998;&#19982;&#21387;&#32553;&#22240;&#23376;&#26377;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.02427</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#36229;&#21442;&#25968;&#35843;&#33410;&#22312;&#26080;&#30417;&#30563;&#36328;&#35821;&#31181;&#20998;&#35789;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Self-tuning hyper-parameters for unsupervised cross-lingual tokenization. (arXiv:2303.02427v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.02427
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#22312;&#22810;&#35821;&#35328;&#20013;&#36827;&#34892;&#35821;&#35328;&#26080;&#20851;&#30340;&#26080;&#30417;&#30563;&#20998;&#35789;&#30340;&#20803;&#23398;&#20064;&#21487;&#33021;&#24615;&#65292;&#24182;&#20351;&#29992;&#22810;&#31181;&#36866;&#24212;&#24230;&#20989;&#25968;&#33258;&#21160;&#30830;&#23450;&#26080;&#30417;&#30563;&#20998;&#35789;&#27169;&#22411;&#30340;&#36229;&#21442;&#25968;&#12290;&#32467;&#26524;&#34920;&#26126;&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#20013;&#65292;&#21069;&#19977;&#31181;&#24230;&#37327;&#30340;&#21152;&#24615;&#32452;&#21512;&#19982; F1 &#20998;&#35789;&#24471;&#20998;&#20043;&#38388;&#26377;&#30456;&#24403;&#22909;&#30340;&#30456;&#20851;&#24615;&#65292;&#22312;&#20013;&#25991;&#20013;&#65292;F1 &#24471;&#20998;&#19982;&#21387;&#32553;&#22240;&#23376;&#26377;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#33521;&#35821;&#12289;&#20420;&#35821;&#21644;&#20013;&#25991;&#31561;&#22810;&#31181;&#35821;&#35328;&#20013;&#36827;&#34892;&#35821;&#35328;&#26080;&#20851;&#30340;&#26080;&#30417;&#30563;&#20998;&#35789;&#38382;&#39064;&#30340;&#20803;&#23398;&#20064;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#20351;&#29992;&#19981;&#21516;&#30340;&#20154;&#31867;&#26080;&#20851;&#36866;&#24212;&#24230;&#20989;&#25968;&#65292;&#22914;&#26631;&#20934;&#21270;&#21453;&#29109;&#12289;&#21387;&#32553;&#22240;&#23376;&#21644;&#20132;&#21449;&#20998;&#21106; F1 &#24471;&#20998;&#65292;&#20197;&#21450;&#19977;&#20010;&#24230;&#37327;&#30340;&#21152;&#24615;&#21644;&#20056;&#24615;&#32452;&#21512;&#65292;&#23454;&#29616;&#20102;&#23545;&#26089;&#26399;&#20316;&#21697;&#25552;&#20986;&#30340;&#26080;&#30417;&#30563;&#20998;&#35789;&#27169;&#22411;&#36229;&#21442;&#25968;&#30340;&#33258;&#21160;&#30830;&#23450;&#30340;&#20803;&#23398;&#20064;&#26041;&#27861;&#65292;&#24182;&#27979;&#35797;&#20854;&#19982;&#20256;&#32479; F1 &#20998;&#35789;&#24471;&#20998;&#30340;&#20851;&#31995;&#12290;&#25105;&#20204;&#21457;&#29616;&#22312;&#33521;&#35821;&#21644;&#20420;&#35821;&#26041;&#38754;&#65292;&#22312;&#21069;&#19977;&#31181;&#24230;&#37327;&#30340;&#21152;&#24615;&#32452;&#21512;&#19982;&#21518;&#32773;&#20043;&#38388;&#26377;&#30456;&#24403;&#22909;&#30340;&#30456;&#20851;&#24615;&#12290;&#22312;&#20013;&#25991;&#26041;&#38754;&#65292;&#25105;&#20204;&#21457;&#29616; F1 &#24471;&#20998;&#19982;&#21387;&#32553;&#22240;&#23376;&#20043;&#38388;&#23384;&#22312;&#26174;&#33879;&#30340;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#20102;&#23545;&#20110;&#36164;&#28304;&#31232;&#32570;&#21644;&#27515;&#35821;&#35328;&#30340;&#22362;&#23454;&#30340;&#26080;&#30417;&#30563;&#20998;&#35789;&#21487;&#33021;&#24615;&#65292;&#24182;&#35753;&#20154;&#20204;&#21487;&#20197;&#20174;&#25928;&#29575;&#28436;&#21270;&#30340;&#35282;&#24230;&#24605;&#32771;&#20154;&#31867;&#35821;&#35328;&#12290;
&lt;/p&gt;
&lt;p&gt;
We explore the possibility of meta-learning for the language-independent unsupervised tokenization problem for English, Russian, and Chinese. We implement the meta-learning approach for automatic determination of hyper-parameters of the unsupervised tokenization model proposed in earlier works, relying on various human-independent fitness functions such as normalised anti-entropy, compression factor and cross-split F1 score, as well as additive and multiplicative composite combinations of the three metrics, testing them against the conventional F1 tokenization score. We find a fairly good correlation between the latter and the additive combination of the former three metrics for English and Russian. In case of Chinese, we find a significant correlation between the F 1 score and the compression factor. Our results suggest the possibility of robust unsupervised tokenization of low-resource and dead languages and allow us to think about human languages in terms of the evolution of efficie
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#23545;ChatGPT&#30340;11&#20010;&#22833;&#36133;&#31867;&#21035;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#25324;&#25512;&#29702;&#12289;&#20107;&#23454;&#38169;&#35823;&#12289;&#25968;&#23398;&#12289;&#32534;&#30721;&#21644;&#20559;&#35265;&#12290;&#25214;&#20986;&#22833;&#36133;&#21407;&#22240;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25913;&#36827;&#26410;&#26469;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;</title><link>http://arxiv.org/abs/2302.03494</link><description>&lt;p&gt;
ChatGPT&#22833;&#36133;&#20998;&#31867;&#23384;&#26723;
&lt;/p&gt;
&lt;p&gt;
A Categorical Archive of ChatGPT Failures. (arXiv:2302.03494v8 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03494
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#23545;ChatGPT&#30340;11&#20010;&#22833;&#36133;&#31867;&#21035;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#20854;&#20013;&#21253;&#25324;&#25512;&#29702;&#12289;&#20107;&#23454;&#38169;&#35823;&#12289;&#25968;&#23398;&#12289;&#32534;&#30721;&#21644;&#20559;&#35265;&#12290;&#25214;&#20986;&#22833;&#36133;&#21407;&#22240;&#20197;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#25913;&#36827;&#26410;&#26469;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#22312;&#19981;&#21516;&#39046;&#22495;&#35777;&#26126;&#20102;&#20854;&#20215;&#20540;&#12290;&#30001;OpenAI&#24320;&#21457;&#30340;ChatGPT&#20351;&#29992;&#22823;&#37327;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#36890;&#36807;&#29702;&#35299;&#19978;&#19979;&#25991;&#24182;&#29983;&#25104;&#36866;&#24403;&#30340;&#21709;&#24212;&#26469;&#27169;&#25311;&#20154;&#31867;&#23545;&#35805;&#12290;&#23427;&#22240;&#33021;&#22815;&#26377;&#25928;&#22320;&#22238;&#31572;&#24191;&#27867;&#30340;&#20154;&#31867;&#38382;&#39064;&#32780;&#21463;&#21040;&#37325;&#35270;&#65292;&#20854;&#27969;&#21033;&#21644;&#20840;&#38754;&#30340;&#31572;&#26696;&#22312;&#23433;&#20840;&#24615;&#21644;&#23454;&#29992;&#24615;&#26041;&#38754;&#36229;&#36234;&#20102;&#20808;&#21069;&#30340;&#20844;&#20849;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;ChatGPT&#22833;&#25928;&#30340;&#20840;&#38754;&#20998;&#26512;&#65292;&#36825;&#26159;&#26412;&#30740;&#31350;&#30340;&#37325;&#28857;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#24182;&#35752;&#35770;&#20102;11&#20010;&#22833;&#36133;&#31867;&#21035;&#65292;&#21253;&#25324;&#25512;&#29702;&#12289;&#20107;&#23454;&#38169;&#35823;&#12289;&#25968;&#23398;&#12289;&#32534;&#30721;&#21644;&#20559;&#35265;&#12290;&#36824;&#31361;&#20986;&#20102;ChatGPT&#30340;&#39118;&#38505;&#12289;&#38480;&#21046;&#21644;&#31038;&#20250;&#24433;&#21709;&#12290;&#26412;&#30740;&#31350;&#30340;&#30446;&#26631;&#26159;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#21644;&#24320;&#21457;&#20154;&#21592;&#22686;&#24378;&#26410;&#26469;&#30340;&#35821;&#35328;&#27169;&#22411;&#21644;&#32842;&#22825;&#26426;&#22120;&#20154;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models have been demonstrated to be valuable in different fields. ChatGPT, developed by OpenAI, has been trained using massive amounts of data and simulates human conversation by comprehending context and generating appropriate responses. It has garnered significant attention due to its ability to effectively answer a broad range of human inquiries, with fluent and comprehensive answers surpassing prior public chatbots in both security and usefulness. However, a comprehensive analysis of ChatGPT's failures is lacking, which is the focus of this study. Eleven categories of failures, including reasoning, factual errors, math, coding, and bias, are presented and discussed. The risks, limitations, and societal implications of ChatGPT are also highlighted. The goal of this study is to assist researchers and developers in enhancing future language models and chatbots.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25919;&#31574;&#25193;&#23637;&#26041;&#26696;&#65292;&#29992;&#20110;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#20445;&#30041;&#31163;&#32447;&#23398;&#20064;&#30340;&#31574;&#30053;&#24182;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2302.00935</link><description>&lt;p&gt;
&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#30340;&#25919;&#31574;&#25193;&#23637;
&lt;/p&gt;
&lt;p&gt;
Policy Expansion for Bridging Offline-to-Online Reinforcement Learning. (arXiv:2302.00935v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.00935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#25919;&#31574;&#25193;&#23637;&#26041;&#26696;&#65292;&#29992;&#20110;&#31163;&#32447;&#21040;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#65292;&#20197;&#20445;&#30041;&#31163;&#32447;&#23398;&#20064;&#30340;&#31574;&#30053;&#24182;&#22312;&#22312;&#32447;&#23398;&#20064;&#20013;&#36827;&#34892;&#33258;&#36866;&#24212;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#31163;&#32447;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#65292;&#24182;&#20351;&#29992;&#22312;&#32447;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#24494;&#35843;&#65292;&#26159;&#19968;&#31181;&#23398;&#20064;&#25511;&#21046;&#31574;&#30053;&#30340;&#26377;&#24076;&#26395;&#30340;&#31574;&#30053;&#65292;&#33021;&#22815;&#22312;&#26679;&#26412;&#25928;&#29575;&#21644;&#24615;&#33021;&#26041;&#38754;&#20805;&#20998;&#21033;&#29992;&#20004;&#32773;&#30340;&#20248;&#28857;&#12290;&#19968;&#31181;&#33258;&#28982;&#30340;&#26041;&#27861;&#26159;&#20351;&#29992;&#31163;&#32447;&#35757;&#32451;&#30340;&#31574;&#30053;&#21021;&#22987;&#21270;&#22312;&#32447;&#23398;&#20064;&#30340;&#31574;&#30053;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#27492;&#20219;&#21153;&#30340;&#25919;&#31574;&#25193;&#23637;&#26041;&#26696;&#12290;&#22312;&#23398;&#20064;&#31163;&#32447;&#31574;&#30053;&#21518;&#65292;&#25105;&#20204;&#23558;&#20854;&#29992;&#20316;&#31574;&#30053;&#38598;&#20013;&#30340;&#19968;&#20010;&#20505;&#36873;&#31574;&#30053;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#36890;&#36807;&#21478;&#19968;&#20010;&#31574;&#30053;&#26469;&#25193;&#23637;&#31574;&#30053;&#38598;&#65292;&#35813;&#31574;&#30053;&#23558;&#36127;&#36131;&#36827;&#19968;&#27493;&#30340;&#23398;&#20064;&#12290;&#20004;&#20010;&#31574;&#30053;&#23558;&#20197;&#33258;&#36866;&#24212;&#30340;&#26041;&#24335;&#32452;&#21512;&#36215;&#26469;&#19982;&#29615;&#22659;&#36827;&#34892;&#20132;&#20114;&#12290;&#36890;&#36807;&#36825;&#31181;&#26041;&#27861;&#65292;&#20808;&#21069;&#31163;&#32447;&#23398;&#20064;&#30340;&#31574;&#30053;&#23436;&#20840;&#22312;&#22312;&#32447;&#23398;&#20064;&#36807;&#31243;&#20013;&#24471;&#20197;&#20445;&#30041;&#65292;&#22240;&#27492;&#20943;&#36731;&#20102;&#28508;&#22312;&#38382;&#39064;&#65292;&#20363;&#22914;&#22312;&#22312;&#32447;&#23398;&#20064;&#30340;&#21021;&#22987;&#38454;&#27573;&#30772;&#22351;&#31163;&#32447;&#31574;&#30053;&#30340;&#26377;&#29992;&#34892;&#20026;&#65292;&#21516;&#26102;&#20801;&#35768;&#31163;&#32447;&#31574;&#30053;&#22312;&#33258;&#36866;&#24212;&#26041;&#24335;&#19979;&#33258;&#28982;&#21442;&#19982;
&lt;/p&gt;
&lt;p&gt;
Pre-training with offline data and online fine-tuning using reinforcement learning is a promising strategy for learning control policies by leveraging the best of both worlds in terms of sample efficiency and performance. One natural approach is to initialize the policy for online learning with the one trained offline. In this work, we introduce a policy expansion scheme for this task. After learning the offline policy, we use it as one candidate policy in a policy set. We then expand the policy set with another policy which will be responsible for further learning. The two policies will be composed in an adaptive manner for interacting with the environment. With this approach, the policy previously learned offline is fully retained during online learning, thus mitigating the potential issues such as destroying the useful behaviors of the offline policy in the initial stage of online learning while allowing the offline policy participate in the exploration naturally in an adaptive mann
&lt;/p&gt;</description></item><item><title>DIFFormer&#26159;&#19968;&#31181;&#33021;&#37327;&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#28176;&#34701;&#21512;&#20854;&#20182;&#23454;&#20363;&#20449;&#24687;&#30340;&#28436;&#21270;&#29366;&#24577;&#65292;&#23548;&#20986;&#20102;&#19968;&#31867;&#26032;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;&#31216;&#20026;DIFFormer&#65288;&#22522;&#20110;&#25193;&#25955;&#30340;Transformer&#65289;&#65292;&#33021;&#22815;&#25581;&#31034;&#30495;&#23454;&#19990;&#30028;&#20013;&#22797;&#26434;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2301.09474</link><description>&lt;p&gt;
DIFFormer&#65306;&#36890;&#36807;&#21463;&#33021;&#37327;&#38480;&#21046;&#30340;&#25193;&#25955;&#24341;&#20986;&#30340;&#21487;&#25193;&#23637;&#65288;&#22270;&#24418;&#65289;Transformer
&lt;/p&gt;
&lt;p&gt;
DIFFormer: Scalable (Graph) Transformers Induced by Energy Constrained Diffusion. (arXiv:2301.09474v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.09474
&lt;/p&gt;
&lt;p&gt;
DIFFormer&#26159;&#19968;&#31181;&#33021;&#37327;&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#65292;&#36890;&#36807;&#36880;&#28176;&#34701;&#21512;&#20854;&#20182;&#23454;&#20363;&#20449;&#24687;&#30340;&#28436;&#21270;&#29366;&#24577;&#65292;&#23548;&#20986;&#20102;&#19968;&#31867;&#26032;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#65292;&#31216;&#20026;DIFFormer&#65288;&#22522;&#20110;&#25193;&#25955;&#30340;Transformer&#65289;&#65292;&#33021;&#22815;&#25581;&#31034;&#30495;&#23454;&#19990;&#30028;&#20013;&#22797;&#26434;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30495;&#23454;&#19990;&#30028;&#30340;&#25968;&#25454;&#29983;&#25104;&#24120;&#24120;&#28041;&#21450;&#23454;&#20363;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20381;&#36182;&#65292;&#36829;&#21453;&#20102;&#26631;&#20934;&#23398;&#20064;&#33539;&#24335;&#30340;IID&#25968;&#25454;&#20551;&#35774;&#65292;&#20174;&#32780;&#23545;&#25581;&#31034;&#20960;&#20309;&#32467;&#26500;&#20197;&#23398;&#20064;&#25152;&#38656;&#35201;&#30340;&#23454;&#20363;&#34920;&#31034;&#24418;&#25104;&#20102;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#33021;&#37327;&#21463;&#38480;&#25193;&#25955;&#27169;&#22411;&#65292;&#23558;&#19968;&#25209;&#25968;&#25454;&#38598;&#20013;&#30340;&#23454;&#20363;&#32534;&#30721;&#20026;&#36880;&#28176;&#34701;&#21512;&#20102;&#20854;&#20182;&#23454;&#20363;&#20449;&#24687;&#30340;&#28436;&#21270;&#29366;&#24577;&#12290;&#25193;&#25955;&#36807;&#31243;&#21463;&#38480;&#20110;&#22522;&#20110;&#21512;&#29702;&#33021;&#37327;&#20989;&#25968;&#30340;&#19979;&#38477;&#26631;&#20934;&#65292;&#35813;&#20989;&#25968;&#34920;&#24449;&#20102;&#28508;&#22312;&#32467;&#26500;&#19978;&#23454;&#20363;&#34920;&#31034;&#30340;&#20840;&#23616;&#19968;&#33268;&#24615;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#20005;&#35880;&#30340;&#29702;&#35770;&#65292;&#35813;&#29702;&#35770;&#26263;&#31034;&#20102;&#20219;&#24847;&#23454;&#20363;&#23545;&#20043;&#38388;&#30340;&#26368;&#20248;&#25193;&#25955;&#24378;&#24230;&#30340;&#38381;&#21512;&#24418;&#24335;&#20272;&#35745;&#65292;&#36825;&#23548;&#33268;&#20102;&#19968;&#31867;&#26032;&#30340;&#31070;&#32463;&#32534;&#30721;&#22120;&#30340;&#20135;&#29983;&#65306;DIFFormer&#65288;&#22522;&#20110;&#25193;&#25955;&#30340;Transformer&#65289;&#65292;&#20854;&#20013;&#21253;&#21547;&#20004;&#20010;&#29256;&#26412;&#65306;&#19968;&#20010;&#31616;&#21333;&#29256;&#26412;&#20855;&#26377;&#32447;&#24615;&#22797;&#26434;&#24230;&#65292;&#38754;&#20020;&#30528;&#31105;&#24524;&#30340;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Real-world data generation often involves complex inter-dependencies among instances, violating the IID-data hypothesis of standard learning paradigms and posing a challenge for uncovering the geometric structures for learning desired instance representations. To this end, we introduce an energy constrained diffusion model which encodes a batch of instances from a dataset into evolutionary states that progressively incorporate other instances' information by their interactions. The diffusion process is constrained by descent criteria w.r.t.~a principled energy function that characterizes the global consistency of instance representations over latent structures. We provide rigorous theory that implies closed-form optimal estimates for the pairwise diffusion strength among arbitrary instance pairs, which gives rise to a new class of neural encoders, dubbed as DIFFormer (diffusion-based Transformers), with two instantiations: a simple version with linear complexity for prohibitive instanc
&lt;/p&gt;</description></item><item><title>D-Adaptation&#26159;&#19968;&#31181;&#21487;&#20197;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#29992;&#20110;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#32780;&#26080;&#38656;&#36229;&#21442;&#25968;&#65292;&#20063;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#12290;</title><link>http://arxiv.org/abs/2301.07733</link><description>&lt;p&gt;
&#36890;&#36807;D&#36866;&#24212;&#23454;&#29616;&#23398;&#20064;&#29575;&#33258;&#30001;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Learning-Rate-Free Learning by D-Adaptation. (arXiv:2301.07733v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.07733
&lt;/p&gt;
&lt;p&gt;
D-Adaptation&#26159;&#19968;&#31181;&#21487;&#20197;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#38024;&#23545;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#29992;&#20110;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#32780;&#26080;&#38656;&#36229;&#21442;&#25968;&#65292;&#20063;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#65292;&#33021;&#22815;&#22312;&#21508;&#31181;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
D&#36866;&#24212;&#26159;&#19968;&#31181;&#33258;&#21160;&#35774;&#32622;&#23398;&#20064;&#29575;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#28176;&#36817;&#22320;&#23454;&#29616;&#26368;&#20248;&#25910;&#25947;&#36895;&#29575;&#65292;&#29992;&#20110;&#26368;&#23567;&#21270;&#20984;&#24615;Lipschitz&#20989;&#25968;&#65292;&#26080;&#38656;&#22238;&#28335;&#25110;&#32447;&#24615;&#25628;&#32034;&#65292;&#24182;&#19988;&#27599;&#27493;&#26080;&#38656;&#36827;&#34892;&#39069;&#22806;&#30340;&#20989;&#25968;&#20540;&#25110;&#26799;&#24230;&#35780;&#20272;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#36825;&#19968;&#31867;&#38382;&#39064;&#30340;&#31532;&#19968;&#20010;&#26080;&#36229;&#21442;&#25968;&#19988;&#25910;&#25947;&#36895;&#29575;&#26080;&#38656;&#39069;&#22806;&#23545;&#25968;&#22240;&#23376;&#25913;&#36827;&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#38024;&#23545;SGD&#21644;Adam&#21464;&#20307;&#23637;&#31034;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20854;&#20013;&#35813;&#26041;&#27861;&#33258;&#21160;&#21305;&#37197;&#25163;&#21160;&#35843;&#25972;&#30340;&#23398;&#20064;&#29575;&#65292;&#22312;&#21313;&#22810;&#20010;&#19981;&#21516;&#30340;&#26426;&#22120;&#23398;&#20064;&#38382;&#39064;&#20013;&#24212;&#29992;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;&#21644;&#35821;&#35328;&#38382;&#39064;&#12290;&#24320;&#28304;&#23454;&#29616;&#22312; \url{https://github.com/facebookresearch/dadaptation}.
&lt;/p&gt;
&lt;p&gt;
D-Adaptation is an approach to automatically setting the learning rate which asymptotically achieves the optimal rate of convergence for minimizing convex Lipschitz functions, with no back-tracking or line searches, and no additional function value or gradient evaluations per step. Our approach is the first hyper-parameter free method for this class without additional multiplicative log factors in the convergence rate. We present extensive experiments for SGD and Adam variants of our method, where the method automatically matches hand-tuned learning rates across more than a dozen diverse machine learning problems, including large-scale vision and language problems.  An open-source implementation is available at \url{https://github.com/facebookresearch/dadaptation}.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#21407;&#22411;&#35299;&#37322;&#22120;&#65292;&#33021;&#22815;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#35270;&#39057;&#21160;&#20316;&#30340;&#20998;&#31867;&#65292;&#21516;&#26102;&#33021;&#22815;&#23558;&#31867;&#21644;&#21407;&#22411;&#24314;&#31435;&#25104;&#26356;&#26377;&#23618;&#27425;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#12290;</title><link>http://arxiv.org/abs/2301.00436</link><description>&lt;p&gt;
&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#30340;&#20998;&#23618;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Hierarchical Explanations for Video Action Recognition. (arXiv:2301.00436v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.00436
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#20998;&#23618;&#21407;&#22411;&#35299;&#37322;&#22120;&#65292;&#33021;&#22815;&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#23545;&#35270;&#39057;&#21160;&#20316;&#30340;&#20998;&#31867;&#65292;&#21516;&#26102;&#33021;&#22815;&#23558;&#31867;&#21644;&#21407;&#22411;&#24314;&#31435;&#25104;&#26356;&#26377;&#23618;&#27425;&#30340;&#20851;&#31995;&#65292;&#21487;&#20197;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#37322;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#20027;&#35201;&#26041;&#27861;&#20043;&#19968;&#26159;&#20998;&#35299;&#35270;&#35273;&#36755;&#20837;&#24182;&#25214;&#21040;&#36127;&#36131;&#20998;&#31867;&#30340;&#20856;&#22411;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#36890;&#24120;&#24573;&#30053;&#36825;&#20123;&#21407;&#22411;&#20043;&#38388;&#30340;&#20998;&#23618;&#20851;&#31995;&#65292;&#22240;&#27492;&#26080;&#27861;&#22312;&#26356;&#39640;&#23618;&#27425;&#65288;&#20363;&#22914;&#65292;&#27700;&#19978;&#36816;&#21160;&#65289;&#21644;&#26356;&#20302;&#23618;&#27425;&#65288;&#20363;&#22914;&#65292;&#28216;&#27891;&#65289;&#19978;&#35299;&#37322;&#35821;&#20041;&#27010;&#24565;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21463;&#21040;&#20154;&#31867;&#35748;&#30693;&#31995;&#32479;&#30340;&#21551;&#21457;&#65292;&#21033;&#29992;&#20998;&#23618;&#20449;&#24687;&#22788;&#29702;&#19981;&#30830;&#23450;&#24615;&#65306;&#24403;&#25105;&#20204;&#35266;&#23519;&#21040;&#27700;&#21644;&#20154;&#31867;&#27963;&#21160;&#65292;&#20294;&#27809;&#26377;&#26126;&#30830;&#30340;&#21160;&#20316;&#26102;&#65292;&#21487;&#20197;&#23558;&#20854;&#35782;&#21035;&#20026;&#27700;&#19978;&#36816;&#21160;&#30340;&#29238;&#31867;&#12290;&#21482;&#26377;&#35266;&#23519;&#21040;&#19968;&#20010;&#20154;&#22312;&#28216;&#27891;&#21518;&#65292;&#25105;&#20204;&#25165;&#33021;&#26126;&#30830;&#23558;&#20854;&#32454;&#20998;&#20026;&#28216;&#27891;&#21160;&#20316;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20998;&#23618;&#21407;&#22411;&#35299;&#37322;&#22120;&#65288;HIPE&#65289;&#26469;&#24314;&#31435;&#21407;&#22411;&#21644;&#31867;&#20043;&#38388;&#30340;&#20998;&#23618;&#20851;&#31995;&#12290; HIPE&#36890;&#36807;&#22312;&#31867;&#23618;&#27425;&#32467;&#26500;&#30340;&#22810;&#20010;&#32423;&#21035;&#19978;&#20998;&#35299;&#36755;&#20837;&#35270;&#39057;&#24103;&#65292;&#23454;&#29616;&#20102;&#35270;&#39057;&#21160;&#20316;&#20998;&#31867;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20063;&#36866;&#29992;&#20110;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#20043;&#22806;&#30340;&#20854;&#20182;&#35782;&#21035;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
To interpret deep neural networks, one main approach is to dissect the visual input and find the prototypical parts responsible for the classification. However, existing methods often ignore the hierarchical relationship between these prototypes, and thus can not explain semantic concepts at both higher level (e.g., water sports) and lower level (e.g., swimming). In this paper inspired by human cognition system, we leverage hierarchal information to deal with uncertainty: When we observe water and human activity, but no definitive action it can be recognized as the water sports parent class. Only after observing a person swimming can we definitively refine it to the swimming action. To this end, we propose HIerarchical Prototype Explainer (HIPE) to build hierarchical relations between prototypes and classes. HIPE enables a reasoning process for video action classification by dissecting the input video frames on multiple levels of the class hierarchy, our method is also applicable to ot
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;(PCNNs) &#22312;&#27169;&#25311;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;PCNNs&#26082;&#30830;&#20445;&#20102;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21448;&#33021;&#22312;&#22797;&#26434;&#30340;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#21462;&#24471;&#39640;&#31934;&#24230;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#22312;&#21487;&#29992;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2212.12380</link><description>&lt;p&gt;
&#38754;&#21521;&#21487;&#25193;&#23637;&#29289;&#29702;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#22312;&#25968;&#25454;&#39537;&#21160;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models. (arXiv:2212.12380v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;(PCNNs) &#22312;&#27169;&#25311;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;PCNNs&#26082;&#30830;&#20445;&#20102;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21448;&#33021;&#22312;&#22797;&#26434;&#30340;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#21462;&#24471;&#39640;&#31934;&#24230;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#22312;&#21487;&#29992;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#34987;&#25910;&#38598;&#65292;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#34429;&#28982;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#22312;&#29289;&#29702;&#19978;&#26159;&#21487;&#38752;&#30340;&#65292;&#20294;&#36890;&#24120;&#24456;&#38590;&#35782;&#21035;&#21644;&#25193;&#23637;&#65292;&#24182;&#19988;&#21463;&#20854;&#26377;&#38480;&#30340;&#34920;&#29616;&#21147;&#24433;&#21709;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#20934;&#30830;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24120;&#24120;&#20381;&#36182;&#31070;&#32463;&#32593;&#32476; (NNs) &#30340;&#32463;&#20856;&#40657;&#30418;&#26041;&#27861;&#36890;&#24120;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#32479;&#35745;&#27169;&#24335;&#65292;&#21363;&#20351;&#22312;&#25193;&#23637;&#26041;&#38754;&#20063;&#33021;&#21462;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#28508;&#22312;&#30340;&#29289;&#29702;&#23450;&#24459;&#23436;&#20840;&#26080;&#35270;&#65292;&#22914;&#26524;&#22522;&#20110;&#23427;&#20204;&#20570;&#20915;&#31574;&#29992;&#20110;&#23454;&#38469;&#29289;&#29702;&#31995;&#32479;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#26368;&#36817;&#24320;&#21457;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476; (PCNNs) &#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#30830;&#20445;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992; NNs &#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558; PCNN &#25193;&#23637;&#21040;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#19982;&#32463;&#20856;&#28784;&#30418;&#21644;&#40657;&#30418;&#26041;&#27861;&#30340;&#24443;&#24213;&#27604;&#36739;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#22810;&#21306;&#22495;&#24314;&#31569;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#21306;&#22495;&#30340;&#28909;&#34892;&#20026;&#30001;&#33021;&#37327;&#24179;&#34913;&#26041;&#31243;&#24335;&#32479;&#27835;&#65292;&#20854;&#21442;&#25968;&#24517;&#39035;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#36827;&#34892;&#35782;&#21035;&#12290;&#25152;&#24471;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#28041;&#21450;&#35768;&#22810;&#30456;&#20114;&#20316;&#29992;&#30340;&#32452;&#20214;&#26500;&#25104;&#30340;&#22797;&#26434;&#21644;&#21160;&#24577;&#31995;&#32479;&#65292;PCNNs &#20063;&#21487;&#20197;&#22312;&#30830;&#20445;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126; PCNN &#22312;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#65292;&#22312;&#26377;&#38480;&#30340;&#21487;&#29992;&#35757;&#32451;&#25968;&#25454;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more data being collected, data-driven modeling methods have been gaining in popularity in recent years. While physically sound, classical gray-box models are often cumbersome to identify and scale, and their accuracy might be hindered by their limited expressiveness. On the other hand, classical black-box methods, typically relying on Neural Networks (NNs) nowadays, often achieve impressive performance, even at scale, by deriving statistical patterns from data. However, they remain completely oblivious to the underlying physical laws, which may lead to potentially catastrophic failures if decisions for real-world physical systems are based on them. Physically Consistent Neural Networks (PCNNs) were recently developed to address these aforementioned issues, ensuring physical consistency while still leveraging NNs to attain state-of-the-art accuracy.  In this work, we scale PCNNs to model building temperature dynamics and propose a thorough comparison with classical gray-b
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Hi-VT5&#65292;&#23427;&#26159;&#19968;&#31181;&#20998;&#23618; transformer &#32467;&#26500;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#39029;&#25991;&#26723; DocVQA &#20219;&#21153;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22238;&#31572;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.05935</link><description>&lt;p&gt;
&#22810;&#32423;&#27169;&#24577;&#21464;&#21387;&#22120;&#29992;&#20110;&#22810;&#39029; DocVQA
&lt;/p&gt;
&lt;p&gt;
Hierarchical multimodal transformers for Multi-Page DocVQA. (arXiv:2212.05935v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.05935
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;Hi-VT5&#65292;&#23427;&#26159;&#19968;&#31181;&#20998;&#23618; transformer &#32467;&#26500;&#65292;&#33021;&#22815;&#22788;&#29702;&#22810;&#39029;&#25991;&#26723; DocVQA &#20219;&#21153;&#12290;&#23454;&#39564;&#34920;&#26126;&#35813;&#26041;&#27861;&#33021;&#22815;&#26377;&#25928;&#22320;&#22238;&#31572;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25991;&#26723;&#35270;&#35273;&#38382;&#31572;&#65288;DocVQA&#65289;&#26159;&#25351;&#22238;&#31572;&#25991;&#26723;&#22270;&#20687;&#20013;&#30340;&#38382;&#39064;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340; DocVQA &#24037;&#20316;&#20165;&#32771;&#34385;&#21333;&#39029;&#25991;&#26723;&#12290;&#20294;&#26159;&#65292;&#22312;&#23454;&#38469;&#22330;&#26223;&#20013;&#65292;&#25991;&#26723;&#20027;&#35201;&#30001;&#22810;&#20010;&#39029;&#38754;&#32452;&#25104;&#65292;&#24212;&#35813;&#19968;&#36215;&#22788;&#29702;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558; DocVQA &#25193;&#23637;&#21040;&#22810;&#39029;&#38754;&#22330;&#26223;&#12290;&#20026;&#27492;&#65292;&#39318;&#20808;&#21019;&#24314;&#19968;&#20010;&#26032;&#30340;&#25968;&#25454;&#38598; MP-DocVQA&#65292;&#20854;&#20013;&#38382;&#39064;&#26159;&#38024;&#23545;&#22810;&#39029;&#25991;&#26723;&#32780;&#38750;&#21333;&#39029;&#25552;&#20986;&#30340;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#23618;&#26041;&#27861; Hi-VT5&#65292;&#22522;&#20110; T5 &#32467;&#26500;&#65292;&#20811;&#26381;&#20102;&#22788;&#29702;&#38271;&#22810;&#39029;&#25991;&#26723;&#30340;&#24403;&#21069;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22522;&#20110;&#20998;&#23618;&#21464;&#21387;&#22120;&#32467;&#26500;&#65292;&#32534;&#30721;&#22120;&#23545;&#27599;&#20010;&#39029;&#38754;&#30340;&#26368;&#30456;&#20851;&#20449;&#24687;&#36827;&#34892;&#25688;&#35201;&#65292;&#28982;&#21518;&#35299;&#30721;&#22120;&#21033;&#29992;&#36825;&#20123;&#25688;&#35201;&#20449;&#24687;&#29983;&#25104;&#26368;&#32456;&#31572;&#26696;&#12290;&#36890;&#36807;&#24191;&#27867;&#23454;&#39564;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#33021;&#22815;&#22312;&#21333;&#20010;&#38454;&#27573;&#20013;&#22238;&#31572;&#38382;&#39064;&#24182;&#25552;&#20379;
&lt;/p&gt;
&lt;p&gt;
Document Visual Question Answering (DocVQA) refers to the task of answering questions from document images. Existing work on DocVQA only considers single-page documents. However, in real scenarios documents are mostly composed of multiple pages that should be processed altogether. In this work we extend DocVQA to the multi-page scenario. For that, we first create a new dataset, MP-DocVQA, where questions are posed over multi-page documents instead of single pages. Second, we propose a new hierarchical method, Hi-VT5, based on the T5 architecture, that overcomes the limitations of current methods to process long multi-page documents. The proposed method is based on a hierarchical transformer architecture where the encoder summarizes the most relevant information of every page and then, the decoder takes this summarized information to generate the final answer. Through extensive experimentation, we demonstrate that our method is able, in a single stage, to answer the questions and provid
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#26292;&#38706;&#21407;&#22987;&#25968;&#25454;&#30340;&#30693;&#35782;&#22270;&#35889;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#36136;&#37327;&#35780;&#20272;&#20219;&#21153;&#36716;&#21270;&#20026;&#20004;&#20010;KG&#20043;&#38388;&#30340;&#23545;&#25239;&#24615;&#38382;&#31572;&#28216;&#25103;&#65292;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#21407;&#22987;&#25968;&#25454;&#21644;&#26356;&#22810;&#32771;&#34385;&#25968;&#25454;&#36136;&#37327;&#32780;&#38750;&#33021;&#21147;&#23618;&#38754;&#30340;&#32570;&#38519;&#12290;</title><link>http://arxiv.org/abs/2212.00994</link><description>&lt;p&gt;
&#19981;&#23436;&#25972;&#20449;&#24687;&#19979;&#30340;&#30693;&#35782;&#22270;&#35889;&#36136;&#37327;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Quality Evaluation under Incomplete Information. (arXiv:2212.00994v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.00994
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#19968;&#31181;&#19981;&#26292;&#38706;&#21407;&#22987;&#25968;&#25454;&#30340;&#30693;&#35782;&#22270;&#35889;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;&#65292;&#36890;&#36807;&#23558;&#36136;&#37327;&#35780;&#20272;&#20219;&#21153;&#36716;&#21270;&#20026;&#20004;&#20010;KG&#20043;&#38388;&#30340;&#23545;&#25239;&#24615;&#38382;&#31572;&#28216;&#25103;&#65292;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#39640;&#24230;&#20381;&#36182;&#21407;&#22987;&#25968;&#25454;&#21644;&#26356;&#22810;&#32771;&#34385;&#25968;&#25454;&#36136;&#37327;&#32780;&#38750;&#33021;&#21147;&#23618;&#38754;&#30340;&#32570;&#38519;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#20855;&#26377;&#26681;&#26412;&#20316;&#29992;&#65292;&#30693;&#35782;&#22270;&#35889;(KGs)&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#23545;KG&#30340;&#36136;&#37327;&#35780;&#20272;&#33267;&#20851;&#37325;&#35201;&#19988;&#19981;&#21487;&#25110;&#32570;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#39046;&#22495;&#20013;&#30340;&#26041;&#27861;&#36890;&#36807;&#20174;&#19981;&#21516;&#30340;&#32500;&#24230;&#25552;&#20986;&#26032;&#30340;&#36136;&#37327;&#24230;&#37327;&#25110;&#22312;KG&#26500;&#24314;&#38454;&#27573;&#34913;&#37327;&#24615;&#33021;&#26469;&#35780;&#20272;KG&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#26041;&#27861;&#23384;&#22312;&#20004;&#20010;&#20027;&#35201;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#23427;&#20204;&#39640;&#24230;&#20381;&#36182;KG&#20013;&#30340;&#21407;&#22987;&#25968;&#25454;&#65292;&#36825;&#20351;&#24471;KG&#30340;&#20869;&#37096;&#20449;&#24687;&#22312;&#36136;&#37327;&#35780;&#20272;&#26399;&#38388;&#26292;&#38706;&#20986;&#26469;&#12290;&#20854;&#27425;&#65292;&#23427;&#20204;&#26356;&#22810;&#22320;&#32771;&#34385;&#25968;&#25454;&#23618;&#38754;&#30340;&#36136;&#37327;&#65292;&#32780;&#19981;&#26159;&#33021;&#21147;&#23618;&#38754;&#30340;&#36136;&#37327;&#65292;&#23545;&#20110;&#19979;&#28216;&#24212;&#29992;&#26469;&#35828;&#65292;&#21518;&#32773;&#26356;&#20026;&#37325;&#35201;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#19981;&#23436;&#25972;&#20449;&#24687;&#19979;&#30340;&#30693;&#35782;&#22270;&#35889;&#36136;&#37327;&#35780;&#20272;&#26694;&#26550;(QEII)&#12290;&#22312;&#35780;&#20272;&#36807;&#31243;&#20013;&#65292;&#19981;&#26292;&#38706;&#20219;&#20309;&#21407;&#22987;&#25968;&#25454;&#65292;&#32780;&#23558;&#36136;&#37327;&#35780;&#20272;&#20219;&#21153;&#36716;&#21270;&#20026;&#20004;&#20010;KG&#20043;&#38388;&#30340;&#23545;&#25239;&#24615;&#38382;&#31572;&#28216;&#25103;&#12290;&#28216;&#25103;&#32988;&#32773;&#22240;&#27492;&#34987;&#35748;&#20026;&#20855;&#26377;&#26356;&#22909;&#30340;&#36136;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) have attracted more and more attentions because of their fundamental roles in many tasks. Quality evaluation for KGs is thus crucial and indispensable. Existing methods in this field evaluate KGs by either proposing new quality metrics from different dimensions or measuring performances at KG construction stages. However, there are two major issues with those methods. First, they highly rely on raw data in KGs, which makes KGs' internal information exposed during quality evaluation. Second, they consider more about the quality at data level instead of ability level, where the latter one is more important for downstream applications. To address these issues, we propose a knowledge graph quality evaluation framework under incomplete information (QEII). The quality evaluation task is transformed into an adversarial Q&amp;A game between two KGs. Winner of the game is thus considered to have better qualities. During the evaluation process, no raw data is exposed, which en
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29615;&#22659;&#33258;&#36866;&#24212;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2211.15136</link><description>&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#30340;2D&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;
&lt;/p&gt;
&lt;p&gt;
Collective Intelligence for 2D Push Manipulation with Mobile Robots. (arXiv:2211.15136v2 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.15136
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#21033;&#29992;&#22522;&#20110;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#22120;&#30340;&#35268;&#21010;&#22120;&#21644;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#23454;&#29616;&#20102;&#31227;&#21160;&#26426;&#22120;&#20154;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#20013;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#27604;&#20256;&#32479;&#26041;&#27861;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#24182;&#20855;&#22791;&#29615;&#22659;&#33258;&#36866;&#24212;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#28982;&#31995;&#32479;&#36890;&#24120;&#34920;&#29616;&#20986;&#33021;&#22815;&#33258;&#25105;&#32452;&#32455;&#21644;&#36866;&#24212;&#21464;&#21270;&#30340;&#38598;&#20307;&#26234;&#33021;&#65292;&#20294;&#22823;&#22810;&#25968;&#20154;&#24037;&#31995;&#32479;&#32570;&#20047;&#36825;&#31181;&#31561;&#25928;&#24615;&#12290;&#26412;&#25991;&#25506;&#35752;&#20351;&#29992;&#31227;&#21160;&#26426;&#22120;&#20154;&#36827;&#34892;2D&#21327;&#20316;&#25512;&#21160;&#25805;&#20316;&#30340;&#38598;&#20307;&#26234;&#33021;&#31995;&#32479;&#30340;&#21487;&#33021;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#23558;&#20174;&#36719;&#20307;&#29289;&#29702;&#27169;&#25311;&#27966;&#29983;&#30340;&#35268;&#21010;&#22120;&#25552;&#28860;&#20026;&#22522;&#20110;&#27880;&#24847;&#21147;&#30340;&#31070;&#32463;&#32593;&#32476;&#21518;&#65292;&#25105;&#20204;&#30340;&#22810;&#26426;&#22120;&#20154;&#25512;&#21160;&#25805;&#20316;&#31995;&#32479;&#30456;&#23545;&#20110;&#22522;&#32447;&#31995;&#32479;&#20855;&#26377;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#21487;&#36866;&#24212;&#22806;&#37096;&#25200;&#21160;&#21644;&#29615;&#22659;&#21464;&#21270;&#23436;&#25104;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;
While natural systems often present collective intelligence that allows them to self-organize and adapt to changes, the equivalent is missing in most artificial systems. We explore the possibility of such a system in the context of cooperative 2D push manipulations using mobile robots. Although conventional works demonstrate potential solutions for the problem in restricted settings, they have computational and learning difficulties. More importantly, these systems do not possess the ability to adapt when facing environmental changes. In this work, we show that by distilling a planner derived from a differentiable soft-body physics simulator into an attention-based neural network, our multi-robot push manipulation system achieves better performance than baselines. In addition, our system also generalizes to configurations not seen during training and is able to adapt toward task completions when external turbulence and environmental changes are applied. Supplementary videos can be foun
&lt;/p&gt;</description></item><item><title>GAMMT&#26159;&#19968;&#31181;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#20010;Transformer&#22788;&#29702;&#27169;&#31946;&#19981;&#30830;&#23450;&#30340;&#27010;&#29575;&#12290;&#35813;&#27169;&#22411;&#26377;&#26395;&#23454;&#29616;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2211.09812</link><description>&lt;p&gt;
GAMMT: &#20351;&#29992;&#22810;&#20010;Transformer&#30340;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
GAMMT: Generative Ambiguity Modeling Using Multiple Transformers. (arXiv:2211.09812v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.09812
&lt;/p&gt;
&lt;p&gt;
GAMMT&#26159;&#19968;&#31181;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#27169;&#22411;&#65292;&#20351;&#29992;&#22810;&#20010;Transformer&#22788;&#29702;&#27169;&#31946;&#19981;&#30830;&#23450;&#30340;&#27010;&#29575;&#12290;&#35813;&#27169;&#22411;&#26377;&#26395;&#23454;&#29616;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#30340;&#24207;&#21015;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65292;&#31216;&#20026;GAMMT&#65288;&#20351;&#29992;&#22810;&#20010;Transformer&#30340;&#29983;&#25104;&#19981;&#30830;&#23450;&#24615;&#27169;&#22411;&#65289;&#65292;&#29992;&#20110;&#22522;&#20110;&#27010;&#29575;&#38598;&#21512;&#30340;&#24207;&#21015;&#25968;&#25454;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#35748;&#20026;&#24207;&#21015;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#19981;&#26159;&#30830;&#23450;&#24615;&#30340;&#65292;&#32780;&#26159;&#27169;&#31946;&#30340;&#65292;&#24182;&#21463;&#21040;&#19968;&#32452;&#27010;&#29575;&#30340;&#24433;&#21709;&#12290;&#20026;&#20102;&#25429;&#25417;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#65292;GAMMT&#37319;&#29992;&#20102;&#22810;&#20010;&#24182;&#34892;&#30340;Transformer&#65292;&#36890;&#36807;&#36873;&#25321;&#26426;&#21046;&#30456;&#20114;&#20851;&#32852;&#65292;&#20801;&#35768;&#36817;&#20284;&#22788;&#29702;&#27169;&#31946;&#19981;&#30830;&#23450;&#30340;&#27010;&#29575;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#29983;&#25104;&#29305;&#24615;&#36824;&#20351;&#24471;&#36755;&#20837;&#31526;&#21495;&#21644;&#24207;&#21015;&#21487;&#20197;&#26377;&#22810;&#20010;&#34920;&#24449;&#24418;&#24335;&#12290;&#34429;&#28982;&#25105;&#20204;&#30340;&#27169;&#22411;&#23578;&#26410;&#32463;&#36807;&#23454;&#39564;&#39564;&#35777;&#65292;&#20294;&#25105;&#20204;&#30456;&#20449;&#25105;&#20204;&#30340;&#27169;&#22411;&#22312;&#24314;&#27169;&#20855;&#26377;&#19981;&#30830;&#23450;&#30340;&#25968;&#25454;&#29983;&#25104;&#36807;&#31243;&#30340;&#24207;&#21015;&#30340;&#39640;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#26041;&#38754;&#20855;&#26377;&#24040;&#22823;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a novel model called GAMMT (Generative Ambiguity Models using Multiple Transformers) for sequential data that is based on sets of probabilities. Unlike conventional models, our approach acknowledges that the data generation process of a sequence is not deterministic, but rather ambiguous and influenced by a set of probabilities. To capture this ambiguity, GAMMT employs multiple parallel transformers that are linked by a selection mechanism, allowing for the approximation of ambiguous probabilities. The generative nature of our approach also enables multiple representations of input tokens and sequences. While our models have not yet undergone experimental validation, we believe that our model has great potential to achieve high quality and diversity in modeling sequences with uncertain data generation processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#23454;&#26102;&#22312;&#32447;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#22312;&#25152;&#38656;&#35201;&#30340;&#20301;&#32622;&#21046;&#36896;&#19981;&#23384;&#22312;&#30340;&#29289;&#20307;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#32422;&#20026;90\%&#65292;&#25581;&#31034;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#24369;&#28857;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2209.01962</link><description>&lt;p&gt;
&#23545;&#25239;&#26816;&#27979;: &#23454;&#26102;&#25915;&#20987;&#30446;&#26631;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Adversarial Detection: Attacking Object Detection in Real Time. (arXiv:2209.01962v4 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.01962
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#25552;&#20986;&#20102;&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#23454;&#26102;&#22312;&#32447;&#25915;&#20987;&#65292;&#36825;&#20123;&#25915;&#20987;&#21487;&#20197;&#22312;&#25152;&#38656;&#35201;&#30340;&#20301;&#32622;&#21046;&#36896;&#19981;&#23384;&#22312;&#30340;&#29289;&#20307;&#65292;&#25915;&#20987;&#25104;&#21151;&#29575;&#32422;&#20026;90\%&#65292;&#25581;&#31034;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#24369;&#28857;&#21644;&#23433;&#20840;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#26426;&#22120;&#20154;&#20381;&#36182;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#26469;&#24863;&#30693;&#29615;&#22659;&#12290;&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#23433;&#20840;&#24615;&#30340;&#36827;&#27493;&#65292;&#25581;&#31034;&#20102;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#26131;&#21463;&#21040;&#23545;&#25239;&#24615;&#25915;&#20987;&#30340;&#23041;&#32961;&#12290;&#28982;&#32780;&#20808;&#21069;&#30340;&#30740;&#31350;&#20027;&#35201;&#20391;&#37325;&#20110;&#25915;&#20987;&#38745;&#24577;&#22270;&#20687;&#25110;&#31163;&#32447;&#35270;&#39057;&#12290;&#22240;&#27492;&#65292;&#20173;&#19981;&#28165;&#26970;&#27492;&#31867;&#25915;&#20987;&#26159;&#21542;&#20250;&#21361;&#21450;&#21160;&#24577;&#29615;&#22659;&#19979;&#23454;&#38469;&#30340;&#26426;&#22120;&#20154;&#24212;&#29992;&#12290;&#26412;&#25991;&#36890;&#36807;&#25552;&#20986;&#38024;&#23545;&#30446;&#26631;&#26816;&#27979;&#27169;&#22411;&#30340;&#39318;&#27425;&#23454;&#26102;&#22312;&#32447;&#25915;&#20987;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19977;&#31181;&#25915;&#20987;&#26041;&#24335;&#65292;&#21487;&#20197;&#22312;&#25152;&#38656;&#20301;&#32622;&#29983;&#25104;&#19981;&#23384;&#22312;&#23545;&#35937;&#30340;&#36793;&#30028;&#26694;&#12290;&#36825;&#20123;&#25915;&#20987;&#22312;&#32422;20&#27425;&#36845;&#20195;&#20869;&#36798;&#21040;&#32422;90\%&#30340;&#25104;&#21151;&#29575;&#12290;&#28436;&#31034;&#35270;&#39057;&#21487;&#22312;https://youtu.be/zJZ1aNlXsMU&#19978;&#35266;&#30475;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent robots rely on object detection models to perceive the environment. Following advances in deep learning security it has been revealed that object detection models are vulnerable to adversarial attacks. However, prior research primarily focuses on attacking static images or offline videos. Therefore, it is still unclear if such attacks could jeopardize real-world robotic applications in dynamic environments. This paper bridges this gap by presenting the first real-time online attack against object detection models. We devise three attacks that fabricate bounding boxes for nonexistent objects at desired locations. The attacks achieve a success rate of about 90\% within about 20 iterations. The demo video is available at https://youtu.be/zJZ1aNlXsMU.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25277;&#22870;&#27744;&#65288;Lottery Pools&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#24179;&#22343;&#30456;&#37051;&#23398;&#20064;&#24471;&#21040;&#30340;&#23376;&#32593;&#32476;&#30340;&#26435;&#37325;&#25110;&#32773;&#36890;&#36807;&#31616;&#21333;&#30340;&#25554;&#20540;&#31574;&#30053;&#23545;&#36845;&#20195;&#21098;&#26525;&#30830;&#23450;&#30340;&#23376;&#32593;&#32476;&#25191;&#34892;&#8220;&#38598;&#25104;&#8221;&#65292;&#20174;&#32780;&#25552;&#39640;&#25277;&#22870;&#31080;&#65288;LTs&#65289;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.10842</link><description>&lt;p&gt;
&#25277;&#22870;&#27744;&#65306;&#36890;&#36807;&#25554;&#20540;&#31080;&#25454;&#32780;&#19981;&#22686;&#21152;&#35757;&#32451;&#25110;&#25512;&#29702;&#25104;&#26412;&#26469;&#33719;&#32988;
&lt;/p&gt;
&lt;p&gt;
Lottery Pools: Winning More by Interpolating Tickets without Increasing Training or Inference Cost. (arXiv:2208.10842v4 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.10842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#25277;&#22870;&#27744;&#65288;Lottery Pools&#65289;&#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#30452;&#25509;&#24179;&#22343;&#30456;&#37051;&#23398;&#20064;&#24471;&#21040;&#30340;&#23376;&#32593;&#32476;&#30340;&#26435;&#37325;&#25110;&#32773;&#36890;&#36807;&#31616;&#21333;&#30340;&#25554;&#20540;&#31574;&#30053;&#23545;&#36845;&#20195;&#21098;&#26525;&#30830;&#23450;&#30340;&#23376;&#32593;&#32476;&#25191;&#34892;&#8220;&#38598;&#25104;&#8221;&#65292;&#20174;&#32780;&#25552;&#39640;&#25277;&#22870;&#31080;&#65288;LTs&#65289;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25277;&#22870;&#31080;&#65288;LTs&#65289;&#21487;&#20197;&#21457;&#29616;&#31934;&#30830;&#19988;&#31232;&#30095;&#30340;&#23376;&#32593;&#32476;&#65292;&#36825;&#20123;&#23376;&#32593;&#32476;&#21487;&#20197;&#34987;&#21333;&#29420;&#35757;&#32451;&#20197;&#21305;&#37197;&#23494;&#38598;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;&#32780;&#38598;&#25104;&#65288;Ensemble&#65289;&#26159;&#26426;&#22120;&#23398;&#20064;&#20013;&#26368;&#21476;&#32769;&#30340;&#32463;&#36807;&#26102;&#38388;&#39564;&#35777;&#30340;&#25216;&#24039;&#20043;&#19968;&#65292;&#36890;&#36807;&#32452;&#21512;&#22810;&#20010;&#29420;&#31435;&#27169;&#22411;&#30340;&#36755;&#20986;&#26469;&#25552;&#39640;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#22312;LTs&#30340;&#32972;&#26223;&#19979;&#65292;&#38598;&#25104;&#30340;&#22909;&#22788;&#20250;&#34987;&#31232;&#30095;&#23376;&#32593;&#32476;&#30340;&#39044;&#27979;&#32467;&#26524;&#25152;&#21066;&#24369;&#12290;&#26412;&#25991;&#39318;&#20808;&#35266;&#23519;&#21040;&#30452;&#25509;&#24179;&#22343;&#30456;&#37051;&#23398;&#20064;&#24471;&#21040;&#30340;&#27425;&#32423;&#23376;&#32593;&#32476;&#30340;&#26435;&#37325;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;LTs&#30340;&#24615;&#33021;&#12290;&#21463;&#21040;&#36825;&#19968;&#35266;&#23519;&#30340;&#40723;&#33310;&#65292;&#25105;&#20204;&#36827;&#19968;&#27493;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#31616;&#21333;&#30340;&#25554;&#20540;&#31574;&#30053;&#23545;&#36845;&#20195;&#21098;&#26525;&#30830;&#23450;&#30340;&#23376;&#32593;&#32476;&#25191;&#34892;&#8220;&#38598;&#25104;&#8221;&#30340;&#26367;&#20195;&#26041;&#27861;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#31216;&#20026;&#25277;&#22870;&#27744;&#12290;&#19982;&#27809;&#26377;&#24615;&#33021;&#22686;&#30410;&#30340;&#26420;&#32032;&#38598;&#25104;&#19981;&#21516;&#65292;&#25193;&#23637;&#25277;&#22870;&#27744;&#21487;&#20197;&#25552;&#39640;&#27599;&#20010;&#21333;&#29420;&#30340;&#23376;&#32593;&#32476;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Lottery tickets (LTs) is able to discover accurate and sparse subnetworks that could be trained in isolation to match the performance of dense networks. Ensemble, in parallel, is one of the oldest time-proven tricks in machine learning to improve performance by combining the output of multiple independent models. However, the benefits of ensemble in the context of LTs will be diluted since ensemble does not directly lead to stronger sparse subnetworks, but leverages their predictions for a better decision. In this work, we first observe that directly averaging the weights of the adjacent learned subnetworks significantly boosts the performance of LTs. Encouraged by this observation, we further propose an alternative way to perform an 'ensemble' over the subnetworks identified by iterative magnitude pruning via a simple interpolating strategy. We call our method Lottery Pools. In contrast to the naive ensemble which brings no performance gains to each single subnetwork, Lottery Pools yi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#65288;MANAS&#65289;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#21435;&#38632;&#12290; &#35813;&#26041;&#27861;&#33258;&#21160;&#25628;&#32034;&#21435;&#38632;&#32593;&#32476;&#30340;&#20869;&#37096;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#32467;&#26500;&#65292;&#24182;&#37319;&#29992;&#26377;&#25928;&#30340;&#22810;&#23610;&#24230;&#35757;&#32451;&#31574;&#30053;&#30830;&#20445;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;</title><link>http://arxiv.org/abs/2207.00728</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#22270;&#20687;&#21435;&#38632;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Attentive Image De-raining Networks via Neural Architecture Search. (arXiv:2207.00728v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.00728
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#65288;MANAS&#65289;&#26694;&#26550;&#65292;&#21487;&#29992;&#20110;&#22270;&#20687;&#21435;&#38632;&#12290; &#35813;&#26041;&#27861;&#33258;&#21160;&#25628;&#32034;&#21435;&#38632;&#32593;&#32476;&#30340;&#20869;&#37096;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#32467;&#26500;&#65292;&#24182;&#37319;&#29992;&#26377;&#25928;&#30340;&#22810;&#23610;&#24230;&#35757;&#32451;&#31574;&#30053;&#30830;&#20445;&#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#26550;&#26500;&#21644;&#27880;&#24847;&#27169;&#22359;&#24050;&#32463;&#35777;&#26126;&#22312;&#35768;&#22810;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#30340;&#22270;&#20687;&#21435;&#38632;&#26041;&#27861;&#20013;&#24456;&#26377;&#25928;&#12290;&#28982;&#32780;&#65292;&#25163;&#21160;&#35774;&#35745;&#21644;&#38598;&#25104;&#36825;&#20004;&#20010;&#32452;&#20214;&#21040;&#31070;&#32463;&#32593;&#32476;&#20013;&#38656;&#35201;&#22823;&#37327;&#30340;&#21171;&#21160;&#21644;&#24191;&#27867;&#30340;&#19987;&#19994;&#30693;&#35782;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#20026;&#22270;&#20687;&#21435;&#38632;&#25216;&#26415;&#24320;&#21457;&#20102;&#19968;&#31181;&#39640;&#24615;&#33021;&#30340;&#22810;&#23610;&#24230;&#27880;&#24847;&#31070;&#32463;&#20307;&#31995;&#32467;&#26500;&#25628;&#32034;&#65288;MANAS&#65289;&#26694;&#26550;&#12290;&#35813;&#26041;&#27861;&#21033;&#29992;&#22810;&#20010;&#28789;&#27963;&#30340;&#27169;&#22359;&#26500;&#24314;&#20102;&#19968;&#20010;&#26032;&#30340;&#22810;&#23610;&#24230;&#20851;&#27880;&#25628;&#32034;&#31354;&#38388;&#65292;&#36825;&#20123;&#27169;&#22359;&#38750;&#24120;&#36866;&#21512;&#20110;&#22270;&#20687;&#21435;&#38632;&#20219;&#21153;&#12290;&#22312;&#35813;&#25628;&#32034;&#31354;&#38388;&#19979;&#65292;&#26500;&#24314;&#20102;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#21333;&#20803;&#65292;&#36827;&#19968;&#27493;&#29992;&#20110;&#26500;&#24314;&#19968;&#20010;&#24378;&#22823;&#30340;&#22270;&#20687;&#21435;&#38632;&#32593;&#32476;&#12290;&#21435;&#38632;&#32593;&#32476;&#30340;&#20869;&#37096;&#22810;&#23610;&#24230;&#27880;&#24847;&#21147;&#32467;&#26500;&#36890;&#36807;&#22522;&#20110;&#26799;&#24230;&#30340;&#25628;&#32034;&#31639;&#27861;&#33258;&#21160;&#25628;&#32034;&#65292;&#20174;&#26576;&#31181;&#31243;&#24230;&#19978;&#36991;&#20813;&#20102;&#25163;&#21160;&#35774;&#35745;&#30340;&#32321;&#29712;&#36807;&#31243;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#33719;&#24471;&#40065;&#26834;&#30340;&#22270;&#20687;&#21435;&#38632;&#27169;&#22411;&#65292;&#36827;&#19968;&#27493;&#37319;&#29992;&#20102;&#23454;&#29992;&#19988;&#26377;&#25928;&#30340;&#22810;&#23610;&#24230;&#35757;&#32451;&#31574;&#30053;&#65292;&#30830;&#20445;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-scale architectures and attention modules have shown effectiveness in many deep learning-based image de-raining methods. However, manually designing and integrating these two components into a neural network requires a bulk of labor and extensive expertise. In this article, a high-performance multi-scale attentive neural architecture search (MANAS) framework is technically developed for image deraining. The proposed method formulates a new multi-scale attention search space with multiple flexible modules that are favorite to the image de-raining task. Under the search space, multi-scale attentive cells are built, which are further used to construct a powerful image de-raining network. The internal multiscale attentive architecture of the de-raining network is searched automatically through a gradient-based search algorithm, which avoids the daunting procedure of the manual design to some extent. Moreover, in order to obtain a robust image de-raining model, a practical and effecti
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Yankee Swap&#36807;&#31243;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#29992;&#20110;&#25311;&#38453;&#31561;&#32423;&#20272;&#20540;&#30340;&#26080;&#20998;&#21106;&#29289;&#21697;&#20844;&#24179;&#20998;&#37197;&#65292;&#36825;&#31181;&#26041;&#27861;&#26131;&#20110;&#29702;&#35299;&#65292;&#24555;&#36895;&#21487;&#25193;&#23637;&#12290;</title><link>http://arxiv.org/abs/2206.08495</link><description>&lt;p&gt;
Yankee Swap&#65306;&#29992;&#20110;&#25311;&#38453;&#31561;&#32423;&#20272;&#20540;&#30340;&#24555;&#36895;&#20844;&#24179;&#20998;&#37197;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Yankee Swap: a Fast and Simple Fair Allocation Mechanism for Matroid Rank Valuations. (arXiv:2206.08495v5 [cs.DS] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2206.08495
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;Yankee Swap&#36807;&#31243;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#29992;&#20110;&#25311;&#38453;&#31561;&#32423;&#20272;&#20540;&#30340;&#26080;&#20998;&#21106;&#29289;&#21697;&#20844;&#24179;&#20998;&#37197;&#65292;&#36825;&#31181;&#26041;&#27861;&#26131;&#20110;&#29702;&#35299;&#65292;&#24555;&#36895;&#21487;&#25193;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21442;&#19982;&#32773;&#20855;&#26377;&#25311;&#38453;&#31561;&#32423;&#20272;&#20540;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#26080;&#20998;&#21106;&#29289;&#21697;&#30340;&#20844;&#24179;&#20998;&#37197;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#22522;&#20110;&#20442;&#35821;Yankee Swap&#36807;&#31243;&#30340;&#31616;&#21333;&#31639;&#27861;&#65292;&#23427;&#35745;&#31639;&#20986;&#21487;&#35777;&#26126;&#20844;&#24179;&#21644;&#39640;&#25928;&#30340;Lorenz&#25903;&#37197;&#20998;&#37197;&#12290;&#34429;&#28982;&#26377;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#21487;&#29992;&#20110;&#35745;&#31639;&#27492;&#31867;&#20998;&#37197;&#65292;&#20294;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#26377;&#20004;&#31181;&#25913;&#36827;&#26041;&#24335;&#12290; &#65288;a&#65289;&#25105;&#20204;&#30340;&#26041;&#27861;&#26131;&#20110;&#29702;&#35299;&#65292;&#19981;&#20351;&#29992;&#22797;&#26434;&#30340;&#25311;&#38453;&#20248;&#21270;&#31639;&#27861;&#20316;&#20026;&#23376;&#20363;&#31243;&#12290; &#65288;b&#65289;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#21487;&#25193;&#23637;&#30340;&#65307; &#23427;&#34987;&#35777;&#26126;&#27604;&#25152;&#26377;&#24050;&#30693;&#30340;&#35745;&#31639;Lorenz&#25903;&#37197;&#20998;&#37197;&#30340;&#31639;&#27861;&#26356;&#24555;&#12290;&#36825;&#20004;&#20010;&#23646;&#24615;&#23545;&#20110;&#20219;&#20309;&#30495;&#27491;&#20844;&#24179;&#20998;&#37197;&#29615;&#22659;&#20013;&#31639;&#27861;&#30340;&#37319;&#29992;&#33267;&#20851;&#37325;&#35201;&#65307;&#25105;&#20204;&#30340;&#36129;&#29486;&#20351;&#25105;&#20204;&#31163;&#36825;&#20010;&#30446;&#26631;&#26356;&#36817;&#20102;&#19968;&#27493;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study fair allocation of indivisible goods when agents have matroid rank valuations. Our main contribution is a simple algorithm based on the colloquial Yankee Swap procedure that computes provably fair and efficient Lorenz dominating allocations. While there exist polynomial time algorithms to compute such allocations, our proposed method improves on them in two ways. (a) Our approach is easy to understand and does not use complex matroid optimization algorithms as subroutines. (b) Our approach is scalable; it is provably faster than all known algorithms to compute Lorenz dominating allocations. These two properties are key to the adoption of algorithms in any real fair allocation setting; our contribution brings us one step closer to this goal.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#21442;&#25968;&#27491;&#21017;&#21270;&#20197;&#36991;&#20813;&#37325;&#22797;&#35757;&#32451;&#65292;&#24182;&#22312;&#19981;&#20250;&#36864;&#21270;&#24050;&#23398;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.17269</link><description>&lt;p&gt;
&#12298;&#28145;&#20837;&#30740;&#31350;&#26080;&#38656;&#37325;&#22797;&#35757;&#32451;&#30340;&#28176;&#36827;&#24335;&#23398;&#20064;&#12299;
&lt;/p&gt;
&lt;p&gt;
A Closer Look at Rehearsal-Free Continual Learning. (arXiv:2203.17269v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.17269
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#26032;&#30340;&#28176;&#36827;&#24335;&#23398;&#20064;&#26041;&#27861;&#65292;&#20351;&#29992;&#30693;&#35782;&#33976;&#39311;&#21644;&#21442;&#25968;&#27491;&#21017;&#21270;&#20197;&#36991;&#20813;&#37325;&#22797;&#35757;&#32451;&#65292;&#24182;&#22312;&#19981;&#20250;&#36864;&#21270;&#24050;&#23398;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#20102;&#24378;&#22823;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28176;&#36827;&#24335;&#23398;&#20064;&#26159;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#19981;&#26029;&#21464;&#21270;&#30340;&#35757;&#32451;&#25968;&#25454;&#20013;&#23398;&#20064;&#26032;&#27010;&#24565;&#30340;&#19968;&#31181;&#29615;&#22659;&#65292;&#21516;&#26102;&#36991;&#20813;&#20197;&#21069;&#23398;&#20064;&#30340;&#31867;&#21035;&#20986;&#29616;&#8220;&#28798;&#38590;&#24615;&#36951;&#24536;&#8221;&#29616;&#35937;&#12290;&#24403;&#21069;&#30340;&#21333;&#20219;&#21153;&#25193;&#23637;&#24615;&#28176;&#36827;&#24335;&#23398;&#20064;&#26041;&#27861;&#38656;&#35201;&#22823;&#37327;&#37325;&#22797;&#35757;&#32451;&#20197;&#36991;&#20813;&#30693;&#35782;&#36864;&#21270;&#65292;&#20294;&#37325;&#22797;&#35757;&#32451;&#20250;&#21344;&#29992;&#22823;&#37327;&#20869;&#23384;&#65292;&#24182;&#21487;&#33021;&#36829;&#21453;&#25968;&#25454;&#38544;&#31169;&#12290;&#30456;&#21453;&#65292;&#25105;&#20204;&#25506;&#32034;&#20102;&#23558;&#30693;&#35782;&#33976;&#39311;&#21644;&#21442;&#25968;&#27491;&#21017;&#21270;&#20197;&#26032;&#30340;&#26041;&#24335;&#32467;&#21512;&#36215;&#26469;&#65292;&#20197;&#22312;&#19981;&#36827;&#34892;&#37325;&#22797;&#35757;&#32451;&#30340;&#24773;&#20917;&#19979;&#23454;&#29616;&#24378;&#22823;&#30340;&#28176;&#36827;&#24335;&#23398;&#20064;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continual learning is a setting where machine learning models learn novel concepts from continuously shifting training data, while simultaneously avoiding degradation of knowledge on previously seen classes which may disappear from the training data for extended periods of time (a phenomenon known as the catastrophic forgetting problem). Current approaches for continual learning of a single expanding task (aka class-incremental continual learning) require extensive rehearsal of previously seen data to avoid this degradation of knowledge. Unfortunately, rehearsal comes at a cost to memory, and it may also violate data-privacy. Instead, we explore combining knowledge distillation and parameter regularization in new ways to achieve strong continual learning performance without rehearsal. Specifically, we take a deep dive into common continual learning techniques: prediction distillation, feature distillation, L2 parameter regularization, and EWC parameter regularization. We first disprove
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#26088;&#22312;&#36830;&#25509;&#24191;&#27867;&#30340;&#22810;&#23398;&#31185;&#29616;&#26377;&#30693;&#35782;&#65292;&#20026;&#31227;&#21160;&#26426;&#22120;&#20154;&#26500;&#24314;&#23436;&#25972;&#30340;&#24773;&#22659;&#24863;&#30693;&#65288;SA&#65289;&#31995;&#32479;&#65292;&#20197;&#25552;&#21319;&#20854;&#33258;&#20027;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2110.00273</link><description>&lt;p&gt;
&#20174;SLAM&#21040;&#24773;&#22659;&#24863;&#30693;&#65306;&#25361;&#25112;&#19982;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
From SLAM to Situational Awareness: Challenges and Survey. (arXiv:2110.00273v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2110.00273
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#26088;&#22312;&#36830;&#25509;&#24191;&#27867;&#30340;&#22810;&#23398;&#31185;&#29616;&#26377;&#30693;&#35782;&#65292;&#20026;&#31227;&#21160;&#26426;&#22120;&#20154;&#26500;&#24314;&#23436;&#25972;&#30340;&#24773;&#22659;&#24863;&#30693;&#65288;SA&#65289;&#31995;&#32479;&#65292;&#20197;&#25552;&#21319;&#20854;&#33258;&#20027;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31227;&#21160;&#26426;&#22120;&#20154;&#33021;&#22815;&#39640;&#25928;&#12289;&#23433;&#20840;&#22320;&#25191;&#34892;&#22797;&#26434;&#20219;&#21153;&#30340;&#33021;&#21147;&#21463;&#38480;&#20110;&#20854;&#23545;&#29615;&#22659;&#65288;&#21363;&#24773;&#20917;&#65289;&#30340;&#20102;&#35299;&#12290;&#20808;&#36827;&#30340;&#25512;&#29702;&#12289;&#20915;&#31574;&#21644;&#25191;&#34892;&#25216;&#33021;&#20351;&#26234;&#33021;&#20307;&#33021;&#22815;&#22312;&#26410;&#30693;&#29615;&#22659;&#20013;&#33258;&#20027;&#34892;&#21160;&#12290;&#24773;&#22659;&#24863;&#30693;&#65288;SA&#65289;&#26159;&#20154;&#31867;&#30340;&#19968;&#31181;&#22522;&#26412;&#33021;&#21147;&#65292;&#24050;&#22312;&#24515;&#29702;&#23398;&#12289;&#20891;&#20107;&#12289;&#33322;&#31354;&#33322;&#22825;&#21644;&#25945;&#32946;&#31561;&#21508;&#20010;&#39046;&#22495;&#28145;&#20837;&#30740;&#31350;&#12290;&#28982;&#32780;&#65292;&#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#23578;&#26410;&#32771;&#34385;&#24773;&#22659;&#24863;&#30693;&#65292;&#32780;&#26159;&#19987;&#27880;&#20110;&#21333;&#19968;&#30340;&#27010;&#24565;&#65292;&#22914;&#24863;&#30693;&#12289;&#31354;&#38388;&#24863;&#30693;&#12289;&#20256;&#24863;&#22120;&#34701;&#21512;&#12289;&#29366;&#24577;&#20272;&#35745;&#21644;&#21516;&#26102;&#23450;&#20301;&#19982;&#26144;&#23556;&#65288;SLAM&#65289;&#12290;&#22240;&#27492;&#65292;&#26412;&#30740;&#31350;&#26088;&#22312;&#36830;&#25509;&#24191;&#27867;&#30340;&#22810;&#23398;&#31185;&#29616;&#26377;&#30693;&#35782;&#65292;&#20026;&#25105;&#20204;&#35748;&#20026;&#23545;&#20110;&#33258;&#20027;&#24615;&#33267;&#20851;&#37325;&#35201;&#30340;&#31227;&#21160;&#26426;&#22120;&#20154;&#23436;&#25972;&#30340;SA&#31995;&#32479;&#38138;&#24179;&#36947;&#36335;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#26500;&#24314;&#26426;&#22120;&#20154;SA&#30340;&#20027;&#35201;&#32452;&#25104;&#37096;&#20998;&#21450;&#20854;&#25152;&#28041;&#21450;&#30340;&#39046;&#22495;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;
&lt;/p&gt;
&lt;p&gt;
The capability of a mobile robot to efficiently and safely perform complex missions is limited by its knowledge of the environment, namely the situation. Advanced reasoning, decision-making, and execution skills enable an intelligent agent to act autonomously in unknown environments. Situational Awareness (SA) is a fundamental capability of humans that has been deeply studied in various fields, such as psychology, military, aerospace, and education. Nevertheless, it has yet to be considered in robotics, which has focused on single compartmentalized concepts such as sensing, spatial perception, sensor fusion, state estimation, and Simultaneous Localization and Mapping (SLAM). Hence, the present research aims to connect the broad multidisciplinary existing knowledge to pave the way for a complete SA system for mobile robotics that we deem paramount for autonomy. To this aim, we define the principal components to structure a robotic SA and their area of competence. Accordingly, this paper
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#25511;&#21046;&#22120;&#34701;&#21512;&#65288;BCF&#65289;&#30340;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#34701;&#21512;&#27599;&#20010;&#31995;&#32479;&#30340;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#20998;&#24067;&#36755;&#20986;&#65292;&#21033;&#29992;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#33719;&#24471;&#25104;&#21151;&#12290;</title><link>http://arxiv.org/abs/2107.09822</link><description>&lt;p&gt;
&#22522;&#20110;&#36125;&#21494;&#26031;&#25511;&#21046;&#22120;&#34701;&#21512;&#30340;&#26426;&#22120;&#20154;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#65306;&#21033;&#29992;&#25511;&#21046;&#20808;&#39564;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Bayesian Controller Fusion: Leveraging Control Priors in Deep Reinforcement Learning for Robotics. (arXiv:2107.09822v3 [cs.RO] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.09822
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#36125;&#21494;&#26031;&#25511;&#21046;&#22120;&#34701;&#21512;&#65288;BCF&#65289;&#30340;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#65292;&#36890;&#36807;&#34701;&#21512;&#27599;&#20010;&#31995;&#32479;&#30340;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#20998;&#24067;&#36755;&#20986;&#65292;&#21033;&#29992;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#21183;&#65292;&#22312;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#33719;&#24471;&#25104;&#21151;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#36125;&#21494;&#26031;&#25511;&#21046;&#22120;&#34701;&#21512; (BCF) &#30340;&#28151;&#21512;&#25511;&#21046;&#31574;&#30053;&#65292;&#23558;&#20256;&#32479;&#25163;&#24037;&#25511;&#21046;&#22120;&#21644;&#22522;&#20110;&#27169;&#22411;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064; (RL) &#30340;&#20248;&#28857;&#32467;&#21512;&#36215;&#26469;&#12290;BCF &#22312;&#26426;&#22120;&#20154;&#39046;&#22495;&#20013;&#33719;&#24471;&#20102;&#25104;&#21151;&#65292;&#22312;&#35768;&#22810;&#20219;&#21153;&#20013;&#37117;&#23384;&#22312;&#21487;&#38752;&#20294;&#27425;&#20248;&#30340;&#25511;&#21046;&#20808;&#39564;&#30693;&#35782;&#65292;&#20294;&#20174;&#22836;&#24320;&#22987;&#36827;&#34892;RL&#20173;&#28982;&#19981;&#23433;&#20840;&#19988;&#25968;&#25454;&#20302;&#25928;&#12290;&#36890;&#36807;&#34701;&#21512;&#27599;&#20010;&#31995;&#32479;&#30340;&#20855;&#26377;&#19981;&#30830;&#23450;&#24615;&#30340;&#20998;&#24067;&#36755;&#20986;&#65292;BCF &#22312;&#23427;&#20204;&#20043;&#38388;&#20210;&#35009;&#25511;&#21046;&#65292;&#24182;&#21033;&#29992;&#23427;&#20204;&#21508;&#33258;&#30340;&#20248;&#21183;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#20219;&#21153;&#20013;&#30740;&#31350;&#20102;BCF&#65292;&#36825;&#20123;&#20219;&#21153;&#28041;&#21450;&#22312;&#24191;&#38420;&#32780;&#38271;&#26102;&#38388;&#30340;&#29615;&#22659;&#20013;&#23548;&#33322;&#65292;&#20197;&#21450;&#19968;&#20010;&#28041;&#21450;&#25805;&#20316;&#28789;&#27963;&#24615;&#26368;&#22823;&#21270;&#30340;&#22797;&#26434;&#25235;&#21462;&#20219;&#21153;&#12290;&#23545;&#20110;&#36825;&#20004;&#20010;&#22495;&#65292;&#23384;&#22312;&#31616;&#21333;&#30340;&#25163;&#24037;&#25511;&#21046;&#22120;&#65292;&#21487;&#20197;&#20197;&#39118;&#38505;&#35268;&#36991;&#30340;&#26041;&#24335;&#35299;&#20915;&#25163;&#22836;&#30340;&#20219;&#21153;&#65292;&#20294;&#24182;&#19981;&#19968;&#23450;&#21576;&#29616;&#20986;&#26368;&#20339;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#26159;&#30001;&#20110;&#20998;&#26512;&#24314;&#27169;&#30340;&#38480;&#21046;&#12289;&#25511;&#21046;&#22120;&#35843;&#25972;&#19981;&#31934;&#30830;&#21644;&#20219;&#21153;&#30340;&#21464;&#21270;&#12290;&#30001;&#20110;&#20808;&#39564;&#22312;&#26089;&#26399;&#38454;&#27573;&#33258;&#28982;&#22320;&#25351;&#23548;&#25506;&#32034;&#65292;&#22240;&#27492;BCF&#26500;&#25104;&#20102;&#19968;&#20010;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#65292;&#20174;&#32780;&#36991;&#20813;&#20102;RL&#20174;&#22836;&#24320;&#22987;&#21487;&#33021;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Bayesian Controller Fusion (BCF): a hybrid control strategy that combines the strengths of traditional hand-crafted controllers and model-free deep reinforcement learning (RL). BCF thrives in the robotics domain, where reliable but suboptimal control priors exist for many tasks, but RL from scratch remains unsafe and data-inefficient. By fusing uncertainty-aware distributional outputs from each system, BCF arbitrates control between them, exploiting their respective strengths. We study BCF on two real-world robotics tasks involving navigation in a vast and long-horizon environment, and a complex reaching task that involves manipulability maximisation. For both these domains, simple handcrafted controllers exist that can solve the task at hand in a risk-averse manner but do not necessarily exhibit the optimal solution given limitations in analytical modelling, controller miscalibration and task variation. As exploration is naturally guided by the prior in the early stages of 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#36807;&#25311;&#21512;&#29305;&#24615;&#30340;&#21518;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#38169;&#35823;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#24182;&#36880;&#27493;&#21024;&#38500;&#23545;&#20915;&#31574;&#36793;&#30028;&#26377;&#36739;&#39640;&#24433;&#21709;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2106.07217</link><description>&lt;p&gt;
&#22522;&#20110;&#36807;&#25311;&#21512;&#27169;&#22411;&#29305;&#24615;&#30340;&#22122;&#22768;&#26631;&#31614;&#26816;&#27979;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Over-Fit: Noisy-Label Detection based on the Overfitted Model Property. (arXiv:2106.07217v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2106.07217
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#27169;&#22411;&#36807;&#25311;&#21512;&#29305;&#24615;&#30340;&#21518;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#65292;&#33021;&#22815;&#35782;&#21035;&#38169;&#35823;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#24182;&#36880;&#27493;&#21024;&#38500;&#23545;&#20915;&#31574;&#36793;&#30028;&#26377;&#36739;&#39640;&#24433;&#21709;&#30340;&#26679;&#26412;&#65292;&#20174;&#32780;&#25552;&#39640;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30001;&#20110;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20855;&#26377;&#39640;&#23481;&#37327;&#29305;&#24615;&#65292;&#21363;&#20351;&#26159;&#22122;&#22768;&#26631;&#31614;&#65292;&#20063;&#23481;&#26131;&#36807;&#24230;&#25311;&#21512;&#65292;&#20174;&#32780;&#38477;&#20302;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#21518;&#35757;&#32451;&#23398;&#20064;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#21547;&#22122;&#22768;&#26631;&#31614;&#30340;&#25968;&#25454;&#20013;&#26174;&#33879;&#25552;&#39640;&#20219;&#20309;&#39044;&#35757;&#32451;&#27169;&#22411;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#21033;&#29992;&#35757;&#32451;&#27169;&#22411;&#30340;&#36807;&#25311;&#21512;&#29305;&#24615;&#26469;&#35782;&#21035;&#38169;&#35823;&#26631;&#35760;&#30340;&#26679;&#26412;&#65292;&#36880;&#27493;&#21024;&#38500;&#23545;&#20915;&#31574;&#36793;&#30028;&#26377;&#36739;&#39640;&#24433;&#21709;&#30340;&#26679;&#26412;&#65292;&#24182;&#25913;&#21892;&#20915;&#31574;&#36793;&#30028;&#26469;&#25552;&#39640;&#27867;&#21270;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#21518;&#35757;&#32451;&#26041;&#27861;&#19982;&#29616;&#26377;&#30340;&#22122;&#22768;&#26631;&#31614;&#23398;&#20064;&#26041;&#27861;&#30456;&#32467;&#21512;&#20855;&#26377;&#24456;&#22909;&#30340;&#21327;&#21516;&#25928;&#26524;&#12290;&#22312;&#22810;&#31181;&#30495;&#23454;&#19990;&#30028;&#21644;&#21512;&#25104;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#20102;&#25105;&#20204;&#26041;&#27861;&#22312;&#21508;&#31181;&#23454;&#38469;&#24773;&#20917;&#19979;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural network can easily overfit to even noisy labels due to its high capacity, which degrades the generalization performance of a model. To overcome this issue, we propose a new approach for learning from noisy labels (LNL) via post-training, which can significantly improve the generalization performance of any pre-trained model on noisy label data. To this end, we rather exploit the overfitting property of a trained model to identify mislabeled samples. Specifically, our post-training approach gradually removes samples with high influence on the decision boundary and refines the decision boundary to improve generalization performance. Our post-training approach creates great synergies when combined with the existing LNL methods. Experimental results on various real-world and synthetic benchmark datasets demonstrate the validity of our approach in diverse realistic scenarios.
&lt;/p&gt;</description></item></channel></rss>