<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25193;&#23637;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#21457;&#29616;&#26469;&#33258;&#21160;&#21270;&#35757;&#32451;&#24182;&#28040;&#38500;&#36890;&#20449;&#24320;&#38144;&#65292;&#36890;&#36807;&#23558;&#35821;&#26009;&#24211;&#32858;&#31867;&#25104;&#30456;&#20851;&#25991;&#26723;&#38598;&#26469;&#35757;&#32451;&#21333;&#29420;&#30340;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#19968;&#20010;&#31232;&#30095;&#30340;&#38598;&#21512;&#36827;&#34892;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.14177</link><description>&lt;p&gt;
&#29992;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#21457;&#29616;&#26041;&#27861;&#25193;&#23637;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scaling Expert Language Models with Unsupervised Domain Discovery. (arXiv:2303.14177v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14177
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#29992;&#20110;&#25193;&#23637;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#30340;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#26080;&#30417;&#30563;&#30340;&#39046;&#22495;&#21457;&#29616;&#26469;&#33258;&#21160;&#21270;&#35757;&#32451;&#24182;&#28040;&#38500;&#36890;&#20449;&#24320;&#38144;&#65292;&#36890;&#36807;&#23558;&#35821;&#26009;&#24211;&#32858;&#31867;&#25104;&#30456;&#20851;&#25991;&#26723;&#38598;&#26469;&#35757;&#32451;&#21333;&#29420;&#30340;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#24182;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#19968;&#20010;&#31232;&#30095;&#30340;&#38598;&#21512;&#36827;&#34892;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36890;&#24120;&#36827;&#34892;&#23494;&#38598;&#35757;&#32451;&#65306;&#25152;&#26377;&#21442;&#25968;&#22343;&#23545;&#25152;&#26377;&#36755;&#20837;&#36827;&#34892;&#26356;&#26032;&#12290;&#36825;&#35201;&#27714;&#22312;&#25968;&#21315;&#20010;GPU&#20043;&#38388;&#21516;&#27493;&#25968;&#21313;&#20159;&#20010;&#21442;&#25968;&#12290;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#65292;&#33021;&#22815;&#24322;&#27493;&#22320;&#22312;&#20219;&#24847;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#35757;&#32451;&#22823;&#22411;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#19968;&#20010;&#35821;&#26009;&#24211;&#32858;&#31867;&#25104;&#30456;&#20851;&#25991;&#26723;&#38598;&#65292;&#23545;&#27599;&#20010;&#38598;&#32676;&#35757;&#32451;&#19968;&#20010;&#21333;&#29420;&#30340;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#65292;&#28982;&#21518;&#22312;&#25512;&#29702;&#26102;&#23558;&#23427;&#20204;&#32452;&#21512;&#25104;&#31232;&#30095;&#30340;&#38598;&#21512;&#12290;&#36825;&#31181;&#26041;&#27861;&#36890;&#36807;&#33258;&#21160;&#21457;&#29616;&#27599;&#20010;&#19987;&#23478;&#30340;&#39046;&#22495;&#26469;&#25512;&#24191;&#20102;&#23604;&#23596;&#24179;&#34892;&#35757;&#32451;&#65292;&#24182;&#28040;&#38500;&#20102;&#29616;&#26377;&#31232;&#30095;&#35821;&#35328;&#27169;&#22411;&#20013;&#20960;&#20046;&#25152;&#26377;&#30340;&#36890;&#20449;&#24320;&#38144;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#22312;&#22810;&#20010;&#35821;&#26009;&#24211;&#21644;&#23569;&#37327;&#35757;&#32451;&#20219;&#21153;&#19978;&#20248;&#20110;&#23494;&#38598;&#22522;&#32447;&#65292;&#24182;&#19988;&#25105;&#20204;&#30340;&#20998;&#26512;&#34920;&#26126;&#23558;&#19987;&#23478;&#29305;&#21270;&#21040;&#26377;&#24847;&#20041;&#30340;&#38598;&#32676;&#26159;&#21462;&#24471;&#36825;&#20123;&#22686;&#30410;&#30340;&#20851;&#38190;&#12290;&#24615;&#33021;&#36824;&#38543;&#30528;&#19987;&#23478;&#25968;&#37327;&#21644;&#35757;&#32451;&#25968;&#25454;&#30340;&#22823;&#23567;&#32780;&#25552;&#39640;&#65292;&#36825;&#34920;&#26126;&#36825;&#26159;&#19968;&#31181;&#39640;&#25928;&#19988;&#21487;&#35775;&#38382;&#30340;&#32553;&#25918;&#19987;&#23478;&#35821;&#35328;&#27169;&#22411;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models are typically trained densely: all parameters are updated with respect to all inputs. This requires synchronization of billions of parameters across thousands of GPUs. We introduce a simple but effective method to asynchronously train large, sparse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models. Our technique outperforms dense baselines on multiple corpora and few-shot tasks, and our analysis shows that specializing experts to meaningful clusters is key to these gains. Performance also improves with the number of experts and size of training data, suggesting this is a highly efficient and accessibl
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;ANN-SNN&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#29992;&#20302;&#36895;&#29575;&#30340;&#36741;&#21161;ANN&#21021;&#22987;&#21270;&#29366;&#24577;&#65292;&#35299;&#20915;&#20102;SNN&#19982;&#32463;&#20856;ANN&#22312;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#12289;&#21151;&#32791;&#26041;&#38754;&#30340;&#24179;&#34913;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#20302;&#24310;&#36831;&#21644;&#20302;&#21151;&#32791;&#30340;&#39044;&#27979;&#12290;</title><link>http://arxiv.org/abs/2303.14176</link><description>&lt;p&gt;
&#20302;&#21151;&#32791;&#20302;&#24310;&#36831;&#35270;&#35273;&#24863;&#30693;&#30340;&#28151;&#21512;ANN-SNN&#20307;&#31995;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
A Hybrid ANN-SNN Architecture for Low-Power and Low-Latency Visual Perception. (arXiv:2303.14176v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#28151;&#21512;ANN-SNN&#20307;&#31995;&#32467;&#26500;&#65292;&#36890;&#36807;&#29992;&#20302;&#36895;&#29575;&#30340;&#36741;&#21161;ANN&#21021;&#22987;&#21270;&#29366;&#24577;&#65292;&#35299;&#20915;&#20102;SNN&#19982;&#32463;&#20856;ANN&#22312;&#20934;&#30830;&#24615;&#12289;&#24310;&#36831;&#12289;&#21151;&#32791;&#26041;&#38754;&#30340;&#24179;&#34913;&#24615;&#38382;&#39064;&#65292;&#23454;&#29616;&#20102;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#20302;&#24310;&#36831;&#21644;&#20302;&#21151;&#32791;&#30340;&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#26159;&#19968;&#31867;&#20223;&#29983;&#31070;&#32463;&#32593;&#32476;&#65292;&#36890;&#36807;&#24322;&#27493;&#21644;&#31232;&#30095;&#22788;&#29702;&#65292;&#25215;&#35834;&#23558;&#20302;&#21151;&#32791;&#21644;&#20302;&#24310;&#36831;&#30340;&#25512;&#29702;&#24212;&#29992;&#20110;&#36793;&#32536;&#35774;&#22791;&#12290;&#28982;&#32780;&#65292;&#20316;&#20026;&#26102;&#38388;&#27169;&#22411;&#65292;SNN&#20381;&#36182;&#20110;&#34920;&#36798;&#20016;&#23500;&#30340;&#29366;&#24577;&#25165;&#33021;&#29983;&#25104;&#19982;&#32463;&#20856;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#30456;&#24403;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36825;&#20123;&#29366;&#24577;&#20165;&#22312;&#38271;&#26102;&#38388;&#30636;&#24577;&#21608;&#26399;&#21518;&#25910;&#25947;&#65292;&#24182;&#22312;&#27809;&#26377;&#36755;&#20837;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#36805;&#36895;&#34928;&#20943;&#65292;&#23548;&#33268;&#26356;&#39640;&#30340;&#24310;&#36831;&#12289;&#21151;&#32791;&#21644;&#36739;&#20302;&#30340;&#20934;&#30830;&#24615;&#12290;&#26412;&#25991;&#36890;&#36807;&#20351;&#29992;&#20197;&#20302;&#36895;&#29575;&#36816;&#34892;&#30340;&#36741;&#21161;ANN&#21021;&#22987;&#21270;&#29366;&#24577;&#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#28982;&#21518;&#65292;SNN&#20351;&#29992;&#29366;&#24577;&#20197;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#29983;&#25104;&#39044;&#27979;&#65292;&#30452;&#21040;&#19979;&#19968;&#20010;&#21021;&#22987;&#21270;&#38454;&#27573;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#28151;&#21512;ANN-SNN&#27169;&#22411;&#32467;&#21512;&#20102;&#20004;&#20010;&#19990;&#30028;&#30340;&#20248;&#28857;&#65306;&#20248;&#21270;&#20102;ANN&#21644;SNN&#20043;&#38388;&#30340;&#24615;&#33021;&#24179;&#34913;&#65292;&#19981;&#21463;&#38271;&#26102;&#38388;&#29366;&#24577;&#36716;&#25442;&#21644;&#29366;&#24577;&#34928;&#20943;&#30340;&#24433;&#21709;&#65292;&#21487;&#20197;&#23454;&#29616;&#39640;&#26102;&#38388;&#20998;&#36776;&#29575;&#12289;&#20302;&#24310;&#36831;&#21644;&#20302;&#21151;&#32791;&#30340;&#39044;&#27979;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#35813;&#27169;&#22411;&#22312;&#20302;&#21151;&#32791;/&#20302;&#24310;&#36831;&#35270;&#35273;&#24863;&#30693;&#20219;&#21153;&#19978;&#30340;&#33391;&#22909;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNN) are a class of bio-inspired neural networks that promise to bring low-power and low-latency inference to edge devices through asynchronous and sparse processing. However, being temporal models, SNNs depend heavily on expressive states to generate predictions on par with classical artificial neural networks (ANNs). These states converge only after long transient periods, and quickly decay without input data, leading to higher latency, power consumption, and lower accuracy. This work addresses this issue by initializing the state with an auxiliary ANN running at a low rate. The SNN then uses the state to generate predictions with high temporal resolution until the next initialization phase. Our hybrid ANN-SNN model thus combines the best of both worlds: It does not suffer from long state transients and state decay thanks to the ANN, and can generate predictions with high temporal resolution, low latency, and low power thanks to the SNN. We show for the task 
&lt;/p&gt;</description></item><item><title>&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36827;&#34892;&#26410;&#30693;&#23545;&#35937;&#30340;6-DoF&#36319;&#36394;&#21644;3D&#37325;&#24314;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#35270;&#35273;&#32441;&#29702;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#22823;&#23039;&#24577;&#21464;&#21270;&#65292;&#37096;&#20998;&#21644;&#23436;&#20840;&#36974;&#25377;&#65292;&#26080;&#32441;&#29702;&#34920;&#38754;&#21644;&#21453;&#23556;&#39640;&#20809;&#30340;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2303.14158</link><description>&lt;p&gt;
BundleSDF:&#26410;&#30693;&#29289;&#20307;&#30340;&#31070;&#32463;6-DoF&#36319;&#36394;&#21644;3D&#37325;&#24314;
&lt;/p&gt;
&lt;p&gt;
BundleSDF: Neural 6-DoF Tracking and 3D Reconstruction of Unknown Objects. (arXiv:2303.14158v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14158
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#36827;&#34892;&#26410;&#30693;&#23545;&#35937;&#30340;6-DoF&#36319;&#36394;&#21644;3D&#37325;&#24314;&#65292;&#21363;&#20351;&#22312;&#27809;&#26377;&#35270;&#35273;&#32441;&#29702;&#30340;&#24773;&#20917;&#19979;&#20063;&#36866;&#29992;&#65292;&#24182;&#19988;&#33021;&#22815;&#22788;&#29702;&#22823;&#23039;&#24577;&#21464;&#21270;&#65292;&#37096;&#20998;&#21644;&#23436;&#20840;&#36974;&#25377;&#65292;&#26080;&#32441;&#29702;&#34920;&#38754;&#21644;&#21453;&#23556;&#39640;&#20809;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#20960;&#20046;&#23454;&#26102;&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#20174;&#21333;&#30446;RGBD&#35270;&#39057;&#24207;&#21015;&#20013;&#36827;&#34892;&#26410;&#30693;&#23545;&#35937;&#30340;6-DoF&#36319;&#36394;&#65292;&#21516;&#26102;&#25191;&#34892;&#23545;&#35937;&#30340;&#31070;&#32463;3D&#37325;&#24314;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#24847;&#21018;&#24615;&#23545;&#35937;&#65292;&#21363;&#20351;&#35270;&#35273;&#32441;&#29702;&#22522;&#26412;&#32570;&#22833;&#12290;&#20165;&#22312;&#31532;&#19968;&#24103;&#20013;&#20551;&#23450;&#23545;&#35937;&#34987;&#20998;&#21106;&#12290;&#19981;&#38656;&#35201;&#39069;&#22806;&#20449;&#24687;&#65292;&#24182;&#19988;&#19981;&#23545;&#20132;&#20114;&#20195;&#29702;&#20570;&#20219;&#20309;&#20551;&#35774;&#12290;&#25105;&#20204;&#26041;&#27861;&#30340;&#20851;&#38190;&#26159;&#22312;&#23039;&#24577;&#22270;&#20248;&#21270;&#36807;&#31243;&#20013;&#21516;&#26102;&#23398;&#20064;&#30340;&#31070;&#32463;&#23545;&#35937;&#22330;&#65292;&#20174;&#32780;&#23558;&#20449;&#24687;&#31283;&#20581;&#22320;&#32047;&#31215;&#21040;&#19968;&#33268;&#30340;3D&#34920;&#31034;&#20013;&#65292;&#25429;&#25417;&#20960;&#20309;&#21644;&#22806;&#35266;&#12290;&#33258;&#21160;&#32500;&#25252;&#19968;&#20010;&#21160;&#24577;&#30340;&#30446;&#21069;&#23384;&#20648;&#30340;&#20869;&#23384;&#24103;&#27744;&#65292;&#20197;&#20415;&#36825;&#20123;&#32447;&#31243;&#20043;&#38388;&#36827;&#34892;&#36890;&#20449;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#20855;&#26377;&#22823;&#23039;&#24577;&#21464;&#21270;&#65292;&#37096;&#20998;&#21644;&#23436;&#20840;&#36974;&#25377;&#65292;&#26080;&#32441;&#29702;&#34920;&#38754;&#21644;&#21453;&#23556;&#39640;&#20809;&#30340;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#24207;&#21015;&#19978;&#12290;&#25105;&#20204;&#22312;HO3D&#65292;YCBInEOAT&#21644;BEHAVE&#25968;&#25454;&#38598;&#19978;&#23637;&#31034;&#20102;&#32467;&#26524;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a near real-time method for 6-DoF tracking of an unknown object from a monocular RGBD video sequence, while simultaneously performing neural 3D reconstruction of the object. Our method works for arbitrary rigid objects, even when visual texture is largely absent. The object is assumed to be segmented in the first frame only. No additional information is required, and no assumption is made about the interaction agent. Key to our method is a Neural Object Field that is learned concurrently with a pose graph optimization process in order to robustly accumulate information into a consistent 3D representation capturing both geometry and appearance. A dynamic pool of posed memory frames is automatically maintained to facilitate communication between these threads. Our approach handles challenging sequences with large pose changes, partial and full occlusion, untextured surfaces, and specular highlights. We show results on HO3D, YCBInEOAT, and BEHAVE datasets, demonstrating that ou
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#29992;&#25143;&#24847;&#22270;&#21644;&#20135;&#29983;&#36866;&#24403;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#21709;&#24212;&#30340;&#21487;&#34892;&#24615;&#65292;&#20026;&#26356;&#26234;&#33021;&#30340;&#26234;&#33021;&#31354;&#38388;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.14143</link><description>&lt;p&gt;
&#8220;Get ready for a party&#8221;&#65306;&#8220;&#26356;&#26234;&#33021;&#30340;&#26234;&#33021;&#31354;&#38388;&#8221;&#25506;&#32034;&#22312;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#24110;&#21161;&#19979;
&lt;/p&gt;
&lt;p&gt;
"Get ready for a party": Exploring smarter smart spaces with help from large language models. (arXiv:2303.14143v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14143
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#32034;&#20102;&#20351;&#29992;&#22823;&#35821;&#35328;&#27169;&#22411;&#25512;&#26029;&#29992;&#25143;&#24847;&#22270;&#21644;&#20135;&#29983;&#36866;&#24403;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#21709;&#24212;&#30340;&#21487;&#34892;&#24615;&#65292;&#20026;&#26356;&#26234;&#33021;&#30340;&#26234;&#33021;&#31354;&#38388;&#38138;&#24179;&#20102;&#36947;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#26377;&#20154;&#35828;&#8220;&#20934;&#22791;&#22909;&#32858;&#20250;&#8221;&#65292;&#27491;&#30830;&#30340;&#22238;&#31572;&#28145;&#21463;&#21547;&#20041;&#21644;&#19978;&#19979;&#25991;&#30340;&#24433;&#21709;&#12290;&#23545;&#20110;&#26234;&#33021;&#23478;&#23621;&#21161;&#25163;&#65288;&#20363;&#22914;Google Home&#65289;&#65292;&#29702;&#24819;&#30340;&#22238;&#31572;&#21487;&#33021;&#26159;&#35843;&#26597;&#23478;&#20013;&#21487;&#29992;&#30340;&#35774;&#22791;&#65292;&#24182;&#25913;&#21464;&#23427;&#20204;&#30340;&#29366;&#24577;&#20197;&#33829;&#36896;&#27426;&#20048;&#30340;&#27675;&#22260;&#12290;&#26412;&#25991;&#21033;&#29992;&#26368;&#36817;&#30340;&#20219;&#21153;&#26080;&#20851;&#30340;&#22823;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#65292;&#22914;GPT-3&#65292;&#23427;&#20204;&#20855;&#26377;&#24191;&#27867;&#30340;&#36328;&#39046;&#22495;&#12289;&#26377;&#26102;&#19981;&#21487;&#39044;&#27979;&#30340;&#19978;&#19979;&#25991;&#30693;&#35782;&#65292;&#36825;&#31181;&#30693;&#35782;&#29616;&#26377;&#30340;&#22522;&#20110;&#35268;&#21017;&#30340;&#23478;&#29992;&#21161;&#25163;&#31995;&#32479;&#32570;&#20047;&#65292;&#21487;&#20197;&#20351;&#23427;&#20204;&#25104;&#20026;&#25512;&#26029;&#29992;&#25143;&#24847;&#22270;&#24182;&#22312;&#26234;&#33021;&#23478;&#23621;&#20132;&#20114;&#26399;&#38388;&#29983;&#25104;&#36866;&#24403;&#30340;&#19978;&#19979;&#25991;&#20381;&#36182;&#21709;&#24212;&#30340;&#24378;&#22823;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;
The right response to someone who says "get ready for a party" is deeply influenced by meaning and context. For a smart home assistant (e.g., Google Home), the ideal response might be to survey the available devices in the home and change their state to create a festive atmosphere. Current practical systems cannot service such requests since they require the ability to (1) infer meaning behind an abstract statement and (2) map that inference to a concrete course of action appropriate for the context (e.g., changing the settings of specific devices). In this paper, we leverage the observation that recent task-agnostic large language models (LLMs) like GPT-3 embody a vast amount of cross-domain, sometimes unpredictable contextual knowledge that existing rule-based home assistant systems lack, which can make them powerful tools for inferring user intent and generating appropriate context-dependent responses during smart home interactions. We first explore the feasibility of a system that 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MindDiffuser&#30340;&#20004;&#38454;&#27573;&#22270;&#20687;&#37325;&#24314;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#37325;&#24314;&#22270;&#20687;&#32570;&#20047;&#26126;&#30830;&#35821;&#20041;&#20449;&#24687;&#21644;&#22270;&#20687;&#32467;&#26500;&#19981;&#21487;&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.14139</link><description>&lt;p&gt;
MindDiffuser&#65306;&#22522;&#20110;&#20154;&#33041;&#27963;&#21160;&#30340;&#21463;&#25511;&#22270;&#20687;&#37325;&#24314;&#19982;&#35821;&#20041;&#32467;&#26500;&#25193;&#25955;
&lt;/p&gt;
&lt;p&gt;
MindDiffuser: Controlled Image Reconstruction from Human Brain Activity with Semantic and Structural Diffusion. (arXiv:2303.14139v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14139
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MindDiffuser&#30340;&#20004;&#38454;&#27573;&#22270;&#20687;&#37325;&#24314;&#27169;&#22411;&#65292;&#36890;&#36807;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#65292;&#21516;&#26102;&#35299;&#20915;&#20102;&#37325;&#24314;&#22270;&#20687;&#32570;&#20047;&#26126;&#30830;&#35821;&#20041;&#20449;&#24687;&#21644;&#22270;&#20687;&#32467;&#26500;&#19981;&#21487;&#25511;&#21046;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#27979;&#37327;&#30340;&#21151;&#33021;&#30913;&#20849;&#25391;&#25104;&#20687;&#65288;fMRI&#65289;&#20013;&#37325;&#24314;&#35270;&#35273;&#21050;&#28608;&#26159;&#19968;&#39033;&#26377;&#24847;&#20041;&#19988;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#20197;&#21069;&#30340;&#30740;&#31350;&#24050;&#25104;&#21151;&#23454;&#29616;&#20102;&#37325;&#24314;&#19982;&#21407;&#22987;&#22270;&#20687;&#30456;&#20284;&#30340;&#32467;&#26500;&#65292;&#20363;&#22914;&#19968;&#20123;&#33258;&#28982;&#22270;&#20687;&#30340;&#36718;&#24275;&#21644;&#22823;&#23567;&#12290;&#20294;&#36825;&#20123;&#37325;&#24314;&#32570;&#20047;&#26126;&#30830;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#38590;&#20197;&#35782;&#21035;&#12290;&#36817;&#24180;&#26469;&#65292;&#35768;&#22810;&#30740;&#31350;&#21033;&#29992;&#20855;&#26377;&#26356;&#24378;&#29983;&#25104;&#33021;&#21147;&#30340;&#22810;&#27169;&#24577;&#39044;&#35757;&#32451;&#27169;&#22411;&#37325;&#24314;&#19982;&#21407;&#22987;&#22270;&#20687;&#22312;&#35821;&#20041;&#19978;&#30456;&#20284;&#30340;&#22270;&#20687;&#12290;&#20294;&#26159;&#65292;&#36825;&#20123;&#22270;&#20687;&#20855;&#26377;&#19981;&#21487;&#25511;&#21046;&#30340;&#32467;&#26500;&#20449;&#24687;&#65292;&#20363;&#22914;&#20301;&#32622;&#21644;&#26041;&#21521;&#12290;&#20026;&#20102;&#21516;&#26102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;MindDiffuser&#30340;&#20004;&#38454;&#27573;&#22270;&#20687;&#37325;&#24314;&#27169;&#22411;&#65292;&#21033;&#29992;&#31283;&#23450;&#25193;&#25955;&#25216;&#26415;&#12290;&#22312;&#31532;&#19968;&#38454;&#27573;&#20013;&#65292;&#23558;&#20174;fMRI&#35299;&#30721;&#30340;VQ-VAE&#28508;&#22312;&#34920;&#31034;&#21644;CLIP&#25991;&#26412;&#23884;&#20837;&#25918;&#20837;&#31283;&#23450;&#25193;&#25955;&#30340;&#22270;&#20687;&#23545;&#22270;&#20687;&#22788;&#29702;&#20013;&#65292;&#36825;&#20135;&#29983;&#20102;...
&lt;/p&gt;
&lt;p&gt;
Reconstructing visual stimuli from measured functional magnetic resonance imaging (fMRI) has been a meaningful and challenging task. Previous studies have successfully achieved reconstructions with structures similar to the original images, such as the outlines and size of some natural images. However, these reconstructions lack explicit semantic information and are difficult to discern. In recent years, many studies have utilized multi-modal pre-trained models with stronger generative capabilities to reconstruct images that are semantically similar to the original ones. However, these images have uncontrollable structural information such as position and orientation. To address both of the aforementioned issues simultaneously, we propose a two-stage image reconstruction model called MindDiffuser, utilizing Stable Diffusion. In Stage 1, the VQ-VAE latent representations and the CLIP text embeddings decoded from fMRI are put into the image-to-image process of Stable Diffusion, which yie
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CIFAKE&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#23558;&#30495;&#23454;&#29031;&#29255;&#19982;AI&#29983;&#25104;&#22270;&#20687;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;</title><link>http://arxiv.org/abs/2303.14126</link><description>&lt;p&gt;
CIFAKE: AI&#29983;&#25104;&#22270;&#20687;&#30340;&#20998;&#31867;&#21644;&#21487;&#35299;&#37322;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
CIFAKE: Image Classification and Explainable Identification of AI-Generated Synthetic Images. (arXiv:2303.14126v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14126
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;CIFAKE&#30340;&#26041;&#27861;&#65292;&#21487;&#20197;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#23558;&#30495;&#23454;&#29031;&#29255;&#19982;AI&#29983;&#25104;&#22270;&#20687;&#36827;&#34892;&#20108;&#20803;&#20998;&#31867;&#12290;&#35813;&#26041;&#27861;&#20351;&#29992;&#20102;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23545;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#21512;&#25104;&#25968;&#25454;&#25216;&#26415;&#30340;&#36827;&#27493;&#20351;&#24471;&#29983;&#25104;&#30340;&#22270;&#20687;&#36136;&#37327;&#22914;&#27492;&#20043;&#39640;&#65292;&#20197;&#33267;&#20110;&#20154;&#31867;&#26080;&#27861;&#21306;&#20998;&#30495;&#23454;&#29031;&#29255;&#21644;&#20154;&#24037;&#26234;&#33021;&#29983;&#25104;&#30340;&#22270;&#20687;&#12290;&#37492;&#20110;&#25968;&#25454;&#21487;&#38752;&#24615;&#21644;&#35748;&#35777;&#30340;&#33267;&#20851;&#37325;&#35201;&#24615;&#65292;&#26412;&#25991;&#25552;&#20986;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#22686;&#24378;&#25105;&#20204;&#35782;&#21035;AI&#29983;&#25104;&#22270;&#20687;&#30340;&#33021;&#21147;&#12290;&#39318;&#20808;&#65292;&#36890;&#36807;&#28508;&#22312;&#25193;&#25955;&#29983;&#25104;&#21512;&#25104;&#25968;&#25454;&#38598;&#65292;&#35813;&#25968;&#25454;&#38598;&#19982;&#24050;&#26377;&#30340;CIFAR-10&#25968;&#25454;&#38598;&#20013;&#30340;&#21313;&#20010;&#31867;&#21035;&#30456;&#20284;&#65292;&#25552;&#20379;&#19982;&#30495;&#23454;&#29031;&#29255;&#23545;&#27604;&#30340;&#19981;&#21516;&#31867;&#22411;&#30340;&#22270;&#20687;&#12290;&#35813;&#27169;&#22411;&#33021;&#22815;&#29983;&#25104;&#22797;&#26434;&#30340;&#35270;&#35273;&#23646;&#24615;&#65292;&#20363;&#22914;&#27700;&#20013;&#36924;&#30495;&#30340;&#21453;&#23556;&#12290;&#36825;&#20004;&#32452;&#25968;&#25454;&#20043;&#38388;&#23384;&#22312;&#20108;&#20803;&#20998;&#31867;&#38382;&#39064;&#65292;&#21363;&#29031;&#29255;&#26159;&#30495;&#23454;&#30340;&#36824;&#26159;&#30001;AI&#29983;&#25104;&#30340;&#12290;&#26412;&#30740;&#31350;&#38543;&#21518;&#25552;&#20986;&#20351;&#29992;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#65288;CNN&#65289;&#23558;&#22270;&#20687;&#20998;&#31867;&#20026;&#20004;&#20010;&#31867;&#21035;&#65306;&#30495;&#23454;&#25110;&#20266;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent technological advances in synthetic data have enabled the generation of images with such high quality that human beings cannot tell the difference between real-life photographs and Artificial Intelligence (AI) generated images. Given the critical necessity of data reliability and authentication, this article proposes to enhance our ability to recognise AI-generated images through computer vision. Initially, a synthetic dataset is generated that mirrors the ten classes of the already available CIFAR-10 dataset with latent diffusion which provides a contrasting set of images for comparison to real photographs. The model is capable of generating complex visual attributes, such as photorealistic reflections in water. The two sets of data present as a binary classification problem with regard to whether the photograph is real or generated by AI. This study then proposes the use of a Convolutional Neural Network (CNN) to classify the images into two categories; Real or Fake. Following
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20316;&#32773;&#20851;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20013;&#30340;&#28508;&#21147;&#12290;&#36825;&#26159;&#22522;&#30784;&#19982;&#24212;&#29992;&#30740;&#31350;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#32780;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.14116</link><description>&lt;p&gt;
&#20174;&#22522;&#30784;&#19982;&#24212;&#29992;&#30740;&#31350;&#35270;&#35282;&#25552;&#39640;&#39044;&#27979;&#24615;&#33021;&#21644;&#27169;&#22411;&#21487;&#35299;&#37322;&#24615;&#65306;&#27880;&#24847;&#21147;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Improving Prediction Performance and Model Interpretability through Attention Mechanisms from Basic and Applied Research Perspectives. (arXiv:2303.14116v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14116
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#24635;&#32467;&#20102;&#20316;&#32773;&#20851;&#20110;&#27880;&#24847;&#21147;&#26426;&#21046;&#22312;&#25552;&#39640;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#20013;&#30340;&#28508;&#21147;&#12290;&#36825;&#26159;&#22522;&#30784;&#19982;&#24212;&#29992;&#30740;&#31350;&#30340;&#37325;&#35201;&#38382;&#39064;&#65292;&#23588;&#20854;&#22312;&#21307;&#23398;&#39046;&#22495;&#65292;&#32780;&#27880;&#24847;&#21147;&#26426;&#21046;&#21487;&#20197;&#25104;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#30340;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#26426;&#22120;&#23398;&#20064;&#30740;&#31350;&#20851;&#27880;&#20110;&#25552;&#39640;&#27169;&#22411;&#39044;&#27979;&#30340;&#21487;&#35299;&#37322;&#24615;&#21644;&#24615;&#33021;&#65292;&#28085;&#30422;&#22522;&#30784;&#19982;&#24212;&#29992;&#30740;&#31350;&#30340;&#21508;&#20010;&#39046;&#22495;&#12290;&#34429;&#28982;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#27604;&#20256;&#32479;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#20855;&#26377;&#26356;&#39640;&#30340;&#39044;&#27979;&#24615;&#33021;&#65292;&#20294;&#20855;&#20307;&#39044;&#27979;&#36807;&#31243;&#20173;&#38590;&#20197;&#35299;&#37322;&#21644;&#35828;&#26126;&#65292;&#36825;&#34987;&#31216;&#20026;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#40657;&#30418;&#21270;&#65292;&#24182;&#34987;&#24191;&#27867;&#35748;&#20026;&#26159;&#35768;&#22810;&#30740;&#31350;&#39046;&#22495;&#30340;&#19968;&#20010;&#29305;&#21035;&#37325;&#35201;&#30340;&#38382;&#39064;&#65292;&#21253;&#25324;&#21046;&#36896;&#19994;&#12289;&#21830;&#19994;&#12289;&#26426;&#22120;&#20154;&#21644;&#20854;&#20182;&#34892;&#19994;&#31561;&#26222;&#36941;&#20351;&#29992;&#35813;&#25216;&#26415;&#65292;&#20197;&#21450;&#21307;&#23398;&#39046;&#22495;&#65292;&#22312;&#36825;&#20123;&#39046;&#22495;&#20013;&#38169;&#35823;&#26159;&#19981;&#21487;&#23481;&#24525;&#30340;&#12290;&#26412;&#25991;&#22522;&#20110;&#20316;&#32773;&#35770;&#25991;&#30340;&#25688;&#35201;&#65292;&#35813;&#35770;&#25991;&#30340;&#26680;&#24515;&#30740;&#31350;&#20851;&#27880;&#20110;&#36817;&#24180;&#26469;&#22791;&#21463;&#20851;&#27880;&#30340;&#27880;&#24847;&#21147;&#26426;&#21046;&#65292;&#24182;&#25506;&#35752;&#20102;&#20854;&#22312;&#22522;&#30784;&#30740;&#31350;&#21644;&#24212;&#29992;&#30740;&#31350;&#20013;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the dramatic advances in deep learning technology, machine learning research is focusing on improving the interpretability of model predictions as well as prediction performance in both basic and applied research. While deep learning models have much higher prediction performance than traditional machine learning models, the specific prediction process is still difficult to interpret and/or explain. This is known as the black-boxing of machine learning models and is recognized as a particularly important problem in a wide range of research fields, including manufacturing, commerce, robotics, and other industries where the use of such technology has become commonplace, as well as the medical field, where mistakes are not tolerated. This bulletin is based on the summary of the author's dissertation. The research summarized in the dissertation focuses on the attention mechanism, which has been the focus of much attention in recent years, and discusses its potential for both basic res
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#23398;&#20064;&#26377;&#38480;&#33258;&#21160;&#26426;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#21644;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14111</link><description>&lt;p&gt;
&#36890;&#36807;&#31163;&#25955;&#20248;&#21270;&#23454;&#29616;&#21487;&#35299;&#37322;&#24615;&#24322;&#24120;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Interpretable Anomaly Detection via Discrete Optimization. (arXiv:2303.14111v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14111
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#36890;&#36807;&#23398;&#20064;&#26377;&#38480;&#33258;&#21160;&#26426;&#36827;&#34892;&#24322;&#24120;&#26816;&#27979;&#30340;&#26694;&#26550;&#65292;&#24182;&#36890;&#36807;&#32422;&#26463;&#20248;&#21270;&#31639;&#27861;&#21644;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#25552;&#39640;&#20102;&#21487;&#35299;&#37322;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24322;&#24120;&#26816;&#27979;&#22312;&#35768;&#22810;&#24212;&#29992;&#39046;&#22495;&#20013;&#37117;&#26159;&#24517;&#19981;&#21487;&#23569;&#30340;&#65292;&#20363;&#22914;&#32593;&#32476;&#23433;&#20840;&#12289;&#25191;&#27861;&#12289;&#21307;&#23398;&#21644;&#27450;&#35784;&#20445;&#25252;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#30340;&#20915;&#31574;&#36807;&#31243;&#24448;&#24448;&#38590;&#20197;&#29702;&#35299;&#65292;&#36825;&#36890;&#24120;&#38480;&#21046;&#20102;&#23427;&#20204;&#30340;&#23454;&#38469;&#24212;&#29992;&#24615;&#12290;&#20026;&#20102;&#20811;&#26381;&#36825;&#20010;&#38480;&#21046;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#23398;&#20064;&#26694;&#26550;&#65292;&#21487;&#20197;&#20174;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#21487;&#35299;&#37322;&#24615;&#30340;&#24322;&#24120;&#26816;&#27979;&#22120;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#32771;&#34385;&#20174;&#32473;&#23450;&#30340;&#26410;&#26631;&#35760;&#24207;&#21015;&#22810;&#37325;&#38598;&#20013;&#23398;&#20064;&#30830;&#23450;&#24615;&#26377;&#38480;&#33258;&#21160;&#26426; &#65288;DFA&#65289;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#35777;&#26126;&#20102;&#36825;&#20010;&#38382;&#39064;&#26159;&#35745;&#31639;&#38590;&#39064;&#65292;&#24182;&#22522;&#20110;&#32422;&#26463;&#20248;&#21270;&#24320;&#21457;&#20102;&#20004;&#20010;&#23398;&#20064;&#31639;&#27861;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#20026;&#20248;&#21270;&#38382;&#39064;&#24341;&#20837;&#20102;&#26032;&#30340;&#27491;&#21017;&#21270;&#26041;&#26696;&#65292;&#20197;&#25552;&#39640;&#25105;&#20204;&#30340;DFA&#30340;&#25972;&#20307;&#21487;&#35299;&#37322;&#24615;&#12290;&#36890;&#36807;&#21407;&#22411;&#23454;&#29616;&#65292;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20934;&#30830;&#24615;&#21644;F1&#20998;&#25968;&#26041;&#38754;&#34920;&#29616;&#20986;&#26377;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Anomaly detection is essential in many application domains, such as cyber security, law enforcement, medicine, and fraud protection. However, the decision-making of current deep learning approaches is notoriously hard to understand, which often limits their practical applicability. To overcome this limitation, we propose a framework for learning inherently interpretable anomaly detectors from sequential data. More specifically, we consider the task of learning a deterministic finite automaton (DFA) from a given multi-set of unlabeled sequences. We show that this problem is computationally hard and develop two learning algorithms based on constraint optimization. Moreover, we introduce novel regularization schemes for our optimization problems that improve the overall interpretability of our DFAs. Using a prototype implementation, we demonstrate that our approach shows promising results in terms of accuracy and F1 score.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#24555;&#36895;&#35745;&#31639; Silhouette &#25351;&#26631;&#30340;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#25968;&#25454;&#22330;&#26223;&#19979;&#32858;&#31867;&#35780;&#20272;&#30340;&#38590;&#28857;&#12290;</title><link>http://arxiv.org/abs/2303.14102</link><description>&lt;p&gt;
&#20998;&#24067;&#24335;Silhouette&#31639;&#27861;&#65306;&#35780;&#20272;&#22823;&#25968;&#25454;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Distributed Silhouette Algorithm: Evaluating Clustering on Big Data. (arXiv:2303.14102v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#24555;&#36895;&#35745;&#31639; Silhouette &#25351;&#26631;&#30340;&#31639;&#27861;&#65292;&#35299;&#20915;&#20102;&#22823;&#25968;&#25454;&#22330;&#26223;&#19979;&#32858;&#31867;&#35780;&#20272;&#30340;&#38590;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#25968;&#25454;&#26102;&#20195;&#65292;&#27599;&#20010;&#31639;&#27861;&#38656;&#35201;&#20855;&#22791;&#30340;&#20851;&#38190;&#29305;&#24449;&#26159;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#19979;&#39640;&#25928;&#36816;&#34892;&#30340;&#21487;&#33021;&#24615;&#12290;&#28982;&#32780;&#65292;&#27969;&#34892;&#30340;Silhouette&#24230;&#37327;&#35780;&#20272;&#32858;&#31867;&#36136;&#37327;&#30340;&#31639;&#27861;&#27809;&#26377;&#36825;&#20010;&#24615;&#36136;&#65292;&#24182;&#19988;&#19982;&#36755;&#20837;&#25968;&#25454;&#38598;&#30340;&#22823;&#23567;&#21576;&#20108;&#27425;&#35745;&#31639;&#22797;&#26434;&#24230;&#12290;&#22240;&#27492;&#65292;&#22312;&#22823;&#25968;&#25454;&#22330;&#26223;&#20013;&#65292;&#32858;&#31867;&#35780;&#20272;&#24517;&#39035;&#37319;&#29992;&#20854;&#20182;&#26041;&#27861;&#12290;&#20026;&#20102;&#22635;&#34917;&#36825;&#20010;&#31354;&#30333;&#65292;&#26412;&#25991;&#20171;&#32461;&#20102;&#31532;&#19968;&#20010;&#21487;&#20197;&#20197;&#32447;&#24615;&#22797;&#26434;&#24230;&#35745;&#31639;Silhouette&#25351;&#26631;&#24182;&#21487;&#20197;&#22312;&#20998;&#24067;&#24335;&#29615;&#22659;&#20013;&#36731;&#26494;&#25191;&#34892;&#30340;&#31639;&#27861;&#12290;&#23427;&#30340;&#23454;&#29616;&#22312;Apache Spark ML&#24211;&#20013;&#20813;&#36153;&#25552;&#20379;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the big data era, the key feature that each algorithm needs to have is the possibility of efficiently running in parallel in a distributed environment. The popular Silhouette metric to evaluate the quality of a clustering, unfortunately, does not have this property and has a quadratic computational complexity with respect to the size of the input dataset. For this reason, its execution has been hindered in big data scenarios, where clustering had to be evaluated otherwise. To fill this gap, in this paper we introduce the first algorithm that computes the Silhouette metric with linear complexity and can easily execute in parallel in a distributed environment. Its implementation is freely available in the Apache Spark ML library.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23398;&#29983;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#26102;&#65292;&#23436;&#32654;&#27867;&#21270;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#36755;&#20837;&#32500;&#24230;&#27604;&#65292;&#23398;&#29983;&#37117;&#26080;&#27861;&#23436;&#32654;&#27867;&#21270;&#12290;</title><link>http://arxiv.org/abs/2303.14083</link><description>&lt;p&gt;
&#23398;&#29983;-&#25945;&#24072;&#26694;&#26550;&#19979;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Online Learning for the Random Feature Model in the Student-Teacher Framework. (arXiv:2303.14083v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14083
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#32771;&#34385;&#20102;&#19968;&#31181;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#30340;&#22312;&#32447;&#23398;&#20064;&#65292;&#30740;&#31350;&#21457;&#29616;&#65292;&#24403;&#23398;&#29983;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#26102;&#65292;&#23436;&#32654;&#27867;&#21270;&#26159;&#21487;&#34892;&#30340;&#65292;&#20294;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#36755;&#20837;&#32500;&#24230;&#27604;&#65292;&#23398;&#29983;&#37117;&#26080;&#27861;&#23436;&#32654;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26159;&#19968;&#31181;&#24191;&#27867;&#24212;&#29992;&#30340;&#39044;&#27979;&#31639;&#27861;&#65292;&#38543;&#30528;&#26435;&#37325;&#25968;&#37327;&#30340;&#22686;&#21152;&#65292;&#20854;&#24615;&#33021;&#36890;&#24120;&#20250;&#25552;&#39640;&#65292;&#23548;&#33268;&#36807;&#24230;&#21442;&#25968;&#21270;&#12290;&#25105;&#20204;&#32771;&#34385;&#19968;&#31181;&#20004;&#23618;&#31070;&#32463;&#32593;&#32476;&#65292;&#20854;&#31532;&#19968;&#23618;&#26159;&#20923;&#32467;&#30340;&#65292;&#32780;&#26368;&#21518;&#19968;&#23618;&#26159;&#21487;&#35757;&#32451;&#30340;&#65292;&#31216;&#20026;&#38543;&#26426;&#29305;&#24449;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#23398;&#29983;-&#25945;&#24072;&#26694;&#26550;&#19979;&#30740;&#31350;&#20102;&#36807;&#24230;&#21442;&#25968;&#21270;&#65292;&#36890;&#36807;&#23548;&#20986;&#19968;&#32452;&#23398;&#20064;&#21160;&#24577;&#30340;&#24494;&#20998;&#26041;&#31243;&#12290;&#23545;&#20110;&#20219;&#20309;&#26377;&#38480;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21644;&#36755;&#20837;&#32500;&#24230;&#27604;&#65292;&#23398;&#29983;&#37117;&#26080;&#27861;&#23436;&#32654;&#27867;&#21270;&#65292;&#24182;&#35745;&#31639;&#38750;&#38646;&#28176;&#36817;&#27867;&#21270;&#35823;&#24046;&#12290;&#21482;&#26377;&#24403;&#23398;&#29983;&#30340;&#38544;&#34255;&#23618;&#22823;&#23567;&#21576;&#25351;&#25968;&#22686;&#38271;&#26102;&#65292;&#25165;&#26377;&#21487;&#33021;&#23454;&#29616;&#23436;&#32654;&#27867;&#21270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks are widely used prediction algorithms whose performance often improves as the number of weights increases, leading to over-parametrization. We consider a two-layered neural network whose first layer is frozen while the last layer is trainable, known as the random feature model. We study over-parametrization in the context of a student-teacher framework by deriving a set of differential equations for the learning dynamics. For any finite ratio of hidden layer size and input dimension, the student cannot generalize perfectly, and we compute the non-zero asymptotic generalization error. Only when the student's hidden layer size is exponentially larger than the input dimension, an approach to perfect generalization is possible.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;(Instance-adaptive Adversarial Training, IAAT)&#36890;&#36807;&#24179;&#28369;&#23454;&#20363;&#32423;&#21035;&#30340;&#23545;&#25239;&#24615;&#25439;&#22833;&#65292;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#8220;&#38590;&#8221;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#36991;&#20813;&#29306;&#29298;&#29305;&#23450;&#30340;&#26679;&#26412;&#32780;&#20559;&#29233;&#20854;&#20182;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19979;&#30340;&#26368;&#26032;&#12289;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#19979;&#22343;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.14077</link><description>&lt;p&gt;
&#33258;&#36866;&#24212;&#23454;&#20363;&#32423;&#25439;&#22833;&#24179;&#28369;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;
&lt;/p&gt;
&lt;p&gt;
Improved Adversarial Training Through Adaptive Instance-wise Loss Smoothing. (arXiv:2303.14077v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14077
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;(Instance-adaptive Adversarial Training, IAAT)&#36890;&#36807;&#24179;&#28369;&#23454;&#20363;&#32423;&#21035;&#30340;&#23545;&#25239;&#24615;&#25439;&#22833;&#65292;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#8220;&#38590;&#8221;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#36991;&#20813;&#29306;&#29298;&#29305;&#23450;&#30340;&#26679;&#26412;&#32780;&#20559;&#29233;&#20854;&#20182;&#26679;&#26412;&#65292;&#21462;&#24471;&#20102;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19979;&#30340;&#26368;&#26032;&#12289;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#19979;&#22343;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#23545;&#36755;&#20837;&#36827;&#34892;&#23545;&#25239;&#25200;&#21160;&#65306;&#21363;&#20154;&#31867;&#38590;&#20197;&#23519;&#35273;&#30340;&#20154;&#36896;&#22122;&#22768;&#65292;&#21487;&#20197;&#36731;&#26131;&#22320;&#36855;&#24785;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20174;&#32780;&#20570;&#20986;&#19981;&#27491;&#30830;&#30340;&#39044;&#27979;&#12290;&#30446;&#21069;&#23545;&#25239;&#35757;&#32451;&#24050;&#25104;&#20026;&#26368;&#25104;&#21151;&#30340;&#23545;&#25239;&#25915;&#20987;&#38450;&#24481;&#26041;&#27861;&#65292;&#26412;&#25991;&#33268;&#21147;&#20110;&#25913;&#36827;&#23545;&#25239;&#35757;&#32451;&#20197;&#25552;&#21319;&#23545;&#25239;&#40065;&#26834;&#24615;&#12290;&#39318;&#20808;&#20174;&#23454;&#20363;&#32423;&#21035;&#30340;&#35282;&#24230;&#20998;&#26512;&#20102;&#23545;&#25239;&#35757;&#32451;&#26399;&#38388;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#30340;&#28436;&#21464;&#12290;&#21457;&#29616;&#22312;&#35757;&#32451;&#26399;&#38388;&#65292;&#36890;&#36807;&#29306;&#29298;&#30456;&#24403;&#27604;&#20363;&#30340;&#35757;&#32451;&#26679;&#26412;&#26469;&#25552;&#39640;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#65292;&#20174;&#32780;&#23454;&#29616;&#23545;&#25239;&#24615;&#25439;&#22833;&#30340;&#25972;&#20307;&#38477;&#20302;&#65292;&#36825;&#23548;&#33268;&#20102;&#19981;&#21516;&#25968;&#25454;&#30340;&#23545;&#25239;&#24615;&#33030;&#24369;&#24615;&#20998;&#24067;&#19981;&#22343;&#34913;&#12290;&#36825;&#31181;&#8220;&#19981;&#22343;&#34913;&#33030;&#24369;&#24615;&#8221;&#22312;&#20960;&#31181;&#27969;&#34892;&#30340;&#40065;&#26834;&#24615;&#35757;&#32451;&#26041;&#27861;&#20013;&#26222;&#36941;&#23384;&#22312;&#65292;&#24182;&#19988;&#19982;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#36807;&#25311;&#21512;&#30456;&#20851;&#12290;&#22522;&#20110;&#27492;&#35266;&#23519;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#23545;&#25239;&#35757;&#32451;&#26041;&#27861;&#65306;Instance-adaptive Adversarial Training (IAAT)&#12290;&#35813;&#26041;&#27861;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#24179;&#28369;&#23454;&#20363;&#32423;&#21035;&#30340;&#23545;&#25239;&#24615;&#25439;&#22833;&#65292;&#40723;&#21169;&#27169;&#22411;&#20851;&#27880;&#8220;&#38590;&#8221;&#30340;&#26679;&#26412;&#65292;&#21516;&#26102;&#36991;&#20813;&#29306;&#29298;&#29305;&#23450;&#30340;&#26679;&#26412;&#32780;&#20559;&#29233;&#20854;&#20182;&#26679;&#26412;&#12290;&#26412;&#26041;&#27861;&#22312;&#21508;&#31181;&#25968;&#25454;&#38598;&#19979;&#37117;&#21462;&#24471;&#20102;&#26368;&#26032;&#30340;&#26368;&#20339;&#32467;&#26524;&#65292;&#24182;&#22312;&#30333;&#30418;&#21644;&#40657;&#30418;&#25915;&#20987;&#19979;&#22343;&#20248;&#20110;&#20197;&#21069;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks can be easily fooled into making incorrect predictions through corruption of the input by adversarial perturbations: human-imperceptible artificial noise. So far adversarial training has been the most successful defense against such adversarial attacks. This work focuses on improving adversarial training to boost adversarial robustness. We first analyze, from an instance-wise perspective, how adversarial vulnerability evolves during adversarial training. We find that during training an overall reduction of adversarial loss is achieved by sacrificing a considerable proportion of training samples to be more vulnerable to adversarial attack, which results in an uneven distribution of adversarial vulnerability among data. Such "uneven vulnerability", is prevalent across several popular robust training methods and, more importantly, relates to overfitting in adversarial training. Motivated by this observation, we propose a new adversarial training method: Instance-adapt
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;&#26694;&#26550;&#30340;&#27010;&#24565;&#65292;&#24182;&#32473;&#20986;&#20102;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#34892;&#20026;&#30340;&#25193;&#23637;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#26694;&#26550;&#25191;&#34892;&#21644;&#26412;&#22320;&#21270;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.14067</link><description>&lt;p&gt;
SEAL: &#29992;&#20110;&#29702;&#35299;&#26426;&#22120;&#20154;&#34892;&#20026;&#30340;&#35821;&#20041;&#26694;&#26550;&#25191;&#34892;&#21644;&#26412;&#22320;&#21270;
&lt;/p&gt;
&lt;p&gt;
SEAL: Semantic Frame Execution And Localization for Perceiving Afforded Robot Actions. (arXiv:2303.14067v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14067
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#35821;&#20041;&#26694;&#26550;&#30340;&#27010;&#24565;&#65292;&#24182;&#32473;&#20986;&#20102;&#29992;&#20110;&#26426;&#22120;&#20154;&#25805;&#20316;&#34892;&#20026;&#30340;&#25193;&#23637;&#34920;&#31034;&#65292;&#35299;&#20915;&#20102;&#35821;&#20041;&#26694;&#26550;&#25191;&#34892;&#21644;&#26412;&#22320;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26426;&#22120;&#20154;&#31227;&#21160;&#25805;&#20316;&#30340;&#26368;&#26032;&#36827;&#23637;&#24050;&#32463;&#20419;&#36827;&#20102;&#26426;&#22120;&#20154;&#30340;&#25805;&#20316;&#29615;&#22659;&#20174;&#21463;&#38480;&#30340;&#24037;&#20316;&#31354;&#38388;&#25193;&#23637;&#21040;&#22823;&#35268;&#27169;&#30340;&#20154;&#31867;&#29615;&#22659;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#23436;&#25104;&#36825;&#20123;&#31354;&#38388;&#20013;&#30340;&#20219;&#21153;&#65292;&#26426;&#22120;&#20154;&#24517;&#39035;&#33021;&#22815;&#24863;&#30693;&#12289;&#25512;&#29702;&#21644;&#25191;&#34892;&#21508;&#31181;&#21487;&#34892;&#24615;&#65292;&#36828;&#36828;&#36229;&#20986;&#20102;&#31616;&#21333;&#30340;&#25342;&#21462;&#21644;&#25918;&#32622;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#35821;&#20041;&#26694;&#26550;&#30340;&#27010;&#24565;&#65292;&#20026;&#26426;&#22120;&#20154;&#34892;&#20026;&#25552;&#20379;&#20102;&#19968;&#31181;&#26377;&#21033;&#20110;&#34892;&#21160;&#24863;&#30693;&#12289;&#20219;&#21153;&#32423;&#21035;&#25512;&#29702;&#12289;&#34892;&#21160;&#32423;&#21035;&#25191;&#34892;&#21644;&#35821;&#35328;&#38598;&#25104;&#30340;&#34920;&#24449;&#12290;&#26412;&#25991;&#25193;&#23637;&#20102;&#26426;&#22120;&#20154;&#25805;&#20316;&#34892;&#20026;&#30340;&#35821;&#20041;&#26694;&#26550;&#34920;&#31034;&#65292;&#24182;&#24341;&#20837;&#20102;&#35821;&#20041;&#26694;&#26550;&#25191;&#34892;&#21644;&#26412;&#22320;&#21270;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent advances in robotic mobile manipulation have spurred the expansion of the operating environment for robots from constrained workspaces to large-scale, human environments. In order to effectively complete tasks in these spaces, robots must be able to perceive, reason, and execute over a diversity of affordances, well beyond simple pick-and-place. We posit the notion of semantic frames provides a compelling representation for robot actions that is amenable to action-focused perception, task-level reasoning, action-level execution, and integration with language. Semantic frames, a product of the linguistics community, define the necessary elements, pre- and post- conditions, and a set of sequential robot actions necessary to successfully execute an action evoked by a verb phrase. In this work, we extend the semantic frame representation for robot manipulation actions and introduce the problem of Semantic Frame Execution And Localization for Perceiving Afforded Robot Actions (SEAL) 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21512;&#20316;&#20219;&#21153;&#20998;&#35299;&#19982;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#22870;&#21169;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20063;&#38477;&#20302;&#20102;&#21512;&#20316;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.14061</link><description>&lt;p&gt;
&#21512;&#20316;&#22810;&#26234;&#33021;&#20307;&#20219;&#21153;&#20013;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;
&lt;/p&gt;
&lt;p&gt;
Learning Reward Machines in Cooperative Multi-Agent Tasks. (arXiv:2303.14061v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21512;&#20316;&#20219;&#21153;&#20998;&#35299;&#19982;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;&#30456;&#32467;&#21512;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#35299;&#20915;&#37096;&#20998;&#21487;&#35266;&#23519;&#29615;&#22659;&#20013;&#22870;&#21169;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65292;&#24182;&#25552;&#39640;&#20102;&#23398;&#20064;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#21516;&#26102;&#20063;&#38477;&#20302;&#20102;&#21512;&#20316;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#65292;&#23558;&#21512;&#20316;&#20219;&#21153;&#20998;&#35299;&#19982;&#23398;&#20064;&#22870;&#21169;&#26426;&#21046;&#30456;&#32467;&#21512;&#65292;&#20197;&#32534;&#30721;&#23376;&#20219;&#21153;&#30340;&#32467;&#26500;&#12290;&#35813;&#26041;&#27861;&#26377;&#21161;&#20110;&#24212;&#23545;&#37096;&#20998;&#35266;&#27979;&#29615;&#22659;&#20013;&#22870;&#21169;&#30340;&#38750;&#39532;&#23572;&#21487;&#22827;&#24615;&#36136;&#65292;&#24182;&#25552;&#39640;&#25152;&#23398;&#20064;&#30340;&#31574;&#30053;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#20197;&#23436;&#25104;&#21512;&#20316;&#20219;&#21153;&#12290;&#19982;&#27599;&#20010;&#23376;&#20219;&#21153;&#30456;&#20851;&#32852;&#30340;&#22870;&#21169;&#26426;&#21046;&#26159;&#20197;&#20998;&#25955;&#30340;&#26041;&#24335;&#23398;&#20064;&#30340;&#65292;&#28982;&#21518;&#29992;&#20110;&#25351;&#23548;&#27599;&#20010;&#26234;&#33021;&#20307;&#30340;&#34892;&#20026;&#12290;&#36890;&#36807;&#36825;&#26679;&#20570;&#65292;&#21512;&#20316;&#20219;&#21153;&#30340;&#22797;&#26434;&#24615;&#24471;&#21040;&#20102;&#38477;&#20302;&#65292;&#20174;&#32780;&#26356;&#26377;&#25928;&#22320;&#23398;&#20064;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26159;&#26410;&#26469;&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#30740;&#31350;&#20013;&#30340;&#19968;&#20010;&#26377; promising &#30340;&#26041;&#21521;&#65292;&#29305;&#21035;&#26159;&#22312;&#20855;&#26377;&#22823;&#29366;&#24577;&#31354;&#38388;&#21644;&#22810;&#20010;&#26234;&#33021;&#20307;&#30340;&#22797;&#26434;&#29615;&#22659;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents a novel approach to Multi-Agent Reinforcement Learning (MARL) that combines cooperative task decomposition with the learning of reward machines (RMs) encoding the structure of the sub-tasks. The proposed method helps deal with the non-Markovian nature of the rewards in partially observable environments and improves the interpretability of the learnt policies required to complete the cooperative task. The RMs associated with each sub-task are learnt in a decentralised manner and then used to guide the behaviour of each agent. By doing so, the complexity of a cooperative multi-agent problem is reduced, allowing for more effective learning. The results suggest that our approach is a promising direction for future research in MARL, especially in complex environments with large state spaces and multiple agents.
&lt;/p&gt;</description></item><item><title>&#31038;&#20132;&#36741;&#21161;&#26426;&#22120;&#20154;&#65288;SAR&#65289;&#22312;&#27835;&#30103;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#24212;&#35813;&#32771;&#34385;&#23558;&#36879;&#26126;&#27807;&#36890;&#32452;&#20214;&#32435;&#20837;&#35774;&#35745;&#20013;&#21435;&#20419;&#36827;&#20154;-&#26426;&#20132;&#20114;&#12290;</title><link>http://arxiv.org/abs/2303.14054</link><description>&lt;p&gt;
&#26426;&#22120;&#20154;&#36741;&#21161;&#27835;&#30103;&#20013;&#30340;&#22797;&#26434;&#20915;&#31574;&#20256;&#36798;
&lt;/p&gt;
&lt;p&gt;
Communicating Complex Decisions in Robot-Assisted Therapy. (arXiv:2303.14054v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14054
&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#36741;&#21161;&#26426;&#22120;&#20154;&#65288;SAR&#65289;&#22312;&#27835;&#30103;&#20013;&#21457;&#25381;&#37325;&#35201;&#20316;&#29992;&#65292;&#24212;&#35813;&#32771;&#34385;&#23558;&#36879;&#26126;&#27807;&#36890;&#32452;&#20214;&#32435;&#20837;&#35774;&#35745;&#20013;&#21435;&#20419;&#36827;&#20154;-&#26426;&#20132;&#20114;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#27835;&#30103;&#22330;&#26223;&#20013;&#65292;&#31038;&#20132;&#36741;&#21161;&#26426;&#22120;&#20154;&#65288;SAR&#65289;&#20316;&#20026;&#20915;&#31574;&#25351;&#23548;&#25110;&#28608;&#21169;&#20276;&#20387;&#24050;&#32463;&#26174;&#31034;&#20986;&#26497;&#22823;&#28508;&#21147;&#12290;&#22312;&#20154;&#19982;&#20154;&#20043;&#38388;&#30340;&#27835;&#30103;&#20013;&#65292;&#19987;&#23478;&#36890;&#24120;&#20250;&#27807;&#36890;&#20182;&#20204;&#20316;&#20986;&#20915;&#31574;&#30340;&#24605;&#32771;&#36807;&#31243;&#20197;&#20419;&#36827;&#36879;&#26126;&#24230;&#21644;&#24314;&#31435;&#20449;&#20219;&#12290;&#38543;&#30528;&#30740;&#31350;&#26088;&#22312;&#23558;&#26356;&#22797;&#26434;&#30340;&#20915;&#31574;&#27169;&#22411;&#32435;&#20837;&#36825;&#20123;&#26426;&#22120;&#20154;&#20197;&#25512;&#21160;&#26356;&#22909;&#30340;&#20132;&#20114;&#65292;SAR&#35299;&#37322;&#20854;&#20915;&#31574;&#30340;&#33021;&#21147;&#21464;&#24471;&#36234;&#26469;&#36234;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#26032;&#30340;&#22797;&#26434;SAR&#20915;&#31574;&#21046;&#23450;&#32773;&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#22522;&#20110;&#20154;&#19982;&#20154;&#20043;&#38388;&#27835;&#30103;&#20013;&#36879;&#26126;&#27807;&#36890;&#30340;&#37325;&#35201;&#24615;&#65292;SAR&#24212;&#35813;&#23558;&#36825;&#20123;&#32452;&#20214;&#32435;&#20837;&#20854;&#35774;&#35745;&#20043;&#20013;&#12290;&#20026;&#20102;&#21050;&#28608;&#23545;&#36825;&#20010;&#35805;&#39064;&#30340;&#35752;&#35770;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#35774;&#35745;&#32771;&#34385;&#22240;&#32032;&#20379;&#30740;&#31350;&#20154;&#21592;&#21442;&#32771;&#12290;
&lt;/p&gt;
&lt;p&gt;
Socially Assistive Robots (SARs) have shown promising potential in therapeutic scenarios as decision-making instructors or motivational companions. In human-human therapy, experts often communicate the thought process behind the decisions they make to promote transparency and build trust. As research aims to incorporate more complex decision-making models into these robots to drive better interaction, the ability for the SAR to explain its decisions becomes an increasing challenge. We present the latest examples of complex SAR decision-makers. We argue that, based on the importance of transparent communication in human-human therapy, SARs should incorporate such components into their design. To stimulate discussion around this topic, we present a set of design considerations for researchers.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#25277;&#35937;&#25991;&#25688;&#30340;&#24635;&#32467;&#20559;&#22909;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#21482;&#26377;&#23569;&#37327;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#29983;&#25104;&#22120;&#12290;&#20854;&#20013;&#65292;&#20197;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#23569;&#37327;&#26679;&#26412;&#30340;&#23398;&#20064;&#36807;&#31243;&#20174;&#28304;&#35821;&#26009;&#24211;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#26009;&#24211;&#12290;</title><link>http://arxiv.org/abs/2303.14011</link><description>&lt;p&gt;
SPEC: &#20302;&#36164;&#28304;&#25277;&#35937;&#25991;&#25688;&#30340;&#24635;&#32467;&#20559;&#22909;&#20998;&#35299;
&lt;/p&gt;
&lt;p&gt;
SPEC: Summary Preference Decomposition for Low-Resource Abstractive Summarization. (arXiv:2303.14011v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14011
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20302;&#36164;&#28304;&#25277;&#35937;&#25991;&#25688;&#30340;&#24635;&#32467;&#20559;&#22909;&#20998;&#35299;&#30340;&#26041;&#27861;&#65292;&#20197;&#22312;&#21482;&#26377;&#23569;&#37327;&#31034;&#20363;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#29983;&#25104;&#22120;&#12290;&#20854;&#20013;&#65292;&#20197;&#39044;&#35757;&#32451;&#30340;&#35821;&#35328;&#27169;&#22411;&#20026;&#22522;&#30784;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20803;&#23398;&#20064;&#26694;&#26550;&#65292;&#23558;&#23569;&#37327;&#26679;&#26412;&#30340;&#23398;&#20064;&#36807;&#31243;&#20174;&#28304;&#35821;&#26009;&#24211;&#36716;&#31227;&#21040;&#30446;&#26631;&#35821;&#26009;&#24211;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#25277;&#35937;&#25688;&#35201;&#24050;&#32463;&#34987;&#24191;&#27867;&#30740;&#31350;&#65292;&#24182;&#22312;&#22823;&#35268;&#27169;&#35821;&#26009;&#24211;&#20013;&#21462;&#24471;&#20102;&#24040;&#22823;&#25104;&#21151;&#12290; &#28982;&#32780;&#65292;&#27880;&#37322;&#25968;&#25454;&#30340;&#30456;&#24403;&#39640;&#30340;&#25104;&#26412;&#20419;&#20351;&#25105;&#20204;&#38656;&#35201;&#22312;&#20302;&#36164;&#28304;&#29615;&#22659;&#19979;&#23398;&#20064;&#31574;&#30053;&#12290; &#26412;&#25991;&#30740;&#31350;&#20102;&#21482;&#26377;&#23569;&#37327;&#31034;&#20363;&#24773;&#20917;&#19979;&#23398;&#20064;&#25688;&#35201;&#29983;&#25104;&#22120;&#30340;&#38382;&#39064;&#65292;&#24182;&#25552;&#20986;&#20102;&#30456;&#24212;&#30340;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural abstractive summarization has been widely studied and achieved great success with large-scale corpora. However, the considerable cost of annotating data motivates the need for learning strategies under low-resource settings. In this paper, we investigate the problems of learning summarizers with only few examples and propose corresponding methods for improvements. First, typical transfer learning methods are prone to be affected by data properties and learning objectives in the pretext tasks. Therefore, based on pretrained language models, we further present a meta learning framework to transfer few-shot learning processes from source corpora to the target corpus. Second, previous methods learn from training examples without decomposing the content and preference. The generated summaries could therefore be constrained by the preference bias in the training set, especially under low-resource settings. As such, we propose decomposing the contents and preferences during learning th
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#22242;&#38431;&#22312; AI &#31995;&#32479;&#20013;&#30340;&#30417;&#31649;&#27969;&#31243;&#30340;&#32437;&#21521;&#35266;&#23519;&#65292;&#25506;&#35752;&#20102; AI &#31995;&#32479;&#23545;&#20020;&#24202;&#20915;&#31574;&#21046;&#23450;&#20013;&#22242;&#38431;&#30417;&#31649;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#21457;&#29616;&#27492;&#21069;&#30340;&#19987;&#19994;&#22242;&#38431;&#30417;&#31649;&#26041;&#27861;&#20027;&#35201;&#20381;&#38752;&#35299;&#37322;&#21644;&#38382;&#35810;&#26469;&#33719;&#21462;&#20449;&#24687;&#65292;&#32780; AI &#30340;&#24341;&#20837;&#23558;&#21487;&#33021;&#22312;&#20449;&#24687;&#25259;&#38706;&#21644;&#20915;&#31574;&#21046;&#23450;&#26041;&#38754;&#36896;&#25104;&#19968;&#23450;&#31243;&#24230;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.14007</link><description>&lt;p&gt;
&#39640;&#39118;&#38505; AI &#30340;&#22242;&#38431;&#30417;&#31649;&#65306;&#22242;&#38431;&#22312;&#24490;&#29615;&#20013;
&lt;/p&gt;
&lt;p&gt;
'Team-in-the-loop' organisational oversight of high-stakes AI. (arXiv:2303.14007v1 [cs.CY])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.14007
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#36890;&#36807;&#23545;&#22242;&#38431;&#22312; AI &#31995;&#32479;&#20013;&#30340;&#30417;&#31649;&#27969;&#31243;&#30340;&#32437;&#21521;&#35266;&#23519;&#65292;&#25506;&#35752;&#20102; AI &#31995;&#32479;&#23545;&#20020;&#24202;&#20915;&#31574;&#21046;&#23450;&#20013;&#22242;&#38431;&#30417;&#31649;&#30340;&#24433;&#21709;&#65292;&#30740;&#31350;&#21457;&#29616;&#27492;&#21069;&#30340;&#19987;&#19994;&#22242;&#38431;&#30417;&#31649;&#26041;&#27861;&#20027;&#35201;&#20381;&#38752;&#35299;&#37322;&#21644;&#38382;&#35810;&#26469;&#33719;&#21462;&#20449;&#24687;&#65292;&#32780; AI &#30340;&#24341;&#20837;&#23558;&#21487;&#33021;&#22312;&#20449;&#24687;&#25259;&#38706;&#21644;&#20915;&#31574;&#21046;&#23450;&#26041;&#38754;&#36896;&#25104;&#19968;&#23450;&#31243;&#24230;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30417;&#31649;&#23545;&#20110;&#39640;&#39118;&#38505;&#20844;&#20849;&#37096;&#38376; AI &#24212;&#29992;&#31243;&#24207;&#33267;&#20851;&#37325;&#35201;&#65292;&#22240;&#20026;&#20915;&#31574;&#21487;&#33021;&#20250;&#23545;&#20010;&#20154;&#21644;&#38598;&#20307;&#20135;&#29983;&#28145;&#36828;&#24433;&#21709;&#12290;&#30446;&#21069;&#22312;&#20844;&#20849;&#37096;&#38376;&#20013;&#20851;&#20110; AI &#30417;&#31649;&#26426;&#21046;&#30340;&#35768;&#22810;&#24605;&#32771;&#37117;&#22260;&#32469;&#30528;&#20154;&#31867;&#20915;&#31574;&#32773;&#22788;&#20110; "&#24490;&#29615;&#20013; "&#36825;&#19968;&#27010;&#24565;&#65292;&#24182;&#19988;&#33021;&#22815;&#24178;&#39044;&#20197;&#38450;&#27490;&#38169;&#35823;&#21644;&#28508;&#22312;&#21361;&#23475;&#12290;&#28982;&#32780;&#65292;&#22312;&#35768;&#22810;&#39640;&#39118;&#38505;&#20844;&#20849;&#37096;&#38376;&#32972;&#26223;&#19979;&#65292;&#20915;&#31574;&#30340;&#36816;&#33829;&#30417;&#31649;&#26159;&#30001;&#19987;&#19994;&#22242;&#38431;&#32780;&#19981;&#26159;&#20010;&#20154;&#36827;&#34892;&#30340;&#12290;&#37096;&#32626;&#30340; AI &#31995;&#32479;&#22914;&#20309;&#25972;&#21512;&#21040;&#36825;&#20123;&#29616;&#26377;&#30340;&#22242;&#38431;&#30417;&#31649;&#27969;&#31243;&#20013;&#65292;&#23578;&#26410;&#24341;&#36215;&#22826;&#22810;&#27880;&#24847;&#12290;&#25105;&#20204;&#36890;&#36807;&#21046;&#24230;&#20998;&#26512;&#25506;&#35752; AI &#23545;&#20020;&#24202;&#20915;&#31574;&#21046;&#23450;&#30340;&#29616;&#26377;&#30417;&#31649;&#30340;&#24433;&#21709;&#65292;&#22635;&#34917;&#35813;&#26041;&#38754;&#30340;&#31354;&#30333;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29616;&#26377;&#30340;&#30417;&#31649;&#23884;&#22871;&#22312;&#19987;&#19994;&#22521;&#35757;&#35201;&#27714;&#20013;&#65292;&#24182;&#19988;&#22312;&#24449;&#35810;&#20851;&#38190;&#20449;&#24687;&#26102; heavilyrely  &#20110;&#35299;&#37322;&#21644;&#25552;&#38382;&#12290;&#19987;&#19994;&#22242;&#38431;&#20351;&#29992;&#21508;&#31181;&#20250;&#35745;&#25259;&#38706;&#25216;&#26415;&#26469;&#35686;&#21578;&#21516;&#20107;&#21644;&#30417;&#31649;&#34892;&#20026;&#12290;&#25105;&#20204;&#32771;&#34385;&#20102;&#22312; AI &#31995;&#32479;&#24341;&#20837;&#21040;&#29616;&#26377;&#30340;&#22242;&#38431;&#30417;&#31649;&#27969;&#31243;&#20013;&#65292;&#20449;&#24687;&#25259;&#38706;&#21644;&#20915;&#31574;&#21046;&#23450;&#21487;&#33021;&#21457;&#29983;&#25913;&#21464;&#30340;&#20960;&#31181;&#26041;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Oversight is rightly recognised as vital within high-stakes public sector AI applications, where decisions can have profound individual and collective impacts. Much current thinking regarding forms of oversight mechanisms for AI within the public sector revolves around the idea of human decision makers being 'in-the-loop' and thus being able to intervene to prevent errors and potential harm. However, in a number of high-stakes public sector contexts, operational oversight of decisions is made by expert teams rather than individuals. The ways in which deployed AI systems can be integrated into these existing operational team oversight processes has yet to attract much attention. We address this gap by exploring the impacts of AI upon pre-existing oversight of clinical decision-making through institutional analysis. We find that existing oversight is nested within professional training requirements and relies heavily upon explanation and questioning to elicit vital information. Professio
&lt;/p&gt;</description></item><item><title>PowerPruning&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#33021;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#20013;MAC&#25805;&#20316;&#21151;&#32791;&#30340;&#26435;&#37325;&#26469;&#20248;&#21270;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;DNN&#22312;&#30828;&#20214;&#19978;&#30340;&#21151;&#32791;&#38477;&#20302;&#39640;&#36798;78.3&#65285;&#65292;&#21516;&#26102;&#27809;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;</title><link>http://arxiv.org/abs/2303.13997</link><description>&lt;p&gt;
PowerPruning: &#38024;&#23545;&#21151;&#32791;&#20248;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#26435;&#37325;&#21644;&#28608;&#27963;&#36873;&#25321;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
PowerPruning: Selecting Weights and Activations for Power-Efficient Neural Network Acceleration. (arXiv:2303.13997v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13997
&lt;/p&gt;
&lt;p&gt;
PowerPruning&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#33021;&#20943;&#23569;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#20013;MAC&#25805;&#20316;&#21151;&#32791;&#30340;&#26435;&#37325;&#26469;&#20248;&#21270;&#25928;&#29575;&#12290;&#35813;&#26041;&#27861;&#21487;&#20197;&#23558;DNN&#22312;&#30828;&#20214;&#19978;&#30340;&#21151;&#32791;&#38477;&#20302;&#39640;&#36798;78.3&#65285;&#65292;&#21516;&#26102;&#27809;&#26377;&#26174;&#33879;&#30340;&#20934;&#30830;&#24230;&#25439;&#22833;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#22312;&#22810;&#20010;&#39046;&#22495;&#37117;&#33719;&#24471;&#20102;&#25104;&#21151;&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#23558;&#36825;&#20123;&#32593;&#32476;&#37096;&#32626;&#21040;&#36793;&#32536;&#35774;&#22791;&#19978;&#65292;&#23588;&#20854;&#26159;&#21151;&#32791;&#38382;&#39064;&#19978;&#20173;&#28982;&#26159;&#19968;&#20010;&#20851;&#38190;&#25361;&#25112;&#65292;&#20854;&#20013;&#26368;&#20027;&#35201;&#30340;&#22240;&#32032;&#26159;&#22823;&#37327;&#30340;&#20056;&#21152;&#65288;MAC&#65289;&#25805;&#20316;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;PowerPruning&#26041;&#27861;&#65292;&#36890;&#36807;&#36873;&#25321;&#23548;&#33268;MAC&#25805;&#20316;&#28040;&#32791;&#26356;&#23569;&#21151;&#32791;&#30340;&#26435;&#37325;&#26469;&#20943;&#23569;&#25968;&#23383;&#31070;&#32463;&#32593;&#32476;&#21152;&#36895;&#22120;&#30340;&#21151;&#32791;&#12290;&#22312;&#27492;&#22522;&#30784;&#19978;&#65292;&#23545;&#25152;&#36873;&#26435;&#37325;&#21450;&#20854;&#19982;&#25152;&#26377;&#28608;&#27963;&#36716;&#25442;&#30340;&#26102;&#24207;&#29305;&#24449;&#36827;&#34892;&#35780;&#20272;&#65292;&#25361;&#36873;&#20986;&#22312;&#24341;&#36215;&#36739;&#23567;&#24310;&#36831;&#30340;&#26435;&#37325;&#21644;&#28608;&#27963;&#12290;&#22240;&#27492;&#65292;&#21363;&#20351;&#19981;&#20462;&#25913;MAC&#21333;&#20803;&#65292;MAC&#21333;&#20803;&#20013;&#25935;&#24863;&#30005;&#36335;&#36335;&#24452;&#30340;&#26368;&#22823;&#24310;&#36831;&#20063;&#23558;&#34987;&#20943;&#23567;&#65292;&#20174;&#32780;&#20801;&#35768;&#28789;&#27963;&#32553;&#23567;&#20379;&#30005;&#30005;&#21387;&#20197;&#36827;&#19968;&#27493;&#20943;&#23569;&#21151;&#32791;&#12290;&#32467;&#21512;&#37325;&#26032;&#35757;&#32451;&#65292;&#26412;&#26041;&#27861;&#21487;&#20197;&#22312;&#19981;&#26174;&#33879;&#38477;&#20302;&#20934;&#30830;&#24230;&#30340;&#24773;&#20917;&#19979;&#65292;&#23558;DNN&#22312;&#30828;&#20214;&#19978;&#30340;&#21151;&#32791;&#38477;&#20302;&#39640;&#36798;78.3&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have been successfully applied in various fields. A major challenge of deploying DNNs, especially on edge devices, is power consumption, due to the large number of multiply-and-accumulate (MAC) operations. To address this challenge, we propose PowerPruning, a novel method to reduce power consumption in digital neural network accelerators by selecting weights that lead to less power consumption in MAC operations. In addition, the timing characteristics of the selected weights together with all activation transitions are evaluated. The weights and activations that lead to small delays are further selected. Consequently, the maximum delay of the sensitized circuit paths in the MAC units is reduced even without modifying MAC units, which thus allows a flexible scaling of supply voltage to reduce power consumption further. Together with retraining, the proposed method can reduce power consumption of DNNs on hardware by up to 78.3% with only a slight accuracy loss
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#30340;&#26041;&#27861;&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#35782;&#21035;&#20986;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21487;&#33021;&#34987;&#29289;&#29702;&#35302;&#21457;&#22120;&#28608;&#27963;&#30340;&#21361;&#38505;&#21306;&#22495;&#21644;&#21487;&#36798;&#36335;&#24452;&#65292;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#25104;&#21151;&#29575;&#25509;&#36817;100&#65285;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#21457;&#29616;AV&#30340;&#28431;&#27934;&#24182;&#23454;&#26045;&#26377;&#25928;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2303.13992</link><description>&lt;p&gt;
&#21033;&#29992;&#21487;&#36798;&#24615;&#20998;&#26512;&#28608;&#27963;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#30340;&#29289;&#29702;&#21518;&#38376;&#35302;&#21457;&#22120;
&lt;/p&gt;
&lt;p&gt;
Physical Backdoor Trigger Activation of Autonomous Vehicle using Reachability Analysis. (arXiv:2303.13992v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13992
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#30340;&#26041;&#27861;&#36890;&#36807;&#21487;&#36798;&#24615;&#20998;&#26512;&#35782;&#21035;&#20986;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21487;&#33021;&#34987;&#29289;&#29702;&#35302;&#21457;&#22120;&#28608;&#27963;&#30340;&#21361;&#38505;&#21306;&#22495;&#21644;&#21487;&#36798;&#36335;&#24452;&#65292;&#27979;&#35797;&#32467;&#26524;&#26174;&#31034;&#25104;&#21151;&#29575;&#25509;&#36817;100&#65285;&#12290;&#36825;&#23558;&#26377;&#21161;&#20110;&#21457;&#29616;AV&#30340;&#28431;&#27934;&#24182;&#23454;&#26045;&#26377;&#25928;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#25581;&#31034;&#20102;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#65288;AV&#65289;&#21487;&#33021;&#34987;&#38544;&#34255;&#30340;&#21518;&#38376;&#25805;&#32437;&#65292;&#23548;&#33268;&#23427;&#20204;&#22312;&#34987;&#29289;&#29702;&#35302;&#21457;&#22120;&#28608;&#27963;&#26102;&#25191;&#34892;&#26377;&#23475;&#25805;&#20316;&#12290;&#28982;&#32780;&#65292;&#20173;&#19981;&#28165;&#26970;&#36825;&#20123;&#35302;&#21457;&#22120;&#22914;&#20309;&#22312;&#36981;&#23432;&#20132;&#36890;&#35268;&#21017;&#30340;&#24773;&#20917;&#19979;&#34987;&#28608;&#27963;&#12290;&#22312;&#21160;&#24577;&#20132;&#36890;&#29615;&#22659;&#20013;&#20102;&#35299;&#36825;&#31181;&#28431;&#27934;&#26159;&#33267;&#20851;&#37325;&#35201;&#30340;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#23558;&#29289;&#29702;&#35302;&#21457;&#22120;&#28608;&#27963;&#35270;&#20026;&#21487;&#25511;&#21160;&#24577;&#31995;&#32479;&#30340;&#21487;&#36798;&#24615;&#38382;&#39064;&#26469;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#12290;&#25105;&#20204;&#30340;&#25216;&#26415;&#35782;&#21035;&#20132;&#36890;&#31995;&#32479;&#20013;&#30340;&#23433;&#20840;&#20851;&#38190;&#21306;&#22495;&#65292;&#21487;&#20197;&#21040;&#36798;&#20107;&#25925;&#35302;&#21457;&#26465;&#20214;&#65292;&#24182;&#25552;&#20379;&#21040;&#36798;&#36825;&#20123;&#26465;&#20214;&#30340;&#24847;&#22270;&#36712;&#36857;&#12290;&#22312;&#20856;&#22411;&#20132;&#36890;&#22330;&#26223;&#20013;&#30340;&#27979;&#35797;&#34920;&#26126;&#65292;&#35813;&#31995;&#32479;&#21487;&#20197;&#25104;&#21151;&#22320;&#25512;&#21160;&#35302;&#21457;&#22120;&#26465;&#20214;&#65292;&#35302;&#21457;&#29575;&#25509;&#36817;100&#65285;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#26377;&#21161;&#20110;&#35782;&#21035;AV&#30340;&#33030;&#24369;&#24615;&#65292;&#24182;&#21551;&#29992;&#26377;&#25928;&#30340;&#23433;&#20840;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent studies reveal that Autonomous Vehicles (AVs) can be manipulated by hidden backdoors, causing them to perform harmful actions when activated by physical triggers. However, it is still unclear how these triggers can be activated while adhering to traffic principles. Understanding this vulnerability in a dynamic traffic environment is crucial. This work addresses this gap by presenting physical trigger activation as a reachability problem of controlled dynamic system. Our technique identifies security-critical areas in traffic systems where trigger conditions for accidents can be reached, and provides intended trajectories for how those conditions can be reached. Testing on typical traffic scenarios showed the system can be successfully driven to trigger conditions with near 100% activation rate. Our method benefits from identifying AV vulnerability and enabling effective safety strategies.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#37322;&#20041;&#20869;&#23481;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#20154;&#31867;&#37322;&#20041;&#38590;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#30456;&#20284;&#24615;&#36229;&#36807;&#26426;&#22120;&#37322;&#20041;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19981;&#36275;&#12290;Transformer &#26159;&#26368;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#32780; SVM-based &#26041;&#27861;&#19981;&#21450; BERT &#21644; RoBERTa &#21464;&#20307;&#12290;</title><link>http://arxiv.org/abs/2303.13989</link><description>&lt;p&gt;
&#20154;&#31867;&#19982;&#26426;&#22120;&#20869;&#23481;&#30340;&#37322;&#20041;&#26816;&#27979;&#27604;&#36739;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Paraphrase Detection: Human vs. Machine Content. (arXiv:2303.13989v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13989
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20998;&#26512;&#20102;&#19981;&#21516;&#25968;&#25454;&#38598;&#21644;&#26816;&#27979;&#26041;&#27861;&#65292;&#23545;&#20154;&#31867;&#21644;&#26426;&#22120;&#37322;&#20041;&#20869;&#23481;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#21457;&#29616;&#20154;&#31867;&#37322;&#20041;&#38590;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#30456;&#20284;&#24615;&#36229;&#36807;&#26426;&#22120;&#37322;&#20041;&#65292;&#26426;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#19981;&#36275;&#12290;Transformer &#26159;&#26368;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;&#32780; SVM-based &#26041;&#27861;&#19981;&#21450; BERT &#21644; RoBERTa &#21464;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;&#22914; GPT-4 &#21644; ChatGPT&#65289;&#26085;&#30410;&#37325;&#35201;&#65292;&#20294;&#20063;&#24341;&#36215;&#20102;&#23398;&#26415;&#35802;&#20449;&#38382;&#39064;&#65292;&#22240;&#20026;&#23384;&#22312;&#26426;&#22120;&#29983;&#25104;&#30340;&#20869;&#23481;&#21644;&#37322;&#20041;&#12290;&#34429;&#28982;&#26377;&#30740;&#31350;&#25506;&#35752;&#20102;&#20154;&#31867;&#21644;&#26426;&#22120;&#37322;&#20041;&#20869;&#23481;&#30340;&#26816;&#27979;&#65292;&#20294;&#36825;&#20123;&#31867;&#22411;&#20869;&#23481;&#20043;&#38388;&#30340;&#27604;&#36739;&#20173;&#26410;&#24471;&#21040;&#24191;&#27867;&#30740;&#31350;&#12290;&#26412;&#25991;&#23545;&#24120;&#29992;&#30340;&#21508;&#31181;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#20840;&#38754;&#20998;&#26512;&#65292;&#24182;&#35780;&#20272;&#20102;&#21508;&#31181;&#26816;&#27979;&#26041;&#27861;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#21457;&#29616;&#65292;&#30456;&#23545;&#20110;&#26426;&#22120;&#29983;&#25104;&#30340;&#25968;&#25454;&#38598;&#32570;&#20047;&#65292;&#29616;&#26377;&#30340;&#35780;&#20272;&#26041;&#27861;&#22312;&#19981;&#21516;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#21508;&#26377;&#20248;&#21155;&#12290;&#25105;&#20204;&#30340;&#20027;&#35201;&#21457;&#29616;&#26159;&#65292;&#20154;&#31867;&#37322;&#20041;&#36229;&#36807;&#26426;&#22120;&#37322;&#20041;&#30340;&#38590;&#24230;&#12289;&#22810;&#26679;&#24615;&#21644;&#30456;&#20284;&#24615;&#65292;&#26263;&#31034;&#33258;&#21160;&#29983;&#25104;&#30340;&#25991;&#26412;&#36824;&#27809;&#26377;&#36798;&#21040;&#20154;&#31867;&#27700;&#24179;&#12290;Transformer &#26159;&#26368;&#26377;&#25928;&#30340;&#26816;&#27979;&#26041;&#27861;&#65292;BERT &#21644; RoBERTa &#21464;&#20307;&#22312;&#25152;&#26377;&#25968;&#25454;&#38598;&#19978;&#37117;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#32467;&#26524;&#65292;&#32780;&#22522;&#20110; SVM &#30340;&#26041;&#27861;&#21017;&#33853;&#21518;&#20110;&#23427;&#20204;&#12290;
&lt;/p&gt;
&lt;p&gt;
The growing prominence of large language models, such as GPT-4 and ChatGPT, has led to increased concerns over academic integrity due to the potential for machine-generated content and paraphrasing. Although studies have explored the detection of human- and machine-paraphrased content, the comparison between these types of content remains underexplored. In this paper, we conduct a comprehensive analysis of various datasets commonly employed for paraphrase detection tasks and evaluate an array of detection methods. Our findings highlight the strengths and limitations of different detection methods in terms of performance on individual datasets, revealing a lack of suitable machine-generated datasets that can be aligned with human expectations. Our main finding is that human-authored paraphrases exceed machine-generated ones in terms of difficulty, diversity, and similarity implying that automatically generated texts are not yet on par with human-level performance. Transformers emerged a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;</title><link>http://arxiv.org/abs/2303.13988</link><description>&lt;p&gt;
&#26426;&#22120;&#24515;&#29702;&#23398;&#65306;&#21033;&#29992;&#24515;&#29702;&#23398;&#26041;&#27861;&#25506;&#31350;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#26032;&#20852;&#33021;&#21147;&#21644;&#34892;&#20026;
&lt;/p&gt;
&lt;p&gt;
Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13988
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39046;&#22495;&#8212;&#8212;&#26426;&#22120;&#24515;&#29702;&#23398;&#65292;&#21033;&#29992;&#24515;&#29702;&#23398;&#30340;&#26041;&#27861;&#32771;&#23519;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#33021;&#21147;&#12290;&#35813;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#24182;&#23545;&#24515;&#29702;&#23454;&#39564;&#20013;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#36827;&#34892;&#20102;&#25506;&#35752;&#21644;&#21046;&#23450;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#26159;&#23558;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#19982;&#20154;&#31867;&#20132;&#27969;&#21644;&#26085;&#24120;&#29983;&#27963;&#32039;&#23494;&#32467;&#21512;&#30340;&#20808;&#38155;&#12290;&#30001;&#20110;&#24555;&#36895;&#25216;&#26415;&#36827;&#27493;&#21644;&#20854;&#26497;&#39640;&#30340;&#36890;&#29992;&#24615;&#65292;&#29616;&#20170;LLM&#24050;&#32463;&#25317;&#26377;&#25968;&#30334;&#19975;&#29992;&#25143;&#65292;&#24182;&#27491;&#22788;&#20110;&#25104;&#20026;&#20027;&#35201;&#20449;&#24687;&#26816;&#32034;&#12289;&#20869;&#23481;&#29983;&#25104;&#12289;&#38382;&#39064;&#35299;&#20915;&#31561;&#25216;&#26415;&#30340;&#21069;&#27839;&#12290;&#22240;&#27492;&#65292;&#23545;&#20854;&#36827;&#34892;&#20840;&#38754;&#35780;&#20272;&#21644;&#23457;&#26597;&#26174;&#24471;&#23588;&#20026;&#37325;&#35201;&#12290;&#30001;&#20110;&#24403;&#21069;LLM&#20013;&#20986;&#29616;&#24840;&#21152;&#22797;&#26434;&#21644;&#26032;&#39062;&#30340;&#34892;&#20026;&#27169;&#24335;&#65292;&#21487;&#23558;&#20854;&#35270;&#20026;&#21442;&#19982;&#20154;&#31867;&#24515;&#29702;&#23454;&#39564;&#30340;&#23545;&#35937;&#65292;&#20197;&#20415;&#26356;&#20026;&#20840;&#38754;&#22320;&#35780;&#20272;&#20854;&#33021;&#21147;&#12290;&#20026;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#20102;&#19968;&#20010;&#21517;&#20026;"&#26426;&#22120;&#24515;&#29702;&#23398;"&#30340;&#26032;&#20852;&#30740;&#31350;&#39046;&#22495;&#12290;&#26412;&#25991;&#27010;&#36848;&#20102;&#21508;&#31867;&#24515;&#29702;&#23398;&#20998;&#25903;&#22914;&#20309;&#20026;LLM&#30340;&#34892;&#20026;&#27979;&#35797;&#25552;&#20379;&#26377;&#29992;&#21442;&#32771;&#12290;&#21516;&#26102;&#65292;&#26412;&#25991;&#35268;&#33539;&#20102;&#26426;&#22120;&#24515;&#29702;&#23398;&#30740;&#31350;&#30340;&#26041;&#27861;&#35770;&#26631;&#20934;&#65292;&#29305;&#21035;&#26159;&#19987;&#27880;&#20110;&#25552;&#31034;&#35774;&#35745;&#25919;&#31574;&#30340;&#21046;&#23450;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#25551;&#36848;&#20102;&#34892;&#20026;&#27979;&#35797;&#32467;&#26524;&#22914;&#20309;&#20026;&#26410;&#26469;&#30340;LLM&#21457;&#23637;&#25552;&#20379;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behaviora
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#39640;&#23618;&#34892;&#20026;&#35268;&#21010;&#21644;&#20302;&#23618;&#36712;&#36857;&#35268;&#21010;&#12290;&#36890;&#36807;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#39550;&#39542;&#20013;&#30340;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2303.13986</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#30340;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#21487;&#35299;&#37322;&#24335;&#36816;&#21160;&#35268;&#21010;&#22120;
&lt;/p&gt;
&lt;p&gt;
Interpretable Motion Planner for Urban Driving via Hierarchical Imitation Learning. (arXiv:2303.13986v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#30340;&#36816;&#21160;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#39640;&#23618;&#34892;&#20026;&#35268;&#21010;&#21644;&#20302;&#23618;&#36712;&#36857;&#35268;&#21010;&#12290;&#36890;&#36807;&#22312;&#20223;&#30495;&#21644;&#23454;&#38469;&#39550;&#39542;&#20013;&#30340;&#27979;&#35797;&#65292;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#22312;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#23398;&#20064;&#30340;&#26041;&#27861;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#20915;&#31574;&#21644;&#35268;&#21010;&#27169;&#22359;&#19978;&#21462;&#24471;&#20102;&#20196;&#20154;&#30633;&#30446;&#30340;&#24615;&#33021;&#65292;&#20294;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#38752;&#24615;&#21644;&#31283;&#23450;&#24615;&#20173;&#28982;&#23384;&#22312;&#25361;&#25112;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#20998;&#23618;&#27169;&#20223;&#23398;&#20064;&#26041;&#27861;&#65292;&#21253;&#25324;&#39640;&#23618;&#27425;&#30340;&#22522;&#20110;&#32593;&#26684;&#30340;&#34892;&#20026;&#35268;&#21010;&#22120;&#21644;&#20302;&#23618;&#27425;&#30340;&#36712;&#36857;&#35268;&#21010;&#22120;&#65292;&#19981;&#20165;&#26159;&#19968;&#31181;&#20010;&#20307;&#30340;&#25968;&#25454;&#39537;&#21160;&#39550;&#39542;&#31574;&#30053;&#65292;&#32780;&#19988;&#36824;&#21487;&#20197;&#36731;&#26494;&#22320;&#23884;&#20837;&#22522;&#20110;&#35268;&#21017;&#30340;&#26550;&#26500;&#20013;&#12290;&#25105;&#20204;&#22312;&#38381;&#29615;&#20223;&#30495;&#21644;&#23454;&#38469;&#39550;&#39542;&#20013;&#35780;&#20272;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#65292;&#24182;&#35777;&#26126;&#31070;&#32463;&#32593;&#32476;&#35268;&#21010;&#22120;&#22312;&#22797;&#26434;&#30340;&#22478;&#24066;&#33258;&#21160;&#39550;&#39542;&#22330;&#26223;&#20013;&#20855;&#26377;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning-based approaches have achieved impressive performance for autonomous driving and an increasing number of data-driven works are being studied in the decision-making and planning module. However, the reliability and the stability of the neural network is still full of challenges. In this paper, we introduce a hierarchical imitation method including a high-level grid-based behavior planner and a low-level trajectory planner, which is not only an individual data-driven driving policy and can also be easily embedded into the rule-based architecture. We evaluate our method both in closed-loop simulation and real world driving, and demonstrate the neural network planner has outstanding performance in complex urban autonomous driving scenarios.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;AssetField&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#23545;&#35937;&#24863;&#30693;&#30340;&#22320;&#38754;&#29305;&#24449;&#24179;&#38754;&#26469;&#34920;&#31034;&#22330;&#26223;&#65292;&#21487;&#23454;&#29616;&#21508;&#31181;&#25805;&#20316;&#20197;&#37197;&#32622;&#26032;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32852;&#21512;&#21487;&#35270;&#21270;&#34920;&#31034;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#22330;&#26223;&#21644;&#24314;&#27169;&#12290;</title><link>http://arxiv.org/abs/2303.13953</link><description>&lt;p&gt;
AssetField&#65306;&#22320;&#38754;&#29305;&#24449;&#24179;&#38754;&#34920;&#31034;&#20013;&#30340;&#36164;&#20135;&#37319;&#25496;&#21644;&#37325;&#26500;
&lt;/p&gt;
&lt;p&gt;
AssetField: Assets Mining and Reconfiguration in Ground Feature Plane Representation. (arXiv:2303.13953v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13953
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;AssetField&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#36890;&#36807;&#23398;&#20064;&#19968;&#32452;&#23545;&#35937;&#24863;&#30693;&#30340;&#22320;&#38754;&#29305;&#24449;&#24179;&#38754;&#26469;&#34920;&#31034;&#22330;&#26223;&#65292;&#21487;&#23454;&#29616;&#21508;&#31181;&#25805;&#20316;&#20197;&#37197;&#32622;&#26032;&#22330;&#26223;&#65292;&#24182;&#25552;&#20379;&#20102;&#21487;&#35299;&#37322;&#30340;&#32852;&#21512;&#21487;&#35270;&#21270;&#34920;&#31034;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#22330;&#26223;&#21644;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23460;&#20869;&#21644;&#23460;&#22806;&#29615;&#22659;&#26412;&#36136;&#19978;&#37117;&#26159;&#26377;&#32467;&#26500;&#21644;&#37325;&#22797;&#24615;&#30340;&#12290;&#20256;&#32479;&#24314;&#27169;&#27969;&#31243;&#36890;&#36807;&#20445;&#25345;&#19968;&#20010;&#23384;&#20648;&#29420;&#29305;&#23545;&#35937;&#27169;&#26495;&#30340;&#36164;&#20135;&#24211;&#65292;&#23454;&#29616;&#20102;&#22810;&#31181;&#21151;&#33021;&#23454;&#29992;&#19988;&#20869;&#23384;&#39640;&#25928;&#30340;&#26041;&#24335;&#12290;&#21463;&#27492;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;AssetField&#65292;&#19968;&#31181;&#26032;&#22411;&#30340;&#31070;&#32463;&#22330;&#26223;&#34920;&#31034;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#23398;&#20064;&#19968;&#32452;&#23545;&#35937;&#24863;&#30693;&#30340;&#22320;&#38754;&#29305;&#24449;&#24179;&#38754;&#26469;&#34920;&#31034;&#22330;&#26223;&#65292;&#22312;&#20854;&#20013;&#21487;&#20197;&#20197;&#26080;&#30417;&#30563;&#30340;&#26041;&#24335;&#26500;&#24314;&#23384;&#20648;&#27169;&#26495;&#29305;&#24449;&#22359;&#30340;&#36164;&#20135;&#24211;&#12290;&#19982;&#29616;&#26377;&#26041;&#27861;&#38656;&#35201;&#23545;&#35937;&#25513;&#30721;&#26469;&#26597;&#35810;&#31354;&#38388;&#28857;&#20197;&#36827;&#34892;&#23545;&#35937;&#32534;&#36753;&#19981;&#21516;&#65292;&#25105;&#20204;&#30340;&#22320;&#38754;&#29305;&#24449;&#24179;&#38754;&#34920;&#31034;&#22312;&#40479;&#30640;&#35270;&#22270;&#20013;&#20026;&#22330;&#26223;&#25552;&#20379;&#20102;&#19968;&#31181;&#33258;&#28982;&#30340;&#21487;&#35270;&#21270;&#26041;&#27861;&#65292;&#20801;&#35768;&#23545;&#23545;&#35937;&#36827;&#34892;&#21508;&#31181;&#25805;&#20316;&#65288;&#20363;&#22914;&#24179;&#31227;&#65292;&#37325;&#22797;&#65292;&#21464;&#24418;&#65289;&#20197;&#37197;&#32622;&#26032;&#22330;&#26223;&#12290;&#37197;&#21512;&#20351;&#29992;&#29305;&#24449;&#22359;&#27169;&#26495;&#65292;&#23545;&#20110;&#26377;&#35768;&#22810;&#37325;&#22797;&#29289;&#21697;&#30340;&#22330;&#26223;&#65292;&#21487;&#20197;&#21551;&#29992;&#32676;&#32452;&#32534;&#36753;&#65292;&#36991;&#20813;&#22312;&#20010;&#21035;&#23545;&#35937;&#19978;&#37325;&#22797;&#24615;&#24037;&#20316;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;AssetField&#19981;&#20165;&#23454;&#29616;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#32780;&#19988;&#36824;&#25552;&#20379;&#20102;&#19968;&#31181;&#21487;&#35299;&#37322;&#30340;&#32852;&#21512;&#21487;&#35270;&#21270;&#34920;&#31034;&#65292;&#20174;&#32780;&#26356;&#22909;&#22320;&#29702;&#35299;&#22330;&#26223;&#21644;&#24314;&#27169;&#12290;
&lt;/p&gt;
&lt;p&gt;
Both indoor and outdoor environments are inherently structured and repetitive. Traditional modeling pipelines keep an asset library storing unique object templates, which is both versatile and memory efficient in practice. Inspired by this observation, we propose AssetField, a novel neural scene representation that learns a set of object-aware ground feature planes to represent the scene, where an asset library storing template feature patches can be constructed in an unsupervised manner. Unlike existing methods which require object masks to query spatial points for object editing, our ground feature plane representation offers a natural visualization of the scene in the bird-eye view, allowing a variety of operations (e.g. translation, duplication, deformation) on objects to configure a new scene. With the template feature patches, group editing is enabled for scenes with many recurring items to avoid repetitive work on object individuals. We show that AssetField not only achieves com
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#31995;&#32479;&#22320;&#27010;&#36848;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#23427;&#20204;&#22312;AI&#31995;&#32479;&#21644;&#28508;&#22312;&#24212;&#29992;&#39046;&#22495;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22270;&#35889;&#30340;&#25216;&#26415;&#25361;&#25112;&#21253;&#25324;&#23884;&#20837;&#12289;&#33719;&#21462;&#12289;&#34917;&#20840;&#12289;&#34701;&#21512;&#21644;&#25512;&#29702;&#12290;</title><link>http://arxiv.org/abs/2303.13948</link><description>&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#65306;&#26426;&#36935;&#19982;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graphs: Opportunities and Challenges. (arXiv:2303.13948v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#31995;&#32479;&#22320;&#27010;&#36848;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#26426;&#36935;&#21644;&#25361;&#25112;&#12290;&#23427;&#20204;&#22312;AI&#31995;&#32479;&#21644;&#28508;&#22312;&#24212;&#29992;&#39046;&#22495;&#26041;&#38754;&#20855;&#26377;&#24191;&#27867;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#30693;&#35782;&#22270;&#35889;&#30340;&#25216;&#26415;&#25361;&#25112;&#21253;&#25324;&#23884;&#20837;&#12289;&#33719;&#21462;&#12289;&#34917;&#20840;&#12289;&#34701;&#21512;&#21644;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#22823;&#25968;&#25454;&#30340;&#29190;&#28856;&#24335;&#22686;&#38271;&#65292;&#36866;&#24403;&#22320;&#32452;&#32455;&#21644;&#20195;&#34920;&#28023;&#37327;&#30693;&#35782;&#21464;&#24471;&#33267;&#20851;&#37325;&#35201;&#12290;&#20316;&#20026;&#22270;&#24418;&#25968;&#25454;&#65292;&#30693;&#35782;&#22270;&#35889;&#31215;&#32047;&#21644;&#20256;&#36798;&#20102;&#30495;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#12290;&#24050;&#32463;&#24191;&#20026;&#20154;&#30693;&#65292;&#30693;&#35782;&#22270;&#35889;&#26377;&#25928;&#22320;&#20195;&#34920;&#20102;&#22797;&#26434;&#30340;&#20449;&#24687;&#65292;&#22240;&#27492;&#36817;&#24180;&#26469;&#24456;&#24555;&#24341;&#36215;&#20102;&#23398;&#26415;&#30028;&#21644;&#24037;&#19994;&#30028;&#30340;&#20851;&#27880;&#12290;&#22240;&#27492;&#65292;&#20026;&#20102;&#26356;&#28145;&#20837;&#22320;&#20102;&#35299;&#30693;&#35782;&#22270;&#35889;&#65292;&#26412;&#25991;&#23545;&#35813;&#39046;&#22495;&#36827;&#34892;&#20102;&#31995;&#32479;&#30340;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20851;&#27880;&#30693;&#35782;&#22270;&#35889;&#30340;&#26426;&#36935;&#19982;&#25361;&#25112;&#12290;&#25105;&#20204;&#39318;&#20808;&#20174;&#20004;&#20010;&#26041;&#38754;&#22238;&#39038;&#20102;&#30693;&#35782;&#22270;&#35889;&#30340;&#26426;&#20250;&#65306;&#65288;1&#65289;&#24314;&#31435;&#22312;&#30693;&#35782;&#22270;&#35889;&#22522;&#30784;&#19978;&#30340;AI&#31995;&#32479;&#65307;&#65288;2&#65289;&#30693;&#35782;&#22270;&#35889;&#30340;&#28508;&#22312;&#24212;&#29992;&#39046;&#22495;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#35814;&#32454;&#35752;&#35770;&#20102;&#35813;&#39046;&#22495;&#20013;&#30340;&#20005;&#23803;&#25216;&#26415;&#25361;&#25112;&#65292;&#22914;&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#12289;&#30693;&#35782;&#33719;&#21462;&#12289;&#30693;&#35782;&#22270;&#35889;&#34917;&#20840;&#12289;&#30693;&#35782;&#34701;&#21512;&#21644;&#30693;&#35782;&#25512;&#29702;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the explosive growth of artificial intelligence (AI) and big data, it has become vitally important to organize and represent the enormous volume of knowledge appropriately. As graph data, knowledge graphs accumulate and convey knowledge of the real world. It has been well-recognized that knowledge graphs effectively represent complex information; hence, they rapidly gain the attention of academia and industry in recent years. Thus to develop a deeper understanding of knowledge graphs, this paper presents a systematic overview of this field. Specifically, we focus on the opportunities and challenges of knowledge graphs. We first review the opportunities of knowledge graphs in terms of two aspects: (1) AI systems built upon knowledge graphs; (2) potential application fields of knowledge graphs. Then, we thoroughly discuss severe technical challenges in this field, such as knowledge graph embeddings, knowledge acquisition, knowledge graph completion, knowledge fusion, and knowledge r
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#36719;&#20214;&#24320;&#21457;&#34892;&#19994;&#37319;&#29992;&#29983;&#25104;&#24335; AI&#65288;GAI&#65289;&#21161;&#25163;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#30340;&#29616;&#29366;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#36719;&#20214;&#24320;&#21457;&#25945;&#32946;&#30340;&#24895;&#26223;&#21644;&#25945;&#23398;&#24314;&#35758;&#12290;</title><link>http://arxiv.org/abs/2303.13936</link><description>&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#25945;&#32946;&#20013;&#30340;&#29983;&#25104;&#24335; AI &#21161;&#25163;
&lt;/p&gt;
&lt;p&gt;
Generative AI Assistants in Software Development Education. (arXiv:2303.13936v1 [cs.SE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13936
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#24403;&#21069;&#36719;&#20214;&#24320;&#21457;&#34892;&#19994;&#37319;&#29992;&#29983;&#25104;&#24335; AI&#65288;GAI&#65289;&#21161;&#25163;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#30340;&#29616;&#29366;&#21644;&#25361;&#25112;&#65292;&#25552;&#20986;&#20102;&#26410;&#26469;&#36719;&#20214;&#24320;&#21457;&#25945;&#32946;&#30340;&#24895;&#26223;&#21644;&#25945;&#23398;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36719;&#20214;&#24320;&#21457;&#34892;&#19994;&#27491;&#22312;&#36827;&#34892;&#19968;&#27425;&#28508;&#22312;&#30340;&#39072;&#35206;&#24615;&#30340;&#33539;&#24335;&#21464;&#38761;&#8212;&#8212;&#37319;&#29992;&#29983;&#25104;&#24335; AI&#65288;GAI&#65289;&#21161;&#25163;&#36827;&#34892;&#36719;&#20214;&#24320;&#21457;&#12290;&#34429;&#28982; AI &#24050;&#32463;&#22312;&#36719;&#20214;&#24037;&#31243;&#30340;&#21508;&#20010;&#39046;&#22495;&#20013;&#34987;&#20351;&#29992;&#65292;&#20294;&#26159;&#20687; GitHub Copilot &#21644; ChatGPT &#36825;&#26679;&#30340; GAI &#25216;&#26415;&#24050;&#32463;&#28608;&#21457;&#20102;&#35768;&#22810;&#20154;&#30340;&#24819;&#35937;&#21147;&#65288;&#21644;&#24656;&#24807;&#65289;&#12290;&#23613;&#31649;&#30446;&#21069;&#23578;&#19981;&#28165;&#26970;&#35813;&#34892;&#19994;&#23558;&#22914;&#20309;&#37319;&#29992;&#21644;&#36866;&#24212;&#36825;&#20123;&#25216;&#26415;&#65292;&#20294;&#24494;&#36719;&#65288;GitHub&#12289;&#24517;&#24212;&#65289;&#21644;&#35895;&#27468;&#65288;Bard&#65289;&#31561;&#22823;&#22411;&#36719;&#20214;&#20844;&#21496;&#23558;&#36825;&#20123;&#25216;&#26415;&#25972;&#21512;&#21040;&#26356;&#24191;&#27867;&#30340;&#34892;&#19994;&#20013;&#30340;&#20030;&#21160;&#26159;&#26126;&#30830;&#30340;&#24847;&#22270;&#21644;&#26041;&#21521;&#12290;&#25105;&#20204;&#19982;&#34892;&#19994;&#19987;&#19994;&#20154;&#22763;&#36827;&#34892;&#20102;&#25506;&#32034;&#24615;&#35775;&#35848;&#65292;&#20197;&#20102;&#35299;&#24403;&#21069;&#30340;&#23454;&#36341;&#21644;&#25361;&#25112;&#65292;&#23558;&#20854;&#32435;&#20837;&#25105;&#20204;&#23545;&#26410;&#26469;&#36719;&#20214;&#24320;&#21457;&#25945;&#32946;&#30340;&#24895;&#26223;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20123;&#25945;&#23398;&#24314;&#35758;&#12290;
&lt;/p&gt;
&lt;p&gt;
The software development industry is amid another potentially disruptive paradigm change--adopting the use of generative AI (GAI) assistants for software development. Whilst AI is already used in various areas of software engineering, GAI technologies, such as GitHub Copilot and ChatGPT, have ignited the imaginations (and fears) of many people. Whilst it is unclear how the industry will adopt and adapt to these technologies, the move to integrate these technologies into the wider industry by large software companies, such as Microsoft (GitHub, Bing) and Google (Bard), is a clear indication of intent and direction. We performed exploratory interviews with industry professionals to understand current practices and challenges, which we incorporate into our vision of a future of software development education and make some pedagogical recommendations.
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#31639;&#27861;&#65292;Norm&#21644;G-PELT&#65292;&#20197;&#21450;G-Window&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20854;&#24418;&#24335;&#25110;&#32467;&#26500;&#23545;&#31526;&#21495;&#38899;&#20048;&#36827;&#34892;&#20998;&#21106;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#22270;&#34920;&#31034;&#23545;&#31526;&#21495;&#38899;&#20048;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#35745;&#31639;&#20174;&#22270;&#20013;&#33719;&#24471;&#30340;&#37051;&#25509;&#30697;&#38453;&#30340;&#26032;&#39062;&#24615;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#31526;&#21495;&#38899;&#20048;&#29255;&#27573;&#30340;&#32467;&#26500;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#21040;&#36793;&#30028;&#12290;</title><link>http://arxiv.org/abs/2303.13881</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#34920;&#31034;&#21644;&#21464;&#28857;&#26816;&#27979;&#26041;&#27861;&#30340;&#31526;&#21495;&#38899;&#20048;&#32467;&#26500;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Symbolic Music Structure Analysis with Graph Representations and Changepoint Detection Methods. (arXiv:2303.13881v1 [cs.SD])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13881
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#20004;&#20010;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#31639;&#27861;&#65292;Norm&#21644;G-PELT&#65292;&#20197;&#21450;G-Window&#26041;&#27861;&#65292;&#29992;&#20110;&#36890;&#36807;&#20854;&#24418;&#24335;&#25110;&#32467;&#26500;&#23545;&#31526;&#21495;&#38899;&#20048;&#36827;&#34892;&#20998;&#21106;&#12290;&#30740;&#31350;&#21457;&#29616;&#65292;&#20351;&#29992;&#22270;&#34920;&#31034;&#23545;&#31526;&#21495;&#38899;&#20048;&#36827;&#34892;&#32534;&#30721;&#65292;&#24182;&#35745;&#31639;&#20174;&#22270;&#20013;&#33719;&#24471;&#30340;&#37051;&#25509;&#30697;&#38453;&#30340;&#26032;&#39062;&#24615;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#31526;&#21495;&#38899;&#20048;&#29255;&#27573;&#30340;&#32467;&#26500;&#65292;&#24182;&#33021;&#22815;&#26816;&#27979;&#21040;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38899;&#20048;&#32467;&#26500;&#20998;&#26512;&#26159;&#38899;&#20048;&#20449;&#24687;&#26816;&#32034;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#24615;&#30740;&#31350;&#20219;&#21153;&#12290;&#36807;&#21435;&#26377;&#20960;&#39033;&#24037;&#20316;&#35797;&#22270;&#23558;&#38899;&#20048;&#20998;&#21106;&#25104;&#38899;&#39057;&#21644;&#31526;&#21495;&#39046;&#22495;&#65292;&#28982;&#32780;&#65292;&#22312;&#19981;&#21516;&#23618;&#38754;&#19978;&#35782;&#21035;&#21644;&#20998;&#21106;&#38899;&#20048;&#32467;&#26500;&#20173;&#26159;&#36825;&#20010;&#39046;&#22495;&#20013;&#30340;&#19968;&#20010;&#24320;&#25918;&#24615;&#30740;&#31350;&#38382;&#39064;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#31181;&#26041;&#27861;&#65292;&#20854;&#20013;&#20004;&#31181;&#26159;&#26032;&#39062;&#30340;&#22522;&#20110;&#22270;&#30340;&#31639;&#27861;&#65292;&#26088;&#22312;&#36890;&#36807;&#20854;&#24418;&#24335;&#25110;&#32467;&#26500;&#23545;&#31526;&#21495;&#38899;&#20048;&#36827;&#34892;&#20998;&#21106;&#65306;Norm&#12289;G-PELT&#21644;G-Window&#12290;&#25105;&#20204;&#23545;&#20004;&#20010;&#20855;&#26377;&#19981;&#21516;&#24418;&#24335;&#25110;&#32467;&#26500;&#30340;&#20844;&#20849;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#21066;&#24369;&#30740;&#31350;&#65292;&#20197;&#27604;&#36739;&#36825;&#20123;&#26041;&#27861;&#65292;&#25913;&#21464;&#20854;&#21442;&#25968;&#20540;&#65292;&#24182;&#23558;&#24615;&#33021;&#19982;&#19981;&#21516;&#30340;&#38899;&#20048;&#39118;&#26684;&#36827;&#34892;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#29992;&#22270;&#34920;&#31034;&#23545;&#31526;&#21495;&#38899;&#20048;&#36827;&#34892;&#32534;&#30721;&#24182;&#35745;&#31639;&#20174;&#22270;&#20013;&#33719;&#24471;&#30340;&#37051;&#25509;&#30697;&#38453;&#30340;&#26032;&#39062;&#24615;&#33021;&#24456;&#22909;&#22320;&#34920;&#31034;&#31526;&#21495;&#38899;&#20048;&#29255;&#27573;&#30340;&#32467;&#26500;&#65292;&#26080;&#38656;&#20174;&#20013;&#25552;&#21462;&#29305;&#24449;&#12290;&#25105;&#20204;&#33021;&#22815;&#26816;&#27979;&#21040;&#36793;&#30028;&#12290;
&lt;/p&gt;
&lt;p&gt;
Music Structure Analysis is an open research task in Music Information Retrieval (MIR). In the past, there have been several works that attempt to segment music into the audio and symbolic domains, however, the identification and segmentation of the music structure at different levels is still an open research problem in this area. In this work we propose three methods, two of which are novel graph-based algorithms that aim to segment symbolic music by its form or structure: Norm, G-PELT and G-Window. We performed an ablation study with two public datasets that have different forms or structures in order to compare such methods varying their parameter values and comparing the performance against different music styles. We have found that encoding symbolic music with graph representations and computing the novelty of Adjacency Matrices obtained from graphs represent the structure of symbolic music pieces well without the need to extract features from it. We are able to detect the bounda
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26597;&#35810;&#30340;&#35270;&#39057;&#34920;&#31034;&#26469;&#35299;&#20915;&#35270;&#39057;&#26102;&#21051;&#26816;&#32034;&#21644;&#20142;&#28857;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#26597;&#35810;&#20449;&#24687;&#21033;&#29992;&#19981;&#20805;&#20998;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#32534;&#30721;&#27169;&#22359;&#20013;&#37319;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#23558;&#25991;&#26412;&#26597;&#35810;&#30340;&#19978;&#19979;&#25991;&#27880;&#20837;&#35270;&#39057;&#34920;&#31034;&#20013;&#12290;</title><link>http://arxiv.org/abs/2303.13874</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#30340;&#35270;&#39057;&#34920;&#31034;&#29992;&#20110;&#26102;&#21051;&#26816;&#32034;&#21644;&#20142;&#28857;&#26816;&#27979;
&lt;/p&gt;
&lt;p&gt;
Query-Dependent Video Representation for Moment Retrieval and Highlight Detection. (arXiv:2303.13874v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13874
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#26597;&#35810;&#30340;&#35270;&#39057;&#34920;&#31034;&#26469;&#35299;&#20915;&#35270;&#39057;&#26102;&#21051;&#26816;&#32034;&#21644;&#20142;&#28857;&#26816;&#27979;&#20013;&#23384;&#22312;&#30340;&#26597;&#35810;&#20449;&#24687;&#21033;&#29992;&#19981;&#20805;&#20998;&#30340;&#38382;&#39064;&#65292;&#35813;&#26041;&#27861;&#22312;&#32534;&#30721;&#27169;&#22359;&#20013;&#37319;&#29992;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#23558;&#25991;&#26412;&#26597;&#35810;&#30340;&#19978;&#19979;&#25991;&#27880;&#20837;&#35270;&#39057;&#34920;&#31034;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#38543;&#30528;&#23545;&#35270;&#39057;&#29702;&#35299;&#30340;&#38656;&#27714;&#24613;&#21095;&#22686;&#21152;&#65292;&#35270;&#39057;&#26102;&#21051;&#26816;&#32034;&#65288;MR&#65289;&#21644;&#20142;&#28857;&#26816;&#27979;&#65288;HD&#65289;&#22791;&#21463;&#30633;&#30446;&#12290;MR / HD&#30340;&#20851;&#38190;&#30446;&#26631;&#26159;&#23450;&#20301;&#26102;&#21051;&#24182;&#20272;&#35745;&#21098;&#36753;&#32423;&#21035;&#30340;&#31526;&#21512;&#31243;&#24230;&#65292;&#21363;&#31361;&#20986;&#26174;&#31034;&#30340;&#20998;&#25968;&#65292;&#20197;&#32473;&#23450;&#30340;&#25991;&#26412;&#26597;&#35810;&#20026;&#20381;&#25454;&#12290;&#23613;&#31649;&#26368;&#36817;&#30340;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#24102;&#26469;&#20102;&#19968;&#20123;&#36827;&#27493;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#36825;&#20123;&#26041;&#27861;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#32473;&#23450;&#26597;&#35810;&#30340;&#20449;&#24687;&#12290;&#20363;&#22914;&#65292;&#22312;&#39044;&#27979;&#26102;&#21051;&#21450;&#20854;&#26174;&#33879;&#24615;&#26102;&#65292;&#26377;&#26102;&#20250;&#24573;&#30053;&#25991;&#26412;&#26597;&#35810;&#21644;&#35270;&#39057;&#20869;&#23481;&#20043;&#38388;&#30340;&#30456;&#20851;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19968;&#31181;&#38024;&#23545;MR / HD&#37327;&#36523;&#23450;&#21046;&#30340;&#26816;&#27979;Transformer&#8212;&#8212;Query-Dependent DETR&#65288;QD-DETR&#65289;&#12290;&#30001;&#20110;&#25105;&#20204;&#35266;&#23519;&#21040;&#32473;&#23450;&#26597;&#35810;&#22312;Transformer&#26550;&#26500;&#20013;&#30340;&#37325;&#35201;&#24615;&#19981;&#22826;&#26126;&#26174;&#65292;&#25105;&#20204;&#30340;&#32534;&#30721;&#27169;&#22359;&#20174;&#20132;&#21449;&#27880;&#24847;&#21147;&#23618;&#24320;&#22987;&#65292;&#20197;&#26126;&#30830;&#23558;&#25991;&#26412;&#26597;&#35810;&#30340;&#19978;&#19979;&#25991;&#27880;&#20837;&#35270;&#39057;&#34920;&#31034;&#12290;&#28982;&#21518;&#65292;&#20026;&#20102;&#22686;&#24378;&#27169;&#22411;&#21033;&#29992;&#26597;&#35810;&#20449;&#24687;&#30340;&#33021;&#21147;&#65292;&#25105;&#20204;&#25805;&#32437;&#35270;&#39057;
&lt;/p&gt;
&lt;p&gt;
Recently, video moment retrieval and highlight detection (MR/HD) are being spotlighted as the demand for video understanding is drastically increased. The key objective of MR/HD is to localize the moment and estimate clip-wise accordance level, i.e., saliency score, to the given text query. Although the recent transformer-based models brought some advances, we found that these methods do not fully exploit the information of a given query. For example, the relevance between text query and video contents is sometimes neglected when predicting the moment and its saliency. To tackle this issue, we introduce Query-Dependent DETR (QD-DETR), a detection transformer tailored for MR/HD. As we observe the insignificant role of a given query in transformer architectures, our encoding module starts with cross-attention layers to explicitly inject the context of text query into video representation. Then, to enhance the model's capability of exploiting the query information, we manipulate the video
&lt;/p&gt;</description></item><item><title>Fantasia3D&#26159;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#29983;&#25104;3D&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#20960;&#20309;&#21644;&#22806;&#35266;&#24314;&#27169;&#21644;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20960;&#20309;&#32454;&#33410;&#21644;&#36924;&#30495;&#28210;&#26579;&#65292;&#24182;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2303.13873</link><description>&lt;p&gt;
Fantasia3D: &#29992;&#20110;&#39640;&#36136;&#37327;&#25991;&#26412;&#29983;&#25104;3D&#20869;&#23481;&#30340;&#20960;&#20309;&#21644;&#22806;&#35266;&#20998;&#31163;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Fantasia3D: Disentangling Geometry and Appearance for High-quality Text-to-3D Content Creation. (arXiv:2303.13873v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13873
&lt;/p&gt;
&lt;p&gt;
Fantasia3D&#26159;&#19968;&#31181;&#26032;&#30340;&#25991;&#26412;&#29983;&#25104;3D&#20869;&#23481;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#20998;&#31163;&#20960;&#20309;&#21644;&#22806;&#35266;&#24314;&#27169;&#21644;&#23398;&#20064;&#65292;&#25552;&#39640;&#20102;&#20960;&#20309;&#32454;&#33410;&#21644;&#36924;&#30495;&#28210;&#26579;&#65292;&#24182;&#26356;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#65292;&#30001;&#20110;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#21644;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#25552;&#20379;&#65292;&#33258;&#21160;3D&#20869;&#23481;&#30340;&#21019;&#24314;&#21462;&#24471;&#20102;&#24555;&#36895;&#36827;&#23637;&#65292;&#24418;&#25104;&#20102;&#25991;&#26412;&#29983;&#25104;3D&#20869;&#23481;&#30340;&#26032;&#20852;&#35805;&#39064;&#12290;&#29616;&#26377;&#30340;&#25991;&#26412;&#29983;&#25104;3D&#26041;&#27861;&#36890;&#24120;&#20351;&#29992;&#38544;&#24335;&#22330;&#26223;&#34920;&#31034;&#65292;&#36825;&#20123;&#34920;&#31034;&#20351;&#29992;&#20307;&#31215;&#28210;&#26579;&#23558;&#20960;&#20309;&#21644;&#22806;&#35266;&#32806;&#21512;&#22312;&#19968;&#36215;&#65292;&#23545;&#20110;&#24674;&#22797;&#26356;&#31934;&#32454;&#30340;&#20960;&#20309;&#21644;&#23454;&#29616;&#29031;&#29255;&#33324;&#36924;&#30495;&#30340;&#28210;&#26579;&#26159;&#27425;&#20248;&#30340;&#65307;&#22240;&#27492;&#65292;&#23427;&#20204;&#22312;&#20135;&#29983;&#39640;&#36136;&#37327;3D&#36164;&#20135;&#26041;&#38754;&#19981;&#22815;&#26377;&#25928;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;Fantasia3D&#30340;&#26032;&#26041;&#27861;&#65292;&#29992;&#20110;&#39640;&#36136;&#37327;&#25991;&#26412;&#29983;&#25104;3D&#20869;&#23481;&#12290;Fantasia3D&#30340;&#20851;&#38190;&#22312;&#20110;&#20960;&#20309;&#21644;&#22806;&#35266;&#30340;&#20998;&#31163;&#24314;&#27169;&#21644;&#23398;&#20064;&#12290;&#23545;&#20110;&#20960;&#20309;&#23398;&#20064;&#65292;&#25105;&#20204;&#20381;&#38752;&#28151;&#21512;&#22330;&#26223;&#34920;&#31034;&#65292;&#24182;&#24314;&#35758;&#23558;&#20174;&#34920;&#31034;&#20013;&#25552;&#21462;&#30340;&#34920;&#38754;&#27861;&#32447;&#32534;&#30721;&#20316;&#20026;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#36755;&#20837;&#12290;&#23545;&#20110;&#22806;&#35266;&#24314;&#27169;&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#31354;&#38388;&#21487;&#21464;&#21452;&#21521;&#21453;&#23556;&#29575;&#20998;&#24067;&#20989;&#25968;&#65288;SVBRDF&#65289;&#26469;&#20998;&#31163;&#26448;&#26009;&#21644;&#20809;&#29031;&#23646;&#24615;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20960;&#20309;&#32454;&#33410;&#21644;&#36924;&#30495;&#28210;&#26579;&#26041;&#38754;&#36229;&#36234;&#20102;&#29616;&#26377;&#25216;&#26415;&#65292;&#24182;&#19988;&#22312;&#20174;&#33258;&#28982;&#35821;&#35328;&#25991;&#26412;&#20013;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;3D&#20869;&#23481;&#26041;&#38754;&#26356;&#20026;&#26377;&#25928;&#21644;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automatic 3D content creation has achieved rapid progress recently due to the availability of pre-trained, large language models and image diffusion models, forming the emerging topic of text-to-3D content creation. Existing text-to-3D methods commonly use implicit scene representations, which couple the geometry and appearance via volume rendering and are suboptimal in terms of recovering finer geometries and achieving photorealistic rendering; consequently, they are less effective for generating high-quality 3D assets. In this work, we propose a new method of Fantasia3D for high-quality text-to-3D content creation. Key to Fantasia3D is the disentangled modeling and learning of geometry and appearance. For geometry learning, we rely on a hybrid scene representation, and propose to encode surface normal extracted from the representation as the input of the image diffusion model. For appearance modeling, we introduce the spatially varying bidirectional reflectance distribution function 
&lt;/p&gt;</description></item><item><title>MagicEye&#26159;&#19968;&#31181;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#33021;&#22815;&#35782;&#21035;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#23460;&#20869;&#22806;&#29289;&#20307;&#65292;&#25552;&#20379;&#38024;&#23545;&#35270;&#21147;&#21463;&#25439;&#20154;&#22763;&#30340;&#38754;&#37096;&#35782;&#21035;&#21644;&#36135;&#24065;&#35782;&#21035;&#27169;&#22359;&#65292;&#20197;&#21450;&#29992;&#20110;&#23548;&#33322;&#30340;GPS&#20256;&#24863;&#22120;&#12290;</title><link>http://arxiv.org/abs/2303.13863</link><description>&lt;p&gt;
MagicEye&#65306;&#19968;&#31181;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#33268;&#21147;&#20110;&#24110;&#21161;&#35270;&#21147;&#21463;&#25439;&#20154;&#22763;&#29420;&#31435;&#29983;&#27963;&#12290;
&lt;/p&gt;
&lt;p&gt;
MagicEye: An Intelligent Wearable Towards Independent Living of Visually Impaired. (arXiv:2303.13863v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13863
&lt;/p&gt;
&lt;p&gt;
MagicEye&#26159;&#19968;&#31181;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;&#65292;&#33021;&#22815;&#35782;&#21035;&#26085;&#24120;&#29983;&#27963;&#20013;&#30340;&#23460;&#20869;&#22806;&#29289;&#20307;&#65292;&#25552;&#20379;&#38024;&#23545;&#35270;&#21147;&#21463;&#25439;&#20154;&#22763;&#30340;&#38754;&#37096;&#35782;&#21035;&#21644;&#36135;&#24065;&#35782;&#21035;&#27169;&#22359;&#65292;&#20197;&#21450;&#29992;&#20110;&#23548;&#33322;&#30340;GPS&#20256;&#24863;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;&#38556;&#30861;&#20010;&#20307;&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#24120;&#24120;&#38754;&#20020;&#35768;&#22810;&#25361;&#25112;&#24615;&#38556;&#30861;&#65292;&#22914;&#35270;&#35273;&#38556;&#30861;&#21487;&#33021;&#20005;&#37325;&#24433;&#21709;&#19968;&#20010;&#20154;&#30340;&#24037;&#20316;&#12289;&#23548;&#33322;&#21644;&#29420;&#31435;&#29983;&#27963;&#33021;&#21147;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#25361;&#25112;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#19968;&#31181;&#26368;&#26032;&#30340;&#26234;&#33021;&#21487;&#31359;&#25140;&#35774;&#22791;MagicEye&#65292;&#26088;&#22312;&#24110;&#21161;&#35270;&#21147;&#21463;&#25439;&#20154;&#22763;&#12290;MagicEye&#37319;&#29992;&#20102;&#32463;&#36807;&#23450;&#21046;&#35757;&#32451;&#30340;&#22522;&#20110;CNN&#30340;&#29289;&#20307;&#26816;&#27979;&#27169;&#22411;&#65292;&#33021;&#22815;&#35782;&#21035;&#26085;&#24120;&#29983;&#27963;&#20013;&#32463;&#24120;&#36935;&#21040;&#30340;&#23460;&#20869;&#22806;&#21508;&#31181;&#29289;&#20307;&#12290;&#35813;&#31070;&#32463;&#32593;&#32476;&#20849;&#35745;35&#20010;&#31867;&#21035;&#65292;&#26088;&#22312;&#23454;&#29616;&#39640;&#25928;&#12289;&#39640;&#31934;&#24230;&#30340;&#29289;&#20307;&#26816;&#27979;&#12290;&#27492;&#22806;&#65292;&#35813;&#35774;&#22791;&#36824;&#37197;&#22791;&#20102;&#38754;&#37096;&#35782;&#21035;&#21644;&#36135;&#24065;&#35782;&#21035;&#27169;&#22359;&#65292;&#20026;&#35270;&#21147;&#21463;&#25439;&#20154;&#22763;&#25552;&#20379;&#23453;&#36149;&#30340;&#24110;&#21161;&#12290;&#27492;&#22806;&#65292;MagicEye&#36824;&#20855;&#26377;&#29992;&#20110;&#23548;&#33322;&#30340;GPS&#20256;&#24863;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Individuals with visual impairments often face a multitude of challenging obstacles in their daily lives. Vision impairment can severely impair a person's ability to work, navigate, and retain independence. This can result in educational limits, a higher risk of accidents, and a plethora of other issues. To address these challenges, we present MagicEye, a state-of-the-art intelligent wearable device designed to assist visually impaired individuals. MagicEye employs a custom-trained CNN-based object detection model, capable of recognizing a wide range of indoor and outdoor objects frequently encountered in daily life. With a total of 35 classes, the neural network employed by MagicEye has been specifically designed to achieve high levels of efficiency and precision in object detection. The device is also equipped with facial recognition and currency identification modules, providing invaluable assistance to the visually impaired. In addition, MagicEye features a GPS sensor for navigatio
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20803;&#23431;&#23449;&#20013;&#20351;&#29992;ChatGPT&#30340;&#21033;&#24330;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#21160;&#24577;&#21644;&#20010;&#24615;&#21270;&#30340;&#20307;&#39564;&#65292;&#20294;&#20063;&#24517;&#39035;&#32771;&#34385;&#38544;&#31169;&#12289;&#20559;&#35265;&#21644;&#36947;&#24503;&#31561;&#30456;&#20851;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13856</link><description>&lt;p&gt;
ChatGPT&#22312;&#20803;&#23431;&#23449;&#20013;&#30340;&#24212;&#29992;&#65306;&#25327;&#25937;&#32773;&#36824;&#26159;&#27585;&#28781;&#32773;?
&lt;/p&gt;
&lt;p&gt;
Unleasing ChatGPT on the Metaverse: Savior or Destroyer?. (arXiv:2303.13856v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13856
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#22312;&#20803;&#23431;&#23449;&#20013;&#20351;&#29992;ChatGPT&#30340;&#21033;&#24330;&#65292;&#23427;&#21487;&#20197;&#25552;&#20379;&#21160;&#24577;&#21644;&#20010;&#24615;&#21270;&#30340;&#20307;&#39564;&#65292;&#20294;&#20063;&#24517;&#39035;&#32771;&#34385;&#38544;&#31169;&#12289;&#20559;&#35265;&#21644;&#36947;&#24503;&#31561;&#30456;&#20851;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#34394;&#25311;&#29616;&#23454;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#8220;&#20803;&#23431;&#23449;&#8221;&#36825;&#20010;&#26032;&#20852;&#39046;&#22495;&#30340;&#20132;&#20114;&#26041;&#24335;&#21644;&#27785;&#28024;&#20307;&#39564;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#32780;&#20154;&#24037;&#26234;&#33021;&#25216;&#26415;&#21644;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#25216;&#26415;&#22312;&#36825;&#19968;&#39046;&#22495;&#20013;&#30340;&#24212;&#29992;&#21464;&#24471;&#26085;&#30410;&#37325;&#35201;&#12290;&#20854;&#20013;&#65292;&#19968;&#20010;&#34987;&#24191;&#27867;&#20351;&#29992;&#30340;&#24037;&#20855;&#26159;ChatGPT&#65292;&#36825;&#26159;OpenAI&#35757;&#32451;&#30340;&#19968;&#20010;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#12290;&#26412;&#25991;&#35814;&#32454;&#25506;&#35752;&#20102;&#22312;&#20803;&#23431;&#23449;&#20013;&#24341;&#20837;ChatGPT&#30340;&#21033;&#24330;&#65292;&#21253;&#25324;&#25945;&#32946;&#12289;&#23089;&#20048;&#12289;&#20010;&#24615;&#21270;&#21644;&#25903;&#25345;&#31561;&#26041;&#38754;&#30340;&#24212;&#29992;&#12290;&#34429;&#28982;&#36825;&#39033;&#25216;&#26415;&#21487;&#20197;&#25552;&#20379;&#21160;&#24577;&#21644;&#20010;&#24615;&#21270;&#30340;&#20307;&#39564;&#65292;&#20294;&#20063;&#24517;&#39035;&#32771;&#34385;&#38544;&#31169;&#12289;&#20559;&#35265;&#21644;&#36947;&#24503;&#31561;&#30456;&#20851;&#38382;&#39064;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35780;&#20272;&#36825;&#20123;&#26426;&#36935;&#21644;&#38556;&#30861;&#65292;&#24110;&#21161;&#35835;&#32773;&#29702;&#35299;ChatGPT&#23545;&#20803;&#23431;&#23449;&#21487;&#33021;&#20135;&#29983;&#30340;&#24433;&#21709;&#65292;&#20197;&#21450;&#22914;&#20309;&#26377;&#25928;&#22320;&#21033;&#29992;&#23427;&#21019;&#24314;&#26356;&#21152;&#27785;&#28024;&#21644;&#26377;&#36259;&#30340;&#34394;&#25311;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
The incorporation of artificial intelligence (AI) technology, and in particular natural language processing (NLP), is becoming increasingly vital for the development of immersive and interactive metaverse experiences. One such artificial intelligence tool that is gaining traction in the metaverse is ChatGPT, a large language model trained by OpenAI. The article delves into the pros and cons of utilizing ChatGPT for metaverse-based education, entertainment, personalization, and support. Dynamic and personalized experiences are possible with this technology, but there are also legitimate privacy, bias, and ethical issues to consider. This article aims to help readers understand the possible influence of ChatGPT on the metaverse and how it may be used to effectively create a more immersive and engaging virtual environment by evaluating these opportunities and obstacles.
&lt;/p&gt;</description></item><item><title>&#20026;&#20102;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#19981;&#21516;&#31038;&#20132;&#29305;&#24449;&#30340;&#20132;&#20114;&#20132;&#36890;&#22330;&#26223;&#19979;&#37319;&#21462;&#23433;&#20840;&#26377;&#25928;&#30340;&#26426;&#21160;&#26041;&#24335;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#20132;&#21487;&#25511;&#34892;&#20026;&#29983;&#25104;&#65288;SCBG&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#30495;&#23454;&#39550;&#39542;&#25968;&#25454;&#23454;&#29616;&#20102;&#36924;&#30495;&#32780;&#31867;&#20154;&#30340;&#36712;&#36857;&#29983;&#25104;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#36712;&#36857;&#30340;&#31036;&#35980;&#31243;&#24230;&#12290;</title><link>http://arxiv.org/abs/2303.13830</link><description>&lt;p&gt;
&#20132;&#36890;&#27169;&#25311;&#20013;&#30340;&#21487;&#32534;&#36753;&#39550;&#39542;&#35282;&#33394;&#65306;&#31038;&#20132;&#21487;&#25511;&#34892;&#20026;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
Editing Driver Character: Socially-Controllable Behavior Generation for Interactive Traffic Simulation. (arXiv:2303.13830v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13830
&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#19981;&#21516;&#31038;&#20132;&#29305;&#24449;&#30340;&#20132;&#20114;&#20132;&#36890;&#22330;&#26223;&#19979;&#37319;&#21462;&#23433;&#20840;&#26377;&#25928;&#30340;&#26426;&#21160;&#26041;&#24335;&#65292;&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#20132;&#21487;&#25511;&#34892;&#20026;&#29983;&#25104;&#65288;SCBG&#65289;&#27169;&#22411;&#65292;&#36890;&#36807;&#23398;&#20064;&#30495;&#23454;&#39550;&#39542;&#25968;&#25454;&#23454;&#29616;&#20102;&#36924;&#30495;&#32780;&#31867;&#20154;&#30340;&#36712;&#36857;&#29983;&#25104;&#65292;&#24182;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#36712;&#36857;&#30340;&#31036;&#35980;&#31243;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20132;&#36890;&#27169;&#25311;&#22312;&#35780;&#20272;&#21644;&#25913;&#36827;&#33258;&#21160;&#39550;&#39542;&#35268;&#21010;&#31995;&#32479;&#20013;&#36215;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#22312;&#20844;&#20849;&#36947;&#36335;&#19978;&#37096;&#32626;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#21518;&#65292;&#38656;&#35201;&#19982;&#20855;&#26377;&#19981;&#21516;&#31038;&#20132;&#20559;&#22909;&#65288;&#20363;&#22914;&#65292;&#33258;&#31169;&#25110;&#24428;&#24428;&#26377;&#31036;&#30340;&#20154;&#31867;&#39550;&#39542;&#21592;&#65289;&#30340;&#20154;&#31867;&#36947;&#36335;&#21442;&#19982;&#32773;&#36827;&#34892;&#20132;&#20114;&#12290;&#20026;&#20102;&#30830;&#20445;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#22312;&#19981;&#21516;&#30340;&#20132;&#20114;&#20132;&#36890;&#22330;&#26223;&#20013;&#37319;&#21462;&#23433;&#20840;&#26377;&#25928;&#30340;&#26426;&#21160;&#26041;&#24335;&#65292;&#25105;&#20204;&#24212;&#35813;&#33021;&#22815;&#22312;&#27169;&#25311;&#29615;&#22659;&#20013;&#35780;&#20272;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#19982;&#24102;&#26377;&#19981;&#21516;&#31038;&#20132;&#29305;&#24449;&#30340;&#21453;&#24212;&#20195;&#29702;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31038;&#20132;&#21487;&#25511;&#34892;&#20026;&#29983;&#25104;&#65288;SCBG&#65289;&#27169;&#22411;&#65292;&#23427;&#20801;&#35768;&#29992;&#25143;&#25351;&#23450;&#29983;&#25104;&#36712;&#36857;&#30340;&#31036;&#35980;&#31243;&#24230;&#65292;&#24182;&#36890;&#36807;&#20174;&#30495;&#23454;&#39550;&#39542;&#25968;&#25454;&#20013;&#23398;&#20064;&#26469;&#30830;&#20445;&#36924;&#30495;&#21644;&#31867;&#20154;&#30340;&#36712;&#36857;&#29983;&#25104;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#23450;&#20041;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21487;&#24494;&#24230;&#37327;&#65292;&#29992;&#20110;&#37327;&#21270;&#39550;&#39542;&#34892;&#20026;&#30340;&#31036;&#35980;&#31243;&#24230;&#65292;&#24182;&#21033;&#29992;&#36793;&#38469;&#21644;&#26465;&#20214;&#34892;&#20026;&#39044;&#27979;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traffic simulation plays a crucial role in evaluating and improving autonomous driving planning systems. After being deployed on public roads, autonomous vehicles need to interact with human road participants with different social preferences (e.g., selfish or courteous human drivers). To ensure that autonomous vehicles take safe and efficient maneuvers in different interactive traffic scenarios, we should be able to evaluate autonomous vehicles against reactive agents with different social characteristics in the simulation environment. We propose a socially-controllable behavior generation (SCBG) model for this purpose, which allows the users to specify the level of courtesy of the generated trajectory while ensuring realistic and human-like trajectory generation through learning from real-world driving data. Specifically, we define a novel and differentiable measure to quantify the level of courtesy of driving behavior, leveraging marginal and conditional behavior prediction models t
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;$k$NN&#25552;&#31034;&#65292;&#19968;&#31181;&#19981;&#38656;&#35201;&#26657;&#20934;&#23601;&#21487;&#20197;&#25512;&#29702;&#26368;&#36817;&#30456;&#37051;&#30340;&#31639;&#27861;&#65292;&#29992;&#26469;&#35299;&#20915;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#21644;&#20559;&#35265;&#38382;&#39064;&#65292;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13824</link><description>&lt;p&gt;
$k$NN&#25552;&#31034;&#65306;&#26080;&#38656;&#26657;&#20934;&#30340;&#26368;&#36817;&#30456;&#37051;&#25512;&#29702;&#65292;&#36229;&#36234;&#19978;&#19979;&#25991;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
$k$NN Prompting: Beyond-Context Learning with Calibration-Free Nearest Neighbor Inference. (arXiv:2303.13824v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;$k$NN&#25552;&#31034;&#65292;&#19968;&#31181;&#19981;&#38656;&#35201;&#26657;&#20934;&#23601;&#21487;&#20197;&#25512;&#29702;&#26368;&#36817;&#30456;&#37051;&#30340;&#31639;&#27861;&#65292;&#29992;&#26469;&#35299;&#20915;&#19978;&#19979;&#25991;&#23398;&#20064;&#30340;&#38480;&#21046;&#21644;&#20559;&#35265;&#38382;&#39064;&#65292;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#19978;&#19979;&#25991;&#23398;&#20064;&#20013;&#65292;&#23558;&#30446;&#26631;&#20219;&#21153;&#21046;&#23450;&#20026;&#22312;&#19978;&#19979;&#25991;&#28436;&#31034;&#30340;&#26465;&#20214;&#19979;&#23436;&#25104;&#25552;&#31034;&#23436;&#25104;&#65292;&#24050;&#25104;&#20026;LLM&#30340;&#20027;&#35201;&#29992;&#36884;&#12290;&#26412;&#25991;&#39318;&#20808;&#25259;&#38706;&#20102;&#36825;&#31181;&#20856;&#22411;&#29992;&#27861;&#30340;&#23454;&#38469;&#38382;&#39064;&#65292;&#30001;&#20110;&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#38480;&#21046;&#65292;&#23427;&#26080;&#27861;&#38543;&#30528;&#35757;&#32451;&#25968;&#25454;&#25193;&#23637;&#12290;&#27492;&#22806;&#65292;&#29616;&#26377;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;ICL&#36824;&#21463;&#21040;&#21508;&#31181;&#20559;&#35265;&#30340;&#24433;&#21709;&#65292;&#24182;&#38656;&#35201;&#31934;&#32454;&#30340;&#26657;&#20934;&#22788;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20004;&#20010;&#25361;&#25112;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#26377;&#25928;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;$k$NN&#25552;&#31034;&#65292;&#23427;&#39318;&#20808;&#20351;&#29992;&#35757;&#32451;&#25968;&#25454;&#26597;&#35810;LLM&#30340;&#20998;&#24067;&#24335;&#34920;&#31034;&#65292;&#28982;&#21518;&#36890;&#36807;&#31616;&#21333;&#22320;&#21442;&#32771;&#26368;&#36817;&#37051;&#26469;&#39044;&#27979;&#27979;&#35797;&#23454;&#20363;&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#23454;&#39564;&#26469;&#35777;&#26126;&#20854;&#20248;&#36234;&#24615;&#65306;1&#65289;&#26080;&#38656;&#26657;&#20934;&#65306;$k$NN&#25552;&#31034;&#19981;&#30452;&#25509;&#23558;LLM&#36755;&#20986;&#20998;&#24067;&#19982;&#29305;&#23450;&#20219;&#21153;&#26631;&#31614;&#31354;&#38388;&#23545;&#20934;&#65292;&#32780;&#26159;&#21033;&#29992;&#36825;&#31181;&#20998;&#24067;&#23558;&#27979;&#35797;&#21644;&#35757;&#32451;&#23454;&#20363;&#23545;&#20934;&#12290;&#23427;&#26174;&#30528;&#20248;&#20110;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In-Context Learning (ICL), which formulates target tasks as prompt completion conditioned on in-context demonstrations, has become the prevailing utilization of LLMs. In this paper, we first disclose an actual predicament for this typical usage that it can not scale up with training data due to context length restriction. Besides, existing works have shown that ICL also suffers from various biases and requires delicate calibration treatment. To address both challenges, we advocate a simple and effective solution, $k$NN Prompting, which first queries LLM with training data for distributed representations, then predicts test instances by simply referring to nearest neighbors. We conduct comprehensive experiments to demonstrate its two-fold superiority: 1) Calibration-Free: $k$NN Prompting does not directly align LLM output distribution with task-specific label space, instead leverages such distribution to align test and training instances. It significantly outperforms state-of-the-art ca
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#20803;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#35782;&#27169;&#22411;&#65292;&#20197;&#27979;&#37327;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20449;&#20219;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#24120;&#34987;&#24341;&#29992;&#21644;&#26368;&#22909;&#39564;&#35777;&#30340;&#20154;&#26426;&#21644;&#20154;&#26426;&#22120;&#20154;&#20449;&#20219;&#38382;&#21367;&#20197;&#21450;&#30456;&#20851;&#22240;&#32032;&#12290;</title><link>http://arxiv.org/abs/2303.13799</link><description>&lt;p&gt;
&#19968;&#31181;&#26032;&#30340;&#27169;&#22411;&#65306;&#23545;&#20154;&#26426;&#20449;&#20219;&#38382;&#21367;&#30340;&#20803;&#20998;&#26512;
&lt;/p&gt;
&lt;p&gt;
Converging Measures and an Emergent Model: A Meta-Analysis of Human-Automation Trust Questionnaires. (arXiv:2303.13799v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13799
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#20803;&#20998;&#26512;&#25552;&#20986;&#20102;&#19968;&#31181;&#20849;&#35782;&#27169;&#22411;&#65292;&#20197;&#27979;&#37327;&#20154;&#26426;&#20132;&#20114;&#20013;&#30340;&#20449;&#20219;&#65292;&#24182;&#30830;&#23450;&#20102;&#26368;&#24120;&#34987;&#24341;&#29992;&#21644;&#26368;&#22909;&#39564;&#35777;&#30340;&#20154;&#26426;&#21644;&#20154;&#26426;&#22120;&#20154;&#20449;&#20219;&#38382;&#21367;&#20197;&#21450;&#30456;&#20851;&#22240;&#32032;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27979;&#37327;&#20154;&#26426;&#20449;&#20219;&#30340;&#19968;&#20010;&#26174;&#33879;&#25361;&#25112;&#26159;&#23384;&#22312;&#22823;&#37327;&#30340;&#26500;&#24314;&#12289;&#27169;&#22411;&#21644;&#38382;&#21367;&#65292;&#36825;&#20123;&#38382;&#21367;&#39564;&#35777;&#39640;&#24230;&#21464;&#21270;&#12290;&#28982;&#32780;&#65292;&#25152;&#26377;&#20154;&#37117;&#21516;&#24847;&#65292;&#20449;&#20219;&#26159;&#25216;&#26415;&#25509;&#21463;&#12289;&#25345;&#32493;&#20351;&#29992;&#12289;&#27969;&#21033;&#24230;&#21644;&#22242;&#38431;&#21512;&#20316;&#30340;&#20851;&#38190;&#20803;&#32032;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#32463;&#36807;&#39564;&#35777;&#21644;&#21487;&#38752;&#30340;&#20449;&#20219;&#35843;&#26597;&#24037;&#20855;&#36827;&#34892;&#20803;&#20998;&#26512;&#65292;&#32508;&#21512;&#20102;&#20154;&#26426;&#20132;&#20114;&#20013;&#20449;&#20219;&#30340;&#20849;&#35782;&#27169;&#22411;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#20010;&#30446;&#26631;&#65292;&#36825;&#39033;&#24037;&#20316;&#30830;&#23450;&#20102;&#26368;&#24120;&#34987;&#24341;&#29992;&#21644;&#26368;&#22909;&#39564;&#35777;&#30340;&#20154;&#26426;&#21644;&#20154;&#26426;&#22120;&#20154;&#20449;&#20219;&#38382;&#21367;&#65292;&#20197;&#21450;&#24418;&#25104;&#36825;&#31181;&#20449;&#20219;&#30340;&#32500;&#24230;&#21644;&#21069;&#22240;&#12290;&#20026;&#20102;&#20943;&#23569;&#28151;&#28102;&#21644;&#26500;&#24314;&#25193;&#25955;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#38382;&#21367;&#26415;&#35821;&#20043;&#38388;&#30340;&#35814;&#32454;&#26144;&#23556;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23545;&#20351;&#29992;&#22810;&#22240;&#32032;&#35843;&#26597;&#24037;&#20855;&#30340;&#23454;&#39564;&#20013;&#20986;&#29616;&#30340;&#22238;&#24402;&#27169;&#22411;&#36827;&#34892;&#20102;&#20803;&#20998;&#26512;&#12290;&#22522;&#20110;&#36825;&#20010;&#20803;&#20998;&#26512;&#65292;
&lt;/p&gt;
&lt;p&gt;
A significant challenge to measuring human-automation trust is the amount of construct proliferation, models, and questionnaires with highly variable validation. However, all agree that trust is a crucial element of technological acceptance, continued usage, fluency, and teamwork. Herein, we synthesize a consensus model for trust in human-automation interaction by performing a meta-analysis of validated and reliable trust survey instruments. To accomplish this objective, this work identifies the most frequently cited and best-validated human-automation and human-robot trust questionnaires, as well as the most well-established factors, which form the dimensions and antecedents of such trust. To reduce both confusion and construct proliferation, we provide a detailed mapping of terminology between questionnaires. Furthermore, we perform a meta-analysis of the regression models that emerged from those experiments which used multi-factorial survey instruments. Based on this meta-analysis, 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#24179;&#30340;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#30149;&#20154;&#20934;&#21017;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#32771;&#34385;&#20102;&#21253;&#21547;&#21644;&#25490;&#38500;&#26631;&#20934;&#30340;&#23884;&#20837;&#24046;&#24322;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;&#20013;&#30340;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13790</link><description>&lt;p&gt;
&#36890;&#36807;&#30149;&#20154;&#20934;&#21017;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#23454;&#29616;&#20844;&#24179;&#30340;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;
&lt;/p&gt;
&lt;p&gt;
Towards Fair Patient-Trial Matching via Patient-Criterion Level Fairness Constraint. (arXiv:2303.13790v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13790
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#24179;&#30340;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#30149;&#20154;&#20934;&#21017;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#65292;&#32771;&#34385;&#20102;&#21253;&#21547;&#21644;&#25490;&#38500;&#26631;&#20934;&#30340;&#23884;&#20837;&#24046;&#24322;&#65292;&#26377;&#25928;&#22320;&#35299;&#20915;&#20102;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;&#20013;&#30340;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20020;&#24202;&#35797;&#39564;&#22312;&#26032;&#22411;&#27835;&#30103;&#26041;&#27861;&#30340;&#24320;&#21457;&#20013;&#19981;&#21487;&#25110;&#32570;&#65292;&#20294;&#30001;&#20110;&#25307;&#21215;&#21644;&#30041;&#23384;&#30149;&#20154;&#30340;&#38590;&#24230;&#65292;&#24448;&#24448;&#38590;&#20197;&#25307;&#21215;&#36275;&#22815;&#25968;&#37327;&#30340;&#21442;&#19982;&#32773;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#24050;&#32463;&#21019;&#24314;&#20102;&#22522;&#20110;&#28145;&#24230;&#23398;&#20064;&#26694;&#26550;&#30340;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;&#26041;&#27861;&#12290;&#36825;&#20123;&#26694;&#26550;&#20250;&#35745;&#31639;&#30149;&#20154;&#21644;&#20020;&#24202;&#35797;&#39564;&#30340;&#30456;&#20284;&#24230;&#65292;&#32771;&#34385;&#21253;&#21547;&#21644;&#25490;&#38500;&#26631;&#20934;&#20043;&#38388;&#30340;&#24046;&#24322;&#12290;&#36817;&#26399;&#30340;&#30740;&#31350;&#34920;&#26126;&#36825;&#20123;&#26694;&#26550;&#30340;&#24615;&#33021;&#20248;&#20110;&#26089;&#26399;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#33021;&#20250;&#23548;&#33268;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;&#20013;&#30340;&#20844;&#24179;&#24615;&#38382;&#39064;&#65292;&#24403;&#26576;&#20123;&#25935;&#24863;&#20154;&#32676;&#22312;&#20020;&#24202;&#35797;&#39564;&#20013;&#34987;&#20302;&#20272;&#26102;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#25968;&#25454;&#19981;&#23436;&#25972;&#25110;&#19981;&#20934;&#30830;&#65292;&#20174;&#32780;&#23545;&#24739;&#32773;&#36896;&#25104;&#28508;&#22312;&#21361;&#23475;&#12290;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20844;&#24179;&#30340;&#30149;&#20154;-&#35797;&#39564;&#21305;&#37197;&#26694;&#26550;&#65292;&#36890;&#36807;&#29983;&#25104;&#19968;&#20010;&#30149;&#20154;&#20934;&#21017;&#32423;&#21035;&#30340;&#20844;&#24179;&#24615;&#32422;&#26463;&#26469;&#35299;&#20915;&#19981;&#20844;&#24179;&#24615;&#38382;&#39064;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#32771;&#34385;&#20102;&#21253;&#21547;&#21644;&#25490;&#38500;&#26631;&#20934;&#30340;&#23884;&#20837;&#24046;&#24322;&#65292;&#22522;&#20110;&#23884;&#20837;&#24046;&#24322;&#21046;&#23450;&#20102;&#20844;&#24179;&#24615;&#32422;&#26463;&#12290;&#35813;&#26694;&#26550;&#30340;&#26377;&#25928;&#24615;&#22312;&#19968;&#20010;&#30495;&#23454;&#30340;&#20020;&#24202;&#35797;&#39564;&#25968;&#25454;&#38598;&#19978;&#24471;&#21040;&#20102;&#39564;&#35777;&#12290;
&lt;/p&gt;
&lt;p&gt;
Clinical trials are indispensable in developing new treatments, but they face obstacles in patient recruitment and retention, hindering the enrollment of necessary participants. To tackle these challenges, deep learning frameworks have been created to match patients to trials. These frameworks calculate the similarity between patients and clinical trial eligibility criteria, considering the discrepancy between inclusion and exclusion criteria. Recent studies have shown that these frameworks outperform earlier approaches. However, deep learning models may raise fairness issues in patient-trial matching when certain sensitive groups of individuals are underrepresented in clinical trials, leading to incomplete or inaccurate data and potential harm. To tackle the issue of fairness, this work proposes a fair patient-trial matching framework by generating a patient-criterion level fairness constraint. The proposed framework considers the inconsistency between the embedding of inclusion and e
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;GNN&#30340;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20248;&#21270;&#26381;&#21153;&#36136;&#37327;&#65292;&#35299;&#20915;ONTS&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13773</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#26041;&#27861;&#65306;&#23398;&#20064;&#28151;&#21512;&#25972;&#25968;&#27169;&#22411;&#30340;&#27934;&#35265;
&lt;/p&gt;
&lt;p&gt;
A Graph Neural Network Approach to Nanosatellite Task Scheduling: Insights into Learning Mixed-Integer Models. (arXiv:2303.13773v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13773
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#22522;&#20110;GNN&#30340;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#26041;&#27861;&#65292;&#20197;&#26356;&#22909;&#22320;&#20248;&#21270;&#26381;&#21153;&#36136;&#37327;&#65292;&#35299;&#20915;ONTS&#38382;&#39064;&#30340;&#22797;&#26434;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#22914;&#20309;&#21033;&#29992;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26356;&#26377;&#25928;&#22320;&#35843;&#24230;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#12290;&#22312;&#31163;&#32447;&#32435;&#31859;&#21355;&#26143;&#20219;&#21153;&#35843;&#24230;&#65288;ONTS&#65289;&#38382;&#39064;&#20013;&#65292;&#30446;&#26631;&#26159;&#25214;&#21040;&#22312;&#36712;&#36947;&#19978;&#25191;&#34892;&#20219;&#21153;&#30340;&#26368;&#20339;&#23433;&#25490;&#65292;&#21516;&#26102;&#32771;&#34385;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#26041;&#38754;&#30340;&#32771;&#34385;&#22240;&#32032;&#65292;&#22914;&#20248;&#20808;&#32423;&#65292;&#26368;&#23567;&#21644;&#26368;&#22823;&#28608;&#27963;&#20107;&#20214;&#65292;&#25191;&#34892;&#26102;&#38388;&#26694;&#26550;&#65292;&#21608;&#26399;&#21644;&#25191;&#34892;&#31383;&#21475;&#65292;&#20197;&#21450;&#21355;&#26143;&#30005;&#21147;&#36164;&#28304;&#21644;&#33021;&#37327;&#25910;&#38598;&#21644;&#31649;&#29702;&#30340;&#22797;&#26434;&#24615;&#30340;&#32422;&#26463;&#12290;ONTS&#38382;&#39064;&#24050;&#32463;&#20351;&#29992;&#20256;&#32479;&#30340;&#25968;&#23398;&#20844;&#24335;&#21644;&#31934;&#30830;&#26041;&#27861;&#36827;&#34892;&#20102;&#22788;&#29702;&#65292;&#20294;&#26159;&#23427;&#20204;&#22312;&#38382;&#39064;&#30340;&#25361;&#25112;&#24615;&#26696;&#20363;&#20013;&#30340;&#36866;&#29992;&#24615;&#26377;&#38480;&#12290;&#26412;&#30740;&#31350;&#32771;&#23519;&#20102;&#22312;&#36825;&#31181;&#24773;&#20917;&#19979;&#20351;&#29992;GNN&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#24050;&#32463;&#25104;&#21151;&#24212;&#29992;&#20110;&#35768;&#22810;&#20248;&#21270;&#38382;&#39064;&#65292;&#21253;&#25324;&#26053;&#34892;&#21830;&#38382;&#39064;&#65292;&#35843;&#24230;&#38382;&#39064;&#21644;&#35774;&#26045;&#25918;&#32622;&#38382;&#39064;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#23558;ONTS&#38382;&#39064;&#30340;MILP&#23454;&#20363;&#23436;&#20840;&#34920;&#31034;&#25104;&#20108;&#20998;&#22270;&#32593;&#32476;&#32467;&#26500;&#26469;&#24212;&#29992;GNN&#12290;
&lt;/p&gt;
&lt;p&gt;
This study investigates how to schedule nanosatellite tasks more efficiently using Graph Neural Networks (GNN). In the Offline Nanosatellite Task Scheduling (ONTS) problem, the goal is to find the optimal schedule for tasks to be carried out in orbit while taking into account Quality-of-Service (QoS) considerations such as priority, minimum and maximum activation events, execution time-frames, periods, and execution windows, as well as constraints on the satellite's power resources and the complexity of energy harvesting and management. The ONTS problem has been approached using conventional mathematical formulations and precise methods, but their applicability to challenging cases of the problem is limited. This study examines the use of GNNs in this context, which has been effectively applied to many optimization problems, including traveling salesman problems, scheduling problems, and facility placement problems. Here, we fully represent MILP instances of the ONTS problem in biparti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#65292;&#29992;&#20110;&#21516;&#26102;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#21644;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#25552;&#39640;&#26410;&#30693;&#23545;&#35937;&#22312;&#32972;&#26223;&#20013;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#26368;&#20339;&#26694;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2303.13769</link><description>&lt;p&gt;
&#26410;&#30693;&#21957;&#25506;&#22120;&#29992;&#20110;&#30446;&#26631;&#26816;&#27979;&#65306;&#19981;&#35201;&#23545;&#26410;&#30693;&#23545;&#35937;&#35270;&#32780;&#19981;&#35265;
&lt;/p&gt;
&lt;p&gt;
Unknown Sniffer for Object Detection: Don't Turn a Blind Eye to Unknown Objects. (arXiv:2303.13769v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13769
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#65292;&#29992;&#20110;&#21516;&#26102;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#36890;&#36807;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#21644;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#25552;&#39640;&#26410;&#30693;&#23545;&#35937;&#22312;&#32972;&#26223;&#20013;&#30340;&#26816;&#27979;&#20934;&#30830;&#29575;&#65292;&#24182;&#35299;&#20915;&#20102;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#26368;&#20339;&#26694;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#25552;&#20986;&#30340;&#24320;&#25918;&#19990;&#30028;&#30446;&#26631;&#21644;&#24320;&#25918;&#38598;&#26816;&#27979;&#22312;&#23547;&#25214;&#20174;&#26410;&#35265;&#36807;&#30340;&#29289;&#20307;&#24182;&#23558;&#20854;&#19982;&#24050;&#30693;&#31867;&#21035;&#21306;&#20998;&#24320;&#26041;&#38754;&#21462;&#24471;&#20102;&#31361;&#30772;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#23545;&#20174;&#24050;&#30693;&#31867;&#21035;&#21521;&#26410;&#30693;&#31867;&#21035;&#30340;&#30693;&#35782;&#20256;&#36882;&#30340;&#30740;&#31350;&#38656;&#35201;&#26356;&#28145;&#20837;&#65292;&#20174;&#32780;&#23548;&#33268;&#25506;&#27979;&#38544;&#34255;&#22312;&#32972;&#26223;&#20013;&#30340;&#26410;&#30693;&#29289;&#20307;&#30340;&#33021;&#21147;&#19981;&#36275;&#12290;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#26410;&#30693;&#21957;&#25506;&#22120;(UnSniffer)&#26469;&#23547;&#25214;&#26410;&#30693;&#21644;&#24050;&#30693;&#30340;&#30446;&#26631;&#12290;&#39318;&#20808;&#65292;&#24341;&#20837;&#24191;&#20041;&#29289;&#20307;&#32622;&#20449;&#24230;(GOC)&#20998;&#25968;&#65292;&#20165;&#20351;&#29992;&#24050;&#30693;&#31867;&#21035;&#26679;&#26412;&#36827;&#34892;&#30417;&#30563;&#21644;&#36991;&#20813;&#22312;&#32972;&#26223;&#20013;&#19981;&#36866;&#24403;&#22320;&#21387;&#21046;&#26410;&#30693;&#29289;&#20307;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#20174;&#24050;&#30693;&#29289;&#20307;&#23398;&#20064;&#21040;&#30340;&#36825;&#31181;&#32622;&#20449;&#24230;&#20998;&#25968;&#21487;&#20197;&#25512;&#24191;&#21040;&#26410;&#30693;&#29289;&#20307;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#36127;&#33021;&#37327;&#25233;&#21046;&#25439;&#22833;&#26469;&#36827;&#19968;&#27493;&#38480;&#21046;&#32972;&#26223;&#20013;&#38750;&#29289;&#20307;&#26679;&#26412;&#12290;&#25509;&#19979;&#26469;&#65292;&#22312;&#25512;&#26029;&#36807;&#31243;&#20013;&#30001;&#20110;&#32570;&#20047;&#23427;&#20204;&#22312;&#35757;&#32451;&#20013;&#30340;&#35821;&#20041;&#20449;&#24687;&#65292;&#38590;&#20197;&#33719;&#24471;&#27599;&#20010;&#26410;&#30693;&#30446;&#26631;&#30340;&#26368;&#20339;&#26694;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;
&lt;/p&gt;
&lt;p&gt;
The recently proposed open-world object and open-set detection achieve a breakthrough in finding never-seen-before objects and distinguishing them from class-known ones. However, their studies on knowledge transfer from known classes to unknown ones need to be deeper, leading to the scanty capability for detecting unknowns hidden in the background. In this paper, we propose the unknown sniffer (UnSniffer) to find both unknown and known objects. Firstly, the generalized object confidence (GOC) score is introduced, which only uses class-known samples for supervision and avoids improper suppression of unknowns in the background. Significantly, such confidence score learned from class-known objects can be generalized to unknown ones. Additionally, we propose a negative energy suppression loss to further limit the non-object samples in the background. Next, the best box of each unknown is hard to obtain during inference due to lacking their semantic information in training. To solve this is
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20107;&#20214;&#30340;&#26102;&#31354;&#25554;&#20540;&#26469;&#23454;&#29616;&#38543;&#26426;&#23610;&#24230;&#19979;&#30340;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;&#12290;&#26041;&#27861;&#21253;&#25324;&#20174;RGB&#24103;&#21644;&#20107;&#20214;&#30340;&#26597;&#35810;&#31354;&#38388;-&#26102;&#38388;&#22352;&#26631;&#21644;&#29305;&#24449;&#20013;&#23398;&#20064;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12290;</title><link>http://arxiv.org/abs/2303.13767</link><description>&lt;p&gt;
&#23398;&#20064;&#26102;&#31354;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#26469;&#23454;&#29616;&#20107;&#20214;&#24341;&#23548;&#30340;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;
&lt;/p&gt;
&lt;p&gt;
Learning Spatial-Temporal Implicit Neural Representations for Event-Guided Video Super-Resolution. (arXiv:2303.13767v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13767
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26694;&#26550;&#65292;&#21033;&#29992;&#20107;&#20214;&#30340;&#26102;&#31354;&#25554;&#20540;&#26469;&#23454;&#29616;&#38543;&#26426;&#23610;&#24230;&#19979;&#30340;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;&#12290;&#26041;&#27861;&#21253;&#25324;&#20174;RGB&#24103;&#21644;&#20107;&#20214;&#30340;&#26597;&#35810;&#31354;&#38388;-&#26102;&#38388;&#22352;&#26631;&#21644;&#29305;&#24449;&#20013;&#23398;&#20064;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20107;&#20214;&#30456;&#26426;&#24322;&#27493;&#24863;&#30693;&#24378;&#24230;&#21464;&#21270;&#65292;&#20135;&#29983;&#20855;&#26377;&#39640;&#21160;&#24577;&#33539;&#22260;&#21644;&#20302;&#24310;&#36831;&#30340;&#20107;&#20214;&#27969;&#12290;&#36825;&#28608;&#21457;&#20102;&#21033;&#29992;&#20107;&#20214;&#24341;&#23548;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#35270;&#39057;&#36229;&#20998;&#36776;&#29575;&#65288;VSR&#65289;&#20219;&#21153;&#30340;&#30740;&#31350;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#21033;&#29992;&#20107;&#20214;&#30340;&#39640;&#26102;&#24207;&#20998;&#36776;&#29575;&#24615;&#36136;&#65292;&#36890;&#36807;&#21033;&#29992;&#20107;&#20214;&#30340;&#26102;&#31354;&#25554;&#20540;&#26469;&#23454;&#29616;&#38543;&#26426;&#23610;&#24230;&#19979;&#30340;VSR&#12290;&#24403;&#24341;&#23548;VSR&#26102;&#65292;&#20107;&#20214;&#30340;&#26102;&#31354;&#20449;&#24687;&#30340;&#34920;&#31034;&#20855;&#26377;&#22256;&#38590;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#23558;&#20107;&#20214;&#30340;&#26102;&#31354;&#25554;&#20540;&#19982;VSR&#32467;&#21512;&#22312;&#19968;&#20010;&#32479;&#19968;&#30340;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24605;&#24819;&#26159;&#20174;RGB&#24103;&#21644;&#20107;&#20214;&#30340;&#26597;&#35810;&#31354;&#38388;-&#26102;&#38388;&#22352;&#26631;&#21644;&#29305;&#24449;&#20013;&#23398;&#20064;&#38544;&#24335;&#31070;&#32463;&#34920;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#20998;&#20026;&#19977;&#37096;&#20998;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#31354;&#26102;&#34701;&#21512;&#65288;STF&#65289;&#27169;&#22359;&#39318;&#20808;&#23398;&#20064;&#20107;&#20214;&#21644;RGB&#24103;&#30340;3D&#29305;&#24449;&#12290;&#28982;&#21518;&#65292;&#26102;&#22495;&#28388;&#27874;&#22120;&#65288;TF&#65289;&#27169;&#22359;&#35299;&#38145;&#20102;&#26356;&#22810;&#29305;&#24449;&#24182;&#32473;&#20986;&#20102;&#31934;&#32454;&#30340;VSR&#32467;&#26524;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;VSR&#25968;&#25454;&#38598;&#65292;&#20197;&#20415;&#28145;&#24230;&#23398;&#20064;&#30340;VSR&#30740;&#31350;&#20154;&#21592;&#21487;&#20197;&#35780;&#20272;&#20854;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Event cameras sense the intensity changes asynchronously and produce event streams with high dynamic range and low latency. This has inspired research endeavors utilizing events to guide the challenging video superresolution (VSR) task. In this paper, we make the first attempt to address a novel problem of achieving VSR at random scales by taking advantages of the high temporal resolution property of events. This is hampered by the difficulties of representing the spatial-temporal information of events when guiding VSR. To this end, we propose a novel framework that incorporates the spatial-temporal interpolation of events to VSR in a unified framework. Our key idea is to learn implicit neural representations from queried spatial-temporal coordinates and features from both RGB frames and events. Our method contains three parts. Specifically, the Spatial-Temporal Fusion (STF) module first learns the 3D features from events and RGB frames. Then, the Temporal Filter (TF) module unlocks mo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;</title><link>http://arxiv.org/abs/2303.13763</link><description>&lt;p&gt;
&#26080;&#38656;&#36793;&#32536;&#20294;&#20855;&#26377;&#32467;&#26500;&#24863;&#30693;&#24615;&#65306;&#20174;GNN&#21040;MLP&#30340;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#12290;
&lt;/p&gt;
&lt;p&gt;
Edge-free but Structure-aware: Prototype-Guided Knowledge Distillation from GNNs to MLPs. (arXiv:2303.13763v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23558;&#39640;&#31934;&#24230;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#22312;&#22270;&#20219;&#21153;&#20013;&#21387;&#32553;&#25104;&#20302;&#24310;&#36831;&#30340;&#22810;&#23618;&#24863;&#30693;&#22120;&#65288;MLP&#65289;&#24050;&#25104;&#20026;&#28909;&#38376;&#30740;&#31350;&#35838;&#39064;&#12290;&#20197;&#21069;&#30340;&#26041;&#27861;&#20250;&#23558;&#22270;&#30340;&#36793;&#32536;&#22788;&#29702;&#25104;&#39069;&#22806;&#30340;&#36755;&#20837;&#32473;MLP&#65292;&#20294;&#36825;&#26679;&#30340;&#22270;&#32467;&#26500;&#23545;&#20110;&#21508;&#31181;&#22330;&#26223;&#21487;&#33021;&#26080;&#27861;&#33719;&#24471;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21407;&#22411;&#24341;&#23548;&#30693;&#35782;&#33976;&#39311;&#65288;PGKD&#65289;&#26041;&#27861;&#65292;&#23427;&#19981;&#38656;&#35201;&#22270;&#24418;&#36793;&#32536;&#65292;&#20294;&#21487;&#20197;&#22312;&#19981;&#32771;&#34385;&#36793;&#32536;&#30340;&#24773;&#20917;&#19979;&#23398;&#20064;&#32467;&#26500;&#24863;&#30693;&#30340;MLP&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#20998;&#26512;&#20102;GNN&#25945;&#24072;&#20013;&#30340;&#22270;&#24418;&#32467;&#26500;&#20449;&#24687;&#65292;&#24182;&#36890;&#36807;&#21407;&#22411;&#22312;&#26080;&#36793;&#32536;&#35774;&#32622;&#20013;&#20174;GNN&#21040;MLP&#36827;&#34892;&#20102;&#30693;&#35782;&#33976;&#39311;&#12290;&#22312;&#27969;&#34892;&#30340;&#22270;&#24418;&#22522;&#20934;&#23454;&#39564;&#20013;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;&#25152;&#25552;&#20986;&#30340;PGKD&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distilling high-accuracy Graph Neural Networks~(GNNs) to low-latency multilayer perceptrons~(MLPs) on graph tasks has become a hot research topic. However, MLPs rely exclusively on the node features and fail to capture the graph structural information. Previous methods address this issue by processing graph edges into extra inputs for MLPs, but such graph structures may be unavailable for various scenarios. To this end, we propose a Prototype-Guided Knowledge Distillation~(PGKD) method, which does not require graph edges~(edge-free) yet learns structure-aware MLPs. Specifically, we analyze the graph structural information in GNN teachers, and distill such information from GNNs to MLPs via prototypes in an edge-free setting. Experimental results on popular graph benchmarks demonstrate the effectiveness and robustness of the proposed PGKD.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22270;&#22686;&#24378;&#26041;&#27861; (SAug)&#65292;&#36890;&#36807;&#35782;&#21035;&#22270;&#20013;&#30340;&#20013;&#24515;&#33410;&#28857;&#21644;&#23614;&#33410;&#28857;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22686;&#24378;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#23398;&#20064;&#23614;&#33410;&#28857;&#26102;&#12290;</title><link>http://arxiv.org/abs/2303.13757</link><description>&lt;p&gt;
&#32467;&#26500;&#19981;&#24179;&#34913;&#24863;&#30693;&#30340;&#22270;&#22686;&#24378;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Structural Imbalance Aware Graph Augmentation Learning. (arXiv:2303.13757v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13757
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22270;&#22686;&#24378;&#26041;&#27861; (SAug)&#65292;&#36890;&#36807;&#35782;&#21035;&#22270;&#20013;&#30340;&#20013;&#24515;&#33410;&#28857;&#21644;&#23614;&#33410;&#28857;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22686;&#24378;&#31574;&#30053;&#65292;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#22270;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#29305;&#21035;&#26159;&#22312;&#23398;&#20064;&#23614;&#33410;&#28857;&#26102;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#26426;&#22120;&#23398;&#20064; (GML) &#22312;&#33410;&#28857;&#20998;&#31867;&#12289;&#38142;&#25509;&#39044;&#27979;&#12289;&#22270;&#20998;&#31867;&#31561;&#26041;&#38754;&#21462;&#24471;&#20102;&#24040;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#23454;&#20013;&#30340;&#22270;&#24448;&#24448;&#20855;&#26377;&#32467;&#26500;&#19981;&#24179;&#34913;&#24615;&#65292;&#21363;&#21482;&#26377;&#23569;&#25968;&#20960;&#20010;&#20013;&#24515;&#33410;&#28857;&#20855;&#26377;&#26356;&#23494;&#38598;&#30340;&#23616;&#37096;&#32467;&#26500;&#21644;&#26356;&#39640;&#30340;&#24433;&#21709;&#21147;&#12290;&#36825;&#31181;&#19981;&#24179;&#34913;&#21487;&#33021;&#20250;&#25439;&#23475;&#29616;&#26377; GML &#27169;&#22411;&#30340;&#40065;&#26834;&#24615;&#65292;&#29305;&#21035;&#26159;&#22312;&#23398;&#20064;&#23614;&#33410;&#28857;&#26102;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22270;&#22686;&#24378;&#26041;&#27861; (SAug) &#26469;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#39318;&#20808;&#65292;&#35774;&#35745;&#20102;&#19968;&#31181;&#22522;&#20110;Pagerank&#30340;&#37319;&#26679;&#31574;&#30053;&#26469;&#35782;&#21035;&#22270;&#20013;&#30340;&#20013;&#24515;&#33410;&#28857;&#21644;&#23614;&#33410;&#28857;&#12290;&#20854;&#27425;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#36873;&#25321;&#24615;&#22686;&#24378;&#31574;&#30053;&#65292;&#23558;&#20013;&#24515;&#33410;&#28857;&#30340;&#22122;&#22768;&#37051;&#23621;&#25918;&#24323;&#22312;&#19968;&#20391;&#65292;&#24182;&#22312;&#21478;&#19968;&#20391;&#21457;&#29616;&#28508;&#22312;&#30340;&#37051;&#23621;&#21644;&#20026;&#23614;&#33410;&#28857;&#29983;&#25104;&#20266;&#37051;&#23621;&#12290;&#23427;&#36824;&#21487;&#20197;&#20943;&#36731;&#20004;&#31181;&#31867;&#22411;&#33410;&#28857;&#20043;&#38388;&#30340;&#32467;&#26500;&#19981;&#24179;&#34913;&#12290;&#26368;&#21518;&#65292;&#22312;&#22686;&#24378;&#21518;&#30340;&#22270;&#19978;&#37325;&#26032;&#35757;&#32451; GNN &#27169;&#22411;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;SAug &#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#29616;&#26377;&#27169;&#22411;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph machine learning (GML) has made great progress in node classification, link prediction, graph classification and so on. However, graphs in reality are often structurally imbalanced, that is, only a few hub nodes have a denser local structure and higher influence. The imbalance may compromise the robustness of existing GML models, especially in learning tail nodes. This paper proposes a selective graph augmentation method (SAug) to solve this problem. Firstly, a Pagerank-based sampling strategy is designed to identify hub nodes and tail nodes in the graph. Secondly, a selective augmentation strategy is proposed, which drops the noisy neighbors of hub nodes on one side, and discovers the latent neighbors and generates pseudo neighbors for tail nodes on the other side. It can also alleviate the structural imbalance between two types of nodes. Finally, a GNN model will be retrained on the augmented graph. Extensive experiments demonstrate that SAug can significantly improve the backb
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;ViT&#31639;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#23398;&#20064;&#23454;&#20363;&#30456;&#20851;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;</title><link>http://arxiv.org/abs/2303.13755</link><description>&lt;p&gt;
Sparsifiner&#65306;&#23398;&#20064;&#31232;&#30095;&#30340;&#23454;&#20363;&#30456;&#20851;&#27880;&#24847;&#21147;&#29992;&#20110;&#39640;&#25928;&#30340;&#35270;&#35273;Transformer
&lt;/p&gt;
&lt;p&gt;
Sparsifiner: Learning Sparse Instance-Dependent Attention for Efficient Vision Transformers. (arXiv:2303.13755v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13755
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;ViT&#31639;&#27861;&#30340;&#35745;&#31639;&#25104;&#26412;&#39640;&#30340;&#38382;&#39064;&#65292;&#24182;&#19988;&#36890;&#36807;&#23398;&#20064;&#23454;&#20363;&#30456;&#20851;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#26469;&#25552;&#39640;&#35745;&#31639;&#25928;&#29575;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#35273;Transformer&#30456;&#27604;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#22312;&#24615;&#33021;&#26041;&#38754;&#20855;&#26377;&#31454;&#20105;&#20248;&#21183;&#65292;&#20294;&#24448;&#24448;&#20276;&#38543;&#30528;&#39640;&#35745;&#31639;&#25104;&#26412;&#12290;&#20026;&#27492;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#36807;&#38480;&#21046;&#19968;&#23450;&#25968;&#37327;&#30340;&#31354;&#38388;&#30456;&#37051;&#20196;&#29260;&#26469;&#25506;&#32034;&#19981;&#21516;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#65292;&#20197;&#21152;&#36895;ViT&#30340;&#22810;&#22836;&#33258;&#27880;&#24847;&#21147;&#65288;MHSA&#65289;&#25805;&#20316;&#12290;&#20294;&#26159;&#65292;&#36825;&#31181;&#32467;&#26500;&#21270;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#23558;&#20196;&#29260;&#19982;&#20854;&#31354;&#38388;&#30456;&#20851;&#24615;&#30340;&#20196;&#29260;&#20043;&#38388;&#30340;&#20196;&#29260; - &#20196;&#29260;&#36830;&#25509;&#38480;&#21046;&#22312;&#20102;&#19968;&#23450;&#33539;&#22260;&#20869;&#65292;&#36825;&#19981;&#32771;&#34385;&#20174;&#23436;&#25972;&#30340;&#27880;&#24847;&#21147;&#25513;&#30721;&#20013;&#23398;&#20064;&#30340;&#35821;&#20041;&#36830;&#25509;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#35774;&#35745;&#19968;&#20010;&#36731;&#37327;&#32423;&#30340;&#36830;&#25509;&#24615;&#39044;&#27979;&#27169;&#22359;&#26469;&#23398;&#20064;&#23454;&#20363;&#30456;&#20851;&#30340;&#27880;&#24847;&#21147;&#27169;&#24335;&#12290;&#30452;&#35266;&#30340;&#35828;&#65292;&#22914;&#26524;&#35748;&#20026;&#29305;&#24449;&#22312;&#31354;&#38388;&#25110;&#35821;&#20041;&#19978;&#26159;&#30456;&#20851;&#30340;&#65292;&#21017;&#20004;&#20010;&#26631;&#35760;&#20855;&#26377;&#39640;&#30340;&#36830;&#25509;&#24471;&#20998;&#12290;&#30001;&#20110;&#27599;&#20010;&#26631;&#35760;&#21482;&#19982;&#23569;&#37327;&#20854;&#20182;&#26631;&#35760;&#30456;&#20851;&#65292;&#22240;&#27492;&#20108;&#20803;&#21270;&#36830;&#25509;&#25513;&#30721;&#36890;&#24120;&#26159;&#26377;&#25928;&#30340; &#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers (ViT) have shown their competitive advantages performance-wise compared to convolutional neural networks (CNNs) though they often come with high computational costs. To this end, previous methods explore different attention patterns by limiting a fixed number of spatially nearby tokens to accelerate the ViT's multi-head self-attention (MHSA) operations. However, such structured attention patterns limit the token-to-token connections to their spatial relevance, which disregards learned semantic connections from a full attention mask. In this work, we propose a novel approach to learn instance-dependent attention patterns, by devising a lightweight connectivity predictor module to estimate the connectivity score of each pair of tokens. Intuitively, two tokens have high connectivity scores if the features are considered relevant either spatially or semantically. As each token only attends to a small number of other tokens, the binarized connectivity masks are often ver
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;LONGNN&#65292;&#23427;&#37319;&#29992;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#65292;&#24182;&#35299;&#20915;&#20102;&#22266;&#23450;&#22810;&#39033;&#24335;&#22522;&#21644;&#38750;&#24402;&#19968;&#21270;&#22522;&#30784;&#25152;&#24102;&#26469;&#30340;&#32570;&#38519;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;&#22270;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2303.13750</link><description>&lt;p&gt;
LONGNN: &#20855;&#26377;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#30340;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
LONGNN: Spectral GNNs with Learnable Orthonormal Basis. (arXiv:2303.13750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;LONGNN&#65292;&#23427;&#37319;&#29992;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#65292;&#24182;&#35299;&#20915;&#20102;&#22266;&#23450;&#22810;&#39033;&#24335;&#22522;&#21644;&#38750;&#24402;&#19968;&#21270;&#22522;&#30784;&#25152;&#24102;&#26469;&#30340;&#32570;&#38519;&#65292;&#32463;&#23454;&#39564;&#35777;&#26126;&#20854;&#22312;&#21508;&#31181;&#22270;&#25968;&#25454;&#38598;&#19978;&#20855;&#26377;&#20248;&#24322;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#22823;&#37327;&#30340;&#35889;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#26041;&#27861;&#21033;&#29992;&#21487;&#23398;&#20064;&#31995;&#25968;&#30340;&#22810;&#39033;&#24335;&#22522;&#22312;&#35768;&#22810;&#33410;&#28857;&#32423;&#20219;&#21153;&#19978;&#23454;&#29616;&#20102;&#39030;&#32423;&#24615;&#33021;&#12290;&#34429;&#28982;&#24050;&#32463;&#25506;&#32034;&#20102;&#21508;&#31181;&#22810;&#39033;&#24335;&#22522;&#65292;&#20294;&#26159;&#27599;&#31181;&#26041;&#27861;&#37117;&#37319;&#29992;&#20102;&#22266;&#23450;&#30340;&#22810;&#39033;&#24335;&#22522;&#65292;&#21487;&#33021;&#19981;&#26159;&#32473;&#23450;&#22270;&#24418;&#30340;&#26368;&#20339;&#36873;&#25321;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#36825;&#20123;&#26041;&#27861;&#25152;&#35859;&#30340;&#36234;&#30028;&#38382;&#39064;&#65292;&#24182;&#34920;&#26126;&#36825;&#22312;&#23427;&#20204;&#19981;&#22826;&#31995;&#32479;&#21270;&#30340;&#27491;&#21017;&#21270;&#31574;&#30053;&#21644;&#38750;&#24402;&#19968;&#21270;&#22522;&#30784;&#19978;&#26377;&#25152;&#26681;&#28304;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#39318;&#27425;&#23581;&#35797;&#35299;&#20915;&#36825;&#20004;&#20010;&#38382;&#39064;&#12290;&#21033;&#29992;&#38597;&#21508;&#27604;&#22810;&#39033;&#24335;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#31181;&#26032;&#30340;&#20855;&#26377;&#21487;&#23398;&#20064;&#27491;&#20132;&#26631;&#20934;&#22522;&#30340;&#35889;GNN&#65292;LON-GNN&#65292;&#24182;&#35777;&#26126;&#20102;&#27491;&#21017;&#21270;&#31995;&#25968;&#29616;&#22312;&#31561;&#25928;&#20110;&#27491;&#21017;&#21270;&#25152;&#23398;&#28388;&#27874;&#20989;&#25968;&#30340;&#33539;&#25968;&#12290;&#25105;&#20204;&#22312;&#22810;&#26679;&#30340;&#22270;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#20197;&#35780;&#20272;LON-GNN&#30340;&#25311;&#21512;&#21644;&#27867;&#21270;&#33021;&#21147;&#65292;&#32467;&#26524;&#34920;&#26126;&#20854;&#20248;&#20110;&#20960;&#31181;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, a plethora of spectral graph neural networks (GNN) methods have utilized polynomial basis with learnable coefficients to achieve top-tier performances on many node-level tasks. Although various kinds of polynomial bases have been explored, each such method adopts a fixed polynomial basis which might not be the optimal choice for the given graph. Besides, we identify the so-called over-passing issue of these methods and show that it is somewhat rooted in their less-principled regularization strategy and unnormalized basis. In this paper, we make the first attempts to address these two issues. Leveraging Jacobi polynomials, we design a novel spectral GNN, LON-GNN, with Learnable OrthoNormal bases and prove that regularizing coefficients becomes equivalent to regularizing the norm of learned filter function now. We conduct extensive experiments on diverse graph datasets to evaluate the fitting and generalization capability of LON-GNN, where the results imply its superiori
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#24341;&#23548;&#26041;&#27861;DOODL&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#30495;&#23454;&#29983;&#25104;&#30340;&#20687;&#32032;&#19978;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#26469;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25351;&#23548;&#65292;&#24182;&#22312;&#35745;&#31639;&#21644;&#20154;&#31867;&#35780;&#20272;&#24230;&#37327;&#19978;&#20248;&#20110;&#19968;&#27493;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2303.13703</link><description>&lt;p&gt;
&#31471;&#21040;&#31471;&#25193;&#25955;&#28508;&#22312;&#20248;&#21270;&#25552;&#39640;&#20998;&#31867;&#22120;&#24341;&#23548;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
End-to-End Diffusion Latent Optimization Improves Classifier Guidance. (arXiv:2303.13703v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13703
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#20998;&#31867;&#22120;&#24341;&#23548;&#26041;&#27861;DOODL&#65292;&#23427;&#21487;&#20197;&#36890;&#36807;&#38024;&#23545;&#30495;&#23454;&#29983;&#25104;&#30340;&#20687;&#32032;&#19978;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#26469;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25351;&#23548;&#65292;&#24182;&#22312;&#35745;&#31639;&#21644;&#20154;&#31867;&#35780;&#20272;&#24230;&#37327;&#19978;&#20248;&#20110;&#19968;&#27493;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#31867;&#22120;&#25351;&#23548;&#8212;&#8212;&#21033;&#29992;&#22270;&#20687;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#26469;&#24341;&#23548;&#25193;&#25955;&#27169;&#22411;&#30340;&#29983;&#25104;&#8212;&#8212;&#26377;&#28508;&#21147;&#22823;&#24133;&#25193;&#23637;&#23545;&#22270;&#20687;&#29983;&#25104;&#21644;&#32534;&#36753;&#30340;&#21019;&#36896;&#24615;&#25511;&#21046;&#12290;&#28982;&#32780;&#65292;&#30446;&#21069;&#20998;&#31867;&#22120;&#25351;&#23548;&#35201;&#20040;&#38656;&#35201;&#35757;&#32451;&#26032;&#30340;&#22122;&#22768;&#24863;&#30693;&#27169;&#22411;&#20197;&#33719;&#24471;&#31934;&#30830;&#30340;&#26799;&#24230;&#65292;&#35201;&#20040;&#20351;&#29992;&#19968;&#27493;&#21435;&#22122;&#30340;&#36817;&#20284;&#26368;&#32456;&#29983;&#25104;&#29289;&#65292;&#24182;&#23548;&#33268;&#26799;&#24230;&#19981;&#23545;&#40784;&#21644;&#27425;&#20248;&#25511;&#21046;&#12290;&#25105;&#20204;&#24378;&#35843;&#20102;&#36825;&#31181;&#36817;&#20284;&#30340;&#32570;&#28857;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#25351;&#23548;&#26041;&#27861;&#65306;&#30452;&#25509;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#65288;DOODL&#65289;&#65292;&#23427;&#36890;&#36807;&#38024;&#23545;&#30495;&#23454;&#29983;&#25104;&#30340;&#20687;&#32032;&#19978;&#39044;&#35757;&#32451;&#20998;&#31867;&#22120;&#30340;&#26799;&#24230;&#20248;&#21270;&#25193;&#25955;&#28508;&#21464;&#26469;&#23454;&#29616;&#21363;&#25554;&#21363;&#29992;&#30340;&#25351;&#23548;&#65292;&#20351;&#29992;&#21487;&#36870;&#25193;&#25955;&#36807;&#31243;&#23454;&#29616;&#20869;&#23384;&#26377;&#25928;&#30340;&#21453;&#21521;&#20256;&#36882;&#12290;&#23637;&#31034;&#20102;&#26356;&#31934;&#30830;&#25351;&#23548;&#28508;&#21147;&#30340; DOODL &#22312;&#19981;&#21516;&#24418;&#24335;&#30340;&#25351;&#23548;&#30340;&#35745;&#31639;&#21644;&#20154;&#31867;&#35780;&#20272;&#24230;&#37327;&#19978;&#20248;&#20110;&#19968;&#27493;&#20998;&#31867;&#22120;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
Classifier guidance -- using the gradients of an image classifier to steer the generations of a diffusion model -- has the potential to dramatically expand the creative control over image generation and editing. However, currently classifier guidance requires either training new noise-aware models to obtain accurate gradients or using a one-step denoising approximation of the final generation, which leads to misaligned gradients and sub-optimal control. We highlight this approximation's shortcomings and propose a novel guidance method: Direct Optimization of Diffusion Latents (DOODL), which enables plug-and-play guidance by optimizing diffusion latents w.r.t. the gradients of a pre-trained classifier on the true generated pixels, using an invertible diffusion process to achieve memory-efficient backpropagation. Showcasing the potential of more precise guidance, DOODL outperforms one-step classifier guidance on computational and human evaluation metrics across different forms of guidanc
&lt;/p&gt;</description></item><item><title>OFA$^2$&#26159;&#19968;&#20010;&#22522;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#25628;&#32034;&#38454;&#27573;&#26500;&#24819;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#26435;&#34913;&#30446;&#26631;&#20043;&#38388;&#25214;&#21040;&#39640;&#25928;&#21644;&#22810;&#26679;&#21270;&#30340;&#23376;&#32593;&#32476;&#12290;</title><link>http://arxiv.org/abs/2303.13683</link><description>&lt;p&gt;
OFA$^2$: &#19968;&#31181;&#22522;&#20110;&#22810;&#30446;&#26631;&#30340;Once-for-All&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;
&lt;/p&gt;
&lt;p&gt;
OFA$^2$: A Multi-Objective Perspective for the Once-for-All Neural Architecture Search. (arXiv:2303.13683v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13683
&lt;/p&gt;
&lt;p&gt;
OFA$^2$&#26159;&#19968;&#20010;&#22522;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#27169;&#22411;&#65292;&#23427;&#36890;&#36807;&#23558;&#25628;&#32034;&#38454;&#27573;&#26500;&#24819;&#20026;&#19968;&#20010;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#20174;&#32780;&#22312;&#22810;&#20010;&#26435;&#34913;&#30446;&#26631;&#20043;&#38388;&#25214;&#21040;&#39640;&#25928;&#21644;&#22810;&#26679;&#21270;&#30340;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Once-for-All&#65288;OFA&#65289;&#26159;&#19968;&#20010;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#26694;&#26550;&#65292;&#26088;&#22312;&#36890;&#36807;&#20998;&#31163;&#35757;&#32451;&#21644;&#25628;&#32034;&#38454;&#27573;&#26469;&#35299;&#20915;&#20026;&#19981;&#21516;&#36164;&#28304;&#32422;&#26463;&#30340;&#35774;&#22791;&#25628;&#32034;&#39640;&#25928;&#26550;&#26500;&#30340;&#38382;&#39064;&#12290; Ofa&#31070;&#32463;&#32593;&#32476;&#30340;&#35757;&#32451;&#36807;&#31243;&#21482;&#38656;&#35201;&#36827;&#34892;&#19968;&#27425;&#65292;&#28982;&#21518;&#21487;&#20197;&#26681;&#25454;&#27599;&#20010;&#37096;&#32626;&#26041;&#26696;&#20174;&#27492;&#35757;&#32451;&#22909;&#30340;&#32593;&#32476;&#20013;&#25552;&#21462;&#22810;&#20010;&#23376;&#32593;&#32476;&#36827;&#34892;&#22810;&#27425;&#25628;&#32034;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#23558;&#25628;&#32034;&#38454;&#27573;&#26126;&#30830;&#26500;&#24819;&#20026;&#22810;&#30446;&#26631;&#20248;&#21270;&#38382;&#39064;&#65292;&#36827;&#19968;&#27493;&#23547;&#27714;&#25928;&#29575;&#12290; &#28982;&#21518;&#20351;&#29992;&#20219;&#20309;&#22810;&#30446;&#26631;&#36827;&#21270;&#31639;&#27861;&#65288;&#20363;&#22914;NSGA-II&#21644;SMS-EMOA&#65289;&#22312;&#25628;&#32034;&#38454;&#27573;&#22635;&#20805;Pareto&#21069;&#27839;&#65292;&#20854;&#20013;&#21253;&#21547;&#20855;&#26377;&#19981;&#21516;&#26435;&#34913;&#30340;&#39640;&#25928;&#39044;&#35757;&#32451;&#31070;&#32463;&#32467;&#26500;&#12290;&#25442;&#21477;&#35805;&#35828;&#65292;&#31070;&#32463;&#32593;&#32476;&#21482;&#38656;&#35757;&#32451;&#19968;&#27425;&#65292;&#28982;&#21518;&#20197;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#24418;&#24335;&#25191;&#34892;&#23376;&#32593;&#25628;&#32034;&#65292;&#24182;&#33719;&#24471;&#19968;&#32452;&#39640;&#25928;&#12289;&#39044;&#35757;&#32451;&#19988;&#22810;&#26679;&#21270;&#30340;&#23376;&#32593;&#32476;&#12290;
&lt;/p&gt;
&lt;p&gt;
Once-for-All (OFA) is a Neural Architecture Search (NAS) framework designed to address the problem of searching efficient architectures for devices with different resources constraints by decoupling the training and the searching stages. The computationally expensive process of training the OFA neural network is done only once, and then it is possible to perform multiple searches for subnetworks extracted from this trained network according to each deployment scenario. In this work we aim to give one step further in the search for efficiency by explicitly conceiving the search stage as a multi-objective optimization problem. A Pareto frontier is then populated with efficient, and already trained, neural architectures exhibiting distinct trade-offs among the conflicting objectives. This could be achieved by using any multi-objective evolutionary algorithm during the search stage, such as NSGA-II and SMS-EMOA. In other words, the neural network is trained once, the searching for subnetwo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#19982;&#22242;&#38431;&#36816;&#21160;&#20998;&#26512;&#32852;&#31995;&#36215;&#26469;&#65292;&#29305;&#21035;&#32771;&#34385;&#20837;&#20405;&#24335;&#28216;&#25103;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#30701;&#26399;&#27604;&#36187;&#31574;&#30053;&#21644;&#38271;&#26399;&#22242;&#38431;&#35268;&#21010;&#20004;&#20010;&#26041;&#21521;&#65292;&#20026;MAS&#30340;&#23454;&#29616;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;</title><link>http://arxiv.org/abs/2303.13660</link><description>&lt;p&gt;
&#21576;&#29616;&#22242;&#38431;&#36816;&#21160;&#20998;&#26512;&#20013;&#30340;&#22810;&#26234;&#33021;&#20307;&#25361;&#25112;
&lt;/p&gt;
&lt;p&gt;
Presenting Multiagent Challenges in Team Sports Analytics. (arXiv:2303.13660v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13660
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#19982;&#22242;&#38431;&#36816;&#21160;&#20998;&#26512;&#32852;&#31995;&#36215;&#26469;&#65292;&#29305;&#21035;&#32771;&#34385;&#20837;&#20405;&#24335;&#28216;&#25103;&#65292;&#24182;&#23558;&#20854;&#20998;&#20026;&#30701;&#26399;&#27604;&#36187;&#31574;&#30053;&#21644;&#38271;&#26399;&#22242;&#38431;&#35268;&#21010;&#20004;&#20010;&#26041;&#21521;&#65292;&#20026;MAS&#30340;&#23454;&#29616;&#21644;&#21457;&#23637;&#25552;&#20379;&#20102;&#26426;&#20250;&#21644;&#25361;&#25112;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#22242;&#38431;&#36816;&#21160;&#20998;&#26512;&#39046;&#22495;&#20013;&#30340;&#33509;&#24178;&#25361;&#25112;&#21644;&#26426;&#36935;&#19982;&#22810;&#26234;&#33021;&#20307;&#31995;&#32479;&#65288;MAS&#65289;&#30340;&#20851;&#38190;&#30740;&#31350;&#39046;&#22495;&#30456;&#32852;&#31995;&#12290;&#25105;&#20204;&#29305;&#21035;&#32771;&#34385;&#20837;&#20405;&#24335;&#28216;&#25103;&#65292;&#21363;&#29699;&#21592;&#20837;&#20405;&#23545;&#26041;&#22242;&#38431;&#39046;&#22320;&#24182;&#21487;&#20197;&#22312;&#29699;&#22330;&#19978;&#30340;&#20219;&#20309;&#22320;&#26041;&#36827;&#34892;&#20114;&#21160;&#30340;&#36816;&#21160;&#39033;&#30446;&#65292;&#22914;&#20912;&#29699;&#12289;&#36275;&#29699;&#21644;&#31726;&#29699;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;MAS&#26377;&#33021;&#21147;&#30740;&#31350;&#20837;&#20405;&#22411;&#28216;&#25103;&#65292;&#24182;&#23558;&#20351;MAS&#21644;&#36816;&#21160;&#20998;&#26512;&#39046;&#22495;&#21463;&#30410;&#12290;&#25105;&#20204;&#30340;&#35752;&#35770;&#37325;&#28857;&#24378;&#35843;&#20102;MAS&#23454;&#29616;&#21644;&#36827;&#19968;&#27493;&#24320;&#21457;&#30340;&#20004;&#20010;&#26041;&#21521;&#65306;&#30701;&#26399;&#30340;&#27604;&#36187;&#31574;&#30053;&#65288;&#25945;&#32451;&#65289;&#21644;&#38271;&#26399;&#30340;&#22242;&#38431;&#35268;&#21010;&#65288;&#31649;&#29702;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper draws correlations between several challenges and opportunities within the area of team sports analytics and key research areas within multiagent systems (MAS). We specifically consider invasion games, defined as sports where players invade the opposing team's territory and can interact anywhere on a playing surface such as ice hockey, soccer, and basketball. We argue that MAS is well-equipped to study invasion games and will benefit both MAS and sports analytics fields. Our discussion highlights areas for MAS implementation and further development along two axes: short-term in-game strategy (coaching) and long-term team planning (management).
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#26500;&#24314;&#39046;&#22495;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#24605;&#36335;&#21644;&#21407;&#21017;&#65292;&#36890;&#36807;&#30740;&#31350;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#21644;&#31995;&#32479;&#32423;&#20998;&#24067;&#32593;&#32476;&#36890;&#20449;&#12289;&#36882;&#24402;&#21644;&#30701;&#26399;&#25299;&#25169;&#21464;&#21270;&#65292;&#20026;&#24314;&#31435;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#23453;&#36149;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2303.13651</link><description>&lt;p&gt;
&#24314;&#31435;&#20154;&#24037;&#31070;&#32463;&#30005;&#36335;&#29992;&#20110;&#39046;&#22495;&#36890;&#29992;&#35748;&#30693;&#65306;&#33041;&#21551;&#21457;&#24335;&#31995;&#32479;&#32423;&#26550;&#26500;&#20837;&#38376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Building artificial neural circuits for domain-general cognition: a primer on brain-inspired systems-level architecture. (arXiv:2303.13651v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13651
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20851;&#20110;&#26500;&#24314;&#39046;&#22495;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#30340;&#24605;&#36335;&#21644;&#21407;&#21017;&#65292;&#36890;&#36807;&#30740;&#31350;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#21644;&#31995;&#32479;&#32423;&#20998;&#24067;&#32593;&#32476;&#36890;&#20449;&#12289;&#36882;&#24402;&#21644;&#30701;&#26399;&#25299;&#25169;&#21464;&#21270;&#65292;&#20026;&#24314;&#31435;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#23453;&#36149;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#26377;&#19968;&#31995;&#21015;&#30340;&#21162;&#21147;&#33268;&#21147;&#20110;&#24314;&#31435;&#33021;&#22815;&#35299;&#20915;&#24191;&#27867;&#35748;&#30693;&#20219;&#21153;&#19988;&#26080;&#38656;&#22312;&#21508;&#20010;&#38382;&#39064;&#31354;&#38388;&#21644;&#39046;&#22495;&#36827;&#34892;&#31934;&#32454;&#35843;&#25972;&#30340;&#36890;&#29992;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#65292;&#20197;&#26500;&#24314;&#39046;&#22495;&#36890;&#29992;&#20154;&#24037;&#26234;&#33021;&#12290;&#20026;&#20102;&#20570;&#21040;&#36825;&#19968;&#28857;&#65292;&#27169;&#22411;&#38656;&#35201;&#36866;&#24403;&#30340;&#20808;&#39564;&#21450;&#24402;&#32435;&#20559;&#35265;&#65292;&#35757;&#32451;&#24471;&#21040;&#30340;&#27169;&#22411;&#21487;&#20197;&#25512;&#24191;&#21040;&#23547;&#25214;&#26032;&#38382;&#39064;&#31354;&#38388;&#30340;&#20363;&#23376;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#29983;&#29289;&#31070;&#32463;&#32593;&#32476;&#36171;&#20104;&#20854;&#26580;&#24615;&#35748;&#30693;&#21151;&#33021;&#30340;&#26631;&#24535;&#24615;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#29305;&#23450;&#30340;&#31995;&#32479;&#23618;&#20998;&#24067;&#32593;&#32476;&#36890;&#20449;&#21450;&#36882;&#24402;&#30340;&#20316;&#29992;&#65292;&#27492;&#22806;&#36824;&#35752;&#35770;&#20102;&#30701;&#26399;&#25299;&#25169;&#21464;&#21270;&#22312;&#39640;&#25928;&#23616;&#37096;&#35745;&#31639;&#26041;&#38754;&#30340;&#20316;&#29992;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#21464;&#24471;&#36234;&#26469;&#36234;&#22797;&#26434;&#30340;&#26102;&#20505;&#65292;&#36825;&#20123;&#21407;&#21017;&#21487;&#33021;&#20250;&#23545;&#36825;&#20010;&#22797;&#26434;&#19988;&#21160;&#24577;&#30340;&#39046;&#22495;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is a concerted effort to build domain-general artificial intelligence in the form of universal neural network models with sufficient computational flexibility to solve a wide variety of cognitive tasks but without requiring fine-tuning on individual problem spaces and domains. To do this, models need appropriate priors and inductive biases, such that trained models can generalise to out-of-distribution examples and new problem sets. Here we provide an overview of the hallmarks endowing biological neural networks with the functionality needed for flexible cognition, in order to establish which features might also be important to achieve similar functionality in artificial systems. We specifically discuss the role of system-level distribution of network communication and recurrence, in addition to the role of short-term topological changes for efficient local computation. As machine learning models become more complex, these principles may provide valuable directions in an otherwis
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#21551;&#21457;&#24335;&#35268;&#21010;&#20316;&#20026;&#23450;&#29702;&#35777;&#26126;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#23450;&#29702;&#35777;&#26126;&#25552;&#21319;&#21551;&#21457;&#24335;(TPLH)&#35268;&#21010;&#22120;&#65292;&#22312;&#24773;&#20917;&#26641;&#20013;&#25628;&#32034;&#30701;&#30340;&#35745;&#21010;&#65292;&#24182;&#20943;&#23569;&#25506;&#35752;&#30340;&#29366;&#24577;&#25968;&#37327;&#12290;</title><link>http://arxiv.org/abs/2303.13638</link><description>&lt;p&gt;
&#29992;&#21551;&#21457;&#24335;&#35268;&#21010;&#20316;&#20026;&#23450;&#29702;&#35777;&#26126;
&lt;/p&gt;
&lt;p&gt;
Planning as Theorem Proving with Heuristics. (arXiv:2303.13638v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13638
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#29992;&#21551;&#21457;&#24335;&#35268;&#21010;&#20316;&#20026;&#23450;&#29702;&#35777;&#26126;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#24320;&#21457;&#19968;&#31181;&#23450;&#29702;&#35777;&#26126;&#25552;&#21319;&#21551;&#21457;&#24335;(TPLH)&#35268;&#21010;&#22120;&#65292;&#22312;&#24773;&#20917;&#26641;&#20013;&#25628;&#32034;&#30701;&#30340;&#35745;&#21010;&#65292;&#24182;&#20943;&#23569;&#25506;&#35752;&#30340;&#29366;&#24577;&#25968;&#37327;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#24773;&#22659;&#28436;&#31639;&#20013;&#23558;&#35745;&#21010;&#20316;&#20026;&#23450;&#29702;&#35777;&#26126;&#22312;50&#24180;&#21069;&#34987;&#25918;&#24323;&#65292;&#22240;&#20026;&#36825;&#26159;&#19968;&#20010;&#19981;&#21487;&#33021;&#23436;&#25104;&#30340;&#39033;&#30446;&#12290;&#20294;&#26159;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#23450;&#29702;&#35777;&#26126;&#25552;&#21319;&#21551;&#21457;&#24335;(TPLH)&#35268;&#21010;&#22120;&#65292;&#23427;&#20351;&#29992;A*&#25628;&#32034;&#31639;&#27861;&#22312;&#24773;&#20917;&#26641;&#20013;&#25628;&#32034;&#35745;&#21010;&#12290;&#23427;&#30001;&#22522;&#20110;&#21024;&#38500;&#26494;&#24347;&#30340;&#19982;&#39046;&#22495;&#26080;&#20851;&#30340;&#21551;&#21457;&#24335;&#25511;&#21046;&#12290;&#25105;&#20204;&#23558;TPLH&#19982;Fast Downward&#65288;FD&#65289;&#21644;Best First Width Search&#65288;BFWS&#65289;&#35268;&#21010;&#22120;&#22312;&#20960;&#20010;&#26631;&#20934;&#22522;&#20934;&#27979;&#35797;&#20013;&#36827;&#34892;&#27604;&#36739;&#12290;&#30001;&#20110;&#25105;&#20204;&#30340;&#21551;&#21457;&#24335;&#21151;&#33021;&#23454;&#29616;&#26410;&#32463;&#20248;&#21270;&#65292;TPLH&#27604;FD&#21644;BFWS&#24930;&#12290;&#20294;&#23427;&#20250;&#35745;&#31639;&#20986;&#26356;&#30701;&#30340;&#35745;&#21010;&#65292;&#24182;&#20943;&#23569;&#20102;&#25506;&#35752;&#30340;&#29366;&#24577;&#25968;&#37327;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20197;&#21069;&#22312;&#30693;&#35782;&#34920;&#31034;&#21644;&#25512;&#29702;&#39046;&#22495;&#20869;&#36827;&#34892;&#35268;&#21010;&#30340;&#30740;&#31350;&#65292;&#24182;&#30830;&#23450;&#20102;&#30456;&#20851;&#26041;&#21521;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#34920;&#26126;&#24773;&#22659;&#28436;&#31639;&#20013;&#30340;&#28436;&#32462;&#24335;&#25552;&#21319;&#21551;&#21457;&#24335;&#35268;&#21010;&#23454;&#38469;&#19978;&#26159;&#21487;&#20197;&#23436;&#25104;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Planning as theorem proving in situation calculus was abandoned 50 years ago as an impossible project. But we have developed a Theorem Proving Lifted Heuristic (TPLH) planner that searches for a plan in a tree of situations using the A* search algorithm. It is controlled by a delete relaxation-based domain independent heuristic. We compare TPLH with Fast Downward (FD) and Best First Width Search (BFWS) planners over several standard benchmarks. Since our implementation of the heuristic function is not optimized, TPLH is slower than FD and BFWS. But it computes shorter plans, and it explores fewer states. We discuss previous research on planning within KR\&amp;R and identify related directions. Thus, we show that deductive lifted heuristic planning in situation calculus is actually doable.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#20351;&#29992;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#25512;&#26029;&#20986;&#24515;&#29575;&#21464;&#24322;&#24615;&#12290;&#22312;&#22823;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#21333;&#29420;&#20351;&#29992;&#20449;&#21495;&#22788;&#29702;&#25110;&#26426;&#22120;&#23398;&#20064;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13637</link><description>&lt;p&gt;
&#21033;&#29992;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#39640;&#25928;&#30452;&#25509;&#25512;&#26029;&#24515;&#29575;&#21464;&#24322;&#24615;
&lt;/p&gt;
&lt;p&gt;
Efficient and Direct Inference of Heart Rate Variability using Both Signal Processing and Machine Learning. (arXiv:2303.13637v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13637
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#20351;&#29992;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#32467;&#21512;&#30340;&#26041;&#27861;&#65292;&#30452;&#25509;&#25512;&#26029;&#20986;&#24515;&#29575;&#21464;&#24322;&#24615;&#12290;&#22312;&#22823;&#25968;&#25454;&#38598;&#30340;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#34920;&#29616;&#20986;&#27604;&#21333;&#29420;&#20351;&#29992;&#20449;&#21495;&#22788;&#29702;&#25110;&#26426;&#22120;&#23398;&#20064;&#26356;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24515;&#29575;&#21464;&#24322;&#24615;(HRV)&#27979;&#37327;&#36830;&#32493;&#24515;&#36339;&#26102;&#38388;&#30340;&#21464;&#21270;&#65292;&#26159;&#36523;&#24515;&#20581;&#24247;&#30340;&#20027;&#35201;&#25351;&#26631;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#21487;&#20197;&#20351;&#29992;&#20809;&#30005;&#23481;&#31215;&#25551;&#35760;&#20202;(PPG)&#20256;&#24863;&#22120;&#25512;&#26029;HRV&#12290;&#28982;&#32780;&#65292;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#23384;&#22312;&#36739;&#39640;&#30340;&#35823;&#24046;&#65292;&#22240;&#20026;&#23427;&#20204;&#20165;&#20351;&#29992;&#20449;&#21495;&#22788;&#29702;&#25110;&#26426;&#22120;&#23398;&#20064;(ML)&#65292;&#25110;&#32773;&#22240;&#20026;&#23427;&#20204;&#38388;&#25509;&#25512;&#26029;HRV&#65292;&#25110;&#32773;&#22240;&#20026;&#32570;&#23569;&#22823;&#25968;&#25454;&#38598;&#12290;&#35768;&#22810;&#20808;&#21069;&#30340;&#30740;&#31350;&#20063;&#21487;&#33021;&#38656;&#35201;&#22823;&#30340;ML&#27169;&#22411;&#12290;&#20302;&#30340;&#20934;&#30830;&#29575;&#21644;&#22823;&#30340;&#27169;&#22411;&#22823;&#23567;&#38480;&#21046;&#20102;&#23427;&#20204;&#22312;&#23567;&#22411;&#23884;&#20837;&#24335;&#35774;&#22791;&#21644;&#26410;&#26469;&#21487;&#33021;&#22312;&#21307;&#30103;&#20445;&#20581;&#20013;&#30340;&#24212;&#29992;&#12290;&#20026;&#20102;&#35299;&#20915;&#19978;&#36848;&#38382;&#39064;&#65292;&#25105;&#20204;&#39318;&#20808;&#25910;&#38598;&#20102;&#19968;&#20010;&#22823;&#30340;PPG&#20449;&#21495;&#21644;HRV&#30340;&#22522;&#26412;&#20107;&#23454;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#21644;ML&#30340;HRV&#27169;&#22411;&#65292;&#30452;&#25509;&#25512;&#26029;HRV&#12290;&#35780;&#20272;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#30340;&#35823;&#24046;&#22312;3.5%&#21040;25.7%&#20043;&#38388;&#65292;&#24182;&#19988;&#20248;&#20110;&#21333;&#29420;&#20351;&#29992;&#20449;&#21495;&#22788;&#29702;&#25110;ML&#30340;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#19981;&#21516;&#30340;&#27169;&#22411;&#30340;&#22823;&#23567;&#21644;&#35745;&#31639;&#22797;&#26434;&#24230;&#30340;&#24179;&#34913;&#12290;
&lt;/p&gt;
&lt;p&gt;
Heart Rate Variability (HRV) measures the variation of the time between consecutive heartbeats and is a major indicator of physical and mental health. Recent research has demonstrated that photoplethysmography (PPG) sensors can be used to infer HRV. However, many prior studies had high errors because they only employed signal processing or machine learning (ML), or because they indirectly inferred HRV, or because there lacks large training datasets. Many prior studies may also require large ML models. The low accuracy and large model sizes limit their applications to small embedded devices and potential future use in healthcare. To address the above issues, we first collected a large dataset of PPG signals and HRV ground truth. With this dataset, we developed HRV models that combine signal processing and ML to directly infer HRV. Evaluation results show that our method had errors between 3.5% to 25.7% and outperformed signal-processing-only and ML-only methods. We also explored differe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;PPG&#24515;&#29575;&#20272;&#35745;&#25216;&#26415;&#24212;&#29992;&#20110;&#20302;&#21151;&#32791;&#21644;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#20013;&#65292;&#22312;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#25104;&#21151;&#23558;PPG&#37319;&#26679;&#39057;&#29575;&#38477;&#33267;&#20165;25Hz&#65292;&#24182;&#25552;&#39640;&#20102;&#24515;&#29575;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#20063;&#20943;&#23567;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#22823;&#23567;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#23567;&#12290;</title><link>http://arxiv.org/abs/2303.13636</link><description>&lt;p&gt;
&#22522;&#20110;&#39640;&#25928;&#20256;&#24863;&#22120;&#37319;&#26679;&#21644;&#23398;&#20064;&#27169;&#22411;&#30340;PPG&#24515;&#29575;&#20272;&#35745;
&lt;/p&gt;
&lt;p&gt;
PPG-based Heart Rate Estimation with Efficient Sensor Sampling and Learning Models. (arXiv:2303.13636v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13636
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#23558;PPG&#24515;&#29575;&#20272;&#35745;&#25216;&#26415;&#24212;&#29992;&#20110;&#20302;&#21151;&#32791;&#21644;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#20013;&#65292;&#22312;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#30340;&#22522;&#30784;&#19978;&#65292;&#25104;&#21151;&#23558;PPG&#37319;&#26679;&#39057;&#29575;&#38477;&#33267;&#20165;25Hz&#65292;&#24182;&#25552;&#39640;&#20102;&#24515;&#29575;&#20272;&#35745;&#30340;&#31934;&#24230;&#65292;&#21516;&#26102;&#20063;&#20943;&#23567;&#20102;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#22823;&#23567;&#65292;&#20351;&#24471;&#27169;&#22411;&#26356;&#23567;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#22312;&#21487;&#31359;&#25140;&#35774;&#22791;&#20013;&#23884;&#20837;&#30340;PPG&#20256;&#24863;&#22120;&#21487;&#20197;&#39640;&#31934;&#24230;&#22320;&#20272;&#35745;&#24515;&#29575;&#12290;&#28982;&#32780;&#65292;&#23613;&#31649;&#20043;&#21069;&#30340;&#30740;&#31350;&#21162;&#21147;&#65292;&#23558;&#22522;&#20110;PPG&#20256;&#24863;&#22120;&#30340;&#24515;&#29575;&#20272;&#35745;&#24212;&#29992;&#20110;&#23884;&#20837;&#24335;&#35774;&#22791;&#20173;&#38754;&#20020;&#30528;&#39640;&#33021;&#32791;&#30340;&#39640;&#39057;PPG&#37319;&#26679;&#21644;&#36164;&#28304;&#23494;&#38598;&#22411;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#25361;&#25112;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#26088;&#22312;&#25506;&#32034;&#26356;&#36866;&#21512;&#20302;&#21151;&#32791;&#21644;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#30340;&#24515;&#29575;&#20272;&#35745;&#25216;&#26415;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#24076;&#26395;&#35774;&#35745;&#20986;&#21487;&#20197;&#25552;&#20379;&#39640;&#31934;&#24230;&#24515;&#29575;&#20272;&#35745;&#30340;&#25216;&#26415;&#65292;&#20854;&#37319;&#26679;&#39057;&#29575;&#20302;&#65292;&#27169;&#22411;&#23610;&#23544;&#23567;&#19988;&#25512;&#26029;&#26102;&#38388;&#24555;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#36890;&#36807;&#32467;&#21512;&#20449;&#21495;&#22788;&#29702;&#21644;&#26426;&#22120;&#23398;&#20064;&#65292;&#21487;&#20197;&#23558;PPG&#37319;&#26679;&#39057;&#29575;&#20174;125 Hz&#38477;&#33267;&#20165;25 Hz&#65292;&#21516;&#26102;&#25552;&#20379;&#26356;&#39640;&#30340;&#24515;&#29575;&#20272;&#35745;&#31934;&#24230;&#12290;&#36825;&#31181;&#32452;&#21512;&#36824;&#26377;&#21161;&#20110;&#20943;&#23567;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#29305;&#24449;&#22823;&#23567;&#65292;&#23548;&#33268;&#27169;&#22411;&#26356;&#23567;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21576;&#29616;&#20102;&#19968;&#20010;&#20840;&#38754;&#30340;
&lt;/p&gt;
&lt;p&gt;
Recent studies showed that Photoplethysmography (PPG) sensors embedded in wearable devices can estimate heart rate (HR) with high accuracy. However, despite of prior research efforts, applying PPG sensor based HR estimation to embedded devices still faces challenges due to the energy-intensive high-frequency PPG sampling and the resource-intensive machine-learning models. In this work, we aim to explore HR estimation techniques that are more suitable for lower-power and resource-constrained embedded devices. More specifically, we seek to design techniques that could provide high-accuracy HR estimation with low-frequency PPG sampling, small model size, and fast inference time. First, we show that by combining signal processing and ML, it is possible to reduce the PPG sampling frequency from 125 Hz to only 25 Hz while providing higher HR estimation accuracy. This combination also helps to reduce the ML model feature size, leading to smaller models. Additionally, we present a comprehensiv
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20809;&#23398;&#25104;&#20687;&#25216;&#26415;&#65292;&#21487;&#20197;&#24555;&#36895;&#12289;&#26080;&#26631;&#35760;&#30340;&#23545;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#36827;&#34892;&#20998;&#23376;&#35786;&#26029;&#65292;&#20026;&#20854;&#27835;&#30103;&#25552;&#20379;&#26356;&#21152;&#20934;&#30830;&#30340;&#25351;&#23548;&#12290;</title><link>http://arxiv.org/abs/2303.13610</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#12289;&#26080;&#26631;&#35760;&#20809;&#23398;&#25104;&#20687;&#20998;&#23376;&#20998;&#31867;&#35786;&#26029;&#30340;&#24212;&#29992;&#20110;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Artificial-intelligence-based molecular classification of diffuse gliomas using rapid, label-free optical imaging. (arXiv:2303.13610v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13610
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#20809;&#23398;&#25104;&#20687;&#25216;&#26415;&#65292;&#21487;&#20197;&#24555;&#36895;&#12289;&#26080;&#26631;&#35760;&#30340;&#23545;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#36827;&#34892;&#20998;&#23376;&#35786;&#26029;&#65292;&#20026;&#20854;&#27835;&#30103;&#25552;&#20379;&#26356;&#21152;&#20934;&#30830;&#30340;&#25351;&#23548;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#23376;&#20998;&#31867;&#30340;&#24212;&#29992;&#20351;&#24471;&#33041;&#32959;&#30244;&#30340;&#27835;&#30103;&#24471;&#21040;&#36716;&#21464;&#65292;&#20351;&#35786;&#26029;&#26356;&#21152;&#20934;&#30830;&#65292;&#27835;&#30103;&#26356;&#21152;&#20010;&#24615;&#21270;&#12290;&#28982;&#32780;&#65292;&#23545;&#20110;&#24739;&#26377;&#33041;&#32959;&#30244;&#30340;&#30149;&#20154;&#65292;&#21450;&#26102;&#36827;&#34892;&#20998;&#23376;&#35786;&#26029;&#27979;&#35797;&#20173;&#28982;&#23384;&#22312;&#38480;&#21046;&#65292;&#20351;&#24471;&#25163;&#26415;&#21644;&#21103;&#36741;&#21161;&#27835;&#30103;&#26356;&#20026;&#22797;&#26434;&#65292;&#38459;&#25747;&#20102;&#20020;&#24202;&#35797;&#39564;&#30340;&#25253;&#21517;&#12290;&#26412;&#25991;&#26500;&#24314;&#20102;DeepGlioma&#35786;&#26029;&#31579;&#36873;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#26159;&#19968;&#31181;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#30340;&#24555;&#36895;&#65288;&lt;90&#31186;&#65289;&#12289;&#26080;&#26631;&#35760;&#30340;&#20809;&#23398;&#25104;&#20687;&#35786;&#26029;&#25216;&#26415;&#12290;DeepGlioma&#20351;&#29992;&#22810;&#27169;&#24577;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#21253;&#25324;&#21050;&#28608;&#25289;&#26364;&#32452;&#32455;&#23398;&#65288;SRH&#65289;&#21644;&#22823;&#22411;&#12289;&#20844;&#20849;&#22522;&#22240;&#32452;&#25968;&#25454;&#12290;&#22312;153&#20363;&#36827;&#34892;SRH&#25104;&#20687;&#30340;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#24739;&#32773;&#32452;&#25104;&#30340;&#21069;&#30651;&#24615;&#12289;&#22810;&#20013;&#24515;&#12289;&#22269;&#38469;&#33539;&#22260;&#30340;&#27979;&#35797;&#38431;&#21015;&#20013;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;DeepGlioma&#21487;&#20197;&#39044;&#27979;&#19990;&#30028;&#21355;&#29983;&#32452;&#32455;&#29992;&#20110;&#23450;&#20041;&#25104;&#20154;&#22411;&#24357;&#28459;&#24615;&#33014;&#36136;&#30244;&#20998;&#31867;&#30340;&#20998;&#23376;&#25913;&#21464;&#65288;IDH mut&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
Molecular classification has transformed the management of brain tumors by enabling more accurate prognostication and personalized treatment. However, timely molecular diagnostic testing for patients with brain tumors is limited, complicating surgical and adjuvant treatment and obstructing clinical trial enrollment. In this study, we developed DeepGlioma, a rapid ($&lt; 90$ seconds), artificial-intelligence-based diagnostic screening system to streamline the molecular diagnosis of diffuse gliomas. DeepGlioma is trained using a multimodal dataset that includes stimulated Raman histology (SRH); a rapid, label-free, non-consumptive, optical imaging method; and large-scale, public genomic data. In a prospective, multicenter, international testing cohort of patients with diffuse glioma ($n=153$) who underwent real-time SRH imaging, we demonstrate that DeepGlioma can predict the molecular alterations used by the World Health Organization to define the adult-type diffuse glioma taxonomy (IDH mut
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#27425;&#27169;&#25910;&#30410;&#21644;&#20840;&#36172;&#24466;&#24310;&#36831;&#21453;&#39304;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#24310;&#36831;&#21453;&#39304;&#27169;&#22411;&#24182;&#23548;&#20986;&#20102;&#21518;&#24724;&#19978;&#38480;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31639;&#27861;&#33021;&#22815;&#22312;&#32771;&#34385;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#21453;&#39304;&#26102;&#32988;&#36807;&#20854;&#20182;&#20840;&#36172;&#24466;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.13604</link><description>&lt;p&gt;
&#24102;&#26377;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#36172;&#24466;&#21453;&#39304;&#30340;&#38543;&#26426;&#27425;&#27169;&#36172;&#21338;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
Stochastic Submodular Bandits with Delayed Composite Anonymous Bandit Feedback. (arXiv:2303.13604v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13604
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#20855;&#26377;&#38543;&#26426;&#27425;&#27169;&#25910;&#30410;&#21644;&#20840;&#36172;&#24466;&#24310;&#36831;&#21453;&#39304;&#30340;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#30740;&#31350;&#20102;&#19977;&#31181;&#24310;&#36831;&#21453;&#39304;&#27169;&#22411;&#24182;&#23548;&#20986;&#20102;&#21518;&#24724;&#19978;&#38480;&#12290;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#31639;&#27861;&#33021;&#22815;&#22312;&#32771;&#34385;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#21453;&#39304;&#26102;&#32988;&#36807;&#20854;&#20182;&#20840;&#36172;&#24466;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#32452;&#21512;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#20854;&#20013;&#21253;&#21547;&#20102;&#26399;&#26395;&#19979;&#30340;&#38543;&#26426;&#27425;&#27169;&#25910;&#30410;&#21644;&#20840;&#36172;&#24466;&#24310;&#36831;&#21453;&#39304;&#65292;&#24310;&#36831;&#21453;&#39304;&#34987;&#20551;&#23450;&#20026;&#32452;&#21512;&#21644;&#21311;&#21517;&#12290;&#20063;&#23601;&#26159;&#35828;&#65292;&#24310;&#36831;&#21453;&#39304;&#26159;&#30001;&#36807;&#21435;&#34892;&#21160;&#30340;&#22870;&#21169;&#32452;&#25104;&#30340;&#65292;&#36825;&#20123;&#22870;&#21169;&#30001;&#23376;&#32452;&#20214;&#26500;&#25104;&#65292;&#20854;&#26410;&#30693;&#30340;&#20998;&#37197;&#26041;&#24335;&#12290;&#30740;&#31350;&#20102;&#19977;&#31181;&#24310;&#36831;&#21453;&#39304;&#27169;&#22411;&#65306;&#26377;&#30028;&#23545;&#25239;&#27169;&#22411;&#12289;&#38543;&#26426;&#29420;&#31435;&#27169;&#22411;&#21644;&#38543;&#26426;&#26465;&#20214;&#29420;&#31435;&#27169;&#22411;&#65292;&#24182;&#38024;&#23545;&#27599;&#31181;&#24310;&#36831;&#27169;&#22411;&#23548;&#20986;&#20102;&#21518;&#24724;&#30028;&#12290;&#24573;&#30053;&#38382;&#39064;&#30456;&#20851;&#21442;&#25968;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#25152;&#26377;&#24310;&#36831;&#27169;&#22411;&#30340;&#21518;&#24724;&#30028;&#20026; $\tilde{O}(T^{2/3} + T^{1/3} \nu)$&#65292;&#20854;&#20013; $T$ &#26159;&#26102;&#38388;&#33539;&#22260;&#65292;$\nu$ &#26159;&#19977;&#31181;&#24773;&#20917;&#19979;&#19981;&#21516;&#23450;&#20041;&#30340;&#24310;&#36831;&#21442;&#25968;&#65292;&#22240;&#27492;&#23637;&#31034;&#20102;&#24102;&#26377;&#24310;&#36831;&#30340;&#34917;&#20607;&#39033;&#12290;&#25152;&#32771;&#34385;&#30340;&#31639;&#27861;&#34987;&#35777;&#26126;&#33021;&#22815;&#32988;&#36807;&#20854;&#20182;&#32771;&#34385;&#20102;&#24310;&#36831;&#32452;&#21512;&#21311;&#21517;&#21453;&#39304;&#30340;&#20840;&#36172;&#24466;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper investigates the problem of combinatorial multiarmed bandits with stochastic submodular (in expectation) rewards and full-bandit delayed feedback, where the delayed feedback is assumed to be composite and anonymous. In other words, the delayed feedback is composed of components of rewards from past actions, with unknown division among the sub-components. Three models of delayed feedback: bounded adversarial, stochastic independent, and stochastic conditionally independent are studied, and regret bounds are derived for each of the delay models. Ignoring the problem dependent parameters, we show that regret bound for all the delay models is $\tilde{O}(T^{2/3} + T^{1/3} \nu)$ for time horizon $T$, where $\nu$ is a delay parameter defined differently in the three cases, thus demonstrating an additive term in regret with delay in all the three delay models. The considered algorithm is demonstrated to outperform other full-bandit approaches with delayed composite anonymous feedbac
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;</title><link>http://arxiv.org/abs/2303.13592</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#30340;&#25552;&#31034;&#65306;&#19996;&#21335;&#20122;&#35821;&#35328;&#30340;&#26696;&#20363;
&lt;/p&gt;
&lt;p&gt;
Prompting Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13592
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#20102;&#20351;&#29992;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#29983;&#25104;&#19996;&#21335;&#20122;&#20116;&#31181;&#35821;&#35328;&#21644;Singlish&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#30340;&#26041;&#27861;&#65292;&#21457;&#29616;ChatGPT&#23637;&#29616;&#20986;&#26368;&#39640;&#30340;&#28508;&#21147;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;ChatGPT&#21644;InstructGPT&#22312;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#26102;&#30340;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#38480;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#28151;&#21512;&#20195;&#30721;&#22312;&#19990;&#30028;&#35768;&#22810;&#22320;&#21306;&#26159;&#19968;&#31181;&#24120;&#35265;&#30340;&#35821;&#35328;&#23454;&#36341;&#65292;&#20294;&#25910;&#38598;&#39640;&#36136;&#37327;&#19988;&#20302;&#25104;&#26412;&#30340;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#20173;&#28982;&#26159;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#65288;NLP&#65289;&#30740;&#31350;&#30340;&#37325;&#22823;&#25361;&#25112;&#12290;&#26368;&#36817;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#26222;&#21450;&#36843;&#20351;&#20154;&#20204;&#38382;&#65306;&#36825;&#20123;&#31995;&#32479;&#33021;&#29992;&#20110;&#25968;&#25454;&#29983;&#25104;&#21527;&#65311;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#22312;&#19968;&#20010;&#38646;-shot&#30340;&#26041;&#24335;&#19979;&#22914;&#20309;&#25552;&#31034;LLMs&#20026;&#19996;&#21335;&#20122;&#65288;SEA&#65289;&#30340;&#20116;&#31181;&#35821;&#35328;&#65288;&#21360;&#23612;&#35821;&#65292;&#39532;&#26469;&#35821;&#65292;&#20013;&#25991;&#65292;&#22612;&#21152;&#36335;&#35821;&#65292;&#36234;&#21335;&#35821;&#65289;&#21450;&#20811;&#37324;&#22885;&#23572;&#35821;S ingl ish&#21019;&#36896;&#28151;&#21512;&#20195;&#30721;&#25968;&#25454;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;ChatGPT&#26174;&#31034;&#20986;&#26368;&#22823;&#30340;&#28508;&#21147;&#65292;&#24403;&#26126;&#30830;&#23450;&#20041;&#8220;&#28151;&#21512;&#20195;&#30721;&#8221;&#26415;&#35821;&#26102;&#65292;&#33021;&#22815;68%&#30340;&#26102;&#38388;&#29983;&#25104;&#28151;&#21512;&#20195;&#30721;&#25991;&#26412;&#12290;&#27492;&#22806;&#65292;ChatGPT&#21644;InstructGPT&#65288;davinci-003&#65289;&#29983;&#25104;S ingl ish&#25991;&#26412;&#30340;&#34920;&#29616;&#20063;&#20540;&#24471;&#27880;&#24847;&#65292;&#23427;&#20204;&#22312;&#21508;&#31181;&#25552;&#31034;&#19979;&#30340;&#25104;&#21151;&#29575;&#24179;&#22343;&#20026;96%&#12290;&#20294;&#26159;&#65292;ChatGPT&#21644;InstructGPT&#30340;&#28151;&#21512;&#20195;&#30721;&#29087;&#32451;&#31243;&#24230;&#21463;&#21040;&#35789;&#27719;&#36873;&#25321;&#38169;&#35823;&#30340;&#24433;&#21709;&#65292;&#23548;&#33268;&#35821;&#20041;&#19981;&#27491;&#30830;&#30340;&#36755;&#20986;&#12290;
&lt;/p&gt;
&lt;p&gt;
While code-mixing is a common linguistic practice in many parts of the world, collecting high-quality and low-cost code-mixed data remains a challenge for natural language processing (NLP) research. The proliferation of Large Language Models (LLMs) in recent times compels one to ask: can these systems be used for data generation? In this article, we explore prompting LLMs in a zero-shot manner to create code-mixed data for five languages in South East Asia (SEA) -Indonesian, Malay, Chinese, Tagalog, Vietnamese, as well as the creole language Singlish. We find that ChatGPT shows the most potential, capable of producing code-mixed text 68% of the time when the term "code-mixing" is explicitly defined. Moreover, both ChatGPT and InstructGPT's (davinci-003) performances in generating Singlish texts are noteworthy, averaging a 96% success rate across a variety of prompts. The code-mixing proficiency of ChatGPT and InstructGPT, however, is dampened by word choice errors that lead to semant
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31526;&#21495;&#25512;&#29702;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#30340;&#39564;&#35777;&#38382;&#39064;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#32534;&#30721;&#35768;&#22810;&#39564;&#35777;&#38382;&#39064;&#20026;&#20108;&#27425;&#31243;&#24207;&#65292;&#24182;&#23558;&#20854;&#26494;&#24347;&#20026;&#21322;&#23450;&#31243;&#24207;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#39564;&#35777;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#23646;&#24615;&#65292;&#24182;&#20026;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2303.13588</link><description>&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#30340;&#39640;&#25928;&#31526;&#21495;&#25512;&#29702;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Efficient Symbolic Reasoning for Neural-Network Verification. (arXiv:2303.13588v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13588
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#39640;&#25928;&#30340;&#31526;&#21495;&#25512;&#29702;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#30340;&#39564;&#35777;&#38382;&#39064;&#12290;&#35813;&#25216;&#26415;&#21487;&#20197;&#32534;&#30721;&#35768;&#22810;&#39564;&#35777;&#38382;&#39064;&#20026;&#20108;&#27425;&#31243;&#24207;&#65292;&#24182;&#23558;&#20854;&#26494;&#24347;&#20026;&#21322;&#23450;&#31243;&#24207;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#12290;&#35813;&#26694;&#26550;&#21487;&#20197;&#39564;&#35777;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#23646;&#24615;&#65292;&#24182;&#20026;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#38382;&#39064;&#25552;&#20379;&#20102;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31070;&#32463;&#32593;&#32476;&#24050;&#25104;&#20026;&#29616;&#20195;&#36719;&#20214;&#31995;&#32479;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#19968;&#37096;&#20998;&#12290;&#28982;&#32780;&#65292;&#20182;&#20204;&#20173;&#28982;&#23384;&#22312;&#35768;&#22810;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#31243;&#24207;&#25512;&#29702;&#26694;&#26550;&#65292;&#31216;&#20026;&#31526;&#21495;&#25512;&#29702;&#12290;&#25105;&#20204;&#26694;&#26550;&#30340;&#20851;&#38190;&#32452;&#25104;&#37096;&#20998;&#26159;&#31526;&#21495;&#22495;&#21644;&#20108;&#27425;&#20851;&#31995;&#30340;&#20351;&#29992;&#12290;&#31526;&#21495;&#22495;&#20855;&#26377;&#38750;&#24120;&#28789;&#27963;&#30340;&#35821;&#20041;&#65292;&#32780;&#20108;&#27425;&#20851;&#31995;&#21017;&#38750;&#24120;&#34920;&#36798;&#33021;&#21147;&#24378;&#12290;&#23427;&#20204;&#20801;&#35768;&#25105;&#20204;&#23558;&#35768;&#22810;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#38382;&#39064;&#32534;&#30721;&#20026;&#20108;&#27425;&#31243;&#24207;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#30340;&#26041;&#26696;&#23558;&#20108;&#27425;&#31243;&#24207;&#26494;&#24347;&#20026;&#21322;&#23450;&#31243;&#24207;&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#35299;&#20915;&#12290;&#36825;&#20010;&#26694;&#26550;&#35753;&#25105;&#20204;&#33021;&#22815;&#22312;&#19981;&#21516;&#30340;&#22330;&#26223;&#19979;&#39564;&#35777;&#21508;&#31181;&#31070;&#32463;&#32593;&#32476;&#23646;&#24615;&#65292;&#29305;&#21035;&#26159;&#37027;&#20123;&#23545;&#38750;&#31526;&#21495;&#22495;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#23646;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#36824;&#24341;&#20837;&#20102;&#26032;&#30340;&#34920;&#31034;&#21644;&#39564;&#35777;&#20219;&#21153;&#30340;&#35266;&#28857;&#12290;&#25105;&#20204;&#30456;&#20449;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#20197;&#20026;&#31070;&#32463;&#32593;&#32476;&#39564;&#35777;&#24102;&#26469;&#37325;&#35201;&#30340;&#25913;&#36827;&#65292;&#29305;&#21035;&#26159;&#35299;&#20915;&#23427;&#20204;&#23545;&#23545;&#25239;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The neural network has become an integral part of modern software systems. However, they still suffer from various problems, in particular, vulnerability to adversarial attacks. In this work, we present a novel program reasoning framework for neural-network verification, which we refer to as symbolic reasoning. The key components of our framework are the use of the symbolic domain and the quadratic relation. The symbolic domain has very flexible semantics, and the quadratic relation is quite expressive. They allow us to encode many verification problems for neural networks as quadratic programs. Our scheme then relaxes the quadratic programs to semidefinite programs, which can be efficiently solved. This framework allows us to verify various neural-network properties under different scenarios, especially those that appear challenging for non-symbolic domains. Moreover, it introduces new representations and perspectives for the verification tasks. We believe that our framework can bring
&lt;/p&gt;</description></item><item><title>TinyML&#26159;&#19968;&#31181;&#23884;&#20837;&#24335;ML&#25216;&#26415;&#65292;&#21487;&#22312;&#22810;&#31181;&#24265;&#20215;&#65292;&#36164;&#28304;&#26377;&#38480;&#21644;&#21151;&#29575;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#23454;&#29616;ML&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#22312;&#20854;&#23454;&#29616;&#36807;&#31243;&#20013;&#65292;&#38656;&#35201;&#21450;&#26102;&#35299;&#20915;&#22810;&#31181;&#25361;&#25112;&#65292;&#20363;&#22914;&#22788;&#29702;&#33021;&#21147;&#20248;&#21270;&#65292;&#25552;&#39640;&#21487;&#38752;&#24615;&#20197;&#21450;&#32500;&#25252;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13569</link><description>&lt;p&gt;
TinyML&#65306;&#24037;&#20855;&#65292;&#24212;&#29992;&#65292;&#25361;&#25112;&#21644;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;
&lt;/p&gt;
&lt;p&gt;
TinyML: Tools, Applications, Challenges, and Future Research Directions. (arXiv:2303.13569v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13569
&lt;/p&gt;
&lt;p&gt;
TinyML&#26159;&#19968;&#31181;&#23884;&#20837;&#24335;ML&#25216;&#26415;&#65292;&#21487;&#22312;&#22810;&#31181;&#24265;&#20215;&#65292;&#36164;&#28304;&#26377;&#38480;&#21644;&#21151;&#29575;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#23454;&#29616;ML&#24212;&#29992;&#12290;&#20294;&#26159;&#65292;&#22312;&#20854;&#23454;&#29616;&#36807;&#31243;&#20013;&#65292;&#38656;&#35201;&#21450;&#26102;&#35299;&#20915;&#22810;&#31181;&#25361;&#25112;&#65292;&#20363;&#22914;&#22788;&#29702;&#33021;&#21147;&#20248;&#21270;&#65292;&#25552;&#39640;&#21487;&#38752;&#24615;&#20197;&#21450;&#32500;&#25252;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#22312;&#24037;&#19994;&#21644;&#23398;&#26415;&#30028;&#37117;&#21463;&#21040;&#20102;&#26497;&#22823;&#30340;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#20256;&#32479;&#30340;ML&#25216;&#26415;&#38656;&#35201;&#22823;&#37327;&#30340;&#35745;&#31639;&#36164;&#28304;&#25165;&#33021;&#28385;&#36275;&#25152;&#38656;&#30340;&#20934;&#30830;&#24615;&#65292;&#36825;&#38480;&#21046;&#20102;&#23427;&#20204;&#20027;&#35201;&#29992;&#20110;&#39640;&#24615;&#33021;&#35774;&#22791;&#65288;&#22914;&#32593;&#32476;&#33410;&#28857;&#65289;&#12290;&#28982;&#32780;&#65292;&#38543;&#30528;&#29289;&#32852;&#32593;&#21644;&#36793;&#32536;&#35745;&#31639;&#31561;&#25216;&#26415;&#30340;&#19981;&#26029;&#21457;&#23637;&#65292;&#23558;ML&#25216;&#26415;&#32435;&#20837;&#21040;&#36164;&#28304;&#21463;&#38480;&#30340;&#23884;&#20837;&#24335;&#35774;&#22791;&#19978;&#20197;&#23454;&#29616;&#20998;&#24067;&#24335;&#21644;&#26222;&#36866;&#24615;&#26234;&#33021;&#20063;&#21464;&#24471;&#21313;&#20998;&#26377;&#24517;&#35201;&#12290;&#36825;&#20419;&#20351;&#20986;&#29616;&#20102;TinyML&#33539; paradigm&#65292;&#23427;&#26159;&#19968;&#31181;&#23884;&#20837;&#24335;ML&#25216;&#26415;&#65292;&#21487;&#22312;&#22810;&#31181;&#24265;&#20215;&#65292;&#36164;&#28304;&#26377;&#38480;&#21644;&#21151;&#29575;&#26377;&#38480;&#30340;&#35774;&#22791;&#19978;&#23454;&#29616;ML&#24212;&#29992;&#12290;&#28982;&#32780;&#65292;&#22312;&#21521;TinyML&#25216;&#26415;&#36827;&#34892;&#36866;&#24403;&#23454;&#29616;&#30340;&#36807;&#31243;&#20013;&#65292;&#38656;&#35201;&#21450;&#26102;&#35299;&#20915;&#22810;&#20010;&#25361;&#25112;&#65292;&#20363;&#22914;&#22788;&#29702;&#33021;&#21147;&#20248;&#21270;&#65292;&#25552;&#39640;&#21487;&#38752;&#24615;&#20197;&#21450;&#32500;&#25252;&#23398;&#20064;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, Artificial Intelligence (AI) and Machine learning (ML) have gained significant interest from both, industry and academia. Notably, conventional ML techniques require enormous amounts of power to meet the desired accuracy, which has limited their use mainly to high-capability devices such as network nodes. However, with many advancements in technologies such as the Internet of Things (IoT) and edge computing, it is desirable to incorporate ML techniques into resource-constrained embedded devices for distributed and ubiquitous intelligence. This has motivated the emergence of the TinyML paradigm which is an embedded ML technique that enables ML applications on multiple cheap, resource- and power-constrained devices. However, during this transition towards appropriate implementation of the TinyML technology, multiple challenges such as processing capacity optimization, improved reliability, and maintenance of learning models' accuracy require timely solutions. In this art
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20351;&#29992;&#25913;&#36827;&#21518;&#30340;&#26041;&#27861;&#20174;&#22823;&#37327;&#24179;&#38754;&#22270;&#20687;&#20013;&#25552;&#21462;&#35775;&#38382;&#22270;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#8220;&#24179;&#38754;&#22270;&#20215;&#20540;&#8221;&#20272;&#35745;&#35775;&#38382;&#22270;&#30340;&#25151;&#22320;&#20135;&#20215;&#20540;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#31199;&#37329;&#20272;&#35745;&#30340;&#31934;&#24230;&#12290;&#35813;&#27169;&#22411;&#20026;&#20840;&#38754;&#20272;&#35745;&#24179;&#38754;&#22270;&#30340;&#20215;&#20540;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;</title><link>http://arxiv.org/abs/2303.13568</link><description>&lt;p&gt;
&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#25552;&#21462;&#31199;&#36161;&#20844;&#23507;&#24179;&#38754;&#22270;&#30340;&#25151;&#22320;&#20135;&#20215;&#20540;
&lt;/p&gt;
&lt;p&gt;
Extracting real estate values of rental apartment floor plans using graph convolutional networks. (arXiv:2303.13568v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13568
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20351;&#29992;&#25913;&#36827;&#21518;&#30340;&#26041;&#27861;&#20174;&#22823;&#37327;&#24179;&#38754;&#22270;&#20687;&#20013;&#25552;&#21462;&#35775;&#38382;&#22270;&#65292;&#21033;&#29992;&#22270;&#21367;&#31215;&#31070;&#32463;&#32593;&#32476;&#27169;&#22411;&#8220;&#24179;&#38754;&#22270;&#20215;&#20540;&#8221;&#20272;&#35745;&#35775;&#38382;&#22270;&#30340;&#25151;&#22320;&#20135;&#20215;&#20540;&#65292;&#22823;&#24133;&#25552;&#39640;&#20102;&#31199;&#37329;&#20272;&#35745;&#30340;&#31934;&#24230;&#12290;&#35813;&#27169;&#22411;&#20026;&#20840;&#38754;&#20272;&#35745;&#24179;&#38754;&#22270;&#30340;&#20215;&#20540;&#25552;&#20379;&#20102;&#26032;&#24605;&#36335;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20174;&#22823;&#37327;&#20301;&#20110;&#26085;&#26412;&#22823;&#38442;&#30340;&#23478;&#24237;&#24335;&#31199;&#36161;&#20844;&#23507;&#30340;&#24179;&#38754;&#22270;&#20687;&#20013;&#25552;&#21462;&#21453;&#26144;&#25151;&#38388;&#27969;&#32447;&#37051;&#25509;&#20851;&#31995;&#30340;&#20960;&#20309;&#22270;&#24418;&#65292;&#24182;&#20351;&#29992;&#25913;&#36827;&#21518;&#30340;&#35775;&#38382;&#22270;&#25552;&#21462;&#26041;&#27861;&#23450;&#20041;&#21644;&#23454;&#29616;&#35775;&#38382;&#22270;&#30340;&#22270;&#21367;&#31215;&#32593;&#32476;&#65288;GCN&#65289;&#27169;&#22411;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20272;&#35745;&#35775;&#38382;&#22270;&#30340;&#25151;&#22320;&#20135;&#20215;&#20540;&#30340;&#26041;&#27861;&#65292;&#21363;&#8220;&#24179;&#38754;&#22270;&#20215;&#20540;&#8221;&#12290;&#35813;&#27169;&#22411;&#21253;&#25324;&#24179;&#38754;&#22270;&#20215;&#20540;&#12289;&#20351;&#29992;&#20854;&#20182;&#19968;&#33324;&#24615;&#35299;&#37322;&#21464;&#37327;&#30340;&#20139;&#21463;&#26041;&#27861;&#65292;&#24182;&#29992;&#20110;&#20272;&#35745;&#31199;&#37329;&#65292;&#27604;&#36739;&#20272;&#35745;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23398;&#20064;&#21367;&#31215;&#32593;&#32476;&#65292;&#20998;&#26512;&#20102;&#35299;&#37322;&#31199;&#37329;&#30340;&#24179;&#38754;&#22270;&#30340;&#29305;&#24449;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#24182;&#39564;&#35777;&#20102;&#19968;&#31181;&#20840;&#38754;&#20272;&#35745;&#25151;&#22320;&#20135;&#24179;&#38754;&#22270;&#20215;&#20540;&#30340;&#26032;&#27169;&#22411;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#31199;&#37329;&#20272;&#35745;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Access graphs that indicate adjacency relationships from the perspective of flow lines of rooms are extracted automatically from a large number of floor plan images of a family-oriented rental apartment complex in Osaka Prefecture, Japan, based on a recently proposed access graph extraction method with slight modifications. We define and implement a graph convolutional network (GCN) for access graphs and propose a model to estimate the real estate value of access graphs as the floor plan value. The model, which includes the floor plan value and hedonic method using other general explanatory variables, is used to estimate rents and their estimation accuracies are compared. In addition, the features of the floor plan that explain the rent are analyzed from the learned convolution network. Therefore, a new model for comprehensively estimating the value of real estate floor plans is proposed and validated. The results show that the proposed method significantly improves the accuracy of ren
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#22686;&#24378;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#24212;&#29992;&#20110;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;PharmKG&#25968;&#25454;&#38598;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#23454;&#39564;&#29615;&#22659;&#20013;&#20851;&#31995;&#20107;&#23454;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#12290;</title><link>http://arxiv.org/abs/2303.13566</link><description>&lt;p&gt;
&#20351;&#29992;&#36923;&#36753;&#30693;&#35782;&#22686;&#24378;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#23884;&#20837;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Enhancing Embedding Representations of Biomedical Data using Logic Knowledge. (arXiv:2303.13566v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13566
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20351;&#29992;&#36923;&#36753;&#35268;&#21017;&#22686;&#24378;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#30340;&#23884;&#20837;&#34920;&#31034;&#65292;&#24212;&#29992;&#20110;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;PharmKG&#25968;&#25454;&#38598;&#65292;&#24182;&#33021;&#22815;&#22788;&#29702;&#22797;&#26434;&#30340;&#23454;&#39564;&#29615;&#22659;&#20013;&#20851;&#31995;&#20107;&#23454;&#20043;&#38388;&#30340;&#20381;&#36182;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#23884;&#20837;&#65288;KGE&#65289;&#24050;&#32463;&#25104;&#20026;&#19968;&#31181;&#27969;&#34892;&#30340;&#27169;&#22411;&#65292;&#19987;&#38376;&#35774;&#35745;&#29992;&#20110;&#22788;&#29702;&#26412;&#20307;&#35770;&#21644;&#22270;&#24418;&#32467;&#26500;&#25968;&#25454;&#65292;&#22240;&#20026;&#23427;&#20204;&#21487;&#20197;&#38544;&#24335;&#22320;&#22312;&#28508;&#22312;&#31354;&#38388;&#20013;&#32534;&#30721;&#23454;&#20307;&#21644;&#20851;&#31995;&#20043;&#38388;&#30340;&#32479;&#35745;&#20381;&#36182;&#20851;&#31995;&#12290; KGE&#25216;&#26415;&#22312;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#29305;&#21035;&#26377;&#25928;&#65292;&#22240;&#20026;&#22312;&#29983;&#29289;&#21644;&#21270;&#23398;&#23545;&#35937;&#20043;&#38388;&#23384;&#22312;&#22797;&#26434;&#30340;&#30456;&#20114;&#20316;&#29992;&#65292;&#22240;&#27492;&#22788;&#29702;&#22823;&#22411;&#30693;&#35782;&#22270;&#35889;&#38750;&#24120;&#24120;&#35265;&#12290;&#26368;&#36817;&#65292;PharmKG&#25968;&#25454;&#38598;&#34987;&#25552;&#20986;&#20316;&#20026;&#26368;&#20855;&#25361;&#25112;&#24615;&#30340;&#30693;&#35782;&#22270;&#35889;&#29983;&#29289;&#21307;&#23398;&#22522;&#20934;&#20043;&#19968;&#65292;&#20854;&#20013;&#28041;&#21450;&#22522;&#22240;&#65292;&#30142;&#30149;&#21644;&#21270;&#23398;&#29289;&#36136;&#20043;&#38388;&#25968;&#21313;&#19975;&#20010;&#20851;&#31995;&#20107;&#23454;&#12290;&#23613;&#31649; KGE &#21487;&#20197;&#22788;&#29702;&#38750;&#24120;&#22823;&#30340;&#20851;&#31995;&#22495;&#65292;&#20294;&#23427;&#20204;&#36890;&#24120;&#26080;&#27861;&#34920;&#31034;&#20851;&#31995;&#20107;&#23454;&#20043;&#38388;&#26356;&#22797;&#26434;&#30340;&#20851;&#31995;&#20381;&#36182;&#24615;&#65292;&#20363;&#22914;&#36923;&#36753;&#35268;&#21017;&#65292;&#22312;&#22797;&#26434;&#30340;&#23454;&#39564;&#29615;&#22659;&#20013;&#21487;&#33021;&#26159;&#22522;&#26412;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#21033;&#29992;&#36923;&#36753;&#35268;&#21017;&#22686;&#24378;KGE&#22312;PharmKG&#25968;&#25454;&#38598;&#19978;&#30340;&#23884;&#20837;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge Graph Embeddings (KGE) have become a quite popular class of models specifically devised to deal with ontologies and graph structure data, as they can implicitly encode statistical dependencies between entities and relations in a latent space. KGE techniques are particularly effective for the biomedical domain, where it is quite common to deal with large knowledge graphs underlying complex interactions between biological and chemical objects. Recently in the literature, the PharmKG dataset has been proposed as one of the most challenging knowledge graph biomedical benchmark, with hundreds of thousands of relational facts between genes, diseases and chemicals. Despite KGEs can scale to very large relational domains, they generally fail at representing more complex relational dependencies between facts, like logic rules, which may be fundamental in complex experimental settings. In this paper, we exploit logic rules to enhance the embedding representations of KGEs on the PharmKG
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#22359;&#25968;&#25454;&#23384;&#20648;&#26041;&#27861;&#30340;&#22312;&#32447;&#22260;&#26827;&#28216;&#25103;&#31995;&#32479; CH-Go&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;</title><link>http://arxiv.org/abs/2303.13553</link><description>&lt;p&gt;
&#22522;&#20110;&#20998;&#22359;&#25968;&#25454;&#23384;&#20648;&#30340;&#22312;&#32447;&#22260;&#26827;&#31995;&#32479;CH-Go
&lt;/p&gt;
&lt;p&gt;
CH-Go: Online Go System Based on Chunk Data Storage. (arXiv:2303.13553v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13553
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#22359;&#25968;&#25454;&#23384;&#20648;&#26041;&#27861;&#30340;&#22312;&#32447;&#22260;&#26827;&#28216;&#25103;&#31995;&#32479; CH-Go&#65292;&#21487;&#20197;&#39640;&#25928;&#22320;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#26469;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26500;&#24314;&#19968;&#20010;&#26377;&#25928;&#30340;&#25968;&#25454;&#31649;&#29702;&#31995;&#32479;&#26469;&#22788;&#29702;&#28023;&#37327;&#25968;&#25454;&#65292;&#22914;&#21021;&#22987;&#22260;&#26827;&#28216;&#25103;&#35760;&#24405;&#65292;&#34920;&#31034;&#23398;&#20064;&#33719;&#24471;&#30340;&#29305;&#24449;&#25968;&#25454;&#38598;&#65292;&#33258;&#25105;&#23545;&#24328;&#30340;&#32463;&#39564;&#25968;&#25454;&#38598;&#65292;&#38543;&#26426;&#37319;&#26679;&#30340;&#33945;&#29305;&#21345;&#32599;&#26641;&#31561;&#65292;&#26159;&#23454;&#29616;&#22312;&#32447;&#22260;&#26827;&#31995;&#32479;&#30340;&#35757;&#32451;&#21644;&#36816;&#34892;&#25152;&#24517;&#38656;&#30340;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#24456;&#23569;&#25552;&#21040;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#25968;&#25454;&#31649;&#29702;&#31995;&#32479;&#30340;&#33021;&#21147;&#21644;&#25928;&#29575;&#20915;&#23450;&#20102;&#22260;&#26827;&#31995;&#32479;&#30340;&#20934;&#30830;&#24615;&#21644;&#36895;&#24230;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20998;&#22359;&#25968;&#25454;&#23384;&#20648;&#26041;&#27861;&#30340;&#22312;&#32447;&#22260;&#26827;&#28216;&#25103;&#31995;&#32479; (CH-Go)&#65292;&#23427;&#22788;&#29702;&#20102;Kiseido Go Server (KGS)&#21457;&#24067;&#30340;160k&#26684;&#24335;&#30340;&#22260;&#26827;&#28216;&#25103;&#25968;&#25454;&#65292;&#24182;&#35774;&#35745;&#20102;&#19968;&#20010;&#20855;&#26377;11&#20010;&#24179;&#38754;&#30340;&#22260;&#26827;&#32534;&#30721;&#22120;&#12289;&#19968;&#20010;&#24182;&#34892;&#22788;&#29702;&#22120;&#21644;&#19968;&#20010;&#29983;&#25104;&#22120;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#20869;&#23384;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#25226;&#25968;&#25454;&#23384;&#20648;&#22312;&#22359;&#20013;&#65292;&#20197;1024&#20026;&#25209;&#22788;&#29702;&#22823;&#23567;&#65292;&#24182;&#23558;&#27599;&#20010;&#22359;&#30340;&#29305;&#24449;&#21644;&#26631;&#31614;&#20445;&#23384;&#20026;&#20108;&#36827;&#21046;&#25991;&#20214;&#12290;&#28982;&#21518;&#27599;&#27425;&#38543;&#26426;&#25277;&#21462;&#19968;&#23567;&#32452;&#25968;&#25454;&#36827;&#34892;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
The training and running of an online Go system require the support of effective data management systems to deal with vast data, such as the initial Go game records, the feature data set obtained by representation learning, the experience data set of self-play, the randomly sampled Monte Carlo tree, and so on. Previous work has rarely mentioned this problem, but the ability and efficiency of data management systems determine the accuracy and speed of the Go system. To tackle this issue, we propose an online Go game system based on the chunk data storage method (CH-Go), which processes the format of 160k Go game data released by Kiseido Go Server (KGS) and designs a Go encoder with 11 planes, a parallel processor and generator for better memory performance. Specifically, we store the data in chunks, take the chunk size of 1024 as a batch, and save the features and labels of each chunk as binary files. Then a small set of data is randomly sampled each time for the neural network training
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DaToBS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#29031;&#29255;&#20013;&#26816;&#27979;&#21644;&#36716;&#24405;Tifinagh&#23383;&#31526;&#65292;&#20197;&#25552;&#39640;&#38750;&#27954;&#20302;&#36164;&#28304;&#35821;&#35328;&#38463;&#39532;&#40784;&#35821;&#22312;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#32593;&#32476;&#24212;&#29992;&#31561;&#26041;&#38754;&#30340;&#25903;&#25345;&#12290;</title><link>http://arxiv.org/abs/2303.13549</link><description>&lt;p&gt;
&#20174;&#20302;&#36164;&#28304;&#35821;&#35328;&#38463;&#39532;&#40784;&#35821;&#30340;&#22270;&#20687;&#20013;&#36827;&#34892;Tifinagh&#23383;&#31526;&#30340;&#20809;&#23398;&#23383;&#31526;&#35782;&#21035;&#21644;&#36716;&#24405;
&lt;/p&gt;
&lt;p&gt;
Optical Character Recognition and Transcription of Berber Signs from Images in a Low-Resource Language Amazigh. (arXiv:2303.13549v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13549
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;DaToBS&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#33258;&#28982;&#29615;&#22659;&#20013;&#30340;&#29031;&#29255;&#20013;&#26816;&#27979;&#21644;&#36716;&#24405;Tifinagh&#23383;&#31526;&#65292;&#20197;&#25552;&#39640;&#38750;&#27954;&#20302;&#36164;&#28304;&#35821;&#35328;&#38463;&#39532;&#40784;&#35821;&#22312;&#25945;&#32946;&#12289;&#30740;&#31350;&#21644;&#32593;&#32476;&#24212;&#29992;&#31561;&#26041;&#38754;&#30340;&#25903;&#25345;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26575;&#26575;&#23572;&#35821;&#26159;&#19968;&#31181;&#20302;&#36164;&#28304;&#30340;&#21271;&#38750;&#22303;&#35821;&#65292;&#22312;&#25705;&#27931;&#21733;&#12289;&#38463;&#23572;&#21450;&#21033;&#20122;&#31561;&#22320;&#30340;&#26575;&#26575;&#23572;&#31038;&#21306;&#20013;&#20351;&#29992;&#33258;&#24049;&#29420;&#29305;&#30340;&#23383;&#27597;&#34920;Tifinagh&#12290;&#36825;&#31181;&#38750;&#27954;&#20122;&#32454;&#20122;&#35821;&#35328;&#26159;&#30001;1400&#19975;&#20154;&#20351;&#29992;&#30340;&#65292;&#20294;&#32570;&#20047;&#36275;&#22815;&#30340;&#25945;&#32946;&#12289;&#30740;&#31350;&#12289;&#32593;&#32476;&#24212;&#29992;&#31561;&#26041;&#38754;&#30340;&#25903;&#25345;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;DaToBS&#30340;&#26377;&#30417;&#30563;&#26041;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#36716;&#24405;Berber&#23383;&#31526;&#65292;&#26088;&#22312;&#23454;&#29616;&#20174;&#33258;&#28982;&#29615;&#22659;&#30340;&#29031;&#29255;&#20013;&#33258;&#21160;&#35782;&#21035;&#21644;&#36716;&#24405;Tifinagh&#23383;&#31526;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Berber, or Amazigh language family is a low-resource North African vernacular language spoken by the indigenous Berber ethnic group. It has its own unique alphabet called Tifinagh used across Berber communities in Morocco, Algeria, and others. The Afroasiatic language Berber is spoken by 14 million people, yet lacks adequate representation in education, research, web applications etc. For instance, there is no option of translation to or from Amazigh / Berber on Google Translate, which hosts over 100 languages today. Consequently, we do not find specialized educational apps, L2 (2nd language learner) acquisition, automated language translation, and remote-access facilities enabled in Berber. Motivated by this background, we propose a supervised approach called DaToBS for Detection and Transcription of Berber Signs. The DaToBS approach entails the automatic recognition and transcription of Tifinagh characters from signs in photographs of natural environments. This is achieved by sel
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#28436;&#31034;&#20102;&#26234;&#33021;&#20010;&#20154;&#21161;&#25163;Dona&#65292;&#29992;&#20110;&#23398;&#29983;&#36873;&#35838;&#30340;&#33258;&#21160;&#21270;&#25805;&#20316;&#65292;&#37319;&#29992;&#35821;&#38899;&#36755;&#20837;&#12289;&#20219;&#21153;&#35268;&#21010;&#20248;&#21270;&#21644;&#35821;&#35328;&#32763;&#35793;&#31561;&#25216;&#26415;&#65292;&#20351;&#23398;&#29983;&#19981;&#38656;&#35201;&#33258;&#24049;&#23436;&#25104;&#22797;&#26434;&#30340;&#36873;&#35838;&#34920;&#26684;&#12290;</title><link>http://arxiv.org/abs/2303.13548</link><description>&lt;p&gt;
&#22079;&#65292;Dona&#65281;&#20320;&#33021;&#24110;&#25105;&#22788;&#29702;&#23398;&#29983;&#36873;&#35838;&#21527;&#65311;
&lt;/p&gt;
&lt;p&gt;
Hey Dona! Can you help me with student course registration?. (arXiv:2303.13548v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13548
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28436;&#31034;&#20102;&#26234;&#33021;&#20010;&#20154;&#21161;&#25163;Dona&#65292;&#29992;&#20110;&#23398;&#29983;&#36873;&#35838;&#30340;&#33258;&#21160;&#21270;&#25805;&#20316;&#65292;&#37319;&#29992;&#35821;&#38899;&#36755;&#20837;&#12289;&#20219;&#21153;&#35268;&#21010;&#20248;&#21270;&#21644;&#35821;&#35328;&#32763;&#35793;&#31561;&#25216;&#26415;&#65292;&#20351;&#23398;&#29983;&#19981;&#38656;&#35201;&#33258;&#24049;&#23436;&#25104;&#22797;&#26434;&#30340;&#36873;&#35838;&#34920;&#26684;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#28436;&#31034;&#20102;&#19968;&#31181;&#21517;&#20026;Hey Dona&#65288;&#25110;&#20165;&#31216;&#20026;Dona&#65289;&#30340;&#26234;&#33021;&#20010;&#20154;&#21161;&#25163;&#65292;&#23427;&#20855;&#26377;&#34394;&#25311;&#35821;&#38899;&#21161;&#25163;&#65292;&#29992;&#20110;&#23398;&#29983;&#36873;&#35838;&#30340;&#33258;&#21160;&#21270;&#25805;&#20316;&#65292;&#36825;&#26159;&#19968;&#20010;&#26088;&#22312;&#24212;&#29992;&#20110;&#25945;&#32946;&#39046;&#22495;&#30340;&#39033;&#30446;&#12290;Hey Dona &#36866;&#29992;&#20110;&#21508;&#31181;&#21475;&#38899;&#65292;&#24182;&#33021;&#22815;&#36827;&#34892;&#20219;&#21153;&#35268;&#21010;&#20248;&#21270;&#21644;&#35821;&#35328;&#32763;&#35793;&#65292;&#25509;&#21463;&#21487;&#20197;&#36890;&#36807;&#40614;&#20811;&#39118;&#65288;&#34013;&#29273;&#12289;&#26377;&#32447;&#40614;&#20811;&#39118;&#65289;&#30340;&#35821;&#38899;&#36755;&#20837;&#65292;&#24182;&#25353;&#29031;&#29992;&#25143;&#21629;&#20196;&#25191;&#34892;&#26597;&#35810;&#22788;&#29702;&#65292;&#36830;&#25509;&#32593;&#32476;&#25628;&#32034;&#22238;&#31572;&#65292;&#24314;&#31435;&#20219;&#21153;&#20381;&#36182;&#20851;&#31995;&#65292;&#30830;&#20445;&#25104;&#21151;&#27880;&#20876;&#12290;Dona &#36991;&#20813;&#20102;&#23398;&#29983;&#33258;&#24049;&#36755;&#20837;&#12289;&#28857;&#20987;&#21644;&#27983;&#35272;&#22797;&#26434;&#30340;&#36873;&#35838;&#34920;&#26684;&#30340;&#38656;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a demo of an intelligent personal agent called Hey Dona (or just Dona) with virtual voice assistance in student course registration. It is a deployed project in the theme of AI for education. In this digital age with a myriad of smart devices, users often delegate tasks to agents. While pointing and clicking supersedes the erstwhile command-typing, modern devices allow users to speak commands for agents to execute tasks, enhancing speed and convenience. In line with this progress, Dona is an intelligent agent catering to student needs by automated, voice-operated course registration, spanning a multitude of accents, entailing task planning optimization, with some language translation as needed. Dona accepts voice input by microphone (Bluetooth, wired microphone), converts human voice to computer understandable language, performs query processing as per user commands, connects with the Web to search for answers, models task dependencies, imbibes quality control
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;ChatGPT&#30340;Text-to-SQL&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20854;&#24378;&#22823;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#23588;&#20854;&#22312;ADVETA(RPL)&#24773;&#22659;&#19979;&#20248;&#20110;&#38656;&#35201;&#24494;&#35843;&#30340;SOTA&#27169;&#22411;&#65292;&#26377;&#26395;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21457;&#25381;&#28508;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.13547</link><description>&lt;p&gt;
ChatGPT &#30340;&#38646;-shot Text-to-SQL &#33021;&#21147;&#30340;&#32508;&#21512;&#35780;&#20272;
&lt;/p&gt;
&lt;p&gt;
A comprehensive evaluation of ChatGPT's zero-shot Text-to-SQL capability. (arXiv:2303.13547v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13547
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;ChatGPT&#30340;Text-to-SQL&#33021;&#21147;&#36827;&#34892;&#20102;&#20840;&#38754;&#30340;&#35780;&#20272;&#65292;&#23637;&#31034;&#20854;&#24378;&#22823;&#30340;&#38646;-shot&#34920;&#29616;&#65292;&#23588;&#20854;&#22312;ADVETA(RPL)&#24773;&#22659;&#19979;&#20248;&#20110;&#38656;&#35201;&#24494;&#35843;&#30340;SOTA&#27169;&#22411;&#65292;&#26377;&#26395;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#21457;&#25381;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#20840;&#38754;&#20998;&#26512;&#20102; ChatGPT &#30340; Text-to-SQL &#33021;&#21147;&#12290;&#32771;&#34385;&#21040;&#22823;&#22411;&#23545;&#35805;&#35821;&#35328;&#27169;&#22411; ChatGPT &#21644;&#20854;&#22312;&#23545;&#35805;&#33021;&#21147;&#21644;&#20195;&#30721;&#29983;&#25104;&#33021;&#21147;&#19978;&#30340;&#21360;&#35937;&#28145;&#21051;&#30340;&#34920;&#29616;&#65292;&#25105;&#20204;&#35797;&#22270;&#35780;&#20272;&#20854; Text-to-SQL &#24615;&#33021;&#12290;&#25105;&#20204;&#23545;12&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#36827;&#34892;&#20102;&#23454;&#39564;&#65292;&#28041;&#21450;&#19981;&#21516;&#30340;&#35821;&#35328;&#12289;&#35774;&#32622;&#25110;&#22330;&#26223;&#65292;&#24182;&#19988;&#32467;&#26524;&#34920;&#26126; ChatGPT &#20855;&#26377;&#24378;&#22823;&#30340; Text-to-SQL &#33021;&#21147;&#12290;&#34429;&#28982;&#19982;&#24403;&#21069;&#26368;&#20808;&#36827;&#30340;&#27169;&#22411;&#34920;&#29616;&#20173;&#26377;&#24046;&#36317;&#65292;&#20294;&#32771;&#34385;&#21040; &#23454;&#39564;&#26159;&#22312;&#38646;-shot&#22330;&#26223;&#19979;&#36827;&#34892;&#30340;&#65292;ChatGPT &#30340;&#34920;&#29616;&#20173;&#28982;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#22312; ADVETA&#65288;RPL&#65289;&#22330;&#26223;&#20013;&#65292;&#21363;&#20351;&#26159;&#38646;-shot ChatGPT &#22312; Spider &#25968;&#25454;&#38598;&#19978;&#20173;&#28982;&#20248;&#20110;&#38656;&#35201;&#24494;&#35843;&#30340; SOTA &#27169;&#22411;&#65292;&#34920;&#29616;&#25552;&#21319;&#20102;4.1\%&#65292;&#23637;&#31034;&#20102;&#23427;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#30340;&#28508;&#21147;&#12290;&#20026;&#20102;&#25903;&#25345;&#30456;&#20851;&#39046;&#22495;&#30340;&#36827;&#19968;&#27493;&#30740;&#31350;&#65292;&#25105;&#20204;&#24050;&#32463;&#20844;&#24320; ChatGPT &#29983;&#25104;&#30340;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper presents the first comprehensive analysis of ChatGPT's Text-to-SQL ability. Given the recent emergence of large-scale conversational language model ChatGPT and its impressive capabilities in both conversational abilities and code generation, we sought to evaluate its Text-to-SQL performance. We conducted experiments on 12 benchmark datasets with different languages, settings, or scenarios, and the results demonstrate that ChatGPT has strong text-to-SQL abilities. Although there is still a gap from the current state-of-the-art (SOTA) model performance, considering that the experiment was conducted in a zero-shot scenario, ChatGPT's performance is still impressive. Notably, in the ADVETA (RPL) scenario, the zero-shot ChatGPT even outperforms the SOTA model that requires fine-tuning on the Spider dataset by 4.1\%, demonstrating its potential for use in practical applications. To support further research in related fields, we have made the data generated by ChatGPT publicly avai
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;OntoMath${}^{\mathrm{PRO}}$&#30340;&#26032;&#29256;&#26412;&#65292;&#35813;&#26412;&#20307;&#20351;&#29992;&#24418;&#24335;&#27169;&#22411;&#23558;&#25968;&#23398;&#20107;&#23454;&#34920;&#31034;&#20026;Linked Open Data&#12290;</title><link>http://arxiv.org/abs/2303.13542</link><description>&lt;p&gt;
OntoMath${}^{\mathbf{PRO}}$ 2.0&#26412;&#20307;&#35770;&#65306;&#27491;&#24335;&#27169;&#22411;&#30340;&#26356;&#26032;
&lt;/p&gt;
&lt;p&gt;
OntoMath${}^{\mathbf{PRO}}$ 2.0 Ontology: Updates of the Formal Model. (arXiv:2303.13542v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13542
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;OntoMath${}^{\mathrm{PRO}}$&#30340;&#26032;&#29256;&#26412;&#65292;&#35813;&#26412;&#20307;&#20351;&#29992;&#24418;&#24335;&#27169;&#22411;&#23558;&#25968;&#23398;&#20107;&#23454;&#34920;&#31034;&#20026;Linked Open Data&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25506;&#35752;&#26412;&#20307;&#35770;&#29992;&#20110;&#25968;&#23398;&#30693;&#35782;&#31649;&#29702;&#21644;&#34920;&#36798;&#30340;&#38382;&#39064;&#12290;&#20027;&#35201;&#20851;&#27880;&#28857;&#22312;&#20110;&#24320;&#21457;&#19968;&#31181;&#24418;&#24335;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;Open Linked Data&#20113;&#20013;&#34920;&#31034;&#25968;&#23398;&#38472;&#36848;&#12290;&#35813;&#27169;&#22411;&#26088;&#22312;&#29992;&#20110;&#20174;&#33258;&#28982;&#35821;&#35328;&#25968;&#23398;&#25991;&#26412;&#20013;&#25552;&#21462;&#25968;&#23398;&#20107;&#23454;&#24182;&#23558;&#36825;&#20123;&#20107;&#23454;&#34920;&#31034;&#20026;Linked Open Data&#30340;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;OntoMath${}^{\mathrm{PRO}}$&#19968;&#20010;&#26032;&#29256;&#26412;&#30340;&#26412;&#20307;&#65292;&#35813;&#26412;&#20307;&#29992;&#20110;&#19987;&#19994;&#25968;&#23398;&#65292;&#24182;&#19988;&#26159;&#35821;&#20041;&#21457;&#24067;&#24179;&#21488;&#30340;&#22522;&#30784;&#65292;&#25509;&#21463;&#20197;LaTeX&#26684;&#24335;&#32534;&#20889;&#30340;&#25968;&#23398;&#35770;&#25991;&#38598;&#65292;&#24182;&#24314;&#31435;&#20854;&#22522;&#20110;&#26412;&#20307;&#30340;Linked Open Data&#34920;&#31034;&#12290;&#35821;&#20041;&#21457;&#24067;&#24179;&#21488;&#26159;OntoMath&#25968;&#23383;&#29983;&#24577;&#31995;&#32479;&#30340;&#26680;&#24515;&#32452;&#20214;&#65292;&#35813;&#29983;&#24577;&#31995;&#32479;&#30001;&#26412;&#20307;&#12289;&#25991;&#26412;&#20998;&#26512;&#24037;&#20855;&#21644;&#25968;&#23398;&#30693;&#35782;&#31649;&#29702;&#24212;&#29992;&#31243;&#24207;&#65288;&#21253;&#25324;&#35821;&#20041;&#25628;&#32034;&#31561;&#65289;&#32452;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper is devoted to the problems of ontology-based mathematical knowledge management and representation. The main attention is paid to the development of a formal model for the representation of mathematical statements in the Open Linked Data cloud. The proposed model is intended for applications that extract mathematical facts from natural language mathematical texts and represent these facts as Linked Open Data. The model is used in development of a new version of the OntoMath${}^{\mathrm{PRO}}$ ontology of professional mathematics is described. OntoMath${}^{\mathrm{PRO}}$ underlies a semantic publishing platform, that takes as an input a collection of mathematical papers in LaTeX format and builds their ontology-based Linked Open Data representation. The semantic publishing platform, in turn, is a central component of OntoMath digital ecosystem, an ecosystem of ontologies, text analytics tools, and applications for mathematical knowledge management, including semantic search fo
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20135;&#21697;&#29983;&#20135;&#21644;&#20351;&#29992;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26816;&#27979;&#20135;&#21697;&#30340;&#30952;&#25439;&#29366;&#24577;&#24182;&#29992;&#20110;&#25913;&#36827;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#30340;&#38598;&#25104;&#21644;&#32467;&#26524;&#21462;&#21521;&#12290;</title><link>http://arxiv.org/abs/2303.13540</link><description>&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#30340;&#24212;&#29992;: &#21033;&#29992;&#35745;&#31639;&#26426;&#35270;&#35273;&#20419;&#36827;&#21487;&#25345;&#32493;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence for Sustainability: Facilitating Sustainable Smart Product-Service Systems with Computer Vision. (arXiv:2303.13540v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13540
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#22312;&#21487;&#25345;&#32493;&#24615;&#26041;&#38754;&#30340;&#20027;&#35201;&#36129;&#29486;&#26159;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20135;&#21697;&#29983;&#20135;&#21644;&#20351;&#29992;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#36890;&#36807;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26816;&#27979;&#20135;&#21697;&#30340;&#30952;&#25439;&#29366;&#24577;&#24182;&#29992;&#20110;&#25913;&#36827;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#30340;&#38598;&#25104;&#21644;&#32467;&#26524;&#21462;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30446;&#21069;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#26469;&#23454;&#29616;&#28165;&#27905;&#29983;&#20135;&#21644;&#21487;&#25345;&#32493;&#24615;&#30446;&#30340;&#30340;&#30740;&#31350;&#36824;&#38750;&#24120;&#26377;&#38480;&#12290;&#26412;&#35770;&#25991;&#23637;&#31034;&#20102;&#22914;&#20309;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#25216;&#26415;&#25552;&#39640;&#20135;&#21697;&#29983;&#20135;&#21644;&#20351;&#29992;&#30340;&#21487;&#25345;&#32493;&#24615;&#65292;&#23588;&#20854;&#26159;&#37319;&#29992;&#28145;&#24230;&#23398;&#20064;&#30340;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#26469;&#35782;&#21035;&#20135;&#21697;&#30340;&#30952;&#25439;&#29366;&#24577;&#65292;&#24182;&#23558;&#36825;&#20123;&#32467;&#26524;&#29992;&#20110;&#25913;&#36827;&#26234;&#33021;&#20135;&#21697;-&#26381;&#21153;&#31995;&#32479;&#30340;&#38598;&#25104;&#21644;&#32467;&#26524;&#21462;&#21521;&#12290;&#27492;&#22806;&#65292;&#36825;&#20123;&#25104;&#26524;&#39044;&#35745;&#23558;&#20419;&#36827;&#20135;&#21697;&#20351;&#29992;&#30340;&#25913;&#36827;&#21644;&#30740;&#21457;&#21019;&#26032;&#12290;&#25105;&#20204;&#22312;&#20004;&#31181;&#20135;&#21697;&#19978;&#28436;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;:&#21152;&#24037;&#24037;&#20855;&#21644;&#26059;&#36716;X&#23556;&#32447;&#38451;&#26497;&#12290;
&lt;/p&gt;
&lt;p&gt;
The usage and impact of deep learning for cleaner production and sustainability purposes remain little explored. This work shows how deep learning can be harnessed to increase sustainability in production and product usage. Specifically, we utilize deep learning-based computer vision to determine the wear states of products. The resulting insights serve as a basis for novel product-service systems with improved integration and result orientation. Moreover, these insights are expected to facilitate product usage improvements and R&amp;D innovations. We demonstrate our approach on two products: machining tools and rotating X-ray anodes. From a technical standpoint, we show that it is possible to recognize the wear state of these products using deep-learning-based computer vision. In particular, we detect wear through microscopic images of the two products. We utilize a U-Net for semantic segmentation to detect wear based on pixel granularity. The resulting mean dice coefficients of 0.631 and
&lt;/p&gt;</description></item><item><title>PBSHM&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#32467;&#26500;&#32676;&#20307;&#36827;&#34892;&#30417;&#27979;&#65292;&#23558;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#22312;&#32467;&#26500;&#23454;&#20363;&#20043;&#38388;&#20256;&#36882;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2303.13533</link><description>&lt;p&gt;
&#22522;&#20110;&#39118;&#38505;&#30340;&#32676;&#20307;&#32467;&#26500;&#20581;&#24247;&#30417;&#35270;&#29702;&#35770;&#65306;&#20197;&#23618;&#27425;&#31995;&#32479;&#20013;&#30340;&#20154;&#21475;&#20026;&#20363;
&lt;/p&gt;
&lt;p&gt;
Towards risk-informed PBSHM: Populations as hierarchical systems. (arXiv:2303.13533v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13533
&lt;/p&gt;
&lt;p&gt;
PBSHM&#26159;&#19968;&#31181;&#26032;&#30340;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#26041;&#27861;&#65292;&#23427;&#36890;&#36807;&#23545;&#32467;&#26500;&#32676;&#20307;&#36827;&#34892;&#30417;&#27979;&#65292;&#23558;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#22312;&#32467;&#26500;&#23454;&#20363;&#20043;&#38388;&#20256;&#36882;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#20915;&#31574;&#30340;&#20934;&#30830;&#24615;&#21644;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;&#65288;SHM&#65289;&#31995;&#32479;&#30340;&#21457;&#23637;&#20013;&#65292;&#26377;&#19968;&#31181;&#22522;&#20110;&#27010;&#29575;&#39118;&#38505;&#30340;&#20915;&#31574;&#26694;&#26550;&#24050;&#32463;&#34987;&#25552;&#20986;&#12290;&#28982;&#32780;&#65292;&#20026;&#20102;&#23398;&#20064;&#20915;&#31574;&#25152;&#38656;&#30340;&#32479;&#35745;&#27169;&#22411;&#65292;&#38656;&#35201;&#20174;&#24863;&#20852;&#36259;&#30340;&#32467;&#26500;&#33719;&#24471;&#27979;&#37327;&#25968;&#25454;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#36825;&#20123;&#25968;&#25454;&#24456;&#23569;&#22312;&#24517;&#35201;&#30340;&#29615;&#22659;&#21644;&#25805;&#20316;&#26465;&#20214;&#19979;&#28085;&#30422;&#36275;&#22815;&#30340;&#33539;&#22260;&#65292;&#20197;&#30830;&#20445;&#27169;&#22411;&#30340;&#33391;&#22909;&#27867;&#21270;&#12290;&#26368;&#36817;&#65292;&#20986;&#29616;&#20102;&#21487;&#20197;&#23558;SHM&#25193;&#23637;&#21040;&#32467;&#26500;&#32676;&#20307;&#30340;&#25216;&#26415;&#65292;&#20197;&#20415;&#22312;&#36275;&#22815;&#30456;&#20284;&#30340;&#32467;&#26500;&#23454;&#20363;&#20043;&#38388;&#20256;&#36882;&#26377;&#20215;&#20540;&#30340;&#30693;&#35782;&#12290;&#36825;&#20010;&#26032;&#26041;&#27861;&#34987;&#31216;&#20026;&#22522;&#20110;&#32676;&#20307;&#30340;&#32467;&#26500;&#20581;&#24247;&#30417;&#27979;(PBSHM)&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#20154;&#32676;&#32467;&#26500;&#30340;&#27491;&#24335;&#34920;&#36798;&#65292;&#20197;&#36827;&#34892;&#22522;&#20110;&#39118;&#38505;&#30340;&#20915;&#31574;&#30340;&#34920;&#31034;&#12290;
&lt;/p&gt;
&lt;p&gt;
The prospect of informed and optimal decision-making regarding the operation and maintenance (O&amp;M) of structures provides impetus to the development of structural health monitoring (SHM) systems. A probabilistic risk-based framework for decision-making has already been proposed. However, in order to learn the statistical models necessary for decision-making, measured data from the structure of interest are required. Unfortunately, these data are seldom available across the range of environmental and operational conditions necessary to ensure good generalisation of the model.  Recently, technologies have been developed that overcome this challenge, by extending SHM to populations of structures, such that valuable knowledge may be transferred between instances of structures that are sufficiently similar. This new approach is termed population-based structural heath monitoring (PBSHM).  The current paper presents a formal representation of populations of structures, such that risk-based d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;&#36845;&#20195;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#26469;&#35299;&#20915;&#25216;&#26415;&#21592;&#36335;&#30001;&#21644;&#35843;&#24230;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#39640;&#25928;&#21021;&#22987;&#21270;&#36807;&#31243;&#12289;&#31934;&#24515;&#36873;&#25321;&#30340;&#37051;&#22495;&#32467;&#26500;&#21644;&#26032;&#39062;&#30340;&#25671;&#21160;&#26426;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;</title><link>http://arxiv.org/abs/2303.13532</link><description>&lt;p&gt;
&#22686;&#24378;&#36845;&#20195;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#29992;&#20110;&#25216;&#26415;&#21592;&#36335;&#30001;&#21644;&#35843;&#24230;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Enhanced Iterated local search for the technician routing and scheduling problem. (arXiv:2303.13532v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13532
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#22686;&#24378;&#30340;&#36845;&#20195;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#26469;&#35299;&#20915;&#25216;&#26415;&#21592;&#36335;&#30001;&#21644;&#35843;&#24230;&#38382;&#39064;&#65292;&#35813;&#31639;&#27861;&#32467;&#21512;&#20102;&#39640;&#25928;&#21021;&#22987;&#21270;&#36807;&#31243;&#12289;&#31934;&#24515;&#36873;&#25321;&#30340;&#37051;&#22495;&#32467;&#26500;&#21644;&#26032;&#39062;&#30340;&#25671;&#21160;&#26426;&#21046;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#34920;&#29616;&#20248;&#24322;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#27954;&#22269;&#23478;&#30340;&#22823;&#22810;&#25968;&#20844;&#20849;&#35774;&#26045;&#65292;&#21253;&#25324;&#27861;&#22269;&#12289;&#24503;&#22269;&#21644;&#33521;&#22269;&#65292;&#22312;1950&#24180;&#33267;1980&#24180;&#30340;&#37325;&#24314;&#39033;&#30446;&#20013;&#24314;&#36896;&#12290;&#30001;&#20110;&#36825;&#20123;&#37325;&#35201;&#22522;&#30784;&#35774;&#26045;&#30340;&#29366;&#20917;&#26085;&#30410;&#24694;&#21270;&#65292;&#36817;&#20960;&#21313;&#24180;&#26469;&#32500;&#25252;&#25805;&#20316;&#25104;&#26412;&#30456;&#23545;&#36739;&#39640;&#12290;&#32500;&#25252;&#25805;&#20316;&#25104;&#26412;&#30340;&#19968;&#37096;&#20998;&#36164;&#37329;&#29992;&#20110;&#25216;&#26415;&#20154;&#21592;&#12290;&#22240;&#27492;&#65292;&#20805;&#20998;&#21033;&#29992;&#21487;&#29992;&#30340;&#21171;&#21160;&#21147;&#23545;&#20110;&#20248;&#21270;&#25805;&#20316;&#25104;&#26412;&#33267;&#20851;&#37325;&#35201;&#12290;&#36825;&#21253;&#25324;&#35745;&#21010;&#25216;&#26415;&#24178;&#39044;&#12289;&#24037;&#20316;&#36127;&#36733;&#24179;&#34913;&#12289;&#29983;&#20135;&#29575;&#25913;&#36827;&#31561;&#12290;&#26412;&#25991;&#20851;&#27880;&#25216;&#26415;&#21592;&#30340;&#36335;&#30001;&#21644;&#20219;&#21153;&#35843;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#35299;&#20915;&#20102;&#19968;&#31181;&#21517;&#20026;&#25216;&#26415;&#21592;&#36335;&#30001;&#21644;&#35843;&#24230;&#38382;&#39064;&#65288;TRSP&#65289;&#30340;&#24037;&#20316;&#21147;&#35843;&#24230;&#38382;&#39064;&#12290;&#36825;&#20010;&#38382;&#39064;&#22312;&#19981;&#21516;&#30340;&#39046;&#22495;&#26377;&#24212;&#29992;&#65292;&#20363;&#22914;&#20132;&#36890;&#22522;&#30784;&#35774;&#26045;&#65288;&#38081;&#36335;&#21644;&#36947;&#36335;&#32593;&#32476;&#65289;&#12289;&#30005;&#20449;&#21644;&#27745;&#27700;&#35774;&#26045;&#12290;&#20026;&#20102;&#35299;&#20915;TRSP&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22686;&#24378;&#30340;&#36845;&#20195;&#23616;&#37096;&#25628;&#32034;&#31639;&#27861;&#65292;&#32467;&#21512;&#20102;&#22522;&#20110;&#36138;&#24515;&#21551;&#21457;&#24335;&#30340;&#39640;&#25928;&#21021;&#22987;&#21270;&#36807;&#31243;&#12289;&#19968;&#32452;&#31934;&#24515;&#36873;&#25321;&#30340;&#37051;&#22495;&#32467;&#26500;&#21644;&#19968;&#31181;&#26032;&#39062;&#30340;&#25671;&#21160;&#26426;&#21046;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;&#31639;&#27861;&#20248;&#20110;&#25991;&#29486;&#20013;&#29616;&#26377;&#31639;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Most public facilities in the European countries, including France, Germany, and the UK, were built during the reconstruction projects between 1950 and 1980. Owing to the deteriorating state of such vital infrastructure has become relatively expensive in the recent decades. A significant part of the maintenance operation costs is spent on the technical staff. Therefore, the optimal use of the available workforce is essential to optimize the operation costs. This includes planning technical interventions, workload balancing, productivity improvement, etc. In this paper, we focus on the routing of technicians and scheduling of their tasks. We address for this purpose a variant of the workforce scheduling problem called the technician routing and scheduling problem (TRSP). This problem has applications in different fields, such as transportation infrastructure (rail and road networks), telecommunications, and sewage facilities. To solve the TRSP, we propose an enhanced iterated local sear
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21512;&#25104;&#26356;&#26131;&#35835;&#21644;&#29702;&#35299;&#30340;&#20998;&#23618;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2303.13531</link><description>&lt;p&gt;
&#22522;&#20110;&#20107;&#20214;&#32858;&#31867;&#30340;&#23618;&#27425;&#36807;&#31243;&#27169;&#22411;&#21457;&#29616;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Discovering Hierarchical Process Models: an Approach Based on Events Clustering. (arXiv:2303.13531v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13531
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20107;&#20214;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#33258;&#21160;&#21512;&#25104;&#26356;&#26131;&#35835;&#21644;&#29702;&#35299;&#30340;&#20998;&#23618;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#31243;&#25366;&#25496;&#26159;&#35745;&#31639;&#26426;&#31185;&#23398;&#30340;&#19968;&#20010;&#39046;&#22495;&#65292;&#22788;&#29702;&#22522;&#20110;&#33258;&#21160;&#29983;&#25104;&#20107;&#20214;&#26085;&#24535;&#30340;&#36807;&#31243;&#27169;&#22411;&#30340;&#21457;&#29616;&#21644;&#20998;&#26512;&#12290;&#30446;&#21069;&#65292;&#35768;&#22810;&#20844;&#21496;&#20351;&#29992;&#27492;&#25216;&#26415;&#26469;&#20248;&#21270;&#21644;&#25913;&#36827;&#20854;&#27969;&#31243;&#12290;&#26412;&#25991;&#32771;&#34385;&#20102;&#20174;&#20302;&#32423;&#20107;&#20214;&#26085;&#24535;&#20013;&#21457;&#29616;&#20998;&#23618;&#19994;&#21153;&#27969;&#31243;&#27169;&#22411;&#30340;&#38382;&#39064;&#65292;&#21363;&#22522;&#20110;&#20449;&#24687;&#31995;&#32479;&#20107;&#20214;&#26085;&#24535;&#20013;&#23384;&#20648;&#30340;&#20449;&#24687;&#33258;&#21160;&#21512;&#25104;&#26356;&#26131;&#35835;&#21644;&#29702;&#35299;&#30340;&#36807;&#31243;&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#39044;&#23450;&#20041;&#20107;&#20214;&#32858;&#31867;&#30340;&#31639;&#27861;&#65292;&#29992;&#20110;&#21457;&#29616;&#34920;&#31034;&#20026;&#20004;&#32423;&#24037;&#20316;&#27969;&#32593;&#30340;&#20998;&#23618;&#36807;&#31243;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
Process mining is a field of computer science that deals with discovery and analysis of process models based on automatically generated event logs. Currently, many companies use this technology for optimization and improving their processes. However, a discovered process model may be too detailed, sophisticated and difficult for experts to understand. In this paper, we consider the problem of discovering a hierarchical business process model from a low-level event log, i.e., the problem of automatic synthesis of more readable and understandable process models based on information stored in event logs of information systems.  Discovery of better structured and more readable process models is intensively studied in the frame of process mining research from different perspectives. In this paper, we present an algorithm for discovering hierarchical process models represented as two-level workflow nets. The algorithm is based on predefined event ilustering so that the cluster defines a sub-
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#34394;&#25311;&#29616;&#23454;&#29615;&#22659;&#20013;&#26197;&#21160;&#30151;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#20010;&#26469;&#33258;&#19981;&#21516;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#35814;&#32454;&#25551;&#36848;&#20102;&#25968;&#25454;&#38598;&#20869;&#23481;&#21644;&#37319;&#38598;&#36807;&#31243;&#65292;&#23558;&#26377;&#21161;&#20110;&#30740;&#31350;&#26197;&#21160;&#30151;&#32531;&#35299;&#21644;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#12290;</title><link>http://arxiv.org/abs/2303.13527</link><description>&lt;p&gt;
&#39044;&#27979;&#34394;&#25311;&#23548;&#33322;&#20219;&#21153;&#20013;&#26197;&#21160;&#30151;&#30340;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
Dataset for predicting cybersickness from a virtual navigation task. (arXiv:2303.13527v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13527
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#34394;&#25311;&#29616;&#23454;&#29615;&#22659;&#20013;&#26197;&#21160;&#30151;&#30340;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#22810;&#20010;&#26469;&#33258;&#19981;&#21516;&#21442;&#19982;&#32773;&#30340;&#25968;&#25454;&#28857;&#65292;&#24182;&#35814;&#32454;&#25551;&#36848;&#20102;&#25968;&#25454;&#38598;&#20869;&#23481;&#21644;&#37319;&#38598;&#36807;&#31243;&#65292;&#23558;&#26377;&#21161;&#20110;&#30740;&#31350;&#26197;&#21160;&#30151;&#32531;&#35299;&#21644;&#39044;&#27979;&#27169;&#22411;&#24320;&#21457;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#20010;&#29992;&#20110;&#39044;&#27979;&#34394;&#25311;&#29616;&#23454;&#29615;&#22659;&#20013;&#26197;&#21160;&#30151;&#30340;&#25968;&#25454;&#38598;&#12290;&#25968;&#25454;&#38598;&#21253;&#25324;&#22312;&#35825;&#21457;&#26197;&#21160;&#30151;&#30340;&#34394;&#25311;&#29615;&#22659;&#20013;&#36827;&#34892;&#23548;&#33322;&#20219;&#21153;&#30340;&#22810;&#20010;&#25968;&#25454;&#28857;&#65292;&#21253;&#25324;&#29983;&#29702;&#21453;&#24212;(EDA&#21644;&#24515;&#29575;)&#21644;&#33258;&#25253;&#26197;&#21160;&#30151;&#29366;&#12290;&#26412;&#25991;&#23558;&#35814;&#32454;&#25551;&#36848;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#25490;&#21015;&#30340;&#23548;&#33322;&#20219;&#21153;&#12289;&#25968;&#25454;&#37319;&#38598;&#36807;&#31243;&#21644;&#25968;&#25454;&#26684;&#24335;&#12290;&#36825;&#20010;&#25968;&#25454;&#38598;&#23558;&#20026;&#30740;&#31350;&#20154;&#21592;&#24320;&#21457;&#21644;&#35780;&#20272;&#26197;&#21160;&#30151;&#39044;&#27979;&#27169;&#22411;&#25552;&#20379;&#26377;&#20215;&#20540;&#30340;&#36164;&#28304;&#65292;&#24182;&#20419;&#36827;&#26197;&#21160;&#30151;&#32531;&#35299;&#30340;&#26356;&#22810;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
This work presents a dataset collected to predict cybersickness in virtual reality environments. The data was collected from navigation tasks in a virtual environment designed to induce cybersickness. The dataset consists of many data points collected from diverse participants, including physiological responses (EDA and Heart Rate) and self-reported cybersickness symptoms. The paper will provide a detailed description of the dataset, including the arranged navigation task, the data collection procedures, and the data format. The dataset will serve as a valuable resource for researchers to develop and evaluate predictive models for cybersickness and will facilitate more research in cybersickness mitigation.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#21333;&#21464;&#37327;&#21644;&#21452;&#21464;&#37327;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20113;&#35745;&#31639;&#20013;&#39044;&#27979;&#26410;&#26469;&#30340;&#36164;&#28304;&#38656;&#27714;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25506;&#35752;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#24335;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.13525</link><description>&lt;p&gt;
&#20113;&#35745;&#31639;&#19979;&#30340;&#19981;&#30830;&#23450;&#24615;&#24037;&#20316;&#36127;&#36733;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Uncertainty-Aware Workload Prediction in Cloud Computing. (arXiv:2303.13525v1 [cs.DC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13525
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#21333;&#21464;&#37327;&#21644;&#21452;&#21464;&#37327;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#22312;&#20113;&#35745;&#31639;&#20013;&#39044;&#27979;&#26410;&#26469;&#30340;&#36164;&#28304;&#38656;&#27714;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#65292;&#24182;&#25506;&#35752;&#19981;&#21516;&#30340;&#35757;&#32451;&#26041;&#24335;&#23545;&#39044;&#27979;&#20934;&#30830;&#24615;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#20113;&#35745;&#31639;&#20013;&#39044;&#27979;&#26410;&#26469;&#30340;&#36164;&#28304;&#38656;&#27714;&#23545;&#20110;&#31649;&#29702;&#20113;&#25968;&#25454;&#20013;&#24515;&#24182;&#20445;&#35777;&#23458;&#25143;&#26368;&#20302;&#26381;&#21153;&#36136;&#37327;&#65288;QoS&#65289;&#27700;&#24179;&#33267;&#20851;&#37325;&#35201;&#12290;&#24314;&#27169;&#26410;&#26469;&#38656;&#27714;&#30340;&#19981;&#30830;&#23450;&#24615;&#21487;&#20197;&#25552;&#39640;&#39044;&#27979;&#30340;&#36136;&#37327;&#24182;&#20943;&#23569;&#30001;&#20110;&#36164;&#28304;&#36807;&#24230;&#20998;&#37197;&#32780;&#24102;&#26469;&#30340;&#28010;&#36153;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#21333;&#21464;&#37327;&#21644;&#21452;&#21464;&#37327;&#36125;&#21494;&#26031;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#29992;&#20110;&#39044;&#27979;&#26410;&#26469;&#36164;&#28304;&#38656;&#27714;&#30340;&#20998;&#24067;&#21450;&#20854;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#35774;&#35745;&#20102;&#19981;&#21516;&#30340;&#35757;&#32451;&#24773;&#26223;&#26469;&#35757;&#32451;&#36825;&#20123;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#36807;&#31243;&#26159;&#22312;&#22810;&#20010;&#25968;&#25454;&#38598;&#37197;&#32622;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#27493;&#39588;&#30340;&#19981;&#21516;&#32452;&#21512;&#12290;&#25105;&#20204;&#36824;&#23558;&#21452;&#21464;&#37327;&#27169;&#22411;&#19982;&#20854;&#21333;&#21464;&#37327;&#23545;&#24212;&#27169;&#22411;&#36827;&#34892;&#27604;&#36739;&#65292;&#29992;&#21333;&#20010;&#25110;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#35757;&#32451;&#65292;&#20197;&#30740;&#31350;&#19981;&#21516;&#32452;&#25104;&#37096;&#20998;&#22914;&#20309;&#24433;&#21709;&#39044;&#27979;&#30340;&#20934;&#30830;&#24615;&#24182;&#24433;&#21709;QoS&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#25105;&#20204;&#30340;&#27169;&#22411;&#26159;&#21542;&#20855;&#26377;&#36801;&#31227;&#23398;&#20064;&#33021;&#21147;&#12290;&#24191;&#27867;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#20351;&#29992;&#22810;&#20010;&#25968;&#25454;&#38598;&#36827;&#34892;&#39044;&#35757;&#32451;&#21487;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predicting future resource demand in Cloud Computing is essential for managing Cloud data centres and guaranteeing customers a minimum Quality of Service (QoS) level. Modelling the uncertainty of future demand improves the quality of the prediction and reduces the waste due to overallocation. In this paper, we propose univariate and bivariate Bayesian deep learning models to predict the distribution of future resource demand and its uncertainty. We design different training scenarios to train these models, where each procedure is a different combination of pretraining and fine-tuning steps on multiple datasets configurations. We also compare the bivariate model to its univariate counterpart training with one or more datasets to investigate how different components affect the accuracy of the prediction and impact the QoS. Finally, we investigate whether our models have transfer learning capabilities. Extensive experiments show that pretraining with multiple datasets boosts performances 
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20351;&#29992;AI&#27450;&#39575;&#39575;&#23376;&#24182;&#28010;&#36153;&#20182;&#20204;&#26102;&#38388;&#21644;&#36164;&#28304;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#26469;&#22238;&#22797;&#27450;&#35784;&#37038;&#20214;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#21487;&#20197;&#25104;&#21151;&#27450;&#39575;&#39575;&#23376;&#65292;&#35777;&#26126;AI&#26159;&#21453;&#20987;&#30005;&#23376;&#37038;&#20214;&#23041;&#32961;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;</title><link>http://arxiv.org/abs/2303.13521</link><description>&lt;p&gt;
&#27450;&#39575;&#39575;&#23376;&#65306;&#20351;&#29992;ChatGPT&#22238;&#22797;&#27450;&#35784;&#37038;&#20214;&#28010;&#36153;&#26102;&#38388;&#21644;&#36164;&#28304;
&lt;/p&gt;
&lt;p&gt;
Scamming the Scammers: Using ChatGPT to Reply Mails for Wasting Time and Resources. (arXiv:2303.13521v1 [cs.CR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13521
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#25506;&#35752;&#20351;&#29992;AI&#27450;&#39575;&#39575;&#23376;&#24182;&#28010;&#36153;&#20182;&#20204;&#26102;&#38388;&#21644;&#36164;&#28304;&#30340;&#26041;&#27861;&#65292;&#21033;&#29992;ChatGPT&#30340;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#26469;&#22238;&#22797;&#27450;&#35784;&#37038;&#20214;&#12290;&#21021;&#27493;&#32467;&#26524;&#26174;&#31034;&#65292;ChatGPT&#21487;&#20197;&#25104;&#21151;&#27450;&#39575;&#39575;&#23376;&#65292;&#35777;&#26126;AI&#26159;&#21453;&#20987;&#30005;&#23376;&#37038;&#20214;&#23041;&#32961;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20351;&#29992;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#25903;&#25345;&#32593;&#32476;&#23433;&#20840;&#25805;&#20316;&#29616;&#22312;&#26159;&#19968;&#20010;&#34987;&#39564;&#35777;&#36807;&#30340;&#20570;&#27861;&#65292;&#20363;&#22914;&#65292;&#26816;&#27979;&#24694;&#24847;&#20195;&#30721;&#25110;&#37197;&#32622;&#27969;&#37327;&#36807;&#28388;&#31574;&#30053;&#12290;&#26368;&#36817;&#28044;&#29616;&#30340;AI&#29983;&#25104;&#25216;&#26415;&#21644;&#20855;&#26377;&#39640;&#25928;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#33021;&#21147;&#30340;&#26694;&#26550;&#22823;&#22823;&#25193;&#22823;&#20102;&#21487;&#33021;&#30340;&#24212;&#29992;&#33539;&#22260;&#65292;&#26088;&#22312;&#22686;&#21152;&#20114;&#32852;&#32593;&#23433;&#20840;&#24615;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;ChatGPT&#29983;&#20135;&#25991;&#26412;&#30340;&#33021;&#21147;&#65292;&#21516;&#26102;&#27169;&#20223;&#36924;&#30495;&#30340;&#20154;&#31867;&#20114;&#21160;&#65292;&#21487;&#20197;&#29992;&#20110;&#20943;&#36731;&#21253;&#21547;&#39575;&#23616;&#30340;&#30005;&#23376;&#37038;&#20214;&#30340;&#22256;&#25200;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#30740;&#31350;&#20102;&#20351;&#29992;AI&#19982;&#39575;&#23376;&#36827;&#34892;&#33258;&#21160;&#21644;&#26080;&#24847;&#20041;&#36890;&#20449;&#30340;&#26041;&#27861;&#65292;&#20197;&#27492;&#26469;&#28010;&#36153;&#20182;&#20204;&#30340;&#26102;&#38388;&#21644;&#36164;&#28304;&#12290;&#21021;&#27493;&#32467;&#26524;&#23637;&#31034;&#20102;ChatGPT&#33021;&#22815;&#27450;&#39575;&#39575;&#23376;&#65292;&#20174;&#32780;&#35777;&#23454;&#20102;AI&#26159;&#21453;&#20987;&#36890;&#36807;&#30005;&#23376;&#37038;&#20214;&#20256;&#36882;&#30340;&#23041;&#32961;&#30340;&#26377;&#25928;&#24037;&#20855;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#24378;&#35843;&#20102;&#22810;&#31181;&#24433;&#21709;&#21644;&#38656;&#35201;&#35299;&#20915;&#30340;&#24320;&#25918;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The use of Artificial Intelligence (AI) to support cybersecurity operations is now a consolidated practice, e.g., to detect malicious code or configure traffic filtering policies. The recent surge of AI, generative techniques and frameworks with efficient natural language processing capabilities dramatically magnifies the number of possible applications aimed at increasing the security of the Internet. Specifically, the ability of ChatGPT to produce textual contents while mimicking realistic human interactions can be used to mitigate the plague of emails containing scams. Therefore, this paper investigates the use of AI to engage scammers in automatized and pointless communications, with the goal of wasting both their time and resources. Preliminary results showcase that ChatGPT is able to decoy scammers, thus confirming that AI is an effective tool to counteract threats delivered via mail. In addition, we highlight the multitude of implications and open research questions to be addres
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#39044;&#35774;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#31070;&#32463;&#39068;&#33394;&#26144;&#23556;&#26041;&#27861;&#65288;DNCM&#65289;&#21644;&#20004;&#38454;&#27573;&#27969;&#27700;&#32447;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2303.13511</link><description>&lt;p&gt;
&#31070;&#32463;&#39044;&#35774;&#65306;&#29992;&#20110;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#30340;&#26032;&#25216;&#26415;
&lt;/p&gt;
&lt;p&gt;
Neural Preset for Color Style Transfer. (arXiv:2303.13511v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.13511
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#39044;&#35774;&#30340;&#25216;&#26415;&#65292;&#36890;&#36807;&#30830;&#23450;&#24615;&#31070;&#32463;&#39068;&#33394;&#26144;&#23556;&#26041;&#27861;&#65288;DNCM&#65289;&#21644;&#20004;&#38454;&#27573;&#27969;&#27700;&#32447;&#23454;&#29616;&#39640;&#36136;&#37327;&#30340;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#65292;&#24182;&#19988;&#20855;&#26377;&#21508;&#31181;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;&#31070;&#32463;&#39044;&#35774;&#30340;&#25216;&#26415;&#65292;&#29992;&#20110;&#35299;&#20915;&#29616;&#26377;&#39068;&#33394;&#39118;&#26684;&#36716;&#31227;&#26041;&#27861;&#30340;&#38480;&#21046;&#65292;&#21253;&#25324;&#21487;&#35270;&#21270;&#20266;&#24433;&#12289;&#22823;&#37327;&#20869;&#23384;&#38656;&#27714;&#21644;&#32531;&#24930;&#30340;&#39118;&#26684;&#20999;&#25442;&#36895;&#24230;&#12290;&#26412;&#26041;&#27861;&#22522;&#20110;&#20004;&#20010;&#26680;&#24515;&#35774;&#35745;&#12290;&#39318;&#20808;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#30830;&#23450;&#24615;&#31070;&#32463;&#39068;&#33394;&#26144;&#23556;&#26041;&#27861;&#65288;DNCM&#65289;&#65292;&#36890;&#36807;&#33258;&#36866;&#24212;&#30340;&#39068;&#33394;&#26144;&#23556;&#30697;&#38453;&#22312;&#27599;&#20010;&#20687;&#32032;&#19978;&#36827;&#34892;&#19968;&#33268;&#30340;&#25805;&#20316;&#65292;&#36991;&#20813;&#20102;&#20266;&#24433;&#65292;&#24182;&#25903;&#25345;&#20855;&#26377;&#23567;&#20869;&#23384;&#21344;&#29992;&#30340;&#39640;&#20998;&#36776;&#29575;&#36755;&#20837;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#23558;&#20219;&#21153;&#20998;&#20026;&#39068;&#33394;&#24402;&#19968;&#21270;&#21644;&#39118;&#26684;&#21270;&#20004;&#20010;&#38454;&#27573;&#26469;&#24320;&#21457;&#19968;&#20010;&#20004;&#38454;&#27573;&#27969;&#27700;&#32447;&#65292;&#21487;&#20197;&#36890;&#36807;&#23558;&#39068;&#33394;&#39118;&#26684;&#20316;&#20026;&#39044;&#35774;&#25552;&#21462;&#65292;&#24182;&#22312;&#24402;&#19968;&#21270;&#30340;&#36755;&#20837;&#22270;&#20687;&#19978;&#37325;&#22797;&#20351;&#29992;&#23427;&#20204;&#26469;&#23454;&#29616;&#26377;&#25928;&#30340;&#39118;&#26684;&#20999;&#25442;&#12290;&#30001;&#20110;&#23384;&#22312;&#25104;&#23545;&#25968;&#25454;&#38598;&#30340;&#38382;&#39064;&#65292;&#25105;&#20204;&#25551;&#36848;&#20102;&#22914;&#20309;&#36890;&#36807;&#33258;&#30417;&#30563;&#31574;&#30053;&#35757;&#32451;&#31070;&#32463;&#39044;&#35774;&#27169;&#22411;&#12290;&#36890;&#36807;&#20840;&#38754;&#30340;&#35780;&#20272;&#23637;&#31034;&#20102;&#31070;&#32463;&#39044;&#35774;&#30456;&#23545;&#20110;&#29616;&#26377;&#26041;&#27861;&#30340;&#21508;&#31181;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#33258;&#28982;&#22320;&#25903;&#25345;&#22810;&#20010;&#39118;&#26684;&#65292;&#24182;&#22312;&#23450;&#37327;&#21644;&#23450;&#24615;&#24615;&#33021;&#19978;&#23454;&#29616;&#20102;&#26368;&#26032;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we present a Neural Preset technique to address the limitations of existing color style transfer methods, including visual artifacts, vast memory requirement, and slow style switching speed. Our method is based on two core designs. First, we propose Deterministic Neural Color Mapping (DNCM) to consistently operate on each pixel via an image-adaptive color mapping matrix, avoiding artifacts and supporting high-resolution inputs with a small memory footprint. Second, we develop a two-stage pipeline by dividing the task into color normalization and stylization, which allows efficient style switching by extracting color styles as presets and reusing them on normalized input images. Due to the unavailability of pairwise datasets, we describe how to train Neural Preset via a self-supervised strategy. Various advantages of Neural Preset over existing methods are demonstrated through comprehensive evaluations. Besides, we show that our trained model can naturally support multipl
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;GPT&#65288;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#21644;&#30456;&#20851;&#25216;&#26415;&#23545;&#32654;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#21457;&#29616;&#22823;&#32422;80%&#30340;&#32654;&#22269;&#21171;&#21160;&#21147;&#21487;&#33021;&#20250;&#21463;&#21040;10%&#30340;&#24037;&#20316;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#24037;&#36164;&#27700;&#24179;&#21644;&#21508;&#34892;&#21508;&#19994;&#65292;&#39044;&#31034;&#30528;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#25919;&#31574;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2303.10130</link><description>&lt;p&gt;
GPT&#26159;GPT&#65306;&#22823;&#35821;&#35328;&#27169;&#22411;&#23545;&#21171;&#21160;&#21147;&#24066;&#22330;&#24433;&#21709;&#30340;&#26089;&#26399;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
GPTs are GPTs: An Early Look at the Labor Market Impact Potential of Large Language Models. (arXiv:2303.10130v1 [econ.GN])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.10130
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#35843;&#26597;&#20102;GPT&#65288;&#22823;&#35821;&#35328;&#27169;&#22411;&#65289;&#21644;&#30456;&#20851;&#25216;&#26415;&#23545;&#32654;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#65292;&#21457;&#29616;&#22823;&#32422;80%&#30340;&#32654;&#22269;&#21171;&#21160;&#21147;&#21487;&#33021;&#20250;&#21463;&#21040;10%&#30340;&#24037;&#20316;&#20219;&#21153;&#30340;&#24433;&#21709;&#65292;&#28085;&#30422;&#20102;&#25152;&#26377;&#24037;&#36164;&#27700;&#24179;&#21644;&#21508;&#34892;&#21508;&#19994;&#65292;&#39044;&#31034;&#30528;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#25919;&#31574;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#65288;GPT&#65289;&#27169;&#22411;&#21644;&#30456;&#20851;&#25216;&#26415;&#23545;&#32654;&#22269;&#21171;&#21160;&#21147;&#24066;&#22330;&#30340;&#28508;&#22312;&#24433;&#21709;&#12290;&#20351;&#29992;&#26032;&#30340;&#26631;&#20934;&#65292;&#25105;&#20204;&#35780;&#20272;&#32844;&#19994;&#19982;GPT&#33021;&#21147;&#30340;&#23545;&#24212;&#20851;&#31995;&#65292;&#32467;&#21512;&#20154;&#31867;&#19987;&#19994;&#30693;&#35782;&#21644;GPT-4&#30340;&#20998;&#31867;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#32422;80%&#30340;&#32654;&#22269;&#21171;&#21160;&#21147;&#21487;&#33021;&#20250;&#33267;&#23569;&#26377;10%&#30340;&#24037;&#20316;&#20219;&#21153;&#21463;&#21040;GPT&#24341;&#20837;&#30340;&#24433;&#21709;&#65292;&#32780;&#32422;19%&#30340;&#24037;&#20154;&#21487;&#33021;&#20250;&#30475;&#21040;&#33267;&#23569;50%&#30340;&#20219;&#21153;&#21463;&#21040;&#24433;&#21709;&#12290;&#24433;&#21709;&#33539;&#22260;&#28085;&#30422;&#20102;&#25152;&#26377;&#24037;&#36164;&#27700;&#24179;&#65292;&#39640;&#25910;&#20837;&#24037;&#20316;&#21487;&#33021;&#38754;&#20020;&#26356;&#22823;&#30340;&#39118;&#38505;&#12290;&#20540;&#24471;&#27880;&#24847;&#30340;&#26159;&#65292;&#24433;&#21709;&#24182;&#19981;&#23616;&#38480;&#20110;&#26368;&#36817;&#29983;&#20135;&#29575;&#22686;&#38271;&#36739;&#39640;&#30340;&#34892;&#19994;&#12290;&#25105;&#20204;&#24471;&#20986;&#32467;&#35770;&#65292;&#29983;&#25104;&#39044;&#35757;&#32451;&#21464;&#21387;&#22120;&#20855;&#26377;&#36890;&#29992;&#25216;&#26415;&#65288;GPT&#65289;&#30340;&#29305;&#24615;&#65292;&#34920;&#26126;&#36825;&#20123;&#27169;&#22411;&#21487;&#33021;&#20855;&#26377;&#26174;&#33879;&#30340;&#32463;&#27982;&#12289;&#31038;&#20250;&#21644;&#25919;&#31574;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the potential implications of Generative Pre-trained Transformer (GPT) models and related technologies on the U.S. labor market. Using a new rubric, we assess occupations based on their correspondence with GPT capabilities, incorporating both human expertise and classifications from GPT-4. Our findings indicate that approximately 80% of the U.S. workforce could have at least 10% of their work tasks affected by the introduction of GPTs, while around 19% of workers may see at least 50% of their tasks impacted. The influence spans all wage levels, with higher-income jobs potentially facing greater exposure. Notably, the impact is not limited to industries with higher recent productivity growth. We conclude that Generative Pre-trained Transformers exhibit characteristics of general-purpose technologies (GPTs), suggesting that as these models could have notable economic, social, and policy implications.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28304;&#20195;&#30721;&#25511;&#21046;&#27969;&#30340;&#32423;&#32852;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;VMCDL&#65292;&#36890;&#36807;&#23545;SARD&#25968;&#25454;&#38598;&#20013;&#30340;&#28304;&#20195;&#30721;&#36827;&#34892;&#22788;&#29702;&#26469;&#26377;&#25928;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#12290;</title><link>http://arxiv.org/abs/2303.07128</link><description>&lt;p&gt;
&#22522;&#20110;&#28304;&#20195;&#30721;&#25511;&#21046;&#27969;&#30340;&#32423;&#32852;&#28145;&#24230;&#23398;&#20064;&#28431;&#27934;&#25366;&#25496;
&lt;/p&gt;
&lt;p&gt;
VMCDL: Vulnerability Mining Based on Cascaded Deep Learning Under Source Control Flow. (arXiv:2303.07128v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.07128
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28304;&#20195;&#30721;&#25511;&#21046;&#27969;&#30340;&#32423;&#32852;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;VMCDL&#65292;&#36890;&#36807;&#23545;SARD&#25968;&#25454;&#38598;&#20013;&#30340;&#28304;&#20195;&#30721;&#36827;&#34892;&#22788;&#29702;&#26469;&#26377;&#25928;&#26816;&#27979;&#36719;&#20214;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#35745;&#31639;&#26426;&#34892;&#19994;&#21644;&#36719;&#20214;&#30340;&#24555;&#36895;&#21457;&#23637;&#65292;&#36719;&#20214;&#28431;&#27934;&#34987;&#21033;&#29992;&#30340;&#39118;&#38505;&#22823;&#22823;&#22686;&#21152;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#27844;&#38706;&#28304;&#30740;&#31350;&#25366;&#25496;&#25216;&#26415;&#20173;&#23384;&#22312;&#35768;&#22810;&#32570;&#28857;&#65292;&#20363;&#22914;&#39640;&#35823;&#25253;&#29575;&#12289;&#31895;&#31890;&#24230;&#26816;&#27979;&#21644;&#23545;&#19987;&#23478;&#32463;&#39564;&#30340;&#20381;&#36182;&#12290;&#26412;&#25991;&#20027;&#35201;&#21033;&#29992;SARD&#25968;&#25454;&#38598;&#30340;c/c++&#28304;&#20195;&#30721;&#25968;&#25454;&#65292;&#22788;&#29702;CWE476&#12289;CWE469&#12289;CWE516&#21644;CWE570&#28431;&#27934;&#31867;&#22411;&#30340;&#28304;&#20195;&#30721;&#65292;&#27979;&#35797;&#26368;&#21069;&#27839;&#24037;&#20855;Joern&#28431;&#27934;&#25195;&#25551;&#21151;&#33021;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#28304;&#20195;&#30721;&#25511;&#21046;&#27969;&#30340;&#26032;&#22411;&#32423;&#32852;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;VMCDL&#26469;&#26377;&#25928;&#26816;&#27979;&#28431;&#27934;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the rapid development of the computer industry and computer software, the risk of software vulnerabilities being exploited has greatly increased. However, there are still many shortcomings in the existing mining techniques for leakage source research, such as high false alarm rate, coarse-grained detection, and dependence on expert experience. In this paper, we mainly use the c/c++ source code data of the SARD dataset, process the source code of CWE476, CWE469, CWE516 and CWE570 vulnerability types, test the Joern vulnerability scanning function of the cutting-edge tool, and propose a new cascading deep learning model VMCDL based on source code control flow to effectively detect vulnerabilities. First, this paper uses joern to locate and extract sensitive functions and statements to form a sensitive statement library of vulnerable code. Then, the CFG flow vulnerability code snippets are generated by bidirectional breadth-first traversal, and then vectorized by Doc2vec. Finally, th
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23558;&#21333;&#27493;&#23545;&#25239;&#31034;&#20363;&#20998;&#35299;&#20026;&#25968;&#25454;&#20449;&#24687;&#21644;&#33258;&#20449;&#24687;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#31216;&#20026;&#8220;&#33258;&#25105;&#25311;&#21512;&#8221;&#65292;&#21363;&#32593;&#32476;&#23398;&#20064;&#21333;&#27493;&#25200;&#21160;&#20013;&#23884;&#20837;&#30340;&#33258;&#20449;&#24687;&#65292;&#23548;&#33268;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#12290;</title><link>http://arxiv.org/abs/2302.11963</link><description>&lt;p&gt;
&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#20013;&#30340;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#30340;&#33258;&#25105;&#25311;&#21512;&#35270;&#35282;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Investigating Catastrophic Overfitting in Fast Adversarial Training: A Self-fitting Perspective. (arXiv:2302.11963v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.11963
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23558;&#21333;&#27493;&#23545;&#25239;&#31034;&#20363;&#20998;&#35299;&#20026;&#25968;&#25454;&#20449;&#24687;&#21644;&#33258;&#20449;&#24687;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#31216;&#20026;&#8220;&#33258;&#25105;&#25311;&#21512;&#8221;&#65292;&#21363;&#32593;&#32476;&#23398;&#20064;&#21333;&#27493;&#25200;&#21160;&#20013;&#23884;&#20837;&#30340;&#33258;&#20449;&#24687;&#65292;&#23548;&#33268;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34429;&#28982;&#24555;&#36895;&#23545;&#25239;&#35757;&#32451;&#25552;&#20379;&#20102;&#19968;&#31181;&#26500;&#24314;&#24378;&#20581;&#32593;&#32476;&#30340;&#39640;&#25928;&#26041;&#27861;&#65292;&#20294;&#23427;&#21487;&#33021;&#20250;&#38754;&#20020;&#19968;&#20010;&#20005;&#37325;&#38382;&#39064;&#65292;&#21363;&#28798;&#38590;&#24615;&#36807;&#25311;&#21512;&#65288;CO&#65289;&#65292;&#20854;&#20013;&#22810;&#27493;&#24378;&#20581;&#20934;&#30830;&#29575;&#31361;&#28982;&#38477;&#33267;&#38646;&#12290;&#26412;&#25991;&#39318;&#27425;&#23558;&#21333;&#27493;&#23545;&#25239;&#31034;&#20363;&#20998;&#35299;&#20026;&#25968;&#25454;&#20449;&#24687;&#21644;&#33258;&#20449;&#24687;&#65292;&#25581;&#31034;&#20102;&#19968;&#31181;&#26377;&#36259;&#30340;&#29616;&#35937;&#65292;&#31216;&#20026;&#8220;&#33258;&#25105;&#25311;&#21512;&#8221;&#12290;
&lt;/p&gt;
&lt;p&gt;
Although fast adversarial training provides an efficient approach for building robust networks, it may suffer from a serious problem known as catastrophic overfitting (CO), where multi-step robust accuracy suddenly collapses to zero. In this paper, we for the first time decouple single-step adversarial examples into data-information and self-information, which reveals an interesting phenomenon called "self-fitting". Self-fitting, i.e., the network learns the self-information embedded in single-step perturbations, naturally leads to the occurrence of CO. When self-fitting occurs, the network experiences an obvious "channel differentiation" phenomenon that some convolution channels accounting for recognizing self-information become dominant, while others for data-information are suppressed. In this way, the network can only recognize images with sufficient self-information and loses generalization ability to other types of data. Based on self-fitting, we provide new insights into the exi
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;UX 3.0&#8221;&#33539;&#24335;&#26694;&#26550;&#21644;&#30456;&#24212;&#30340;UX&#26041;&#27861;&#31995;&#32479;&#65292;&#26088;&#22312;&#28385;&#36275;&#24773;&#25253;&#26102;&#20195;&#23545;&#29992;&#25143;&#20307;&#39564;&#30340;&#26032;&#35201;&#27714;&#12290;</title><link>http://arxiv.org/abs/2302.06681</link><description>&lt;p&gt;
&#29992;&#25143;&#20013;&#24515;&#35774;&#35745; (IX): &#24773;&#25253;&#26102;&#20195;&#19979;&#30340;&#8220;&#29992;&#25143;&#20307;&#39564;3.0&#8221;&#33539;&#24335;&#26694;&#26550;
&lt;/p&gt;
&lt;p&gt;
User-Centered Design (IX): A "User Experience 3.0" Paradigm Framework in the Intelligence Era. (arXiv:2302.06681v5 [cs.HC] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.06681
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;UX 3.0&#8221;&#33539;&#24335;&#26694;&#26550;&#21644;&#30456;&#24212;&#30340;UX&#26041;&#27861;&#31995;&#32479;&#65292;&#26088;&#22312;&#28385;&#36275;&#24773;&#25253;&#26102;&#20195;&#23545;&#29992;&#25143;&#20307;&#39564;&#30340;&#26032;&#35201;&#27714;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#8220;&#29992;&#25143;&#20013;&#24515;&#35774;&#35745;&#8221;&#29702;&#24565;&#30340;&#29992;&#25143;&#20307;&#39564;&#65288;UX&#65289;&#39046;&#22495;&#27491;&#22312;&#21521;&#24773;&#25253;&#26102;&#20195;&#36808;&#36827;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;UX&#33539;&#24335;&#20027;&#35201;&#38024;&#23545;&#38750;&#26234;&#33021;&#31995;&#32479;&#65292;&#24182;&#32570;&#20047;&#38024;&#23545;&#26234;&#33021;&#31995;&#32479;&#30340;&#31995;&#32479;&#21270;&#26041;&#27861;&#12290;&#22312;UX&#30340;&#21457;&#23637;&#36807;&#31243;&#20013;&#65292;UX&#33539;&#24335;&#21576;&#29616;&#20986;&#36328;&#25216;&#26415;&#26102;&#20195;&#30340;&#28436;&#36827;&#29305;&#24449;&#12290;&#30446;&#21069;&#65292;&#24773;&#25253;&#26102;&#20195;&#23545;UX&#33539;&#24335;&#25552;&#20986;&#20102;&#26032;&#30340;&#35201;&#27714;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#8220;UX 3.0&#8221;&#33539;&#24335;&#26694;&#26550;&#21644;&#30456;&#24212;&#30340;UX&#26041;&#27861;&#31995;&#32479;&#12290; &#8220;UX 3.0&#8221;&#33539;&#24335;&#26694;&#26550;&#21253;&#25324;&#20116;&#31867;UX&#26041;&#27861;&#65306;&#29983;&#24577;&#20307;&#39564;&#65292;&#21019;&#26032;&#20307;&#39564;&#65292;AI&#20307;&#39564;&#65292;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#20132;&#20114;&#30340;&#20307;&#39564;&#65292;&#20197;&#21450;&#22522;&#20110;&#20154;&#24037;&#26234;&#33021;&#21327;&#20316;&#30340;&#20307;&#39564;&#26041;&#27861;&#65292;&#27599;&#20010;&#26041;&#27861;&#37117;&#25552;&#20379;&#30456;&#24212;&#30340;&#22810;&#37325;UX&#33539;&#24335;&#21462;&#21521;&#12290;&#25552;&#20986;&#8220;UX 3.0&#8221;&#33539;&#24335;&#30340;&#24314;&#35758;&#26377;&#21161;&#20110;&#25913;&#36827;&#29616;&#26377;&#30340;UX&#26041;&#27861;&#65292;&#24182;&#25552;&#20379;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The field of user experience (UX) based on the design philosophy of "user-centered design" is moving towards the intelligence era. Still, the existing UX paradigm mainly aims at non-intelligent systems and lacks a systematic approach to UX for intelligent systems. Throughout the development of UX, the UX paradigm shows the evolution characteristics of the cross-technology era. At present, the intelligence era has put forward new demands on the UX paradigm. For this reason, this paper proposes a "UX 3.0" paradigm framework and the corresponding UX methodology system in the intelligence era. The "UX 3.0" paradigm framework includes five categories of UX methods: ecological experience, innovation-enabled experience, AI-enabled experience, human-AI interaction-based experience, and human-AI collaboration-based experience methods, each providing corresponding multiple UX paradigmatic orientations. The proposal of the "UX 3.0" paradigm helps improve the existing UX methods and provides metho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22686;&#24378;&#29983;&#25104;&#25968;&#25454;&#38598;&#20013;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#26080;&#20851;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;1.4mAP&#65288;CUB&#25968;&#25454;&#38598;&#65289;&#21644;8.4mAP&#65288;ImageNet&#25968;&#25454;&#38598;&#65289;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#19979;&#28216;&#30340;&#20998;&#31867;&#26550;&#26500;&#12290;</title><link>http://arxiv.org/abs/2302.03298</link><description>&lt;p&gt;
&#38656;&#35201;&#22810;&#26679;&#24615;&#65306;&#36890;&#36807;&#31283;&#23450;&#30340;&#25193;&#25955;&#25913;&#21892;&#27169;&#22411;&#26080;&#20851;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Diversity is Definitely Needed: Improving Model-Agnostic Zero-shot Classification via Stable Diffusion. (arXiv:2302.03298v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03298
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#22686;&#24378;&#29983;&#25104;&#25968;&#25454;&#38598;&#20013;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;&#27169;&#22411;&#26080;&#20851;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;1.4mAP&#65288;CUB&#25968;&#25454;&#38598;&#65289;&#21644;8.4mAP&#65288;ImageNet&#25968;&#25454;&#38598;&#65289;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#20219;&#20309;&#19979;&#28216;&#30340;&#20998;&#31867;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#27169;&#22411;&#26080;&#20851;&#30340;&#38646;&#26679;&#26412;&#20998;&#31867;&#65288;MA-ZSC&#65289;&#38382;&#39064;&#65292;&#36825;&#24847;&#21619;&#30528;&#35757;&#32451;&#20998;&#31867;&#26550;&#26500;&#26469;&#23545;&#30495;&#23454;&#22270;&#20687;&#36827;&#34892;&#20998;&#31867;&#65292;&#32780;&#22312;&#35757;&#32451;&#36807;&#31243;&#20013;&#19981;&#20351;&#29992;&#20219;&#20309;&#30495;&#23454;&#22270;&#20687;&#12290;&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#20351;&#29992;&#25193;&#25955;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#35757;&#32451;&#22270;&#20687;&#21487;&#20197;&#25552;&#20379;&#28508;&#22312;&#30340;&#35299;&#20915;MA-ZSC&#38382;&#39064;&#30340;&#26041;&#27861;&#12290;&#28982;&#32780;&#65292;&#35813;&#26041;&#27861;&#30340;&#24615;&#33021;&#30446;&#21069;&#20173;&#28982;&#19981;&#22914;&#22823;&#35268;&#27169;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#25152;&#21462;&#24471;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#25913;&#21892;&#29983;&#25104;&#25968;&#25454;&#38598;&#20013;&#22270;&#20687;&#30340;&#22810;&#26679;&#24615;&#26469;&#25552;&#39640;MA-ZSC&#24615;&#33021;&#30340;&#26032;&#24605;&#36335;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#32452;&#20462;&#25913;&#25991;&#26412;&#21040;&#22270;&#20687;&#29983;&#25104;&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#25193;&#25955;&#27169;&#22411;&#26469;&#22686;&#24378;&#22810;&#26679;&#24615;&#65292;&#25105;&#20204;&#31216;&#20043;&#20026;&#8220;&#32477;&#25307;&#8221;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#36866;&#29992;&#20110;&#20219;&#20309;&#19979;&#28216;&#30340;&#20998;&#31867;&#26550;&#26500;&#65292;&#23613;&#31649;&#25105;&#20204;&#37325;&#28857;&#20851;&#27880;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#22312;CUB&#25968;&#25454;&#38598;&#19978;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#27604;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#25552;&#39640;&#20102;1.4mAP&#65292;&#22312;ImageNet&#25968;&#25454;&#38598;&#19978;&#25552;&#39640;&#20102;8.4mAP&#65292;&#21516;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;&#29983;&#25104;&#22270;&#20687;&#36827;&#34892;&#35757;&#32451;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we investigate the problem of Model-Agnostic Zero-Shot Classification (MA-ZSC), which refers to training non-specific classification architectures (downstream models) to classify real images without using any real images during training. Recent research has demonstrated that generating synthetic training images using diffusion models provides a potential solution to address MA-ZSC. However, the performance of this approach currently falls short of that achieved by large-scale vision-language models. One possible explanation is a potential significant domain gap between synthetic and real images. Our work offers a fresh perspective on the problem by providing initial insights that MA-ZSC performance can be improved by improving the diversity of images in the generated dataset. We propose a set of modifications to the text-to-image generation process using a pre-trained diffusion model to enhance diversity, which we refer to as our $\textbf{bag of tricks}$. Our approach sho
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23558;&#19968;&#20123;&#38750;&#32463;&#20856;&#30340;&#20154;&#31867;&#20915;&#31574;&#27169;&#22411;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#37327;&#23376;&#35745;&#31639;&#26426;&#30005;&#36335;&#65292;&#24182;&#36890;&#36807;&#37327;&#23376;&#27010;&#29575;&#12289;&#35282;&#24230;&#21644;&#23376;&#31354;&#38388;&#31561;&#26041;&#24335;&#25551;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#21644;&#23427;&#20204;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#23454;&#29616;&#21644;&#30740;&#31350;&#12290;</title><link>http://arxiv.org/abs/2302.03012</link><description>&lt;p&gt;
&#29992;&#20110;&#35748;&#30693;&#20915;&#31574;&#30340;&#37327;&#23376;&#30005;&#36335;&#32452;&#20214;
&lt;/p&gt;
&lt;p&gt;
Quantum Circuit Components for Cognitive Decision-Making. (arXiv:2302.03012v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.03012
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23558;&#19968;&#20123;&#38750;&#32463;&#20856;&#30340;&#20154;&#31867;&#20915;&#31574;&#27169;&#22411;&#25104;&#21151;&#22320;&#24212;&#29992;&#20110;&#37327;&#23376;&#35745;&#31639;&#26426;&#30005;&#36335;&#65292;&#24182;&#36890;&#36807;&#37327;&#23376;&#27010;&#29575;&#12289;&#35282;&#24230;&#21644;&#23376;&#31354;&#38388;&#31561;&#26041;&#24335;&#25551;&#36848;&#20102;&#36825;&#20123;&#27169;&#22411;&#21644;&#23427;&#20204;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#30340;&#23454;&#29616;&#21644;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#34920;&#26126;&#65292;&#19968;&#20123;&#38750;&#32463;&#20856;&#30340;&#20154;&#31867;&#20915;&#31574;&#27169;&#22411;&#21487;&#20197;&#25104;&#21151;&#22320;&#20316;&#20026;&#30005;&#36335;&#22312;&#37327;&#23376;&#35745;&#31639;&#26426;&#19978;&#36816;&#34892;&#12290;&#33258;20&#19990;&#32426;60&#24180;&#20195;&#20197;&#26469;&#65292;&#35768;&#22810;&#35266;&#23519;&#21040;&#30340;&#35748;&#30693;&#34892;&#20026;&#24050;&#34987;&#35777;&#26126;&#36829;&#21453;&#20102;&#22522;&#20110;&#32463;&#20856;&#27010;&#29575;&#21644;&#38598;&#21512;&#35770;&#30340;&#35268;&#21017;&#12290;&#20363;&#22914;&#65292;&#38382;&#39064;&#25552;&#20986;&#30340;&#39034;&#24207;&#20250;&#24433;&#21709;&#21442;&#19982;&#32773;&#26159;&#21542;&#22238;&#31572;&#8220;&#26159;&#8221;&#25110;&#8220;&#21542;&#8221;&#65292;&#22240;&#27492;&#22238;&#31572;&#20004;&#20010;&#38382;&#39064;&#8220;&#26159;&#8221;&#30340;&#20154;&#21475;&#19981;&#33021;&#34987;&#24314;&#27169;&#20026;&#20004;&#20010;&#22266;&#23450;&#38598;&#21512;&#30340;&#20132;&#38598;&#12290;&#28982;&#32780;&#65292;&#23427;&#21487;&#20197;&#34987;&#24314;&#27169;&#20026;&#19981;&#21516;&#39034;&#24207;&#36827;&#34892;&#30340;&#19968;&#31995;&#21015;&#25237;&#24433;&#12290;&#36825;&#21644;&#20854;&#20182;&#31034;&#20363;&#24050;&#32463;&#25104;&#21151;&#22320;&#20351;&#29992;&#20102;&#37327;&#23376;&#27010;&#29575;&#26469;&#25551;&#36848;&#65292;&#36825;&#20381;&#36182;&#20110;&#27604;&#36739;&#23376;&#31354;&#38388;&#20043;&#38388;&#30340;&#35282;&#24230;&#65292;&#32780;&#19981;&#26159;&#23376;&#38598;&#20043;&#38388;&#30340;&#20307;&#31215;&#12290;&#29616;&#22312;&#22312;2020&#24180;&#21021;&#65292;&#37327;&#23376;&#35745;&#31639;&#26426;&#24050;&#32463;&#36798;&#21040;&#20102;&#19968;&#23450;&#27700;&#24179;&#65292;&#19968;&#20123;&#37327;&#23376;&#35748;&#30693;&#27169;&#22411;&#21487;&#20197;&#22312;&#37327;&#23376;&#30828;&#20214;&#19978;&#23454;&#29616;&#24182;&#30740;&#31350;&#65292;&#23558;&#24515;&#29702;&#29366;&#24577;&#34920;&#31034;&#20026;qubit&#23492;&#23384;&#22120;&#20013;&#30340;&#29366;&#24577;&#65292;&#35748;&#30693;&#25805;&#20316;&#21644;&#20915;&#31574;&#25805;&#20316;&#36890;&#36807;&#37327;&#23376;&#38376;&#26469;&#23454;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper demonstrates that some nonclassical models of human decision-making can be run successfully as circuits on quantum computers. Since the 1960s, many observed cognitive behaviors have been shown to violate rules based on classical probability and set theory. For example, the order in which questions are posed affects whether participants answer 'yes' or 'no', so the population that answers `yes' to both questions cannot be modeled as the intersection of two fixed sets. It can however be modeled as a sequence of projections carried out in different orders. This and other examples have been described successfully using quantum probability, which relies on comparing angles between subspaces rather than volumes between subsets. Now in the early 2020s, quantum computers have reached the point where some of these quantum cognitive models can be implemented and investigated on quantum hardware, representing the mental states in qubit registers, and the cognitive operations and decisi
&lt;/p&gt;</description></item><item><title>&#35813;&#30740;&#31350;&#38024;&#23545;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#36830;&#32493;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#22312;&#21253;&#21547;3900&#19975;&#20010;&#26102;&#38388;&#25139;&#26631;&#35760;&#22270;&#20687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLOC&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CL&#26041;&#27861;&#65292;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.01047</link><description>&lt;p&gt;
&#22312;&#22312;&#32447;&#36830;&#32493;&#23398;&#20064;&#20013;&#36827;&#34892;&#23454;&#26102;&#35780;&#20272;&#65306;&#19968;&#20010;&#26032;&#24076;&#26395;
&lt;/p&gt;
&lt;p&gt;
Real-Time Evaluation in Online Continual Learning: A New Hope. (arXiv:2302.01047v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.01047
&lt;/p&gt;
&lt;p&gt;
&#35813;&#30740;&#31350;&#38024;&#23545;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#36830;&#32493;&#23398;&#20064;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35780;&#20272;&#26041;&#27861;&#65292;&#24182;&#22312;&#21253;&#21547;3900&#19975;&#20010;&#26102;&#38388;&#25139;&#26631;&#35760;&#22270;&#20687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLOC&#19978;&#36827;&#34892;&#20102;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CL&#26041;&#27861;&#65292;&#35777;&#26126;&#29616;&#26377;&#26041;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#19979;&#30340;&#36866;&#29992;&#24615;&#23384;&#22312;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#36830;&#32493;&#23398;&#20064;&#26041;&#27861;&#30340;&#35780;&#20272;&#36890;&#24120;&#20551;&#35774;&#22312;&#35757;&#32451;&#26102;&#38388;&#21644;&#35745;&#31639;&#26041;&#38754;&#27809;&#26377;&#38480;&#21046;&#12290;&#36825;&#23545;&#20110;&#20219;&#20309;&#23454;&#38469;&#19990;&#30028;&#30340;&#29615;&#22659;&#37117;&#26159;&#19981;&#29616;&#23454;&#30340;&#12290;&#22240;&#27492;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#23454;&#26102;&#35780;&#20272;&#36830;&#32493;&#23398;&#20064;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#27969;&#19981;&#31561;&#24453;&#27169;&#22411;&#23436;&#25104;&#35757;&#32451;&#21363;&#25581;&#31034;&#19979;&#19968;&#20010;&#25968;&#25454;&#36827;&#34892;&#39044;&#27979;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#25105;&#20204;&#20174;&#35745;&#31639;&#25104;&#26412;&#30340;&#35282;&#24230;&#35780;&#20272;&#24403;&#21069;&#30340;CL&#26041;&#27861;&#65292;&#24182;&#22312;&#21253;&#21547;3900&#19975;&#20010;&#26102;&#38388;&#25139;&#26631;&#35760;&#22270;&#20687;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;CLOC&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#22312;&#36825;&#31181;&#35780;&#20272;&#19979;&#65292;&#19968;&#20010;&#31616;&#21333;&#30340;&#22522;&#32447;&#27169;&#22411;&#32988;&#36807;&#20102;&#26368;&#20808;&#36827;&#30340;CL&#26041;&#27861;&#65292;&#36825;&#23545;&#29616;&#26377;&#26041;&#27861;&#22312;&#29616;&#23454;&#29615;&#22659;&#20013;&#30340;&#36866;&#29992;&#24615;&#25552;&#20986;&#20102;&#36136;&#30097;&#12290;&#21478;&#22806;&#65292;&#25105;&#20204;&#36824;&#25506;&#35752;&#20102;&#25991;&#29486;&#20013;&#24120;&#29992;&#30340;&#21508;&#31181;CL&#32452;&#20214;&#65292;&#21253;&#25324;&#35760;&#24518;&#37319;&#26679;&#31574;&#30053;&#21644;&#27491;&#21017;&#21270;&#26041;&#27861;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;&#25152;&#26377;&#32771;&#34385;&#30340;&#26041;&#27861;&#37117;&#26080;&#27861;&#19982;&#25105;&#20204;&#30340;&#31616;&#21333;&#22522;&#32447;&#27169;&#22411;&#31454;&#20105;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current evaluations of Continual Learning (CL) methods typically assume that there is no constraint on training time and computation. This is an unrealistic assumption for any real-world setting, which motivates us to propose: a practical real-time evaluation of continual learning, in which the stream does not wait for the model to complete training before revealing the next data for predictions. To do this, we evaluate current CL methods with respect to their computational costs. We conduct extensive experiments on CLOC, a large-scale dataset containing 39 million time-stamped images with geolocation labels. We show that a simple baseline outperforms state-of-the-art CL methods under this evaluation, questioning the applicability of existing methods in realistic settings. In addition, we explore various CL components commonly used in the literature, including memory sampling strategies and regularization approaches. We find that all considered methods fail to be competitive against ou
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2301.10410</link><description>&lt;p&gt;
&#36866;&#29992;&#20110;&#25152;&#26377;&#39046;&#22495;&#30340;&#19968;&#20010;&#27169;&#22411;&#65306;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
One Model for All Domains: Collaborative Domain-Prefix Tuning for Cross-Domain NER. (arXiv:2301.10410v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.10410
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#22522;&#20110;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#30340;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#65292;&#20351;&#29992;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#65292;&#36991;&#20813;&#20102;&#20808;&#21069;&#30340;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35299;&#20915;&#23454;&#38469;&#22330;&#26223;&#20013;&#20302;&#36164;&#28304;&#38382;&#39064;&#26159;&#36328;&#39046;&#22495;&#23454;&#20307;&#35782;&#21035;&#30340;&#19968;&#20010;&#25361;&#25112;&#24615;&#20219;&#21153;&#12290;&#20808;&#21069;&#20856;&#22411;&#30340;&#35299;&#20915;&#26041;&#26696;&#20027;&#35201;&#36890;&#36807;&#20351;&#29992;&#26469;&#33258;&#20016;&#23500;&#36164;&#28304;&#39046;&#22495;&#30340;&#25968;&#25454;&#36827;&#34892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;(PLMs)&#33719;&#24471;NER&#27169;&#22411;&#24182;&#23558;&#20854;&#36866;&#24212;&#20110;&#30446;&#26631;&#39046;&#22495;&#12290;&#30001;&#20110;&#19981;&#21516;&#39046;&#22495;&#23454;&#20307;&#31867;&#22411;&#20043;&#38388;&#30340;&#19981;&#21305;&#37197;&#38382;&#39064;&#65292;&#20808;&#21069;&#30340;&#26041;&#27861;&#36890;&#24120;&#35843;&#25972;&#25152;&#26377;PLMs&#30340;&#21442;&#25968;&#65292;&#20174;&#32780;&#20026;&#27599;&#20010;&#39046;&#22495;&#32467;&#26463;&#19968;&#20010;&#20840;&#26032;&#30340;NER&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#24403;&#21069;&#30340;&#27169;&#22411;&#21482;&#20851;&#27880;&#20110;&#21033;&#29992;&#19968;&#20010;&#26222;&#36890;&#26469;&#28304;&#39046;&#22495;&#20013;&#30340;&#30693;&#35782;&#65292;&#32780;&#26410;&#33021;&#25104;&#21151;&#22320;&#23558;&#26469;&#33258;&#22810;&#20010;&#26469;&#28304;&#39046;&#22495;&#30340;&#30693;&#35782;&#36716;&#31227;&#21040;&#30446;&#26631;&#19978;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#22522;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;PLM&#24341;&#20837;&#20102;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#36328;&#39046;&#22495;NER(CP-NER)&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#21576;&#29616;&#20102;&#29992;&#20110;&#25991;&#26412;&#21040;&#25991;&#26412;&#29983;&#25104;&#30340;&#25903;&#25745;&#39046;&#22495;&#30456;&#20851;&#25351;&#23548;&#26469;&#23558;&#30693;&#35782;&#36716;&#31227;&#33267;&#26032;&#22495;NER&#20219;&#21153;&#32780;&#26080;&#38656;&#32467;&#26500;&#20462;&#25913;&#12290;&#25105;&#20204;&#21033;&#29992;&#20923;&#32467;&#30340;PLMs&#24182;&#36827;&#34892;&#21327;&#20316;&#22495;&#21069;&#32512;&#35843;&#25972;&#12290;
&lt;/p&gt;
&lt;p&gt;
Cross-domain NER is a challenging task to address the low-resource problem in practical scenarios. Previous typical solutions mainly obtain a NER model by pre-trained language models (PLMs) with data from a rich-resource domain and adapt it to the target domain. Owing to the mismatch issue among entity types in different domains, previous approaches normally tune all parameters of PLMs, ending up with an entirely new NER model for each domain. Moreover, current models only focus on leveraging knowledge in one general source domain while failing to successfully transfer knowledge from multiple sources to the target. To address these issues, we introduce Collaborative Domain-Prefix Tuning for cross-domain NER (CP-NER) based on text-to-text generative PLMs. Specifically, we present text-to-text generation grounding domain-related instructors to transfer knowledge to new domain NER tasks without structural modifications. We utilize frozen PLMs and conduct collaborative domain-prefix tuning
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DCMQ&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;Huber&#33021;&#37327;&#26680;&#30340;&#26041;&#27861;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#26465;&#20214;&#27979;&#24230;&#37327;&#21270;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#20363;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2301.06907</link><description>&lt;p&gt;
&#28145;&#24230;&#26465;&#20214;&#27979;&#24230;&#37327;&#21270;
&lt;/p&gt;
&lt;p&gt;
Deep Conditional Measure Quantization. (arXiv:2301.06907v2 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.06907
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;DCMQ&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;Huber&#33021;&#37327;&#26680;&#30340;&#26041;&#27861;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#65292;&#29992;&#20110;&#26465;&#20214;&#27979;&#24230;&#37327;&#21270;&#65292;&#24182;&#22312;&#22810;&#20010;&#23454;&#20363;&#20013;&#21462;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#29575;&#27979;&#24230;&#30340;&#37327;&#21270;&#24847;&#21619;&#30528;&#20351;&#29992;&#19968;&#32452;&#26377;&#38480;&#30340;&#29380;&#25289;&#20811;&#20998;&#24067;&#26469;&#36817;&#20284;&#34920;&#31034;&#36755;&#20837;&#20998;&#24067;&#65288;&#22312;&#19968;&#20123;&#27010;&#29575;&#27979;&#24230;&#24230;&#37327;&#31354;&#38388;&#20013;&#65289;&#12290;&#26377;&#21508;&#31181;&#21508;&#26679;&#30340;&#26041;&#27861;&#21487;&#20197;&#23454;&#29616;&#36825;&#19968;&#28857;&#65292;&#20294;&#21487;&#33021;&#20250;&#23384;&#22312;&#26465;&#20214;&#27861;&#30340;&#37327;&#21270;&#38656;&#35201;&#25506;&#32034;&#30340;&#24773;&#20917;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;DCMQ&#30340;&#26041;&#27861;&#65292;&#23427;&#28041;&#21450;&#21040;&#22522;&#20110;Huber&#33021;&#37327;&#26680;&#30340;&#26041;&#27861;&#21644;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20307;&#31995;&#32467;&#26500;&#30340;&#32806;&#21512;&#12290;&#35813;&#26041;&#27861;&#22312;&#22810;&#20010;&#23454;&#20363;&#19978;&#36827;&#34892;&#20102;&#27979;&#35797;&#65292;&#24182;&#33719;&#24471;&#20102;&#26377;&#24076;&#26395;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Quantization of a probability measure means representing it with a finite set of Dirac masses that approximates the input distribution well enough (in some metric space of probability measures). Various methods exists to do so, but the situation of quantizing a conditional law has been less explored. We propose a method, called DCMQ, involving a Huber-energy kernel-based approach coupled with a deep neural network architecture. The method is tested on several examples and obtains promising results.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;(PCNNs) &#22312;&#27169;&#25311;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;PCNNs&#26082;&#30830;&#20445;&#20102;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21448;&#33021;&#22312;&#22797;&#26434;&#30340;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#21462;&#24471;&#39640;&#31934;&#24230;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#22312;&#21487;&#29992;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;</title><link>http://arxiv.org/abs/2212.12380</link><description>&lt;p&gt;
&#38754;&#21521;&#21487;&#25193;&#23637;&#29289;&#29702;&#19968;&#33268;&#30340;&#31070;&#32463;&#32593;&#32476;&#65306;&#22312;&#25968;&#25454;&#39537;&#21160;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#30340;&#24212;&#29992;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Scalable Physically Consistent Neural Networks: an Application to Data-driven Multi-zone Thermal Building Models. (arXiv:2212.12380v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.12380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#30740;&#31350;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476;(PCNNs) &#22312;&#27169;&#25311;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#26041;&#38754;&#30340;&#25193;&#23637;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;PCNNs&#26082;&#30830;&#20445;&#20102;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21448;&#33021;&#22312;&#22797;&#26434;&#30340;&#22810;&#21306;&#22495;&#28909;&#24314;&#31569;&#27169;&#22411;&#20013;&#21462;&#24471;&#39640;&#31934;&#24230;&#30340;&#24615;&#33021;&#34920;&#29616;&#65292;&#19988;&#22312;&#21487;&#29992;&#25968;&#25454;&#37327;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36229;&#36234;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#65292;&#20855;&#26377;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#36234;&#26469;&#36234;&#22810;&#30340;&#25968;&#25454;&#34987;&#25910;&#38598;&#65292;&#25968;&#25454;&#39537;&#21160;&#24314;&#27169;&#26041;&#27861;&#36817;&#24180;&#26469;&#36234;&#26469;&#36234;&#21463;&#27426;&#36814;&#12290;&#34429;&#28982;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#22312;&#29289;&#29702;&#19978;&#26159;&#21487;&#38752;&#30340;&#65292;&#20294;&#36890;&#24120;&#24456;&#38590;&#35782;&#21035;&#21644;&#25193;&#23637;&#65292;&#24182;&#19988;&#21463;&#20854;&#26377;&#38480;&#30340;&#34920;&#29616;&#21147;&#24433;&#21709;&#21487;&#33021;&#20250;&#24433;&#21709;&#20854;&#20934;&#30830;&#24615;&#12290;&#21478;&#19968;&#26041;&#38754;&#65292;&#24120;&#24120;&#20381;&#36182;&#31070;&#32463;&#32593;&#32476; (NNs) &#30340;&#32463;&#20856;&#40657;&#30418;&#26041;&#27861;&#36890;&#24120;&#33021;&#22815;&#20174;&#25968;&#25454;&#20013;&#25512;&#23548;&#20986;&#32479;&#35745;&#27169;&#24335;&#65292;&#21363;&#20351;&#22312;&#25193;&#23637;&#26041;&#38754;&#20063;&#33021;&#21462;&#24471;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#23545;&#28508;&#22312;&#30340;&#29289;&#29702;&#23450;&#24459;&#23436;&#20840;&#26080;&#35270;&#65292;&#22914;&#26524;&#22522;&#20110;&#23427;&#20204;&#20570;&#20915;&#31574;&#29992;&#20110;&#23454;&#38469;&#29289;&#29702;&#31995;&#32479;&#65292;&#21487;&#33021;&#20250;&#23548;&#33268;&#28508;&#22312;&#30340;&#28798;&#38590;&#24615;&#21518;&#26524;&#12290;&#26368;&#36817;&#24320;&#21457;&#20102;&#29289;&#29702;&#19968;&#33268;&#31070;&#32463;&#32593;&#32476; (PCNNs) &#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#30830;&#20445;&#29289;&#29702;&#19968;&#33268;&#24615;&#65292;&#21516;&#26102;&#21033;&#29992; NNs &#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#20934;&#30830;&#24615;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#23558; PCNN &#25193;&#23637;&#21040;&#24314;&#31569;&#28201;&#24230;&#21160;&#24577;&#24314;&#27169;&#65292;&#24182;&#25552;&#20986;&#19982;&#32463;&#20856;&#28784;&#30418;&#21644;&#40657;&#30418;&#26041;&#27861;&#30340;&#24443;&#24213;&#27604;&#36739;&#12290;&#29305;&#21035;&#26159;&#65292;&#25105;&#20204;&#30740;&#31350;&#22810;&#21306;&#22495;&#24314;&#31569;&#27169;&#22411;&#65292;&#20854;&#20013;&#27599;&#20010;&#21306;&#22495;&#30340;&#28909;&#34892;&#20026;&#30001;&#33021;&#37327;&#24179;&#34913;&#26041;&#31243;&#24335;&#32479;&#27835;&#65292;&#20854;&#21442;&#25968;&#24517;&#39035;&#36890;&#36807;&#27979;&#37327;&#25968;&#25454;&#36827;&#34892;&#35782;&#21035;&#12290;&#25152;&#24471;&#32467;&#26524;&#34920;&#26126;&#65292;&#21363;&#20351;&#28041;&#21450;&#35768;&#22810;&#30456;&#20114;&#20316;&#29992;&#30340;&#32452;&#20214;&#26500;&#25104;&#30340;&#22797;&#26434;&#21644;&#21160;&#24577;&#31995;&#32479;&#65292;PCNNs &#20063;&#21487;&#20197;&#22312;&#30830;&#20445;&#29289;&#29702;&#19968;&#33268;&#24615;&#30340;&#21516;&#26102;&#23454;&#29616;&#39640;&#31934;&#24230;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#35777;&#26126; PCNN &#22312;&#32463;&#20856;&#28784;&#30418;&#27169;&#22411;&#19978;&#25552;&#20379;&#20102;&#21487;&#25193;&#23637;&#24615;&#20248;&#21183;&#65292;&#22312;&#26377;&#38480;&#30340;&#21487;&#29992;&#35757;&#32451;&#25968;&#25454;&#19979;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
With more and more data being collected, data-driven modeling methods have been gaining in popularity in recent years. While physically sound, classical gray-box models are often cumbersome to identify and scale, and their accuracy might be hindered by their limited expressiveness. On the other hand, classical black-box methods, typically relying on Neural Networks (NNs) nowadays, often achieve impressive performance, even at scale, by deriving statistical patterns from data. However, they remain completely oblivious to the underlying physical laws, which may lead to potentially catastrophic failures if decisions for real-world physical systems are based on them. Physically Consistent Neural Networks (PCNNs) were recently developed to address these aforementioned issues, ensuring physical consistency while still leveraging NNs to attain state-of-the-art accuracy.  In this work, we scale PCNNs to model building temperature dynamics and propose a thorough comparison with classical gray-b
&lt;/p&gt;</description></item><item><title>GPT-3&#22312;&#35768;&#22810;&#31867;&#27604;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32039;&#24613;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2212.09196</link><description>&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#30340;&#31867;&#27604;&#25512;&#29702;&#30340;&#32039;&#24613;&#24615;
&lt;/p&gt;
&lt;p&gt;
Emergent Analogical Reasoning in Large Language Models. (arXiv:2212.09196v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.09196
&lt;/p&gt;
&lt;p&gt;
GPT-3&#22312;&#35768;&#22810;&#31867;&#27604;&#20219;&#21153;&#20013;&#34920;&#29616;&#20986;&#19982;&#29978;&#33267;&#36229;&#36234;&#20154;&#31867;&#30340;&#33021;&#21147;&#65292;&#25581;&#31034;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#32039;&#24613;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#37325;&#26032;&#28857;&#29123;&#20102;&#20154;&#20204;&#23545;&#20110;&#36825;&#26679;&#19968;&#31181;&#38382;&#39064;&#30340;&#36777;&#35770;&#65306;&#36275;&#22815;&#30340;&#35757;&#32451;&#25968;&#25454;&#26159;&#21542;&#33021;&#20351;&#36825;&#20123;&#36890;&#29992;&#27169;&#22411;&#20869;&#28085;&#20154;&#31867;&#35748;&#30693;&#33021;&#21147;&#12290;&#29305;&#21035;&#30340;&#65292;&#36825;&#20123;&#27169;&#22411;&#30340;&#38646;&#26679;&#26412;&#25512;&#29702;&#33021;&#21147;&#8212;&#8212;&#19981;&#32463;&#36807;&#20219;&#20309;&#30452;&#25509;&#35757;&#32451;&#65292;&#23601;&#33021;&#22815;&#25512;&#29702;&#20986;&#26032;&#38382;&#39064;&#65292;&#29305;&#21035;&#20196;&#20154;&#20851;&#27880;&#12290;&#22312;&#20154;&#31867;&#35748;&#30693;&#20013;&#65292;&#36825;&#31181;&#33021;&#21147;&#19982;&#19968;&#31181;&#36890;&#36807;&#31867;&#27604;&#25512;&#29702;&#30340;&#33021;&#21147;&#23494;&#20999;&#30456;&#20851;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#22312;&#19968;&#31995;&#21015;&#31867;&#27604;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#30452;&#25509;&#30340;&#20154;&#26426;&#27604;&#36739;&#65292;&#21253;&#25324;&#19968;&#31181;&#26032;&#39062;&#30340;&#22522;&#20110;&#25991;&#26412;&#30340;&#30697;&#38453;&#25512;&#29702;&#20219;&#21153;&#65292;&#35813;&#20219;&#21153;&#19982; Raven's Progressive Matrices&#23494;&#20999;&#30456;&#20851;&#12290;&#25105;&#20204;&#21457;&#29616;&#65292;GPT-3&#21576;&#29616;&#20986;&#20102;&#19968;&#31181;&#20196;&#20154;&#24778;&#35766;&#30340;&#25277;&#35937;&#27169;&#24335;&#24402;&#32435;&#33021;&#21147;&#65292;&#29978;&#33267;&#22312;&#22823;&#37096;&#20998;&#24773;&#20917;&#19979;&#19982;&#25110;&#29978;&#33267;&#36229;&#36234;&#20102;&#20154;&#31867;&#30340;&#33021;&#21147;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#20687;GPT-3&#36825;&#26679;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#24050;&#32463;&#33719;&#24471;&#20102;&#22312;&#24191;&#27867;&#30340;&#31867;&#27604;&#38382;&#39064;&#19978;&#25214;&#21040;&#38646;&#26679;&#26412;&#35299;&#20915;&#26041;&#26696;&#30340;&#32039;&#24613;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here, we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of GPT-3) on a range of analogical tasks, including a novel text-based matrix reasoning task closely modeled on Raven's Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.
&lt;/p&gt;</description></item><item><title>POTATO&#26159;&#19968;&#20010;&#20813;&#36153;&#12289;&#24320;&#28304;&#30340;&#20415;&#25658;&#24335;&#25991;&#26412;&#27880;&#37322;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26631;&#27880;&#65292;&#25552;&#20379;&#26131;&#20110;&#37197;&#32622;&#30340;&#21151;&#33021;&#20197;&#26368;&#22823;&#21270;&#29983;&#20135;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#25991;&#26723;&#21644;&#22797;&#26434;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2212.08620</link><description>&lt;p&gt;
POTATO: &#20415;&#25658;&#24335;&#25991;&#26412;&#27880;&#37322;&#24037;&#20855;
&lt;/p&gt;
&lt;p&gt;
POTATO: The Portable Text Annotation Tool. (arXiv:2212.08620v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08620
&lt;/p&gt;
&lt;p&gt;
POTATO&#26159;&#19968;&#20010;&#20813;&#36153;&#12289;&#24320;&#28304;&#30340;&#20415;&#25658;&#24335;&#25991;&#26412;&#27880;&#37322;&#24037;&#20855;&#65292;&#25903;&#25345;&#22810;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#30340;&#26631;&#27880;&#65292;&#25552;&#20379;&#26131;&#20110;&#37197;&#32622;&#30340;&#21151;&#33021;&#20197;&#26368;&#22823;&#21270;&#29983;&#20135;&#21147;&#65292;&#29305;&#21035;&#26159;&#23545;&#20110;&#38271;&#25991;&#26723;&#21644;&#22797;&#26434;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102;POTATO&#65292;&#21363;&#20415;&#25658;&#24335;&#25991;&#26412;&#27880;&#37322;&#24037;&#20855;&#65292;&#36825;&#26159;&#19968;&#20010;&#23436;&#20840;&#20813;&#36153;&#12289;&#24320;&#28304;&#30340;&#27880;&#37322;&#31995;&#32479;&#65292;&#25903;&#25345;&#26631;&#27880;&#22810;&#31181;&#31867;&#22411;&#30340;&#25991;&#26412;&#21644;&#22810;&#27169;&#24577;&#25968;&#25454;&#65292;&#25552;&#20379;&#26131;&#20110;&#37197;&#32622;&#30340;&#21151;&#33021;&#20197;&#26368;&#22823;&#21270;&#37096;&#32626;&#21644;&#27880;&#37322;&#32773;&#30340;&#29983;&#20135;&#21147;&#65288;&#20415;&#25463;&#30340;&#26426;&#22120;&#23398;&#20064;/&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#27169;&#26495;&#12289;&#20027;&#21160;&#23398;&#20064;&#12289;&#25353;&#38190;&#32553;&#20889;&#38190;&#12289;&#20851;&#38190;&#23383;&#39640;&#20142;&#12289;&#25552;&#31034;&#24037;&#20855;&#65289;&#65292;&#24182;&#25903;&#25345;&#39640;&#24230;&#23450;&#21046;&#21270;&#65288;&#21487;&#32534;&#36753;&#30340;UI&#65292;&#25554;&#20837;&#39044;&#31579;&#36873;&#38382;&#39064;&#65292;&#27880;&#24847;&#21147;&#21644;&#36164;&#26684;&#27979;&#35797;&#65289;&#12290;&#20004;&#39033;&#27880;&#37322;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;POTATO&#36890;&#36807;&#20854;&#29305;&#21035;&#35774;&#35745;&#30340;&#29983;&#20135;&#21147;&#21151;&#33021;&#65292;&#29305;&#21035;&#26159;&#38271;&#25991;&#26723;&#21644;&#22797;&#26434;&#20219;&#21153;&#65292;&#25552;&#39640;&#20102;&#26631;&#27880;&#36895;&#24230;&#12290;POTATO&#21487;&#22312; https://github.com/davidjurgens/potato &#19978;&#33719;&#21462;&#65292;&#24182;&#23558;&#32487;&#32493;&#26356;&#26032;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present POTATO, the Portable text annotation tool, a free, fully open-sourced annotation system that 1) supports labeling many types of text and multimodal data; 2) offers easy-to-configure features to maximize the productivity of both deployers and annotators (convenient templates for common ML/NLP tasks, active learning, keypress shortcuts, keyword highlights, tooltips); and 3) supports a high degree of customization (editable UI, inserting pre-screening questions, attention and qualification tests). Experiments over two annotation tasks suggest that POTATO improves labeling speed through its specially-designed productivity features, especially for long documents and complex tasks. POTATO is available at https://github.com/davidjurgens/potato and will continue to be updated.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FlexiViT&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#19968;&#32452;&#26435;&#37325;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#34917;&#19969;&#22823;&#23567;&#65292;&#20174;&#32780;&#20351;&#20854;&#22312;&#37096;&#32626;&#26102;&#23481;&#26131;&#26681;&#25454;&#19981;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#26469;&#23450;&#21046;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2212.08013</link><description>&lt;p&gt;
FlexiViT&#65306;&#36866;&#29992;&#20110;&#25152;&#26377;&#34917;&#19969;&#22823;&#23567;&#30340;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
FlexiViT: One Model for All Patch Sizes. (arXiv:2212.08013v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.08013
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026;FlexiViT&#30340;&#27169;&#22411;&#65292;&#23427;&#21487;&#20197;&#20351;&#29992;&#19968;&#32452;&#26435;&#37325;&#34920;&#29616;&#33391;&#22909;&#65292;&#24182;&#19988;&#36866;&#29992;&#20110;&#21508;&#31181;&#34917;&#19969;&#22823;&#23567;&#65292;&#20174;&#32780;&#20351;&#20854;&#22312;&#37096;&#32626;&#26102;&#23481;&#26131;&#26681;&#25454;&#19981;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#26469;&#23450;&#21046;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Vision Transformer&#23558;&#22270;&#20687;&#20999;&#25104;&#34917;&#19969;&#20197;&#23558;&#20854;&#36716;&#25442;&#20026;&#24207;&#21015;&#12290;&#36825;&#20123;&#34917;&#19969;&#30340;&#22823;&#23567;&#25511;&#21046;&#36895;&#24230;/&#20934;&#30830;&#24615;&#26435;&#34913;&#65292;&#34917;&#19969;&#36234;&#23567;&#65292;&#31934;&#24230;&#36234;&#39640;&#65292;&#20294;&#35745;&#31639;&#25104;&#26412;&#20063;&#36234;&#39640;&#65292;&#20294;&#26356;&#25913;&#34917;&#19969;&#22823;&#23567;&#36890;&#24120;&#38656;&#35201;&#37325;&#26032;&#35757;&#32451;&#27169;&#22411;&#12290;&#26412;&#25991;&#35777;&#26126;&#65292;&#31616;&#21333;&#38543;&#26426;&#21270;&#35757;&#32451;&#26102;&#30340;&#34917;&#19969;&#22823;&#23567;&#20250;&#23548;&#33268;&#19968;&#32452;&#26435;&#37325;&#34920;&#29616;&#33391;&#22909;&#65292;&#21487;&#22312;&#21508;&#31181;&#34917;&#19969;&#22823;&#23567;&#33539;&#22260;&#20869;&#20351;&#29992;&#65292;&#20174;&#32780;&#20351;&#20854;&#22312;&#37096;&#32626;&#26102;&#23481;&#26131;&#26681;&#25454;&#19981;&#21516;&#30340;&#35745;&#31639;&#39044;&#31639;&#26469;&#23450;&#21046;&#27169;&#22411;&#12290;&#25105;&#20204;&#22312;&#35768;&#22810;&#20219;&#21153;&#19978;&#23545;&#29983;&#25104;&#30340;&#27169;&#22411;&#36827;&#34892;&#20102;&#20840;&#38754;&#35780;&#20272;&#65292;&#21253;&#25324;&#20998;&#31867;&#65292;&#22270;&#20687;-&#25991;&#26412;&#26816;&#32034;&#65292;&#24320;&#25918;&#24335;&#26816;&#27979;&#65292;&#20840;&#26223;&#20998;&#21106;&#21644;&#35821;&#20041;&#20998;&#21106;&#65292;&#24471;&#20986;&#32467;&#35770;&#65306;&#23427;&#36890;&#24120;&#21487;&#20197;&#21305;&#37197;&#65292;&#24182;&#19988;&#26377;&#26102;&#20250;&#20248;&#20110;&#20197;&#21333;&#20010;&#34917;&#19969;&#22823;&#23567;&#35757;&#32451;&#30340;&#26631;&#20934;ViT&#27169;&#22411;&#12290;&#22240;&#27492;&#65292;&#22312;ViT&#20013;&#20351;&#29992;FlexiViT&#36827;&#34892;&#35757;&#32451;&#26159;&#19968;&#31181;&#31616;&#21333;&#26131;&#34892;&#30340;&#25913;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Vision Transformers convert images to sequences by slicing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but changing the patch size typically requires retraining the model. In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, making it possible to tailor the model to different compute budgets at deployment time. We extensively evaluate the resulting model, which we call FlexiViT, on a wide range of tasks, including classification, image-text retrieval, open-world detection, panoptic segmentation, and semantic segmentation, concluding that it usually matches, and sometimes outperforms, standard ViT models trained at a single patch size in an otherwise identical setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that makes it easy to a
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;Flow-Lenia&#20316;&#20026;Lenia&#30340;&#19968;&#31181;&#25193;&#23637;&#26469;&#35299;&#20915;Lenia&#26080;&#27861;&#29983;&#25104;&#23436;&#20840;&#33258;&#30001;&#30340;&#28436;&#21270;&#12289;&#26080;&#27861;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#38656;&#35201;&#39640;&#32423;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#25506;&#32034;&#31561;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;Flow-Lenia&#22312;&#29983;&#25104;&#20855;&#26377;&#22797;&#26434;&#34892;&#20026;&#30340;&#29983;&#21629;&#24418;&#24577;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.07906</link><description>&lt;p&gt;
Flow-Lenia: &#36890;&#36807;&#36136;&#37327;&#23432;&#24658;&#21644;&#21442;&#25968;&#23450;&#20301;&#23454;&#29616;&#20803;&#32990;&#33258;&#21160;&#26426;&#20013;&#24320;&#25918;&#24335;&#28436;&#21270;
&lt;/p&gt;
&lt;p&gt;
Flow-Lenia: Towards open-ended evolution in cellular automata through mass conservation and parameter localization. (arXiv:2212.07906v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.07906
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;Flow-Lenia&#20316;&#20026;Lenia&#30340;&#19968;&#31181;&#25193;&#23637;&#26469;&#35299;&#20915;Lenia&#26080;&#27861;&#29983;&#25104;&#23436;&#20840;&#33258;&#30001;&#30340;&#28436;&#21270;&#12289;&#26080;&#27861;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#38656;&#35201;&#39640;&#32423;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#25506;&#32034;&#31561;&#38382;&#39064;&#65292;&#23637;&#31034;&#20102;Flow-Lenia&#22312;&#29983;&#25104;&#20855;&#26377;&#22797;&#26434;&#34892;&#20026;&#30340;&#29983;&#21629;&#24418;&#24577;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22797;&#26434;&#33258;&#32452;&#32455;&#31995;&#32479;&#30340;&#35774;&#35745;&#26159;&#20154;&#24037;&#29983;&#21629;&#30340;&#20027;&#35201;&#30446;&#26631;&#20043;&#19968;&#12290;Lenia&#26159;&#19968;&#31181;&#24191;&#20041;&#24247;&#23041;&#29983;&#21629;&#28216;&#25103;&#30340;&#20803;&#32990;&#33258;&#21160;&#26426;&#65292;&#23558;&#20854;&#25512;&#24191;&#21040;&#20102;&#36830;&#32493;&#31354;&#38388;&#12289;&#26102;&#38388;&#21644;&#29366;&#24577;&#12290;&#23427;&#21560;&#24341;&#20102;&#35768;&#22810;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#21487;&#20197;&#20135;&#29983;&#21508;&#31181;&#33258;&#32452;&#32455;&#27169;&#24335;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;Flow-Lenia&#20316;&#20026;Lenia&#30340;&#36136;&#37327;&#23432;&#24658;&#25193;&#23637;&#65292;&#23427;&#35299;&#20915;&#20102;Lenia&#26080;&#27861;&#29983;&#25104;&#23436;&#20840;&#33258;&#30001;&#30340;&#28436;&#21270;&#12289;&#26080;&#27861;&#30456;&#20114;&#20316;&#29992;&#20197;&#21450;&#38656;&#35201;&#39640;&#32423;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#25506;&#32034;&#31561;&#38382;&#39064;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#20854;&#22312;&#29983;&#25104;&#20855;&#26377;&#22797;&#26434;&#34892;&#20026;&#30340;&#29983;&#21629;&#24418;&#24577;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
The design of complex self-organising systems producing life-like phenomena, such as the open-ended evolution of virtual creatures, is one of the main goals of artificial life. Lenia, a family of cellular automata (CA) generalizing Conway's Game of Life to continuous space, time and states, has attracted a lot of attention because of the wide diversity of self-organizing patterns it can generate. Among those, some spatially localized patterns (SLPs) resemble life-like artificial creatures and display complex behaviors. However, those creatures are found in only a small subspace of the Lenia parameter space and are not trivial to discover, necessitating advanced search algorithms. Furthermore, each of these creatures exist only in worlds governed by specific update rules and thus cannot interact in the same one. This paper proposes as mass-conservative extension of Lenia, called Flow Lenia, that solve both of these issues. We present experiments demonstrating its effectiveness in genera
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#25925;&#20107;&#29983;&#25104;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#26041;&#27861;&#19982;&#25216;&#26415;&#65292;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2212.04634</link><description>&lt;p&gt;
&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#24320;&#25918;&#19990;&#30028;&#25925;&#20107;&#29983;&#25104;&#65306;&#19968;&#39033;&#20840;&#38754;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Open-world Story Generation with Structured Knowledge Enhancement: A Comprehensive Survey. (arXiv:2212.04634v2 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.04634
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#25925;&#20107;&#29983;&#25104;&#36827;&#34892;&#20102;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#26041;&#27861;&#19982;&#25216;&#26415;&#65292;&#25351;&#20986;&#20102;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#21644;&#23578;&#26410;&#35299;&#20915;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35762;&#25925;&#20107;&#21644;&#21465;&#20107;&#26159;&#20154;&#31867;&#20307;&#39564;&#30340;&#22522;&#30784;&#65292;&#19982;&#25105;&#20204;&#30340;&#31038;&#20250;&#21644;&#25991;&#21270;&#21442;&#19982;&#23494;&#19981;&#21487;&#20998;&#12290;&#22240;&#27492;&#65292;&#38271;&#26399;&#20197;&#26469;&#65292;&#30740;&#31350;&#20154;&#21592;&#19968;&#30452;&#23581;&#35797;&#21019;&#24314;&#33021;&#22815;&#33258;&#21160;&#29983;&#25104;&#25925;&#20107;&#30340;&#31995;&#32479;&#12290;&#36817;&#24180;&#26469;&#65292;&#21463;&#28145;&#24230;&#23398;&#20064;&#21644;&#22823;&#35268;&#27169;&#25968;&#25454;&#36164;&#28304;&#30340;&#25512;&#21160;&#65292;&#33258;&#21160;&#29983;&#25104;&#25925;&#20107;&#24050;&#32463;&#21462;&#24471;&#20102;&#24456;&#22823;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#20173;&#28982;&#23384;&#22312;&#19968;&#20123;&#37325;&#22823;&#25361;&#25112;&#65292;&#20363;&#22914;&#65292;&#38656;&#35201;&#22312;&#29983;&#25104;&#30340;&#25925;&#20107;&#20013;&#23454;&#29616;&#20840;&#23616;&#19968;&#33268;&#24615;&#65292;&#36825;&#20351;&#24471;&#29983;&#25104;&#27169;&#22411;&#26080;&#27861;&#36798;&#21040;&#19982;&#20154;&#31867;&#21465;&#36848;&#32773;&#30456;&#21516;&#30340;&#21465;&#20107;&#33021;&#21147;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#25361;&#25112;&#65292;&#35768;&#22810;&#30740;&#31350;&#35797;&#22270;&#22312;&#29983;&#25104;&#36807;&#31243;&#20013;&#27880;&#20837;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#36825;&#34987;&#31216;&#20026;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#30340;&#25925;&#20107;&#29983;&#25104;&#12290;&#23558;&#22806;&#37096;&#30693;&#35782;&#32435;&#20837;&#20854;&#20013;&#21487;&#20197;&#22686;&#24378;&#25925;&#20107;&#20107;&#20214;&#20043;&#38388;&#30340;&#36923;&#36753;&#36830;&#36143;&#24615;&#65292;&#23454;&#29616;&#26356;&#22909;&#30340;&#30693;&#35782;&#22522;&#30784;&#65292;&#24182;&#20943;&#36731;&#25925;&#20107;&#20013;&#36807;&#24230;&#27010;&#25324;&#21644;&#37325;&#22797;&#38382;&#39064;&#12290;&#26412;&#27425;&#35843;&#26597;&#25552;&#20379;&#20102;&#23545;&#35813;&#30740;&#31350;&#39046;&#22495;&#30340;&#26368;&#26032;&#21644;&#20840;&#38754;&#30340;&#22238;&#39038;&#65306;&#65288;i&#65289;&#25105;&#20204;&#25552;&#20379;&#20102;&#23545;&#32467;&#26500;&#21270;&#30693;&#35782;&#22686;&#24378;&#25925;&#20107;&#29983;&#25104;&#30340;&#32508;&#36848;&#65292;&#65288;ii&#65289;&#25105;&#20204;&#24635;&#32467;&#20102;&#30446;&#21069;&#30340;&#26041;&#27861;&#19982;&#25216;&#26415;&#65292;&#65288;iii&#65289;&#25105;&#20204;&#25351;&#20986;&#20102;&#23578;&#24453;&#35299;&#20915;&#30340;&#38382;&#39064;&#19982;&#26410;&#26469;&#30340;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Storytelling and narrative are fundamental to human experience, intertwined with our social and cultural engagement. As such, researchers have long attempted to create systems that can generate stories automatically. In recent years, powered by deep learning and massive data resources, automatic story generation has shown significant advances. However, considerable challenges, like the need for global coherence in generated stories, still hamper generative models from reaching the same storytelling ability as human narrators. To tackle these challenges, many studies seek to inject structured knowledge into the generation process, which is referred to as structured knowledge-enhanced story generation. Incorporating external knowledge can enhance the logical coherence among story events, achieve better knowledge grounding, and alleviate over-generalization and repetition problems in stories. This survey provides the latest and comprehensive review of this research field: (i) we present a
&lt;/p&gt;</description></item><item><title>&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26080;&#23613;&#22312;&#32447;&#20851;&#21345;&#29983;&#25104;&#65292;&#25552;&#20986;&#20102;&#29366;&#24577;&#31354;&#38388;&#23553;&#38381;&#27010;&#24565;&#65292;&#23558;&#26377;&#38480;&#26102;&#27573;&#35757;&#32451;&#30340;&#32463;&#39564;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#20869;&#23481;&#29983;&#25104;&#25512;&#24191;&#21040;&#26080;&#38480;&#26102;&#27573;&#65292;&#32780;&#19981;&#25439;&#22833;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;EDRL&#29983;&#25104;&#30340;&#20869;&#23481;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.02951</link><description>&lt;p&gt;
&#29366;&#24577;&#31354;&#38388;&#23553;&#38381;&#65306;&#36890;&#36807;&#24378;&#21270;&#23398;&#20064;&#37325;&#26032;&#23457;&#35270;&#26080;&#23613;&#22312;&#32447;&#20851;&#21345;&#29983;&#25104;
&lt;/p&gt;
&lt;p&gt;
State Space Closure: Revisiting Endless Online Level Generation via Reinforcement Learning. (arXiv:2212.02951v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02951
&lt;/p&gt;
&lt;p&gt;
&#36825;&#31687;&#35770;&#25991;&#37325;&#26032;&#23457;&#35270;&#20102;&#26080;&#23613;&#22312;&#32447;&#20851;&#21345;&#29983;&#25104;&#65292;&#25552;&#20986;&#20102;&#29366;&#24577;&#31354;&#38388;&#23553;&#38381;&#27010;&#24565;&#65292;&#23558;&#26377;&#38480;&#26102;&#27573;&#35757;&#32451;&#30340;&#32463;&#39564;&#39537;&#21160;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#20869;&#23481;&#29983;&#25104;&#25512;&#24191;&#21040;&#26080;&#38480;&#26102;&#27573;&#65292;&#32780;&#19981;&#25439;&#22833;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#39564;&#35777;&#20102;EDRL&#29983;&#25104;&#30340;&#20869;&#23481;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20351;&#29992;&#26368;&#36817;&#25552;&#20986;&#30340;&#22522;&#20110;&#32463;&#39564;&#39537;&#21160;&#30340;&#24378;&#21270;&#23398;&#20064;&#31243;&#24207;&#20869;&#23481;&#29983;&#25104;&#65288;EDRL&#65289;&#26694;&#26550;&#37325;&#26032;&#23457;&#35270;&#20102;&#26080;&#23613;&#22312;&#32447;&#20851;&#21345;&#29983;&#25104;&#12290;&#36890;&#36807;&#35266;&#23519;EDRL&#20542;&#21521;&#20110;&#29983;&#25104;&#37325;&#22797;&#27169;&#24335;&#30340;&#29616;&#35937;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#29366;&#24577;&#31354;&#38388;&#23553;&#38381;&#30340;&#27010;&#24565;&#65292;&#23427;&#20351;&#24471;&#26080;&#38480;&#26102;&#27573;&#30340;&#22312;&#32447;&#29983;&#25104;&#36807;&#31243;&#20013;&#21487;&#33021;&#20986;&#29616;&#30340;&#20219;&#20309;&#38543;&#26426;&#29366;&#24577;&#37117;&#21487;&#20197;&#22312;&#26377;&#38480;&#26102;&#27573;&#20869;&#25214;&#21040;&#12290;&#36890;&#36807;&#29702;&#35770;&#20998;&#26512;&#65292;&#25105;&#20204;&#21457;&#29616;&#29366;&#24577;&#31354;&#38388;&#23553;&#38381;&#34429;&#28982;&#24341;&#36215;&#20102;&#22810;&#26679;&#24615;&#30340;&#20851;&#27880;&#65292;&#20294;&#23427;&#23558;&#26377;&#38480;&#26102;&#27573;&#35757;&#32451;&#30340;EDRL&#25512;&#24191;&#21040;&#26080;&#38480;&#26102;&#27573;&#30340;&#24773;&#20917;&#65292;&#32780;&#19981;&#20250;&#38477;&#20302;&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#12290;&#27492;&#22806;&#65292;&#36890;&#36807;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#39564;&#35777;&#20102;EDRL&#29983;&#25104;&#20869;&#23481;&#30340;&#36136;&#37327;&#21644;&#22810;&#26679;&#24615;&#65292;&#20351;&#29992;&#20102;&#24191;&#27867;&#20351;&#29992;&#30340;Super Mario Bros&#22522;&#20934;&#27979;&#35797;&#36827;&#34892;&#35780;&#20272;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#30001;&#20110;&#29366;&#24577;&#31354;&#38388;&#23553;&#38381;&#65292;EDRL&#29983;&#25104;&#30340;&#20851;&#21345;&#22810;&#26679;&#24615;&#26377;&#38480;&#65292;&#20294;&#23427;&#20204;&#30340;&#36136;&#37327;&#22312;&#26102;&#27573;&#20869;&#27809;&#26377;&#19979;&#38477;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we revisit endless online level generation with the recently proposed experience-driven procedural content generation via reinforcement learning (EDRL) framework. Inspired by an observation that EDRL tends to generate recurrent patterns, we formulate a notion of state space closure which makes any stochastic state appeared possibly in an infinite-horizon online generation process can be found within a finite-horizon. Through theoretical analysis, we find that even though state space closure arises a concern about diversity, it generalises EDRL trained with a finite-horizon to the infinite-horizon scenario without deterioration of content quality. Moreover, we verify the quality and the diversity of contents generated by EDRL via empirical studies, on the widely used Super Mario Bros. benchmark. Experimental results reveal that the diversity of levels generated by EDRL is limited due to the state space closure, whereas their quality does not deteriorate in a horizon which
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MHCCL&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#23618;&#27425;&#32858;&#31867;&#32467;&#26500;&#20013;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#26469;&#28388;&#38500;&#34394;&#20551;&#36127;&#26679;&#26412;&#21644;&#34917;&#20805;&#27491;&#26679;&#26412;&#12290;</title><link>http://arxiv.org/abs/2212.01141</link><description>&lt;p&gt;
MHCCL&#65306;&#29992;&#20110;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#30340;&#23618;&#27425;&#25513;&#34109;&#32858;&#31867;&#23545;&#27604;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
MHCCL: Masked Hierarchical Cluster-Wise Contrastive Learning for Multivariate Time Series. (arXiv:2212.01141v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01141
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MHCCL&#30340;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#21487;&#20197;&#20174;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#34920;&#31034;&#65292;&#24182;&#21033;&#29992;&#23618;&#27425;&#32858;&#31867;&#32467;&#26500;&#20013;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#26469;&#28388;&#38500;&#34394;&#20551;&#36127;&#26679;&#26412;&#21644;&#34917;&#20805;&#27491;&#26679;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#21407;&#22987;&#26080;&#26631;&#31614;&#26102;&#38388;&#24207;&#21015;&#25968;&#25454;&#20013;&#23398;&#20064;&#35821;&#20041;&#20016;&#23500;&#30340;&#34920;&#31034;&#23545;&#20110;&#20998;&#31867;&#21644;&#39044;&#27979;&#31561;&#19979;&#28216;&#20219;&#21153;&#33267;&#20851;&#37325;&#35201;&#12290;&#23545;&#27604;&#23398;&#20064;&#26368;&#36817;&#23637;&#31034;&#20102;&#22312;&#32570;&#20047;&#19987;&#23478;&#27880;&#37322;&#30340;&#24773;&#20917;&#19979;&#20855;&#26377;&#33391;&#22909;&#30340;&#34920;&#31034;&#23398;&#20064;&#33021;&#21147;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#30340;&#23545;&#27604;&#23398;&#20064;&#26041;&#27861;&#36890;&#24120;&#29420;&#31435;&#22788;&#29702;&#27599;&#20010;&#23454;&#20363;&#65292;&#23548;&#33268;&#20849;&#20139;&#30456;&#21516;&#35821;&#20041;&#30340;&#20551;&#36127;&#26679;&#26412;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;MHCCL&#65292;&#19968;&#31181;&#23618;&#27425;&#25513;&#34109;&#32858;&#31867;&#23545;&#27604;&#23398;&#20064;&#27169;&#22411;&#65292;&#23427;&#21033;&#29992;&#30001;&#22810;&#20010;&#28508;&#22312;&#20998;&#21306;&#32452;&#25104;&#30340;&#23618;&#27425;&#32467;&#26500;&#33719;&#24471;&#30340;&#35821;&#20041;&#20449;&#24687;&#26469;&#20026;&#22810;&#20803;&#26102;&#38388;&#24207;&#21015;&#24314;&#27169;&#12290;&#21463;&#21040;&#32454;&#31890;&#24230;&#32858;&#31867;&#20445;&#30041;&#26356;&#39640;&#32431;&#24230;&#65292;&#32780;&#31895;&#31890;&#24230;&#32858;&#31867;&#21453;&#26144;&#26356;&#39640;&#32423;&#21035;&#35821;&#20041;&#30340;&#35266;&#23519;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#21521;&#19979;&#25513;&#34109;&#31574;&#30053;&#65292;&#36890;&#36807;&#32467;&#21512;&#32858;&#31867;&#23618;&#27425;&#32467;&#26500;&#20013;&#30340;&#22810;&#31890;&#24230;&#20449;&#24687;&#65292;&#36807;&#28388;&#25481;&#34394;&#20551;&#36127;&#38754;&#23454;&#20363;&#24182;&#34917;&#20805;&#27491;&#38754;&#23454;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning semantic-rich representations from raw unlabeled time series data is critical for downstream tasks such as classification and forecasting. Contrastive learning has recently shown its promising representation learning capability in the absence of expert annotations. However, existing contrastive approaches generally treat each instance independently, which leads to false negative pairs that share the same semantics. To tackle this problem, we propose MHCCL, a Masked Hierarchical Cluster-wise Contrastive Learning model, which exploits semantic information obtained from the hierarchical structure consisting of multiple latent partitions for multivariate time series. Motivated by the observation that fine-grained clustering preserves higher purity while coarse-grained one reflects higher-level semantics, we propose a novel downward masking strategy to filter out fake negatives and supplement positives by incorporating the multi-granularity information from the clustering hierarchy
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#21457;&#29616;&#20102;&#35299;&#37322;&#22270;&#20687; DNN &#30340;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#23646;&#24615;&#65306;&#36890;&#36807;&#24494;&#23567;&#35270;&#35273;&#26356;&#25913;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#35299;&#37322;&#21487;&#20197;&#36890;&#36807;&#36827;&#21270;&#31574;&#30053;&#20219;&#24847;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; AttaXAI&#65292;&#19968;&#20010;&#38024;&#23545; XAI &#31639;&#27861;&#30340;&#25932;&#23545;&#25915;&#20987;&#65292;&#21487;&#35775;&#38382;&#20998;&#31867;&#22120;&#36755;&#20986;&#20449;&#24687;&#21644;&#35299;&#37322;&#22270;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2211.14860</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#20013;&#30340;&#35299;&#37322;&#26041;&#27861;&#30772;&#35299;
&lt;/p&gt;
&lt;p&gt;
Foiling Explanations in Deep Neural Networks. (arXiv:2211.14860v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.14860
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21457;&#29616;&#20102;&#35299;&#37322;&#22270;&#20687; DNN &#30340;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#23646;&#24615;&#65306;&#36890;&#36807;&#24494;&#23567;&#35270;&#35273;&#26356;&#25913;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#35299;&#37322;&#21487;&#20197;&#36890;&#36807;&#36827;&#21270;&#31574;&#30053;&#20219;&#24847;&#25805;&#32437;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102; AttaXAI&#65292;&#19968;&#20010;&#38024;&#23545; XAI &#31639;&#27861;&#30340;&#25932;&#23545;&#25915;&#20987;&#65292;&#21487;&#35775;&#38382;&#20998;&#31867;&#22120;&#36755;&#20986;&#20449;&#24687;&#21644;&#35299;&#37322;&#22270;&#65292;&#36825;&#20351;&#24471;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#40657;&#30418;&#29305;&#24615;&#23545;&#20110;&#21487;&#35299;&#37322;&#24615;&#20173;&#28982;&#23384;&#22312;&#37325;&#22823;&#25361;&#25112;&#65292;&#22240;&#20026;&#20165;&#20165;&#33719;&#24471;&#20854;&#36755;&#20986;&#26159;&#19981;&#22815;&#26377;&#29992;&#30340;&#12290;&#26412;&#25991;&#21457;&#29616;&#20102;&#22270;&#20687; DNN &#30340;&#35299;&#37322;&#26041;&#27861;&#30340;&#19968;&#20010;&#20196;&#20154;&#25285;&#24551;&#30340;&#23646;&#24615;&#65306;&#36890;&#36807;&#23545;&#36755;&#20837;&#22270;&#20687;&#36827;&#34892;&#24494;&#23567;&#30340;&#35270;&#35273;&#26356;&#25913;&#65292;&#25105;&#20204;&#28436;&#31034;&#20102;&#22914;&#20309;&#36890;&#36807;&#36827;&#21270;&#31574;&#30053;&#20219;&#24847;&#25805;&#32437;&#35299;&#37322;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#31639;&#27861; AttaXAI&#65292;&#19968;&#20010;&#38024;&#23545; XAI &#31639;&#27861;&#30340;&#27169;&#22411;&#26080;&#20851;&#30340;&#25932;&#23545;&#25915;&#20987;&#65292;&#21482;&#38656;&#35201;&#35775;&#38382;&#20998;&#31867;&#22120;&#30340;&#36755;&#20986;&#20449;&#24687;&#21644;&#35299;&#37322;&#22270;&#65292;&#36825;&#20123;&#24369;&#20551;&#35774;&#20351;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23454;&#38469;&#24212;&#29992;&#20013;&#38750;&#24120;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep neural networks (DNNs) have greatly impacted numerous fields over the past decade. Yet despite exhibiting superb performance over many problems, their black-box nature still poses a significant challenge with respect to explainability. Indeed, explainable artificial intelligence (XAI) is crucial in several fields, wherein the answer alone -- sans a reasoning of how said answer was derived -- is of little value. This paper uncovers a troubling property of explanation methods for image-based DNNs: by making small visual changes to the input image -- hardly influencing the network's output -- we demonstrate how explanations may be arbitrarily manipulated through the use of evolution strategies. Our novel algorithm, AttaXAI, a model-agnostic, adversarial attack on XAI algorithms, only requires access to the output logits of a classifier and to the explanation map; these weak assumptions render our approach highly useful where real-world models and data are concerned. We compare our me
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#30340;&#26694;&#26550;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#20197;&#35299;&#37322;&#35757;&#32451;&#26377;&#22810;&#20010;&#29289;&#20307;&#22270;&#20687;&#30340;&#20915;&#31574;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2211.12380</link><description>&lt;p&gt;
OCTET: &#23545;&#35937;&#24863;&#30693;&#30340;&#21453;&#20107;&#23454;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
OCTET: Object-aware Counterfactual Explanations. (arXiv:2211.12380v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.12380
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#30340;&#26694;&#26550;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#20197;&#35299;&#37322;&#35757;&#32451;&#26377;&#22810;&#20010;&#29289;&#20307;&#22270;&#20687;&#30340;&#20915;&#31574;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#65292;&#28145;&#24230;&#35270;&#35273;&#27169;&#22411;&#34987;&#24191;&#27867;&#24212;&#29992;&#20110;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#65292;&#22914;&#33258;&#21160;&#39550;&#39542;&#12290;&#27169;&#22411;&#30340;&#21487;&#35299;&#37322;&#24615;&#27491;&#22312;&#25104;&#20026;&#19968;&#20010;&#32039;&#36843;&#30340;&#38382;&#39064;&#12290;&#22312;&#20247;&#22810;&#35299;&#37322;&#26041;&#27861;&#20013;&#65292;&#21453;&#20107;&#23454;&#35299;&#37322;&#26088;&#22312;&#25214;&#21040;&#26368;&#23567;&#21644;&#21487;&#35299;&#37322;&#30340;&#21464;&#21270;&#26469;&#25913;&#21464;&#27169;&#22411;&#36755;&#20986;&#32467;&#26524;&#30340;&#36755;&#20837;&#22270;&#20687;&#12290;&#36825;&#26679;&#30340;&#35299;&#37322;&#20351;&#26368;&#32456;&#29992;&#25143;&#20102;&#35299;&#24433;&#21709;&#27169;&#22411;&#20915;&#31574;&#30340;&#20027;&#35201;&#22240;&#32032;&#12290;&#28982;&#32780;&#65292;&#20197;&#21069;&#30340;&#26041;&#27861;&#24456;&#38590;&#35299;&#37322;&#35757;&#32451;&#26377;&#22810;&#20010;&#29289;&#20307;&#22270;&#20687;&#65288;&#20363;&#22914;&#22478;&#24066;&#22330;&#26223;&#65289;&#30340;&#20915;&#31574;&#27169;&#22411;&#65292;&#36825;&#20123;&#22270;&#20687;&#26356;&#38590;&#22788;&#29702;&#65292;&#20294;&#20063;&#21487;&#33021;&#26356;&#20851;&#38190;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23545;&#35937;&#30340;&#26694;&#26550;&#26469;&#29983;&#25104;&#21453;&#20107;&#23454;&#35299;&#37322;&#65292;&#26088;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#21463;&#21040;&#26368;&#36817;&#29983;&#25104;&#24314;&#27169;&#24037;&#20316;&#30340;&#21551;&#21457;&#65292;&#23558;&#26597;&#35810;&#22270;&#20687;&#32534;&#30721;&#20026;&#19968;&#20010;&#32467;&#26500;&#21270;&#30340;&#28508;&#31354;&#38388;&#65292;&#20197;&#20415;&#36827;&#34892;&#22522;&#20110;&#23545;&#35937;&#30340;&#25805;&#20316;&#12290;&#36825;&#26679;&#20570;&#65292;&#21487;&#20197;&#20026;&#26368;&#32456;&#29992;&#25143;&#25552;&#20379;&#23545;&#27169;&#22411;&#30340;&#25511;&#21046;&#12290;
&lt;/p&gt;
&lt;p&gt;
Nowadays, deep vision models are being widely deployed in safety-critical applications, e.g., autonomous driving, and explainability of such models is becoming a pressing concern. Among explanation methods, counterfactual explanations aim to find minimal and interpretable changes to the input image that would also change the output of the model to be explained. Such explanations point end-users at the main factors that impact the decision of the model. However, previous methods struggle to explain decision models trained on images with many objects, e.g., urban scenes, which are more difficult to work with but also arguably more critical to explain. In this work, we propose to tackle this issue with an object-centric framework for counterfactual explanation generation. Our method, inspired by recent generative modeling works, encodes the query image into a latent space that is structured in a way to ease object-level manipulations. Doing so, it provides the end-user with control over w
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2210.14891</link><description>&lt;p&gt;
&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;
&lt;/p&gt;
&lt;p&gt;
Broken Neural Scaling Laws. (arXiv:2210.14891v7 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.14891
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65292;&#21487;&#20197;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#65292;&#21253;&#25324;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;&#26426;&#22120;&#20154;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
This paper proposes a smoothly broken power law functional form (referred to as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks for various architectures and a large and diverse set of tasks, including vision, language, audio, video, generative modeling, contrastive learning, robotics, uncertainty estimation/calibration, adversarial robustness, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforcement learning.
&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#24179;&#28369;&#30772;&#30862;&#30340;&#24130;&#24459;&#20989;&#25968;&#24418;&#24335;&#65288;&#25105;&#20204;&#31216;&#20043;&#20026;&#30772;&#30862;&#30340;&#31070;&#32463;&#32553;&#25918;&#23450;&#24459;&#65288;BNSL&#65289;&#65289;&#65292;&#23427;&#20934;&#30830;&#22320;&#27169;&#25311;&#21644;&#22806;&#25512;&#20102;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30340;&#32553;&#25918;&#34892;&#20026;&#65288;&#21363;&#24863;&#20852;&#36259;&#30340;&#35780;&#20272;&#25351;&#26631;&#38543;&#29992;&#20110;&#35757;&#32451;&#30340;&#35745;&#31639;&#37327;&#12289;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#12289;&#35757;&#32451;&#25968;&#25454;&#38598;&#22823;&#23567;&#25110;&#19978;&#28216;&#24615;&#33021;&#21464;&#21270;&#32780;&#21464;&#21270;&#65289;&#23545;&#20110;&#21508;&#31181;&#26550;&#26500;&#21644;&#22823;&#37327;&#19981;&#21516;&#20219;&#21153;&#20013;&#30340;&#27599;&#20010;&#20219;&#21153;&#65292;&#21253;&#25324;&#22823;&#35268;&#27169;&#35270;&#35273;&#12289;&#35821;&#35328;&#12289;&#38899;&#39057;&#12289;&#35270;&#39057;&#12289;&#25193;&#25955;&#12289;&#29983;&#25104;&#24314;&#27169;&#12289;&#22810;&#27169;&#24577;&#23398;&#20064;&#12289;&#23545;&#27604;&#23398;&#20064;&#12289;AI&#23545;&#40784;&#12289;&#26426;&#22120;&#20154;&#12289;&#36229;&#20986;&#20998;&#24067;&#65288;OOD&#65289;&#27867;&#21270;&#12289;&#25345;&#32493;&#23398;&#20064;&#12289;&#19981;&#30830;&#23450;&#24615;&#20272;&#35745;/&#26657;&#20934;&#12289;&#36229;&#20986;&#20998;&#24067;&#26816;&#27979;&#12289;&#23545;&#25239;&#40065;&#26834;&#24615;&#12289;&#33976;&#39311;&#12289;&#20998;&#23376;&#12289;&#35745;&#31639;&#26426;&#32534;&#31243;/&#32534;&#30721;&#12289;&#25968;&#23398;&#21333;&#35789;&#38382;&#39064;&#12289;&#31639;&#26415;&#12289;&#26080;&#30417;&#30563;/&#33258;&#30417;&#30563;&#23398;&#20064;&#21644;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a smoothly broken power law functional form (referred to by us as a Broken Neural Scaling Law (BNSL)) that accurately models and extrapolates the scaling behaviors of deep neural networks (i.e. how the evaluation metric of interest varies as the amount of compute used for training, number of model parameters, training dataset size, or upstream performance varies) for various architectures and for each of various tasks within a large and diverse set of upstream and downstream tasks, in zero-shot, prompted, and fine-tuned settings. This set includes large-scale vision, language, audio, video, diffusion, generative modeling, multimodal learning, contrastive learning, AI alignment, robotics, out-of-distribution (OOD) generalization, continual learning, uncertainty estimation / calibration, out-of-distribution detection, adversarial robustness, distillation, molecules, computer programming/coding, math word problems, arithmetic, unsupervised/self-supervised learning, and reinforc
&lt;/p&gt;</description></item><item><title>&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#30721;&#32452;&#21512;&#20449;&#24687;&#26041;&#38754;&#30340;&#34920;&#29616;&#23384;&#22312;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Attribution&#12289;Relation&#21644;Order&#65288;ARO&#65289;&#22522;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;VLMs&#36827;&#34892;Attention&#26426;&#21046;&#21644;&#23545;&#25239;&#35757;&#32451;&#31561;&#20462;&#25913;&#20197;&#25552;&#39640;&#20854;&#32452;&#21512;&#29702;&#35299;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2210.01936</link><description>&lt;p&gt;
&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#20309;&#26102;&#20197;&#21450;&#20026;&#20309;&#34892;&#20026;&#20687;&#35789;&#34955;&#65292;&#20197;&#21450;&#22914;&#20309;&#35299;&#20915;&#65311;
&lt;/p&gt;
&lt;p&gt;
When and why vision-language models behave like bags-of-words, and what to do about it?. (arXiv:2210.01936v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.01936
&lt;/p&gt;
&lt;p&gt;
&#38024;&#23545;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#32534;&#30721;&#32452;&#21512;&#20449;&#24687;&#26041;&#38754;&#30340;&#34920;&#29616;&#23384;&#22312;&#38382;&#39064;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Attribution&#12289;Relation&#21644;Order&#65288;ARO&#65289;&#22522;&#20934;&#65292;&#24182;&#25552;&#20986;&#20102;&#23545;VLMs&#36827;&#34892;Attention&#26426;&#21046;&#21644;&#23545;&#25239;&#35757;&#32451;&#31561;&#20462;&#25913;&#20197;&#25552;&#39640;&#20854;&#32452;&#21512;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#22823;&#22411;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65288;VLM&#65289;&#22312;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#25104;&#21151;&#65292;&#20294;&#23427;&#20204;&#32534;&#30721;&#32452;&#21512;&#20449;&#24687;&#30340;&#33021;&#21147;&#23578;&#19981;&#28165;&#26970;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21019;&#24314;&#20102;Attribution&#12289;Relation&#21644;Order&#65288;ARO&#65289;&#22522;&#20934;&#26469;&#31995;&#32479;&#35780;&#20272;VLM&#29702;&#35299;&#19981;&#21516;&#31867;&#22411;&#20851;&#31995;&#12289;&#23646;&#24615;&#21644;&#39034;&#24207;&#30340;&#33021;&#21147;&#12290;ARO&#30001;Visual Genome Attribution&#27979;&#35797;&#23545;&#35937;&#23646;&#24615;&#30340;&#29702;&#35299;&#33021;&#21147;&#65307;Visual Genome Relation&#27979;&#35797;&#20851;&#31995;&#29702;&#35299;&#33021;&#21147;&#65307;&#20197;&#21450;COCO&#65286;Flickr30k-Order&#27979;&#35797;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;ARO&#27604;&#20197;&#21069;&#30340;&#32452;&#21512;&#24615;&#22522;&#20934;&#22823;&#22810;&#20010;&#25968;&#37327;&#32423;&#65292;&#21253;&#25324;50,000&#22810;&#20010;&#27979;&#35797;&#29992;&#20363;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#26368;&#20808;&#36827;&#30340;VLM&#22312;&#21738;&#20123;&#26041;&#38754;&#23384;&#22312;&#20851;&#31995;&#29702;&#35299;&#38382;&#39064;&#65292;&#24403;&#38142;&#25509;&#23545;&#35937;&#21644;&#23646;&#24615;&#26102;&#23481;&#26131;&#20986;&#38169;&#65292;&#24182;&#23637;&#31034;&#20102;&#20854;&#26126;&#26174;&#30340;&#32570;&#20047;&#39034;&#24207;&#25935;&#24863;&#24615;&#12290;VLM&#20027;&#35201;&#22312;&#20855;&#26377;&#20016;&#23500;&#32452;&#21512;&#32467;&#26500;&#30340;&#22270;&#20687;&#21644;&#26631;&#39064;&#30340;&#22823;&#22411;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#35757;&#32451;&#21644;&#35780;&#20272;&#12290;&#28982;&#32780;&#65292;&#20165;&#20165;&#22312;&#36825;&#20123;&#25968;&#25454;&#38598;&#19978;&#35757;&#32451;&#24182;&#19981;&#33021;&#20445;&#35777;&#22312;&#38656;&#35201;&#32452;&#21512;&#29702;&#35299;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#33391;&#22909;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20960;&#31181;&#20462;&#25913;VLMs&#30340;&#26041;&#27861;&#65292;&#21253;&#25324;&#24378;&#35843;&#32452;&#25104;&#20851;&#31995;&#30340;&#27880;&#24847;&#26426;&#21046;&#21644;&#20351;&#29992;&#23545;&#25239;&#35757;&#32451;&#26469;&#25913;&#21892;&#23646;&#24615;&#39044;&#27979;&#12290;&#25105;&#20204;&#25552;&#20986;&#30340;&#20462;&#25913;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;VLM&#22312;ARO&#22522;&#20934;&#19978;&#30340;&#32452;&#21512;&#29702;&#35299;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite the success of large vision and language models (VLMs) in many downstream applications, it is unclear how well they encode compositional information. Here, we create the Attribution, Relation, and Order (ARO) benchmark to systematically evaluate the ability of VLMs to understand different types of relationships, attributes, and order. ARO consists of Visual Genome Attribution, to test the understanding of objects' properties; Visual Genome Relation, to test for relational understanding; and COCO &amp; Flickr30k-Order, to test for order sensitivity. ARO is orders of magnitude larger than previous benchmarks of compositionality, with more than 50,000 test cases. We show where state-of-the-art VLMs have poor relational understanding, can blunder when linking objects to their attributes, and demonstrate a severe lack of order sensitivity. VLMs are predominantly trained and evaluated on large datasets with rich compositional structure in the images and captions. Yet, training on these d
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#25104;&#26412;&#21644;&#28145;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20854;&#20250;&#25910;&#25947;&#21040;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#31209;&#30340;&#27010;&#24565;&#12290;&#21516;&#26102;&#65292;&#21457;&#29616;&#22312;&#19968;&#23450;&#30340;&#28145;&#24230;&#33539;&#22260;&#20869;&#65292;&#20840;&#23616;&#26368;&#23567;&#20540;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#30340;&#25968;&#25454;&#31209;&#65292;&#24182;&#25506;&#35752;&#20102;&#20998;&#31867;&#22120;&#31209;&#23545;&#31867;&#36793;&#30028;&#25299;&#25169;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2209.15055</link><description>&lt;p&gt;
&#22823;&#28145;&#24230;&#32593;&#32476;&#30340;&#38544;&#24335;&#20559;&#35265;&#65306;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#31209;&#23450;&#20041;&#12290;
&lt;/p&gt;
&lt;p&gt;
Implicit Bias of Large Depth Networks: a Notion of Rank for Nonlinear Functions. (arXiv:2209.15055v4 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.15055
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#25104;&#26412;&#21644;&#28145;&#24230;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#21457;&#29616;&#20854;&#20250;&#25910;&#25947;&#21040;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#31209;&#30340;&#27010;&#24565;&#12290;&#21516;&#26102;&#65292;&#21457;&#29616;&#22312;&#19968;&#23450;&#30340;&#28145;&#24230;&#33539;&#22260;&#20869;&#65292;&#20840;&#23616;&#26368;&#23567;&#20540;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#30340;&#25968;&#25454;&#31209;&#65292;&#24182;&#25506;&#35752;&#20102;&#20998;&#31867;&#22120;&#31209;&#23545;&#31867;&#36793;&#30028;&#25299;&#25169;&#32467;&#26500;&#30340;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#23637;&#31034;&#20102;&#20855;&#26377;&#40784;&#27425;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#20840;&#36830;&#25509;&#31070;&#32463;&#32593;&#32476;&#30340;&#34920;&#31034;&#25104;&#26412;&#8212;&#8212;&#25551;&#36848;&#20102;&#20855;&#26377;$L_2$&#27491;&#21017;&#21270;&#25110;&#20132;&#21449;&#29109;&#31561;&#25439;&#22833;&#32593;&#32476;&#22312;&#20989;&#25968;&#31354;&#38388;&#20013;&#30340;&#38544;&#24335;&#20559;&#35265;&#8212;&#8212;&#38543;&#30528;&#32593;&#32476;&#28145;&#24230;&#36235;&#36817;&#20110;&#26080;&#31351;&#22823;&#65292;&#20250;&#25910;&#25947;&#21040;&#38750;&#32447;&#24615;&#20989;&#25968;&#30340;&#31209;&#30340;&#27010;&#24565;&#12290;&#28982;&#21518;&#65292;&#25105;&#20204;&#25506;&#31350;&#20102;&#20840;&#23616;&#26368;&#23567;&#20540;&#22312;&#21738;&#20123;&#26465;&#20214;&#19979;&#21487;&#20197;&#24674;&#22797;&#8220;&#30495;&#23454;&#8221;&#25968;&#25454;&#31209;&#65306;&#25105;&#20204;&#23637;&#31034;&#20986;&#65292;&#23545;&#20110;&#22826;&#22823;&#30340;&#28145;&#24230;&#65292;&#20840;&#23616;&#26368;&#23567;&#20540;&#20250;&#36817;&#20284;&#20026;&#31209;1&#65288;&#20302;&#20272;&#31209;&#65289;&#65307;&#25105;&#20204;&#38543;&#21518;&#35770;&#35777;&#20102;&#26377;&#19968;&#31995;&#21015;&#28145;&#24230;&#65292;&#38543;&#30528;&#25968;&#25454;&#28857;&#25968;&#37327;&#22686;&#21152;&#65292;&#21487;&#20197;&#24674;&#22797;&#30495;&#23454;&#31209;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#35752;&#35770;&#20102;&#20998;&#31867;&#22120;&#31209;&#23545;&#32467;&#26524;&#31867;&#36793;&#30028;&#30340;&#25299;&#25169;&#32467;&#26500;&#30340;&#24433;&#21709;&#65292;&#24182;&#23637;&#31034;&#20102;&#20855;&#26377;&#26368;&#20339;&#38750;&#32447;&#24615;&#31209;&#30340;&#33258;&#32534;&#30721;&#22120;&#20855;&#26377;&#33258;&#28982;&#21435;&#22122;&#30340;&#29305;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show that the representation cost of fully connected neural networks with homogeneous nonlinearities - which describes the implicit bias in function space of networks with $L_2$-regularization or with losses such as the cross-entropy - converges as the depth of the network goes to infinity to a notion of rank over nonlinear functions. We then inquire under which conditions the global minima of the loss recover the `true' rank of the data: we show that for too large depths the global minimum will be approximately rank 1 (underestimating the rank); we then argue that there is a range of depths which grows with the number of datapoints where the true rank is recovered. Finally, we discuss the effect of the rank of a classifier on the topology of the resulting class boundaries and show that autoencoders with optimal nonlinear rank are naturally denoising.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36830;&#32493;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#23558;&#21487;&#35745;&#31639;&#27010;&#29575;&#27169;&#22411;&#21644;&#22522;&#20110;&#36830;&#32493;&#28508;&#31354;&#38388;&#30340;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27010;&#29575;&#25512;&#26029;&#12290;</title><link>http://arxiv.org/abs/2209.10584</link><description>&lt;p&gt;
&#21487;&#35745;&#31639;&#27010;&#29575;&#27169;&#22411;&#30340;&#36830;&#32493;&#28151;&#21512;
&lt;/p&gt;
&lt;p&gt;
Continuous Mixtures of Tractable Probabilistic Models. (arXiv:2209.10584v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.10584
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#36830;&#32493;&#28151;&#21512;&#30340;&#26041;&#27861;&#65292;&#23558;&#21487;&#35745;&#31639;&#27010;&#29575;&#27169;&#22411;&#21644;&#22522;&#20110;&#36830;&#32493;&#28508;&#31354;&#38388;&#30340;&#27169;&#22411;&#32467;&#21512;&#36215;&#26469;&#65292;&#20174;&#32780;&#23454;&#29616;&#20102;&#39640;&#25928;&#30340;&#27010;&#29575;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#36830;&#32493;&#28508;&#31354;&#38388;&#30340;&#27010;&#29575;&#27169;&#22411;&#65292;&#22914;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#65292;&#21487;&#20197;&#29702;&#35299;&#20026;&#22312;&#28508;&#21464;&#37327;&#19978;&#36830;&#32493;&#20381;&#36182;&#30340;&#19981;&#21487;&#25968;&#28151;&#21512;&#27169;&#22411;&#12290;&#23427;&#20204;&#24050;&#34987;&#35777;&#26126;&#26159;&#34920;&#36798;&#29983;&#25104;&#21644;&#27010;&#29575;&#24314;&#27169;&#30340;&#26377;&#25928;&#24037;&#20855;&#65292;&#20294;&#19982;&#33021;&#22815;&#35745;&#31639;&#25152;&#34920;&#31034;&#30340;&#27010;&#29575;&#20998;&#24067;&#30340;&#36739;&#20026;&#31616;&#21333;&#30340;&#21487;&#35745;&#31639;&#27010;&#29575;&#25512;&#26029;&#26041;&#27861;&#23384;&#22312;&#30683;&#30462;&#12290;&#19982;&#27492;&#21516;&#26102;&#65292;&#21487;&#35745;&#31639;&#27010;&#29575;&#27169;&#22411;&#65292;&#22914;&#27010;&#29575;&#30005;&#36335;(PCs)&#65292;&#21487;&#20197;&#29702;&#35299;&#20026;&#23618;&#27425;&#31163;&#25955;&#28151;&#21512;&#27169;&#22411;&#65292;&#22240;&#27492;&#33021;&#22815;&#39640;&#25928;&#22320;&#36827;&#34892;&#20934;&#30830;&#25512;&#26029;&#65292;&#20294;&#22312;&#19982;&#36830;&#32493;&#28508;&#31354;&#38388;&#27169;&#22411;&#30456;&#27604;&#26102;&#36890;&#24120;&#34920;&#29616;&#19981;&#20339;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#28151;&#21512;&#26041;&#27861;&#65292;&#21363;&#20855;&#26377;&#23567;&#28508;&#21464;&#37327;&#32500;&#24230;&#30340;&#21487;&#35745;&#31639;&#27169;&#22411;&#30340;&#36830;&#32493;&#28151;&#21512;&#12290;&#23613;&#31649;&#36825;&#20123;&#27169;&#22411;&#22312;&#35299;&#26512;&#19978;&#26159;&#38590;&#20197;&#22788;&#29702;&#30340;&#65292;&#20294;&#23427;&#20204;&#36866;&#21512;&#20110;&#22522;&#20110;&#26377;&#38480;&#31215;&#20998;&#28857;&#38598;&#30340;&#25968;&#20540;&#31215;&#20998;&#26041;&#26696;&#12290;&#36890;&#36807;&#36275;&#22815;&#22823;&#30340;&#31215;&#20998;&#28857;&#38598;&#65292;&#25105;&#20204;&#21487;&#20197;&#32039;&#23494;&#22320;&#36817;&#20284;&#27010;&#29575;&#27169;&#22411;&#24182;&#36827;&#34892;&#39640;&#25928;&#25512;&#26029;&#12290;
&lt;/p&gt;
&lt;p&gt;
Probabilistic models based on continuous latent spaces, such as variational autoencoders, can be understood as uncountable mixture models where components depend continuously on the latent code. They have proven to be expressive tools for generative and probabilistic modelling, but are at odds with tractable probabilistic inference, that is, computing marginals and conditionals of the represented probability distribution. Meanwhile, tractable probabilistic models such as probabilistic circuits (PCs) can be understood as hierarchical discrete mixture models, and thus are capable of performing exact inference efficiently but often show subpar performance in comparison to continuous latent-space models. In this paper, we investigate a hybrid approach, namely continuous mixtures of tractable models with a small latent dimension. While these models are analytically intractable, they are well amenable to numerical integration schemes based on a finite set of integration points. With a large 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#20132;&#36890;&#30417;&#27979;&#31995;&#32479;&#21644;CCTV&#30456;&#26426;&#30340;&#33258;&#21160;&#21270;&#36335;&#28783;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#26816;&#27979;&#34892;&#20154;&#25110;&#36710;&#36742;&#30340;&#23384;&#22312;&#24182;&#35843;&#33410;LED&#36335;&#28783;&#30340;&#20142;&#24230;&#65292;&#20943;&#23569;&#20102;&#33021;&#28304;&#28010;&#36153;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2209.08633</link><description>&lt;p&gt;
&#22522;&#20110;&#26234;&#33021;CCTV&#30456;&#26426;&#21644;&#35821;&#20041;&#20998;&#21106;&#30340;CNN&#26234;&#33021;&#36335;&#28783;&#31649;&#29702;
&lt;/p&gt;
&lt;p&gt;
CNN based Intelligent Streetlight Management Using Smart CCTV Camera and Semantic Segmentation. (arXiv:2209.08633v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2209.08633
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#26234;&#33021;&#20132;&#36890;&#30417;&#27979;&#31995;&#32479;&#21644;CCTV&#30456;&#26426;&#30340;&#33258;&#21160;&#21270;&#36335;&#28783;&#25511;&#21046;&#26041;&#27861;&#65292;&#21033;&#29992;&#35821;&#20041;&#20998;&#21106;&#25216;&#26415;&#26816;&#27979;&#34892;&#20154;&#25110;&#36710;&#36742;&#30340;&#23384;&#22312;&#24182;&#35843;&#33410;LED&#36335;&#28783;&#30340;&#20142;&#24230;&#65292;&#20943;&#23569;&#20102;&#33021;&#28304;&#28010;&#36153;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#34903;&#28783;&#26159;&#26368;&#23481;&#26131;&#34987;&#24573;&#35270;&#30340;&#33021;&#28304;&#28010;&#36153;&#28304;&#20043;&#19968;&#65292;&#20854;&#22312;&#19981;&#38656;&#35201;&#29031;&#26126;&#30340;&#21306;&#22495;&#21457;&#20986;&#36807;&#22810;&#30340;&#20809;&#32447;&#65292;&#36896;&#25104;&#24040;&#22823;&#30340;&#32463;&#27982;&#21644;&#29615;&#22659;&#24433;&#21709;&#12290;&#30001;&#20110;&#20256;&#32479;&#30340;&#20154;&#24037;&#25805;&#20316;&#26041;&#24335;&#65292;&#34903;&#28783;&#24120;&#24120;&#20250;&#22312;&#30333;&#22825;&#34987;&#25171;&#24320;&#65292;&#22312;&#26202;&#19978;&#34987;&#20851;&#38381;&#65292;&#36825;&#22312;21&#19990;&#32426;&#20173;&#28982;&#24456;&#36951;&#25022;&#12290;&#22240;&#27492;&#38656;&#35201;&#33258;&#21160;&#21270;&#30340;&#36335;&#28783;&#25511;&#21046;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#23558;&#30001;&#35745;&#31639;&#26426;&#35270;&#35273;&#25216;&#26415;&#39537;&#21160;&#30340;&#26234;&#33021;&#20132;&#36890;&#30417;&#27979;&#31995;&#32479;&#19982;&#38381;&#36335;&#30005;&#35270;(CCTV)&#30456;&#26426;&#30456;&#32467;&#21512;&#65292;&#21033;&#29992;CCTV&#35270;&#39057;&#27969;&#20013;&#30340;&#35821;&#20041;&#22270;&#20687;&#20998;&#21106;&#26816;&#27979;&#34892;&#20154;&#25110;&#36710;&#36742;&#30340;&#23384;&#22312;&#24182;&#22312;&#20854;&#32570;&#24109;&#26102;&#35843;&#33410;&#34903;&#28783;&#30340;&#20142;&#24230;&#65292;&#20174;&#32780;&#24320;&#21457;&#19968;&#31181;&#26032;&#22411;&#30340;&#36335;&#28783;&#25511;&#21046;&#26041;&#27861;&#12290;&#22240;&#27492;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#21306;&#20998;&#30333;&#22825;&#21644;&#40657;&#22812;&#65292;&#24182;&#26681;&#25454;&#38656;&#35201;&#33258;&#21160;&#35843;&#33410;LED&#36335;&#28783;&#30340;&#20142;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;
One of the most neglected sources of energy loss is streetlights which generate too much light in areas where it is not required. Energy waste has enormous economic and environmental effects. In addition, due to the conventional manual nature of the operation, streetlights are frequently seen being turned ON during the day and OFF in the evening, which is regrettable even in the twenty-first century. These issues require automated streetlight control in order to be resolved. This study aims to develop a novel streetlight controlling method by combining a smart transport monitoring system powered by computer vision technology with a closed circuit television (CCTV) camera that allows the light-emitting diode (LED) streetlight to automatically light up with the appropriate brightness by detecting the presence of pedestrians or vehicles and dimming the streetlight in their absence using semantic image segmentation from the CCTV video streaming. Consequently, our model distinguishes daylig
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#20013;&#65292;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#35774;&#35745;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#30340;&#39640;&#25928;&#21033;&#29992;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#31232;&#30095;&#37319;&#26679;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#30446;&#26631;&#26816;&#27979;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2208.11356</link><description>&lt;p&gt;
&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#20013;&#22810;&#23610;&#24230;&#29305;&#24449;&#30340;&#26377;&#25928;&#21033;&#29992;&#26041;&#27861;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Towards Efficient Use of Multi-Scale Features in Transformer-Based Object Detectors. (arXiv:2208.11356v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2208.11356
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#20013;&#65292;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#35774;&#35745;&#23454;&#29616;&#20102;&#22810;&#23610;&#24230;&#29305;&#24449;&#30340;&#39640;&#25928;&#21033;&#29992;&#65292;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#36890;&#36807;&#31232;&#30095;&#37319;&#26679;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#20197;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#65292;&#21516;&#26102;&#20445;&#35777;&#20102;&#30446;&#26631;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#23610;&#24230;&#29305;&#24449;&#24050;&#34987;&#35777;&#26126;&#23545;&#20110;&#30446;&#26631;&#26816;&#27979;&#38750;&#24120;&#26377;&#25928;&#65292;&#20294;&#24448;&#24448;&#38656;&#35201;&#24040;&#22823;&#29978;&#33267;&#26159;&#31105;&#27490;&#24615;&#30340;&#39069;&#22806;&#35745;&#31639;&#25104;&#26412;&#65292;&#23588;&#20854;&#26159;&#23545;&#20110;&#26368;&#36817;&#30340;&#22522;&#20110;Transformer&#30340;&#26816;&#27979;&#22120;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36845;&#20195;&#22810;&#23610;&#24230;&#29305;&#24449;&#32858;&#21512;&#65288;IMFA&#65289;&#30340;&#36890;&#29992;&#26041;&#27861;&#65292;&#20197;&#23454;&#29616;&#22312;&#22522;&#20110;Transformer&#30340;&#30446;&#26631;&#26816;&#27979;&#22120;&#20013;&#39640;&#25928;&#20351;&#29992;&#22810;&#23610;&#24230;&#29305;&#24449;&#12290;&#20854;&#26680;&#24515;&#24605;&#24819;&#26159;&#21033;&#29992;&#26469;&#33258;&#20165;&#26377;&#20960;&#20010;&#20851;&#38190;&#20301;&#32622;&#30340;&#31232;&#30095;&#22810;&#23610;&#24230;&#29305;&#24449;&#65292;&#24182;&#36890;&#36807;&#20004;&#20010;&#26032;&#30340;&#35774;&#35745;&#23454;&#29616;&#12290;&#39318;&#20808;&#65292;IMFA&#37325;&#26032;&#25490;&#21015;Transformer&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#31649;&#36947;&#65292;&#20197;&#20415;&#26681;&#25454;&#26816;&#27979;&#39044;&#27979;&#36845;&#20195;&#26356;&#26032;&#32534;&#30721;&#29305;&#24449;&#12290;&#20854;&#27425;&#65292;IMFA&#22312;&#20197;&#20808;&#21069;&#30340;&#26816;&#27979;&#39044;&#27979;&#20316;&#20026;&#25351;&#23548;&#30340;&#24773;&#20917;&#19979;&#65292;&#20174;&#20165;&#26377;&#20960;&#20010;&#20851;&#38190;&#28857;&#37319;&#29992;&#31232;&#30095;&#30340;&#23610;&#24230;&#33258;&#36866;&#24212;&#29305;&#24449;&#29992;&#20110;&#31934;&#32454;&#26816;&#27979;&#12290;&#22240;&#27492;&#65292;&#37319;&#26679;&#30340;&#22810;&#23610;&#24230;&#29305;&#24449;&#26159;&#31232;&#30095;&#30340;&#65292;&#20294;&#23545;&#20110;&#30446;&#26631;&#26816;&#27979;&#20173;&#28982;&#38750;&#24120;&#26377;&#30410;&#12290;&#22823;&#37327;&#23454;&#39564;&#35777;&#26126;&#65292;&#25152;&#25552;&#20986;&#30340;IMFA&#25216;&#26415;&#21487;&#20197;&#22312;&#20445;&#35777;&#30446;&#26631;&#26816;&#27979;&#24615;&#33021;&#30340;&#21516;&#26102;&#26174;&#33879;&#20943;&#23569;&#35745;&#31639;&#25104;&#26412;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multi-scale features have been proven highly effective for object detection but often come with huge and even prohibitive extra computation costs, especially for the recent Transformer-based detectors. In this paper, we propose Iterative Multi-scale Feature Aggregation (IMFA) -- a generic paradigm that enables efficient use of multi-scale features in Transformer-based object detectors. The core idea is to exploit sparse multi-scale features from just a few crucial locations, and it is achieved with two novel designs. First, IMFA rearranges the Transformer encoder-decoder pipeline so that the encoded features can be iteratively updated based on the detection predictions. Second, IMFA sparsely samples scale-adaptive features for refined detection from just a few keypoint locations under the guidance of prior detection predictions. As a result, the sampled multi-scale features are sparse yet still highly beneficial for object detection. Extensive experiments show that the proposed IMFA bo
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#20869;&#37096;&#34920;&#31034;&#30340;&#23545;&#31216;&#24615;&#65292;&#21457;&#29616;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#20256;&#25773;&#21040;&#25968;&#25454;&#30340;&#34920;&#31034;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;&#26550;&#26500;&#24433;&#21709;&#23398;&#20064;&#21644;&#39044;&#27979;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2205.14258</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#20869;&#37096;&#34920;&#31034;&#30340;&#23545;&#31216;&#24615;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Symmetries of Deep Learning Models and their Internal Representations. (arXiv:2205.14258v5 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2205.14258
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21450;&#20854;&#20869;&#37096;&#34920;&#31034;&#30340;&#23545;&#31216;&#24615;&#65292;&#21457;&#29616;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#20256;&#25773;&#21040;&#25968;&#25454;&#30340;&#34920;&#31034;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#20026;&#26356;&#22909;&#22320;&#29702;&#35299;&#26550;&#26500;&#24433;&#21709;&#23398;&#20064;&#21644;&#39044;&#27979;&#36807;&#31243;&#25552;&#20379;&#20102;&#19968;&#31181;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#31216;&#24615;&#26159;&#30740;&#31350;&#22797;&#26434;&#31995;&#32479;&#30340;&#22522;&#26412;&#24037;&#20855;&#12290;&#22312;&#26426;&#22120;&#23398;&#20064;&#20013;&#65292;&#23545;&#31216;&#24615;&#24050;&#32463;&#22312;&#27169;&#22411;&#21644;&#25968;&#25454;&#20013;&#24471;&#21040;&#20102;&#30740;&#31350;&#12290;&#26412;&#25991;&#26088;&#22312;&#36890;&#36807;&#35745;&#31639;&#27169;&#22411;&#30340;&#19968;&#32452;&#22522;&#26412;&#23545;&#31216;&#32676;&#65292;&#21363;&#27169;&#22411;&#30340;intertwiner groups&#65292;&#23558;&#27169;&#22411;&#26550;&#26500;&#30340;&#23545;&#31216;&#24615;&#19982;&#35813;&#27169;&#22411;&#23545;&#25968;&#25454;&#30340;&#20869;&#37096;&#34920;&#31034;&#30340;&#23545;&#31216;&#24615;&#30456;&#32852;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#19968;&#31995;&#21015;&#23454;&#39564;&#23558;intertwiner groups&#36830;&#25509;&#21040;&#20855;&#26377;&#30456;&#21516;&#26550;&#26500;&#30340;&#27169;&#22411;&#30340;&#38544;&#34255;&#29366;&#24577;&#20043;&#38388;&#30340;&#30456;&#20284;&#20043;&#22788;&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#32593;&#32476;&#30340;&#23545;&#31216;&#24615;&#20256;&#25773;&#21040;&#35813;&#32593;&#32476;&#23545;&#25968;&#25454;&#30340;&#34920;&#31034;&#20013;&#30340;&#23545;&#31216;&#24615;&#65292;&#26356;&#22909;&#22320;&#29702;&#35299;&#26550;&#26500;&#22914;&#20309;&#24433;&#21709;&#23398;&#20064;&#21644;&#39044;&#27979;&#36807;&#31243;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25512;&#27979;&#65292;&#23545;&#20110;ReLU&#32593;&#32476;&#65292;intertwiner groups&#21487;&#33021;&#20026;&#24120;&#35265;&#30340;&#22534;&#21472;&#32593;&#32476;&#32467;&#26500;&#25552;&#20379;&#20102;&#27491;&#24403;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Symmetry is a fundamental tool in the exploration of a broad range of complex systems. In machine learning symmetry has been explored in both models and data. In this paper we seek to connect the symmetries arising from the architecture of a family of models with the symmetries of that family's internal representation of data. We do this by calculating a set of fundamental symmetry groups, which we call the intertwiner groups of the model. We connect intertwiner groups to a model's internal representations of data through a range of experiments that probe similarities between hidden states across models with the same architecture. Our work suggests that the symmetries of a network are propagated into the symmetries in that network's representation of data, providing us with a better understanding of how architecture affects the learning and prediction process. Finally, we speculate that for ReLU networks, the intertwiner groups may provide a justification for the common practice of con
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27700;&#24211;&#35745;&#31639;&#27169;&#22411;EuSN&#65292;&#20854;&#21033;&#29992;&#21069;&#21521;&#27431;&#25289;&#31163;&#25955;&#21270;&#21644;&#21453;&#23545;&#31216;&#24490;&#29615;&#30697;&#38453;&#26469;&#35774;&#35745;&#27700;&#24211;&#21160;&#21147;&#23398;&#65292;&#20855;&#26377;&#25509;&#36817;&#31283;&#23450;&#36793;&#32536;&#30340;&#39281;&#21644;&#26377;&#25928;&#35889;&#21322;&#24452;&#21644;&#38646;&#23616;&#37096;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#12290;&#22312;&#38271;&#26399;&#35760;&#24518;&#20219;&#21153;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2203.09382</link><description>&lt;p&gt;
Euler State Networks: &#38750;&#32791;&#25955;&#24615;&#27700;&#24211;&#35745;&#31639;
&lt;/p&gt;
&lt;p&gt;
Euler State Networks: Non-dissipative Reservoir Computing. (arXiv:2203.09382v3 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2203.09382
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27700;&#24211;&#35745;&#31639;&#27169;&#22411;EuSN&#65292;&#20854;&#21033;&#29992;&#21069;&#21521;&#27431;&#25289;&#31163;&#25955;&#21270;&#21644;&#21453;&#23545;&#31216;&#24490;&#29615;&#30697;&#38453;&#26469;&#35774;&#35745;&#27700;&#24211;&#21160;&#21147;&#23398;&#65292;&#20855;&#26377;&#25509;&#36817;&#31283;&#23450;&#36793;&#32536;&#30340;&#39281;&#21644;&#26377;&#25928;&#35889;&#21322;&#24452;&#21644;&#38646;&#23616;&#37096;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#12290;&#22312;&#38271;&#26399;&#35760;&#24518;&#20219;&#21153;&#21644;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#20102;&#20248;&#24322;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#21463;&#21040;&#24120;&#24494;&#20998;&#26041;&#31243;&#30340;&#25968;&#20540;&#35299;&#21551;&#21457;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#22411;&#27700;&#24211;&#35745;&#31639;(EuSN)&#27169;&#22411;&#65292;&#35813;&#26041;&#27861;&#21033;&#29992;&#21069;&#21521;&#27431;&#25289;&#31163;&#25955;&#21270;&#21644;&#21453;&#23545;&#31216;&#24490;&#29615;&#30697;&#38453;&#26469;&#35774;&#35745;&#27700;&#24211;&#21160;&#21147;&#23398;&#12290;&#35813;&#27169;&#22411;&#30340;&#25968;&#23398;&#20998;&#26512;&#34920;&#26126;&#65292;&#20854;&#20855;&#26377;&#25509;&#36817;&#31283;&#23450;&#36793;&#32536;&#30340;&#39281;&#21644;&#26377;&#25928;&#35889;&#21322;&#24452;&#21644;&#38646;&#23616;&#37096;&#26446;&#38597;&#26222;&#35834;&#22827;&#25351;&#25968;&#12290;&#23545;&#38271;&#26399;&#35760;&#24518;&#20219;&#21153;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;&#19982;&#26631;&#20934;&#27700;&#24211;&#35745;&#31639;&#27169;&#22411;&#30456;&#27604;&#65292;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#38656;&#35201;&#22810;&#20010;&#26102;&#27493;&#26377;&#25928;&#20256;&#25773;&#36755;&#20837;&#20449;&#24687;&#30340;&#38382;&#39064;&#19978;&#20855;&#26377;&#26126;&#26174;&#30340;&#20248;&#21183;&#12290;&#27492;&#22806;&#65292;&#26102;&#38388;&#24207;&#21015;&#20998;&#31867;&#22522;&#20934;&#27979;&#35797;&#32467;&#26524;&#34920;&#26126;&#65292;EuSN&#33021;&#22815;&#21305;&#37197;&#29978;&#33267;&#36229;&#36807;&#21487;&#35757;&#32451;&#24490;&#29615;&#31070;&#32463;&#32593;&#32476;&#30340;&#20934;&#30830;&#24615;&#65292;&#21516;&#26102;&#20445;&#25345;&#27700;&#24211;&#35745;&#31639;&#23478;&#26063;&#30340;&#35757;&#32451;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;
Inspired by the numerical solution of ordinary differential equations, in this paper we propose a novel Reservoir Computing (RC) model, called the Euler State Network (EuSN). The presented approach makes use of forward Euler discretization and antisymmetric recurrent matrices to design reservoir dynamics that are both stable and non-dissipative by construction.  Our mathematical analysis shows that the resulting model is biased towards a unitary effective spectral radius and zero local Lyapunov exponents, intrinsically operating near to the edge of stability. Experiments on long-term memory tasks show the clear superiority of the proposed approach over standard RC models in problems requiring effective propagation of input information over multiple time-steps. Furthermore, results on time-series classification benchmarks indicate that EuSN is able to match (or even exceed) the accuracy of trainable Recurrent Neural Networks, while retaining the training efficiency of the RC family, res
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20915;&#31574;&#30340;&#31232;&#30095;&#25915;&#20987;&#26041;&#27861;SparseEvo&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25200;&#21160;&#30340;&#20687;&#32032;&#25968;&#37327;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#27450;&#39575;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#20102;&#35299;&#23427;&#30340;&#20869;&#37096;&#26500;&#36896;&#12290;</title><link>http://arxiv.org/abs/2202.00091</link><description>&lt;p&gt;
&#22522;&#20110;&#20915;&#31574;&#30340;&#31232;&#30095;&#25915;&#20987;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#26377;&#25928;&#26597;&#35810;
&lt;/p&gt;
&lt;p&gt;
Query Efficient Decision Based Sparse Attacks Against Black-Box Deep Learning Models. (arXiv:2202.00091v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2202.00091
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#25552;&#20986;&#20102;&#22522;&#20110;&#20915;&#31574;&#30340;&#31232;&#30095;&#25915;&#20987;&#26041;&#27861;SparseEvo&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25200;&#21160;&#30340;&#20687;&#32032;&#25968;&#37327;&#65292;&#33021;&#22815;&#26377;&#25928;&#22320;&#27450;&#39575;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#20102;&#35299;&#23427;&#30340;&#20869;&#37096;&#26500;&#36896;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23613;&#31649;&#25105;&#20204;&#24050;&#32463;&#20570;&#20102;&#26368;&#22823;&#30340;&#21162;&#21147;&#65292;&#20294;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#20173;&#28982;&#23481;&#26131;&#21463;&#21040;&#24494;&#23567;&#30340;&#23545;&#25239;&#24615;&#25200;&#21160;&#12290;&#20174;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#36755;&#20986;&#20013;&#25552;&#21462;&#20449;&#24687;&#20197;&#21046;&#20316;&#40657;&#30418;&#27169;&#22411;&#30340;&#23545;&#25239;&#25200;&#21160;&#26159;&#29616;&#23454;&#19990;&#30028;&#31995;&#32479;&#65288;&#20363;&#22914;&#33258;&#21160;&#39550;&#39542;&#27773;&#36710;&#25110;&#20316;&#20026;&#26381;&#21153;&#65288;MLaaS&#65289;&#20844;&#24320;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65289;&#38754;&#20020;&#30340;&#23041;&#32961;&#12290; &#31232;&#30095;&#25915;&#20987;&#23588;&#20854;&#21463;&#21040;&#20851;&#27880;&#12290; &#31232;&#30095;&#25915;&#20987;&#22312;&#40657;&#30418;&#27169;&#22411;&#20013;&#30340;&#23454;&#29616;&#34920;&#26126;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#27604;&#25105;&#20204;&#24819;&#35937;&#30340;&#26356;&#23481;&#26131;&#21463;&#21040;&#25915;&#20987;&#12290;&#36825;&#39033;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20915;&#31574;&#30340;&#31232;&#30095;&#25915;&#20987;&#26041;&#27861;&#65292;&#36890;&#36807;&#26368;&#23567;&#21270;&#25200;&#21160;&#30340;&#20687;&#32032;&#25968;&#37327;&#65292;&#26377;&#25928;&#22320;&#27450;&#39575;&#40657;&#30418;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#65292;&#32780;&#19981;&#38656;&#35201;&#20102;&#35299;&#23427;&#30340;&#20869;&#37096;&#26500;&#36896;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#22522;&#20110;&#36827;&#21270;&#31639;&#27861;&#30340;SparseEvo&#65292;&#29992;&#20110;&#35299;&#20915;NP&#38590;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Despite our best efforts, deep learning models remain highly vulnerable to even tiny adversarial perturbations applied to the inputs. The ability to extract information from solely the output of a machine learning model to craft adversarial perturbations to black-box models is a practical threat against real-world systems, such as autonomous cars or machine learning models exposed as a service (MLaaS). Of particular interest are sparse attacks. The realization of sparse attacks in black-box models demonstrates that machine learning models are more vulnerable than we believe. Because these attacks aim to minimize the number of perturbed pixels measured by l_0 norm-required to mislead a model by solely observing the decision (the predicted label) returned to a model query; the so-called decision-based attack setting. But, such an attack leads to an NP-hard optimization problem. We develop an evolution-based algorithm-SparseEvo-for the problem and evaluate against both convolutional deep 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#35821;&#20041;&#22270;&#20687;&#32472;&#21046;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#36924;&#30495;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21644;&#25991;&#26412;-&#22270;&#20687;&#35821;&#20041;&#30456;&#20284;&#24615;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20351;&#24471;&#33021;&#22815;&#22312;&#24320;&#25918;&#24335;&#30340;&#25991;&#26412;&#25551;&#36848;&#19979;&#36827;&#34892;&#35821;&#20041;&#30011;&#20316;&#12290;</title><link>http://arxiv.org/abs/2103.10951</link><description>&lt;p&gt;
&#36890;&#36807;&#25991;&#23383;&#21019;&#20316;&#30011;&#20316;
&lt;/p&gt;
&lt;p&gt;
Paint by Word. (arXiv:2103.10951v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2103.10951
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#38646;&#26679;&#26412;&#35821;&#20041;&#22270;&#20687;&#32472;&#21046;&#38382;&#39064;&#65292;&#24182;&#20351;&#29992;&#26368;&#20808;&#36827;&#30340;&#36924;&#30495;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21644;&#25991;&#26412;-&#22270;&#20687;&#35821;&#20041;&#30456;&#20284;&#24615;&#32593;&#32476;&#30456;&#32467;&#21512;&#65292;&#20351;&#24471;&#33021;&#22815;&#22312;&#24320;&#25918;&#24335;&#30340;&#25991;&#26412;&#25551;&#36848;&#19979;&#36827;&#34892;&#35821;&#20041;&#30011;&#20316;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#38646;&#26679;&#26412;&#35821;&#20041;&#22270;&#20687;&#32472;&#21046;&#38382;&#39064;&#65292;&#30446;&#26631;&#26159;&#22522;&#20110;&#24320;&#25918;&#24335;&#25991;&#26412;&#25551;&#36848;&#21019;&#24314;&#35821;&#20041;&#30011;&#20316;&#12290;&#25991;&#31456;&#36890;&#36807;&#23558;&#26368;&#20808;&#36827;&#30340;&#36924;&#30495;&#22270;&#20687;&#29983;&#25104;&#27169;&#22411;&#21644;&#25991;&#26412;-&#22270;&#20687;&#35821;&#20041;&#30456;&#20284;&#24615;&#32593;&#32476;&#30456;&#32467;&#21512;&#26469;&#23436;&#25104;&#36825;&#19968;&#30446;&#26631;&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;&#20026;&#20102;&#36827;&#34892;&#22823;&#30340;&#21464;&#21270;&#65292;&#20351;&#29992;&#38750;&#26799;&#24230;&#26041;&#27861;&#26469;&#25506;&#32034;&#28508;&#31354;&#38388;&#26159;&#37325;&#35201;&#30340;&#65292;&#32780;&#19988;&#37325;&#35201;&#30340;&#26159;&#25918;&#23485;GAN&#30340;&#35745;&#31639;&#20197;&#38024;&#23545;&#29305;&#23450;&#21306;&#22495;&#36827;&#34892;&#21464;&#21270;&#12290;&#25991;&#31456;&#36827;&#34892;&#20102;&#29992;&#25143;&#35843;&#30740;&#65292;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#19982;&#20960;&#20010;&#22522;&#32447;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
We investigate the problem of zero-shot semantic image painting. Instead of painting modifications into an image using only concrete colors or a finite set of semantic concepts, we ask how to create semantic paint based on open full-text descriptions: our goal is to be able to point to a location in a synthesized image and apply an arbitrary new concept such as "rustic" or "opulent" or "happy dog." To do this, our method combines a state-of-the art generative model of realistic images with a state-of-the-art text-image semantic similarity network. We find that, to make large changes, it is important to use non-gradient methods to explore latent space, and it is important to relax the computations of the GAN to target changes to a specific region. We conduct user studies to compare our methods to several baselines.
&lt;/p&gt;</description></item></channel></rss>