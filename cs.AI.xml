<rss version="2.0"><channel><title>Chat Arxiv cs.AI</title><link>https://github.com/qhduan/cn-chat-arxiv</link><description>This is arxiv RSS feed for cs.AI</description><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#29983;&#25104;AI&#24037;&#20316;&#27969;&#31243;&#65292;&#21487;&#20197;&#20811;&#26381;&#24403;&#21069;&#29983;&#25104;AI&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#21435;&#35774;&#35745;&#20986;&#20855;&#26377;&#39640;&#39044;&#27979;&#20146;&#21644;&#21147;&#30340;&#21487;&#34892;&#21270;&#23398;&#20998;&#23376;&#12290;</title><link>http://arxiv.org/abs/2305.06334</link><description>&lt;p&gt;
&#23558;&#29983;&#25104;AI&#19982;&#20027;&#21160;&#23398;&#20064;&#26694;&#26550;&#30456;&#32467;&#21512;&#26469;&#20248;&#21270;&#33647;&#29289;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
Optimizing Drug Design by Merging Generative AI With Active Learning Frameworks. (arXiv:2305.06334v1 [q-bio.BM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06334
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20027;&#21160;&#23398;&#20064;&#30340;&#29983;&#25104;AI&#24037;&#20316;&#27969;&#31243;&#65292;&#21487;&#20197;&#20811;&#26381;&#24403;&#21069;&#29983;&#25104;AI&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#21435;&#35774;&#35745;&#20986;&#20855;&#26377;&#39640;&#39044;&#27979;&#20146;&#21644;&#21147;&#30340;&#21487;&#34892;&#21270;&#23398;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#33647;&#29289;&#30740;&#21457;&#39033;&#30446;&#27491;&#22312;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#36716;&#22411;&#12290;&#20854;&#20013;&#65292;&#29983;&#25104;AI&#26041;&#27861;&#22240;&#20854;&#35774;&#35745;&#26032;&#20998;&#23376;&#21644;&#22686;&#24378;&#29616;&#26377;&#20998;&#23376;&#29305;&#24615;&#30340;&#33021;&#21147;&#32780;&#21463;&#21040;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#24403;&#21069;&#30340;&#29983;&#25104;AI&#26041;&#27861;&#23384;&#22312;&#23616;&#38480;&#24615;&#65292;&#20363;&#22914;&#23545;&#30446;&#26631;&#30340;&#20146;&#21644;&#21147;&#20302;&#65292;ADME/PK&#29305;&#24615;&#26410;&#30693;&#25110;&#32570;&#20047;&#21512;&#25104;&#21487;&#36861;&#28335;&#24615;&#31561;&#12290;&#20026;&#20102;&#25552;&#39640;&#29983;&#25104;AI&#26041;&#27861;&#30340;&#36866;&#29992;&#33539;&#22260;&#65292;&#25105;&#20204;&#24320;&#21457;&#20102;&#22522;&#20110;&#21464;&#20998;&#33258;&#32534;&#30721;&#22120;&#21644;&#20027;&#21160;&#23398;&#20064;&#27493;&#39588;&#30340;&#24037;&#20316;&#27969;&#31243;&#12290;&#35774;&#35745;&#30340;&#29983;&#25104;AI&#24037;&#20316;&#27969;&#31243;&#20174;&#20998;&#23376;&#25351;&#26631;&#65292;&#21253;&#25324;&#33647;&#29289;&#30456;&#20284;&#24615;&#12289;&#21487;&#21512;&#25104;&#24615;&#12289;&#30456;&#20284;&#24615;&#21644;&#23545;&#25509;&#24471;&#20998;&#20013;&#36845;&#20195;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#22312;&#26368;&#21518;&#30340;&#36873;&#25321;&#27493;&#39588;&#20013;&#21253;&#25324;&#20102;&#19968;&#32452;&#22522;&#20110;&#20808;&#36827;&#30340;&#20998;&#23376;&#24314;&#27169;&#27169;&#25311;&#30340;&#23618;&#27425;&#21270;&#26631;&#20934;&#12290;&#25105;&#20204;&#22312;&#20004;&#20010;&#27169;&#22411;&#31995;&#32479;CDK2&#21644;KRAS&#19978;&#27979;&#35797;&#20102;&#25105;&#20204;&#30340;&#29983;&#25104;AI&#24037;&#20316;&#27969;&#31243;&#12290;&#22312;&#20004;&#31181;&#24773;&#20917;&#19979;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#29983;&#25104;&#20102;&#20855;&#26377;&#39640;&#39044;&#27979;&#20146;&#21644;&#21147;&#30340;&#21487;&#34892;&#21270;&#23398;&#20998;&#23376;&#12290;
&lt;/p&gt;
&lt;p&gt;
Traditional drug discovery programs are being transformed by the advent of machine learning methods. Among these, Generative AI methods (GM) have gained attention due to their ability to design new molecules and enhance specific properties of existing ones. However, current GM methods have limitations, such as low affinity towards the target, unknown ADME/PK properties, or the lack of synthetic tractability. To improve the applicability domain of GM methods, we have developed a workflow based on a variational autoencoder coupled with active learning steps. The designed GM workflow iteratively learns from molecular metrics, including drug likeliness, synthesizability, similarity, and docking scores. In addition, we also included a hierarchical set of criteria based on advanced molecular modeling simulations during a final selection step. We tested our GM workflow on two model systems, CDK2 and KRAS. In both cases, our model generated chemically viable molecules with a high predicted aff
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;&#65288;IMP&#65289;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#38598;&#25104;&#21040;&#21333;&#20010;&#32534;&#30721;&#22120;&#20013;&#65292;&#37319;&#29992;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;AGD&#65289;&#21644;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#25193;&#23637;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06324</link><description>&lt;p&gt;
AGD&#21644;MoE&#29992;&#20110;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;
&lt;/p&gt;
&lt;p&gt;
Alternating Gradient Descent and Mixture-of-Experts for Integrated Multimodal Perception. (arXiv:2305.06324v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06324
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;&#65288;IMP&#65289;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#36755;&#20837;&#38598;&#25104;&#21040;&#21333;&#20010;&#32534;&#30721;&#22120;&#20013;&#65292;&#37319;&#29992;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;AGD&#65289;&#21644;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#30456;&#32467;&#21512;&#30340;&#26041;&#27861;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#25193;&#23637;&#65292;&#21462;&#24471;&#20102;&#22312;&#22810;&#20010;&#22522;&#20934;&#27979;&#35797;&#20013;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31616;&#21333;&#21487;&#25193;&#23637;&#30340;&#22810;&#27169;&#24577;&#22810;&#20219;&#21153;&#35757;&#32451;&#21644;&#24314;&#27169;&#26041;&#27861;&#8212;&#8212;&#38598;&#25104;&#22810;&#27169;&#24577;&#24863;&#30693;&#65288;IMP&#65289;&#12290;IMP&#23558;&#22270;&#20687;&#12289;&#35270;&#39057;&#12289;&#25991;&#26412;&#21644;&#38899;&#39057;&#31561;&#22810;&#27169;&#24577;&#36755;&#20837;&#38598;&#25104;&#21040;&#21333;&#20010;Transformer&#32534;&#30721;&#22120;&#20013;&#65292;&#24182;&#20855;&#26377;&#26368;&#23567;&#30340;&#27169;&#24577;&#29305;&#23450;&#32452;&#20214;&#12290;IMP&#20351;&#29992;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#35774;&#35745;&#65292;&#23558;&#20132;&#26367;&#26799;&#24230;&#19979;&#38477;&#27861;&#65288;AGD&#65289;&#21644;&#28151;&#21512;&#19987;&#23478;&#65288;MoE&#65289;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#27169;&#22411;&#21644;&#20219;&#21153;&#25193;&#23637;&#12290;&#36890;&#36807;&#24191;&#27867;&#30340;&#23454;&#35777;&#30740;&#31350;&#65292;&#25105;&#20204;&#25581;&#31034;&#20102;&#20197;&#19979;&#20851;&#38190;&#35265;&#35299;&#65306;1&#65289;&#22312;&#22810;&#26679;&#21270;&#30340;&#24322;&#26500;&#27169;&#24577;&#12289;&#25439;&#22833;&#20989;&#25968;&#21644;&#20219;&#21153;&#19978;&#20132;&#26367;&#25191;&#34892;&#26799;&#24230;&#19979;&#38477;&#26356;&#26032;&#65292;&#24182;&#21516;&#26102;&#25913;&#21464;&#36755;&#20837;&#20998;&#36776;&#29575;&#65292;&#21487;&#20197;&#26377;&#25928;&#25552;&#39640;&#22810;&#27169;&#24577;&#29702;&#35299;&#12290;2&#65289;&#22312;&#21333;&#19968;&#30340;&#27169;&#24577;&#19981;&#21487;&#30693;&#32534;&#30721;&#22120;&#19978;&#20351;&#29992;MoE&#36827;&#34892;&#27169;&#22411;&#31232;&#30095;&#21270;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#24615;&#33021;&#65292;&#32988;&#36807;&#20351;&#29992;&#27169;&#24577;&#29305;&#23450;&#32534;&#30721;&#22120;&#25110;&#39069;&#22806;&#34701;&#21512;&#23618;&#30340;&#31264;&#23494;&#27169;&#22411;&#65292;&#24182;&#22823;&#22823;&#32531;&#35299;&#27169;&#24577;&#20043;&#38388;&#30340;&#20914;&#31361;&#12290;IMP&#22312;&#19977;&#20010;&#22810;&#27169;&#24577;&#22522;&#20934;&#27979;&#35797;&#20013;&#21462;&#24471;&#20102;&#20855;&#26377;&#31454;&#20105;&#21147;&#30340;&#24615;&#33021;&#65292;&#32988;&#36807;&#20102;&#22823;&#37096;&#20998;&#24050;&#21457;&#34920;&#30340;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present Integrated Multimodal Perception (IMP), a simple and scalable multimodal multi-task training and modeling approach. IMP integrates multimodal inputs including image, video, text, and audio into a single Transformer encoder with minimal modality-specific components. IMP makes use of a novel design that combines Alternating Gradient Descent (AGD) and Mixture-of-Experts (MoE) for efficient model \&amp; task scaling. We conduct extensive empirical studies about IMP and reveal the following key insights: 1) performing gradient descent updates by alternating on diverse heterogeneous modalities, loss functions, and tasks, while also varying input resolutions, efficiently improves multimodal understanding. 2) model sparsification with MoE on a single modality-agnostic encoder substantially improves the performance, outperforming dense models that use modality-specific encoders or additional fusion layers and greatly mitigating the conflicts between modalities. IMP achieves competitive p
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;Scan2LoD3&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#22806;&#22681;&#23618;&#27425;&#30340;&#35821;&#20041;&#19977;&#32500;&#20998;&#21106;&#26469;&#31934;&#30830;&#37325;&#24314;&#20855;&#26377;&#35821;&#20041;&#20449;&#24687;&#30340;LoD3&#24314;&#31569;&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06314</link><description>&lt;p&gt;
Scan2LoD3: &#20351;&#29992;&#23556;&#32447;&#25237;&#23556;&#21644;&#36125;&#21494;&#26031;&#32593;&#32476;&#37325;&#24314;&#20855;&#26377;&#35821;&#20041;&#20449;&#24687;&#30340;LoD3&#19977;&#32500;&#24314;&#31569;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Scan2LoD3: Reconstructing semantic 3D building models at LoD3 using ray casting and Bayesian networks. (arXiv:2305.06314v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06314
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#31216;&#20026;Scan2LoD3&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#22806;&#22681;&#23618;&#27425;&#30340;&#35821;&#20041;&#19977;&#32500;&#20998;&#21106;&#26469;&#31934;&#30830;&#37325;&#24314;&#20855;&#26377;&#35821;&#20041;&#20449;&#24687;&#30340;LoD3&#24314;&#31569;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#31934;&#32454;&#31243;&#24230;&#20026;LoD3&#30340;&#32423;&#21035;&#19978;&#37325;&#24314;&#24102;&#26377;&#35821;&#20041;&#20449;&#24687;&#30340;&#19977;&#32500;&#24314;&#31569;&#27169;&#22411;&#19968;&#30452;&#26159;&#19968;&#20010;&#38271;&#26399;&#23384;&#22312;&#30340;&#25361;&#25112;&#12290;&#19982;&#22522;&#20110;&#32593;&#26684;&#30340;&#27169;&#22411;&#19981;&#21516;&#65292;&#36825;&#20123;&#27169;&#22411;&#38656;&#35201;&#20855;&#26377;&#23436;&#20840;&#30340;&#20960;&#20309;&#24418;&#29366;&#21644;&#22806;&#22681;&#23618;&#27425;&#30340;&#29289;&#20307;&#35821;&#20041;&#20449;&#24687;&#12290;&#36825;&#31181;&#35201;&#27714;&#20005;&#26684;&#30340;&#35821;&#20041;&#19977;&#32500;&#37325;&#24314;&#30340;&#20027;&#35201;&#25361;&#25112;&#22312;&#20110;&#21487;&#38752;&#22320;&#22312;&#19977;&#32500;&#36755;&#20837;&#25968;&#25454;&#30340;&#22806;&#22681;&#23618;&#27425;&#36827;&#34892;&#35821;&#20041;&#20998;&#21106;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;Scan2LoD3&#30340;&#26032;&#26041;&#27861;&#65292;&#36890;&#36807;&#25913;&#36827;&#22806;&#22681;&#23618;&#27425;&#30340;&#35821;&#20041;&#19977;&#32500;&#20998;&#21106;&#26469;&#31934;&#30830;&#37325;&#24314;&#20855;&#26377;&#35821;&#20041;&#20449;&#24687;&#30340;LoD3&#24314;&#31569;&#27169;&#22411;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#21033;&#29992;&#28608;&#20809;&#29289;&#29702;&#23398;&#21644;&#19977;&#32500;&#24314;&#31569;&#27169;&#22411;&#30340;&#20808;&#39564;&#30693;&#35782;&#26469;&#27010;&#29575;&#22320;&#35782;&#21035;&#27169;&#22411;&#20914;&#31361;&#12290;&#36825;&#20123;&#27010;&#29575;&#29289;&#29702;&#20914;&#31361;&#25552;&#20986;&#20102;&#27169;&#22411;&#24320;&#21475;&#30340;&#20301;&#32622;&#65306;&#23427;&#20204;&#30340;&#26368;&#32456;&#35821;&#20041;&#21644;&#24418;&#29366;&#26159;&#36890;&#36807;&#34701;&#21512;&#20914;&#31361;&#12289;&#19977;&#32500;&#28857;&#20113;&#21644;&#20108;&#32500;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#27010;&#29575;&#22270;&#30340;&#36125;&#21494;&#26031;&#32593;&#32476;&#25512;&#26029;&#20986;&#26469;&#30340;&#12290;&#20026;&#20102;&#28385;&#36275;&#20005;&#26684;&#30340;LoD3&#35201;&#27714;&#65292;&#25105;&#20204;&#21033;&#29992;&#20272;&#35745;&#30340;&#24418;&#29366;&#22312;&#19977;&#32500;&#24314;&#31569;&#20808;&#39564;&#20013;&#20999;&#21106;&#20986;&#24320;&#21475;&#65292;&#24182;&#20174;&#24211;&#20013;&#36866;&#37197;&#35821;&#20041;&#19977;&#32500;&#23545;&#35937;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reconstructing semantic 3D building models at the level of detail (LoD) 3 is a long-standing challenge. Unlike mesh-based models, they require watertight geometry and object-wise semantics at the fa\c{c}ade level. The principal challenge of such demanding semantic 3D reconstruction is reliable fa\c{c}ade-level semantic segmentation of 3D input data. We present a novel method, called Scan2LoD3, that accurately reconstructs semantic LoD3 building models by improving fa\c{c}ade-level semantic 3D segmentation. To this end, we leverage laser physics and 3D building model priors to probabilistically identify model conflicts. These probabilistic physical conflicts propose locations of model openings: Their final semantics and shapes are inferred in a Bayesian network fusing multimodal probabilistic maps of conflicts, 3D point clouds, and 2D images. To fulfill demanding LoD3 requirements, we use the estimated shapes to cut openings in 3D building priors and fit semantic 3D objects from a libra
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#20351;&#29992;&#25235;&#21462;&#20132;&#20114;&#26469;&#25910;&#38598;&#20998;&#21106;&#30417;&#30563;&#12290;&#21033;&#29992;&#20998;&#21106;&#20986;&#30340;&#25235;&#21462;&#23545;&#35937;&#65292;&#37319;&#29992;&#8220;&#21098;&#20999;&#21644;&#31896;&#36148;&#8221;&#29983;&#25104;&#26041;&#27861;&#65292;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#23436;&#20840;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06305</link><description>&lt;p&gt;
&#36890;&#36807;&#25235;&#21462;&#30340;&#26041;&#24335;&#36827;&#34892;&#33258;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Self-Supervised Instance Segmentation by Grasping. (arXiv:2305.06305v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06305
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#26041;&#27861;&#65292;&#20351;&#29992;&#25235;&#21462;&#20132;&#20114;&#26469;&#25910;&#38598;&#20998;&#21106;&#30417;&#30563;&#12290;&#21033;&#29992;&#20998;&#21106;&#20986;&#30340;&#25235;&#21462;&#23545;&#35937;&#65292;&#37319;&#29992;&#8220;&#21098;&#20999;&#21644;&#31896;&#36148;&#8221;&#29983;&#25104;&#26041;&#27861;&#65292;&#35757;&#32451;&#30340;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#23436;&#20840;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23454;&#20363;&#20998;&#21106;&#22312;&#35768;&#22810;&#26426;&#22120;&#20154;&#24212;&#29992;&#20013;&#37117;&#26159;&#19968;&#39033;&#22522;&#26412;&#30340;&#25216;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#26041;&#27861;&#65292;&#20351;&#29992;&#25235;&#21462;&#20132;&#20114;&#26469;&#25910;&#38598;&#19968;&#20010;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#30340;&#20998;&#21106;&#30417;&#30563;&#12290;&#24403;&#26426;&#22120;&#20154;&#25235;&#21462;&#29289;&#21697;&#26102;&#65292;&#21487;&#20197;&#20174;&#25235;&#21462;&#21069;&#21644;&#25235;&#21462;&#21518;&#30340;&#22270;&#20687;&#20013;&#25512;&#26029;&#20986;&#25235;&#21462;&#29289;&#21697;&#30340;&#25513;&#30721;&#12290;&#21033;&#29992;&#36825;&#20010;&#35266;&#28857;&#65292;&#25105;&#20204;&#23398;&#20064;&#20102;&#19968;&#20010;&#25235;&#21462;&#20998;&#21106;&#27169;&#22411;&#65292;&#29992;&#20110;&#20174;&#25235;&#21462;&#21069;&#21644;&#25235;&#21462;&#21518;&#30340;&#22270;&#20687;&#20013;&#20998;&#21106;&#20986;&#25235;&#21462;&#30340;&#29289;&#20307;&#12290;&#36825;&#26679;&#30340;&#27169;&#22411;&#21487;&#20197;&#20174;&#25968;&#21315;&#20010;&#25235;&#21462;&#20132;&#20114;&#20013;&#20998;&#21106;&#20986;&#25235;&#21462;&#30340;&#23545;&#35937;&#65292;&#32780;&#19981;&#38656;&#35201;&#26114;&#36149;&#30340;&#20154;&#24037;&#27880;&#37322;&#12290;&#21033;&#29992;&#20998;&#21106;&#20986;&#30340;&#25235;&#21462;&#23545;&#35937;&#65292;&#25105;&#20204;&#21487;&#20197;&#8220;&#21098;&#20999;&#8221;&#29289;&#20307;&#20174;&#21407;&#22987;&#22330;&#26223;&#20013;&#65292;&#24182;&#23558;&#23427;&#20204;&#8220;&#31896;&#36148;&#8221;&#21040;&#26032;&#30340;&#22330;&#26223;&#20013;&#26469;&#29983;&#25104;&#23454;&#20363;&#30417;&#30563;&#12290;&#25105;&#20204;&#35777;&#26126;&#65292;&#19982;&#20256;&#32479;&#30340;&#22270;&#20687;&#20943;&#27861;&#26041;&#27861;&#30456;&#27604;&#65292;&#25105;&#20204;&#30340;&#25235;&#21462;&#20998;&#21106;&#27169;&#22411;&#22312;&#20998;&#21106;&#25235;&#21462;&#23545;&#35937;&#26102;&#25552;&#20379;&#20102;5&#20493;&#30340;&#35823;&#24046;&#38477;&#20302;&#12290;&#32467;&#21512;&#25105;&#20204;&#30340;&#8220;&#21098;&#20999;&#21644;&#31896;&#36148;&#8221;&#29983;&#25104;&#26041;&#27861;&#65292;&#35757;&#32451;&#20351;&#29992;&#25105;&#20204;&#26041;&#27861;&#30340;&#23454;&#20363;&#20998;&#21106;&#27169;&#22411;&#21487;&#20197;&#36798;&#21040;&#19982;&#23436;&#20840;&#30417;&#30563;&#23454;&#20363;&#20998;&#21106;&#25968;&#25454;&#38598;&#35757;&#32451;&#30340;&#27169;&#22411;&#30456;&#24403;&#25110;&#26356;&#20248;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Instance segmentation is a fundamental skill for many robotic applications. We propose a self-supervised method that uses grasp interactions to collect segmentation supervision for an instance segmentation model. When a robot grasps an item, the mask of that grasped item can be inferred from the images of the scene before and after the grasp. Leveraging this insight, we learn a grasp segmentation model to segment the grasped object from before and after grasp images. Such a model can segment grasped objects from thousands of grasp interactions without costly human annotation. Using the segmented grasped objects, we can "cut" objects from their original scenes and "paste" them into new scenes to generate instance supervision. We show that our grasp segmentation model provides a 5x error reduction when segmenting grasped objects compared with traditional image subtraction approaches. Combined with our "cut-and-paste" generation method, instance segmentation models trained with our method
&lt;/p&gt;</description></item><item><title>&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31995;&#32479;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#35299;&#37322;&#21487;&#20197;&#25913;&#21464;&#29992;&#25143;&#30340;&#30693;&#35782;&#24182;&#20419;&#36827;&#20182;&#20204;&#22312;&#22797;&#26434;&#30340;&#25216;&#26415;&#29615;&#22659;&#20013;&#37319;&#21462;&#34892;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26144;&#23556;&#35299;&#37322;&#20013;&#21576;&#29616;&#30340;&#20449;&#24687;&#21644;&#29992;&#25143;&#37319;&#21462;&#30340;&#34892;&#21160;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25506;&#35752;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#30340;&#20449;&#24687;&#32570;&#21475;&#12290;</title><link>http://arxiv.org/abs/2305.06297</link><description>&lt;p&gt;
&#20026;&#20160;&#20040;&#19981;&#34892;&#21160;&#36215;&#26469;&#65311;&#25581;&#31034;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#21644;&#29992;&#25143;&#34892;&#21160;&#20043;&#38388;&#30340;&#32852;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;
Why Don't You Do Something About It? Outlining Connections between AI Explanations and User Actions. (arXiv:2305.06297v1 [cs.HC])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06297
&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#65288;XAI&#65289;&#31995;&#32479;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#35299;&#37322;&#21487;&#20197;&#25913;&#21464;&#29992;&#25143;&#30340;&#30693;&#35782;&#24182;&#20419;&#36827;&#20182;&#20204;&#22312;&#22797;&#26434;&#30340;&#25216;&#26415;&#29615;&#22659;&#20013;&#37319;&#21462;&#34892;&#21160;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#26469;&#26144;&#23556;&#35299;&#37322;&#20013;&#21576;&#29616;&#30340;&#20449;&#24687;&#21644;&#29992;&#25143;&#37319;&#21462;&#30340;&#34892;&#21160;&#20043;&#38388;&#30340;&#32852;&#31995;&#65292;&#24182;&#25506;&#35752;&#20102;&#29616;&#26377;&#24037;&#20316;&#20013;&#30340;&#20449;&#24687;&#32570;&#21475;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31995;&#32479;&#30340;&#26680;&#24515;&#20551;&#35774;&#26159;&#35299;&#37322;&#21487;&#20197;&#25913;&#21464;&#29992;&#25143;&#30340;&#30693;&#35782;&#65292;&#20174;&#32780;&#20351;&#20182;&#20204;&#33021;&#22815;&#22312;&#22797;&#26434;&#30340;&#31038;&#20250;&#25216;&#26415;&#29615;&#22659;&#20013;&#34892;&#21160;&#12290;&#23613;&#31649;&#34892;&#21160;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#35299;&#37322;&#36890;&#24120;&#26159;&#22522;&#20110;&#25216;&#26415;&#26041;&#38754;&#32452;&#32455;&#21644;&#35780;&#20272;&#30340;&#12290;&#20808;&#21069;&#30340;&#24037;&#20316;&#22312;&#25552;&#20379;&#30340;&#20449;&#24687;&#21644;&#29992;&#25143;&#34892;&#20026;&#20043;&#38388;&#30340;&#32852;&#31995;&#26041;&#38754;&#24046;&#24322;&#24456;&#22823;&#12290;&#22312;&#35780;&#20272;&#20013;&#23558;&#34892;&#21160;&#32622;&#20110;&#20013;&#24515;&#20301;&#32622;&#30340;&#19968;&#20010;&#37325;&#35201;&#30340;&#31532;&#19968;&#27493;&#26159;&#29702;&#35299;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#31038;&#21306;&#38598;&#20307;&#35748;&#21487;&#30340;&#35299;&#37322;&#21487;&#20197;&#25552;&#20379;&#30340;&#20449;&#24687;&#33539;&#22260;&#20197;&#21450;&#19982;&#20043;&#30456;&#20851;&#32852;&#30340;&#34892;&#21160;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#23558;&#20851;&#20110;&#21487;&#35299;&#37322;&#20154;&#24037;&#26234;&#33021;&#35299;&#37322;&#20013;&#21576;&#29616;&#30340;&#20449;&#24687;&#21644;&#29992;&#25143;&#34892;&#21160;&#30340;&#20808;&#21069;&#24037;&#20316;&#36827;&#34892;&#20102;&#26144;&#23556;&#65292;&#24182;&#35752;&#35770;&#20102;&#25105;&#20204;&#21457;&#29616;&#30340;&#21576;&#29616;&#32473;&#29992;&#25143;&#30340;&#20449;&#24687;&#26041;&#38754;&#30340;&#24046;&#36317;&#12290;
&lt;/p&gt;
&lt;p&gt;
A core assumption of explainable AI systems is that explanations change what users know, thereby enabling them to act within their complex socio-technical environments. Despite the centrality of action, explanations are often organized and evaluated based on technical aspects. Prior work varies widely in the connections it traces between information provided in explanations and resulting user actions. An important first step in centering action in evaluations is understanding what the XAI community collectively recognizes as the range of information that explanations can present and what actions are associated with them. In this paper, we present our framework, which maps prior work on information presented in explanations and user action, and we discuss the gaps we uncovered about the information presented to users.
&lt;/p&gt;</description></item><item><title>&#25991;&#31456;&#21033;&#29992;BEIT&#36866;&#37197;&#22120;&#21644;Mask2Former&#24320;&#21457;&#20102;&#19968;&#31181;&#35821;&#20041;&#20998;&#21106;&#31639;&#27861;&#65292;&#26368;&#32456;&#25104;&#21151;&#25552;&#39640;&#20102;9&#65285;&#21644;33&#65285;&#30340;mIoU&#24471;&#20998;&#26469;&#26816;&#27979;&#21644;&#35782;&#21035;&#22810;&#31181;&#19981;&#21516;&#30340;&#29273;&#31185;&#30142;&#30149;&#21644;&#24322;&#24120;&#12290;</title><link>http://arxiv.org/abs/2305.06236</link><description>&lt;p&gt;
BEIT&#36866;&#37197;&#22120;&#21644;Mask2Former&#22312;&#35821;&#20041;&#20998;&#21106;&#20013;&#35299;&#23494;&#29273;&#31185;&#25918;&#23556;&#32447;&#23398;&#30340;&#20559;&#35265;
&lt;/p&gt;
&lt;p&gt;
Radious: Unveiling the Enigma of Dental Radiology with BEIT Adaptor and Mask2Former in Semantic Segmentation. (arXiv:2305.06236v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06236
&lt;/p&gt;
&lt;p&gt;
&#25991;&#31456;&#21033;&#29992;BEIT&#36866;&#37197;&#22120;&#21644;Mask2Former&#24320;&#21457;&#20102;&#19968;&#31181;&#35821;&#20041;&#20998;&#21106;&#31639;&#27861;&#65292;&#26368;&#32456;&#25104;&#21151;&#25552;&#39640;&#20102;9&#65285;&#21644;33&#65285;&#30340;mIoU&#24471;&#20998;&#26469;&#26816;&#27979;&#21644;&#35782;&#21035;&#22810;&#31181;&#19981;&#21516;&#30340;&#29273;&#31185;&#30142;&#30149;&#21644;&#24322;&#24120;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
X&#20809;&#22270;&#20687;&#26159;&#35786;&#26029;&#21644;&#27835;&#30103;&#29273;&#40831;&#38382;&#39064;&#30340;&#31532;&#19968;&#27493;&#12290;&#22240;&#27492;&#65292;&#26089;&#26399;&#35786;&#26029;&#21487;&#20197;&#38450;&#27490;&#21475;&#33108;&#21644;&#29273;&#40831;&#30142;&#30149;&#30340;&#21457;&#23637;&#21644;&#21152;&#37325;&#12290;&#26412;&#25991;&#22522;&#20110;BEIT&#36866;&#37197;&#22120;&#21644;Mask2Former&#24320;&#21457;&#20102;&#19968;&#31181;&#35821;&#20041;&#20998;&#21106;&#31639;&#27861;&#65292;&#29992;&#20110;&#26816;&#27979;&#21644;&#35782;&#21035;&#20840;&#26223;&#12289;&#26681;&#23574;&#12289;&#32764;&#29366;&#31361;&#12289;&#20462;&#22797;&#12289;&#26681;&#31649;&#27835;&#30103;&#12289;&#20896;&#12289;&#40843;&#40831;&#12289;&#38025;&#12289;&#22797;&#21512;&#26448;&#26009;&#12289;&#26725;&#26753;&#12289;&#29273;&#39635;&#28814;&#12289;&#26681;&#23574;&#22218;&#32959;&#12289;&#29273;&#27133;&#22218;&#32959;&#12289;&#22218;&#32959;&#12289;&#31181;&#26893;&#29289;&#21644;&#39592;&#31227;&#26893;&#26448;&#26009;&#31561;&#22810;&#31181;&#29273;&#31185;&#30142;&#30149;&#21644;&#24322;&#24120;&#12290;&#25105;&#20204;&#23558;&#31639;&#27861;&#32467;&#26524;&#19982;&#20004;&#31181;&#22270;&#20687;&#20998;&#21106;&#30340;&#29616;&#26377;&#31639;&#27861;Deeplabv3&#21644;Segformer&#22312;&#33258;&#24049;&#30340;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;&#25105;&#20204;&#21457;&#29616;Radious&#31639;&#27861;&#30340;mIoU&#24471;&#20998;&#22312;Deeplabv3+&#21644;Segformer&#20013;&#20998;&#21035;&#27604;&#29616;&#26377;&#31639;&#27861;&#25552;&#39640;&#20102;9&#65285;&#21644;33&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
X-ray images are the first steps for diagnosing and further treating dental problems. So, early diagnosis prevents the development and increase of oral and dental diseases. In this paper, we developed a semantic segmentation algorithm based on BEIT adaptor and Mask2Former to detect and identify teeth, roots, and multiple dental diseases and abnormalities such as pulp chamber, restoration, endodontics, crown, decay, pin, composite, bridge, pulpitis, orthodontics, radicular cyst, periapical cyst, cyst, implant, and bone graft material in panoramic, periapical, and bitewing X-ray images. We compared the result of our algorithm to two state-of-the-art algorithms in image segmentation named: Deeplabv3 and Segformer on our own data set. We discovered that Radious outperformed those algorithms by increasing the mIoU scores by 9% and 33% in Deeplabv3+ and Segformer, respectively.
&lt;/p&gt;</description></item><item><title>DaGAN++&#26159;&#19968;&#31181;&#28145;&#24230;&#24863;&#30693;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#38754;&#37096;&#35270;&#39057;&#20013;&#33258;&#23398;&#20064;&#23494;&#38598;&#30340;3D&#38754;&#37096;&#20960;&#20309;&#65292;&#23558;&#23427;&#20204;&#34701;&#20837;&#21040;&#29983;&#25104;&#22120;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#38754;&#21521;&#35821;&#38899;&#22836;&#35270;&#39057;&#29983;&#25104;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.06225</link><description>&lt;p&gt;
DaGAN++&#65306;&#38754;&#21521;&#35821;&#38899;&#22836;&#35270;&#39057;&#29983;&#25104;&#30340;&#28145;&#24230;&#24863;&#30693;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
DaGAN++: Depth-Aware Generative Adversarial Network for Talking Head Video Generation. (arXiv:2305.06225v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06225
&lt;/p&gt;
&lt;p&gt;
DaGAN++&#26159;&#19968;&#31181;&#28145;&#24230;&#24863;&#30693;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#65292;&#33021;&#22815;&#20174;&#38754;&#37096;&#35270;&#39057;&#20013;&#33258;&#23398;&#20064;&#23494;&#38598;&#30340;3D&#38754;&#37096;&#20960;&#20309;&#65292;&#23558;&#23427;&#20204;&#34701;&#20837;&#21040;&#29983;&#25104;&#22120;&#20013;&#65292;&#20174;&#32780;&#23454;&#29616;&#38754;&#21521;&#35821;&#38899;&#22836;&#35270;&#39057;&#29983;&#25104;&#65292;&#24182;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20197;&#24448;&#30340;&#35821;&#38899;&#22836;&#35270;&#39057;&#29983;&#25104;&#25216;&#26415;&#20027;&#35201;&#20381;&#36182;&#20110;2D&#20449;&#24687;&#65292;&#21253;&#25324;&#38754;&#37096;&#22806;&#35980;&#21644;&#21160;&#20316;&#12290;&#28982;&#32780;&#65292;&#20687;&#32032;&#32423;&#30340;&#28145;&#24230;&#31561;&#23494;&#38598;&#30340;3D&#38754;&#37096;&#20960;&#20309;&#25968;&#25454;&#22312;&#26500;&#24314;&#20934;&#30830;&#30340;3D&#38754;&#37096;&#32467;&#26500;&#21644;&#25233;&#21046;&#32972;&#26223;&#22122;&#22768;&#26041;&#38754;&#21457;&#25381;&#20102;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#33258;&#30417;&#30563;&#30340;&#27169;&#22359;&#65292;&#33021;&#22815;&#20174;&#38754;&#37096;&#35270;&#39057;&#20013;&#23398;&#20064;&#23494;&#38598;&#30340;3D&#38754;&#37096;&#20960;&#20309;&#25968;&#25454;&#65288;&#21363;&#28145;&#24230;&#65289;&#65292;&#32780;&#19981;&#38656;&#35201;&#35757;&#32451;&#26102;&#30340;&#25668;&#20687;&#26426;&#21442;&#25968;&#21644;&#20960;&#20309;&#27880;&#37322;&#12290;&#35770;&#25991;&#36824;&#25552;&#20986;&#20102;&#19968;&#20010;&#31574;&#30053;&#65292;&#23398;&#20064;&#20687;&#32032;&#32423;&#30340;&#19981;&#30830;&#23450;&#24615;&#65292;&#20197;&#20415;&#26356;&#21487;&#38752;&#22320;&#24863;&#30693;&#21018;&#24615;&#36816;&#21160;&#20687;&#32032;&#12290;&#27492;&#22806;&#65292;&#36824;&#35774;&#35745;&#20102;&#19968;&#20010;&#26377;&#25928;&#30340;&#38754;&#37096;&#20851;&#38190;&#28857;&#20272;&#35745;&#27169;&#22359;&#65292;&#20026;&#29983;&#25104;&#36816;&#21160;&#22330;&#25552;&#20379;&#20934;&#30830;&#30340;&#20851;&#38190;&#28857;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;3D&#24863;&#30693;&#30340;&#36328;&#27169;&#24577;&#65288;&#21363;&#22806;&#35980;&#21644;&#28145;&#24230;&#65289;&#27880;&#24847;&#26426;&#21046;&#65292;&#24182;&#23558;&#20854;&#34701;&#20837;&#29983;&#25104;&#23545;&#25239;&#32593;&#32476;&#26694;&#26550;&#20013;&#65292;&#23454;&#29616;&#38754;&#21521;&#35821;&#38899;&#22836;&#35270;&#39057;&#29983;&#25104;&#12290;&#25152;&#25552;&#20986;&#30340;DaGAN ++&#27169;&#22411;&#22312;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#65292;&#34920;&#29616;&#20986;&#21331;&#36234;&#30340;&#35270;&#35273;&#36136;&#37327;&#65292;&#31283;&#23450;&#24615;&#21644;&#22810;&#26679;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Predominant techniques on talking head generation largely depend on 2D information, including facial appearances and motions from input face images. Nevertheless, dense 3D facial geometry, such as pixel-wise depth, plays a critical role in constructing accurate 3D facial structures and suppressing complex background noises for generation. However, dense 3D annotations for facial videos is prohibitively costly to obtain. In this work, firstly, we present a novel self-supervised method for learning dense 3D facial geometry (ie, depth) from face videos, without requiring camera parameters and 3D geometry annotations in training. We further propose a strategy to learn pixel-level uncertainties to perceive more reliable rigid-motion pixels for geometry learning. Secondly, we design an effective geometry-guided facial keypoint estimation module, providing accurate keypoints for generating motion fields. Lastly, we develop a 3D-aware cross-modal (ie, appearance and depth) attention mechanism,
&lt;/p&gt;</description></item><item><title>ComputeGPT&#26159;&#19968;&#31181;&#35745;&#31639;&#22411;&#32842;&#22825;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#36816;&#34892;&#20195;&#30721;&#35299;&#20915;&#25968;&#20540;&#38382;&#39064;&#65292;&#32467;&#21512;&#26412;&#22320;&#27983;&#35272;&#22120;&#30340;Python&#35299;&#37322;&#22120;&#21644;&#20248;&#21270;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#25968;&#20540;&#38382;&#39064;&#25928;&#29575;&#24182;&#20026;&#20195;&#30721;&#25552;&#20379;&#21512;&#36866;&#30340;&#21069;&#31471;&#21644;&#23433;&#20840;&#29615;&#22659;&#12290;</title><link>http://arxiv.org/abs/2305.06223</link><description>&lt;p&gt;
ComputeGPT&#65306;&#36866;&#29992;&#20110;&#25968;&#20540;&#38382;&#39064;&#30340;&#35745;&#31639;&#22411;&#32842;&#22825;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
ComputeGPT: A computational chat model for numerical problems. (arXiv:2305.06223v1 [cs.PL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06223
&lt;/p&gt;
&lt;p&gt;
ComputeGPT&#26159;&#19968;&#31181;&#35745;&#31639;&#22411;&#32842;&#22825;&#27169;&#22411;&#65292;&#21487;&#20197;&#36890;&#36807;&#36816;&#34892;&#20195;&#30721;&#35299;&#20915;&#25968;&#20540;&#38382;&#39064;&#65292;&#32467;&#21512;&#26412;&#22320;&#27983;&#35272;&#22120;&#30340;Python&#35299;&#37322;&#22120;&#21644;&#20248;&#21270;&#30340;&#25552;&#31034;&#65292;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#25968;&#20540;&#38382;&#39064;&#25928;&#29575;&#24182;&#20026;&#20195;&#30721;&#25552;&#20379;&#21512;&#36866;&#30340;&#21069;&#31471;&#21644;&#23433;&#20840;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35821;&#35328;&#27169;&#22411;&#22312;&#35299;&#20915;&#25968;&#20540;&#38382;&#39064;&#19978;&#19981;&#22815;&#31934;&#30830;&#65292;&#20854;&#32467;&#26500;&#35201;&#27714;&#30340;&#26159;&#27010;&#29575;&#24615;&#30340;&#19979;&#19968;&#20010;&#21333;&#35789;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ComputeGPT&#65306;&#19968;&#31181;&#36890;&#36807;&#25353;&#38656;&#36816;&#34892;&#20195;&#30721;&#26469;&#35299;&#20915;&#35745;&#31639;&#38382;&#39064;&#30340;&#32842;&#22825;&#27169;&#22411;&#12290;ComputeGPT&#23558;&#27599;&#20010;&#38382;&#39064;&#36716;&#25442;&#20026;&#30456;&#20851;&#30340;&#20195;&#30721;&#65292;&#36816;&#34892;&#20195;&#30721;&#24182;&#23558;&#35745;&#31639;&#32467;&#26524;&#20316;&#20026;&#32842;&#22825;&#30340;&#19968;&#37096;&#20998;&#36820;&#22238;&#12290;&#25105;&#20204;&#23558;&#36825;&#31181;&#26041;&#27861;&#19982;&#22522;&#20110;&#26412;&#22320;&#27983;&#35272;&#22120;&#30340;Python&#35299;&#37322;&#22120;&#21644;&#20248;&#21270;&#30340;&#25552;&#31034;&#30456;&#32467;&#21512;&#65292;&#20197;&#23454;&#29616;&#26368;&#20808;&#36827;&#30340;&#25968;&#20540;&#38382;&#39064;&#25928;&#29575;&#65292;&#24182;&#20026;&#20195;&#30721;&#25552;&#20379;&#21512;&#36866;&#30340;&#21069;&#31471;&#21644;&#23433;&#20840;&#29615;&#22659;&#12290;
&lt;/p&gt;
&lt;p&gt;
Language models are not accurate in numerical problems. Their architecture does not allow for anything less than a probabilistic next word. This paper introduces ComputeGPT: an approach of creating a chat model able to answer computational problems through running on-demand code. ComputeGPT converts each question to relevant code, runs the code, and returns the computed answer as part of the chat. We combine this approach with a local browser-based Python interpretation and fine-tuned prompts in order to achieve state-of-the-art efficiency on numerical problems and provide a suitable front-end and safe environment for the code to be executed in.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21010;&#20998;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;PMPO&#65289;&#26041;&#27861;&#65292;&#23558;&#36719;&#25552;&#31034;&#20174;&#19968;&#20010;&#25193;&#23637;&#21040;&#22810;&#20010;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#20010;&#25552;&#31034;&#21040;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#19981;&#21516;&#28145;&#24230;&#19978;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#35270;&#35273;&#34920;&#31034;&#30340;&#19978;&#19979;&#25991;&#28145;&#24230;&#65292;&#19982;&#20256;&#32479;&#21333;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#19979;&#28216;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;</title><link>http://arxiv.org/abs/2305.06221</link><description>&lt;p&gt;
&#24102;&#28145;&#24230;&#21010;&#20998;&#30340;&#22810;&#25552;&#31034;&#27169;&#24577;&#20132;&#21449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Multi-Prompt with Depth Partitioned Cross-Modal Learning. (arXiv:2305.06221v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06221
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#21010;&#20998;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;PMPO&#65289;&#26041;&#27861;&#65292;&#23558;&#36719;&#25552;&#31034;&#20174;&#19968;&#20010;&#25193;&#23637;&#21040;&#22810;&#20010;&#65292;&#36890;&#36807;&#36830;&#25509;&#22810;&#20010;&#25552;&#31034;&#21040;&#35270;&#35273;&#32534;&#30721;&#22120;&#30340;&#19981;&#21516;&#28145;&#24230;&#19978;&#65292;&#33021;&#22815;&#26356;&#22909;&#22320;&#25429;&#25417;&#35270;&#35273;&#34920;&#31034;&#30340;&#19978;&#19979;&#25991;&#28145;&#24230;&#65292;&#19982;&#20256;&#32479;&#21333;&#25552;&#31034;&#26041;&#27861;&#30456;&#27604;&#65292;&#22312;&#19979;&#28216;&#35270;&#35273;&#35821;&#35328;&#20219;&#21153;&#20013;&#20855;&#26377;&#26356;&#22909;&#30340;&#34920;&#29616;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36719;&#25552;&#31034;&#23398;&#20064;&#26041;&#27861;&#34987;&#25552;&#20986;&#26469;&#24494;&#35843;&#22823;&#22411;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#27169;&#22411;&#20197;&#23436;&#25104;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#12290;&#36825;&#20123;&#26041;&#27861;&#36890;&#24120;&#23558;&#21487;&#23398;&#20064;&#30340;&#25991;&#26412;&#26631;&#35760;&#19982;&#31867;&#21035;&#26631;&#35760;&#32452;&#21512;&#20316;&#20026;&#27169;&#22411;&#30340;&#36755;&#20837;&#65292;&#20854;&#20013;&#27169;&#22411;&#30340;&#21442;&#25968;&#34987;&#20923;&#32467;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#32463;&#24120;&#20351;&#29992;&#21333;&#19968;&#25552;&#31034;&#26469;&#25551;&#36848;&#31867;&#21035;&#19978;&#19979;&#25991;&#65292;&#32780;&#19981;&#33021;&#20805;&#20998;&#25429;&#25417;&#31867;&#21035;&#30340;&#22810;&#26679;&#23646;&#24615;&#12290;&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#21010;&#20998;&#30340;&#22810;&#27169;&#24577;&#25552;&#31034;&#65288;PMPO&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#22810;&#27169;&#24577;&#25552;&#31034;&#25216;&#26415;&#65292;&#23558;&#36719;&#25552;&#31034;&#20174;&#19968;&#20010;&#21487;&#23398;&#20064;&#25552;&#31034;&#25193;&#23637;&#21040;&#22810;&#20010;&#25552;&#31034;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#35270;&#35273;&#32534;&#30721;&#22120;&#28145;&#24230;&#36827;&#34892;&#20998;&#21106;&#65292;&#24182;&#23558;&#21487;&#23398;&#20064;&#25552;&#31034;&#36830;&#25509;&#21040;&#20998;&#31163;&#30340;&#35270;&#35273;&#28145;&#24230;&#19978;&#65292;&#20351;&#19981;&#21516;&#25552;&#31034;&#33021;&#22815;&#25429;&#25417;&#35270;&#35273;&#34920;&#31034;&#30340;&#23618;&#27425;&#19978;&#19979;&#25991;&#28145;&#24230;&#12290;&#27492;&#22806;&#65292;&#20026;&#20102;&#26368;&#22823;&#38480;&#24230;&#22320;&#21033;&#29992;&#22810;&#25552;&#31034;&#23398;&#20064;&#30340;&#20248;&#21183;&#65292;&#25105;&#20204;&#23558;&#26469;&#33258;&#25163;&#21160;&#35774;&#35745;&#30340;&#27169;&#26495;&#21644;&#21487;&#23398;&#20064;&#30340;&#22810;&#25552;&#31034;&#30340;&#20808;&#39564;&#20449;&#24687;&#32467;&#21512;&#22312;&#19968;&#36215;&#65292;&#20174;&#32780;&#25552;&#39640;&#20102;&#27169;&#22411;&#30340;&#27867;&#21270;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, soft prompt learning methods have been proposed to fine-tune large-scale vision-language pre-trained models for various downstream tasks. These methods typically combine learnable textual tokens with class tokens as input for models with frozen parameters. However, they often employ a single prompt to describe class contexts, failing to capture categories' diverse attributes adequately. This study introduces the Partitioned Multi-modal Prompt (PMPO), a multi-modal prompting technique that extends the soft prompt from a single learnable prompt to multiple prompts. Our method divides the visual encoder depths and connects learnable prompts to the separated visual depths, enabling different prompts to capture the hierarchical contextual depths of visual representations. Furthermore, to maximize the advantages of multi-prompt learning, we incorporate prior information from manually designed templates and learnable multi-prompts, thus improving the generalization capabiliti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#34920;&#26126;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;Transformer&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#24335;&#25512;&#33616;&#30340;&#24615;&#33021;&#65292;&#21487;&#31454;&#20105;&#20110;&#20808;&#21069;&#37319;&#29992;&#22797;&#26434;&#22810;&#32452;&#20214;&#26041;&#27861;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#39069;&#22806;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#23454;&#29616;&#20102;&#30693;&#35782;&#22312;&#39046;&#22495;&#38388;&#30340;&#36716;&#31227;&#12290;</title><link>http://arxiv.org/abs/2305.06218</link><description>&lt;p&gt;
&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;&#35757;&#32451;&#25913;&#36827;&#20102;&#23545;&#35805;&#24335;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Multi-Task End-to-End Training Improves Conversational Recommendation. (arXiv:2305.06218v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06218
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#34920;&#26126;&#65292;&#37319;&#29992;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;Transformer&#27169;&#22411;&#21487;&#20197;&#25552;&#39640;&#23545;&#35805;&#24335;&#25512;&#33616;&#30340;&#24615;&#33021;&#65292;&#21487;&#31454;&#20105;&#20110;&#20808;&#21069;&#37319;&#29992;&#22797;&#26434;&#22810;&#32452;&#20214;&#26041;&#27861;&#30340;&#27169;&#22411;&#65292;&#36890;&#36807;&#24494;&#35843;&#25105;&#20204;&#30340;&#27169;&#22411;&#21644;&#39069;&#22806;&#30340;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#65292;&#23454;&#29616;&#20102;&#30693;&#35782;&#22312;&#39046;&#22495;&#38388;&#30340;&#36716;&#31227;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#19968;&#20010;&#22810;&#20219;&#21153;&#31471;&#21040;&#31471;Transformer&#27169;&#22411;&#22312;&#23545;&#35805;&#24335;&#25512;&#33616;&#20219;&#21153;&#19978;&#30340;&#24615;&#33021;&#65292;&#35813;&#20219;&#21153;&#26088;&#22312;&#22522;&#20110;&#29992;&#25143;&#22312;&#23545;&#35805;&#20013;&#26126;&#30830;&#34920;&#31034;&#30340;&#20559;&#22909;&#25552;&#20379;&#25512;&#33616;&#12290;&#34429;&#28982;&#20808;&#21069;&#30340;&#30740;&#31350;&#22312;&#27492;&#39046;&#22495;&#37319;&#29992;&#20102;&#22797;&#26434;&#30340;&#22810;&#32452;&#20214;&#26041;&#27861;&#65292;&#20854;&#20013;&#23545;&#35805;&#31649;&#29702;&#21644;&#23454;&#20307;&#25512;&#33616;&#20219;&#21153;&#30001;&#21333;&#29420;&#30340;&#32452;&#20214;&#22788;&#29702;&#65292;&#20294;&#25105;&#20204;&#34920;&#26126;&#65292;&#22522;&#20110;T5&#25991;&#26412;-&#25991;&#26412;Transformer&#27169;&#22411;&#30340;&#32479;&#19968;Transformer&#27169;&#22411;&#22312;&#25512;&#33616;&#30456;&#20851;&#39033;&#30446;&#21644;&#29983;&#25104;&#23545;&#35805;&#26041;&#38754;&#37117;&#21487;&#20197;&#31454;&#20105;&#12290;&#25105;&#20204;&#22312;ReDIAL&#23545;&#35805;&#24335;&#30005;&#24433;&#25512;&#33616;&#25968;&#25454;&#38598;&#19978;&#24494;&#35843;&#25105;&#20204;&#30340;&#27169;&#22411;&#65292;&#24182;&#22312;&#22810;&#20219;&#21153;&#23398;&#20064;&#35774;&#32622;&#20013;&#21019;&#24314;&#20102;&#34893;&#29983;&#33258;MovieLens&#30340;&#39069;&#22806;&#35757;&#32451;&#20219;&#21153;&#65288;&#20363;&#22914;&#22522;&#20110;&#36755;&#20837;&#30005;&#24433;&#39044;&#27979;&#30005;&#24433;&#23646;&#24615;&#21644;&#30456;&#20851;&#30005;&#24433;&#65289;&#12290;&#20351;&#29992;&#19968;&#31995;&#21015;&#25506;&#38024;&#30740;&#31350;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#39069;&#22806;&#20219;&#21153;&#20013;&#23398;&#20064;&#21040;&#30340;&#30693;&#35782;&#34987;&#36716;&#31227;&#21040;&#20102;&#23545;&#35805;&#24335;&#25512;&#33616;&#39046;&#22495;&#20013;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we analyze the performance of a multitask end-to-end transformer model on the task of conversational recommendations, which aim to provide recommendations based on a user's explicit preferences expressed in dialogue. While previous works in this area adopt complex multi-component approaches where the dialogue management and entity recommendation tasks are handled by separate components, we show that a unified transformer model, based on the T5 text-to-text transformer model, can perform competitively in both recommending relevant items and generating conversation dialogue. We fine-tune our model on the ReDIAL conversational movie recommendation dataset, and create additional training tasks derived from MovieLens (such as the prediction of movie attributes and related movies based on an input movie), in a multitask learning setting. Using a series of probe studies, we demonstrate that the learned knowledge in the additional tasks is transferred to the conversational setti
&lt;/p&gt;</description></item><item><title>&#34917;&#19969;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#24322;&#26500;&#25968;&#25454;&#26469;&#28304;&#21644;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#25361;&#25112;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#20114;&#34917;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20174;&#32780;&#23454;&#29616;&#24320;&#21457;&#26356;&#20840;&#38754;&#21644;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;ML&#27169;&#22411;&#12290;</title><link>http://arxiv.org/abs/2305.06217</link><description>&lt;p&gt;
&#34917;&#19969;&#23398;&#20064;&#65306;&#23454;&#29616;&#36328;&#19981;&#21516;&#29983;&#29289;&#21307;&#23398;&#25968;&#25454;&#28304;&#30340;&#32508;&#21512;&#20998;&#26512;&#30340;&#33539;&#24335;&#12290;
&lt;/p&gt;
&lt;p&gt;
Patchwork Learning: A Paradigm Towards Integrative Analysis across Diverse Biomedical Data Sources. (arXiv:2305.06217v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06217
&lt;/p&gt;
&lt;p&gt;
&#34917;&#19969;&#23398;&#20064;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#25972;&#21512;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#28304;&#30340;&#20449;&#24687;&#65292;&#35299;&#20915;&#20102;&#25968;&#25454;&#38544;&#31169;&#12289;&#24322;&#26500;&#25968;&#25454;&#26469;&#28304;&#21644;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#25361;&#25112;&#12290;&#23427;&#21487;&#20197;&#21516;&#26102;&#21033;&#29992;&#20114;&#34917;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20174;&#32780;&#23454;&#29616;&#24320;&#21457;&#26356;&#20840;&#38754;&#21644;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;ML&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#25552;&#20379;&#20102;&#35768;&#22810;&#22686;&#24378;&#24739;&#32773;&#25252;&#29702;&#12289;&#20154;&#21475;&#20581;&#24247;&#21644;&#21307;&#30103;&#20445;&#20581;&#25552;&#20379;&#32773;&#24037;&#20316;&#27969;&#31243;&#30340;&#26426;&#20250;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#25968;&#25454;&#38544;&#31169;&#12289;&#24322;&#26500;&#25968;&#25454;&#26469;&#28304;&#21644;&#26080;&#27861;&#20805;&#20998;&#21033;&#29992;&#22810;&#20010;&#25968;&#25454;&#27169;&#24577;&#30340;&#25361;&#25112;&#65292;&#29616;&#23454;&#20013;&#30340;&#20020;&#24202;&#21644;&#25104;&#26412;&#25928;&#30410;&#20173;&#28982;&#26377;&#38480;&#12290;&#22312;&#36825;&#31687;&#35266;&#28857;&#35770;&#25991;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#8220;&#34917;&#19969;&#23398;&#20064;&#8221;&#65288;PL&#65289;&#65292;&#36825;&#26159;&#19968;&#31181;&#26032;&#30340;&#33539;&#24335;&#65292;&#36890;&#36807;&#38598;&#25104;&#26469;&#33258;&#19981;&#21516;&#25968;&#25454;&#26469;&#28304;&#65288;&#20363;&#22914;&#65292;&#20020;&#24202;&#20813;&#36153;&#25991;&#26412;&#12289;&#21307;&#23398;&#22270;&#20687;&#12289;&#32452;&#23398;&#65289;&#21644;&#20998;&#24067;&#22312;&#19981;&#21516;&#23433;&#20840;&#31449;&#28857;&#19978;&#30340;&#19981;&#21516;&#25968;&#25454;&#27169;&#24577;&#30340;&#19981;&#21516;&#25968;&#25454;&#38598;&#30340;&#20449;&#24687;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#12290;PL&#20801;&#35768;&#21516;&#26102;&#21033;&#29992;&#20114;&#34917;&#30340;&#25968;&#25454;&#26469;&#28304;&#65292;&#21516;&#26102;&#20445;&#25252;&#25968;&#25454;&#38544;&#31169;&#65292;&#20174;&#32780;&#23454;&#29616;&#24320;&#21457;&#26356;&#20840;&#38754;&#21644;&#20855;&#26377;&#26222;&#36866;&#24615;&#30340;ML&#27169;&#22411;&#12290;&#25105;&#20204;&#20171;&#32461;&#20102;&#34917;&#19969;&#23398;&#20064;&#30340;&#27010;&#24565;&#20197;&#21450;&#20854;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#30340;&#24403;&#21069;&#23454;&#29616;&#65292;&#25506;&#35752;&#20102;&#35299;&#20915;&#21508;&#31181;&#38382;&#39064;&#30340;&#28508;&#22312;&#26426;&#20250;&#21644;&#36866;&#29992;&#25968;&#25454;&#26469;&#28304;&#12290;
&lt;/p&gt;
&lt;p&gt;
Machine learning (ML) in healthcare presents numerous opportunities for enhancing patient care, population health, and healthcare providers' workflows. However, the real-world clinical and cost benefits remain limited due to challenges in data privacy, heterogeneous data sources, and the inability to fully leverage multiple data modalities. In this perspective paper, we introduce "patchwork learning" (PL), a novel paradigm that addresses these limitations by integrating information from disparate datasets composed of different data modalities (e.g., clinical free-text, medical images, omics) and distributed across separate and secure sites. PL allows the simultaneous utilization of complementary data sources while preserving data privacy, enabling the development of more holistic and generalizable ML models. We present the concept of patchwork learning and its current implementations in healthcare, exploring the potential opportunities and applicable data sources for addressing various
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24207;&#22810;&#29289;&#20307;&#23548;&#33322;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#22870;&#21169;&#35268;&#33539;&#65292;&#22870;&#21169;&#21333;&#20010;&#21644;&#22810;&#20010;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#30340;&#36827;&#23637;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#20934;&#30830;&#23450;&#20301;&#65292;&#36866;&#29992;&#20110;&#21160;&#24577;&#21464;&#21270;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.06178</link><description>&lt;p&gt;
&#26080;&#24207;&#22810;&#29289;&#20307;&#23548;&#33322;
&lt;/p&gt;
&lt;p&gt;
Sequence-Agnostic Multi-Object Navigation. (arXiv:2305.06178v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06178
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#24207;&#22810;&#29289;&#20307;&#23548;&#33322;&#31639;&#27861;&#65292;&#35813;&#31639;&#27861;&#21033;&#29992;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#36890;&#36807;&#36866;&#24403;&#30340;&#22870;&#21169;&#35268;&#33539;&#65292;&#22870;&#21169;&#21333;&#20010;&#21644;&#22810;&#20010;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#30340;&#36827;&#23637;&#65292;&#23454;&#29616;&#20102;&#23545;&#22810;&#20010;&#23545;&#35937;&#31867;&#21035;&#30340;&#20934;&#30830;&#23450;&#20301;&#65292;&#36866;&#29992;&#20110;&#21160;&#24577;&#21464;&#21270;&#30340;&#23454;&#38469;&#24212;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#29289;&#20307;&#23548;&#33322; (MultiON) &#20219;&#21153;&#35201;&#27714;&#26426;&#22120;&#20154;&#23450;&#20301;&#22810;&#31181;&#29289;&#20307;&#31867;&#21035;&#30340;&#23454;&#20363;&#12290;&#36825;&#26159;&#23478;&#24237;&#25110;&#24037;&#21378;&#36741;&#21161;&#26426;&#22120;&#20154;&#30340;&#22522;&#26412;&#20219;&#21153;&#12290;&#24050;&#26377;&#30340; MultiON &#26041;&#27861;&#23558;&#27492;&#35270;&#20026;&#23545;&#35937;&#23548;&#33322; (ON) &#30340;&#30452;&#25509;&#25193;&#23637;&#65292;&#21363;&#26412;&#25991;&#25152;&#36848;&#30340; ON &#20219;&#21153;&#38656;&#35201;&#25552;&#21069;&#25351;&#23450;&#25506;&#32034;&#23545;&#35937;&#31867;&#21035;&#30340;&#39034;&#24207;&#12290;&#36825;&#22312;&#21160;&#24577;&#21464;&#21270;&#30340;&#23454;&#38469;&#24212;&#29992;&#20013;&#20855;&#26377;&#24456;&#22823;&#30340;&#23616;&#38480;&#24615;&#12290;&#26412;&#25991;&#25551;&#36848;&#20102;&#19968;&#31181;&#22522;&#20110;&#28436;&#21592;-&#35780;&#35770;&#23478;&#20307;&#31995;&#32467;&#26500;&#21644;&#36866;&#24403;&#30340;&#22870;&#21169;&#35268;&#33539;&#30340;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#26694;&#26550;&#65292;&#29992;&#20110;&#26080;&#24207; MultiON&#12290;&#25105;&#20204;&#30340;&#26694;&#26550;&#21033;&#29992;&#36807;&#21435;&#30340;&#32463;&#39564;&#65292;&#24182;&#35797;&#22270;&#22870;&#21169;&#21333;&#20010;&#21644;&#22810;&#20010;&#30446;&#26631;&#23545;&#35937;&#31867;&#21035;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#22312; AI Habitat 3D &#27169;&#25311;&#29615;&#22659;&#20013;&#20351;&#29992; Gibson &#22522;&#20934;&#25968;&#25454;&#38598;&#20013;&#30340;&#29031;&#29255;&#36924;&#30495;&#22330;&#26223;&#36827;&#34892;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#24615;&#33021;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
The Multi-Object Navigation (MultiON) task requires a robot to localize an instance (each) of multiple object classes. It is a fundamental task for an assistive robot in a home or a factory. Existing methods for MultiON have viewed this as a direct extension of Object Navigation (ON), the task of localising an instance of one object class, and are pre-sequenced, i.e., the sequence in which the object classes are to be explored is provided in advance. This is a strong limitation in practical applications characterized by dynamic changes. This paper describes a deep reinforcement learning framework for sequence-agnostic MultiON based on an actor-critic architecture and a suitable reward specification. Our framework leverages past experiences and seeks to reward progress toward individual as well as multiple target object classes. We use photo-realistic scenes from the Gibson benchmark dataset in the AI Habitat 3D simulation environment to experimentally show that our method performs bett
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06176</link><description>&lt;p&gt;
&#36890;&#36807;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#23545;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning Language Models with Generative Adversarial Feedback. (arXiv:2305.06176v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06176
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#23545;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#21462;&#20195;&#20165;&#21463;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLHF)&#65292;&#20174;&#32780;&#28040;&#38500;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#38480;&#21046;&#24182;&#25552;&#39640;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36890;&#36807;&#20154;&#31867;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;&#24050;&#32463;&#26174;&#33879;&#25552;&#39640;&#20102;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;(LLMs)&#30340;&#24615;&#33021;&#65292;&#20351;&#20854;&#36755;&#20986;&#19982;&#20154;&#31867;&#26399;&#26395;&#30340;&#20215;&#20540;&#35266;&#20445;&#25345;&#19968;&#33268;&#12290;&#28982;&#32780;&#65292;RLHF&#21463;&#21040;&#20154;&#31867;&#35780;&#20272;&#32773;&#30340;&#19987;&#19994;&#30693;&#35782;&#21644;&#29983;&#20135;&#21147;&#38480;&#21046;&#12290;&#22312;&#26412;&#30740;&#31350;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#19968;&#31181;&#26367;&#20195;&#26041;&#27861;: &#20351;&#29992;&#29983;&#25104;&#23545;&#25239;&#21453;&#39304;&#30340;&#24378;&#21270;&#23398;&#20064;(RLGAF)&#20195;&#26367;RLHF&#12290;&#25105;&#20204;&#30340;&#21021;&#27493;&#21457;&#29616;&#34920;&#26126;&#65292;RLGAF&#21487;&#20197;&#24110;&#21161;&#23545;&#40784;LLM&#30340;&#36755;&#20986;&#65292;&#21516;&#26102;&#19981;&#20250;&#21463;&#21040;RLHF&#22266;&#26377;&#30340;&#38480;&#21046;&#65292;&#20026;&#36827;&#19968;&#27493;&#33258;&#21160;&#21270;AI&#23545;&#40784;&#30340;&#30740;&#31350;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#36884;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;
Reinforcement Learning with Human Feedback (RLHF) has been demonstrated to significantly enhance the performance of large language models (LLMs) by aligning their outputs with desired human values. However, RLHF is constrained by the expertise and productivity limitations of human evaluators. In this study, we investigate an alternative approach: Reinforcement Learning with Generative Adversarial Feedback (RLGAF) to RLHF. Our preliminary findings indicate that RLGAF can help align LLMs outputs while not suffering from the inherent restrictions of RLHF, suggesting promising avenues for further research on automating AI alignment.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20998;&#26512;&#20102;&#24037;&#19994;&#12289;&#20513;&#23548;&#32452;&#32455;&#21644;&#27668;&#20505;&#20513;&#23548;&#32452;&#32455;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#22914;&#20309;&#24433;&#21709;&#27668;&#20505;&#21464;&#21270;&#30340;&#21465;&#20107;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30417;&#30563;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;Facebook&#19978;&#27668;&#20505;&#24191;&#21578;&#30340;&#31435;&#22330;&#12290;</title><link>http://arxiv.org/abs/2305.06174</link><description>&lt;p&gt;
&#20351;&#29992;&#36125;&#21494;&#26031;&#27169;&#22411;&#24179;&#22343;&#20998;&#26512;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#27668;&#20505;&#23459;&#20256;&#27963;&#21160;
&lt;/p&gt;
&lt;p&gt;
Analysis of Climate Campaigns on Social Media using Bayesian Model Averaging. (arXiv:2305.06174v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06174
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20998;&#26512;&#20102;&#24037;&#19994;&#12289;&#20513;&#23548;&#32452;&#32455;&#21644;&#27668;&#20505;&#20513;&#23548;&#32452;&#32455;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#22914;&#20309;&#24433;&#21709;&#27668;&#20505;&#21464;&#21270;&#30340;&#21465;&#20107;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30417;&#30563;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#29992;&#20110;&#35782;&#21035;Facebook&#19978;&#27668;&#20505;&#24191;&#21578;&#30340;&#31435;&#22330;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27668;&#20505;&#21464;&#21270;&#26159;&#25105;&#20204;&#26102;&#20195;&#30340;&#26680;&#24515;&#38382;&#39064;&#65292;&#25105;&#20204;&#27491;&#22788;&#20110;&#19968;&#20010;&#20851;&#38190;&#26102;&#21051;&#12290;&#21508;&#31181;&#21033;&#30410;&#38598;&#22242;&#12289;&#31038;&#20250;&#36816;&#21160;&#32452;&#32455;&#21644;&#20010;&#20154;&#22312;&#31038;&#20132;&#23186;&#20307;&#19978;&#24320;&#23637;&#38024;&#23545;&#36825;&#20010;&#38382;&#39064;&#30340;&#38598;&#20307;&#34892;&#21160;&#12290;&#27492;&#22806;&#65292;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#38382;&#39064;&#20513;&#23548;&#27963;&#21160;&#24448;&#24448;&#26159;&#38024;&#23545;&#24403;&#21069;&#31038;&#20250;&#20851;&#27880;&#30340;&#38382;&#39064;&#65292;&#29305;&#21035;&#26159;&#33021;&#28304;&#34892;&#19994;&#38754;&#20020;&#30340;&#38382;&#39064;&#12290;&#26412;&#25991;&#30340;&#30446;&#26631;&#26159;&#20998;&#26512;&#24037;&#19994;&#12289;&#20513;&#23548;&#32452;&#32455;&#21644;&#27668;&#20505;&#20513;&#23548;&#32452;&#32455;&#22914;&#20309;&#21033;&#29992;&#31038;&#20132;&#23186;&#20307;&#24433;&#21709;&#27668;&#20505;&#21464;&#21270;&#30340;&#21465;&#20107;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26368;&#23567;&#21270;&#30417;&#30563;&#27169;&#22411;&#32452;&#21512;&#26041;&#27861;&#65292;&#24182;&#32467;&#21512;&#28040;&#24687;&#20027;&#39064;&#26469;&#35782;&#21035;Facebook&#19978;&#27668;&#20505;&#24191;&#21578;&#30340;&#31435;&#22330;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#21457;&#24067;&#20102;&#25105;&#20204;&#30340;&#31435;&#22330;&#25968;&#25454;&#38598;&#12289;&#27169;&#22411;&#21644;&#19982;&#27668;&#20505;&#23459;&#20256;&#27963;&#21160;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#20379;&#26410;&#26469;&#30340;&#33286;&#24773;&#25366;&#25496;&#21644;&#33258;&#21160;&#26816;&#27979;&#27668;&#20505;&#21464;&#21270;&#31435;&#22330;&#30340;&#30740;&#31350;&#20351;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;
Climate change is the defining issue of our time, and we are at a defining moment. Various interest groups, social movement organizations, and individuals engage in collective action on this issue on social media. In addition, issue advocacy campaigns on social media often arise in response to ongoing societal concerns, especially those faced by energy industries. Our goal in this paper is to analyze how those industries, their advocacy group, and climate advocacy group use social media to influence the narrative on climate change. In this work, we propose a minimally supervised model soup [56] approach combined with messaging themes to identify the stances of climate ads on Facebook. Finally, we release our stance dataset, model, and set of themes related to climate campaigns for future work on opinion mining and the automatic detection of climate change stances.
&lt;/p&gt;</description></item><item><title>QICHWABASE&#20026;&#23569;&#25968;&#27665;&#26063;&#35821;&#35328;&#21644;&#30693;&#35782;&#26500;&#24314;Wikibase&#23454;&#20363;&#65292;&#25903;&#25345;&#20975;&#26970;&#20122;&#31038;&#21306;&#21644;&#35856;&#36827;&#31243;&#65292;&#33021;&#22686;&#24378;&#23569;&#25968;&#27665;&#26063;&#22312;&#32593;&#32476;&#19978;&#30340;&#23384;&#22312;&#24863;&#12290;</title><link>http://arxiv.org/abs/2305.06173</link><description>&lt;p&gt;
QICHWABASE: &#19968;&#20010;&#38754;&#21521;&#20975;&#26970;&#20122;&#31038;&#21306;&#30340;&#20975;&#26970;&#20122;&#35821;&#35328;&#21644;&#30693;&#35782;&#24211;
&lt;/p&gt;
&lt;p&gt;
QICHWABASE: A Quechua Language and Knowledge Base for Quechua Communities. (arXiv:2305.06173v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06173
&lt;/p&gt;
&lt;p&gt;
QICHWABASE&#20026;&#23569;&#25968;&#27665;&#26063;&#35821;&#35328;&#21644;&#30693;&#35782;&#26500;&#24314;Wikibase&#23454;&#20363;&#65292;&#25903;&#25345;&#20975;&#26970;&#20122;&#31038;&#21306;&#21644;&#35856;&#36827;&#31243;&#65292;&#33021;&#22686;&#24378;&#23569;&#25968;&#27665;&#26063;&#22312;&#32593;&#32476;&#19978;&#30340;&#23384;&#22312;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#21313;&#24180;&#26469;&#65292;&#32593;&#32476;&#36234;&#26469;&#36234;&#25104;&#20026;&#35821;&#35328;&#21644;&#30693;&#35782;&#34920;&#31034;&#30340;&#31354;&#38388;&#12290;&#28982;&#32780;&#65292;&#36825;&#21482;&#38024;&#23545;&#24191;&#27867;&#27969;&#34892;&#30340;&#35821;&#35328;&#21644;&#31038;&#21306;&#65292;&#22312;&#23569;&#25968;&#27665;&#26063;&#31038;&#21306;&#21644;&#20854;&#36164;&#28304;&#26041;&#38754;&#21364;&#21463;&#21040;&#36739;&#23569;&#20851;&#27880;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;QICHWABASE&#20197;&#25903;&#25345;&#20975;&#26970;&#20122;&#35821;&#35328;&#21644;&#30693;&#35782;&#21450;&#20854;&#31038;&#21306;&#30340;&#21644;&#35856;&#36827;&#31243;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#37319;&#29992;&#20102;&#19968;&#20123;&#26041;&#27861;&#21644;&#24037;&#20855;&#65292;&#33021;&#22815;&#25104;&#20026;&#20840;&#29699;&#20975;&#26970;&#20122;&#31038;&#21306;&#30340;&#21161;&#25512;&#22120;&#12290;&#25105;&#20204;&#24471;&#20986;&#30340;&#32467;&#35770;&#26159;&#65292;&#22312;&#26500;&#24314;QICHWABASE&#65292;&#21363;&#19968;&#20010;Wikibase&#23454;&#20363;&#26102;&#37319;&#29992;&#30340;&#26041;&#27861;&#21644;&#24037;&#20855;&#33021;&#22815;&#22686;&#24378;&#23569;&#25968;&#27665;&#26063;&#22312;&#32593;&#32476;&#19978;&#30340;&#23384;&#22312;&#24863;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the last decade, the Web has increasingly become a space of language and knowledge representation. However, it is only true for well-spread languages and well-established communities, while minority communities and their resources received less attention. In this paper, we propose QICHWABASE to support the harmonization process of the Quechua language and knowledge, and its community. For doing it, we adopt methods and tools that could become a game changer in favour of Quechua communities around the world. We conclude that the methodology and tools adopted on building QICHWABASE, which is a Wikibase instance, could enhance the presence of minorities on the Web.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#21019;&#24314;&#30340;&#23376;&#22270;&#34920;&#31034;&#35805;&#35821;&#21450;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#26469;&#36827;&#34892;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#65292;&#21487;&#34920;&#31034;&#22823;&#37327;&#30475;&#19981;&#35265;&#30340;&#33410;&#28857;&#65292;&#27604;&#38745;&#24577;&#26041;&#27861;&#26356;&#20026;&#20248;&#36234;&#12290;</title><link>http://arxiv.org/abs/2305.06164</link><description>&lt;p&gt;
&#21160;&#24577;&#19978;&#19979;&#25991;&#22270;&#24418;&#23454;&#29616;&#23545;&#19975;&#29289;&#30693;&#35782;&#22270;&#35889;&#30340;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;
&lt;/p&gt;
&lt;p&gt;
Conversational Semantic Parsing using Dynamic Context Graphs. (arXiv:2305.06164v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06164
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65292;&#20351;&#29992;&#21160;&#24577;&#21019;&#24314;&#30340;&#23376;&#22270;&#34920;&#31034;&#35805;&#35821;&#21450;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#26469;&#36827;&#34892;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;&#65292;&#24182;&#21033;&#29992;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#32534;&#30721;&#65292;&#21487;&#34920;&#31034;&#22823;&#37327;&#30475;&#19981;&#35265;&#30340;&#33410;&#28857;&#65292;&#27604;&#38745;&#24577;&#26041;&#27861;&#26356;&#20026;&#20248;&#36234;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32771;&#34385;&#20102;&#22312;&#25317;&#26377;&#25968;&#30334;&#19975;&#20010;&#23454;&#20307;&#21644;&#25968;&#21315;&#31181;&#20851;&#31995;&#31867;&#22411;&#30340;&#36890;&#29992;&#30693;&#35782;&#22270;&#35889;&#19978;&#36827;&#34892;&#20250;&#35805;&#35821;&#20041;&#35299;&#26512;&#30340;&#20219;&#21153;&#12290;&#25105;&#20204;&#33268;&#21147;&#20110;&#24320;&#21457;&#33021;&#22815;&#20132;&#20114;&#22320;&#23558;&#29992;&#25143;&#35821;&#35328;&#26144;&#23556;&#20026;&#21487;&#25191;&#34892;&#36923;&#36753;&#24418;&#24335;&#65288;&#20363;&#22914;SPARQL&#65289;&#30340;&#27169;&#22411;&#65292;&#21516;&#26102;&#32771;&#34385;&#21040;&#23545;&#35805;&#21382;&#21490;&#30340;&#19978;&#19979;&#25991;&#12290;&#25105;&#20204;&#30340;&#20851;&#38190;&#24819;&#27861;&#26159;&#36890;&#36807;&#19968;&#20010;&#21160;&#24577;&#21019;&#24314;&#30340;&#23376;&#22270;&#26469;&#34920;&#31034;&#26377;&#20851;&#35805;&#35821;&#21450;&#20854;&#19978;&#19979;&#25991;&#30340;&#20449;&#24687;&#65292;&#21363;&#27599;&#20010;&#35805;&#35821;&#30340;&#33410;&#28857;&#25968;&#20250;&#21457;&#29983;&#21464;&#21270;&#12290;&#32780;&#19988;&#65292;&#25105;&#20204;&#21033;&#29992;&#23376;&#22270;&#30340;&#22522;&#26412;&#32467;&#26500;&#65292;&#32780;&#19981;&#26159;&#23558;&#20854;&#35270;&#20026;&#24207;&#21015;&#65292;&#20351;&#29992;&#22270;&#24418;&#31070;&#32463;&#32593;&#32476;&#36827;&#34892;&#32534;&#30721;&#65292;&#20174;&#32780;&#36827;&#19968;&#27493;&#20801;&#35768;&#25105;&#20204;&#34920;&#31034;&#22823;&#37327;&#65288;&#30475;&#19981;&#35265;&#30340;&#65289;&#33410;&#28857;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#21160;&#24577;&#24314;&#27169;&#19978;&#19979;&#25991;&#20248;&#20110;&#38745;&#24577;&#26041;&#27861;&#65292;&#21487;&#22312;&#21508;&#20010;&#26041;&#38754;&#65288;&#21363;&#31616;&#21333;&#21644;&#22797;&#26434;&#38382;&#39064;&#65289;&#25552;&#39640;&#24615;&#33021;&#12290;&#25105;&#20204;&#30340;&#32467;&#26524;&#36827;&#19968;&#27493;&#35777;&#23454;&#65292;&#27169;&#22411;&#21270;&#19978;&#19979;&#25991;&#32467;&#26500;&#27604;&#20165;&#32771;&#34385;&#21333;&#20010;&#35805;&#35821;&#26356;&#22909;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper we consider the task of conversational semantic parsing over general purpose knowledge graphs (KGs) with millions of entities, and thousands of relation-types. We are interested in developing models capable of interactively mapping user utterances into executable logical forms (e.g., SPARQL) in the context of the conversational history. Our key idea is to represent information about an utterance and its context via a subgraph which is created dynamically, i.e., the number of nodes varies per utterance. Moreover, rather than treating the subgraph as a sequence we exploit its underlying structure, and thus encode it using a graph neural network which further allows us to represent a large number of (unseen) nodes. Experimental results show that modeling context dynamically is superior to static approaches, delivering performance improvements across the board (i.e., for simple and complex questions). Our results further confirm that modeling the structure of context is bette
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#25968;&#38169;&#35823;&#20998;&#31867;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#22788;&#29702;&#23398;&#29983;&#22238;&#31572;&#30340;&#33021;&#21147;&#12290;</title><link>http://arxiv.org/abs/2305.06163</link><description>&lt;p&gt;
&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#25968;&#38169;&#35823;&#20998;&#31867;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Algebra Error Classification with Large Language Models. (arXiv:2305.06163v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06163
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22823;&#35821;&#35328;&#27169;&#22411;&#30340;&#20195;&#25968;&#38169;&#35823;&#20998;&#31867;&#26041;&#27861;&#65292;&#30456;&#36739;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#20855;&#26377;&#26356;&#22909;&#30340;&#27867;&#21270;&#33021;&#21147;&#21644;&#22788;&#29702;&#23398;&#29983;&#22238;&#31572;&#30340;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22238;&#31572;&#24320;&#25918;&#24335;&#25968;&#23398;&#38382;&#39064;&#26102;&#33258;&#21160;&#21453;&#39304;&#20855;&#26377;&#26174;&#33879;&#30340;&#28508;&#21147;&#26469;&#25552;&#39640;&#22823;&#35268;&#27169;&#23398;&#20064;&#25104;&#26524;&#12290;&#33258;&#21160;&#21453;&#39304;&#31995;&#32479;&#30340;&#20851;&#38190;&#37096;&#20998;&#26159;&#38169;&#35823;&#20998;&#31867;&#32452;&#20214;&#65292;&#35813;&#32452;&#20214;&#35782;&#21035;&#23398;&#29983;&#30340;&#38169;&#35823;&#65292;&#24182;&#21551;&#29992;&#36866;&#24403;&#30340;&#39044;&#23450;&#20041;&#21453;&#39304;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#30340;&#38169;&#35823;&#20998;&#31867;&#26041;&#27861;&#20351;&#29992;&#22522;&#20110;&#35268;&#21017;&#30340;&#26041;&#27861;&#65292;&#20854;&#27867;&#21270;&#33021;&#21147;&#26377;&#38480;&#12290;&#29616;&#26377;&#30340;&#25968;&#25454;&#39537;&#21160;&#26041;&#27861;&#36991;&#20813;&#20102;&#36825;&#20123;&#38480;&#21046;&#65292;&#20294;&#20855;&#20307;&#35201;&#27714;&#23558;&#23398;&#29983;&#31572;&#22797;&#20013;&#30340;&#25968;&#23398;&#34920;&#36798;&#24335;&#35299;&#26512;&#20026;&#35821;&#27861;&#26641;&#12290;&#36825;&#19968;&#35201;&#27714;&#26412;&#36523;&#23601;&#26159;&#19968;&#20010;&#38480;&#21046;&#65292;&#22240;&#20026;&#23398;&#29983;&#30340;&#31572;&#22797;&#24182;&#19981;&#24635;&#26159;&#31526;&#21512;&#35821;&#27861;&#30340;&#65292;&#26080;&#27861;&#36716;&#25442;&#20026;&#26641;&#24418;&#32467;&#26500;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#19968;&#31181;&#20351;&#29992;&#39044;&#35757;&#32451;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#38169;&#35823;&#20998;&#31867;&#30340;&#28789;&#27963;&#26041;&#27861;&#12290;&#25105;&#20204;&#35777;&#26126;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#22312;&#20195;&#25968;&#38169;&#35823;&#20998;&#31867;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#24182;&#33021;&#22815;&#23545;&#26356;&#22823;&#30340;&#23398;&#29983;&#22238;&#31572;&#36827;&#34892;&#20998;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#21487;&#20197;&#25512;&#24191;&#21040;&#20854;&#20182;&#39046;&#22495;&#21644;&#35821;&#35328;&#65292;&#32780;&#26080;&#38656;&#39069;&#22806;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;
&lt;/p&gt;
&lt;p&gt;
Automated feedback as students answer open-ended math questions has significant potential in improving learning outcomes at large scale. A key part of automated feedback systems is an error classification component, which identifies student errors and enables appropriate, predefined feedback to be deployed. Most existing approaches to error classification use a rule-based method, which has limited capacity to generalize. Existing data-driven methods avoid these limitations but specifically require mathematical expressions in student responses to be parsed into syntax trees. This requirement is itself a limitation, since student responses are not always syntactically valid and cannot be converted into trees. In this work, we introduce a flexible method for error classification using pre-trained large language models. We demonstrate that our method can outperform existing methods in algebra error classification, and is able to classify a larger set of student responses. Additionally, we 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;15.5B&#21442;&#25968;&#21644;8K&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;StarCoder&#65292;&#20854;&#21487;&#20197;&#36827;&#34892;&#24555;&#36895;&#22823;&#25209;&#37327;&#25512;&#29702;&#12290;&#32463;&#35780;&#20272;&#35777;&#26126;&#65292;&#22312;Python&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#33719;&#24471;40\%&#30340;pass@1&#30340;&#24471;&#20998;&#65292;&#19988;&#22312;&#20854;&#20182;&#31243;&#24207;&#20013;&#20063;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06161</link><description>&lt;p&gt;
StarCoder: &#28304;&#20195;&#30721;&#19982;&#20320;&#21516;&#22312;&#65281;
&lt;/p&gt;
&lt;p&gt;
StarCoder: may the source be with you!. (arXiv:2305.06161v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06161
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#20010;&#20855;&#26377;15.5B&#21442;&#25968;&#21644;8K&#19978;&#19979;&#25991;&#38271;&#24230;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#8212;&#8212;StarCoder&#65292;&#20854;&#21487;&#20197;&#36827;&#34892;&#24555;&#36895;&#22823;&#25209;&#37327;&#25512;&#29702;&#12290;&#32463;&#35780;&#20272;&#35777;&#26126;&#65292;&#22312;Python&#19978;&#34920;&#29616;&#20248;&#24322;&#65292;&#33021;&#22815;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#33719;&#24471;40\%&#30340;pass@1&#30340;&#24471;&#20998;&#65292;&#19988;&#22312;&#20854;&#20182;&#31243;&#24207;&#20013;&#20063;&#34920;&#29616;&#20986;&#20196;&#20154;&#28385;&#24847;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
BigCode&#31038;&#21306;&#26159;&#19968;&#20010;&#24320;&#25918;&#30340;&#31185;&#23398;&#21512;&#20316;&#32452;&#32455;&#65292;&#33268;&#21147;&#20110;&#24320;&#21457;&#20195;&#34920;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;Code LLMs&#65289;&#30340;&#36127;&#36131;&#20219;&#21457;&#23637;&#12290;&#35813;&#25991;&#20171;&#32461;&#20102;StarCoder&#21644;StarCoderBase&#65292;&#36825;&#26159;&#20855;&#26377;15.5B&#21442;&#25968;&#27169;&#22411;&#21644;8K&#19978;&#19979;&#25991;&#38271;&#24230;&#12289;&#22635;&#20805;&#33021;&#21147;&#20197;&#21450;&#22810;&#31181;&#26597;&#35810;&#27880;&#24847;&#21147;&#23454;&#29616;&#30340;&#24555;&#36895;&#22823;&#25209;&#37327;&#25512;&#29702;&#30340;&#27169;&#22411;&#12290;&#25105;&#20204;&#23545;StarCoderBase&#30340;1&#19975;&#20159;&#20010;&#26631;&#35760;&#36827;&#34892; fine-tuning&#65292;&#21019;&#24314;&#20102;StarCoder&#12290;&#25105;&#20204;&#36827;&#34892;&#20102;&#36804;&#20170;&#20026;&#27490;&#26368;&#20840;&#38754;&#30340;Code LLMs&#35780;&#20272;&#65292;&#24182;&#34920;&#26126;StarCoderBase&#20248;&#20110;&#25903;&#25345;&#22810;&#31181;&#32534;&#31243;&#35821;&#35328;&#30340;&#27599;&#20010;&#24320;&#25918;Code LLM&#65292;&#24182;&#19982;OpenAI code-cushman-001&#27169;&#22411;&#30456;&#21305;&#37197;&#25110;&#20248;&#20110;&#35813;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;StarCoder&#22312;Python&#19978;&#20063;&#34920;&#29616;&#20986;&#20248;&#24322;&#24615;&#33021;&#65292;&#33021;&#22815;&#36890;&#36807;&#20154;&#24037;&#35780;&#20272;&#33719;&#24471;40\%&#30340;pass@1&#30340;&#24471;&#20998;&#65292;&#24182;&#20173;&#28982;&#20445;&#25345;&#20854;&#22312;&#20854;&#20182;&#31243;&#24207;&#20013;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\% pass@1 on HumanEval, and still retains its performance on other program
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#22312;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#19978;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#21457;&#29616;&#26089;&#26399;&#34701;&#21512;&#27169;&#22411;&#27604;&#26202;&#26399;&#34701;&#21512;&#27169;&#22411;&#26356;&#26377;&#25928;&#65292;&#20854;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#26089;&#26399;&#34701;&#21512;&#27169;&#22411;&#26159;ClipBERT&#12290;</title><link>http://arxiv.org/abs/2305.06159</link><description>&lt;p&gt;
&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#32508;&#36848;&#21450;&#22312;&#8220;&#24694;&#24847;&#34920;&#24773;&#8221;&#25361;&#25112;&#20013;&#30340;&#34920;&#29616;
&lt;/p&gt;
&lt;p&gt;
A Review of Vision-Language Models and their Performance on the Hateful Memes Challenge. (arXiv:2305.06159v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06159
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#21450;&#20854;&#22312;&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#23457;&#26680;&#19978;&#30340;&#24212;&#29992;&#65292;&#30740;&#31350;&#21457;&#29616;&#26089;&#26399;&#34701;&#21512;&#27169;&#22411;&#27604;&#26202;&#26399;&#34701;&#21512;&#27169;&#22411;&#26356;&#26377;&#25928;&#65292;&#20854;&#20013;&#34920;&#29616;&#26368;&#20339;&#30340;&#26089;&#26399;&#34701;&#21512;&#27169;&#22411;&#26159;ClipBERT&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31038;&#20132;&#23186;&#20307;&#20869;&#23481;&#30340;&#23457;&#26680;&#30446;&#21069;&#20173;&#28982;&#26159;&#19968;&#39033;&#39640;&#24230;&#25163;&#21160;&#30340;&#20219;&#21153;&#65292;&#28982;&#32780;&#27599;&#22825;&#21457;&#24067;&#30340;&#20869;&#23481;&#37327;&#22826;&#22810;&#65292;&#38590;&#20197;&#26377;&#25928;&#25191;&#34892;&#12290;&#38543;&#30528;&#35768;&#22810;&#22810;&#27169;&#24577;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#26377;&#28508;&#21147;&#38477;&#20302;&#35813;&#20219;&#21153;&#30340;&#25163;&#21160;&#21171;&#21160;&#37327;&#12290;&#26412;&#25991;&#26088;&#22312;&#25506;&#35752;&#19981;&#21516;&#30340;&#27169;&#22411;&#24182;&#30830;&#23450;&#22312;&#8220;&#24694;&#24847;&#34920;&#24773;&#8221;&#25361;&#25112;&#20013;&#26368;&#26377;&#25928;&#30340;&#27169;&#22411;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#25506;&#35752;&#20102;&#26089;&#26399;&#34701;&#21512;&#21644;&#26202;&#26399;&#34701;&#21512;&#27169;&#22411;&#22312;&#20998;&#31867;&#21253;&#21547;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#22810;&#27169;&#24577;&#34920;&#24773;&#20013;&#30340;&#24046;&#24322;&#12290;&#25105;&#20204;&#39318;&#20808;&#20351;&#29992;BERT&#21644;ResNet-152&#20998;&#21035;&#23454;&#29616;&#20102;&#25991;&#26412;&#21644;&#22270;&#20687;&#30340;&#21333;&#27169;&#24577;&#22522;&#32447;&#27169;&#22411;&#12290;&#28982;&#21518;&#23558;&#36825;&#20123;&#21333;&#27169;&#24577;&#27169;&#22411;&#30340;&#36755;&#20986;&#36830;&#25509;&#22312;&#19968;&#36215;&#21019;&#24314;&#20102;&#19968;&#20010;&#26202;&#26399;&#34701;&#21512;&#27169;&#22411;&#12290;&#22312;&#26089;&#26399;&#34701;&#21512;&#27169;&#22411;&#26041;&#38754;&#65292;&#25105;&#20204;&#23454;&#29616;&#20102;ConcatBERT&#12289;VisualBERT&#12289;ViLT&#12289;CLIP&#21644;BridgeTower&#12290;&#32467;&#26524;&#21457;&#29616;&#65292;&#26202;&#26399;&#34701;&#21512;&#27169;&#22411;&#30340;&#34920;&#29616;&#26126;&#26174;&#19981;&#22914;&#26089;&#26399;&#34701;&#21512;&#27169;&#22411;&#65292;&#32780;&#34920;&#29616;&#26368;&#20339;&#30340;&#26089;&#26399;&#34701;&#21512;&#27169;&#22411;&#26159;ClipBERT&#12290;&#25105;&#20204;&#30340;&#30740;&#31350;&#32467;&#26524;&#34920;&#26126;&#65292;&#35270;&#35273;-&#35821;&#35328;&#27169;&#22411;&#26377;&#28508;&#21147;&#25913;&#21892;&#31038;&#20132;&#23186;&#20307;&#19978;&#30340;&#24694;&#24847;&#20869;&#23481;&#23457;&#26680;&#65292;&#20294;&#36824;&#38656;&#35201;&#26356;&#22810;&#30340;&#30740;&#31350;&#26469;&#36827;&#19968;&#27493;&#25552;&#39640;&#20854;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Moderation of social media content is currently a highly manual task, yet there is too much content posted daily to do so effectively. With the advent of a number of multimodal models, there is the potential to reduce the amount of manual labor for this task. In this work, we aim to explore different models and determine what is most effective for the Hateful Memes Challenge, a challenge by Meta designed to further machine learning research in content moderation. Specifically, we explore the differences between early fusion and late fusion models in classifying multimodal memes containing text and images. We first implement a baseline using unimodal models for text and images separately using BERT and ResNet-152, respectively. The outputs from these unimodal models were then concatenated together to create a late fusion model. In terms of early fusion models, we implement ConcatBERT, VisualBERT, ViLT, CLIP, and BridgeTower. It was found that late fusion performed significantly worse th
&lt;/p&gt;</description></item><item><title>EdgeNet&#26159;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#29983;&#25104;&#32593;&#32476;&#65292;&#21487;&#29992;&#20110;&#22312;&#32447;&#30005;&#21830;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#31454;&#20215;&#35774;&#35745;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;EdgeNet &#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#24191;&#21578;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#65292;&#24182;&#21033;&#29992;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#24191;&#21578;&#31454;&#25293;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.06158</link><description>&lt;p&gt;
EdgeNet&#65306;&#30005;&#23376;&#21830;&#21153;&#22312;&#32447;&#24191;&#21578;&#31454;&#20215;&#35774;&#35745;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#29983;&#25104;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
EdgeNet : Encoder-decoder generative Network for Auction Design in E-commerce Online Advertising. (arXiv:2305.06158v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06158
&lt;/p&gt;
&lt;p&gt;
EdgeNet&#26159;&#19968;&#31181;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#29983;&#25104;&#32593;&#32476;&#65292;&#21487;&#29992;&#20110;&#22312;&#32447;&#30005;&#21830;&#24191;&#21578;&#25293;&#21334;&#20013;&#30340;&#25968;&#25454;&#39537;&#21160;&#31454;&#20215;&#35774;&#35745;&#12290;&#19982;&#20256;&#32479;&#27169;&#22411;&#30456;&#27604;&#65292;EdgeNet &#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#25417;&#24191;&#21578;&#38388;&#30340;&#30456;&#20114;&#24433;&#21709;&#65292;&#24182;&#21033;&#29992;&#20016;&#23500;&#30340;&#19978;&#19979;&#25991;&#20449;&#24687;&#65292;&#25552;&#39640;&#24191;&#21578;&#31454;&#25293;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#29983;&#25104;&#32593;&#32476;EdgeNet&#65292;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#32534;&#30721;&#22120;-&#35299;&#30721;&#22120;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;&#32447;&#30005;&#23376;&#21830;&#21153;&#24191;&#21578;&#20013;&#25968;&#25454;&#39537;&#21160;&#30340;&#25293;&#21334;&#35774;&#35745;&#12290;&#25105;&#20204;&#25171;&#30772;&#20102;&#24191;&#20041;&#27425;&#39640;&#20215;&#65288;GSP&#65289;&#30340;&#31070;&#32463;&#25293;&#21334;&#33539;&#24335;&#65292;&#25552;&#39640;&#20102;&#25968;&#25454;&#21033;&#29992;&#25928;&#29575;&#65292;&#21516;&#26102;&#30830;&#20445;&#20102;&#25293;&#21334;&#26426;&#21046;&#30340;&#32463;&#27982;&#29305;&#24449;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;EdgeNet&#24341;&#20837;&#20102;&#22522;&#20110;transformer&#30340;&#32534;&#30721;&#22120;&#26469;&#26356;&#22909;&#22320;&#25429;&#25417;&#19981;&#21516;&#24191;&#21578;&#38388;&#30340;&#30456;&#20114;&#20316;&#29992;&#12290;&#19982;&#22522;&#20110;GSP&#30340;&#31070;&#32463;&#25293;&#21334;&#27169;&#22411;&#30456;&#27604;&#65292;&#25105;&#20204;&#35774;&#35745;&#20102;&#19968;&#20010;&#33258;&#22238;&#24402;&#35299;&#30721;&#22120;&#65292;&#20197;&#26356;&#22909;&#22320;&#21033;&#29992;&#22312;&#32447;&#24191;&#21578;&#31454;&#25293;&#20013;&#30340;&#20016;&#23500;&#19978;&#19979;&#25991;&#20449;&#24687;&#12290;EdgeNet&#22312;&#27010;&#24565;&#19978;&#31616;&#21333;&#26131;&#25026;&#65292;&#24182;&#26131;&#20110;&#25193;&#23637;&#21040;&#29616;&#26377;&#30340;&#31471;&#21040;&#31471;&#31070;&#32463;&#25293;&#21334;&#26694;&#26550;&#20013;&#12290;&#25105;&#20204;&#22312;&#24191;&#27867;&#30340;&#30005;&#23376;&#21830;&#21153;&#24191;&#21578;&#31454;&#25293;&#20013;&#39564;&#35777;&#20102;EdgeNet&#30340;&#26377;&#25928;&#24615;&#65292;&#23637;&#31034;&#20102;&#20854;&#22312;&#25552;&#39640;&#29992;&#25143;&#20307;&#39564;&#21644;&#24179;&#21488;&#25910;&#20837;&#26041;&#38754;&#30340;&#28508;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a new encoder-decoder generative network dubbed EdgeNet, which introduces a novel encoder-decoder framework for data-driven auction design in online e-commerce advertising. We break the neural auction paradigm of Generalized-Second-Price(GSP), and improve the utilization efficiency of data while ensuring the economic characteristics of the auction mechanism. Specifically, EdgeNet introduces a transformer-based encoder to better capture the mutual influence among different candidate advertisements. In contrast to GSP based neural auction model, we design an autoregressive decoder to better utilize the rich context information in online advertising auctions. EdgeNet is conceptually simple and easy to extend to the existing end-to-end neural auction framework. We validate the efficiency of EdgeNet on a wide range of e-commercial advertising auction, demonstrating its potential in improving user experience and platform revenue.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#36890;&#36807;&#32473;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#28155;&#21152;&#35821;&#35328;&#30693;&#35782;&#21450;&#22810;&#35789;&#35821;&#32763;&#35793;&#23376;&#27169;&#22359;&#65292;&#22312;&#33521;&#35821;&#21040;&#24052;&#25552;&#30450;&#25991;&#26426;&#22120;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.06157</link><description>&lt;p&gt;
&#22810;&#35789;&#35821;&#23545;&#20110;&#33521;&#35821;&#21040;&#24052;&#25552;&#30450;&#25991;&#26426;&#22120;&#32763;&#35793;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
Implications of Multi-Word Expressions on English to Bharti Braille Machine Translation. (arXiv:2305.06157v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06157
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#32473;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#28155;&#21152;&#35821;&#35328;&#30693;&#35782;&#21450;&#22810;&#35789;&#35821;&#32763;&#35793;&#23376;&#27169;&#22359;&#65292;&#22312;&#33521;&#35821;&#21040;&#24052;&#25552;&#30450;&#25991;&#26426;&#22120;&#32763;&#35793;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22914;&#20309;&#25552;&#39640;&#33521;&#35821;&#21040;&#24052;&#25552;&#30450;&#25991;&#26426;&#22120;&#32763;&#35793;&#31995;&#32479;&#65292;&#36890;&#36807;&#22312;&#22522;&#32447;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#20013;&#21152;&#20837;&#35821;&#35328;&#30693;&#35782;&#65292;&#23545;&#20116;&#31181;&#35821;&#35328;&#23545;&#36827;&#34892;&#23454;&#39564;&#65292;&#21363;&#23558;&#33521;&#35821;&#21477;&#23376;&#32763;&#35793;&#25104;&#20116;&#31181;&#21360;&#24230;&#35821;&#35328;&#65292;&#24182;&#38543;&#21518;&#32763;&#35793;&#25104;&#30456;&#24212;&#30340;&#24052;&#25552;&#30450;&#25991;&#12290;&#36890;&#36807;&#28155;&#21152;&#23376;&#27169;&#22359;&#32763;&#35793;&#22810;&#35789;&#35821;&#65292;&#26412;&#30740;&#31350;&#26174;&#31034;&#20102;&#21487;&#34892;&#30340;&#26041;&#27861;&#65292;&#36328;&#35821;&#35328;&#23545; NMT &#36755;&#20986;&#36136;&#37327;&#26377;&#25152;&#25552;&#39640;&#12290;&#26368;&#23567;&#30340;&#25913;&#36827;&#20986;&#29616;&#22312;&#33521;&#35821;-&#23612;&#27850;&#23572;&#35821;&#23545;&#20013;&#65292;&#20026; 22.08%&#65292;&#26368;&#22823;&#30340;&#25913;&#36827;&#20986;&#29616;&#22312;&#33521;&#35821;-&#21360;&#22320;&#35821;&#23545;&#20013;&#65292;&#20026; 23.30%&#12290;
&lt;/p&gt;
&lt;p&gt;
In this paper, we have shown the improvement of English to Bharti Braille machine translation system. We have shown how we can improve a baseline NMT model by adding some linguistic knowledge to it. This was done for five language pairs where English sentences were translated into five Indian languages and then subsequently to corresponding Bharti Braille. This has been demonstrated by adding a sub-module for translating multi-word expressions. The approach shows promising results as across language pairs, we could see improvement in the quality of NMT outputs. The least improvement was observed in English-Nepali language pair with 22.08% and the most improvement was observed in the English-Hindi language pair with 23.30%.
&lt;/p&gt;</description></item><item><title>The Vault&#26159;&#19968;&#20010;&#25552;&#20379;&#20102;10&#31181;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#30340;40&#30334;&#19975;&#34892;&#20195;&#30721;-&#25991;&#26412;&#23545;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#38754;&#21521;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#65292;&#26377;&#26395;&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.06156</link><description>&lt;p&gt;
The Vault&#65306;&#19968;&#20010;&#20840;&#38754;&#30340;&#22810;&#35821;&#35328;&#25968;&#25454;&#38598;&#65292;&#20026;&#20419;&#36827;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#32780;&#35774;&#35745;
&lt;/p&gt;
&lt;p&gt;
The Vault: A Comprehensive Multilingual Dataset for Advancing Code Understanding and Generation. (arXiv:2305.06156v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06156
&lt;/p&gt;
&lt;p&gt;
The Vault&#26159;&#19968;&#20010;&#25552;&#20379;&#20102;10&#31181;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#30340;40&#30334;&#19975;&#34892;&#20195;&#30721;-&#25991;&#26412;&#23545;&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#38754;&#21521;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#65292;&#26377;&#26395;&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#20171;&#32461;&#20102; The Vault&#65292;&#36825;&#26159;&#19968;&#20010;&#24320;&#28304;&#30340;&#22823;&#35268;&#27169;&#20195;&#30721;&#25991;&#26412;&#25968;&#25454;&#38598;&#65292;&#26088;&#22312;&#22686;&#24378;&#38754;&#21521;&#20195;&#30721;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#30340;&#35757;&#32451;&#12290;&#29616;&#26377;&#30340;&#29992;&#20110;&#35757;&#32451;&#22522;&#20110;&#20195;&#30721;&#30340;LLM&#30340;&#24320;&#28304;&#25968;&#25454;&#38598;&#22312;&#22823;&#23567;&#12289;&#36136;&#37327;(&#30001;&#20110;&#22122;&#22768;&#20449;&#21495;)&#21644;&#26684;&#24335;&#65288;&#20165;&#21253;&#21547;&#20195;&#30721;&#20989;&#25968;&#21644;&#25991;&#26412;&#35828;&#26126;&#37197;&#23545;&#65289;&#26041;&#38754;&#32463;&#24120;&#38754;&#20020;&#25361;&#25112;&#12290;The Vault&#36890;&#36807;&#25552;&#20379;10&#31181;&#27969;&#34892;&#32534;&#31243;&#35821;&#35328;&#30340;40&#30334;&#19975;&#34892;&#20195;&#30721;-&#25991;&#26412;&#23545;&#65292;&#24443;&#24213;&#28165;&#38500;10&#31181;&#22810;&#26679;&#30340;&#38382;&#39064;&#65292;&#20197;&#21450;&#21508;&#31181;&#32423;&#21035;&#30340;&#20195;&#30721;-&#25991;&#26412;&#23545;&#65292;&#21253;&#25324;&#31867;&#12289;&#20989;&#25968;&#21644;&#20195;&#30721;&#34892;&#31561;&#32423;&#21035;&#65292;&#26469;&#20811;&#26381;&#36825;&#20123;&#38480;&#21046;&#12290;&#30740;&#31350;&#20154;&#21592;&#21644;&#20174;&#19994;&#20154;&#21592;&#21487;&#20197;&#21033;&#29992;The Vault&#26469;&#35757;&#32451;&#19981;&#21516;&#30340;&#38754;&#21521;&#20195;&#30721;&#30340;LLM&#65292;&#25110;&#32773;&#23558;&#25552;&#20379;&#30340;&#25968;&#25454;&#28165;&#27927;&#26041;&#27861;&#21644;&#33050;&#26412;&#21512;&#24182;&#21040;&#33258;&#24049;&#30340;&#25968;&#25454;&#38598;&#20013;&#26469;&#25913;&#36827;&#25968;&#25454;&#38598;&#12290;&#36890;&#36807;&#23558;The Vault&#20316;&#20026;&#38754;&#21521;&#20195;&#30721;&#30340;LLMs&#30340;&#35757;&#32451;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#39044;&#35745;&#22312;&#20195;&#30721;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#19978;&#21462;&#24471;&#26174;&#33879;&#36827;&#23637;&#65292;&#20419;&#36827;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#21644;&#23454;&#36341;&#30340;&#21457;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present The Vault, an open-source, large-scale code-text dataset designed to enhance the training of code-focused large language models (LLMs). Existing open-source datasets for training code-based LLMs often face challenges in terms of size, quality (due to noisy signals), and format (only containing code function and text explanation pairings). The Vault overcomes these limitations by providing 40 million code-text pairs across 10 popular programming languages, thorough cleaning for 10+ prevalent issues, and various levels of code-text pairings, including class, function, and line levels. Researchers and practitioners can utilize The Vault for training diverse code-focused LLMs or incorporate the provided data cleaning methods and scripts to improve their datasets. By employing The Vault as the training dataset for code-centric LLMs, we anticipate significant advancements in code understanding and generation tasks, fostering progress in both artificial intelligence research and so
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#30446;&#26631;&#20174;&#32780;&#25552;&#39640;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#19981;&#21516;&#27979;&#35797;&#22522;&#20934;&#19979;&#30340;&#34920;&#29616;&#20248;&#20110;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#35757;&#32451;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#26377;&#38480;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#26377;&#29992;&#12290;</title><link>http://arxiv.org/abs/2305.06155</link><description>&lt;p&gt;
&#21033;&#29992;&#21512;&#25104;&#30446;&#26631;&#36827;&#34892;&#26426;&#22120;&#32763;&#35793;
&lt;/p&gt;
&lt;p&gt;
Leveraging Synthetic Targets for Machine Translation. (arXiv:2305.06155v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06155
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#19968;&#31181;&#21033;&#29992;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#21512;&#25104;&#30446;&#26631;&#20174;&#32780;&#25552;&#39640;&#26426;&#22120;&#32763;&#35793;&#24615;&#33021;&#30340;&#26041;&#27861;&#65292;&#24182;&#21457;&#29616;&#20854;&#22312;&#19981;&#21516;&#27979;&#35797;&#22522;&#20934;&#19979;&#30340;&#34920;&#29616;&#20248;&#20110;&#20351;&#29992;&#30495;&#23454;&#25968;&#25454;&#35757;&#32451;&#65292;&#36825;&#19968;&#26041;&#27861;&#22312;&#26377;&#38480;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#23588;&#20854;&#26377;&#29992;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#31181;&#22312;&#26377;&#38480;&#36164;&#28304;&#24773;&#20917;&#19979;&#35757;&#32451;&#26426;&#22120;&#32763;&#35793;&#27169;&#22411;&#30340;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#36890;&#36807;&#21033;&#29992;&#36890;&#36807;&#22823;&#22411;&#39044;&#35757;&#32451;&#27169;&#22411;&#29983;&#25104;&#30340;&#21512;&#25104;&#30446;&#26631;&#25968;&#25454;&#12290;&#25105;&#20204;&#34920;&#26126;&#65292;&#22312;&#21452;&#35821;&#12289;&#22810;&#35821;&#35328;&#21644;&#35821;&#38899;&#32763;&#35793;&#35774;&#32622;&#30340;&#19981;&#21516;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;&#20351;&#29992;&#21512;&#25104;&#30446;&#26631;&#35757;&#32451;&#27169;&#22411;&#30340;&#24615;&#33021;&#20248;&#20110;&#20351;&#29992;&#23454;&#38469;&#30340;&#27491;&#30830;&#25968;&#25454;&#35757;&#32451;&#27169;&#22411;&#12290;&#36825;&#31181;&#24615;&#33021;&#24046;&#36317;&#38543;&#30528;&#21487;&#29992;&#36164;&#28304;&#30340;&#38480;&#21046;&#65288;&#25968;&#25454;&#38598;&#22823;&#23567;&#21644;&#27169;&#22411;&#21442;&#25968;&#25968;&#37327;&#65289;&#30340;&#22686;&#21152;&#32780;&#21464;&#24471;&#26356;&#22823;&#12290;&#25105;&#20204;&#36824;&#23545;&#24615;&#33021;&#25552;&#21319;&#26159;&#21542;&#19982;&#20248;&#21270;&#30340;&#20415;&#21033;&#24615;&#25110;&#39044;&#27979;&#30340;&#26356;&#30830;&#23450;&#24615;&#30456;&#20851;&#36827;&#34892;&#20102;&#21021;&#27493;&#20998;&#26512;&#65292;&#20197;&#21450;&#36825;&#31181;&#33539;&#20363;&#26159;&#21542;&#33021;&#22815;&#25552;&#39640;&#22312;&#19981;&#21516;&#27979;&#35797;&#39046;&#22495;&#30340;&#36229;&#20986;&#20998;&#24067;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this work, we provide a recipe for training machine translation models in a limited resource setting by leveraging synthetic target data generated using a large pre-trained model. We show that consistently across different benchmarks in bilingual, multilingual, and speech translation setups, training models on synthetic targets outperforms training on the actual ground-truth data. This performance gap grows bigger with increasing limits on the amount of available resources in the form of the size of the dataset and the number of parameters in the model. We also provide preliminary analysis into whether this boost in performance is linked to ease of optimization or more deterministic nature of the predictions, and whether this paradigm leads to better out-of-distribution performance across different testing domains.
&lt;/p&gt;</description></item><item><title>Structure-CLIP&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20351;&#29992;&#22330;&#26223;&#22270;&#24378;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.06152</link><description>&lt;p&gt;
Structure-CLIP: &#32467;&#21512;&#32467;&#26500;&#30693;&#35782;&#20248;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;
&lt;/p&gt;
&lt;p&gt;
Structure-CLIP: Enhance Multi-modal Language Representations with Structure Knowledge. (arXiv:2305.06152v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06152
&lt;/p&gt;
&lt;p&gt;
Structure-CLIP&#20351;&#29992;&#25991;&#26412;&#20013;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;&#20351;&#29992;&#22330;&#26223;&#22270;&#24378;&#21270;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#20174;&#32780;&#22312;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#20013;&#23637;&#29616;&#20102;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#35268;&#27169;&#30340;&#35270;&#35273;-&#35821;&#35328;&#39044;&#35757;&#32451;&#22312;&#21508;&#31181;&#19979;&#28216;&#20219;&#21153;&#20013;&#23637;&#29616;&#20986;&#20102;&#24456;&#22909;&#30340;&#24615;&#33021;&#65292;&#24182;&#22312;&#22810;&#27169;&#24577;&#29702;&#35299;&#21644;&#29983;&#25104;&#20219;&#21153;&#20013;&#21462;&#24471;&#20102;&#26174;&#33879;&#30340;&#36827;&#23637;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#22312;&#38656;&#35201;&#23545;&#25991;&#26412;&#36827;&#34892;&#35814;&#32454;&#35821;&#20041;&#29702;&#35299;&#30340;&#22270;&#20687;-&#25991;&#26412;&#21305;&#37197;&#20219;&#21153;&#19978;&#36890;&#24120;&#34920;&#29616;&#36739;&#24046;&#12290;&#23613;&#31649;&#24050;&#32463;&#26377;&#19968;&#20123;&#30740;&#31350;&#22312;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#20294;&#23427;&#20204;&#27809;&#26377;&#20805;&#20998;&#21033;&#29992;&#21477;&#23376;&#20013;&#23384;&#22312;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#26469;&#22686;&#24378;&#22810;&#27169;&#24577;&#35821;&#35328;&#34920;&#31034;&#65292;&#23548;&#33268;&#24615;&#33021;&#36739;&#24046;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31471;&#21040;&#31471;&#30340;&#26694;&#26550;Structure-CLIP&#65292;&#35813;&#26694;&#26550;&#32467;&#21512;&#20102;&#20174;&#25991;&#26412;&#20013;&#25552;&#21462;&#30340;&#38544;&#24335;&#35814;&#32454;&#35821;&#20041;&#65292;&#20197;&#22686;&#24378;&#31934;&#32454;&#30340;&#35821;&#20041;&#34920;&#31034;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;(1)&#25105;&#20204;&#20351;&#29992;&#22330;&#26223;&#22270;&#26469;&#26356;&#21152;&#20851;&#27880;&#25991;&#26412;&#20013;&#30340;&#35814;&#32454;&#35821;&#20041;&#23398;&#20064;&#65292;&#24182;&#20805;&#20998;&#25506;&#32034;&#32454;&#31890;&#24230;&#35821;&#20041;&#20043;&#38388;&#30340;&#32467;&#26500;&#21270;&#30693;&#35782;&#65292;(2)&#25105;&#20204;&#32467;&#21512;&#22330;&#26223;&#22270;&#30340;&#30693;&#35782;&#24378;&#21270;&#26694;&#26550;&#26469;&#20805;&#20998;&#21033;&#29992;&#36825;&#20123;&#20449;&#24687;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large-scale vision-language pre-training has shown promising advances on various downstream tasks and achieved significant performance in multi-modal understanding and generation tasks. However, existing methods often perform poorly on image-text matching tasks that require a detailed semantics understanding of the text. Although there have been some works on this problem, they do not sufficiently exploit the structural knowledge present in sentences to enhance multi-modal language representations, which leads to poor performance. In this paper, we present an end-to-end framework Structure-CLIP, which integrates latent detailed semantics from the text to enhance fine-grained semantic representations. Specifically, (1) we use scene graphs in order to pay more attention to the detailed semantic learning in the text and fully explore structured knowledge between fine-grained semantics, and (2) we utilize the knowledge-enhanced framework with the help of the scene graph to make full use of
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29702;&#35770;&#23618;&#38754;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#24120;&#35268;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.06137</link><description>&lt;p&gt;
&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#30340;&#25910;&#25947;&#24615;&#35777;&#26126;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
A proof of convergence of inverse reinforcement learning for multi-objective optimization. (arXiv:2305.06137v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06137
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#35777;&#26126;&#20102;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#22312;&#29702;&#35770;&#23618;&#38754;&#19978;&#30340;&#25910;&#25947;&#24615;&#65292;&#21253;&#25324;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#21644;&#24120;&#35268;&#36870;&#24378;&#21270;&#23398;&#20064;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#36890;&#36807;&#23558;&#31561;&#25928;&#20110;&#22810;&#30446;&#26631;&#20248;&#21270;&#30340;WIRL&#38382;&#39064;&#30340;&#36870;&#38382;&#39064;&#19982;&#25237;&#24433;&#27425;&#26799;&#24230;&#27861;&#30456;&#32467;&#21512;&#65292;&#35777;&#26126;&#20102;Wasserstein&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;WIRL&#65289;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#35777;&#26126;&#20102;&#36870;&#24378;&#21270;&#23398;&#20064;&#65288;&#26368;&#22823;&#29109;&#36870;&#24378;&#21270;&#23398;&#20064;&#65292;&#23548;&#24341;&#25104;&#26412;&#23398;&#20064;&#65289;&#22312;&#22810;&#30446;&#26631;&#20248;&#21270;&#20013;&#30340;&#25910;&#25947;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We show the convergence of Wasserstein inverse reinforcement learning (WIRL) for multi-objective optimizations with the projective subgradient method by formulating an inverse problem of the optimization problem that is equivalent to WIRL for multi-objective optimizations.  In addition, we prove convergence of inverse reinforcement learning (maximum entropy inverse reinforcement learning, guid cost learning) for multi-objective optimization with the projective subgradient method.
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;VIM&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#33258;&#36866;&#24212;&#31354;&#38388;-&#26102;&#38388;&#35270;&#39057;&#37319;&#26679;&#22120;&#21644;&#26102;&#31354;&#21160;&#20316;&#22686;&#24378;&#22120;&#65292;&#29992;&#20110;&#23567;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#65292;&#20805;&#20998;&#21033;&#29992;&#35270;&#39057;&#20869;&#37096;&#21644;&#35270;&#39057;&#38388;&#20449;&#24687;&#20197;&#25552;&#39640;&#35782;&#21035;&#31934;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.06114</link><description>&lt;p&gt;
&#22522;&#20110;&#35270;&#39057;&#20869;&#37096;&#21644;&#35270;&#39057;&#38388;&#20449;&#24687;&#26368;&#22823;&#21270;&#30340;&#23567;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;
&lt;/p&gt;
&lt;p&gt;
Few-shot Action Recognition via Intra- and Inter-Video Information Maximization. (arXiv:2305.06114v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06114
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;VIM&#30340;&#26694;&#26550;&#65292;&#37319;&#29992;&#33258;&#36866;&#24212;&#31354;&#38388;-&#26102;&#38388;&#35270;&#39057;&#37319;&#26679;&#22120;&#21644;&#26102;&#31354;&#21160;&#20316;&#22686;&#24378;&#22120;&#65292;&#29992;&#20110;&#23567;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#65292;&#20805;&#20998;&#21033;&#29992;&#35270;&#39057;&#20869;&#37096;&#21644;&#35270;&#39057;&#38388;&#20449;&#24687;&#20197;&#25552;&#39640;&#35782;&#21035;&#31934;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#30340;&#23567;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#28041;&#21450;&#20004;&#20010;&#20027;&#35201;&#30340;&#20449;&#24687;&#28304;&#29992;&#20110;&#20998;&#31867;: (1) &#26469;&#33258;&#35270;&#39057;&#29255;&#27573;&#20869;&#37096;&#30340;&#35270;&#39057;&#20869;&#37096;&#20449;&#24687;&#65292;&#30001;&#21333;&#20010;&#35270;&#39057;&#21098;&#36753;&#20013;&#30340;&#24103;&#20869;&#23481;&#30830;&#23450;&#65292;&#20197;&#21450; (2) &#36890;&#36807;&#35270;&#39057;&#20043;&#38388;&#30340;&#20851;&#31995;(&#20363;&#22914;&#65292;&#29305;&#24449;&#30456;&#20284;&#24615;)&#27979;&#37327;&#30340;&#35270;&#39057;&#38388;&#20449;&#24687;&#12290;&#28982;&#32780;&#65292;&#29616;&#26377;&#26041;&#27861;&#26410;&#20805;&#20998;&#21033;&#29992;&#36825;&#20004;&#20010;&#20449;&#24687;&#28304;&#12290;&#20851;&#20110;&#35270;&#39057;&#20869;&#37096;&#20449;&#24687;&#65292;&#24403;&#21069;&#30340;&#36755;&#20837;&#35270;&#39057;&#37319;&#26679;&#25805;&#20316;&#21487;&#33021;&#20250;&#36951;&#28431;&#20851;&#38190;&#30340;&#21160;&#20316;&#20449;&#24687;&#65292;&#38477;&#20302;&#35270;&#39057;&#25968;&#25454;&#30340;&#21033;&#29992;&#25928;&#29575;&#12290;&#23545;&#20110;&#35270;&#39057;&#38388;&#20449;&#24687;&#65292;&#35270;&#39057;&#20043;&#38388;&#30340;&#21160;&#20316;&#19981;&#23545;&#40784;&#20351;&#24471;&#35745;&#31639;&#31934;&#30830;&#20851;&#31995;&#21464;&#24471;&#22256;&#38590;&#12290;&#27492;&#22806;&#65292;&#22914;&#20309;&#20849;&#21516;&#32771;&#34385;&#35270;&#39057;&#20869;&#22806;&#20449;&#24687;&#22312;&#23567;&#26679;&#26412;&#21160;&#20316;&#35782;&#21035;&#20013;&#20173;&#26410;&#34987;&#20805;&#20998;&#25506;&#32034;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#30340;&#26694;&#26550;&#65292;&#31216;&#20026;&#35270;&#39057;&#20449;&#24687;&#26368;&#22823;&#21270; (VIM)&#65292;&#29992;&#20110;&#23567;&#26679;&#26412;&#35270;&#39057;&#21160;&#20316;&#35782;&#21035;&#12290;VIM&#37197;&#22791;&#20102;&#19968;&#20010;&#33258;&#36866;&#24212;&#30340;&#26102;&#31354;&#35270;&#39057;&#37319;&#26679;&#22120;&#21644;&#19968;&#20010;&#26102;&#31354;&#21160;&#20316;&#22686;&#24378;&#22120;&#12290;
&lt;/p&gt;
&lt;p&gt;
Current few-shot action recognition involves two primary sources of information for classification:(1) intra-video information, determined by frame content within a single video clip, and (2) inter-video information, measured by relationships (e.g., feature similarity) among videos. However, existing methods inadequately exploit these two information sources. In terms of intra-video information, current sampling operations for input videos may omit critical action information, reducing the utilization efficiency of video data. For the inter-video information, the action misalignment among videos makes it challenging to calculate precise relationships. Moreover, how to jointly consider both inter- and intra-video information remains under-explored for few-shot action recognition. To this end, we propose a novel framework, Video Information Maximization (VIM), for few-shot video action recognition. VIM is equipped with an adaptive spatial-temporal video sampler and a spatiotemporal actio
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#21453;&#28436;&#22312;&#22797;&#26434;&#32452;&#21512;&#32467;&#26500;&#20013;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#23558;&#20854;&#29992;&#20316;&#32479;&#35745;&#25512;&#26029;&#30340;&#31867;&#22411;&#39537;&#21160;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.06112</link><description>&lt;p&gt;
&#36125;&#21494;&#26031;&#25512;&#26029;&#30340;&#32452;&#21512;&#32467;&#26500;
&lt;/p&gt;
&lt;p&gt;
The Compositional Structure of Bayesian Inference. (arXiv:2305.06112v1 [math.CT])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06112
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#30740;&#31350;&#20102;&#36125;&#21494;&#26031;&#21453;&#28436;&#22312;&#22797;&#26434;&#32452;&#21512;&#32467;&#26500;&#20013;&#30340;&#35745;&#31639;&#26041;&#27861;&#65292;&#24182;&#25506;&#35752;&#20102;&#23558;&#20854;&#29992;&#20316;&#32479;&#35745;&#25512;&#26029;&#30340;&#31867;&#22411;&#39537;&#21160;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36125;&#21494;&#26031;&#35268;&#21017;&#21578;&#35785;&#25105;&#20204;&#22914;&#20309;&#21453;&#36716;&#22240;&#26524;&#36807;&#31243;&#20197;&#26681;&#25454;&#26032;&#35777;&#25454;&#26356;&#26032;&#25105;&#20204;&#30340;&#20449;&#24565;&#12290;&#22914;&#26524;&#35748;&#20026;&#35813;&#36807;&#31243;&#20855;&#26377;&#22797;&#26434;&#30340;&#32452;&#21512;&#32467;&#26500;&#65292;&#25105;&#20204;&#21487;&#20197;&#35266;&#23519;&#21040;&#25972;&#20010;&#36807;&#31243;&#30340;&#21453;&#36716;&#21487;&#20197;&#25353;&#37096;&#20214;&#36807;&#31243;&#35745;&#31639;&#12290;&#25105;&#20204;&#30740;&#31350;&#20102;&#36825;&#31181;&#32452;&#25104;&#35268;&#21017;&#30340;&#32467;&#26500;&#65292;&#27880;&#24847;&#21040;&#23427;&#19982;&#20989;&#25968;&#24335;&#32534;&#31243;&#20013;&#30340;&#20984;&#36879;&#38236;&#27169;&#24335;&#30456;&#20851;&#12290;&#22312;&#36866;&#24403;&#30340;Markov&#26680;&#33539;&#30068;&#30340;&#20844;&#29702;&#21270;&#34920;&#36848;&#20013;&#24037;&#20316;&#65292;&#25105;&#20204;&#30475;&#21040;&#20102;&#22914;&#20309;&#23558;&#36125;&#21494;&#26031;&#21453;&#28436;&#30475;&#20316;&#26159;&#32420;&#32500;&#33539;&#30068;&#20013;&#29366;&#24577;&#20381;&#36182;&#24577;&#23556;&#30340;&#29305;&#23450;&#23454;&#20363;&#12290;&#25105;&#20204;&#35752;&#35770;&#20102;&#20854;&#32452;&#21512;&#24615;&#36136;&#65292;&#20197;&#24213;&#23618;&#31867;&#21035;&#19978;&#30340;&#20989;&#23376;&#34920;&#36848;&#65292;&#24182;&#25506;&#35752;&#20102;&#22914;&#20309;&#23558;&#20854;&#29992;&#20110;&#26356;&#21152;&#31867;&#22411;&#39537;&#21160;&#30340;&#32479;&#35745;&#25512;&#26029;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Bayes' rule tells us how to invert a causal process in order to update our beliefs in light of new evidence. If the process is believed to have a complex compositional structure, we may observe that the inversion of the whole can be computed piecewise in terms of the component processes. We study the structure of this compositional rule, noting that it relates to the lens pattern in functional programming. Working in a suitably general axiomatic presentation of a category of Markov kernels, we see how we can think of Bayesian inversion as a particular instance of a state-dependent morphism in a fibred category. We discuss the compositional nature of this, formulated as a functor on the underlying category and explore how this can used for a more type-driven approach to statistical inference.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLEN&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;FLEN&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#20013;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;</title><link>http://arxiv.org/abs/2305.06104</link><description>&lt;p&gt;
N-&#20803;&#20107;&#23454;&#30340;&#23569;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;
&lt;/p&gt;
&lt;p&gt;
Few-shot Link Prediction on N-ary Facts. (arXiv:2305.06104v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06104
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#21517;&#20026;FLEN&#30340;&#27169;&#22411;&#26469;&#23454;&#29616;&#12290;FLEN&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65292;&#21487;&#20197;&#20174;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#20013;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
N-&#20803;&#20107;&#23454;&#30001;&#20027;&#35201;&#19977;&#20803;&#32452;&#65288;&#22836;&#23454;&#20307;&#12289;&#20851;&#31995;&#12289;&#23614;&#23454;&#20307;&#65289;&#21644;&#20219;&#24847;&#25968;&#37327;&#30340;&#36741;&#21161;&#23646;&#24615;&#20540;&#23545;&#32452;&#25104;&#65292;&#36825;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#30693;&#35782;&#22270;&#35889;&#20013;&#24456;&#24120;&#35265;&#12290;&#23545;&#20110;N-&#20803;&#20107;&#23454;&#30340;&#38142;&#25509;&#39044;&#27979;&#26159;&#39044;&#27979;&#20854;&#20013;&#19968;&#20010;&#20803;&#32032;&#30340;&#32570;&#22833;&#65292;&#22635;&#34917;&#32570;&#22833;&#20803;&#32032;&#26377;&#21161;&#20110;&#20016;&#23500;&#30693;&#35782;&#22270;&#35889;&#24182;&#20419;&#36827;&#35768;&#22810;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#12290;&#20197;&#24448;&#30340;&#30740;&#31350;&#36890;&#24120;&#38656;&#35201;&#22823;&#37327;&#39640;&#36136;&#37327;&#30340;&#25968;&#25454;&#26469;&#29702;&#35299;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#20803;&#32032;&#65292;&#20294;&#36825;&#20123;&#30740;&#31350;&#24573;&#35270;&#20102;&#23569;&#26679;&#26412;&#20851;&#31995;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#30340;&#22330;&#26223;&#20013;&#21364;&#24456;&#24120;&#35265;&#12290;&#22240;&#27492;&#65292;&#26412;&#25991;&#24341;&#20837;&#19968;&#20010;&#26032;&#20219;&#21153;&#8212;&#8212;&#23569;&#26679;&#26412;N-&#20803;&#20107;&#23454;&#38142;&#25509;&#39044;&#27979;&#65292;&#26088;&#22312;&#20351;&#29992;&#26377;&#38480;&#30340;&#26631;&#35760;&#23454;&#20363;&#26469;&#39044;&#27979;N-&#20803;&#20107;&#23454;&#20013;&#30340;&#32570;&#22833;&#23454;&#20307;&#12290;&#25105;&#20204;&#20063;&#25552;&#20986;&#20102;&#19968;&#20010;&#38024;&#23545;N-&#20803;&#20107;&#23454;&#30340;&#23569;&#26679;&#26412;&#38142;&#25509;&#39044;&#27979;&#27169;&#22411;FLEN&#65292;&#23427;&#30001;&#19977;&#20010;&#27169;&#22359;&#32452;&#25104;&#65306;&#20851;&#31995;&#23398;&#20064;&#27169;&#22359;&#12289;&#25903;&#25345;&#29305;&#23450;&#35843;&#25972;&#27169;&#22359;&#21644;&#26597;&#35810;&#25512;&#29702;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;
N-ary facts composed of a primary triple (head entity, relation, tail entity) and an arbitrary number of auxiliary attribute-value pairs, are prevalent in real-world knowledge graphs (KGs). Link prediction on n-ary facts is to predict a missing element in an n-ary fact. This helps populate and enrich KGs and further promotes numerous downstream applications. Previous studies usually require a substantial amount of high-quality data to understand the elements in n-ary facts. However, these studies overlook few-shot relations, which have limited labeled instances, yet are common in real-world scenarios. Thus, this paper introduces a new task, few-shot link prediction on n-ary facts. It aims to predict a missing entity in an n-ary fact with limited labeled instances. We further propose a model for Few-shot Link prEdict on N-ary facts, thus called FLEN, which consists of three modules: the relation learning, support-specific adjusting, and query inference modules. FLEN captures relation me
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#37319;&#29992;&#21442;&#25968;&#21270;&#20998;&#35299;&#21644;&#28388;&#27874;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;GNN&#30340;&#28789;&#27963;&#24615;&#65292;&#32531;&#35299;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#24179;&#28369;&#21644;&#25918;&#22823;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2305.06102</link><description>&lt;p&gt;
&#37319;&#29992;&#21442;&#25968;&#20998;&#35299;&#21644;&#28388;&#27874;&#25216;&#26415;&#23454;&#29616;&#26356;&#22909;&#30340;&#22270;&#34920;&#24449;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
Towards Better Graph Representation Learning with Parameterized Decomposition &amp; Filtering. (arXiv:2305.06102v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06102
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#37319;&#29992;&#21442;&#25968;&#21270;&#20998;&#35299;&#21644;&#28388;&#27874;&#65292;&#32479;&#19968;&#20102;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#65292;&#25552;&#39640;&#20102;GNN&#30340;&#28789;&#27963;&#24615;&#65292;&#32531;&#35299;&#20102;&#29616;&#26377;&#27169;&#22411;&#30340;&#24179;&#28369;&#21644;&#25918;&#22823;&#38382;&#39064;&#65292;&#24182;&#24320;&#21457;&#20102;&#31616;&#21333;&#20294;&#26377;&#25928;&#30340;&#27169;&#22411;&#65292;&#22312;&#21508;&#31181;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#19968;&#31181;&#26377;&#25928;&#32780;&#28789;&#27963;&#30340;&#30697;&#38453;&#26469;&#34920;&#31034;&#22270;&#24418;&#26159;&#19968;&#20010;&#22522;&#26412;&#30340;&#25361;&#25112;&#65292;&#20854;&#24050;&#20174;&#22810;&#20010;&#35282;&#24230;&#36827;&#34892;&#20102;&#25506;&#32034;&#65292;&#20363;&#22914;&#65292;&#20351;&#29992;&#22270;&#20613;&#37324;&#21494;&#21464;&#25442;&#20013;&#30340;&#28388;&#27874;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#20174;&#21442;&#25968;&#21270;&#20998;&#35299;&#21644;&#28388;&#27874;&#30340;&#35282;&#24230;&#24320;&#21457;&#20102;&#19968;&#20010;&#26032;&#30340;&#36890;&#29992;&#26694;&#26550;&#65292;&#32479;&#19968;&#20102;&#35768;&#22810;&#29616;&#26377;&#30340;GNN&#27169;&#22411;&#65292;&#24182;&#23637;&#31034;&#20102;&#22914;&#20309;&#25552;&#39640;GNN&#30340;&#28789;&#27963;&#24615;&#65292;&#21516;&#26102;&#20943;&#36731;&#29616;&#26377;&#27169;&#22411;&#30340;&#24179;&#28369;&#21644;&#25918;&#22823;&#38382;&#39064;&#12290;&#26412;&#36136;&#19978;&#65292;&#25105;&#20204;&#34920;&#26126;&#65292;&#20855;&#26377;&#21487;&#23398;&#20064;&#22810;&#39033;&#24335;&#28388;&#27874;&#22120;&#30340;&#35889;&#22270;&#21367;&#31215;&#26159;&#36825;&#31181;&#34920;&#36848;&#30340;&#32422;&#26463;&#21464;&#20307;&#65292;&#25918;&#24323;&#36825;&#20123;&#32422;&#26463;&#20351;&#25105;&#20204;&#30340;&#27169;&#22411;&#33021;&#22815;&#21516;&#26102;&#34920;&#36798;&#25152;&#38656;&#30340;&#20998;&#35299;&#21644;&#28388;&#27874;&#12290;&#22522;&#20110;&#36825;&#20010;&#24191;&#20041;&#26694;&#26550;&#65292;&#25105;&#20204;&#24320;&#21457;&#30340;&#27169;&#22411;&#22312;&#23454;&#29616;&#19978;&#31616;&#21333;&#65292;&#20294;&#22312;&#21508;&#31181;&#22270;&#24418;&#23398;&#20064;&#20219;&#21153;&#20013;&#23454;&#29616;&#20102;&#26174;&#33879;&#30340;&#25913;&#36827;&#21644;&#35745;&#31639;&#25928;&#29575;&#12290; &#20195;&#30721;&#21487;&#22312; https://github.com/qslim/PDF &#33719;&#24471;&#12290;
&lt;/p&gt;
&lt;p&gt;
Proposing an effective and flexible matrix to represent a graph is a fundamental challenge that has been explored from multiple perspectives, e.g., filtering in Graph Fourier Transforms. In this work, we develop a novel and general framework which unifies many existing GNN models from the view of parameterized decomposition and filtering, and show how it helps to enhance the flexibility of GNNs while alleviating the smoothness and amplification issues of existing models. Essentially, we show that the extensively studied spectral graph convolutions with learnable polynomial filters are constrained variants of this formulation, and releasing these constraints enables our model to express the desired decomposition and filtering simultaneously. Based on this generalized framework, we develop models that are simple in implementation but achieve significant improvements and computational efficiency on a variety of graph learning tasks. Code is available at https://github.com/qslim/PDF.
&lt;/p&gt;</description></item><item><title>PAI&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#31561;&#22806;&#37096;&#23454;&#20307;&#20449;&#24687;&#30340;&#36890;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;SemEval-2023&#20219;&#21153;2&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20849;&#36194;&#24471;&#20102;7&#20010;&#22870;&#39033;&#12290;</title><link>http://arxiv.org/abs/2305.06099</link><description>&lt;p&gt;
PAI&#22312;SemEval-2023&#20219;&#21153;2&#20013;&#65306;&#21033;&#29992;&#22806;&#37096;&#23454;&#20307;&#20449;&#24687;&#36827;&#34892;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#30340;&#36890;&#29992;&#31995;&#32479;
&lt;/p&gt;
&lt;p&gt;
PAI at SemEval-2023 Task 2: A Universal System for Named Entity Recognition with External Entity Information. (arXiv:2305.06099v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06099
&lt;/p&gt;
&lt;p&gt;
PAI&#22242;&#38431;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#32500;&#22522;&#30334;&#31185;&#31561;&#22806;&#37096;&#23454;&#20307;&#20449;&#24687;&#30340;&#36890;&#29992;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#22312;SemEval-2023&#20219;&#21153;2&#20013;&#34920;&#29616;&#20986;&#33394;&#65292;&#20849;&#36194;&#24471;&#20102;7&#20010;&#22870;&#39033;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
MultiCoNER II&#20219;&#21153;&#26088;&#22312;&#22312;&#20302;&#25991;&#26412;&#24773;&#22659;&#21644;&#23384;&#22312;&#25340;&#20889;&#38169;&#35823;&#21644;&#38169;&#21035;&#23383;&#31561;&#22122;&#38899;&#22330;&#26223;&#19979;&#65292;&#26816;&#27979;&#22810;&#31181;&#35821;&#35328;&#30340;&#22797;&#26434;&#12289;&#27169;&#31946;&#21644;&#32454;&#31890;&#24230;&#21629;&#21517;&#23454;&#20307;&#12290;&#35813;&#20219;&#21153;&#30001;&#20110;&#19978;&#19979;&#25991;&#20449;&#24687;&#30340;&#32570;&#20047;&#12289;&#23454;&#20307;&#30340;&#39640;&#31890;&#24230;&#65288;&#39640;&#36798;33&#31181;&#31867;&#65289;&#20197;&#21450;&#22122;&#38899;&#25968;&#25454;&#30340;&#24178;&#25200;&#32780;&#20855;&#26377;&#37325;&#35201;&#30340;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#24212;&#23545;&#36825;&#20123;&#38382;&#39064;&#65292;&#25105;&#20204;&#30340;&#22242;&#38431;PAI&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#31995;&#32479;&#65292;&#35813;&#31995;&#32479;&#38598;&#25104;&#20102;&#22806;&#37096;&#23454;&#20307;&#20449;&#24687;&#20197;&#25552;&#39640;&#24615;&#33021;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#20174;&#30693;&#35782;&#24211;&#65288;&#21363;&#32500;&#22522;&#30334;&#31185;&#65289;&#20013;&#26816;&#32034;&#32473;&#23450;&#25991;&#26412;&#30340;&#23454;&#20307;&#23646;&#24615;&#65292;&#28982;&#21518;&#23558;&#23454;&#20307;&#20449;&#24687;&#19982;&#36755;&#20837;&#21477;&#23376;&#36830;&#25509;&#36215;&#26469;&#65292;&#23558;&#20854;&#39304;&#20837;&#22522;&#20110;Transformer&#30340;&#27169;&#22411;&#12290;&#26368;&#32456;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22312;13&#20010;&#36712;&#36947;&#20013;&#36194;&#24471;&#20102;2&#20010;&#19968;&#31561;&#22870;&#65292;4&#20010;&#20108;&#31561;&#22870;&#21644;1&#20010;&#19977;&#31561;&#22870;&#12290;&#35813;&#31995;&#32479;&#30340;&#20195;&#30721;&#20844;&#24320;&#21487;&#20379;&#20351;&#29992;&#65292;&#32593;&#22336;&#20026;\url{https://github.com/diqiuzhuanzhuan/semeval-2023}&#12290;
&lt;/p&gt;
&lt;p&gt;
The MultiCoNER II task aims to detect complex, ambiguous, and fine-grained named entities in low-context situations and noisy scenarios like the presence of spelling mistakes and typos for multiple languages. The task poses significant challenges due to the scarcity of contextual information, the high granularity of the entities(up to 33 classes), and the interference of noisy data. To address these issues, our team {\bf PAI} proposes a universal Named Entity Recognition (NER) system that integrates external entity information to improve performance. Specifically, our system retrieves entities with properties from the knowledge base (i.e. Wikipedia) for a given text, then concatenates entity information with the input sentence and feeds it into Transformer-based models. Finally, our system wins 2 first places, 4 second places, and 1 third place out of 13 tracks. The code is publicly available at \url{https://github.com/diqiuzhuanzhuan/semeval-2023}.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;iTelos&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#30446;&#30340;&#39537;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#26469;&#26500;&#24314;&#21487;&#20114;&#25805;&#20316;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;&#20854;&#20013;&#20851;&#38190;&#24605;&#24819;&#26159;&#25968;&#25454;&#32423;&#21035;&#21644;&#27169;&#24335;&#32423;&#21035;&#24212;&#29420;&#31435;&#24320;&#21457;&#65292;&#20801;&#35768;&#26368;&#22823;&#38480;&#24230;&#22320;&#37325;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#33021;&#21147;&#26597;&#35810;&#26469;&#35268;&#33539;&#38656;&#35201;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;eHealth&#39046;&#22495;&#23454;&#26045;&#24182;&#36890;&#36807;&#30495;&#23454;&#26696;&#20363;&#35780;&#20272;&#65292;&#26174;&#31034;&#20102;&#24320;&#21457;&#26102;&#38388;&#21644;&#25104;&#26412;&#30340;&#26174;&#33879;&#38477;&#20302;&#12290;</title><link>http://arxiv.org/abs/2305.06088</link><description>&lt;p&gt;
&#22522;&#20110;&#30446;&#30340;&#39537;&#21160;&#30693;&#35782;&#22270;&#35889;&#24314;&#31435;&#21487;&#20114;&#25805;&#20316;&#30340;&#30005;&#23376;&#20581;&#24247;&#26723;&#26696;
&lt;/p&gt;
&lt;p&gt;
Building Interoperable Electronic Health Records as Purpose-Driven Knowledge Graphs. (arXiv:2305.06088v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06088
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;iTelos&#30340;&#38598;&#25104;&#26041;&#27861;&#65292;&#35813;&#26041;&#27861;&#20351;&#29992;&#30446;&#30340;&#39537;&#21160;&#30340;&#30693;&#35782;&#22270;&#35889;&#26469;&#26500;&#24314;&#21487;&#20114;&#25805;&#20316;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#12290;&#20854;&#20013;&#20851;&#38190;&#24605;&#24819;&#26159;&#25968;&#25454;&#32423;&#21035;&#21644;&#27169;&#24335;&#32423;&#21035;&#24212;&#29420;&#31435;&#24320;&#21457;&#65292;&#20801;&#35768;&#26368;&#22823;&#38480;&#24230;&#22320;&#37325;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#24182;&#36890;&#36807;&#33021;&#21147;&#26597;&#35810;&#26469;&#35268;&#33539;&#38656;&#35201;&#30340;&#21151;&#33021;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;eHealth&#39046;&#22495;&#23454;&#26045;&#24182;&#36890;&#36807;&#30495;&#23454;&#26696;&#20363;&#35780;&#20272;&#65292;&#26174;&#31034;&#20102;&#24320;&#21457;&#26102;&#38388;&#21644;&#25104;&#26412;&#30340;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26500;&#24314;&#26032;&#24212;&#29992;&#31243;&#24207;&#26102;&#65292;&#25105;&#20204;&#36234;&#26469;&#36234;&#38656;&#35201;&#37325;&#29992;&#21644;&#25972;&#21512;&#24050;&#26377;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#23454;&#38469;&#24773;&#20917;&#26159;&#65292;&#36825;&#20123;&#20808;&#21069;&#30340;&#30693;&#35782;&#20960;&#20046;&#19981;&#21487;&#33021;&#30452;&#25509;&#37325;&#29992;&#12290;&#36825;&#22312;eHealth&#31561;&#39046;&#22495;&#23588;&#20854;&#22914;&#27492;&#65292;&#36825;&#20123;&#39046;&#22495;&#24050;&#32463;&#20184;&#20986;&#20102;&#22823;&#37327;&#30340;&#21162;&#21147;&#26469;&#24320;&#21457;&#39640;&#36136;&#37327;&#30340;&#26631;&#20934;&#21644;&#21442;&#32771;&#26412;&#20307;&#65292;&#20363;&#22914;FHIR&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38598;&#25104;&#26041;&#27861;&#65292;&#31216;&#20026;iTelos&#65292;&#35813;&#26041;&#27861;&#25903;&#25345;&#25968;&#25454;&#21644;&#30693;&#35782;&#30340;&#37325;&#29992;&#65292;&#20197;&#26500;&#24314;&#21487;&#20114;&#25805;&#20316;&#30340;&#30005;&#23376;&#20581;&#24247;&#35760;&#24405;&#65288;iEHR&#65289;&#12290;&#20851;&#38190;&#24605;&#24819;&#26159;&#24212;&#29992;&#31243;&#24207;&#30340;&#25968;&#25454;&#32423;&#21035;&#21644;&#27169;&#24335;&#32423;&#21035;&#24212;&#35813;&#29420;&#31435;&#24320;&#21457;&#65292;&#20174;&#32780;&#22312;&#28385;&#36275;&#38656;&#35201;&#30340;&#21069;&#25552;&#19979;&#26368;&#22823;&#38480;&#24230;&#22320;&#28789;&#27963;&#37325;&#29992;&#20808;&#21069;&#30340;&#30693;&#35782;&#65292;&#36825;&#20123;&#38656;&#27714;&#34987;&#35268;&#33539;&#21270;&#20026;&#33021;&#21147;&#26597;&#35810;&#12290;&#26412;&#25991;&#21033;&#29992;&#39044;&#20808;&#23450;&#20041;&#30340;&#30446;&#30340;&#26469;&#23454;&#29616;&#36825;&#31181;&#30452;&#35273;&#65292;&#36825;&#20010;&#30446;&#30340;&#34987;&#29992;&#20110;&#39537;&#21160;&#29305;&#23450;&#39046;&#22495;&#20013;&#22522;&#20110;&#30446;&#30340;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;PD-KG&#65289;&#30340;&#24320;&#21457;&#12290;iTelos&#26041;&#27861;&#21253;&#25324;&#19971;&#20010;&#27493;&#39588;&#65292;&#28041;&#21450;&#30446;&#26631;&#30340;&#23450;&#20041;&#12289;&#35201;&#27714;&#30340;&#25910;&#38598;&#12289;&#29616;&#26377;&#26631;&#20934;&#21644;&#30693;&#35782;&#28304;&#30340;&#35782;&#21035;&#12289;&#24322;&#26500;&#25968;&#25454;&#21040;PD-KG&#30340;&#38598;&#25104;&#12289;PD-KG&#33021;&#21147;&#26597;&#35810;&#30340;&#27880;&#37322;&#12289;&#22522;&#20110;&#33021;&#21147;&#38656;&#27714;&#30340;&#25512;&#29702;&#26426;&#21046;&#30340;&#23454;&#29616;&#20197;&#21450;&#23545;iEHR&#27169;&#22411;&#19982;&#21442;&#32771;&#29992;&#20363;&#30340;&#39564;&#35777;&#12290;&#35813;&#26041;&#27861;&#24050;&#22312;eHealth&#39046;&#22495;&#23454;&#26045;&#65292;&#37325;&#29992;&#20102;&#20960;&#31181;&#26631;&#20934;&#21644;&#21442;&#32771;&#26412;&#20307;&#65292;&#21253;&#25324;FHIR&#21644;SNOMED-CT&#65292;&#24182;&#36890;&#36807;&#19968;&#20010;&#30495;&#23454;&#30340;&#26696;&#20363;&#30740;&#31350;&#36827;&#34892;&#20102;&#35780;&#20272;&#65292;&#26174;&#31034;&#20986;&#20102;&#22312;&#30830;&#20445;&#31526;&#21512;&#30456;&#20851;&#27861;&#35268;&#21644;&#36136;&#37327;&#26631;&#20934;&#30340;&#24773;&#20917;&#19979;&#65292;&#24320;&#21457;&#26102;&#38388;&#21644;&#25104;&#26412;&#30340;&#26174;&#33879;&#38477;&#20302;&#12290;
&lt;/p&gt;
&lt;p&gt;
When building a new application we are increasingly confronted with the need of reusing and integrating pre-existing knowledge. Nevertheless, it is a fact that this prior knowledge is virtually impossible to reuse as-is. This is true also in domains, e.g., eHealth, where a lot of effort has been put into developing high-quality standards and reference ontologies, e.g. FHIR1. In this paper, we propose an integrated methodology, called iTelos, which enables data and knowledge reuse towards the construction of Interoperable Electronic Health Records (iEHR). The key intuition is that the data level and the schema level of an application should be developed independently, thus allowing for maximum flexibility in the reuse of the prior knowledge, but under the overall guidance of the needs to be satisfied, formalized as competence queries. This intuition is implemented by codifying all the requirements, including those concerning reuse, as part of a purpose defined a priori, which is then us
&lt;/p&gt;</description></item><item><title>ChatGPT&#23637;&#31034;&#20102;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#36816;&#34892;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#39640;&#26114;&#30340;&#25104;&#26412;&#65292;&#39044;&#35745;&#20250;&#32473;AI&#30740;&#31350;&#24102;&#26469;&#37325;&#22823;&#24433;&#21709;&#12290;</title><link>http://arxiv.org/abs/2305.06087</link><description>&lt;p&gt;
ChatGPT&#33021;&#21147;&#23637;&#31034;&#21450;&#20854;&#23545;AI&#30740;&#31350;&#30340;&#24433;&#21709;
&lt;/p&gt;
&lt;p&gt;
A Glimpse in ChatGPT Capabilities and its impact for AI research. (arXiv:2305.06087v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06087
&lt;/p&gt;
&lt;p&gt;
ChatGPT&#23637;&#31034;&#20102;LLMs&#30340;&#24378;&#22823;&#33021;&#21147;&#65292;&#20294;&#36825;&#20123;&#27169;&#22411;&#30340;&#35757;&#32451;&#21644;&#36816;&#34892;&#38656;&#35201;&#24040;&#22823;&#30340;&#35745;&#31639;&#36164;&#28304;&#21644;&#39640;&#26114;&#30340;&#25104;&#26412;&#65292;&#39044;&#35745;&#20250;&#32473;AI&#30740;&#31350;&#24102;&#26469;&#37325;&#22823;&#24433;&#21709;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#26368;&#36817;&#22312;&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#30740;&#31350;&#39046;&#22495;&#25104;&#20026;&#28909;&#38376;&#35805;&#39064;&#12290;&#20687;Google&#12289;&#20122;&#39532;&#36874;&#12289;Facebook&#12289;&#29305;&#26031;&#25289;&#21644;&#33529;&#26524;&#65288;GAFA&#65289;&#36825;&#26679;&#30340;&#20844;&#21496;&#27491;&#22312;&#22823;&#21147;&#21457;&#23637;&#36825;&#20123;&#27169;&#22411;&#12290;&#36825;&#20123;&#27169;&#22411;&#20250;&#20351;&#29992;&#28023;&#37327;&#30340;&#25968;&#25454;&#36827;&#34892;&#35757;&#32451;&#65292;&#21487;&#29992;&#20110;&#21508;&#31181;&#20219;&#21153;&#65292;&#21253;&#25324;&#35821;&#35328;&#32763;&#35793;&#12289;&#25991;&#31456;&#29983;&#25104;&#21644;&#38382;&#31572;&#31995;&#32479;&#12290;&#28982;&#32780;&#65292;&#35757;&#32451;&#21644;&#36816;&#34892;&#36825;&#20123;&#27169;&#22411;&#25152;&#38656;&#30340;&#35745;&#31639;&#36164;&#28304;&#38750;&#24120;&#24040;&#22823;&#65292;&#32780;&#30828;&#20214;&#21644;&#30005;&#21147;&#30340;&#25104;&#26412;&#21487;&#33021;&#38480;&#21046;&#20102;&#37027;&#20123;&#27809;&#26377;GAFA&#36164;&#37329;&#21644;&#36164;&#28304;&#30340;&#30740;&#31350;&#23454;&#39564;&#23460;&#12290;&#26412;&#25991;&#23558;&#30740;&#31350;LLMs&#23545;AI&#30740;&#31350;&#30340;&#24433;&#21709;&#65292;&#37325;&#28857;&#20851;&#27880;GPT3.5/ChatGPT3.4&#65292;&#24182;&#32473;&#20986;&#20351;&#29992;&#36825;&#20123;&#27169;&#22411;&#36827;&#34892;&#30740;&#31350;&#30340;&#19968;&#20123;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have recently become a popular topic in the field of Artificial Intelligence (AI) research, with companies such as Google, Amazon, Facebook, Amazon, Tesla, and Apple (GAFA) investing heavily in their development. These models are trained on massive amounts of data and can be used for a wide range of tasks, including language translation, text generation, and question answering. However, the computational resources required to train and run these models are substantial, and the cost of hardware and electricity can be prohibitive for research labs that do not have the funding and resources of the GAFA. In this paper, we will examine the impact of LLMs on AI research. The pace at which such models are generated as well as the range of domains covered is an indication of the trend which not only the public but also the scientific community is currently experiencing. We give some examples on how to use such models in research by focusing on GPT3.5/ChatGPT3.4 and
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#31934;&#24230;&#19979;&#37319;&#26679;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36319;&#36394;&#31639;&#27861;&#26469;&#22788;&#29702;&#26368;&#20248;&#20998;&#37197;&#30340;&#38750;&#21807;&#19968;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#28176;&#36827;&#26368;&#20248;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.06082</link><description>&lt;p&gt;
&#26377;&#38480;&#31934;&#24230;&#37319;&#26679;&#19979;&#30340;&#36172;&#21338;&#26426;&#26368;&#20248;&#33218;&#35782;&#21035;&#38382;&#39064;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Best Arm Identification in Bandits with Limited Precision Sampling. (arXiv:2305.06082v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06082
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#26377;&#38480;&#31934;&#24230;&#19979;&#37319;&#26679;&#30340;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#20462;&#25913;&#30340;&#36319;&#36394;&#31639;&#27861;&#26469;&#22788;&#29702;&#26368;&#20248;&#20998;&#37197;&#30340;&#38750;&#21807;&#19968;&#24615;&#65292;&#24182;&#35777;&#26126;&#20102;&#20854;&#28176;&#36827;&#26368;&#20248;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#19968;&#31181;&#22810;&#33218;&#36172;&#21338;&#26426;&#38382;&#39064;&#30340;&#21464;&#20307;&#65292;&#22312;&#35813;&#38382;&#39064;&#20013;&#65292;&#23398;&#20064;&#32773;&#22312;&#36873;&#25321;&#33218;&#26102;&#26377;&#38480;&#30340;&#31934;&#24230;&#12290;&#23398;&#20064;&#32773;&#21482;&#33021;&#36890;&#36807;&#29305;&#23450;&#30340;&#25506;&#32034;&#32452;&#21512;&#65288;&#21363;&#25152;&#35859;&#30340;&#31665;&#23376;&#65289;&#37319;&#26679;&#33218;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#22312;&#27599;&#20010;&#37319;&#26679;&#26102;&#21051;&#65292;&#23398;&#20064;&#32773;&#36873;&#25321;&#19968;&#20010;&#31665;&#23376;&#65292;&#28982;&#21518;&#26681;&#25454;&#31665;&#23376;&#29305;&#23450;&#30340;&#27010;&#29575;&#20998;&#24067;&#25289;&#21160;&#33218;&#65292;&#25581;&#31034;&#34987;&#25289;&#21160;&#30340;&#33218;&#21450;&#20854;&#30636;&#26102;&#25910;&#30410;&#65292;&#20854;&#30446;&#26631;&#26159;&#36890;&#36807;&#26368;&#23567;&#21270;&#26399;&#26395;&#20572;&#27490;&#26102;&#38388;&#26469;&#25214;&#21040;&#26368;&#20339;&#33218;&#65292;&#32780;&#35823;&#24046;&#27010;&#29575;&#21463;&#21040;&#19978;&#38480;&#32422;&#26463;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study best arm identification in a variant of the multi-armed bandit problem where the learner has limited precision in arm selection. The learner can only sample arms via certain exploration bundles, which we refer to as boxes. In particular, at each sampling epoch, the learner selects a box, which in turn causes an arm to get pulled as per a box-specific probability distribution. The pulled arm and its instantaneous reward are revealed to the learner, whose goal is to find the best arm by minimising the expected stopping time, subject to an upper bound on the error probability. We present an asymptotic lower bound on the expected stopping time, which holds as the error probability vanishes. We show that the optimal allocation suggested by the lower bound is, in general, non-unique and therefore challenging to track. We propose a modified tracking-based algorithm to handle non-unique optimal allocations, and demonstrate that it is asymptotically optimal. We also present non-asympto
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#35273;&#35843;&#25972;&#30340;&#21457;&#23637;&#19982;&#29616;&#29366;&#65292;&#23558;&#36817;&#26399;&#30340;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#20998;&#20026;&#20116;&#31867;&#65292;&#21253;&#25324;&#25552;&#31034;&#35843;&#25972;&#12289;&#36866;&#37197;&#22120;&#35843;&#25972;&#12289;&#21442;&#25968;&#32763;&#35793;&#12289;&#32039;&#20945;&#35843;&#25972;&#21644;&#27169;&#22359;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;</title><link>http://arxiv.org/abs/2305.06061</link><description>&lt;p&gt;
&#35270;&#35273;&#35843;&#25972;
&lt;/p&gt;
&lt;p&gt;
Visual Tuning. (arXiv:2305.06061v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06061
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#35270;&#35273;&#35843;&#25972;&#30340;&#21457;&#23637;&#19982;&#29616;&#29366;&#65292;&#23558;&#36817;&#26399;&#30340;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#20998;&#20026;&#20116;&#31867;&#65292;&#21253;&#25324;&#25552;&#31034;&#35843;&#25972;&#12289;&#36866;&#37197;&#22120;&#35843;&#25972;&#12289;&#21442;&#25968;&#32763;&#35793;&#12289;&#32039;&#20945;&#35843;&#25972;&#21644;&#27169;&#22359;&#35843;&#25972;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23545;&#35270;&#35273;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#24050;&#34987;&#24191;&#27867;&#35777;&#26126;&#22312;&#35768;&#22810;&#19979;&#28216;&#35270;&#35273;&#20219;&#21153;&#20013;&#20855;&#26377;&#26377;&#21069;&#36884;&#30340;&#34920;&#29616;&#12290;&#38543;&#30528;&#39044;&#35757;&#32451;&#35270;&#35273;&#22522;&#30784;&#27169;&#22411;&#30340;&#24778;&#20154;&#21457;&#23637;&#65292;&#35270;&#35273;&#35843;&#25972;&#36339;&#20986;&#20102;&#26631;&#20934;&#30340;&#27169;&#24335;&#25805;&#20316;&#65292;&#21363;&#24494;&#35843;&#25972;&#20010;&#39044;&#35757;&#32451;&#27169;&#22411;&#25110;&#20165;&#24494;&#35843;&#23436;&#20840;&#36830;&#25509;&#23618;&#12290;&#30456;&#21453;&#65292;&#36817;&#26399;&#30340;&#36827;&#23637;&#21487;&#20197;&#36890;&#36807;&#26356;&#26032;&#26356;&#23569;&#30340;&#21442;&#25968;&#23454;&#29616;&#27604;&#20840;&#38754;&#24494;&#35843;&#25972;&#20010;&#39044;&#35757;&#32451;&#21442;&#25968;&#26356;&#20248;&#24322;&#30340;&#34920;&#29616;&#65292;&#20351;&#36793;&#32536;&#35774;&#22791;&#21644;&#19979;&#28216;&#24212;&#29992;&#31243;&#24207;&#21487;&#20197;&#37325;&#22797;&#20351;&#29992;&#37096;&#32626;&#22312;&#20113;&#31471;&#30340;&#26085;&#30410;&#24222;&#22823;&#30340;&#22522;&#30784;&#27169;&#22411;&#12290;&#20026;&#20102;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20840;&#38754;&#20102;&#35299;&#35270;&#35273;&#35843;&#25972;&#30340;&#20840;&#35980;&#21644;&#26410;&#26469;&#26041;&#21521;&#65292;&#26412;&#32508;&#36848;&#25551;&#32472;&#20102;&#22823;&#37327;&#30340;&#36817;&#26399;&#30740;&#31350;&#20316;&#21697;&#65292;&#25552;&#20379;&#20102;&#29616;&#26377;&#24037;&#20316;&#21644;&#27169;&#22411;&#31995;&#32479;&#21644;&#20840;&#38754;&#30340;&#27010;&#36848;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#23427;&#25552;&#20379;&#20102;&#35270;&#35273;&#35843;&#25972;&#30340;&#35814;&#32454;&#32972;&#26223;&#65292;&#24182;&#23558;&#26368;&#36817;&#30340;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#20998;&#20026;&#20116;&#32452;&#65306;&#25552;&#31034;&#35843;&#25972;&#12289;&#36866;&#37197;&#22120;&#35843;&#25972;&#12289;&#21442;&#25968;&#32763;&#35793;&#12289;&#32039;&#20945;&#35843;&#25972;&#21644;&#27169;&#22359;&#35843;&#25972;&#12290;&#26412;&#25991;&#36824;&#24378;&#35843;&#20102;&#24403;&#21069;&#35270;&#35273;&#35843;&#25972;&#25216;&#26415;&#30340;&#38480;&#21046;&#21644;&#25361;&#25112;&#65292;&#24182;&#25552;&#20986;&#20102;&#20960;&#20010;&#26410;&#26469;&#30740;&#31350;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
Fine-tuning visual models has been widely shown promising performance on many downstream visual tasks. With the surprising development of pre-trained visual foundation models, visual tuning jumped out of the standard modus operandi that fine-tunes the whole pre-trained model or just the fully connected layer. Instead, recent advances can achieve superior performance than full-tuning the whole pre-trained parameters by updating far fewer parameters, enabling edge devices and downstream applications to reuse the increasingly large foundation models deployed on the cloud. With the aim of helping researchers get the full picture and future directions of visual tuning, this survey characterizes a large and thoughtful selection of recent works, providing a systematic and comprehensive overview of existing work and models. Specifically, it provides a detailed background of visual tuning and categorizes recent visual tuning techniques into five groups: prompt tuning, adapter tuning, parameter 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#21464;&#21442;&#25968;&#32534;&#30721;&#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#21487;&#21464;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#20197;VGG-16&#30340;&#27979;&#35797;&#31934;&#24230;&#25552;&#39640;&#20026;&#20363;&#12290;</title><link>http://arxiv.org/abs/2305.06058</link><description>&lt;p&gt;
&#20351;&#29992;&#25351;&#25968;&#32423;&#21035;&#30340;&#23569;&#37327;&#21464;&#20998;&#21442;&#25968;&#30340;&#24352;&#37327;&#32593;&#32476;&#21387;&#32553;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Compressing neural network by tensor network with exponentially fewer variational parameters. (arXiv:2305.06058v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06058
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#29992;&#30340;&#21387;&#32553;&#26041;&#26696;&#65292;&#23558;&#31070;&#32463;&#32593;&#32476;&#30340;&#21487;&#21464;&#21442;&#25968;&#32534;&#30721;&#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65292;&#26126;&#26174;&#20943;&#23569;&#20102;&#21487;&#21464;&#21442;&#25968;&#30340;&#25968;&#37327;&#65292;&#24182;&#22312;&#22810;&#20010;&#31070;&#32463;&#32593;&#32476;&#21644;&#25968;&#25454;&#38598;&#19978;&#34920;&#29616;&#20986;&#20102;&#21331;&#36234;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#20197;VGG-16&#30340;&#27979;&#35797;&#31934;&#24230;&#25552;&#39640;&#20026;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20026;&#20102;&#35299;&#20915;&#31070;&#32463;&#32593;&#32476;&#65288;NN&#65289;&#25152;&#21253;&#21547;&#30340;&#24040;&#22823;&#21487;&#21464;&#30340;&#21442;&#25968;&#38382;&#39064;&#65292;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#23558;&#36825;&#20123;&#21442;&#25968; encoding &#20026;&#22810;&#23618;&#24352;&#37327;&#32593;&#32476;&#65288;TN&#65289;&#30340;&#21387;&#32553;&#26041;&#26696;&#12290;&#36825;&#31181;&#26041;&#26696;&#28436;&#31034;&#20102;&#20986;&#33394;&#30340;&#21387;&#32553;&#24615;&#33021;&#65292;&#36229;&#36807;&#20102;&#20197;&#27973;&#23618;&#24352;&#37327;&#32593;&#32476;&#20026;&#22522;&#30784;&#30340;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;&#20363;&#22914;&#65292;VGG-16&#20013;&#30340;3&#20010;&#21367;&#31215;&#23618;&#30340;&#22823;&#32422;1000&#19975;&#21442;&#25968;&#34987;&#21387;&#32553;&#21040;&#20855;&#26377;&#20165;632&#20010;&#21442;&#25968;&#30340;TN&#20013;&#65292;&#32780;&#22312;CIFAR-10&#19978;&#30340;&#27979;&#35797;&#20934;&#30830;&#24615;&#20196;&#20154;&#24778;&#21916;&#22320;&#25552;&#39640;&#20102;81.14&#65285;&#12290;
&lt;/p&gt;
&lt;p&gt;
Neural network (NN) designed for challenging machine learning tasks is in general a highly nonlinear mapping that contains massive variational parameters. High complexity of NN, if unbounded or unconstrained, might unpredictably cause severe issues including over-fitting, loss of generalization power, and unbearable cost of hardware. In this work, we propose a general compression scheme that significantly reduces the variational parameters of NN by encoding them to multi-layer tensor networks (TN's) that contain exponentially-fewer free parameters. Superior compression performance of our scheme is demonstrated on several widely-recognized NN's (FC-2, LeNet-5, and VGG-16) and datasets (MNIST and CIFAR-10), surpassing the state-of-the-art method based on shallow tensor networks. For instance, about 10 million parameters in the three convolutional layers of VGG-16 are compressed in TN's with just $632$ parameters, while the testing accuracy on CIFAR-10 is surprisingly improved from $81.14
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.06026</link><description>&lt;p&gt;
&#25628;&#32034;UGLE&#30495;&#30456;&#65306;&#26080;&#30417;&#30563;GNN&#23398;&#20064;&#29615;&#22659;&#30340;&#35843;&#26597;
&lt;/p&gt;
&lt;p&gt;
Search for the UGLE Truth: An Investigation into Unsupervised GNN Learning Environments. (arXiv:2305.06026v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06026
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;GNN&#23398;&#20064;&#29615;&#22659;&#19979;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#26694;&#26550;&#65292;&#21253;&#25324;&#25968;&#25454;&#38598;&#21644;&#35780;&#20272;&#25351;&#26631;&#65292;&#20197;&#35299;&#20915;&#30446;&#21069;&#25991;&#29486;&#20013;&#23545;&#20110;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#30340;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22270;&#31070;&#32463;&#32593;&#32476; (GNN) &#26159;&#20219;&#20309;&#26426;&#22120;&#23398;&#20064;&#20219;&#21153;&#20013;&#30340;&#19968;&#20010;&#37325;&#35201;&#24037;&#20855;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#23398;&#20064;&#22270;&#32467;&#26500;&#19978;&#30340;&#20989;&#25968;&#65292;&#36825;&#26159;&#19968;&#31181;&#24378;&#22823;&#21644;&#34920;&#36798;&#24615;&#24378;&#30340;&#25968;&#25454;&#34920;&#31034;&#12290;&#31038;&#21306;&#26816;&#27979;&#26159;&#19968;&#31181;&#26080;&#30417;&#30563;&#20219;&#21153;&#65292;&#36234;&#26469;&#36234;&#22810;&#22320;&#20351;&#29992;GNN&#36827;&#34892;&#12290;&#21033;&#29992;&#33410;&#28857;&#29305;&#24449;&#30340;&#22810;&#32500;&#24230;&#19982;&#22270;&#30340;&#36830;&#25509;&#24615;&#23545;&#22270;&#20013;&#30340;&#33410;&#28857;&#36827;&#34892;&#32858;&#31867;&#65292;&#23545;&#20174;&#31038;&#20132;&#32593;&#32476;&#21040;&#22522;&#22240;&#32452;&#23398;&#30340;&#30495;&#23454;&#19990;&#30028;&#20219;&#21153;&#26377;&#35768;&#22810;&#24212;&#29992;&#12290;&#19981;&#24184;&#30340;&#26159;&#65292;&#30446;&#21069;&#25991;&#29486;&#20013;&#32570;&#20047;&#20844;&#24179;&#19988;&#20005;&#35880;&#35780;&#20272;&#22522;&#20110;GNN&#30340;&#31038;&#21306;&#26816;&#27979;&#30340;&#20805;&#20998;&#22522;&#20934;&#29615;&#22659;&#65292;&#20174;&#32780;&#21487;&#33021;&#38459;&#30861;&#36825;&#19968;&#26032;&#20852;&#39046;&#22495;&#30340;&#36827;&#23637;&#12290;&#25105;&#20204;&#35266;&#23519;&#21040;&#36825;&#31181;&#24773;&#20917;&#19979;&#30340;&#29305;&#23450;&#22256;&#38590;&#26159;&#27169;&#31946;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#29615;&#22659;&#19982;&#24615;&#33021;&#21644;&#35780;&#20272;&#25968;&#25454;&#38598;&#30340;&#20914;&#31361;&#25351;&#26631;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#21644;&#35780;&#20272;&#20102;&#26694;&#26550;&#65292;&#29992;&#20110;&#22312;GNN&#23398;&#20064;&#29615;&#22659;&#20013;&#36827;&#34892;&#19968;&#33268;&#30340;&#31038;&#21306;&#26816;&#27979;&#31639;&#27861;&#27604;&#36739;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#65292;&#24182;&#25552;&#20986;&#20102;&#35780;&#20272;&#25351;&#26631;&#65292;&#21453;&#26144;&#20102;&#26816;&#27979;&#21040;&#30340;&#31038;&#21306;&#30340;&#20869;&#22312;&#36136;&#37327;&#20197;&#21450;&#32858;&#31867;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Graph Neural Networks (GNNs) are a pertinent tool for any machine learning task due to their ability to learn functions over graph structures, a powerful and expressive data representation. The detection of communities, an unsupervised task has increasingly been performed with GNNs. Clustering nodes in a graph using the multi-dimensionality of node features with the connectivity of the graph has many applications to real world tasks from social networks to genomics. Unfortunately, there is currently a gap in the literature with no established sufficient benchmarking environment for fairly and rigorously evaluating GNN based community detection, thereby potentially impeding progress in this nascent field. We observe the particular difficulties in this setting is the ambiguous hyperparameter tuning environments combined with conflicting metrics of performance and evaluation datasets. In this work, we propose and evaluate frameworks for the consistent comparisons of community detection al
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#20154;&#29366;&#24577;&#21644;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#23433;&#20840;&#36816;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#36125;&#21494;&#26031;&#28388;&#27874;&#20272;&#35745;&#26694;&#26550;&#32771;&#34385;&#20102;&#22320;&#26631;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#23545;&#22270;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#25913;&#36827;&#65292;&#24341;&#20837;&#20102;&#26426;&#22120;&#20154;&#21160;&#24577;&#24863;&#30693;&#30340;&#26032;&#35270;&#35282;&#12290;</title><link>http://arxiv.org/abs/2305.06004</link><description>&lt;p&gt;
&#24102;&#26377;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#30340;&#23433;&#20840;&#36816;&#21160;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe motion planning with environment uncertainty. (arXiv:2305.06004v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.06004
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#20154;&#29366;&#24577;&#21644;&#29615;&#22659;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#23433;&#20840;&#36816;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;&#65292;&#20854;&#20013;&#36890;&#36807;&#36125;&#21494;&#26031;&#28388;&#27874;&#20272;&#35745;&#26694;&#26550;&#32771;&#34385;&#20102;&#22320;&#26631;&#19981;&#30830;&#23450;&#24615;&#65292;&#36890;&#36807;&#23545;&#22270;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#25913;&#36827;&#65292;&#24341;&#20837;&#20102;&#26426;&#22120;&#20154;&#21160;&#24577;&#24863;&#30693;&#30340;&#26032;&#35270;&#35282;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#26426;&#22120;&#20154;&#29366;&#24577;&#21644;&#29615;&#22659;&#65288;&#38556;&#30861;&#21644;&#22320;&#26631;&#20301;&#32622;&#65289;&#19981;&#30830;&#23450;&#24615;&#19979;&#36827;&#34892;&#23433;&#20840;&#36816;&#21160;&#35268;&#21010;&#30340;&#26041;&#27861;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#39318;&#20808;&#24320;&#21457;&#20102;&#19968;&#31181;&#26041;&#27861;&#65292;&#32771;&#34385;&#26426;&#22120;&#20154;&#23450;&#20301;&#20013;&#30340;&#22320;&#26631;&#19981;&#30830;&#23450;&#24615;&#12290;&#29616;&#26377;&#30340;&#35268;&#21010;&#26041;&#27861;&#20551;&#23450;&#22320;&#26631;&#20301;&#32622;&#24050;&#32463;&#24456;&#22909;&#22320;&#30693;&#36947;&#65292;&#25110;&#32773;&#21482;&#26377;&#24456;&#23567;&#30340;&#19981;&#30830;&#23450;&#24615;&#12290;&#28982;&#32780;&#65292;&#36825;&#22312;&#23454;&#36341;&#20013;&#21487;&#33021;&#19981;&#25104;&#31435;&#12290;&#22122;&#22768;&#20256;&#24863;&#22120;&#21644;&#19981;&#23436;&#32654;&#30340;&#36816;&#21160;&#20351;&#24471;&#26469;&#33258;&#29615;&#22659;&#29305;&#24449;&#20272;&#35745;&#30340;&#35823;&#24046;&#21152;&#21095;&#12290;&#27492;&#22806;&#65292;&#29615;&#22659;&#20013;&#21487;&#33021;&#20986;&#29616;&#36974;&#25377;&#21644;&#21160;&#24577;&#29289;&#20307;&#65292;&#20174;&#32780;&#23548;&#33268;&#22320;&#26631;&#20272;&#35745;&#19981;&#20934;&#30830;&#12290;&#22240;&#27492;&#65292;&#19981;&#32771;&#34385;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#21487;&#33021;&#20250;&#38169;&#35823;&#22320;&#23450;&#20301;&#26426;&#22120;&#20154;&#65292;&#23548;&#33268;&#20302;&#25928;&#30340;&#35268;&#21010;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#36125;&#21494;&#26031;&#28388;&#27874;&#22120;&#20272;&#35745;&#26694;&#26550;&#20013;&#32435;&#20837;&#20102;&#22320;&#26631;&#19981;&#30830;&#23450;&#24615;&#12290;&#25105;&#20204;&#36824;&#20998;&#26512;&#20102;&#32771;&#34385;&#36825;&#31181;&#19981;&#30830;&#23450;&#24615;&#30340;&#24433;&#21709;&#65292;&#24182;&#21246;&#21202;&#20102;&#21487;&#20197;&#24573;&#30053;&#23427;&#30340;&#24773;&#20917;&#12290;&#20854;&#27425;&#65292;&#25105;&#20204;&#36890;&#36807;&#23545;&#22270;&#25628;&#32034;&#31639;&#27861;&#36827;&#34892;&#25913;&#36827;&#65292;&#24341;&#20837;&#20102;&#26426;&#22120;&#20154;&#21160;&#24577;&#24863;&#30693;&#30340;&#26032;&#35270;&#35282;&#26469;&#25193;&#23637;&#26368;&#26032;&#25216;&#26415;&#30340;&#24212;&#29992;&#33539;&#22260;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present an approach for safe motion planning under robot state and environment (obstacle and landmark location) uncertainties. To this end, we first develop an approach that accounts for the landmark uncertainties during robot localization. Existing planning approaches assume that the landmark locations are well known or are known with little uncertainty. However, this might not be true in practice. Noisy sensors and imperfect motions compound to the errors originating from the estimate of environment features. Moreover, possible occlusions and dynamic objects in the environment render imperfect landmark estimation. Consequently, not considering this uncertainty can wrongly localize the robot, leading to inefficient plans. Our approach thus incorporates the landmark uncertainty within the Bayes filter estimation framework. We also analyze the effect of considering this uncertainty and delineate the conditions under which it can be ignored. Second, we extend the state-of-the-art by c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;ANALOGYKB&#65292;&#19968;&#31181;&#20351;&#29992;&#30334;&#19975;&#35268;&#27169;&#30693;&#35782;&#24211;&#30340;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.05994</link><description>&lt;p&gt;
ANALOGYKB&#65306;&#20351;&#29992;&#30334;&#19975;&#35268;&#27169;&#30693;&#35782;&#24211;&#24320;&#21551;&#35821;&#35328;&#27169;&#22411;&#30340;&#31867;&#27604;&#25512;&#29702;&#33021;&#21147;&#12290;
&lt;/p&gt;
&lt;p&gt;
ANALOGYKB: Unlocking Analogical Reasoning of Language Models with A Million-scale Knowledge Base. (arXiv:2305.05994v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05994
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;ANALOGYKB&#65292;&#19968;&#31181;&#20351;&#29992;&#30334;&#19975;&#35268;&#27169;&#30693;&#35782;&#24211;&#30340;&#31867;&#27604;&#25512;&#29702;&#26041;&#27861;&#65292;&#33021;&#22815;&#20351;&#35821;&#35328;&#27169;&#22411;&#22312;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#19978;&#21462;&#24471;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31867;&#27604;&#25512;&#29702;&#26159;&#20154;&#31867;&#30340;&#19968;&#39033;&#22522;&#26412;&#35748;&#30693;&#33021;&#21147;&#65292;&#28982;&#32780;&#65292;&#30001;&#20110;&#32570;&#20047;&#27169;&#22411;&#35757;&#32451;&#36164;&#28304;&#65292;&#30446;&#21069;&#30340;&#35821;&#35328;&#27169;&#22411;&#20173;&#28982;&#38590;&#20197;&#22312;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#20013;&#36798;&#21040;&#20154;&#31867;&#30340;&#34920;&#29616;&#27700;&#24179;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;ANALOGYKB&#65292;&#36825;&#26159;&#19968;&#20010;&#30334;&#19975;&#35268;&#27169;&#30340;&#31867;&#27604;&#30693;&#35782;&#24211;&#65292;&#23427;&#30001;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#23548;&#20986;&#12290;ANALOGYKB&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#35782;&#21035;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#31867;&#27604;&#65306;1&#65289;&#30456;&#21516;&#20851;&#31995;&#30340;&#31867;&#27604;&#65292;&#21487;&#20197;&#30452;&#25509;&#20174;&#30693;&#35782;&#22270;&#35889;&#20013;&#25552;&#21462;&#65307;2&#65289;&#31867;&#20284;&#20851;&#31995;&#30340;&#31867;&#27604;&#65292;&#21017;&#30001;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;InstructGPT&#65289;&#21551;&#29992;&#30340;&#36873;&#25321;&#21644;&#36807;&#28388;&#31649;&#36947;&#36827;&#34892;&#35782;&#21035;&#65292;&#20877;&#32463;&#36807;&#23569;&#37327;&#20154;&#24037;&#36136;&#37327;&#25511;&#21046;&#12290;&#22312;&#20004;&#20010;&#31867;&#27604;&#25512;&#29702;&#20219;&#21153;&#65288;&#31867;&#27604;&#35782;&#21035;&#21644;&#29983;&#25104;&#65289;&#30340;&#19968;&#31995;&#21015;&#25968;&#25454;&#38598;&#19978;&#30340;&#35780;&#20272;&#34920;&#26126;&#65292;ANALOGYKB&#25104;&#21151;&#22320;&#20351;&#35821;&#35328;&#27169;&#22411;&#21462;&#24471;&#20102;&#27604;&#20043;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#26356;&#22909;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Analogical reasoning is a fundamental cognitive ability of humans. However, current language models (LMs) still struggle to achieve human-like performance in analogical reasoning tasks due to a lack of resources for model training. In this work, we address this gap by proposing ANALOGYKB, a million-scale analogy knowledge base (KB) derived from existing knowledge graphs (KGs). ANALOGYKB identifies two types of analogies from the KGs: 1) analogies of the same relations, which can be directly extracted from the KGs, and 2) analogies of analogous relations, which are identified with a selection and filtering pipeline enabled by large LMs (InstructGPT), followed by minor human efforts for data quality control. Evaluations on a series of datasets of two analogical reasoning tasks (analogy recognition and generation) demonstrate that ANALOGYKB successfully enables LMs to achieve much better results than previous state-of-the-art methods.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#32467;&#26500;Hawkes&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30636;&#26102;&#25928;&#24212;&#23398;&#20064;&#31163;&#25955;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#20013;&#20107;&#20214;&#31867;&#22411;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;</title><link>http://arxiv.org/abs/2305.05986</link><description>&lt;p&gt;
&#20174;&#31163;&#25955;&#21270;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#20013;&#23398;&#20064;&#22240;&#26524;&#32467;&#26500;&#30340;&#32467;&#26500;Hawkes&#36807;&#31243;
&lt;/p&gt;
&lt;p&gt;
Structural Hawkes Processes for Learning Causal Structure from Discrete-Time Event Sequences. (arXiv:2305.05986v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05986
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21483;&#20570;&#32467;&#26500;Hawkes&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#36890;&#36807;&#21033;&#29992;&#30636;&#26102;&#25928;&#24212;&#23398;&#20064;&#31163;&#25955;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#20013;&#20107;&#20214;&#31867;&#22411;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20174;&#31163;&#25955;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#20013;&#23398;&#20064;&#20107;&#20214;&#31867;&#22411;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#26159;&#19968;&#20010;&#37325;&#35201;&#20294;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#20219;&#21153;&#12290;&#29616;&#26377;&#30340;&#26041;&#27861;&#65292;&#22914;&#22522;&#20110;&#22810;&#20803;Hawkes&#36807;&#31243;&#30340;&#26041;&#27861;&#65292;&#22823;&#22810;&#37117;&#24402;&#32467;&#20026;&#23398;&#20064;&#25152;&#35859;&#30340;Granger&#22240;&#26524;&#20851;&#31995;&#65292;&#23427;&#20551;&#23450;&#22240;&#26524;&#20107;&#20214;&#22312;&#25928;&#24212;&#20107;&#20214;&#20043;&#21069;&#20005;&#26684;&#21457;&#29983;&#12290;&#36825;&#31181;&#20551;&#35774;&#22312;&#20302;&#20998;&#36776;&#29575;&#31163;&#25955;&#26102;&#38388;&#20107;&#20214;&#24207;&#21015;&#20013;&#24448;&#24448;&#26159;&#19981;&#21487;&#34892;&#30340;&#65307;&#32780;&#20856;&#22411;&#30340;&#31163;&#25955;Hawkes&#36807;&#31243;&#20027;&#35201;&#21463;&#21040;&#30636;&#26102;&#25928;&#24212;&#24341;&#36215;&#30340;&#21487;&#35782;&#21035;&#24615;&#38382;&#39064;&#30340;&#22256;&#25200;&#65292;&#21363;&#30001;&#20110;&#20302;&#20998;&#36776;&#29575;&#25968;&#25454;&#32780;&#21516;&#26102;&#21457;&#29983;&#30340;&#22240;&#26524;&#20851;&#31995;&#19981;&#20250;&#34987;Granger&#22240;&#26524;&#24615;&#25152;&#25429;&#25417;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#32467;&#26500;Hawkes&#36807;&#31243;&#65288;SHPs&#65289;&#65292;&#21033;&#29992;&#30636;&#26102;&#25928;&#24212;&#26469;&#23398;&#20064;&#31163;&#25955;&#20107;&#20214;&#24207;&#21015;&#20013;&#20107;&#20214;&#31867;&#22411;&#20043;&#38388;&#30340;&#22240;&#26524;&#32467;&#26500;&#12290;&#25152;&#25552;&#20986;&#30340;&#26041;&#27861;&#20855;&#26377;&#26368;&#23567;&#21270;-&#26368;&#22823;&#21270;&#20284;&#28982;&#20989;&#25968;&#30340;&#29305;&#28857;&#12290;
&lt;/p&gt;
&lt;p&gt;
Learning causal structure among event types from discrete-time event sequences is a particularly important but challenging task. Existing methods, such as the multivariate Hawkes processes based methods, mostly boil down to learning the so-called Granger causality which assumes that the cause event happens strictly prior to its effect event. Such an assumption is often untenable beyond applications, especially when dealing with discrete-time event sequences in low-resolution; and typical discrete Hawkes processes mainly suffer from identifiability issues raised by the instantaneous effect, i.e., the causal relationship that occurred simultaneously due to the low-resolution data will not be captured by Granger causality. In this work, we propose Structure Hawkes Processes (SHPs) that leverage the instantaneous effect for learning the causal structure among events type in discrete-time event sequence. The proposed method is featured with the minorization-maximization of the likelihood fu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#21487;&#35299;&#37322;&#24615;&#36923;&#36753;&#23376;&#21477;&#34920;&#36798;&#30446;&#26631;&#20219;&#21153;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#34920;&#24449;&#21442;&#25968;&#21270;&#31526;&#21495;&#36923;&#36753;&#20803;&#32032;&#65292;&#20174;&#32780;&#20415;&#20110;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#26377;&#24847;&#20041;&#30340;&#36923;&#36753;&#23376;&#21477;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#20116;&#20010;&#20803;&#39044;&#27979;&#22120;&#26469;&#25429;&#33719;&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#26412;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#65292;&#32780;&#19988;&#25552;&#20379;&#20102;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#12290;</title><link>http://arxiv.org/abs/2305.05964</link><description>&lt;p&gt;
&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#35299;&#37322;&#24615;&#26816;&#27979;&#19982;&#36923;&#36753;&#25512;&#29702;
&lt;/p&gt;
&lt;p&gt;
Interpretable Multimodal Misinformation Detection with Logic Reasoning. (arXiv:2305.05964v1 [cs.MM])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05964
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#31070;&#32463;&#27169;&#22411;&#65292;&#36890;&#36807;&#38598;&#25104;&#21487;&#35299;&#37322;&#24615;&#36923;&#36753;&#23376;&#21477;&#34920;&#36798;&#30446;&#26631;&#20219;&#21153;&#30340;&#25512;&#29702;&#36807;&#31243;&#65292;&#24182;&#20351;&#29992;&#31070;&#32463;&#34920;&#24449;&#21442;&#25968;&#21270;&#31526;&#21495;&#36923;&#36753;&#20803;&#32032;&#65292;&#20174;&#32780;&#20415;&#20110;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#26377;&#24847;&#20041;&#30340;&#36923;&#36753;&#23376;&#21477;&#12290;&#27492;&#22806;&#65292;&#24341;&#20837;&#20102;&#20116;&#20010;&#20803;&#39044;&#27979;&#22120;&#26469;&#25429;&#33719;&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#26412;&#27169;&#24335;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#35813;&#27169;&#22411;&#19981;&#20165;&#24615;&#33021;&#26174;&#33879;&#20248;&#20110;&#24403;&#21069;&#26041;&#27861;&#65292;&#32780;&#19988;&#25552;&#20379;&#20102;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#32447;&#31038;&#20132;&#24179;&#21488;&#19978;&#30340;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#30001;&#20110;&#22810;&#23186;&#20307;&#20869;&#23481;&#30340;&#21487;&#20449;&#24230;&#21644;&#20256;&#25773;&#26356;&#23481;&#26131;&#32780;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#34429;&#28982;&#29616;&#26377;&#30340;&#22810;&#27169;&#24577;&#26816;&#27979;&#26041;&#27861;&#24050;&#32463;&#36798;&#21040;&#20102;&#36739;&#39640;&#30340;&#24615;&#33021;&#65292;&#20294;&#32570;&#20047;&#35299;&#37322;&#24615;&#38459;&#30861;&#20102;&#36825;&#20123;&#31995;&#32479;&#30340;&#21487;&#38752;&#24615;&#21644;&#23454;&#38469;&#37096;&#32626;&#12290;&#21463;&#21040; NeuralSymbolic AI &#30340;&#21551;&#21457;&#65292;&#35813;&#26041;&#27861;&#32467;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#30340;&#23398;&#20064;&#33021;&#21147;&#21644;&#31526;&#21495;&#23398;&#20064;&#30340;&#21487;&#35299;&#37322;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#22522;&#20110;&#36923;&#36753;&#30340;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#26816;&#27979;&#31070;&#32463;&#27169;&#22411;&#65292;&#23427;&#38598;&#25104;&#20102;&#21487;&#35299;&#37322;&#24615;&#36923;&#36753;&#23376;&#21477;&#20197;&#34920;&#36798;&#30446;&#26631;&#20219;&#21153;&#30340;&#25512;&#29702;&#36807;&#31243;&#12290;&#20026;&#20102;&#20351;&#23398;&#20064;&#26377;&#25928;&#65292;&#25105;&#20204;&#20351;&#29992;&#31070;&#32463;&#34920;&#24449;&#26469;&#21442;&#25968;&#21270;&#31526;&#21495;&#36923;&#36753;&#20803;&#32032;&#65292;&#20174;&#32780;&#20415;&#20110;&#33258;&#21160;&#29983;&#25104;&#21644;&#35780;&#20272;&#26377;&#24847;&#20041;&#30340;&#36923;&#36753;&#23376;&#21477;&#12290;&#21478;&#22806;&#65292;&#20026;&#20102;&#20351;&#25105;&#20204;&#30340;&#26694;&#26550;&#21487;&#36866;&#29992;&#20110;&#21508;&#31181;&#34394;&#20551;&#20449;&#24687;&#26469;&#28304;&#65292;&#25105;&#20204;&#22312;&#22810;&#27169;&#24577;&#34701;&#21512;&#32593;&#32476;&#20013;&#24341;&#20837;&#20102;&#20116;&#20010;&#20803;&#39044;&#27979;&#22120;&#26469;&#25429;&#33719;&#34394;&#20551;&#20449;&#24687;&#30340;&#22522;&#26412;&#27169;&#24335;&#12290;&#25105;&#20204;&#22312;&#23454;&#38469;&#30340;&#22810;&#27169;&#24577;&#34394;&#20551;&#20449;&#24687;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#22823;&#37327;&#23454;&#39564;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#19981;&#20165;&#26174;&#30528;&#20248;&#20110;&#29616;&#26377;&#26041;&#27861;&#65292;&#36824;&#20026;&#27599;&#20010;&#39044;&#27979;&#25552;&#20379;&#20102;&#36879;&#26126;&#19988;&#21487;&#35299;&#37322;&#30340;&#36923;&#36753;&#25512;&#29702;&#36807;&#31243;&#12290;
&lt;/p&gt;
&lt;p&gt;
Multimodal misinformation on online social platforms is becoming a critical concern due to increasing credibility and easier dissemination brought by multimedia content, compared to traditional text-only information. While existing multimodal detection approaches have achieved high performance, the lack of interpretability hinders these systems' reliability and practical deployment. Inspired by NeuralSymbolic AI which combines the learning ability of neural networks with the explainability of symbolic learning, we propose a novel logic-based neural model for multimodal misinformation detection which integrates interpretable logic clauses to express the reasoning process of the target task. To make learning effective, we parameterize symbolic logical elements using neural representations, which facilitate the automatic generation and evaluation of meaningful logic clauses. Additionally, to make our framework generalizable across diverse misinformation sources, we introduce five meta-pre
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#36335;&#24452;&#32467;&#26500;&#23545;Transformer&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#23376;&#23618;&#20013;&#28155;&#21152;&#24402;&#19968;&#21270;&#12289;&#20135;&#29983;&#26356;&#22810;&#29305;&#24449;&#30340;&#24265;&#20215;&#25805;&#20316;&#21644;&#21487;&#23398;&#20064;&#30340;&#21152;&#26435;&#26426;&#21046;&#26469;&#34701;&#21512;&#20174;&#19981;&#21516;&#36335;&#24452;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#23454;&#39564;&#21457;&#29616;&#30456;&#21516;&#21442;&#25968;&#19979;&#27973;&#23618;&#22810;&#36335;&#24452;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19982;&#28145;&#23618;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05948</link><description>&lt;p&gt;
&#22810;&#36335;&#24452;Transformer&#26356;&#22909;&#65306;&#31070;&#32463;&#26426;&#22120;&#32763;&#35793;&#30340;&#26696;&#20363;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Multi-Path Transformer is Better: A Case Study on Neural Machine Translation. (arXiv:2305.05948v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05948
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22810;&#36335;&#24452;&#32467;&#26500;&#23545;Transformer&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#36890;&#36807;&#22312;&#27599;&#20010;&#23376;&#23618;&#20013;&#28155;&#21152;&#24402;&#19968;&#21270;&#12289;&#20135;&#29983;&#26356;&#22810;&#29305;&#24449;&#30340;&#24265;&#20215;&#25805;&#20316;&#21644;&#21487;&#23398;&#20064;&#30340;&#21152;&#26435;&#26426;&#21046;&#26469;&#34701;&#21512;&#20174;&#19981;&#21516;&#36335;&#24452;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#23454;&#39564;&#21457;&#29616;&#30456;&#21516;&#21442;&#25968;&#19979;&#27973;&#23618;&#22810;&#36335;&#24452;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19982;&#28145;&#23618;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22810;&#24180;&#26469;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#36981;&#24490;&#21442;&#25968;&#23610;&#23544;&#20026;&#24130;&#24459;&#20998;&#24067;&#30340;&#35268;&#24459;&#12290;&#20026;&#20102;&#32771;&#34385;&#21442;&#25968;&#25928;&#29575;&#65292;&#26368;&#36817;&#30340;&#30740;&#31350;&#38598;&#20013;&#20110;&#22686;&#21152;&#27169;&#22411;&#28145;&#24230;&#32780;&#38750;&#23485;&#24230;&#65292;&#20197;&#23454;&#29616;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#26412;&#25991;&#36890;&#36807;&#19968;&#20010;&#21442;&#25968;&#39640;&#25928;&#30340;&#22810;&#36335;&#24452;&#32467;&#26500;&#26469;&#30740;&#31350;&#27169;&#22411;&#23485;&#24230;&#22914;&#20309;&#24433;&#21709;Transformer&#27169;&#22411;&#12290;&#20026;&#20102;&#26356;&#22909;&#22320;&#34701;&#21512;&#20174;&#19981;&#21516;&#36335;&#24452;&#25552;&#21462;&#30340;&#29305;&#24449;&#65292;&#25105;&#20204;&#22312;&#27599;&#20010;&#23376;&#23618;&#20013;&#28155;&#21152;&#20102;&#19977;&#20010;&#38468;&#21152;&#25805;&#20316;&#65306;&#27599;&#20010;&#36335;&#24452;&#26411;&#23614;&#30340;&#24402;&#19968;&#21270;&#12289;&#20135;&#29983;&#26356;&#22810;&#29305;&#24449;&#30340;&#24265;&#20215;&#25805;&#20316;&#20197;&#21450;&#21487;&#23398;&#20064;&#30340;&#21152;&#26435;&#26426;&#21046;&#65292;&#20197;&#28789;&#27963;&#22320;&#34701;&#21512;&#25152;&#26377;&#29305;&#24449;&#12290;&#22312;12&#20010;WMT&#26426;&#22120;&#32763;&#35793;&#20219;&#21153;&#19978;&#30340;&#22823;&#37327;&#23454;&#39564;&#34920;&#26126;&#65292;&#25317;&#26377;&#30456;&#21516;&#25968;&#37327;&#21442;&#25968;&#30340;&#27973;&#23618;&#22810;&#36335;&#24452;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#19982;&#28145;&#23618;&#27169;&#22411;&#30456;&#20284;&#29978;&#33267;&#26356;&#22909;&#30340;&#24615;&#33021;&#65292;&#25581;&#31034;&#20102;&#24212;&#26356;&#21152;&#20851;&#27880;&#22810;&#36335;&#24452;&#32467;&#26500;&#65292;&#24182;&#24212;&#22312;&#27169;&#22411;&#28145;&#24230;&#21644;&#23485;&#24230;&#20043;&#38388;&#36798;&#25104;&#24179;&#34913;&#65292;&#20197;&#35757;&#32451;&#26356;&#22909;&#30340;&#22823;&#35268;&#27169;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;
&lt;/p&gt;
&lt;p&gt;
For years the model performance in machine learning obeyed a power-law relationship with the model size. For the consideration of parameter efficiency, recent studies focus on increasing model depth rather than width to achieve better performance. In this paper, we study how model width affects the Transformer model through a parameter-efficient multi-path structure. To better fuse features extracted from different paths, we add three additional operations to each sublayer: a normalization at the end of each path, a cheap operation to produce more features, and a learnable weighted mechanism to fuse all features flexibly. Extensive experiments on 12 WMT machine translation tasks show that, with the same number of parameters, the shallower multi-path model can achieve similar or even better performance than the deeper model. It reveals that we should pay more attention to the multi-path structure, and there should be a balance between the model depth and width to train a better large-sc
&lt;/p&gt;</description></item><item><title>V2X-Seq&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#26102;&#24207;V2X&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26102;&#24207;&#24863;&#30693;&#25968;&#25454;&#38598;&#21644;&#36712;&#36857;&#39044;&#27979;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36710;&#36335;&#21327;&#21516;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.05938</link><description>&lt;p&gt;
V2X-Seq&#65306;&#19968;&#31181;&#29992;&#20110;&#36710;&#36335;&#21327;&#21516;&#24863;&#30693;&#21644;&#39044;&#27979;&#30340;&#22823;&#35268;&#27169;&#26102;&#24207;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
V2X-Seq: A Large-Scale Sequential Dataset for Vehicle-Infrastructure Cooperative Perception and Forecasting. (arXiv:2305.05938v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05938
&lt;/p&gt;
&lt;p&gt;
V2X-Seq&#26159;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#30340;&#26102;&#24207;V2X&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26102;&#24207;&#24863;&#30693;&#25968;&#25454;&#38598;&#21644;&#36712;&#36857;&#39044;&#27979;&#25968;&#25454;&#38598;&#12290;&#22522;&#20110;&#36825;&#20010;&#25968;&#25454;&#38598;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19977;&#20010;&#26032;&#30340;&#36710;&#36335;&#21327;&#21516;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#21033;&#29992;&#22522;&#30784;&#35774;&#26045;&#21644;&#36710;&#36742;&#31471;&#20449;&#24687;&#36319;&#36394;&#21644;&#39044;&#27979;&#21608;&#22260;&#20132;&#36890;&#21442;&#19982;&#32773;&#30340;&#34892;&#20026;&#21487;&#20197;&#26174;&#33879;&#25552;&#39640;&#33258;&#21160;&#39550;&#39542;&#20915;&#31574;&#21644;&#23433;&#20840;&#24615;&#12290;&#28982;&#32780;&#65292;&#32570;&#20047;&#30495;&#23454;&#19990;&#30028;&#30340;&#26102;&#24207;&#25968;&#25454;&#38598;&#38480;&#21046;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#12290;&#20026;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;V2X-Seq&#65292;&#31532;&#19968;&#20010;&#22823;&#35268;&#27169;&#26102;&#24207;V2X&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#20174;&#33258;&#28982;&#26223;&#35266;&#20013;&#25429;&#33719;&#30340;&#25968;&#25454;&#24103;&#12289;&#36712;&#36857;&#12289;&#21521;&#37327;&#22320;&#22270;&#21644;&#20132;&#36890;&#28783;&#12290;V2X-Seq&#21253;&#25324;&#20004;&#20010;&#37096;&#20998;&#65306;&#26102;&#24207;&#24863;&#30693;&#25968;&#25454;&#38598;&#65292;&#21253;&#25324;&#26469;&#33258;95&#20010;&#22330;&#26223;&#30340;&#36229;&#36807;15,000&#20010;&#24103;&#65292;&#20197;&#21450;&#36712;&#36857;&#39044;&#27979;&#25968;&#25454;&#38598;&#65292;&#21253;&#21547;&#20174;28&#20010;&#20132;&#21449;&#21475;&#21306;&#22495;&#25429;&#33719;&#30340;&#32422;80,000&#20010;&#22522;&#30784;&#35774;&#26045;&#35270;&#22270;&#22330;&#26223;&#65292;80,000&#20010;&#36710;&#36742;&#35270;&#22270;&#22330;&#26223;&#21644;50,000&#20010;&#21327;&#21516;&#35270;&#22270;&#22330;&#26223;&#65292;&#28085;&#30422;&#20102;672&#23567;&#26102;&#30340;&#25968;&#25454;&#12290;&#22522;&#20110;V2X-Seq&#65292;&#25105;&#20204;&#24341;&#20837;&#20102;&#19977;&#20010;&#26032;&#30340;&#36710;&#36335;&#21327;&#21516;&#33258;&#21160;&#39550;&#39542;&#20219;&#21153;&#65306;VIC 3D&#36319;&#36394;&#65292;&#22312;&#32447;VIC&#39044;&#27979;&#21644;&#22810;&#27169;VIC&#39044;&#27979;&#12290;
&lt;/p&gt;
&lt;p&gt;
Utilizing infrastructure and vehicle-side information to track and forecast the behaviors of surrounding traffic participants can significantly improve decision-making and safety in autonomous driving. However, the lack of real-world sequential datasets limits research in this area. To address this issue, we introduce V2X-Seq, the first large-scale sequential V2X dataset, which includes data frames, trajectories, vector maps, and traffic lights captured from natural scenery. V2X-Seq comprises two parts: the sequential perception dataset, which includes more than 15,000 frames captured from 95 scenarios, and the trajectory forecasting dataset, which contains about 80,000 infrastructure-view scenarios, 80,000 vehicle-view scenarios, and 50,000 cooperative-view scenarios captured from 28 intersections' areas, covering 672 hours of data. Based on V2X-Seq, we introduce three new tasks for vehicle-infrastructure cooperative (VIC) autonomous driving: VIC3D Tracking, Online-VIC Forecasting, an
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; HCTM &#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#20026; 3D &#32593;&#26684;&#29983;&#25104;&#39640;&#28165;&#26224;&#24230;&#21644;&#19968;&#33268;&#30340;&#32441;&#29702;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#39044;&#35757;&#32451;&#28145;&#24230;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#25193;&#25955;&#31574;&#30053;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#21644;&#19968;&#33268;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#27490;&#22122;&#22768;&#20986;&#29616;&#30340;&#31574;&#30053;&#12290;</title><link>http://arxiv.org/abs/2305.05901</link><description>&lt;p&gt;
&#25991;&#26412;&#24341;&#23548;&#19979;&#30340;&#39640;&#28165;&#19968;&#33268;&#32441;&#29702;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Text-guided High-definition Consistency Texture Model. (arXiv:2305.05901v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05901
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#21517;&#20026; HCTM &#30340;&#26041;&#27861;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#20026; 3D &#32593;&#26684;&#29983;&#25104;&#39640;&#28165;&#26224;&#24230;&#21644;&#19968;&#33268;&#30340;&#32441;&#29702;&#12290;&#35813;&#26041;&#27861;&#32467;&#21512;&#39044;&#35757;&#32451;&#28145;&#24230;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#21644;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#65292;&#21033;&#29992;&#22810;&#25193;&#25955;&#31574;&#30053;&#29983;&#25104;&#39640;&#20998;&#36776;&#29575;&#21644;&#19968;&#33268;&#30340;&#32467;&#26524;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#31181;&#38450;&#27490;&#22122;&#22768;&#20986;&#29616;&#30340;&#31574;&#30053;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#38543;&#30528;&#28145;&#24230;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#30340;&#20986;&#29616;&#65292;&#36924;&#30495;&#32441;&#29702;&#30340;&#25991;&#26412;&#24341;&#23548;&#29983;&#25104;&#12289;&#32534;&#36753;&#21644;&#36716;&#31227;&#19981;&#20877;&#22256;&#38590;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#39044;&#35757;&#32451;&#25193;&#25955;&#27169;&#22411;&#30340;&#38480;&#21046;&#65292;&#23427;&#20204;&#21482;&#33021;&#21019;&#24314;&#20302;&#20998;&#36776;&#29575;&#12289;&#19981;&#19968;&#33268;&#30340;&#32441;&#29702;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#26041;&#27861;&#65306;&#39640;&#28165;&#19968;&#33268;&#32441;&#29702;&#27169;&#22411;&#65288;HCTM&#65289;&#65292;&#23427;&#21487;&#20197;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#20026; 3D &#32593;&#26684;&#29983;&#25104;&#39640;&#28165;&#26224;&#24230;&#21644;&#19968;&#33268;&#30340;&#32441;&#29702;&#12290;&#25105;&#20204;&#21033;&#29992;&#39044;&#35757;&#32451;&#30340;&#28145;&#24230;&#21040;&#22270;&#20687;&#25193;&#25955;&#27169;&#22411;&#26681;&#25454;&#25991;&#26412;&#25552;&#31034;&#21644;&#28145;&#24230;&#22270;&#29983;&#25104;&#21333;&#35270;&#28857;&#32467;&#26524;&#65292;&#21033;&#29992;&#21442;&#25968;&#39640;&#25928;&#24494;&#35843;&#26041;&#27861;&#23545;&#25193;&#25955;&#27169;&#22411;&#36827;&#34892;&#24494;&#35843;&#65292;&#20197;&#24555;&#36895;&#23398;&#20064;&#25152;&#29983;&#25104;&#32467;&#26524;&#30340;&#39118;&#26684;&#65292;&#24182;&#37319;&#29992;&#22810;&#25193;&#25955;&#31574;&#30053;&#20174;&#19981;&#21516;&#35270;&#28857;&#20135;&#29983;&#39640;&#20998;&#36776;&#29575;&#21644;&#19968;&#33268;&#30340;&#32467;&#26524;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#31574;&#30053;&#65292;&#38450;&#27490;&#30001;&#21453;&#21521;&#20256;&#25773;&#24341;&#36215;&#30340;&#32441;&#29702;&#19978;&#20986;&#29616;&#22122;&#22768;&#12290;
&lt;/p&gt;
&lt;p&gt;
With the advent of depth-to-image diffusion models, text-guided generation, editing, and transfer of realistic textures are no longer difficult. However, due to the limitations of pre-trained diffusion models, they can only create low-resolution, inconsistent textures. To address this issue, we present the High-definition Consistency Texture Model (HCTM), a novel method that can generate high-definition and consistent textures for 3D meshes according to the text prompts. We achieve this by leveraging a pre-trained depth-to-image diffusion model to generate single viewpoint results based on the text prompt and a depth map. We fine-tune the diffusion model with Parameter-Efficient Fine-Tuning to quickly learn the style of the generated result, and apply the multi-diffusion strategy to produce high-resolution and consistent results from different viewpoints. Furthermore, we propose a strategy that prevents the appearance of noise on the textures caused by backpropagation. Our proposed app
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#26684;&#28151;&#21512;&#30340;&#23574;&#23792;&#28436;&#21592;&#32593;&#32476;&#65292;&#36890;&#36807;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#21644;&#21160;&#24577;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#34701;&#21512;&#23454;&#29616;&#20102;&#22810;&#26041;&#21327;&#20316;&#20013;&#30340;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#12290;</title><link>http://arxiv.org/abs/2305.05898</link><description>&lt;p&gt;
&#22522;&#20110;&#20154;&#26684;&#28151;&#21512;&#30340;&#23574;&#23792;&#28436;&#21592;&#32593;&#32476;&#22312;&#39640;&#25928;&#22810;&#26041;&#21327;&#20316;&#20013;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Mixture of personality improved Spiking actor network for efficient multi-agent cooperation. (arXiv:2305.05898v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05898
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20154;&#26684;&#28151;&#21512;&#30340;&#23574;&#23792;&#28436;&#21592;&#32593;&#32476;&#65292;&#36890;&#36807;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#21644;&#21160;&#24577;&#23574;&#23792;&#31070;&#32463;&#20803;&#30340;&#34701;&#21512;&#23454;&#29616;&#20102;&#22810;&#26041;&#21327;&#20316;&#20013;&#30340;&#39640;&#25928;&#24378;&#21270;&#23398;&#20064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22810;&#26234;&#33021;&#20307;&#24378;&#21270;&#23398;&#20064;&#39046;&#22495;&#65292;&#33258;&#36866;&#24212;&#30340;&#20154;&#19982;&#26234;&#33021;&#20307;&#20197;&#21450;&#26234;&#33021;&#20307;&#20043;&#38388;&#30340;&#21327;&#20316;&#21464;&#24471;&#36234;&#26469;&#36234;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;MoP&#30340;&#28151;&#21512;&#20154;&#26684;&#25913;&#36827;&#23574;&#23792;&#28436;&#21592;&#32593;&#32476;&#12290;&#35813;&#32593;&#32476;&#20351;&#29992;&#34892;&#21015;&#24335;&#28857;&#36807;&#31243;&#26469;&#27169;&#25311;&#19981;&#21516;&#31867;&#22411;&#20154;&#26684;&#30340;&#22797;&#26434;&#32452;&#21512;&#21644;&#25972;&#21512;&#65292;&#24182;&#23558;&#21160;&#24577;&#21644;&#23574;&#23792;&#31070;&#32463;&#20803;&#34701;&#20837;&#21040;SAN&#20013;&#20197;&#23454;&#29616;&#39640;&#25928;&#30340;&#24378;&#21270;&#23398;&#20064;&#12290;&#22312;&#35780;&#20272;&#20013;&#65292;&#35813;&#26041;&#27861;&#22312;Overcooked&#22522;&#20934;&#27979;&#35797;&#20013;&#34920;&#29616;&#20986;&#33394;&#12290;
&lt;/p&gt;
&lt;p&gt;
Adaptive human-agent and agent-agent cooperation are becoming more and more critical in the research area of multi-agent reinforcement learning (MARL), where remarked progress has been made with the help of deep neural networks. However, many established algorithms can only perform well during the learning paradigm but exhibit poor generalization during cooperation with other unseen partners. The personality theory in cognitive psychology describes that humans can well handle the above cooperation challenge by predicting others' personalities first and then their complex actions. Inspired by this two-step psychology theory, we propose a biologically plausible mixture of personality (MoP) improved spiking actor network (SAN), whereby a determinantal point process is used to simulate the complex formation and integration of different types of personality in MoP, and dynamic and spiking neurons are incorporated into the SAN for the efficient reinforcement learning. The benchmark Overcooke
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#21644;GPT-4&#22312;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#22312;&#25968;&#20540;&#25512;&#29702;&#19978;&#34920;&#29616;&#20986;&#33394;&#20294;&#22312;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;</title><link>http://arxiv.org/abs/2305.05862</link><description>&lt;p&gt;
ChatGPT&#21644;GPT-4&#26159;&#21542;&#26159;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#30340;&#36890;&#29992;&#27714;&#35299;&#22120;&#65311;&#23545;&#20960;&#31181;&#20856;&#22411;&#20219;&#21153;&#36827;&#34892;&#26816;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;
Are ChatGPT and GPT-4 General-Purpose Solvers for Financial Text Analytics? An Examination on Several Typical Tasks. (arXiv:2305.05862v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05862
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25506;&#35752;&#20102;ChatGPT&#21644;GPT-4&#22312;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#20219;&#21153;&#20013;&#30340;&#28508;&#21147;&#65292;&#32467;&#26524;&#26174;&#31034;&#23427;&#20204;&#22312;&#25968;&#20540;&#25512;&#29702;&#19978;&#34920;&#29616;&#20986;&#33394;&#20294;&#22312;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22914;ChatGPT&#21644;GPT-4&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#65292;&#22240;&#20026;&#23427;&#20204;&#33021;&#22815;&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#23545;&#35805;&#22238;&#24212;&#12290;&#23613;&#31649;ChatGPT&#21644;GPT-4&#22312;&#36890;&#29992;&#25991;&#26412;&#35821;&#26009;&#24211;&#19978;&#32463;&#36807;&#20102;&#24191;&#27867;&#30340;&#27979;&#35797;&#65292;&#23637;&#31034;&#20102;&#23427;&#20204;&#20196;&#20154;&#21360;&#35937;&#28145;&#21051;&#30340;&#33021;&#21147;&#65292;&#20294;&#36824;&#27809;&#26377;&#23545;&#37329;&#34701;&#35821;&#26009;&#24211;&#36827;&#34892;&#30740;&#31350;&#12290;&#26412;&#30740;&#31350;&#26088;&#22312;&#36890;&#36807;&#22312;&#38646;&#26679;&#26412;&#25110;&#23569;&#26679;&#26412;&#24773;&#20917;&#19979;&#32771;&#23519;ChatGPT&#21644;GPT-4&#20316;&#20026;&#20856;&#22411;&#37329;&#34701;&#25991;&#26412;&#20998;&#26512;&#38382;&#39064;&#27714;&#35299;&#22120;&#30340;&#28508;&#21147;&#26469;&#24357;&#34917;&#36825;&#19968;&#24046;&#36317;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35780;&#20272;&#20102;&#23427;&#20204;&#22312;&#20116;&#20010;&#19981;&#21516;&#30340;&#37329;&#34701;&#25991;&#26412;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#30340;&#22235;&#39033;&#20195;&#34920;&#24615;&#20219;&#21153;&#30340;&#33021;&#21147;&#12290;&#21021;&#27493;&#30740;&#31350;&#34920;&#26126;&#65292;ChatGPT&#21644;GPT-4&#22312;&#38656;&#35201;&#39046;&#22495;&#29305;&#23450;&#30693;&#35782;&#30340;&#37329;&#34701;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#65288;NER&#65289;&#21644;&#24773;&#24863;&#20998;&#26512;&#31561;&#20219;&#21153;&#19978;&#34920;&#29616;&#19981;&#20339;&#65292;&#32780;&#22312;&#25968;&#20540;&#25512;&#29702;&#20219;&#21153;&#19978;&#34920;&#29616;&#20986;&#33394;&#12290;&#25105;&#20204;&#25253;&#21578;&#20102;&#24403;&#21069;&#29256;&#26412;ChatGPT&#21644;GPT-4&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#65292;&#24182;&#23558;&#23427;&#20204;&#19982;&#29616;&#26377;&#25216;&#26415;&#36827;&#34892;&#20102;&#27604;&#36739;&#12290;
&lt;/p&gt;
&lt;p&gt;
The most recent large language models such as ChatGPT and GPT-4 have garnered significant attention, as they are capable of generating high-quality responses to human input. Despite the extensive testing of ChatGPT and GPT-4 on generic text corpora, showcasing their impressive capabilities, a study focusing on financial corpora has not been conducted. In this study, we aim to bridge this gap by examining the potential of ChatGPT and GPT-4 as a solver for typical financial text analytic problems in the zero-shot or few-shot setting. Specifically, we assess their capabilities on four representative tasks over five distinct financial textual datasets. The preliminary study shows that ChatGPT and GPT-4 struggle on tasks such as financial named entity recognition (NER) and sentiment analysis, where domain-specific knowledge is required, while they excel in numerical reasoning tasks. We report both the strengths and limitations of the current versions of ChatGPT and GPT-4, comparing them to 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#24847;&#22270;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#21382;&#21490;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#26080;&#27861;&#25512;&#33616;&#26032;&#20135;&#21697;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05848</link><description>&lt;p&gt;
&#22522;&#20110;&#21452;&#24847;&#22270;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20250;&#35805;&#26032;&#21697;&#25512;&#33616;
&lt;/p&gt;
&lt;p&gt;
Dual Intent Enhanced Graph Neural Network for Session-based New Item Recommendation. (arXiv:2305.05848v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05848
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21452;&#24847;&#22270;&#22686;&#24378;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#29992;&#20110;&#35299;&#20915;&#22522;&#20110;&#21382;&#21490;&#20250;&#35805;&#30340;&#25512;&#33616;&#31995;&#32479;&#20013;&#26080;&#27861;&#25512;&#33616;&#26032;&#20135;&#21697;&#30340;&#38382;&#39064;&#65292;&#24182;&#22312;&#23454;&#39564;&#20013;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25512;&#33616;&#31995;&#32479;&#26159;&#21508;&#20010;&#39046;&#22495;&#65292;&#20363;&#22914;&#30005;&#23376;&#21830;&#21153;&#12289;&#30005;&#23376;&#23398;&#20064;&#21644;&#27969;&#23186;&#20307;&#31561;&#20013;&#19981;&#21487;&#25110;&#32570;&#30340;&#37096;&#20998;&#12290;&#30446;&#21069;&#65292;&#29992;&#20110;&#20250;&#35805;&#25512;&#33616;&#30340;&#22270;&#31070;&#32463;&#32593;&#32476;&#36890;&#24120;&#21482;&#33021;&#25512;&#33616;&#29992;&#25143;&#21382;&#21490;&#20250;&#35805;&#20013;&#24050;&#23384;&#22312;&#30340;&#39033;&#30446;&#12290;&#22240;&#27492;&#36825;&#20123;&#22270;&#31070;&#32463;&#32593;&#32476;&#22312;&#25512;&#33616;&#29992;&#25143;&#20174;&#26410;&#19982;&#20043;&#20132;&#20114;&#30340;&#39033;&#30446;&#65288;&#26032;&#20135;&#21697;&#65289;&#26102;&#38754;&#20020;&#20449;&#24687;&#23553;&#38381;&#30340;&#38382;&#39064;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#21521;&#29992;&#25143;&#25512;&#33616;&#26032;&#30340;&#20135;&#21697;&#12290;&#30001;&#20110;&#26032;&#20135;&#21697;&#19982;&#29992;&#25143;&#20043;&#38388;&#27809;&#26377;&#20132;&#20114;&#65292;&#22240;&#27492;&#22312;&#26500;&#24314;&#22522;&#20110;&#22270;&#31070;&#32463;&#32593;&#32476;&#30340;&#20250;&#35805;&#25512;&#33616;&#31995;&#32479;&#26102;&#65292;&#25105;&#20204;&#19981;&#33021;&#23558;&#26032;&#20135;&#21697;&#21253;&#25324;&#22312;&#20250;&#35805;&#22270;&#20013;&#12290;&#22240;&#27492;&#65292;&#20351;&#29992;&#22522;&#20110;GNN&#65288;&#22270;&#31070;&#32463;&#32593;&#32476;&#65289;&#30340;&#26041;&#27861;&#21521;&#29992;&#25143;&#25512;&#33616;&#26032;&#20135;&#21697;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21452;&#24847;&#22270;&#22686;&#24378;&#22270;&#31070;&#32463;&#32593;&#32476;&#12290;&#30001;&#20110;&#26032;&#30340;&#20135;&#21697;&#19982;&#21382;&#21490;&#20250;&#35805;&#27809;&#26377;&#32465;&#23450;&#20851;&#31995;&#65292;&#27169;&#22411;&#23398;&#20064;&#22914;&#20309;&#25429;&#25417;&#29992;&#25143;&#21644;&#39033;&#30446;&#30340;&#24847;&#22270;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#30340;&#27169;&#22411;&#24341;&#20837;&#20102;&#21452;&#24847;&#22270;&#22270;&#31070;&#32463;&#32593;&#32476;&#65292;&#19968;&#24352;&#22270;&#26469;&#25429;&#33719;&#39033;&#30446;&#24847;&#22270;&#30340;&#34920;&#31034;&#65292;&#21478;&#19968;&#24352;&#22270;&#26469;&#25429;&#33719;&#29992;&#25143;&#24847;&#22270;&#30340;&#34920;&#31034;&#12290;&#35813;&#27169;&#22411;&#23558;&#21382;&#21490;&#20250;&#35805;&#19982;&#29992;&#25143;&#21644;&#39033;&#30446;&#21452;&#37325;&#24847;&#22270;&#34920;&#31034;&#30456;&#32467;&#21512;&#65292;&#29992;&#20110;&#25512;&#33616;&#12290;&#20004;&#20010;&#30495;&#23454;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recommender systems are essential to various fields, e.g., e-commerce, e-learning, and streaming media. At present, graph neural networks (GNNs) for session-based recommendations normally can only recommend items existing in users' historical sessions. As a result, these GNNs have difficulty recommending items that users have never interacted with (new items), which leads to a phenomenon of information cocoon. Therefore, it is necessary to recommend new items to users. As there is no interaction between new items and users, we cannot include new items when building session graphs for GNN session-based recommender systems. Thus, it is challenging to recommend new items for users when using GNN-based methods. We regard this challenge as '\textbf{G}NN \textbf{S}ession-based \textbf{N}ew \textbf{I}tem \textbf{R}ecommendation (GSNIR)'. To solve this problem, we propose a dual-intent enhanced graph neural network for it. Due to the fact that new items are not tied to historical sessions, the
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#19982;ControlNet&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21033;&#29992;&#25554;&#20540;&#24103;&#35270;&#39057;&#20316;&#20026;&#25511;&#21046;&#25216;&#26415;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#19968;&#33268;&#30340;&#35270;&#39057;&#20869;&#23481;&#65292;&#26356;&#20934;&#30830;&#22320;&#31526;&#21512;&#29992;&#25143;&#23545;&#35270;&#39057;&#20013;&#20027;&#20307;&#36816;&#21160;&#30340;&#24847;&#22270;&#12290;</title><link>http://arxiv.org/abs/2305.05845</link><description>&lt;p&gt;
&#30011;&#20986;&#26410;&#26469;&#65306;&#23558;&#26465;&#20214;&#25511;&#21046;&#25216;&#26415;&#24212;&#29992;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#27169;&#22411;&#20013;
&lt;/p&gt;
&lt;p&gt;
Sketching the Future (STF): Applying Conditional Control Techniques to Text-to-Video Models. (arXiv:2305.05845v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05845
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#23558;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#19982;ControlNet&#30456;&#32467;&#21512;&#30340;&#26032;&#39062;&#26041;&#27861;&#65292;&#21033;&#29992;&#25554;&#20540;&#24103;&#35270;&#39057;&#20316;&#20026;&#25511;&#21046;&#25216;&#26415;&#29983;&#25104;&#39640;&#36136;&#37327;&#19988;&#19968;&#33268;&#30340;&#35270;&#39057;&#20869;&#23481;&#65292;&#26356;&#20934;&#30830;&#22320;&#31526;&#21512;&#29992;&#25143;&#23545;&#35270;&#39057;&#20013;&#20027;&#20307;&#36816;&#21160;&#30340;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20869;&#23481;&#30340;&#29190;&#28856;&#24335;&#22686;&#38271;&#23545;&#20110;&#29983;&#25104;&#26032;&#30340;&#35270;&#39057;&#20869;&#23481;&#38656;&#35201;&#39640;&#25928;&#28789;&#27963;&#30340;&#31070;&#32463;&#32593;&#32476;&#26041;&#27861;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#26041;&#27861;&#65292;&#23558;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#19982;ControlNet&#30456;&#32467;&#21512;&#65292;&#20174;&#32780;&#25913;&#21892;&#27169;&#22411;&#30340;&#36755;&#20986;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#23558;&#22810;&#20010;&#33609;&#22270;&#24103;&#20316;&#20026;&#36755;&#20837;&#65292;&#24182;&#29983;&#25104;&#31526;&#21512;&#36825;&#20123;&#24103;&#27969;&#31243;&#30340;&#35270;&#39057;&#36755;&#20986;&#65292;&#22522;&#20110;&#25991;&#26412;&#21040;&#35270;&#39057;&#38646;&#26550;&#26500;&#65292;&#32467;&#21512;ControlNet&#20197;&#21551;&#29992;&#39069;&#22806;&#30340;&#36755;&#20837;&#26465;&#20214;&#12290;&#36890;&#36807;&#39318;&#20808;&#25554;&#20540;&#36755;&#20837;&#33609;&#22270;&#20043;&#38388;&#30340;&#24103;&#65292;&#28982;&#21518;&#20351;&#29992;&#26032;&#30340;&#25554;&#20540;&#24103;&#35270;&#39057;&#20316;&#20026;&#25511;&#21046;&#25216;&#26415;&#26469;&#36816;&#34892;&#25991;&#26412;&#21040;&#35270;&#39057;&#38646;&#65292;&#25105;&#20204;&#21033;&#29992;&#20102;&#38646;&#26679;&#26412;&#25991;&#26412;&#21040;&#35270;&#39057;&#29983;&#25104;&#21644;ControlNet&#25552;&#20379;&#30340;&#24378;&#22823;&#25511;&#21046;&#30340;&#20248;&#21183;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#25797;&#38271;&#29983;&#25104;&#39640;&#36136;&#37327;&#65292;&#38750;&#24120;&#19968;&#33268;&#30340;&#35270;&#39057;&#20869;&#23481;&#65292;&#26356;&#20934;&#30830;&#22320;&#19982;&#29992;&#25143;&#23545;&#35270;&#39057;&#20013;&#20027;&#20307;&#36816;&#21160;&#30340;&#24847;&#22270;&#30456;&#21563;&#21512;&#12290;
&lt;/p&gt;
&lt;p&gt;
The proliferation of video content demands efficient and flexible neural network based approaches for generating new video content. In this paper, we propose a novel approach that combines zero-shot text-to-video generation with ControlNet to improve the output of these models. Our method takes multiple sketched frames as input and generates video output that matches the flow of these frames, building upon the Text-to-Video Zero architecture and incorporating ControlNet to enable additional input conditions. By first interpolating frames between the inputted sketches and then running Text-to-Video Zero using the new interpolated frames video as the control technique, we leverage the benefits of both zero-shot text-to-video generation and the robust control provided by ControlNet. Experiments demonstrate that our method excels at producing high-quality and remarkably consistent video content that more accurately aligns with the user's intended motion for the subject within the video. We
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#20013;&#22810;&#26679;&#24615;&#30340;&#27010;&#24565;&#21450;&#20854;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#29983;&#25104;&#22810;&#31181;&#22240;&#26524;&#20851;&#31995;&#20363;&#23376;&#20197;&#35299;&#37322;&#19968;&#20010;&#39044;&#27979;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05840</link><description>&lt;p&gt;
&#23454;&#29616;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#30340;&#22810;&#26679;&#24615;&#65306;&#19968;&#39033;&#32508;&#36848;&#21644;&#35752;&#35770;
&lt;/p&gt;
&lt;p&gt;
Achieving Diversity in Counterfactual Explanations: a Review and Discussion. (arXiv:2305.05840v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05840
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#32508;&#36848;&#20102;&#22240;&#26524;&#20851;&#31995;&#35299;&#37322;&#20013;&#22810;&#26679;&#24615;&#30340;&#27010;&#24565;&#21450;&#20854;&#23450;&#20041;&#65292;&#25552;&#20986;&#20102;&#29983;&#25104;&#22810;&#31181;&#22240;&#26524;&#20851;&#31995;&#20363;&#23376;&#20197;&#35299;&#37322;&#19968;&#20010;&#39044;&#27979;&#32467;&#26524;&#30340;&#26041;&#27861;&#65292;&#25506;&#35752;&#20102;&#36825;&#31181;&#26041;&#27861;&#30340;&#20248;&#28857;&#21644;&#23616;&#38480;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21487;&#35299;&#37322;&#30340;&#20154;&#24037;&#26234;&#33021;&#39046;&#22495;&#20013;&#65292;&#22240;&#26524;&#20851;&#31995;&#20363;&#23376;&#36890;&#36807;&#25351;&#20986;&#26356;&#25913;&#23454;&#20363;&#20197;&#26356;&#25913;&#20854;&#39044;&#27979;&#30340;&#20462;&#25913;&#26469;&#35299;&#37322;&#21463;&#36807;&#35757;&#32451;&#30340;&#20915;&#31574;&#27169;&#22411;&#30340;&#39044;&#27979;&#32467;&#26524;&#12290;&#36825;&#20123;&#22240;&#26524;&#20851;&#31995;&#20363;&#23376;&#36890;&#24120;&#23450;&#20041;&#20026;&#35299;&#20915;&#19968;&#20010;&#20248;&#21270;&#38382;&#39064;&#30340;&#35299;&#20915;&#26041;&#26696;&#65292;&#20854;&#20013;&#20195;&#20215;&#20989;&#25968;&#32467;&#21512;&#20102;&#20960;&#20010;&#34913;&#37327;&#28385;&#36275;&#29992;&#25143;&#38656;&#27714;&#30340;&#33391;&#22909;&#35299;&#37322;&#30340;&#26631;&#20934;&#12290;&#21487;&#20197;&#32771;&#34385;&#22810;&#31181;&#36825;&#26679;&#36866;&#24403;&#30340;&#24615;&#36136;&#65292;&#22240;&#20026;&#29992;&#25143;&#38656;&#27714;&#36890;&#24120;&#26159;&#26410;&#30693;&#30340;&#65292;&#32780;&#19988;&#19981;&#21516;&#29992;&#25143;&#20043;&#38388;&#20063;&#26377;&#25152;&#19981;&#21516;&#65307;&#23427;&#20204;&#30340;&#36873;&#25321;&#21644;&#35268;&#33539;&#21270;&#26159;&#22256;&#38590;&#30340;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#19968;&#20123;&#26041;&#27861;&#25552;&#20986;&#29983;&#25104;&#19968;&#32452;&#19981;&#21516;&#30340;&#22240;&#26524;&#20851;&#31995;&#20363;&#23376;&#26469;&#35299;&#37322;&#19968;&#20010;&#39044;&#27979;&#32467;&#26524;&#65292;&#32780;&#19981;&#26159;&#21333;&#20010;&#30340;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#23545;&#36825;&#20010;&#22810;&#26679;&#24615;&#27010;&#24565;&#30340;&#35768;&#22810;&#12289;&#26377;&#26102;&#30456;&#20114;&#30683;&#30462;&#30340;&#23450;&#20041;&#30340;&#32508;&#36848;&#12290;&#23427;&#35752;&#35770;&#20102;&#23427;&#20204;&#30340;&#22522;&#26412;&#21407;&#21017;&#12289;&#20551;&#35774;&#12289;&#20248;&#21183;&#21644;&#23616;&#38480;&#24615;&#12290;&#27492;&#22806;&#65292;&#23427;&#20171;&#32461;&#20102;&#19968;&#20123;&#20851;&#20110;&#36825;&#20010;&#29305;&#23450;&#30740;&#31350;&#20027;&#39064;&#30340;&#26368;&#26032;&#24037;&#20316;&#21644;&#26410;&#26469;&#21457;&#23637;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the field of Explainable Artificial Intelligence (XAI), counterfactual examples explain to a user the predictions of a trained decision model by indicating the modifications to be made to the instance so as to change its associated prediction. These counterfactual examples are generally defined as solutions to an optimization problem whose cost function combines several criteria that quantify desiderata for a good explanation meeting user needs. A large variety of such appropriate properties can be considered, as the user needs are generally unknown and differ from one user to another; their selection and formalization is difficult. To circumvent this issue, several approaches propose to generate, rather than a single one, a set of diverse counterfactual examples to explain a prediction. This paper proposes a review of the numerous, sometimes conflicting, definitions that have been proposed for this notion of diversity. It discusses their underlying principles as well as the hypothe
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#30340;&#20195;&#29702;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20197;&#24212;&#23545;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05832</link><description>&lt;p&gt;
&#22240;&#26524;&#20449;&#24687;&#20998;&#31163;&#65306;&#20026;&#25239;&#20998;&#24067;&#36716;&#31227;&#35774;&#35745;&#20195;&#29702;&#29305;&#24449;
&lt;/p&gt;
&lt;p&gt;
Causal Information Splitting: Engineering Proxy Features for Robustness to Distribution Shifts. (arXiv:2305.05832v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05832
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#30340;&#20195;&#29702;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#29992;&#20197;&#24212;&#23545;&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#22312;&#20998;&#24067;&#36716;&#31227;&#24773;&#20917;&#19979;&#30340;&#31283;&#23450;&#24615;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32479;&#35745;&#39044;&#27979;&#27169;&#22411;&#36890;&#24120;&#26159;&#22312;&#19982;&#26368;&#32456;&#20351;&#29992;&#24773;&#20917;&#19981;&#21516;&#30340;&#27010;&#29575;&#20998;&#24067;&#20013;&#36827;&#34892;&#35757;&#32451;&#30340;&#12290;&#20026;&#20102;&#39044;&#27979;&#20998;&#24067;&#36716;&#31227;&#65292;&#26377;&#19968;&#31181;&#26041;&#27861;&#26159;&#21033;&#29992;&#22240;&#26524;&#26426;&#21046;&#22312;&#19981;&#21516;&#29615;&#22659;&#19979;&#20445;&#25345;&#19981;&#21464;&#30340;&#30452;&#35273;&#26469;&#20027;&#21160;&#20934;&#22791;&#12290;&#26412;&#25991;&#38024;&#23545;&#19968;&#20010;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#22330;&#26223;&#65292;&#20854;&#20013;&#30446;&#26631;&#30340;&#22240;&#26524;&#21644;&#21453;&#22240;&#26524;&#21464;&#37327;&#37117;&#26159;&#26410;&#34987;&#35266;&#23519;&#21040;&#30340;&#12290;&#21033;&#29992;&#20449;&#24687;&#35770;&#65292;&#25105;&#20204;&#20026;&#19979;&#28216;&#35266;&#27979;&#21464;&#37327;&#24320;&#21457;&#20102;&#29305;&#24449;&#36873;&#25321;&#21644;&#24037;&#31243;&#25216;&#26415;&#65292;&#36825;&#20123;&#21464;&#37327;&#20805;&#24403;&#20195;&#29702;&#12290;&#25105;&#20204;&#36873;&#25321;&#26377;&#21161;&#20110;&#24314;&#31435;&#31283;&#23450;&#27169;&#22411;&#30340;&#20195;&#29702;&#65292;&#24182;&#20351;&#29992;&#36741;&#21161;&#35757;&#32451;&#20219;&#21153;&#20174;&#20195;&#29702;&#20013;&#25552;&#21462;&#22686;&#24378;&#31283;&#23450;&#24615;&#30340;&#20449;&#24687;&#12290;&#25105;&#20204;&#22312;&#21512;&#25104;&#25968;&#25454;&#21644;&#30495;&#23454;&#25968;&#25454;&#19978;&#23637;&#31034;&#20102;&#25105;&#20204;&#25216;&#26415;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Statistical prediction models are often trained on data that is drawn from different probability distributions than their eventual use cases. One approach to proactively prepare for these shifts harnesses the intuition that causal mechanisms should remain invariant between environments. Here we focus on a challenging setting in which the causal and anticausal variables of the target are unobserved. Leaning on information theory, we develop feature selection and engineering techniques for the observed downstream variables that act as proxies. We identify proxies that help to build stable models and moreover utilize auxiliary training tasks to extract stability-enhancing information from proxies. We demonstrate the effectiveness of our techniques on synthetic and real data.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#35789;&#27719;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#21387;&#21147;&#20419;&#36827;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#30340;&#20986;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25509;&#25910;&#32773;&#26080;&#27861;&#22788;&#29702;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#36865;&#32773;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#30340;&#21046;&#32422;&#22240;&#32032;&#23454;&#29616;&#27807;&#36890;&#12290;</title><link>http://arxiv.org/abs/2305.05821</link><description>&lt;p&gt;
&#29615;&#22659;&#32422;&#26463;&#19979;&#30340;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;
&lt;/p&gt;
&lt;p&gt;
Context-dependent communication under environmental constraints. (arXiv:2305.05821v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05821
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#21387;&#32553;&#35789;&#27719;&#37327;&#30340;&#24773;&#20917;&#19979;&#65292;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#21387;&#21147;&#20419;&#36827;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#30340;&#20986;&#29616;&#65292;&#24182;&#30740;&#31350;&#20102;&#22312;&#25509;&#25910;&#32773;&#26080;&#27861;&#22788;&#29702;&#27495;&#20041;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#36865;&#32773;&#22914;&#20309;&#21033;&#29992;&#29615;&#22659;&#30340;&#21046;&#32422;&#22240;&#32032;&#23454;&#29616;&#27807;&#36890;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#23384;&#22312;&#22823;&#37327;&#30340;&#35777;&#25454;&#34920;&#26126;&#65292;&#29616;&#23454;&#19990;&#30028;&#20013;&#30340;&#27807;&#36890;&#19981;&#33021;&#31616;&#21333;&#22320;&#36890;&#36807;&#21457;&#36865;&#20855;&#26377;&#29420;&#31435;&#20110;&#24773;&#22659;&#24847;&#20041;&#30340;&#20449;&#21495;&#26469;&#23454;&#29616;&#12290;&#26412;&#25991;&#20197;&#32463;&#20856;&#30340;Lewis(1969)&#20449;&#21495;&#27169;&#22411;&#30340;&#21464;&#20307;&#20026;&#22522;&#30784;&#65292;&#25506;&#35752;&#22312;&#24773;&#22659;&#21270;&#22330;&#26223;&#19979;&#20135;&#29983;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#30340;&#26465;&#20214;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;&#25105;&#20204;&#35777;&#26126;&#20102;&#22312;&#26368;&#23567;&#21270;&#35789;&#27719;&#37327;&#30340;&#21387;&#21147;&#19979;&#65292;&#36825;&#31181;&#27807;&#36890;&#30340;&#20986;&#29616;&#26159;&#36275;&#22815;&#30340;&#12290;&#21516;&#26102;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#21487;&#33021;&#20351;&#31526;&#21495;&#21547;&#20041;&#24471;&#21040;&#24773;&#22659;&#21306;&#20998;&#30340;&#29615;&#22659;&#26465;&#20214;&#21644;&#35748;&#30693;&#33021;&#21147;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#22312;&#25509;&#21463;&#32773;&#30340;&#25351;&#20195;&#36873;&#25321;&#21463;&#21040;&#29615;&#22659;&#38480;&#21046;&#30340;&#24773;&#20917;&#19979;&#65292;&#21457;&#36865;&#32773;&#21487;&#20197;&#21333;&#26041;&#38754;&#22320;&#21033;&#29992;&#36825;&#20123;&#38480;&#21046;&#65292;&#32780;&#26080;&#38656;&#25509;&#25910;&#32773;&#20855;&#26377;&#28548;&#28165;&#27495;&#20041;&#30340;&#33021;&#21147;&#12290;&#19982;&#24120;&#35265;&#30340;&#20551;&#35774;&#19968;&#33268;&#65292;&#21457;&#36865;&#32773;&#23545;&#24773;&#22659;&#30340;&#24847;&#35782;&#20284;&#20046;&#26159;&#38656;&#35201;&#30340;&#12290;&#25105;&#20204;&#35748;&#20026;&#65292;&#24773;&#22659;&#20381;&#36182;&#24615;&#27807;&#36890;&#26159;&#19968;&#31181;&#22810;&#23618;&#27425;&#30340;&#24773;&#22659;&#21270;&#29616;&#35937;&#65292;&#20854;&#21463;&#29615;&#22659;&#29305;&#24615;&#30340;&#24433;&#21709;&#33267;&#20851;&#37325;&#35201;&#12290;
&lt;/p&gt;
&lt;p&gt;
There is significant evidence that real-world communication cannot be reduced to sending signals with context-independent meaning. In this work, based on a variant of the classical Lewis (1969) signaling model, we explore the conditions for the emergence of context-dependent communication in a situated scenario. In particular, we demonstrate that pressure to minimise the vocabulary size is sufficient for such emergence. At the same time, we study the environmental conditions and cognitive capabilities that enable contextual disambiguation of symbol meanings. We show that environmental constraints on the receiver's referent choice can be unilaterally exploited by the sender, without disambiguation capabilities on the receiver's end. Consistent with common assumptions, the sender's awareness of the context appears to be required for contextual communication. We suggest that context-dependent communication is a situated multilayered phenomenon, crucially influenced by environment properti
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#36741;&#21161;&#33258;&#21160;&#29983;&#25104;&#20248;&#21270;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20248;&#21270;&#27169;&#22411;&#26159;&#21487;&#34892;&#30340;&#12290;</title><link>http://arxiv.org/abs/2305.05811</link><description>&lt;p&gt;
&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#36741;&#21161;&#33258;&#21160;&#29983;&#25104;&#20248;&#21270;&#27169;&#22411;&#30340;&#27169;&#22411;&#29983;&#25104;&#22120;
&lt;/p&gt;
&lt;p&gt;
Towards an Automatic Optimisation Model Generator Assisted with Generative Pre-trained Transformer. (arXiv:2305.05811v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05811
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#21033;&#29992;&#29983;&#25104;&#24335;&#39044;&#35757;&#32451;&#21464;&#25442;&#22120;&#36741;&#21161;&#33258;&#21160;&#29983;&#25104;&#20248;&#21270;&#27169;&#22411;&#30340;&#26694;&#26550;&#65292;&#24182;&#36827;&#34892;&#20102;&#23454;&#39564;&#39564;&#35777;&#65292;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20248;&#21270;&#27169;&#22411;&#26159;&#21487;&#34892;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#39044;&#35757;&#32451;&#29983;&#25104;&#24335;&#21464;&#25442;&#22120;&#26469;&#29983;&#25104;&#20248;&#21270;&#27169;&#22411;&#30340;&#26694;&#26550;&#12290;&#35813;&#26694;&#26550;&#28041;&#21450;&#25351;&#23450;&#20248;&#21270;&#27169;&#22411;&#24212;&#20855;&#26377;&#30340;&#29305;&#24449;&#65292;&#24182;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#27169;&#22411;&#30340;&#21021;&#22987;&#29256;&#26412;&#12290;&#28982;&#21518;&#65292;&#27169;&#22411;&#36827;&#34892;&#27979;&#35797;&#21644;&#39564;&#35777;&#65292;&#22914;&#26524;&#21253;&#21547;&#26500;&#24314;&#38169;&#35823;&#65292;&#21017;&#35302;&#21457;&#33258;&#21160;&#32534;&#36753;&#36807;&#31243;&#12290;&#20351;&#29992;MiniZinc&#20316;&#20026;&#30446;&#26631;&#35821;&#35328;&#21644;&#20004;&#20010;GPT-3.5&#35821;&#35328;&#27169;&#22411;&#36827;&#34892;&#20102;&#23454;&#39564;&#26469;&#36827;&#34892;&#29983;&#25104;&#21644;&#35843;&#35797;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#20351;&#29992;&#35821;&#35328;&#27169;&#22411;&#29983;&#25104;&#20248;&#21270;&#27169;&#22411;&#26159;&#21487;&#34892;&#30340;&#65292;&#26377;&#20123;&#27169;&#22411;&#28385;&#36275;&#25152;&#38656;&#30340;&#35268;&#26684;&#65292;&#32780;&#20854;&#20182;&#27169;&#22411;&#38656;&#35201;&#36827;&#19968;&#27493;&#32454;&#21270;&#12290;&#35813;&#30740;&#31350;&#20026;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#26469;&#24314;&#27169;&#20248;&#21270;&#38382;&#39064;&#25552;&#20379;&#20102;&#26377;&#24076;&#26395;&#30340;&#35777;&#25454;&#65292;&#24182;&#25552;&#20986;&#20102;&#26410;&#26469;&#30740;&#31350;&#30340;&#26041;&#21521;&#12290;
&lt;/p&gt;
&lt;p&gt;
This article presents a framework for generating optimisation models using a pre-trained generative transformer. The framework involves specifying the features that the optimisation model should have and using a language model to generate an initial version of the model. The model is then tested and validated, and if it contains build errors, an automatic edition process is triggered. An experiment was performed using MiniZinc as the target language and two GPT-3.5 language models for generation and debugging. The results show that the use of language models for the generation of optimisation models is feasible, with some models satisfying the requested specifications, while others require further refinement. The study provides promising evidence for the use of language models in the modelling of optimisation problems and suggests avenues for future research.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#21327;&#35758;&#26469;&#20998;&#26512;&#22810;&#26679;&#24615;&#21464;&#21270;&#21644;&#30456;&#20851;&#24615;&#21464;&#21270;&#12290;&#20351;&#29992;&#30382;&#32932;&#30284;&#20998;&#26512;&#20998;&#31867;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#21457;&#29616;&#27169;&#22411;&#19981;&#20165;&#20250;&#23398;&#20064;&#21644;&#20256;&#25773;&#30456;&#20851;&#24615;&#21464;&#21270;&#65292;&#32780;&#19988;&#21487;&#33021;&#20250;&#20351;&#29992;&#38169;&#35823;&#30340;&#29305;&#24449;&#12290;</title><link>http://arxiv.org/abs/2305.05807</link><description>&lt;p&gt;
&#21363;&#20351;&#24456;&#23567;&#30340;&#30456;&#20851;&#24615;&#21644;&#22810;&#26679;&#24615;&#21464;&#21270;&#20063;&#20250;&#23548;&#33268;&#25968;&#25454;&#38598;&#20559;&#24046;&#38382;&#39064;
&lt;/p&gt;
&lt;p&gt;
Even Small Correlation and Diversity Shifts Pose Dataset-Bias Issues. (arXiv:2305.05807v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05807
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20102;&#25968;&#25454;&#38598;&#20013;&#30340;&#20998;&#24067;&#21464;&#21270;&#23545;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24433;&#21709;&#65292;&#24182;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#21327;&#35758;&#26469;&#20998;&#26512;&#22810;&#26679;&#24615;&#21464;&#21270;&#21644;&#30456;&#20851;&#24615;&#21464;&#21270;&#12290;&#20351;&#29992;&#30382;&#32932;&#30284;&#20998;&#26512;&#20998;&#31867;&#38382;&#39064;&#30340;&#23454;&#20363;&#65292;&#21457;&#29616;&#27169;&#22411;&#19981;&#20165;&#20250;&#23398;&#20064;&#21644;&#20256;&#25773;&#30456;&#20851;&#24615;&#21464;&#21270;&#65292;&#32780;&#19988;&#21487;&#33021;&#20250;&#20351;&#29992;&#38169;&#35823;&#30340;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20998;&#24067;&#21464;&#21270;&#22312;&#23454;&#38469;&#25968;&#25454;&#38598;&#20013;&#24456;&#24120;&#35265;&#65292;&#20250;&#24433;&#21709;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#30340;&#24615;&#33021;&#21644;&#21487;&#38752;&#24615;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#20004;&#31181;&#31867;&#22411;&#30340;&#20998;&#24067;&#21464;&#21270;&#65306;&#22810;&#26679;&#24615;&#21464;&#21270;&#21644;&#30456;&#20851;&#24615;&#21464;&#21270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#32508;&#21512;&#21327;&#35758;&#65292;&#20351;&#29992;&#21516;&#26102;&#23384;&#22312;&#36825;&#20004;&#31181;&#21464;&#21270;&#30340;&#25968;&#25454;&#38598;&#26469;&#20998;&#26512;&#23427;&#20204;&#12290;&#26368;&#21518;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;&#26041;&#27861;&#24212;&#29992;&#20110;&#19968;&#20010;&#30495;&#23454;&#30340;&#30382;&#32932;&#30284;&#20998;&#26512;&#20998;&#31867;&#38382;&#39064;&#20013;&#65292;&#20351;&#29992;&#20102;&#36229;&#20986;&#25968;&#25454;&#38598;&#21644;&#19987;&#38376;&#30340;&#20559;&#24046;&#27880;&#37322;&#12290;&#25105;&#20204;&#30340;&#21327;&#35758;&#25581;&#31034;&#20102;&#19977;&#20010;&#21457;&#29616;&#65306;1&#65289;&#27169;&#22411;&#21363;&#20351;&#36827;&#34892;&#20102;&#20302;&#20559;&#24046;&#35757;&#32451;&#20063;&#20250;&#23398;&#20064;&#24182;&#20256;&#25773;&#30456;&#20851;&#24615;&#21464;&#21270;&#65292;&#36825;&#21487;&#33021;&#20250;&#32047;&#31215;&#21644;&#32467;&#21512;&#38590;&#20197;&#35299;&#37322;&#30340;&#24369;&#20559;&#24046;&#30340;&#39118;&#38505;&#65307;2&#65289;&#27169;&#22411;&#22312;&#39640;&#12289;&#20302;&#20559;&#24046;&#24773;&#20917;&#19979;&#21487;&#20197;&#23398;&#20064;&#21040;&#31283;&#20581;&#30340;&#29305;&#24449;&#65292;&#20294;&#26159;&#22914;&#26524;&#27979;&#35797;&#26679;&#26412;&#26377;&#38169;&#35823;&#30340;&#29305;&#24449;&#23427;&#20204;&#21487;&#33021;&#20250;&#20351;&#29992;&#36825;&#20123;&#29305;&#24449;&#12290;
&lt;/p&gt;
&lt;p&gt;
Distribution shifts are common in real-world datasets and can affect the performance and reliability of deep learning models. In this paper, we study two types of distribution shifts: diversity shifts, which occur when test samples exhibit patterns unseen during training, and correlation shifts, which occur when test data present a different correlation between seen invariant and spurious features. We propose an integrated protocol to analyze both types of shifts using datasets where they co-exist in a controllable manner. Finally, we apply our approach to a real-world classification problem of skin cancer analysis, using out-of-distribution datasets and specialized bias annotations. Our protocol reveals three findings: 1) Models learn and propagate correlation shifts even with low-bias training; this poses a risk of accumulating and combining unaccountable weak biases; 2) Models learn robust features in highand low-bias scenarios but use spurious ones if test samples have them; this
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2305.05803</link><description>&lt;p&gt;
&#22522;&#20110;Segment Anything Model (SAM)&#22686;&#24378;&#20266;&#26631;&#31614;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;
&lt;/p&gt;
&lt;p&gt;
Segment Anything Model (SAM) Enhanced Pseudo Labels for Weakly Supervised Semantic Segmentation. (arXiv:2305.05803v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05803
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#36890;&#36807;&#21033;&#29992;Segment Anything Model (SAM)&#22686;&#24378;Class Activation Maps (CAM)&#29983;&#25104;&#39640;&#36136;&#37327;&#20266;&#26631;&#31614;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;&#20013;CAM&#30340;&#23616;&#37096;&#28608;&#27963;&#21644;&#34394;&#20551;&#28608;&#27963;&#30340;&#38480;&#21046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20165;&#20351;&#29992;&#22270;&#20687;&#32423;&#21035;&#30340;&#30417;&#30563;&#30340;&#24369;&#30417;&#30563;&#35821;&#20041;&#20998;&#21106;(WSSS)&#30001;&#20110;&#20854;&#19982;&#20687;&#32032;&#32423;&#27880;&#37322;&#30456;&#27604;&#30340;&#20302;&#25104;&#26412;&#32780;&#36234;&#26469;&#36234;&#21463;&#21040;&#20851;&#27880;&#12290;&#22823;&#22810;&#25968;&#29616;&#26377;&#26041;&#27861;&#20381;&#36182;&#20110;&#31867;&#28608;&#27963;&#22270;(CAM)&#29983;&#25104;&#20687;&#32032;&#32423;&#30340;&#20266;&#26631;&#31614;&#36827;&#34892;&#30417;&#30563;&#35757;&#32451;&#12290;&#28982;&#32780;&#65292;&#20247;&#25152;&#21608;&#30693;&#65292;CAM&#32463;&#24120;&#36973;&#21463;&#23616;&#37096;&#28608;&#27963;&#30340;&#38480;&#21046;-&#21482;&#28608;&#27963;&#26368;&#20855;&#21306;&#20998;&#24615;&#30340;&#37096;&#20998;&#65292;&#32780;&#19981;&#26159;&#25972;&#20010;&#23545;&#35937;&#21306;&#22495;&#21644;&#34394;&#20551;&#30340;&#28608;&#27963;-&#19981;&#24517;&#35201;&#22320;&#28608;&#27963;&#29289;&#20307;&#21608;&#22260;&#30340;&#32972;&#26223;&#12290;&#26412;&#30740;&#31350;&#24341;&#20837;&#20102;&#19968;&#31181;&#31616;&#21333;&#32780;&#26377;&#25928;&#30340;&#26041;&#27861;&#26469;&#35299;&#20915;&#36825;&#20123;&#38480;&#21046;&#65292;&#21363;&#21033;&#29992;&#26368;&#36817;&#21457;&#24067;&#30340;Segment Anything Model (SAM)&#22686;&#24378;CAM&#29983;&#25104;&#39640;&#36136;&#37327;&#30340;&#20266;&#26631;&#31614;&#12290;SAM&#26159;&#19968;&#20010;&#20998;&#21106;&#22522;&#30784;&#27169;&#22411;&#65292;&#23637;&#31034;&#20102;&#23558;&#22270;&#20687;&#20998;&#21106;&#25104;&#27573;&#33853;&#30340;&#24378;&#38646;-shot&#33021;&#21147;&#65292;&#20294;&#32570;&#20047;&#36825;&#20123;&#21306;&#22495;&#30340;&#35821;&#20041;&#26631;&#31614;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#37319;&#29992;&#29305;&#23450;&#31867;&#21035;&#30340;&#20266;&#26631;&#31614;&#20316;&#20026;&#20449;&#21495;&#26469;&#36873;&#25321;m&#12290;
&lt;/p&gt;
&lt;p&gt;
Weakly Supervised Semantic Segmentation (WSSS) with only image-level supervision has garnered increasing attention due to its low annotation cost compared to pixel-level annotation. Most existing methods rely on Class Activation Maps (CAM) to generate pixel-level pseudo labels for supervised training. However, it is well known that CAM often suffers from partial activation -- activating the most discriminative part instead of the entire object area, and false activation -- unnecessarily activating the background around the object. In this study, we introduce a simple yet effective approach to address these limitations by harnessing the recently released Segment Anything Model (SAM) to generate higher-quality pseudo labels with CAM. SAM is a segmentation foundation model that demonstrates strong zero-shot ability in partitioning images into segments but lacks semantic labels for these regions. To circumvent this, we employ pseudo labels for a specific class as the signal to select the m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#22522;&#20110;&#21608;&#26399;&#26102;&#38388;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#20351;&#24471;PPO&#21644;SAC&#22312;&#24191;&#27867;&#30340;&#24490;&#29615;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#25509;&#36817;&#32791;&#26102;&#30340;&#22312;&#32447;&#36229;&#21442;&#25968;&#35843;&#25972;&#33719;&#24471;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2305.05760</link><description>&lt;p&gt;
&#38477;&#20302;&#29616;&#23454;&#31574;&#30053;&#20248;&#21270;&#20013;&#24490;&#29615;&#26102;&#38388;&#35843;&#25972;&#30340;&#20195;&#20215;
&lt;/p&gt;
&lt;p&gt;
Reducing the Cost of Cycle-Time Tuning for Real-World Policy Optimization. (arXiv:2305.05760v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05760
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;&#65292;&#21487;&#22522;&#20110;&#21608;&#26399;&#26102;&#38388;&#35774;&#32622;&#36229;&#21442;&#25968;&#65292;&#20351;&#24471;PPO&#21644;SAC&#22312;&#24191;&#27867;&#30340;&#24490;&#29615;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#25509;&#36817;&#32791;&#26102;&#30340;&#22312;&#32447;&#36229;&#21442;&#25968;&#35843;&#25972;&#33719;&#24471;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36830;&#32493;&#26102;&#38388;&#24378;&#21270;&#23398;&#20064;&#20219;&#21153;&#36890;&#24120;&#20351;&#29992;&#22266;&#23450;&#21608;&#26399;&#26102;&#38388;&#30340;&#31163;&#25955;&#27493;&#39588;&#36827;&#34892;&#25805;&#20316;&#12290;&#23454;&#36341;&#20013;&#38656;&#35201;&#20026;&#32473;&#23450;&#20219;&#21153;&#36873;&#25321;&#25805;&#20316;&#21608;&#26399;&#26102;&#38388;&#65292;&#19968;&#20010;&#37325;&#35201;&#38382;&#39064;&#26159;&#23398;&#20064;&#31639;&#27861;&#30340;&#36229;&#21442;&#25968;&#26159;&#21542;&#38656;&#35201;&#20026;&#27599;&#20010;&#21608;&#26399;&#26102;&#38388;&#37325;&#26032;&#35843;&#25972;&#65292;&#36825;&#23545;&#20110;&#29616;&#23454;&#19990;&#30028;&#30340;&#26426;&#22120;&#20154;&#26469;&#35828;&#26159;&#19981;&#21487;&#34892;&#30340;&#12290;&#22312;&#26412;&#25991;&#20013;&#65292;&#25105;&#20204;&#30740;&#31350;&#20102;&#20004;&#31181;&#31574;&#30053;&#26799;&#24230;&#31639;&#27861;--PPO&#21644;SAC--&#22312;&#19981;&#21516;&#30340;&#21608;&#26399;&#26102;&#38388;&#19979;&#20351;&#29992;&#22522;&#32447;&#36229;&#21442;&#25968;&#20540;&#30340;&#24773;&#20917;&#12290;&#36890;&#36807;&#20351;&#29992;&#22522;&#32447;&#36229;&#21442;&#25968;&#22312;&#19968;&#20010;&#22522;&#20934;&#20219;&#21153;&#20013;&#23637;&#31034;&#36825;&#20004;&#31181;&#31639;&#27861;&#34920;&#29616;&#33391;&#22909;&#30340;&#24773;&#20917;&#65292;&#25105;&#20204;&#21457;&#29616;&#24403;&#36873;&#25321;&#19981;&#21516;&#20110;&#20219;&#21153;&#40664;&#35748;&#20540;&#30340;&#21608;&#26399;&#26102;&#38388;&#26102;&#65292;&#20351;&#29992;&#22522;&#32447;&#36229;&#21442;&#25968;&#30340;PPO&#26080;&#27861;&#23398;&#20064;&#12290;&#27492;&#22806;&#65292;&#24403;&#36229;&#21442;&#25968;&#29992;&#20110;&#27599;&#20010;&#21608;&#26399;&#26102;&#38388;&#26102;&#65292;&#22522;&#20110;&#22522;&#32447;&#30340;PPO&#21644;SAC&#34920;&#29616;&#22343;&#26126;&#26174;&#21155;&#20110;&#23427;&#20204;&#30340;&#35843;&#25972;&#20540;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#21608;&#26399;&#26102;&#38388;&#35774;&#32622;&#36825;&#20123;&#36229;&#21442;&#25968;&#30340;&#26032;&#26041;&#27861;&#12290;&#22312;&#25105;&#20204;&#30340;&#20223;&#30495;&#26426;&#22120;&#20154;&#36816;&#21160;&#20219;&#21153;&#23454;&#39564;&#20013;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#20351;&#24471;PPO&#21644;SAC&#22312;&#26497;&#20854;&#24191;&#27867;&#30340;&#21608;&#26399;&#26102;&#38388;&#33539;&#22260;&#20869;&#36827;&#34892;&#23398;&#20064;&#65292;&#21516;&#26102;&#23454;&#29616;&#20102;&#25509;&#36817;&#20110;&#32791;&#26102;&#30340;&#22312;&#32447;&#36229;&#21442;&#25968;&#35843;&#25972;&#33719;&#24471;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
Continuous-time reinforcement learning tasks commonly use discrete steps of fixed cycle times for actions. As practitioners need to choose the action-cycle time for a given task, a significant concern is whether the hyper-parameters of the learning algorithm need to be re-tuned for each choice of the cycle time, which is prohibitive for real-world robotics. In this work, we investigate the widely-used baseline hyper-parameter values of two policy gradient algorithms -- PPO and SAC -- across different cycle times. Using a benchmark task where the baseline hyper-parameters of both algorithms were shown to work well, we reveal that when a cycle time different than the task default is chosen, PPO with baseline hyper-parameters fails to learn. Moreover, both PPO and SAC with their baseline hyper-parameters perform substantially worse than their tuned values for each cycle time. We propose novel approaches for setting these hyper-parameters based on the cycle time. In our experiments on simu
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25240;&#25187;&#32047;&#31215;&#22686;&#30410;&#65288;DCG&#65289;&#25490;&#24207;&#24182;&#21152;&#26435;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#20302;&#20195;&#34920;&#24615;&#32452;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05759</link><description>&lt;p&gt;
&#25490;&#21517;&#21644;&#37325;&#26032;&#21152;&#26435;&#25552;&#39640;&#20102;&#32452;&#30340;&#20998;&#24067;&#40065;&#26834;&#24615;
&lt;/p&gt;
&lt;p&gt;
Ranking &amp; Reweighting Improves Group Distributional Robustness. (arXiv:2305.05759v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05759
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21033;&#29992;&#25240;&#25187;&#32047;&#31215;&#22686;&#30410;&#65288;DCG&#65289;&#25490;&#24207;&#24182;&#21152;&#26435;&#22788;&#29702;&#35757;&#32451;&#25968;&#25454;&#20197;&#25552;&#39640;&#27169;&#22411;&#23545;&#20302;&#20195;&#34920;&#24615;&#32452;&#30340;&#40065;&#26834;&#24615;&#30340;&#26041;&#27861;&#65292;&#23454;&#39564;&#35777;&#26126;&#20854;&#20248;&#20110;&#20808;&#21069;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26368;&#36817;&#30340;&#30740;&#31350;&#34920;&#26126;&#65292;&#36890;&#36807;&#32463;&#39564;&#39118;&#38505;&#26368;&#23567;&#21270;&#65288;ERM&#65289;&#36827;&#34892;&#26631;&#20934;&#35757;&#32451;&#21487;&#33021;&#20250;&#20135;&#29983;&#22312;&#24179;&#22343;&#31934;&#24230;&#19978;&#34920;&#29616;&#20986;&#33394;&#20294;&#22312;&#20302;&#20195;&#34920;&#24615;&#32452;&#19978;&#20934;&#30830;&#24615;&#36739;&#20302;&#30340;&#27169;&#22411;&#65292;&#36825;&#26159;&#30001;&#20110;&#34920;&#24449;&#20013;&#34394;&#20551;&#29305;&#24449;&#30340;&#26222;&#36941;&#23384;&#22312;&#25152;&#33268;&#12290;&#35299;&#20915;&#36825;&#20010;&#32452;&#40065;&#26834;&#24615;&#38382;&#39064;&#30340;&#20027;&#35201;&#26041;&#27861;&#26159;&#22312;&#35757;&#32451;&#25968;&#25454;&#19978;&#26368;&#23567;&#21270;&#26368;&#22351;&#30340;&#32452;&#35823;&#24046;&#65288;&#31867;&#20284;&#20110;&#26497;&#23567;&#20540;&#31574;&#30053;&#65289;&#65292;&#24076;&#26395;&#23427;&#20250;&#22312;&#27979;&#35797;&#25968;&#25454;&#19978;&#26377;&#33391;&#22909;&#30340;&#27867;&#21270;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#31181;&#26041;&#27861;&#24448;&#24448;&#26159;&#27425;&#20248;&#30340;&#65292;&#23588;&#20854;&#26159;&#24403;&#27979;&#35797;&#25968;&#25454;&#38598;&#20013;&#21253;&#21547;&#20197;&#21069;&#26410;&#35265;&#36807;&#30340;&#32452;&#26102;&#12290;&#26412;&#25991;&#21463;&#20449;&#24687;&#26816;&#32034;&#21644;Learning-to-Rank&#25991;&#29486;&#30340;&#21551;&#21457;&#65292;&#39318;&#20808;&#25552;&#20986;&#20351;&#29992;&#25240;&#25187;&#32047;&#31215;&#22686;&#30410;&#65288;DCG&#65289;&#20316;&#20026;&#27169;&#22411;&#36136;&#37327;&#24230;&#37327;&#26631;&#20934;&#65292;&#20197;&#20419;&#36827;&#26356;&#22909;&#30340;&#36229;&#21442;&#25968;&#35843;&#25972;&#21644;&#27169;&#22411;&#36873;&#25321;&#12290;&#20316;&#20026;&#19968;&#31181;&#22522;&#20110;&#25490;&#24207;&#30340;&#24230;&#37327;&#26631;&#20934;&#65292;DCG&#21152;&#26435;&#22810;&#20010;&#24615;&#33021;&#36739;&#24046;&#30340;&#32452;&#65288;&#32780;&#19981;&#20165;&#20165;&#26159;&#32771;&#34385;&#24615;&#33021;&#26368;&#24046;&#30340;&#32452;&#65289;&#12290;&#20316;&#20026;&#33258;&#28982;&#30340;&#19979;&#19968;&#27493;&#65292;&#25105;&#20204;&#22522;&#20110;&#36825;&#20123;&#32467;&#26524;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#30340;&#32452;&#37325;&#26032;&#21152;&#26435;&#26041;&#27861;&#65292;&#22312;&#35757;&#32451;&#26399;&#38388;&#40723;&#21169;&#27169;&#22411;&#38598;&#20013;&#20110;&#20302;&#20195;&#34920;&#24615;&#30340;&#32452;&#12290;&#22312;&#20960;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#20248;&#20110;&#20808;&#21069;&#30340;&#26368;&#20808;&#36827;&#26041;&#27861;&#65292;&#24182;&#26174;&#33879;&#25552;&#39640;&#20102;&#23545;&#32452;&#19981;&#24179;&#34913;&#30340;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Recent work has shown that standard training via empirical risk minimization (ERM) can produce models that achieve high accuracy on average but low accuracy on underrepresented groups due to the prevalence of spurious features. A predominant approach to tackle this group robustness problem minimizes the worst group error (akin to a minimax strategy) on the training data, hoping it will generalize well on the testing data. However, this is often suboptimal, especially when the out-of-distribution (OOD) test data contains previously unseen groups. Inspired by ideas from the information retrieval and learning-to-rank literature, this paper first proposes to use Discounted Cumulative Gain (DCG) as a metric of model quality for facilitating better hyperparameter tuning and model selection. Being a ranking-based metric, DCG weights multiple poorly-performing groups (instead of considering just the group with the worst performance). As a natural next step, we build on our results to propose a
&lt;/p&gt;</description></item><item><title>&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#21327;&#20316;&#24314;&#31569;&#20219;&#21153;&#20013;&#22914;&#20309;&#35299;&#20915;&#27169;&#26865;&#20004;&#21487;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#20309;&#26102;&#23547;&#27714;&#28548;&#28165;&#20197;&#21450;&#24212;&#35813;&#35810;&#38382;&#20160;&#20040;&#28548;&#28165;&#38382;&#39064;&#36825;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#30340;&#35299;&#31572;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.05754</link><description>&lt;p&gt;
&#36890;&#36807;&#19990;&#30028;&#29366;&#24577;&#21644;&#25991;&#26412;&#25351;&#20196;&#25552;&#38382;&#30340;&#26102;&#38388;&#21644;&#38382;&#39064;&#65306;IGLU NLP&#25361;&#25112;&#35299;&#20915;&#26041;&#26696;
&lt;/p&gt;
&lt;p&gt;
When and What to Ask Through World States and Text Instructions: IGLU NLP Challenge Solution. (arXiv:2305.05754v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05754
&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#35299;&#20915;&#20102;&#22312;&#21327;&#20316;&#24314;&#31569;&#20219;&#21153;&#20013;&#22914;&#20309;&#35299;&#20915;&#27169;&#26865;&#20004;&#21487;&#30340;&#24773;&#20917;&#65292;&#20174;&#32780;&#25552;&#20986;&#20102;&#20309;&#26102;&#23547;&#27714;&#28548;&#28165;&#20197;&#21450;&#24212;&#35813;&#35810;&#38382;&#20160;&#20040;&#28548;&#28165;&#38382;&#39064;&#36825;&#20004;&#20010;&#20851;&#38190;&#38382;&#39064;&#30340;&#35299;&#31572;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#21327;&#20316;&#20219;&#21153;&#20013;&#65292;&#26377;&#25928;&#30340;&#20132;&#27969;&#23545;&#20110;&#23454;&#29616;&#20849;&#21516;&#30446;&#26631;&#33267;&#20851;&#37325;&#35201;&#12290;&#20854;&#20013;&#19968;&#20010;&#20219;&#21153;&#26159;&#21327;&#20316;&#24314;&#31569;&#65292;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#65292;&#24314;&#31569;&#32773;&#24517;&#39035;&#30456;&#20114;&#36890;&#20449;&#65292;&#22312;&#35832;&#22914;Minecraft&#20043;&#31867;&#30340;&#27169;&#25311;&#29615;&#22659;&#20013;&#26500;&#24314;&#25152;&#38656;&#30340;&#32467;&#26500;&#12290;&#25105;&#20204;&#26088;&#22312;&#24320;&#21457;&#19968;&#20010;&#26234;&#33021;&#24314;&#31569;&#20195;&#29702;&#65292;&#26681;&#25454;&#29992;&#25143;&#23545;&#35805;&#24314;&#36896;&#32467;&#26500;&#12290;&#28982;&#32780;&#65292;&#22312;&#21327;&#20316;&#24314;&#31569;&#20013;&#65292;&#24314;&#31569;&#32773;&#21487;&#33021;&#20250;&#36935;&#21040;&#38590;&#20197;&#35299;&#35835;&#30340;&#24773;&#20917;&#65292;&#22240;&#20026;&#20449;&#24687;&#21644;&#25351;&#20196;&#26377;&#38480;&#65292;&#23548;&#33268;&#27169;&#26865;&#20004;&#21487;&#12290;&#22312;NeurIPS 2022&#31454;&#36187;&#30340;NLP&#20219;&#21153;&#20013;&#65292;&#25105;&#20204;&#22238;&#31572;&#20102;&#20004;&#20010;&#20851;&#38190;&#30340;&#30740;&#31350;&#38382;&#39064;&#65292;&#26088;&#22312;&#22635;&#34917;&#36825;&#19968;&#31354;&#30333;&#65306;&#20195;&#29702;&#20309;&#26102;&#24212;&#35813;&#23547;&#27714;&#28548;&#28165;&#65292;&#24212;&#35813;&#35810;&#38382;&#20160;&#20040;&#28548;&#28165;&#38382;&#39064;&#65311;&#25105;&#20204;&#36890;&#36807;&#20004;&#20010;&#23376;&#20219;&#21153;&#26397;&#30528;&#36825;&#20010;&#30446;&#26631;&#36808;&#36827;&#65292;&#19968;&#20010;&#26159;&#20998;&#31867;&#20219;&#21153;&#65292;&#19968;&#20010;&#26159;&#25490;&#24207;&#20219;&#21153;&#12290;&#23545;&#20110;&#20998;&#31867;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#26681;&#25454;&#24403;&#21069;&#30340;&#19990;&#30028;&#29366;&#24577;&#21644;&#23545;&#35805;&#21382;&#21490;&#30830;&#23450;&#20195;&#29702;&#26159;&#21542;&#24212;&#35813;&#23547;&#27714;&#28548;&#28165;&#12290;&#23545;&#20110;&#25490;&#24207;&#20219;&#21153;&#65292;&#30446;&#26631;&#26159;&#25552;&#20379;&#19968;&#20221;&#21487;&#33021;&#30340;&#28548;&#28165;&#38382;&#39064;&#30340;&#25490;&#21517;&#21015;&#34920;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#32467;&#21512;&#20102;&#22522;&#20110;&#35268;&#21017;&#30340;&#21551;&#21457;&#24335;&#26041;&#27861;&#21644;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24182;&#22312;IGLU NLP&#25361;&#25112;&#25968;&#25454;&#38598;&#19978;&#21462;&#24471;&#20102;&#31454;&#20105;&#24615;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
In collaborative tasks, effective communication is crucial for achieving joint goals. One such task is collaborative building where builders must communicate with each other to construct desired structures in a simulated environment such as Minecraft. We aim to develop an intelligent builder agent to build structures based on user input through dialogue. However, in collaborative building, builders may encounter situations that are difficult to interpret based on the available information and instructions, leading to ambiguity. In the NeurIPS 2022 Competition NLP Task, we address two key research questions, with the goal of filling this gap: when should the agent ask for clarification, and what clarification questions should it ask? We move towards this target with two sub-tasks, a classification task and a ranking task. For the classification task, the goal is to determine whether the agent should ask for clarification based on the current world state and dialogue history. For the ran
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#21487;&#38752;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#21487;&#38752;&#24615;&#35780;&#20272;&#26041;&#27861;&#20197;&#21450;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#32570;&#12290;</title><link>http://arxiv.org/abs/2305.05750</link><description>&lt;p&gt;
&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#21487;&#38752;&#24615;&#35780;&#20272;&#26041;&#27861;&#30340;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;
&lt;/p&gt;
&lt;p&gt;
A Systematic Literature Review on Hardware Reliability Assessment Methods for Deep Neural Networks. (arXiv:2305.05750v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05750
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20026;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#21487;&#38752;&#24615;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65292;&#24635;&#32467;&#20102;&#29616;&#26377;&#30340;&#21487;&#38752;&#24615;&#35780;&#20272;&#26041;&#27861;&#20197;&#21450;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#30340;&#35780;&#20272;&#65292;&#25581;&#31034;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#30740;&#31350;&#31354;&#32570;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20154;&#24037;&#26234;&#33021;&#65288;AI&#65289;&#21644;&#26426;&#22120;&#23398;&#20064;&#65288;ML&#65289;&#29305;&#21035;&#26159;&#30001;&#20110;&#20854;&#23398;&#20064;&#35299;&#20915;&#22797;&#26434;&#38382;&#39064;&#30340;&#33021;&#21147;&#32780;&#34987;&#24212;&#29992;&#20110;&#21508;&#31181;&#24212;&#29992;&#31243;&#24207;&#20013;&#12290;&#36807;&#21435;&#21313;&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#30340;&#24555;&#36895;&#36827;&#23637;&#25552;&#20986;&#20102;&#30001;&#22823;&#37327;&#31070;&#32463;&#20803;&#21644;&#23618;&#32452;&#25104;&#30340;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#65288;DNN&#65289;&#12290;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#21152;&#36895;&#22120;&#65288;DHA&#65289;&#29992;&#20110;&#23558;DNNs&#37096;&#32626;&#21040;&#30446;&#26631;&#24212;&#29992;&#31243;&#24207;&#20013;&#65292;&#21516;&#26102;&#22312;&#30828;&#20214;&#25925;&#38556;/&#38169;&#35823;&#20250;&#23548;&#33268;&#28798;&#38590;&#24615;&#21518;&#26524;&#30340;&#23433;&#20840;&#20851;&#38190;&#24212;&#29992;&#31243;&#24207;&#20013;&#20063;&#21463;&#30410;&#20110;DHA&#12290;&#22240;&#27492;&#65292;DNN&#30340;&#21487;&#38752;&#24615;&#26159;&#30740;&#31350;&#30340;&#37325;&#35201;&#20027;&#39064;&#12290;&#36817;&#24180;&#26469;&#65292;&#24050;&#32463;&#21457;&#34920;&#20102;&#22810;&#39033;&#30740;&#31350;&#65292;&#20197;&#35780;&#20272;DNNs&#30340;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#24050;&#32463;&#25552;&#20986;&#20102;&#21508;&#31181;&#21508;&#26679;&#30340;&#21487;&#38752;&#24615;&#35780;&#20272;&#26041;&#27861;&#65292;&#36866;&#29992;&#20110;&#21508;&#31181;&#24179;&#21488;&#21644;&#24212;&#29992;&#31243;&#24207;&#12290;&#22240;&#27492;&#65292;&#26377;&#24517;&#35201;&#24635;&#32467;&#29616;&#26377;&#25216;&#26415;&#20197;&#30830;&#23450;&#30740;&#31350;DNN&#30340;&#21487;&#38752;&#24615;&#30340;&#30740;&#31350;&#20013;&#23384;&#22312;&#30340;&#24046;&#36317;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#36827;&#34892;&#20102;&#31995;&#32479;&#25991;&#29486;&#32508;&#36848;&#65288;SLR&#65289;&#65292;&#20197;&#30830;&#23450;&#29616;&#26377;&#30340;DNN&#30828;&#20214;&#21487;&#38752;&#24615;&#35780;&#20272;&#26041;&#27861;&#12290;&#25105;&#20204;&#36824;&#22238;&#39038;&#20102;&#29616;&#26377;&#30340;&#21487;&#29992;&#20110;DNN&#21487;&#38752;&#24615;&#35780;&#20272;&#30340;&#25968;&#25454;&#38598;&#21644;&#22522;&#20934;&#12290;&#26412;&#30740;&#31350;&#30340;&#32467;&#26524;&#23558;&#24110;&#21161;&#30740;&#31350;&#20154;&#21592;&#20102;&#35299;&#24403;&#21069;&#28145;&#24230;&#31070;&#32463;&#32593;&#32476;&#30828;&#20214;&#21487;&#38752;&#24615;&#35780;&#20272;&#26041;&#27861;&#30340;&#26368;&#26032;&#25216;&#26415;&#65292;&#21516;&#26102;&#20063;&#25581;&#31034;&#20102;&#36825;&#19968;&#39046;&#22495;&#30340;&#36951;&#28431;&#31354;&#32570;&#12290;
&lt;/p&gt;
&lt;p&gt;
Artificial Intelligence (AI) and, in particular, Machine Learning (ML) have emerged to be utilized in various applications due to their capability to learn how to solve complex problems. Over the last decade, rapid advances in ML have presented Deep Neural Networks (DNNs) consisting of a large number of neurons and layers. DNN Hardware Accelerators (DHAs) are leveraged to deploy DNNs in the target applications. Safety-critical applications, where hardware faults/errors would result in catastrophic consequences, also benefit from DHAs. Therefore, the reliability of DNNs is an essential subject of research. In recent years, several studies have been published accordingly to assess the reliability of DNNs. In this regard, various reliability assessment methods have been proposed on a variety of platforms and applications. Hence, there is a need to summarize the state of the art to identify the gaps in the study of the reliability of DNNs. In this work, we conduct a Systematic Literature R
&lt;/p&gt;</description></item><item><title>&#30740;&#31350;&#20102;&#21442;&#25968;&#21270;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#21152;&#26435;&#21487;&#36798;&#24615;&#30340;&#38477;&#20302;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35745;&#31639;&#25105;&#20204;&#25152;&#30740;&#31350;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#25490;&#24207;&#30340;&#31561;&#20215;&#31867;&#65292;&#24182;&#23454;&#29616;&#20102;&#20004;&#20010;&#35268;&#21017;&#26469;&#27424;&#20272;&#35745;&#8220;&#27704;&#19981;&#26356;&#31967;&#8221;&#30340;&#20851;&#31995;&#12290;</title><link>http://arxiv.org/abs/2305.05739</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#30340;&#26041;&#24335;&#38477;&#20302;&#21442;&#25968;&#21270;&#21644;&#21152;&#26435;MDP&#30340;&#22797;&#26434;&#24230;
&lt;/p&gt;
&lt;p&gt;
Graph-Based Reductions for Parametric and Weighted MDPs. (arXiv:2305.05739v1 [cs.LO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05739
&lt;/p&gt;
&lt;p&gt;
&#30740;&#31350;&#20102;&#21442;&#25968;&#21270;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#21152;&#26435;&#21487;&#36798;&#24615;&#30340;&#38477;&#20302;&#22797;&#26434;&#24230;&#38382;&#39064;&#65292;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35745;&#31639;&#25105;&#20204;&#25152;&#30740;&#31350;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#25490;&#24207;&#30340;&#31561;&#20215;&#31867;&#65292;&#24182;&#23454;&#29616;&#20102;&#20004;&#20010;&#35268;&#21017;&#26469;&#27424;&#20272;&#35745;&#8220;&#27704;&#19981;&#26356;&#31967;&#8221;&#30340;&#20851;&#31995;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#30740;&#31350;&#20102;&#21442;&#25968;&#21270;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20013;&#21152;&#26435;&#21487;&#36798;&#24615;&#30340;&#38477;&#20302;&#22797;&#26434;&#24230;&#38382;&#39064;&#12290;&#21363;&#65292;&#25105;&#20204;&#35828;&#19968;&#20010;&#29366;&#24577;p&#27704;&#36828;&#19981;&#20250;&#27604;q&#26356;&#31967;&#65292;&#22914;&#26524;&#23545;&#20110;&#22810;&#39033;&#24335;&#26410;&#30693;&#25968;&#30340;&#25152;&#26377;&#20272;&#20540;&#65292;&#20174;p&#21040;&#36798;&#30340;&#26368;&#22823;&#39044;&#26399;&#26435;&#37325;&#37117;&#27604;&#20174;q&#21040;&#36798;&#30340;&#26368;&#22823;&#39044;&#26399;&#26435;&#37325;&#22823;&#12290;&#22312;&#35745;&#31639;&#22797;&#26434;&#24230;&#26041;&#38754;&#65292;&#25105;&#20204;&#30830;&#23450;&#20102;&#21028;&#26029;p&#26159;&#21542;&#27704;&#36828;&#19981;&#20250;&#27604;q&#26356;&#31967;&#30340;coETR&#23436;&#20840;&#24615;&#12290;&#22312;&#31215;&#26497;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#22810;&#39033;&#24335;&#26102;&#38388;&#31639;&#27861;&#26469;&#35745;&#31639;&#25105;&#20204;&#25152;&#30740;&#31350;&#30340;&#39532;&#23572;&#21487;&#22827;&#38142;&#25490;&#24207;&#30340;&#31561;&#20215;&#31867;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#25551;&#36848;&#24182;&#23454;&#29616;&#20102;&#20004;&#20010;&#25512;&#29702;&#35268;&#21017;&#26469;&#27424;&#20272;&#35745;&#8220;&#27704;&#19981;&#26356;&#31967;&#8221;&#30340;&#20851;&#31995;&#65292;&#24182;&#19988;&#22312;&#23454;&#39564;&#20013;&#23637;&#31034;&#23427;&#21487;&#20197;&#29992;&#20316;&#22823;&#22411;&#39532;&#23572;&#31185;&#22827;&#20915;&#31574;&#36807;&#31243;&#20998;&#26512;&#30340;&#39640;&#25928;&#39044;&#22788;&#29702;&#27493;&#39588;&#12290;
&lt;/p&gt;
&lt;p&gt;
We study the complexity of reductions for weighted reachability in parametric Markov decision processes. That is, we say a state p is never worse than q if for all valuations of the polynomial indeterminates it is the case that the maximal expected weight that can be reached from p is greater than the same value from q. In terms of computational complexity, we establish that determining whether p is never worse than q is coETR-complete. On the positive side, we give a polynomial-time algorithm to compute the equivalence classes of the order we study for Markov chains. Additionally, we describe and implement two inference rules to under-approximate the never-worse relation and empirically show that it can be used as an efficient preprocessing step for the analysis of large Markov decision processes.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#36965;&#24863;&#22270;&#20687;&#20013;&#24212;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29616;&#29366;&#21450;&#26410;&#26469;&#36235;&#21183;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25512;&#29702;&#22270;&#20687;&#20013;&#30340;&#24213;&#23618;&#35821;&#20041;&#65292;&#21487;&#20197;&#35782;&#21035;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#21450;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;</title><link>http://arxiv.org/abs/2305.05726</link><description>&lt;p&gt;
&#36965;&#24863;&#20013;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#65306;&#29616;&#29366;&#19982;&#26410;&#26469;&#36235;&#21183;
&lt;/p&gt;
&lt;p&gt;
Vision-Language Models in Remote Sensing: Current Progress and Future Trends. (arXiv:2305.05726v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05726
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#36965;&#24863;&#22270;&#20687;&#20013;&#24212;&#29992;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#29616;&#29366;&#21450;&#26410;&#26469;&#36235;&#21183;&#65292;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#25512;&#29702;&#22270;&#20687;&#20013;&#30340;&#24213;&#23618;&#35821;&#20041;&#65292;&#21487;&#20197;&#35782;&#21035;&#23545;&#35937;&#20043;&#38388;&#30340;&#20851;&#31995;&#21450;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35770;&#25991;&#20171;&#32461;&#20102;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#22312;&#36965;&#24863;&#39046;&#22495;&#30340;&#24212;&#29992;&#29616;&#29366;&#21644;&#26410;&#26469;&#36235;&#21183;&#12290;&#36965;&#24863;&#39046;&#22495;&#20013;&#29616;&#26377;&#30340;&#20154;&#24037;&#26234;&#33021;&#30740;&#31350;&#20027;&#35201;&#38598;&#20013;&#22312;&#22270;&#20687;&#30340;&#35270;&#35273;&#29702;&#35299;&#19978;&#65292;&#24573;&#30053;&#20102;&#23545;&#35937;&#21644;&#23427;&#20204;&#20043;&#38388;&#20851;&#31995;&#30340;&#35821;&#20041;&#29702;&#35299;&#65292;&#32780;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#30340;&#20986;&#29616;&#21017;&#22635;&#34917;&#20102;&#36825;&#19968;&#31354;&#32570;&#12290;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#21487;&#20197;&#36890;&#36807;&#22270;&#20687;&#21450;&#20854;&#30456;&#20851;&#30340;&#25991;&#23383;&#25551;&#36848;&#36827;&#34892;&#25512;&#29702;&#65292;&#28145;&#20837;&#29702;&#35299;&#20854;&#24213;&#23618;&#35821;&#20041;&#65292;&#27604;&#20165;&#20165;&#35782;&#21035;&#22270;&#20687;&#20013;&#30340;&#23545;&#35937;&#65292;&#33021;&#22815;&#25512;&#26029;&#20986;&#23427;&#20204;&#20043;&#38388;&#30340;&#20851;&#31995;&#65292;&#29978;&#33267;&#29983;&#25104;&#33258;&#28982;&#35821;&#35328;&#25551;&#36848;&#12290;
&lt;/p&gt;
&lt;p&gt;
The remarkable achievements of ChatGPT and GPT-4 have sparked a wave of interest and research in the field of large language models for Artificial General Intelligence (AGI). These models provide us with intelligent solutions that are more similar to human thinking, enabling us to use general artificial intelligence to solve problems in various applications. However, in the field of remote sensing, the scientific literature on the implementation of AGI remains relatively scant. Existing AI-related research primarily focuses on visual understanding tasks while neglecting the semantic understanding of the objects and their relationships. This is where vision-language models excel, as they enable reasoning about images and their associated textual descriptions, allowing for a deeper understanding of the underlying semantics. Vision-language models can go beyond recognizing the objects in an image and can infer the relationships between them, as well as generate natural language descriptio
&lt;/p&gt;</description></item><item><title>CodeIE&#25552;&#20986;&#20102;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65288;Code-LLMs&#65289;&#20195;&#26367;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;NL-LLMs&#65289;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#36825;&#31867;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#21462;&#24471;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#39640;&#36798;4.5%&#30340;&#32477;&#23545;&#31934;&#24230;&#25913;&#36827;&#12290;</title><link>http://arxiv.org/abs/2305.05711</link><description>&lt;p&gt;
CodeIE: &#22823;&#22411;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#20248;&#20110;&#23569;&#26679;&#26412;&#20449;&#24687;&#25552;&#21462;&#22120;
&lt;/p&gt;
&lt;p&gt;
CodeIE: Large Code Generation Models are Better Few-Shot Information Extractors. (arXiv:2305.05711v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05711
&lt;/p&gt;
&lt;p&gt;
CodeIE&#25552;&#20986;&#20102;&#20351;&#29992;&#20195;&#30721;&#29983;&#25104;&#27169;&#22411;&#65288;Code-LLMs&#65289;&#20195;&#26367;&#33258;&#28982;&#35821;&#35328;&#29983;&#25104;&#27169;&#22411;&#65288;NL-LLMs&#65289;&#23545;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#36825;&#31867;&#20449;&#24687;&#25552;&#21462;&#20219;&#21153;&#36827;&#34892;&#23569;&#26679;&#26412;&#23398;&#20064;&#65292;&#21462;&#24471;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#39640;&#36798;4.5%&#30340;&#32477;&#23545;&#31934;&#24230;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#22823;&#35268;&#27169;&#35821;&#35328;&#27169;&#22411;&#65288;LLMs&#65289;&#30340;&#39044;&#35757;&#32451;&#26041;&#38754;&#65292;&#24050;&#32463;&#34920;&#29616;&#20986;&#22312;&#35768;&#22810;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#20219;&#21153;&#19978;&#20855;&#26377;&#24778;&#20154;&#30340;&#23569;&#26679;&#26412;&#23398;&#20064;&#33021;&#21147;&#12290;&#36890;&#24120;&#30340;&#20570;&#27861;&#26159;&#23558;&#20219;&#21153;&#37325;&#26500;&#20026;&#25991;&#26412;&#21040;&#25991;&#26412;&#30340;&#26684;&#24335;&#65292;&#20197;&#20415;&#33258;&#28982;&#35821;&#35328;&#30340;&#29983;&#25104;&#24335;LLMs&#65288;&#22914;GPT-3&#65289;&#21487;&#20197;&#34987;&#25552;&#31034;&#35299;&#20915;&#23427;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;NL-LLMs&#36827;&#34892;&#20449;&#24687;&#25552;&#21462;&#65288;IE&#65289;&#20219;&#21153;&#26159;&#19981;&#26131;&#30340;&#65292;&#22240;&#20026;IE&#20219;&#21153;&#30340;&#36755;&#20986;&#36890;&#24120;&#26159;&#32467;&#26500;&#21270;&#30340;&#65292;&#22240;&#27492;&#24456;&#38590;&#36716;&#25442;&#25104;&#32431;&#25991;&#26412;&#12290;&#25105;&#20204;&#25552;&#20986;&#20351;&#29992;&#20195;&#30721;&#24418;&#24335;&#32780;&#38750;&#33258;&#28982;&#35821;&#35328;&#26469;&#34920;&#36798;&#32467;&#26500;&#21270;&#30340;&#36755;&#20986;&#65292;&#24182;&#21033;&#29992;&#20195;&#30721;&#29983;&#25104;LLMs&#65288;&#22914;Codex&#65289;&#26469;&#25191;&#34892;IE&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#21629;&#21517;&#23454;&#20307;&#35782;&#21035;&#21644;&#20851;&#31995;&#25277;&#21462;&#12290;&#19982;NL-LLMs&#30456;&#27604;&#65292;&#25105;&#20204;&#34920;&#26126;&#36890;&#36807;&#35774;&#35745;&#20195;&#30721;&#39118;&#26684;&#30340;&#25552;&#31034;&#21644;&#23558;&#36825;&#20123;IE&#20219;&#21153;&#26356;&#25913;&#20026;&#20195;&#30721;&#29983;&#25104;&#20219;&#21153;&#65292;Code-LLMs&#21487;&#20197;&#19982;&#36825;&#20123;IE&#20219;&#21153;&#24456;&#22909;&#22320;&#23545;&#40784;&#12290;&#22312;&#19971;&#20010;&#22522;&#20934;&#27979;&#35797;&#19978;&#30340;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#23569;&#26679;&#26412;&#23398;&#20064;&#29615;&#22659;&#19979;&#19968;&#30452;&#20248;&#20110;&#20960;&#20010;&#24378;&#22522;&#20934;&#65292;&#24182;&#21462;&#24471;&#20102;&#39640;&#36798;4.5%&#30340;&#32477;&#23545;&#31934;&#24230;&#25913;&#36827;&#12290;
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) pre-trained on massive corpora have demonstrated impressive few-shot learning ability on many NLP tasks. A common practice is to recast the task into a text-to-text format such that generative LLMs of natural language (NL-LLMs) like GPT-3 can be prompted to solve it. However, it is nontrivial to perform information extraction (IE) tasks with NL-LLMs since the output of the IE task is usually structured and therefore is hard to be converted into plain text. In this paper, we propose to recast the structured output in the form of code instead of natural language and utilize generative LLMs of code (Code-LLMs) such as Codex to perform IE tasks, in particular, named entity recognition and relation extraction. In contrast to NL-LLMs, we show that Code-LLMs can be well-aligned with these IE tasks by designing code-style prompts and formulating these IE tasks as code generation tasks. Experiment results on seven benchmarks show that our method consistently outperf
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#30340;&#21487;&#38752;DBD&#31995;&#32479;&#65292;&#21033;&#29992;&#20844;&#20849;&#20256;&#24863;&#22120;&#25552;&#39640;&#20102;&#39550;&#39542;&#34892;&#20026;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.05670</link><description>&lt;p&gt;
&#22522;&#20110;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#30340;&#21361;&#38505;&#39550;&#39542;&#34892;&#20026;&#31934;&#20934;&#26816;&#27979;&#25552;&#39640;&#36947;&#36335;&#23433;&#20840;&#24615;
&lt;/p&gt;
&lt;p&gt;
Enhancing Road Safety through Accurate Detection of Hazardous Driving Behaviors with Graph Convolutional Recurrent Networks. (arXiv:2305.05670v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05670
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22270;&#21367;&#31215;&#24490;&#29615;&#32593;&#32476;&#30340;&#21487;&#38752;DBD&#31995;&#32479;&#65292;&#21033;&#29992;&#20844;&#20849;&#20256;&#24863;&#22120;&#25552;&#39640;&#20102;&#39550;&#39542;&#34892;&#20026;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#21644;&#23454;&#29992;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36710;&#31096;&#20173;&#28982;&#26159;&#20840;&#29699;&#37325;&#22823;&#30340;&#20844;&#20849;&#23433;&#20840;&#38382;&#39064;&#65292;&#20854;&#20013;&#30340;&#22823;&#22810;&#25968;&#24402;&#22240;&#20110;&#39550;&#39542;&#21592;&#38169;&#35823;&#65292;&#21253;&#25324;&#19981;&#36275;&#30340;&#39550;&#39542;&#30693;&#35782;&#12289;&#19981;&#21512;&#35268;&#23450;&#20197;&#21450;&#19981;&#33391;&#39550;&#39542;&#20064;&#24815;&#12290;&#20026;&#20102;&#25552;&#39640;&#36947;&#36335;&#23433;&#20840;&#24615;&#65292;&#22810;&#39033;&#39550;&#39542;&#34892;&#20026;&#26816;&#27979;&#65288;DBD&#65289;&#31995;&#32479;&#24050;&#32463;&#34987;&#25552;&#20986;&#65292;&#20197;&#35782;&#21035;&#23433;&#20840;&#21644;&#19981;&#23433;&#20840;&#30340;&#39550;&#39542;&#34892;&#20026;&#12290;&#36825;&#20123;&#30740;&#31350;&#20013;&#30340;&#35768;&#22810;&#21033;&#29992;&#20102;&#20174;&#25511;&#21046;&#22120;&#21306;&#22495;&#32593;&#32476;&#65288;CAN&#65289;&#24635;&#32447;&#33719;&#21462;&#30340;&#20256;&#24863;&#22120;&#25968;&#25454;&#26469;&#26500;&#24314;&#27169;&#22411;&#12290;&#28982;&#32780;&#65292;&#20351;&#29992;&#20844;&#20849;&#21487;&#29992;&#20256;&#24863;&#22120;&#24050;&#30693;&#20250;&#38477;&#20302;&#26816;&#27979;&#27169;&#22411;&#30340;&#20934;&#30830;&#24615;&#65292;&#32780;&#23558;&#20379;&#24212;&#21830;&#29305;&#23450;&#30340;&#20256;&#24863;&#22120;&#32435;&#20837;&#25968;&#25454;&#38598;&#20013;&#21017;&#21487;&#20197;&#22686;&#21152;&#20934;&#30830;&#24615;&#12290;&#20026;&#20102;&#35299;&#20915;&#29616;&#26377;&#26041;&#27861;&#30340;&#23616;&#38480;&#24615;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22522;&#20110;&#22270;&#21367;&#31215;&#38271;&#30701;&#26399;&#35760;&#24518;&#32593;&#32476;&#65288;GConvLSTM&#65289;&#30340;&#21487;&#38752;DBD&#31995;&#32479;&#65292;&#21033;&#29992;&#20844;&#20849;&#20256;&#24863;&#22120;&#25552;&#39640;&#20102;DBD&#27169;&#22411;&#30340;&#31934;&#24230;&#21644;&#23454;&#29992;&#24615;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36824;&#21152;&#20837;&#20102;&#38750;&#20844;&#20849;&#20256;&#24863;&#22120;&#26469;&#35780;&#20272;&#27169;&#22411;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Car accidents remain a significant public safety issue worldwide, with the majority of them attributed to driver errors stemming from inadequate driving knowledge, non-compliance with regulations, and poor driving habits. To improve road safety, Driving Behavior Detection (DBD) systems have been proposed in several studies to identify safe and unsafe driving behavior. Many of these studies have utilized sensor data obtained from the Controller Area Network (CAN) bus to construct their models. However, the use of publicly available sensors is known to reduce the accuracy of detection models, while incorporating vendor-specific sensors into the dataset increases accuracy. To address the limitations of existing approaches, we present a reliable DBD system based on Graph Convolutional Long Short-Term Memory Networks (GConvLSTM) that enhances the precision and practicality of DBD models using public sensors. Additionally, we incorporate non-public sensors to evaluate the model's effectivene
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#39044;&#27979;&#22686;&#26448;&#21046;&#36896;&#32858;&#20083;&#37240;&#65288;PLA&#65289;&#26679;&#21697;&#30340;&#20914;&#20987;&#24378;&#24230;&#12290;</title><link>http://arxiv.org/abs/2305.05668</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#31639;&#27861;&#30340;&#39044;&#27979;&#22686;&#26448;&#21046;&#36896;&#32858;&#20083;&#37240;&#65288;PLA&#65289;&#26679;&#21697;&#20914;&#20987;&#24378;&#24230;
&lt;/p&gt;
&lt;p&gt;
Neurosymbolic Artificial Intelligence (NSAI) based Algorithm for predicting the Impact Strength of Additive Manufactured Polylactic Acid (PLA) Specimens. (arXiv:2305.05668v1 [cs.LG])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05668
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#31639;&#27861;&#65292;&#33021;&#22815;&#39640;&#31934;&#24230;&#39044;&#27979;&#22686;&#26448;&#21046;&#36896;&#32858;&#20083;&#37240;&#65288;PLA&#65289;&#26679;&#21697;&#30340;&#20914;&#20987;&#24378;&#24230;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20171;&#32461;&#20102;&#31070;&#32463;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#65288;NSAI&#65289;&#22312;&#39044;&#27979;&#22686;&#26448;&#21046;&#36896;&#32858;&#20083;&#37240;&#65288;PLA&#65289;&#32452;&#20214;&#20914;&#20987;&#24378;&#24230;&#26041;&#38754;&#30340;&#24212;&#29992;&#65292;&#26159;NSAI&#22312;&#22686;&#26448;&#21046;&#36896;&#39046;&#22495;&#39318;&#27425;&#24212;&#29992;&#12290;NSAI&#27169;&#22411;&#34701;&#21512;&#20102;&#31070;&#32463;&#32593;&#32476;&#21644;&#31526;&#21495;&#20154;&#24037;&#26234;&#33021;&#30340;&#20248;&#21183;&#65292;&#30456;&#27604;&#20256;&#32479;&#30340;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#33021;&#22815;&#25552;&#20379;&#26356;&#31934;&#20934;&#21644;&#20934;&#30830;&#30340;&#39044;&#27979;&#12290;&#37319;&#38598;&#20102;&#23454;&#39564;&#25968;&#25454;&#24182;&#36827;&#34892;&#20102;&#21512;&#25104;&#22686;&#24378;&#65292;&#20351;&#27169;&#22411;&#26356;&#21152;&#31934;&#30830;&#12290;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#20351;&#29992;&#21253;&#25324;&#36755;&#20837;&#12289;&#20004;&#20010;&#38544;&#34255;&#23618;&#12289;&#21644;&#19968;&#20010;&#36755;&#20986;&#23618;&#30340;&#31070;&#32463;&#32593;&#32476;&#26550;&#26500;&#26469;&#24320;&#21457;&#65292;&#25509;&#30528;&#26159;&#19968;&#20010;&#20195;&#34920;&#31526;&#21495;&#32452;&#20214;&#30340;&#20915;&#31574;&#26641;&#22238;&#24402;&#22120;&#12290;&#36890;&#36807;&#23545;&#27604;&#35780;&#20272;&#35757;&#32451;&#38598;&#21644;&#39564;&#35777;&#38598;&#30340;&#22343;&#26041;&#35823;&#24046;&#65288;MSE&#65289;&#21644;R2&#20540;&#65292;&#27169;&#22411;&#30340;&#34920;&#29616;&#34987;&#19982;&#31616;&#21333;&#20154;&#24037;&#31070;&#32463;&#32593;&#32476;&#65288;ANN&#65289;&#27169;&#22411;&#36827;&#34892;&#20102;&#22522;&#20934;&#27979;&#35797;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#31070;&#32463;&#31526;&#21495;&#27169;&#22411;&#20248;&#20110;&#31616;&#21333;&#30340;ANN&#27169;&#22411;&#65292;&#22312;&#20004;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;MSE&#26356;&#20302;&#65292;R2&#20540;&#26356;&#39640;&#12290;&#24635;&#30340;&#26469;&#35828;&#65292;&#25152;&#25552;&#20986;&#30340;&#22522;&#20110;NSAI&#31639;&#27861;&#30340;&#26041;&#27861;&#22312;&#39044;&#27979;&#22686;&#26448;&#21046;&#36896;PLA&#32452;&#20214;&#30340;&#20914;&#20987;&#24378;&#24230;&#26041;&#38754;&#34920;&#29616;&#20986;&#26497;&#39640;&#30340;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
In this study, we introduce application of Neurosymbolic Artificial Intelligence (NSAI) for predicting the impact strength of additive manufactured polylactic acid (PLA) components, representing the first-ever use of NSAI in the domain of additive manufacturing. The NSAI model amalgamates the advantages of neural networks and symbolic AI, offering a more robust and accurate prediction than traditional machine learning techniques. Experimental data was collected and synthetically augmented to 1000 data points, enhancing the model's precision. The Neurosymbolic model was developed using a neural network architecture comprising input, two hidden layers, and an output layer, followed by a decision tree regressor representing the symbolic component. The model's performance was benchmarked against a Simple Artificial Neural Network (ANN) model by assessing mean squared error (MSE) and R-squared (R2) values for both training and validation datasets. The results reveal that the Neurosymbolic m
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;</title><link>http://arxiv.org/abs/2305.05640</link><description>&lt;p&gt;
&#38754;&#21521;&#20010;&#20154;&#25110;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#34920;&#31034;&#23398;&#20064;&#65306;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#24212;&#29992;&#30340;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
Representation Learning for Person or Entity-centric Knowledge Graphs: an application in Healthcare. (arXiv:2305.05640v1 [cs.AI])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05640
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#22312;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861; HEER&#65292;&#36890;&#36807;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#65292;&#26377;&#25928;&#22320;&#25913;&#21892;&#20102;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#30693;&#35782;&#22270;&#35889;&#26159;&#19968;&#31181;&#25353;&#26412;&#20307;&#25110;&#27169;&#24335;&#32452;&#32455;&#20449;&#24687;&#30340;&#27969;&#34892;&#26041;&#24335;&#65292;&#24050;&#32463;&#22312;&#20174;&#25628;&#32034;&#21040;&#25512;&#33616;&#30340;&#21508;&#31181;&#22330;&#26223;&#20013;&#24471;&#21040;&#20102;&#24212;&#29992;&#12290;&#23613;&#31649;&#22312;&#30693;&#35782;&#22270;&#35889;&#26041;&#38754;&#26377;&#20102;&#19968;&#20123;&#36827;&#23637;&#65292;&#20294;&#30693;&#35782;&#34920;&#31034;&#20173;&#28982;&#26159;&#36328;&#34892;&#19994;&#30340;&#19968;&#20010;&#38750;&#24120;&#26840;&#25163;&#30340;&#20219;&#21153;&#65292;&#29305;&#21035;&#26159;&#22312;&#29983;&#29289;&#21307;&#23398;&#21644;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#65292;&#30001;&#20110;&#23454;&#20307;&#20043;&#38388;&#30340;&#22797;&#26434;&#30456;&#20114;&#20851;&#31995;&#12289;&#24322;&#36136;&#24615;&#12289;&#32570;&#20047;&#26631;&#20934;&#21270;&#21644;&#25968;&#25454;&#31232;&#30095;&#24615;&#31561;&#22240;&#32032;&#65292;&#36825;&#19968;&#20219;&#21153;&#23588;&#20854;&#20855;&#26377;&#25361;&#25112;&#24615;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#38754;&#21521;&#21307;&#30103;&#20445;&#20581;&#39046;&#22495;&#26500;&#24314;&#38754;&#21521;&#23454;&#20307;&#30340;&#30693;&#35782;&#22270;&#35889;&#30340;&#31471;&#21040;&#31471;&#34920;&#31034;&#23398;&#20064;&#26041;&#27861;&#65292;&#37325;&#28857;&#26159;&#25429;&#25417;&#29983;&#29289;&#21307;&#23398;&#39046;&#22495;&#30340;&#29420;&#29305;&#29305;&#24449;&#12290;&#25152;&#25552;&#20986;&#30340;&#26694;&#26550;&#21517;&#20026;HEER&#65288;Healthcare Entity-Entity Representation learning&#65289;&#65292;&#23558;&#39046;&#22495;&#29305;&#23450;&#30340;&#32422;&#26463;&#21644;&#29305;&#24449;&#32435;&#20837;&#21040;&#22270;&#23884;&#20837;&#31639;&#27861;&#20013;&#12290;&#23545;&#22810;&#20010;&#22522;&#20934;&#25968;&#25454;&#38598;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;&#19982;&#26368;&#20808;&#36827;&#30340;&#26041;&#27861;&#30456;&#27604;&#65292;HEER&#22312;&#25913;&#21892;&#19979;&#28216;&#39044;&#27979;&#20219;&#21153;&#26041;&#38754;&#20855;&#26377;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Knowledge graphs (KGs) are a popular way to organise information based on ontologies or schemas and have been used across a variety of scenarios from search to recommendation. Despite advances in KGs, representing knowledge remains a non-trivial task across industries and it is especially challenging in the biomedical and healthcare domains due to complex interdependent relations between entities, heterogeneity, lack of standardization, and sparseness of data. KGs are used to discover diagnoses or prioritize genes relevant to disease, but they often rely on schemas that are not centred around a node or entity of interest, such as a person. Entity-centric KGs are relatively unexplored but hold promise in representing important facets connected to a central node and unlocking downstream tasks beyond graph traversal and reasoning, such as generating graph embeddings and training graph neural networks for a wide range of predictive tasks. This paper presents an end-to-end representation le
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23433;&#20840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26894;&#31649;&#26426;&#22120;&#20154;&#25163;&#26415;&#26415;&#20013;&#35268;&#21010;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30456;&#23545;&#20256;&#32479;&#30340;&#26415;&#21069;&#35268;&#21010;&#21644;&#26415;&#20013;&#27880;&#20876;&#24320;&#29615;&#25511;&#21046;&#30340;&#23616;&#38480;&#65292;&#33021;&#22815;&#25552;&#39640;&#25918;&#32622;&#20934;&#30830;&#24230;&#21644;&#20445;&#35777;&#25163;&#26415;&#23433;&#20840;&#12290;</title><link>http://arxiv.org/abs/2305.05354</link><description>&lt;p&gt;
&#33034;&#26609;&#34701;&#21512;&#25163;&#26415;&#20013;&#32463;&#30382;&#26894;&#24339;&#26681;&#34746;&#38025;&#25918;&#32622;&#30340;&#23433;&#20840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Safe Deep RL for Intraoperative Planning of Pedicle Screw Placement. (arXiv:2305.05354v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05354
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#23433;&#20840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#30340;&#26894;&#31649;&#26426;&#22120;&#20154;&#25163;&#26415;&#26415;&#20013;&#35268;&#21010;&#26041;&#27861;&#65292;&#35299;&#20915;&#20102;&#30456;&#23545;&#20256;&#32479;&#30340;&#26415;&#21069;&#35268;&#21010;&#21644;&#26415;&#20013;&#27880;&#20876;&#24320;&#29615;&#25511;&#21046;&#30340;&#23616;&#38480;&#65292;&#33021;&#22815;&#25552;&#39640;&#25918;&#32622;&#20934;&#30830;&#24230;&#21644;&#20445;&#35777;&#25163;&#26415;&#23433;&#20840;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33034;&#26609;&#34701;&#21512;&#25163;&#26415;&#38656;&#35201;&#39640;&#31934;&#24230;&#23454;&#26045;&#26894;&#24339;&#26681;&#34746;&#38025;&#26893;&#20837;&#65292;&#20854;&#24517;&#39035;&#22312;&#26497;&#24230;&#25509;&#36817;&#37325;&#35201;&#32467;&#26500;&#24182;&#19988;&#35299;&#21078;&#35270;&#37326;&#26377;&#38480;&#30340;&#24773;&#20917;&#19979;&#36827;&#34892;&#12290;&#34429;&#28982;&#25552;&#20986;&#20102;&#26426;&#22120;&#20154;&#25163;&#26415;&#31995;&#32479;&#20197;&#25913;&#21892;&#25918;&#32622;&#20934;&#30830;&#24230;&#65292;&#20294;&#26159;&#29616;&#26377;&#31995;&#32479;&#20173;&#28982;&#23384;&#22312;&#20256;&#32479;&#30340;&#26415;&#21069;&#35268;&#21010;&#21644;&#26415;&#20013;&#27880;&#20876;&#24320;&#29615;&#25511;&#21046;&#30340;&#23616;&#38480;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#37319;&#29992;&#23433;&#20840;&#28145;&#24230;&#24378;&#21270;&#23398;&#20064;&#36827;&#34892;&#23454;&#26102;&#35266;&#23519;&#30340;&#26894;&#31649;&#26426;&#22120;&#20154;&#25163;&#26415;&#26415;&#20013;&#35268;&#21010;&#26041;&#27861;&#12290;&#25105;&#20204;&#20027;&#35201;&#30340;&#36129;&#29486;&#26377;&#65306;&#65288;1&#65289;&#36890;&#36807;&#24341;&#20837;&#22522;&#20110;&#36317;&#31163;&#30340;&#19981;&#30830;&#23450;&#24615;&#24863;&#30693;&#23433;&#20840;&#36807;&#28388;&#22120;&#20445;&#35777;&#23433;&#20840;&#25805;&#20316;&#33021;&#21147;&#65307;&#65288;2&#65289;&#36890;&#36807;&#20351;&#29992;&#20808;&#39564;&#35299;&#21078;&#32467;&#26500;&#20449;&#24687;&#23545;&#19981;&#23436;&#25972;&#30340;&#26415;&#20013;&#35299;&#21078;&#20449;&#24687;&#36827;&#34892;&#34917;&#20607;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spinal fusion surgery requires highly accurate implantation of pedicle screw implants, which must be conducted in critical proximity to vital structures with a limited view of anatomy. Robotic surgery systems have been proposed to improve placement accuracy, however, state-of-the-art systems suffer from the limitations of open-loop approaches, as they follow traditional concepts of preoperative planning and intraoperative registration, without real-time recalculation of the surgical plan. In this paper, we propose an intraoperative planning approach for robotic spine surgery that leverages real-time observation for drill path planning based on Safe Deep Reinforcement Learning (DRL). The main contributions of our method are (1) the capability to guarantee safe actions by introducing an uncertainty-aware distance-based safety filter; and (2) the ability to compensate for incomplete intraoperative anatomical information, by encoding a-priori knowledge about anatomical structures with a ne
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#39532;&#36874;&#30340;&#26032;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23558;&#39038;&#23458;&#30340;&#22312;&#32447;&#34892;&#20026;&#26144;&#23556;&#25104;&#20026;&#39640;&#32423;&#21035;&#36141;&#29289;&#24847;&#22270;&#65292;&#20197;&#20415;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#25552;&#20379;&#26356;&#30456;&#20851;&#12289;&#21487;&#35299;&#37322;&#21644;&#22810;&#26679;&#21270;&#30340;&#36141;&#29289;&#20307;&#39564;&#12290;</title><link>http://arxiv.org/abs/2305.05279</link><description>&lt;p&gt;
&#23398;&#20064;&#20010;&#24615;&#21270;&#25512;&#33616;&#20197;&#22522;&#20110;&#23458;&#25143;&#36141;&#29289;&#24847;&#22270;
&lt;/p&gt;
&lt;p&gt;
Learning to Personalize Recommendation based on Customers' Shopping Intents. (arXiv:2305.05279v1 [cs.IR])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05279
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#20122;&#39532;&#36874;&#30340;&#26032;&#31995;&#32479;&#65292;&#21033;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23558;&#39038;&#23458;&#30340;&#22312;&#32447;&#34892;&#20026;&#26144;&#23556;&#25104;&#20026;&#39640;&#32423;&#21035;&#36141;&#29289;&#24847;&#22270;&#65292;&#20197;&#20415;&#20010;&#24615;&#21270;&#25512;&#33616;&#65292;&#25552;&#20379;&#26356;&#30456;&#20851;&#12289;&#21487;&#35299;&#37322;&#21644;&#22810;&#26679;&#21270;&#30340;&#36141;&#29289;&#20307;&#39564;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#29702;&#35299;&#39038;&#23458;&#30340;&#39640;&#32423;&#21035;&#36141;&#29289;&#24847;&#22270;&#65292;&#22914;&#20182;&#20204;&#21435;&#38706;&#33829;&#25110;&#20030;&#21150;&#29983;&#26085;&#27966;&#23545;&#30340;&#24895;&#26395;&#65292;&#23545;&#20110;&#30005;&#21830;&#24179;&#21488;&#38750;&#24120;&#37325;&#35201;&#65307;&#23427;&#21487;&#20197;&#36890;&#36807;&#25552;&#20379;&#26356;&#30456;&#20851;&#12289;&#21487;&#35299;&#37322;&#21644;&#22810;&#26679;&#21270;&#30340;&#25512;&#33616;&#26469;&#25552;&#39640;&#36141;&#29289;&#20307;&#39564;&#30340;&#36136;&#37327;&#12290;&#28982;&#32780;&#65292;&#30001;&#20110;&#23454;&#38469;&#25361;&#25112;&#65292;&#36825;&#31181;&#39640;&#32423;&#21035;&#30340;&#36141;&#29289;&#24847;&#22270;&#22312;&#34892;&#19994;&#20013;&#34987;&#24573;&#35270;&#12290;&#22312;&#36825;&#39033;&#24037;&#20316;&#20013;&#65292;&#25105;&#20204;&#20171;&#32461;&#20102;&#20122;&#39532;&#36874;&#30340;&#26032;&#31995;&#32479;&#65292;&#26126;&#30830;&#22320;&#35782;&#21035;&#21644;&#21033;&#29992;&#27599;&#20010;&#23458;&#25143;&#30340;&#39640;&#32423;&#21035;&#36141;&#29289;&#24847;&#22270;&#26469;&#20010;&#24615;&#21270;&#25512;&#33616;&#12290;&#25105;&#20204;&#24320;&#21457;&#20102;&#19968;&#31181;&#26032;&#25216;&#26415;&#65292;&#33258;&#21160;&#35782;&#21035;&#20122;&#39532;&#36874;&#23458;&#25143;&#27491;&#22312;&#36861;&#27714;&#30340;&#21508;&#31181;&#39640;&#32423;&#21035;&#30446;&#26631;&#65292;&#22914;&#8220;&#21435;&#38706;&#33829;&#8221;&#21644;&#8220;&#20934;&#22791;&#28023;&#28393;&#27966;&#23545;&#8221;&#12290;&#25105;&#20204;&#30340;&#35299;&#20915;&#26041;&#26696;&#36827;&#34892;&#20102;&#25193;&#23637;&#65288;&#36328;&#36234;21&#20010;&#22269;&#23478;&#30340;14&#31181;&#35821;&#35328;&#65289;&#12290;&#28982;&#21518;&#65292;&#19968;&#20010;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#23558;&#27599;&#20010;&#23458;&#25143;&#30340;&#22312;&#32447;&#34892;&#20026;&#65292;&#22914;&#20135;&#21697;&#25628;&#32034;&#21644;&#20010;&#20307;&#39033;&#30446;&#21442;&#19982;&#65292;&#26144;&#23556;&#25104;&#19968;&#32452;&#39640;&#32423;&#21035;&#30340;&#36141;&#29289;&#24847;&#22270;&#12290;
&lt;/p&gt;
&lt;p&gt;
Understanding the customers' high level shopping intent, such as their desire to go camping or hold a birthday party, is critically important for an E-commerce platform; it can help boost the quality of shopping experience by enabling provision of more relevant, explainable, and diversified recommendations. However, such high level shopping intent has been overlooked in the industry due to practical challenges. In this work, we introduce Amazon's new system that explicitly identifies and utilizes each customer's high level shopping intents for personalizing recommendations. We develop a novel technique that automatically identifies various high level goals being pursued by the Amazon customers, such as "go camping", and "preparing for a beach party". Our solution is in a scalable fashion (in 14 languages across 21 countries). Then a deep learning model maps each customer's online behavior, e.g. product search and individual item engagements, into a subset of high level shopping intents
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;</title><link>http://arxiv.org/abs/2305.05252</link><description>&lt;p&gt;
&#20174;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#25552;&#21462;&#33050;&#26412;&#30693;&#35782;&#20197;&#36827;&#34892;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;
&lt;/p&gt;
&lt;p&gt;
Distilling Script Knowledge from Large Language Models for Constrained Language Planning. (arXiv:2305.05252v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.05252
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#65292;&#25552;&#20986;&#20102;&#19968;&#31181;&#26041;&#27861;&#26469;&#25552;&#39640;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#25552;&#21462;&#20102;&#19968;&#20010;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;&#12290;&#23454;&#39564;&#35777;&#26126;&#35813;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;&#20854;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#24182;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;&#35821;&#35328;&#27169;&#22411;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#38750;&#24120;&#26377;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#26085;&#24120;&#29983;&#27963;&#20013;&#65292;&#20154;&#20204;&#32463;&#24120;&#36890;&#36807;&#36981;&#24490;&#30446;&#26631;&#23548;&#21521;&#30340;&#33050;&#26412;&#24418;&#24335;&#30340;&#36880;&#27493;&#35828;&#26126;&#26469;&#35268;&#21010;&#33258;&#24049;&#30340;&#34892;&#21160;&#12290;&#20197;&#24448;&#30340;&#24037;&#20316;&#21033;&#29992;&#35821;&#35328;&#27169;&#22411;&#65288;LM&#65289;&#26469;&#20026;&#31435;&#20307;&#27963;&#21160;&#30340;&#25277;&#35937;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#36827;&#34892;&#35268;&#21010;&#65292;&#20294;&#23545;&#20110;&#20855;&#26377;&#22810;&#26041;&#38754;&#32422;&#26463;&#30340;&#26356;&#20855;&#20307;&#30446;&#26631;&#65288;&#20363;&#22914;&#65292;&#8220;&#20026;&#31958;&#23615;&#30149;&#24739;&#32773;&#21046;&#20316;&#34507;&#31957;&#8221;&#65289;&#40092;&#26377;&#30740;&#31350;&#12290;&#26412;&#25991;&#39318;&#27425;&#23450;&#20041;&#20102;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#20219;&#21153;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#36807;&#24230;&#29983;&#25104;&#24182;&#36807;&#28388;&#30340;&#26041;&#27861;&#26469;&#25913;&#21892;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#22312;&#36825;&#20010;&#20219;&#21153;&#20013;&#30340;&#34920;&#29616;&#65292;&#24182;&#21033;&#29992;&#23427;&#26469;&#25552;&#21462;&#19968;&#31181;&#26032;&#39062;&#30340;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#25968;&#25454;&#38598;CoScript&#65292;&#20854;&#20013;&#21253;&#25324;55,000&#20010;&#33050;&#26412;&#12290;&#23454;&#39564;&#35777;&#26126;&#65292;&#25105;&#20204;&#30340;&#26041;&#27861;&#26174;&#33879;&#25552;&#39640;&#20102;LLM&#22312;&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#26041;&#38754;&#30340;&#33021;&#21147;&#65292;&#29305;&#21035;&#26159;&#22312;&#32422;&#26463;&#24544;&#23454;&#24230;&#26041;&#38754;&#12290;&#27492;&#22806;&#65292;CoScript&#34987;&#35777;&#26126;&#23545;&#36171;&#20104;&#36739;&#23567;&#30340;LM&#21463;&#38480;&#35821;&#35328;&#35268;&#21010;&#33021;&#21147;&#26159;&#38750;&#24120;&#26377;&#25928;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
In everyday life, humans often plan their actions by following step-by-step instructions in the form of goal-oriented scripts. Previous work has exploited language models (LMs) to plan for abstract goals of stereotypical activities (e.g., "make a cake"), but leaves more specific goals with multi-facet constraints understudied (e.g., "make a cake for diabetics"). In this paper, we define the task of constrained language planning for the first time. We propose an overgenerate-then-filter approach to improve large language models (LLMs) on this task, and use it to distill a novel constrained language planning dataset, CoScript, which consists of 55,000 scripts. Empirical results demonstrate that our method significantly improves the constrained language planning ability of LLMs, especially on constraint faithfulness. Furthermore, CoScript is demonstrated to be quite effective in endowing smaller LMs with constrained language planning ability.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedZKP&#30340;&#32852;&#37030;&#27169;&#22411;&#25152;&#26377;&#26435;&#39564;&#35777;&#26041;&#26696;&#65292;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#65292;&#21487;&#22312;&#19981;&#20844;&#24320;&#20973;&#25454;&#30340;&#24773;&#20917;&#19979;&#25269;&#24481;&#21508;&#31181;&#25915;&#20987;&#65292;&#24182;&#22312;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#20854;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;</title><link>http://arxiv.org/abs/2305.04507</link><description>&lt;p&gt;
FedZKP&#65306;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#30340;&#32852;&#37030;&#27169;&#22411;&#25152;&#26377;&#26435;&#39564;&#35777;
&lt;/p&gt;
&lt;p&gt;
FedZKP: Federated Model Ownership Verification with Zero-knowledge Proof. (arXiv:2305.04507v2 [cs.CR] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04507
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;FedZKP&#30340;&#32852;&#37030;&#27169;&#22411;&#25152;&#26377;&#26435;&#39564;&#35777;&#26041;&#26696;&#65292;&#20351;&#29992;&#38646;&#30693;&#35782;&#35777;&#26126;&#65292;&#21487;&#22312;&#19981;&#20844;&#24320;&#20973;&#25454;&#30340;&#24773;&#20917;&#19979;&#25269;&#24481;&#21508;&#31181;&#25915;&#20987;&#65292;&#24182;&#22312;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#30740;&#31350;&#20013;&#35777;&#26126;&#20102;&#20854;&#23433;&#20840;&#24615;&#21644;&#20934;&#30830;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32852;&#37030;&#23398;&#20064;&#20801;&#35768;&#22810;&#20010;&#21442;&#19982;&#26041;&#22312;&#19981;&#20849;&#20139;&#31169;&#26377;&#25968;&#25454;&#30340;&#24773;&#20917;&#19979;&#21512;&#20316;&#23398;&#20064;&#32852;&#37030;&#27169;&#22411;&#12290;&#20026;&#20102;&#20445;&#25252;&#36825;&#31181;&#32852;&#37030;&#27169;&#22411;&#20813;&#21463;&#25220;&#34989;&#25110;&#28389;&#29992;&#30340;&#20405;&#23475;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#21487;&#35777;&#26126;&#23433;&#20840;&#30340;&#27169;&#22411;&#25152;&#26377;&#26435;&#39564;&#35777;&#26041;&#26696;&#65292;&#31216;&#20026;FedZKP&#12290;&#30740;&#31350;&#34920;&#26126;&#65292;FedZKP&#26041;&#26696;&#22312;&#19981;&#20844;&#24320;&#20973;&#25454;&#30340;&#24773;&#20917;&#19979;&#65292;&#33021;&#22815;&#25269;&#24481;&#21508;&#31181;&#29616;&#26377;&#21644;&#28508;&#22312;&#30340;&#25915;&#20987;&#12290;&#29702;&#35770;&#20998;&#26512;&#21644;&#23454;&#35777;&#30740;&#31350;&#22343;&#35777;&#26126;&#20102;FedZKP&#30340;&#23433;&#20840;&#24615;&#65292;&#25915;&#20987;&#32773;&#30772;&#35299;&#35813;&#26041;&#26696;&#30340;&#27010;&#29575;&#21487;&#20197;&#24573;&#30053;&#19981;&#35745;&#12290;&#27492;&#22806;&#65292;&#24191;&#27867;&#30340;&#23454;&#39564;&#32467;&#26524;&#35777;&#23454;&#20102;&#25105;&#20204;&#26041;&#26696;&#30340;&#20934;&#30830;&#24615;&#21644;&#40065;&#26834;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Federated learning (FL) allows multiple parties to cooperatively learn a federated model without sharing private data with each other. The need of protecting such federated models from being plagiarized or misused, therefore, motivates us to propose a provable secure model ownership verification scheme using zero-knowledge proof, named FedZKP. It is shown that the FedZKP scheme without disclosing credentials is guaranteed to defeat a variety of existing and potential attacks. Both theoretical analysis and empirical studies demonstrate the security of FedZKP in the sense that the probability for attackers to breach the proposed FedZKP is negligible. Moreover, extensive experimental results confirm the fidelity and robustness of our scheme.
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#34920;&#31034;AST&#65292;&#24182;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#24418;&#36827;&#34892;&#20195;&#30721;&#20998;&#31867;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2305.04228</link><description>&lt;p&gt;
&#22522;&#20110;&#25277;&#35937;&#35821;&#27861;&#26641;&#30340;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#29992;&#20110;&#20195;&#30721;&#20998;&#31867;
&lt;/p&gt;
&lt;p&gt;
Heterogeneous Directed Hypergraph Neural Network over abstract syntax tree (AST) for Code Classification. (arXiv:2305.04228v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#34920;&#31034;AST&#65292;&#24182;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#22788;&#29702;&#22270;&#24418;&#36827;&#34892;&#20195;&#30721;&#20998;&#31867;&#65292;&#36229;&#36807;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20195;&#30721;&#20998;&#31867;&#26159;&#31243;&#24207;&#29702;&#35299;&#21644;&#33258;&#21160;&#32534;&#30721;&#20013;&#30340;&#19968;&#20010;&#38590;&#39064;&#12290;&#30001;&#20110;&#31243;&#24207;&#30340;&#27169;&#31946;&#35821;&#27861;&#21644;&#22797;&#26434;&#35821;&#20041;&#65292;&#22823;&#22810;&#25968;&#29616;&#26377;&#30740;&#31350;&#20351;&#29992;&#22522;&#20110;&#25277;&#35937;&#35821;&#27861;&#26641;&#65288;AST&#65289;&#21644;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;GNN&#65289;&#30340;&#25216;&#26415;&#21019;&#24314;&#20195;&#30721;&#34920;&#31034;&#29992;&#20110;&#20195;&#30721;&#20998;&#31867;&#12290;&#36825;&#20123;&#25216;&#26415;&#21033;&#29992;&#20195;&#30721;&#30340;&#32467;&#26500;&#21644;&#35821;&#20041;&#20449;&#24687;&#65292;&#20294;&#21482;&#32771;&#34385;&#33410;&#28857;&#20043;&#38388;&#30340;&#25104;&#23545;&#20851;&#31995;&#65292;&#24573;&#30053;&#20102;AST&#20013;&#33410;&#28857;&#20043;&#38388;&#24050;&#32463;&#23384;&#22312;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#65292;&#21487;&#33021;&#23548;&#33268;&#20195;&#30721;&#32467;&#26500;&#20449;&#24687;&#30340;&#20002;&#22833;&#12290;&#26412;&#30740;&#31350;&#25552;&#20986;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#65288;HDHG&#65289;&#34920;&#31034;AST&#65292;&#24182;&#20351;&#29992;&#24322;&#26500;&#26377;&#21521;&#36229;&#22270;&#31070;&#32463;&#32593;&#32476;&#65288;HDHGN&#65289;&#22788;&#29702;&#22270;&#24418;&#12290;HDHG&#20445;&#30041;&#20102;&#33410;&#28857;&#20043;&#38388;&#30340;&#39640;&#38454;&#30456;&#20851;&#24615;&#65292;&#24182;&#26356;&#20840;&#38754;&#22320;&#32534;&#30721;&#20102;AST&#30340;&#35821;&#20041;&#21644;&#32467;&#26500;&#20449;&#24687;&#12290;HDHGN&#36890;&#36807;&#32858;&#21512;&#19981;&#21516;&#33410;&#28857;&#30340;&#29305;&#24449;&#24182;&#20351;&#29992;&#19981;&#21516;&#30340;&#20989;&#25968;&#23545;&#20854;&#36827;&#34892;&#22788;&#29702;&#26469;&#23545;AST&#36827;&#34892;&#24314;&#27169;&#12290;&#22312;&#22235;&#20010;&#25968;&#25454;&#38598;&#19978;&#30340;&#23454;&#39564;&#34920;&#26126;&#65292;HDHG&#21644;HDHGN&#22312;&#20195;&#30721;&#20998;&#31867;&#20219;&#21153;&#20013;&#36229;&#36234;&#20102;&#29616;&#26377;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Code classification is a difficult issue in program understanding and automatic coding. Due to the elusive syntax and complicated semantics in programs, most existing studies use techniques based on abstract syntax tree (AST) and graph neural network (GNN) to create code representations for code classification. These techniques utilize the structure and semantic information of the code, but they only take into account pairwise associations and neglect the high-order correlations that already exist between nodes in the AST, which may result in the loss of code structural information. On the other hand, while a general hypergraph can encode high-order data correlations, it is homogeneous and undirected which will result in a lack of semantic and structural information such as node types, edge types, and directions between child nodes and parent nodes when modeling AST. In this study, we propose to represent AST as a heterogeneous directed hypergraph (HDHG) and process the graph by hetero
&lt;/p&gt;</description></item><item><title>&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-LLM&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#23545;&#20110;LLM&#21152;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#31350;&#21644;&#25299;&#23637;&#12290;</title><link>http://arxiv.org/abs/2305.04160</link><description>&lt;p&gt;
X-LLM: &#36890;&#36807;&#23558;&#22810;&#27169;&#24577;&#35270;&#20026;&#22806;&#35821;&#24341;&#20837;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#26469;&#21551;&#21160;&#39640;&#32423;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
X-LLM: Bootstrapping Advanced Large Language Models by Treating Multi-Modalities as Foreign Languages. (arXiv:2305.04160v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.04160
&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;X-LLM&#30340;&#26041;&#27861;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#20174;&#32780;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#23545;&#20110;LLM&#21152;&#20837;&#22810;&#27169;&#24577;&#20449;&#24687;&#30340;&#33021;&#21147;&#36827;&#34892;&#20102;&#25506;&#31350;&#21644;&#25299;&#23637;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;LLM&#65289;&#23637;&#31034;&#20102;&#21331;&#36234;&#30340;&#35821;&#35328;&#33021;&#21147;&#12290;&#22522;&#20110;&#39640;&#32423;LLM&#30340;GPT-4&#34920;&#29616;&#20986;&#36229;&#24120;&#30340;&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#36229;&#36234;&#20102;&#20197;&#24448;&#30340;&#35270;&#35273;&#35821;&#35328;&#27169;&#22411;&#12290;&#25105;&#20204;&#23558;&#36825;&#24402;&#21151;&#20110;&#19982;&#20197;&#21069;&#30340;&#22810;&#27169;&#24577;&#27169;&#22411;&#30456;&#27604;&#20351;&#29992;&#20102;&#26356;&#20808;&#36827;&#30340;LLM&#12290;&#20294;&#19981;&#24184;&#30340;&#26159;&#65292;GPT-4&#30340;&#27169;&#22411;&#26550;&#26500;&#21644;&#35757;&#32451;&#31574;&#30053;&#26159;&#26410;&#30693;&#30340;&#12290;&#20026;&#20102;&#36171;&#20104;LLM&#22810;&#27169;&#24577;&#33021;&#21147;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;X-LLM&#65292;&#36890;&#36807;&#20351;&#29992;X2L&#25509;&#21475;&#23558;&#22810;&#27169;&#24577;&#65288;&#22270;&#20687;&#12289;&#35821;&#38899;&#12289;&#35270;&#39057;&#65289;&#36716;&#25442;&#20026;&#22806;&#35821;&#24182;&#23558;&#20854;&#36755;&#20837;&#21040;&#22823;&#22411;&#35821;&#35328;&#27169;&#22411;&#65288;ChatGLM&#65289;&#20013;&#12290;&#20855;&#20307;&#32780;&#35328;&#65292;X-LLM&#20351;&#29992;X2L&#25509;&#21475;&#23558;&#22810;&#20010;&#20923;&#32467;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#21644;&#20923;&#32467;&#30340;LLM&#23545;&#40784;&#65292;&#20854;&#20013;&#8220;X&#8221;&#34920;&#31034;&#22810;&#27169;&#24577;&#65292;&#20363;&#22914;&#22270;&#20687;&#12289;&#35821;&#38899;&#21644;&#35270;&#39057;&#65292;&#8220;L&#8221;&#34920;&#31034;&#35821;&#35328;&#12290;X-LLM&#30340;&#35757;&#32451;&#30001;&#19977;&#20010;&#38454;&#27573;&#32452;&#25104;&#65306;&#65288;1&#65289;&#36716;&#25442;&#22810;&#27169;&#24577;&#20449;&#24687;&#65306;&#31532;&#19968;&#38454;&#27573;&#20998;&#21035;&#35757;&#32451;&#27599;&#20010;X2L&#25509;&#21475;&#19982;&#20854;&#21508;&#33258;&#30340;&#21333;&#27169;&#24577;&#32534;&#30721;&#22120;&#23545;&#40784;&#65292;&#23558;&#22810;&#27169;&#24577;&#20449;&#24687;&#36716;&#25442;&#20026;&#22806;&#35821;&#36755;&#20837;&#21040;ChatGLM&#20013;&#12290;...
&lt;/p&gt;
&lt;p&gt;
Large language models (LLMs) have demonstrated remarkable language abilities. GPT-4, based on advanced LLMs, exhibits extraordinary multimodal capabilities beyond previous visual language models. We attribute this to the use of more advanced LLMs compared with previous multimodal models. Unfortunately, the model architecture and training strategies of GPT-4 are unknown. To endow LLMs with multimodal capabilities, we propose X-LLM, which converts Multi-modalities (images, speech, videos) into foreign languages using X2L interfaces and inputs them into a large Language model (ChatGLM). Specifically, X-LLM aligns multiple frozen single-modal encoders and a frozen LLM using X2L interfaces, where ``X'' denotes multi-modalities such as image, speech, and videos, and ``L'' denotes languages. X-LLM's training consists of three stages: (1) Converting Multimodal Information: The first stage trains each X2L interface to align with its respective single-modal encoder separately to convert multimod
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2305.01876</link><description>&lt;p&gt;
&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#21477;&#23376;&#25552;&#21462;
&lt;/p&gt;
&lt;p&gt;
Causality-aware Concept Extraction based on Knowledge-guided Prompting. (arXiv:2305.01876v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2305.01876
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#22240;&#26524;&#24863;&#30693;&#30340;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#30340;&#21477;&#23376;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#32531;&#35299;&#27010;&#24565;&#20559;&#24046;&#12290;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#24191;&#27867;&#23454;&#39564;&#65292;&#33719;&#24471;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27010;&#24565;&#26377;&#21161;&#20110;&#33258;&#28982;&#35821;&#35328;&#29702;&#35299;&#65292;&#20294;&#29616;&#26377;&#30340;&#30693;&#35782;&#22270;&#35889;&#65288;KG&#65289;&#20013;&#36828;&#26410;&#23436;&#21892;&#12290;&#26368;&#36817;&#65292;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#65288;PLM&#65289;&#24050;&#34987;&#24191;&#27867;&#29992;&#20110;&#22522;&#20110;&#25991;&#26412;&#30340;&#27010;&#24565;&#25552;&#21462;&#65288;CE&#65289;&#12290;&#28982;&#32780;&#65292;PLM&#24448;&#24448;&#20174;&#22823;&#37327;&#35821;&#26009;&#24211;&#30340;&#20849;&#29616;&#20851;&#32852;&#20013;&#36827;&#34892;&#39044;&#35757;&#32451;&#30693;&#35782;&#25366;&#25496;&#65292;&#32780;&#38750;Token&#20043;&#38388;&#30340;&#30495;&#23454;&#22240;&#26524;&#20851;&#31995;&#12290;&#22240;&#27492;&#65292;&#39044;&#35757;&#32451;&#30693;&#35782;&#28151;&#28102;&#20102;PLM&#65292;&#23548;&#33268;&#25552;&#21462;&#22522;&#20110;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#30340;&#26377;&#20559;&#27010;&#24565;&#65292;&#19981;&#21487;&#36991;&#20813;&#22320;&#23548;&#33268;&#20302;&#31934;&#24230;&#12290;&#26412;&#25991;&#36890;&#36807;&#32467;&#26500;&#22240;&#26524;&#27169;&#22411;&#65288;SCM&#65289;&#25552;&#20986;&#20102;&#19968;&#31181;&#30693;&#35782;&#24341;&#23548;&#25552;&#31034;&#26041;&#27861;&#65292;&#23558;&#20854;&#20316;&#20026;&#24178;&#39044;&#22120;&#35013;&#22791;&#21040;&#22522;&#20110;PLM&#30340;&#25552;&#21462;&#22120;&#20013;&#65292;&#20197;&#20943;&#36731;&#27010;&#24565;&#20559;&#24046;&#12290;&#25552;&#31034;&#37319;&#29992;&#29616;&#26377;KG&#20013;&#30340;&#32473;&#23450;&#23454;&#20307;&#20027;&#39064;&#26469;&#32531;&#35299;&#23454;&#20307;&#21644;&#26377;&#20559;&#27010;&#24565;&#20043;&#38388;&#30340;&#34394;&#20551;&#20849;&#29616;&#30456;&#20851;&#24615;&#12290;&#25105;&#20204;&#22312;&#20195;&#34920;&#24615;&#30340;&#22810;&#35821;&#35328;KG&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#20102;&#24191;&#27867;&#30340;&#23454;&#39564;&#65292;&#35777;&#26126;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#25552;&#31034;&#26174;&#33879;&#25913;&#36827;&#20102;&#25552;&#21462;&#24615;&#33021;&#65292;&#24182;&#36798;&#21040;&#20102;&#26368;&#20808;&#36827;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;
Concepts benefit natural language understanding but are far from complete in existing knowledge graphs (KGs). Recently, pre-trained language models (PLMs) have been widely used in text-based concept extraction (CE). However, PLMs tend to mine the co-occurrence associations from massive corpus as pre-trained knowledge rather than the real causal effect between tokens.As a result, the pre-trained knowledge confounds PLMs to extract biased concepts based on spurious co-occurrence correlations, inevitably resulting in low precision. In this paper, through the lens of a Structural Causal Model (SCM), we propose equipping the PLM-based extractor with a knowledge-guided prompt as an intervention to alleviate concept bias. The prompt adopts the topic of the given entity from the existing knowledge in KGs to mitigate the spurious co-occurrence correlations between entities and biased concepts. Our extensive experiments on representative multilingual KG datasets justify that our proposed prompt 
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;(TSDiff)&#65292;&#29992;&#20110;&#20174;&#20108;&#32500;&#20998;&#23376;&#22270;&#20013;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#12290;&#19982;&#29616;&#26377;&#20855;&#26377; 3D &#20960;&#20309;&#32467;&#26500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;TSdiff &#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#37117;&#26356;&#39640;&#12290;TSDiff &#33021;&#22815;&#25214;&#21040;&#27604;&#21442;&#32771;&#25968;&#25454;&#24211;&#26356;&#20248;&#21270;&#30340;&#21453;&#24212;&#36884;&#24452;&#65292;&#22312;&#21453;&#24212;&#21183;&#22418;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26356;&#20026;&#20248;&#21270;&#30340;&#21453;&#24212;&#36335;&#24452;&#12290;</title><link>http://arxiv.org/abs/2304.12233</link><description>&lt;p&gt;
&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#24335;&#20154;&#24037;&#26234;&#33021;&#22312;&#25506;&#32034;&#20108;&#32500;&#20998;&#23376;&#22270;&#30340;&#36807;&#28193;&#24577;&#19978;&#30340;&#24212;&#29992;
&lt;/p&gt;
&lt;p&gt;
Diffusion-based Generative AI for Exploring Transition States from 2D Molecular Graphs. (arXiv:2304.12233v2 [physics.chem-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.12233
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#25193;&#25955;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;(TSDiff)&#65292;&#29992;&#20110;&#20174;&#20108;&#32500;&#20998;&#23376;&#22270;&#20013;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#12290;&#19982;&#29616;&#26377;&#20855;&#26377; 3D &#20960;&#20309;&#32467;&#26500;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#30456;&#27604;&#65292;TSdiff &#30340;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#37117;&#26356;&#39640;&#12290;TSDiff &#33021;&#22815;&#25214;&#21040;&#27604;&#21442;&#32771;&#25968;&#25454;&#24211;&#26356;&#20248;&#21270;&#30340;&#21453;&#24212;&#36884;&#24452;&#65292;&#22312;&#21453;&#24212;&#21183;&#22418;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26356;&#20026;&#20248;&#21270;&#30340;&#21453;&#24212;&#36335;&#24452;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25506;&#31350;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#23545;&#20110;&#38416;&#26126;&#21270;&#23398;&#21453;&#24212;&#26426;&#29702;&#21644;&#27169;&#25311;&#21453;&#24212;&#21160;&#21147;&#23398;&#33267;&#20851;&#37325;&#35201;&#12290;&#26368;&#36817;&#65292;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#22312;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#26041;&#38754;&#34920;&#29616;&#20986;&#20102;&#20986;&#33394;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#23427;&#20204;&#38656;&#35201;&#21453;&#24212;&#29289;&#21644;&#20135;&#29289;&#30340; 3D &#24418;&#24577;&#21644;&#26041;&#21521;&#20316;&#20026;&#36755;&#20837;&#65292;&#36825;&#38656;&#35201;&#22823;&#37327;&#30340;&#21162;&#21147;&#21644;&#35745;&#31639;&#25104;&#26412;&#12290;&#22312;&#36825;&#37324;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38543;&#26426;&#25193;&#25955;&#26041;&#27861;&#30340;&#29983;&#25104;&#24335;&#26041;&#27861;(TSDiff)&#65292;&#29992;&#20110;&#20174;&#20108;&#32500;&#20998;&#23376;&#22270;&#20013;&#39044;&#27979;&#36807;&#28193;&#24577;&#20960;&#20309;&#32467;&#26500;&#12290;TSDiff&#22312;&#20934;&#30830;&#24615;&#21644;&#25928;&#29575;&#26041;&#38754;&#20248;&#20110;&#29616;&#26377;&#30340;&#20855;&#26377; 3D &#20960;&#20309;&#32467;&#26500;&#30340;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#12290;&#27492;&#22806;&#65292;&#23427;&#21487;&#20197;&#20174;&#35757;&#32451;&#20013;&#20102;&#35299;&#21040;&#21508;&#31181;&#21453;&#24212;&#30340;&#36807;&#28193;&#24577;&#20960;&#20309;&#20998;&#24067;&#65292;&#20174;&#32780;&#33021;&#22815;&#37319;&#26679;&#21508;&#31181;&#36807;&#28193;&#24577;&#26500;&#35937;&#12290;&#22240;&#27492;&#65292;TSDiff &#33021;&#22815;&#25214;&#21040;&#27604;&#21442;&#32771;&#25968;&#25454;&#24211;&#20013;&#26356;&#20026;&#26377;&#21033;&#30340;&#21453;&#24212;&#36884;&#24452;&#65292;&#22312;&#21453;&#24212;&#21183;&#22418;&#26356;&#20302;&#30340;&#24773;&#20917;&#19979;&#25214;&#21040;&#26356;&#20026;&#20248;&#21270;&#30340;&#21453;&#24212;&#36335;&#24452;&#12290;&#36825;&#20123;&#32467;&#26524;&#34920;&#26126;&#65292;TSDiff &#22312;&#21152;&#36895;&#21270;&#23398;&#21453;&#24212;&#21644;&#21453;&#24212;&#36884;&#24452;&#30340;&#21457;&#29616;&#26041;&#38754;&#20855;&#26377;&#28508;&#22312;&#30340;&#20215;&#20540;&#12290;
&lt;/p&gt;
&lt;p&gt;
The exploration of transition state (TS) geometries is crucial for elucidating chemical reaction mechanisms and modeling their kinetics. Recently, machine learning (ML) models have shown remarkable performance for prediction of TS geometries. However, they require 3D conformations of reactants and products often with their appropriate orientations as input, which demands substantial efforts and computational cost. Here, we propose a generative approach based on the stochastic diffusion method, namely TSDiff, for prediction of TS geometries just from 2D molecular graphs. TSDiff outperformed the existing ML models with 3D geometries in terms of both accuracy and efficiency. Moreover, it enables to sample various TS conformations, because it learned the distribution of TS geometries for diverse reactions in training. Thus, TSDiff was able to find more favorable reaction pathways with lower barrier heights than those in the reference database. These results demonstrate that TSDiff shows pr
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;--&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#24494;&#35266;&#12289;&#20013;&#35266;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#33041;&#25299;&#25169;&#32467;&#26500;&#12290;MSE-NAS&#21487;&#20197;&#24110;&#21161;SNN&#23454;&#29616;&#22810;&#30005;&#36335;&#27169;&#24335;&#30340;&#33258;&#32452;&#32455;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#24615;&#30340;&#36328;&#27169;&#24335;&#36830;&#25509;&#26469;&#20248;&#21270;&#32593;&#32476;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.10749</link><description>&lt;p&gt;
&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#29992;&#20110;&#28145;&#24230;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;
&lt;/p&gt;
&lt;p&gt;
Multi-scale Evolutionary Neural Architecture Search for Deep Spiking Neural Networks. (arXiv:2304.10749v1 [cs.NE])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.10749
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#26041;&#27861;--&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65292;&#29992;&#20110;&#33258;&#21160;&#35774;&#35745;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65292;&#21516;&#26102;&#32771;&#34385;&#24494;&#35266;&#12289;&#20013;&#35266;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#33041;&#25299;&#25169;&#32467;&#26500;&#12290;MSE-NAS&#21487;&#20197;&#24110;&#21161;SNN&#23454;&#29616;&#22810;&#30005;&#36335;&#27169;&#24335;&#30340;&#33258;&#32452;&#32455;&#38598;&#25104;&#65292;&#24182;&#36890;&#36807;&#20840;&#23616;&#24615;&#30340;&#36328;&#27169;&#24335;&#36830;&#25509;&#26469;&#20248;&#21270;&#32593;&#32476;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#65288;SNN&#65289;&#19981;&#20165;&#22240;&#20854;&#31163;&#25955;&#20449;&#21495;&#22788;&#29702;&#30340;&#33021;&#28304;&#25928;&#29575;&#21331;&#36234;&#65292;&#32780;&#19988;&#22240;&#20854;&#22825;&#28982;&#36866;&#21512;&#20110;&#38598;&#25104;&#22810;&#23610;&#24230;&#29983;&#29289;&#21487;&#22609;&#24615;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#28982;&#32780;&#65292;&#22823;&#22810;&#25968;SNN&#30452;&#25509;&#37319;&#29992;&#25104;&#29087;&#30340;DNN&#32467;&#26500;&#65292;&#24456;&#23569;&#33258;&#21160;&#35774;&#35745;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;NAS&#65289;&#29992;&#20110;SNN&#12290;&#20154;&#31867;&#22823;&#33041;&#31070;&#32463;&#27169;&#24335;&#30340;&#25299;&#25169;&#32467;&#26500;&#65292;&#27169;&#22359;&#21270;&#30340;&#21306;&#22495;&#32467;&#26500;&#21644;&#20840;&#23616;&#24615;&#30340;&#36328;&#33041;&#21306;&#36830;&#25509;&#26159;&#33258;&#28982;&#36827;&#21270;&#30340;&#20135;&#29289;&#65292;&#21487;&#20197;&#20316;&#20026;&#35774;&#35745;&#22522;&#20110;&#33041;&#30340;SNN&#26550;&#26500;&#30340;&#23436;&#32654;&#21442;&#32771;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22810;&#23610;&#24230;&#36827;&#21270;&#31070;&#32463;&#26550;&#26500;&#25628;&#32034;&#65288;MSE-NAS&#65289;&#65292;&#21516;&#26102;&#32771;&#34385;&#24494;&#35266;&#12289;&#20013;&#35266;&#21644;&#23439;&#35266;&#23610;&#24230;&#30340;&#33041;&#25299;&#25169;&#20316;&#20026;&#36827;&#21270;&#25628;&#32034;&#31354;&#38388;&#12290; MSE-NAS&#36890;&#36807;&#22522;&#20110;&#22823;&#33041;&#21551;&#21457;&#30340;&#38388;&#25509;&#26041;&#24335;&#65292;&#36827;&#21270;&#21333;&#20010;&#31070;&#32463;&#20803;&#25805;&#20316;&#65292;&#22810;&#20010;&#30005;&#36335;&#27169;&#24335;&#30340;&#33258;&#32452;&#32455;&#38598;&#25104;&#20197;&#21450;&#36328;&#27169;&#24335;&#30340;&#20840;&#23616;&#36830;&#36890;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
Spiking Neural Networks (SNNs) have received considerable attention not only for their superiority in energy efficient with discrete signal processing, but also for their natural suitability to integrate multi-scale biological plasticity. However, most SNNs directly adopt the structure of the well-established DNN, rarely automatically design Neural Architecture Search (NAS) for SNNs. The neural motifs topology, modular regional structure and global cross-brain region connection of the human brain are the product of natural evolution and can serve as a perfect reference for designing brain-inspired SNN architecture. In this paper, we propose a Multi-Scale Evolutionary Neural Architecture Search (MSE-NAS) for SNN, simultaneously considering micro-, meso- and macro-scale brain topologies as the evolutionary search space. MSE-NAS evolves individual neuron operation, self-organized integration of multiple circuit motifs, and global connectivity across motifs through a brain-inspired indirec
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#26469;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20026;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#25688;&#35201;&#31561;&#20219;&#21153;&#24494;&#35843;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2304.01228</link><description>&lt;p&gt;
&#36890;&#36807;&#33258;&#25105;&#25913;&#36827;&#30340;&#26041;&#24335;&#23454;&#29616;&#26356;&#22909;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;
&lt;/p&gt;
&lt;p&gt;
Better Language Models of Code through Self-Improvement. (arXiv:2304.01228v1 [cs.CL])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2304.01228
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#26469;&#25913;&#21892;&#39044;&#35757;&#32451;&#35821;&#35328;&#27169;&#22411;&#20026;&#20195;&#30721;&#29983;&#25104;&#21644;&#20195;&#30721;&#25688;&#35201;&#31561;&#20219;&#21153;&#24494;&#35843;&#30340;&#29942;&#39048;&#38382;&#39064;&#65292;&#25552;&#39640;&#20102;&#27169;&#22411;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#26399;&#65292;&#21508;&#31181;&#39044;&#35757;&#32451;&#30340;&#20195;&#30721;&#35821;&#35328;&#27169;&#22411;&#24341;&#36215;&#20102;&#20154;&#20204;&#30340;&#24191;&#27867;&#20851;&#27880;&#12290;&#36825;&#20123;&#27169;&#22411;&#36890;&#36807;&#22810;&#27169;&#24335;&#30446;&#26631;&#22312;&#22823;&#35268;&#27169;&#25968;&#25454;&#38598;&#19978;&#36827;&#34892;&#39044;&#35757;&#32451;&#12290;&#20294;&#26159;&#65292;&#23545;&#20854;&#36827;&#34892;&#24494;&#35843;&#38656;&#35201;&#22823;&#37327;&#30417;&#30563;&#65292;&#24182;&#19988;&#21463;&#21040;&#25552;&#20379;&#30340;&#25968;&#25454;&#38598;&#35268;&#27169;&#30340;&#38480;&#21046;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#31616;&#21333;&#30340;&#25968;&#25454;&#22686;&#24378;&#26694;&#26550;&#20197;&#25913;&#21892;&#36825;&#20010;&#38382;&#39064;&#12290;&#36825;&#20010;&#26694;&#26550;&#21033;&#29992;&#20102;&#22312;&#39044;&#35757;&#32451;&#21644;&#24494;&#35843;&#38454;&#27573;&#33719;&#24471;&#30340;&#30693;&#35782;&#26469;&#29983;&#25104;&#20266;&#25968;&#25454;&#65292;&#24182;&#23558;&#20854;&#29992;&#20316;&#19979;&#19968;&#27493;&#30340;&#35757;&#32451;&#25968;&#25454;&#12290;&#25105;&#20204;&#23558;&#36825;&#20010;&#26694;&#26550;&#24212;&#29992;&#21040;&#26368;&#20808;&#36827;&#30340;&#35821;&#35328;&#27169;&#22411;&#20013;&#65292;&#22914;CodeT5&#12289;CodeBERT&#21644;UnixCoder&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#25105;&#20204;&#30340;&#26694;&#26550;&#26174;&#33879;&#25552;&#39640;&#20102;PLMC&#22312;&#19982;&#20195;&#30721;&#30456;&#20851;&#30340;&#24207;&#21015;&#29983;&#25104;&#20219;&#21153;&#20013;&#30340;&#24615;&#33021;&#65292;&#22914;CodeXGLUE&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#20195;&#30721;&#25688;&#35201;&#21644;&#20195;&#30721;&#29983;&#25104;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pre-trained language models for code (PLMCs) have gained attention in recent research. These models are pre-trained on large-scale datasets using multi-modal objectives. However, fine-tuning them requires extensive supervision and is limited by the size of the dataset provided. We aim to improve this issue by proposing a simple data augmentation framework. Our framework utilizes knowledge gained during the pre-training and fine-tuning stage to generate pseudo data, which is then used as training data for the next step. We incorporate this framework into the state-of-the-art language models, such as CodeT5, CodeBERT, and UnixCoder. The results show that our framework significantly improves PLMCs' performance in code-related sequence generation tasks, such as code summarization and code generation in the CodeXGLUE benchmark.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#33258;&#21160;&#39550;&#39542;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#31649;&#36947;&#35268;&#21010;&#21644;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#25552;&#20379;&#20102;&#35752;&#35770;&#65292;&#26377;&#21161;&#20110;&#20026;&#26234;&#33021;&#27773;&#36710;&#30340;&#21457;&#23637;&#25552;&#20379;&#26356;&#22909;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2303.09824</link><description>&lt;p&gt;
&#33258;&#21160;&#39550;&#39542;&#36335;&#24452;&#35268;&#21010;&#65306;&#29616;&#29366;&#19982;&#23637;&#26395;
&lt;/p&gt;
&lt;p&gt;
Path Planning for Autonomous Driving: The State of the Art and Perspectives. (arXiv:2303.09824v1 [cs.RO])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.09824
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#32508;&#36848;&#20102;&#29616;&#26377;&#30340;&#33258;&#21160;&#39550;&#39542;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#31649;&#36947;&#35268;&#21010;&#21644;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#12290;&#22312;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#26041;&#38754;&#25552;&#20379;&#20102;&#35752;&#35770;&#65292;&#26377;&#21161;&#20110;&#20026;&#26234;&#33021;&#27773;&#36710;&#30340;&#21457;&#23637;&#25552;&#20379;&#26356;&#22909;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26234;&#33021;&#27773;&#36710;&#30001;&#20110;&#25552;&#39640;&#30340;&#20415;&#21033;&#24615;&#12289;&#23433;&#20840;&#24615;&#21644;&#28508;&#22312;&#30340;&#21830;&#19994;&#20215;&#20540;&#32780;&#21463;&#21040;&#24191;&#27867;&#20851;&#27880;&#12290;&#20294;&#30001;&#20110;&#21508;&#31181;&#38382;&#39064;&#65292;&#22914;&#23433;&#20840;&#24615;&#12289;&#21487;&#38752;&#24615;&#21644;&#35268;&#21010;&#26041;&#27861;&#30340;&#27867;&#21270;&#31561;&#38480;&#21046;&#65292;&#23427;&#20204;&#30340;&#37096;&#32626;&#20173;&#23616;&#38480;&#20110;&#23567;&#35268;&#27169;&#39564;&#35777;&#38454;&#27573;&#12290;&#26412;&#25991;&#26088;&#22312;&#32508;&#36848;&#26368;&#20808;&#36827;&#30340;&#36335;&#24452;&#35268;&#21010;&#26041;&#27861;&#65292;&#21253;&#25324;&#31649;&#36947;&#35268;&#21010;&#21644;&#31471;&#21040;&#31471;&#35268;&#21010;&#26041;&#27861;&#12290;&#38024;&#23545;&#31649;&#36947;&#26041;&#27861;&#65292;&#26412;&#25991;&#25552;&#20379;&#20102;&#36873;&#21462;&#31639;&#27861;&#30340;&#27010;&#36848;&#65292;&#24182;&#35752;&#35770;&#20102;&#25193;&#23637;&#21644;&#20248;&#21270;&#26426;&#21046;&#65307;&#38024;&#23545;&#31471;&#21040;&#31471;&#26041;&#27861;&#65292;&#26412;&#25991;&#24378;&#35843;&#22521;&#35757;&#21644;&#39564;&#35777;&#26041;&#27861;&#12290;&#27492;&#22806;&#65292;&#26412;&#25991;&#36824;&#35752;&#35770;&#20102;&#25361;&#25112;&#21644;&#28508;&#22312;&#35299;&#20915;&#26041;&#26696;&#65292;&#36825;&#26377;&#21161;&#20110;&#20026;&#26234;&#33021;&#27773;&#36710;&#30340;&#21457;&#23637;&#25552;&#20379;&#26356;&#22909;&#30340;&#35268;&#21010;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
Intelligent vehicles (IVs) have attracted wide attention thanks to the augmented convenience, safety advantages, and potential commercial value. Although a few of autonomous driving unicorns assert that IVs will be commercially deployable by 2025, their deployment is still restricted to small-scale validation due to various issues, among which safety, reliability, and generalization of planning methods are prominent concerns. Precise computation of control commands or trajectories by planning methods remains a prerequisite for IVs, owing to perceptual imperfections under complex environments, which pose an obstacle to the successful commercialization of IVs. This paper aims to review state-of-the-art planning methods, including pipeline planning and end-to-end planning methods. In terms of pipeline methods, a survey of selecting algorithms is provided along with a discussion of the expansion and optimization mechanisms, whereas in end-to-end methods, the training approaches and verific
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#19977;&#32500;&#28857;&#20113;&#20998;&#26512;&#32593;&#32476;Point-NN&#12290;&#23427;&#22312;&#21508;&#31181;&#19977;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#19981;&#38656;&#35201;&#21442;&#25968;&#25110;&#35757;&#32451;&#65292;&#21487;&#20197;&#20316;&#20026;&#22522;&#30784;&#26550;&#26500;&#26694;&#26550;&#26500;&#24314;&#21442;&#25968;&#21270;&#32593;&#32476;&#21644;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#19977;&#32500;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#12290;</title><link>http://arxiv.org/abs/2303.08134</link><description>&lt;p&gt;
&#38750;&#21442;&#25968;&#21270;&#32593;&#32476;&#22312;&#19977;&#32500;&#28857;&#20113;&#20998;&#26512;&#20013;&#30340;&#24212;&#29992;&#65306;&#21442;&#25968;&#19981;&#26159;&#21807;&#19968;&#38656;&#35201;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Parameter is Not All You Need: Starting from Non-Parametric Networks for 3D Point Cloud Analysis. (arXiv:2303.08134v1 [cs.CV])
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2303.08134
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#19977;&#32500;&#28857;&#20113;&#20998;&#26512;&#32593;&#32476;Point-NN&#12290;&#23427;&#22312;&#21508;&#31181;&#19977;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#19981;&#38656;&#35201;&#21442;&#25968;&#25110;&#35757;&#32451;&#65292;&#21487;&#20197;&#20316;&#20026;&#22522;&#30784;&#26550;&#26500;&#26694;&#26550;&#26500;&#24314;&#21442;&#25968;&#21270;&#32593;&#32476;&#21644;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#19977;&#32500;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#38750;&#21442;&#25968;&#21270;&#27169;&#22411;&#30340;&#19977;&#32500;&#28857;&#20113;&#20998;&#26512;&#32593;&#32476;&#8212;&#8212;Point-NN&#12290;&#35813;&#32593;&#32476;&#20165;&#30001;&#19981;&#21487;&#23398;&#20064;&#32452;&#20214;&#32452;&#25104;&#65292;&#21253;&#25324;&#26368;&#36828;&#28857;&#37319;&#26679;&#65288;FPS&#65289;&#12289;K&#36817;&#37051;&#65288;k-NN&#65289;&#21644;&#21152;&#26435;&#24179;&#22343;&#27744;&#21270;&#31561;&#25805;&#20316;&#21450;&#19977;&#35282;&#20989;&#25968;&#12290;&#20196;&#20154;&#24778;&#35766;&#30340;&#26159;&#65292;&#23427;&#22312;&#21508;&#31181;&#19977;&#32500;&#20219;&#21153;&#20013;&#34920;&#29616;&#33391;&#22909;&#65292;&#19981;&#38656;&#35201;&#21442;&#25968;&#25110;&#35757;&#32451;&#65292;&#29978;&#33267;&#36229;&#36807;&#20102;&#29616;&#26377;&#30340;&#23436;&#20840;&#35757;&#32451;&#27169;&#22411;&#12290;&#22522;&#20110;&#36825;&#31181;&#38750;&#21442;&#25968;&#27169;&#22411;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#25193;&#23637;&#12290;&#39318;&#20808;&#65292;Point-NN&#21487;&#20197;&#20316;&#20026;&#22522;&#30784;&#26550;&#26500;&#26694;&#26550;&#65292;&#36890;&#36807;&#22312;&#20854;&#19978;&#31616;&#21333;&#25554;&#20837;&#32447;&#24615;&#23618;&#26469;&#26500;&#24314;&#21442;&#25968;&#21270;&#32593;&#32476;&#12290;&#22312;&#38750;&#21442;&#25968;&#22522;&#30784;&#19978;&#65292;&#24471;&#21040;&#30340;Point-PN&#20855;&#26377;&#36739;&#39640;&#30340;&#24615;&#33021;&#25928;&#29575;&#26435;&#34913;&#65292;&#21482;&#38656;&#35201;&#24456;&#23569;&#30340;&#21487;&#23398;&#20064;&#21442;&#25968;&#12290;&#20854;&#27425;&#65292;Point-NN&#21487;&#20197;&#34987;&#35270;&#20026;&#24050;&#32463;&#35757;&#32451;&#22909;&#30340;&#19977;&#32500;&#27169;&#22411;&#30340;&#21363;&#25554;&#21363;&#29992;&#27169;&#22359;&#12290;Point-NN&#25429;&#33719;&#20114;&#34917;&#30340;&#20960;&#20309;&#30693;&#35782;&#65292;&#22686;&#24378;&#29616;&#26377;&#26041;&#27861;&#23545;&#19981;&#21516;&#19977;&#32500;&#22522;&#20934;&#30340;&#24615;&#33021;&#12290;&#25105;&#20204;&#24076;&#26395;&#25105;&#20204;&#30340;&#24037;&#20316;&#33021;&#22815;&#28608;&#21457;&#26356;&#22810;&#23545;&#38750;&#32447;&#24615;&#25805;&#20316;&#21644;&#20960;&#20309;&#30693;&#35782;&#30340;&#30740;&#31350;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present a Non-parametric Network for 3D point cloud analysis, Point-NN, which consists of purely non-learnable components: farthest point sampling (FPS), k-nearest neighbors (k-NN), and pooling operations, with trigonometric functions. Surprisingly, it performs well on various 3D tasks, requiring no parameters or training, and even surpasses existing fully trained models. Starting from this basic non-parametric model, we propose two extensions. First, Point-NN can serve as a base architectural framework to construct Parametric Networks by simply inserting linear layers on top. Given the superior non-parametric foundation, the derived Point-PN exhibits a high performance-efficiency trade-off with only a few learnable parameters. Second, Point-NN can be regarded as a plug-and-play module for the already trained 3D models during inference. Point-NN captures the complementary geometric knowledge and enhances existing methods for different 3D benchmarks without re-training. We hope our w
&lt;/p&gt;</description></item><item><title>CEMA&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#22240;&#26524;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#37319;&#26679;&#21453;&#20107;&#23454;&#19990;&#30028;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#21644;&#25490;&#21517;&#20915;&#31574;&#32972;&#21518;&#30340;&#26174;&#33879;&#21407;&#22240;&#12290;&#35813;&#31995;&#32479;&#36824;&#21487;&#20197;&#29983;&#25104;&#22522;&#20110;&#25152;&#36873;&#21407;&#22240;&#30340;&#23545;&#27604;&#35299;&#37322;&#65292;&#24182;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#24490;&#29615;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#30456;&#20851;&#24615;&#21644;&#21487;&#35835;&#24615;&#12290;</title><link>http://arxiv.org/abs/2302.10809</link><description>&lt;p&gt;
&#38543;&#26426;&#24207;&#21015;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#22240;&#26524;&#35299;&#37322;
&lt;/p&gt;
&lt;p&gt;
Causal Explanations for Stochastic Sequential Multi-Agent Decision-Making. (arXiv:2302.10809v2 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10809
&lt;/p&gt;
&lt;p&gt;
CEMA&#26159;&#19968;&#20010;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#22240;&#26524;&#35299;&#37322;&#30340;&#31995;&#32479;&#65292;&#20351;&#29992;&#37319;&#26679;&#21453;&#20107;&#23454;&#19990;&#30028;&#30340;&#26041;&#27861;&#21487;&#20197;&#35782;&#21035;&#21644;&#25490;&#21517;&#20915;&#31574;&#32972;&#21518;&#30340;&#26174;&#33879;&#21407;&#22240;&#12290;&#35813;&#31995;&#32479;&#36824;&#21487;&#20197;&#29983;&#25104;&#22522;&#20110;&#25152;&#36873;&#21407;&#22240;&#30340;&#23545;&#27604;&#35299;&#37322;&#65292;&#24182;&#19982;&#29992;&#25143;&#36827;&#34892;&#20132;&#20114;&#24490;&#29615;&#20197;&#30830;&#20445;&#35299;&#37322;&#30340;&#30456;&#20851;&#24615;&#21644;&#21487;&#35835;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;CEMA&#65306;&#29992;&#20110;&#22810;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#22240;&#26524;&#35299;&#37322;&#31995;&#32479;&#65307;&#29992;&#20110;&#22312;&#38543;&#26426;&#24207;&#21015;&#22810;&#26234;&#33021;&#20307;&#29615;&#22659;&#20013;&#29983;&#25104;&#20851;&#20110;&#26234;&#33021;&#20307;&#20915;&#31574;&#30340;&#22240;&#26524;&#35299;&#37322;&#12290;CEMA&#30340;&#26680;&#24515;&#26159;&#19968;&#31181;&#26032;&#39062;&#30340;&#22240;&#26524;&#36873;&#25321;&#26041;&#27861;&#65292;&#19981;&#21516;&#20110;&#20043;&#21069;&#20551;&#35774;&#29305;&#23450;&#22240;&#26524;&#32467;&#26500;&#30340;&#26041;&#27861;&#65292;&#21482;&#38656;&#35201;&#19968;&#20010;&#21487;&#20197;&#39044;&#27979;&#29615;&#22659;&#26410;&#26469;&#29366;&#24577;&#30340;&#27010;&#29575;&#27169;&#22411;&#21363;&#21487;&#24212;&#29992;&#12290;&#25105;&#20204;&#20351;&#29992;&#35813;&#27169;&#22411;&#37319;&#26679;&#21453;&#20107;&#23454;&#19990;&#30028;&#65292;&#20197;&#35782;&#21035;&#21644;&#25490;&#21517;&#20915;&#31574;&#32972;&#21518;&#30340;&#26174;&#33879;&#21407;&#22240;&#12290;&#25105;&#20204;&#36824;&#35774;&#35745;&#20102;CEMA&#20197;&#28385;&#36275;&#31038;&#20250;&#21487;&#35299;&#37322;AI&#30340;&#35201;&#27714;&#12290;&#23427;&#21487;&#20197;&#22522;&#20110;&#25152;&#36873;&#21407;&#22240;&#29983;&#25104;&#23545;&#27604;&#35299;&#37322;&#65292;&#36890;&#36807;&#19982;&#29992;&#25143;&#30340;&#20132;&#20114;&#24490;&#29615;&#26469;&#30830;&#20445;&#23545;&#29992;&#25143;&#30340;&#30456;&#20851;&#24615;&#21644;&#21487;&#35835;&#24615;&#12290;&#25105;&#20204;&#23558;CEMA&#23454;&#29616;&#22312;&#33258;&#21160;&#39550;&#39542;&#30340;&#36816;&#21160;&#35268;&#21010;&#20013;&#65292;&#24182;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#27169;&#25311;&#22330;&#26223;&#20013;&#36827;&#34892;&#27979;&#35797;&#12290;&#25105;&#20204;&#23637;&#31034;CEMA&#33021;&#22815;&#27491;&#30830;&#32780;&#19988;&#40065;&#26834;&#22320;&#35782;&#21035;&#20915;&#31574;&#32972;&#21518;&#30340;&#30456;&#20851;&#21407;&#22240;&#65292;&#24182;&#25552;&#20379;&#30456;&#20851;&#35299;&#37322;&#12290;
&lt;/p&gt;
&lt;p&gt;
We present CEMA: Causal Explanations for Multi-Agent decision-making; a system to generate causal explanations for agents' decisions in stochastic sequential multi-agent environments. The core of CEMA is a novel causal selection method which, unlike prior work that assumes a specific causal structure, is applicable whenever a probabilistic model for predicting future states of the environment is available. We sample counterfactual worlds with this model which are used to identify and rank the salient causes behind decisions. We also designed CEMA to meet the requirements of social explainable AI. It can generate contrastive explanations based on selected causes and it works as an interaction loop with users to assure relevance and intelligibility for them. We implement CEMA for motion planning for autonomous driving and test it in four diverse simulated scenarios. We show that CEMA correctly and robustly identifies the relevant causes behind decisions and delivers relevant explanations
&lt;/p&gt;</description></item><item><title>&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#27431;&#30431;&#25552;&#20986;&#30340;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12299;&#21644;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#23545;&#20110;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#24615;&#30340;&#22522;&#26412;&#23450;&#20041;&#65292;&#24378;&#35843;&#20102;&#23558;&#36825;&#20123;&#23450;&#20041;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#25216;&#26415;&#23454;&#36341;&#31526;&#21512;&#27861;&#35268;&#12290;</title><link>http://arxiv.org/abs/2302.10766</link><description>&lt;p&gt;
&#35753;&#20320;&#30340;&#34892;&#20026;&#20117;&#28982;&#26377;&#24207;&#65306;AI&#27861;&#26696;&#21644;&#25216;&#26415;&#36879;&#26126;&#24230;&#30340;&#27604;&#36739;&#35270;&#35282;
&lt;/p&gt;
&lt;p&gt;
Get Your Act Together: A Comparative View on Transparency in the AI Act and Technology. (arXiv:2302.10766v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.10766
&lt;/p&gt;
&lt;p&gt;
&#35813;&#35770;&#25991;&#27604;&#36739;&#20102;&#27431;&#30431;&#25552;&#20986;&#30340;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12299;&#21644;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#23545;&#20110;&#36879;&#26126;&#24230;&#21644;&#35299;&#37322;&#24615;&#30340;&#22522;&#26412;&#23450;&#20041;&#65292;&#24378;&#35843;&#20102;&#23558;&#36825;&#20123;&#23450;&#20041;&#23545;&#40784;&#30340;&#37325;&#35201;&#24615;&#65292;&#20197;&#30830;&#20445;&#25216;&#26415;&#23454;&#36341;&#31526;&#21512;&#27861;&#35268;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#27431;&#30431;&#25552;&#20986;&#20102;&#12298;&#20154;&#24037;&#26234;&#33021;&#27861;&#26696;&#12299;&#65292;&#24341;&#20837;&#20102;&#22522;&#20110;&#39118;&#38505;&#30340;&#27604;&#20363;&#26041;&#27861;&#26469;&#35268;&#33539;&#20154;&#24037;&#26234;&#33021;&#65292;&#20854;&#20013;&#35814;&#32454;&#35201;&#27714;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#12290;&#34429;&#28982;&#21487;&#35299;&#37322;AI&#65288;XAI&#65289;&#39046;&#22495;&#21487;&#20197;&#35299;&#20915;&#36825;&#20123;&#35201;&#27714;&#20013;&#30340;&#35768;&#22810;&#38382;&#39064;&#65292;&#20294;&#22312;&#36879;&#26126;&#24230;&#21644;&#21487;&#35299;&#37322;&#24615;&#30340;&#20855;&#20307;&#23450;&#20041;&#19978;&#65292;XAI&#19982;&#35813;&#27861;&#26696;&#23384;&#22312;&#22522;&#26412;&#24046;&#24322;&#12290;&#20026;&#20102;&#23454;&#29616;&#36825;&#31181;&#23545;&#40784;&#65292;&#25105;&#20204;&#39318;&#20808;&#27010;&#36848;&#20102;XAI&#21644;&#27431;&#27954;&#27861;&#35268;&#26159;&#22914;&#20309;&#30475;&#24453;&#36879;&#26126;&#24230;&#30340;&#22522;&#26412;&#23450;&#20041;&#30340;&#65292;&#29305;&#21035;&#26159;AI&#27861;&#26696;&#21644;&#30456;&#20851;&#30340;&#36890;&#29992;&#25968;&#25454;&#20445;&#25252;&#26465;&#20363;&#65288;GDPR&#65289;&#12290;&#28982;&#21518;&#25105;&#20204;&#36827;&#34892;&#20102;&#27604;&#36739;&#65292;&#26088;&#22312;&#30830;&#23450;&#25913;&#21892;&#39046;&#22495;&#20043;&#38388;&#23545;&#40784;&#30340;&#20027;&#35201;&#35201;&#28857;&#65306;&#28548;&#28165;&#36879;&#26126;&#24230;&#30340;&#33539;&#22260;&#65292;XAI&#30340;&#27861;&#24459;&#22320;&#20301;&#65292;&#30417;&#31649;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The European Union has proposed the Artificial Intelligence Act which introduces a proportional risk-based approach to AI regulation including detailed requirements for transparency and explainability. Many of these requirements may be addressed in practice by the field of explainable AI (XAI), however, there are fundamental differences between XAI and the Act regarding what transparency and explainability are. These basic definitions should be aligned to assure that regulation continually translates into appropriate technical practices. To facilitate this alignment, we first give an overview of how XAI and European regulation view basic definitions of transparency with a particular focus on the AI Act and the related General Data Protection Regulation (GDPR). We then present a comparison of XAI and regulatory approaches to identify the main points that would improve alignment between the fields: clarification of the scope of transparency, the legal status of XAI, oversight issues in c
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65292;&#20197;&#36991;&#20813;&#30001;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#39044;&#27979;&#30340;&#23454;&#20363;&#36873;&#25321;&#32780;&#23548;&#33268;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2302.08883</link><description>&lt;p&gt;
&#36817;&#20046;&#36125;&#21494;&#26031;&#26368;&#20248;&#30340;&#20266;&#26631;&#31614;&#36873;&#25321;
&lt;/p&gt;
&lt;p&gt;
Approximately Bayes-Optimal Pseudo Label Selection. (arXiv:2302.08883v3 [stat.ML] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2302.08883
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65292;&#20197;&#36991;&#20813;&#30001;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#39044;&#27979;&#30340;&#23454;&#20363;&#36873;&#25321;&#32780;&#23548;&#33268;&#30340;&#30830;&#35748;&#20559;&#24046;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#33258;&#35757;&#32451;&#30340;&#21322;&#30417;&#30563;&#23398;&#20064;&#20005;&#37325;&#20381;&#36182;&#20110;&#20266;&#26631;&#31614;&#36873;&#25321;&#65288;PLS&#65289;&#12290;&#36873;&#25321;&#36890;&#24120;&#21462;&#20915;&#20110;&#21021;&#22987;&#27169;&#22411;&#25311;&#21512;&#26631;&#35760;&#25968;&#25454;&#30340;&#31243;&#24230;&#12290;&#36807;&#26089;&#30340;&#36807;&#25311;&#21512;&#21487;&#33021;&#36890;&#36807;&#36873;&#25321;&#20855;&#26377;&#36807;&#24230;&#33258;&#20449;&#20294;&#38169;&#35823;&#30340;&#39044;&#27979;&#30340;&#23454;&#20363;&#65288;&#36890;&#24120;&#31216;&#20026;&#30830;&#35748;&#20559;&#24046;&#65289;&#32780;&#20256;&#25773;&#21040;&#26368;&#32456;&#27169;&#22411;&#12290;&#26412;&#25991;&#20171;&#32461;&#20102;BPLS&#65292;&#36825;&#26159;&#19968;&#31181;&#29992;&#20110;PLS&#30340;&#36125;&#21494;&#26031;&#26694;&#26550;&#65292;&#26088;&#22312;&#20943;&#36731;&#36825;&#20010;&#38382;&#39064;&#12290;&#20854;&#26680;&#24515;&#26159;&#36873;&#25321;&#26631;&#31614;&#23454;&#20363;&#30340;&#26631;&#20934;&#65306;&#20266;&#26679;&#26412;&#30340;&#21518;&#39564;&#39044;&#27979;&#30340;&#20998;&#26512;&#36817;&#20284;&#12290;&#25105;&#20204;&#36890;&#36807;&#35777;&#26126;&#20266;&#26679;&#26412;&#30340;&#21518;&#39564;&#39044;&#27979;&#30340;&#36125;&#21494;&#26031;&#26368;&#20248;&#24615;&#33719;&#24471;&#20102;&#36825;&#31181;&#36873;&#25321;&#26631;&#20934;&#12290;&#25105;&#20204;&#36827;&#19968;&#27493;&#36890;&#36807;&#35299;&#26512;&#36924;&#36817;&#20811;&#26381;&#35745;&#31639;&#38590;&#39064;&#12290;&#23427;&#19982;&#36793;&#38469;&#20284;&#28982;&#30340;&#20851;&#31995;&#20351;&#25105;&#20204;&#33021;&#22815;&#25552;&#20986;&#22522;&#20110;&#25289;&#26222;&#25289;&#26031;&#26041;&#27861;&#21644;&#39640;&#26031;&#31215;&#20998;&#30340;&#36924;&#36817;&#12290;&#25105;&#20204;&#38024;&#23545;&#21442;&#25968;&#24191;&#20041;&#32447;&#24615;&#21644;&#38750;&#21442;&#25968;&#24191;&#20041;&#21152;&#24615;&#27169;&#22411;&#23545;BPLS&#36827;&#34892;&#20102;&#23454;&#35777;&#35780;&#20272;&#12290;
&lt;/p&gt;
&lt;p&gt;
Semi-supervised learning by self-training heavily relies on pseudo-label selection (PLS). The selection often depends on the initial model fit on labeled data. Early overfitting might thus be propagated to the final model by selecting instances with overconfident but erroneous predictions, often referred to as confirmation bias. This paper introduces BPLS, a Bayesian framework for PLS that aims to mitigate this issue. At its core lies a criterion for selecting instances to label: an analytical approximation of the posterior predictive of pseudo-samples. We derive this selection criterion by proving Bayes optimality of the posterior predictive of pseudo-samples. We further overcome computational hurdles by approximating the criterion analytically. Its relation to the marginal likelihood allows us to come up with an approximation based on Laplace's method and the Gaussian integral. We empirically assess BPLS for parametric generalized linear and non-parametric generalized additive models
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SNeRL&#30340;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;3D-aware&#30340;&#38544;&#24335;&#34920;&#31034;&#26469;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#22312;&#22522;&#20110;&#20687;&#32032;&#30340;&#20197;&#21450;&#26368;&#26032;&#30340;3D&#24863;&#30693;&#34920;&#31034;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;</title><link>http://arxiv.org/abs/2301.11520</link><description>&lt;p&gt;
SNeRL: &#35821;&#20041;&#24863;&#30693;&#30340;&#31070;&#32463;&#36752;&#23556;&#22330;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;
&lt;/p&gt;
&lt;p&gt;
SNeRL: Semantic-aware Neural Radiance Fields for Reinforcement Learning. (arXiv:2301.11520v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.11520
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SNeRL&#30340;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#23427;&#36890;&#36807;&#23398;&#20064;3D-aware&#30340;&#38544;&#24335;&#34920;&#31034;&#26469;&#36827;&#34892;&#24378;&#21270;&#23398;&#20064;&#65292;&#24182;&#22312;&#22522;&#20110;&#20687;&#32032;&#30340;&#20197;&#21450;&#26368;&#26032;&#30340;3D&#24863;&#30693;&#34920;&#31034;&#26041;&#27861;&#20013;&#34920;&#29616;&#20986;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#20256;&#32479;&#30340;&#24378;&#21270;&#23398;&#20064;&#34920;&#31034;&#26041;&#27861;&#24456;&#38590;&#26377;&#25928;&#22320;&#34701;&#21512;&#20154;&#31867;&#30452;&#35266;&#30340;3D&#29615;&#22659;&#29702;&#35299;&#65292;&#22240;&#27492;&#32463;&#24120;&#34920;&#29616;&#20986;&#27425;&#20248;&#24615;&#33021;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#31216;&#20026;SNeRL&#30340;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65292;&#23427;&#36890;&#36807;&#32852;&#21512;&#20248;&#21270;&#21367;&#31215;&#32534;&#30721;&#22120;&#21644;&#35821;&#20041;&#24863;&#30693;&#31070;&#32463;&#36752;&#23556;&#22330;&#65288;NeRF&#65289;&#26469;&#20174;&#22810;&#35270;&#35282;&#22270;&#20687;&#20013;&#23398;&#20064;3D&#24863;&#30693;&#31070;&#32463;&#38544;&#24335;&#34920;&#31034;&#12290;&#25105;&#20204;&#22312;NeRF&#20013;&#24341;&#20837;&#20102;3D&#35821;&#20041;&#21644;&#33976;&#39311;&#29305;&#24449;&#22330;&#65292;&#24182;&#19982;RGB&#36752;&#23556;&#22330;&#24182;&#34892;&#29992;&#20110;&#24378;&#21270;&#23398;&#20064;&#20013;&#30340;&#35821;&#20041;&#21644;&#23545;&#35937;&#20013;&#24515;&#34920;&#31034;&#23398;&#20064;&#12290;SNeRL&#22312;&#26080;&#27169;&#22411;&#21644;&#26377;&#27169;&#22411;&#24378;&#21270;&#23398;&#20064;&#20013;&#19981;&#20165;&#20248;&#20110;&#20197;&#24448;&#30340;&#22522;&#20110;&#20687;&#32032;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#36824;&#20248;&#20110;&#26368;&#36817;&#30340;3D&#24863;&#30693;&#34920;&#31034;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
As previous representations for reinforcement learning cannot effectively incorporate a human-intuitive understanding of the 3D environment, they usually suffer from sub-optimal performances. In this paper, we present Semantic-aware Neural Radiance Fields for Reinforcement Learning (SNeRL), which jointly optimizes semantic-aware neural radiance fields (NeRF) with a convolutional encoder to learn 3D-aware neural implicit representation from multi-view images. We introduce 3D semantic and distilled feature fields in parallel to the RGB radiance fields in NeRF to learn semantic and object-centric representation for reinforcement learning. SNeRL outperforms not only previous pixel-based representations but also recent 3D-aware representations both in model-free and model-based reinforcement learning.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#22122;&#22768;&#35889;&#23398;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#27604;&#26631;&#20934;&#25216;&#26415;&#26356;&#21152;&#20934;&#30830;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#24207;&#21015;&#12290;</title><link>http://arxiv.org/abs/2301.05079</link><description>&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#22686;&#24378;&#33258;&#26059;&#37327;&#23376;&#27604;&#29305;&#29615;&#22659;&#30340;&#22122;&#22768;&#35889;&#23398;
&lt;/p&gt;
&lt;p&gt;
Deep learning enhanced noise spectroscopy of a spin qubit environment. (arXiv:2301.05079v2 [quant-ph] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.05079
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#30740;&#31350;&#20351;&#29992;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#25552;&#39640;&#22122;&#22768;&#35889;&#23398;&#20934;&#30830;&#24615;&#65292;&#23454;&#39564;&#34920;&#26126;&#27492;&#26041;&#27861;&#21487;&#20197;&#27604;&#26631;&#20934;&#25216;&#26415;&#26356;&#21152;&#20934;&#30830;&#65292;&#24182;&#19988;&#38656;&#35201;&#26356;&#23569;&#30340;&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#37327;&#23376;&#31995;&#32479;&#19982;&#29615;&#22659;&#20043;&#38388;&#30340;&#19981;&#33391;&#30456;&#20114;&#20316;&#29992;&#36890;&#24120;&#23548;&#33268;&#36229;&#24577;&#22312;&#26102;&#38388;&#19978;&#30340;&#30456;&#24178;&#34928;&#20943;&#12290;&#31934;&#30830;&#20102;&#35299;&#29615;&#22659;&#24341;&#36215;&#30340;&#22122;&#22768;&#30340;&#39057;&#35889;&#20869;&#23481;&#23545;&#20110;&#20445;&#25252;&#37327;&#23376;&#27604;&#29305;&#30456;&#24178;&#24615;&#24182;&#20248;&#21270;&#20854;&#22312;&#37327;&#23376;&#22120;&#20214;&#24212;&#29992;&#20013;&#30340;&#20351;&#29992;&#33267;&#20851;&#37325;&#35201;&#12290;&#25105;&#20204;&#23454;&#39564;&#35777;&#26126;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#21487;&#20197;&#26497;&#22823;&#22320;&#22686;&#21152;&#22122;&#22768;&#35889;&#23398;&#30340;&#20934;&#30830;&#24615;&#65292;&#36890;&#36807;&#37325;&#26500;&#30899;&#26434;&#36136;&#32452;&#21512;&#22260;&#32469;&#38075;&#30707;&#27694;-&#31354;&#20301;&#65288;NV&#65289;&#20013;&#24515;&#25152;&#29305;&#24449;&#21270;&#30340;&#21151;&#29575;&#35889;&#23494;&#24230;&#12290;&#31070;&#32463;&#32593;&#32476;&#35757;&#32451;&#20110;NV&#20013;&#24515;&#30340;&#33258;&#26059;&#30456;&#24178;&#20989;&#25968;&#20043;&#19978;&#65292;NV&#20013;&#24515;&#21463;&#19981;&#21516;&#30340;Carr-Purcell&#24207;&#21015;&#25511;&#21046;&#65292;&#36890;&#24120;&#29992;&#20110;&#21160;&#21147;&#23398;&#35299;&#20598;&#65288;DD&#65289;&#12290;&#32467;&#26524;&#34920;&#26126;&#65292;&#28145;&#24230;&#23398;&#20064;&#27169;&#22411;&#21487;&#20197;&#27604;&#26631;&#20934;DD&#22122;&#22768;&#35889;&#23398;&#25216;&#26415;&#26356;&#21152;&#20934;&#30830;&#65292;&#24182;&#19988;&#21516;&#26102;&#38656;&#35201;&#26356;&#23569;&#30340;DD&#24207;&#21015;&#12290;
&lt;/p&gt;
&lt;p&gt;
The undesired interaction of a quantum system with its environment generally leads to a coherence decay of superposition states in time. A precise knowledge of the spectral content of the noise induced by the environment is crucial to protect qubit coherence and optimize its employment in quantum device applications. We experimentally show that the use of neural networks can highly increase the accuracy of noise spectroscopy, by reconstructing the power spectral density that characterizes an ensemble of carbon impurities around a nitrogen-vacancy (NV) center in diamond. Neural networks are trained over spin coherence functions of the NV center subjected to different Carr-Purcell sequences, typically used for dynamical decoupling (DD). As a result, we determine that deep learning models can be more accurate than standard DD noise-spectroscopy techniques, by requiring at the same time a much smaller number of DD sequences.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TarViS&#30340;&#26032;&#39062;&#12289;&#32479;&#19968;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#38656;&#35201;&#22312;&#35270;&#39057;&#20013;&#20998;&#27573;&#20219;&#24847;&#23450;&#20041;&#30340;&#8220;&#30446;&#26631;&#8221;&#38598;&#30340;&#20219;&#21153;&#12290;&#21487;&#20197;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#25512;&#26029;&#26399;&#38388;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#28909;&#20999;&#25442;&#12290;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#65292;&#22343;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;</title><link>http://arxiv.org/abs/2301.02657</link><description>&lt;p&gt;
TarViS&#65306;&#19968;&#31181;&#38754;&#21521;&#30446;&#26631;&#30340;&#35270;&#39057;&#20998;&#21106;&#30340;&#32479;&#19968;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
TarViS: A Unified Approach for Target-based Video Segmentation. (arXiv:2301.02657v2 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2301.02657
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;TarViS&#30340;&#26032;&#39062;&#12289;&#32479;&#19968;&#30340;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#29992;&#20110;&#20219;&#20309;&#38656;&#35201;&#22312;&#35270;&#39057;&#20013;&#20998;&#27573;&#20219;&#24847;&#23450;&#20041;&#30340;&#8220;&#30446;&#26631;&#8221;&#38598;&#30340;&#20219;&#21153;&#12290;&#21487;&#20197;&#32852;&#21512;&#35757;&#32451;&#19981;&#21516;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#25512;&#26029;&#26399;&#38388;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#28909;&#20999;&#25442;&#12290;&#22312;&#22235;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#20013;&#36827;&#34892;&#20102;&#24212;&#29992;&#65292;&#22343;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#35270;&#39057;&#20998;&#21106;&#39046;&#22495;&#30446;&#21069;&#34987;&#21010;&#20998;&#20026;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#28085;&#30422;&#20102;&#22810;&#20010;&#22522;&#20934;&#12290;&#23613;&#31649;&#26368;&#20808;&#36827;&#25216;&#26415;&#27491;&#22312;&#24555;&#36895;&#21457;&#23637;&#65292;&#20294;&#26159;&#24403;&#21069;&#26041;&#27861;&#20027;&#35201;&#26159;&#38024;&#23545;&#29305;&#23450;&#20219;&#21153;&#30340;&#65292;&#36825;&#20123;&#26041;&#27861;&#19981;&#33021;&#27010;&#24565;&#19978;&#25512;&#24191;&#21040;&#20854;&#20182;&#20219;&#21153;&#12290;&#21463;&#21040;&#20855;&#26377;&#22810;&#20219;&#21153;&#33021;&#21147;&#30340;&#26368;&#36817;&#26041;&#27861;&#30340;&#21551;&#21457;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;TarViS&#65306;&#19968;&#31181;&#26032;&#39062;&#30340;&#32479;&#19968;&#32593;&#32476;&#26550;&#26500;&#65292;&#21487;&#24212;&#29992;&#20110;&#38656;&#35201;&#22312;&#35270;&#39057;&#20013;&#20998;&#27573;&#20219;&#24847;&#23450;&#20041;&#30340;&#8220;&#30446;&#26631;&#8221;&#38598;&#30340;&#20219;&#20309;&#20219;&#21153;&#12290;&#25105;&#20204;&#30340;&#26041;&#27861;&#22312;&#20219;&#21153;&#23450;&#20041;&#36825;&#20123;&#30446;&#26631;&#30340;&#26041;&#24335;&#26041;&#38754;&#20855;&#26377;&#28789;&#27963;&#24615;&#65292;&#22240;&#20026;&#23427;&#23558;&#21518;&#32773;&#24314;&#27169;&#20026;&#25277;&#35937;&#30340;&#8220;&#26597;&#35810;&#8221;&#65292;&#28982;&#21518;&#29992;&#20110;&#39044;&#27979;&#20687;&#32032;&#31934;&#30830;&#30340;&#30446;&#26631;&#25513;&#30721;&#12290;&#21333;&#20010;TarViS&#27169;&#22411;&#21487;&#20197;&#32852;&#21512;&#35757;&#32451;&#36328;&#36234;&#19981;&#21516;&#20219;&#21153;&#30340;&#25968;&#25454;&#38598;&#38598;&#21512;&#65292;&#24182;&#19988;&#21487;&#20197;&#22312;&#25512;&#26029;&#26399;&#38388;&#22312;&#20219;&#21153;&#20043;&#38388;&#36827;&#34892;&#28909;&#20999;&#25442;&#65292;&#26080;&#38656;&#20219;&#20309;&#29305;&#23450;&#20110;&#20219;&#21153;&#30340;&#37325;&#26032;&#35757;&#32451;&#12290;&#20026;&#20102;&#35777;&#26126;&#20854;&#26377;&#25928;&#24615;&#65292;&#25105;&#20204;&#23558;TarViS&#24212;&#29992;&#20110;&#22235;&#20010;&#19981;&#21516;&#30340;&#20219;&#21153;&#65292;&#21363;&#35270;&#39057;&#23454;&#20363;&#20998;&#21106;(VIS)&#12289;&#35270;&#39057;&#20840;&#26223;&#20998;&#21106;(VPS)&#12289;&#35270;&#39057;&#35821;&#20041;&#20998;&#21106;&#21644;&#35270;&#39057;&#30446;&#26631;&#20998;&#21106;(VOS)&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#65292;TarViS&#22312;&#25152;&#26377;&#20219;&#21153;&#20013;&#22343;&#20248;&#20110;&#29616;&#26377;&#26368;&#20808;&#36827;&#26041;&#27861;&#12290;
&lt;/p&gt;
&lt;p&gt;
The general domain of video segmentation is currently fragmented into different tasks spanning multiple benchmarks. Despite rapid progress in the state-of-the-art, current methods are overwhelmingly task-specific and cannot conceptually generalize to other tasks. Inspired by recent approaches with multi-task capability, we propose TarViS: a novel, unified network architecture that can be applied to any task that requires segmenting a set of arbitrarily defined 'targets' in video. Our approach is flexible with respect to how tasks define these targets, since it models the latter as abstract 'queries' which are then used to predict pixel-precise target masks. A single TarViS model can be trained jointly on a collection of datasets spanning different tasks, and can hot-swap between tasks during inference without any task-specific retraining. To demonstrate its effectiveness, we apply TarViS to four different tasks, namely Video Instance Segmentation (VIS), Video Panoptic Segmentation (VPS
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20379;&#20102;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;VISEM-Tracking&#65292;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#19987;&#23478;&#20998;&#26512;&#30340;&#31934;&#23376;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#20197;&#20379;&#26131;&#20110;&#35775;&#38382;&#21644;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;</title><link>http://arxiv.org/abs/2212.02842</link><description>&lt;p&gt;
VISEM-Tracking&#65292;&#19968;&#20221;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;
&lt;/p&gt;
&lt;p&gt;
VISEM-Tracking, a human spermatozoa tracking dataset. (arXiv:2212.02842v3 [cs.CV] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.02842
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20379;&#20102;&#20154;&#31867;&#31934;&#23376;&#36319;&#36394;&#25968;&#25454;&#38598;VISEM-Tracking&#65292;&#21253;&#21547;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#19987;&#23478;&#20998;&#26512;&#30340;&#31934;&#23376;&#29305;&#24449;&#65292;&#24182;&#25552;&#20379;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#20197;&#20379;&#26131;&#20110;&#35775;&#38382;&#21644;&#20998;&#26512;&#65292;&#26377;&#21161;&#20110;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#31934;&#23376;&#36816;&#21160;&#30340;&#25163;&#21160;&#35780;&#20272;&#38656;&#35201;&#26174;&#24494;&#38236;&#35266;&#23519;&#65292;&#30001;&#20110;&#25152;&#35266;&#23519;&#30340;&#31934;&#23376;&#22312;&#35270;&#37326;&#20013;&#30340;&#24555;&#36895;&#31227;&#21160;&#65292;&#36825;&#26159;&#20855;&#26377;&#25361;&#25112;&#24615;&#30340;&#12290;&#20026;&#20102;&#33719;&#24471;&#27491;&#30830;&#30340;&#32467;&#26524;&#65292;&#25163;&#21160;&#35780;&#20272;&#38656;&#35201;&#36827;&#34892;&#24191;&#27867;&#30340;&#22521;&#35757;&#12290;&#22240;&#27492;&#65292;&#22312;&#35786;&#25152;&#20013;&#65292;&#35745;&#31639;&#26426;&#36741;&#21161;&#31934;&#23376;&#20998;&#26512;&#65288;CASA&#65289;&#21464;&#24471;&#36234;&#26469;&#36234;&#24120;&#29992;&#12290;&#23613;&#31649;&#22914;&#27492;&#65292;&#38656;&#35201;&#26356;&#22810;&#25968;&#25454;&#26469;&#35757;&#32451;&#30417;&#30563;&#24335;&#26426;&#22120;&#23398;&#20064;&#26041;&#27861;&#65292;&#20197;&#25552;&#39640;&#22312;&#35780;&#20272;&#31934;&#23376;&#36816;&#21160;&#21644;&#36816;&#21160;&#23398;&#26041;&#38754;&#30340;&#31934;&#24230;&#21644;&#21487;&#38752;&#24615;&#12290;&#22312;&#36825;&#26041;&#38754;&#65292;&#25105;&#20204;&#25552;&#20379;&#20102;&#19968;&#20010;&#21517;&#20026;VISEM-Tracking&#30340;&#25968;&#25454;&#38598;&#65292;&#20854;&#20013;&#21253;&#21547;20&#20010;30&#31186;&#30340;&#35270;&#39057;&#35760;&#24405;&#65288;&#21253;&#25324;29,196&#24103;&#65289;&#30340;&#28287;&#24615;&#31934;&#23376;&#21046;&#22791;&#29289;&#65292;&#20855;&#22791;&#25163;&#21160;&#27880;&#37322;&#30340;&#21253;&#22260;&#26694;&#22352;&#26631;&#21644;&#30001;&#35813;&#39046;&#22495;&#30340;&#19987;&#23478;&#20998;&#26512;&#30340;&#19968;&#32452;&#31934;&#23376;&#29305;&#24449;&#12290;&#38500;&#20102;&#24050;&#27880;&#37322;&#30340;&#25968;&#25454;&#65292;&#25105;&#20204;&#36824;&#25552;&#20379;&#20102;&#26410;&#26631;&#35760;&#30340;&#35270;&#39057;&#21098;&#36753;&#65292;&#20197;&#20415;&#36890;&#36807;&#33258;&#30417;&#30563;&#25110;&#26080;&#30417;&#30563;&#23398;&#20064;&#31561;&#26041;&#27861;&#36731;&#26494;&#35775;&#38382;&#21644;&#20998;&#26512;&#25968;&#25454;&#12290;&#20316;&#20026;&#26412;&#25991;&#30340;&#19968;&#37096;&#20998;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#22522;&#32447;&#31934;&#23376;&#26816;&#27979;&#24615;&#33021;&#12290;
&lt;/p&gt;
&lt;p&gt;
A manual assessment of sperm motility requires microscopy observation, which is challenging due to the fast-moving spermatozoa in the field of view. To obtain correct results, manual evaluation requires extensive training. Therefore, computer-assisted sperm analysis (CASA) has become increasingly used in clinics. Despite this, more data is needed to train supervised machine learning approaches in order to improve accuracy and reliability in the assessment of sperm motility and kinematics. In this regard, we provide a dataset called VISEM-Tracking with 20 video recordings of 30 seconds (comprising 29,196 frames) of wet sperm preparations with manually annotated bounding-box coordinates and a set of sperm characteristics analyzed by experts in the domain. In addition to the annotated data, we provide unlabeled video clips for easy-to-use access and analysis of the data via methods such as selfor unsupervised learning. As part of this paper, we present baseline sperm detection performan
&lt;/p&gt;</description></item><item><title>&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#30693;&#35782;&#24211;&#36827;&#34892;&#34917;&#20840;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;</title><link>http://arxiv.org/abs/2212.01923</link><description>&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#30693;&#35782;&#24211;&#34917;&#20840;
&lt;/p&gt;
&lt;p&gt;
Query-Driven Knowledge Base Completion using Multimodal Path Fusion over Multimodal Knowledge Graph. (arXiv:2212.01923v2 [cs.DB] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2212.01923
&lt;/p&gt;
&lt;p&gt;
&#22522;&#20110;&#26597;&#35810;&#30340;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#31639;&#27861;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#30693;&#35782;&#24211;&#36827;&#34892;&#34917;&#20840;&#65292;&#25552;&#39640;&#20102;&#24615;&#33021;&#65292;&#24182;&#19988;&#20351;&#29992;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#25216;&#26415;&#26469;&#25552;&#39640;&#31995;&#32479;&#30340;&#25928;&#29575;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#20960;&#24180;&#20013;&#65292;&#22823;&#22411;&#30693;&#35782;&#24211;&#24050;&#32463;&#34987;&#26500;&#24314;&#26469;&#23384;&#20648;&#22823;&#37327;&#30340;&#30693;&#35782;&#12290;&#28982;&#32780;&#65292;&#36825;&#20123;&#30693;&#35782;&#24211;&#39640;&#24230;&#19981;&#23436;&#25972;&#65292;&#20363;&#22914;Freebase&#20013;&#26377;70%&#30340;&#20154;&#27809;&#26377;&#20986;&#29983;&#22320;&#28857;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#20351;&#29992;&#32467;&#26500;&#21270;&#21644;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#30340;&#22810;&#27169;&#24577;&#34701;&#21512;&#30340;&#12289;&#22522;&#20110;&#26597;&#35810;&#39537;&#21160;&#30340;&#30693;&#35782;&#24211;&#34917;&#20840;&#31995;&#32479;&#12290;&#20026;&#20102;&#26377;&#25928;&#22320;&#34701;&#21512;&#26469;&#33258;Web&#30340;&#38750;&#32467;&#26500;&#21270;&#20449;&#24687;&#21644;&#30693;&#35782;&#24211;&#20013;&#30340;&#32467;&#26500;&#21270;&#20449;&#24687;&#20197;&#23454;&#29616;&#33391;&#22909;&#30340;&#24615;&#33021;&#65292;&#25105;&#20204;&#30340;&#31995;&#32479;&#22522;&#20110;&#38382;&#31572;&#21644;&#35268;&#21017;&#25512;&#29702;&#26500;&#24314;&#20102;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#22810;&#27169;&#24577;&#36335;&#24452;&#34701;&#21512;&#31639;&#27861;&#65292;&#22312;&#22810;&#27169;&#24577;&#30693;&#35782;&#22270;&#20013;&#22522;&#20110;&#19981;&#21516;&#30340;&#36335;&#24452;&#23545;&#20505;&#36873;&#31572;&#26696;&#36827;&#34892;&#25490;&#21517;&#65292;&#21462;&#24471;&#20102;&#27604;&#38382;&#31572;&#12289;&#35268;&#21017;&#25512;&#29702;&#21644;&#22522;&#32447;&#34701;&#21512;&#31639;&#27861;&#26356;&#22909;&#30340;&#24615;&#33021;&#12290;&#20026;&#20102;&#25552;&#39640;&#31995;&#32479;&#25928;&#29575;&#65292;&#25105;&#20204;&#20351;&#29992;&#20102;&#22522;&#20110;&#26597;&#35810;&#30340;&#25216;&#26415;&#26469;&#20943;&#23569;&#31995;&#32479;&#30340;&#36816;&#34892;&#26102;&#38388;&#65292;&#20026;&#29992;&#25143;&#26597;&#35810;&#25552;&#20379;&#24555;&#36895;&#21709;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;
Over the past few years, large knowledge bases have been constructed to store massive amounts of knowledge. However, these knowledge bases are highly incomplete, for example, over 70% of people in Freebase have no known place of birth. To solve this problem, we propose a query-driven knowledge base completion system with multimodal fusion of unstructured and structured information. To effectively fuse unstructured information from the Web and structured information in knowledge bases to achieve good performance, our system builds multimodal knowledge graphs based on question answering and rule inference. We propose a multimodal path fusion algorithm to rank candidate answers based on different paths in the multimodal knowledge graphs, achieving much better performance than question answering, rule inference and a baseline fusion algorithm. To improve system efficiency, query-driven techniques are utilized to reduce the runtime of our system, providing fast responses to user queries. Ex
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20449;&#20219;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#32676;&#20307;&#26234;&#33021;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#12290;</title><link>http://arxiv.org/abs/2211.08407</link><description>&lt;p&gt;
&#22522;&#20110;&#20449;&#20219;&#24863;&#30693;&#30340;&#32676;&#20307;&#26234;&#33021;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#38450;&#24481;
&lt;/p&gt;
&lt;p&gt;
Trust-Awareness to Secure Swarm Intelligence from Data Injection Attack. (arXiv:2211.08407v3 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.08407
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#19968;&#31181;&#22522;&#20110;&#20449;&#20219;&#24863;&#30693;&#30340;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545;&#32676;&#20307;&#26234;&#33021;&#30340;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32676;&#20307;&#26234;&#33021;&#65288;SI&#65289;&#20511;&#21161;&#26032;&#20852;&#30340;&#24037;&#19994;&#26234;&#33021;&#20307;&#65288;IA&#65289;&#25216;&#26415;&#24471;&#20197;&#23454;&#29616;&#65292;&#34987;&#39044;&#35745;&#22312;&#30001;&#31532;&#20845;&#20195;&#31227;&#21160;&#36890;&#20449;&#21644;&#25968;&#23383;&#23402;&#29983;&#65288;DT&#65289;&#26500;&#25104;&#30340;&#26410;&#26469;&#24037;&#19994;&#29289;&#32852;&#32593;&#65288;IIoT&#65289;&#20013;&#25198;&#28436;&#37325;&#35201;&#35282;&#33394;&#12290;&#28982;&#32780;&#65292;SI &#23545;&#20110;&#25968;&#25454;&#27880;&#20837;&#25915;&#20987;&#30340;&#33030;&#24369;&#24615;&#21487;&#33021;&#20250;&#20351;&#20854;&#26080;&#27861;&#23454;&#38469;&#37096;&#32626;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26377;&#25928;&#30340;&#20449;&#20219;&#26041;&#27861;&#65292;&#20197;&#24212;&#23545; SI &#30340;&#36825;&#19968;&#23433;&#20840;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
Enabled by the emerging industrial agent (IA) technology, swarm intelligence (SI) is envisaged to play an important role in future industrial Internet of Things (IIoT) that is shaped by Sixth Generation (6G) mobile communications and digital twin (DT). However, its fragility against data injection attack may halt it from practical deployment. In this paper we propose an efficient trust approach to address this security concern for SI.
&lt;/p&gt;</description></item><item><title>&#25552;&#20986;&#20102;&#21033;&#29992;&#26102;&#21464;&#29305;&#24449;&#35843;&#21046;&#26469;&#27169;&#25311;&#40657;&#30418;&#38899;&#39057;&#25928;&#24212;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#33719;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#29992;&#20110;fuzz&#21644;&#21387;&#32553;&#31561;&#38899;&#39057;&#25928;&#24212;&#12290;</title><link>http://arxiv.org/abs/2211.00497</link><description>&lt;p&gt;
&#21033;&#29992;&#26102;&#21464;&#29305;&#24449;&#35843;&#21046;&#27169;&#25311;&#40657;&#30418;&#38899;&#39057;&#25928;&#24212;
&lt;/p&gt;
&lt;p&gt;
Modelling black-box audio effects with time-varying feature modulation. (arXiv:2211.00497v2 [cs.SD] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2211.00497
&lt;/p&gt;
&lt;p&gt;
&#25552;&#20986;&#20102;&#21033;&#29992;&#26102;&#21464;&#29305;&#24449;&#35843;&#21046;&#26469;&#27169;&#25311;&#40657;&#30418;&#38899;&#39057;&#25928;&#24212;&#65292;&#21487;&#20197;&#26356;&#22909;&#22320;&#25429;&#33719;&#38271;&#26102;&#38388;&#23610;&#24230;&#19978;&#30340;&#20381;&#36182;&#20851;&#31995;&#65292;&#36866;&#29992;&#20110;fuzz&#21644;&#21387;&#32553;&#31561;&#38899;&#39057;&#25928;&#24212;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#28145;&#24230;&#23398;&#20064;&#26041;&#27861;&#29992;&#20110;&#40657;&#30418;&#24314;&#27169;&#38899;&#25928;&#24050;&#26174;&#31034;&#20986;&#24456;&#22823;&#30340;&#28508;&#21147;&#12290;&#20294;&#26159;&#65292;&#29616;&#26377;&#22823;&#37096;&#20998;&#24037;&#20316;&#38598;&#20013;&#22312;&#20855;&#26377;&#30456;&#23545;&#36739;&#30701;&#26102;&#38388;&#23610;&#24230;&#34892;&#20026;&#30340;&#38750;&#32447;&#24615;&#25928;&#24212;&#65292;&#20363;&#22914;&#21513;&#20182;&#25918;&#22823;&#22120;&#21644;&#22833;&#30495;&#12290;&#34429;&#28982;&#36882;&#24402;&#21644;&#21367;&#31215;&#32467;&#26500;&#22312;&#29702;&#35770;&#19978;&#21487;&#20197;&#25193;&#23637;&#21040;&#38271;&#26102;&#38388;&#23610;&#24230;&#26469;&#25429;&#33719;&#34892;&#20026;&#65292;&#20294;&#25105;&#20204;&#21457;&#29616;&#31616;&#21333;&#22320;&#25193;&#23637;&#29616;&#26377;&#32467;&#26500;&#30340;&#23485;&#24230;&#12289;&#28145;&#24230;&#25110;&#33192;&#32960;&#22240;&#23376;&#19981;&#33021;&#20196;&#20854;&#22312;&#27169;&#25311;fuzz&#21644;&#21160;&#24577;&#33539;&#22260;&#21387;&#32553;&#31561;&#38899;&#39057;&#25928;&#24212;&#26102;&#34920;&#29616;&#24471;&#21313;&#20998;&#20196;&#20154;&#28385;&#24847;&#12290;&#20026;&#20102;&#35299;&#20915;&#36825;&#20010;&#38382;&#39064;&#65292;&#25105;&#20204;&#25552;&#20986;&#22312;&#29616;&#26377;&#30340;&#26102;&#38388;&#21367;&#31215;&#39592;&#24178;&#20013;&#25972;&#21512;&#26102;&#21464;&#29305;&#24449;&#30340;&#32447;&#24615;&#35843;&#21046;&#30340;&#26041;&#27861;&#65292;&#20351;&#20013;&#38388;&#28608;&#27963;&#21487;&#20197;&#36827;&#34892;&#21487;&#23398;&#20064;&#30340;&#33258;&#36866;&#24212;&#12290;&#25105;&#20204;&#23637;&#31034;&#20102;&#25105;&#20204;&#30340;&#26041;&#27861;&#26356;&#20934;&#30830;&#25429;&#33719;&#20102;&#19968;&#31995;&#21015;fuzz&#21644;&#21387;&#32553;&#23454;&#29616;&#30340;&#38271;&#36317;&#31163;&#20381;&#36182;&#20851;&#31995;&#65292;&#21253;&#25324;&#26102;&#38388;&#21644;&#39057;&#29575;&#39046;&#22495;&#30340;&#25351;&#26631;&#12290;&#25105;&#20204;&#25552;&#20379;&#20102;&#22768;&#38899;&#31034;&#20363;&#12290;
&lt;/p&gt;
&lt;p&gt;
Deep learning approaches for black-box modelling of audio effects have shown promise, however, the majority of existing work focuses on nonlinear effects with behaviour on relatively short time-scales, such as guitar amplifiers and distortion. While recurrent and convolutional architectures can theoretically be extended to capture behaviour at longer time scales, we show that simply scaling the width, depth, or dilation factor of existing architectures does not result in satisfactory performance when modelling audio effects such as fuzz and dynamic range compression. To address this, we propose the integration of time-varying feature-wise linear modulation into existing temporal convolutional backbones, an approach that enables learnable adaptation of the intermediate activations. We demonstrate that our approach more accurately captures long-range dependencies for a range of fuzz and compressor implementations across both time and frequency domain metrics. We provide sound examples, s
&lt;/p&gt;</description></item><item><title>&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CANDLE&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32593;&#32476;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#25991;&#21270;&#24120;&#35782;&#30693;&#35782;&#65292;&#20854;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;&#36825;&#20123;&#30693;&#35782;&#23545;&#20110;&#24773;&#22659;&#21270;&#20154;&#24037;&#26234;&#33021;&#21644;GPT-3&#35821;&#35328;&#27169;&#22411;&#37117;&#26377;&#22909;&#22788;&#12290;</title><link>http://arxiv.org/abs/2210.07763</link><description>&lt;p&gt;
&#22823;&#35268;&#27169;&#25552;&#21462;&#25991;&#21270;&#24120;&#35782;&#30693;&#35782;
&lt;/p&gt;
&lt;p&gt;
Extracting Cultural Commonsense Knowledge at Scale. (arXiv:2210.07763v3 [cs.CL] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.07763
&lt;/p&gt;
&lt;p&gt;
&#26412;&#31687;&#35770;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#21517;&#20026;CANDLE&#30340;&#26041;&#27861;&#65292;&#29992;&#20110;&#20174;&#32593;&#32476;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;&#25991;&#21270;&#24120;&#35782;&#30693;&#35782;&#65292;&#20854;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#12290;&#36825;&#20123;&#30693;&#35782;&#23545;&#20110;&#24773;&#22659;&#21270;&#20154;&#24037;&#26234;&#33021;&#21644;GPT-3&#35821;&#35328;&#27169;&#22411;&#37117;&#26377;&#22909;&#22788;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#32467;&#26500;&#21270;&#30693;&#35782;&#23545;&#20110;&#35768;&#22810;&#20154;&#24037;&#26234;&#33021;&#24212;&#29992;&#38750;&#24120;&#37325;&#35201;&#12290;&#24120;&#35782;&#30693;&#35782;&#23545;&#20110;&#24378;&#22823;&#30340;&#20197;&#20154;&#20026;&#20013;&#24515;&#30340;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#65292;&#20294;&#26159;&#24403;&#21069;&#21015;&#20986;&#30340;&#23569;&#25968;&#32467;&#26500;&#21270;&#24120;&#35782;&#39033;&#30446;&#32570;&#20047;&#26377;&#20851;&#31038;&#20250;&#25991;&#21270;&#32972;&#26223;&#19979;&#20154;&#31867;&#29305;&#24449;&#21644;&#34892;&#20026;&#30340;&#30693;&#35782;&#65292;&#36825;&#23545;&#20110;&#24773;&#22659;&#21270;&#20154;&#24037;&#26234;&#33021;&#33267;&#20851;&#37325;&#35201;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;CANDLE&#65292;&#19968;&#31181;&#29992;&#20110;&#22823;&#35268;&#27169;&#25552;&#21462;&#39640;&#36136;&#37327;&#25991;&#21270;&#24120;&#35782;&#30693;&#35782;&#65288;CCSK&#65289;&#30340;&#31471;&#21040;&#31471;&#26041;&#27861;&#12290;CANDLE&#20174;&#24222;&#22823;&#30340;&#32593;&#32476;&#35821;&#26009;&#24211;&#20013;&#25552;&#21462;CCSK&#26029;&#35328;&#65292;&#24182;&#23558;&#20854;&#32452;&#32455;&#25104;&#19968;&#33268;&#30340;&#32858;&#31867;&#65292;&#38024;&#23545;&#19977;&#20010;&#20027;&#39064;&#39046;&#22495;&#65288;&#22320;&#29702;&#65292;&#23447;&#25945;&#65292;&#32844;&#19994;&#65289;&#21644;&#20960;&#20010;&#25991;&#21270;&#26041;&#38754;&#36827;&#34892;&#20998;&#31867;&#65288;&#39135;&#29289;&#65292;&#39278;&#26009;&#65292;&#26381;&#35013;&#65292;&#20256;&#32479;&#65292;&#20202;&#24335;&#65292;&#34892;&#20026;&#65289;&#12290;CANDLE&#21253;&#25324;&#20998;&#31867;&#36807;&#28388;&#21644;&#36259;&#21619;&#24615;&#35780;&#20998;&#30340;&#23457;&#24910;&#25216;&#26415;&#12290;&#23454;&#39564;&#35780;&#20272;&#26174;&#31034;&#65292;CANDLE CCSK&#38598;&#21512;&#20248;&#20110;&#20043;&#21069;&#30340;&#24037;&#20316;&#65292;&#24182;&#19988;&#22806;&#37096;&#29992;&#20363;&#23637;&#31034;&#20102;CCSK&#23545;&#20110;GPT-3&#35821;&#35328;&#27169;&#22411;&#30340;&#22909;&#22788;&#12290;CANDLE&#30340;&#20195;&#30721;&#21644;&#25968;&#25454;&#26159;&#20844;&#24320;&#21487;&#29992;&#30340;&#12290;
&lt;/p&gt;
&lt;p&gt;
Structured knowledge is important for many AI applications. Commonsense knowledge, which is crucial for robust human-centric AI, is covered by a small number of structured knowledge projects. However, they lack knowledge about human traits and behaviors conditioned on socio-cultural contexts, which is crucial for situative AI. This paper presents CANDLE, an end-to-end methodology for extracting high-quality cultural commonsense knowledge (CCSK) at scale. CANDLE extracts CCSK assertions from a huge web corpus and organizes them into coherent clusters, for 3 domains of subjects (geography, religion, occupation) and several cultural facets (food, drinks, clothing, traditions, rituals, behaviors). CANDLE includes judicious techniques for classification-based filtering and scoring of interestingness. Experimental evaluations show the superiority of the CANDLE CCSK collection over prior works, and an extrinsic use case demonstrates the benefits of CCSK for the GPT-3 language model. Code and 
&lt;/p&gt;</description></item><item><title>Learnware&#33539;&#24335;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20805;&#20998;&#21033;&#29992;&#23567;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#36229;&#36234;&#21407;&#22987;&#30446;&#30340;&#30340;&#20107;&#24773;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38754;&#20020;&#30340;&#25968;&#25454;&#37327;&#22823;&#12289;&#35757;&#32451;&#38590;&#24230;&#39640;&#12289;&#25968;&#25454;&#23433;&#20840;&#31561;&#38382;&#39064;&#12290;</title><link>http://arxiv.org/abs/2210.03647</link><description>&lt;p&gt;
Learnware: &#23567;&#27169;&#22411;&#23454;&#29616;&#22823;&#20316;&#20026;
&lt;/p&gt;
&lt;p&gt;
Learnware: Small Models Do Big. (arXiv:2210.03647v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03647
&lt;/p&gt;
&lt;p&gt;
Learnware&#33539;&#24335;&#26088;&#22312;&#24110;&#21161;&#29992;&#25143;&#20805;&#20998;&#21033;&#29992;&#23567;&#22411;&#27169;&#22411;&#65292;&#23454;&#29616;&#36229;&#36234;&#21407;&#22987;&#30446;&#30340;&#30340;&#20107;&#24773;&#65292;&#20197;&#35299;&#20915;&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#38754;&#20020;&#30340;&#25968;&#25454;&#37327;&#22823;&#12289;&#35757;&#32451;&#38590;&#24230;&#39640;&#12289;&#25968;&#25454;&#23433;&#20840;&#31561;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#24403;&#21069;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#23384;&#22312;&#35757;&#32451;&#25968;&#25454;&#37327;&#22823;&#12289;&#35757;&#32451;&#25216;&#33021;&#39640;&#12289;&#36830;&#32493;&#23398;&#20064;&#38590;&#12289;&#36951;&#24536;&#39118;&#38505;&#22823;&#12289;&#25968;&#25454;&#38544;&#31169;&#21644;&#19987;&#26377;&#20449;&#24687;&#27844;&#38706;&#31561;&#38382;&#39064;&#65292;&#32780;&#36807;&#21435;&#30340;&#22823;&#27169;&#22411;&#33539;&#24335;&#34429;&#28982;&#22312;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#21644;&#35745;&#31639;&#26426;&#35270;&#35273;&#24212;&#29992;&#20013;&#21462;&#24471;&#20102;&#24778;&#20154;&#30340;&#32467;&#26524;&#65292;&#20294;&#24182;&#26410;&#35299;&#20915;&#36825;&#20123;&#38382;&#39064;&#65292;&#21453;&#32780;&#25104;&#20026;&#20005;&#37325;&#30340;&#30899;&#25490;&#25918;&#28304;&#12290;&#35813;&#25991;&#27010;&#36848;&#20102;Learnware&#33539;&#24335;&#65292;&#35753;&#29992;&#25143;&#19981;&#38656;&#35201;&#20174;&#22836;&#26500;&#24314;&#26426;&#22120;&#23398;&#20064;&#27169;&#22411;&#65292;&#24076;&#26395;&#21033;&#29992;&#23567;&#22411;&#27169;&#22411;&#21487;&#20197;&#23454;&#29616;&#36229;&#36234;&#21407;&#22987;&#30446;&#30340;&#30340;&#20107;&#24773;&#65292;&#20854;&#20013;&#20851;&#38190;&#26159;&#35268;&#33539;&#65292;&#21487;&#20197;&#20351;&#35757;&#32451;&#30340;&#27169;&#22411;&#24471;&#21040;&#20805;&#20998;&#37492;&#21035;&#12290;
&lt;/p&gt;
&lt;p&gt;
There are complaints about current machine learning techniques such as the requirement of a huge amount of training data and proficient training skills, the difficulty of continual learning, the risk of catastrophic forgetting, the leaking of data privacy/proprietary, etc. Most research efforts have been focusing on one of those concerned issues separately, paying less attention to the fact that most issues are entangled in practice. The prevailing big model paradigm, which has achieved impressive results in natural language processing and computer vision applications, has not yet addressed those issues, whereas becoming a serious source of carbon emissions. This article offers an overview of the learnware paradigm, which attempts to enable users not need to build machine learning models from scratch, with the hope of reusing small models to do things even beyond their original purposes, where the key ingredient is the specification which enables a trained model to be adequately identi
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;Stack Overflow&#20013;&#19982;Pandas&#20027;&#39064;&#35752;&#35770;&#30456;&#20851;&#30340;&#24086;&#23376;&#65292;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#65292;&#21457;&#29616;&#20102;Pandas&#25968;&#25454;&#25805;&#20316;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#31561;&#20027;&#39064;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#32780;&#25968;&#25454;&#28165;&#27927;&#21644;&#25968;&#25454;&#25366;&#25496;&#31561;&#20027;&#39064;&#30456;&#23545;&#36739;&#38590;&#12290;</title><link>http://arxiv.org/abs/2210.03519</link><description>&lt;p&gt;
&#24320;&#21457;&#32773;&#22914;&#20309;&#35752;&#35770;Pandas&#20027;&#39064;&#30340;&#23454;&#35777;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
An Empirical Study on How the Developers Discussed about Pandas Topics. (arXiv:2210.03519v2 [cs.SE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.03519
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#25910;&#38598;Stack Overflow&#20013;&#19982;Pandas&#20027;&#39064;&#35752;&#35770;&#30456;&#20851;&#30340;&#24086;&#23376;&#65292;&#36827;&#34892;&#20027;&#39064;&#24314;&#27169;&#65292;&#21457;&#29616;&#20102;Pandas&#25968;&#25454;&#25805;&#20316;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#31561;&#20027;&#39064;&#38750;&#24120;&#21463;&#27426;&#36814;&#65292;&#32780;&#25968;&#25454;&#28165;&#27927;&#21644;&#25968;&#25454;&#25366;&#25496;&#31561;&#20027;&#39064;&#30456;&#23545;&#36739;&#38590;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
Pandas&#26159;Python&#32534;&#31243;&#35821;&#35328;&#20013;&#29992;&#20110;&#25968;&#25454;&#20998;&#26512;&#30340;&#36719;&#20214;&#24211;&#12290;&#30001;&#20110;Pandas&#26159;&#19968;&#31181;&#24555;&#36895;&#12289;&#26131;&#20110;&#20351;&#29992;&#19988;&#24320;&#28304;&#30340;&#25968;&#25454;&#20998;&#26512;&#24037;&#20855;&#65292;&#22240;&#27492;&#23427;&#22312;&#36719;&#20214;&#24037;&#31243;&#39033;&#30446;&#65288;&#22914;&#36719;&#20214;&#24320;&#21457;&#12289;&#26426;&#22120;&#23398;&#20064;&#12289;&#35745;&#31639;&#26426;&#35270;&#35273;&#12289;&#33258;&#28982;&#35821;&#35328;&#22788;&#29702;&#12289;&#26426;&#22120;&#20154;&#25216;&#26415;&#31561;&#65289;&#20013;&#34987;&#24191;&#27867;&#20351;&#29992;&#12290;&#22240;&#27492;&#65292;&#36719;&#20214;&#24320;&#21457;&#32773;&#23545;Pandas&#34920;&#29616;&#20986;&#20102;&#24040;&#22823;&#30340;&#20852;&#36259;&#65292;&#24182;&#19988;&#29616;&#22312;&#22312;&#32447;&#24320;&#21457;&#32773;&#35770;&#22363;&#65288;&#22914;Stack Overflow&#65289;&#20013;&#30340;&#35752;&#35770;&#25968;&#37327;&#20063;&#36234;&#26469;&#36234;&#22810;&#12290;&#36825;&#20123;&#35752;&#35770;&#21487;&#20197;&#24110;&#21161;&#20102;&#35299;Pandas&#24211;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#65292;&#20063;&#21487;&#20197;&#24110;&#21161;&#20102;&#35299;Pandas&#20027;&#39064;&#30340;&#37325;&#35201;&#24615;&#12289;&#26222;&#21450;&#29575;&#21644;&#22256;&#38590;&#31243;&#24230;&#12290;&#26412;&#30740;&#31350;&#30340;&#20027;&#35201;&#30446;&#30340;&#26159;&#25214;&#21040;Pandas&#20027;&#39064;&#30340;&#21463;&#27426;&#36814;&#31243;&#24230;&#21644;&#22256;&#38590;&#31243;&#24230;&#12290;&#20026;&#27492;&#65292;&#25105;&#20204;&#25910;&#38598;&#20102;&#19982;Pandas&#20027;&#39064;&#35752;&#35770;&#30456;&#20851;&#30340;Stack Overflow&#24086;&#23376;&#65292;&#23545;&#24086;&#23376;&#30340;&#25991;&#26412;&#20869;&#23481;&#36827;&#34892;&#20102;&#20027;&#39064;&#24314;&#27169;&#12290;&#25105;&#20204;&#21457;&#29616;&#20102;26&#20010;&#20027;&#39064;&#65292;&#36827;&#19968;&#27493;&#23558;&#20854;&#20998;&#31867;&#20026;5&#20010;&#24191;&#27867;&#30340;&#31867;&#21035;&#12290;&#32467;&#26524;&#26174;&#31034;&#65292;&#26368;&#21463;&#27426;&#36814;&#30340;&#20027;&#39064;&#26159;&#19982;&#25968;&#25454;&#25805;&#20316;&#12289;&#25968;&#25454;&#20998;&#26512;&#21644;&#25968;&#25454;&#21487;&#35270;&#21270;&#30456;&#20851;&#30340;&#20027;&#39064;&#65292;&#32780;&#26368;&#22256;&#38590;&#30340;&#20027;&#39064;&#19982;&#25968;&#25454;&#28165;&#27927;&#21644;&#25968;&#25454;&#25366;&#25496;&#30456;&#20851;&#12290;
&lt;/p&gt;
&lt;p&gt;
Pandas is defined as a software library which is used for data analysis in Python programming language. As pandas is a fast, easy and open source data analysis tool, it is rapidly used in different software engineering projects like software development, machine learning, computer vision, natural language processing, robotics, and others. So a huge interests are shown in software developers regarding pandas and a huge number of discussions are now becoming dominant in online developer forums, like Stack Overflow (SO). Such discussions can help to understand the popularity of pandas library and also can help to understand the importance, prevalence, difficulties of pandas topics. The main aim of this research paper is to find the popularity and difficulty of pandas topics. For this regard, SO posts are collected which are related to pandas topic discussions. Topic modeling are done on the textual contents of the posts. We found 26 topics which we further categorized into 5 board categor
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#25506;&#32034;&#26080;&#32447;&#30005;&#39057;&#35889;&#27963;&#21160;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#31934;&#20934;&#30340;&#39057;&#35889;&#27963;&#21160;&#32858;&#31867;&#12290;</title><link>http://arxiv.org/abs/2210.02899</link><description>&lt;p&gt;
&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#29992;&#20110;&#26080;&#32447;&#30005;&#39057;&#35889;&#27963;&#21160;&#32858;&#31867;
&lt;/p&gt;
&lt;p&gt;
Self-supervised Learning for Clustering of Wireless Spectrum Activity. (arXiv:2210.02899v2 [cs.NI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2210.02899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#20351;&#29992;&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#25216;&#26415;&#25506;&#32034;&#26080;&#32447;&#30005;&#39057;&#35889;&#27963;&#21160;&#65292;&#27604;&#36739;&#20102;&#20004;&#31181;&#19981;&#21516;&#30340;&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#27169;&#22411;&#21644;&#19968;&#31181;&#28151;&#21512;&#27169;&#22411;&#65292;&#23454;&#29616;&#20102;&#31934;&#20934;&#30340;&#39057;&#35889;&#27963;&#21160;&#32858;&#31867;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36817;&#24180;&#26469;&#65292;&#36890;&#36807;&#26426;&#22120;&#23398;&#20064;&#25216;&#26415;&#22788;&#29702;&#26080;&#32447;&#30005;&#39057;&#35889;&#25968;&#25454;&#20197;&#35299;&#20915;&#35748;&#30693;&#26080;&#32447;&#30005;&#32593;&#32476;&#30456;&#20851;&#38382;&#39064;&#65292;&#22914;&#24322;&#24120;&#26816;&#27979;&#12289;&#35843;&#21046;&#20998;&#31867;&#12289;&#25216;&#26415;&#20998;&#31867;&#21644;&#35774;&#22791;&#25351;&#32441;&#31561;&#39046;&#22495;&#65292;&#21462;&#24471;&#20102;&#24456;&#22823;&#36827;&#23637;&#12290;&#22823;&#22810;&#25968;&#35299;&#20915;&#26041;&#26696;&#37117;&#26159;&#22522;&#20110;&#21463;&#25511;&#21046;&#30340;&#12289;&#24102;&#26631;&#31614;&#30340;&#25968;&#25454;&#65292;&#24182;&#20351;&#29992;&#30417;&#30563;&#24335;&#23398;&#20064;&#26041;&#27861;&#36827;&#34892;&#22788;&#29702;&#12290;&#28982;&#32780;&#65292;&#22312;&#29616;&#23454;&#19990;&#30028;&#29615;&#22659;&#19979;&#27979;&#37327;&#30340;&#39057;&#35889;&#25968;&#25454;&#39640;&#24230;&#19981;&#30830;&#23450;&#65292;&#20854;&#26631;&#35760;&#26159;&#19968;&#39033;&#36153;&#26102;&#19988;&#26114;&#36149;&#30340;&#36807;&#31243;&#65292;&#38656;&#35201;&#39046;&#22495;&#19987;&#19994;&#30693;&#35782;&#65292;&#22240;&#27492;&#25104;&#20026;&#22312;&#35813;&#39046;&#22495;&#20351;&#29992;&#30417;&#30563;&#24335;&#23398;&#20064;&#26041;&#27861;&#30340;&#20027;&#35201;&#32570;&#28857;&#20043;&#19968;&#12290;&#26412;&#25991;&#30740;&#31350;&#20102;&#22312;&#29616;&#23454;&#19990;&#30028;&#26410;&#26631;&#35760;&#25968;&#25454;&#20013;&#21033;&#29992;&#26080;&#20154;&#30417;&#30563;&#23398;&#20064;&#65288;SSL&#65289;&#26469;&#25506;&#32034;&#39057;&#35889;&#27963;&#21160;&#30340;&#20351;&#29992;&#12290;&#20855;&#20307;&#26469;&#35828;&#65292;&#25105;&#20204;&#27604;&#36739;&#20102;&#20004;&#31181; SSL &#27169;&#22411;&#30340;&#24615;&#33021;&#65292;&#19968;&#31181;&#22522;&#20110; DeepCluster &#21442;&#32771;&#20307;&#31995;&#32467;&#26500;&#65292;&#21478;&#19968;&#31181;&#36866;&#29992;&#20110;&#39057;&#35889;&#27963;&#21160;&#35782;&#21035;&#21644;&#32858;&#31867;&#65292;&#20197;&#21450;&#19968;&#31181;&#26032;&#22411;&#30340;&#28151;&#21512; SSL &#26041;&#27861;&#65292;&#32467;&#21512;&#20102;&#20004;&#31181;&#27169;&#22411;&#12290;&#25105;&#20204;&#30340;&#23454;&#39564;&#35780;&#20272;&#34920;&#26126;&#65292;&#36825;&#19977;&#31181;&#26041;&#27861;&#37117;&#21487;&#20197;&#26377;&#25928;&#22320;&#23545;&#39057;&#35889;&#27963;&#21160;&#25968;&#25454;&#36827;&#34892;&#32858;&#31867;&#65292;&#28151;&#21512;&#26041;&#27861;&#30340;&#24615;&#33021;&#26368;&#20339;&#12290;
&lt;/p&gt;
&lt;p&gt;
In recent years, much work has been done on processing of wireless spectrum data involving machine learning techniques in domain-related problems for cognitive radio networks, such as anomaly detection, modulation classification, technology classification and device fingerprinting. Most of the solutions are based on labeled data, created in a controlled manner and processed with supervised learning approaches. However, spectrum data measured in real-world environment is highly nondeterministic, making its labeling a laborious and expensive process, requiring domain expertise, thus being one of the main drawbacks of using supervised learning approaches in this domain. In this paper, we investigate the use of self-supervised learning (SSL) for exploring spectrum activities in a real-world unlabeled data. In particular, we compare the performance of two SSL models, one based on a reference DeepCluster architecture and one adapted for spectrum activity identification and clustering, and a 
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#27493;&#39588;&#30340;&#36827;&#21270;&#24335;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;FreeREA&#65292;&#21487;&#30452;&#25509;&#22312;&#30446;&#26631;&#30828;&#20214;&#19978;&#20248;&#21270;&#25628;&#32034;&#65292;&#19988;&#33021;&#22815;&#22312;&#24615;&#33021;&#26368;&#22823;&#21270;&#30340;&#21516;&#26102;&#65292;&#26497;&#22823;&#22320;&#20943;&#23567;&#20869;&#23384;&#21344;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#39640;&#25928;&#12290;</title><link>http://arxiv.org/abs/2207.05135</link><description>&lt;p&gt;
FreeREA: &#26080;&#38656;&#35757;&#32451;&#30340;&#36827;&#21270;&#24335;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;
&lt;/p&gt;
&lt;p&gt;
FreeREA: Training-Free Evolution-based Architecture Search. (arXiv:2207.05135v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.05135
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#20102;&#19968;&#31181;&#26080;&#38656;&#35757;&#32451;&#27493;&#39588;&#30340;&#36827;&#21270;&#24335;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;FreeREA&#65292;&#21487;&#30452;&#25509;&#22312;&#30446;&#26631;&#30828;&#20214;&#19978;&#20248;&#21270;&#25628;&#32034;&#65292;&#19988;&#33021;&#22815;&#22312;&#24615;&#33021;&#26368;&#22823;&#21270;&#30340;&#21516;&#26102;&#65292;&#26497;&#22823;&#22320;&#20943;&#23567;&#20869;&#23384;&#21344;&#29992;&#12290;&#23454;&#39564;&#32467;&#26524;&#35777;&#26126;&#20854;&#27604;&#25163;&#21160;&#35774;&#35745;&#26356;&#39640;&#25928;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#22312;&#36807;&#21435;&#30340;&#21313;&#24180;&#20013;&#65292;&#26426;&#22120;&#23398;&#20064;&#39046;&#22495;&#30340;&#22823;&#37096;&#20998;&#30740;&#31350;&#37117;&#26159;&#20026;&#20102;&#25913;&#36827;&#29616;&#26377;&#27169;&#22411;&#65292;&#20197;&#22686;&#21152;&#31070;&#32463;&#32593;&#32476;&#22312;&#21508;&#31181;&#19981;&#21516;&#20219;&#21153;&#30340;&#35299;&#20915;&#26041;&#26696;&#20013;&#30340;&#24615;&#33021;&#12290;&#28982;&#32780;&#65292;&#36825;&#26679;&#30340;&#36827;&#27493;&#24448;&#24448;&#20197;&#22686;&#21152;&#27169;&#22411;&#20869;&#23384;&#21644;&#35745;&#31639;&#38656;&#27714;&#30340;&#20195;&#20215;&#20026;&#20195;&#20215;&#12290;&#36825;&#23545;&#20110;&#30740;&#31350;&#25104;&#26524;&#22312;&#23454;&#38469;&#29615;&#22659;&#20013;&#30340;&#37096;&#32626;&#20855;&#26377;&#30456;&#24403;&#22823;&#30340;&#38480;&#21046;&#24615;&#65292;&#20854;&#20013;&#25104;&#26412;&#12289;&#33021;&#28304;&#28040;&#32791;&#21644;&#26694;&#26550;&#30340;&#22797;&#26434;&#24615;&#21457;&#25381;&#30528;&#33267;&#20851;&#37325;&#35201;&#30340;&#20316;&#29992;&#12290;&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#26032;&#39062;&#30340;&#36827;&#21270;&#24335;&#26550;&#26500;&#25628;&#32034;&#31639;&#27861;FreeREA&#65292;&#36890;&#36807;&#35813;&#31639;&#27861;&#21487;&#20197;&#24555;&#36895;&#35782;&#21035;&#24615;&#33021;&#26368;&#22823;&#21270;&#19988;&#20869;&#23384;&#21344;&#29992;&#26368;&#23567;&#21270;&#30340;&#31070;&#32463;&#32593;&#32476;&#65292;&#32780;&#26080;&#38656;&#35757;&#32451;&#27493;&#39588;&#65292;&#24182;&#22312;&#30446;&#26631;&#30828;&#20214;&#19978;&#30452;&#25509;&#20248;&#21270;&#26550;&#26500;&#25628;&#32034;&#65292;&#26080;&#38656;&#20195;&#29702;&#25351;&#26631;&#12290;&#23454;&#39564;&#32467;&#26524;&#34920;&#26126;&#20102;FreeREA&#30340;&#26377;&#25928;&#24615;&#65292;&#23427;&#33021;&#22815;&#22312;&#38656;&#35201;&#23569;&#36798;487&#20493;&#35745;&#31639;&#36164;&#28304;&#30340;&#24773;&#20917;&#19979;&#65292;&#32988;&#36807;&#26368;&#20808;&#36827;&#30340;&#25163;&#21160;&#35774;&#35745;&#26550;&#26500;&#12290;
&lt;/p&gt;
&lt;p&gt;
In the last decade, most research in Machine Learning contributed to the improvement of existing models, with the aim of increasing the performance of neural networks for the solution of a variety of different tasks. However, such advancements often come at the cost of an increase of model memory and computational requirements. This represents a significant limitation for the deployability of research output in realistic settings, where the cost, the energy consumption, and the complexity of the framework play a crucial role. To solve this issue, the designer should search for models that maximise the performance while limiting its footprint. Typical approaches to reach this goal rely either on manual procedures, which cannot guarantee the optimality of the final design, or upon Neural Architecture Search algorithms to automatise the process, at the expenses of extremely high computational time. This paper provides a solution for the fast identification of a neural network that maximis
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#21644;&#32467;&#26500;&#31283;&#23450;&#24615;&#20026;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#25351;&#23450;&#20102;&#26368;&#22823;&#31283;&#23450;&#20998;&#27495;&#35299;&#25968;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;</title><link>http://arxiv.org/abs/2207.04876</link><description>&lt;p&gt;
&#22522;&#20110;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21644;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#30340;&#27867;&#21270;&#30740;&#31350;
&lt;/p&gt;
&lt;p&gt;
On the Generalization of Spiking Neural Networks via Minimum Description Length and Structural Stability. (arXiv:2207.04876v2 [cs.NE] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2207.04876
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#21644;&#32467;&#26500;&#31283;&#23450;&#24615;&#20026;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#20102;&#19968;&#20010;&#26126;&#30830;&#30340;&#27867;&#21270;&#30028;&#38480;&#65292;&#24182;&#25351;&#23450;&#20102;&#26368;&#22823;&#31283;&#23450;&#20998;&#27495;&#35299;&#25968;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#36807;&#21435;&#20960;&#21313;&#24180;&#20013;&#65292;&#30001;&#20110;&#20854;&#23545;&#20110;&#24314;&#27169;&#26102;&#38388;&#30456;&#20851;&#25968;&#25454;&#30340;&#28508;&#21147;&#65292;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#24341;&#36215;&#20102;&#36234;&#26469;&#36234;&#22810;&#30340;&#20851;&#27880;&#12290;&#35768;&#22810;&#32463;&#39564;&#31639;&#27861;&#21644;&#25216;&#26415;&#24050;&#32463;&#34987;&#24320;&#21457;&#20986;&#26469;&#12290;&#28982;&#32780;&#65292;&#20174;&#29702;&#35770;&#19978;&#35762;&#65292;&#35757;&#32451;&#21518;&#30340;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#22312;&#26410;&#35265;&#25968;&#25454;&#19978;&#30340;&#34920;&#29616;&#20173;&#28982;&#26159;&#26410;&#30693;&#30340;&#12290;&#26412;&#30740;&#31350;&#36890;&#36807;&#21033;&#29992;&#26368;&#23567;&#25551;&#36848;&#38271;&#24230;&#21407;&#21017;&#65292;&#20026;&#33033;&#20914;&#31070;&#32463;&#32593;&#32476;&#25552;&#20379;&#19968;&#20010;&#26126;&#30830;&#30340;&#27867;&#21270;&#30028;&#38480;&#12290;&#27492;&#22806;&#65292;&#25105;&#20204;&#36890;&#36807;&#32467;&#26500;&#31283;&#23450;&#24615;&#23454;&#26045;&#20102;SNN&#30340;&#25551;&#36848;&#38271;&#24230;&#65292;&#24182;&#25351;&#23450;&#20102;&#26368;&#22823;&#31283;&#23450;&#20998;&#27495;&#35299;&#25968;&#30340;&#19979;&#38480;&#21644;&#19978;&#38480;&#65292;&#23558;&#22312;SNN&#20013;&#30830;&#23450;&#32467;&#26500;&#31283;&#23450;&#24615;&#30340;&#25361;&#25112;&#36716;&#21270;&#20026;&#19968;&#20010;&#20855;&#26377;&#23450;&#37327;&#29305;&#24615;&#30340;&#25968;&#23398;&#38382;&#39064;&#12290;
&lt;/p&gt;
&lt;p&gt;
The past decades have witnessed an increasing interest in spiking neural networks due to their great potential of modeling time-dependent data. Many empirical algorithms and techniques have been developed. However, theoretically, it remains unknown whether and to what extent a trained spiking neural network performs well on unseen data. This work takes one step in this direction by exploiting the minimum description length principle and thus, presents an explicit generalization bound for spiking neural networks. Further, we implement the description length of SNNs through structural stability and specify the lower and upper bounds of the maximum number of stable bifurcation solutions, which convert the challenge of qualifying structural stability in SNNs into a mathematical problem with quantitative properties.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30830;&#23450;&#24615;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#31616;&#27905;&#20989;&#25968;&#34920;&#31034;&#26041;&#27861;FABE&#65292;&#21487;&#20197;&#26356;&#24555;&#22320;&#22788;&#29702;&#22270;&#27169;&#22411;&#20013;&#30340;&#31934;&#30830;&#26368;&#21487;&#33021;&#35299;&#21644;&#32422;&#26463;&#20248;&#21270;&#20219;&#21153;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;FABE&#36890;&#24120;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#23548;&#33268;&#26174;&#30528;&#30340;&#36816;&#34892;&#26102;&#38388;&#25913;&#36827;&#65288;&#39640;&#36798;5&#20010;&#25968;&#37327;&#32423;&#65289;&#12290;</title><link>http://arxiv.org/abs/2108.03899</link><description>&lt;p&gt;
&#22522;&#20110;&#30830;&#23450;&#24615;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#31934;&#30830;&#26368;&#21487;&#33021;&#35299;&#21644;&#32422;&#26463;&#20248;&#21270;&#30340;&#26356;&#24555;&#26041;&#27861;
&lt;/p&gt;
&lt;p&gt;
Faster Exact MPE and Constrained Optimization with Deterministic Finite State Automata. (arXiv:2108.03899v3 [cs.AI] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2108.03899
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30830;&#23450;&#24615;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#31616;&#27905;&#20989;&#25968;&#34920;&#31034;&#26041;&#27861;FABE&#65292;&#21487;&#20197;&#26356;&#24555;&#22320;&#22788;&#29702;&#22270;&#27169;&#22411;&#20013;&#30340;&#31934;&#30830;&#26368;&#21487;&#33021;&#35299;&#21644;&#32422;&#26463;&#20248;&#21270;&#20219;&#21153;&#12290;&#22312;&#22522;&#20934;&#27979;&#35797;&#20013;&#65292;FABE&#36890;&#24120;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#23548;&#33268;&#26174;&#30528;&#30340;&#36816;&#34892;&#26102;&#38388;&#25913;&#36827;&#65288;&#39640;&#36798;5&#20010;&#25968;&#37327;&#32423;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#30830;&#23450;&#24615;&#26377;&#38480;&#29366;&#24577;&#33258;&#21160;&#26426;&#30340;&#31616;&#27905;&#20989;&#25968;&#34920;&#31034;&#65292;&#29992;&#20110;&#35299;&#20915;&#22270;&#27169;&#22411;&#20013;&#30340;&#31934;&#30830;&#26368;&#21487;&#33021;&#35299;&#21644;&#32422;&#26463;&#20248;&#21270;&#20219;&#21153;&#12290;&#28982;&#21518;&#25105;&#20204;&#22312;Bucket&#28040;&#38500;&#65288;BE&#65289;&#20013;&#21033;&#29992;&#25105;&#20204;&#31616;&#27905;&#30340;&#34920;&#31034;&#26041;&#27861;&#65292;&#25105;&#20204;&#23558;&#25105;&#20204;&#30340;BE&#29256;&#26412;&#31216;&#20026;FABE&#12290;&#36890;&#36807;&#26368;&#23567;&#21270;&#20887;&#20313;&#65292;FABE&#26174;&#33879;&#25913;&#21892;&#20102;BE&#22312;&#36816;&#34892;&#26102;&#38388;&#21644;&#20869;&#23384;&#38656;&#27714;&#26041;&#38754;&#30340;&#24615;&#33021;&#12290;&#22312;&#26368;&#21487;&#33021;&#35299;&#21644;&#21152;&#26435;&#32422;&#26463;&#28385;&#36275;&#22522;&#20934;&#27979;&#35797;&#20013;&#30340;&#32467;&#26524;&#34920;&#26126;&#65292;FABE&#36890;&#24120;&#20248;&#20110;&#29616;&#26377;&#25216;&#26415;&#65292;&#23548;&#33268;&#26174;&#30528;&#30340;&#36816;&#34892;&#26102;&#38388;&#25913;&#36827;&#65288;&#22312;&#25105;&#20204;&#30340;&#27979;&#35797;&#20013;&#39640;&#36798;5&#20010;&#25968;&#37327;&#32423;&#65289;&#12290;
&lt;/p&gt;
&lt;p&gt;
We propose a concise function representation based on deterministic finite state automata for exact most probable explanation and constrained optimization tasks in graphical models. We then exploit our concise representation within Bucket Elimination (BE). We denote our version of BE as FABE. FABE significantly improves the performance of BE in terms of runtime and memory requirements by minimizing redundancy. Results on most probable explanation and weighted constraint satisfaction benchmarks show that FABE often outperforms the state of the art, leading to significant runtime improvements (up to 5 orders of magnitude in our tests).
&lt;/p&gt;</description></item><item><title>&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#38544;&#34255;&#23618;&#23454;&#29616;&#23545;&#21151;&#33021;&#21709;&#24212;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#27169;&#22411;&#25311;&#21512;&#31574;&#30053;&#65288;FDNN&#21644;FBNN&#65289;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#25216;&#26415;&#24471;&#21040;&#26356;&#21152;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;</title><link>http://arxiv.org/abs/2107.14151</link><description>&lt;p&gt;
&#29616;&#20195;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65306;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;
&lt;/p&gt;
&lt;p&gt;
Modern Non-Linear Function-on-Function Regression. (arXiv:2107.14151v1 [stat.ME] CROSS LISTED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2107.14151
&lt;/p&gt;
&lt;p&gt;
&#26412;&#30740;&#31350;&#25552;&#20986;&#19968;&#31181;&#21033;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#65292;&#36890;&#36807;&#36830;&#32493;&#38544;&#34255;&#23618;&#23454;&#29616;&#23545;&#21151;&#33021;&#21709;&#24212;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#27169;&#22411;&#25311;&#21512;&#31574;&#30053;&#65288;FDNN&#21644;FBNN&#65289;&#65292;&#24182;&#36890;&#36807;&#27491;&#21017;&#21270;&#25216;&#26415;&#24471;&#21040;&#26356;&#21152;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#35770;&#25991;&#24341;&#20837;&#20102;&#19968;&#31181;&#26032;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#22238;&#24402;&#27169;&#22411;&#31867;&#65292;&#20351;&#29992;&#31070;&#32463;&#32593;&#32476;&#20998;&#26512;&#21151;&#33021;&#25968;&#25454;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#19968;&#20010;&#26694;&#26550;&#65292;&#20351;&#29992;&#30001;&#36830;&#32493;&#31070;&#32463;&#20803;&#32452;&#25104;&#30340;&#38544;&#34255;&#23618;&#65292;&#31216;&#20026;&#36830;&#32493;&#38544;&#34255;&#23618;&#65292;&#29992;&#20110;&#21151;&#33021;&#21709;&#24212;&#24314;&#27169;&#65292;&#24182;&#25552;&#20379;&#20102;&#20004;&#31181;&#27169;&#22411;&#25311;&#21512;&#31574;&#30053;&#65306;&#21151;&#33021;&#30452;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FDNN&#65289;&#21644;&#21151;&#33021;&#22522;&#30784;&#31070;&#32463;&#32593;&#32476;&#65288;FBNN&#65289;&#12290;&#36825;&#20004;&#31181;&#26041;&#27861;&#37117;&#26159;&#19987;&#38376;&#35774;&#35745;&#26469;&#21033;&#29992;&#21151;&#33021;&#25968;&#25454;&#22266;&#26377;&#30340;&#32467;&#26500;&#65292;&#24182;&#25429;&#25417;&#21151;&#33021;&#39044;&#27979;&#21464;&#37327;&#21644;&#21151;&#33021;&#21709;&#24212;&#21464;&#37327;&#20043;&#38388;&#23384;&#22312;&#30340;&#22797;&#26434;&#20851;&#31995;&#12290;&#25105;&#20204;&#36890;&#36807;&#27714;&#35299;&#20989;&#25968;&#26799;&#24230;&#24182;&#23454;&#26045;&#27491;&#21017;&#21270;&#25216;&#26415;&#36827;&#34892;&#27169;&#22411;&#25311;&#21512;&#65292;&#24471;&#21040;&#26356;&#31616;&#26126;&#30340;&#32467;&#26524;&#12290;&#25105;&#20204;&#36890;&#36807;&#24191;&#27867;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#23637;&#31034;&#20102;&#25105;&#20204;&#25552;&#20986;&#30340;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#21151;&#33021;&#27169;&#22411;&#26041;&#38754;&#30340;&#24378;&#22823;&#28789;&#27963;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new class of non-linear function-on-function regression models for functional data using neural networks. We propose a framework using a hidden layer consisting of continuous neurons, called a continuous hidden layer, for functional response modeling and give two model fitting strategies, Functional Direct Neural Network (FDNN) and Functional Basis Neural Network (FBNN). Both are designed explicitly to exploit the structure inherent in functional data and capture the complex relations existing between the functional predictors and the functional response. We fit these models by deriving functional gradients and implement regularization techniques for more parsimonious results. We demonstrate the power and flexibility of our proposed method in handling complex functional models through extensive simulation studies as well as real data examples.
&lt;/p&gt;</description></item><item><title>&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#12289;&#36866;&#29992;&#20110;&#20989;&#25968;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21464;&#20307;&#65292;&#26088;&#22312;&#26174;&#24335;&#21033;&#29992;&#20989;&#25968;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#20840;&#38754;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;</title><link>http://arxiv.org/abs/2104.09371</link><description>&lt;p&gt;
&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#38750;&#32447;&#24615;&#20989;&#25968;&#24314;&#27169;
&lt;/p&gt;
&lt;p&gt;
Non-linear Functional Modeling using Neural Networks. (arXiv:2104.09371v2 [cs.LG] UPDATED)
&lt;/p&gt;
&lt;p&gt;
http://arxiv.org/abs/2104.09371
&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#25552;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#12289;&#36866;&#29992;&#20110;&#20989;&#25968;&#25968;&#25454;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#27169;&#22411;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21464;&#20307;&#65292;&#26088;&#22312;&#26174;&#24335;&#21033;&#29992;&#20989;&#25968;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#32467;&#26500;&#65292;&#24182;&#36890;&#36807;&#20840;&#38754;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#35777;&#26126;&#20102;&#35813;&#26041;&#27861;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;

&lt;/p&gt;
&lt;p&gt;
&#26412;&#25991;&#20171;&#32461;&#20102;&#19968;&#31181;&#22522;&#20110;&#31070;&#32463;&#32593;&#32476;&#30340;&#26032;&#22411;&#38750;&#32447;&#24615;&#20989;&#25968;&#25968;&#25454;&#27169;&#22411;&#12290;&#28145;&#24230;&#23398;&#20064;&#22312;&#38750;&#32447;&#24615;&#24314;&#27169;&#26041;&#38754;&#38750;&#24120;&#25104;&#21151;&#65292;&#20294;&#22312;&#20989;&#25968;&#25968;&#25454;&#35774;&#32622;&#26041;&#38754;&#21364;&#24456;&#23569;&#26377;&#30740;&#31350;&#12290;&#25105;&#20204;&#25552;&#20986;&#20102;&#20004;&#31181;&#21464;&#20307;&#65306;&#19968;&#31181;&#26159;&#20855;&#26377;&#36830;&#32493;&#38544;&#34255;&#23618;&#30340;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#65292;&#31216;&#20026;&#20989;&#25968;&#30452;&#25509;&#31070;&#32463;&#32593;&#32476;&#65288;FDNN&#65289;&#65292;&#21478;&#19968;&#31181;&#21017;&#21033;&#29992;&#22522;&#25193;&#23637;&#21644;&#36830;&#32493;&#38544;&#34255;&#23618;&#65292;&#31216;&#20026;&#22522;&#20989;&#25968;&#31070;&#32463;&#32593;&#32476;&#65288;FBNN&#65289;&#12290;&#20004;&#31181;&#21464;&#20307;&#37117;&#26159;&#35774;&#35745;&#29992;&#26469;&#26174;&#24335;&#21033;&#29992;&#20989;&#25968;&#25968;&#25454;&#20013;&#22266;&#26377;&#30340;&#32467;&#26500;&#12290;&#20026;&#20102;&#25311;&#21512;&#36825;&#20123;&#27169;&#22411;&#65292;&#25105;&#20204;&#23548;&#20986;&#20102;&#19968;&#31181;&#22522;&#20110;&#20989;&#25968;&#26799;&#24230;&#30340;&#20248;&#21270;&#31639;&#27861;&#12290;&#25105;&#20204;&#36890;&#36807;&#20840;&#38754;&#30340;&#27169;&#25311;&#30740;&#31350;&#21644;&#23454;&#38469;&#25968;&#25454;&#31034;&#20363;&#23637;&#31034;&#20102;&#25152;&#25552;&#20986;&#26041;&#27861;&#22312;&#22788;&#29702;&#22797;&#26434;&#20989;&#25968;&#27169;&#22411;&#26041;&#38754;&#30340;&#26377;&#25928;&#24615;&#12290;
&lt;/p&gt;
&lt;p&gt;
We introduce a new class of non-linear models for functional data based on neural networks. Deep learning has been very successful in non-linear modeling, but there has been little work done in the functional data setting. We propose two variations of our framework: a functional neural network with continuous hidden layers, called the Functional Direct Neural Network (FDNN), and a second version that utilizes basis expansions and continuous hidden layers, called the Functional Basis Neural Network (FBNN). Both are designed explicitly to exploit the structure inherent in functional data. To fit these models we derive a functional gradient based optimization algorithm. The effectiveness of the proposed methods in handling complex functional models is demonstrated by comprehensive simulation studies and real data examples.
&lt;/p&gt;</description></item></channel></rss>